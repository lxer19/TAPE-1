URL: http://www.cs.indiana.edu/l/www/pub/liu/Thesis-TR95.ps.Z
Refering-URL: http://www.cs.indiana.edu/l/www/pub/liu/
Root-URL: http://www.cs.indiana.edu
Title: INCREMENTAL COMPUTATION: A SEMANTICS-BASED SYSTEMATIC TRANSFORMATIONAL APPROACH  
Author: Yanhong Annie Liu 
Degree: A Dissertation Presented to the Faculty of the Graduate School  in Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy by  
Date: January 1996  
Affiliation: of Cornell University  
Abstract-found: 0
Intro-found: 1
Reference: [ACK81] <author> F. E. Allen, John Cocke, and Ken Kennedy. </author> <title> Reduction of operator strength. </title> <editor> In Steven S. Muchnick and Neil D. Jones, editors, </editor> <title> Program Flow Analysis, </title> <booktitle> chapter 3, </booktitle> <pages> pages 79-101. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: This information needs to be maintained efficiently as well. Some approaches to incremental computation have exploited specific kinds of auxiliary information, e.g., auxiliary arithmetic associated with some classical strength-reduction rules <ref> [ACK81] </ref>, dynamic mappings maintained by finite differencing rules for aggregate primitives in SETL [PK82] and INC [YS91], and auxiliary data structures for problems with certain properties like stable decomposition [PT89]. However, until now, systematic discovery of auxiliary information for arbitrary programs has not been studied. <p> For static incremental attribute evaluation algorithms [Kas80,Kat84], where no auxiliary information is needed, the approach can cache intermediate results and maintain them automatically, as described in Chapter 4. Strength reduction <ref> [All69, CK77, ACK81, SKR91] </ref> is a traditional compiler optimization technique that aims at computing each iteration incrementally based on the result of the previous iteration. Basically, a fixed set of strength-reduction rules for primitive operators like times and plus are used.
Reference: [AHR + 90] <author> B. Alpern, R. Hoover, B. Rosen, P. Sweeney, and K. Zadeck. </author> <title> Incremental evaluation of computational circuits. </title> <booktitle> In Proceedings of the 1st Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 32-42, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: Examples include incremental parsing [GM79,JG82], incremental attribute evaluation [RTD83,Yeh83,YK88,LMOW88, Jon90], incremental data-flow analysis [Zad84,RP88,Bur90,MR90,RR94], incremental circuit evaluation <ref> [AHR + 90] </ref>, and incremental constraint solving [Van88,FMB90]. The study of dynamic graph algorithms, such as transitive closure algorithms [Yel93], can be viewed as falling into this class. These incremental algorithms are called explicit incremental algorithms by Pugh [Pug88b] and ad hoc incremental algorithms by Field [Fie91].
Reference: [AHU74] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1974. </year>
Reference-contexts: We call this schema-based integrated caching. A nice survey of most of these ideas can be found in [Bir80], following which some uses of the word "tabulation" mainly refer to techniques in this class [Par90]. Typical examples of these techniques are dynamic programming <ref> [AHU74] </ref>, schemas of redundancies [Coh83], and tupling [Pet84,Pet87,Chi93,CK93]. Dynamic programming applies to problems that can be divided into subproblems and solved from small subproblems to larger ones by storing and using results of smaller ones.
Reference: [ALBB92] <author> Toshiro Wakayama Allen L. Brown, Jr. and Howard A. Blair. </author> <title> A reconstruction of context-dependent document processing in SGML. </title> <booktitle> In Proceedins of EP '92: the International Conference on Electronic Publishing, Document Manipulation and Typography, </booktitle> <year> 1992. </year>
Reference: [All69] <author> Frances E. Allen. </author> <title> Program optimization. </title> <booktitle> In Annual Review of Automatic Programming, </booktitle> <volume> volume 5, </volume> <pages> pages 239-307. </pages> <publisher> Pergamon: </publisher> <address> Elmsford, New York, </address> <year> 1969. </year>
Reference-contexts: For static incremental attribute evaluation algorithms [Kas80,Kat84], where no auxiliary information is needed, the approach can cache intermediate results and maintain them automatically, as described in Chapter 4. Strength reduction <ref> [All69, CK77, ACK81, SKR91] </ref> is a traditional compiler optimization technique that aims at computing each iteration incrementally based on the result of the previous iteration. Basically, a fixed set of strength-reduction rules for primitive operators like times and plus are used.
Reference: [AN91] <author> Shail Aditya and Rishiyur S. Nikhil. </author> <title> Incremental polymorphism. </title> <booktitle> In Proceedings of the 5th ACM Conference on FPCA, volume 523 of Lecture Notes in Computer Science, </booktitle> <pages> pages 379-405, </pages> <address> Cambridge, Mas-sachusetts, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag, Berlin. </note>
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. Addison-Wesley Series in Computer Science. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference: [BC88] <author> P. Borras and D. Clement. </author> <title> CENTAUR: The system. </title> <booktitle> In Proceedings of the ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <pages> pages 14-24, </pages> <address> Boston, Massachusetts, </address> <month> November </month> <year> 1988. </year> <note> Published as SIGPLAN Notices, 24(2). </note>
Reference: [BD77] <author> R. M. Burstall and John Darlington. </author> <title> A transformation system for developing recursive programs. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 44-67, </pages> <month> Jan-uary </month> <year> 1977. </year> <pages> 115 116 </pages>
Reference-contexts: Then, we obtain a definition of f 0 by the following three steps. First, we unfold <ref> [BD77] </ref> (also called expand [Weg76]) the application. Second, we incrementalize the unfolded application. <p> Fourth, a global definition set is maintained and used to replace function applications, with corresponding relevant cache elements and valid context information, by applications of introduced functions. Function introduction with generalization and function replacement use the unfold/define/fold scheme <ref> [BD77] </ref> in a regulated manner so that the transformations are deterministic and the derived programs converge if the originals do. <p> The integrated computation is usually more efficient; so is its incremental version. We do not describe the integration in detail. Basically, it uses traditional transformation techniques <ref> [BD77] </ref> like those used in tupling tactic [Fea82] and partial evaluation [JGS93]: introducing functions to compute function applications, unfolding, 83 simplifying primitive function applications, driving, replacing recursive applications with introduced functions, etc.
Reference: [BD91] <author> Anders Bondorf and Olivier Danvy. </author> <title> Automatic autoprojection of recursive equations with global variables and abstract data types. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 16 </volume> <pages> 151-195, </pages> <year> 1991. </year>
Reference: [Ber92] <author> Arthur Michael Berman. </author> <title> Lower and Upper Bounds for Incremental Algorithms. </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science, Rut-gers University, </institution> <address> New Brunswick, New Jersey, </address> <month> October </month> <year> 1992. </year>
Reference: [BG94] <author> John Boyland and Susan L. Graham. </author> <title> Composing tree attributions. </title> <booktitle> In Conference Record of the 21th Annual ACM Symposium on POPL, </booktitle> <pages> pages 375-388, </pages> <address> Portland, Oregon, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Modular attribute evaluation. Heavy program transformations are usually composed of separate phases, where each phase conducts relevant, and often different and smaller, program analyses and performs lighter transformations. Several approaches have been proposed for modular attribute specification <ref> [GG84,DC90,Far92,FMY92, BG94] </ref> and modular attribute evaluation [GG84,FMY92]. Modular specification improves the readability of attribute grammars and allows more convenient modular evaluation. Modular evaluation provides the flexibility of turning on and off attribution modules as necessary, which results in evaluators that are speedier and more storage-efficient [FMY92]. <p> Attribution has been traditionally used in programming environments for code generation [RT88]; it has also been proposed for general program transformation, including phase-based transformation. Approaches include attribute coupled grammars [GG84], higher-order attribute grammars [VSK89], composable attribute grammars [FMY92], and simple tree attributions <ref> [BG94] </ref>. Incremental attribute evaluation algorithms for these frameworks could be used for incremental code generation. However, these frameworks do not address external input. We need to extend them and study how annotations should be incorporated with incremental attribute evaluation for incremental program transformation. 110 Programming effort.
Reference: [BGV92] <author> Robert A. Ballance, Susan L. Graham, and Michael L. Van De Van-ter. </author> <title> The Pan language-based editing system. </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> 1(1) </volume> <pages> 95-127, </pages> <month> January </month> <year> 1992. </year>
Reference: [Bir80] <author> Richard S. Bird. </author> <title> Tabulation techniques for recursive programs. </title> <journal> ACM Computing Surveys, </journal> <volume> 12(4) </volume> <pages> 403-417, </pages> <month> December </month> <year> 1980. </year>
Reference-contexts: Here we compare our work with related work in program improvement using caching techniques. Caching has been the basis of many techniques for developing efficient programs and optimizing programs. Bird <ref> [Bir80] </ref> and Cohen [Coh83] provide nice overviews. Most of these techniques fall into one of the following three classes. Separate caching. <p> In the first class, a global cache separate from a subject program is employed to record values of subcomputations that may be needed later, and certain strategies are chosen for using and managing the cache. We call this technique separate caching. It corresponds to the "exact tabulation" in <ref> [Bir80] </ref> and the "large-table method" in [Coh83]. The initial idea of memoization, "memo" functions, proposed by Michie [Mic68], belongs to this class. <p> The pros and cons of separate caching are well discussed by Bird <ref> [Bir80] </ref> and Cohen [Coh83]. To summarize, the idea is simple, and the subject programs are basically unchanged. But the caching methods are dynamic, and thus are fundamentally interpretive. <p> Techniques in the second class apply transformations based on special properties and schemas of subject programs. We call this schema-based integrated caching. A nice survey of most of these ideas can be found in <ref> [Bir80] </ref>, following which some uses of the word "tabulation" mainly refer to techniques in this class [Par90]. Typical examples of these techniques are dynamic programming [AHU74], schemas of redundancies [Coh83], and tupling [Pet84,Pet87,Chi93,CK93].
Reference: [Bir84] <author> Richard S. Bird. </author> <title> The promotion and accumulation strategies in transformational programming. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(4) </volume> <pages> 487-504, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: on inductive variables [FU76,Fon77,Fon79], by Paige, Schwartz, and Koenig on finite differencing [PS77,Pai81,PK82], by Dijkstra, Gries, and Reynolds [Dij76,Gri81,Rey81,Gri84] on maintaining and strengthening loop invariants, by Boyle, Moore, Manna, and Waldinger on induction, generalization, and deductive synthesis [BM79,MW80,MW93], by Dershowitz on extension techniques [Der83], by Bird on promotion and accumulation <ref> [Bir84, Bir85] </ref>, by Broy, Bauer, Partsch, etc. on transforming recursive functional programs in CIP [Bro84,BMPP89, Par90], by Smith on finite differencing of functional programs in KIDS [Smi90,Smi91], as well as the work pioneered by Michie on memoization [Mic68,Bir80,Coh83,Web95]. <p> Following our systematic approach, we even discover that an extra shift is done in [OLHA94]. Thus, such systematic transformational approach is not only greatly desired for automating designs and guaranteeing correctness, but also also for helping reduce the cost. 5.4.2 Path sequence problem This example is from <ref> [Bir84] </ref>. Given a directed acyclic graph, and a string whose elements are vertices in the graph, the problem is to compute the length of the longest subsequence in the string that forms a path in the graph. <p> We focus on the second half of the example, where an exponential-time recursive solution is improved (incorrectly in <ref> [Bir84] </ref>, correctly in [Bir85]). A function llp is defined to compute the desired length, as below. <p> Thus, promotion can be regarded as deriving incremental programs, and accumulation as identifying appropriate intermediate results or auxiliary information. Bird used two nice examples to illustrate the general strategies, with the help of succinct notations. However, we can discern no systematic steps being followed in <ref> [Bir84] </ref>. As demonstrated with the path sequence problem, our approach can be regarded as a systematic formulation of the promotion and accumulation strategies. As such, it helps avoid the kind of errors reported and corrected in [Bir85].
Reference: [Bir85] <author> Richard S. Bird. </author> <title> Addendum: The promotion and accumulation strategies in transformational programming. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 490-492, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: on inductive variables [FU76,Fon77,Fon79], by Paige, Schwartz, and Koenig on finite differencing [PS77,Pai81,PK82], by Dijkstra, Gries, and Reynolds [Dij76,Gri81,Rey81,Gri84] on maintaining and strengthening loop invariants, by Boyle, Moore, Manna, and Waldinger on induction, generalization, and deductive synthesis [BM79,MW80,MW93], by Dershowitz on extension techniques [Der83], by Bird on promotion and accumulation <ref> [Bir84, Bir85] </ref>, by Broy, Bauer, Partsch, etc. on transforming recursive functional programs in CIP [Bro84,BMPP89, Par90], by Smith on finite differencing of functional programs in KIDS [Smi90,Smi91], as well as the work pioneered by Michie on memoization [Mic68,Bir80,Coh83,Web95]. <p> We focus on the second half of the example, where an exponential-time recursive solution is improved (incorrectly in [Bir84], correctly in <ref> [Bir85] </ref>). A function llp is defined to compute the desired length, as below. <p> However, we can discern no systematic steps being followed in [Bir84]. As demonstrated with the path sequence problem, our approach can be regarded as a systematic formulation of the promotion and accumulation strategies. As such, it helps avoid the kind of errors reported and corrected in <ref> [Bir85] </ref>.
Reference: [BM79] <author> Robert S. Boyer and J Strother Moore. </author> <title> A Computational Logic. </title> <booktitle> ACM Monograph Series. </booktitle> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1979. </year>
Reference: [BMPP89] <author> Friedrich Ludwig Bauer, Bernhard Moller, Helmut Partsch, and Peter Pepper. </author> <title> Formal program construction by transformations|computer-aided, </title> <journal> intuition-guided programming. IEEE Transactions on Software Engineering, </journal> <volume> 15(2) </volume> <pages> 165-180, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Eminent systems among them and recent systems include APTS [Pai94], KIDS [Smi90,Smi91], CIP <ref> [BMPP89, Par90] </ref>, Focus [Red88], and ZAP [Fea82]. Compared to these systems, the most important and unique characteristic of CACHET is its use of attribute grammars. This has at least two advantages.
Reference: [BP92] <author> Bard Bloom and Robert Paige. </author> <title> Computing ready simulations efficiently. </title> <editor> In A. Zwarico and S. Purushothaman, editors, </editor> <booktitle> Proceedings of the 1st North American Process Algebra Workshop, Workshops in Computer Science Series, </booktitle> <pages> pages 119-134. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1992. </year>
Reference: [BPR90] <author> Arthur M. Berman, M. C. Paull, and Barbara G. Ryder. </author> <title> Proving relative lower bounds for incremental algorithms. </title> <journal> Acta Informatica, </journal> <volume> 27 </volume> <pages> 665-683, </pages> <year> 1990. </year>
Reference: [Bro84] <author> Manfred Broy. </author> <title> Algebraic methods for program construction: The project CIP. </title> <editor> In Peter. Pepper, editor, </editor> <booktitle> Program Transformation and Programming Environments, volume 8 of NATO Advanced Science Institutes Series F: Computer and System Sciences, </booktitle> <pages> pages 199-222. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year> <booktitle> Proceedings of the NATO Advanced Research Workshop on Program Transformation and Programming Environments, </booktitle> <editor> directed by F. L. Bauer and H. Remus, </editor> <address> Munich, Germany, </address> <month> September </month> <year> 1983. </year> <month> 117 </month>
Reference: [BS86] <author> Rofl Bahlke and Gregor Snelting. </author> <title> The PSG system: From formal language definitions to interactive programming environments. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 547-576, </pages> <month> October </month> <year> 1986. </year>
Reference: [Bur69] <author> R. M. Burstall. </author> <title> Proving properties of programs by structural induction. </title> <journal> The Computer Journal, </journal> <volume> 12(1) </volume> <pages> 41-48, </pages> <year> 1969. </year>
Reference: [Bur90] <author> Michael Burke. </author> <title> An interval-based approach to exhaustive and incremental interprocedural data-flow analysis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 341-395, </pages> <month> July </month> <year> 1990. </year>
Reference: [C + 86] <author> Robert L. Constable et al. </author> <title> Implementing Mathematics with the Nuprl Proof Development System. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1986. </year>
Reference-contexts: In that work, a strength-reduced program was manually discovered and then proved correct using Nuprl <ref> [C + 86] </ref>. Here, we show how our method can automatically derive the strength reductions. This is of particular interest in light of the recent Pentium chip flaw, since the current technology for proving correctness of a chip is still being argued [Gla95]. <p> Work on loop invariants stressed mental tools for programming, rather than mechanical assistance, so no systematic procedures were proposed. Induction and generalization [BM79,MW93] are the logical foundations for recursive calls and iterative loops in deductive program synthesis [MW80] and constructive logics <ref> [C + 86] </ref>. These corpora have for the most part ignored the efficiency of the programs derived, and the resulting programs "are often wantonly wasteful of time and 94 space" [MW92]. In contrast, the approach in this thesis is particularly concerned with the efficiency of the derived programs.
Reference: [Car84] <author> Robert Cartwright. </author> <title> Recursive programs as definitions in first order logic. </title> <journal> SIAM Journal on Computing, </journal> <volume> 13(2) </volume> <pages> 374-408, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: We summarize techniques that are relevant to the program analyses and transformations used for caching and pruning. First, the transformation Ext is similar to the construction of call-by-value complete recursive programs by Cartwright <ref> [Car84] </ref>. However, a call-by-value computation sequence returned by such a program is a flat list of all intermediate results, while our extended function returns a computation tree, a structure that mirrors the hierarchy of function calls.
Reference: [Chi93] <author> Wei-Ngan Chin. </author> <title> Towards an automated tupling strategy. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on PEPM, </booktitle> <address> Copenhagen, Den-mark, </address> <month> June </month> <year> 1993. </year>
Reference: [CHKS93] <author> S. Ceri, M. A. W. Houtsma, A. M. Keller, and P. Samarati. </author> <title> Achieving incremental consistency among autonomous replicated databases. </title> <journal> IFIP Transactions A [Computer Science and Technology], </journal> <volume> A-25:223-237, </volume> <year> 1993. </year>
Reference: [CK77] <author> John Cocke and Ken Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Communications of the ACM, </journal> 20(11) 850-856, November 1977. 
Reference-contexts: Our work is closest in spirit to the finite differencing techniques of the third class. The name "finite differencing" was originally given by Paige and Koenig [PK82]. Their work generalizes Cocke and Kennedy's strength reduction <ref> [CK77] </ref> and provides a convenient framework for implementing a host of transformations including Ear-ley's "iterator inversion" [Ear76]. They develop a set of rules for differentiating set-theoretic expressions and combine these rules using a chain rule to derive inexpensive programs with incremental loop bodies. <p> For static incremental attribute evaluation algorithms [Kas80,Kat84], where no auxiliary information is needed, the approach can cache intermediate results and maintain them automatically, as described in Chapter 4. Strength reduction <ref> [All69, CK77, ACK81, SKR91] </ref> is a traditional compiler optimization technique that aims at computing each iteration incrementally based on the result of the previous iteration. Basically, a fixed set of strength-reduction rules for primitive operators like times and plus are used.
Reference: [CK93] <author> Wei-Ngan Chin and Siau-Cheng Khoo. </author> <title> Tupling functions with multiple recursion parameters. </title> <editor> In Patrick Cousot, Moreno Falaschi, Gilberto File, and Antoine Rauzy, editors, </editor> <booktitle> Proceedings of the 3rd International Workshop on Static Analysis, volume 724 of Lecture Notes in Computer Science, </booktitle> <pages> pages 124-140. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> September </month> <year> 1993. </year>
Reference: [Coh83] <author> N. H. Cohen. </author> <title> Eliminating redundant recursive calls. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 265-299, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: Here we compare our work with related work in program improvement using caching techniques. Caching has been the basis of many techniques for developing efficient programs and optimizing programs. Bird [Bir80] and Cohen <ref> [Coh83] </ref> provide nice overviews. Most of these techniques fall into one of the following three classes. Separate caching. <p> We call this technique separate caching. It corresponds to the "exact tabulation" in [Bir80] and the "large-table method" in <ref> [Coh83] </ref>. The initial idea of memoization, "memo" functions, proposed by Michie [Mic68], belongs to this class. Thus, some uses of the word "memoization" mainly refers to techniques in this class [Par90]. 71 In recent years, there has been additional work on general strategies for separate caching. <p> The pros and cons of separate caching are well discussed by Bird [Bir80] and Cohen <ref> [Coh83] </ref>. To summarize, the idea is simple, and the subject programs are basically unchanged. But the caching methods are dynamic, and thus are fundamentally interpretive. <p> We call this schema-based integrated caching. A nice survey of most of these ideas can be found in [Bir80], following which some uses of the word "tabulation" mainly refer to techniques in this class [Par90]. Typical examples of these techniques are dynamic programming [AHU74], schemas of redundancies <ref> [Coh83] </ref>, and tupling [Pet84,Pet87,Chi93,CK93]. Dynamic programming applies to problems that can be divided into subproblems and solved from small subproblems to larger ones by storing and using results of smaller ones.
Reference: [CP92] <author> Chia-Hsiang Chang and Robert Paige. </author> <title> From regular expressions to dfa's using compressed nfa's. </title> <booktitle> In Proceedings of the 3rd Symposium on Combinatorial Pattern Matching, volume 644 of Lecture Notes in Computer Science, </booktitle> <pages> pages 88-108, </pages> <address> Tucson, Arizona, April 1992. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference: [CP89] <author> Jiazhen Cai and Robert Paige. </author> <title> Program derivation by fixed point computation. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 11 </volume> <pages> 197-261, </pages> <month> September </month> <year> 1988/89. </year> <month> 118 </month>
Reference-contexts: Examples are high-level iterators [Ear76], 8 finite differencing of set expressions in SETL [PK82], optimizing relational database operations [Pai84,HT86], fixed-point computation and recomputation <ref> [CP89] </ref>, differentiation of functional programs in KIDS [Smi90,Smi91], etc. The pioneering work on finite differencing by Paige [PS77,Pai81,PK82,CP89] has been one of the most successful contributions in this class. It uses set-theoretic languages, such as SETL, which provide convenient mathematical notations.
Reference: [CPT92] <author> J. Cai, R. Paige, and R. Tarjan. </author> <title> More efficient bottom-up multi-pattern matching in trees. </title> <journal> Theoretical Computer Science, </journal> <volume> 106(1) </volume> <pages> 21-60, </pages> <month> November </month> <year> 1992. </year> <title> Special issue of best papers selected from A. </title> <editor> Arnold, editor, </editor> <booktitle> Proceedings of CAAP '90: the 15th Colloquium on Trees in Algebra and Programming, </booktitle> <address> Copenhagen, Denmark, </address> <month> May 15-18, </month> <year> 1990. </year>
Reference: [CS70] <author> John Cocke and John T. Schwartz. </author> <title> Programming Languages and Their Compilers; Preliminary Notes. </title> <type> Technical report, </type> <institution> Courant Institute of Mathematical Sciences, </institution> <address> New York University, </address> <year> 1970. </year>
Reference: [DC90] <author> G. D. P. Dueck and G. V. Cormack. </author> <title> Modular attribute grammars. </title> <journal> The Computer Journal, </journal> <volume> 33(2) </volume> <pages> 164-172, </pages> <month> April </month> <year> 1990. </year> <note> Special issue on procedural programming. </note>
Reference-contexts: A few frameworks have been proposed for specifying attribution rules grouped around functionalities and, in particular, attribute grammars can be generated from such specifications <ref> [DC90] </ref>. Mechanisms that can also compose functions from attribution rules would be very helpful.
Reference: [Der83] <author> Nachum Dershowitz. </author> <title> The Evolution of Programs, </title> <booktitle> volume 5 of Progress in Computer Science. </booktitle> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1983. </year>
Reference-contexts: high-level iterators [Ear76], by Fong and Ullman on inductive variables [FU76,Fon77,Fon79], by Paige, Schwartz, and Koenig on finite differencing [PS77,Pai81,PK82], by Dijkstra, Gries, and Reynolds [Dij76,Gri81,Rey81,Gri84] on maintaining and strengthening loop invariants, by Boyle, Moore, Manna, and Waldinger on induction, generalization, and deductive synthesis [BM79,MW80,MW93], by Dershowitz on extension techniques <ref> [Der83] </ref>, by Bird on promotion and accumulation [Bir84, Bir85], by Broy, Bauer, Partsch, etc. on transforming recursive functional programs in CIP [Bro84,BMPP89, Par90], by Smith on finite differencing of functional programs in KIDS [Smi90,Smi91], as well as the work pioneered by Michie on memoization [Mic68,Bir80,Coh83,Web95]. <p> As such, it helps avoid the kind of errors reported and corrected in [Bir85]. Other work on transformational programming for improving program efficiency, including the extension techniques in <ref> [Der83] </ref>, the transformation of recursive functional programs in the CIP project [Bro84,BMPP89,Par90], and the finite differencing of functional programs in the semi-automatic program development system KIDS [Smi90,Smi91], can also be further automated with our systematic approach.
Reference: [DGHKL84] <author> V. Donzeau-Gouge, G. Huet, G. Kahn, and B. Lang. </author> <title> Programming environments based on structure editor: The Mentor experience. </title> <editor> In D. R. Barstow, H. E. Shrobe, and E. Sandewall, editors, </editor> <booktitle> Interactive Programming Environments, </booktitle> <pages> pages 128-140. </pages> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1984. </year>
Reference: [Dij76] <author> Edsger Wybe Dijkstra. </author> <title> A Discipline of Programming. </title> <booktitle> Prentice-Hall Series in Automatic Computation. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1976. </year>
Reference: [DJL88] <author> Pierre Deransart, Martin Jourdan, and Bernard Lorho. </author> <title> Attribute Grammars: Definitions, Systems, and Bibliography, </title> <booktitle> volume 323 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1988. </year>
Reference-contexts: It is implemented using the Synthesizer Generator, with extensions to support complex tree transformations. Incremental program analysis is performed by incremental attribute evaluation, provided automatically by the Synthesizer Generator. Attribute grammars and incremental attribute evaluation methods have been well addressed in the literature <ref> [DJL88] </ref> for describing static semantics of programs. They are not the subject of this chapter.
Reference: [DRO92] <author> Jr. Dan R. Olsen. </author> <title> User Interface Management Systems: Models and Algorithms. The Morgan Kaufmann Series in Computer Graphics and Geometric Modeling. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, Cali-fornia, </address> <year> 1992. </year>
Reference: [Ear76] <author> Jay Earley. </author> <title> High level iterators and a method for automatically designing data structure representation. </title> <journal> Journal of Computer Languages, </journal> <volume> 1 </volume> <pages> 321-342, </pages> <year> 1976. </year>
Reference-contexts: This approach states explicitly that incremental computation is the essence of improving the efficiency of computations. The principles of the approach are essentially the same as those underlying the work by Allen, Cocke, Kennedy, and others on strength reduction [All69,CS70,Gri71,CK77,ACK81], by Earley on high-level iterators <ref> [Ear76] </ref>, by Fong and Ullman on inductive variables [FU76,Fon77,Fon79], by Paige, Schwartz, and Koenig on finite differencing [PS77,Pai81,PK82], by Dijkstra, Gries, and Reynolds [Dij76,Gri81,Rey81,Gri84] on maintaining and strengthening loop invariants, by Boyle, Moore, Manna, and Waldinger on induction, generalization, and deductive synthesis [BM79,MW80,MW93], by Dershowitz on extension techniques [Der83], by Bird <p> These approaches aim to be general, as do those in the second class; they also aim to derive explicit incremental programs, like those manually derived in the first class. Examples are high-level iterators <ref> [Ear76] </ref>, 8 finite differencing of set expressions in SETL [PK82], optimizing relational database operations [Pai84,HT86], fixed-point computation and recomputation [CP89], differentiation of functional programs in KIDS [Smi90,Smi91], etc. The pioneering work on finite differencing by Paige [PS77,Pai81,PK82,CP89] has been one of the most successful contributions in this class. <p> The name "finite differencing" was originally given by Paige and Koenig [PK82]. Their work generalizes Cocke and Kennedy's strength reduction [CK77] and provides a convenient framework for implementing a host of transformations including Ear-ley's "iterator inversion" <ref> [Ear76] </ref>. They develop a set of rules for differentiating set-theoretic expressions and combine these rules using a chain rule to derive inexpensive programs with incremental loop bodies. Such techniques are indispensable as part of an optimizing compiler for languages like SETL or APL [PH87,CP89]. <p> Therefore, there has been a need of general theories and methods for incremental computation. Although there have been many attempts to provide such general approaches <ref> [Ear76, Pai81,PK82,Pai84,HT86,CP89,PT89,FT90,Smi90,Smi91,YS91,SH91,Sun91,Hoo92, van92,Fie93] </ref>, none has incorporated many others. After all, the subject still suffered from the lack of a general framework and a generally applicable systematic approach. What is also unsatisfactory is the treatment of the relationship between incremental computation and program efficiency improvement in general.
Reference: [EGIN92] <author> D. Eppstein, Z. Galil, G. F. Italiano, and A. Nissenzweig. </author> <title> Sparsification a technique for speeding up dynamic graph algorithms. </title> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium on FOCS, </booktitle> <address> Pittsburgh, Pennsyl-vania, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: For example, in SETL, several kinds of maps are maintained for functions that are not continuous for certain input changes [Pai83]. In dynamic graph algorithms, several special data structures are employed: dynamic trees [ST83], topology trees [Fre85], and sparsification trees <ref> [EGIN92] </ref>. Ideally, we can collect classes of special parameterized data structures as auxiliary information parameterized with certain data types.
Reference: [Far86] <author> Rodney Farrow. </author> <title> Automatic generation of fixed-point-finding evaluators for circular, but well-defined, attribute grammars. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 85-98, </pages> <month> July </month> <year> 1986. </year> <note> Published as SIGPLAN Notices, 21(7). 119 </note>
Reference: [Far92] <author> Charles Farnum. </author> <title> Pattern-based tree attribution. </title> <booktitle> In Conference Record of the 19th Annual ACM Symposium on POPL, </booktitle> <pages> pages 211-222, </pages> <month> January </month> <year> 1992. </year>
Reference: [Fea82] <author> Martin S. Feather. </author> <title> A system for assisting program transformation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(1) </volume> <pages> 1-20, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: The integrated computation is usually more efficient; so is its incremental version. We do not describe the integration in detail. Basically, it uses traditional transformation techniques [BD77] like those used in tupling tactic <ref> [Fea82] </ref> and partial evaluation [JGS93]: introducing functions to compute function applications, unfolding, 83 simplifying primitive function applications, driving, replacing recursive applications with introduced functions, etc. <p> Another distinguishing aspect is that the resulting program here returns all intermediate results on the arguments of interest as well. Step B.1 merges candidate auxiliary information with intermediate results, which may make use of a collection of existing transformation techniques like tupling <ref> [Fea82, Pet84,Chi93] </ref>. Step B.2 uses the incrementalization method in Chapter 3, just as 86 Stage II of the method in Chapter 4. Finally, Step B.3 prunes the resulting programs using the backward dependency analysis just as in Stage III of the method in Chapter 4. <p> In particular, for phases not involving circular attributes, efficient evaluators for non-circular attribute grammars can be generated. 6.2.4 Replay We want to consider replaying transformations. A minimal approach is to record the history or script of external input [Red88]. Powerful metalanguages can help reduce the recording work <ref> [Fea82] </ref>; for example, with a metalanguage that allows rewrite, we can record one rewrite in place of a sequence of transformations involved in the rewrite. Replay is important not only for helping to understand the whole transformation, but also for incremental transformation under changes to the input program. <p> Eminent systems among them and recent systems include APTS [Pai94], KIDS [Smi90,Smi91], CIP [BMPP89, Par90], Focus [Red88], and ZAP <ref> [Fea82] </ref>. Compared to these systems, the most important and unique characteristic of CACHET is its use of attribute grammars. This has at least two advantages. <p> Of course, implementing CACHET as a programming environment also facilitates the integration of program derivation and validation with interactive editing, compiling, debugging, and execution. CACHET can benefit from a stronger metalanguage, such as that pioneered by ZAP <ref> [Fea82] </ref>, and certain replay functionality, such as that in Focus [Red88]. As discussed in Sections 6.2 and 6.3, a metalanguage for powerful tree transformation has been designed and is being implemented for CACHET.
Reference: [Fea87] <author> Martin S. Feather. </author> <title> A survey and classification of some program transformation approaches and techniques. </title> <booktitle> In Program Specification and Transformation, </booktitle> <pages> pages 165-195. </pages> <publisher> North-Holland, </publisher> <year> 1987. </year> <booktitle> Proceedings of the IFIP TC2/WG 2.1 Working Conference on Program Specification and Transformation, </booktitle> <address> Bad Tolz, FRG, </address> <month> April </month> <year> 1986. </year>
Reference: [Fie91] <author> John Henry Field. </author> <title> Incremental Reduction in the Lambda Calculus and Related Reduction Systems. </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <year> 1991. </year> <note> Also appeared as Technical Report TR 91-1243, </note> <month> November </month> <year> 1991. </year>
Reference-contexts: The study of dynamic graph algorithms, such as transitive closure algorithms [Yel93], can be viewed as falling into this class. These incremental algorithms are called explicit incremental algorithms by Pugh [Pug88b] and ad hoc incremental algorithms by Field <ref> [Fie91] </ref>. Although efforts in this class are directed towards particular incremental algo 6 7 rithms, they apply to a broad class of problems, e.g., any attribute grammar and any circuit. <p> Such frameworks are called incremental evaluators by Pugh [Pug88b] and general approaches by Field <ref> [Fie91] </ref>. Note that an incremental execution framework always employs a particular incremental algorithm, where the algorithm is designed for executing a particular class of application programs on their inputs and deals with a particular class of changes in an application program and/or its input.
Reference: [Fie93] <author> John Field. </author> <title> A graph reduction approach to incremental term rewriting. </title> <booktitle> In Proceedings of the 5th International Conference on Rewriting Techniques and Applications, volume 690 of Lecture Notes in Computer Science, </booktitle> <address> Montreal, June 1993. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference: [Flo63] <author> Ivan Flores. </author> <booktitle> The Logic of Computer Arithmetic. Prentice-Hall International Series in Electrical Engineering. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1963. </year>
Reference: [FMB90] <author> Bjorn N. Freeman-Benson, John Maloney, and Alan Borning. </author> <title> An incremental constraint solver. </title> <journal> Communications of the ACM, </journal> <volume> 33(1) </volume> <pages> 54-63, </pages> <month> January </month> <year> 1990. </year>
Reference: [FMY92] <author> Rodney Farrow, Thomas J. Marlowe, and Daniel M. Yellin. </author> <title> Compos-able attribute grammars: Support for modularity in translator design and implementation. </title> <booktitle> In Conference Record of the 19th Annual ACM Symposium on POPL, </booktitle> <pages> pages 223-234, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Modular specification improves the readability of attribute grammars and allows more convenient modular evaluation. Modular evaluation provides the flexibility of turning on and off attribution modules as necessary, which results in evaluators that are speedier and more storage-efficient <ref> [FMY92] </ref>. They are particularly useful for phase-based transformations. In particular, for phases not involving circular attributes, efficient evaluators for non-circular attribute grammars can be generated. 6.2.4 Replay We want to consider replaying transformations. A minimal approach is to record the history or script of external input [Red88]. <p> Attribution has been traditionally used in programming environments for code generation [RT88]; it has also been proposed for general program transformation, including phase-based transformation. Approaches include attribute coupled grammars [GG84], higher-order attribute grammars [VSK89], composable attribute grammars <ref> [FMY92] </ref>, and simple tree attributions [BG94]. Incremental attribute evaluation algorithms for these frameworks could be used for incremental code generation. However, these frameworks do not address external input.
Reference: [FN88] <author> Y. Futamura and K. Nogi. </author> <title> Generalized partial evaluation. </title> <booktitle> In Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 133-151. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1988. </year>
Reference-contexts: An underlying logic L 0 is used to make inferences based on the facts in an information set. We require that L 0 be compatible with the semantics of the programming language we are using <ref> [FN88] </ref>, i.e., if two expressions are proved to be equal under L 0 , then they compute the same value. <p> We summarize the major transformation techniques used and emphasize how they are combined to achieve the goal. First, context information is collected for each subcomputation and used to simplify the computation, which mimics the main techniques of generalized partial evaluation <ref> [FN88] </ref>, where program states are represented symbolically and programs are specialized with the help of a theorem prover. In addition to simplification, context information has another important role in our work, i.e., it serves as keys to cached results and introduced functions for valid replacement to happen.
Reference: [Fon77] <author> Amelia C. Fong. </author> <title> Generalized common subexpressions in very high level languages. </title> <booktitle> In Conference Record of the 4th Annual ACM Symposium on POPL, </booktitle> <pages> pages 48-57, </pages> <address> Los Angeles, California, </address> <month> January </month> <year> 1977. </year>
Reference: [Fon79] <author> Amelia C. Fong. </author> <title> Inductively computable constructs in very high level languages. </title> <booktitle> In Conference Record of the 6th Annual ACM Symposium on POPL, </booktitle> <pages> pages 21-28, </pages> <address> San Antonio, Texas, </address> <month> January </month> <year> 1979. </year>
Reference: [Fre85] <author> Greg N. Frederickson. </author> <title> Data structures for on-line updating of minimum spanning trees, with applications. </title> <journal> SIAM Journal on Computing, </journal> <volume> 14(4) </volume> <pages> 781-798, </pages> <month> November </month> <year> 1985. </year> <month> 120 </month>
Reference-contexts: For example, in SETL, several kinds of maps are maintained for functions that are not continuous for certain input changes [Pai83]. In dynamic graph algorithms, several special data structures are employed: dynamic trees [ST83], topology trees <ref> [Fre85] </ref>, and sparsification trees [EGIN92]. Ideally, we can collect classes of special parameterized data structures as auxiliary information parameterized with certain data types.
Reference: [FT90] <author> John Field and Tim Teitelbaum. </author> <title> Incremental reduction in the lambda calculus. </title> <booktitle> In Proceedings of the ACM '90 Conference on LFP, </booktitle> <pages> pages 307-322, </pages> <year> 1990. </year>
Reference-contexts: Attempts to provide general incremental mechanisms have become more active in the past several years and they mostly fall into this class, e.g., incremental attribute evaluation frameworks [RT88], incremental computation via function caching [PT89], incremental lambda reduction <ref> [FT90] </ref>, formal program manipulation using traditional partial evaluation [SH91,Sun91], the change detailing network of INC [YS91], incremental computation as program abstraction [Hoo92], and incremental term rewriting [van92,Fie93]. Such frameworks are called incremental evaluators by Pugh [Pug88b] and general approaches by Field [Fie91].
Reference: [FU76] <author> Amelia C. Fong and Jeffrey D. Ullman. </author> <title> Inductive variables in very high level languages. </title> <booktitle> In Conference Record of the 3rd Annual ACM Symposium on POPL, </booktitle> <pages> pages 104-112, </pages> <address> Atlanta, Georgia, </address> <month> January </month> <year> 1976. </year>
Reference: [GG84] <author> Harald Ganzinger and Robert Giegerich. </author> <title> Attribute coupled grammars. </title> <booktitle> In Proceedings of the ACM SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <pages> pages 157-170, </pages> <month> June </month> <year> 1984. </year> <note> Published as SIGPLAN Notices, 19(6). </note>
Reference-contexts: Thus, tree attribution is used not only for semantics analysis, but also for recording transformed versions of the program. Attribution has been traditionally used in programming environments for code generation [RT88]; it has also been proposed for general program transformation, including phase-based transformation. Approaches include attribute coupled grammars <ref> [GG84] </ref>, higher-order attribute grammars [VSK89], composable attribute grammars [FMY92], and simple tree attributions [BG94]. Incremental attribute evaluation algorithms for these frameworks could be used for incremental code generation. However, these frameworks do not address external input.
Reference: [Gla95] <author> James Glanz. </author> <title> Mathematical logic flushes out the bugs in chip designs. </title> <journal> Science, </journal> <volume> 267 </volume> <pages> 332-333, </pages> <month> January 20, </month> <year> 1995. </year>
Reference-contexts: Here, we show how our method can automatically derive the strength reductions. This is of particular interest in light of the recent Pentium chip flaw, since the current technology for proving correctness of a chip is still being argued <ref> [Gla95] </ref>. The initial specification of the algorithm is given in Figure 5.7.
Reference: [GM79] <author> C. Ghezzi and D. Mandrioli. </author> <title> Incremental parsing. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 58-70, </pages> <month> July </month> <year> 1979. </year>
Reference: [Gol72] <author> Herman H. Goldstine. </author> <title> Charles babbage and his analytical engine. In The Computer from Pascal to von Neumann, </title> <booktitle> chapter 2, </booktitle> <pages> pages 10-26. </pages> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1972. </year>
Reference-contexts: The most basic idea can be traced back to the Difference Engine of Charles Babbage in the 19th century <ref> [Gol72] </ref>. The approach in this thesis is formalized for a simple functional language with all the most basic program constructs: variables, data constructors, primitive functions, user-defined recursive functions, conditionals, and binding expressions. The underlying principles also apply to other programming languages.
Reference: [Gri71] <author> David Gries. </author> <title> Compiler Construction for Digital Computers. </title> <publisher> John Wi-ley & Sons, </publisher> <address> New York, </address> <year> 1971. </year>
Reference: [Gri81] <editor> David Gries. </editor> <booktitle> The Science of Programming. Texts and Monographs in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference: [Gri84] <author> David Gries. </author> <title> A note on a standard strategy for developing loop invariants and loops. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 2 </volume> <pages> 207-214, </pages> <year> 1984. </year>
Reference-contexts: In order to produce efficient programs, loop invariants need to be maintained by the derived programs in an incremental fashion. To make a loop more efficient, the strategy of strengthening a loop invariant, often by introducing fresh variables, is proposed <ref> [Gri84] </ref>. This corresponds to discovering appropriate auxiliary information and deriving incremental programs that maintain such information. Work on loop invariants stressed mental tools for programming, rather than mechanical assistance, so no systematic procedures were proposed.
Reference: [GT92] <editor> Philip Gray and Roger Took, editors. </editor> <title> Building Interactive Systems: Architectures and Tools. Workshops in Computing. </title> <publisher> Springer-Verlag, </publisher> <address> London, </address> <year> 1992. </year> <title> Published in collaboration with the British Computer Society. Proceedings of two separate meetings: Architectures for Interactive Systems, </title> <institution> University of York, </institution> <month> March 4, </month> <year> 1991, </year> <title> and Object-Oriented Tools for User Interface Construction, </title> <institution> University of Glasgow, </institution> <month> April 5, </month> <year> 1991. </year>
Reference: [Gun92] <author> Carl A. Gunter. </author> <title> Semantics of Programming Languages. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year>
Reference: [Hal90] <author> Robert J. Hall. </author> <title> Program improvement by automatic redistribution of intermediate results. </title> <type> Technical Report AI-TR-1251, </type> <institution> Artificial Intelligence Laboratory, MIT, Cambridge, Massachusetts, </institution> <month> Decemeber </month> <year> 1990. </year>
Reference: [Hal91] <author> Robert J. Hall. </author> <title> Program improvement by automatic redistribution of intermediate results: An overview. </title> <editor> In Michael R. Lowry and Robert D. McCartney, editors, </editor> <booktitle> Automating Software Design, chapter 14, </booktitle> <pages> pages 339-372. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1991. </year> <booktitle> Proceedings of the AAAI '88 Workshop on Automating Software Design. </booktitle> <pages> 121 </pages>
Reference: [Hec88] <author> Reinhold Heckmann. </author> <title> A functional language for the specification of complex tree transformations. </title> <booktitle> In Proceedings of the 2nd ESOP, volume 300 of Lecture Notes in Computer Science, </booktitle> <pages> pages 175-190, </pages> <address> Nancy, France, March 1988. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: The mechanisms needed for tree transformations are mainly pattern matching and pattern instantiation. Languages designed specifically for specifying complex tree transformations <ref> [Hec88] </ref>, which may include sophisticated pattern matching languages with, for example, second-order patterns [HL78], can greatly facilitate programming various transformations. Transformations should be able to access attributes associated with the tree, since they are often conditioned on the results of program analyses, which are performed by tree attribution.
Reference: [HL78] <author> Gerard Huet and Bernard Lang. </author> <title> Proving and applying program transformations expressed with second-order patterns. </title> <journal> Acta Informatica, </journal> <volume> 11(1) </volume> <pages> 31-55, </pages> <year> 1978. </year>
Reference-contexts: The mechanisms needed for tree transformations are mainly pattern matching and pattern instantiation. Languages designed specifically for specifying complex tree transformations [Hec88], which may include sophisticated pattern matching languages with, for example, second-order patterns <ref> [HL78] </ref>, can greatly facilitate programming various transformations. Transformations should be able to access attributes associated with the tree, since they are often conditioned on the results of program analyses, which are performed by tree attribution.
Reference: [HN86] <author> A. N. Habermann and D. Notkin. </author> <title> Gandalf: Software development environments. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(12):1117-1127, </volume> <month> December </month> <year> 1986. </year>
Reference: [Hoo92] <author> Roger Hoover. Alphonse: </author> <title> Incremental computation as a programming abstraction. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on PLDI, </booktitle> <pages> pages 261-272, </pages> <address> California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: more active in the past several years and they mostly fall into this class, e.g., incremental attribute evaluation frameworks [RT88], incremental computation via function caching [PT89], incremental lambda reduction [FT90], formal program manipulation using traditional partial evaluation [SH91,Sun91], the change detailing network of INC [YS91], incremental computation as program abstraction <ref> [Hoo92] </ref>, and incremental term rewriting [van92,Fie93]. Such frameworks are called incremental evaluators by Pugh [Pug88b] and general approaches by Field [Fie91]. <p> Examples of the latter include work by Keller and Sleep [KS86], which proposes annotating applicative languages, work by Sundaresh and Hudak [SH91,Sun91], which decides what to cache based on given input partitions of programs, and work by Hoover <ref> [Hoo92] </ref>, which proposes annotating an imperative language. The pros and cons of separate caching are well discussed by Bird [Bir80] and Cohen [Coh83]. To summarize, the idea is simple, and the subject programs are basically unchanged. But the caching methods are dynamic, and thus are fundamentally interpretive.
Reference: [HT86] <author> Susan Horwitz and Tim Teitelbaum. </author> <title> Generating editing environments based on relations and attributes. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 577-608, </pages> <month> October </month> <year> 1986. </year>
Reference: [Hug85] <author> John Hughes. </author> <title> Lazy memo-functions. </title> <booktitle> In Proceedings of the 2nd Conference on FPCA, volume 201 of Lecture Notes in Computer Science, </booktitle> <pages> pages 129-146, </pages> <address> Nancy, France, September 1985. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: The initial idea of memoization, "memo" functions, proposed by Michie [Mic68], belongs to this class. Thus, some uses of the word "memoization" mainly refers to techniques in this class [Par90]. 71 In recent years, there has been additional work on general strategies for separate caching. For example, Hughes <ref> [Hug85] </ref> discusses lazy memo-functions that are suitable for use in systems with lazy evaluation. Mostow and Cohen [MC85] discuss some issues for speeding up Interlisp programs by caching in the presence of side effects. Pugh [Pug88a] provides some improved cache replacement strategies for a simple functional language.
Reference: [JG82] <author> Fahimeh Jalili and Jean H. Gallier. </author> <title> Building friendly parsers. </title> <booktitle> In Conference Record of the 9th Annual ACM Symposium on POPL, </booktitle> <pages> pages 196-206, </pages> <address> Albuquerque, New Mexico, </address> <month> January </month> <year> 1982. </year>
Reference-contexts: These values are called intermediate results useful for computing f 0 incrementally under . Examples where intermediate results are needed for incremental computation include incremental parsing <ref> [JG82] </ref> and incremental attribute evaluation [RTD83, Yeh83]. An incremental parser may cache, in addition to the derived parse tree, the LR (0) state corresponding to each shift and reduction.
Reference: [JGS93] <author> Neil D. Jones, Carsten K. Gomard, and Peter Sestoft. </author> <title> Partial Evaluation and Automatic Program Generation. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1993. </year>
Reference-contexts: First, time analysis [Weg75,Ros89] is used when replacing subcomputations by retrievals or replacing function applications by applications of introduced functions. 3 It is a must if we want to guarantee the efficiency of the derived programs. Then, several analyses <ref> [JGS93] </ref> are used to assist transforming function applications. Dependency analysis enables us to recognize subcomputations that are possibly computed incrementally, i.e., subcomputations depending on x, and thus avoid introducing functions for function applications that depend only on y, which then helps the derivation procedure terminate. <p> The integrated computation is usually more efficient; so is its incremental version. We do not describe the integration in detail. Basically, it uses traditional transformation techniques [BD77] like those used in tupling tactic [Fea82] and partial evaluation <ref> [JGS93] </ref>: introducing functions to compute function applications, unfolding, 83 simplifying primitive function applications, driving, replacing recursive applications with introduced functions, etc. <p> Then, to collect the candidate auxiliary information, Step A.3 modifies the extension transformation to make use of the results of a forward dependency analysis. Our forward dependency analysis is equivalent to binding-time analysis [JSS85, Lau88]. However, the application here is different from that in partial evaluation <ref> [JGS93] </ref>.
Reference: [JL89] <author> Simon B. Jones and Daniel Le Metayer. </author> <title> Compile-time garbage collection by sharing analysis. </title> <booktitle> In Proceedings of the 4th International Conference on FPCA, </booktitle> <pages> pages 54-74, </pages> <address> London, U.K., </address> <month> September </month> <year> 1989. </year>
Reference-contexts: The necessity interpretation by Jones and Le Metayer <ref> [JL89] </ref> is in the same spirit of our analysis, where their notion of necessity patterns correspond to our notion of projections. While necessity patterns specify heads and tails of list values, the projections here specify specific components of tuple values and thus provide more accurate information.
Reference: [Jon90] <author> Larry G. Jones. </author> <title> Efficient evaluation of circular attribute grammars. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 429-462, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: This classification provided a new prospective on the literature in incremental computation, which then motivated the approach in this thesis. 2.1 Incremental algorithm Our first class includes particular incremental algorithms designed for particular problems dealing with particular input changes. Examples include incremental parsing [GM79,JG82], incremental attribute evaluation <ref> [RTD83,Yeh83,YK88,LMOW88, Jon90] </ref>, incremental data-flow analysis [Zad84,RP88,Bur90,MR90,RR94], incremental circuit evaluation [AHR + 90], and incremental constraint solving [Van88,FMB90]. The study of dynamic graph algorithms, such as transitive closure algorithms [Yel93], can be viewed as falling into this class.
Reference: [JP94] <author> Richard Johnson and Keshav Pingali. </author> <title> The program structure tree: Computing control regions in linear time. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on PLDI, </booktitle> <pages> pages 171-185, </pages> <address> Orlando, Florida, </address> <month> June </month> <year> 1994. </year>
Reference: [JSS85] <author> Neil D. Jones, Peter Sestoft, and H. Stndergaard. </author> <title> An experiment in partial evaluation: The generation of a compiler generator. </title> <editor> In J.-P. Jouannaud, editor, </editor> <booktitle> Rewriting Techniques and Applications, volume 202 of Lecture Notes in Computer Science, </booktitle> <pages> pages 124-140, </pages> <address> Dijon, France, May 1985. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address> <month> 122 </month>
Reference-contexts: Then, to collect the candidate auxiliary information, Step A.3 modifies the extension transformation to make use of the results of a forward dependency analysis. Our forward dependency analysis is equivalent to binding-time analysis <ref> [JSS85, Lau88] </ref>. However, the application here is different from that in partial evaluation [JGS93].
Reference: [Kai89] <author> Gail E. Kaiser. </author> <title> Incremental dynamic semantics for language-based programming environments. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(2) </volume> <pages> 168-193, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: As program transformation generalizes symbolic, and thus normal, executions of programs, external input includes dynamic information that can be set during program executions, e.g., the breakpoints in a debugger or the instruction pointer in an interpreter. Such dynamic semantics <ref> [Kai89] </ref> is needed for run-time support, symbolic debuggers, interpreters, as well as interactive program transformation systems. External input scattered in the middle of program execution or transformation is a new concept lacking in traditional declarative attribute-grammar-based environments. <p> Techniques that address such dynamic program semantics have been lacking in traditional attribute-grammar-based programming environments <ref> [Kai89] </ref>. For example, OPTRAN [LMW88] is an attribute-grammar-based system extended with rewrite mechanisms. However, it is still a batch-oriented system mainly for compiler applications rather than general program transformations. Another difference is that traditional attribute-grammar-based programming environments perform code generation by attribution [GG84,RT88,BG94], while CACHET transforms programs by direct manipulation.
Reference: [Kas80] <author> Uwe Kastens. </author> <title> Ordered attributed grammars. </title> <journal> Acta Informatica, </journal> <volume> 13(3) </volume> <pages> 229-256, </pages> <year> 1980. </year>
Reference: [Kat84] <author> Takuya Katayama. </author> <title> Translation of attribute grammars into procedures. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(3) </volume> <pages> 345-369, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: An incremental parser may cache, in addition to the derived parse tree, the LR (0) state corresponding to each shift and reduction. An attribute evaluator may only return some designated synthesized attribute of the root <ref> [Kat84] </ref>, but the corresponding incremental attribute evaluator may cache the whole attributed tree. This chapter describes a three-stage method, called cache-and-prune, for statically transforming a program f 0 to cache intermediate results useful for the incremental computation under . <p> O (n) time, although it uses O (n log n) space to store the intermediate results of the previous sort. 4.6.3 Attribute evaluation using Katayama functions Given an attribute grammar, a set of recursive functions can be constructed to evaluate the attribute values for any derivation tree of the grammar <ref> [Kat84] </ref>. Basically, each function evaluates a synthesized attribute of a non-terminal, and the value of a synthesized attribute of the root symbol is the final return value of interest.
Reference: [Kle52] <author> Stephen Cole Kleene. </author> <title> Introduction to Metamathematics. </title> <publisher> Van Nos-trand, </publisher> <address> New York, </address> <year> 1952. </year> <booktitle> Tenth reprint, </booktitle> <publisher> Wolters-Noordhoff Publishing, Groningen and North-Holland Publishing Company, </publisher> <address> Amsterdam, </address> <year> 1991. </year>
Reference-contexts: This starting point is similar to that of partial evaluation, which starts with a trivial specialized program given by Kleene's s-m-n theorem <ref> [Kle52] </ref> and attempts improvements by symbolic reductions or similar techniques. We summarize the major transformation techniques used and emphasize how they are combined to achieve the goal. <p> In cooperation with the approach for deriving incremental programs, we achieve the goal of identifying and maintaining intermediate results useful for incremental computation. The idea of caching all intermediate results followed by incrementalization can be regarded as a realization of the reduction from Kleene's course-of-values recursion to primitive recursion <ref> [Kle52, Ner95] </ref>; subsequent pruning straightforwardly eliminates unnecessary computations in the resulting function. We summarize techniques that are relevant to the program analyses and transformations used for caching and pruning. First, the transformation Ext is similar to the construction of call-by-value complete recursive programs by Cartwright [Car84]. <p> In contrast, the approach in this thesis is particularly concerned with the efficiency of the derived programs. Moreover, we can see that induction, whether course-of-value induction <ref> [Kle52] </ref>, structural induction [Bur69,BM79], or well-founded induction [BM79,MW93], enables derived programs to use results of previous iterations in each iteration, and generalization [BM79,MW93] enables derived programs to use appropriate auxiliary information by strengthening induction hypotheses, just like strengthening loop invariants. <p> The approach in this thesis may be used for systematically constructing induction steps <ref> [Kle52] </ref> and strengthening induction hypotheses. The promotion and accumulation strategies are proposed by Bird [Bir84,Bir85] as general methods for achieving efficient transformed programs. Promotion attempts to derive a program that defines f (cons (a; x)) in terms of f (x), and accumulation generalizes a definition by including an extra argument.
Reference: [KRS94] <author> Jens Knoop, Oliver Ruthing, and Bernhard Steffen. </author> <title> Partial dead code elimination. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on PLDI, </booktitle> <pages> pages 147-158, </pages> <address> Orlando, Florida, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: (4.4), we have 1st ( ^ f 0 Thus, ^ f 0 0 (x; y; ^r) incrementally computes the desired output and the corresponding intermediate results and is asymptotically at least as fast as computing the desired 1 Note that this is different from the partial dead code elimination in <ref> [KRS94] </ref>, where partial dead code refers to code that is dead on some but not all computation paths. 44 output from scratch. Therefore, we do not have to conduct a derivation on ^ f 0 and to obtain such an incremental function.
Reference: [KS86] <author> Robert M. Keller and M. Ronan Sleep. </author> <title> Applicative caching. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(1) </volume> <pages> 88-108, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: An example of the former is the stable decomposition scheme proposed by Pugh and Teitelbaum [PT89], who also advocate a closer study of using memoization for incremental evaluation. Examples of the latter include work by Keller and Sleep <ref> [KS86] </ref>, which proposes annotating applicative languages, work by Sundaresh and Hudak [SH91,Sun91], which decides what to cache based on given input partitions of programs, and work by Hoover [Hoo92], which proposes annotating an imperative language.
Reference: [Lar92] <author> James A. Larson. </author> <title> Interactive Software: Tools For Building Interactive User Interfaces. </title> <publisher> Yourdon Press Computing Series. Yourdon Press, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1992. </year>
Reference: [Lau88] <author> John Launchbury. </author> <title> Projections for specialisation. </title> <booktitle> In Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 299-315. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: Then, to collect the candidate auxiliary information, Step A.3 modifies the extension transformation to make use of the results of a forward dependency analysis. Our forward dependency analysis is equivalent to binding-time analysis <ref> [JSS85, Lau88] </ref>. However, the application here is different from that in partial evaluation [JGS93].
Reference: [Lau89] <author> John Launchbury. </author> <title> Projection Factorisations in Partial Evaluation. </title> <type> Ph.D. dissertation, </type> <institution> Department of Computing, University of Glasgow, </institution> <year> 1989. </year>
Reference-contexts: Other uses of projections include the strictness analysis by Wadler and Hughes [WH87], where necessary information needs to be specified and thus accounts for some complications, and binding-time analysis by Launchbury <ref> [Lau89] </ref>, which is a forward analysis and is proved equivalent to strictness analysis [Lau91]. The necessity interpretation by Jones and Le Metayer [JL89] is in the same spirit of our analysis, where their notion of necessity patterns correspond to our notion of projections.
Reference: [Lau91] <author> John Launchbury. </author> <title> Strictness and binding-time analysis: Two for the price of one. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on PLDI, </booktitle> <pages> pages 80-91, </pages> <address> Toronto, Ontario, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Other uses of projections include the strictness analysis by Wadler and Hughes [WH87], where necessary information needs to be specified and thus accounts for some complications, and binding-time analysis by Launchbury [Lau89], which is a forward analysis and is proved equivalent to strictness analysis <ref> [Lau91] </ref>. The necessity interpretation by Jones and Le Metayer [JL89] is in the same spirit of our analysis, where their notion of necessity patterns correspond to our notion of projections.
Reference: [LD93] <author> Julia L. Lawall and Olivier Danvy. </author> <title> Separating stages in the continuation-passing style transformation. </title> <booktitle> In Conference Record of the 20th Annual ACM Symposium on POPL, </booktitle> <pages> pages 124-136, </pages> <month> January </month> <year> 1993. </year>
Reference: [Liu95] <author> Yanhong A. Liu. CACHET: </author> <title> An interactive, incremental-attribution-based program transformation system for deriving incremental programs. </title> <booktitle> In Proceedings of the 10th Knowledge-Based Software Engineering Conference, </booktitle> <pages> pages 19-26, </pages> <address> Boston, Massachusetts, November 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: As an aid to the reader, an index of symbols, terms, and names are included at the end of the thesis. Much of Chapters 3, 4, 5, and 6 appears in separate papers (specifically, [LT95b], [LT95a], [LST96], and <ref> [Liu95] </ref>, respectively). Each of them is largely self-contained. Chapter 2 Providing a general systematic approach to efficient computation Incremental computation takes advantage of repeated computations on inputs that differ slightly from one another, computing each new output incrementally by making use of the previous output rather than from scratch.
Reference: [LMOW88] <author> Peter Lipps, Ulrich Moncke, Matthias Olk, and Reinhard Wilhelm. </author> <title> Attribute (re)evaluation in OPTRAN. </title> <journal> Acta Informatica, </journal> <volume> 26 </volume> <pages> 213-239, </pages> <year> 1988. </year> <month> 123 </month>
Reference: [LMW88] <author> Peter Lipps, Ulrich Moncke, and Reinhard Wilhelm. OPTRAN: </author> <title> A language/system for the specification of program transformation|system overview and experiences. </title> <editor> In Dieter Hammer, editor, </editor> <booktitle> Proceedings of the 2nd CCHSC Workshop: Workshop on Compiler Compilers and High Speed Compilation, volume 371 of Lecture Notes in Computer Science, </booktitle> <pages> pages 52-65, </pages> <address> Berlin, GDR, October 1988. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: Techniques that address such dynamic program semantics have been lacking in traditional attribute-grammar-based programming environments [Kai89]. For example, OPTRAN <ref> [LMW88] </ref> is an attribute-grammar-based system extended with rewrite mechanisms. However, it is still a batch-oriented system mainly for compiler applications rather than general program transformations. Another difference is that traditional attribute-grammar-based programming environments perform code generation by attribution [GG84,RT88,BG94], while CACHET transforms programs by direct manipulation.
Reference: [LS92] <author> E. Levy and A. Silberschatz. </author> <title> Incremental recovery in main memory database systems. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 4(6) </volume> <pages> 529-540, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers [Ear76,CK77,MJ81,ASU86] and transformational programming [Pai83,Par90, Smi90], interactive systems like programming environments [MF81,Rep84] and editors [RTD83,RT88,BGV92], and dynamic systems like distributed databases <ref> [LS92, CHKS93,ZGMHW94] </ref> and real-time systems [VC92]. The premise of this work is that methods of incremental computation can be generalized and systematized by finding a conceptual model that captures all the fundamentals. A comprehensive guide to methods of incremental computation has appeared in [RR93].
Reference: [LST96] <author> Yanhong A. Liu, Scott D. Stoller, and Tim Teitelbaum. </author> <title> Discovering auxiliary information for incremental computation. </title> <booktitle> In Conference Record of the 23rd Annual ACM Symposium on POPL, </booktitle> <address> St. Petersburg Beach, Florida, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: As an aid to the reader, an index of symbols, terms, and names are included at the end of the thesis. Much of Chapters 3, 4, 5, and 6 appears in separate papers (specifically, [LT95b], [LT95a], <ref> [LST96] </ref>, and [Liu95], respectively). Each of them is largely self-contained.
Reference: [LT95a] <author> Yanhong A. Liu and Tim Teitelbaum. </author> <title> Caching intermediate results for program improvement. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on PEPM, </booktitle> <pages> pages 190-201, </pages> <address> La Jolla, California, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: As an aid to the reader, an index of symbols, terms, and names are included at the end of the thesis. Much of Chapters 3, 4, 5, and 6 appears in separate papers (specifically, [LT95b], <ref> [LT95a] </ref>, [LST96], and [Liu95], respectively). Each of them is largely self-contained.
Reference: [LT95b] <author> Yanhong A. Liu and Tim Teitelbaum. </author> <title> Systematic derivation of incremental programs. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 24(1) </volume> <pages> 1-39, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: As an aid to the reader, an index of symbols, terms, and names are included at the end of the thesis. Much of Chapters 3, 4, 5, and 6 appears in separate papers (specifically, <ref> [LT95b] </ref>, [LT95a], [LST96], and [Liu95], respectively). Each of them is largely self-contained.
Reference: [MC85] <author> D. J. Mostow and D. Cohen. </author> <title> Automating program speedup by deciding what to cache. </title> <booktitle> In Proceedings of the 9th IJCAI, </booktitle> <pages> pages 165-172, </pages> <address> Los Angeles, </address> <month> August </month> <year> 1985. </year>
Reference-contexts: For example, Hughes [Hug85] discusses lazy memo-functions that are suitable for use in systems with lazy evaluation. Mostow and Cohen <ref> [MC85] </ref> discuss some issues for speeding up Interlisp programs by caching in the presence of side effects. Pugh [Pug88a] provides some improved cache replacement strategies for a simple functional language.
Reference: [Mee94] <author> Lambert Meertens. </author> <title> Incremental optimum-fit line breaking. </title> <type> Technical Report WG 2.1 TR 720 REN-5, </type> <institution> CWI, </institution> <month> January </month> <year> 1994. </year>
Reference: [MF81] <author> R. Medina-Mora and P. Feiler. </author> <title> An incremental programming environment. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-7(5):472-482, </volume> <month> September </month> <year> 1981. </year>
Reference: [Mic68] <author> Donald Michie. </author> <title> "memo" functions and machine learning. </title> <journal> Nature, </journal> <volume> 218 </volume> <pages> 19-22, </pages> <month> April </month> <year> 1968. </year>
Reference-contexts: We call this technique separate caching. It corresponds to the "exact tabulation" in [Bir80] and the "large-table method" in [Coh83]. The initial idea of memoization, "memo" functions, proposed by Michie <ref> [Mic68] </ref>, belongs to this class. Thus, some uses of the word "memoization" mainly refers to techniques in this class [Par90]. 71 In recent years, there has been additional work on general strategies for separate caching.
Reference: [Mil91] <author> Peter Bro Miltersen. </author> <title> On-line reevaluation of functions. </title> <type> Technical Report ALCOM-91-63, </type> <institution> Aarhus University, Aarhus, Demark, </institution> <month> May </month> <year> 1991. </year>
Reference: [MJ81] <editor> Steven S. Muchnick and Neil D. Jones, editors. </editor> <title> Program Flow Analysis: Theory and Applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference: [Mog88] <author> Torben Mogensen. </author> <title> Partially static structures in a self-applicable partial evaluator. </title> <booktitle> In Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 325-347. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: Note that, even without this technique, the efficiency of our derived programs is guaranteed with the help of time analysis. But in partial evaluation where no time analysis is employed, a transformed program could take exponential time while the original program takes only polynomial time <ref> [Mog88] </ref>. As a matter of fact, even with this technique, time analysis is still needed in our derivation, since we replace subcompu-tations by retrievals from a cache result only when we can save time by doing so.
Reference: [MR90] <author> Thomas J. Marlowe and Barbara G. Ryder. </author> <title> An efficient hybrid algorithm for incremental data flow analysis. </title> <booktitle> In Conference Record of the 17th Annual ACM Symposium on POPL, </booktitle> <pages> pages 184-196, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1990. </year> <month> 124 </month>
Reference: [MSVT94] <author> Peter Bro Miltersen, Sairam Subramanian, Jeffrey Scott Vitter, and Roberto Tamassia. </author> <title> Complexity models fo incremental computation. </title> <journal> Theoretical Computer Science, </journal> <volume> 130(1) </volume> <pages> 203-236, </pages> <year> 1994. </year>
Reference: [MW80] <author> Zohar Manna and Richard Waldinger. </author> <title> A deductive approach to program synthesis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 2(1) </volume> <pages> 90-121, </pages> <month> January </month> <year> 1980. </year>
Reference-contexts: Work on loop invariants stressed mental tools for programming, rather than mechanical assistance, so no systematic procedures were proposed. Induction and generalization [BM79,MW93] are the logical foundations for recursive calls and iterative loops in deductive program synthesis <ref> [MW80] </ref> and constructive logics [C + 86]. These corpora have for the most part ignored the efficiency of the programs derived, and the resulting programs "are often wantonly wasteful of time and 94 space" [MW92].
Reference: [MW92] <author> Zohar Manna and Richard Waldinger. </author> <title> Fundamentals of deductive program synthesis. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(8) </volume> <pages> 674-704, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: These corpora have for the most part ignored the efficiency of the programs derived, and the resulting programs "are often wantonly wasteful of time and 94 space" <ref> [MW92] </ref>. In contrast, the approach in this thesis is particularly concerned with the efficiency of the derived programs.
Reference: [MW93] <author> Zohar Manna and Richard Waldinger. </author> <booktitle> The Deductive Foundations of Computer Programming. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1993. </year>
Reference: [Ner92] <author> Anil Nerode. </author> <title> Private communication and email communication on deriving incremental programs, </title> <month> May-July </month> <year> 1992. </year>
Reference-contexts: Thus, no incremental programs can be derived when the given rules do not apply. Such approaches are characterized as syntactic, using only formal notations, whereas a semantic solution that explores the mathematical content of the subject has not been developed <ref> [Ner92] </ref>. Another highly successful contribution for deriving efficient programs is by Smith [Smi90, Smi91]. It advocates a more flexible approach to transforming programs written in a functional language.
Reference: [Ner95] <author> Anil Nerode. </author> <title> Private communication on caching intermediate results, </title> <month> August </month> <year> 1995. </year>
Reference-contexts: In cooperation with the approach for deriving incremental programs, we achieve the goal of identifying and maintaining intermediate results useful for incremental computation. The idea of caching all intermediate results followed by incrementalization can be regarded as a realization of the reduction from Kleene's course-of-values recursion to primitive recursion <ref> [Kle52, Ner95] </ref>; subsequent pruning straightforwardly eliminates unnecessary computations in the resulting function. We summarize techniques that are relevant to the program analyses and transformations used for caching and pruning. First, the transformation Ext is similar to the construction of call-by-value complete recursive programs by Cartwright [Car84].
Reference: [OLHA94] <author> John O'Leary, Miriam Leeser, Jason Hickey, and Mark Aagaard. </author> <title> Non-restoring integer square root: A case study in design by principled optimization. </title> <editor> In Ramayya Kumar and Thomas Kropf, editors, </editor> <booktitle> Proceedings of TPCD '94: the 2nd International Conference on Theorem Provers in Circuit Design|Theory, Practice and Experience, volume 901 of Lecture Notes in Computer Science, </booktitle> <pages> pages 52-71, </pages> <address> Bad Herrenalb (Black Forest), Germany, September 1994. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: This section presents an example for each of these two applications. The examples are based on problems in VLSI design and graph algorithms, respectively. 5.4.1 Binary integer square root This example is from <ref> [OLHA94] </ref>, where a specification of a non-restoring binary integer square root algorithm is transformed into a VLSI circuit design and implementation. In that work, a strength-reduced program was manually discovered and then proved correct using Nuprl [C + 86]. <p> Following our systematic approach, we even discover that an extra shift is done in <ref> [OLHA94] </ref>. Thus, such systematic transformational approach is not only greatly desired for automating designs and guaranteeing correctness, but also also for helping reduce the cost. 5.4.2 Path sequence problem This example is from [Bir84].
Reference: [Pai81] <author> Robert Paige. </author> <title> Formal Differentiation: A Program Synthesis Technique, </title> <booktitle> volume 6 of Computer Science and Artificial Intelligence. </booktitle> <publisher> UMI Research Press, </publisher> <address> Ann Arbor, Michigan, </address> <year> 1981. </year> <title> Revision of Ph.D. dissertation, </title> <address> New York University, </address> <year> 1979. </year>
Reference-contexts: the incremental computation problem for particular applications are not readily comparable with explicitly derived incremental algorithms such as those in the first class. 2.3 Incremental-program derivation approach In our third class, systematic approaches are studied to derive explicit incremental programs from non-incremental programs using program transformation techniques like finite differencing <ref> [Pai81, PK82] </ref>. These approaches aim to be general, as do those in the second class; they also aim to derive explicit incremental programs, like those manually derived in the first class.
Reference: [Pai83] <author> Robert Paige. </author> <title> Transformational programming|applications to algorithms and systems. </title> <booktitle> In Conference Record of the 10th Annual ACM Symposium on POPL, </booktitle> <pages> pages 73-87, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: In these cases, use of the auxiliary information typically involves special high-level properties of the particular problem at hand. For example, in SETL, several kinds of maps are maintained for functions that are not continuous for certain input changes <ref> [Pai83] </ref>. In dynamic graph algorithms, several special data structures are employed: dynamic trees [ST83], topology trees [Fre85], and sparsification trees [EGIN92]. Ideally, we can collect classes of special parameterized data structures as auxiliary information parameterized with certain data types.
Reference: [Pai84] <author> Robert Paige. </author> <title> Applications of finite differencing to database integrity control and query/transaction optimization. </title> <editor> In H. Gallaire, J. Minker, and J.-M. Nicolas, editors, </editor> <booktitle> Advances in Database Theory, </booktitle> <volume> Vol. 2, </volume> <pages> pages 171-210. </pages> <publisher> Plenum Press, </publisher> <address> New York, </address> <month> March </month> <year> 1984. </year>
Reference: [Pai86] <author> Robert Paige. </author> <title> Programming with invariants. </title> <journal> IEEE Software, </journal> <pages> pages 56-69, </pages> <month> January </month> <year> 1986. </year>
Reference: [Pai89] <author> Robert Paige. </author> <title> Real-time simulation of a set machine on a RAM. </title> <booktitle> In Computing and Information, </booktitle> <volume> Vol. II, </volume> <pages> pages 69-73. </pages> <publisher> Canadian Scholars Press, </publisher> <year> 1989. </year> <booktitle> Proceedings of ICCI '89: the International Conference on Computing and Information, </booktitle> <address> Toronto, Canada, </address> <month> May 23-27, </month> <year> 1989. </year> <month> 125 </month>
Reference: [Pai90] <author> Robert Paige. </author> <title> Symbolic finite differencing|part I. </title> <booktitle> In Proceedings of the 3rd ESOP, volume 432 of Lecture Notes in Computer Science, </booktitle> <pages> pages 36-56, </pages> <address> Copenhagen, Denmark, May 1990. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference: [Pai94] <author> Robert Paige. </author> <title> Viewing a program transformation system at work. </title> <editor> In M. Hermenegildo and J. Penjam, editors, </editor> <booktitle> Proceedings of Joint 6th International Conference on PLILP and 4th International Conference on ALP, volume 844 of Lecture Notes in Computer Science, </booktitle> <pages> pages 5-24. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: The simple storage of the results of the analyses makes it hard to incrementalize the analyses under the transformations. This creates a performance bottleneck that is more severe for more automated systems <ref> [Pai94] </ref>. Using an attribute-grammar-based programming environment. Interactive program transformation and incremental program analysis naturally lead one to consider an attribute-grammar-based programming environment. The Synthesizer Generator is a commercially available system that generates such environments [RT88]. <p> However, to see all the interesting intermediate results during the derivation, manual invocation is appropriate. 6.5 Related work Program transformation systems and the approaches and techniques used therein are described in a number of surveys, e.g., [PS83,Fea87]. Eminent systems among them and recent systems include APTS <ref> [Pai94] </ref>, KIDS [Smi90,Smi91], CIP [BMPP89, Par90], Focus [Red88], and ZAP [Fea82]. Compared to these systems, the most important and unique characteristic of CACHET is its use of attribute grammars. This has at least two advantages. <p> To our knowledge, the BMF-editor [VVF90] is the only program transformation system that uses the incremental attribution paradigm, although incremental semantic analysis is desirable in all these systems, especially for the more automatic approaches in APTS 108 <ref> [Pai94] </ref>. The BMF-editor emphasizes dynamic transformations but does not explicitly address the problem of external inputs. Since CACHET is implemented using a language-based editor generator, it has a flexible interactive user interface, as is provided by the existing technologies of programming environments.
Reference: [Par90] <author> Helmut A. Partsch. </author> <title> Specification and Transformation of Programs|A Formal Approach to Software Development. Texts and Monographs in Computer Science. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: Dijkstra, Gries, and Reynolds [Dij76,Gri81,Rey81,Gri84] on maintaining and strengthening loop invariants, by Boyle, Moore, Manna, and Waldinger on induction, generalization, and deductive synthesis [BM79,MW80,MW93], by Dershowitz on extension techniques [Der83], by Bird on promotion and accumulation [Bir84, Bir85], by Broy, Bauer, Partsch, etc. on transforming recursive functional programs in CIP <ref> [Bro84,BMPP89, Par90] </ref>, by Smith on finite differencing of functional programs in KIDS [Smi90,Smi91], as well as the work pioneered by Michie on memoization [Mic68,Bir80,Coh83,Web95]. The most basic idea can be traced back to the Difference Engine of Charles Babbage in the 19th century [Gol72]. <p> We call this technique separate caching. It corresponds to the "exact tabulation" in [Bir80] and the "large-table method" in [Coh83]. The initial idea of memoization, "memo" functions, proposed by Michie [Mic68], belongs to this class. Thus, some uses of the word "memoization" mainly refers to techniques in this class <ref> [Par90] </ref>. 71 In recent years, there has been additional work on general strategies for separate caching. For example, Hughes [Hug85] discusses lazy memo-functions that are suitable for use in systems with lazy evaluation. <p> We call this schema-based integrated caching. A nice survey of most of these ideas can be found in [Bir80], following which some uses of the word "tabulation" mainly refer to techniques in this class <ref> [Par90] </ref>. Typical examples of these techniques are dynamic programming [AHU74], schemas of redundancies [Coh83], and tupling [Pet84,Pet87,Chi93,CK93]. Dynamic programming applies to problems that can be divided into subproblems and solved from small subproblems to larger ones by storing and using results of smaller ones. <p> Sometimes it is not sufficient to have only a fixed set of strategies and rules. Seeking more flexibility and broader applicability, KIDS [Smi90] advocates certain high-level strategies but leaves the choice of which intermediate results to maintain to manual decisions. CIP <ref> [Par90] </ref> also proposes a general strategy for caching, but it may even lead to less efficient programs. Recently, certain principles that can directly guide program transformations have been proposed. Webber's principle of least computation [Web93,Web95] avoids subcomputations whose values have been computed before or are not needed. <p> Eminent systems among them and recent systems include APTS [Pai94], KIDS [Smi90,Smi91], CIP <ref> [BMPP89, Par90] </ref>, Focus [Red88], and ZAP [Fea82]. Compared to these systems, the most important and unique characteristic of CACHET is its use of attribute grammars. This has at least two advantages.
Reference: [Pau83] <author> Lawrence Paulson. </author> <title> A higher-order implementation of rewriting. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 3 </volume> <pages> 119-149, </pages> <year> 1983. </year>
Reference-contexts: With such an engine, repeated invocation of transformations in certain fixed patterns can be easily automated, saving both programmers' and users' effort. Higher-order term rewrite <ref> [Pau83] </ref> offers a framework for defining such rewrites. What needs to be provided is the ability to automatically interleave such repeated rewrite with incremental attribute evaluation. 98 6.2.2 External input as annotation Program transformations are often semi-automatic and involve interaction with users or other external facilities, such as theorem provers.
Reference: [Pet84] <author> Alberto Pettorossi. </author> <title> A powerful strategy for deriving efficient programs by transformation. </title> <booktitle> In Proceedings of the ACM '84 Symposium on LFP, </booktitle> <address> Austin, Texas, </address> <month> August </month> <year> 1984. </year>
Reference: [Pet87] <author> Alberto Pettorossi. </author> <title> Strategical derivation of on-line programs. </title> <editor> In L. G. L. T. Meertens, editor, </editor> <booktitle> Program Specification and Transformation, </booktitle> <pages> pages 73-88. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1987. </year> <booktitle> Proceedings of the IFIP TC2/WG 2.1 Working Conference on Program Specification and Transformation, </booktitle> <address> Bad Tolz, FRG, </address> <month> April </month> <year> 1986. </year>
Reference: [PH87] <author> Robert Paige and Fritz Henglein. </author> <title> Mechanical translation of set theoretic problem specifications into efficient RAM code|a case study. </title> <journal> Journal of Symbolic Computation, </journal> <volume> 4(2) </volume> <pages> 207-232, </pages> <year> 1987. </year>
Reference: [PK82] <author> Robert Paige and Shaye Koenig. </author> <title> Finite differencing of computable expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: the incremental computation problem for particular applications are not readily comparable with explicitly derived incremental algorithms such as those in the first class. 2.3 Incremental-program derivation approach In our third class, systematic approaches are studied to derive explicit incremental programs from non-incremental programs using program transformation techniques like finite differencing <ref> [Pai81, PK82] </ref>. These approaches aim to be general, as do those in the second class; they also aim to derive explicit incremental programs, like those manually derived in the first class. <p> These approaches aim to be general, as do those in the second class; they also aim to derive explicit incremental programs, like those manually derived in the first class. Examples are high-level iterators [Ear76], 8 finite differencing of set expressions in SETL <ref> [PK82] </ref>, optimizing relational database operations [Pai84,HT86], fixed-point computation and recomputation [CP89], differentiation of functional programs in KIDS [Smi90,Smi91], etc. The pioneering work on finite differencing by Paige [PS77,Pai81,PK82,CP89] has been one of the most successful contributions in this class. <p> Our work is closest in spirit to the finite differencing techniques of the third class. The name "finite differencing" was originally given by Paige and Koenig <ref> [PK82] </ref>. Their work generalizes Cocke and Kennedy's strength reduction [CK77] and provides a convenient framework for implementing a host of transformations including Ear-ley's "iterator inversion" [Ear76]. <p> For example, the conventional optimizing-compiler technique of strength reduction [All69,CK77,ACK81] identifies subcompu-tations like multiplications that can be replaced with subcomputations like additions while maintaining the values of these subcomputations. Similarly, the APTS program transformation system [Pai83,Pai90,Pai94] identifies set expressions in SETL that can be maintained using finite differencing rules <ref> [PK82] </ref>. Sometimes it is not sufficient to have only a fixed set of strategies and rules. Seeking more flexibility and broader applicability, KIDS [Smi90] advocates certain high-level strategies but leaves the choice of which intermediate results to maintain to manual decisions. <p> This information needs to be maintained efficiently as well. Some approaches to incremental computation have exploited specific kinds of auxiliary information, e.g., auxiliary arithmetic associated with some classical strength-reduction rules [ACK81], dynamic mappings maintained by finite differencing rules for aggregate primitives in SETL <ref> [PK82] </ref> and INC [YS91], and auxiliary data structures for problems with certain properties like stable decomposition [PT89]. However, until now, systematic discovery of auxiliary information for arbitrary programs has not been studied. <p> This functionality provides a general solution to the finite differencing problem, which must be addressed in program derivation from specification and program improvement in general <ref> [PK82, BMPP89,Par90,Pai90,Smi90,Smi91] </ref> 6.6 Future work A number of problems have been suggested that need to be further studied. We discuss them here. Tree rewrite. First, the Synthesizer Generator is being interfaced with Scheme, which will allow us to implement a more powerful metalanguage with rewrite engines in Scheme.
Reference: [Plo75] <author> Gordon D. Plotkin. </author> <title> Call-by-name, call-by-value and the -calculus. </title> <journal> Theoretical Computer Science, </journal> <volume> 1 </volume> <pages> 125-159, </pages> <year> 1975. </year>
Reference: [PS77] <author> Bob Paige and J. T. Schwartz. </author> <title> Expression continuity and the formal differentiation of algorithms. </title> <booktitle> In Conference Record of the 4th Annual ACM Symposium on POPL, </booktitle> <pages> pages 58-71, </pages> <month> January </month> <year> 1977. </year>
Reference: [PS83] <author> H. Partsch and R. Steinbruggen. </author> <title> Program transformation systems. </title> <journal> Computing Surveys, </journal> <volume> 15(3) </volume> <pages> 199-236, </pages> <month> September </month> <year> 1983. </year>
Reference: [PS92] <author> Lori L. Pollock and Mary Lou Soffa. </author> <title> Incremental global reoptimiza-tion of programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 14(2) </volume> <pages> 173-200, </pages> <month> April </month> <year> 1992. </year>
Reference: [PT87] <author> Robert Paige and Robert Tarjan. </author> <title> Three partition refinement algorithms. </title> <journal> SIAM J. Comput., </journal> <volume> 16(6) </volume> <pages> 973-989, </pages> <month> December </month> <year> 1987. </year> <month> 126 </month>
Reference: [PT89] <author> William Pugh and Tim Teitelbaum. </author> <title> Incremental computation via function caching. </title> <booktitle> In Conference Record of the 16th Annual ACM Symposium on POPL, </booktitle> <pages> pages 315-328, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Attempts to provide general incremental mechanisms have become more active in the past several years and they mostly fall into this class, e.g., incremental attribute evaluation frameworks [RT88], incremental computation via function caching <ref> [PT89] </ref>, incremental lambda reduction [FT90], formal program manipulation using traditional partial evaluation [SH91,Sun91], the change detailing network of INC [YS91], incremental computation as program abstraction [Hoo92], and incremental term rewriting [van92,Fie93]. Such frameworks are called incremental evaluators by Pugh [Pug88b] and general approaches by Field [Fie91]. <p> Two trends seem obvious: studying specialized cache strategies for classes of problems, and adding annotations or certain specifications to subject programs that provide hints to the cache strategies. An example of the former is the stable decomposition scheme proposed by Pugh and Teitelbaum <ref> [PT89] </ref>, who also advocate a closer study of using memoization for incremental evaluation. <p> Some approaches to incremental computation have exploited specific kinds of auxiliary information, e.g., auxiliary arithmetic associated with some classical strength-reduction rules [ACK81], dynamic mappings maintained by finite differencing rules for aggregate primitives in SETL [PK82] and INC [YS91], and auxiliary data structures for problems with certain properties like stable decomposition <ref> [PT89] </ref>. However, until now, systematic discovery of auxiliary information for arbitrary programs has not been studied. This chapter presents a two-phase method that discovers a general class of auxiliary information for any incremental computation problem.
Reference: [PTB85] <author> Robert Paige, Robert Tarjan, and Robert Bonic. </author> <title> A linear time solution to the single coarsest partition problem. </title> <journal> Theoretical Computer Science, </journal> <volume> 40 </volume> <pages> 67-84, </pages> <year> 1985. </year>
Reference: [Pug88a] <author> William Pugh. </author> <title> An improved cache replacement strategy for function caching. </title> <booktitle> In Proceedings of the ACM '88 Conference on LFP, </booktitle> <pages> pages 269-276, </pages> <year> 1988. </year>
Reference-contexts: For example, Hughes [Hug85] discusses lazy memo-functions that are suitable for use in systems with lazy evaluation. Mostow and Cohen [MC85] discuss some issues for speeding up Interlisp programs by caching in the presence of side effects. Pugh <ref> [Pug88a] </ref> provides some improved cache replacement strategies for a simple functional language. Two trends seem obvious: studying specialized cache strategies for classes of problems, and adding annotations or certain specifications to subject programs that provide hints to the cache strategies.
Reference: [Pug88b] <author> William Worthington Pugh, Jr. </author> <title> Incremental Computation and the Incremental Evaluation of Functional Programs. </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: The study of dynamic graph algorithms, such as transitive closure algorithms [Yel93], can be viewed as falling into this class. These incremental algorithms are called explicit incremental algorithms by Pugh <ref> [Pug88b] </ref> and ad hoc incremental algorithms by Field [Fie91]. Although efforts in this class are directed towards particular incremental algo 6 7 rithms, they apply to a broad class of problems, e.g., any attribute grammar and any circuit. <p> Such frameworks are called incremental evaluators by Pugh <ref> [Pug88b] </ref> and general approaches by Field [Fie91]. Note that an incremental execution framework always employs a particular incremental algorithm, where the algorithm is designed for executing a particular class of application programs on their inputs and deals with a particular class of changes in an application program and/or its input.
Reference: [Ram93] <author> G. Ramalingam. </author> <title> Bounded Incremental Computation. </title> <type> Ph.D. dissertation, </type> <institution> Computer Sciences Department, University of Wisconsin, Madi-son, Wisconsin, </institution> <year> 1993. </year> <note> Also appeared as Technical Report #1172, </note> <month> August </month> <year> 1993. </year>
Reference: [Red88] <author> Uday Reddy. </author> <title> Transformational derivation of programs using the Focus system. </title> <booktitle> In Proceedings of the ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <pages> pages 163-172, </pages> <year> 1988. </year> <note> Published as SIGSOFT Software Engineering Notes, </note> <month> 13(5), </month> <journal> November 1988 and SIGPLAN Notices, </journal> <volume> 23(2), </volume> <month> February </month> <year> 1989. </year>
Reference-contexts: They are particularly useful for phase-based transformations. In particular, for phases not involving circular attributes, efficient evaluators for non-circular attribute grammars can be generated. 6.2.4 Replay We want to consider replaying transformations. A minimal approach is to record the history or script of external input <ref> [Red88] </ref>. Powerful metalanguages can help reduce the recording work [Fea82]; for example, with a metalanguage that allows rewrite, we can record one rewrite in place of a sequence of transformations involved in the rewrite. <p> Eminent systems among them and recent systems include APTS [Pai94], KIDS [Smi90,Smi91], CIP [BMPP89, Par90], Focus <ref> [Red88] </ref>, and ZAP [Fea82]. Compared to these systems, the most important and unique characteristic of CACHET is its use of attribute grammars. This has at least two advantages. <p> Of course, implementing CACHET as a programming environment also facilitates the integration of program derivation and validation with interactive editing, compiling, debugging, and execution. CACHET can benefit from a stronger metalanguage, such as that pioneered by ZAP [Fea82], and certain replay functionality, such as that in Focus <ref> [Red88] </ref>. As discussed in Sections 6.2 and 6.3, a metalanguage for powerful tree transformation has been designed and is being implemented for CACHET. How to integrate replay and incremental replay in an attribute grammar framework with annotation is a problem to be studied.
Reference: [Rei84a] <author> Steven P. Reiss. </author> <title> An approach to incremental compilation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <pages> pages 144-156, </pages> <address> Montreal, Canada, </address> <month> June </month> <year> 1984. </year> <note> Published as SIGPLAN Notices, 19(6). </note>
Reference: [Rei84b] <author> Steven P. Reiss. </author> <title> Graphical program development with PECAN program development systems. </title> <booktitle> In Proceedings of the ACM SIG-SOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <pages> pages 30-41, </pages> <address> Pittsburgh, Philadel-phia, </address> <month> April </month> <year> 1984. </year>
Reference: [Rei90a] <author> Steven P. Reiss. </author> <title> Connecting tools using message passing in the Field environment. </title> <journal> IEEE Software, </journal> <volume> 7(4) </volume> <pages> 57-66, </pages> <month> July </month> <year> 1990. </year>
Reference: [Rei90b] <author> Steven P. Reiss. </author> <title> Interacting with the FIELD environment. </title> <journal> Software| Practice and Experience, </journal> <volume> 20(S1):89-115, </volume> <month> June </month> <year> 1990. </year>
Reference: [Rep84] <author> Thomas William Reps. </author> <title> Generating Language-Based Environments. ACM Doctoral Dissertation Award. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1984. </year> <type> Ph.D. dissertation, </type> <institution> Cornell University, </institution> <year> 1982. </year>
Reference: [Rey81] <author> John C. Reynolds. </author> <title> The Craft of Programming. </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year> <month> 127 </month>
Reference: [Ros89] <author> Mads Rosendahl. </author> <title> Automatic complexity analysis. </title> <booktitle> In Proceedings of the 4th International Conference on FPCA, </booktitle> <pages> pages 144-156, </pages> <address> London, U.K., </address> <month> September </month> <year> 1989. </year>
Reference: [RP88] <author> B. G. Ryder and M. C. Paull. </author> <title> Incremental data flow analysis algorithms. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(1) </volume> <pages> 1-50, </pages> <month> January </month> <year> 1988. </year>
Reference: [RR91] <author> G. Ramalingam and Thomas Reps. </author> <title> On the computational complexity of incremental algorithms. </title> <type> Technical Report TR-1033, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> August </month> <year> 1991. </year>
Reference: [RR93] <author> G. Ramalingam and Thomas Reps. </author> <title> A categorized bibliography on incremental computation. </title> <booktitle> In Conference Record of the 20th Annual ACM Symposium on POPL, </booktitle> <pages> pages 502-510, </pages> <address> Charleston, South Carolina, </address> <month> Jan-uary </month> <year> 1993. </year>
Reference-contexts: The large number of works on incremental computation in recent years and their many applications <ref> [RR93] </ref>, demonstrated by various incremental algorithms such as [GM79,JG82,RTD83,Yeh83,RP88,Van88,FMB90,AHR + 90,RR94] and general incremental computation approaches such as [Ear76,Pai81,PK82,Pai84,HT86, CP89,PT89,FT90,Smi90,Smi91,YS91,SH91,Sun91,Hoo92,van92,Fie93], motivated us to look for the fundamentals of incremental computation and their role in efficient computation. <p> The premise of this work is that methods of incremental computation can be generalized and systematized by finding a conceptual model that captures all the fundamentals. A comprehensive guide to methods of incremental computation has appeared in <ref> [RR93] </ref>. Despite the relatively diverse categories discussed in [RR93], we divide most of the work into three classes: incremental algorithms, incremental execution frameworks, and incremental-program derivation approaches. <p> The premise of this work is that methods of incremental computation can be generalized and systematized by finding a conceptual model that captures all the fundamentals. A comprehensive guide to methods of incremental computation has appeared in <ref> [RR93] </ref>. Despite the relatively diverse categories discussed in [RR93], we divide most of the work into three classes: incremental algorithms, incremental execution frameworks, and incremental-program derivation approaches. <p> By specializing the general techniques to different applications, we will be able to obtain particular incremental algorithms, particular incremental computation techniques, and particular incremental computation languages. Their applications could include most problems discussed in the literature <ref> [RR93] </ref>. 114 7.3 Future work Several of the ideas presented in this thesis, including the transformations for incre-mentalization and caching all intermediate results, have been implemented in CACHET, a prototype system for deriving incremental programs from non-incremental programs.
Reference: [RR94] <author> G. Ramalingam and Thomas Reps. </author> <title> An incremental algorithm for maintaining the dominator tree of a reducible flowgraph. </title> <booktitle> In Conference Record of the 21th Annual ACM Symposium on POPL, </booktitle> <address> Portland, Ore-gon, </address> <month> January </month> <year> 1994. </year>
Reference: [RT88] <author> Thomas Reps and Tim Teitelbaum. </author> <title> The Synthesizer Generator: A System for Constructing Language-Based Editors. Texts and Monographs in Computer Science. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Attempts to provide general incremental mechanisms have become more active in the past several years and they mostly fall into this class, e.g., incremental attribute evaluation frameworks <ref> [RT88] </ref>, incremental computation via function caching [PT89], incremental lambda reduction [FT90], formal program manipulation using traditional partial evaluation [SH91,Sun91], the change detailing network of INC [YS91], incremental computation as program abstraction [Hoo92], and incremental term rewriting [van92,Fie93]. <p> This creates a performance bottleneck that is more severe for more automated systems [Pai94]. Using an attribute-grammar-based programming environment. Interactive program transformation and incremental program analysis naturally lead one to consider an attribute-grammar-based programming environment. The Synthesizer Generator is a commercially available system that generates such environments <ref> [RT88] </ref>. With such a tool, the syntax of programs can be described using a context-free grammar, and properties of programs can be described using attribute equations. <p> An alternative to the direct tree manipulation framework for the purpose of replay is discussed in Section 6.6. 6.3 Implementation A prototype system, CACHET, based on the design principles in the previous section, has been implemented. It uses the Synthesizer Generator <ref> [RT88] </ref>, a system for generating language-based editors, and consists of about 18,000 lines of code written 100 in SSL, the Synthesizer Generator language for specifying editors. <p> The values of these attributes can be displayed any time, as facilitated by the Synthesizer Generator <ref> [RT88] </ref>, to help a user understand the derivation. Of course, these attributes are evaluated incrementally after each transformation step. Methods for circular attribute evaluation still need to be implemented, either by extending the Synthesizer Generator or using the simulation proposed in the previous section. <p> Thus, tree attribution is used not only for semantics analysis, but also for recording transformed versions of the program. Attribution has been traditionally used in programming environments for code generation <ref> [RT88] </ref>; it has also been proposed for general program transformation, including phase-based transformation. Approaches include attribute coupled grammars [GG84], higher-order attribute grammars [VSK89], composable attribute grammars [FMY92], and simple tree attributions [BG94]. Incremental attribute evaluation algorithms for these frameworks could be used for incremental code generation.
Reference: [RTD83] <author> Thomas Reps, Tim Teitelbaum, and Alan Demers. </author> <title> Incremental context-dependent analysis for language-based editors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 449-477, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: These values are called intermediate results useful for computing f 0 incrementally under . Examples where intermediate results are needed for incremental computation include incremental parsing [JG82] and incremental attribute evaluation <ref> [RTD83, Yeh83] </ref>. An incremental parser may cache, in addition to the derived parse tree, the LR (0) state corresponding to each shift and reduction. An attribute evaluator may only return some designated synthesized attribute of the root [Kat84], but the corresponding incremental attribute evaluator may cache the whole attributed tree. <p> program computes in O (jPATHj + jAFFECTEDj) time, where PATH is the path from the root of the whole tree to the root of the new subtree, and AFFECTED is the set of attributes whose values are different in the new tree than in the old after the subtree replacement <ref> [RTD83] </ref>. 4.6.4 Local neighborhood problems in image processing We have mentioned that the general principles underlying our approach also apply to other languages. This section gives an example where the cache-and-prune method is used to improve imperative programs with arrays. <p> Since explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover, the general approach in this chapter provides a helpful systematic method for developing particular incremental algorithms. For example, for the dynamic incremental attribute evaluation algorithm in <ref> [RTD83] </ref>, the characteristic graph is a kind of auxiliary information that would be discovered following the general principles underlying our approach. For static incremental attribute evaluation algorithms [Kas80,Kat84], where no auxiliary information is needed, the approach can cache intermediate results and maintain them automatically, as described in Chapter 4.
Reference: [Sco82] <author> Dana S. Scott. </author> <title> Lectures on a mathematical theory of computation. </title> <editor> In Manfred Broy and Gunther Schmidt, editors, </editor> <booktitle> Theoretical Foundations of Programming Methodology, </booktitle> <pages> pages 145-292. </pages> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1982. </year> <booktitle> Lecture notes of 1981 Marktoberdorf Summer School on Theoretical Foundations of Programming Methodology, directed by F.L. Bauer, E.W. Dijkstra, and C.A.R. Hoare. </booktitle>
Reference: [SDB84] <author> Mayer D. Schwartz, Norman M. Delisle, and Vimal S. Begwani. </author> <title> Incremental compilation in magpie. </title> <booktitle> In Proceedings of the ACM SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <pages> pages 122-131, </pages> <month> June </month> <year> 1984. </year> <note> Published as SIGPLAN Notices, 19(6). </note>
Reference: [SH91] <author> R. S. Sundaresh and Paul Hudak. </author> <title> Incremental computation via partial evaluation. </title> <booktitle> In Conference Record of the 18th Annual ACM Symposium on POPL, </booktitle> <pages> pages 1-13, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Since the transformational approach is related to partial evaluation in some aspects, it is worthwhile to compare it with the work by Sundaresh and Hudak <ref> [SH91, Sun91] </ref> in the second class. The common aspect is that both works aim at obtaining incremental computation by transforming non-incremental programs. However, the two approaches follow different lines. Their work mostly uses partial evaluation, with extra efforts on partitioning program inputs and combining residual programs.
Reference: [SKR91] <author> Bernhard Steffen, Jens Knoop, and Oliver Ruthing. </author> <title> Efficient code motion and an adaption to strength reduction. </title> <booktitle> In Proceedings of the 128 4th International Joint Conference on TAPSOFT, volume 494 of Lecture Notes in Computer Science, </booktitle> <pages> pages 394-415, </pages> <address> Brighton, U.K., 1991. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: For static incremental attribute evaluation algorithms [Kas80,Kat84], where no auxiliary information is needed, the approach can cache intermediate results and maintain them automatically, as described in Chapter 4. Strength reduction <ref> [All69, CK77, ACK81, SKR91] </ref> is a traditional compiler optimization technique that aims at computing each iteration incrementally based on the result of the previous iteration. Basically, a fixed set of strength-reduction rules for primitive operators like times and plus are used.
Reference: [SL90] <author> Douglas R. Smith and Michael R. Lowry. </author> <title> Algorithm theories and design tactics. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 14 </volume> <pages> 305-321, </pages> <year> 1990. </year>
Reference-contexts: Smith's work in KIDS [Smi90,Smi91] is closely related to this work. KIDS is a 39 semi-automatic program development system that aims to derive efficient programs from high-level specifications <ref> [SL90] </ref>, as is APTS. Its version of finite differencing was developed for the optimization of its derived functional programs and has two basic operations: abstraction and simplification. Abstraction of a function f adds an extra cache parameter to f .
Reference: [Smi90] <author> Douglas R. Smith. KIDS: </author> <title> A semiautomatic program development system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(9) </volume> <pages> 1024-1043, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers [Ear76,CK77,MJ81,ASU86] and transformational programming <ref> [Pai83,Par90, Smi90] </ref>, interactive systems like programming environments [MF81,Rep84] and editors [RTD83,RT88,BGV92], and dynamic systems like distributed databases [LS92, CHKS93,ZGMHW94] and real-time systems [VC92]. The premise of this work is that methods of incremental computation can be generalized and systematized by finding a conceptual model that captures all the fundamentals. <p> Such approaches are characterized as syntactic, using only formal notations, whereas a semantic solution that explores the mathematical content of the subject has not been developed [Ner92]. Another highly successful contribution for deriving efficient programs is by Smith <ref> [Smi90, Smi91] </ref>. It advocates a more flexible approach to transforming programs written in a functional language. In this work, a high-level strategy is given and a general simplifier is used for finite differencing of functional programs that takes advantage mainly of distributivity laws. <p> Similarly, the APTS program transformation system [Pai83,Pai90,Pai94] identifies set expressions in SETL that can be maintained using finite differencing rules [PK82]. Sometimes it is not sufficient to have only a fixed set of strategies and rules. Seeking more flexibility and broader applicability, KIDS <ref> [Smi90] </ref> advocates certain high-level strategies but leaves the choice of which intermediate results to maintain to manual decisions. CIP [Par90] also proposes a general strategy for caching, but it may even lead to less efficient programs. Recently, certain principles that can directly guide program transformations have been proposed.
Reference: [Smi91] <author> Douglas R. Smith. </author> <title> KIDS|a knowledge-based software development system. </title> <editor> In Michael R. Lowry and Robert D. McCartney, editors, </editor> <booktitle> Automating Software Design, chapter 19, </booktitle> <pages> pages 483-514. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1991. </year> <booktitle> Proceedings of the AAAI '88 Workshop on Automating Software Design. </booktitle>
Reference-contexts: Such approaches are characterized as syntactic, using only formal notations, whereas a semantic solution that explores the mathematical content of the subject has not been developed [Ner92]. Another highly successful contribution for deriving efficient programs is by Smith <ref> [Smi90, Smi91] </ref>. It advocates a more flexible approach to transforming programs written in a functional language. In this work, a high-level strategy is given and a general simplifier is used for finite differencing of functional programs that takes advantage mainly of distributivity laws.
Reference: [SP93] <author> Douglas R. Smith and Eduardo A. </author> <title> Parra. Transformational approach to transportation scheduling. </title> <booktitle> In Proceedings of the 8th Knowledge-Base Software Engineering Conference, </booktitle> <address> Chicago, Illinois, September 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: [ST83] <author> Daniel D. Sleator and Robert Endre Tarjan. </author> <title> A data structure for dynamic trees. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 26 </volume> <pages> 362-391, </pages> <year> 1983. </year>
Reference-contexts: For example, in SETL, several kinds of maps are maintained for functions that are not continuous for certain input changes [Pai83]. In dynamic graph algorithms, several special data structures are employed: dynamic trees <ref> [ST83] </ref>, topology trees [Fre85], and sparsification trees [EGIN92]. Ideally, we can collect classes of special parameterized data structures as auxiliary information parameterized with certain data types.
Reference: [Sun91] <author> R. S. Sundaresh. </author> <title> Building incremental programs using partial evaluation. </title> <booktitle> In Proceedings of the Symposium on PEPM, </booktitle> <pages> pages 83-93, </pages> <institution> Yale University, </institution> <month> June </month> <year> 1991. </year> <note> Published as SIGPLAN Notices, 26(9). </note>
Reference-contexts: Since the transformational approach is related to partial evaluation in some aspects, it is worthwhile to compare it with the work by Sundaresh and Hudak <ref> [SH91, Sun91] </ref> in the second class. The common aspect is that both works aim at obtaining incremental computation by transforming non-incremental programs. However, the two approaches follow different lines. Their work mostly uses partial evaluation, with extra efforts on partitioning program inputs and combining residual programs.
Reference: [SVT93] <author> S. Sairam, Jeffrey Scott Vitter, and Roberto Tamassia. </author> <title> A complexity theoretic approach to incremental computation. </title> <booktitle> In Proceedings of STACS '93: the 10th Annual Symposium on TACS, volume 665 of Lecture Notes in Computer Science, </booktitle> <pages> pages 640-649, </pages> <address> Wurzburg, Germany, </address> <month> February </month> <year> 1993. </year> <note> Springer-Verlag, Berlin. </note>
Reference: [Tra86] <author> Kenneth R. Traub. </author> <title> A compiler for the MIT tagged-token dataflow architecture. </title> <type> Masters dissertation, </type> <institution> Department of Electrical Engineering and Computer Science, MIT, </institution> <year> 1986. </year> <note> Appeared as Technical Report LCS TR-370, </note> <month> August, </month> <year> 1986. </year>
Reference-contexts: It would be nice to avoid constructing and passing the unnecessary components. Related work is done in optimizing compilers that 61 eliminate unnecessary tuple constructions and destructions in functional programs; for example, the Id compiler <ref> [Tra86] </ref> does tuple elimination. We think our analyses and transformations provide a straightforward solution to such problems. For us, it is more lightweight than trying to adopt any of the existing techniques.
Reference: [Tur86] <author> Valentin F. Turchin. </author> <title> The concept of a supercompiler. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(3) </volume> <pages> 292-325, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: An interaction with this comes from using a version of generalization <ref> [Tur86] </ref> that enables f 0 to be used in more general settings and, at the same time, does not impede the discovery of incrementality. Considerations. Our use of generalization ignores substructures of expressions to introduce functions for more general uses. <p> Third, consistent with the strict semantics of our language, we apply simplification and replacement on subcomputations in applicative order and, moreover, lift conditions and bindings out of subcomputations. This lifting technique is similar in spirit to the driving transformation by supercompilation <ref> [Tur86] </ref>. It causes relatively drastic reorganization of program structures that helps expose incrementality that is otherwise hidden. Fourth, a global definition set is maintained and used to replace function applications, with corresponding relevant cache elements and valid context information, by applications of introduced functions.
Reference: [van86] <editor> J. C. van Vliet, editor. </editor> <booktitle> Text Processing and Document Manipulation: Proceedings of the International Conference, </booktitle> <institution> University of Nottingham, </institution> <address> April 1986. </address> <publisher> British Computer Society, Cambridge University Press. </publisher>
Reference: [Van88] <author> Bradley T. Vander Zanden. </author> <title> Incremental Constraint Satisfaction and its Application to Graphical Interfaces. </title> <type> Ph.D. dissertation, </type> <institution> Department of 129 Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <year> 1988. </year> <note> Also appeared as Technical Report TR 88-941, </note> <month> October </month> <year> 1988. </year>
Reference: [van92] <author> Emma A. van der Meulen. </author> <title> Deriving incremental implementations from algebraic sepcifications. </title> <booktitle> In Proceedings of the 2nd International Conference on Algebraic Methodology and Software Technology, </booktitle> <pages> pages 277-286. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1992. </year>
Reference: [VC92] <author> A. Varma and S. Chalasani. </author> <title> An incremental algorithm for TDM switching assignments in satellite and terrestrial networks. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 10(2) </volume> <pages> 364-377, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers [Ear76,CK77,MJ81,ASU86] and transformational programming [Pai83,Par90, Smi90], interactive systems like programming environments [MF81,Rep84] and editors [RTD83,RT88,BGV92], and dynamic systems like distributed databases [LS92, CHKS93,ZGMHW94] and real-time systems <ref> [VC92] </ref>. The premise of this work is that methods of incremental computation can be generalized and systematized by finding a conceptual model that captures all the fundamentals. A comprehensive guide to methods of incremental computation has appeared in [RR93].
Reference: [VSK89] <author> Harald H. Vogt, Doaitse Swierstra, and M. F. Kuiper. </author> <title> Higher order attribute grammars. </title> <booktitle> In Proceedings of the ACM SIGPLAN '89 Conference on PLDI, </booktitle> <pages> pages 131-145, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Attribution has been traditionally used in programming environments for code generation [RT88]; it has also been proposed for general program transformation, including phase-based transformation. Approaches include attribute coupled grammars [GG84], higher-order attribute grammars <ref> [VSK89] </ref>, composable attribute grammars [FMY92], and simple tree attributions [BG94]. Incremental attribute evaluation algorithms for these frameworks could be used for incremental code generation. However, these frameworks do not address external input.
Reference: [VVF90] <author> Harald Vogt, Aswin Van den Berg, and Arend Freije. </author> <title> Rapid development of a program transformation system with attribute grammars and dynamic trnsformations. </title> <booktitle> In Proceedings of the International Workshop on Attribute Grammars and their Applications, volume 461 of Lecture Notes in Computer Science, </booktitle> <pages> pages 101-115, </pages> <address> Paris, France, September 1990. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: Second, incremental program analysis is important for speeding up the overall program transformation process; using attribute evaluation for program analysis allows us to take advantage of known techniques for incremental attribute evaluation. To our knowledge, the BMF-editor <ref> [VVF90] </ref> is the only program transformation system that uses the incremental attribution paradigm, although incremental semantic analysis is desirable in all these systems, especially for the more automatic approaches in APTS 108 [Pai94]. The BMF-editor emphasizes dynamic transformations but does not explicitly address the problem of external inputs.
Reference: [Web92] <author> J. Webb. </author> <title> Steps towards architecture-independent image processing. </title> <booktitle> IEEE Computer, </booktitle> <month> February </month> <year> 1992. </year>
Reference: [Web93] <author> Adam Brooks Webber. </author> <title> Principled Optimization of Functional Programs. </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <year> 1993. </year> <note> Also appeared as Technical Report TR 93-1363, </note> <month> June </month> <year> 1993. </year>
Reference: [Web95] <author> Adam Webber. </author> <title> Optimization of functional programs by grammar thinning. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17(2) </volume> <pages> 293-330, </pages> <month> March </month> <year> 1995. </year>
Reference: [Weg75] <author> Ben Wegbreit. </author> <title> Mechanical program analysis. </title> <journal> Communications of the ACM, </journal> <volume> 18(9) </volume> <pages> 528-538, </pages> <month> September </month> <year> 1975. </year>
Reference: [Weg76] <author> Ben Wegbreit. </author> <title> Goal-directed program transformation. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-2(2):69-80, </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: Then, we obtain a definition of f 0 by the following three steps. First, we unfold [BD77] (also called expand <ref> [Weg76] </ref>) the application. Second, we incrementalize the unfolded application.
Reference: [Wei84] <author> Mark Weiser. </author> <title> Program slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(4):352-357, </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: In this aspect, the resulting program here is similar to the slice obtained from forward slicing <ref> [Wei84] </ref>. However, our forward dependency analysis is different from the forward slicing analysis in that forward dependency analysis finds parts of a program that depend only on certain information, while forward slicing finds parts of a program that depend possibly on certain information.
Reference: [Wel86] <author> William M. Wells, III. </author> <title> Efficient synthesis of Gaussian filters by cascaded uniform filters. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8(2) </volume> <pages> 234-239, </pages> <month> March </month> <year> 1986. </year>
Reference: [WH87] <author> Philip Wadler and R. J. M. Hughes. </author> <title> Projections for strictness analysis. </title> <booktitle> In Proceedings of the 3rd International Conference on FPCA, volume 274 of Lecture Notes in Computer Science, </booktitle> <pages> pages 385-407, </pages> <address> Portland, Oregon, </address> <month> September </month> <year> 1987. </year> <month> 130 </month>
Reference-contexts: Dependency analysis. To compute which components of r are needed for computing certain components of f 0 0 (x; y; r), we apply a backward dependency analysis to the program f 0 0 . Following the style of <ref> [WH87] </ref>, for each function f of n parameters, and each i from 1 to n, we define f i to be a dependency transformer that takes a projection that is applied to the result of f and returns a projection that is sufficient to be applied to the ith parameter. <p> Second, the backward dependency analysis and pruning transformations in Stage III use domain projections to specify sufficient information, which is natural and thus simple. Other uses of projections include the strictness analysis by Wadler and Hughes <ref> [WH87] </ref>, where necessary information needs to be specified and thus accounts for some complications, and binding-time analysis by Launchbury [Lau89], which is a forward analysis and is proved equivalent to strictness analysis [Lau91].
Reference: [Yeh83] <author> D. Yeh. </author> <title> On incremental evaluation of ordered attribute grammars. </title> <journal> BIT, </journal> <volume> 23 </volume> <pages> 308-320, </pages> <year> 1983. </year>
Reference-contexts: These values are called intermediate results useful for computing f 0 incrementally under . Examples where intermediate results are needed for incremental computation include incremental parsing [JG82] and incremental attribute evaluation <ref> [RTD83, Yeh83] </ref>. An incremental parser may cache, in addition to the derived parse tree, the LR (0) state corresponding to each shift and reduction. An attribute evaluator may only return some designated synthesized attribute of the root [Kat84], but the corresponding incremental attribute evaluator may cache the whole attributed tree.
Reference: [Yel93] <author> Daniel M. Yellin. </author> <title> Speeding up dynamic transitive closure for bounded degree graphs. </title> <journal> Acta Informatica, </journal> <volume> 30(4) </volume> <pages> 369-384, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Examples include incremental parsing [GM79,JG82], incremental attribute evaluation [RTD83,Yeh83,YK88,LMOW88, Jon90], incremental data-flow analysis [Zad84,RP88,Bur90,MR90,RR94], incremental circuit evaluation [AHR + 90], and incremental constraint solving [Van88,FMB90]. The study of dynamic graph algorithms, such as transitive closure algorithms <ref> [Yel93] </ref>, can be viewed as falling into this class. These incremental algorithms are called explicit incremental algorithms by Pugh [Pug88b] and ad hoc incremental algorithms by Field [Fie91].
Reference: [YK88] <author> D. Yeh and U. Kastens. </author> <title> Improvements on an incremental evaluation algorithm for ordered attribute grammars. </title> <journal> SIGPLAN Notices, </journal> <volume> 23(12) </volume> <pages> 45-50, </pages> <year> 1988. </year>
Reference: [YS91] <author> Daniel M. Yellin and Robert E. Strom. INC: </author> <title> A language for incremental computations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 211-236, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: provide general incremental mechanisms have become more active in the past several years and they mostly fall into this class, e.g., incremental attribute evaluation frameworks [RT88], incremental computation via function caching [PT89], incremental lambda reduction [FT90], formal program manipulation using traditional partial evaluation [SH91,Sun91], the change detailing network of INC <ref> [YS91] </ref>, incremental computation as program abstraction [Hoo92], and incremental term rewriting [van92,Fie93]. Such frameworks are called incremental evaluators by Pugh [Pug88b] and general approaches by Field [Fie91]. <p> This information needs to be maintained efficiently as well. Some approaches to incremental computation have exploited specific kinds of auxiliary information, e.g., auxiliary arithmetic associated with some classical strength-reduction rules [ACK81], dynamic mappings maintained by finite differencing rules for aggregate primitives in SETL [PK82] and INC <ref> [YS91] </ref>, and auxiliary data structures for problems with certain properties like stable decomposition [PT89]. However, until now, systematic discovery of auxiliary information for arbitrary programs has not been studied. This chapter presents a two-phase method that discovers a general class of auxiliary information for any incremental computation problem.
Reference: [Zab94] <author> Ramin Zabih. </author> <title> Individuating Unknown Objects by Combining Motion and Stereo. </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science, Stan-ford University, Stanford, California, </institution> <year> 1994. </year>
Reference: [Zad84] <author> Frank Kenneth Zadeck. </author> <title> Incremental data flow analysis in a structured program editor. </title> <booktitle> In Proceedings of the ACM SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <pages> pages 132-143, </pages> <month> June </month> <year> 1984. </year> <note> Published as SIGPLAN Notices, 19(6). </note>
Reference: [ZGMHW94] <author> Yue Zhuge, Hector Garcia-Molina, Joachim Hammer, and Jennifer Widom. </author> <title> View maintenance in a warehousing environment. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <address> San Jose, California, </address> <month> June </month> <year> 1994. </year>

References-found: 185

