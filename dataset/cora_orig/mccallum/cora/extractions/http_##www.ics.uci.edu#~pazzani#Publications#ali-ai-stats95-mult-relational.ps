URL: http://www.ics.uci.edu/~pazzani/Publications/ali-ai-stats95-mult-relational.ps
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: ali@ics.uci.edu, pazzani@ics.uci.edu  
Title: Classification using Bayes Averaging of Multiple, Relational Rule-based Models  
Author: Kamal Ali and Michael Pazzani 
Note: 1.1 Introduction  
Address: Irvine, CA, 92717, USA.  
Affiliation: Department of Information and Computer Science, University of California,  
Abstract: We present a way of approximating the posterior probability of a rule-set model that is comprised of a set of class descriptions. Each class description, in turn, consists of a set of relational rules. The ability to compute this posterior and to learn many models from the same training set allows us to approximate the expectation that an example to be classified belongs to some class. The example is assigned to the class maximizing the expectation. By assuming a uniform prior distribution of models, the posterior of the model does not depend on the structure of the model: it only depends on how the training examples are partitioned by the rules of the rule-set model. This uniform distribution assumption allows us to compute the posterior for models containing relational and recursive rules. Our approximation to the posterior probability yields significant improvements in accuracy as measured on four relational data sets and four attribute-value data sets from the UCI repository. We also provide evidence that learning multiple There has been much work in learning relational models of data in recent years (e.g. FOIL: [Quinlan90]; FOCL: [Pazzani-Kibler91]; GOLEM: [Muggleton-Feng90], CLINT: [deRaedt-Bruynooghe88], ML-SMART: [Bergadano-et-al88]). Here we present results that apply Bayesian probability theory (e.g. [Berger85]) to learning and classification using class descriptions consisting of first-order (relational) rules. According to this theory, it is necessary to learn several models of the data. In our work, each model consists of one class description per class in the data so learning multiple models implies learning many descriptions for each class. Although there has been prior work in applying Bayesian probability theory for learning and combining evidence from multiple decision trees ([Buntine90]) there has been no such work for relational rules or for rule-set models. A rule-set model is defined to consist of a set of class descriptions, one description per class in the data. Each description, in turn, consists of a set of rules that all conclude for the same class (Table 1.1). Our core algorithm is based on FOIL which learns a disjoint covering for a class. That is, it learns rules for a class that cover disjoint subsets of the training examples of that class. Therefore, ideally, each learned rule should correspond to a disjunct or subclass in the class. By applying FOIL stochastically many times for each class, we are able to model each disjunct more than once. This in turn leads to a better overall model of each subclass and hence of the class. Our goal is to demonstrate that classifications obtained models helps most in data sets in which there are many, apparently equally good rules to learn.
Abstract-found: 1
Intro-found: 1
Reference: [Ali-Pazzani93] <author> Ali K. and Pazzani M. </author> <year> (1993). </year> <title> HYDRA: A Noise-tolerant Relational Concept Learning Algorithm. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence. </booktitle> <address> Chambery, France: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The second result is that learning and using multiple models is especially helpful in data sets where there are many, apparently equally good rules to learn. To demonstrate these results, we adapt a relational learning algorithm (HYDRA, <ref> [Ali-Pazzani93] </ref>) to learn multiple models, yielding HYDRA-MM. HYDRA-MM learns multiple rule-set models, therefore it also learns more than one class description for each class.
Reference: [Bergadano-et-al88] <author> Bergadano F., Giordana A. </author> <title> (1988) A Knowledge Intensive Approach to Concept Induction. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning., </booktitle> <address> Ann Arbor, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1.1 Introduction There has been much work in learning relational models of data in recent years (e.g. FOIL: [Quinlan90]; FOCL: [Pazzani-Kibler91]; GOLEM: [Muggleton-Feng90], CLINT: [deRaedt-Bruynooghe88], ML-SMART: <ref> [Bergadano-et-al88] </ref>). Here we present results that apply Bayesian probability theory (e.g. [Berger85]) to learning and classification using class descriptions consisting of first-order (relational) rules. According to this theory, it is necessary to learn several models of the data.
Reference: [Berger85] <author> Berger J. O. </author> <year> (1985). </year> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: 1.1 Introduction There has been much work in learning relational models of data in recent years (e.g. FOIL: [Quinlan90]; FOCL: [Pazzani-Kibler91]; GOLEM: [Muggleton-Feng90], CLINT: [deRaedt-Bruynooghe88], ML-SMART: [Bergadano-et-al88]). Here we present results that apply Bayesian probability theory (e.g. <ref> [Berger85] </ref>) to learning and classification using class descriptions consisting of first-order (relational) rules. According to this theory, it is necessary to learn several models of the data.
Reference: [Buntine90] <author> Buntine W. </author> <year> (1990). </year> <title> A Theory of Learning Classification Rules. </title> <type> Doctoral dissertation. </type> <institution> School of Computing Science, University of Technology, </institution> <address> Sydney, Australia. </address>
Reference-contexts: In practice, we only compute this expectation over a small set of highly probable models. Using the notation in <ref> [Buntine90] </ref>, let c be a class, H denote the model space, x be a test example, ~x denote the training examples and ~c denote the class labels of the training examples. <p> Such an ordering is needed firstly to produce a partition of the training data as required by the framework of <ref> [Buntine90] </ref> and secondly to avoid the issue of how to combine evidence from rules (called the "rule overlap problem," [?]). The foregoing paragraphs illustrate how to compute the posterior probability of a rule-set model and how to estimate it from data.
Reference: [deRaedt-Bruynooghe88] <author> De Raedt L. and Bruynooghe M. </author> <year> (1988). </year> <title> On Interactive concept-learning and assimilation. </title> <editor> In D. Sleeman (Ed.), </editor> <booktitle> Proceeings of the Third European Working Session on Learning. </booktitle> <pages> (pp. </pages> <month> 167-176). </month> <title> Pitman. Classification using Bayes Averaging of Multiple, Relational Rule-based Models xi </title>
Reference-contexts: 1.1 Introduction There has been much work in learning relational models of data in recent years (e.g. FOIL: [Quinlan90]; FOCL: [Pazzani-Kibler91]; GOLEM: [Muggleton-Feng90], CLINT: <ref> [deRaedt-Bruynooghe88] </ref>, ML-SMART: [Bergadano-et-al88]). Here we present results that apply Bayesian probability theory (e.g. [Berger85]) to learning and classification using class descriptions consisting of first-order (relational) rules. According to this theory, it is necessary to learn several models of the data.
Reference: [Esposito93] <author> Esposito F., Malerba D. and Semeraro G. </author> <year> (1992). </year> <title> Classification in Noisy Environments Using a Distance Measure Between Structural Symbolic Descriptions. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14, </volume> <pages> 3. </pages>
Reference: [Gams89] <institution> New Measurements Highlight the Importance of Redundant Knowledge. </institution> <note> In European Working Session on Learning (4th : 1989 : Montpeiller, France). Pitman. </note>
Reference-contexts: Previous work in learning multiple models consisting of rules has been done by <ref> [Gams89] </ref> and [Kononenko92]. The limitation of Gams' approach is that it does not retain the multiple models for classification. Instead, after learning several models of the data, a single model is constructed whose rules are chosen from the rules of the previously learned models.
Reference: [Kononenko92] <author> Kononenko I. and Kovacic M. </author> <year> (1992). </year> <title> Learning as Optimization: Stochastic Generation of Multiple Knowledge. </title> <booktitle> In Machine Learning: Proceedings of the Ninth International Workshop. </booktitle> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Previous work in learning multiple models consisting of rules has been done by [Gams89] and <ref> [Kononenko92] </ref>. The limitation of Gams' approach is that it does not retain the multiple models for classification. Instead, after learning several models of the data, a single model is constructed whose rules are chosen from the rules of the previously learned models. Kononenko & Kovacic ([Kononenko92]) have also done work in
Reference: [Kruskal78] <editor> Kruskal W.H. and Tanur J.M. </editor> <booktitle> (1978). International encyclopedia of statistics. </booktitle> <address> New York, NY: </address> <publisher> Free Press. </publisher>
Reference: [Kwok90] <author> Kwok S. and Carter C. </author> <year> (1990). </year> <title> Multiple decision trees. </title> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <volume> 4, </volume> <pages> 327-335. </pages>
Reference: [Muggleton89] <author> Muggleton S., Bain M., Hayes-Michie J. and Michie D. </author> <year> (1989). </year> <title> An experimental comparison of human and machine-learning formalisms. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning. </booktitle> <address> Ithaca, NY. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Muggleton-Feng90] <author> Muggleton S. and Feng C. </author> <year> (1990). </year> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Learning Theory. </booktitle> <address> Tokyo. </address> <publisher> Ohmsha Press. </publisher>
Reference-contexts: 1.1 Introduction There has been much work in learning relational models of data in recent years (e.g. FOIL: [Quinlan90]; FOCL: [Pazzani-Kibler91]; GOLEM: <ref> [Muggleton-Feng90] </ref>, CLINT: [deRaedt-Bruynooghe88], ML-SMART: [Bergadano-et-al88]). Here we present results that apply Bayesian probability theory (e.g. [Berger85]) to learning and classification using class descriptions consisting of first-order (relational) rules. According to this theory, it is necessary to learn several models of the data.
Reference: [Pazzani-Brunk91] <author> Pazzani M. and Brunk C. </author> <year> (1991). </year> <title> Detecting and correcting errors in rule-based expert systems: an integration of empirical and explanation-based learning. </title> <journal> Knowledge Acquisition, </journal> <volume> 3, </volume> <pages> 157-173. </pages>
Reference: [Pazzani-Kibler91] <author> Pazzani M. and Kibler D. </author> <year> (1991). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9, 1, </volume> <pages> 57-94. </pages>
Reference-contexts: 1.1 Introduction There has been much work in learning relational models of data in recent years (e.g. FOIL: [Quinlan90]; FOCL: <ref> [Pazzani-Kibler91] </ref>; GOLEM: [Muggleton-Feng90], CLINT: [deRaedt-Bruynooghe88], ML-SMART: [Bergadano-et-al88]). Here we present results that apply Bayesian probability theory (e.g. [Berger85]) to learning and classification using class descriptions consisting of first-order (relational) rules. According to this theory, it is necessary to learn several models of the data.
Reference: [Quinlan90] <author> Quinlan R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5, 3. </volume> <editor> [1] segal-etzioni94 Segal R. and Etzioni O. </editor> <booktitle> (1994). "Learning Decision Lists Using Homogoneous Rules" in Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: 1.1 Introduction There has been much work in learning relational models of data in recent years (e.g. FOIL: <ref> [Quinlan90] </ref>; FOCL: [Pazzani-Kibler91]; GOLEM: [Muggleton-Feng90], CLINT: [deRaedt-Bruynooghe88], ML-SMART: [Bergadano-et-al88]). Here we present results that apply Bayesian probability theory (e.g. [Berger85]) to learning and classification using class descriptions consisting of first-order (relational) rules. According to this theory, it is necessary to learn several models of the data.
Reference: [Smyth92] <author> Smyth P. and Goodman R. </author> <year> (1992). </year> <title> Rule Induction Using Information Theory. </title> <editor> In G. Piatetsky-Shapiro (ed.) </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press, MIT Press. </publisher>
References-found: 16

