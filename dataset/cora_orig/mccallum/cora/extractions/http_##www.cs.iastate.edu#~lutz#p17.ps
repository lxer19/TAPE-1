URL: http://www.cs.iastate.edu/~lutz/p17.ps
Refering-URL: http://www.cs.iastate.edu/~lutz/papers.html
Root-URL: http://www.cs.iastate.edu
Title: Computational Depth and Reducibility  
Author: David W. Juedes, James I. Lathrop, and Jack H. Lutz 
Note: Contents  
Address: Ames, IA 50011  
Affiliation: Department of Computer Science Iowa State University  
Abstract: This paper reviews and investigates Bennett's notions of strong and weak computational depth (also called logical depth) for infinite binary sequences. Roughly, an infinite binary sequence x is defined to be weakly useful if every element of a non-negligible set of decidable sequences is reducible to x in recursively bounded time. It is shown that every weakly useful sequence is strongly deep. This result (which generalizes Bennett's observation that the halting problem is strongly deep) implies that every high Turing degree contains strongly deep sequences. It is also shown that, in the sense of Baire category, almost every infinite binary sequence is weakly deep, but not strongly deep. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Adleman. </author> <title> Time, space, and randomness. </title> <type> Technical Report MIT/LCS/79/TM-131, </type> <institution> Massachusettes Institute of Technology, Laboratory for Computer Science, </institution> <month> March </month> <year> 1979. </year>
Reference-contexts: appear in the sections to follow.) Since this shortest description contains all the information in the object, the depth thus represents the amount of "computational work" that has been "added" to this information and "stored in the organization" of the object. (Depth is closely related to Adleman's notion of "potential" <ref> [1] </ref> and Koppel's notion of "sophistication" [24, 25].) One way to investigate the computational usefulness of an object is to investigate the class of computational problems that can be solved efficiently, given access to the object.
Reference: [2] <author> J. L. Balcazar, J. Daz, and J. Gabarro. </author> <title> Structural Complexity I. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Turing machine model to define algorithmic information, algorithmic probability, and algorithmic depth in sections 4 and 5, we assume that the reader is already familiar with the general ideas of Turing machine computation, including computation by oracle Turing machines. (Discussion of such machines may be found in many texts, e.g., <ref> [2, 19, 44, 50] </ref>.) Given a recursive time bound s : N ! N, we say that an oracle Turing machine M is s-time-bounded if, given any input n 2 N and oracle y 2 f0; 1g 1 , M outputs a bit M y (n) 2 f0; 1g in at <p> Using standard techniques <ref> [2, 19] </ref>, it is easy to show that, for every recursive function r : N ! N, there is a strictly increasing, time-constructible function s : N ! N such that, for all n 2 N, r (n) s (n). Lemma 5.5.
Reference: [3] <author> Y. M. </author> <title> Barzdin 0 . Complexity of programs to determine whether natural numbers not greater than n belong to a recursively enumerable set. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 9 </volume> <pages> 1251-1254, </pages> <year> 1968. </year>
Reference-contexts: An interesting feature of this example is that K has relatively low information content. In fact, an n-bit prefix of K , denoted K [0::n 1], contains only O (log n) bits of algorithmic information <ref> [3] </ref>. Intuitively, this is because K [0::n 1] is completely specified by the number of indices 2 i 2 f0; : : : ; n1g such that the i th Turing machine M i halts on input i.
Reference: [4] <author> C. H. Bennett. </author> <title> Dissipation, information, computational complexity and the definition of organization. </title> <editor> In D. Pines, editor, </editor> <booktitle> Emerging Syntheses in Science, Proceedings of the Founding Workshops of the Santa Fe Institute, </booktitle> <pages> pages 297-313, </pages> <year> 1985. </year>
Reference-contexts: However, a given quantity of information may be organized in various ways, rendering it more or less useful for various computational purposes. In order to quantify the degree to which the information in a computational, physical, or biological object has been organized, Bennett <ref> [4, 5] </ref> has extended algorithmic information theory by defining and investigating the computational depth of binary strings and binary sequences. Roughly speaking, the computational depth (called "logical depth" by Bennett [4, 5]) of an object is the amount of time required for an algorithm to derive the object from its shortest <p> order to quantify the degree to which the information in a computational, physical, or biological object has been organized, Bennett <ref> [4, 5] </ref> has extended algorithmic information theory by defining and investigating the computational depth of binary strings and binary sequences. Roughly speaking, the computational depth (called "logical depth" by Bennett [4, 5]) of an object is the amount of time required for an algorithm to derive the object from its shortest description. (Precise definitions appear in the sections to follow.) Since this shortest description contains all the information in the object, the depth thus represents the amount of "computational work" that
Reference: [5] <author> C. H. Bennett. </author> <title> Logical depth and physical complexity. </title> <editor> In R. Herken, editor, </editor> <booktitle> The Universal Turing Machine: A Half-Century Survey, </booktitle> <pages> pages 227-257. </pages> <publisher> Oxford University Press, </publisher> <year> 1988. </year>
Reference-contexts: However, a given quantity of information may be organized in various ways, rendering it more or less useful for various computational purposes. In order to quantify the degree to which the information in a computational, physical, or biological object has been organized, Bennett <ref> [4, 5] </ref> has extended algorithmic information theory by defining and investigating the computational depth of binary strings and binary sequences. Roughly speaking, the computational depth (called "logical depth" by Bennett [4, 5]) of an object is the amount of time required for an algorithm to derive the object from its shortest <p> order to quantify the degree to which the information in a computational, physical, or biological object has been organized, Bennett <ref> [4, 5] </ref> has extended algorithmic information theory by defining and investigating the computational depth of binary strings and binary sequences. Roughly speaking, the computational depth (called "logical depth" by Bennett [4, 5]) of an object is the amount of time required for an algorithm to derive the object from its shortest description. (Precise definitions appear in the sections to follow.) Since this shortest description contains all the information in the object, the depth thus represents the amount of "computational work" that <p> While every recursive sequence is Turing reducible to K in polynomial time, a recursive sequence y 2 f0; 1g 1 is Turing reducible to z in polynomial time if and only if y is in the complexity class BPP <ref> [5, 8] </ref>. (The class BPP, defined by Gill [17], consists of those sequences y 2 f0; 1g 1 such that there is a randomized algorithm that decides y [n], the n th bit of y, with error probability less than 1 n , using time that is at most polynomial in <p> Bennett has argued that the computational usefulness of K derives not from its algorithmic information content (which is relatively low), but rather from its computational depth. In support of this thesis, Bennett <ref> [5] </ref> has proven that K is strongly deep, while no algorithmically random sequence can even be weakly deep. (Precise definitions of these terms appear in sections 5 and 6 below.) This paper furthers Bennett's investigation of the computational depth of infinite binary sequences. <p> z is not weakly useful, by the following two facts. (i) For every recursive time bound s : N ! N there exists a recursive time bound b s : N ! N such that, for all algorithmically random sequences z, DTIME z (s) " REC DTIME ( b s) <ref> [5, 8, 7] </ref>. (ii) For every recursive time bound b s : N ! N, DTIME ( b s) has measure 0 in REC [37]. Our main result, Theorem 5.11 below, establishes that every weakly useful sequence is strongly deep. <p> Our main result, Theorem 5.11 below, establishes that every weakly useful sequence is strongly deep. This implies that every high Turing degree contains strongly deep sequences (Corollary 5.15). Since the Turing degree of K is one of many high Turing degrees, our main result thus generalizes Bennett's result <ref> [5] </ref> that K is strongly deep. More importantly, our main result rigorously confirms Bennett's intuitive arguments relating the computational usefulness of K to its depth. The fact that the useful sequence K is strongly deep is no coincidence. Every sequence that is even weakly useful must be strongly deep. Bennett [5] <p> <ref> [5] </ref> that K is strongly deep. More importantly, our main result rigorously confirms Bennett's intuitive arguments relating the computational usefulness of K to its depth. The fact that the useful sequence K is strongly deep is no coincidence. Every sequence that is even weakly useful must be strongly deep. Bennett [5] also defines the class of weakly deep binary sequences. (As noted by Bennett, this class has been investigated in other guises by Levin and V'jugin [28, 31, 32, 53, 54, 55].) A sequence x 2 f0; 1g 1 is weakly deep if there do not exist a recursive time bound <p> Bennett <ref> [5] </ref> notes that every strongly deep sequence is weakly deep, but that there exist weakly deep sequences that are not strongly deep. <p> This framework is used to prove our main result (Theorem 5.11), that every weakly useful sequence is strongly deep. In the course of our development, we prove several results, some of which were already proven by Bennett <ref> [5] </ref>, giving precise, quantitative relationships among depth, randomness, and recursiveness. We also prove (Theorem 5.16) that strongly deep sequences are extremely rare, in that they form a meager, measure 0 subset of f0; 1g 1 . <p> Lemma 4.4 (Bennett <ref> [5] </ref>). <p> It is easily verified that the sequences k 0 ; k 1 ; : : : and 0 ; 1 ; : : : satisfy conditions (1), (2), and (3). 2 Bennett <ref> [5] </ref> has noted that no algorithmically random sequence is strongly deep. We now prove this fact. Moreover, we show that it holds in a very strong way. Intuitively, we show that every algorithmically random sequence lies "very near the top" of the diagram in Figure 2. Theorem 5.2 (Bennett [5]). <p> Bennett <ref> [5] </ref> has noted that no algorithmically random sequence is strongly deep. We now prove this fact. Moreover, we show that it holds in a very strong way. Intuitively, we show that every algorithmically random sequence lies "very near the top" of the diagram in Figure 2. Theorem 5.2 (Bennett [5]). RAND " strDEEP = ;. In fact, there exist a recursive function t (n) = O (n log n) and a constant c 2 N such that RAND " D t c = ;. Proof. Let M be a Turing machine that, with program y, does the following. <p> For this, we need some notation and a lemma. We first recall Bennett's definition of the computational depth of finite strings. Definition <ref> [5] </ref>. Let w 2 f0; 1g fl and c 2 N. <p> This result was proven in a slightly different form in <ref> [5] </ref>. Lemma 5.3 (Bennett [5]). <p> This result was proven in a slightly different form in <ref> [5] </ref>. Lemma 5.3 (Bennett [5]). <p> Theorem 5.4 (Bennett <ref> [5] </ref>). <p> The equivalence of (1) and (2) follows immediately from the defini-tions. The equivalence of (1), (3), and (4) follows immediately from Lemma 5.3. 2 In <ref> [5] </ref>, Bennett uses condition (2) of Theorem 5.4 above as the definition of strong computational depth. As noted above, this is trivially equivalent to condition (1), i.e., to our definition in terms of the classes D t c . Bennett [5] also considers definitions in terms similar to those used in <p> (3), and (4) follows immediately from Lemma 5.3. 2 In <ref> [5] </ref>, Bennett uses condition (2) of Theorem 5.4 above as the definition of strong computational depth. As noted above, this is trivially equivalent to condition (1), i.e., to our definition in terms of the classes D t c . Bennett [5] also considers definitions in terms similar to those used in defining the classes b D t c and e D t c and implicitly proves the equivalence of conditions (1), (3), and (4). <p> We next prove a technical lemma on the quantitative relationship between computational depth and time-bounded Turing reducibility. This can be regarded as a quantitative, infinitary version of Bennett's deterministic slow-growth law <ref> [5] </ref>. We need two special notations for this lemma. <p> This implies the fact, noted by Bennett <ref> [5] </ref>, that strong depth is invariant under truth-table equivalence. Theorem 5.6. Let x; y 2 f0; 1g 1 . If y tt x and y is strongly deep, then x is strongly deep. Proof. Assume the hypothesis. <p> Since y is strongly deep, it follows that y 2 D b t bg . It follows by Lemma 5.5 that x 2 D t c . 2 We now note that no recursive sequence is strongly deep. Corollary 5.7 (Bennett <ref> [5] </ref>). REC " strDEEP = ;. 31 Proof. Let x 2 REC; it suffices to show that x 62 strDEEP. Fix z 2 RAND. Then, trivially, x tt z. <p> , M i (n) halts; K [n] = 1 , M n (n) halts; where M 0 ; M 1 ; : : : is a standard enumeration of all deterministic Turing ma chines and h; i is a standard pairing function, e.g., hi; ni = i+n+1 Corollary 5.12 (Bennett <ref> [5] </ref>). The sequences H and K are strongly deep. Proof. It is well-known that H and K are polynomial-time complete for the set of all recursively enumerable subsets of N, so H and K are weakly useful. <p> In this brief section, we show that the situation is different for weakly deep sequences. We first recall the definition. Definition (Bennett <ref> [5] </ref>). A sequence x 2 f0; 1g 1 is weakly deep, and we write x 2 wkDEEP, if there is no sequence z 2 RAND such that x tt z. <p> As the terminology suggests, every strongly deep sequence is weakly deep. Theorem 6.1 (Bennett <ref> [5] </ref>). strDEEP wkDEEP. Proof. Assume that x 2 strDEEP and x tt y. To see that x 2 wkDEEP, it suffices to show that y 62 RAND. But this follows immediately from Theorems 5.2 and 5.6. 2 In particular, Theorems 5.11 and 6.1 imply that weakly deep sequences exist. <p> It is clear that REC tt (RAND) is closed under finite variations. Also, by Corollary 5.12 and Theorem 6.1, REC tt (RAND) 6= f0; 1g 1 . Thus, by Fact 3.3, REC tt (RAND) is meager, whence wkDEEP = REC tt (RAND) c is comeager. 2 Bennett <ref> [5] </ref> noted that there exist sequences that are weakly deep, but not strongly deep. The following corollary shows that such sequences are, in the sense of Baire category, commonplace. Corollary 6.3. The set wkDEEP strDEEP is comeager. Proof. <p> Corollary 6.3. The set wkDEEP strDEEP is comeager. Proof. This follows immediately from Theorems 5.16 and 6.2. 2 Thus, in the sense of Baire category, almost every sequence x 2 f0; 1g 1 is weakly deep, but not strongly deep. Corollary 6.4 (Bennett <ref> [5] </ref>). strDEEP 6= wkDEEP. Proof. This follows immediately from Theorem 6.1 and Corollary 6.2. 2 and strDEEP. In the sense of Lebesgue measure, almost every binary sequence is in RAND.
Reference: [6] <author> P. Billingsley. </author> <title> Probability and Measure, second edition. </title> <publisher> John Wiley and Sons, </publisher> <year> 1986. </year>
Reference-contexts: be meager (also known as first category), in which case it is small "in the sense of Baire category." This section reviews the basic ideas from Lebesgue measure, resource-bounded measure, and Baire category that are involved in our use of these three notions of "smallness." The interested reader may consult <ref> [6, 18, 36, 37, 43, 45] </ref> for further discussion of these notions, but the material in the present section is sufficient for following the arguments of this paper. Resource-bounded measure [36, 37] is a generalization of classical Lebesgue measure. <p> In this latter case, we say that X contains almost every sequence x 2 f0; 1g 1 . It is a routine exercise to check that this definition is equivalent to "standard textbook" definitions <ref> [6, 18, 43, 45] </ref> of measure 0 and measure 1 sets. The main advantage of the above definition is that it naturally yields analogous notions of measure in REC and various complexity classes. To specify the analogous measure in REC, we need to define the computability of density systems.
Reference: [7] <author> R. V. </author> <title> Book. On languages reducible to algorithmically random languages. </title> <journal> SIAM Journal on Computing, </journal> <note> 1993. to appear. </note>
Reference-contexts: z is not weakly useful, by the following two facts. (i) For every recursive time bound s : N ! N there exists a recursive time bound b s : N ! N such that, for all algorithmically random sequences z, DTIME z (s) " REC DTIME ( b s) <ref> [5, 8, 7] </ref>. (ii) For every recursive time bound b s : N ! N, DTIME ( b s) has measure 0 in REC [37]. Our main result, Theorem 5.11 below, establishes that every weakly useful sequence is strongly deep.
Reference: [8] <author> R. V. Book, J. H. Lutz, and K. W. Wagner. </author> <title> An observation on probability versus randomness with applications to complexity classes. </title> <journal> Mathematical Systems Theory. </journal> <note> to appear. </note>
Reference-contexts: While every recursive sequence is Turing reducible to K in polynomial time, a recursive sequence y 2 f0; 1g 1 is Turing reducible to z in polynomial time if and only if y is in the complexity class BPP <ref> [5, 8] </ref>. (The class BPP, defined by Gill [17], consists of those sequences y 2 f0; 1g 1 such that there is a randomized algorithm that decides y [n], the n th bit of y, with error probability less than 1 n , using time that is at most polynomial in <p> z is not weakly useful, by the following two facts. (i) For every recursive time bound s : N ! N there exists a recursive time bound b s : N ! N such that, for all algorithmically random sequences z, DTIME z (s) " REC DTIME ( b s) <ref> [5, 8, 7] </ref>. (ii) For every recursive time bound b s : N ! N, DTIME ( b s) has measure 0 in REC [37]. Our main result, Theorem 5.11 below, establishes that every weakly useful sequence is strongly deep.
Reference: [9] <author> G. J. Chaitin. </author> <title> On the length of programs for computing finite binary sequences. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 13 </volume> <pages> 547-569, </pages> <year> 1966. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin <ref> [9, 10, 11, 12] </ref>, Martin-Lof [39, 40], Levin [26, 27, 28, 29, 30, 31, 55], Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite). <p> We are especially concerned with self-delimiting Kolmogorov complexity and algorithmic randomness. The interested reader is referred to [33, 35] for more details, discussion, and proofs. Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff [51], Kolmogorov [21], and Chaitin <ref> [9] </ref>. Self-delimiting Kolmogorov complexity is a technical improvement of the original formulation that was developed independently, in slightly different forms, by Levin [26, 27], Schnorr [47], and Chaitin [11]. The advantage of the self-delimiting version is that it gives precise characterizations of algorithmic probability and randomness.
Reference: [10] <author> G. J. Chaitin. </author> <title> On the length of programs for computing finite binary sequences: statistical considerations. </title> <journal> Journal of the ACM, </journal> <volume> 16 </volume> <pages> 145-159, </pages> <year> 1969. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin <ref> [9, 10, 11, 12] </ref>, Martin-Lof [39, 40], Levin [26, 27, 28, 29, 30, 31, 55], Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite).
Reference: [11] <author> G. J. Chaitin. </author> <title> A theory of program size formally identical to information theory. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 22 </volume> <pages> 329-340, </pages> <year> 1975. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin <ref> [9, 10, 11, 12] </ref>, Martin-Lof [39, 40], Levin [26, 27, 28, 29, 30, 31, 55], Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite). <p> In contrast, consider a sequence z 2 f0; 1g 1 that is algorithmically random in the equivalent senses of Martin-Lof [39], Levin [26], Schnorr [47], Chaitin <ref> [11] </ref>, Solovay [52], and Shen 0 [48, 49]. (See section 4 below for a precise definition and basic properties of algorithmic randomness.) An n-bit prefix z [0::n 1] of an algorithmically random sequence z contains approximately n bits of algorithmic information [39], so the information content of z is exponentially greater <p> Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff [51], Kolmogorov [21], and Chaitin [9]. Self-delimiting Kolmogorov complexity is a technical improvement of the original formulation that was developed independently, in slightly different forms, by Levin [26, 27], Schnorr [47], and Chaitin <ref> [11] </ref>. The advantage of the self-delimiting version is that it gives precise characterizations of algorithmic probability and randomness. Self-delimiting Kolmogorov complexity employs a slightly restricted model of (deterministic) Turing machine computation. <p> Theorem 4.2 (Levin [26, 27], Chaitin <ref> [11] </ref>). <p> Algorithmic randomness was originally defined by Martin-Lof [39], using constructive versions of ideas from measure theory. Subsequently, Levin [26, 27], Schnorr [47], and Chaitin <ref> [11] </ref> showed that algorithmic randomness could be characterized in terms of self-delimiting Kolmogorov complexity. (Indeed, this was an important motivation for developing the self-delimiting formulation.) For the purposes of the present paper, it is convenient to use this characterization as the definition. Definition.
Reference: [12] <author> G. J. Chaitin. </author> <title> Incompleteness theorems for random reals. </title> <booktitle> Advances in Applied Mathematics, </booktitle> <volume> 8 </volume> <pages> 119-146, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin <ref> [9, 10, 11, 12] </ref>, Martin-Lof [39, 40], Levin [26, 27, 28, 29, 30, 31, 55], Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite).
Reference: [13] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1991. </year>
Reference-contexts: M = f 2 f0; 1g fl j M ()#g must be an instantaneous code, i.e., must be a set of nonempty strings, no one of which is a prefix of another. (It is this feature of the model that the adjective "self-delimiting" describes.) It follows by Kraft's inequality (see <ref> [13] </ref>, for example) that, for all Turing machines M , X 2PROG M 2 jj 1: It is well-known that there are Turing machines U that are universal, in the sense that, for every Turing machine M , there exists a program prefix M 2 f0; 1g fl such that, for
Reference: [14] <author> R. I. Freidzon. </author> <title> Families of recursive predicates of measure zero. </title> <journal> translated in Journal of Soviet Mathematics, </journal> <volume> 6(1976) </volume> <pages> 449-455, </pages> <year> 1972. </year>
Reference-contexts: Taken together, the above facts justify the intuition that, if X has measure 0 in REC, then X " REC is a negligibly small subset of REC. Further discussion of this intuition may be found in [37, 43]. Other formulations of measure in REC have been investigated by Freid-zon <ref> [14] </ref>, Mehlhorn [41], and others. The advantage of the formulation here is that it uniformly yields Lebesgue measure, measure in REC, and measure in various complexity classes [37]. It is easy to show that, if X has "measure 0 in REC" in the sense of [14], then X has measure 0 <p> have been investigated by Freid-zon <ref> [14] </ref>, Mehlhorn [41], and others. The advantage of the formulation here is that it uniformly yields Lebesgue measure, measure in REC, and measure in various complexity classes [37]. It is easy to show that, if X has "measure 0 in REC" in the sense of [14], then X has measure 0 in REC in our sense. We now turn to the fundamentals of Baire category. Baire category gives a topological notion of smallness, usually defined in terms of "countable unions of nowhere dense sets" [42, 43, 45].
Reference: [15] <author> P. Gacs. </author> <title> On the symmetry of algorithmic information. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 15:1477, </volume> <year> 1974. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin [26, 27, 28, 29, 30, 31, 55], Schnorr [47], Gacs <ref> [15] </ref>, Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite). However, a given quantity of information may be organized in various ways, rendering it more or less useful for various computational purposes.
Reference: [16] <author> P. Gacs. </author> <title> Every sequence is reducible to a random one. </title> <journal> Information and Control, </journal> <volume> 70 </volume> <pages> 186-192, </pages> <year> 1986. </year>
Reference-contexts: To see that x 2 wkDEEP, it suffices to show that y 62 RAND. But this follows immediately from Theorems 5.2 and 5.6. 2 In particular, Theorems 5.11 and 6.1 imply that weakly deep sequences exist. It should be noted that Gacs <ref> [16] </ref> has proven that, for every sequence x 2 f0; 1g 1 , there exists a sequence z 2 RAND such that x T z. Thus T -reducibility cannot be used in place of tt -reducibility in the definition of wkDEEP.
Reference: [17] <author> J. Gill. </author> <title> Computational complexity of probabilistic Turing machines. </title> <journal> SIAM Journal on Computing, </journal> <volume> 6 </volume> <pages> 675-695, </pages> <year> 1977. </year>
Reference-contexts: While every recursive sequence is Turing reducible to K in polynomial time, a recursive sequence y 2 f0; 1g 1 is Turing reducible to z in polynomial time if and only if y is in the complexity class BPP [5, 8]. (The class BPP, defined by Gill <ref> [17] </ref>, consists of those sequences y 2 f0; 1g 1 such that there is a randomized algorithm that decides y [n], the n th bit of y, with error probability less than 1 n , using time that is at most polynomial in the number of bits in the binary representation
Reference: [18] <author> P. R. Halmos. </author> <title> Measure Theory. </title> <publisher> Springer-Verlag, </publisher> <year> 1950. </year>
Reference-contexts: be meager (also known as first category), in which case it is small "in the sense of Baire category." This section reviews the basic ideas from Lebesgue measure, resource-bounded measure, and Baire category that are involved in our use of these three notions of "smallness." The interested reader may consult <ref> [6, 18, 36, 37, 43, 45] </ref> for further discussion of these notions, but the material in the present section is sufficient for following the arguments of this paper. Resource-bounded measure [36, 37] is a generalization of classical Lebesgue measure. <p> In this latter case, we say that X contains almost every sequence x 2 f0; 1g 1 . It is a routine exercise to check that this definition is equivalent to "standard textbook" definitions <ref> [6, 18, 43, 45] </ref> of measure 0 and measure 1 sets. The main advantage of the above definition is that it naturally yields analogous notions of measure in REC and various complexity classes. To specify the analogous measure in REC, we need to define the computability of density systems.
Reference: [19] <author> J. E. Hopcroft and J. D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference-contexts: Turing machine model to define algorithmic information, algorithmic probability, and algorithmic depth in sections 4 and 5, we assume that the reader is already familiar with the general ideas of Turing machine computation, including computation by oracle Turing machines. (Discussion of such machines may be found in many texts, e.g., <ref> [2, 19, 44, 50] </ref>.) Given a recursive time bound s : N ! N, we say that an oracle Turing machine M is s-time-bounded if, given any input n 2 N and oracle y 2 f0; 1g 1 , M outputs a bit M y (n) 2 f0; 1g in at <p> Using standard techniques <ref> [2, 19] </ref>, it is easy to show that, for every recursive function r : N ! N, there is a strictly increasing, time-constructible function s : N ! N such that, for all n 2 N, r (n) s (n). Lemma 5.5.
Reference: [20] <author> J. L. Kelley. </author> <title> General Topology. </title> <publisher> Van Nostrand, </publisher> <year> 1955. </year>
Reference-contexts: The proofs of our Baire category results, Theorems 5.16 and 6.2 below, are easy, given some elementary properties of the Cantor topology on the set f0; 1g 1 . For completeness, we review these properties. Further details may be found in a number of texts, e.g., <ref> [20, 42] </ref>. A set X f0; 1g 1 is open, or 0 1 , if it can be expressed as a (countable) union of cylinders. A set X f0; 1g 1 is closed, or 0 1 , if X c is open. <p> We use the following two facts. For completeness, we sketch proofs. Further details may be found in standard texts, e.g., <ref> [20, 42] </ref>. 12 Fact 3.3. 1. Let X and Y be disjoint subsets of f0; 1g 1 . If X is 0 2 , Y 6= ;, and Y is closed under finite variations, then X is meager. 2.
Reference: [21] <author> A. N. </author> <title> Kolmogorov. Three approaches to the quantitative definition of `information'. </title> <journal> Problems of Information Transmission, </journal> <volume> 1 </volume> <pages> 1-7, </pages> <year> 1965. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov <ref> [21, 22, 23] </ref>, Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin [26, 27, 28, 29, 30, 31, 55], Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite). <p> We are especially concerned with self-delimiting Kolmogorov complexity and algorithmic randomness. The interested reader is referred to [33, 35] for more details, discussion, and proofs. Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff [51], Kolmogorov <ref> [21] </ref>, and Chaitin [9]. Self-delimiting Kolmogorov complexity is a technical improvement of the original formulation that was developed independently, in slightly different forms, by Levin [26, 27], Schnorr [47], and Chaitin [11]. The advantage of the self-delimiting version is that it gives precise characterizations of algorithmic probability and randomness.
Reference: [22] <author> A. N. </author> <title> Kolmogorov. Logical basis for information theory and probability theory. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:662-664, </volume> <year> 1968. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov <ref> [21, 22, 23] </ref>, Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin [26, 27, 28, 29, 30, 31, 55], Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite).
Reference: [23] <author> A. N. Kolmogorov and V. A. Uspenskii. </author> <title> Algorithms and randomness. </title> <journal> translated in Theory of Probability and its Applications, </journal> <volume> 32 </volume> <pages> 389-412, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov <ref> [21, 22, 23] </ref>, Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin [26, 27, 28, 29, 30, 31, 55], Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite).
Reference: [24] <author> M. Koppel. </author> <title> Complexity, depth, </title> <journal> and sophistication. Complex Systems, </journal> <volume> 1 </volume> <pages> 1087-1091, </pages> <year> 1987. </year>
Reference-contexts: Since this shortest description contains all the information in the object, the depth thus represents the amount of "computational work" that has been "added" to this information and "stored in the organization" of the object. (Depth is closely related to Adleman's notion of "potential" [1] and Koppel's notion of "sophistication" <ref> [24, 25] </ref>.) One way to investigate the computational usefulness of an object is to investigate the class of computational problems that can be solved efficiently, given access to the object.
Reference: [25] <author> M. Koppel. </author> <title> Structure. </title> <editor> In R. Herken, editor, </editor> <booktitle> The Universal Turing Machine: A Half-Century Survey, </booktitle> <pages> pages 435-452. </pages> <publisher> Oxford University Press, </publisher> <year> 1988. </year>
Reference-contexts: Since this shortest description contains all the information in the object, the depth thus represents the amount of "computational work" that has been "added" to this information and "stored in the organization" of the object. (Depth is closely related to Adleman's notion of "potential" [1] and Koppel's notion of "sophistication" <ref> [24, 25] </ref>.) One way to investigate the computational usefulness of an object is to investigate the class of computational problems that can be solved efficiently, given access to the object.
Reference: [26] <author> L. A. Levin. </author> <title> On the notion of a random sequence. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 14 </volume> <pages> 1413-1416, </pages> <year> 1973. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin <ref> [26, 27, 28, 29, 30, 31, 55] </ref>, Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite). <p> In contrast, consider a sequence z 2 f0; 1g 1 that is algorithmically random in the equivalent senses of Martin-Lof [39], Levin <ref> [26] </ref>, Schnorr [47], Chaitin [11], Solovay [52], and Shen 0 [48, 49]. (See section 4 below for a precise definition and basic properties of algorithmic randomness.) An n-bit prefix z [0::n 1] of an algorithmically random sequence z contains approximately n bits of algorithmic information [39], so the information content of <p> Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff [51], Kolmogorov [21], and Chaitin [9]. Self-delimiting Kolmogorov complexity is a technical improvement of the original formulation that was developed independently, in slightly different forms, by Levin <ref> [26, 27] </ref>, Schnorr [47], and Chaitin [11]. The advantage of the self-delimiting version is that it gives precise characterizations of algorithmic probability and randomness. Self-delimiting Kolmogorov complexity employs a slightly restricted model of (deterministic) Turing machine computation. <p> There is a constant c 0 2 N such that, for all x 2 f0; 1g fl and all 2 PROG (x), K (x) K () + c 0 : The next two important theorems express the fundamental relationship between Kolmogorov complexity and algorithmic probability. Theorem 4.2 (Levin <ref> [26, 27] </ref>, Chaitin [11]). <p> Algorithmic randomness was originally defined by Martin-Lof [39], using constructive versions of ideas from measure theory. Subsequently, Levin <ref> [26, 27] </ref>, Schnorr [47], and Chaitin [11] showed that algorithmic randomness could be characterized in terms of self-delimiting Kolmogorov complexity. (Indeed, this was an important motivation for developing the self-delimiting formulation.) For the purposes of the present paper, it is convenient to use this characterization as the definition. Definition.
Reference: [27] <author> L. A. Levin. </author> <title> Laws of information conservation (nongrowth) and aspects of the foundation of probability theory. </title> <journal> Problems of Information Transmission, </journal> <volume> 10 </volume> <pages> 206-210, </pages> <year> 1974. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin <ref> [26, 27, 28, 29, 30, 31, 55] </ref>, Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite). <p> Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff [51], Kolmogorov [21], and Chaitin [9]. Self-delimiting Kolmogorov complexity is a technical improvement of the original formulation that was developed independently, in slightly different forms, by Levin <ref> [26, 27] </ref>, Schnorr [47], and Chaitin [11]. The advantage of the self-delimiting version is that it gives precise characterizations of algorithmic probability and randomness. Self-delimiting Kolmogorov complexity employs a slightly restricted model of (deterministic) Turing machine computation. <p> There is a constant c 0 2 N such that, for all x 2 f0; 1g fl and all 2 PROG (x), K (x) K () + c 0 : The next two important theorems express the fundamental relationship between Kolmogorov complexity and algorithmic probability. Theorem 4.2 (Levin <ref> [26, 27] </ref>, Chaitin [11]). <p> Algorithmic randomness was originally defined by Martin-Lof [39], using constructive versions of ideas from measure theory. Subsequently, Levin <ref> [26, 27] </ref>, Schnorr [47], and Chaitin [11] showed that algorithmic randomness could be characterized in terms of self-delimiting Kolmogorov complexity. (Indeed, this was an important motivation for developing the self-delimiting formulation.) For the purposes of the present paper, it is convenient to use this characterization as the definition. Definition.
Reference: [28] <author> L. A. Levin. </author> <title> On the principle of conservation of information in intu-itionistic mathematics. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 17 </volume> <pages> 601-605, </pages> <year> 1976. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin <ref> [26, 27, 28, 29, 30, 31, 55] </ref>, Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite). <p> Every sequence that is even weakly useful must be strongly deep. Bennett [5] also defines the class of weakly deep binary sequences. (As noted by Bennett, this class has been investigated in other guises by Levin and V'jugin <ref> [28, 31, 32, 53, 54, 55] </ref>.) A sequence x 2 f0; 1g 1 is weakly deep if there do not exist a recursive time bound s : N ! N and an algorithmi cally random sequence z such that x DTIME (s) T z.
Reference: [29] <author> L. A. Levin. </author> <title> Uniform tests of randomness. </title> <journal> Soviet Mathematics Doklady, </journal> <pages> pages 337-340, </pages> <year> 1976. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin <ref> [26, 27, 28, 29, 30, 31, 55] </ref>, Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite).
Reference: [30] <author> L. A. Levin. </author> <title> Various measures of complexity for finite objects (axiomatic description). </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 17 </volume> <pages> 522-526, </pages> <year> 1976. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin <ref> [26, 27, 28, 29, 30, 31, 55] </ref>, Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite).
Reference: [31] <author> L. A. Levin. </author> <title> Randomness conservation inequalities; information and independence in mathematical theories. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 15-37, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin <ref> [26, 27, 28, 29, 30, 31, 55] </ref>, Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite). <p> Every sequence that is even weakly useful must be strongly deep. Bennett [5] also defines the class of weakly deep binary sequences. (As noted by Bennett, this class has been investigated in other guises by Levin and V'jugin <ref> [28, 31, 32, 53, 54, 55] </ref>.) A sequence x 2 f0; 1g 1 is weakly deep if there do not exist a recursive time bound s : N ! N and an algorithmi cally random sequence z such that x DTIME (s) T z.
Reference: [32] <author> L. A. Levin and V. V. V'jugin. </author> <title> Invariant properties of informational bulks. </title> <booktitle> Proceedings of the Sixth Symposium on Mathematical Foundations of Computer Science, </booktitle> <pages> pages 359-364, </pages> <year> 1977. </year>
Reference-contexts: Every sequence that is even weakly useful must be strongly deep. Bennett [5] also defines the class of weakly deep binary sequences. (As noted by Bennett, this class has been investigated in other guises by Levin and V'jugin <ref> [28, 31, 32, 53, 54, 55] </ref>.) A sequence x 2 f0; 1g 1 is weakly deep if there do not exist a recursive time bound s : N ! N and an algorithmi cally random sequence z such that x DTIME (s) T z.
Reference: [33] <author> M. Li and P. M. B. Vitanyi. </author> <title> Kolmogorov complexity and its applications. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A, </booktitle> <pages> pages 187-254. </pages> <publisher> Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: We are especially concerned with self-delimiting Kolmogorov complexity and algorithmic randomness. The interested reader is referred to <ref> [33, 35] </ref> for more details, discussion, and proofs. Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff [51], Kolmogorov [21], and Chaitin [9]. <p> We now come to the central ideas of algorithmic information theory. (See <ref> [33] </ref> for a history of the development of these definitions.) Definition. Let x 2 f0; 1g fl , let t : N ! N be a time bound, and let M be a Turing machine. 1. <p> The discussions of depth by Li and Vitanyi in the Handbook of Theoretical Computer Science <ref> [33] </ref> and their recent book [35] essentially use condition (4) as the definition.
Reference: [34] <author> M. Li and P. M. B. Vitanyi. </author> <title> Learning simple concepts under simple distributions. </title> <journal> SIAM Journal on Computing, </journal> <volume> 20 </volume> <pages> 911-935, </pages> <year> 1991. </year>
Reference-contexts: constant e c 2 N such that, for all x 2 f0; 1g fl , log m (x) K (x) &lt; log m (x) + e c: A straightforward modification of the proof of Theorem 4.2 yields the following time-bounded version. (This result also follows immediately from Lemma 3 of <ref> [34] </ref>.) Theorem 4.3. Let t : N ! N be recursive. 1. For all x 2 f0; 1g fl , 2.
Reference: [35] <author> M. Li and P. M. B. Vitanyi. </author> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer, </publisher> <year> 1993. </year>
Reference-contexts: We are especially concerned with self-delimiting Kolmogorov complexity and algorithmic randomness. The interested reader is referred to <ref> [33, 35] </ref> for more details, discussion, and proofs. Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff [51], Kolmogorov [21], and Chaitin [9]. <p> The discussions of depth by Li and Vitanyi in the Handbook of Theoretical Computer Science [33] and their recent book <ref> [35] </ref> essentially use condition (4) as the definition.
Reference: [36] <author> J. H. Lutz. </author> <title> Resource-bounded measure. </title> <note> in preparation. </note>
Reference-contexts: We are interested in the size of DTIME x (s) " REC as a subset of REC. To quantify 3 this, we use a special case of the resource-bounded measure theory of Lutz <ref> [37, 36] </ref>. (A detailed description of the relevant special case appears in section 3 below.) Intuitively, this theory, a generalization of classical Lebesgue measure theory, defines a set X of infinite binary sequences to have measure 0 in REC if X " REC is a negligibly small subset of REC. <p> be meager (also known as first category), in which case it is small "in the sense of Baire category." This section reviews the basic ideas from Lebesgue measure, resource-bounded measure, and Baire category that are involved in our use of these three notions of "smallness." The interested reader may consult <ref> [6, 18, 36, 37, 43, 45] </ref> for further discussion of these notions, but the material in the present section is sufficient for following the arguments of this paper. Resource-bounded measure [36, 37] is a generalization of classical Lebesgue measure. <p> Resource-bounded measure <ref> [36, 37] </ref> is a generalization of classical Lebesgue measure. As such it has classical Lebesgue measure and measure in REC as special cases. We use this fact to present the notions "measure 0" and "measure 0 in REC" more or less simultaneously.
Reference: [37] <author> J. H. Lutz. </author> <title> Almost everywhere high nonuniform complexity. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 44 </volume> <pages> 220-258, </pages> <year> 1992. </year> <month> 43 </month>
Reference-contexts: We are interested in the size of DTIME x (s) " REC as a subset of REC. To quantify 3 this, we use a special case of the resource-bounded measure theory of Lutz <ref> [37, 36] </ref>. (A detailed description of the relevant special case appears in section 3 below.) Intuitively, this theory, a generalization of classical Lebesgue measure theory, defines a set X of infinite binary sequences to have measure 0 in REC if X " REC is a negligibly small subset of REC. <p> recursive time bound b s : N ! N such that, for all algorithmically random sequences z, DTIME z (s) " REC DTIME ( b s) [5, 8, 7]. (ii) For every recursive time bound b s : N ! N, DTIME ( b s) has measure 0 in REC <ref> [37] </ref>. Our main result, Theorem 5.11 below, establishes that every weakly useful sequence is strongly deep. This implies that every high Turing degree contains strongly deep sequences (Corollary 5.15). <p> be meager (also known as first category), in which case it is small "in the sense of Baire category." This section reviews the basic ideas from Lebesgue measure, resource-bounded measure, and Baire category that are involved in our use of these three notions of "smallness." The interested reader may consult <ref> [6, 18, 36, 37, 43, 45] </ref> for further discussion of these notions, but the material in the present section is sufficient for following the arguments of this paper. Resource-bounded measure [36, 37] is a generalization of classical Lebesgue measure. <p> Resource-bounded measure <ref> [36, 37] </ref> is a generalization of classical Lebesgue measure. As such it has classical Lebesgue measure and measure in REC as special cases. We use this fact to present the notions "measure 0" and "measure 0 in REC" more or less simultaneously. <p> The cylinder generated by a string w 2 f0; 1g fl is C w = fx 2 f0; 1g 1 j w v xg; i.e., the set of all infinite binary sequences beginning with the string w. Definition <ref> [37] </ref>. A density function is a function d : f0; 1g fl ! [0; 1) satisfying d (w) = 2 for all w 2 f0; 1g fl . The global value of a density function d is d (). <p> We thus intuitively regard d as a "detailed verification" that Pr [x 2 X] d () for all X S [d]. With this intuition in mind, we present the central idea of resource-bounded measure 0 sets. Definition <ref> [37] </ref>. A null cover of a set X f0; 1g 1 is a 1-DS d that satisfies the following two conditions for all k 2 N. (i) X S [d k ]. (ii) d k () 2 k . Definition [37]. <p> Definition <ref> [37] </ref>. A null cover of a set X f0; 1g 1 is a 1-DS d that satisfies the following two conditions for all k 2 N. (i) X S [d k ]. (ii) d k () 2 k . Definition [37]. A set X f0; 1g 1 has (Lebesgue) measure 0, and we write (X) = 0, if it has a null cover. A set X f0; 1g 1 has (Lebesgue) measure 1, and we write (X) = 1, if (X c ) = 0. <p> Since density systems are real-valued, they must be computed via approximations. For this purpose, it is natural to use the set D = fm2 n j m 2 Z; n 2 Ng of dyadic rationals. These are real numbers whose standard binary representations are finite. Definition <ref> [37] </ref>. <p> Definition <ref> [37] </ref>. A recursive null cover of a set X f0; 1g 1 is a null cover of X that is computable. A set X f0; 1g 1 has recursive measure 0, and we write rec (X) = 0, if X has a recursive null cover. <p> In fact, the recursive measure 0 sets enjoy a stronger closure property, which we now define. Definition <ref> [37] </ref>. Let Z; Z 0 ; Z 1 ; : : : f0; 1g 1 . <p> Theorem 3.1 (Lutz <ref> [37] </ref>). If Z f0; 1g 1 is a recursive union of sets of measure 0 in REC, then Z has measure 0 in REC. On the other hand, the following result shows that not every set has measure 0 in REC. Theorem 3.2 (Lutz [37]). <p> Theorem 3.1 (Lutz <ref> [37] </ref>). If Z f0; 1g 1 is a recursive union of sets of measure 0 in REC, then Z has measure 0 in REC. On the other hand, the following result shows that not every set has measure 0 in REC. Theorem 3.2 (Lutz [37]). No cylinder C w has measure 0 in REC. In particular, REC does not have measure 0 in REC. Taken together, the above facts justify the intuition that, if X has measure 0 in REC, then X " REC is a negligibly small subset of REC. <p> In particular, REC does not have measure 0 in REC. Taken together, the above facts justify the intuition that, if X has measure 0 in REC, then X " REC is a negligibly small subset of REC. Further discussion of this intuition may be found in <ref> [37, 43] </ref>. Other formulations of measure in REC have been investigated by Freid-zon [14], Mehlhorn [41], and others. The advantage of the formulation here is that it uniformly yields Lebesgue measure, measure in REC, and measure in various complexity classes [37]. <p> Further discussion of this intuition may be found in [37, 43]. Other formulations of measure in REC have been investigated by Freid-zon [14], Mehlhorn [41], and others. The advantage of the formulation here is that it uniformly yields Lebesgue measure, measure in REC, and measure in various complexity classes <ref> [37] </ref>. It is easy to show that, if X has "measure 0 in REC" in the sense of [14], then X has measure 0 in REC in our sense. We now turn to the fundamentals of Baire category. <p> These classes contain those sequences for which this Kolmogorov complexity is below the threshold value for infinitely many prefixes. The following theorem, which is used in proving our main result, says that almost every recursive sequence has very high time-bounded Kolmogorov complexity almost everywhere. Theorem 4.6 (Lutz <ref> [37] </ref>). For every recursive bound t : N ! N and every real number 0 &lt; ff &lt; 1, (K t fi fi REC) = 0: (In fact, Corollary 4.9 of [37] is stronger than this in several respects.) 21 We conclude this section with a brief discussion of the algorithmic <p> Theorem 4.6 (Lutz <ref> [37] </ref>). For every recursive bound t : N ! N and every real number 0 &lt; ff &lt; 1, (K t fi fi REC) = 0: (In fact, Corollary 4.9 of [37] is stronger than this in several respects.) 21 We conclude this section with a brief discussion of the algorithmic ran-domness of infinite binary sequences. Algorithmic randomness was originally defined by Martin-Lof [39], using constructive versions of ideas from measure theory.
Reference: [38] <author> D. A. Martin. </author> <title> Classes of recursively enumerable sets and degrees of unsolvability. </title> <journal> Z. Math. Logik Grundlag. Math., </journal> <volume> 12 </volume> <pages> 295-310, </pages> <year> 1966. </year>
Reference-contexts: Theorem 5.13 (Sacks [46]). There exist r.e. sequences that are high and not Turing equivalent to K . Theorem 5.14 (Martin <ref> [38] </ref>). A sequence y 2 f0; 1g 1 satisfies jump ( K ) T jump (y) if and only if there exists x T y such that REC is uniformly re cursive in x. Corollary 5.15. Every high Turing degree contains a strongly deep se quence. Proof.
Reference: [39] <author> P. Martin-Lof. </author> <title> On the definition of random sequences. </title> <journal> Information and Control, </journal> <volume> 9 </volume> <pages> 602-619, </pages> <year> 1966. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin [9, 10, 11, 12], Martin-Lof <ref> [39, 40] </ref>, Levin [26, 27, 28, 29, 30, 31, 55], Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite). <p> In contrast, consider a sequence z 2 f0; 1g 1 that is algorithmically random in the equivalent senses of Martin-Lof <ref> [39] </ref>, Levin [26], Schnorr [47], Chaitin [11], Solovay [52], and Shen 0 [48, 49]. (See section 4 below for a precise definition and basic properties of algorithmic randomness.) An n-bit prefix z [0::n 1] of an algorithmically random sequence z contains approximately n bits of algorithmic information [39], so the information <p> senses of Martin-Lof <ref> [39] </ref>, Levin [26], Schnorr [47], Chaitin [11], Solovay [52], and Shen 0 [48, 49]. (See section 4 below for a precise definition and basic properties of algorithmic randomness.) An n-bit prefix z [0::n 1] of an algorithmically random sequence z contains approximately n bits of algorithmic information [39], so the information content of z is exponentially greater than that of K . On the other hand, z is much less useful than K , in the following sense. <p> that weakly deep sequences are "topolog-ically abundant." (They "cannot be avoided" by one player in a two-person game described in section 3.) In contrast, weakly deep sequences are "probabilistically scarce," in the sense that, with respect to Lebesgue measure, almost every sequence x 2 f0; 1g 1 is algorithmically random <ref> [39] </ref>, hence not weakly deep. In order to provide a basis for further investigation of Bennett's fundamental ideas, this paper also includes a self-contained mathematical treatment of the weak and strong computational depth of infinite sequences. In section 2 we introduce our basic terminology and notation. <p> For example, in section 4 below, we define the set RAND, consisting of all algorithmically 13 random sequences. Consider also the set REC of all recursive sequences. It is well-known <ref> [39] </ref> that REC " RAND = ;, that RAND is meager, and that RAND has measure 1. (See also Theorems 4.7 and 6.2 below.) Also, since REC is countable, REC is meager and has measure 0. <p> Algorithmic randomness was originally defined by Martin-Lof <ref> [39] </ref>, using constructive versions of ideas from measure theory. <p> That is, RAND = 1 [ K i:o: [&lt; n k] c : The following theorem summarizes some elementary properties of RAND that are used in this paper. Theorem 4.7 (Martin-Lof <ref> [39] </ref>).
Reference: [40] <author> P. Martin-Lof. </author> <title> Complexity oscillations in infinite binary sequences. </title> <journal> Zeitschrift fur Wahrscheinlichkeitstheory und Verwandte Gebiete, </journal> <volume> 19 </volume> <pages> 225-230, </pages> <year> 1971. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin [9, 10, 11, 12], Martin-Lof <ref> [39, 40] </ref>, Levin [26, 27, 28, 29, 30, 31, 55], Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite).
Reference: [41] <author> K. Mehlhorn. </author> <title> The "almost all" theory of subrecursive degrees is decidable. </title> <booktitle> In Proceedings of the Second Colloquium on Automata, Languages, and Programming, </booktitle> <pages> pages 317-325. </pages> <booktitle> Springer Lecture Notes in Computer Science, </booktitle> <volume> vol. 14, </volume> <year> 1974. </year>
Reference-contexts: Further discussion of this intuition may be found in [37, 43]. Other formulations of measure in REC have been investigated by Freid-zon [14], Mehlhorn <ref> [41] </ref>, and others. The advantage of the formulation here is that it uniformly yields Lebesgue measure, measure in REC, and measure in various complexity classes [37].
Reference: [42] <author> Y. N. Moschovakis. </author> <title> Descriptive Set Theory. </title> <publisher> North-Holland, </publisher> <year> 1980. </year>
Reference-contexts: We now turn to the fundamentals of Baire category. Baire category gives a topological notion of smallness, usually defined in terms of "countable unions of nowhere dense sets" <ref> [42, 43, 45] </ref>. <p> The proof that the above definition is equivalent to the "standard textbook" definition of the meager sets is due to Banach and may be found in <ref> [42] </ref> or [43]. It is clear that every subset of a meager set is meager and that every countable set X f0; 1g 1 is meager. In fact, it is well-known that every countable union of meager sets is meager [43]. <p> The proofs of our Baire category results, Theorems 5.16 and 6.2 below, are easy, given some elementary properties of the Cantor topology on the set f0; 1g 1 . For completeness, we review these properties. Further details may be found in a number of texts, e.g., <ref> [20, 42] </ref>. A set X f0; 1g 1 is open, or 0 1 , if it can be expressed as a (countable) union of cylinders. A set X f0; 1g 1 is closed, or 0 1 , if X c is open. <p> This hierarchy is closely analo gous to the "lightface" arithmetical hierarchy 0 1 ; 0 2 ; 0 2 ; : : : of recursion theory <ref> [42] </ref>.) A finite variation of a sequence x 2 f0; 1g 1 is a sequence y 2 f0; 1g 1 such that y [n] = x [n] for all but finitely many n 2 N. <p> We use the following two facts. For completeness, we sketch proofs. Further details may be found in standard texts, e.g., <ref> [20, 42] </ref>. 12 Fact 3.3. 1. Let X and Y be disjoint subsets of f0; 1g 1 . If X is 0 2 , Y 6= ;, and Y is closed under finite variations, then X is meager. 2.
Reference: [43] <author> J. C. Oxtoby. </author> <title> Measure and Category. </title> <publisher> Springer-Verlag, </publisher> <year> 1980. </year> <note> second edition. </note>
Reference-contexts: be meager (also known as first category), in which case it is small "in the sense of Baire category." This section reviews the basic ideas from Lebesgue measure, resource-bounded measure, and Baire category that are involved in our use of these three notions of "smallness." The interested reader may consult <ref> [6, 18, 36, 37, 43, 45] </ref> for further discussion of these notions, but the material in the present section is sufficient for following the arguments of this paper. Resource-bounded measure [36, 37] is a generalization of classical Lebesgue measure. <p> In this latter case, we say that X contains almost every sequence x 2 f0; 1g 1 . It is a routine exercise to check that this definition is equivalent to "standard textbook" definitions <ref> [6, 18, 43, 45] </ref> of measure 0 and measure 1 sets. The main advantage of the above definition is that it naturally yields analogous notions of measure in REC and various complexity classes. To specify the analogous measure in REC, we need to define the computability of density systems. <p> In particular, REC does not have measure 0 in REC. Taken together, the above facts justify the intuition that, if X has measure 0 in REC, then X " REC is a negligibly small subset of REC. Further discussion of this intuition may be found in <ref> [37, 43] </ref>. Other formulations of measure in REC have been investigated by Freid-zon [14], Mehlhorn [41], and others. The advantage of the formulation here is that it uniformly yields Lebesgue measure, measure in REC, and measure in various complexity classes [37]. <p> We now turn to the fundamentals of Baire category. Baire category gives a topological notion of smallness, usually defined in terms of "countable unions of nowhere dense sets" <ref> [42, 43, 45] </ref>. <p> The proof that the above definition is equivalent to the "standard textbook" definition of the meager sets is due to Banach and may be found in [42] or <ref> [43] </ref>. It is clear that every subset of a meager set is meager and that every countable set X f0; 1g 1 is meager. In fact, it is well-known that every countable union of meager sets is meager [43]. <p> sets is due to Banach and may be found in [42] or <ref> [43] </ref>. It is clear that every subset of a meager set is meager and that every countable set X f0; 1g 1 is meager. In fact, it is well-known that every countable union of meager sets is meager [43]. On the other hand, for every w 2 f0; 1g fl , the strategy m (u) = w if u &lt; u0 otherwise is a winning strategy for Player I in G [C w ], so no cylinder is meager. (This is the Baire Category Theorem [43].) These facts justify <p> sets is meager <ref> [43] </ref>. On the other hand, for every w 2 f0; 1g fl , the strategy m (u) = w if u &lt; u0 otherwise is a winning strategy for Player I in G [C w ], so no cylinder is meager. (This is the Baire Category Theorem [43].) These facts justify the intuition that meager sets are "topologically small," or (negligibly) small in the sense of Baire category. <p> As Oxtoby <ref> [43] </ref> has noted, "There is of course nothing paradoxical in the fact that a set that is small in one sense may be large in some other sense." 4 Algorithmic Information and Randomness In this section we review some fundamentals of algorithmic information theory that are used in this paper.
Reference: [44] <author> H. Rogers, Jr. </author> <title> Theory of Recursive Functions and Effective Computability. </title> <publisher> McGraw Hill, </publisher> <year> 1967. </year>
Reference-contexts: Turing machine model to define algorithmic information, algorithmic probability, and algorithmic depth in sections 4 and 5, we assume that the reader is already familiar with the general ideas of Turing machine computation, including computation by oracle Turing machines. (Discussion of such machines may be found in many texts, e.g., <ref> [2, 19, 44, 50] </ref>.) Given a recursive time bound s : N ! N, we say that an oracle Turing machine M is s-time-bounded if, given any input n 2 N and oracle y 2 f0; 1g 1 , M outputs a bit M y (n) 2 f0; 1g in at <p> to a sequence y 2 f0; 1g 1 , and we write x tt y, if there exists a recursive time bound s : N ! N such that x DTIME (s) T y. (This definition is easily seen to be equivalent to 6 standard textbook definitions of truth-table reducibility <ref> [44, 50] </ref>.) Given a set Y f0; 1g 1 , we write REC tt (Y ) = fx 2 f0; 1g 1 j (9y 2 Y ) x tt yg [ recursive s [ DTIME y (s): We write REC for the set of all recursive (i.e., decidable) sequences x 2
Reference: [45] <author> H. L. Royden. </author> <title> Real Analysis, third edition. </title> <publisher> Macmillan Publishing Company, </publisher> <year> 1988. </year>
Reference-contexts: be meager (also known as first category), in which case it is small "in the sense of Baire category." This section reviews the basic ideas from Lebesgue measure, resource-bounded measure, and Baire category that are involved in our use of these three notions of "smallness." The interested reader may consult <ref> [6, 18, 36, 37, 43, 45] </ref> for further discussion of these notions, but the material in the present section is sufficient for following the arguments of this paper. Resource-bounded measure [36, 37] is a generalization of classical Lebesgue measure. <p> In this latter case, we say that X contains almost every sequence x 2 f0; 1g 1 . It is a routine exercise to check that this definition is equivalent to "standard textbook" definitions <ref> [6, 18, 43, 45] </ref> of measure 0 and measure 1 sets. The main advantage of the above definition is that it naturally yields analogous notions of measure in REC and various complexity classes. To specify the analogous measure in REC, we need to define the computability of density systems. <p> We now turn to the fundamentals of Baire category. Baire category gives a topological notion of smallness, usually defined in terms of "countable unions of nowhere dense sets" <ref> [42, 43, 45] </ref>.
Reference: [46] <author> G. E. Sacks. </author> <title> Degrees of Unsolvability. </title> <publisher> Princeton University Press, </publisher> <year> 1966. </year>
Reference-contexts: Theorem 5.13 (Sacks <ref> [46] </ref>). There exist r.e. sequences that are high and not Turing equivalent to K . Theorem 5.14 (Martin [38]).
Reference: [47] <author> C. P. Schnorr. </author> <title> Process complexity and effective random tests. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 7 </volume> <pages> 376-388, </pages> <year> 1973. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin [26, 27, 28, 29, 30, 31, 55], Schnorr <ref> [47] </ref>, Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite). However, a given quantity of information may be organized in various ways, rendering it more or less useful for various computational purposes. <p> In contrast, consider a sequence z 2 f0; 1g 1 that is algorithmically random in the equivalent senses of Martin-Lof [39], Levin [26], Schnorr <ref> [47] </ref>, Chaitin [11], Solovay [52], and Shen 0 [48, 49]. (See section 4 below for a precise definition and basic properties of algorithmic randomness.) An n-bit prefix z [0::n 1] of an algorithmically random sequence z contains approximately n bits of algorithmic information [39], so the information content of z is <p> Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff [51], Kolmogorov [21], and Chaitin [9]. Self-delimiting Kolmogorov complexity is a technical improvement of the original formulation that was developed independently, in slightly different forms, by Levin [26, 27], Schnorr <ref> [47] </ref>, and Chaitin [11]. The advantage of the self-delimiting version is that it gives precise characterizations of algorithmic probability and randomness. Self-delimiting Kolmogorov complexity employs a slightly restricted model of (deterministic) Turing machine computation. <p> Algorithmic randomness was originally defined by Martin-Lof [39], using constructive versions of ideas from measure theory. Subsequently, Levin [26, 27], Schnorr <ref> [47] </ref>, and Chaitin [11] showed that algorithmic randomness could be characterized in terms of self-delimiting Kolmogorov complexity. (Indeed, this was an important motivation for developing the self-delimiting formulation.) For the purposes of the present paper, it is convenient to use this characterization as the definition. Definition.
Reference: [48] <author> A. Kh. </author> <title> Shen 0 . The frequency approach to defining a random sequence. </title> <journal> Semiotika i Informatika, </journal> <volume> 19 </volume> <pages> 14-42, </pages> <year> 1982. </year> <note> (In Russian.). </note>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin [26, 27, 28, 29, 30, 31, 55], Schnorr [47], Gacs [15], Shen 0 <ref> [48, 49] </ref>, and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite). However, a given quantity of information may be organized in various ways, rendering it more or less useful for various computational purposes. <p> In contrast, consider a sequence z 2 f0; 1g 1 that is algorithmically random in the equivalent senses of Martin-Lof [39], Levin [26], Schnorr [47], Chaitin [11], Solovay [52], and Shen 0 <ref> [48, 49] </ref>. (See section 4 below for a precise definition and basic properties of algorithmic randomness.) An n-bit prefix z [0::n 1] of an algorithmically random sequence z contains approximately n bits of algorithmic information [39], so the information content of z is exponentially greater than that of K .
Reference: [49] <author> A. Kh. </author> <title> Shen 0 . On relations between different algorithmic definitions of randomness. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 38 </volume> <pages> 316-319, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin [26, 27, 28, 29, 30, 31, 55], Schnorr [47], Gacs [15], Shen 0 <ref> [48, 49] </ref>, and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite). However, a given quantity of information may be organized in various ways, rendering it more or less useful for various computational purposes. <p> In contrast, consider a sequence z 2 f0; 1g 1 that is algorithmically random in the equivalent senses of Martin-Lof [39], Levin [26], Schnorr [47], Chaitin [11], Solovay [52], and Shen 0 <ref> [48, 49] </ref>. (See section 4 below for a precise definition and basic properties of algorithmic randomness.) An n-bit prefix z [0::n 1] of an algorithmically random sequence z contains approximately n bits of algorithmic information [39], so the information content of z is exponentially greater than that of K .
Reference: [50] <author> R. I. Soare. </author> <title> Recursively Enumerable Sets and Degrees. </title> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: Turing machine model to define algorithmic information, algorithmic probability, and algorithmic depth in sections 4 and 5, we assume that the reader is already familiar with the general ideas of Turing machine computation, including computation by oracle Turing machines. (Discussion of such machines may be found in many texts, e.g., <ref> [2, 19, 44, 50] </ref>.) Given a recursive time bound s : N ! N, we say that an oracle Turing machine M is s-time-bounded if, given any input n 2 N and oracle y 2 f0; 1g 1 , M outputs a bit M y (n) 2 f0; 1g in at <p> to a sequence y 2 f0; 1g 1 , and we write x tt y, if there exists a recursive time bound s : N ! N such that x DTIME (s) T y. (This definition is easily seen to be equivalent to 6 standard textbook definitions of truth-table reducibility <ref> [44, 50] </ref>.) Given a set Y f0; 1g 1 , we write REC tt (Y ) = fx 2 f0; 1g 1 j (9y 2 Y ) x tt yg [ recursive s [ DTIME y (s): We write REC for the set of all recursive (i.e., decidable) sequences x 2 <p> To see that Theorem 5.11 is actually stronger than Corollary 5.12, we use two known facts concerning high Turing degrees. We first review the relevant definitions. (More detailed discussion can be found in a standard recursion theory text, e.g. <ref> [50] </ref>.) Recall from section 2 that the characteristic sequence of a set A N is the sequence A 2 f0; 1g 1 such that A = fn 2 N j A [n] = 1g. <p> A Turing degree a is then high n if it contains a high n sequence. (See <ref> [50] </ref>, for example.) If a sequence or degree is high n , then it is clearly high n+1 . The Turing degree of K is clearly the only high 0 degree. It is also clear that a sequence or degree is high 1 if and only if it is high.
Reference: [51] <author> R. J. Solomonoff. </author> <title> A formal theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22, 224-254, </pages> <year> 1964. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff <ref> [51] </ref>, Kolmogorov [21, 22, 23], Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin [26, 27, 28, 29, 30, 31, 55], Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite). <p> We are especially concerned with self-delimiting Kolmogorov complexity and algorithmic randomness. The interested reader is referred to [33, 35] for more details, discussion, and proofs. Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff <ref> [51] </ref>, Kolmogorov [21], and Chaitin [9]. Self-delimiting Kolmogorov complexity is a technical improvement of the original formulation that was developed independently, in slightly different forms, by Levin [26, 27], Schnorr [47], and Chaitin [11].
Reference: [52] <author> R. M. Solovay, </author> <year> 1975. </year> <note> reported in [12]. 44 </note>
Reference-contexts: In contrast, consider a sequence z 2 f0; 1g 1 that is algorithmically random in the equivalent senses of Martin-Lof [39], Levin [26], Schnorr [47], Chaitin [11], Solovay <ref> [52] </ref>, and Shen 0 [48, 49]. (See section 4 below for a precise definition and basic properties of algorithmic randomness.) An n-bit prefix z [0::n 1] of an algorithmically random sequence z contains approximately n bits of algorithmic information [39], so the information content of z is exponentially greater than that
Reference: [53] <author> V. V. V'jugin. </author> <title> On Turing invariant sets. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 17 </volume> <pages> 1090-1094, </pages> <year> 1976. </year>
Reference-contexts: Every sequence that is even weakly useful must be strongly deep. Bennett [5] also defines the class of weakly deep binary sequences. (As noted by Bennett, this class has been investigated in other guises by Levin and V'jugin <ref> [28, 31, 32, 53, 54, 55] </ref>.) A sequence x 2 f0; 1g 1 is weakly deep if there do not exist a recursive time bound s : N ! N and an algorithmi cally random sequence z such that x DTIME (s) T z.
Reference: [54] <author> V. V. V'jugin. </author> <title> The algebra of invariant properties of finite sequences. </title> <journal> Problems of Information Transmission, </journal> <volume> 18 </volume> <pages> 147-161, </pages> <year> 1982. </year>
Reference-contexts: Every sequence that is even weakly useful must be strongly deep. Bennett [5] also defines the class of weakly deep binary sequences. (As noted by Bennett, this class has been investigated in other guises by Levin and V'jugin <ref> [28, 31, 32, 53, 54, 55] </ref>.) A sequence x 2 f0; 1g 1 is weakly deep if there do not exist a recursive time bound s : N ! N and an algorithmi cally random sequence z such that x DTIME (s) T z.
Reference: [55] <author> A. K. Zvonkin and L. A. Levin. </author> <title> The complexity of finite objects and the development of the concepts of information and randomness by means of the theory of algorithms. </title> <journal> Russian Mathematical Surveys, </journal> <volume> 25 </volume> <pages> 83-124, </pages> <year> 1970. </year>
Reference-contexts: 1 Introduction Algorithmic information theory, as developed by Solomonoff [51], Kolmogorov [21, 22, 23], Chaitin [9, 10, 11, 12], Martin-Lof [39, 40], Levin <ref> [26, 27, 28, 29, 30, 31, 55] </ref>, Schnorr [47], Gacs [15], Shen 0 [48, 49], and others, gives a satisfactory, quantitative account of the information content of individual binary strings (finite) and binary sequences (infinite). <p> Every sequence that is even weakly useful must be strongly deep. Bennett [5] also defines the class of weakly deep binary sequences. (As noted by Bennett, this class has been investigated in other guises by Levin and V'jugin <ref> [28, 31, 32, 53, 54, 55] </ref>.) A sequence x 2 f0; 1g 1 is weakly deep if there do not exist a recursive time bound s : N ! N and an algorithmi cally random sequence z such that x DTIME (s) T z.
References-found: 55

