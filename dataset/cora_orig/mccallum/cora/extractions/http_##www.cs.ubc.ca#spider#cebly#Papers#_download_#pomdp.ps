URL: http://www.cs.ubc.ca/spider/cebly/Papers/_download_/pomdp.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: cebly@cs.ubc.ca, poole@cs.ubc.ca  
Title: Computing Optimal Policies for Partially Observable Decision Processes using Compact Representations  
Author: Craig Boutilier and David Poole 
Address: Vancouver, BC V6T 1Z4, CANADA  
Affiliation: Department of Computer Science University of British Columbia  
Abstract: Partially-observable Markov decision processes provide a general model for decision theoretic planning problems, allowing trade-offs between various courses of actions to be determined under conditions of uncertainty, and incorporating partial observations made by an agent. Dynamic programming algorithms based on the belief state of an agent can be used to construct optimal policies without explicit consideration of past history, but at high computational cost. In this paper, we discuss how structured representations of system dynamics can be incorporated in classic POMDP solution algorithms. We use Bayesian networks with structured conditional probability matrices to represent POMDPs, and use this model to structure the belief space for POMDP algorithms, allowing irrelevant distinctions to be ignored. Apart from speeding up optimal policy construction, we suggest that such representations can be exploited in the development of useful approxi mation methods.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton, </publisher> <year> 1957. </year>
Reference-contexts: In particular, let VS k and Q k be the k-stage-to-go value and Q-functions. If the function VS k1 is known, then Bellman's <ref> [1] </ref> optimality equation ensures that Q k (s i ; a)=C (a; s i )+R (s i )+ s j 2S Pr (s j js i ; a)VS k1 (s j ) (1) Intuitively, once the agent has determined a course of action for the last k 1 stages of the
Reference: [2] <author> C. Boutilier, T. Dean, and S. Hanks. </author> <title> Planning under uncertainty: Structural assumptions and computational leverage. </title> <booktitle> 3rd Eur. Workshop on Planning, </booktitle> <address> Assisi, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction Recent interest in decision-theoretic planning (DTP) has been spurred by the need to extend planning algorithms to deal with quantified uncertainty regarding an agent's knowledge of the world and action effects, as well as competing objectives [9, 7, 4, 16] (see <ref> [2] </ref> for a brief survey). A useful underlying semantic model for such DTP problems is that of partially observable Markov decision processes (POMDPs) [6].
Reference: [3] <author> C. Boutilier and R. Dearden. </author> <title> Approximating value trees in structured dynamic programming. </title> <journal> ML-96, </journal> <note> to appear, </note> <year> 1996. </year>
Reference-contexts: Of great interest are extensions of this work to algorithms that enumerate vectors (in our case, trees) in a more direct fashion (rather than by exhaustive enumeration and elimination), as well as empirical evaluation of the overhead of tree 6 See <ref> [3] </ref> on this type of pruning. 7 In [13], a continuous approximation of the value function is adjusted via gradient descent on the Bellman error; but there is one adjustable parameter per state. A (dynamic) tree-based representation of the value function may be exploited here. construction.
Reference: [4] <author> C. Boutilier, R. Dearden, and M. Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> IJCAI-95, </booktitle> <address> pp.1104-1111, Montreal, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction Recent interest in decision-theoretic planning (DTP) has been spurred by the need to extend planning algorithms to deal with quantified uncertainty regarding an agent's knowledge of the world and action effects, as well as competing objectives <ref> [9, 7, 4, 16] </ref> (see [2] for a brief survey). A useful underlying semantic model for such DTP problems is that of partially observable Markov decision processes (POMDPs) [6]. <p> As such, problems can be specified much more compactly and naturally <ref> [8, 4, 16] </ref>. In addition, algorithms for solving IDs can exploit such regularities for computational gain in decision-making. Classic solution methods for POMDPs within the OR community, in contrast, have been developed primarily using explicit state-based representations which adds a sometimes unwanted computational burden. <p> In this paper we propose a method for optimal policy construction, based on standard POMDP algorithms, that exploits BN representations of actions and reward, as well as tree <ref> [4] </ref> or rule [16] representations within the BN itself. In this way, our technique exploits the advantages of classic POMDP and ID representations and provides leverage for approximation methods. <p> In Section 4, we describe how we can incorporate the structure captured by our representations to reduce the effective state space of the Monahan algorithm at any point in its computation. Our algorithm exploits ideas from the SPI algorithm of <ref> [4] </ref> for fully observable processes. In Section 5 we suggest that our method may enable good approximation schemes for POMDPs. 2 POMDPs and Structured Representations In this section we build upon the classic presentation of POMDPs adopted in much of the OR community. <p> We require that the induced graph be acyclic. For simplicity we assume also that arcs are directed only from pre-action to post-action nodes. 1 See <ref> [8, 4] </ref> for details. The post-action nodes have the usual conditional probability tables (CPTs) describing the probability of their values 1 We often denote post-action variables by P 0 instead of P to prevent confusion. <p> Causal influences between post-action variables should be viewed as ramifications and will complicate our algorithm slightly, but only in minor detail. given the values of their parents, under action a. We assume that these CPTs are represented using a decision tree, as in <ref> [4] </ref> (or if-then rules as in [15]). These are essentially compact function representations that exploit regularities in the CPTs. We will exploit the compactness and structure of such representations when producing optimal policies. <p> For clarity, we let ff denote the ff-tree for ff a;o and Q a denote the tree-structured function Q ff k a;o we wish to construct (i.e., a and o are fixed). Our method for generating the new Q-tree exploits the ideas described in <ref> [4] </ref>, and is closely related to [15] (we refer to [4] for further details). <p> Our method for generating the new Q-tree exploits the ideas described in <ref> [4] </ref>, and is closely related to [15] (we refer to [4] for further details). Roughly, given a structured value function ff, the conditions under which two states can have different expected future value given by ff (under action a) can be easily determined by appeal to the action network for a. <p> These propositions are related to each other via the state-transition trees for action a. Space precludes a full exposition of the methodwe refer to <ref> [4] </ref> for details of this method (applied to fully observable MDPs)so we present a simple example. Example To illustrate this process, consider the following example, illustrated in Figure 5. <p> We note, however, that we need not generate the trees for Q ff k a;o for each observation strategy individu ally. This tree depends only on a and ff k a;o , not on o. Thus, tion (for example, see <ref> [4] </ref> where similar tree merging is used for a different purpose).
Reference: [5] <author> A. R. Cassandra. </author> <title> Optimal policies for partially observable Markov decision processes. </title> <type> TR CS-94-14, </type> <institution> Brown Univ., Providence, </institution> <year> 1994. </year>
Reference-contexts: In Section 5 we suggest that our method may enable good approximation schemes for POMDPs. 2 POMDPs and Structured Representations In this section we build upon the classic presentation of POMDPs adopted in much of the OR community. We refer to [17, 11, 6] for further details and <ref> [12, 5] </ref> for a survey. We describe the main components of POMDPs and related concepts. <p> For instance, ff 3 in Figure 3 is dominated by one of ff 1 or ff 2 at all points in the belief space. Monahan [12] suggests that such dominated elements be detected by means of a simple linear program and eliminated from @ (see also <ref> [5] </ref>). Once again, the use of ff-trees can in many cases considerably reduce the size of these LPs, which normally involve variables for each state.
Reference: [6] <author> A. R. Cassandra, L. P. Kaelbling, and M. L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> AAAI-94, </booktitle> <address> pp.1023-1028, Seattle, </address> <year> 1994. </year>
Reference-contexts: A useful underlying semantic model for such DTP problems is that of partially observable Markov decision processes (POMDPs) <ref> [6] </ref>. This model, used in operations research [17, 12] and stochastic control, accounts for the tradeoffs between competing objectives, action costs, uncertainty of action effects and observations that provide incomplete information about the world. <p> In Section 5 we suggest that our method may enable good approximation schemes for POMDPs. 2 POMDPs and Structured Representations In this section we build upon the classic presentation of POMDPs adopted in much of the OR community. We refer to <ref> [17, 11, 6] </ref> for further details and [12, 5] for a survey. We describe the main components of POMDPs and related concepts. <p> This allows the direct use of T ree (Obsja) in assessing the influence of observations on the values of pre-action states. However, should observations depend instead on the post-action state as is usual in the POMDP literature <ref> [17, 6] </ref>, our algorithm is complicated only in slight detail. In this case, T ree (Obsja) refers to variables in the state following the action, (recall we are interested in the values of states prior to the action). <p> We are currently investigating how algorithms that use more direct vector generation can be adapted to our representation. The Witness algorithm <ref> [6] </ref> appears to be a promising candidate in this respect, for the LPs used to generate promising ff-vectors are amenable to the representations described here. 4.4 Executing Policies Given @ k and a belief state , we can determine the optimal action with k stages-to-go by choosing an ff 2 @
Reference: [7] <author> T. Dean, L. P. Kaelbling, J. Kirman, and A. Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> AAAI-93, </booktitle> <address> pp.574-579, Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Recent interest in decision-theoretic planning (DTP) has been spurred by the need to extend planning algorithms to deal with quantified uncertainty regarding an agent's knowledge of the world and action effects, as well as competing objectives <ref> [9, 7, 4, 16] </ref> (see [2] for a brief survey). A useful underlying semantic model for such DTP problems is that of partially observable Markov decision processes (POMDPs) [6].
Reference: [8] <author> T. Dean and K. </author> <title> Kanazawa. A model for reasoning about persistence and causation. </title> <journal> Comp. Intel., </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: As such, problems can be specified much more compactly and naturally <ref> [8, 4, 16] </ref>. In addition, algorithms for solving IDs can exploit such regularities for computational gain in decision-making. Classic solution methods for POMDPs within the OR community, in contrast, have been developed primarily using explicit state-based representations which adds a sometimes unwanted computational burden. <p> One can represent the transition probabilities associated with action a explicitly using a jSj fi jSj probability matrix. However, the fact that jSj increases exponentially with the number of problem characteristics jPj generally requires more compact representation; thus we represent an action's effects using a two-slice (temporal) Bayes net <ref> [8] </ref>: we have one set of nodes representing the state prior to the action (one node for each variable P ), another set representing the state after the action has been performed, and directed arcs representing causal influences between these sets (see Figure 1). <p> We require that the induced graph be acyclic. For simplicity we assume also that arcs are directed only from pre-action to post-action nodes. 1 See <ref> [8, 4] </ref> for details. The post-action nodes have the usual conditional probability tables (CPTs) describing the probability of their values 1 We often denote post-action variables by P 0 instead of P to prevent confusion.
Reference: [9] <author> T. Dean and M. Wellman. </author> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction Recent interest in decision-theoretic planning (DTP) has been spurred by the need to extend planning algorithms to deal with quantified uncertainty regarding an agent's knowledge of the world and action effects, as well as competing objectives <ref> [9, 7, 4, 16] </ref> (see [2] for a brief survey). A useful underlying semantic model for such DTP problems is that of partially observable Markov decision processes (POMDPs) [6].
Reference: [10] <author> R. A. Howard and J. E. Matheson. </author> <title> Influence diagrams. </title> <editor> R. A. Howard and J. Matheson, eds., </editor> <booktitle> The Principles and Applications of Decision Analysis, </booktitle> <address> pp.720-762, </address> <year> 1981. </year>
Reference-contexts: Influence diagrams (IDs) and Bayesian networks (BNs) <ref> [10, 14] </ref> provide a much more natural way of specifying the dynamics of a system, including the effects of actions and observation probabilities, by exploiting problem structure and independencies among random variables. As such, problems can be specified much more compactly and naturally [8, 4, 16]. <p> This assumption makes our algorithm somewhat simpler to describe; but it can generalized (see Section 4). 4 These are similar to observation variables in influence diagrams <ref> [10] </ref>; however, there are no emanating information arcs. Example The variable Obs in Figure 1 (a) takes on a single value (Null), obtained with certainty when GetC is executed (i.e., the action provides no feedback). More interesting is the action TestC shown in Figure 1 (b). <p> Different action-observation histories can lead an agent to choose different actions. Thus, a policy can be represented as a mapping from any initial state estimate, and k-stage history, to the action for stage k + 1. This is roughly the approach adopted by solution techniques for IDs <ref> [10] </ref>. However, an elegant way to treat this problem is to maintain a current belief state, and treat policies as mapping over from belief states to actions. 2.5 Belief States A belief state 2 D (S) is a probability distribution over states.
Reference: [11] <author> W. S. Lovejoy. </author> <title> Computationally feasible bounds for partially observed Markov processes. Op. </title> <journal> Res., </journal> <volume> 39 </volume> <pages> 162-175, </pages> <year> 1991. </year>
Reference-contexts: In Section 5 we suggest that our method may enable good approximation schemes for POMDPs. 2 POMDPs and Structured Representations In this section we build upon the classic presentation of POMDPs adopted in much of the OR community. We refer to <ref> [17, 11, 6] </ref> for further details and [12, 5] for a survey. We describe the main components of POMDPs and related concepts.
Reference: [12] <author> G. E. Monahan. </author> <title> A survey of partially observable Markov decision processes: Theory, models and algorithms. </title> <journal> Mgmt. Sci., </journal> <volume> 28 </volume> <pages> 1-16, </pages> <year> 1982. </year>
Reference-contexts: A useful underlying semantic model for such DTP problems is that of partially observable Markov decision processes (POMDPs) [6]. This model, used in operations research <ref> [17, 12] </ref> and stochastic control, accounts for the tradeoffs between competing objectives, action costs, uncertainty of action effects and observations that provide incomplete information about the world. <p> In Section 2, we define POMDPs and associated notions, at the same time showing how structured representations, based on BNs (augmented with tree-structured conditional probability tables), can be used to specify POMDPs. In Section 3, we describe a particular POMDP algorithm due to Monahan <ref> [12] </ref>, based on the work of Sondik [17]. In Section 4, we describe how we can incorporate the structure captured by our representations to reduce the effective state space of the Monahan algorithm at any point in its computation. <p> In Section 5 we suggest that our method may enable good approximation schemes for POMDPs. 2 POMDPs and Structured Representations In this section we build upon the classic presentation of POMDPs adopted in much of the OR community. We refer to [17, 11, 6] for further details and <ref> [12, 5] </ref> for a survey. We describe the main components of POMDPs and related concepts. <p> However, the assignment of value to states via value and Q-functions can also be viewed as an assignment of value to belief states. In particular, any state value function VS induces a value function over belief states: ff () = VS = s i 2S Following Monahan <ref> [12] </ref> we call these ff-functions. The value of a belief state is the weighted sum of the individual state values; thus, such ff-functions our linear functions over belief states. Q-functions can be applied similarly to belief states. <p> For instance, ff 3 in Figure 3 is dominated by one of ff 1 or ff 2 at all points in the belief space. Monahan <ref> [12] </ref> suggests that such dominated elements be detected by means of a simple linear program and eliminated from @ (see also [5]). Once again, the use of ff-trees can in many cases considerably reduce the size of these LPs, which normally involve variables for each state. <p> We begin by presenting the intuitions underlying Monahan's <ref> [12] </ref> variant of Sondik's [17] algorithm, and how the p.l.c. nature of value functions is exploited. We describe how our compact tree representations can be exploited in the next section.
Reference: [13] <author> R. Parr and S. Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> IJCAI-95, </booktitle> <address> pp.1088-1094, Montreal, </address> <year> 1995. </year>
Reference-contexts: Again, the compactness of the ff-trees can be exploited in such tests, as in Section 2.6. Indeed, this complements certain work that reduces the number of ff-functions, such as <ref> [13] </ref>. 7 These suggestions are, admittedly, not developed completely at this point. <p> Of great interest are extensions of this work to algorithms that enumerate vectors (in our case, trees) in a more direct fashion (rather than by exhaustive enumeration and elimination), as well as empirical evaluation of the overhead of tree 6 See [3] on this type of pruning. 7 In <ref> [13] </ref>, a continuous approximation of the value function is adjusted via gradient descent on the Bellman error; but there is one adjustable parameter per state. A (dynamic) tree-based representation of the value function may be exploited here. construction.
Reference: [14] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Influence diagrams (IDs) and Bayesian networks (BNs) <ref> [10, 14] </ref> provide a much more natural way of specifying the dynamics of a system, including the effects of actions and observation probabilities, by exploiting problem structure and independencies among random variables. As such, problems can be specified much more compactly and naturally [8, 4, 16].
Reference: [15] <author> D. Poole. </author> <title> Probabilistic Horn abduction and Bayesian networks. </title> <booktitle> Art. Intel., </booktitle> <volume> 64(1) </volume> <pages> 81-129, </pages> <year> 1993. </year>
Reference-contexts: Causal influences between post-action variables should be viewed as ramifications and will complicate our algorithm slightly, but only in minor detail. given the values of their parents, under action a. We assume that these CPTs are represented using a decision tree, as in [4] (or if-then rules as in <ref> [15] </ref>). These are essentially compact function representations that exploit regularities in the CPTs. We will exploit the compactness and structure of such representations when producing optimal policies. <p> propositions P and Q by performing one action, two distinct variables Obs 1 and Obs 2 might be used); we ignore this possibility here. 2 The network structure is not strictly necessary: the parent of a post-action node can be determined from its CPT or decision tree (see, e.g., Poole's <ref> [15] </ref> rule-based representation of Bayes nets). 3 This is a natural assumption for information-gathering actions, but others are possible; e.g., Sondik's [17] original presentation of POMDPs assumes the observation depends only on the resulting state. <p> Our method for generating the new Q-tree exploits the ideas described in [4], and is closely related to <ref> [15] </ref> (we refer to [4] for further details). Roughly, given a structured value function ff, the conditions under which two states can have different expected future value given by ff (under action a) can be easily determined by appeal to the action network for a.
Reference: [16] <author> D. Poole. </author> <title> Exploiting the rule structure for decision making within the independent choice logic. </title> <address> UAI-95, pp.454-463, Montreal, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction Recent interest in decision-theoretic planning (DTP) has been spurred by the need to extend planning algorithms to deal with quantified uncertainty regarding an agent's knowledge of the world and action effects, as well as competing objectives <ref> [9, 7, 4, 16] </ref> (see [2] for a brief survey). A useful underlying semantic model for such DTP problems is that of partially observable Markov decision processes (POMDPs) [6]. <p> As such, problems can be specified much more compactly and naturally <ref> [8, 4, 16] </ref>. In addition, algorithms for solving IDs can exploit such regularities for computational gain in decision-making. Classic solution methods for POMDPs within the OR community, in contrast, have been developed primarily using explicit state-based representations which adds a sometimes unwanted computational burden. <p> In this paper we propose a method for optimal policy construction, based on standard POMDP algorithms, that exploits BN representations of actions and reward, as well as tree [4] or rule <ref> [16] </ref> representations within the BN itself. In this way, our technique exploits the advantages of classic POMDP and ID representations and provides leverage for approximation methods. <p> Recall that the branches of this tree correspond to the conditions relevant to observation probability, and the leaves are labeled with the probability of making any observation o j . To the leaves of T ree (Obsja) we add the weighted sum of the explanation trees (see also <ref> [16] </ref>). More specifically, at each leaf of T ree (Obsja) we have a set of possible (nonzero probability) observations; for exposition, assume for some leaf these are o i and o j . <p> This tree depends only on a and ff k a;o , not on o. Thus, tion (for example, see [4] where similar tree merging is used for a different purpose). In terms of rules <ref> [16] </ref>, this effect is obtained by explaining the conjunction of the roots of the trees. we need only construct jAjj@ k j such trees; the jAjjOjj@ k j different trees in @ k+1 are simply different weighted combinations of these (corresponding to different observational strategies).
Reference: [17] <author> R. D. Smallwood and E. J. Sondik. </author> <title> The optimal control of partially observable Markov processes over a finite horizon. Op. </title> <journal> Res., </journal> <volume> 21 </volume> <pages> 1071-1088, </pages> <year> 1973. </year>
Reference-contexts: A useful underlying semantic model for such DTP problems is that of partially observable Markov decision processes (POMDPs) [6]. This model, used in operations research <ref> [17, 12] </ref> and stochastic control, accounts for the tradeoffs between competing objectives, action costs, uncertainty of action effects and observations that provide incomplete information about the world. <p> In Section 3, we describe a particular POMDP algorithm due to Monahan [12], based on the work of Sondik <ref> [17] </ref>. In Section 4, we describe how we can incorporate the structure captured by our representations to reduce the effective state space of the Monahan algorithm at any point in its computation. Our algorithm exploits ideas from the SPI algorithm of [4] for fully observable processes. <p> In Section 5 we suggest that our method may enable good approximation schemes for POMDPs. 2 POMDPs and Structured Representations In this section we build upon the classic presentation of POMDPs adopted in much of the OR community. We refer to <ref> [17, 11, 6] </ref> for further details and [12, 5] for a survey. We describe the main components of POMDPs and related concepts. <p> this possibility here. 2 The network structure is not strictly necessary: the parent of a post-action node can be determined from its CPT or decision tree (see, e.g., Poole's [15] rule-based representation of Bayes nets). 3 This is a natural assumption for information-gathering actions, but others are possible; e.g., Sondik's <ref> [17] </ref> original presentation of POMDPs assumes the observation depends only on the resulting state. This assumption makes our algorithm somewhat simpler to describe; but it can generalized (see Section 4). 4 These are similar to observation variables in influence diagrams [10]; however, there are no emanating information arcs. <p> This is the essential assumption behind classical POMDP techniques: at any stage of the decision process, assuming k accurately summarizes past actions and observations, the optimal decision can be based solely on k history (now summarized) can be ignored <ref> [17] </ref>. Intuitively, we can think of this as converting a partially observable MDP over the original state space S into a fully observable MDP over the belief space B (the set of belief states ). <p> In determining optimal policies for POMDPs, we need to represent the optimal (k-stage-to-go) value functions V : D (S) ! R for belief states. Clearly, ff-functions, being linear, are quite restrictive in expressiveness. However, a key observation of Sondik <ref> [17] </ref> is that optimal value functions are piecewise linear and convex (p.l.c.) over the belief space. <p> We begin by presenting the intuitions underlying Monahan's [12] variant of Sondik's <ref> [17] </ref> algorithm, and how the p.l.c. nature of value functions is exploited. We describe how our compact tree representations can be exploited in the next section. <p> This allows the direct use of T ree (Obsja) in assessing the influence of observations on the values of pre-action states. However, should observations depend instead on the post-action state as is usual in the POMDP literature <ref> [17, 6] </ref>, our algorithm is complicated only in slight detail. In this case, T ree (Obsja) refers to variables in the state following the action, (recall we are interested in the values of states prior to the action).
Reference: [18] <author> C. J. C. H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-Learning. Mach. Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: In addition, a state Q-function Q : S fi A ! R denotes the value Q (s; a) of performing an action a in state s, assuming future value is dictated by a fixed course of action <ref> [18] </ref>. In particular, let VS k and Q k be the k-stage-to-go value and Q-functions.
References-found: 18

