URL: http://www.cs.princeton.edu/~ristad/papers/pu-532-96.ps.gz
Refering-URL: http://www.cs.princeton.edu/~ristad/papers/pu-532-96.html
Root-URL: http://www.cs.princeton.edu
Title: Learning String Edit Distance  
Author: Eric Sven Ristad Peter N. Yianilos 
Keyword: string edit distance, Levenshtein distance, stochastic transduction, syntactic pattern recognition, prototype dictionary, spelling correction, string correction, string similarity, string classification, speech recognition, pronunciation modeling, Switchboard corpus.  
Abstract: Department of Computer Science Princeton University Research Report CS-TR-532-96 October 1996; Revised October 1997 Abstract In many applications, it is necessary to determine the similarity of two strings. A widely-used notion of string similarity is the edit distance: the minimum number of insertions, deletions, and substitutions required to transform one string into the other. In this report, we provide a stochastic model for string edit distance. Our stochastic model allows us to learn a string edit distance function from a corpus of examples. We illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in conversational speech. In this application, we learn a string edit distance with nearly one fifth the error rate of the untrained Levenshtein distance. Our approach is applicable to any string classification problem that may be solved using a similarity function against a database of labeled prototypes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> COMLEX pronouncing lexicon, version 0.2. </editor> <booktitle> Linguistic Data Consortium LDC95L3, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Each step of the transduction generates either a substitution pair ha; bi, a deletion pair ha; *i, an insertion pair h*; bi, or the distinguished termination symbol # according to a probability function ffi : E <ref> [ f#g ! [0; 1] </ref>. Being a probability function, ffi () satisfies the following constraints: a: 8z 2 E [ f#g [ 0 ffi (z) 1 ] P Note that the null operation h*; *i is not included in the alphabet E of edit operations. <p> Each step of the transduction generates either a substitution pair ha; bi, a deletion pair ha; *i, an insertion pair h*; bi, or the distinguished termination symbol # according to a probability function ffi : E [ f#g ! <ref> [0; 1] </ref>. Being a probability function, ffi () satisfies the following constraints: a: 8z 2 E [ f#g [ 0 ffi (z) 1 ] P Note that the null operation h*; *i is not included in the alphabet E of edit operations. <p> This decision rule correctly aggregates the similarity between an observed string and all prototypes for a given class. 3.3 Estimation Given a prototype lexicon L : W fi2 A fl ! <ref> [0; 1] </ref> and a corpus C = hw 1 ; y V 1 i; : : : ; hw n ; y V n i of labeled strings, we estimate the parameters of our model (5) using expectation maximization for finite mixture models [6]. <p> Over 200,000 words of Switchboard have been manually assigned phonetic transcripts at ICSI using a proprietary phonetic alphabet [9]. The Switchboard corpus also includes a pronouncing lexicon with 71,100 entries using a modified Pronlex phonetic alphabet (long form) <ref> [1] </ref>. In order to make the pronouncing lexicon compatible with the ICSI corpus of phonetic transcripts, we removed 148 entries from the lexicon and 73,068 samples from the ICSI corpus. 4 After filtering, our pronouncing lexicon had 70,952 entries for 66,284 syntactic words over an alphabet of 42 phonemes.
Reference: [2] <author> Bahl, L. R., and Jelinek, F. </author> <title> Decoding for channels with insertions, deletions, and substitutions with applications to speech recognition. </title> <journal> IEEE Trans. Inform. Theory IT-21, </journal> <volume> 4 (1975), </volume> <pages> 404-411. </pages>
Reference-contexts: Several variants of the edit distance have been proposed, including the constrained edit distance [19] and the normalized edit distance [17]. A stochastic interpretation of string edit distance was first provided by Bahl and Jelinek <ref> [2] </ref>, but without an algorithm for learning the edit costs. The need for such a learning algorithm is widely acknowledged [10, 20, 29]. The principal contribution of this report is an efficient algorithm for learning the primitive edit costs from a corpus of examples. <p> Each string pair is generated by exponentially many edit sequences, and so it would not be feasible to evaluate the probability of a string pair by actually summing over all its edit sequences. The following dynamic programming algorithm, due to Bahl and Jelinek <ref> [2] </ref>, calculates the probability p (x T ; y V j) in O (T V ) time and space. <p> An EM algorithm is an iterative algorithm that maximizes the probability of the training data according to the model. See [22] for a review. The applicability of EM to the problem of optimizing the parameters of a memoryless stochastic transducer was first noted by Bahl, Jelinek, and Mercer <ref> [2, 12] </ref>, although they did not publish an explicit algorithm for this purpose. As its name suggests, an EM algorithm consists of two steps. In the expectation step, we accumulate the expectation of each hidden event on the training corpus. <p> The details of this approach, which is strictly more powerful than the class of transducers considered by Bahl and Jelinek <ref> [2] </ref>, are presented in forthcoming work. 3 String Classification In the preceding section, we presented an algorithm to automatically learn a string edit distance from a corpus of similar string pairs. Unfortunately, this algorithm cannot be directly applied to solve string classification problems.
Reference: [3] <author> Baum, L., and Eagon, J. </author> <title> An inequality with applications to statistical estimation for probabilistic functions of a Markov process and to models for ecology. Bull. </title> <booktitle> AMS 73 (1967), </booktitle> <pages> 360-363. </pages>
Reference-contexts: For this task, we employ the powerful expectation maximization (EM) framework <ref> [3, 4, 6] </ref>. An EM algorithm is an iterative algorithm that maximizes the probability of the training data according to the model. See [22] for a review.
Reference: [4] <author> Baum, L., Petrie, T., Soules, G., and Weiss, N. </author> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> Ann. Math. Stat. </journal> <volume> 41 (1970), </volume> <pages> 164-171. </pages>
Reference-contexts: For this task, we employ the powerful expectation maximization (EM) framework <ref> [3, 4, 6] </ref>. An EM algorithm is an iterative algorithm that maximizes the probability of the training data according to the model. See [22] for a review.
Reference: [5] <author> Bunke, H., and Csirik, J. </author> <title> Parametric string edit distance and its application to pattern recognition. </title> <journal> IEEE Trans. Systems, Man and Cybernetics 25, </journal> <volume> 1 (1995), </volume> <pages> 202-206. </pages>
Reference-contexts: In this setting, we would train a different transducer on each section of the corpus, and then combine the resulting transducers into a mixture model. The mixing parameters could be set to the relative sizes of the corpus sections, 2 Bunke and Csirik <ref> [5] </ref> propose an even weaker "parametric edit distance" whose only free parameter is a single substitution cost r. The insertion and deletion costs are fixed to unity while the identity cost is zero. 9 or they could be optimized using withheld training data.
Reference: [6] <author> Dempster, A., Laird, N., and Rubin, D. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statist. Soc. Ser. B (methodological) 39 (1977), </journal> <pages> 1-38. </pages>
Reference-contexts: For this task, we employ the powerful expectation maximization (EM) framework <ref> [3, 4, 6] </ref>. An EM algorithm is an iterative algorithm that maximizes the probability of the training data according to the model. See [22] for a review. <p> L : W fi2 A fl ! [0; 1] and a corpus C = hw 1 ; y V 1 i; : : : ; hw n ; y V n i of labeled strings, we estimate the parameters of our model (5) using expectation maximization for finite mixture models <ref> [6] </ref>. If the prototype dictionary is not provided, one may be constructed from the training corpus.
Reference: [7] <author> Forney, G. D. </author> <title> The Viterbi algorithm. </title> <booktitle> Proc. IEEE 61, 3 (1973), </booktitle> <pages> 268-278. </pages>
Reference: [8] <author> Godfrey, J., Holliman, E., and McDaniel, J. </author> <title> Switchboard: telephone speech corpus for research and development. </title> <booktitle> In Proc. IEEE ICASSP (Detroit, </booktitle> <year> 1995), </year> <pages> pp. 517-520. </pages>
Reference-contexts: It also leads to a variant of string edit distance, that aggregates the many different ways to transform one string into another. We illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in the Switchboard corpus of conversational speech <ref> [8] </ref>. In this application, we learn a string edit distance that reduces the error rate of the untrained Levenshtein distance by a factor of 4.7, to within 4% of the minimum error rate achievable by any classifier. Let us first define our notation. <p> So let us now apply our stochastic solution to the Switchboard corpus of conversational speech. 4.1 Switchboard Corpus The Switchboard corpus contains over 3 million words of spontaneous telephone speech conversations <ref> [8] </ref>. It is considered one of the most difficult corpora for 14 speech recognition (and pronunciation recognition) because of the tremendous variability of spontaneous speech. As of Summer 1996, speech recognition technology has a word error rate above 45% on the Switchboard corpus.
Reference: [9] <author> Greenberg, S., Hollenbach, J., and Ellis, D. </author> <title> Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus. </title> <booktitle> In Proc. ICSLP (Philadelphia, </booktitle> <month> October </month> <year> 1996). </year>
Reference-contexts: The same speech recognition technology achieves a word error rate of less than 5% on read speech. Over 200,000 words of Switchboard have been manually assigned phonetic transcripts at ICSI using a proprietary phonetic alphabet <ref> [9] </ref>. The Switchboard corpus also includes a pronouncing lexicon with 71,100 entries using a modified Pronlex phonetic alphabet (long form) [1].
Reference: [10] <author> Hall, P., and Dowling, G. </author> <title> Approximate string matching. </title> <journal> Computing Surveys 12, </journal> <volume> 4 (1980), </volume> <pages> 381-402. </pages>
Reference-contexts: The edit distance may be computed in O (t v) time using dynamic programming [18, 31]. Many excellent reviews of the string edit distance literature are available <ref> [10, 14, 21, 29] </ref>. Several variants of the edit distance have been proposed, including the constrained edit distance [19] and the normalized edit distance [17]. A stochastic interpretation of string edit distance was first provided by Bahl and Jelinek [2], but without an algorithm for learning the edit costs. <p> A stochastic interpretation of string edit distance was first provided by Bahl and Jelinek [2], but without an algorithm for learning the edit costs. The need for such a learning algorithm is widely acknowledged <ref> [10, 20, 29] </ref>. The principal contribution of this report is an efficient algorithm for learning the primitive edit costs from a corpus of examples. To the best of our knowledge, this is the first published algorithm to automatically learn the primitive edit costs.
Reference: [11] <author> IEEE. </author> <title> IEEE standard for binary floating-point arithmetic. </title> <journal> SIGPLAN Notices 22, </journal> <volume> 2 (1985), </volume> <pages> 9-25. </pages>
Reference: [12] <author> Jelinek, F., Bahl, L. R., and Mercer, R. L. </author> <title> The design of a linguistic statistical decoder for the recognition of continuous speech. </title> <journal> IEEE Trans. Inform. Theory IT-21, </journal> <volume> 3 (1975), </volume> <pages> 250-256. </pages>
Reference-contexts: An EM algorithm is an iterative algorithm that maximizes the probability of the training data according to the model. See [22] for a review. The applicability of EM to the problem of optimizing the parameters of a memoryless stochastic transducer was first noted by Bahl, Jelinek, and Mercer <ref> [2, 12] </ref>, although they did not publish an explicit algorithm for this purpose. As its name suggests, an EM algorithm consists of two steps. In the expectation step, we accumulate the expectation of each hidden event on the training corpus.
Reference: [13] <author> Jelinek, F., and Mercer, R. L. </author> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Pattern Recognition in Practice (Amsterdam, </booktitle> <month> May 21-23 </month> <year> 1980), </year> <editor> E. S. Gelsema and L. N. Kanal, Eds., </editor> <publisher> North Holland, </publisher> <pages> pp. 381-397. 22 </pages>
Reference-contexts: Alternately, we might condition the probability of an edit operation z t on (a finite suffix of) the yield -(z t1 )) of the past edit sequence. These stochastic transducers can be further strengthened with state-conditional interpolation <ref> [13, 25] </ref> or by conditioning our edit probabilities ffi (z t jz t1 tn ; s) on a hidden state s drawn from a finite state space.
Reference: [14] <author> Kukich, K. </author> <title> Techniques for automatically correcting words in text. </title> <booktitle> ACM Compute. Surveys 24 (1992), </booktitle> <pages> 377-439. </pages>
Reference-contexts: The edit distance may be computed in O (t v) time using dynamic programming [18, 31]. Many excellent reviews of the string edit distance literature are available <ref> [10, 14, 21, 29] </ref>. Several variants of the edit distance have been proposed, including the constrained edit distance [19] and the normalized edit distance [17]. A stochastic interpretation of string edit distance was first provided by Bahl and Jelinek [2], but without an algorithm for learning the edit costs.
Reference: [15] <author> Levenshtein, V. </author> <title> Binary codes capable of correcting deletions, </title> <journal> insertions, and reversals. Soviet Physics - Doklady 10, </journal> <volume> 10 (1966), </volume> <pages> 707-710. </pages>
Reference-contexts: 1 Introduction In many applications, it is necessary to determine the similarity of two strings. A widely-used notion of string similarity is the edit distance: the minimum number of insertions, deletions, and substitutions required to transform one string into the other <ref> [15] </ref>. In this report, we provide a stochastic model for string edit distance. Our stochastic interpretation allows us to automatically learn a string edit distance from a corpus of examples. <p> In all cases, we partitioned our corpus of 214,310 samples 9:1 into 192,879 training samples and 21,431 test samples. In no experiment did we adapt our probability model (5) to the test data. Our seven models consist of Levenshtein distance <ref> [15] </ref> as well as six variants resulting from our two interpretations of three models. 5 Our two interpretations are the stochastic edit distance (4) and the classic edit distance (3), also called the Viterbi edit distance.
Reference: [16] <author> Llorens, D., and Vidal, E. </author> <title> Application of extended generalized linear discriminant functions (EGLDF) to planar shape recognition. </title> <booktitle> In Proc. IEE European Workshop on Handwriting Analysis (London, </booktitle> <month> May </month> <year> 1996). </year>
Reference: [17] <author> Marzal, A., and Vidal, E. </author> <title> Computation of normalized edit distance and applications. </title> <journal> IEEE Trans. </journal> <volume> PAMI 15, 9 (1993), </volume> <pages> 926-932. </pages>
Reference-contexts: Many excellent reviews of the string edit distance literature are available [10, 14, 21, 29]. Several variants of the edit distance have been proposed, including the constrained edit distance [19] and the normalized edit distance <ref> [17] </ref>. A stochastic interpretation of string edit distance was first provided by Bahl and Jelinek [2], but without an algorithm for learning the edit costs. The need for such a learning algorithm is widely acknowledged [10, 20, 29].
Reference: [18] <author> Masek, W., and Paterson, M. </author> <title> A faster algorithm computing string edit distances. </title> <journal> J. Comput. System Sci. </journal> <volume> 20 (1980), </volume> <pages> 18-31. </pages>
Reference-contexts: The edit distance may be computed in O (t v) time using dynamic programming <ref> [18, 31] </ref>. Many excellent reviews of the string edit distance literature are available [10, 14, 21, 29]. Several variants of the edit distance have been proposed, including the constrained edit distance [19] and the normalized edit distance [17].
Reference: [19] <author> Oomman, B. </author> <title> Constrained string editing. </title> <booktitle> Information Sciences 40 (1986), </booktitle> <pages> 267-284. </pages>
Reference-contexts: The edit distance may be computed in O (t v) time using dynamic programming [18, 31]. Many excellent reviews of the string edit distance literature are available [10, 14, 21, 29]. Several variants of the edit distance have been proposed, including the constrained edit distance <ref> [19] </ref> and the normalized edit distance [17]. A stochastic interpretation of string edit distance was first provided by Bahl and Jelinek [2], but without an algorithm for learning the edit costs. The need for such a learning algorithm is widely acknowledged [10, 20, 29].
Reference: [20] <author> Oommen, B., and Kashyap, R. </author> <title> Optimal and information theoretic syntactic pattern recognition for traditional errors. In Advances in Structural and Syntactic Pattern Recognition (Berlin, </title> <month> August 20-23 </month> <year> 1996), </year> <editor> P. Perner, P. Wang, and A. Rosenfeld, Eds., </editor> <publisher> Springer, </publisher> <pages> pp. 11-20. </pages>
Reference-contexts: A stochastic interpretation of string edit distance was first provided by Bahl and Jelinek [2], but without an algorithm for learning the edit costs. The need for such a learning algorithm is widely acknowledged <ref> [10, 20, 29] </ref>. The principal contribution of this report is an efficient algorithm for learning the primitive edit costs from a corpus of examples. To the best of our knowledge, this is the first published algorithm to automatically learn the primitive edit costs.
Reference: [21] <author> Peterson, J. </author> <title> Computer programs for detecting and correcting spelling errors. </title> <booktitle> Comm. ACM 23 (1980), </booktitle> <pages> 676-687. </pages>
Reference-contexts: The edit distance may be computed in O (t v) time using dynamic programming [18, 31]. Many excellent reviews of the string edit distance literature are available <ref> [10, 14, 21, 29] </ref>. Several variants of the edit distance have been proposed, including the constrained edit distance [19] and the normalized edit distance [17]. A stochastic interpretation of string edit distance was first provided by Bahl and Jelinek [2], but without an algorithm for learning the edit costs.
Reference: [22] <author> Redner, R. A., and Walker, H. F. </author> <title> Mixture densities, maximum likelihood, and the EM algorithm. </title> <journal> SIAM Review 26, </journal> <volume> 2 (1984), </volume> <pages> 195-239. </pages>
Reference-contexts: For this task, we employ the powerful expectation maximization (EM) framework [3, 4, 6]. An EM algorithm is an iterative algorithm that maximizes the probability of the training data according to the model. See <ref> [22] </ref> for a review. The applicability of EM to the problem of optimizing the parameters of a memoryless stochastic transducer was first noted by Bahl, Jelinek, and Mercer [2, 12], although they did not publish an explicit algorithm for this purpose.
Reference: [23] <author> Riley, M., Ljolje, A., Hindle, D., and Pereira, F. </author> <title> The AT&T 60,000 word speech-to-text system. </title> <booktitle> In Eurospeech'95: ECSA 4th Eu-ropean Conference on Speech Communication and Technology (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <editor> J. M. Pardo, E. Enrquez, J. Ortega, J. Ferreiros, J. Macas, and F.J.Valverde, Eds., </editor> <volume> vol. 1, </volume> <booktitle> European Speech Communication Association, </booktitle> <pages> pp. 207-210. </pages>
Reference-contexts: Current speech recognition technology typically employs a sparse pronouncing lexicon of hand-crafted underlying forms and imposes a uniform distribution on the underlying pronunciations given the words. When the vocabulary is large or contains many proper nouns, then the pronouncing lexicon may be generated by a text-to-speech system <ref> [23] </ref>. Our results suggest that a significant performance improvement is possible by employing a richer pronouncing lexicon, constructed directly from observed pronunciations, along with an adapted lexical entry model.
Reference: [24] <author> Riley, M. D., and Ljolje, A. </author> <title> Automatic generation of detailed pronunciation lexicons. In Automatic Speech and Speaker Recognition: Advanced Topics, </title> <editor> C.-H. Lee, F. K. Soong, and K. K. Paliwal, Eds. </editor> <publisher> Kluwer Academic, </publisher> <address> Boston, </address> <month> March </month> <year> 1996, </year> <note> ch. 12. </note>
Reference-contexts: Our results suggest that a significant performance improvement is possible by employing a richer pronouncing lexicon, constructed directly from observed pronunciations, along with an adapted lexical entry model. This tentative conclusion is supported by Riley and Ljolje <ref> [24] </ref>, who show an improvement in speech recognizer performance by employing a richer pronunciation model than is customary. Our approach differs from their approach in three important ways.
Reference: [25] <author> Ristad, E. S., and Thomas, R. G. </author> <title> Hierarchical non-emitting Markov models. </title> <booktitle> In Proc. 35th Annual Meeting of the ACL (Madrid, </booktitle> <month> July 7-11 </month> <year> 1997), </year> <pages> pp. 381-385. </pages>
Reference-contexts: Alternately, we might condition the probability of an edit operation z t on (a finite suffix of) the yield -(z t1 )) of the past edit sequence. These stochastic transducers can be further strengthened with state-conditional interpolation <ref> [13, 25] </ref> or by conditioning our edit probabilities ffi (z t jz t1 tn ; s) on a hidden state s drawn from a finite state space.
Reference: [26] <author> Ristad, E. S., and Yianilos, P. N. </author> <title> Finite growth models. </title> <type> Tech. Rep. </type> <institution> CS-TR-533-96, Department of Computer Science, Princeton University, Princeton, NJ, </institution> <month> December </month> <year> 1996. </year> <month> 23 </month>
Reference-contexts: Each iteration of our EM algorithm is guaranteed to either increase the probability of the training corpus or not change the model parameters. The correctness of our algorithm is shown in related work <ref> [26] </ref>. expectation-maximization (, C) 1. until convergence 2. forall z in E [ fl (z) := 0; ] 3. for i = 1 to n 4. expectation-step (x T i ; y V i ; ; fl,1); 5. maximization-step (,fl); The fl (z) variable accumulates the expected number of times that <p> Convergence. The expectation-maximization () algorithm given above is guaranteed to converge to a local maximum on a given corpus C, by a reduction to finite growth models <ref> [26, 32] </ref>. <p> Next, we propose a way to combine different transduction distances using the technique of finite mixture modeling. Finally, we suggest an even stronger class of string distances that are based on stochastic transducers with memory. A fourth variant the generalization to k-way transduction appears in related work <ref> [26, 32] </ref>. 2.4.1 Parameter Tying In many applications, the edit cost function is simpler than the one that we have been considering here.
Reference: [27] <author> Ristad, E. S., and Yianilos, P. N. </author> <title> Learning string edit distance. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference (San Francisco, </booktitle> <month> July 8-11 </month> <year> 1997), </year> <editor> D. Fisher, Ed., </editor> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 287-295. </pages>
Reference: [28] <author> Ristad, E. S., and Yianilos, P. N. </author> <title> Library of practical abstractions, release 1.2, </title> <year> 1997. </year> <month> ftp://ftp.cs.princeton.edu/pub/packages/libpa. </month>
Reference: [29] <author> Sankoff, D., and Kruskal, J. B., Eds. </author> <title> Time warps, string edits, and macromolecules: the theory and practice of sequence comparison. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: The edit distance may be computed in O (t v) time using dynamic programming [18, 31]. Many excellent reviews of the string edit distance literature are available <ref> [10, 14, 21, 29] </ref>. Several variants of the edit distance have been proposed, including the constrained edit distance [19] and the normalized edit distance [17]. A stochastic interpretation of string edit distance was first provided by Bahl and Jelinek [2], but without an algorithm for learning the edit costs. <p> A stochastic interpretation of string edit distance was first provided by Bahl and Jelinek [2], but without an algorithm for learning the edit costs. The need for such a learning algorithm is widely acknowledged <ref> [10, 20, 29] </ref>. The principal contribution of this report is an efficient algorithm for learning the primitive edit costs from a corpus of examples. To the best of our knowledge, this is the first published algorithm to automatically learn the primitive edit costs.
Reference: [30] <author> Viterbi, A. </author> <title> Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. </title> <journal> IEEE Trans. Inform. Theory IT-13 (1967), </journal> <pages> 260-269. </pages>
Reference: [31] <author> Wagner, R., and Fisher, M. </author> <title> The string to string correction problem. </title> <booktitle> JACM 21 (1974), </booktitle> <pages> 168-173. </pages>
Reference-contexts: The edit distance may be computed in O (t v) time using dynamic programming <ref> [18, 31] </ref>. Many excellent reviews of the string edit distance literature are available [10, 14, 21, 29]. Several variants of the edit distance have been proposed, including the constrained edit distance [19] and the normalized edit distance [17].
Reference: [32] <author> Yianilos, P. N. </author> <title> Topics in computational hidden state modeling. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Princeton University, Princeton, NJ, </institution> <month> June </month> <year> 1997. </year> <month> 24 </month>
Reference-contexts: Convergence. The expectation-maximization () algorithm given above is guaranteed to converge to a local maximum on a given corpus C, by a reduction to finite growth models <ref> [26, 32] </ref>. <p> Next, we propose a way to combine different transduction distances using the technique of finite mixture modeling. Finally, we suggest an even stronger class of string distances that are based on stochastic transducers with memory. A fourth variant the generalization to k-way transduction appears in related work <ref> [26, 32] </ref>. 2.4.1 Parameter Tying In many applications, the edit cost function is simpler than the one that we have been considering here.
References-found: 32

