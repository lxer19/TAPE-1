URL: ftp://cs.aston.ac.uk/neural/zhuh/reg_fil_prior.ps.Z
Refering-URL: http://wwwipd.ira.uka.de/~prechelt/NIPS_bench.html
Root-URL: 
Title: Bayesian Regression Filters and the Issue of Priors  
Author: Huaiyu Zhu and Richard Rohwer 
Keyword: regression, Bayesian method, Kalman filter, approximation, prior selection, radial basis functions, on-line learning. Running title: Bayesian Regression filter.  
Address: B4 7ET, UK  
Affiliation: Neural Computing Research Group Department of Computer Science and Applied Mathematics, Aston University, Birmingham  
Note: To appear in Neural Computation and Applications  
Email: Email: H.Zhu@aston.ac.uk  
Phone: Fax: 0121 333 3215,  
Web: R.J.Rohwer.ac.uk  
Date: November 9, 1995  
Abstract: We propose a Bayesian framework for regression problems, which covers areas which are usually dealt with by function approximation. An online learning algorithm is derived which solves regression problems with a Kalman filter. Its solution always improves with increasing model complexity, without the risk of over-fitting. In the infinite dimension limit it approaches the true Bayesian posterior. The issues of prior selection and over-fitting are also discussed, showing that some of the commonly held beliefs are misleading. The practical implementation is summarised. Simulations using 13 popular publicly available data sets are used to demonstrate the method and highlight important issues concerning the choice of priors.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. K. Chui and G. Chen. </author> <title> Kalman Filtering with Real-Time Applications. </title> <booktitle> Springer Series in Information Sciences. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1987. </year>
Reference-contexts: The efficiency depends on how well the functions in the prior are modelled by M on average. It increases towards unity as M becomes larger. The actual algorithm is simply a standard Kalman filter algorithm <ref> [1] </ref> with a particular method for setting the initial mean and variance. 1 In one dimensional cases it is usually called a random process. 3 1.
Reference: [2] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4(1) </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: The minimisation problem is equivalent to minimising k b f k gk 2 with respect to g. There is no bias/variance trade-off as discussed in <ref> [2] </ref>, since the generalisation error is already defined as the expected error on test set. 2 The posterior of f also depends on the prior of f , ie. the unconditional distribution Pr (f ).
Reference: [3] <editor> S. J. Hanson, J. D. Cowan, and C. Lee Giles, editors. </editor> <booktitle> Advances in Neural Information Processing Systems, volume 5, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [4] <author> G. M. Jenkins and D. G. Watts. </author> <title> Spectral Analysis and its Applications. </title> <publisher> Holden-Day, </publisher> <address> San Francisco, </address> <year> 1968. </year>
Reference-contexts: time it is used. 3 The kernel, which might or might not be of the form of a Gaussian function, parameterises the prior, which is always Gaussian random filed. 4 * The Gaussian kernel (Figure 1) V 0 * The Laplace kernel (Figure 2) V 0 * The Bachelier-Wiener Process <ref> [4] </ref> (Figure 3) has a covariance function which is usually called the "hat" function V 0 The prior can also be defined in terms of filtered white noise [4], which provides a convenient way to generate sample functions, thus assisting intuition about the prior. <p> filed. 4 * The Gaussian kernel (Figure 1) V 0 * The Laplace kernel (Figure 2) V 0 * The Bachelier-Wiener Process <ref> [4] </ref> (Figure 3) has a covariance function which is usually called the "hat" function V 0 The prior can also be defined in terms of filtered white noise [4], which provides a convenient way to generate sample functions, thus assisting intuition about the prior. <p> Five randomly drawn sample functions from this prior are shown in Figure 5 (a). The general variability reflected by these sample functions is all the prior knowledge. Further suppose that all the training data and test data are contained in the interval <ref> [4; 4] </ref>, with uniform distribution p (x), and that the noise is an independent zero-mean Gaussian process with 2 = :5. The training data is then generated by y = f (x) + , using a particular f drawn from Pr (f ), and is shown in Figure 5 (b). <p> If one really believes that the underlying function f is always smooth, it is advisable to assume a prior with Gaussian covariance kernel, which will assign vanishingly small prior variance to high frequency components <ref> [4] </ref>. This has the effect that it requires a higher than exponentially increasing amount of data to force the regression to have higher frequency components.
Reference: [5] <author> D. J. C. MacKay. </author> <title> Bayesian interpolation. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 415-447, </pages> <year> 1992. </year>
Reference-contexts: shown in Figure 5 (d)-5 (f), together with three error bars, indicating the cumulative effects of the uncertainty about b g k , the approximation, and the additive noise in the test data, in that order. 5 These results can be compared with those obtained by the method of MacKay <ref> [5] </ref>. It is important to note that in that method, the model size and smoothness are implicitly related, because a diagonal prior on the weight space is used with a basis function width inversely proportional to the number of basis functions. <p> Considering the uncertainties in the hyperparameters, this MSE might have been as little as 11.9 or as much as 19.7. 6 Discussion fully consistent theory, neither the data nor the learning rule play any role in this, although proposals of this nature has been studied <ref> [5, 13, 6, 14] </ref>.
Reference: [6] <author> D. J. C. MacKay. Hyperparameters: </author> <title> Optimize, or integrate out? In G. Heidbreder, editor, Maximum Entropy and Bayesian Methods, </title> <address> Santa Barbara 1993, Dordrecht, 1995. </address> <publisher> Kluwer. </publisher>
Reference-contexts: Considering the uncertainties in the hyperparameters, this MSE might have been as little as 11.9 or as much as 19.7. 6 Discussion fully consistent theory, neither the data nor the learning rule play any role in this, although proposals of this nature has been studied <ref> [5, 13, 6, 14] </ref>.
Reference: [7] <author> D. Michie, D. J. Speigelhalter, and C. C. Taylor. </author> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1994. </year>
Reference-contexts: Quinlan indirectly via G. Hinton in Toronto. c These data sets can be obtained from ftp://ics.uci.edu/pub/machinelearning. They are also used in the StatLog Project <ref> [7] </ref>. d These data sets can be obtained from ftp://ftp.santafe.edu/pub/TimeSeries. We use them to construct learning problems of predicting the next step from a window of 50 time steps.
Reference: [8] <author> R. M. Neal. </author> <title> Bayesian learning via stochastic dynamics. </title> <editor> In Hanson et al. </editor> <volume> [3], </volume> <pages> pages 475-482. </pages>
Reference-contexts: Considering the variability of the mean squared error expressed by Table 1, it can be said that to the precision of these experiments, the regression filter, which is a fixed finite dimensional method, performs comparably with exact Bayesian algorithms which either sample the posterior by a Monte Carlo method <ref> [8] </ref> or compute the posterior explicitly [12]. It is important to point out that the "Internet game" is quite artificial, since in applications one really wants the algorithm to perform well on the problems which would arise in a particular application area, rather than on average over many irrelevant problems.
Reference: [9] <author> R. M. Neal. </author> <title> Bayesian Learning for Neural Networks. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Toronto, </institution> <year> 1995. </year> <note> ftp://ftp.cs.utoronto.ca/pub/radford/thesis. ps.Z. </note>
Reference-contexts: The origin of the top four rows is <ref> [9] </ref>. sets. If the above assumptions about Internet data sets are correct, then this exercise should reveal a consistent range of values for these hyperparameters which could be expected to give reasonably good results for a randomly selected data set.
Reference: [10] <author> Quinlan J. R. </author> <title> Combining instance-based and model-based learning. </title> <editor> In P. E. Utgoff, editor, </editor> <booktitle> Proceedings of the Machine Learning Conference '93, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 0 C 2 l 0k s 0k l 2k s 2k StatLib (CMU) data sets a 0 boston 0.8 0.5 -1.2 0.5 1 bodyfat 1.0 1.0 -1.0 1.5 2 tecator 0.5 1.5 -1.5 1.5 3 polution 1.0 1.0 -0.5 1.0 4 cloud 0.0 3.0 0.0 2.0 Data sets used in <ref> [10] </ref> b 5 Mpg 0.5 2.0 -1.0 3.0 7 Aprce 0.0 1.5 -1.0 2.0 UCI data sets c 8 satim 0.5 1.0 -0.5 1.0 9 dna 1.0 1.5 -1.5 3.0 10 shuttle 1.0 2.0 -2.0 1.5 Santa Fe data sets d 11 santafeA 0.0 1.5 -1.0 1.5 12 santafeD 0.0 2.0
Reference: [11] <author> C. K. I. Williams, C. Qazaz, C. M. Bishop, and H. Zhu. </author> <title> On the relationship between Bayesian error bars and the input data density. </title> <booktitle> In Fourth International Conference on Artificial Neural Networks. IEE Conference Publications no. </booktitle> <volume> 409, </volume> <pages> pages 160-165, </pages> <year> 1995. </year> <month> 11 </month>
Reference-contexts: Using a real-world data set, the algorithm was shown to perform comparably to other Bayesian methods which require either Monte Carlo simulations or the maintenance of a sufficient statistic. Acknowledgements: This work was inspired by earlier joint work with C. Bishop, C. Qazaz, and C. Williams <ref> [11] </ref>. We would like to thank C. Williams for many interesting discussions and providing comparable data. The work of H. Zhu is supported by EPSRC Grant GR/J17814.
Reference: [12] <author> C. K. I. Williams and C. E. Rasmussen. </author> <title> Regression with gaussian processes. </title> <editor> In M. Mozer D. Touretzky and M. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year> <note> (To appear). </note>
Reference-contexts: If we actually compute b f k and V k , we shall be able to keep all the information in the posterior, which enables this process to carry on even if the data used to generate them is discarded <ref> [12] </ref>. They are called jointly sufficient statistics. Unfortunately, they can only be represented by vectors and matrices of dimension k, the size of the sample used to calculate them. <p> 83.4 Regression, linear functions 28.9 Monte Carlo, network with 8 hidden units 13.7 Monte Carlo, network with two hidden layers 12.4 Gaussian process regression 11.9 Bayesian Regression filter 13 Table 2: MSE of different methods on the Boston housing data The data in the first five rows are reproduced from <ref> [12] </ref>. The origin of the top four rows is [9]. sets. If the above assumptions about Internet data sets are correct, then this exercise should reveal a consistent range of values for these hyperparameters which could be expected to give reasonably good results for a randomly selected data set. <p> Such comparisons should be put in perspective, however. It is theoretically clear that with identical conditions the regression filter algorithm presented here cannot outperform the "regression with Gaussian processes" algorithm <ref> [12] </ref>, since the latter keeps a sufficient statistic which is of the same size as the training set, and is always the optimal algorithm for any given prior. <p> mean squared error expressed by Table 1, it can be said that to the precision of these experiments, the regression filter, which is a fixed finite dimensional method, performs comparably with exact Bayesian algorithms which either sample the posterior by a Monte Carlo method [8] or compute the posterior explicitly <ref> [12] </ref>. It is important to point out that the "Internet game" is quite artificial, since in applications one really wants the algorithm to perform well on the problems which would arise in a particular application area, rather than on average over many irrelevant problems.
Reference: [13] <author> D. H. Wolpert. </author> <title> On the use of evidence in neural neworks. </title> <editor> In Hanson et al. </editor> <volume> [3], </volume> <pages> pages 539-546. </pages>
Reference-contexts: Considering the uncertainties in the hyperparameters, this MSE might have been as little as 11.9 or as much as 19.7. 6 Discussion fully consistent theory, neither the data nor the learning rule play any role in this, although proposals of this nature has been studied <ref> [5, 13, 6, 14] </ref>.
Reference: [14] <author> H. Zhu and R. Rohwer. </author> <title> Information geometric measurements of generalisation. </title> <type> Technical Report NCRG/4350, </type> <institution> Aston University, </institution> <year> 1995. </year> <note> ftp://cs.aston.ac.uk/neural/zhuh/ generalisation.ps.Z. </note>
Reference-contexts: There is no bias/variance trade-off as discussed in [2], since the generalisation error is already defined as the expected error on test set. 2 The posterior of f also depends on the prior of f , ie. the unconditional distribution Pr (f ). It is shown elsewhere <ref> [14, 15] </ref> that no learning rule can be independent of a prior, either explicitly or implicitly. We shall later explain how the explicit form of the prior can be written down for practical problems. <p> Considering the uncertainties in the hyperparameters, this MSE might have been as little as 11.9 or as much as 19.7. 6 Discussion fully consistent theory, neither the data nor the learning rule play any role in this, although proposals of this nature has been studied <ref> [5, 13, 6, 14] </ref>.
Reference: [15] <author> H. Zhu and R. Rohwer. </author> <title> Measurements of generalisation based on information geometry. </title> <booktitle> Mathematics of Neural Networks and Applications Conference (MANNA), </booktitle> <address> Oxford. Ann. </address> <note> Math. Artif. Intell.(to appear) ftp://cs.aston.ac.uk/neural/zhuh/generalisation-manna.ps.Z., 1995. 12 </note>
Reference-contexts: There is no bias/variance trade-off as discussed in [2], since the generalisation error is already defined as the expected error on test set. 2 The posterior of f also depends on the prior of f , ie. the unconditional distribution Pr (f ). It is shown elsewhere <ref> [14, 15] </ref> that no learning rule can be independent of a prior, either explicitly or implicitly. We shall later explain how the explicit form of the prior can be written down for practical problems.
References-found: 15

