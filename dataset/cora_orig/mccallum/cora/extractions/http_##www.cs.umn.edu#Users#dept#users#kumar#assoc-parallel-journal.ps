URL: http://www.cs.umn.edu/Users/dept/users/kumar/assoc-parallel-journal.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: Scalable Parallel Data Mining for Association Rules  
Author: Eui-Hong (Sam) Han, George Karypis, and Vipin Kumar 
Keyword: Data mining, parallel processing, association rules, load balance, scalability.  
Date: 1999 1  
Note: IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. Y, MONTH  
Abstract: In this paper we propose two new parallel formulations of the Apriori algorithm that is used for computing association rules. These new formulations, IDD and HD, address the shortcomings of two previously proposed parallel formulations CD and DD. Unlike the CD algorithm, the IDD algorithm partitions the candidate set intelligently among processors to efficiently parallelize the step of building the hash tree. The IDD algorithm also eliminates the redundant work inherent in DD, and requires substantially smaller communication overhead than DD. But IDD suffers from the added cost due to communication of transactions among processors. HD is a hybrid algorithm that combines the advantages of CD and DD. Experimental results on a 128-processor Cray T3E show that HD scales just as well as the CD algorithm with respect to the number of transactions, and scales as well as IDD with respect to increasing candidate set size. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Stonebraker, R. Agrawal, U. Dayal, E. J. Neuhold, and A. Reuter, </author> <title> "DBMS research at a crossroads: The vienna update," </title> <booktitle> in Proc. of the 19th VLDB Conference, </booktitle> <address> Dublin, Ireland, </address> <year> 1993, </year> <pages> pp. 688-692. </pages>
Reference: [2] <author> R. Agrawal and R. Srikant, </author> <title> "Fast algorithms for mining association rules," </title> <booktitle> in Proc. of the 20th VLDB Conference, </booktitle> <address> Santiago, Chile, </address> <year> 1994, </year> <pages> pp. 487-499. </pages>
Reference-contexts: The support of a rule is also important, since it indicates how frequent the rule is in the transactions. Rules that have very small support are often uninteresting, since they do not describe significantly large populations. This is one of the reasons why most algorithms <ref> [2] </ref>, [3], [4] disregard any rules that do not satisfy the minimum support condition specified by the user. This filtering due to the minimum required support is also critical in reducing the number of derived association rules to a manageable size. <p> F k = fc 2 C k j c.count minsupg 8. g 9. Answer = S Fig. 1. Apriori Algorithm implementation of the second step is straightforward and is discussed in [6]. A number of sequential algorithms have been developed for discovering frequent item-sets [8], <ref> [2] </ref>, [3]. Our parallel algorithms are based on the Apriori algorithm [2] that has smaller computational complexity compared to other algorithms. In the rest of this section, we briefly describe the Apriori algorithm. The reader should refer to [2] for further details. <p> Answer = S Fig. 1. Apriori Algorithm implementation of the second step is straightforward and is discussed in [6]. A number of sequential algorithms have been developed for discovering frequent item-sets [8], <ref> [2] </ref>, [3]. Our parallel algorithms are based on the Apriori algorithm [2] that has smaller computational complexity compared to other algorithms. In the rest of this section, we briefly describe the Apriori algorithm. The reader should refer to [2] for further details. The high level structure of the Apriori algorithm is given in Figure 1. <p> number of sequential algorithms have been developed for discovering frequent item-sets [8], <ref> [2] </ref>, [3]. Our parallel algorithms are based on the Apriori algorithm [2] that has smaller computational complexity compared to other algorithms. In the rest of this section, we briefly describe the Apriori algorithm. The reader should refer to [2] for further details. The high level structure of the Apriori algorithm is given in Figure 1. The Apriori algorithm consists of a number of passes. Initially F 1 contains all the items (i.e., item set of size one) that satisfy the minimum support requirement. <p> One naive way to compute these counts is to perform string-matching of each transaction against each candidate item-set. A faster way of performing this operation is to use a candidate hash tree in which the candidate item-sets are hashed <ref> [2] </ref>. Here we explain this via an example to facilitate the discussions of parallel algorithms and their analysis. with candidates of size 3. The internal nodes of the hash tree have hash tables that contain links to child nodes. The leaf nodes contain the candidate item-sets. <p> The SP High Performance Switch (HPS) has a theoretical maximum bandwidth of 110 Mbytes/second. We generated a synthetic dataset using a tool provided by [17] and described in <ref> [2] </ref>. The parameters for the data set chosen are average transaction length of 15 and average size of frequent item sets of 6. Data sets with 1000 transactions (63Kbytes) were generated for different processors.
Reference: [3] <author> M. A. W. Houtsma and A. N. Swami, </author> <title> "Set-oriented mining for association rules in relational databases," </title> <booktitle> in Proc. of the 11th Int'l Conf. on Data Eng., </booktitle> <address> Taipei, Taiwan, </address> <year> 1995, </year> <pages> pp. 25-33. </pages>
Reference-contexts: The support of a rule is also important, since it indicates how frequent the rule is in the transactions. Rules that have very small support are often uninteresting, since they do not describe significantly large populations. This is one of the reasons why most algorithms [2], <ref> [3] </ref>, [4] disregard any rules that do not satisfy the minimum support condition specified by the user. This filtering due to the minimum required support is also critical in reducing the number of derived association rules to a manageable size. <p> F k = fc 2 C k j c.count minsupg 8. g 9. Answer = S Fig. 1. Apriori Algorithm implementation of the second step is straightforward and is discussed in [6]. A number of sequential algorithms have been developed for discovering frequent item-sets [8], [2], <ref> [3] </ref>. Our parallel algorithms are based on the Apriori algorithm [2] that has smaller computational complexity compared to other algorithms. In the rest of this section, we briefly describe the Apriori algorithm. The reader should refer to [2] for further details.
Reference: [4] <author> A. Savasere, E. Omiecinski, and S. Navathe, </author> <title> "An efficient algorithm for mining association rules in large databases," </title> <booktitle> in Proc. of the 21st VLDB Conference, </booktitle> <address> Zurich, Switzerland, </address> <year> 1995, </year> <pages> pp. 432-443. </pages>
Reference-contexts: The support of a rule is also important, since it indicates how frequent the rule is in the transactions. Rules that have very small support are often uninteresting, since they do not describe significantly large populations. This is one of the reasons why most algorithms [2], [3], <ref> [4] </ref> disregard any rules that do not satisfy the minimum support condition specified by the user. This filtering due to the minimum required support is also critical in reducing the number of derived association rules to a manageable size.
Reference: [5] <author> R. Srikant and R. Agrawal, </author> <title> "Mining generalized association rules," </title> <booktitle> in Proc. of the 21st VLDB Conference, </booktitle> <address> Zurich, Switzer-land, </address> <year> 1995, </year> <pages> pp. 407-419. </pages>
Reference: [6] <author> R. Agrawal and J.C. Shafer, </author> <title> "Parallel mining of association rules," </title> <journal> IEEE Transactions on Knowledge and Data Eng., </journal> <volume> vol. 8, no. 6, </volume> <pages> pp. 962-969, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: F k = fc 2 C k j c.count minsupg 8. g 9. Answer = S Fig. 1. Apriori Algorithm implementation of the second step is straightforward and is discussed in <ref> [6] </ref>. A number of sequential algorithms have been developed for discovering frequent item-sets [8], [2], [3]. Our parallel algorithms are based on the Apriori algorithm [2] that has smaller computational complexity compared to other algorithms. In the rest of this section, we briefly describe the Apriori algorithm. <p> III. Parallel Algorithms In this section, we will focus on the parallelization of the task that finds all frequent item-sets. We first discuss two parallel algorithms proposed in <ref> [6] </ref> to help motivate our parallel formulations. We also briefly discuss other parallel algorithms. In all our discussions, we assume that the transactions are evenly distributed among the processors. A. Count Distribution Algorithm In the Count Distribution (CD) algorithm proposed in [6], each processor computes how many times all the candidates <p> We first discuss two parallel algorithms proposed in <ref> [6] </ref> to help motivate our parallel formulations. We also briefly discuss other parallel algorithms. In all our discussions, we assume that the transactions are evenly distributed among the processors. A. Count Distribution Algorithm In the Count Distribution (CD) algorithm proposed in [6], each processor computes how many times all the candidates appear in the locally stored transactions. This is done by building the entire hash tree that corresponds to all the candidates and then performing a single pass over the locally stored transactions to collect the counts. <p> Thus, excluding the global reduction, each processor in the CD algorithm executes the serial Apriori algorithm on the locally stored transactions. This algorithm has been shown to scale linearly with the number of transactions <ref> [6] </ref>. This is because each processor can compute the counts independently of the other processors and needs to communicate with the other processors only once at the end of the computation step. However, this algorithm does not parallelize the computation of building the candidate hash tree. <p> Thus the CD algorithm is effective for small number of distinct items and a high minimum support level. B. Data Distribution Algorithm The Data Distribution (DD) algorithm <ref> [6] </ref> addresses the memory problem of the CD algorithm by partitioning the candidate item-sets among the processors. This partitioning is done in a round robin fashion. Each processor is responsible for computing the counts of its locally stored subset of the candidate item-sets for all the transactions in the database. <p> This algorithm exploits the total available memory better than CD, as it partitions the candidate set among processors. As the number of processors increases, the number of candidates that the algorithm can handle also increases. However, as reported in <ref> [6] </ref>, the performance of this algorithm is significantly worse than the CD algorithm. The run time of this algorithm is 10 to 20 times more than that of the CD algorithm on 16 processors [6]. <p> However, as reported in <ref> [6] </ref>, the performance of this algorithm is significantly worse than the CD algorithm. The run time of this algorithm is 10 to 20 times more than that of the CD algorithm on 16 processors [6]. The problem lies with the communication pattern of the algorithm and the redundant work that is performed in processing all the transactions. The communication pattern of this algorithm causes three problems. <p> With 0.1% support, the HD algorithm switched to CD algorithm in pass 5 of total 12 passes, and 88.4% of the overall response time of the serial code was spent in the first 4 passes. These scaleup results are shown in Figure 10. As noted in <ref> [6] </ref>, the DD algorithm scales very poorly. However, the performance achieved by IDD is much better than that of the DD algorithm. In particular, on 32 processors, IDD is faster than DD by a factor of 5.6.
Reference: [7] <author> E.H. Han, G. Karypis, and V. Kumar, </author> <title> "Scalable parallel data mining for association rules," </title> <booktitle> in Proc. of 1997 ACM-SIGMOD Int. Conf. on Management of Data, </booktitle> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference: [8] <author> R. Agrawal, T. Imielinski, and A. Swami, </author> <title> "Mining association rules between sets of items in large databases," </title> <booktitle> in Proc. of 1993 ACM-SIGMOD Int. Conf. on Management of Data, </booktitle> <address> Washing-ton, D.C., </address> <year> 1993. </year>
Reference-contexts: F k = fc 2 C k j c.count minsupg 8. g 9. Answer = S Fig. 1. Apriori Algorithm implementation of the second step is straightforward and is discussed in [6]. A number of sequential algorithms have been developed for discovering frequent item-sets <ref> [8] </ref>, [2], [3]. Our parallel algorithms are based on the Apriori algorithm [2] that has smaller computational complexity compared to other algorithms. In the rest of this section, we briefly describe the Apriori algorithm. The reader should refer to [2] for further details.
Reference: [9] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis, </author> <title> Introduction to Parallel Computing: Algorithm Design and Analysis, </title> <publisher> Benjamin Cummings/ Addison Wesley, </publisher> <address> Redwod City, </address> <year> 1994. </year>
Reference-contexts: The global counts of the candidates are computed by summing these IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. Y, MONTH 1999 4 individual counts using a global reduction operation <ref> [9] </ref>. This algorithm is illustrated in Figure 4. Note that since each processor needs to build a hash tree for all the candidates, these hash trees are identical at each processor. <p> This continues until every processor has processed all the transactions. Having computed the counts of its candidate item-sets, each processor finds the frequent item-sets from its candidate item-set and these frequent item-sets are sent to every other processor using an all-to-all broadcast operation <ref> [9] </ref>. Note that each processor has a different set of candidates in the candidate hash tree. This algorithm exploits the total available memory better than CD, as it partitions the candidate set among processors. <p> C. Intelligent Data Distribution Algorithm We developed the Intelligent Data Distribution (IDD) algorithm that solves the problems of the DD algorithm discussed in Section III-B. In IDD, the locally stored portions of the database are sent to all the other processors by using a ring-based all-to-all broadcast described in <ref> [9] </ref>. This operation does not suffer from the contention problems of the DD algorithm and it takes O (N ) time on any parallel architecture that can be embedded in a ring. Figure 6 shows the pseudo code for this data movement operation. <p> In this analysis, a parallel algorithm is considered scalable when the efficiency can be maintained as the number of processors is increased, provided that the problem size is also increased <ref> [9] </ref>. Let T serial be the runtime of a serial algorithm and T p be the runtime of a parallel algorithm. Efficiency [9] (E) of a parallel algorithm is E = T serial P fi T p A parallel algorithm is scalable if P fi T p and T serial remain <p> analysis, a parallel algorithm is considered scalable when the efficiency can be maintained as the number of processors is increased, provided that the problem size is also increased <ref> [9] </ref>. Let T serial be the runtime of a serial algorithm and T p be the runtime of a parallel algorithm. Efficiency [9] (E) of a parallel algorithm is E = T serial P fi T p A parallel algorithm is scalable if P fi T p and T serial remain of the same order [9]. <p> Efficiency <ref> [9] </ref> (E) of a parallel algorithm is E = T serial P fi T p A parallel algorithm is scalable if P fi T p and T serial remain of the same order [9]. The problem size (i.e., the serial runtime) for the Apriori algorithm increases either by increasing N or by increasing M (as a result of lowering the minimum support) in the algorithms discussed in Section III. Table III describes the symbols used in this section.
Reference: [10] <author> C. H. Papadimitriou and K. Steiglitz, </author> <title> Combinatorial Optimization: Algorithms and Complexity, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1982. </year>
Reference-contexts: To achieve an equal distribution of the candidate item-sets, we use a partitioning algorithm that is based on bin-packing <ref> [10] </ref>. For each item, we first compute the number of candidate item-sets starting with this particular item. Note that at this time we do not actually store the candidate item-sets, but just store the number of candidate item-sets starting with each item.
Reference: [11] <author> Takahiko Shintani and Masaru Kitsuregawa, </author> <title> "Hash based parallel algorithms for mining association rules," </title> <booktitle> in Proc. of the Conference on Paralellel and Distributed Information Systems, </booktitle> <year> 1996. </year>
Reference-contexts: Other Parallel Algorithms In addition to CD and DD, four parallel algorithms (NPA, SPA, HPA and HPA-ELD) for mining association rules were proposed in <ref> [11] </ref>. NPA is very similar to CD and SPA is very similar to DD. HPA and HPA-ELD both have some similarities with IDD, as all 3 algorithms essentially eliminate the redundant computation inherent in DD.
Reference: [12] <author> J.S. Park, M.S. Chen, and P.S. Yu, </author> <title> "Efficient parallel data mining for association rules," </title> <booktitle> in Proceedings of the 4th Int'l Conf. on Information and Knowledge Management, </booktitle> <year> 1995. </year>
Reference-contexts: For small values of k (e.g., k = 2), it is possible for HPA to incur smaller communication overhead than IDD. Several researchers have proposed parallel formulations of association rule algorithms <ref> [12] </ref>, [13], [14]. Park, Chen, and Yu proposed PDM [12], a parallel formulation of the serial association rule algorithm DHP [15]. PDM is similar in nature to the CD algorithm. In [14], Zaki et.al. presented a parallelization of a serial algorithm originally introduced in [16]. <p> For small values of k (e.g., k = 2), it is possible for HPA to incur smaller communication overhead than IDD. Several researchers have proposed parallel formulations of association rule algorithms <ref> [12] </ref>, [13], [14]. Park, Chen, and Yu proposed PDM [12], a parallel formulation of the serial association rule algorithm DHP [15]. PDM is similar in nature to the CD algorithm. In [14], Zaki et.al. presented a parallelization of a serial algorithm originally introduced in [16].
Reference: [13] <author> D. Cheung, V. Ng, A. Fu, and Y. Fu, </author> <title> "Efficient mining of association rules in distributed databases," </title> <journal> IEEE Transactions on Knowledge and Data Eng., </journal> <volume> vol. 8, no. 6, </volume> <pages> pp. 911-922, </pages> <year> 1996. </year>
Reference-contexts: For small values of k (e.g., k = 2), it is possible for HPA to incur smaller communication overhead than IDD. Several researchers have proposed parallel formulations of association rule algorithms [12], <ref> [13] </ref>, [14]. Park, Chen, and Yu proposed PDM [12], a parallel formulation of the serial association rule algorithm DHP [15]. PDM is similar in nature to the CD algorithm. In [14], Zaki et.al. presented a parallelization of a serial algorithm originally introduced in [16].
Reference: [14] <author> Mohammed Javeed Zaki, Srinivasan Parthasarathy, Mitsunori Ogihara, and Wei Li, </author> <title> "New parallel algorithms for fast discovery of association rules," Data Mining and Knowledge Discovery: </title> <journal> An International Journal, </journal> <volume> vol. 1, no. 4, </volume> <year> 1997. </year>
Reference-contexts: For small values of k (e.g., k = 2), it is possible for HPA to incur smaller communication overhead than IDD. Several researchers have proposed parallel formulations of association rule algorithms [12], [13], <ref> [14] </ref>. Park, Chen, and Yu proposed PDM [12], a parallel formulation of the serial association rule algorithm DHP [15]. PDM is similar in nature to the CD algorithm. In [14], Zaki et.al. presented a parallelization of a serial algorithm originally introduced in [16]. <p> Several researchers have proposed parallel formulations of association rule algorithms [12], [13], <ref> [14] </ref>. Park, Chen, and Yu proposed PDM [12], a parallel formulation of the serial association rule algorithm DHP [15]. PDM is similar in nature to the CD algorithm. In [14], Zaki et.al. presented a parallelization of a serial algorithm originally introduced in [16]. This serial algorithm is of entirely different nature than Apriori, hence its parallel formulations cannot be compared to the algorithms discussed in this paper. IV.
Reference: [15] <author> J.S. Park, M.S. Chen, and P.S. Yu, </author> <title> "An effective hash-based algorithm for mining association rules," </title> <booktitle> in Proc. of 1995 ACM-SIGMOD Int. Conf. on Management of Data, </booktitle> <year> 1995. </year>
Reference-contexts: Several researchers have proposed parallel formulations of association rule algorithms [12], [13], [14]. Park, Chen, and Yu proposed PDM [12], a parallel formulation of the serial association rule algorithm DHP <ref> [15] </ref>. PDM is similar in nature to the CD algorithm. In [14], Zaki et.al. presented a parallelization of a serial algorithm originally introduced in [16]. This serial algorithm is of entirely different nature than Apriori, hence its parallel formulations cannot be compared to the algorithms discussed in this paper. IV.
Reference: [16] <author> Mohammed Javeed Zaki, Srinivasan Parthasarathy, Mitsunori Ogihara, and Wei Li, </author> <title> "New algorithms for fast discovery of association rules," </title> <booktitle> in Proc. of the Third Int'l Conference on Knowledge Discovery and Data Mining, </booktitle> <year> 1997. </year>
Reference-contexts: Park, Chen, and Yu proposed PDM [12], a parallel formulation of the serial association rule algorithm DHP [15]. PDM is similar in nature to the CD algorithm. In [14], Zaki et.al. presented a parallelization of a serial algorithm originally introduced in <ref> [16] </ref>. This serial algorithm is of entirely different nature than Apriori, hence its parallel formulations cannot be compared to the algorithms discussed in this paper. IV. Performance Analysis In this section, we analyze the amount of work done by each algorithm and the scalability of each algorithm.
Reference: [17] <author> IBM Quest Data Mining Project, </author> <title> "Quest synthetic data generation code," </title> <note> http://www.almaden.ibm.com/cs/quest/syndata.html, 1996. </note>
Reference-contexts: The SP High Performance Switch (HPS) has a theoretical maximum bandwidth of 110 Mbytes/second. We generated a synthetic dataset using a tool provided by <ref> [17] </ref> and described in [2]. The parameters for the data set chosen are average transaction length of 15 and average size of frequent item sets of 6. Data sets with 1000 transactions (63Kbytes) were generated for different processors.
References-found: 17

