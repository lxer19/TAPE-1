URL: http://www.cs.wustl.edu/cs/techreports/1995/wucs-95-05.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Email: jbf@random.wustl.edu vasu@wuccrc.wustl.edu  
Title: A General Matrix Iterative Model for Dynamic Load Balancing  
Author: Mark A. Franklin Vasudha Govindan 
Note: This research has been sponsored in part by funding from the NSF under Grant CCR-9021041 and ARPA under contract DABT-93-C0057.  
Address: Campus Box 1115, One Brookings Drive Washington University, St. Louis, Missouri 63130  
Affiliation: Computer and Communications Research Center  
Abstract: Effective load balancing algorithms are crucial in fully realizing the performance potential of parallel computer systems. This paper proposes a general matrix iterative model to represent a range of dynamic load balancing algorithms. The model and associated performance measures are used to evaluate and compare various load balancing algorithms and derive optimal algorithms and associated parameters for a given application and multiprocessor system. The model is parameterized to represent three load balancing algorithms the random strategy, diffusion and complete redistribution algorithms. The model is validated by comparing the results with measured performance on a realistic workload. The parallel N-body simulation application used for this purpose has a number of interesting properties and is representative of a wide class of realistic scientific applications. The performance of the three algorithms are compared and optimal algorithm parameters derived for the application. The random strategy outperforms both the diffusion (12% better) and the redistribution (30% better) algorithms and its performance is within 25% of the ideal load balance case. General performance models such as the one presented in this paper can be used to guide the algorithm designer in choosing the best algorithm and associated parameters for a given environment. Keywords: Dynamic load balancing, Performance model, Matrix iterative model, Parallel N-body simulation y Submitted to IEEE Transactions on Parallel and Distributed Systems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ishfaq Ahmad, Arif Ghafoor, and Kishan Mehrotra. </author> <title> Performance Prediction of Distributed Load Balancing on Multicomputer Systems. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <year> 1991. </year>
Reference-contexts: The principal focus of this paper is the analysis of dynamic, distributed, sender-initiated load balancing schemes. The model presented here, however, can be extended to represent other load balancing schemes. There have been several studies on the performance analysis and modeling of load balancing algorithms <ref> [1, 5, 6, 7, 13, 17] </ref>. Eager et.al [7] use analytic queueing models to compare the performance of adaptive load sharing algorithms of varying complexity. Based on their analysis, they conclude that extremely simple adaptive algorithms yield dramatic performance improvements. The models, however, simplify several aspects of a real system. <p> Based on their analysis, they conclude that extremely simple adaptive algorithms yield dramatic performance improvements. The models, however, simplify several aspects of a real system. It is hard to represent complex system constraints and overheads in analytic queueing models. Ahmad et.al <ref> [1] </ref> present and, using simulation techniques, solve a queueing oriented state transition model of a range of sender initiated balancing techniques. The model focuses on the balanced scheduling of independent tasks. The state transition model formulation and techniques differ from the iterative approach taken here.
Reference: [2] <author> Mikhail J. Atallah, Christina Lock Black, Dan C. Marinescu, Howard Jay Siegel, and Thomas L. Casavant. </author> <title> Models and Algorithms for Coscheduling Compute-Intensive Tasks on a Network of Workstations. </title> <journal> Journal of Parallel and Distributed computing, </journal> <volume> 16, </volume> <year> 1992. </year>
Reference-contexts: The various load balancing algorithms presented in the literature [4, 7, 13, 14] generally focus on distributing the workload in some equal fashion among the available processors. In this paper, we focus on load balancing of cooperating tasks belonging to a single application running on multiple processors <ref> [2, 8, 11, 19] </ref>. We develop a simple but effective matrix iterative model to represent a wide range of load balancing algorithms. The model and associated performance measures are used to quantitatively compare various load balancing algorithms.
Reference: [3] <author> Joshua E. Barnes and Piet Hut. </author> <title> A Hierarchical O(N log N) Force Calculation Algo rithm. </title> <journal> Nature, </journal> <volume> 324(4), </volume> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: Due to their non-uniform, time varying computational requirements, some form of dynamic load balancing is necessary to obtain good performance on parallel systems. The N-body simulation discussed here is an implementation of the Barnes-Hut hierarchical algorithm <ref> [3] </ref> for a two dimensional gravitational N-body problem. The input consists of the mass, initial position and initial velocity of the particles distributed over a finite physical domain.
Reference: [4] <author> Thomas L. Casavant and Jon G. Kuhl. </author> <title> A taxonomy of scheduling in general purpose distributed computing systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(2) </volume> <pages> 141-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Effective processor and communication resource utilization is essential in fully realizing the performance potential of parallel computer systems, and central to this is the development of appropriate load balancing (or load sharing [7]) algorithms. The various load balancing algorithms presented in the literature <ref> [4, 7, 13, 14] </ref> generally focus on distributing the workload in some equal fashion among the available processors. In this paper, we focus on load balancing of cooperating tasks belonging to a single application running on multiple processors [2, 8, 11, 19]. <p> The parallel N-body simulation application used for this purpose has a number of interesting properties and is representative of a wide class of realistic scientific applications. Load balancing algorithms can be broadly classified into static and dynamic strategies <ref> [4, 12] </ref>. In static schemes, assignment of tasks to processors is made before execution and it is fixed throughout the execution time. Dynamic (or Adaptive) load balancing schemes periodically reassign tasks as needed during execution to achieve balance.
Reference: [5] <author> Yuan-Chieh Chow and Walter H. Kohler. </author> <title> Models for dynamic load balancing in a heterogeneous multiple processor system. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(5):354-361, </volume> <month> May </month> <year> 1979. </year>
Reference-contexts: The principal focus of this paper is the analysis of dynamic, distributed, sender-initiated load balancing schemes. The model presented here, however, can be extended to represent other load balancing schemes. There have been several studies on the performance analysis and modeling of load balancing algorithms <ref> [1, 5, 6, 7, 13, 17] </ref>. Eager et.al [7] use analytic queueing models to compare the performance of adaptive load sharing algorithms of varying complexity. Based on their analysis, they conclude that extremely simple adaptive algorithms yield dramatic performance improvements. The models, however, simplify several aspects of a real system.
Reference: [6] <author> George Cybenko. </author> <title> Dynamic Load Balancing for Distributed Memory Multiprocessors. </title> <journal> Journal of Parallel Distributed Computing, </journal> <volume> 7(1) </volume> <pages> 279-301, </pages> <year> 1989. </year>
Reference-contexts: The principal focus of this paper is the analysis of dynamic, distributed, sender-initiated load balancing schemes. The model presented here, however, can be extended to represent other load balancing schemes. There have been several studies on the performance analysis and modeling of load balancing algorithms <ref> [1, 5, 6, 7, 13, 17] </ref>. Eager et.al [7] use analytic queueing models to compare the performance of adaptive load sharing algorithms of varying complexity. Based on their analysis, they conclude that extremely simple adaptive algorithms yield dramatic performance improvements. The models, however, simplify several aspects of a real system. <p> The state transition model formulation and techniques differ from the iterative approach taken here. The iterative approach used in this paper is more flexible and can be used to model load balancing on a heterogeneous system of dependent tasks. Cybenko <ref> [6] </ref> presents a model for a simplified diffusion load balancing scheme where the movement of tasks between processors is analogous to the physical process of diffusion, and using matrix iterative techniques shows that the load distribution converges to the uniform distribution. <p> Cybenko <ref> [6] </ref> showed that the diffusion algorithm applied to a system with uneven load distribution and identical processors will eventually result in a balanced load distribution.
Reference: [7] <author> Derek L. Eager, Edward D. Lazowska, and John Zoharjan. </author> <title> Adaptive Load Sharing in Homogeneous Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 12(5), </volume> <month> May </month> <year> 1986. </year>
Reference-contexts: 1 Introduction Effective processor and communication resource utilization is essential in fully realizing the performance potential of parallel computer systems, and central to this is the development of appropriate load balancing (or load sharing <ref> [7] </ref>) algorithms. The various load balancing algorithms presented in the literature [4, 7, 13, 14] generally focus on distributing the workload in some equal fashion among the available processors. <p> 1 Introduction Effective processor and communication resource utilization is essential in fully realizing the performance potential of parallel computer systems, and central to this is the development of appropriate load balancing (or load sharing [7]) algorithms. The various load balancing algorithms presented in the literature <ref> [4, 7, 13, 14] </ref> generally focus on distributing the workload in some equal fashion among the available processors. In this paper, we focus on load balancing of cooperating tasks belonging to a single application running on multiple processors [2, 8, 11, 19]. <p> The principal focus of this paper is the analysis of dynamic, distributed, sender-initiated load balancing schemes. The model presented here, however, can be extended to represent other load balancing schemes. There have been several studies on the performance analysis and modeling of load balancing algorithms <ref> [1, 5, 6, 7, 13, 17] </ref>. Eager et.al [7] use analytic queueing models to compare the performance of adaptive load sharing algorithms of varying complexity. Based on their analysis, they conclude that extremely simple adaptive algorithms yield dramatic performance improvements. The models, however, simplify several aspects of a real system. <p> The model presented here, however, can be extended to represent other load balancing schemes. There have been several studies on the performance analysis and modeling of load balancing algorithms [1, 5, 6, 7, 13, 17]. Eager et.al <ref> [7] </ref> use analytic queueing models to compare the performance of adaptive load sharing algorithms of varying complexity. Based on their analysis, they conclude that extremely simple adaptive algorithms yield dramatic performance improvements. The models, however, simplify several aspects of a real system.
Reference: [8] <author> Geoffrey C. Fox et. al. </author> <title> Solving Problems on Concurrent Processor. </title> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: The various load balancing algorithms presented in the literature [4, 7, 13, 14] generally focus on distributing the workload in some equal fashion among the available processors. In this paper, we focus on load balancing of cooperating tasks belonging to a single application running on multiple processors <ref> [2, 8, 11, 19] </ref>. We develop a simple but effective matrix iterative model to represent a wide range of load balancing algorithms. The model and associated performance measures are used to quantitatively compare various load balancing algorithms. <p> After computation of M (k), all processors would participate and transfer tasks as required. In our implementation of the algorithm for the N-body simulation application, the task information is communicated to processor P 1 and P 1 computes the M matrix using the orthogonal recursive bisection technique <ref> [8] </ref> and communicates the appropriate rows of the matrix to the other processors. The processors then participate in task transfer as required.
Reference: [9] <author> Mark Franklin and Vasudha Govindan. </author> <title> The N-body Problem: Distributed System Load Balancing and Performance Evaluation. </title> <booktitle> In Proceedings of the 6th International Conference on Parallel and Distributed Computing Systems, </booktitle> <month> October </month> <year> 1993. </year> <institution> Louisville, Kentucky, USA. </institution>
Reference-contexts: A mean value performance model for the N-body simulation application on a network of workstations was developed in <ref> [9] </ref>. Measured and model predicted values were found to be within 10% of each other. 5 Model and Experimental Results 5.1 Model Validation In this section validation results for the load balancing model (applied to the N-body application) are presented. <p> In an ideally balanced system, the number of tasks allocated to a processor is proportional to its compute speed. All the processors therefore spend almost equal time for each iteration. The performance model of <ref> [9] </ref> is used in conjunction with equation 16 to obtain the execution time and speedup associated with the ideal balance case. For both the experiment and the model, the initial workload distribution is perfectly balanced (i.e., the task distribution, w (0) = w (0)).
Reference: [10] <author> Al Geist, Adan Beguelin, Jack Dongarra, Weicheng Jiang, and Vaidy Sunderam. </author> <title> PVM 3 User's Guide and Reference Manual. </title> <institution> Oak Ridge National Laboratoty, Oak Ridge, Tennessee 37830, </institution> <address> ornl/tm-12187 edition, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The simulation enables us to study the evolution of such a system over time. The computing platform consists of up to 16 Sun/Sparc workstations networked by a standard ethernet. The application program is implemented under the PVM <ref> [16, 10] </ref> environment. PVM is a programming environment that enables a set of networked computers to be used as a single concurrent computing resource. The processors in our network are of unequal computing abilities.
Reference: [11] <author> Reinhard V. Hanxleden and L. Ridgway Scott. </author> <title> Load Balancing on Message Passing Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(3) </volume> <pages> 312-324, </pages> <year> 1991. </year>
Reference-contexts: The various load balancing algorithms presented in the literature [4, 7, 13, 14] generally focus on distributing the workload in some equal fashion among the available processors. In this paper, we focus on load balancing of cooperating tasks belonging to a single application running on multiple processors <ref> [2, 8, 11, 19] </ref>. We develop a simple but effective matrix iterative model to represent a wide range of load balancing algorithms. The model and associated performance measures are used to quantitatively compare various load balancing algorithms.
Reference: [12] <author> A. J. Harget and I. D. Johnson. </author> <title> Distributed Computer Systems; Part 1, chapter Load Balancing Algorithms in Loosely Coupled Distributed Systems: A Survey. </title> <publisher> Butterworth and Co, </publisher> <year> 1990. </year>
Reference-contexts: The parallel N-body simulation application used for this purpose has a number of interesting properties and is representative of a wide class of realistic scientific applications. Load balancing algorithms can be broadly classified into static and dynamic strategies <ref> [4, 12] </ref>. In static schemes, assignment of tasks to processors is made before execution and it is fixed throughout the execution time. Dynamic (or Adaptive) load balancing schemes periodically reassign tasks as needed during execution to achieve balance.
Reference: [13] <author> Orly Kremien and Jeff Kramer. </author> <title> Methodical Analysis of Adaptive Load Sharing Algorithms. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6), </volume> <month> November </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Effective processor and communication resource utilization is essential in fully realizing the performance potential of parallel computer systems, and central to this is the development of appropriate load balancing (or load sharing [7]) algorithms. The various load balancing algorithms presented in the literature <ref> [4, 7, 13, 14] </ref> generally focus on distributing the workload in some equal fashion among the available processors. In this paper, we focus on load balancing of cooperating tasks belonging to a single application running on multiple processors [2, 8, 11, 19]. <p> The principal focus of this paper is the analysis of dynamic, distributed, sender-initiated load balancing schemes. The model presented here, however, can be extended to represent other load balancing schemes. There have been several studies on the performance analysis and modeling of load balancing algorithms <ref> [1, 5, 6, 7, 13, 17] </ref>. Eager et.al [7] use analytic queueing models to compare the performance of adaptive load sharing algorithms of varying complexity. Based on their analysis, they conclude that extremely simple adaptive algorithms yield dramatic performance improvements. The models, however, simplify several aspects of a real system.
Reference: [14] <author> Frank C. H. Lin and Robert Keller. </author> <title> The Gradient Model Load Balancing Method. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1), </volume> <month> January </month> <year> 1987. </year> <month> 22 </month>
Reference-contexts: 1 Introduction Effective processor and communication resource utilization is essential in fully realizing the performance potential of parallel computer systems, and central to this is the development of appropriate load balancing (or load sharing [7]) algorithms. The various load balancing algorithms presented in the literature <ref> [4, 7, 13, 14] </ref> generally focus on distributing the workload in some equal fashion among the available processors. In this paper, we focus on load balancing of cooperating tasks belonging to a single application running on multiple processors [2, 8, 11, 19].
Reference: [15] <author> M. Henon S. J. Aarseth and R. Wielen. </author> <title> Comparison of Numerical Methods for the Study of Star Cluster Dynamics. Astronomy and Astrophysics, </title> <type> 37(183), </type> <year> 1974. </year>
Reference-contexts: The topology matrix constraints the load balancing activity and the m matrix. The dynamics of the N-body simulations depend on the initial distribution of particles. Our workload simulates the evolution of 1024 particles with an initial mass, velocity and spatial distribution based on the Plummer's model <ref> [15] </ref>. Plummer's model is a standard 13 Table 2: Performance Measure: Definitions max iter total number of iterations. T total total execution time. on p processors. T appl;i (k) time for application task execution for iteration k, on P i, for a p processor run.
Reference: [16] <author> Vaidy S. Sunderam. </author> <title> PVM: A Framework for Parallel Distributed Computing. </title> <journal> Con-currency: Practice and Experience, </journal> <volume> 2, </volume> <month> december </month> <year> 1990. </year>
Reference-contexts: The simulation enables us to study the evolution of such a system over time. The computing platform consists of up to 16 Sun/Sparc workstations networked by a standard ethernet. The application program is implemented under the PVM <ref> [16, 10] </ref> environment. PVM is a programming environment that enables a set of networked computers to be used as a single concurrent computing resource. The processors in our network are of unequal computing abilities.
Reference: [17] <author> Marc H. Willebeek-LeMair and Anthony P. Reeves. </author> <title> Strategies for Load Balancing for Highly Parallel Systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(9), </volume> <month> September </month> <year> 1993. </year>
Reference-contexts: The principal focus of this paper is the analysis of dynamic, distributed, sender-initiated load balancing schemes. The model presented here, however, can be extended to represent other load balancing schemes. There have been several studies on the performance analysis and modeling of load balancing algorithms <ref> [1, 5, 6, 7, 13, 17] </ref>. Eager et.al [7] use analytic queueing models to compare the performance of adaptive load sharing algorithms of varying complexity. Based on their analysis, they conclude that extremely simple adaptive algorithms yield dramatic performance improvements. The models, however, simplify several aspects of a real system. <p> The model does not take into account task dependencies and load balancing costs and is thus of limited use in real parallel environments. Willbeck-LeMair and Reeves <ref> [17] </ref> study the performance benefits and overheads of five dynamic load balancing algorithms. The five algorithms illustrate the tradeoff between the accuracy of load balancing decisions and the costs incurred by the balancing processes.
Reference: [18] <author> Ellen Witte, Roger Chamberlian, and Mark Franklin. </author> <title> Parallel simulated annealing using speculative computation. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 483-494, </pages> <month> october </month> <year> 1991. </year>
Reference-contexts: The migration decisions are typically based on the system state information available. The techniques used in the decision making process range from simple heuristics (e.g., 5 random destination selection) to complex optimization algorithms (e.g., simulated annealing <ref> [18] </ref>.). Task migration decisions are represented by a p fi p matrix (for a p-processor system), M (k). M ij is the fraction of its tasks that processor P i decides to send to P j (i.e., P i sends M ij (k)w i (k) tasks to P j).
Reference: [19] <author> Jian Xu and Kai Hwang. </author> <title> Heuristic Methods for Dynamic Load Balancing in a Message-Passing Multicomputer. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18 </volume> <pages> 1-13, </pages> <year> 1993. </year>
Reference-contexts: The various load balancing algorithms presented in the literature [4, 7, 13, 14] generally focus on distributing the workload in some equal fashion among the available processors. In this paper, we focus on load balancing of cooperating tasks belonging to a single application running on multiple processors <ref> [2, 8, 11, 19] </ref>. We develop a simple but effective matrix iterative model to represent a wide range of load balancing algorithms. The model and associated performance measures are used to quantitatively compare various load balancing algorithms.
References-found: 19

