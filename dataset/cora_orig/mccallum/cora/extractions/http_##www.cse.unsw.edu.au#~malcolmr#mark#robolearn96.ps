URL: http://www.cse.unsw.edu.au/~malcolmr/mark/robolearn96.ps
Refering-URL: http://www.cse.unsw.edu.au/~malcolmr/mark/robolearn96-abs.html
Root-URL: 
Email: E-mail: fpendrith,malcolmrg@cse.unsw.edu.au  
Title: C-Trace: A new algorithm for reinforcement learning of robotic control.  
Author: Mark D. Pendrith and Malcolm R.K. Ryan 
Address: Sydney 2052 Australia  
Affiliation: School of Computer Science and Engineering The University of New South Wales  
Abstract: There has been much recent interest in the potential of using reinforcement learning techniques for control in autonomous robotic agents. How to implement effective reinforcement learning in a real-world robotic environment still involves many open questions. Are standard reinforcement learning algorithms like Watkins' Q-learning appropriate, or are other approaches more suitable? Some specific issues to be considered are noise/disturbance and the possibly non-Markovian aspects of the control problem. These are the particular issues we focus upon in this paper. The test-bed for the experiments described in this paper is a real six-legged insectoid walking robot; the task set is to learn an effectively co-ordinated walking gait. The performance of a new algorithm we call C-Trace is compared to Watkins' well-known 1-step Q-learning reinforcement learning algorithm. We discuss the markedly superior performance of this new algorithm in the context of both theoretical and existing empirical results regarding learning in noisy and non-Markovian domains.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A., & Duff, M. </author> <year> (1994). </year> <title> Monte carlo matrix inversion and reinforcement learning. </title> <editor> In D.S.Touretsky (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <address> San Mateo, Cali-fornia: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is discussed further in section 4. 4 Related work It is instructive to compare the P-Trace and C-Trace algorithms to both multi-step TD methods (e.g. Q ()- learning (Peng & Williams, 1994)), and to Monte Carlo methods (see, for example, <ref> (Barto & Duff, 1994) </ref>). The P-Trace mechanism is conceptually related to a Monte Carlo method, and indeed can be viewed as a Monte Carlo method specialised for sampling Q (s; a) values, where represents the current policy.
Reference: <author> Barto, A., Sutton, R., & Anderson, C. </author> <year> (1983). </year> <title> Neu-ronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-13 (5), </volume> <pages> 834-846. </pages>
Reference: <author> Brooks, R. </author> <year> (1991). </year> <title> Intelligence without reason. </title> <booktitle> In Proceedings of the 12 th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 569-595. </pages>
Reference-contexts: The goal of the reinforcement learning algorithms is to discover a policy that maximises total future-discounted payoff. In the spirit of Brooks' subsumption architecture <ref> (Brooks, 1991) </ref>, the robot has already been provided with a set of hard-wired low-level "step-reflexes". The natural fit between RL and behaviour-based robots has already been successfully exploited by previous workers (Mahadevan & Connell, 1991).
Reference: <author> Cichosz, P. </author> <year> (1995). </year> <title> Truncating temporal differences: On the efficient implementation of TD() for reinforcement learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 287-318. </pages>
Reference-contexts: Further, both P-Trace and C-Trace also avoid a difficulty with the Q ()-learning approach described in both (Peng & Williams, 1994) and in <ref> (Cichosz, 1995) </ref> to do with an inaccuracy proportional to the square of the learning factor fi in the calculated return. Additionally, both P-Trace and C-Trace enjoy better space and time complexity characteristics.
Reference: <author> Jaakkola, T., Singh, S., & Jordan, M. </author> <year> (1995). </year> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Further, it is an implicit assumption in a RL algorithm based upon a temporal differences (TD) design that the Markov property holds (Sutton, 1988; Watkins, 1989; Moore & Atkeson, 1993). Virtually all theoretical analyses of these algorithms have been conducted in this context <ref> (Jaakkola et al., 1995) </ref>. Actual return learners, such as Monte Carlo methods (Barto & Duff, 1994; Jaakkola et al., 1995) and the P-Trace reinforcement learning algorithm (Pendrith & Ryan, 1996) are inherently better at handling such biases than are pure TD methods such as QL.
Reference: <author> Mahadevan, S., & Connell, J. </author> <year> (1991). </year> <title> Automatic programming of behaviour-based robots using reinforcement learning. </title> <booktitle> In Proceedings of the Ninth AAAI, </booktitle> <pages> pp. 768-773. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: RL also has the promise of having the flexibility able to adapt to possibly unforeseen changes in the robot and environment, which is an issue for autonomous robot design <ref> (Mahadevan & Connell, 1991) </ref>. Applying RL techniques to robot learning poses some particular challenges, however. Some of the key issues, (e.g. dealing with the "curse" of dimensionality) apply across the board to many non-robotic domains, and are addressed in the wider RL literature. <p> In the spirit of Brooks' subsumption architecture (Brooks, 1991), the robot has already been provided with a set of hard-wired low-level "step-reflexes". The natural fit between RL and behaviour-based robots has already been successfully exploited by previous workers <ref> (Mahadevan & Connell, 1991) </ref>. At any point in time, the control input for each leg on the robot is in one of two-states, triggered (on) or untriggered (off).
Reference: <author> Michie, D., & Chambers, R. </author> <year> (1968). </year> <title> BOXES: An experiment in adaptive control.. </title> <editor> In E.Dale, & D.Michie (Eds.), </editor> <booktitle> Machine Intelligence 2, </booktitle> <pages> pp. 137-152. </pages> <publisher> Edinburgh: Edinburgh Univ. Press. </publisher>
Reference: <author> Moore, A., & Atkeson, C. </author> <year> (1993). </year> <title> Memory-based reinforcement learning: Converging with less data and less real time. </title> <editor> In Connell, J. H., & Mahade-van, S. (Eds.), </editor> <title> Robot learning. </title> <publisher> Kluwer. </publisher>
Reference: <author> Pendrith, M., & Ryan, M. </author> <year> (1996). </year> <title> Actual return reinforcement learning versus Temporal Differences: Some theoretical and experimental results. </title> <note> To Appear In: </note> <editor> L.Saitta (Ed.), </editor> <booktitle> Proc. of the Thirteenth Int. Conf. on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Virtually all theoretical analyses of these algorithms have been conducted in this context (Jaakkola et al., 1995). Actual return learners, such as Monte Carlo methods (Barto & Duff, 1994; Jaakkola et al., 1995) and the P-Trace reinforcement learning algorithm <ref> (Pendrith & Ryan, 1996) </ref> are inherently better at handling such biases than are pure TD methods such as QL. <p> <ref> (Pendrith & Ryan, 1996) </ref> are inherently better at handling such biases than are pure TD methods such as QL. Mathematical analysis suggests they can in general be expected learn more quickly than methods like 1-step Q-learning, and empirical results show less performance degradation in the presence of noise and disturbance (Pendrith & Ryan, 1996). 3 The algorithms Two algorithms were tested; Watkins' classic 1-step Q-learning (QL) algorithm, and a new algorithm called C-Trace that was developed for these experiments. The benchmark against which both algorithms were compared was a hand-crafted gait. <p> The benchmark against which both algorithms were compared was a hand-crafted gait. The motivation for the design of C-Trace was some earlier work reported in <ref> (Pendrith & Ryan, 1996) </ref> which suggested that actual return (or Monte Carlo) based learning algorithms fared better than TD (temporal difference) based learners like QL in coping with both noise and the non-Markovian effects of a "boxes"-style input representation for the benchmark pole-and-cart learning problem (Michie & Chambers, 1968; Barto et <p> A limitation of the "P-Trace" actual return learning algorithm described in <ref> (Pendrith & Ryan, 1996) </ref> which made it unsuitable for for the "learning to walk" problem is that it is a trial-based algorithm; credit-assignment is only performed when a trial concludes and the total reward or punishment for a whole trial is known. <p> We note the result is also consistent with the empirical results in <ref> (Pendrith & Ryan, 1996) </ref> regarding TD versus actual return learning in simulated noisy and non-Markovian environments. Overall, these results provide empirical evidence supporting the design of C-Trace as an RL algorithm suited to this sort of environment.
Reference: <author> Peng, J., & Williams, R. </author> <year> (1994). </year> <title> Incremental multi-step Q-learning. </title> <editor> In W.Cohen, & H.Hirsh (Eds.), </editor> <booktitle> Proc. of Eleventh Int. Conf. on Machine Learning. </booktitle> <address> New Brunswick, New Jersey: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: This is accomplished by delaying the application of the truncation correction to the return for as long as possible, only applying it at the point when a non-policy action is selected. The property of experimentation insensitivity <ref> (Peng & Williams, 1994) </ref> means that, like 1-step Q-learning, it can learn the optimal policy even if, for the sake of active exploration, it does not always following the optimal policy. <p> This is discussed further in section 4. 4 Related work It is instructive to compare the P-Trace and C-Trace algorithms to both multi-step TD methods (e.g. Q ()- learning <ref> (Peng & Williams, 1994) </ref>), and to Monte Carlo methods (see, for example, (Barto & Duff, 1994)). The P-Trace mechanism is conceptually related to a Monte Carlo method, and indeed can be viewed as a Monte Carlo method specialised for sampling Q (s; a) values, where represents the current policy. <p> state will not cancel each other out properly to provide an accurate return with respect to the greedy policy if a non-policy action is chosen somewhere along the way; Q ()-learning is experimentation sensitive for = 1 (and, in general, for all &gt; 0) if active exploration is to occur <ref> (Peng & Williams, 1994) </ref>. As mentioned above, the P-Trace mechanism for calculating actual returns is experimentation insensitive because the trace is "zeroed" after a non-policy action is selected. <p> C-Trace will additionally apply a CTR update just prior to the trace being reset, but this is done in such a way as to not affect experimentation insensitivity. Further, both P-Trace and C-Trace also avoid a difficulty with the Q ()-learning approach described in both <ref> (Peng & Williams, 1994) </ref> and in (Cichosz, 1995) to do with an inaccuracy proportional to the square of the learning factor fi in the calculated return. Additionally, both P-Trace and C-Trace enjoy better space and time complexity characteristics.
Reference: <author> Singh, S., Jaakkola, T., & Jordan, M. </author> <year> (1994). </year> <title> Learning without state-estimation in partially observable Markovian decision processes. </title> <editor> In W.Cohen, & H.Hirsh (Eds.), </editor> <booktitle> Proc. of Eleventh Int. Conf. on Machine Learning. </booktitle> <address> New Brunswick, New Jersey: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal difference. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R. </author> <year> (1996). </year> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <booktitle> To Appear In Advances in Neural Information Processing 8. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: The policy with added randomness was introduced to attempt to estimate the effect of active exploration on the nett payoff returned for the learnt policies. The exploration heuristic used was semi-uniform distributed exploration (Thrun, 1992) (also sometimes called epsilon-greedy exploration <ref> (Sutton, 1996) </ref>.) Semi-uniform random exploration was chosen primarily because of its extreme simplicity, but also because it has been found empirically to be quite an effective exploration strategy.
Reference: <author> Thrun, S. </author> <year> (1992). </year> <title> The role of exploration in learning control. In White, </title> <editor> D. A., & Sofge, D. A. (Eds.), </editor> <title> Handbook of Intelligent Control: Neural, Fuzzy and Adaptive Approaches. </title> <publisher> Van Nostrand Rein-hold, </publisher> <address> Florence, Kentucky 41022. </address>
Reference-contexts: The policy with added randomness was introduced to attempt to estimate the effect of active exploration on the nett payoff returned for the learnt policies. The exploration heuristic used was semi-uniform distributed exploration <ref> (Thrun, 1992) </ref> (also sometimes called epsilon-greedy exploration (Sutton, 1996).) Semi-uniform random exploration was chosen primarily because of its extreme simplicity, but also because it has been found empirically to be quite an effective exploration strategy.
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> Ph.D. Thesis, </type> <institution> King's College, </institution> <address> Cambridge. </address>
Reference-contexts: C-Trace addresses this problem by using corrected truncated returns (CTRs) <ref> (Watkins, 1989) </ref> in addition to actual returns for credit-assignment; it can then learn in both continuous and trial-based situations. <p> C-Trace performs updates more frequently than P-Trace. Specifically, in addition to updating when a terminal state is reached according to the P-Trace rule, it will perform an update using a corrected truncated return (CTR) <ref> (Watkins, 1989) </ref> whenever a non-policy action has been selected for execution. <p> Watkins points out that the reason CTRs are useful estimators is that the expected value of the CTR tends to be closer to the expected value of the actual return than to U . This is sometimes called the error-reduction property of CTRs <ref> (Watkins, 1989) </ref>. Although not discussed explicitly in Watkins' thesis, it follows naturally from his treatment in Chapter 7 that as the CTR horizon n is increased, the error-reduction property becomes more vigourous.
References-found: 15

