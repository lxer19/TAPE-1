URL: http://www.ee.technion.ac.il/publications/Neural-Networks/colt96.ps
Refering-URL: http://www.ultimode.com/stevew/moe/refs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Towards Robust Model Selection using Estimation and Approximation Error Bounds  
Author: Joel Ratsaby Ronny Meir Vitaly Maiorov 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. A. Adams, </author> <title> Sobolev Spaces, </title> <publisher> Academic Press, </publisher> <year> 1975. </year>
Reference-contexts: This is a standard family of function studied extensively in the function approximation literature. This class may also be defined for r 2 IR (cf. <ref> [1] </ref>) a fact which will be utilized in the sequel. <p> 2 rkNk 1 ; 8N 2 ZZ s + (9) where f N g N2 ZZ s + represents a partition of the Cheby-chev spectrum by mutually exclusive sets, T k (x) = cos (k arccos (x)) are the Chebychev polynomials for k 2 ZZ s + , x 2 <ref> [1; 1] </ref> s , and we define by cos (k arccos (x)) = Q s i=1 cos (k i arccos (x i )), for k i 2 ZZ + and x i 2 IR. <p> 2 log 2 (n 2 s)) r=s 1 e c 1 n 4 s 2 where is a uniform distribution over B r , P is any measure with a probability density function p (x) satis fying 0 &lt; p (x) Q s p i &lt; 1, over x 2 <ref> [1; 1] </ref> s , C l &gt; 0 is a constant which depends on the measure P , c 0 &gt; 1 and c 1 are absolute constants. Proof A sketch of the proof is given in the Appendix. <p> Proof A sketch of the proof is given in the Appendix. The condition on the density function p (x) is rather weak as it is satisfied for any probability density p (x) such that p (x) &gt; 0 over <ref> [1; 1] </ref> s and such that the lim jx i j!1 p (x) i=1 1 x 2 4.2 Estimation error The derivation of the estimation error relies on results from the theory of uniform convergence of empirical processes (see for example [16][22]). <p> Theorem 12 Let m examples be drawn according to the distribution P (x; y) on (x; y) 2 <ref> [1; 1] </ref> s fi [M; M ], and where the marginal distribution P (x) satisfies the condition of Theorem 10. Let confidence parameters ffi and be given.
Reference: [2] <author> A.N. Arkhangel'skii, </author> <title> "Lower bounds for probabilities of large deviations for sums of independent random variables", </title> <journal> Theory Prob. Appl., </journal> <volume> vol. 34, no. 4, </volume> <year> 1989. </year>
Reference-contexts: While the calculation of the upper bounds is possible using the theory of uniform convergence, it turns out that computing lower bounds is more difficult, although some results are available <ref> [2] </ref>.
Reference: [3] <author> A.R. Barron, </author> <title> "Approximation and Estimation Bounds for Artificial Neural Networks", </title> <journal> Machine Learning, </journal> <volume> vol. 14, </volume> <pages> pp. 115-133, </pages> <year> 1994. </year>
Reference-contexts: Other methods such as Vap-nik's structural risk minimization (SRM) and several of its extensions [11, 12], and Barron's complexity regularization method <ref> [3] </ref>, minimize an upper bound on the loss to obtain the selected model.
Reference: [4] <author> J.M. Bernardo and A.F.M. Smith, </author> <title> Bayesian Theory, </title> <publisher> John Wiley, </publisher> <year> 1994. </year>
Reference-contexts: Asymptotic expansions: Methods based on asymptotic expansions are usually concerned with parametric situations where exact forms for the asymptotic behavior of the loss are available. For a comprehensive survey see [25]. Bayesian approaches: Within this paradigm models are selected based on the so called computation of Bayes factors <ref> [4] </ref>. We note in passing, however, that in a strictly Bayesian framework, there is really no problem of model selection, as all models are combined in a straightforward way in accordance with the Bayesian paradigm [5].
Reference: [5] <author> S. Geisser, </author> <title> Predictive Inference: An Introduction, </title> <publisher> Chapman & Hall, </publisher> <year> 1993. </year>
Reference-contexts: We note in passing, however, that in a strictly Bayesian framework, there is really no problem of model selection, as all models are combined in a straightforward way in accordance with the Bayesian paradigm <ref> [5] </ref>. Resampling methods: Finally, a whole range of numerical procedures for assessing the loss of a hypothesis are based on the so called resampling methods, where the data is used both for training as well as for testing, in order to determine the true behavior.
Reference: [6] <author> D. Haussler, </author> <title> "Decision theoretic generalizations of the PAC model for neural net and other learning applications", </title> <journal> Inform. Comput., </journal> <volume> vol. 100 no. 1, </volume> <pages> pp. 78-150, </pages> <year> 1992. </year>
Reference-contexts: Finally, Section 5 is devoted to a discussion of the advantages and disadvantages of the proposed paradigm, as compared with more standard approaches to model selection. 2 Definitions and Basic Concepts We consider the problem of learning from examples in the context of statistical decision theory (see for example <ref> [6] </ref>). Let X , Y and A be sets, called the instance, outcome and decision spaces, respectively. <p> The term model will denote a particular type of a hypothesis space (e.g., neural networks, radial basis functions, polynomial splines etc.). We limit ourselves to the problem of regression; the application of the framework to different paradigms (such as classification and density estimation) is discussed in <ref> [6] </ref>. In the regression context we assume that the outcome Y is generated from the input X according to the conditional probability distribution P (Y jX). <p> We make use of a distribution-free uniform strong law of large numbers appearing as Corollary 2 in <ref> [6] </ref> which upper bounds the deviation between the empirical mean and true mean of any function g 2 G. In our case the functions of interest are the quadratic error functions l (y; h (x)) = (y h (x)) 2 . <p> L 1 (P ) 2 2M kh gk L 1 (P ) : It follows that the L 1 (P )-covering numbers of the class H n and G n are related as N (*; G n ; L 1 (P )) N ( * 2 From Theorem 6 in <ref> [6] </ref> we have as an upper bound on the L 1 (P ) covering number of H n N (*; H n ; L 1 (P )) * 2eM d p (H n ) * ; where d p (H n ) denotes the pseudo-dimension of the class H n . <p> Combining the above with Corollary 2 in <ref> [6] </ref> we have P sup jL (h) L m (h)j &gt; * 8 256eM 3 2d p (H n ) : (12) From (12) and some algebra we obtain as an upper bound *(m; ffi) = 1024M 4 8 + 2d p (H n ) ln (256eM 3 ) + m
Reference: [7] <author> D. Haussler, M. Kearns, H.S. Seung and N. Tishby, </author> <title> "Rigorous Learning Curve Bounds from Statistical Mechanics", </title> <booktitle> in Proceedings of the seventh workshop on Computational Learning Theory", </booktitle> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: While the upper bound becomes tight as the sample size increases, it is not all clear that the upper bound is indicative of the true behavior of the system for finite sample sizes <ref> [7] </ref>, which is really the only region of interest for model selection. Moreover, any upper bound derived is based on a specific bounding technique. It is possible that other bounding methods may lead to different model selection choices.
Reference: [8] <author> M. Karpinski and A. Macintyre, </author> <title> "Polynomial bounds for VC dimension of sigmoidal neural networks", </title> <booktitle> in Proc. 27th ACM Symp. Theory Com-put., </booktitle> <pages> pp. 200-208, </pages> <year> 1995. </year>
Reference-contexts: fixed degree may serve as a universal function approximator [10], in this work we focus on the standard sigmoid, (x) = (1 + e x ) 1 , used extensively in the neural network literature, since finite V C-dimension results for H n using this function have recently been derived <ref> [8] </ref>. <p> Determining upper bounds on pseudo-dimensions of nonlinear manifolds is non trivial as was recently shown in <ref> [8] </ref> for the neural-net type of manifolds. <p> In order to proceed we need to upper bound d p (H n ). We obtain this from the following result by Karpinski and Macintyre <ref> [8] </ref>. Lemma 11 The pseudo dimension of a feedforward neural network with the standard sigmoidal activation function, n computational nodes and l parameters is upper bounded by c (nl) 2 , for n; l 1 and some absolute constant c &gt; 0.
Reference: [9] <author> M. Kearns, Y. Mansour, A. Y. Ng and D. Ron, </author> <title> "An Experimental and Theoretical Comparison of Model Selection Methods", </title> <booktitle> in Proceedings of the seventh workshop on Computational Learning Theory", </booktitle> <publisher> ACM Press, </publisher>
Reference-contexts: We note, however, that in spite of these difficulties, the resampling based methods are probably the most widely used approached to loss estimation and model selection. Moreover, there has been some recent theoretical work <ref> [9] </ref> indicating their superiority over methods based on distribution free upper bounds. At this point it should be clear to the reader that all the classical approaches to model selection described above suffer from some malady.
Reference: [10] <author> Leshno, M., Lin, V., Pinkus, A. and Schocken, S. </author> <title> "Multilayer Feedforward Networks with a Nonpoly-nomial Activation Function Can Approximate any Function", </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 6, </volume> <pages> 861-867, </pages> <year> 1993. </year>
Reference-contexts: While these systems have been shown to be universal function approxi-mators <ref> [10] </ref> when the number of hidden units is allowed to increase without bound, we will mainly be concerned with the more realistic issue of selecting between networks composed of a finite number of computational nodes. <p> While any function () which is not a polynomial of fixed degree may serve as a universal function approximator <ref> [10] </ref>, in this work we focus on the standard sigmoid, (x) = (1 + e x ) 1 , used extensively in the neural network literature, since finite V C-dimension results for H n using this function have recently been derived [8].
Reference: [11] <author> G. Lugosi and K. Zeger, </author> <title> "Concept learning using complexity regularization", </title> <journal> IEEE Trans. Inf. Theory, </journal> <volume> vol. 42, no. 1, </volume> <pages> pp. 48-54, </pages> <year> 1996. </year>
Reference-contexts: Some select the model that minimizes a quality measure which is unrelated to the loss, e.g., in MDL [18] the description length of the model plus the data is minimized. Other methods such as Vap-nik's structural risk minimization (SRM) and several of its extensions <ref> [11, 12] </ref>, and Barron's complexity regularization method [3], minimize an upper bound on the loss to obtain the selected model.
Reference: [12] <author> G. </author> <title> Lugosi and A.B. Nobel "Adaptive model selection using empirical complexities" preprint, </title> <year> 1996. </year>
Reference-contexts: Some select the model that minimizes a quality measure which is unrelated to the loss, e.g., in MDL [18] the description length of the model plus the data is minimized. Other methods such as Vap-nik's structural risk minimization (SRM) and several of its extensions <ref> [11, 12] </ref>, and Barron's complexity regularization method [3], minimize an upper bound on the loss to obtain the selected model.
Reference: [13] <author> V. Maiorov, </author> <title> "Linear widths of function spaces equipped with the Gaussian measure", </title> <journal> J. of Ap-prox. Theory, </journal> <volume> vol. 77, no. 1, </volume> <pages> pp. 74-88, </pages> <year> 1994. </year>
Reference-contexts: Lower bound results using measures over function spaces have only recently become available in the function approximation literature (see for example <ref> [13] </ref>). We note, however, that our method of proof makes a novel use of the VC-dimension, a concept arising in a totally different field, thus demonstrating its usefulness beyond the theory of uniform convergence of empirical measures.
Reference: [14] <author> H.N. Mhaskar, </author> <title> "Neural Networks for Optimal Approximation of Smooth and Analytic Functions", </title> <note> to appear in Neural Computation, </note> <year> 1995. </year>
Reference-contexts: Concentrating first on the upper bound, we make use of the following result of Mhaskar <ref> [14] </ref> which holds also for more general activation functions than the standard sigmoid. Lemma 9 (Approximation error upper bound) Given the definition of the neural-network hypothesis class in (8), and the target class in (10).
Reference: [15] <author> S. M. Nikolski, </author> <title> "Approximation of the Many Variables Functions and Theorems of Embedding", </title> <publisher> Nauka, </publisher> <address> Moscow, </address> <year> 1969. </year>
Reference-contexts: While the definition of the Besov class may seem obscure, it can be shown that it is closely related to the Sobolev class through the following embedding lemma, the proof of which is based on results in <ref> [21, 15] </ref>.
Reference: [16] <author> Pollard D., </author> <title> Convergence of Stochastic Processes, </title> <booktitle> Springer Series in Statistics, </booktitle> <year> (1984). </year>
Reference: [17] <author> J. Ratsaby, R. Meir, </author> <title> "Finite Sample Size Results for Robust Model Selection; Application to Neural Networks", </title> <month> October </month> <year> 1995, </year> <type> Technical Report, </type> <institution> Dept. E.E., Technion, </institution> <note> Publication # CC-117. </note>
Reference-contexts: Moreover, many of the standard methods are only guaranteed to work well asymptotically, leaving their behavior in the face of a finite amount of data completely unknown. In this paper we extend on previous work <ref> [17] </ref> and introduce a novel model selection criterion, based on combining two recent chains of thought.
Reference: [18] <author> Rissanen J., </author> <title> Stochastic Complexity in Statistical Inquiry, </title> <booktitle> (1989) Series in Computer science-Vol. </booktitle> <pages> 15. </pages>
Reference-contexts: There are many such methods each prescribing a different rule of selection and yielding a candidate whose loss is no more than some upper bound. Some select the model that minimizes a quality measure which is unrelated to the loss, e.g., in MDL <ref> [18] </ref> the description length of the model plus the data is minimized. Other methods such as Vap-nik's structural risk minimization (SRM) and several of its extensions [11, 12], and Barron's complexity regularization method [3], minimize an upper bound on the loss to obtain the selected model.
Reference: [19] <author> A. N. Shiryayev, </author> <title> Probability, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin 1984. </address>
Reference: [20] <author> M. Stone, </author> <title> "Cross-validation choice and assessment of statistical predictionss", </title> <journal> J. Royal. Statis. Soc, </journal> <volume> vol. B36, </volume> <pages> pp. 111-147, </pages> <year> 1974. </year>
Reference-contexts: A well known method operating along these lines is cross-validation <ref> [20] </ref>. Having described four major approaches to model selection based on loss estimation, we mention briefly some of the drawbacks of these approaches so as to motivate our proposal below. Consider first the methods based on distribution free upper bounds.
Reference: [21] <author> H. Triebel, </author> <title> Theory of Function Spaces, </title> <publisher> Birkhauser, </publisher> <address> Basel 1983. </address>
Reference-contexts: While the definition of the Besov class may seem obscure, it can be shown that it is closely related to the Sobolev class through the following embedding lemma, the proof of which is based on results in <ref> [21, 15] </ref>.
Reference: [22] <author> V.N. Vapnik, </author> <title> Estimation of Dependences Based on Empirical Data, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin 1982. </address>
Reference-contexts: Model selection criteria based on this approach usually select a hypothesis with the least upper bound among all hypotheses tested. A well known method falling in this class is Vapnik's structural risk (loss) minimization <ref> [22] </ref>. Asymptotic expansions: Methods based on asymptotic expansions are usually concerned with parametric situations where exact forms for the asymptotic behavior of the loss are available. For a comprehensive survey see [25]. Bayesian approaches: Within this paradigm models are selected based on the so called computation of Bayes factors [4]. <p> The Bayes approach suffers from similar problems augmented by a heavy computational burden. The performance of the resampling based methods is very difficult to assess for finite sample sizes. Even the case of a linear model, poses rather difficult computational issues (see for example <ref> [22] </ref>). We note, however, that in spite of these difficulties, the resampling based methods are probably the most widely used approached to loss estimation and model selection. Moreover, there has been some recent theoretical work [9] indicating their superiority over methods based on distribution free upper bounds.
Reference: [23] <author> V.N. Vapnik and A.Y. Chernovenkins, </author> <title> "On the uniform convergence of relative frequencies of events to their probabilities", </title> <journal> Theory of Prob. and Applic., </journal> <volume> vol. 16 no. 2, </volume> <pages> pp. 264-280, </pages> <year> 1971. </year>
Reference-contexts: In this paper we extend on previous work [17] and introduce a novel model selection criterion, based on combining two recent chains of thought. In particular we make use of the powerful framework of uniform convergence of empirical processes pioneered by Vapnik and Chernovenkins <ref> [23] </ref>, combined with recent results concerning the approximation ability of non-linear manifolds of functions, focusing in particular on feedfor-ward neural networks. <p> The basic point is roughly that for a larger hypothesis space, there are many more hypotheses to choose from, giving rise to a large variance in the estimation error. This notion can be quantified exactly using the uniform convergence framework pioneered by Vapnik and Chernovenkins <ref> [23] </ref> and will discussed in detail in Section 4. Let us now introduce the main question which this paper attempts to answer.
Reference: [24] <author> H.E. Warren, </author> <title> "Lower bounds for approximation by non-linear manifolds", </title> <journal> Trans. of the AMS, </journal> <volume> vol. 133, </volume> <pages> pp. 167-178, </pages> <year> 1968. </year>
Reference: [25] <author> H. White, </author> <title> Estimation, Inference and Specification Analysis, </title> <publisher> Cambridge university press, </publisher> <year> 1994. </year>
Reference-contexts: A well known method falling in this class is Vapnik's structural risk (loss) minimization [22]. Asymptotic expansions: Methods based on asymptotic expansions are usually concerned with parametric situations where exact forms for the asymptotic behavior of the loss are available. For a comprehensive survey see <ref> [25] </ref>. Bayesian approaches: Within this paradigm models are selected based on the so called computation of Bayes factors [4].
Reference: [26] <author> A. Zygmund, </author> <title> Trigonometric Series, </title> <publisher> Cambridge university press, </publisher> <year> 1968. </year>
References-found: 26

