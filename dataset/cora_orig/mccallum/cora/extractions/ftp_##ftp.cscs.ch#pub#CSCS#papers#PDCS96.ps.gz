URL: ftp://ftp.cscs.ch/pub/CSCS/papers/PDCS96.ps.gz
Refering-URL: 
Root-URL: 
Title: HPF and MPI Implementation of the NAS Parallel Benchmarks Supported by Integrated Program Engineering Tools  
Author: Christian Cl emencon Karsten M. Decker Vaibhav R. Deshpande Akiyoshi Endo Josef Fritscher Paulo A. R. Lorenzo Norio Masuda Andreas M uller Roland R uhl William Sawyer Brian J. N. Wylie Frank Zimmermann 
Keyword: HPF MPI parallelization; parallel program engineering tools.  
Web: http://www.cscs.ch/Official/Project CSCS-NEC.html  
Address: CH-6928 Manno, Switzerland  
Affiliation: Centro Svizzero di Calcolo Scientifico (CSCS/SCSC) and NEC European Supercomputer Systems, Swiss Branch  
Note: In Proc. 8th IASTED Int'l Conf. on Parallel and Distributed Computing and Systems (Chicago, IL, USA), c IASTED/ACTA Press (Anaheim/Calgary/Z urich), pp. 144-148, Oct. 1996. [ISBN: 0-88986-213-3]  
Abstract: High Performance Fortran (HPF) compilers and communication libraries with the standardized Message Passing Interface (MPI) are becoming widely available, easing the development of portable parallel applications on distributed-memory parallel processor systems. The recently developed Annai tool environment supports programming, debugging and tuning of both HPF- and MPI-based applications. Considering code development and subsequent maintenance time to be as important as ultimate performance, we address how sequential Fortran-77 versions of the familiar NAS Parallel Benchmark kernels can be expediently parallelized with appropriate tool support. While automatic parallelization of scientific applications written in traditional sequential languages remains largely impractical, Annai provides users with high-level language extensions and integrated program engineering support tools. In this paper, Annai support is demonstrated primarily focusing on the MG (multigrid) kernel, with complementary examples selected from the other four kernels. Respectable performance and good scalability in most cases are obtained with this straightforward par-allelization strategy, even without recourse to platform-specific optimizations or major program transformations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran language specification: Version 1.0, </title> <journal> Scientific Programming, </journal> <volume> vol. 2, no. 1&2, </volume> <year> 1993. </year>
Reference-contexts: 1 Introduction Expedient exploitation of distributed computer systems, in a manner which ensures application developer's investments, requires appropriate consideration of the parallelization effort and desired portability. Standard specifications have recently emerged which address these considerations for the two most common parallel programming paradigms: `data-parallel' programming with High Performance Fortran (HPF) <ref> [1] </ref> and explicit use of Message Passing Interface (MPI) [2] communication primitives. While MPI implementations and HPF compilers are currently available, and continually maturing, tools supporting portable parallel programming have not yet adequately addressed the development issues of redesigning algorithms, optimizing data distributions, using optimized libraries, and re-engineering critical sections.
Reference: [2] <author> Message Passing Interface Forum, </author> <title> MPI: A message-passing interface standard, </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> vol. 8, no. 3&4, </volume> <year> 1994. </year>
Reference-contexts: Standard specifications have recently emerged which address these considerations for the two most common parallel programming paradigms: `data-parallel' programming with High Performance Fortran (HPF) [1] and explicit use of Message Passing Interface (MPI) <ref> [2] </ref> communication primitives. While MPI implementations and HPF compilers are currently available, and continually maturing, tools supporting portable parallel programming have not yet adequately addressed the development issues of redesigning algorithms, optimizing data distributions, using optimized libraries, and re-engineering critical sections.
Reference: [3] <author> C. Clemencon, A. Endo, J. Fritscher, A. Muller, R. Ruhl, and B. J. N. Wylie, Annai: </author> <title> An integrated parallel programming environment for multicomputers, in Tools and Environments for Parallel and Distributed Systems (A. </title> <editor> Zaky and T. Lewis, eds.), ch. </editor> <volume> 2, </volume> <pages> pp. 33-59, </pages> <publisher> Kluwer Academic Publishers, </publisher> <month> Feb. </month> <year> 1996. </year> <note> ISBN: 0-7923-9675-8. </note>
Reference-contexts: tool environment, Annai, which supports the engineering of large-scale parallel scientific/engineering applications, demonstrated through its use in the implementation of HPF and MPI versions of the NAS Parallel Benchmark kernels. 2 Data-parallel and message-passing program engineering tool support The Annai environment and component tools for HPF and MPI application engineering <ref> [3] </ref> support the parallelization of sequential applications as a step-wise refinement from straightforward data-parallel to hand-tuned asynchronous message-passing programs. This can be summarized as follows: starting with a sequential Fortran version of the code, a `nave' parallel version is written in HPF.
Reference: [4] <author> D. H. Bailey et al., </author> <title> NAS parallel benchmarks, </title> <type> Tech. Rep. </type> <institution> RNR-94-007, NASA Ames Research Center, Mof-fett Field, </institution> <address> CA 94035-1000, USA, </address> <month> Mar. </month> <year> 1994. </year> <note> Reference codes and regularly updated results available from http://www.nas.nasa.gov/NAS/NPB/. </note>
Reference-contexts: codes, as well as applications written entirely using MPI, also benefit from deterministic program replay and deadlock detection facilities developed to support explicit message-passing programming. 3 NPB parallelization The benchmarks developed for the Numerical Aerodynamic Simulation (NAS) Program are especially designed to evaluate and compare the performance of parallel computers <ref> [4] </ref>. A number of implementations based on various message-passing libraries have been presented, though data-parallel versions have proven more difficult and are only starting to appear [5]. <p> Differences in the underlying communication can be seen from execution traces in Fig. 3b 1&2 . 147 Cenju-3 relative to Cray reference (fl) <ref> [4] </ref>. <p> In Fig. 4, performance results obtained on a NEC Cenju-3 are presented in the tradition of NPB as the ratio of the parallel execution time to the current best execution times of the sequential Fortran77 code on one processor of the Cray C90 <ref> [4] </ref>. This evaluation allows a comparison of the largest problem classes with an optimized sequential version, since these generally do not fit on a single PE. Furthermore, we want to consider the scalability as well as communication overhead.
Reference: [5] <author> L. F. Meadows, D. Miles, and M. Young, </author> <title> Performance results of several High Performance Fortran benchmarks, </title> <booktitle> in Proc. 9th Int'l Parallel Processing Symp. </booktitle> <address> (Santa Barbara, USA), </address> <pages> pp. 516-517, </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: A number of implementations based on various message-passing libraries have been presented, though data-parallel versions have proven more difficult and are only starting to appear <ref> [5] </ref>.
Reference: [6] <author> C. Clemencon, K. M. Decker, V. R. Deshpande, A. Endo, J. Fritscher, P. A. R. Lorenzo, N. Masuda, A. Muller, R. Ruhl, W. Sawyer, B. J. N. Wylie, and F. Zimmermann, </author> <title> Tools-supported HPF and MPI parallelization of the NAS parallel benchmarks, </title> <type> Technical Report CSCS-TR-96-02, </type> <institution> Centro Svizzero di Calcolo Scientifico, CH-6928 Manno, Switzer-land, </institution> <month> Mar. </month> <year> 1996. </year> <month> 148 </month>
Reference-contexts: The performance of the MPI and HPF/PST versions of the NAS-IS kernel is comparable: scalability is reasonable for smaller number of PEs, but poor for larger numbers, reflecting the decreasing ratio of computation to communication. Full details of the NPB parallelizations are available in a technical report <ref> [6] </ref>, which also contains additional references, e.g., for further information about Annai tool components. 4 Conclusions The NAS Parallel Benchmark kernels EP, MG, CG, FT, and IS have been parallelized using HPF and MPI, exploiting functionality of the Annai integrated tool environment during their development.
References-found: 6

