URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/recur-learn.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Email: sontag@control.rutgers.edu  
Title: Recurrent Neural Networks learning problem is considered, for continuous-time recurrent neural networks having sigmoidal activation
Author: Eduardo Sontag 
Address: New Brunswick, NJ 08903  
Affiliation: Dept. of Mathematics, Rutgers University  
Note: Continuous-Time  The following  
Abstract: A Learning Result for Abstract 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> Albertini, F., and E.D. Sontag, </editor> <title> "For neural networks, function determines form," </title> <booktitle> Neural Networks 6(1993): </booktitle> <pages> 975-990. </pages>
Reference-contexts: the class of systems represented by (1) lies in the fact that they can, in a certain sense, approximate arbitrary nonlinear systems, they provide universal models of digital as well as analog computation, and their system-theoretic properties (controllability, observability, minimality, parameter identifiability, etc) can be quite elegantly characterized (see e.g. <ref> [1, 2, 19, 18] </ref>). The Learning Problem We formulate the problem of identification from input/output data in the paradigm of uniform convergence of empirical probabilities due to Vapnik and Chervonenkis and, independently in the computer science context ("probably approximately correct" (PAC) learning) to Valiant. <p> interpret as an input/output pair associated to the plant in Figure 2) and the output that the net , initialized at the state ~, produces in response to the input whose first k1 derivatives are specified by the vector . (The error is normalized, for technical reasons, to the range <ref> [0; 1] </ref>.) Two quantities are of special interest, still for any fixed initialized net (; ~). <p> Let (Z; A) be a set together with a -algebra of subsets of Z, and let ' : R l fi Z ! <ref> [0; 1] </ref>, where l is some positive integer, be a function measurable with respect to B fi A, where B is the Borel -algebra on R l . Write F := f'(; ); 2 R l g. Assume that d = pd (F ) &lt; 1.
Reference: [2] <editor> Albertini, F., and E.D. Sontag, </editor> <title> "State observability in recurrent neural networks," </title> <journal> Systems & Control Letters 22(1994): </journal> <pages> 235-244. </pages>
Reference-contexts: the class of systems represented by (1) lies in the fact that they can, in a certain sense, approximate arbitrary nonlinear systems, they provide universal models of digital as well as analog computation, and their system-theoretic properties (controllability, observability, minimality, parameter identifiability, etc) can be quite elegantly characterized (see e.g. <ref> [1, 2, 19, 18] </ref>). The Learning Problem We formulate the problem of identification from input/output data in the paradigm of uniform convergence of empirical probabilities due to Vapnik and Chervonenkis and, independently in the computer science context ("probably approximately correct" (PAC) learning) to Valiant.
Reference: [3] <author> Back, A.D., and A.C. Tsoi, </author> <title> "FIR and IIR synapses, a new neural network architecture for time-series modeling," </title> <booktitle> Neural Computation 3 (1991): </booktitle> <pages> 375-385. </pages>
Reference-contexts: of the form _x = F x+~(Ax+Bu) | have been proposed as models fl Supported in part by US Air Force Grant AFOSR-94-0293 1 u - B - h + - ~ - - C - y r 6 for control, computation, and signal processing by many researchers, see e.g. <ref> [12, 14, 13, 15, 3, 16, 4, 21, 22] </ref>, as well as in optimization and associative memory design ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
Reference: [4] <author> Bengio, Y., </author> <title> Neural Networks for Speech and Sequence Recognition, </title> <publisher> Thompson Computer Press, </publisher> <address> Boston, </address> <year> 1996. </year>
Reference-contexts: of the form _x = F x+~(Ax+Bu) | have been proposed as models fl Supported in part by US Air Force Grant AFOSR-94-0293 1 u - B - h + - ~ - - C - y r 6 for control, computation, and signal processing by many researchers, see e.g. <ref> [12, 14, 13, 15, 3, 16, 4, 21, 22] </ref>, as well as in optimization and associative memory design ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
Reference: [5] <author> Dasgupta, B., and E.D. Sontag, </author> <title> "Sample complexity for learning recurrent perceptron mappings," </title> <journal> IEEE Trans. Inform. Theory 42 (1996): </journal> <pages> 1479-1487. </pages>
Reference-contexts: Examples leading to infinite VC dimension are well-known (see e.g. [20], Section 10.3.2). Discrete Time We have avoided discussion of discrete-time networks. For discrete-time systems there exist lower bounds on pseudodimension; see <ref> [5] </ref> for the case of linear systems and [10] for discrete time nonlinear nets. Interestingly enough, for = tanh, the lower bounds are at least linear (rather than logarithmic) in k, for discrete-time systems.
Reference: [6] <author> Goldberg, P., and M. Jerrum, </author> <title> "Bounding the Vapnik-Chervonenkis dimension of concept classes parametrized by real numbers," </title> <booktitle> Machine Learning 18(1995): </booktitle> <pages> 131-148. </pages>
Reference-contexts: All these follow from pseudodimension estimates. For example, for the class of minimal linear systems of dimension n, we obtain pd (F ) = O (n log k) : This follows from the estimates for VC dimension of algebraic concept classes given in <ref> [6] </ref> (see [20], Theorem 10.5 or Corollary 10.2), since in the proof of Lemma 2.5 has degree 2k + 5 and one may take l = 2n (because, by linear systems theory, reachable and observable systems can be parameterized linearly using that many parameters).
Reference: [7] <author> Haussler, D., </author> <title> "Decision theoretic generalizations of the PAC model for neural nets and other learning applications", </title> <booktitle> Information and Computation 100(1992): </booktitle> <pages> 78-150. 10 </pages>
Reference-contexts: The pseudo-dimension of F is defined by: pd (F ) := vc (F 0 ) : (This definition is equivalent to the one in <ref> [7] </ref>.) We now review a basic pseudodimension estimate from [9], specialized to a form useful for our purposes. Let = tanh. <p> When r = 3, l 3n + n 2 , n 5, and q 6k, pd (F ) 3n 6 + 5n 3 log 2 k. 2 2.3 Uniform Approximation of Expected Loss The last ingredient is a theorem on approximation of means by empirical means, which we quote from <ref> [7] </ref> (see also [20] for an exposition). <p> Write F := f'(; ); 2 R l g. Assume that d = pd (F ) &lt; 1. Then, applying Corollary 2 in Section 4 of <ref> [7] </ref>: Proposition 2.7 Let ~z be generated by s independent draws according to any distribution P on (Z; A). Pick any "; ffi 2 (0; 1).
Reference: [8] <author> Hautus, M., </author> <title> "A set of IP-functions," </title> <type> unpublished manuscript, </type> <institution> Eindhoven University, </institution> <month> Au--gust </month> <year> 1993. </year>
Reference: [9] <author> Karpinski, M., and A. Macintyre, </author> <title> "Polynomial bounds for VC dimension of sigmoidal and general Pfaffian neural networks," J. Computer Sys. Sci., to appear. (Summary in "Polynomial bounds for VC dimension of sigmoidal neural networks," </title> <booktitle> in Proc. 27th ACM Symposium on Theory of Computing, 1995 , pp. </booktitle> <pages> 200-208.) </pages>
Reference-contexts: The pseudo-dimension of F is defined by: pd (F ) := vc (F 0 ) : (This definition is equivalent to the one in [7].) We now review a basic pseudodimension estimate from <ref> [9] </ref>, specialized to a form useful for our purposes. Let = tanh. <p> Precisely, we apply Theorem 10.7 with "b" instead of "2 log 2 B" and Lemma 10.6 with q; d; D respectively equal to our n; q; r, to obtain respectively equations (7) and (8). (The results cited from the book [20] are based on the paper <ref> [9] </ref>; the only minor difference is in the slight improvement in (7), which the original paper had given as b + 16l.) Some algebraic manipulation gives: Corollary 2.6 When r = 3, l 3n + n 2 , n 5, and q 6k, pd (F ) 3n 6 + 5n 3
Reference: [10] <author> Koiran, P., and E.D. Sontag, </author> <title> "Vapnik-Chervonenkis dimension of recurrent neural networks," </title> <booktitle> Proceedings of Third European Conference on Computational Learning Theory, </booktitle> <address> Jerusalem, </address> <month> March </month> <year> 1997, </year> <note> to appear. </note>
Reference-contexts: Examples leading to infinite VC dimension are well-known (see e.g. [20], Section 10.3.2). Discrete Time We have avoided discussion of discrete-time networks. For discrete-time systems there exist lower bounds on pseudodimension; see [5] for the case of linear systems and <ref> [10] </ref> for discrete time nonlinear nets. Interestingly enough, for = tanh, the lower bounds are at least linear (rather than logarithmic) in k, for discrete-time systems.
Reference: [11] <author> Leshno, M., V.Ya. Lin, A. Pinkus, and S. Schocken, </author> <title> "Multilayer feedforward networks with a non-polynomial activation function can approximate any function," </title> <booktitle> Neural Networks 6(1993): </booktitle> <pages> 861-867. </pages>
Reference: [12] <author> Matthews, M., </author> <title> "A state-space approach to adaptive nonlinear filtering using recurrent neural networks," </title> <booktitle> Proc. 1990 IASTED Symp. on Artificial Intelligence Applications and Neural Networks, </booktitle> <address> Zurich, </address> <month> July </month> <year> 1990, </year> <pages> pp. 197-200. </pages>
Reference-contexts: of the form _x = F x+~(Ax+Bu) | have been proposed as models fl Supported in part by US Air Force Grant AFOSR-94-0293 1 u - B - h + - ~ - - C - y r 6 for control, computation, and signal processing by many researchers, see e.g. <ref> [12, 14, 13, 15, 3, 16, 4, 21, 22] </ref>, as well as in optimization and associative memory design ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
Reference: [13] <author> Polycarpou, </author> <title> M.M., and P.A. Ioannou, "Neural networks and on-line approximators for adaptive control," </title> <booktitle> in Proc. Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pp. 93-798, </pages> <institution> Yale University, </institution> <year> 1992. </year>
Reference-contexts: of the form _x = F x+~(Ax+Bu) | have been proposed as models fl Supported in part by US Air Force Grant AFOSR-94-0293 1 u - B - h + - ~ - - C - y r 6 for control, computation, and signal processing by many researchers, see e.g. <ref> [12, 14, 13, 15, 3, 16, 4, 21, 22] </ref>, as well as in optimization and associative memory design ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
Reference: [14] <author> Siegelmann, H.T., and E.D. Sontag, </author> <title> "Turing computability with neural nets," </title> <journal> Appl. Math. Lett. </journal> <volume> 4(6)(1991): </volume> <pages> 77-80. </pages>
Reference-contexts: of the form _x = F x+~(Ax+Bu) | have been proposed as models fl Supported in part by US Air Force Grant AFOSR-94-0293 1 u - B - h + - ~ - - C - y r 6 for control, computation, and signal processing by many researchers, see e.g. <ref> [12, 14, 13, 15, 3, 16, 4, 21, 22] </ref>, as well as in optimization and associative memory design ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
Reference: [15] <author> Sontag, E.D., </author> <title> "Neural nets as systems models and controllers," </title> <booktitle> in Proc. Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pp. 73-79, </pages> <institution> Yale University, </institution> <year> 1992. </year>
Reference-contexts: of the form _x = F x+~(Ax+Bu) | have been proposed as models fl Supported in part by US Air Force Grant AFOSR-94-0293 1 u - B - h + - ~ - - C - y r 6 for control, computation, and signal processing by many researchers, see e.g. <ref> [12, 14, 13, 15, 3, 16, 4, 21, 22] </ref>, as well as in optimization and associative memory design ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
Reference: [16] <author> Sontag, E.D., </author> <title> "Neural networks for control," in Essays on Control: Perspectives in the Theory and its Applications (H.L. </title> <editor> Trentelman and J.C. Willems, eds.), </editor> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1993, </year> <pages> pp. 339-380. </pages>
Reference-contexts: of the form _x = F x+~(Ax+Bu) | have been proposed as models fl Supported in part by US Air Force Grant AFOSR-94-0293 1 u - B - h + - ~ - - C - y r 6 for control, computation, and signal processing by many researchers, see e.g. <ref> [12, 14, 13, 15, 3, 16, 4, 21, 22] </ref>, as well as in optimization and associative memory design ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
Reference: [17] <author> Sontag, E.D., </author> <title> Mathematical Control Theory: Deterministic Finite Dimensional Systems, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: any initial state ~ 2 R n , there is a unique solution x : [0; T ] ! R n of _x = ~(Ax + Bu) with x (0) = ~; this solution will be denoted as x (t; ~; u). (By "input function" we mean, as usual, cf. <ref> [17] </ref>, a measurable essentially bounded function, but for the purposes of this paper, differentiable inputs would suffice.) We also consider the ensuing output function y (t; ~; u) := Cx (t; ~; u).
Reference: [18] <author> Sontag, E.D., </author> <title> "Recurrent neural networks: Some systems-theoretic aspects," in Dealing with Complexity: a Neural Network Approach (M. </title> <editor> Karny, K. Warwick, and V. Kurkova, eds.), </editor> <publisher> Springer-Verlag, </publisher> <address> London, </address> <year> 1997, </year> <note> to appear. </note>
Reference-contexts: the class of systems represented by (1) lies in the fact that they can, in a certain sense, approximate arbitrary nonlinear systems, they provide universal models of digital as well as analog computation, and their system-theoretic properties (controllability, observability, minimality, parameter identifiability, etc) can be quite elegantly characterized (see e.g. <ref> [1, 2, 19, 18] </ref>). The Learning Problem We formulate the problem of identification from input/output data in the paradigm of uniform convergence of empirical probabilities due to Vapnik and Chervonenkis and, independently in the computer science context ("probably approximately correct" (PAC) learning) to Valiant.
Reference: [19] <author> Sontag, E.D., and H.J. Sussmann, </author> <title> "Complete controllability of continuous-time recurrent neural networks," </title> <journal> Systems and Control Letters, </journal> <note> 1997, to appear. </note>
Reference-contexts: the class of systems represented by (1) lies in the fact that they can, in a certain sense, approximate arbitrary nonlinear systems, they provide universal models of digital as well as analog computation, and their system-theoretic properties (controllability, observability, minimality, parameter identifiability, etc) can be quite elegantly characterized (see e.g. <ref> [1, 2, 19, 18] </ref>). The Learning Problem We formulate the problem of identification from input/output data in the paradigm of uniform convergence of empirical probabilities due to Vapnik and Chervonenkis and, independently in the computer science context ("probably approximately correct" (PAC) learning) to Valiant.
Reference: [20] <author> Vidyasagar, M., </author> <title> A Theory of Learning and Generalization: With Applications to Neural Networks and Control Systems, </title> <publisher> Springer, </publisher> <address> London, </address> <year> 1997. </year>
Reference-contexts: The Learning Problem We formulate the problem of identification from input/output data in the paradigm of uniform convergence of empirical probabilities due to Vapnik and Chervonenkis and, independently in the computer science context ("probably approximately correct" (PAC) learning) to Valiant. An excellent reference is the recent monograph <ref> [20] </ref>. The question that we address can be described in very intuitive terms as follows. Given is a "black box" or "plant" as in Figure 2. <p> The framework allows, in addition, for stochastic plants as well noisy measurements. These issues are discussed at length in <ref> [20] </ref>. We will state and prove a precise result on learning in the sense just described. <p> Thus, each (R i (; z)) is a Pfaffian function of degree 2 + (r 2) = r. The result then follows from <ref> [20] </ref>, Theorem 10.7 and Lemma 10.6, which provide an upper bound for the VC dimension of binary function classes defined by polynomials expressions involving Pfaffian functions, in terms of the degree of the polynomials and the degree and length of the Pfaffian chains involved. <p> Precisely, we apply Theorem 10.7 with "b" instead of "2 log 2 B" and Lemma 10.6 with q; d; D respectively equal to our n; q; r, to obtain respectively equations (7) and (8). (The results cited from the book <ref> [20] </ref> are based on the paper [9]; the only minor difference is in the slight improvement in (7), which the original paper had given as b + 16l.) Some algebraic manipulation gives: Corollary 2.6 When r = 3, l 3n + n 2 , n 5, and q 6k, pd (F <p> 3, l 3n + n 2 , n 5, and q 6k, pd (F ) 3n 6 + 5n 3 log 2 k. 2 2.3 Uniform Approximation of Expected Loss The last ingredient is a theorem on approximation of means by empirical means, which we quote from [7] (see also <ref> [20] </ref> for an exposition). <p> It is too conservative not only in the numeric constant (see other estimates in <ref> [20] </ref>) but also in the fact that it is a "worst possible case" result, valid with respect to all possible probability distributions, and in its use of pseudodimension estimates, which are in general conservative. <p> All these follow from pseudodimension estimates. For example, for the class of minimal linear systems of dimension n, we obtain pd (F ) = O (n log k) : This follows from the estimates for VC dimension of algebraic concept classes given in [6] (see <ref> [20] </ref>, Theorem 10.5 or Corollary 10.2), since in the proof of Lemma 2.5 has degree 2k + 5 and one may take l = 2n (because, by linear systems theory, reachable and observable systems can be parameterized linearly using that many parameters). <p> Note that y 0 (0; ~; u) = C~(A~ + Bu (0)) is a "single-hidden layer neural net" as a function of the inputs at time zero. Examples leading to infinite VC dimension are well-known (see e.g. <ref> [20] </ref>, Section 10.3.2). Discrete Time We have avoided discussion of discrete-time networks. For discrete-time systems there exist lower bounds on pseudodimension; see [5] for the case of linear systems and [10] for discrete time nonlinear nets.
Reference: [21] <author> Zbikowski, R., </author> <title> "Lie algebra of recurrent neural networks and identifiability," </title> <booktitle> Proc. Amer. Automatic Control Conference, </booktitle> <address> San Francisco, </address> <year> 1993, </year> <pages> pp. 2900-2901. </pages>
Reference-contexts: of the form _x = F x+~(Ax+Bu) | have been proposed as models fl Supported in part by US Air Force Grant AFOSR-94-0293 1 u - B - h + - ~ - - C - y r 6 for control, computation, and signal processing by many researchers, see e.g. <ref> [12, 14, 13, 15, 3, 16, 4, 21, 22] </ref>, as well as in optimization and associative memory design ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
Reference: [22] <editor> Zbikowski, R., and K.J. Hunt, eds., </editor> <booktitle> Neural Adaptive Control Technology World Scientific Publishing, </booktitle> <year> 1996. </year> <month> 11 </month>
Reference-contexts: of the form _x = F x+~(Ax+Bu) | have been proposed as models fl Supported in part by US Air Force Grant AFOSR-94-0293 1 u - B - h + - ~ - - C - y r 6 for control, computation, and signal processing by many researchers, see e.g. <ref> [12, 14, 13, 15, 3, 16, 4, 21, 22] </ref>, as well as in optimization and associative memory design ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
References-found: 22

