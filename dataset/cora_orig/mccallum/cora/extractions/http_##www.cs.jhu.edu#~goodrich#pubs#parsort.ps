URL: http://www.cs.jhu.edu/~goodrich/pubs/parsort.ps
Refering-URL: http://www.cs.jhu.edu/~goodrich/pubs/index.html
Root-URL: http://www.cs.jhu.edu
Email: goodrich@cs.jhu.edu  
Title: Communication-Efficient Parallel Sorting  
Author: Michael T. Goodrich 
Keyword: BSP computer requires (log n= log(h 1)) communication rounds. Key words: Parallel algorithms, parallel sorting, parallel processing.  
Note: p and a number of communication rounds that is O( log n  
Address: Baltimore, MD 21218  
Affiliation: Dept. of Computer Science Johns Hopkins Univ.  
Abstract: We study the problem of sorting n numbers on a p-processor bulk-synchronous parallel (BSP) computer, which is a parallel multicomputer that allows for general processor-to-processor communication rounds provided each processor sends and receives at most h items in any round. We provide parallel sorting methods that use internal computation time that is O( n log n log(h+1) ) for h = fi(n=p). The internal computation bound is optimal for any comparison-based sorting algorithm. Moreover, the number of communication rounds is bounded by a constant for the (practical) situations when p n 11=c for a constant c 1. In fact, we show that our bound on the number of communication rounds is asymptotically optimal for the full range of values for p, for we show that just computing the "or" of n bits distributed evenly to the first O(n=h) of an arbitrary number of processors in a 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Adler, J. W. Byers, and R. M. Karp. </author> <title> Parallel sorting with limited bandwidth. </title> <booktitle> In Proc. 7th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 129-136, </pages> <year> 1995. </year>
Reference-contexts: The only previous sorting algorithms we are aware of that were designed with the BSP model in mind are recent methods of Adler, Byers, and Karp <ref> [1] </ref> and Gerbessiotis and Valiant [20]. The method of Adler et al. runs in a combined time that is O ( ng log n p + pg + gL), provided p n 1ffi for some constant 0 &lt; ffi &lt; 1. <p> In addition, we achieve a deterministic combined running time that is O ( n log n p + (L + gn=p)(log n= log (n=p))), which is valid for all values of p and improves the best bounds of Adler et al. <ref> [1] </ref> and Gerbessiotis and Valiant [20] even when p n 1ffi for some constant 0 &lt; ffi &lt; 1, in which case our method sorts in a constant number of communication rounds. In fact, if p 3 n, then our method essentially amounts to a sample sort (with regular sampling). <p> Since this lower bound is independent of the total number of processors and amount of memory in the multicomputer, it joins lower bounds of Mansour et al. [33] and Adler et al. <ref> [1] </ref> in giving further evidence that the prime bottleneck in parallel computing is communication, and not the number of processors nor the memory size. 2 A weak-CREW BSP Sorting Algorithm Let S be a set of n items distributed evenly in a p-processor weak-CREW BSP computer.
Reference: [2] <author> A. Aggarwal, A. K. Chandra, and M. Snir. </author> <title> Communication complexity of PRAMs. </title> <journal> Theoretical Computer Science, </journal> <volume> 71 </volume> <pages> 3-28, </pages> <year> 1990. </year>
Reference-contexts: But, as more and more parallel computer systems are being built, researchers are realizing that processor-to-processor communication is a prime bottleneck in parallel computing (e.g., see Aggarwal et al. <ref> [2] </ref>, Bilardi and Preparata [6], Culler et al. [13], Kruskal et al. [29], Mansour et al. [33], Mehlhorn and Vishkin [34], Papadimitriou and Yannakakis [37], and Valiant [45, 44]).
Reference: [3] <author> M. Ajtai, J. Komlos, and E. Szemeredi. </author> <title> Sorting in c log n parallel steps. </title> <journal> Combinatorica, </journal> <volume> 3 </volume> <pages> 1-19, </pages> <year> 1983. </year>
Reference-contexts: Since this early work there has been much effort directed at fine-grain parallel sorting algorithms (e.g., see Akl [4], Bitton et al. [7], JaJa [25], Karp and Ramachandran [27], and Reif [41]). Nevertheless, it was not until 1983 that it was shown, by Ajtai, Komlos, and Szemeredi <ref> [3] </ref>, that n elements can be sorted in O (log n) time with an O (n log n)-sized network (see also Paterson [38]). <p> Finally, Chvatal [10] describes an approach of Ajtai, Komlos, Paterson, and Szemeredi for adapting the sorting network of Ajtai, Komlos, and Szemeredi <ref> [3] </ref> to achieve a depth of O (log n= log (n=p)) where the basic unit in the network is a "black box" that can sort dn=pe elements.
Reference: [4] <author> S. G. Akl. </author> <title> Parallel Sorting Algorithms. </title> <publisher> Academic Press, </publisher> <year> 1985. </year>
Reference-contexts: 1 Introduction Most of the research on parallel algorithm design in the 1970's and 1980's was focused on fine-grain massively-parallel models of computation (e.g., see Akl <ref> [4] </ref>, Bitton et al. [7], JaJa [25], Karp and Ramachandran [27], Leighton [31], and Reif [41]), where the ratio of memory to processors is fairly small (typically O (1)), and this focus was independent of whether the model of computation was a parallel random-access machine (PRAM) or a network model, such <p> Since this early work there has been much effort directed at fine-grain parallel sorting algorithms (e.g., see Akl <ref> [4] </ref>, Bitton et al. [7], JaJa [25], Karp and Ramachandran [27], and Reif [41]).
Reference: [5] <author> K. E. Batcher. </author> <title> Sorting networks and their applications. </title> <booktitle> In Proc. 1968 Spring Joint Computer Conf., </booktitle> <pages> pages 307-314, </pages> <address> Reston, VA, 1968. </address> <publisher> AFIPS Press. </publisher> <pages> 15 </pages>
Reference-contexts: Batcher <ref> [5] </ref> in 1968 gave what is considered to be the first parallel sorting scheme, showing that in a fine-grained parallel sorting network one can sort in O (log 2 n) time using O (n) processors.
Reference: [6] <author> G. Bilardi and F. P. Preparata. </author> <title> Lower bounds to processor-time tradeoffs under bounded-speed message propagation. </title> <booktitle> In Proc. 4th International Workshop on Algorithms and Data Structures (WADS), </booktitle> <volume> LNCS 955, </volume> <pages> pages 1-12. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: But, as more and more parallel computer systems are being built, researchers are realizing that processor-to-processor communication is a prime bottleneck in parallel computing (e.g., see Aggarwal et al. [2], Bilardi and Preparata <ref> [6] </ref>, Culler et al. [13], Kruskal et al. [29], Mansour et al. [33], Mehlhorn and Vishkin [34], Papadimitriou and Yannakakis [37], and Valiant [45, 44]).
Reference: [7] <author> D. Bitton, D. J. DeWitt, D. K. Hsiao, and J. Menon. </author> <title> A taxonomy of parallel sorting. </title> <journal> ACM Computing Surveys, </journal> <volume> 16(3) </volume> <pages> 287-318, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction Most of the research on parallel algorithm design in the 1970's and 1980's was focused on fine-grain massively-parallel models of computation (e.g., see Akl [4], Bitton et al. <ref> [7] </ref>, JaJa [25], Karp and Ramachandran [27], Leighton [31], and Reif [41]), where the ratio of memory to processors is fairly small (typically O (1)), and this focus was independent of whether the model of computation was a parallel random-access machine (PRAM) or a network model, such as a mesh-of-processors. <p> Since this early work there has been much effort directed at fine-grain parallel sorting algorithms (e.g., see Akl [4], Bitton et al. <ref> [7] </ref>, JaJa [25], Karp and Ramachandran [27], and Reif [41]). Nevertheless, it was not until 1983 that it was shown, by Ajtai, Komlos, and Szemeredi [3], that n elements can be sorted in O (log n) time with an O (n log n)-sized network (see also Paterson [38]).
Reference: [8] <author> G. E. Blelloch, C. E. Leiserson, B. M. Maggs, C. G. Plaxton, S. J. Smith, and M. Zagha. </author> <title> A comparison of sorting algorithms for the connection machine CM-2. </title> <booktitle> In Proc. 3rd ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 3-16, </pages> <year> 1991. </year>
Reference-contexts: Indeed, there are a host of published algorithms for achieving such a result when the ratio of input size to number of processors is as large as this. For example, a randomized strategy, called sample sort, achieves this result with high probability <ref> [8, 19, 20, 23, 24, 32, 40, 42] </ref>, as do deterministic strategies based upon regular sampling [18, 36, 43, 46]. These methods based upon sampling do not seem to scale nicely for smaller n=p ratios, however.
Reference: [9] <author> B. Chazelle and L. J. Guibas. Fractional cascading: I. </author> <title> A data structuring technique. </title> <journal> Algorith-mica, </journal> <volume> 1 </volume> <pages> 133-162, </pages> <year> 1986. </year>
Reference-contexts: We achieve this result using a cascading method similar to one used by Cole [11], which itself is similar to the general fractional cascading technique of Chazelle and Guibas <ref> [9] </ref>. Let T be a complete rooted d-way tree with each of its leaves associated with a sublist S i S of size at most dn=pe, where d = maxfd (n=p) 1=7 e; 2g (the reason for this choice will become apparent in the analysis).
Reference: [10] <author> V. Chvatal. </author> <title> Lecture notes on the new AKS sorting network. </title> <type> Report DCS-TR-294, </type> <institution> Computer Science Dept., Rutgers University, </institution> <year> 1992. </year> <note> Available at ftp://athos.rutgers.edu/pub/technical-reports/dcs-tr-294.ps.Z. </note>
Reference-contexts: Indeed, Plaxton 3 can modify the "sharesort" method of Cypher and Plaxton [15] to achieve T C = O ((log n= log (n=p)) log 2 (log n= log (n=p))). Finally, Chvatal <ref> [10] </ref> describes an approach of Ajtai, Komlos, Paterson, and Szemeredi for adapting the sorting network of Ajtai, Komlos, and Szemeredi [3] to achieve a depth of O (log n= log (n=p)) where the basic unit in the network is a "black box" that can sort dn=pe elements. <p> An effective method for constructing such a network is not included in Chvatal's report, however, for the method he describes is a non-uniform procedure based upon the probabilistic method. In addition, the constant factor in the running time appears to be fairly large. Incidentally, these latter methods <ref> [10, 15, 14, 30, 39] </ref> are actually defined for more-restrictive BSP models where the data elements cannot be duplicated and each internal computation must be a sorting of the internal-memory elements.
Reference: [11] <author> R. Cole. </author> <title> Parallel merge sort. </title> <journal> SIAM J. Comput., </journal> <volume> 17(4) </volume> <pages> 770-785, </pages> <year> 1988. </year>
Reference-contexts: In 1985 Leighton [30] extended this result to show that one can produce an O (n)-node bounded-degree network capable of sorting in O (log n) steps, based upon an algorithm he called "columnsort." In 1988 Cole <ref> [11] </ref> gave simple methods for optimal sorting in the CREW and EREW PRAM models in O (log n) time using O (n) processors, based upon an elegant "cascade mergesort" paradigm using arrays, and this result was recently extended to the Parallel Pointer Machine by Goodrich and Kosaraju [22]. <p> In fact, if p 3 n, then our method essentially amounts to a sample sort (with regular sampling). If p = fi (n), then our method amounts to a pipelined parallel mergesort, achieving the same asymptotic performance as the fine-grained algorithms of Cole <ref> [11] </ref> and Goodrich and Kosaraju [22]. Thus, our method provides a sorting method that is fully-scalable over all values of p while achieving an optimal internal computation time over this entire range. <p> We sort the elements of S using a d-way parallel mergesort, pipelined in a way analogous to the binary parallel mergesort procedures of Cole <ref> [11] </ref> and Goodrich and Kosaraju [22]. Specifically, we choose d = maxfd p n=pe; 2g, and let T be a d-way rooted, complete, balanced tree such that each leaf is associated with a subset S i S of size at most dn=pe. <p> Having given this important lemma and its corollary, let us now turn to the details of implementing each stage in our pipelined procedure using just a constant number of communication rounds. 2.1 Implementing each stage using a constant number of communication rounds We say that a list A is ranked <ref> [11, 22] </ref> into a list B if, for each element a 2 A, we know the rank of a's predecessor in B (based upon the ordering of elements in A [ B). If A is ranked in B and B is ranked in A, then A and B are cross-ranked. <p> Our goal is to sort S in O (log n= log (h + 1)) communication rounds and O (n log n=p) internal computation time without using any broadcasts, for h = fi (n=p). We achieve this result using a cascading method similar to one used by Cole <ref> [11] </ref>, which itself is similar to the general fractional cascading technique of Chazelle and Guibas [9].
Reference: [12] <author> S. A. Cook, C. Dwork, and R. Reischuk. </author> <title> Upper and lower time bounds for parallel random access machines without simultaneous writes. </title> <journal> SIAM J. Comput., </journal> <volume> 15 </volume> <pages> 87-97, </pages> <year> 1986. </year>
Reference-contexts: The goal of the computation is that after some T steps the "or" of the values in S should be stored in memory location m 1 . Our lower bound proof will be an adaptation of a lower bound proof of Cook, Dwork, and Reischuk <ref> [12] </ref> for computing the "or" of n bits on a CREW PRAM. <p> a critical input for function f (I) if f (I) 6= f (I (k)) for all k 2 f1; 2; : : : ; ng. (Note that I = (0; 0; : : : ; 0) is a critical input for the "or" function.) Say that input index k affects <ref> [12] </ref> processor p i in round t with input I if the state of p i on input I after round t differs from the state 12 of processor p i on input I (k) after round t. <p> We will show that if this is the case, then r 6hK t . As done by Cook, Dwork, and Reischuk <ref> [12] </ref>, we employ a combinatorial graph argument to derive a bound on r = jY j. Consider a bipartite graph G whose two node sets are Y and P .
Reference: [13] <author> D. E. Culler, R. M. Karp, D. A. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramo-nian, and T. von Eicken. </author> <title> LogP: Towards a realistic model of parallel computation. </title> <booktitle> In Proc. 4th ACM SIGPLAN Symp. on Princ. and Practice of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <year> 1993. </year>
Reference-contexts: But, as more and more parallel computer systems are being built, researchers are realizing that processor-to-processor communication is a prime bottleneck in parallel computing (e.g., see Aggarwal et al. [2], Bilardi and Preparata [6], Culler et al. <ref> [13] </ref>, Kruskal et al. [29], Mansour et al. [33], Mehlhorn and Vishkin [34], Papadimitriou and Yannakakis [37], and Valiant [45, 44]). <p> We call this the EREW BSP model, noting that it is essentially the same as a model Valiant elsewhere [45] calls the XPRAM and one that Gibbons [21] calls the EREW phase-PRAM. It is also the communication structure assumed by the LogP model <ref> [13, 28] </ref>, which is the same as the BSP model except that the LogP model does not explicitly require bulk-synchronous processing. But it is also natural to allow for a slightly more powerful bulk-synchronous model, which we call the weak-CREW BSP model. <p> Incidentally, this is also the running time of implementing a BSP computation in the analogous 2 LogP model <ref> [13, 28] </ref>.
Reference: [14] <author> R. Cypher and J. L. C. Sanz. Cubesort: </author> <title> A parallel algorithm for sorting N data items with S-sorters. </title> <journal> J. of Algorithms, </journal> <volume> 13 </volume> <pages> 211-234, </pages> <year> 1992. </year>
Reference-contexts: Using an algorithm they call "cubesort," Cypher and Sanz <ref> [14] </ref> show how to improve the T C term in these bounds to be O ((25) (log fl nlog fl (n=p)) [log n= log (n=p)] 2 ), and Plaxton [39] shows how cubesort can be modified to achieve T C = O ([log n= log (n=p)] 2 ). <p> An effective method for constructing such a network is not included in Chvatal's report, however, for the method he describes is a non-uniform procedure based upon the probabilistic method. In addition, the constant factor in the running time appears to be fairly large. Incidentally, these latter methods <ref> [10, 15, 14, 30, 39] </ref> are actually defined for more-restrictive BSP models where the data elements cannot be duplicated and each internal computation must be a sorting of the internal-memory elements.
Reference: [15] <author> R. E. Cypher and C. G. Plaxton. </author> <title> Deterministic sorting in nearly logarithmic time on the hypercube and related computers. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 47 </volume> <pages> 501-548, </pages> <year> 1993. </year>
Reference-contexts: Indeed, Plaxton 3 can modify the "sharesort" method of Cypher and Plaxton <ref> [15] </ref> to achieve T C = O ((log n= log (n=p)) log 2 (log n= log (n=p))). <p> An effective method for constructing such a network is not included in Chvatal's report, however, for the method he describes is a non-uniform procedure based upon the probabilistic method. In addition, the constant factor in the running time appears to be fairly large. Incidentally, these latter methods <ref> [10, 15, 14, 30, 39] </ref> are actually defined for more-restrictive BSP models where the data elements cannot be duplicated and each internal computation must be a sorting of the internal-memory elements.
Reference: [16] <author> F. Dehne, X. Deng, P. Dymond, A. Fabri, and A. A. Khokhar. </author> <title> A randomized parallel 3D convex hull algorithm for course grained multicomputers. </title> <booktitle> In Proc. 7th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 27-33, </pages> <year> 1995. </year>
Reference-contexts: This is essentially the same as a model Dehne et al. <ref> [16, 17] </ref> refer to as the coarse-grain multi-computer. In designing an algorithm for this model one must take care to ensure that, even with message duplication, the number of messages received by a processor in a single round is at most h.
Reference: [17] <author> F. Dehne, A. Fabri, and A. Rau-Chaplin. </author> <title> Scalable parallel geometric algorithms for coarse grained multicomputers. </title> <booktitle> In Proc. 9th Annu. ACM Sympos. Comput. Geom., </booktitle> <pages> pages 298-307, </pages> <year> 1993. </year>
Reference-contexts: This is essentially the same as a model Dehne et al. <ref> [16, 17] </ref> refer to as the coarse-grain multi-computer. In designing an algorithm for this model one must take care to ensure that, even with message duplication, the number of messages received by a processor in a single round is at most h.
Reference: [18] <author> R. S. Francis and L. J. H. Pannan. </author> <title> A parallel partition for enhanced parallel quicksort. </title> <journal> Parallel Computing, </journal> <volume> 18 </volume> <pages> 543-550, </pages> <year> 1992. </year>
Reference-contexts: For example, a randomized strategy, called sample sort, achieves this result with high probability [8, 19, 20, 23, 24, 32, 40, 42], as do deterministic strategies based upon regular sampling <ref> [18, 36, 43, 46] </ref>. These methods based upon sampling do not seem to scale nicely for smaller n=p ratios, however.
Reference: [19] <author> W. D. Frazer and A. C. McKellar. Samplesort: </author> <title> A sampling approach to minimal storage tree sorting. </title> <journal> J. ACM, </journal> <volume> 17(3) </volume> <pages> 496-507, </pages> <year> 1970. </year>
Reference-contexts: Indeed, there are a host of published algorithms for achieving such a result when the ratio of input size to number of processors is as large as this. For example, a randomized strategy, called sample sort, achieves this result with high probability <ref> [8, 19, 20, 23, 24, 32, 40, 42] </ref>, as do deterministic strategies based upon regular sampling [18, 36, 43, 46]. These methods based upon sampling do not seem to scale nicely for smaller n=p ratios, however.
Reference: [20] <author> A. V. Gerbessiotis and L. G. Valiant. </author> <title> Direct bulk-synchronous parallel algorithms. </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 22 </volume> <pages> 251-267, </pages> <year> 1994. </year>
Reference-contexts: Indeed, there are a host of published algorithms for achieving such a result when the ratio of input size to number of processors is as large as this. For example, a randomized strategy, called sample sort, achieves this result with high probability <ref> [8, 19, 20, 23, 24, 32, 40, 42] </ref>, as do deterministic strategies based upon regular sampling [18, 36, 43, 46]. These methods based upon sampling do not seem to scale nicely for smaller n=p ratios, however. <p> The only previous sorting algorithms we are aware of that were designed with the BSP model in mind are recent methods of Adler, Byers, and Karp [1] and Gerbessiotis and Valiant <ref> [20] </ref>. The method of Adler et al. runs in a combined time that is O ( ng log n p + pg + gL), provided p n 1ffi for some constant 0 &lt; ffi &lt; 1. <p> In addition, we achieve a deterministic combined running time that is O ( n log n p + (L + gn=p)(log n= log (n=p))), which is valid for all values of p and improves the best bounds of Adler et al. [1] and Gerbessiotis and Valiant <ref> [20] </ref> even when p n 1ffi for some constant 0 &lt; ffi &lt; 1, in which case our method sorts in a constant number of communication rounds. In fact, if p 3 n, then our method essentially amounts to a sample sort (with regular sampling).
Reference: [21] <author> P. B. Gibbons. </author> <title> A more practical PRAM model. </title> <booktitle> In Proc. (1st) ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 158-168, </pages> <year> 1989. </year>
Reference-contexts: We call this the EREW BSP model, noting that it is essentially the same as a model Valiant elsewhere [45] calls the XPRAM and one that Gibbons <ref> [21] </ref> calls the EREW phase-PRAM. It is also the communication structure assumed by the LogP model [13, 28], which is the same as the BSP model except that the LogP model does not explicitly require bulk-synchronous processing. <p> These models correspond to models Gibbons <ref> [21] </ref> calls the CREW phase-PRAM and CRCW phase-PRAM. In fact, Nash et al. [35] consider even more-powerful bulk-synchronous models in which messages can be combined using more-powerful combining functions (such as "add" or "bitwise-and").
Reference: [22] <author> M. T. Goodrich and S. R. Kosaraju. </author> <title> Sorting on a parallel pointer machine with applications to set expression evaluation. </title> <journal> J. ACM, </journal> <note> to appear. 16 </note>
Reference-contexts: In 1988 Cole [11] gave simple methods for optimal sorting in the CREW and EREW PRAM models in O (log n) time using O (n) processors, based upon an elegant "cascade mergesort" paradigm using arrays, and this result was recently extended to the Parallel Pointer Machine by Goodrich and Kosaraju <ref> [22] </ref>. Thus, one can sort optimally in these fine-grained models. These previous methods are not optimal, however, when implemented in bulk-synchronous models. Nevertheless, Leighton's columnsort method [30] can be used to design a bulk-synchronous parallel sorting algorithm that uses a constant number of communication rounds, provided p 3 n. <p> In fact, if p 3 n, then our method essentially amounts to a sample sort (with regular sampling). If p = fi (n), then our method amounts to a pipelined parallel mergesort, achieving the same asymptotic performance as the fine-grained algorithms of Cole [11] and Goodrich and Kosaraju <ref> [22] </ref>. Thus, our method provides a sorting method that is fully-scalable over all values of p while achieving an optimal internal computation time over this entire range. <p> We sort the elements of S using a d-way parallel mergesort, pipelined in a way analogous to the binary parallel mergesort procedures of Cole [11] and Goodrich and Kosaraju <ref> [22] </ref>. Specifically, we choose d = maxfd p n=pe; 2g, and let T be a d-way rooted, complete, balanced tree such that each leaf is associated with a subset S i S of size at most dn=pe. <p> Having given this important lemma and its corollary, let us now turn to the details of implementing each stage in our pipelined procedure using just a constant number of communication rounds. 2.1 Implementing each stage using a constant number of communication rounds We say that a list A is ranked <ref> [11, 22] </ref> into a list B if, for each element a 2 A, we know the rank of a's predecessor in B (based upon the ordering of elements in A [ B). If A is ranked in B and B is ranked in A, then A and B are cross-ranked.
Reference: [23] <author> W. L. Hightower, J. F. Prins, and J. H. Reif. </author> <title> Implementation of randomized sorting on large parallel machines. </title> <booktitle> In Proc. 4th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 158-167, </pages> <year> 1992. </year>
Reference-contexts: Indeed, there are a host of published algorithms for achieving such a result when the ratio of input size to number of processors is as large as this. For example, a randomized strategy, called sample sort, achieves this result with high probability <ref> [8, 19, 20, 23, 24, 32, 40, 42] </ref>, as do deterministic strategies based upon regular sampling [18, 36, 43, 46]. These methods based upon sampling do not seem to scale nicely for smaller n=p ratios, however.
Reference: [24] <author> J. S. Huang and Y. C. Chow. </author> <title> Parallel sorting and data partitioning by sampling. </title> <booktitle> In Proc. IEEE 7th Int. Computer Software and Applications Conference, </booktitle> <pages> pages 627-631, </pages> <year> 1983. </year>
Reference-contexts: Indeed, there are a host of published algorithms for achieving such a result when the ratio of input size to number of processors is as large as this. For example, a randomized strategy, called sample sort, achieves this result with high probability <ref> [8, 19, 20, 23, 24, 32, 40, 42] </ref>, as do deterministic strategies based upon regular sampling [18, 36, 43, 46]. These methods based upon sampling do not seem to scale nicely for smaller n=p ratios, however.
Reference: [25] <author> J. JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Most of the research on parallel algorithm design in the 1970's and 1980's was focused on fine-grain massively-parallel models of computation (e.g., see Akl [4], Bitton et al. [7], JaJa <ref> [25] </ref>, Karp and Ramachandran [27], Leighton [31], and Reif [41]), where the ratio of memory to processors is fairly small (typically O (1)), and this focus was independent of whether the model of computation was a parallel random-access machine (PRAM) or a network model, such as a mesh-of-processors. <p> Since this early work there has been much effort directed at fine-grain parallel sorting algorithms (e.g., see Akl [4], Bitton et al. [7], JaJa <ref> [25] </ref>, Karp and Ramachandran [27], and Reif [41]). Nevertheless, it was not until 1983 that it was shown, by Ajtai, Komlos, and Szemeredi [3], that n elements can be sorted in O (log n) time with an O (n log n)-sized network (see also Paterson [38]).
Reference: [26] <author> R. Karp, M. Luby, and F. Meyer auf der Heide. </author> <title> Efficient pram simulation on distributed machines. </title> <booktitle> In Proc. 24th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 318-326, </pages> <year> 1992. </year>
Reference-contexts: fundamental problem of sorting n elements distributed evenly across a p-processor BSP computer. 1 Indeed, a PRAM with as many processors and memory cells is a BSP model with h = 1, as is a module parallel computer (MPC) [34], which is also known as is a distributed-memory machine (DMM) <ref> [26] </ref>, for any memory size. 2 There is also an o parameter in the LogP model, but it would be redundant with L and g in this bound. 2 1.2 Previous work on parallel sorting Let us, then, briefly review a small sample of the work previously done for parallel sorting.
Reference: [27] <author> R. M. Karp and V. Ramachandran. </author> <title> Parallel algorithms for shared memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <pages> pages 869-941. </pages> <publisher> Elsevier/The MIT Press, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction Most of the research on parallel algorithm design in the 1970's and 1980's was focused on fine-grain massively-parallel models of computation (e.g., see Akl [4], Bitton et al. [7], JaJa [25], Karp and Ramachandran <ref> [27] </ref>, Leighton [31], and Reif [41]), where the ratio of memory to processors is fairly small (typically O (1)), and this focus was independent of whether the model of computation was a parallel random-access machine (PRAM) or a network model, such as a mesh-of-processors. <p> Since this early work there has been much effort directed at fine-grain parallel sorting algorithms (e.g., see Akl [4], Bitton et al. [7], JaJa [25], Karp and Ramachandran <ref> [27] </ref>, and Reif [41]). Nevertheless, it was not until 1983 that it was shown, by Ajtai, Komlos, and Szemeredi [3], that n elements can be sorted in O (log n) time with an O (n log n)-sized network (see also Paterson [38]).
Reference: [28] <author> R. M. Karp, A. Sahay, E. Santos, and K. E. Schauser. </author> <title> Optimal broadcast and summation in the LogP model. </title> <booktitle> In Proc. 5th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 142-153, </pages> <year> 1993. </year>
Reference-contexts: We call this the EREW BSP model, noting that it is essentially the same as a model Valiant elsewhere [45] calls the XPRAM and one that Gibbons [21] calls the EREW phase-PRAM. It is also the communication structure assumed by the LogP model <ref> [13, 28] </ref>, which is the same as the BSP model except that the LogP model does not explicitly require bulk-synchronous processing. But it is also natural to allow for a slightly more powerful bulk-synchronous model, which we call the weak-CREW BSP model. <p> Incidentally, this is also the running time of implementing a BSP computation in the analogous 2 LogP model <ref> [13, 28] </ref>.
Reference: [29] <author> C. Kruskal, L. Rudolph, and M. Snir. </author> <title> A complexity theory of efficient parallel algorithms. </title> <journal> Theoretical Computer Science, </journal> <volume> 71 </volume> <pages> 95-132, </pages> <year> 1990. </year>
Reference-contexts: But, as more and more parallel computer systems are being built, researchers are realizing that processor-to-processor communication is a prime bottleneck in parallel computing (e.g., see Aggarwal et al. [2], Bilardi and Preparata [6], Culler et al. [13], Kruskal et al. <ref> [29] </ref>, Mansour et al. [33], Mehlhorn and Vishkin [34], Papadimitriou and Yannakakis [37], and Valiant [45, 44]).
Reference: [30] <author> F. T. Leighton. </author> <title> Tight bounds on the complexity of parallel sorting. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(4):344-354, </volume> <year> 1985. </year>
Reference-contexts: Nevertheless, it was not until 1983 that it was shown, by Ajtai, Komlos, and Szemeredi [3], that n elements can be sorted in O (log n) time with an O (n log n)-sized network (see also Paterson [38]). In 1985 Leighton <ref> [30] </ref> extended this result to show that one can produce an O (n)-node bounded-degree network capable of sorting in O (log n) steps, based upon an algorithm he called "columnsort." In 1988 Cole [11] gave simple methods for optimal sorting in the CREW and EREW PRAM models in O (log n) <p> Thus, one can sort optimally in these fine-grained models. These previous methods are not optimal, however, when implemented in bulk-synchronous models. Nevertheless, Leighton's columnsort method <ref> [30] </ref> can be used to design a bulk-synchronous parallel sorting algorithm that uses a constant number of communication rounds, provided p 3 n. <p> An effective method for constructing such a network is not included in Chvatal's report, however, for the method he describes is a non-uniform procedure based upon the probabilistic method. In addition, the constant factor in the running time appears to be fairly large. Incidentally, these latter methods <ref> [10, 15, 14, 30, 39] </ref> are actually defined for more-restrictive BSP models where the data elements cannot be duplicated and each internal computation must be a sorting of the internal-memory elements.
Reference: [31] <author> F. T. Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hyper-cubes. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Most of the research on parallel algorithm design in the 1970's and 1980's was focused on fine-grain massively-parallel models of computation (e.g., see Akl [4], Bitton et al. [7], JaJa [25], Karp and Ramachandran [27], Leighton <ref> [31] </ref>, and Reif [41]), where the ratio of memory to processors is fairly small (typically O (1)), and this focus was independent of whether the model of computation was a parallel random-access machine (PRAM) or a network model, such as a mesh-of-processors.
Reference: [32] <author> H. Li and K. C. Sevcik. </author> <title> Parallel sorting by overpartitioning. </title> <booktitle> In Proc. 6th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 46-56, </pages> <year> 1994. </year>
Reference-contexts: Indeed, there are a host of published algorithms for achieving such a result when the ratio of input size to number of processors is as large as this. For example, a randomized strategy, called sample sort, achieves this result with high probability <ref> [8, 19, 20, 23, 24, 32, 40, 42] </ref>, as do deterministic strategies based upon regular sampling [18, 36, 43, 46]. These methods based upon sampling do not seem to scale nicely for smaller n=p ratios, however.
Reference: [33] <author> Y. Mansour, N. Nisan, and U. Vishkin. </author> <title> Trade-offs between communication throughput and parallel time. </title> <booktitle> In Proc. 26th ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pages 372-381, </pages> <year> 1994. </year>
Reference-contexts: But, as more and more parallel computer systems are being built, researchers are realizing that processor-to-processor communication is a prime bottleneck in parallel computing (e.g., see Aggarwal et al. [2], Bilardi and Preparata [6], Culler et al. [13], Kruskal et al. [29], Mansour et al. <ref> [33] </ref>, Mehlhorn and Vishkin [34], Papadimitriou and Yannakakis [37], and Valiant [45, 44]). <p> This lower bound holds even if the number of additional processors and the number of additional memory cells per processor are unbounded. Since this lower bound is independent of the total number of processors and amount of memory in the multicomputer, it joins lower bounds of Mansour et al. <ref> [33] </ref> and Adler et al. [1] in giving further evidence that the prime bottleneck in parallel computing is communication, and not the number of processors nor the memory size. 2 A weak-CREW BSP Sorting Algorithm Let S be a set of n items distributed evenly in a p-processor weak-CREW BSP computer.
Reference: [34] <author> K. Mehlhorn and U. Vishkin. </author> <title> Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel memories. </title> <journal> Acta Informatica, </journal> <volume> 9(1) </volume> <pages> 29-59, </pages> <year> 1984. </year>
Reference-contexts: But, as more and more parallel computer systems are being built, researchers are realizing that processor-to-processor communication is a prime bottleneck in parallel computing (e.g., see Aggarwal et al. [2], Bilardi and Preparata [6], Culler et al. [13], Kruskal et al. [29], Mansour et al. [33], Mehlhorn and Vishkin <ref> [34] </ref>, Papadimitriou and Yannakakis [37], and Valiant [45, 44]). The real potential of parallel computation, therefore, will most likely only be realized for coarse-to-medium-grain parallel systems, where the ratio of memory to processors is non-constant, for such systems allow an algorithm designer to balance communication latency with internal computation time. <p> to further the study of bulk-synchronous parallel algorithms by addressing the fundamental problem of sorting n elements distributed evenly across a p-processor BSP computer. 1 Indeed, a PRAM with as many processors and memory cells is a BSP model with h = 1, as is a module parallel computer (MPC) <ref> [34] </ref>, which is also known as is a distributed-memory machine (DMM) [26], for any memory size. 2 There is also an o parameter in the LogP model, but it would be redundant with L and g in this bound. 2 1.2 Previous work on parallel sorting Let us, then, briefly review
Reference: [35] <author> J. M. Nash, P. M. Dew, M. E. Dyer, and J. R. Davy. </author> <title> Parallel algorithm design on the WPRAM model. </title> <type> Technical Report 94.24, </type> <institution> School of Computer Science, Univeristy of Leeds, </institution> <year> 1994. </year>
Reference-contexts: These models correspond to models Gibbons [21] calls the CREW phase-PRAM and CRCW phase-PRAM. In fact, Nash et al. <ref> [35] </ref> consider even more-powerful bulk-synchronous models in which messages can be combined using more-powerful combining functions (such as "add" or "bitwise-and").
Reference: [36] <author> D. Nassimi and S. Sahni. </author> <title> Parallel permutation and sorting algorithms and a new generalized connection network. </title> <journal> J. ACM, </journal> <volume> 29(3) </volume> <pages> 642-667, </pages> <year> 1982. </year>
Reference-contexts: For example, a randomized strategy, called sample sort, achieves this result with high probability [8, 19, 20, 23, 24, 32, 40, 42], as do deterministic strategies based upon regular sampling <ref> [18, 36, 43, 46] </ref>. These methods based upon sampling do not seem to scale nicely for smaller n=p ratios, however.
Reference: [37] <author> C. Papadimitriou and M. Yannakakis. </author> <title> Towards an architecture-independent analysis of parallel algorithms. </title> <booktitle> Proc. 20th ACM Symp. Theory Comp. (STOC), </booktitle> <pages> pages 510-513, </pages> <year> 1988. </year>
Reference-contexts: and more parallel computer systems are being built, researchers are realizing that processor-to-processor communication is a prime bottleneck in parallel computing (e.g., see Aggarwal et al. [2], Bilardi and Preparata [6], Culler et al. [13], Kruskal et al. [29], Mansour et al. [33], Mehlhorn and Vishkin [34], Papadimitriou and Yannakakis <ref> [37] </ref>, and Valiant [45, 44]). The real potential of parallel computation, therefore, will most likely only be realized for coarse-to-medium-grain parallel systems, where the ratio of memory to processors is non-constant, for such systems allow an algorithm designer to balance communication latency with internal computation time.
Reference: [38] <author> M. Paterson. </author> <title> Improved sorting networks with o(log n) depth. </title> <journal> Algorithmica, </journal> <volume> 5(1) </volume> <pages> 75-92, </pages> <year> 1990. </year>
Reference-contexts: Nevertheless, it was not until 1983 that it was shown, by Ajtai, Komlos, and Szemeredi [3], that n elements can be sorted in O (log n) time with an O (n log n)-sized network (see also Paterson <ref> [38] </ref>).
Reference: [39] <author> C. G. Plaxton. </author> <title> Efficient Computation on Sparse Interconnection Networks. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, </institution> <year> 1989. </year>
Reference-contexts: Using an algorithm they call "cubesort," Cypher and Sanz [14] show how to improve the T C term in these bounds to be O ((25) (log fl nlog fl (n=p)) [log n= log (n=p)] 2 ), and Plaxton <ref> [39] </ref> shows how cubesort can be modified to achieve T C = O ([log n= log (n=p)] 2 ). Indeed, Plaxton 3 can modify the "sharesort" method of Cypher and Plaxton [15] to achieve T C = O ((log n= log (n=p)) log 2 (log n= log (n=p))). <p> An effective method for constructing such a network is not included in Chvatal's report, however, for the method he describes is a non-uniform procedure based upon the probabilistic method. In addition, the constant factor in the running time appears to be fairly large. Incidentally, these latter methods <ref> [10, 15, 14, 30, 39] </ref> are actually defined for more-restrictive BSP models where the data elements cannot be duplicated and each internal computation must be a sorting of the internal-memory elements.
Reference: [40] <author> M. J. Quinn. </author> <title> Analysis and benchmarking of two parallel sorting algorithms: </title> <journal> hyperquicksort and quickmerge. BIT, </journal> <volume> 29(2) </volume> <pages> 239-250, </pages> <year> 1989. </year> <month> 17 </month>
Reference-contexts: Indeed, there are a host of published algorithms for achieving such a result when the ratio of input size to number of processors is as large as this. For example, a randomized strategy, called sample sort, achieves this result with high probability <ref> [8, 19, 20, 23, 24, 32, 40, 42] </ref>, as do deterministic strategies based upon regular sampling [18, 36, 43, 46]. These methods based upon sampling do not seem to scale nicely for smaller n=p ratios, however.
Reference: [41] <author> J. H. Reif. </author> <title> Synthesis of Parallel Algorithms. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Most of the research on parallel algorithm design in the 1970's and 1980's was focused on fine-grain massively-parallel models of computation (e.g., see Akl [4], Bitton et al. [7], JaJa [25], Karp and Ramachandran [27], Leighton [31], and Reif <ref> [41] </ref>), where the ratio of memory to processors is fairly small (typically O (1)), and this focus was independent of whether the model of computation was a parallel random-access machine (PRAM) or a network model, such as a mesh-of-processors. <p> Since this early work there has been much effort directed at fine-grain parallel sorting algorithms (e.g., see Akl [4], Bitton et al. [7], JaJa [25], Karp and Ramachandran [27], and Reif <ref> [41] </ref>). Nevertheless, it was not until 1983 that it was shown, by Ajtai, Komlos, and Szemeredi [3], that n elements can be sorted in O (log n) time with an O (n log n)-sized network (see also Paterson [38]).
Reference: [42] <author> J. H. Reif and L. G. Valiant. </author> <title> A logarithmic time sort for linear size networks. </title> <journal> J. ACM, </journal> <volume> 34(1) </volume> <pages> 60-76, </pages> <year> 1987. </year>
Reference-contexts: Indeed, there are a host of published algorithms for achieving such a result when the ratio of input size to number of processors is as large as this. For example, a randomized strategy, called sample sort, achieves this result with high probability <ref> [8, 19, 20, 23, 24, 32, 40, 42] </ref>, as do deterministic strategies based upon regular sampling [18, 36, 43, 46]. These methods based upon sampling do not seem to scale nicely for smaller n=p ratios, however.
Reference: [43] <author> H. Shi and J. Schaeffer. </author> <title> Parallel sorting by regular sampling. </title> <journal> J. Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 362-372, </pages> <year> 1992. </year>
Reference-contexts: For example, a randomized strategy, called sample sort, achieves this result with high probability [8, 19, 20, 23, 24, 32, 40, 42], as do deterministic strategies based upon regular sampling <ref> [18, 36, 43, 46] </ref>. These methods based upon sampling do not seem to scale nicely for smaller n=p ratios, however.
Reference: [44] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Comm. ACM, </journal> <volume> 33 </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: computer systems are being built, researchers are realizing that processor-to-processor communication is a prime bottleneck in parallel computing (e.g., see Aggarwal et al. [2], Bilardi and Preparata [6], Culler et al. [13], Kruskal et al. [29], Mansour et al. [33], Mehlhorn and Vishkin [34], Papadimitriou and Yannakakis [37], and Valiant <ref> [45, 44] </ref>). The real potential of parallel computation, therefore, will most likely only be realized for coarse-to-medium-grain parallel systems, where the ratio of memory to processors is non-constant, for such systems allow an algorithm designer to balance communication latency with internal computation time. <p> Indeed, this realization has given rise to several new computation models for parallel algorithm design, which all use what Valiant <ref> [44] </ref> calls "bulk synchronous" processing. In such a model an input of size n is distributed evenly across a p-processor parallel computer. <p> In the weakest version, which is the only version Valiant <ref> [44] </ref> considers, the network may not duplicate nor combine messages, but instead may only realize h-relations between the processors.
Reference: [45] <author> L. G. Valiant. </author> <title> General purpose parallel architectures. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <pages> pages 943-972. </pages> <publisher> Elsevier/The MIT Press, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: computer systems are being built, researchers are realizing that processor-to-processor communication is a prime bottleneck in parallel computing (e.g., see Aggarwal et al. [2], Bilardi and Preparata [6], Culler et al. [13], Kruskal et al. [29], Mansour et al. [33], Mehlhorn and Vishkin [34], Papadimitriou and Yannakakis [37], and Valiant <ref> [45, 44] </ref>). The real potential of parallel computation, therefore, will most likely only be realized for coarse-to-medium-grain parallel systems, where the ratio of memory to processors is non-constant, for such systems allow an algorithm designer to balance communication latency with internal computation time. <p> In the weakest version, which is the only version Valiant [44] considers, the network may not duplicate nor combine messages, but instead may only realize h-relations between the processors. We call this the EREW BSP model, noting that it is essentially the same as a model Valiant elsewhere <ref> [45] </ref> calls the XPRAM and one that Gibbons [21] calls the EREW phase-PRAM. It is also the communication structure assumed by the LogP model [13, 28], which is the same as the BSP model except that the LogP model does not explicitly require bulk-synchronous processing.
Reference: [46] <author> Y. Won and S. Sahni. </author> <title> A balanced bin sort for hypercube multicomputers. </title> <journal> J. of Supercomputing, </journal> <volume> 2 </volume> <pages> 435-448, </pages> <year> 1988. </year>
Reference-contexts: For example, a randomized strategy, called sample sort, achieves this result with high probability [8, 19, 20, 23, 24, 32, 40, 42], as do deterministic strategies based upon regular sampling <ref> [18, 36, 43, 46] </ref>. These methods based upon sampling do not seem to scale nicely for smaller n=p ratios, however.
References-found: 46

