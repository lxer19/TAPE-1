URL: ftp://ftp.idsia.ch/pub/juergen/loconet.ps.gz
Refering-URL: http://www.idsia.ch/journals.html
Root-URL: 
Email: juergen@idsia.ch  
Title: DISCOVERING NEURAL NETS WITH LOW KOLMOGOROV COMPLEXITY AND HIGH GENERALIZATION CAPABILITY Neural Networks 10(5):857-873, 1997  
Author: Jurgen Schmidhuber 
Keyword: Kolmogorov complexity, Levin complexity, Solomonoff-Levin distribution, generalization, universal search, self-sizing programs, neural networks.  
Web: http://www.idsia.ch/~juergen  
Address: Corso Elvezia 36, 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Abstract: Many neural net learning algorithms aim at finding "simple" nets to explain training data. The expectation is: the "simpler" the networks, the better the generalization on test data (! Occam's razor). Previous implementations, however, use measures for "simplicity" that lack the power, universality and elegance of those based on Kolmogorov complexity and Solomonoff's algorithmic probability. Likewise, most previous approaches (especially those of the "Bayesian" kind) suffer from the problem of choosing appropriate priors. This paper addresses both issues. It first reviews some basic concepts of algorithmic complexity theory relevant to machine learning, and how the Solomonoff-Levin distribution (or universal prior) deals with the prior problem. The universal prior leads to a probabilistic method for finding "algorithmically simple" problem solutions with high generalization capability. The method is based on Levin complexity (a time-bounded generalization of Kolmogorov complexity) and inspired by Levin's optimal universal search algorithm. For a given problem, solution candidates are computed by efficient "self-sizing" programs that influence their own runtime and storage size. The probabilistic search algorithm finds the "good" programs (the ones quickly computing algorithmically probable solutions fitting the training data). Simulations focus on the task of discovering "algorithmically simple" neural networks with low Kolmogorov complexity and high generalization capability. It is demonstrated that the method, at least with certain toy problems where it is computationally feasible, can lead to generalization results unmatchable by previous neural net algorithms. Much remains do be done, however, to make large scale applications and "incremental learning" feasible.
Abstract-found: 1
Intro-found: 1
Reference: <author> Adleman, L. </author> <year> (1979). </year> <title> Time, space, and randomness. </title> <type> Technical Report MIT/LCS/79/TM-131, </type> <institution> Laboratory for Computer Science, MIT. </institution>
Reference-contexts: Pippenger. Levin introduced Kt complexity and the universal optimal search algorithm (see, e.g., (Levin, 1973b) and (Levin, 1984), where related ideas are attributed to Adleman see also <ref> (Adleman, 1979) </ref>). Other generalizations of Kolmogorov complexity have been proposed, e.g., (Hartmanis, 1983), but for more details see the contributions in (Watanabe, 1992). Easily computable approximations of the MDL principle were formulated by Wallace and Boulton (1968) and Rissanen (1978, 1983, 1986).
Reference: <author> Allender, A. </author> <year> (1992). </year> <title> Application of time-bounded Kolmogorov complexity in complexity theory. </title> <editor> In Watanabe, O., editor, </editor> <booktitle> Kolmogorov complexity and computational complexity, </booktitle> <pages> pages 6-22. </pages> <booktitle> EATCS Monographs on Theoretical Computer Science, </booktitle> <publisher> Springer. </publisher>
Reference-contexts: This section was partly inspired by presentations found in (Chaitin, 1987), (Li and Vitanyi, 1993), and (Solomonoff, 1986). 3 PROBABILISTIC SEARCH FOR USEFUL SELF-SIZING PROGRAMS WITH LOW LEVIN COMPLEXITY Levin's universal search algorithm was considered of interest for theoretical purposes (see, e.g., <ref> (Allender, 1992) </ref> and (Li and Vitanyi, 1993)). However, it seems that nobody implemented it for experimental applications, perhaps in fear of the ominous "constant factor" which may be large. To my knowledge, general universal search was implemented for the first time during the project that led to this paper.
Reference: <author> Amari, S. and Murata, N. </author> <year> (1993). </year> <title> Statistical theory of learning curves under entropic loss criterion. </title> <journal> Neural Computation, </journal> <volume> 5(1) </volume> <month> 140-153. </month> <title> Nets with low Kolmogorov complexity 19 Atick, </title> <editor> J. J., Li, Z., and Redlich, A. N. </editor> <year> (1992). </year> <title> Understanding retinal color coding from first principles. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 559-572. </pages>
Reference: <author> Barlow, H. B. </author> <year> (1989). </year> <title> Unsupervised learning. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 295-311. </pages>
Reference-contexts: From the standpoint of classical information theory, an optimal compression algorithm is one that builds a factorial code of the input data | a code with statistically independent components, e.g., <ref> (Barlow, 1989) </ref> (see Linsker (1988) for related work on the special, linear case).
Reference: <author> Barron, A. R. </author> <year> (1988). </year> <title> Complexity regularization with application to artificial neural networks. </title> <booktitle> In Nonparametric Functional Estimation and Related Topics, </booktitle> <pages> pages 561-576. </pages> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Barto, A. G. </author> <year> (1989). </year> <title> Connectionist approaches for control. </title> <type> Technical Report COINS 89-89, </type> <institution> University of Massachusetts, </institution> <address> Amherst MA 01003. </address>
Reference: <author> Barto, A. G., Sutton, R. S., and Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, SMC-13:834-846. </journal>
Reference: <author> Barzdin, Y. M. </author> <year> (1988). </year> <title> Algorithmic information theory. </title> <editor> In Reidel, D., editor, </editor> <booktitle> Encyclopaedia of Mathematics, </booktitle> <volume> volume 1, </volume> <pages> pages 140-142. </pages> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Such approximations build the basis of most | if not all | current machine learning applications, e.g., (Quinlan and Rivest, 1989; Gao and Li, 1989; Milosavljevic and Jurka, 1993; Pednault, 1989). Barzdin, referred to in (Zvonkin and Levin, 1970) <ref> (see also Barzdin, 1988) </ref>, related Kolmogorov complexity to a variant of Godel's incompleteness theorem, a subject which became a central theme of Chaitin's research (Chaitin, 1987). Meanwhile, the theory of Kolmogorov complexity has split into many subfields.
Reference: <author> Baum, E. B. and Haussler, D. </author> <year> (1989). </year> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1(1) </booktitle> <pages> 151-160. </pages>
Reference: <author> Becker, S. </author> <year> (1991). </year> <title> Unsupervised learning procedures for neural networks. </title> <journal> International Journal of Neural Systems, </journal> <volume> 2(1 </volume> & 2):17-33. 
Reference-contexts: Various "neural" methods for compressing input data are known. (Schmidhuber, 1992b) describes a "neural" method designed to generate factorial codes. (Atick et al., 1992) focuses on visual inputs. (Schmidhuber, 1992a) focuses on loss-free sequence compression. <ref> (Becker, 1991) </ref> provides numerous additional references. There are also numerous heuristic constructive methods, where network size grows in case of underfitting the training data. MDL approaches in other areas of machine learning include (Quinlan and Rivest, 1989; Gao and Li, 1989; Milosavljevic and Jurka, 1993; Pednault, 1989).
Reference: <author> Bennett, C. H. </author> <year> (1988). </year> <title> Logical depth and physical complexity. In The Universal Turing Machine: A Half Century Survey, </title> <booktitle> volume 1, </booktitle> <pages> pages 227-258. </pages> <publisher> Oxford University Press, Oxford and Kammerer & Unverzagt, </publisher> <address> Hamburg. </address>
Reference-contexts: The solution. The only solution to the problem is: make all w i equal to 1. The Kolmogorov complexity of this solution is small, since there is a short program that computes it. Its Levin complexity is small, too, since its "logical depth" (the runtime of its shortest program <ref> (Bennett, 1988) </ref>) is less than 400 time steps. The training data. To illustrate the generalization capability of search for solution candidates with low Levin complexity, only 3 training examples are used. They were randomly chosen from the 161; 700 possible inputs.
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. </author> <year> (1987). </year> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380. </pages>
Reference-contexts: The nth number is n 4 10n 3 + 35n 2 48n + 24: But an IQ test requires you to answer "10" instead of "34." Why not "34"? The reasons are: (1) "Simple" solutions are preferred over "complex" ones. This idea is often referred to as "Occam's razor" <ref> (e.g., Blumer et al., 1987) </ref>. (2) It is assumed that the "simpler" the rules, the better the generalization on test data. (3) The makers of the IQ test assume that everybody agrees on what "simple" means. 1 Nets with low Kolmogorov complexity 2 Similarly, many neural net and machine learning researchers
Reference: <author> Chaitin, G. </author> <year> (1966). </year> <title> On the length of programs for computing finite binary sequences. </title> <journal> Journal of the ACM, </journal> <volume> 13 </volume> <pages> 547-569. </pages>
Reference: <author> Chaitin, G. </author> <year> (1969). </year> <title> On the length of programs for computing finite binary sequences: statistical considerations. </title> <journal> Journal of the ACM, </journal> <volume> 16 </volume> <pages> 145-159. </pages>
Reference-contexts: Both Solomonoff and Kolmogorov observed K's machine independence. Today, even Solomonoff himself refers to K as "Kolmogorov complexity," e.g., (Solomonoff, 1986). In 1969, G. J. Chaitin independently also published the essential concepts <ref> (Chaitin, 1969) </ref> (some hints were already provided at the end of his 1966 paper). Important related early work is described in (Martin-Lof, 1966; Gacs, 1974; Nets with low Kolmogorov complexity 6 Schnorr, 1971). Apparently, L. A.
Reference: <author> Chaitin, G. </author> <year> (1975). </year> <title> A theory of program size formally identical to information theory. </title> <journal> Journal of the ACM, </journal> <volume> 22 </volume> <pages> 329-340. </pages>
Reference: <author> Chaitin, G. </author> <year> (1987). </year> <title> Algorithmic Information Theory. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference-contexts: Barzdin, referred to in (Zvonkin and Levin, 1970) (see also Barzdin, 1988), related Kolmogorov complexity to a variant of Godel's incompleteness theorem, a subject which became a central theme of Chaitin's research <ref> (Chaitin, 1987) </ref>. Meanwhile, the theory of Kolmogorov complexity has split into many subfields. An excellent overview and many additional details on the history are given in Li and Vitanyi's book (1993) and also in (Cover et al., 1989). See (Schmidhuber, 1995) for an application to fine arts. <p> An excellent overview and many additional details on the history are given in Li and Vitanyi's book (1993) and also in (Cover et al., 1989). See (Schmidhuber, 1995) for an application to fine arts. This section was partly inspired by presentations found in <ref> (Chaitin, 1987) </ref>, (Li and Vitanyi, 1993), and (Solomonoff, 1986). 3 PROBABILISTIC SEARCH FOR USEFUL SELF-SIZING PROGRAMS WITH LOW LEVIN COMPLEXITY Levin's universal search algorithm was considered of interest for theoretical purposes (see, e.g., (Allender, 1992) and (Li and Vitanyi, 1993)). <p> The shortest algorithm computing the test set, given the training set, won't be any shorter. In other words, the "mutual algorithmic information" (e.g., <ref> (Chaitin, 1987) </ref>) between test set and training set will be zero in almost all cases (ignoring an additive constant independent of the problem).
Reference: <author> Cover, T. M., Gacs, P., and Gray, R. M. </author> <year> (1989). </year> <title> Kolmogorov's contributions to information theory and algorithmic complexity. </title> <journal> Annals of Probability Theory, </journal> <volume> 17 </volume> <pages> 840-865. </pages>
Reference-contexts: Meanwhile, the theory of Kolmogorov complexity has split into many subfields. An excellent overview and many additional details on the history are given in Li and Vitanyi's book (1993) and also in <ref> (Cover et al., 1989) </ref>. See (Schmidhuber, 1995) for an application to fine arts.
Reference: <author> Dayan, P. and Sejnowski, T. </author> <year> (1994). </year> <title> TD(): Convergence with probability 1. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 295-301. </pages>
Reference: <author> Deco, G., Finnoff, W., and Zimmermann, H. G. </author> <year> (1993). </year> <title> Elimination of overtraining by a mutual information network. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 744-749. </pages> <publisher> Springer. </publisher>
Reference: <author> Dietterich, T. G. </author> <year> (1989). </year> <title> Limitations of inductive learning. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> Ithaca, NY, </address> <pages> pages 124-128. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Gacs, P. </author> <year> (1974). </year> <title> On the symmetry of algorithmic information. </title> <journal> Soviet Math. Dokl., </journal> <volume> 15 </volume> <pages> 1477-1480. </pages>
Reference: <author> Gallant, S. I. </author> <year> (1990). </year> <title> A connectionist learning algorithm with provable generalization and scaling bounds. Neural Networks, 3 191-201. Nets with low Kolmogorov complexity 20 Gao, </title> <editor> Q. and Li, M. </editor> <year> (1989). </year> <title> The minimum description length principle and its application to online learning of handprinted characters. </title> <booktitle> In Proc. 11th IEEE International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, Mi, </address> <pages> pages 843-848. </pages>
Reference: <author> Guyon, I., Vapnik, V., Boser, B., Bottou, L., and Solla, S. A. </author> <year> (1992). </year> <title> Structural risk minimization for character recognition. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 471-479. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hartmanis, J. </author> <year> (1983). </year> <title> Generalized Kolmogorov complexity and the structure of feasible computations. </title> <booktitle> In Proc. 24th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 439-445. </pages>
Reference-contexts: Pippenger. Levin introduced Kt complexity and the universal optimal search algorithm (see, e.g., (Levin, 1973b) and (Levin, 1984), where related ideas are attributed to Adleman see also (Adleman, 1979)). Other generalizations of Kolmogorov complexity have been proposed, e.g., <ref> (Hartmanis, 1983) </ref>, but for more details see the contributions in (Watanabe, 1992). Easily computable approximations of the MDL principle were formulated by Wallace and Boulton (1968) and Rissanen (1978, 1983, 1986).
Reference: <author> Hassibi, B. and Stork, D. G. </author> <year> (1993). </year> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 164-171. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Haussler, D. </author> <year> (1988). </year> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 177-221. </pages>
Reference: <author> Heil, S. </author> <year> (1995). </year> <title> Universelle Suche und inkrementelles Lernen, </title> <type> diploma thesis. </type> <institution> Fakultat fur Infor-matik, Lehrstuhl Prof. Brauer, Technische Universitat Munchen. </institution>
Reference: <author> Hinton, G. E. and van Camp, D. </author> <year> (1993). </year> <title> Keeping neural networks simple. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 11-18. </pages> <publisher> Springer. </publisher>
Reference-contexts: A special error term (in addition to the standard term enforcing matches between desired outputs and actual outputs) encourages weights close to zero. The idea is that a zero weight does not cost many bits to be specified, thus being "simple" <ref> (Hinton and van Camp, 1993) </ref>. Pearlmutter and Hinton were probably the first to propose weight decay, while Rumelhart was perhaps the first to suggest its use for reducing overfitting.
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1995). </year> <title> Simplifying neural nets by discovering flat minima. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 529-536. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1996). </year> <title> Flat minima. Neural Computation. </title> <note> In press. Extended version available in WWW homepages of Hochreiter and Schmidhuber. </note>
Reference: <author> Kolmogorov, A. </author> <year> (1965). </year> <title> Three approaches to the quantitative definition of information. </title> <journal> Problems of Information Transmission, </journal> <volume> 1 </volume> <pages> 1-11. </pages>
Reference-contexts: History spotlights / Selected references In 1965, A. N. Kolmogorov (1903-1987), founder of modern axiomatic probability theory (Kol-mogorov, 1933), was the first to introduce a variant of the complexity measure K for its own sake <ref> (Kolmogorov, 1965) </ref>. Levin (1984) cites announcements of Kolmogorov's lectures on this subject dating back to 1961. In an independent and even earlier work, R. J.
Reference: <author> Kolmogorov, A. N. </author> <year> (1933). </year> <editor> Grundbegriffe der Wahrscheinlichkeitsrechnung. </editor> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference: <author> Krogh, A. and Hertz, J. A. </author> <year> (1992). </year> <title> A simple weight decay can improve generalization. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 950-957. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> LeCun, Y. </author> <year> (1985). </year> <title> Une procedure d'apprentissage pour reseau a seuil asymetrique. </title> <booktitle> Proceedings of Cognitiva 85, Paris, </booktitle> <pages> pages 599-604. </pages>
Reference: <author> LeCun, Y., Kanter, I., and Solla, S. A. </author> <year> (1991). </year> <title> Second order properties of error surfaces: Learning time and generalization. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 918-924. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Estimates of the probabilities are computed on the basis of Gaussian assumptions. * Optimal Brain Surgeon. Hassibi and Storck (1993) use second order information to obtain "simple" nets by pruning weights whose influence on the error is minimal, and changing other weights to compensate. See <ref> (LeCun et al., 1991) </ref> for a related approach and (Vapnik, 1992; Guyon et al.,1992) for some theoretical analysis. * "Non-algorithmic" MDL methods based on Gaussian priors.
Reference: <author> Levin, L. A. </author> <year> (1973a). </year> <title> On the notion of a random sequence. </title> <journal> Soviet Math. Dokl., </journal> <volume> 14(5) </volume> <pages> 1413-1416. </pages>
Reference: <author> Levin, L. A. </author> <year> (1973b). </year> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266. </pages>
Reference-contexts: Levin proved equation (3): K (s) = H (s) + O (1). The importance of prefix codes was independently mentioned by Chaitin (1975), who also proved (3) and attributes part of the argument to N. Pippenger. Levin introduced Kt complexity and the universal optimal search algorithm (see, e.g., <ref> (Levin, 1973b) </ref> and (Levin, 1984), where related ideas are attributed to Adleman see also (Adleman, 1979)). Other generalizations of Kolmogorov complexity have been proposed, e.g., (Hartmanis, 1983), but for more details see the contributions in (Watanabe, 1992).
Reference: <author> Levin, L. A. </author> <year> (1974). </year> <title> Laws of information (nongrowth) and aspects of the foundation of probability theory. </title> <journal> Problems of Information Transmission, </journal> <volume> 10(3) </volume> <pages> 206-210. </pages>
Reference-contexts: Important related early work is described in (Martin-Lof, 1966; Gacs, 1974; Nets with low Kolmogorov complexity 6 Schnorr, 1971). Apparently, L. A. Levin was the first to introduce and analyze today's "standard form" of Kolmogorov complexity based on halting programs and prefix codes <ref> (Levin, 1974) </ref>, see also (Gacs, 1974; Levin, 1973a; Levin, 1976; Zvonkin and Levin, 1970). Levin proved equation (3): K (s) = H (s) + O (1). The importance of prefix codes was independently mentioned by Chaitin (1975), who also proved (3) and attributes part of the argument to N. Pippenger.
Reference: <author> Levin, L. A. </author> <year> (1976). </year> <title> Various measures of complexity for finite objects (axiomatic description). </title> <journal> Soviet Math. Dokl., </journal> <volume> 17(2) </volume> <month> 522-526. </month> <title> Nets with low Kolmogorov complexity 21 Levin, </title> <editor> L. A. </editor> <year> (1984). </year> <title> Randomness conservation inequalities: Information and independence in mathematical theories. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 15-37. </pages>
Reference: <author> Li, M. and Vitanyi, P. M. B. </author> <year> (1989). </year> <title> A theory of learning simple concepts under simple distributions and average case complexity for the universal distribution. </title> <booktitle> In Proc. 30th American IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 34-39. </pages>
Reference-contexts: Problems that humans consider to be typical are atypical when compared to the general set of all well-defined problems (see also <ref> (Li and Vitanyi, 1989) </ref>). Indeed, for all "interesting" problems, the bias towards algorithmic simplicity seems justified! This may be a miracle. Or perhaps a consequence of the possibility that our universe is run by a short algorithm (every electron behaves the same way).
Reference: <author> Li, M. and Vitanyi, P. M. B. </author> <year> (1993). </year> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer. </publisher>
Reference-contexts: An excellent overview and many additional details on the history are given in Li and Vitanyi's book (1993) and also in (Cover et al., 1989). See (Schmidhuber, 1995) for an application to fine arts. This section was partly inspired by presentations found in (Chaitin, 1987), <ref> (Li and Vitanyi, 1993) </ref>, and (Solomonoff, 1986). 3 PROBABILISTIC SEARCH FOR USEFUL SELF-SIZING PROGRAMS WITH LOW LEVIN COMPLEXITY Levin's universal search algorithm was considered of interest for theoretical purposes (see, e.g., (Allender, 1992) and (Li and Vitanyi, 1993)). <p> This section was partly inspired by presentations found in (Chaitin, 1987), <ref> (Li and Vitanyi, 1993) </ref>, and (Solomonoff, 1986). 3 PROBABILISTIC SEARCH FOR USEFUL SELF-SIZING PROGRAMS WITH LOW LEVIN COMPLEXITY Levin's universal search algorithm was considered of interest for theoretical purposes (see, e.g., (Allender, 1992) and (Li and Vitanyi, 1993)). However, it seems that nobody implemented it for experimental applications, perhaps in fear of the ominous "constant factor" which may be large. To my knowledge, general universal search was implemented for the first time during the project that led to this paper.
Reference: <author> Linsker, R. </author> <year> (1988). </year> <title> Self-organization in a perceptual network. </title> <journal> IEEE Computer, </journal> <volume> 21 </volume> <pages> 105-117. </pages>
Reference: <author> Maass, W. </author> <year> (1994). </year> <title> Perspectives of current research about the complexity of learning on neural nets. </title> <editor> In Roychowdhury, V. P., Siu, K. Y., and Orlitsky, A., editors, </editor> <booktitle> Theoretical Advances in Neural Computation and Learning. </booktitle> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> MacKay, D. J. C. </author> <year> (1992). </year> <title> A practical Bayesian framework for backprop networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 448-472. </pages>
Reference-contexts: MacKay evaluates hyper-parameters (such as weight-decay rates) with respect to their probabilities of generating the observed data <ref> (MacKay, 1992) </ref>. Estimates of the probabilities are computed on the basis of Gaussian assumptions. * Optimal Brain Surgeon. Hassibi and Storck (1993) use second order information to obtain "simple" nets by pruning weights whose influence on the error is minimal, and changing other weights to compensate.
Reference: <author> Martin-Lof, P. </author> <year> (1966). </year> <title> The definition of random sequences. </title> <journal> Information and Control, </journal> <volume> 9 </volume> <pages> 602-619. </pages>
Reference: <author> Milosavljevic, A. and Jurka, J. </author> <year> (1993). </year> <title> Discovery by minimal length encoding: A case study in molecular evolution. </title> <journal> Machine Learning, </journal> <volume> 12 </volume> <pages> 96-87. </pages>
Reference: <author> Moody, J. E. </author> <year> (1992). </year> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 847-854. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mozer, M. C. and Smolensky, P. </author> <year> (1989). </year> <title> Skeletonization: A technique for trimming the fat from a network via relevance assessment. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <pages> pages 107-115. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Nowlan, S. J. and Hinton, G. E. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 173-193. </pages>
Reference: <author> Parker, D. B. </author> <year> (1985). </year> <title> Learning-logic. </title> <type> Technical Report TR-47, </type> <institution> Center for Comp. Research in Economics and Management Sci., MIT. </institution>
Reference: <author> Paul, W. and Solomonoff, R. J. </author> <year> (1991). </year> <title> Autonomous theory building systems. </title> <type> Manuscript, </type> <note> revised 1994. </note>
Reference: <author> Pearlmutter, B. A. and Rosenfeld, R. </author> <year> (1991). </year> <title> Chaitin-Kolmogorov complexity and generalization in neural networks. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 925-931. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pednault, E. P. D. </author> <year> (1989). </year> <title> Some experiments in applying inductive inference principles to surface reconstruction. </title> <booktitle> In 11th IJCAI, </booktitle> <pages> pages 1603-1609. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. and Rivest, R. L. </author> <year> (1989). </year> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248. </pages>
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference: <author> Rissanen, J. </author> <year> (1983). </year> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> The Annals of Statistics, </journal> <volume> 11(2) </volume> <pages> 416-431. </pages>
Reference: <author> Rissanen, J. </author> <year> (1986). </year> <title> Stochastic complexity and modeling. </title> <journal> The Annals of Statistics, </journal> <volume> 14(3) </volume> <month> 1080-1100. </month> <title> Nets with low Kolmogorov complexity 22 Rumelhart, </title> <editor> D. E., Hinton, G. E., and Williams, R. J. </editor> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <booktitle> In Parallel Distributed Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178. </pages>
Reference: <author> Schmidhuber, J. </author> <year> (1989). </year> <title> The Neural Bucket Brigade: A local learning algorithm for dynamic feedforward and recurrent networks. </title> <journal> Connection Science, </journal> <volume> 1(4) </volume> <pages> 403-412. </pages>
Reference: <author> Schmidhuber, J. </author> <year> (1992a). </year> <title> Learning complex, extended sequences using the principle of history compression. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 234-242. </pages>
Reference-contexts: Various "neural" methods for compressing input data are known. (Schmidhuber, 1992b) describes a "neural" method designed to generate factorial codes. (Atick et al., 1992) focuses on visual inputs. <ref> (Schmidhuber, 1992a) </ref> focuses on loss-free sequence compression. (Becker, 1991) provides numerous additional references. There are also numerous heuristic constructive methods, where network size grows in case of underfitting the training data.
Reference: <author> Schmidhuber, J. </author> <year> (1992b). </year> <title> Learning factorial codes by predictability minimization. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 863-879. </pages>
Reference-contexts: Various "neural" methods for compressing input data are known. <ref> (Schmidhuber, 1992b) </ref> describes a "neural" method designed to generate factorial codes. (Atick et al., 1992) focuses on visual inputs. (Schmidhuber, 1992a) focuses on loss-free sequence compression. (Becker, 1991) provides numerous additional references. There are also numerous heuristic constructive methods, where network size grows in case of underfitting the training data.
Reference: <author> Schmidhuber, J. </author> <year> (1993a). </year> <title> On decreasing the ratio between learning complexity and number of time-varying variables in fully recurrent nets. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 460-463. </pages> <publisher> Springer. </publisher>
Reference-contexts: In an earlier work, it was shown (in a different context) how neural nets may "talk about their own weights in terms of activations" and modify their own weight matrix <ref> (Schmidhuber, 1993a, 1993b) </ref>. Such self-modifying capabilities can be used to form the basis of a universal set of primitives. Nets with low Kolmogorov complexity 10 non-deterministic way, of course). 4. Probabilistic setting.
Reference: <author> Schmidhuber, J. </author> <year> (1993b). </year> <title> A self-referential weight matrix. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 446-451. </pages> <publisher> Springer. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1994). </year> <title> Discovering problem solutions with low Kolmogorov complexity and high generalization capability. </title> <type> Technical Report FKI-194-94, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen. </institution> <note> Short version in A. </note> <editor> Prieditis and S. Russell, eds., </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> pages 488-496, </pages> <address> San Francisco, CA, </address> <year> 1995. </year>
Reference-contexts: complexity are indeed of interest for the study of neural network generalization and for machine learning purposes in general, (2) to encourage neural net researchers to study this theory, and (3) to point to some limitations of the current state of the art and to important open problems (see also <ref> (Schmidhuber, 1994) </ref>). Outline. <p> Previous suggestions. The original universal search procedure as formulated by Levin is not designed for incremental learning situations. Suggestions for extending universal search to deal with incremental learning were made in (Solomonoff, 1986; Paul and Solomonoff, 1991), and also in <ref> (Schmidhuber, 1994) </ref> | the first paper presenting simulations based on incremental extensions. Problems with previous suggestions. In realistic, unknown environments, each event / action / trial / learning process occurring in the life of a learning system may affect performance and learning processes at any later time.
Reference: <author> Schmidhuber, J. </author> <year> (1995). </year> <title> Low-complexity art. Accepted by Leonardo, </title> <journal> Journal of the International Society for the Arts, Sciences, and Technology. </journal>
Reference-contexts: Meanwhile, the theory of Kolmogorov complexity has split into many subfields. An excellent overview and many additional details on the history are given in Li and Vitanyi's book (1993) and also in (Cover et al., 1989). See <ref> (Schmidhuber, 1995) </ref> for an application to fine arts.
Reference: <author> Schmidhuber, J. </author> <year> (1996). </year> <title> A general method for incremental self-improvement and multi-agent learning in unrestricted environments. </title> <editor> In Yao, X., editor, </editor> <booktitle> Evolutionary Computation: Theory and Applications. </booktitle> <publisher> Scientific Publ. Co., Singapore. In press. </publisher>
Reference-contexts: Obviously, results with low Levin complexity are preferred over results with high Levin complexity. (In an alternative implementation, the original universal search algorithm was used to systematically generate all solution candidates in order of their Levin complexities. See also Heil's MSc thesis at Technische Universitat Munchen (1995), and <ref> (Wiering and Schmidhuber, 1996) </ref>). Used primitives. The instruction numbers and the semantics of the primitives used in the experiments are listed below. <p> They show that Levin search can be useful not only for supervised learning, but also for more general reinforcement learning, even for difficult, partially observable Markov decision problems (POMDPs). For example, experiments in <ref> (Wiering and Schmidhuber, 1996) </ref> show that LS can solve partially observable mazes (POMs) involving more states and obstacles than certain POMs solved by various previous authors. LS also can solve tasks that cannot be solved by standard reinforcement learning techniques such as Q-learning (Watkins, 1989). <p> Different Q-learning variants were tried but failed to solve the problem. In contrast, LS was indeed able to solve the POMDP by discovering a program computing the shortest path <ref> (details in Wiering and Schmidhuber, 1996) </ref>. Numerous additional experiments were performed by Norbert Jankowski and Stefan Heil at Technische Universitat Munchen. <p> In fact, a reasonable performance criterion for such general (but typical) situations was missing. Performance criterion for incremental learning. Recent work <ref> (Schmidhuber, 1996) </ref> defined such a criterion: the "reward acceleration criterion" (RAC). Although not essential for the message of the current paper, a brief review of the main concepts will follow. Suppose a reinforcement learner's life in an unknown environment E lasts from time 0 to unknown time T . <p> Numerous experiments in the papers mentioned above show its practical feasibility. For instance, in <ref> (Wiering and Schmidhuber, 1996) </ref>, A includes an extension of Levin search for generating the PMPs.
Reference: <author> Schmidhuber, J., Zhao, J., and Wiering, M. </author> <year> (1996). </year> <title> Simple principles of metalearning. </title> <type> Technical Report IDSIA-69-96, </type> <institution> IDSIA. </institution>
Reference-contexts: Obviously, results with low Levin complexity are preferred over results with high Levin complexity. (In an alternative implementation, the original universal search algorithm was used to systematically generate all solution candidates in order of their Levin complexities. See also Heil's MSc thesis at Technische Universitat Munchen (1995), and <ref> (Wiering and Schmidhuber, 1996) </ref>). Used primitives. The instruction numbers and the semantics of the primitives used in the experiments are listed below. <p> They show that Levin search can be useful not only for supervised learning, but also for more general reinforcement learning, even for difficult, partially observable Markov decision problems (POMDPs). For example, experiments in <ref> (Wiering and Schmidhuber, 1996) </ref> show that LS can solve partially observable mazes (POMs) involving more states and obstacles than certain POMs solved by various previous authors. LS also can solve tasks that cannot be solved by standard reinforcement learning techniques such as Q-learning (Watkins, 1989). <p> Different Q-learning variants were tried but failed to solve the problem. In contrast, LS was indeed able to solve the POMDP by discovering a program computing the shortest path <ref> (details in Wiering and Schmidhuber, 1996) </ref>. Numerous additional experiments were performed by Norbert Jankowski and Stefan Heil at Technische Universitat Munchen. <p> In fact, a reasonable performance criterion for such general (but typical) situations was missing. Performance criterion for incremental learning. Recent work <ref> (Schmidhuber, 1996) </ref> defined such a criterion: the "reward acceleration criterion" (RAC). Although not essential for the message of the current paper, a brief review of the main concepts will follow. Suppose a reinforcement learner's life in an unknown environment E lasts from time 0 to unknown time T . <p> Numerous experiments in the papers mentioned above show its practical feasibility. For instance, in <ref> (Wiering and Schmidhuber, 1996) </ref>, A includes an extension of Levin search for generating the PMPs.
Reference: <author> Schnorr, C. P. </author> <year> (1971). </year> <title> A unified approach to the definition of random sequences. </title> <journal> Mathematical Systems Theory, </journal> <volume> 5 </volume> <pages> 246-258. </pages>
Reference: <author> Shannon, C. E. </author> <year> (1948). </year> <title> A mathematical theory of communication (parts I and II). </title> <journal> Bell System Technical Journal, XXVII:379-423. </journal>
Reference-contexts: The weights are taken to be generated by mixtures of Gaussians. The fewer the number of Gaussians and the closer some weight is to the center of some Gaussian, the higher its probability, and the fewer bits are needed to encode it (according to classical information theory <ref> (Shannon, 1948) </ref>). * Bayesian strategies for backprop nets. MacKay evaluates hyper-parameters (such as weight-decay rates) with respect to their probabilities of generating the observed data (MacKay, 1992). Estimates of the probabilities are computed on the basis of Gaussian assumptions. * Optimal Brain Surgeon.
Reference: <author> Solomonoff, R. </author> <year> (1964). </year> <title> A formal theory of inductive inference. Part I. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22. </pages>
Reference: <author> Solomonoff, R. </author> <year> (1986). </year> <title> An application of algorithmic probability to problems in artificial intelligence. </title> <editor> In Kanal, L. N. and Lemmer, J. F., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 473-491. </pages> <publisher> Elsevier Science Publishers. </publisher>
Reference-contexts: Both Solomonoff and Kolmogorov observed K's machine independence. Today, even Solomonoff himself refers to K as "Kolmogorov complexity," e.g., <ref> (Solomonoff, 1986) </ref>. In 1969, G. J. Chaitin independently also published the essential concepts (Chaitin, 1969) (some hints were already provided at the end of his 1966 paper). Important related early work is described in (Martin-Lof, 1966; Gacs, 1974; Nets with low Kolmogorov complexity 6 Schnorr, 1971). Apparently, L. A. <p> See (Schmidhuber, 1995) for an application to fine arts. This section was partly inspired by presentations found in (Chaitin, 1987), (Li and Vitanyi, 1993), and <ref> (Solomonoff, 1986) </ref>. 3 PROBABILISTIC SEARCH FOR USEFUL SELF-SIZING PROGRAMS WITH LOW LEVIN COMPLEXITY Levin's universal search algorithm was considered of interest for theoretical purposes (see, e.g., (Allender, 1992) and (Li and Vitanyi, 1993)).
Reference: <author> Utgoff, P. </author> <year> (1986). </year> <title> Shift of bias for inductive concept learning. </title> <editor> In Michalski, R., Carbonell, J., and Mitchell, T., editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> volume 2, </volume> <pages> pages 163-190. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA. </address>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142. </pages>
Reference-contexts: This may be viewed as the reason why certain worst-case results of PAC-learning theory <ref> (initiated by Valiant, 1984) </ref> appear discouraging. Similarly for problem solving in general: a "problem" is usually defined by a search space of solution candidates, and a computable criterion for the solution. Most solutions to problems from the set of all possible well-defined problems are algorithmically complex (random, incompressible).
Reference: <author> Vapnik, V. </author> <year> (1992). </year> <title> Principles of risk minimization for learning theory. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 831-838. </pages> <address> San Mateo, CA: </address> <note> Morgan Kaufmann. Nets with low Kolmogorov complexity 23 Wallace, </note> <author> C. S. and Boulton, D. M. </author> <year> (1968). </year> <title> An information theoretic measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11(2) </volume> <pages> 185-194. </pages>
Reference: <author> Watanabe, O. </author> <year> (1992). </year> <title> Kolmogorov complexity and computational complexity. </title> <booktitle> EATCS Monographs on Theoretical Computer Science, </booktitle> <publisher> Springer. </publisher>
Reference-contexts: Pippenger. Levin introduced Kt complexity and the universal optimal search algorithm (see, e.g., (Levin, 1973b) and (Levin, 1984), where related ideas are attributed to Adleman see also (Adleman, 1979)). Other generalizations of Kolmogorov complexity have been proposed, e.g., (Hartmanis, 1983), but for more details see the contributions in <ref> (Watanabe, 1992) </ref>. Easily computable approximations of the MDL principle were formulated by Wallace and Boulton (1968) and Rissanen (1978, 1983, 1986).
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College London. </institution>
Reference-contexts: For example, experiments in (Wiering and Schmidhuber, 1996) show that LS can solve partially observable mazes (POMs) involving more states and obstacles than certain POMs solved by various previous authors. LS also can solve tasks that cannot be solved by standard reinforcement learning techniques such as Q-learning <ref> (Watkins, 1989) </ref>. Nets with low Kolmogorov complexity 16 For instance, one simulation involves a 39fi38-maze with a single start position (S) and a single goal position (G). There is a a low-complexity shortest path from S to G involving 127 steps.
Reference: <author> Weigend, A. S., Huberman, B. A., and Rumelhart, D. E. </author> <year> (1990). </year> <title> Predicting the future: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 193-209. </pages>
Reference: <author> Werbos, P. J. </author> <year> (1974). </year> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University. </institution>
Reference: <author> Wiering, M. and Schmidhuber, J. </author> <year> (1996). </year> <title> Solving POMDPs with Levin search and EIRA. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 534-542. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Obviously, results with low Levin complexity are preferred over results with high Levin complexity. (In an alternative implementation, the original universal search algorithm was used to systematically generate all solution candidates in order of their Levin complexities. See also Heil's MSc thesis at Technische Universitat Munchen (1995), and <ref> (Wiering and Schmidhuber, 1996) </ref>). Used primitives. The instruction numbers and the semantics of the primitives used in the experiments are listed below. <p> They show that Levin search can be useful not only for supervised learning, but also for more general reinforcement learning, even for difficult, partially observable Markov decision problems (POMDPs). For example, experiments in <ref> (Wiering and Schmidhuber, 1996) </ref> show that LS can solve partially observable mazes (POMs) involving more states and obstacles than certain POMs solved by various previous authors. LS also can solve tasks that cannot be solved by standard reinforcement learning techniques such as Q-learning (Watkins, 1989). <p> Different Q-learning variants were tried but failed to solve the problem. In contrast, LS was indeed able to solve the POMDP by discovering a program computing the shortest path <ref> (details in Wiering and Schmidhuber, 1996) </ref>. Numerous additional experiments were performed by Norbert Jankowski and Stefan Heil at Technische Universitat Munchen. <p> Numerous experiments in the papers mentioned above show its practical feasibility. For instance, in <ref> (Wiering and Schmidhuber, 1996) </ref>, A includes an extension of Levin search for generating the PMPs.
Reference: <author> Williams, R. J. </author> <year> (1988). </year> <title> Toward a theory of reinforcement-learning connectionist systems. </title> <type> Technical Report NU-CCS-88-3, </type> <institution> College of Comp. Sci., Northeastern University, </institution> <address> Boston, MA. </address>
Reference: <author> Wolpert, D. H. </author> <year> (1993). </year> <type> Technical Report SFI TR 93-03-016, </type> <institution> Santa Fe Institute, </institution> <address> NM 87501. </address>
Reference: <author> Zhao, J. and Schmidhuber, J. </author> <year> (1996). </year> <title> Incremental self-improvement for life-time multi-agent reinforcement learning. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, </address> <pages> pages 516-525. </pages> <publisher> MIT Press, Bradford Books. </publisher>
Reference: <author> Zvonkin, A. K. and Levin, L. A. </author> <year> (1970). </year> <title> The complexity of finite objects and the algorithmic concepts of information and randomness. </title> <journal> Russian Math. Surveys, </journal> <volume> 25(6) </volume> <pages> 83-124. </pages>
Reference-contexts: Such approximations build the basis of most | if not all | current machine learning applications, e.g., (Quinlan and Rivest, 1989; Gao and Li, 1989; Milosavljevic and Jurka, 1993; Pednault, 1989). Barzdin, referred to in <ref> (Zvonkin and Levin, 1970) </ref> (see also Barzdin, 1988), related Kolmogorov complexity to a variant of Godel's incompleteness theorem, a subject which became a central theme of Chaitin's research (Chaitin, 1987). Meanwhile, the theory of Kolmogorov complexity has split into many subfields.
References-found: 83

