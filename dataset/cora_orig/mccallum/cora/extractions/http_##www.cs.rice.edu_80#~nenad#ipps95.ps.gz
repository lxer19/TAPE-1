URL: http://www.cs.rice.edu:80/~nenad/ipps95.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~nenad/papers.html
Root-URL: 
Email: ken@rice.edu nenad@rice.edu  
Title: Combining Dependence and Data-Flow Analyses to Optimize Communication  
Author: Ken Kennedy Nenad Nedeljkovic 
Affiliation: Department of Computer Science, Rice University  
Abstract: Reducing communication overhead is crucial for improving the performance of programs on distributed-memory machines. Compilers for data-parallel languages must perform communication optimizations in order to minimize this overhead. In this paper, we show how to combine dependence analysis, traditionally used to optimize regular communication, and a data-flow analysis method originally developed to improve placement of irregular communication. Our approach allows us to perform more extensive optimizations | message vectorization, elimination of redundant messages, and overlapping communication with computation. We also present preliminary experimental results that demonstrate the benefits of the proposed method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Since their global read and write are monolithic operations, they do not try to overlap communication with computation. Communication optimization described by Amaras-inghe and Lam is based on the last write tree representation <ref> [1] </ref>. They do not handle arbitrary control flow, e.g., loops inside conditional statements, and optimize communication only within a single loop nest. Gong et al. describe a data-flow analysis technique that unifies multiple communication optimizations [3], but they only handle singly nested loops and one-dimensional arrays.
Reference: [2] <author> H. Berryman and J. Saltz. </author> <title> A manual for PARTI runtime primitives. </title> <type> ICASE Interim Report 13, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: For irregular problems, determining which of these references require communication is done at run time through calls to PARTI/CHAOS library routines (Gather) <ref> [2] </ref>. However, in regular codes it is often possible to extract some static information and determine which references are non-local at compile time. When comparing two portions of the same array, the compiler must assume the most conservative facts if these portions are accessed via irregular subscripts.
Reference: [3] <author> C.Gong, R.Gupta, and R.Melhem. </author> <title> Compilation techniques for optimizing communication on distributed-memory systems. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The combined approach enables more extensive optimizations than either of the two components would do on its own. There have been several attempts to use data-flow analysis in optimizing communication <ref> [4, 3, 5] </ref>. Most have focused on extending the existing methods to work with array section descriptors. <p> Using this information we could split the array portion communicated into parts corresponding to array portions in reaching communications. (This is similar to splitting in <ref> [3] </ref>, but we only do it when initializing the data-flow framework.) Since we are only interested in control-flow reachability without taking array kill information into account, this ap proach does not require array data-flow analysis. <p> They do not handle arbitrary control flow, e.g., loops inside conditional statements, and optimize communication only within a single loop nest. Gong et al. describe a data-flow analysis technique that unifies multiple communication optimizations <ref> [3] </ref>, but they only handle singly nested loops and one-dimensional arrays. Although they try to overlap communication with computation, their algorithm only produces the placement of Send statements; if care is not taken in placing Recv statements this can lead to unbalanced communication. <p> However, they do not present any experimental data to show if the cost of their analysis would be justified in practice by the need for extra precision. Much like in <ref> [3] </ref>, they only find the placement of Sends, and not Recvs, facing the same problem as discussed above. 6 Conclusions We have presented a method for optimizing communication when compiling HPF-like languages.
Reference: [4] <author> E. Granston and A. Veidenbaum. </author> <title> Detecting redundant ac cesses to array data. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: The combined approach enables more extensive optimizations than either of the two components would do on its own. There have been several attempts to use data-flow analysis in optimizing communication <ref> [4, 3, 5] </ref>. Most have focused on extending the existing methods to work with array section descriptors. <p> Granston and Veidenbaum use flow analysis of array regions to detect and eliminate redundant global memory accesses <ref> [4] </ref>. Since their global read and write are monolithic operations, they do not try to overlap communication with computation. Communication optimization described by Amaras-inghe and Lam is based on the last write tree representation [1].
Reference: [5] <author> M. Gupta, E. Schonberg, and H. Srinivasan. </author> <title> A unified data flow framework for optimizing communication. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The combined approach enables more extensive optimizations than either of the two components would do on its own. There have been several attempts to use data-flow analysis in optimizing communication <ref> [4, 3, 5] </ref>. Most have focused on extending the existing methods to work with array section descriptors. <p> Although they try to overlap communication with computation, their algorithm only produces the placement of Send statements; if care is not taken in placing Recv statements this can lead to unbalanced communication. Gupta et al. show how partial redundancy elimination can be applied to available section descriptors <ref> [5] </ref>. Since we opt for the efficiency of bit-vector flow analysis, it is possible that their method will be more precise. However, they do not present any experimental data to show if the cost of their analysis would be justified in practice by the need for extra precision.
Reference: [6] <author> R. v. Hanxleden. </author> <title> Handling irregular problems with Fortran D | A preliminary report. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: i = 2, 40, 2 enddo Read Send/Recv a (2:40) 1 fl do i = 1, 39 enddo Read Send/Recv b (1:10) 2 fl do i = 1, 39 Read Send/Recv c (i+1) 3 fl c (i) = c (i) + c (i+1) * b (j) enddo enddo end gram <ref> [6] </ref>. For irregular problems, determining which of these references require communication is done at run time through calls to PARTI/CHAOS library routines (Gather) [2]. However, in regular codes it is often possible to extract some static information and determine which references are non-local at compile time. <p> For example, all Comm sets shown in Figure 1, would go into TAKE init sets for their corresponding locations in the program. In contrast to the original analysis of irregular problems <ref> [6] </ref>, where TAKE init sets contain all referenced portions of distributed arrays, our consumption sets will be smaller whenever the elements to be communicated can be determined at compile time. More precise consumption sets will often yield more optimization opportunities, as shown by the example in Figure 2. <p> left side of Figure 3 would also be split (into 1 c (11) 2 ! 1), and the part Send 2 c (21) ! 1, would be moved all the way to the beginning of the program. 4 Preliminary experience We have modified the existing implementation of the Give-N-Take framework <ref> [6] </ref> to include the support for array portions that can be represented with RSDs. Although the framework can now take advantage of compile-time knowledge about array elements accessed, integration with dependence analysis is not yet complete.
Reference: [7] <author> R. v. Hanxleden and K. Kennedy. </author> <title> Give-N-Take | A bal anced code placement framework. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: On the other hand, communication due to irregular array accesses (those where subscripts are not linear combinations of loop index variables) is optimized via the Give-N-Take data-flow framework <ref> [7] </ref>. Although this framework provides global analysis on the control flow graph, it treats arrays as indivisible units and does not exploit optimization opportunities that come from the compile-time knowledge about array references. <p> In these cases it is hardly possible to extract significant compile-time knowledge about elements accessed, and dependence analysis is of little use. Instead, the Fortran D compiler's analysis is based on the Give-N-Take code placement framework <ref> [7] </ref>.
Reference: [8] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of inter procedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The Fortran D compiler prototype uses regular section descriptors (RSDs) to represent the sets of array elements that need to be communicated <ref> [8] </ref>. These RSDs are augmented to handle simple forms of boundary conditions [11]. While this representation is sufficient for one-dimensional block and cyclic distributions currently supported by the compiler, a more general representation is necessary for communication sets that arise with block-cyclic and multi-dimensional distributions.
Reference: [9] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Data-parallel languages, such as Fortran D <ref> [9] </ref> and High Performance Fortran (HPF) [10], are designed to facilitate writing of portable programs for multi-computers. They provide shared address space and means for the programmer to specify how data should be distributed among processors.
Reference: [10] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Data-parallel languages, such as Fortran D [9] and High Performance Fortran (HPF) <ref> [10] </ref>, are designed to facilitate writing of portable programs for multi-computers. They provide shared address space and means for the programmer to specify how data should be distributed among processors.
Reference: [11] <author> C.-W. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Therefore, it is very important to reduce the number of messages inserted by the compiler. In the Fortran D compiler prototype developed at Rice University, communication resulting from regular array references (those where subscripts are linear combinations of loop index variables) is optimized based primarily on dependence analysis <ref> [11] </ref>. The effectiveness of optimizations, the most important of which are message vectorization and coalescing, is limited by the fact that most of the analysis is performed for a single loop nest at a time, and little is done to optimize communication across arbitrary control flow. <p> It uses the level of loop-carried true dependences to decide if the communication can be hoisted out of the loop, in which case multiple single-element messages are replaced with a single vectorized message <ref> [11] </ref>. In order to avoid redundant communication, the compiler applies message coalescing, combining messages for different references to the same array. In the absence of data-flow analysis, this optimization is performed only within a single loop nest, and many opportunities for eliminating redundant messages are missed. <p> The Fortran D compiler prototype uses regular section descriptors (RSDs) to represent the sets of array elements that need to be communicated [8]. These RSDs are augmented to handle simple forms of boundary conditions <ref> [11] </ref>. While this representation is sufficient for one-dimensional block and cyclic distributions currently supported by the compiler, a more general representation is necessary for communication sets that arise with block-cyclic and multi-dimensional distributions. <p> However, we were able to run an experiment to measure the potential benefits of our approach. We used Livermore 18 explicit hydrodynamics kernel and Shallow weather prediction program written by Paul Swartztrauber (NCAR). Both benchmarks are highly data-parallel and significant speedups were reported with the existing Fortran D compiler <ref> [11] </ref>. However, hand-coded versions were still up to 25% faster, with most of the difference coming from eliminating redundant messages and increasing the overlap of communication with computation | exactly the optimizations that we propose to automate.
References-found: 11

