URL: http://www.cis.udel.edu/~case/papers/mkamst-tr.ps
Refering-URL: http://www.cis.udel.edu/~case/colt.html
Root-URL: http://www.cis.udel.edu
Title: The Power of Vacillation in Language Learning programs for vacillatory criteria if one is willing
Author: John Case def 
Note: 2 the mistakes from final  partly  publication.  
Address: 19716, USA  
Affiliation: Department of Computer and Information Sciences University of Delaware Newark, DE  
Abstract: Some extensions are considered of Gold's influential model of language learning by machine from positive data. Studied are criteria of successful learning featuring convergence in the limit to vacillation between several alternative correct grammars. The main theorem of this paper is that there are classes of languages that can be learned if convergence in the limit to up to (n+1) exactly correct grammars is allowed but which cannot be learned if convergence in the limit is to no more than n grammars, where the no more than n grammars can each make finitely many mistakes. This contrasts sharply with results of Barzdin and Podnieks and, later, Case and Smith, for learnability from both positive and negative data. A subset principle from a 1980 paper of Angluin is extended to the vacillatory and other criteria of this paper. This principle, provides a necessary condition for circumventing overgeneralization in learning from positive data. It is applied to prove another theorem to the effect that one can optimally eliminate 1 Child language learning may be sensitive to the order or timing of data presentation. It is shown, though, that, for the vacillatory success criteria of this paper, there is no loss of learning power for machines which are insensitive to order in several ways simultaneously. For example, , for each language L on which, for some ordering of the positive data about L, M converges in the limit to a finite set of grammars, there is a finite set of grammars D (of cardinality n) such that M converges to a subset of this same D for each ordering of the positive data for L. The most difficult to prove theorem in the paper implies that machines which are simultaneously partly set-driven and weakly n-ary order independent do not lose learning power for converging in the limit to up to n grammars. Several variants of this theorem are obtained by modifying its proof, and some of these variants have application in this and other papers. Along the way it is also shown, for the vacillatory criteria, that learning power is not increased if one restricts the sequence of positive data presentation to be computable. Some of these results are non-trivial lifts of prior work for the n = 1 case due to the Blums; Wiehagen; Osherson, Stob and Weinstein; Schafer; fl Email: `case@cis.udel.edu'. URL: `http://www.eecis.udel.edu/~case'. The research was supported in part by NSF grant # CCR-8713846. The author thanks the University of Rochester's Computer Science Department for support and for providing an excellent working environment academic year 1987-8 during which some of the work on the present paper was completed. The author is also grateful to anonymous referees of a previous draft and others mentioned in the text for many helpful comments. This paper is an expansion with corrections of both the conference article [Cas88] and the subsequent technical report [Cas92a], and a variant is being considered for possible journal and Fulk.
Abstract-found: 1
Intro-found: 1
Reference: [Ang80] <author> D. Angluin. </author> <title> Inductive inference of formal languages from positive data. </title> <journal> Information and Control, </journal> <volume> 45 </volume> <pages> 117-135, </pages> <year> 1980. </year>
Reference-contexts: Angluin <ref> [Ang80, Ang82] </ref> presents other classes L natural from the perspective of formal language theory such that some M identifies each language in L. <p> We say, then, that the restriction to recursive texts is circumvented. 3 Angluin, in her seminal paper <ref> [Ang80] </ref>, presents a severe constraint on TxtFex 1 -identification of classes of languages: the subset principle. <p> Intuitively, this necessary condition circumvents overgeneralization in learning from positive data <ref> [Ang80, Ber85] </ref>. Theorem 3 in Section 4 below generalizes the subset principle to the criteria of success TxtFex a b -identification and TxtBc a -identification, where the a in TxtBc a -identification allows each of the infinitely many final grammars converged to to have up to a anomalies. <p> Therefore, F [ T 0 ; T 0 ] 6 P , a contradiction. The I = Fex 0 1 case of the following Theorem is from Angluin's <ref> [Ang80] </ref>. She calls the finite sets D featured tell tales. The theorem witnesses a severe constraint called the subset principle on learning from positive data. See [Ang80, Ber85] regarding the importance of the subset princi ple for circumventing overgeneralization in learning languages from positive data. <p> The I = Fex 0 1 case of the following Theorem is from Angluin's [Ang80]. She calls the finite sets D featured tell tales. The theorem witnesses a severe constraint called the subset principle on learning from positive data. See <ref> [Ang80, Ber85] </ref> regarding the importance of the subset princi ple for circumventing overgeneralization in learning languages from positive data. See [KLHM93, Wex93] for discussion regarding the possible connection between this subset principle and a more traditionally linguistically oriented one in [MW87]. We let 2fl = fl. <p> Then A = 2a C. Proof of Theorem 3. Suppose the hypotheses. For each T for L, choose a suitably large T T that (8t T j t T )[W F (t) = a L]: P = T for L 13 This complements a related characterization in <ref> [Ang80] </ref> of the uniformly decidable classes of recursive languages in TxtFex 0 1 . [BCJ96] provides a related characterization of the uniformly decidable classes of recursive languages in TxtFex fl 1 . [OSW86c, Page 30] characterizes learning by an agent which is not necessarily algorithmic. [Muk92, LZ92] contain characterizations of uniformly <p> However, Wiehagen [Wie77] presents a class of r.e. languages in TxtFex 0 1 which contains a finite variant of each r.e. language. Wiehagen's class is obviously quite hefty. Angluin presents examples natural from the perspective of formal language theory that also are in TxtFex 0 1 <ref> [Ang80, Ang82] </ref>. All these classes in TxtFex 0 1 (of course) satisfy the subset principle (of Theorem 3), and, in particular, they are not closed under finite sublanguages as is the class of regular languages.
Reference: [Ang82] <author> D. Angluin. </author> <title> Inference of reversible languages. </title> <journal> Journal of the ACM, </journal> <volume> 29 </volume> <pages> 741-765, </pages> <year> 1982. </year>
Reference-contexts: Angluin <ref> [Ang80, Ang82] </ref> presents other classes L natural from the perspective of formal language theory such that some M identifies each language in L. <p> However, Wiehagen [Wie77] presents a class of r.e. languages in TxtFex 0 1 which contains a finite variant of each r.e. language. Wiehagen's class is obviously quite hefty. Angluin presents examples natural from the perspective of formal language theory that also are in TxtFex 0 1 <ref> [Ang80, Ang82] </ref>. All these classes in TxtFex 0 1 (of course) satisfy the subset principle (of Theorem 3), and, in particular, they are not closed under finite sublanguages as is the class of regular languages.
Reference: [BB75] <author> L. Blum and M. Blum. </author> <title> Toward a mathematical theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 28 </volume> <pages> 125-155, </pages> <year> 1975. </year>
Reference-contexts: suitably clever machine M might be able to exploit the recursiveness of texts to learn larger classes of languages than any machine required to succeed on arbitrary texts, but Corollary 1 in Section 3 below implies that this is not the case (generalizing the b = 1 case essentially from <ref> [Wie77, BB75] </ref>). We say, then, that the restriction to recursive texts is circumvented. 3 Angluin, in her seminal paper [Ang80], presents a severe constraint on TxtFex 1 -identification of classes of languages: the subset principle. <p> Theorem 4 is the hardest theorem herein to prove, and the other theorems in Section 5 are proved by modifications, and/or simplifications of the proof of Theorem 4. Some of the theorems in Section 5 generalize predecessors for TxtFex 0 1 -identification <ref> [BB75, WC80, SR84, Ful85, OSW86b, Ful90a] </ref> but are much harder to prove. Some of the theorems in Section 5 are applied in the present paper and in other papers. <p> It is, then, interesting and important to compare learning power where success is required on all texts with the cases where it is only required on all recursive texts. Wiehagen [Wie77] essentially notes that RecTxtFex 0 1 = TxtFex 0 1 (a related result was first proved in <ref> [BB75] </ref>), and [CL82] essentially observes that (8a)[RecTxtFex a 1 = TxtFex a 1 ]. We have, more generally, Corollary 1 1. (8a; b)[RecTxtFex a b = TxtFex a b ]. 2. (8a; b)[RecTxtMfex a b = TxtMfex a b ]. <p> In this way results about learning programs for functions can be interpreted as results about finding predictive explanations for phenomena | as results about scientific induction. For more on this see <ref> [BB75, CS83, CJS92, BCJS94, CJNM94, LW94] </ref>. Re the names of the learning criteria studied in the present paper, originally [CS83] `Ex' stood for `explanatory', `Fex' stood for `finitely explanatory', and Bc for `behaviorally correct'. 11 Proof. <p> Definition 14 in L , content () L. Just below is a variant of a fundamental lemma from [OW82a] convenient for this paper. An original, not so general version of this lemma is from <ref> [BB75] </ref> (see also [OSW83, OSW86b]). Variations on its proof will appear in other proofs. 12 Lemma 1 Suppose L 2 E. Suppose, for each text T for L, an arbitrary T T is chosen. <p> Then: L recursively b-stabilizes F , (8 recursive T for L)(9D j card (D) b)[F (T )+ = D]: The following lemma, useful to this paper, combines the topological with the algorithmic. It generalizes predecessors from <ref> [BB75, Ful85, OSW86b] </ref>. Lemma 3 Suppose L is r.e. and recursively b-stabilizes F. Then (8 in L)(9D j card (D) b)(9t j t in L)(8t 0 t j t 0 in L)[F (t 0 ) 2 D]: (7) Proof. Suppose the hypothesis on L. <p> We consider herein some mathematical versions of this question. Several mathematical definitions have been given for various different notions of insensitivity to order, essentially for the case of TxtFex 0 1 -identification <ref> [BB75, WC80, SR84, Ful85, OSW86b, Ful90a] </ref>. We extend these definitions of insensitive or restricted learning functions naturally to the context of the vacillatory learning criteria of the present paper, 15 and we investigate the interesting mathematical questions of whether learning functions with these insensitivities or restrictions thereby lose learning power. <p> Blum <ref> [BB75] </ref>, essentially show that order independent learning functions can TxtFex 0 1 -identify the same classes of languages that unrestricted learning functions can. <p> The outputting of kt k upon witnessing an instability in the choice of hD 1 ; 1 i is essentially a combinatorial device from <ref> [BB75] </ref>, and it plays the role pad did in the proof of Theorem 4 above, similarly controlling thrashing in the choice of hD 1 ; 1 i when some T for L stabilizes F 0 .
Reference: [BC93] <author> G. Baliga and J. </author> <title> Case. Learnability: Admissible, co-finite, and hypersimple sets. </title> <booktitle> In Proceedings of the 20th International Colloquium on Automata, Languages and Programming, volume 700 of Lecture Notes in Computer Science, </booktitle> <pages> pages 289-300. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Lund, Sweden, </address> <month> July </month> <year> 1993. </year> <note> Journal version in press for Journal of Computer and System Sciences, </note> <year> 1996. </year>
Reference-contexts: Let L = f total f j ' f (0) = m+1 f g. Clearly L 2 TxtFex m+1 1 . Also, L 2 TxtFex m fl together with Theorems 2.6 and 2.9 from [CS83] yields a contradiction. In <ref> [BC93] </ref> it is shown that f L j L = n+1 N g also witnesses the separation of Proposition 2. Clearly from Theorem 1 and Proposition 2 we have our main Corollary 4 TxtFex a b TxtFex c d , [b d and a c].
Reference: [BCJ95] <author> G. Baliga, J. Case, and S. Jain. </author> <title> Language learning with some negative information. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 51 </volume> <pages> 273-285, </pages> <year> 1995. </year>
Reference-contexts: It is not clear if the relation is causal, but Theorem 22 in <ref> [BCJ95] </ref> implies there are cases where a significant improvement in language learning speed (as calibrated by number of mind-changes required to reach a single final correct grammar) results from the presence of minimal negative information. Largely unexplored, but of some interest, is the extension of [BCJ95] to TxtFex a b -identification. <p> causal, but Theorem 22 in <ref> [BCJ95] </ref> implies there are cases where a significant improvement in language learning speed (as calibrated by number of mind-changes required to reach a single final correct grammar) results from the presence of minimal negative information. Largely unexplored, but of some interest, is the extension of [BCJ95] to TxtFex a b -identification. We originally suggested [Cas88] on the basis of our main corollary (Corollary 4 to Theorem 1, each in Section 3 above) that Gold's model be extended to embrace the success criteria TxtFex a b for "small" values of a and b.
Reference: [BCJ96] <author> G. Baliga, J. Case, and S. Jain. </author> <title> Synthesizing enumeration techniques for language learning. </title> <type> Technical Report eC-TR-96-003, </type> <note> Electronic Archive for Computational Learning Theory (http://ecolt.informatik.uni-dortmund.de/), 1996. </note>
Reference-contexts: Suppose F TxtI-identifies L. Then (9D finite L)(8L 0 L j D L 0 ^ L 0 6= 2a L)[F does not TxtI-identify L 0 ]: (5) It would be interesting to have a complete characterization from Theorem 3. Progress was made in <ref> [BCJ96] </ref> where it is shown that, for any uniformly decidable class of recursive languages L, a learning function F witnesses that L is in TxtBc a , each L 2 L satisfies (5) above (for I = Bc a ). 13 To prove Theorem 3 it is useful to have the <p> For each T for L, choose a suitably large T T that (8t T j t T )[W F (t) = a L]: P = T for L 13 This complements a related characterization in [Ang80] of the uniformly decidable classes of recursive languages in TxtFex 0 1 . <ref> [BCJ96] </ref> provides a related characterization of the uniformly decidable classes of recursive languages in TxtFex fl 1 . [OSW86c, Page 30] characterizes learning by an agent which is not necessarily algorithmic. [Muk92, LZ92] contain characterizations of uniformly decidable classes of recursive languages for important special cases of TxtFex 0 1 .
Reference: [BCJS94] <author> G. Baliga, J. Case, S. Jain, and M. Suraj. </author> <title> Machine learning of higher order programs. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 59(2) </volume> <pages> 486-500, </pages> <year> 1994. </year>
Reference-contexts: In this way results about learning programs for functions can be interpreted as results about finding predictive explanations for phenomena | as results about scientific induction. For more on this see <ref> [BB75, CS83, CJS92, BCJS94, CJNM94, LW94] </ref>. Re the names of the learning criteria studied in the present paper, originally [CS83] `Ex' stood for `explanatory', `Fex' stood for `finitely explanatory', and Bc for `behaviorally correct'. 11 Proof.
Reference: [Ber85] <author> R. Berwick. </author> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: Intuitively, this necessary condition circumvents overgeneralization in learning from positive data <ref> [Ang80, Ber85] </ref>. Theorem 3 in Section 4 below generalizes the subset principle to the criteria of success TxtFex a b -identification and TxtBc a -identification, where the a in TxtBc a -identification allows each of the infinitely many final grammars converged to to have up to a anomalies. <p> The I = Fex 0 1 case of the following Theorem is from Angluin's [Ang80]. She calls the finite sets D featured tell tales. The theorem witnesses a severe constraint called the subset principle on learning from positive data. See <ref> [Ang80, Ber85] </ref> regarding the importance of the subset princi ple for circumventing overgeneralization in learning languages from positive data. See [KLHM93, Wex93] for discussion regarding the possible connection between this subset principle and a more traditionally linguistically oriented one in [MW87]. We let 2fl = fl.
Reference: [BH70] <author> R. Brown and C. Hanlon. </author> <title> Derivational complexity and the order of acquisition in child speech. </title> <editor> In J. R. Hayes, editor, </editor> <booktitle> Cognition and the Development of Language. </booktitle> <publisher> Wiley, </publisher> <year> 1970. </year>
Reference-contexts: Gold took quite seriously, as a model of child language learning, the case of data presentation by arbitrary text, the case where M receives all and only positive information about L. Justification for this point of view can be found, for example, in <ref> [BH70, Bra71] </ref>, where it is noted from field work that children don't need corrections to learn language.
Reference: [Blu67a] <author> M. Blum. </author> <title> A machine independent theory of the complexity of recursive functions. </title> <journal> Journal of the ACM, </journal> <volume> 14 </volume> <pages> 322-336, </pages> <year> 1967. </year>
Reference-contexts: Incompleteness Theorem by a self-reference argument, many years afterwards, Paris and Harrington [PH77] and later Friedman [Sim85, Sim87] found quite natural examples of combinatorial truths of first order arithmetic not provable in FOPA. 9 In fairness, regarding the above informal thesis, we note, for example, that the Blum Speed-Up Theorem <ref> [Blu67a] </ref> was originally proved by a self-reference argument 10 , but natural witnesses to even exponential speedup have not (yet) been found. However, even the self-reference proofs of this result are fairly complicated; hence, one might expect that natural examples are especially hard to find. <p> However, in the proof of Theorem 1, for each F, the associated set (s) W e 0 ; W e 1 ; . . . ; W e n are each actually recursive; hence, for each F, there is a Blum Complexity Measure <ref> [Blu67a, HU79] </ref> such that e 0 = e 1 = . . . = e n ; therefore, if performance were measured by such a , vacillatory learning would increase learning power but without a corresponding vacillation in performance.
Reference: [Blu67b] <author> M. Blum. </author> <title> On the size of machines. </title> <journal> Information and Control, </journal> <volume> 11 </volume> <pages> 257-265, </pages> <year> 1967. </year>
Reference-contexts: Mathematically TxtMfex a b -identification is well-behaved, e.g., it turns out not to depend on the choice of acceptable system; it also does not depend on the choice of Blum program size measure <ref> [Blu67b] </ref> (by his recursive-relatedness result in [Blu67b]). The lack of dependence on the choice of acceptable system is in contrast with the variant of TxtMfex 0 1 -identification in which we require h to be the identity function (see [CJS94b]). <p> Mathematically TxtMfex a b -identification is well-behaved, e.g., it turns out not to depend on the choice of acceptable system; it also does not depend on the choice of Blum program size measure <ref> [Blu67b] </ref> (by his recursive-relatedness result in [Blu67b]). The lack of dependence on the choice of acceptable system is in contrast with the variant of TxtMfex 0 1 -identification in which we require h to be the identity function (see [CJS94b]).
Reference: [BP73] <author> J. Barzdin and K. Podnieks. </author> <title> The theory of inductive inference. </title> <booktitle> In Proceedings of the Mathematical Foundations for Computer Science, </booktitle> <pages> pages 9-15, </pages> <year> 1973. </year>
Reference-contexts: It is importantly not required that M signal when it has reached its final conjecture | in general it doesn't know when and if it has. 2 larger class of languages than any machine, however clever, could TxtFex n -identify. Unfortunately, it was already known <ref> [BP73] </ref> that, at least in the case where the data is informant instead of text, one gets no more learning power with (n + 1) correct programs in the limit than with n. <p> -identification to require that the learning function must converge to exactly n grammars, then the hierarchy of Corollary 2 above collapses. 11 If one restricts ones attention to languages which are the (pairing function coded) graphs of total functions, then it is essentially shown (the a = 0 case in <ref> [BP73] </ref> and the a &gt; 0 cases in [CS83]) that 9 See [RC94] for an example from complexity theory. 10 See also Young's version in [You73] and our Operator Recursion Theorem variant in [Smi94]. 11 Just output every n-th grammar. 10 the hierarchy again collapses.
Reference: [Bra71] <author> M. Braine. </author> <title> On two types of models of the internalization of grammars. </title> <editor> In D. S-lobin, editor, </editor> <booktitle> The Ontogenesis of Grammar: A Theoretical Symposium. </booktitle> <publisher> Academic Press, </publisher> <address> NY, </address> <year> 1971. </year>
Reference-contexts: Gold took quite seriously, as a model of child language learning, the case of data presentation by arbitrary text, the case where M receives all and only positive information about L. Justification for this point of view can be found, for example, in <ref> [BH70, Bra71] </ref>, where it is noted from field work that children don't need corrections to learn language.
Reference: [Cas74] <author> J. </author> <title> Case. Periodicity in generations of automata. </title> <journal> Mathematical Systems Theory, </journal> <volume> 8 </volume> <pages> 15-32, </pages> <year> 1974. </year>
Reference-contexts: the proof of Theorem 1, to handle the self-referential character of L n+1 , we employ the (n + 1)-ary recursion theorem, a folk theorem generalizing the Kleene Recursion Theorem [Rog67, Page 214] and the Smullyan Double Recursion Theorem [Smu61]; it is also a consequence of our Operator Recursion Theorem <ref> [Cas74] </ref>, an infinitary analog of the finite-arity recursion theorems. <p> One mechanism to achieve this creation is a generalization of the self replication trick isomorphic to that employed by single-celled organisms <ref> [Cas74] </ref>. <p> Proposition 4 There is in TxtFex 0 1 an infinite r.e. collection of infinite languages of the form f W e 0 W e 1 W e 2 . . . g. Proof. By the Operator Recursion Theorem <ref> [Cas74] </ref>, there is an infinite r.e. sequence of self-other referential programs e 0 ; e 1 ; e 2 ; . . . such that, for each i 2 N, W e i = f e i ; e i+1 ; e i+2 ; . . . g: We omit the
Reference: [Cas86] <author> J. </author> <title> Case. Learning machines. </title> <editor> In W. Demopoulos and A. Marras, editors, </editor> <title> Language Learning and Concept Acquisition. </title> <publisher> Ablex Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: We briefly introduced the case, discussed above, of up to n final grammars in <ref> [Cas86] </ref>. <p> The mistakes are about which objects are (and which are not) in the corresponding language. In <ref> [CS78, CS83, Cas86] </ref> there are discussion, motivation and interpretation of results about inferring anomalous programs for functions. The results in [CS78, CS83, Cas86] and in this paper show that allowing anomalies increases learning power. Clearly anomalous programs are tolerable provided the number of anomalies is small. <p> The mistakes are about which objects are (and which are not) in the corresponding language. In <ref> [CS78, CS83, Cas86] </ref> there are discussion, motivation and interpretation of results about inferring anomalous programs for functions. The results in [CS78, CS83, Cas86] and in this paper show that allowing anomalies increases learning power. Clearly anomalous programs are tolerable provided the number of anomalies is small. Hence, it is plausible that people have evolved language learning strategies that exploit the greater learning power achieved by converging to slightly incorrect grammars. <p> TxtFex a 1 -identification is just TXTEX a -identification from [CL82]. For n &gt; 0, TxtFex 0 n - identification is just our notion of TXTFEX n -identification from <ref> [Cas86] </ref>. Osherson and Weinstein [OW82a] were the first to define TxtFex 0 fl and TxtFex fl fl ; they called them BEXT and BFEXT, respectively. <p> Corollary 2 (8a)[TxtFex a 1 TxtFex a 2 . . . TxtFex a fl ]. Corollary 3 (Osherson and Weinstein [OW82a]) TxtFex 0 1 TxtFex 0 fl . We announced in <ref> [Cas86] </ref> that we could prove TxtFex 0 1 TxtFex 0 2 by analyzing Osherson and Weinstein's proof of the immediately preceding corollary. Under our direction Karen Ehrlich generalized the combinatorics of this proof to get TxtFex 0 2 TxtFex 0 3 . <p> We have considered (among other possibilities) computable models of learning on computable data sequences. The whole universe or humanly significant portions of it may be computable and/or discrete. Such possibilities are taken seriously, for example, in <ref> [Zus69, Tof77, TM87, Fey82, Cas92b, Cas86, CRS94] </ref>. <p> It seems clear that denotation and social reinforcers play crucial roles in the human case | but not in Gold's paradigm. In <ref> [Cas86] </ref> the report on Chapter 6 of [Ful85] is partly motivated by treating negative information as a more mathematically tractable possible substitute for semantic information. [McN66] notes that homes in which parents do supply improvements to child utterances (a subtle form of correction or negative information), there is increased speed of
Reference: [Cas88] <author> J. </author> <title> Case. The power of vacillation. </title> <editor> In D. Haussler and L. Pitt, editors, </editor> <booktitle> Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 133-142. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1988. </year>
Reference-contexts: The proof herein of Theorem 1 makes use of Theorem 5. We believe it is not possible to replace weak b-ary order independence by b-ary order independence in Theorems 4 and 5 above, contrary to our slightly overzealous claims in <ref> [Cas88] </ref>. However, we have the following result (Theorem 6) with Fulk (who is not responsible for the possibly incorrect claims in [Cas88]). <p> We believe it is not possible to replace weak b-ary order independence by b-ary order independence in Theorems 4 and 5 above, contrary to our slightly overzealous claims in <ref> [Cas88] </ref>. However, we have the following result (Theorem 6) with Fulk (who is not responsible for the possibly incorrect claims in [Cas88]). Theorem 6 implies that learning power (with respect to TxtFex a b -identification) is not decreased by simultaneously restricting learning functions to be (fully) b-ary order independent and circumventing the restriction to recursive texts. <p> Largely unexplored, but of some interest, is the extension of [BCJ95] to TxtFex a b -identification. We originally suggested <ref> [Cas88] </ref> on the basis of our main corollary (Corollary 4 to Theorem 1, each in Section 3 above) that Gold's model be extended to embrace the success criteria TxtFex a b for "small" values of a and b. We consider next a possible difficulty.
Reference: [Cas92a] <author> J. </author> <title> Case. The power of vacillation in language learning. </title> <type> Technical Report 93-08, </type> <institution> University of Delaware, </institution> <year> 1992. </year> <month> 27 </month>
Reference: [Cas92b] <author> J. </author> <title> Case. Turing machine. </title> <editor> In Stuart Shapiro, editor, </editor> <booktitle> Encyclopedia of Artificial Intelligence. </booktitle> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <note> second edition, </note> <year> 1992. </year>
Reference-contexts: We have considered (among other possibilities) computable models of learning on computable data sequences. The whole universe or humanly significant portions of it may be computable and/or discrete. Such possibilities are taken seriously, for example, in <ref> [Zus69, Tof77, TM87, Fey82, Cas92b, Cas86, CRS94] </ref>.
Reference: [Cas94] <author> J. </author> <title> Case. Infinitary self-reference in learning theory. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 6 </volume> <pages> 3-16, </pages> <year> 1994. </year>
Reference-contexts: We briefly introduced the case, discussed above, of up to n final grammars in [Cas86]. The proof of Theorem 1 employs an (n + 1)-ary self-reference argument <ref> [Cas94] </ref>, and an informal thesis is presented and discussed after the statement of Theorem 1 that self-referential examples witnessing an existence theorem presage natural examples witnessing that theorem. [CL82] considered, among other things, the learning of grammars for languages where a single final grammar is allowed to have a bounded number <p> Another is for the programs e (p 0 ); . . . ; e (p n ) to look in a common mirror to see which programs they are. <ref> [Cas94] </ref> provides a tutorial on thinking about and applying recursion theorems. 8 Herein, our application of the (n + 1)-ary recursion theorem (to 8 See [RC94] for discussion and applications of recursion theorems in severely resource-limited contexts. 9 prove Theorem 1) will be informal and the sequence p 0 ; . <p> Plausibility: self-reference arguments lay bare an underlying simplest reason for the theorems they prove <ref> [Rog67, Cas94] </ref>; if a theorem is true for such a simple reason, the "space" of reasons for its truth may be broad enough to admit natural examples. <p> Theorem 3 above does not imply that, if a learning function TxtFex 0 1 -identifies an infinite language, it must fail to TxtFex 0 1 -identify each proper sublanguage. In fact we have the following proposition a variant of which, regarding function learning, appears in <ref> [Cas94] </ref>. Proposition 4 There is in TxtFex 0 1 an infinite r.e. collection of infinite languages of the form f W e 0 W e 1 W e 2 . . . g. Proof.
Reference: [Che81] <author> K. Chen. </author> <title> Tradeoffs in Machine Inductive Inference. </title> <type> PhD thesis, </type> <institution> Computer Science Department, SUNY at Buffalo, </institution> <year> 1981. </year>
Reference-contexts: The study of learning nearly minimal size programs began with [Fre75] in the context of learning programs for functions (see also <ref> [Kin77, Che81, Che82, Fre90] </ref>). Definition 10 TxtMfex a b = fL j (9F)[F TxtMfex a b -identifies L]g.
Reference: [Che82] <author> K. Chen. </author> <title> Tradeoffs in the inductive inference of nearly minimal size programs. </title> <journal> Information and Control, </journal> <volume> 52 </volume> <pages> 68-86, </pages> <year> 1982. </year>
Reference-contexts: The study of learning nearly minimal size programs began with [Fre75] in the context of learning programs for functions (see also <ref> [Kin77, Che81, Che82, Fre90] </ref>). Definition 10 TxtMfex a b = fL j (9F)[F TxtMfex a b -identifies L]g.
Reference: [CJNM94] <author> J. Case, S. Jain, and S. Ngo Manguelle. </author> <title> Refinements of inductive inference by Popperian and reliable machines. </title> <journal> Kybernetika, </journal> <volume> 30 </volume> <pages> 23-52, </pages> <year> 1994. </year>
Reference-contexts: In this way results about learning programs for functions can be interpreted as results about finding predictive explanations for phenomena | as results about scientific induction. For more on this see <ref> [BB75, CS83, CJS92, BCJS94, CJNM94, LW94] </ref>. Re the names of the learning criteria studied in the present paper, originally [CS83] `Ex' stood for `explanatory', `Fex' stood for `finitely explanatory', and Bc for `behaviorally correct'. 11 Proof.
Reference: [CJS92] <author> J. Case, S. Jain, and A. Sharma. </author> <title> On learning limiting programs. </title> <journal> International Journal of Foundations of Computer Science, </journal> <volume> 3(1) </volume> <pages> 93-115, </pages> <year> 1992. </year>
Reference-contexts: In this way results about learning programs for functions can be interpreted as results about finding predictive explanations for phenomena | as results about scientific induction. For more on this see <ref> [BB75, CS83, CJS92, BCJS94, CJNM94, LW94] </ref>. Re the names of the learning criteria studied in the present paper, originally [CS83] `Ex' stood for `explanatory', `Fex' stood for `finitely explanatory', and Bc for `behaviorally correct'. 11 Proof.
Reference: [CJS94a] <author> J. Case, S. Jain, and A. Sharma. </author> <title> Complexity issues for vacillatory function identification. </title> <journal> Information and Computation, </journal> <note> 1994. To appear. </note>
Reference-contexts: We should mention, however, that there are some interesting effects on learning power for vacillatory function learning wrought by bounding suitably sensitive measures of the computational complexity of the learning functions themselves <ref> [CJS94a] </ref> and by the introduction of noisy input data [CJS96]. The next proposition provides a dual to Theorem 1 above. <p> As noted in Section 5 above, [CJS94b] studies TxtMfex a b -identification, the restricted variant of TxtFex a b - identification which requires that final programs/grammars be nearly minimal size. For language learning, bounding complexity of learning machines as in [DS86] or <ref> [CJS94a] </ref> largely remains to be explored. Translating relative solvability results into relative feasibility results, as in [WZ92], would be very interesting to pursue in the context of the present paper.
Reference: [CJS94b] <author> J. Case, S. Jain, and A. Sharma. </author> <title> Vacillatory learning of nearly minimal size grammars. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 49(2) </volume> <pages> 189-207, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The lack of dependence on the choice of acceptable system is in contrast with the variant of TxtMfex 0 1 -identification in which we require h to be the identity function (see <ref> [CJS94b] </ref>). The study of learning nearly minimal size programs began with [Fre75] in the context of learning programs for functions (see also [Kin77, Che81, Che82, Fre90]). Definition 10 TxtMfex a b = fL j (9F)[F TxtMfex a b -identifies L]g. <p> The proof of Corollary 1 immediately above is deferred to Section 6 further below since it employs Theorems 4 and 7 from Section 5 below. The topic of learning nearly minimal size programs/grammars is treated in greater depth in <ref> [CJS94b] </ref>. Herein we present results about such criteria only in the contexts of recursive text (Corollary 1 just above) and of restrictions on learning functions (Section 5 below). There is a small amount of additional discussion below in Section 7. <p> It also implies one can also simultaneously have a technical property we call determination by single text (part 2 of the Theorem). This theorem sees application in <ref> [CJS94b] </ref>, and the (full) b-ary order independence is important for that application. Theorem 6 (Case and Fulk) There is an algorithm for transforming any b and (algorithm for) a learning function F into a corresponding (algorithm for) a learning function F 0 such that 1. <p> We do not know if there are analogs of Theorems 4 and 5 above for TxtMfex a b -identification. The use of pad in the proofs of those theorems wreaks havoc with program/grammar size. However, we do have the next three Theorems, the first of which sees application in <ref> [CJS94b] </ref>. <p> Speaking of Theorem 3: It would be mathematically interesting to explore what happens to the subset principle for TxtBc-identification restricted to recursive texts. It is interesting to place further feasibility restrictions on the criteria of success. As noted in Section 5 above, <ref> [CJS94b] </ref> studies TxtMfex a b -identification, the restricted variant of TxtFex a b - identification which requires that final programs/grammars be nearly minimal size. For language learning, bounding complexity of learning machines as in [DS86] or [CJS94a] largely remains to be explored.
Reference: [CJS96] <author> J. Case, S. Jain, and F. Stephan. </author> <title> Vacillatory and BC learning on noisy data. </title> <type> Technical Report eC-TR-96-002, </type> <note> Electronic Archive for Computational Learning Theory (http://ecolt.informatik.uni-dortmund.de/), 1996. </note>
Reference-contexts: We should mention, however, that there are some interesting effects on learning power for vacillatory function learning wrought by bounding suitably sensitive measures of the computational complexity of the learning functions themselves [CJS94a] and by the introduction of noisy input data <ref> [CJS96] </ref>. The next proposition provides a dual to Theorem 1 above.
Reference: [CKKK95] <author> J. Case, S. Kaufmann, E. Kinber, and M. Kummer. </author> <title> Learning recursive functions from approximations. </title> <booktitle> In Proceedings of the Second European Conference on Computational Learning Theory, volume 904 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 140-153, </pages> <month> March </month> <year> 1995. </year> <note> Journal version to appear in Journal of Computer and System Sciences (Special Issue EuroCOLT'95). </note>
Reference-contexts: In the practical context of robot planning, Drew McDermott [McD92] says, "Learning makes the most sense when it is thought of as filling in the details in an algorithm that is already nearly right." In the context of function learning, <ref> [CKKK95] </ref> provides several models of learning from examples together with approximately correct programs. Included are models in which the maximal probability of learning all the computable functions is proportional to how tightly the approximately correct programs envelope the data.
Reference: [CL82] <author> J. Case and C. Lynes. </author> <title> Machine inductive inference and language identification. </title> <booktitle> In Proceedings of the 9-th Annual Colloquium on Automata, Languages, and Programming, Lecture Notes in Computer Science, </booktitle> <volume> 140, </volume> <pages> pages 107-115. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> July </month> <year> 1982. </year>
Reference-contexts: Osherson and Weinstein [OW82a] introduced the case where the number of final grammars is finite, but unbounded, and <ref> [CL82, OW82a] </ref> independently (see also [OW82b]) introduced the case where the number of final grammars is infinite (TxtBc-identification). We briefly introduced the case, discussed above, of up to n final grammars in [Cas86]. <p> The proof of Theorem 1 employs an (n + 1)-ary self-reference argument [Cas94], and an informal thesis is presented and discussed after the statement of Theorem 1 that self-referential examples witnessing an existence theorem presage natural examples witnessing that theorem. <ref> [CL82] </ref> considered, among other things, the learning of grammars for languages where a single final grammar is allowed to have a bounded number of mistakes (anomalies). The mistakes are about which objects are (and which are not) in the corresponding language. <p> TxtFex 0 1 -identification is equivalent to Gold's [Gol67] seminal notion of identification, also referred to as TXTEX-identification in <ref> [CL82] </ref> and (indirectly) as INT in [OW82b, OW82a, OSW86b]. TxtFex a 1 -identification is just TXTEX a -identification from [CL82]. For n &gt; 0, TxtFex 0 n - identification is just our notion of TXTFEX n -identification from [Cas86]. <p> TxtFex 0 1 -identification is equivalent to Gold's [Gol67] seminal notion of identification, also referred to as TXTEX-identification in <ref> [CL82] </ref> and (indirectly) as INT in [OW82b, OW82a, OSW86b]. TxtFex a 1 -identification is just TXTEX a -identification from [CL82]. For n &gt; 0, TxtFex 0 n - identification is just our notion of TXTFEX n -identification from [Cas86]. Osherson and Weinstein [OW82a] were the first to define TxtFex 0 fl and TxtFex fl fl ; they called them BEXT and BFEXT, respectively. <p> Next, for mathematical completeness and interest, are introduced the cases of success criteria for which the number of final grammars is possibly infinite, not necessarily finite as it is for TxtFex a b - identification. Definitions 11 and 12 are from <ref> [CL82] </ref>. The a 2 f 0; fl g cases were independently introduced in [OW82a, OW82b]. The quantifier `8 1 k' means `for all but finitely many k 2 N'. Definition 11 F TxtBc a identifies L , (8 texts T for L)(8 1 k)[W F (T [k]) = a L]. <p> Wiehagen [Wie77] essentially notes that RecTxtFex 0 1 = TxtFex 0 1 (a related result was first proved in [BB75]), and <ref> [CL82] </ref> essentially observes that (8a)[RecTxtFex a 1 = TxtFex a 1 ]. We have, more generally, Corollary 1 1. (8a; b)[RecTxtFex a b = TxtFex a b ]. 2. (8a; b)[RecTxtMfex a b = TxtMfex a b ]. <p> makes no difference in learning power whether or not we restrict the texts to be recursive! By contrast, for TxtBc a , learning procedures can exploit the assumption that they are receiving recursive texts; for TxtBc a , the restriction to recursive text does make a difference in learning power <ref> [CL82, Fre85] </ref>. The proof of Corollary 1 immediately above is deferred to Section 6 further below since it employs Theorems 4 and 7 from Section 5 below. The topic of learning nearly minimal size programs/grammars is treated in greater depth in [CJS94b]. <p> Corollary 5 (Case and Lynes <ref> [CL82] </ref>) TxtFex 0 1 TxtFex 1 1 . . . TxtFex fl 1 . Osherson and Weinstein [OW82a] independently showed the case of TxtFex 0 1 TxtFex fl 1 from the previous corollary. Corollary 6 (Osherson and Weinstein [OW82a]) TxtFex 0 fl TxtFex fl fl . <p> We have not yet worked out all the relationships analogous to those in Theorem 2 and Corollary 7 for the cases TxtBc a -identification is restricted to recursive texts. As noted above in this section, the restriction to recursive texts does affect TxtBc a -identification <ref> [CL82, Fre85] </ref>. 4 Topological Results We next present several useful results which can be described as topological. The exact connections to topology (actually, to Baire Category Theory and Banach-Mazur Games [Jec78]) we will not pursue herein, but, on that subject, the interested reader can consult [OSW83, OSW86b]. <p> Theorem 2 TxtFex m fl TxtBc m 0 , m 2:m 0 ; furthermore, f L j L = 2m+1 N g 2 (TxtFex 2m+1 1 TxtBc m ). Proof. This proof employs previously unpublished techniques used to prove a similar result for TxtFex a 1 in <ref> [CL82] </ref>. Suppose F TxtFex 2m fl -identifies L. We will construct an F 0 which TxtBc m -identifies L, and, then, it will suffice to prove the furthermore clause. Define F 0 on t thus. First calculate p = F (t ).
Reference: [CRS94] <author> J. Case, D. Rajan, and A. Shende. </author> <title> Representing the spatial/kinematic domain and lattice computers. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 6 </volume> <pages> 17-40, </pages> <year> 1994. </year>
Reference-contexts: We have considered (among other possibilities) computable models of learning on computable data sequences. The whole universe or humanly significant portions of it may be computable and/or discrete. Such possibilities are taken seriously, for example, in <ref> [Zus69, Tof77, TM87, Fey82, Cas92b, Cas86, CRS94] </ref>.
Reference: [CS78] <author> J. Case and C. Smith. </author> <title> Anomaly hierarchies of mechanized inductive inference. </title> <booktitle> In Proceedings of the 10-th Annual Symposium on the Theory of Computing, </booktitle> <pages> pages 314-319, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: The mistakes are about which objects are (and which are not) in the corresponding language. In <ref> [CS78, CS83, Cas86] </ref> there are discussion, motivation and interpretation of results about inferring anomalous programs for functions. The results in [CS78, CS83, Cas86] and in this paper show that allowing anomalies increases learning power. Clearly anomalous programs are tolerable provided the number of anomalies is small. <p> The mistakes are about which objects are (and which are not) in the corresponding language. In <ref> [CS78, CS83, Cas86] </ref> there are discussion, motivation and interpretation of results about inferring anomalous programs for functions. The results in [CS78, CS83, Cas86] and in this paper show that allowing anomalies increases learning power. Clearly anomalous programs are tolerable provided the number of anomalies is small. Hence, it is plausible that people have evolved language learning strategies that exploit the greater learning power achieved by converging to slightly incorrect grammars.
Reference: [CS83] <author> J. Case and C. Smith. </author> <title> Comparison of identification criteria for machine inductive inference. </title> <journal> Theoretical Computer Science, </journal> <volume> 25 </volume> <pages> 193-220, </pages> <year> 1983. </year>
Reference-contexts: The mistakes are about which objects are (and which are not) in the corresponding language. In <ref> [CS78, CS83, Cas86] </ref> there are discussion, motivation and interpretation of results about inferring anomalous programs for functions. The results in [CS78, CS83, Cas86] and in this paper show that allowing anomalies increases learning power. Clearly anomalous programs are tolerable provided the number of anomalies is small. <p> The mistakes are about which objects are (and which are not) in the corresponding language. In <ref> [CS78, CS83, Cas86] </ref> there are discussion, motivation and interpretation of results about inferring anomalous programs for functions. The results in [CS78, CS83, Cas86] and in this paper show that allowing anomalies increases learning power. Clearly anomalous programs are tolerable provided the number of anomalies is small. Hence, it is plausible that people have evolved language learning strategies that exploit the greater learning power achieved by converging to slightly incorrect grammars. <p> converge to exactly n grammars, then the hierarchy of Corollary 2 above collapses. 11 If one restricts ones attention to languages which are the (pairing function coded) graphs of total functions, then it is essentially shown (the a = 0 case in [BP73] and the a &gt; 0 cases in <ref> [CS83] </ref>) that 9 See [RC94] for an example from complexity theory. 10 See also Young's version in [You73] and our Operator Recursion Theorem variant in [Smi94]. 11 Just output every n-th grammar. 10 the hierarchy again collapses. <p> We identify total functions f with f hx; f (x)i j x 2 N g. Let L = f total f j ' f (0) = m+1 f g. Clearly L 2 TxtFex m+1 1 . Also, L 2 TxtFex m fl together with Theorems 2.6 and 2.9 from <ref> [CS83] </ref> yields a contradiction. In [BC93] it is shown that f L j L = n+1 N g also witnesses the separation of Proposition 2. Clearly from Theorem 1 and Proposition 2 we have our main Corollary 4 TxtFex a b TxtFex c d , [b d and a c]. <p> In this way results about learning programs for functions can be interpreted as results about finding predictive explanations for phenomena | as results about scientific induction. For more on this see <ref> [BB75, CS83, CJS92, BCJS94, CJNM94, LW94] </ref>. Re the names of the learning criteria studied in the present paper, originally [CS83] `Ex' stood for `explanatory', `Fex' stood for `finitely explanatory', and Bc for `behaviorally correct'. 11 Proof. <p> For more on this see [BB75, CS83, CJS92, BCJS94, CJNM94, LW94]. Re the names of the learning criteria studied in the present paper, originally <ref> [CS83] </ref> `Ex' stood for `explanatory', `Fex' stood for `finitely explanatory', and Bc for `behaviorally correct'. 11 Proof. As in the proof of Proposition 2, we identify total functions f with f hx; f (x)i j x 2 N g. <p> Let L = f total f j (8 1 k)[' f (k) = f ] g. Clearly L 2 TxtBc 0 . Also, L 2 TxtFex fl fl together with Theorems 2.12 and 3.1 from <ref> [CS83] </ref> yields a contradiction. Remark 1 Proposition 3 still holds even if we restrict TxtBc 0 -identification to recursive texts. <p> The next theorem says that, in passing from learning finitely many anomalous grammars in the limit to learning infinitely many, one can eliminate 1 2 the anomalies, and that that's optimal! This contrasts with the function learning case <ref> [CS83] </ref> where, by a result of Steel, one can so eliminate all of finitely many anomalies. Intuitively, in the present context, since one is missing in the input data the negative information, i.e., since one is missing 1 2 the information, one can eliminate 1 2 the anomalies only.
Reference: [dJK96] <author> D. de Jongh and M. </author> <title> Kanazawa. Angluin's thoerem for indexed families of r.e. sets and applications. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, Desenzano del Garda, Italy, </booktitle> <pages> page 193, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: provides a related characterization of the uniformly decidable classes of recursive languages in TxtFex fl 1 . [OSW86c, Page 30] characterizes learning by an agent which is not necessarily algorithmic. [Muk92, LZ92] contain characterizations of uniformly decidable classes of recursive languages for important special cases of TxtFex 0 1 . <ref> [dJK96] </ref> surprisingly characterizes the r.e. classes in TxtFex 0 1 , but by a condition more complicated than in the characterizations already mentioned. 13 Then (8p 2 P )[W p = a L].
Reference: [dMSS56] <author> K. deLeeuw, E. Moore, C. Shannon, and N. Shapiro. </author> <title> Computability by probabilistic machines. Automata Studies, </title> <journal> Annals of Math. Studies, </journal> <volume> 34 </volume> <pages> 183-212, </pages> <year> 1956. </year>
Reference-contexts: Such possibilities are taken seriously, for example, in [Zus69, Tof77, TM87, Fey82, Cas92b, Cas86, CRS94]. In a discrete, random universe with only computable probability distributions for its behavior (e.g., a discrete, quantum mechanical universe), the expected behavior will still be computable <ref> [dMSS56, Gil72, Gil77] </ref>. 25 In such a universe any beings (e.g., human) who do cognition, including language learning and scientific induction, will be subject to the constraint that at least their expected behavior will be computable; hence, any theorems about computable learning agents will inform, to some extent, about the possible <p> Hence, the 25 Sources such as [Pen89, Pen94] sadly seem to have overlooked the important result in <ref> [dMSS56] </ref> that the expected I/O behavior of a Turing machine with random oracle subject to a computable probability distribution is computable (and constructively so). 25 performance vacillation may exist, but significant degradations in articulateness potential might be confined to rare episodes.
Reference: [DS86] <author> R. Daley and C. Smith. </author> <title> On the complexity of inductive inference. </title> <journal> Information and Control, </journal> <volume> 69 </volume> <pages> 12-40, </pages> <year> 1986. </year> <month> 28 </month>
Reference-contexts: As noted in Section 5 above, [CJS94b] studies TxtMfex a b -identification, the restricted variant of TxtFex a b - identification which requires that final programs/grammars be nearly minimal size. For language learning, bounding complexity of learning machines as in <ref> [DS86] </ref> or [CJS94a] largely remains to be explored. Translating relative solvability results into relative feasibility results, as in [WZ92], would be very interesting to pursue in the context of the present paper.
Reference: [Fey82] <author> R. </author> <title> Feynman. Simulating physics with computers. </title> <journal> International Journal of The--oretical Physics, </journal> <volume> 21(6/7), </volume> <year> 1982. </year>
Reference-contexts: We have considered (among other possibilities) computable models of learning on computable data sequences. The whole universe or humanly significant portions of it may be computable and/or discrete. Such possibilities are taken seriously, for example, in <ref> [Zus69, Tof77, TM87, Fey82, Cas92b, Cas86, CRS94] </ref>.
Reference: [Fre75] <author> R. Freivalds. </author> <title> Minimal Godel numbers and their identification in the limit. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 32, </volume> <pages> pages 219-225. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1975. </year>
Reference-contexts: The lack of dependence on the choice of acceptable system is in contrast with the variant of TxtMfex 0 1 -identification in which we require h to be the identity function (see [CJS94b]). The study of learning nearly minimal size programs began with <ref> [Fre75] </ref> in the context of learning programs for functions (see also [Kin77, Che81, Che82, Fre90]). Definition 10 TxtMfex a b = fL j (9F)[F TxtMfex a b -identifies L]g.
Reference: [Fre85] <author> R. Freivalds. </author> <title> Recursiveness of the enumerating functions increases the inferra-bility of recursively enumerable sets. </title> <journal> Bulletin of the European Association for Theoretical Computer Science, </journal> <volume> 27 </volume> <pages> 35-40, </pages> <year> 1985. </year>
Reference-contexts: makes no difference in learning power whether or not we restrict the texts to be recursive! By contrast, for TxtBc a , learning procedures can exploit the assumption that they are receiving recursive texts; for TxtBc a , the restriction to recursive text does make a difference in learning power <ref> [CL82, Fre85] </ref>. The proof of Corollary 1 immediately above is deferred to Section 6 further below since it employs Theorems 4 and 7 from Section 5 below. The topic of learning nearly minimal size programs/grammars is treated in greater depth in [CJS94b]. <p> We have not yet worked out all the relationships analogous to those in Theorem 2 and Corollary 7 for the cases TxtBc a -identification is restricted to recursive texts. As noted above in this section, the restriction to recursive texts does affect TxtBc a -identification <ref> [CL82, Fre85] </ref>. 4 Topological Results We next present several useful results which can be described as topological. The exact connections to topology (actually, to Baire Category Theory and Banach-Mazur Games [Jec78]) we will not pursue herein, but, on that subject, the interested reader can consult [OSW83, OSW86b].
Reference: [Fre90] <author> R. Freivalds. </author> <title> Inductive inference of minimal programs. </title> <editor> In M. Fulk and J. Case, editors, </editor> <booktitle> Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 3-20. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: The study of learning nearly minimal size programs began with [Fre75] in the context of learning programs for functions (see also <ref> [Kin77, Che81, Che82, Fre90] </ref>). Definition 10 TxtMfex a b = fL j (9F)[F TxtMfex a b -identifies L]g.
Reference: [Ful85] <author> M. Fulk. </author> <title> A Study of Inductive Inference machines. </title> <type> PhD thesis, </type> <institution> SUNY at Buffalo, </institution> <year> 1985. </year>
Reference-contexts: Theorem 4 is the hardest theorem herein to prove, and the other theorems in Section 5 are proved by modifications, and/or simplifications of the proof of Theorem 4. Some of the theorems in Section 5 generalize predecessors for TxtFex 0 1 -identification <ref> [BB75, WC80, SR84, Ful85, OSW86b, Ful90a] </ref> but are much harder to prove. Some of the theorems in Section 5 are applied in the present paper and in other papers. <p> Then: L recursively b-stabilizes F , (8 recursive T for L)(9D j card (D) b)[F (T )+ = D]: The following lemma, useful to this paper, combines the topological with the algorithmic. It generalizes predecessors from <ref> [BB75, Ful85, OSW86b] </ref>. Lemma 3 Suppose L is r.e. and recursively b-stabilizes F. Then (8 in L)(9D j card (D) b)(9t j t in L)(8t 0 t j t 0 in L)[F (t 0 ) 2 D]: (7) Proof. Suppose the hypothesis on L. <p> We consider herein some mathematical versions of this question. Several mathematical definitions have been given for various different notions of insensitivity to order, essentially for the case of TxtFex 0 1 -identification <ref> [BB75, WC80, SR84, Ful85, OSW86b, Ful90a] </ref>. We extend these definitions of insensitive or restricted learning functions naturally to the context of the vacillatory learning criteria of the present paper, 15 and we investigate the interesting mathematical questions of whether learning functions with these insensitivities or restrictions thereby lose learning power. <p> The next restriction defined, in effect, provides some degree of sensitivity to timing. Definition 17 (Schafer [SR84, OSW86b], Fulk <ref> [Ful85, Ful90a] </ref>) F is called partly set-driven (synonym [Ful85, Ful90a]: rearrangement independent) , (8; t j kk = kt k ^ content () = content (t ))[F () = F (t )]. <p> The next restriction defined, in effect, provides some degree of sensitivity to timing. Definition 17 (Schafer [SR84, OSW86b], Fulk <ref> [Ful85, Ful90a] </ref>) F is called partly set-driven (synonym [Ful85, Ful90a]: rearrangement independent) , (8; t j kk = kt k ^ content () = content (t ))[F () = F (t )]. <p> Intuitively, F is set-driven (respectively, partly set-driven) iff, for each , F () depends only on the content of (respectively, depends only on the length and content of ). Schafer [SR84, OSW86b], first, and Fulk <ref> [Ful85, Ful90a] </ref>, later, independently showed that set-driven learning functions can't TxtFex 0 1 -identify some classes of languages that unrestricted learning functions can, but that partly set-driven learning functions do not restrict learning power with respect to TxtFex 0 1 -identification. <p> Furthermore, it implies that one can also simultaneously circumvent the restriction to recursive texts. It generalizes parts of Fulk's Kitchen Sink Theorem [Ful90a, Theorem 13, Page 6] and <ref> [Ful85, Chapter 5, Theorem 21] </ref> which covered the TxtFex 0 1 case only; however, the lift to Theorem 4 ostensibly requires a much more difficult proof. 18 For non-trivially vacillatory criteria, it is open whether (full) order independence can be com bined with partly set-driven without loss of learning power. <p> If stopping with a search for hD 1 ; 1 i sufficed, Theorem 1 above could not hold. Nothing like this while loop is needed to handle the cases of TxtFex a 1 . The use of pad is a variant of its use in <ref> [Ful85, Ful90a] </ref> and serves below in the proof of F 0 's weak b-ary order independence in a combinatorially similar, subtle role. Here's an intuitive way to think about this construction. <p> It seems clear that denotation and social reinforcers play crucial roles in the human case | but not in Gold's paradigm. In [Cas86] the report on Chapter 6 of <ref> [Ful85] </ref> is partly motivated by treating negative information as a more mathematically tractable possible substitute for semantic information. [McN66] notes that homes in which parents do supply improvements to child utterances (a subtle form of correction or negative information), there is increased speed of language acquisition.
Reference: [Ful90a] <author> M. Fulk. </author> <title> Prudence and other conditions on formal language learning. </title> <journal> Information and Computation, </journal> <volume> 85 </volume> <pages> 1-11, </pages> <year> 1990. </year>
Reference-contexts: Theorem 4 is the hardest theorem herein to prove, and the other theorems in Section 5 are proved by modifications, and/or simplifications of the proof of Theorem 4. Some of the theorems in Section 5 generalize predecessors for TxtFex 0 1 -identification <ref> [BB75, WC80, SR84, Ful85, OSW86b, Ful90a] </ref> but are much harder to prove. Some of the theorems in Section 5 are applied in the present paper and in other papers. <p> We consider herein some mathematical versions of this question. Several mathematical definitions have been given for various different notions of insensitivity to order, essentially for the case of TxtFex 0 1 -identification <ref> [BB75, WC80, SR84, Ful85, OSW86b, Ful90a] </ref>. We extend these definitions of insensitive or restricted learning functions naturally to the context of the vacillatory learning criteria of the present paper, 15 and we investigate the interesting mathematical questions of whether learning functions with these insensitivities or restrictions thereby lose learning power. <p> The next restriction defined, in effect, provides some degree of sensitivity to timing. Definition 17 (Schafer [SR84, OSW86b], Fulk <ref> [Ful85, Ful90a] </ref>) F is called partly set-driven (synonym [Ful85, Ful90a]: rearrangement independent) , (8; t j kk = kt k ^ content () = content (t ))[F () = F (t )]. <p> The next restriction defined, in effect, provides some degree of sensitivity to timing. Definition 17 (Schafer [SR84, OSW86b], Fulk <ref> [Ful85, Ful90a] </ref>) F is called partly set-driven (synonym [Ful85, Ful90a]: rearrangement independent) , (8; t j kk = kt k ^ content () = content (t ))[F () = F (t )]. <p> Intuitively, F is set-driven (respectively, partly set-driven) iff, for each , F () depends only on the content of (respectively, depends only on the length and content of ). Schafer [SR84, OSW86b], first, and Fulk <ref> [Ful85, Ful90a] </ref>, later, independently showed that set-driven learning functions can't TxtFex 0 1 -identify some classes of languages that unrestricted learning functions can, but that partly set-driven learning functions do not restrict learning power with respect to TxtFex 0 1 -identification. <p> Furthermore, it implies that one can also simultaneously circumvent the restriction to recursive texts. It generalizes parts of Fulk's Kitchen Sink Theorem <ref> [Ful90a, Theorem 13, Page 6] </ref> and [Ful85, Chapter 5, Theorem 21] which covered the TxtFex 0 1 case only; however, the lift to Theorem 4 ostensibly requires a much more difficult proof. 18 For non-trivially vacillatory criteria, it is open whether (full) order independence can be com bined with partly set-driven <p> 1 j content ( 0 ) L)[F [ 1 ; 0 ] D 1 ]: (9) (9) is a useful stability condition. fl) Once hD 1 ; 1 i is found: 18 In the present paper we do not consider the restriction to so-called prudence [OSW86b], a primary concern of <ref> [Ful90a] </ref>. Prudent learning functions are those which never conjecture a grammar p without being able to learn W p . <p> If stopping with a search for hD 1 ; 1 i sufficed, Theorem 1 above could not hold. Nothing like this while loop is needed to handle the cases of TxtFex a 1 . The use of pad is a variant of its use in <ref> [Ful85, Ful90a] </ref> and serves below in the proof of F 0 's weak b-ary order independence in a combinatorially similar, subtle role. Here's an intuitive way to think about this construction.
Reference: [Ful90b] <author> M. Fulk. </author> <title> Robust separations in inductive inference. </title> <booktitle> In Proceedings of the 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 405-410, </pages> <address> St. Louis, Missouri 1990. </address>
Reference-contexts: However, even the self-reference proofs of this result are fairly complicated; hence, one might expect that natural examples are especially hard to find. For some theoretical work instigated by Barzdin and dealing, in part, with eliminating dependence on self-referential examples, see Fulk's work on robust function learning in <ref> [Ful90b] </ref>. Corollary 2 (8a)[TxtFex a 1 TxtFex a 2 . . . TxtFex a fl ]. Corollary 3 (Osherson and Weinstein [OW82a]) TxtFex 0 1 TxtFex 0 fl . <p> Can we get versions of our separation results robust in the sense of <ref> [Ful90b] </ref>? Much of the work in Gold-style learning theory on success criteria extending Gold's is motivated by attempts to assuage the negative results in this area. [Kir92] mentions a common argument to the effect that very strong negative results about language learnability in [Gol67] provide evidence that human language learning must
Reference: [GBC + 91] <author> C. Gallistel, A. Brown, S. Carey, R. Gelman, and F. Keil. </author> <title> Lessons from animal learning for the study of cognitive development. </title> <editor> In S. Carey and R. Gelman, editors, </editor> <booktitle> Epigenesis of Mind: Essays on Biology and Cognition, </booktitle> <pages> pages 3-37. </pages> <publisher> Erlbaum, </publisher> <address> Hillsdale, </address> <year> 1991. </year>
Reference-contexts: language learning must involve some innately stored information! The negative results suggest, among other things, 1. that general purpose learning is not possible and 2. that alleged human general purpose learning is an illusion brought about by our having innate information stored for a large and varied collection of domains <ref> [GBC + 91, Spe94] </ref>.
Reference: [Gil72] <author> J. Gill. </author> <title> Probabilistic Turing Machines and Complexity of Computation. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1972. </year>
Reference-contexts: Such possibilities are taken seriously, for example, in [Zus69, Tof77, TM87, Fey82, Cas92b, Cas86, CRS94]. In a discrete, random universe with only computable probability distributions for its behavior (e.g., a discrete, quantum mechanical universe), the expected behavior will still be computable <ref> [dMSS56, Gil72, Gil77] </ref>. 25 In such a universe any beings (e.g., human) who do cognition, including language learning and scientific induction, will be subject to the constraint that at least their expected behavior will be computable; hence, any theorems about computable learning agents will inform, to some extent, about the possible
Reference: [Gil77] <author> J. Gill. </author> <title> Computational complexity of probabilistic Turing machines. </title> <journal> SIAM Journal on Computing, </journal> <volume> 6 </volume> <pages> 675-695, </pages> <year> 1977. </year>
Reference-contexts: Such possibilities are taken seriously, for example, in [Zus69, Tof77, TM87, Fey82, Cas92b, Cas86, CRS94]. In a discrete, random universe with only computable probability distributions for its behavior (e.g., a discrete, quantum mechanical universe), the expected behavior will still be computable <ref> [dMSS56, Gil72, Gil77] </ref>. 25 In such a universe any beings (e.g., human) who do cognition, including language learning and scientific induction, will be subject to the constraint that at least their expected behavior will be computable; hence, any theorems about computable learning agents will inform, to some extent, about the possible
Reference: [Gle86] <author> L. Gleitman. </author> <title> Biological dispositions to learn language. </title> <editor> In W. Demopoulos and A. Marras, editors, </editor> <title> Language Learning and Concept Acquisition. </title> <publisher> Ablex Publ. Co., </publisher> <year> 1986. </year>
Reference: [God86] <author> K. </author> <title> Godel. On formally undecidable propositions of Principia Mathematica and related systems I. </title> <editor> In S. Feferman, editor, Kurt Godel. </editor> <booktitle> Collected Works. </booktitle> <volume> Vol. I, </volume> <pages> pages 145-195. </pages> <publisher> Oxford Univ. Press, </publisher> <year> 1986. </year>
Reference-contexts: For example, Godel proved his famous Incompleteness Theorem by a self-reference argument <ref> [God86, Men86] </ref>, and his self-referential sentence providing an unprovable truth of, for example, First Order Peano Arithmetic (FOPA) is not natural | no number or combinatorial theorist would care whether it was true or false.
Reference: [Gol67] <author> E. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: 1 Introduction Gold in <ref> [Gol67] </ref> introduced his seminal model of language learning: Imagine, as pictured in (1) just below, a machine M being fed data about membership in a (formal) language L and, as a result, outputting over time a series of grammars p 0 ; p 1 ; p 2 ; . . . <p> M data re L (1) For our present purposes, it will suffice to consider two kinds of data presentation and one kind of success from <ref> [Gol67] </ref>. Data about L is either 1. informant, a listing of every possible language element with an clear indication of whether or not it is in L or 2. text, an arbitrary listing of all and only the elements of L. <p> TxtFex 0 1 -identification is equivalent to Gold's <ref> [Gol67] </ref> seminal notion of identification, also referred to as TXTEX-identification in [CL82] and (indirectly) as INT in [OW82b, OW82a, OSW86b]. TxtFex a 1 -identification is just TXTEX a -identification from [CL82]. For n &gt; 0, TxtFex 0 n - identification is just our notion of TXTFEX n -identification from [Cas86]. <p> Gold <ref> [Gol67] </ref> proved Corollary 8 with TxtFex 0 1 in place of TxtBc fl and was clearly concerned that his result meant that only rather puny language classes could be learned from positive data. <p> separation results robust in the sense of [Ful90b]? Much of the work in Gold-style learning theory on success criteria extending Gold's is motivated by attempts to assuage the negative results in this area. [Kir92] mentions a common argument to the effect that very strong negative results about language learnability in <ref> [Gol67] </ref> provide evidence that human language learning must involve some innately stored information! The negative results suggest, among other things, 1. that general purpose learning is not possible and 2. that alleged human general purpose learning is an illusion brought about by our having innate information stored for a large and
Reference: [Hal74] <author> P. Halmos. </author> <title> Naive Set Theory. </title> <publisher> Springer-Verlag, </publisher> <address> NY, </address> <year> 1974. </year>
Reference-contexts: Set theoretically, as in <ref> [Hal74] </ref>, we treat sequences as functions, and, in general, functions, finite, partial or total, as single-valued sets of ordered pairs. 4 Hence, we can and do meaningfully compare them with `', `', `', and `'. <p> For each non-empty r.e. language, there are continuum many such orders (texts) <ref> [Hal74] </ref>, yet only countably many recursive ones (since there are only countably many Turing Machine programs for computing the recursive texts [Rog67]). In a completely computable universe (which ours might be), there are really only recursive texts available to be presented to learning machines.
Reference: [HU79] <author> J. Hopcroft and J. Ullman. </author> <title> Introduction to Automata Theory Languages and Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1979. </year>
Reference-contexts: In other words, M identifies L , on each text for L, the corresponding conjectures of M converge, in the limit, to some fixed final conjecture and that final conjecture is correct. 1 Gold showed that no M so identifies the entire class of regular languages <ref> [HU79] </ref>, but some M does identify the class of finite languages. Angluin [Ang80, Ang82] presents other classes L natural from the perspective of formal language theory such that some M identifies each language in L. <p> However, in the proof of Theorem 1, for each F, the associated set (s) W e 0 ; W e 1 ; . . . ; W e n are each actually recursive; hence, for each F, there is a Blum Complexity Measure <ref> [Blu67a, HU79] </ref> such that e 0 = e 1 = . . . = e n ; therefore, if performance were measured by such a , vacillatory learning would increase learning power but without a corresponding vacillation in performance.
Reference: [Jec78] <author> Thomas Jech. </author> <title> Set Theory. </title> <publisher> Academic Press, </publisher> <address> NY, </address> <year> 1978. </year>
Reference-contexts: It is well known [Rog67] that equivalent grammars (e.g., p t ; p t+1 ; p t+2 ; . . . as above) can be so different from one another that in some cases it is not possible to prove in Zermelo-Frankel Set Theory <ref> [Jec78] </ref> that they are equivalent. This suggests that a suitably clever M might be able to TxtFex n+1 -identify a 1 N.B. <p> As noted above in this section, the restriction to recursive texts does affect TxtBc a -identification [CL82, Fre85]. 4 Topological Results We next present several useful results which can be described as topological. The exact connections to topology (actually, to Baire Category Theory and Banach-Mazur Games <ref> [Jec78] </ref>) we will not pursue herein, but, on that subject, the interested reader can consult [OSW83, OSW86b]. Definition 13 Suppose t T , T a text.
Reference: [JL88] <author> P. Johnson-Laird. </author> <title> The Computer and the Mind: An Introduction to Cognitive Science. </title> <publisher> Harvard Univesity Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: Angluin [Ang80, Ang82] presents other classes L natural from the perspective of formal language theory such that some M identifies each language in L. Many cognitive scientists seek to model all of cognition by computer program <ref> [Pyl84, JL88] </ref>, and Gold's model of language learning from text (positive information) by machine has been very influential in contemporary theories of natural language and in mathematical work explicitly motivated by its possible connection to human language learning (see, for example, [Pin79, WC80, Wex82, OSW82, OSW84, Ber85, Gle86, Cas86, OSW86a, OSW86b,
Reference: [JS95] <author> S. Jain and A. Sharma. </author> <title> Prudence in vacillatory language identification. </title> <journal> Mathematical Systems Theory, </journal> <volume> 28(3) </volume> <pages> 267-279, </pages> <month> May-June </month> <year> 1995. </year>
Reference-contexts: Prudent learning functions are those which never conjecture a grammar p without being able to learn W p . On that subject the interested reader may also wish to consult <ref> [JS95, KR88] </ref>. 19 It is useful to recall here that, from Section 2 above, h; i is a numerical pairing function and that we identify finite sets and initial segments of texts with their corresponding canonical indices (numbers).
Reference: [Kin77] <author> E. Kinber. </author> <title> On a theory of inductive inference. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 56, </volume> <pages> pages 435-440. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1977. </year> <month> 29 </month>
Reference-contexts: The study of learning nearly minimal size programs began with [Fre75] in the context of learning programs for functions (see also <ref> [Kin77, Che81, Che82, Fre90] </ref>). Definition 10 TxtMfex a b = fL j (9F)[F TxtMfex a b -identifies L]g.
Reference: [Kir92] <author> D. Kirsh. </author> <title> PDP learnability and innate knowledge of language. </title> <editor> In S. Davis, editor, </editor> <booktitle> Connectionism: Theory and Practice, </booktitle> <pages> pages 297-322. </pages> <publisher> Oxford University Press, </publisher> <address> NY, </address> <year> 1992. </year>
Reference-contexts: Can we get versions of our separation results robust in the sense of [Ful90b]? Much of the work in Gold-style learning theory on success criteria extending Gold's is motivated by attempts to assuage the negative results in this area. <ref> [Kir92] </ref> mentions a common argument to the effect that very strong negative results about language learnability in [Gol67] provide evidence that human language learning must involve some innately stored information! The negative results suggest, among other things, 1. that general purpose learning is not possible and 2. that alleged human general
Reference: [KLHM93] <author> S. Kapur, B. Lust, W. Harbert, and G. Martohardjono. </author> <title> Universal grammar and learnability theory: The case of binding domains and the `subset principle'. </title> <editor> In E. Reuland and W. Abraham, editors, </editor> <booktitle> Knowledge and Language, </booktitle> <volume> volume I, </volume> <pages> pages 185-216. </pages> <publisher> Kluwer, </publisher> <year> 1993. </year>
Reference-contexts: She calls the finite sets D featured tell tales. The theorem witnesses a severe constraint called the subset principle on learning from positive data. See [Ang80, Ber85] regarding the importance of the subset princi ple for circumventing overgeneralization in learning languages from positive data. See <ref> [KLHM93, Wex93] </ref> for discussion regarding the possible connection between this subset principle and a more traditionally linguistically oriented one in [MW87]. We let 2fl = fl. Theorem 3 Suppose I 2 f Fex a b ; Bc a g. Suppose F TxtI-identifies L.
Reference: [KR88] <author> S. Kurtz and J. Royer. </author> <title> Prudence in language learning. </title> <editor> In D. Haussler and L. Pitt, editors, </editor> <booktitle> Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 143-156. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1988. </year>
Reference-contexts: Prudent learning functions are those which never conjecture a grammar p without being able to learn W p . On that subject the interested reader may also wish to consult <ref> [JS95, KR88] </ref>. 19 It is useful to recall here that, from Section 2 above, h; i is a numerical pairing function and that we identify finite sets and initial segments of texts with their corresponding canonical indices (numbers).
Reference: [LW94] <author> S. Lange and P. Watson. </author> <title> Machine discovery in the presence of incomplete or ambiguous data. </title> <editor> In K. Jantke and S. Arikawa, editors, </editor> <booktitle> Algorithmic Learning Theory, volume 872 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 438-452. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Reinhardsbrunn Castle, Germany, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: In this way results about learning programs for functions can be interpreted as results about finding predictive explanations for phenomena | as results about scientific induction. For more on this see <ref> [BB75, CS83, CJS92, BCJS94, CJNM94, LW94] </ref>. Re the names of the learning criteria studied in the present paper, originally [CS83] `Ex' stood for `explanatory', `Fex' stood for `finitely explanatory', and Bc for `behaviorally correct'. 11 Proof.
Reference: [LZ92] <author> S. Lange and T. Zeugmann. </author> <title> Types of monotonic language learning and their characterization. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, Pennsylvania, </address> <pages> pages 377-390. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: complements a related characterization in [Ang80] of the uniformly decidable classes of recursive languages in TxtFex 0 1 . [BCJ96] provides a related characterization of the uniformly decidable classes of recursive languages in TxtFex fl 1 . [OSW86c, Page 30] characterizes learning by an agent which is not necessarily algorithmic. <ref> [Muk92, LZ92] </ref> contain characterizations of uniformly decidable classes of recursive languages for important special cases of TxtFex 0 1 . [dJK96] surprisingly characterizes the r.e. classes in TxtFex 0 1 , but by a condition more complicated than in the characterizations already mentioned. 13 Then (8p 2 P )[W p =
Reference: [MB72] <author> D. Moeser and A. Bregman. </author> <title> The role of reference in the acquisition of a miniature artificial language. </title> <journal> Journal of Verbal Learning and Verbal Behavior, </journal> <volume> 11 </volume> <pages> 759-769, </pages> <year> 1972. </year>
Reference-contexts: Hence, computability of cognition may be a pretty good model. Even if cognition is computable (although, perhaps too complicated for mere humans to figure out how its done), there are still problems realistically modeling human language learning with Gold's paradigm. <ref> [MB72, MB73] </ref> present empirical evidence that semantics in addition to positive information may be essential to human language learning. It seems clear that denotation and social reinforcers play crucial roles in the human case | but not in Gold's paradigm.
Reference: [MB73] <author> D. Moeser and A. Bregman. </author> <title> Imagery and language acquisition. </title> <journal> Journal of Verbal Learning and Verbal Behavior, </journal> <volume> 12 </volume> <pages> 91-98, </pages> <year> 1973. </year>
Reference-contexts: Hence, computability of cognition may be a pretty good model. Even if cognition is computable (although, perhaps too complicated for mere humans to figure out how its done), there are still problems realistically modeling human language learning with Gold's paradigm. <ref> [MB72, MB73] </ref> present empirical evidence that semantics in addition to positive information may be essential to human language learning. It seems clear that denotation and social reinforcers play crucial roles in the human case | but not in Gold's paradigm.
Reference: [McD92] <author> D. McDermott. </author> <title> Robot planning. </title> <journal> AI Magazine, </journal> <volume> 13(2) </volume> <pages> 55-79, </pages> <year> 1992. </year>
Reference-contexts: In the practical context of robot planning, Drew McDermott <ref> [McD92] </ref> says, "Learning makes the most sense when it is thought of as filling in the details in an algorithm that is already nearly right." In the context of function learning, [CKKK95] provides several models of learning from examples together with approximately correct programs.
Reference: [McN66] <author> D. McNeill. </author> <title> Developmental psycholinguistics. </title> <editor> In F. Smith and G. A. Miller, editors, </editor> <booktitle> The Genesis of Language, </booktitle> <pages> pages 15-84. </pages> <publisher> MIT Press, </publisher> <year> 1966. </year>
Reference-contexts: It seems clear that denotation and social reinforcers play crucial roles in the human case | but not in Gold's paradigm. In [Cas86] the report on Chapter 6 of [Ful85] is partly motivated by treating negative information as a more mathematically tractable possible substitute for semantic information. <ref> [McN66] </ref> notes that homes in which parents do supply improvements to child utterances (a subtle form of correction or negative information), there is increased speed of language acquisition.
Reference: [Men86] <author> E. Mendelson. </author> <title> Introduction to Mathematical Logic. </title> <address> Brooks-Cole, San Francisco, third edition, </address> <year> 1986. </year>
Reference-contexts: For example, Godel proved his famous Incompleteness Theorem by a self-reference argument <ref> [God86, Men86] </ref>, and his self-referential sentence providing an unprovable truth of, for example, First Order Peano Arithmetic (FOPA) is not natural | no number or combinatorial theorist would care whether it was true or false.
Reference: [Muk92] <author> Y. Mukouchi. </author> <title> Characterization of finite identification. </title> <booktitle> In Proceedings of the Third International Workshop on Analogical and Inductive Inference, </booktitle> <address> Dagstuhl Castle, Germany, </address> <pages> pages 260-267, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: complements a related characterization in [Ang80] of the uniformly decidable classes of recursive languages in TxtFex 0 1 . [BCJ96] provides a related characterization of the uniformly decidable classes of recursive languages in TxtFex fl 1 . [OSW86c, Page 30] characterizes learning by an agent which is not necessarily algorithmic. <ref> [Muk92, LZ92] </ref> contain characterizations of uniformly decidable classes of recursive languages for important special cases of TxtFex 0 1 . [dJK96] surprisingly characterizes the r.e. classes in TxtFex 0 1 , but by a condition more complicated than in the characterizations already mentioned. 13 Then (8p 2 P )[W p =
Reference: [MW87] <author> R. Manzini and K. </author> <title> Wexler. Parameters, binding theory and learnability. </title> <journal> Linguistic Inquiry, </journal> <volume> 18 </volume> <pages> 413-444, </pages> <year> 1987. </year>
Reference-contexts: See [Ang80, Ber85] regarding the importance of the subset princi ple for circumventing overgeneralization in learning languages from positive data. See [KLHM93, Wex93] for discussion regarding the possible connection between this subset principle and a more traditionally linguistically oriented one in <ref> [MW87] </ref>. We let 2fl = fl. Theorem 3 Suppose I 2 f Fex a b ; Bc a g. Suppose F TxtI-identifies L.
Reference: [MY78] <author> M. Machtey and P. Young. </author> <title> An Introduction to the General Theory of Algorithms. </title> <publisher> North-Holland, </publisher> <year> 1978. </year>
Reference-contexts: N denotes the set of natural numbers, f 0; 1; 2; . . . g. ' denotes a fixed acceptable programming system for the partial computable functions: N ! N <ref> [Rog58, MY78, Ric80, Ric81, Roy87] </ref>. ' p denotes the partial computable function computed by the program (with code number) p in the '-system. 2 Thanks to the device of Godel or code numbering 2 The acceptable systems are those universal programming systems such as Turing machines, C, and Lisp into which <p> Fix canonical indexings of the finite sets of natural numbers and of the finite initial segments of texts each 1-1 onto N <ref> [Rog67, MY78] </ref>. 7 In the following finite sets and segments are sometimes identified with their corresponding canonical indices. Hence, a reference to a least finite set or segment really refers to a finite set or segment with least canonical index. <p> F 0 is both partly set-driven and weakly b-ary order independent and 2. (8 r.e. L)[F RecTxtFex a b -identifies L ) F 0 TxtFex a b -identifies L]. Proof. Suppose pad is a 1-1 computable function such that (8n; p)[W pad (p;n) = W p ] <ref> [MY78, Roy87] </ref>. Intuitively, pad (p; 0); pad (p; 1); pad (p; 2); . . . are just padded variants of program p which have the same recognizing behavior as p but which differ from one another syntactically. Suppose F and b are given. Define F 0 on t thus.
Reference: [OSW82] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Ideal learning machines. </title> <journal> Cognitive Science, </journal> <volume> 6 </volume> <pages> 277-290, </pages> <year> 1982. </year>
Reference: [OSW83] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Note on a central lemma of learning theory. </title> <journal> Journal of Mathematical Psychology, </journal> <volume> 27 </volume> <pages> 86-92, </pages> <year> 1983. </year>
Reference-contexts: The exact connections to topology (actually, to Baire Category Theory and Banach-Mazur Games [Jec78]) we will not pursue herein, but, on that subject, the interested reader can consult <ref> [OSW83, OSW86b] </ref>. Definition 13 Suppose t T , T a text. <p> Definition 14 in L , content () L. Just below is a variant of a fundamental lemma from [OW82a] convenient for this paper. An original, not so general version of this lemma is from [BB75] (see also <ref> [OSW83, OSW86b] </ref>). Variations on its proof will appear in other proofs. 12 Lemma 1 Suppose L 2 E. Suppose, for each text T for L, an arbitrary T T is chosen.
Reference: [OSW84] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Learning theory and natural language. </title> <journal> Cognition, </journal> <volume> 17 </volume> <pages> 1-28, </pages> <year> 1984. </year>
Reference: [OSW86a] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> An analysis of a learning paradigm. </title> <editor> In W. Demopoulos and A. Marras, editors, </editor> <title> Language Learning and Concept Acquisition. </title> <publisher> Ablex Publ. Co., </publisher> <year> 1986. </year> <month> 30 </month>
Reference: [OSW86b] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems That Learn: An Introduction to Learning Theory for Cognitive and Computer Scientists. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass, </address> <year> 1986. </year>
Reference-contexts: Theorem 4 is the hardest theorem herein to prove, and the other theorems in Section 5 are proved by modifications, and/or simplifications of the proof of Theorem 4. Some of the theorems in Section 5 generalize predecessors for TxtFex 0 1 -identification <ref> [BB75, WC80, SR84, Ful85, OSW86b, Ful90a] </ref> but are much harder to prove. Some of the theorems in Section 5 are applied in the present paper and in other papers. <p> TxtFex 0 1 -identification is equivalent to Gold's [Gol67] seminal notion of identification, also referred to as TXTEX-identification in [CL82] and (indirectly) as INT in <ref> [OW82b, OW82a, OSW86b] </ref>. TxtFex a 1 -identification is just TXTEX a -identification from [CL82]. For n &gt; 0, TxtFex 0 n - identification is just our notion of TXTFEX n -identification from [Cas86]. <p> In a completely computable universe (which ours might be), there are really only recursive texts available to be presented to learning machines. Of course the universe may be such that, while all the language learners are computable, there are some non-computable phenomena too. As noted in <ref> [OSW86b] </ref>, since the utterances of children's caretakers depend heavily on external environmental events, such influences might introduce a random component into naturally occurring texts. <p> Under our direction Karen Ehrlich generalized the combinatorics of this proof to get TxtFex 0 2 TxtFex 0 3 . The combinatorics for this approach to the general case are unpleasant. <ref> [OSW86b] </ref> contains a recursion theorem proof of the immediately preceding corollary based on the proof in [OW82a], but the same combinatorial difficulties occur in attempting to generalize this proof. We sought a combinatorially cleaner self reference proof. <p> The exact connections to topology (actually, to Baire Category Theory and Banach-Mazur Games [Jec78]) we will not pursue herein, but, on that subject, the interested reader can consult <ref> [OSW83, OSW86b] </ref>. Definition 13 Suppose t T , T a text. <p> Definition 14 in L , content () L. Just below is a variant of a fundamental lemma from [OW82a] convenient for this paper. An original, not so general version of this lemma is from [BB75] (see also <ref> [OSW83, OSW86b] </ref>). Variations on its proof will appear in other proofs. 12 Lemma 1 Suppose L 2 E. Suppose, for each text T for L, an arbitrary T T is chosen. <p> The following stability property is useful for studying the criteria RecTxtFex a b . 14 t is, then, what is suggestively called a locking sequence <ref> [OSW86b] </ref>. 14 Definition 15 Suppose L is r.e. Then: L recursively b-stabilizes F , (8 recursive T for L)(9D j card (D) b)[F (T )+ = D]: The following lemma, useful to this paper, combines the topological with the algorithmic. It generalizes predecessors from [BB75, Ful85, OSW86b]. <p> Then: L recursively b-stabilizes F , (8 recursive T for L)(9D j card (D) b)[F (T )+ = D]: The following lemma, useful to this paper, combines the topological with the algorithmic. It generalizes predecessors from <ref> [BB75, Ful85, OSW86b] </ref>. Lemma 3 Suppose L is r.e. and recursively b-stabilizes F. Then (8 in L)(9D j card (D) b)(9t j t in L)(8t 0 t j t 0 in L)[F (t 0 ) 2 D]: (7) Proof. Suppose the hypothesis on L. <p> We consider herein some mathematical versions of this question. Several mathematical definitions have been given for various different notions of insensitivity to order, essentially for the case of TxtFex 0 1 -identification <ref> [BB75, WC80, SR84, Ful85, OSW86b, Ful90a] </ref>. We extend these definitions of insensitive or restricted learning functions naturally to the context of the vacillatory learning criteria of the present paper, 15 and we investigate the interesting mathematical questions of whether learning functions with these insensitivities or restrictions thereby lose learning power. <p> Answering many of these questions for the vacillatory criteria is much more difficult than for the TxtFex 0 1 case. 16 As noted above, we also apply some of our results in this section to help us prove results in this and other papers. Definition 16 (Wexler <ref> [WC80, OSW86b] </ref>) F is called set-driven , (8; t j content () = content (t ))[F () = F (t )]: 15 For the so-called order independence notions (Definition 19 below), in the interest of conceptual parsimony, but without loss of generality in theorems, we render them purely syntactically rather as <p> The next restriction defined, in effect, provides some degree of sensitivity to timing. Definition 17 (Schafer <ref> [SR84, OSW86b] </ref>, Fulk [Ful85, Ful90a]) F is called partly set-driven (synonym [Ful85, Ful90a]: rearrangement independent) , (8; t j kk = kt k ^ content () = content (t ))[F () = F (t )]. <p> Intuitively, F is set-driven (respectively, partly set-driven) iff, for each , F () depends only on the content of (respectively, depends only on the length and content of ). Schafer <ref> [SR84, OSW86b] </ref>, first, and Fulk [Ful85, Ful90a], later, independently showed that set-driven learning functions can't TxtFex 0 1 -identify some classes of languages that unrestricted learning functions can, but that partly set-driven learning functions do not restrict learning power with respect to TxtFex 0 1 -identification. <p> He interprets the difference in power between set-driven and partly set-driven learning functions as witnessing the need for time &gt; the size of the content of the input to "think" about the input. Osherson, Stob, and Weinstein <ref> [OSW86b] </ref> observed that the power of TxtFex 0 1 -identification on infinite r.e. languages is not limited by set-driveness. The following definition presents a convenient term paralleling that from Definition 15 above. Definition 18 A text T stabilizes F , F (T )+. <p> Definition 18 A text T stabilizes F , F (T )+. While identification of a language L requires identification for each order of presentation of (text for) L, the final (correct) grammar (s) converged to may be different for different texts. As is, in effect, noted in <ref> [OSW86b] </ref>, this would seem to be source of strength, since, for a learning machine to force the final grammars to be the same for each text, might involve its (algorithmically) recognizing grammar equivalence, i.e., recognizing f hx; yi j W x = W y g, but, as is well known [Rog67], <p> text for L stabilizes F)(9D of cardinality b)(8 texts T for L)[F (T )+ D]. 17 In fact, more importantly, since this set is 0 2 -complete [Rog67], it is not even algorithmically recognizable by a limiting [Soa87] or mind-changing procedure (but its complement is). 16 Osherson, Stob, and Weinstein <ref> [OSW86b] </ref>, adapting a related result of L. and M. Blum [BB75], essentially show that order independent learning functions can TxtFex 0 1 -identify the same classes of languages that unrestricted learning functions can. <p> n) approximation to (8 0 1 j content ( 0 ) L)[F [ 1 ; 0 ] D 1 ]: (9) (9) is a useful stability condition. fl) Once hD 1 ; 1 i is found: 18 In the present paper we do not consider the restriction to so-called prudence <ref> [OSW86b] </ref>, a primary concern of [Ful90a]. Prudent learning functions are those which never conjecture a grammar p without being able to learn W p .
Reference: [OSW86c] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems that Learn, An Introduction to Learning Theory for Cognitive and Computer Scientists. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: )[W F (t) = a L]: P = T for L 13 This complements a related characterization in [Ang80] of the uniformly decidable classes of recursive languages in TxtFex 0 1 . [BCJ96] provides a related characterization of the uniformly decidable classes of recursive languages in TxtFex fl 1 . <ref> [OSW86c, Page 30] </ref> characterizes learning by an agent which is not necessarily algorithmic. [Muk92, LZ92] contain characterizations of uniformly decidable classes of recursive languages for important special cases of TxtFex 0 1 . [dJK96] surprisingly characterizes the r.e. classes in TxtFex 0 1 , but by a condition more complicated than
Reference: [OW82a] <author> D. Osherson and S. Weinstein. </author> <title> Criteria for language learning. </title> <journal> Information and Control, </journal> <volume> 52 </volume> <pages> 123-138, </pages> <year> 1982. </year>
Reference-contexts: Osherson and Weinstein <ref> [OW82a] </ref> introduced the case where the number of final grammars is finite, but unbounded, and [CL82, OW82a] independently (see also [OW82b]) introduced the case where the number of final grammars is infinite (TxtBc-identification). We briefly introduced the case, discussed above, of up to n final grammars in [Cas86]. <p> Osherson and Weinstein [OW82a] introduced the case where the number of final grammars is finite, but unbounded, and <ref> [CL82, OW82a] </ref> independently (see also [OW82b]) introduced the case where the number of final grammars is infinite (TxtBc-identification). We briefly introduced the case, discussed above, of up to n final grammars in [Cas86]. <p> TxtFex 0 1 -identification is equivalent to Gold's [Gol67] seminal notion of identification, also referred to as TXTEX-identification in [CL82] and (indirectly) as INT in <ref> [OW82b, OW82a, OSW86b] </ref>. TxtFex a 1 -identification is just TXTEX a -identification from [CL82]. For n &gt; 0, TxtFex 0 n - identification is just our notion of TXTFEX n -identification from [Cas86]. <p> TxtFex a 1 -identification is just TXTEX a -identification from [CL82]. For n &gt; 0, TxtFex 0 n - identification is just our notion of TXTFEX n -identification from [Cas86]. Osherson and Weinstein <ref> [OW82a] </ref> were the first to define TxtFex 0 fl and TxtFex fl fl ; they called them BEXT and BFEXT, respectively. <p> Definitions 11 and 12 are from [CL82]. The a 2 f 0; fl g cases were independently introduced in <ref> [OW82a, OW82b] </ref>. The quantifier `8 1 k' means `for all but finitely many k 2 N'. Definition 11 F TxtBc a identifies L , (8 texts T for L)(8 1 k)[W F (T [k]) = a L]. <p> For some theoretical work instigated by Barzdin and dealing, in part, with eliminating dependence on self-referential examples, see Fulk's work on robust function learning in [Ful90b]. Corollary 2 (8a)[TxtFex a 1 TxtFex a 2 . . . TxtFex a fl ]. Corollary 3 (Osherson and Weinstein <ref> [OW82a] </ref>) TxtFex 0 1 TxtFex 0 fl . We announced in [Cas86] that we could prove TxtFex 0 1 TxtFex 0 2 by analyzing Osherson and Weinstein's proof of the immediately preceding corollary. <p> Under our direction Karen Ehrlich generalized the combinatorics of this proof to get TxtFex 0 2 TxtFex 0 3 . The combinatorics for this approach to the general case are unpleasant. [OSW86b] contains a recursion theorem proof of the immediately preceding corollary based on the proof in <ref> [OW82a] </ref>, but the same combinatorial difficulties occur in attempting to generalize this proof. We sought a combinatorially cleaner self reference proof. A later conversation about this with Royer led to Royer and Kurtz supplying us with essentially the self-referential sets we use in Theorem 1 above. <p> Corollary 5 (Case and Lynes [CL82]) TxtFex 0 1 TxtFex 1 1 . . . TxtFex fl 1 . Osherson and Weinstein <ref> [OW82a] </ref> independently showed the case of TxtFex 0 1 TxtFex fl 1 from the previous corollary. Corollary 6 (Osherson and Weinstein [OW82a]) TxtFex 0 fl TxtFex fl fl . Next are spelled out the connections between TxtFex a b and TxtBc a 0 . <p> Corollary 5 (Case and Lynes [CL82]) TxtFex 0 1 TxtFex 1 1 . . . TxtFex fl 1 . Osherson and Weinstein <ref> [OW82a] </ref> independently showed the case of TxtFex 0 1 TxtFex fl 1 from the previous corollary. Corollary 6 (Osherson and Weinstein [OW82a]) TxtFex 0 fl TxtFex fl fl . Next are spelled out the connections between TxtFex a b and TxtBc a 0 . Of course allowing infinitely many grammars in the limit is not so realistic for modeling language learning, but, nonetheless, it is mathematically interesting to make the comparisons. <p> Then, for example, from Definition 13 immediately above, F [; T ] is the set of all these output programs one sees from the time F is fed all of . Definition 14 in L , content () L. Just below is a variant of a fundamental lemma from <ref> [OW82a] </ref> convenient for this paper. An original, not so general version of this lemma is from [BB75] (see also [OSW83, OSW86b]). Variations on its proof will appear in other proofs. 12 Lemma 1 Suppose L 2 E. Suppose, for each text T for L, an arbitrary T T is chosen.
Reference: [OW82b] <author> D. Osherson and S. Weinstein. </author> <title> A note on formal learning theory. </title> <journal> Cognition, </journal> <volume> 11 </volume> <pages> 77-88, </pages> <year> 1982. </year>
Reference-contexts: Osherson and Weinstein [OW82a] introduced the case where the number of final grammars is finite, but unbounded, and [CL82, OW82a] independently (see also <ref> [OW82b] </ref>) introduced the case where the number of final grammars is infinite (TxtBc-identification). We briefly introduced the case, discussed above, of up to n final grammars in [Cas86]. <p> TxtFex 0 1 -identification is equivalent to Gold's [Gol67] seminal notion of identification, also referred to as TXTEX-identification in [CL82] and (indirectly) as INT in <ref> [OW82b, OW82a, OSW86b] </ref>. TxtFex a 1 -identification is just TXTEX a -identification from [CL82]. For n &gt; 0, TxtFex 0 n - identification is just our notion of TXTFEX n -identification from [Cas86]. <p> Definitions 11 and 12 are from [CL82]. The a 2 f 0; fl g cases were independently introduced in <ref> [OW82a, OW82b] </ref>. The quantifier `8 1 k' means `for all but finitely many k 2 N'. Definition 11 F TxtBc a identifies L , (8 texts T for L)(8 1 k)[W F (T [k]) = a L].
Reference: [Pen89] <author> R. Penrose. </author> <title> The Emperor's New Mind. </title> <publisher> Oxford University Press, </publisher> <address> NY, </address> <year> 1989. </year>
Reference-contexts: In another direction, we note that the proof of Theorem 1 permits a modification so that the rela tive density of output of all the final programs/grammars but one is as small as we like. Hence, the 25 Sources such as <ref> [Pen89, Pen94] </ref> sadly seem to have overlooked the important result in [dMSS56] that the expected I/O behavior of a Turing machine with random oracle subject to a computable probability distribution is computable (and constructively so). 25 performance vacillation may exist, but significant degradations in articulateness potential might be confined to rare
Reference: [Pen94] <author> R. Penrose. </author> <title> Shadows of the Mind. </title> <publisher> Oxford University Press, </publisher> <address> NY, </address> <year> 1994. </year>
Reference-contexts: In another direction, we note that the proof of Theorem 1 permits a modification so that the rela tive density of output of all the final programs/grammars but one is as small as we like. Hence, the 25 Sources such as <ref> [Pen89, Pen94] </ref> sadly seem to have overlooked the important result in [dMSS56] that the expected I/O behavior of a Turing machine with random oracle subject to a computable probability distribution is computable (and constructively so). 25 performance vacillation may exist, but significant degradations in articulateness potential might be confined to rare
Reference: [PH77] <author> J. Paris and L. Harrington. </author> <title> A mathematical incompleteness in Peano arithmetic. </title> <editor> In J. Barwise, editor, </editor> <booktitle> Handbook of Mathematical Logic. </booktitle> <publisher> North Holland, </publisher> <year> 1977. </year>
Reference-contexts: Empirical: Although Godel proved his famous First Incompleteness Theorem by a self-reference argument, many years afterwards, Paris and Harrington <ref> [PH77] </ref> and later Friedman [Sim85, Sim87] found quite natural examples of combinatorial truths of first order arithmetic not provable in FOPA. 9 In fairness, regarding the above informal thesis, we note, for example, that the Blum Speed-Up Theorem [Blu67a] was originally proved by a self-reference argument 10 , but natural witnesses
Reference: [Pin79] <author> S. Pinker. </author> <title> Formal models of language learning. </title> <journal> Cognition, </journal> <volume> 7 </volume> <pages> 217-283, </pages> <year> 1979. </year>
Reference: [Pyl84] <author> Z. Pylyshyn. </author> <title> Computation and Cognition: Toward A Foundation For Cognitive Science. </title> <publisher> MIT , Cambridge, </publisher> <address> MA, </address> <year> 1984. </year>
Reference-contexts: Angluin [Ang80, Ang82] presents other classes L natural from the perspective of formal language theory such that some M identifies each language in L. Many cognitive scientists seek to model all of cognition by computer program <ref> [Pyl84, JL88] </ref>, and Gold's model of language learning from text (positive information) by machine has been very influential in contemporary theories of natural language and in mathematical work explicitly motivated by its possible connection to human language learning (see, for example, [Pin79, WC80, Wex82, OSW82, OSW84, Ber85, Gle86, Cas86, OSW86a, OSW86b,
Reference: [RC94] <author> J. Royer and J. </author> <title> Case. </title> <booktitle> Subrecursive Programming Systems: Complexity and Succinctness. Progress in Theoretical Computer Science. </booktitle> <address> Birkhauser Boston, </address> <year> 1994. </year>
Reference-contexts: programs e (p 0 ); . . . ; e (p n ) to look in a common mirror to see which programs they are. [Cas94] provides a tutorial on thinking about and applying recursion theorems. 8 Herein, our application of the (n + 1)-ary recursion theorem (to 8 See <ref> [RC94] </ref> for discussion and applications of recursion theorems in severely resource-limited contexts. 9 prove Theorem 1) will be informal and the sequence p 0 ; . . . ; p n will be implicit. <p> grammars, then the hierarchy of Corollary 2 above collapses. 11 If one restricts ones attention to languages which are the (pairing function coded) graphs of total functions, then it is essentially shown (the a = 0 case in [BP73] and the a &gt; 0 cases in [CS83]) that 9 See <ref> [RC94] </ref> for an example from complexity theory. 10 See also Young's version in [You73] and our Operator Recursion Theorem variant in [Smi94]. 11 Just output every n-th grammar. 10 the hierarchy again collapses.
Reference: [Ric80] <author> G. Riccardi. </author> <title> The Independence of Control Structures in Abstract Programming Systems. </title> <type> PhD thesis, </type> <institution> State University of New York at Buffalo, </institution> <year> 1980. </year>
Reference-contexts: N denotes the set of natural numbers, f 0; 1; 2; . . . g. ' denotes a fixed acceptable programming system for the partial computable functions: N ! N <ref> [Rog58, MY78, Ric80, Ric81, Roy87] </ref>. ' p denotes the partial computable function computed by the program (with code number) p in the '-system. 2 Thanks to the device of Godel or code numbering 2 The acceptable systems are those universal programming systems such as Turing machines, C, and Lisp into which
Reference: [Ric81] <author> G. Riccardi. </author> <title> The independence of control structures in abstract programming systems. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 22 </volume> <pages> 107-143, </pages> <year> 1981. </year>
Reference-contexts: N denotes the set of natural numbers, f 0; 1; 2; . . . g. ' denotes a fixed acceptable programming system for the partial computable functions: N ! N <ref> [Rog58, MY78, Ric80, Ric81, Roy87] </ref>. ' p denotes the partial computable function computed by the program (with code number) p in the '-system. 2 Thanks to the device of Godel or code numbering 2 The acceptable systems are those universal programming systems such as Turing machines, C, and Lisp into which
Reference: [Rog58] <author> H. Rogers. </author> <title> Godel numberings of partial recursive functions. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 23 </volume> <pages> 331-341, </pages> <year> 1958. </year>
Reference-contexts: N denotes the set of natural numbers, f 0; 1; 2; . . . g. ' denotes a fixed acceptable programming system for the partial computable functions: N ! N <ref> [Rog58, MY78, Ric80, Ric81, Roy87] </ref>. ' p denotes the partial computable function computed by the program (with code number) p in the '-system. 2 Thanks to the device of Godel or code numbering 2 The acceptable systems are those universal programming systems such as Turing machines, C, and Lisp into which
Reference: [Rog67] <author> H. Rogers. </author> <title> Theory of Recursive Functions and Effective Computability. </title> <publisher> McGraw Hill, </publisher> <address> New York, 1967. </address> <publisher> Reprinted, MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Of course Gold's identification criterion above is just TxtFex 1 -identification. It is well known <ref> [Rog67] </ref> that equivalent grammars (e.g., p t ; p t+1 ; p t+2 ; . . . as above) can be so different from one another that in some cases it is not possible to prove in Zermelo-Frankel Set Theory [Jec78] that they are equivalent. <p> We characterized them as those universal systems for the partial computable functions in which one can implement any control structure [Roy87]. 4 <ref> [Rog67] </ref> we can treat languages over any finite alphabet as subsets of N. W p def = the domain of ' p , the r.e. language ( N) recognized (or enumerated) by program (grammar) p in the '-system [Rog67]. <p> computable functions in which one can implement any control structure [Roy87]. 4 <ref> [Rog67] </ref> we can treat languages over any finite alphabet as subsets of N. W p def = the domain of ' p , the r.e. language ( N) recognized (or enumerated) by program (grammar) p in the '-system [Rog67]. Definition 1 A language learning function is a computable mapping from finite sequences, of natural numbers and #'s, into (Godel numbers of ) programs (grammars) in the '-system. E denotes the class of all r.e. languages ( N). <p> Let mingrammar (L) denote min (f p j W p = L g). 5 The r.e. languages are characterized as those which are the content of some recursive text <ref> [Rog67] </ref>. 6 See further discussion in Section 3 below. 6 Definition 9 F TxtMfex a b -identifies a class of languages L , (9 recursive h)(8L 2 L)[F TxtFex a b identifies L ^ (8T for L)(8p 2 F (T ))[p h (mingrammar (L))]]. h in Definition 9 plays the role <p> Fix canonical indexings of the finite sets of natural numbers and of the finite initial segments of texts each 1-1 onto N <ref> [Rog67, MY78] </ref>. 7 In the following finite sets and segments are sometimes identified with their corresponding canonical indices. Hence, a reference to a least finite set or segment really refers to a finite set or segment with least canonical index. <p> Also, when we compare finite sets or segments by &lt;; ; . . . we are comparing their corresponding canonical indices. h; i denotes a fixed pairing function <ref> [Rog67] </ref>, a computable, surjective and injective mapping from N fi N into N. For A N, A denotes (N A), the complement of A. <p> For each non-empty r.e. language, there are continuum many such orders (texts) [Hal74], yet only countably many recursive ones (since there are only countably many Turing Machine programs for computing the recursive texts <ref> [Rog67] </ref>). In a completely computable universe (which ours might be), there are really only recursive texts available to be presented to learning machines. Of course the universe may be such that, while all the language learners are computable, there are some non-computable phenomena too. <p> Proposition 1 There is a learning function which RecTxtFex 0 1 -identifies a language which it fails to TxtFex 0 1 -identify. Proof. Let K = f p j p 2 W p g, a well know r.e., not recursive set <ref> [Rog67] </ref>. Suppose k is a recursive function with range K [Rog67]. Let K s = f k (s 0 ) j s 0 &lt; s g. C A denotes the characteristic function of A N, the function 1 on A and 0 off A. <p> Proof. Let K = f p j p 2 W p g, a well know r.e., not recursive set <ref> [Rog67] </ref>. Suppose k is a recursive function with range K [Rog67]. Let K s = f k (s 0 ) j s 0 &lt; s g. C A denotes the characteristic function of A N, the function 1 on A and 0 off A. Clearly (8x)[lim s!1 C K s (x) = C K (x)]. <p> In the proof of Theorem 1, to handle the self-referential character of L n+1 , we employ the (n + 1)-ary recursion theorem, a folk theorem generalizing the Kleene Recursion Theorem <ref> [Rog67, Page 214] </ref> and the Smullyan Double Recursion Theorem [Smu61]; it is also a consequence of our Operator Recursion Theorem [Cas74], an infinitary analog of the finite-arity recursion theorems. <p> Plausibility: self-reference arguments lay bare an underlying simplest reason for the theorems they prove <ref> [Rog67, Cas94] </ref>; if a theorem is true for such a simple reason, the "space" of reasons for its truth may be broad enough to admit natural examples. <p> [OSW86b], this would seem to be source of strength, since, for a learning machine to force the final grammars to be the same for each text, might involve its (algorithmically) recognizing grammar equivalence, i.e., recognizing f hx; yi j W x = W y g, but, as is well known <ref> [Rog67] </ref>, this set is not algorithmically recognizable (r.e.) (nor is its complement). 17 Order independent machines are insensitive to which text is used for L in that their final grammars depend only on L, not on the order of presentation. <p> We call a learning function, F, weakly b-ary order independent , (8 L r.e. j some text for L stabilizes F)(9D of cardinality b)(8 texts T for L)[F (T )+ D]. 17 In fact, more importantly, since this set is 0 2 -complete <ref> [Rog67] </ref>, it is not even algorithmically recognizable by a limiting [Soa87] or mind-changing procedure (but its complement is). 16 Osherson, Stob, and Weinstein [OSW86b], adapting a related result of L. and M. <p> The exploding sticks are quite like the injuries in a recursion-theoretic priority argument [Soa87]. The sticks correspond to the 's and their extensions, and one should think of them as segments of branches in an infinite branching, upward pointing tree similar to the finite branching (rightward pointing) one in <ref> [Rog67, Page 157] </ref>. For each input t to F 0 , when the while loop finishes, it provides some sequence of successively longer joined together sticks 1 . . . m , with m the final value of i. <p> Suppose F TxtFex 2m fl -identifies L. We will construct an F 0 which TxtBc m -identifies L, and, then, it will suffice to prove the furthermore clause. Define F 0 on t thus. First calculate p = F (t ). By Kleene's S-m-n Theorem <ref> [Rog67] </ref>, find p t such that W p t = ((W p [ content (t )) the m least numbers not in content (t )). Output p t . Suppose T is a text for L 2 L. Let D = F (T )+.
Reference: [Roy87] <author> J. Royer. </author> <title> A Connotational Theory of Program Structure. </title> <booktitle> Lecture Notes in Computer Science 273. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1987. </year>
Reference-contexts: N denotes the set of natural numbers, f 0; 1; 2; . . . g. ' denotes a fixed acceptable programming system for the partial computable functions: N ! N <ref> [Rog58, MY78, Ric80, Ric81, Roy87] </ref>. ' p denotes the partial computable function computed by the program (with code number) p in the '-system. 2 Thanks to the device of Godel or code numbering 2 The acceptable systems are those universal programming systems such as Turing machines, C, and Lisp into which <p> We characterized them as those universal systems for the partial computable functions in which one can implement any control structure <ref> [Roy87] </ref>. 4 [Rog67] we can treat languages over any finite alphabet as subsets of N. W p def = the domain of ' p , the r.e. language ( N) recognized (or enumerated) by program (grammar) p in the '-system [Rog67]. <p> F 0 is both partly set-driven and weakly b-ary order independent and 2. (8 r.e. L)[F RecTxtFex a b -identifies L ) F 0 TxtFex a b -identifies L]. Proof. Suppose pad is a 1-1 computable function such that (8n; p)[W pad (p;n) = W p ] <ref> [MY78, Roy87] </ref>. Intuitively, pad (p; 0); pad (p; 1); pad (p; 2); . . . are just padded variants of program p which have the same recognizing behavior as p but which differ from one another syntactically. Suppose F and b are given. Define F 0 on t thus.
Reference: [Sim85] <author> S. Simpson. </author> <title> Nonprovability of certain combinatorial properties of finite trees. </title> <editor> In L. Harrington, M. Morley, A. Schedrov, and S. Simpson, editors, </editor> <booktitle> Harvey Fried-man's Research on the Foundations of Mathematics, </booktitle> <pages> pages 87-117. </pages> <publisher> North Holland, </publisher> <year> 1985. </year>
Reference-contexts: Empirical: Although Godel proved his famous First Incompleteness Theorem by a self-reference argument, many years afterwards, Paris and Harrington [PH77] and later Friedman <ref> [Sim85, Sim87] </ref> found quite natural examples of combinatorial truths of first order arithmetic not provable in FOPA. 9 In fairness, regarding the above informal thesis, we note, for example, that the Blum Speed-Up Theorem [Blu67a] was originally proved by a self-reference argument 10 , but natural witnesses to even exponential speedup
Reference: [Sim87] <author> S. Simpson. </author> <title> Unprovable theorems and fast-growing functions. </title> <editor> In S. Simpson, editor, </editor> <booktitle> Logic and Combinatorics, AMS Comtemporary Mathematics, </booktitle> <pages> pages 359-394. </pages> <year> 1987. </year>
Reference-contexts: Empirical: Although Godel proved his famous First Incompleteness Theorem by a self-reference argument, many years afterwards, Paris and Harrington [PH77] and later Friedman <ref> [Sim85, Sim87] </ref> found quite natural examples of combinatorial truths of first order arithmetic not provable in FOPA. 9 In fairness, regarding the above informal thesis, we note, for example, that the Blum Speed-Up Theorem [Blu67a] was originally proved by a self-reference argument 10 , but natural witnesses to even exponential speedup
Reference: [Smi94] <author> C. Smith. </author> <title> A Recursive Introduction to the Theory of Computation. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: function coded) graphs of total functions, then it is essentially shown (the a = 0 case in [BP73] and the a &gt; 0 cases in [CS83]) that 9 See [RC94] for an example from complexity theory. 10 See also Young's version in [You73] and our Operator Recursion Theorem variant in <ref> [Smi94] </ref>. 11 Just output every n-th grammar. 10 the hierarchy again collapses. Hence, in the case of "scientific inference," i.e., the case of learning programs for computable functions, there is no power in vacillation. 12 Therefore, Corollary 2 above is very sensitive to minor perturbations.
Reference: [Smu61] <author> R. </author> <title> Smullyan. </title> <journal> Theory of Formal Systems. Annals of Mathematics Studies, </journal> <volume> No. 47. </volume> <year> 1961. </year>
Reference-contexts: In the proof of Theorem 1, to handle the self-referential character of L n+1 , we employ the (n + 1)-ary recursion theorem, a folk theorem generalizing the Kleene Recursion Theorem [Rog67, Page 214] and the Smullyan Double Recursion Theorem <ref> [Smu61] </ref>; it is also a consequence of our Operator Recursion Theorem [Cas74], an infinitary analog of the finite-arity recursion theorems.
Reference: [Soa87] <author> R. Soare. </author> <title> Recursively Enumerable Sets and Degrees. </title> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: learning function, F, weakly b-ary order independent , (8 L r.e. j some text for L stabilizes F)(9D of cardinality b)(8 texts T for L)[F (T )+ D]. 17 In fact, more importantly, since this set is 0 2 -complete [Rog67], it is not even algorithmically recognizable by a limiting <ref> [Soa87] </ref> or mind-changing procedure (but its complement is). 16 Osherson, Stob, and Weinstein [OSW86b], adapting a related result of L. and M. Blum [BB75], essentially show that order independent learning functions can TxtFex 0 1 -identify the same classes of languages that unrestricted learning functions can. <p> The exploding sticks are quite like the injuries in a recursion-theoretic priority argument <ref> [Soa87] </ref>. The sticks correspond to the 's and their extensions, and one should think of them as segments of branches in an infinite branching, upward pointing tree similar to the finite branching (rightward pointing) one in [Rog67, Page 157].
Reference: [Spe94] <author> E. Spelke. </author> <title> Initial knowledge: Six suggestions. </title> <journal> Cognition, </journal> <volume> 50 </volume> <pages> 431-445, </pages> <year> 1994. </year> <month> 31 </month>
Reference-contexts: language learning must involve some innately stored information! The negative results suggest, among other things, 1. that general purpose learning is not possible and 2. that alleged human general purpose learning is an illusion brought about by our having innate information stored for a large and varied collection of domains <ref> [GBC + 91, Spe94] </ref>.
Reference: [SR84] <author> G. Schafer-Richter. </author> <title> Uber Eingabeabhangigkeit und Komplexitat von Inferenzs--trategien. </title> <type> PhD thesis, </type> <institution> RWTH Aachen, </institution> <year> 1984. </year>
Reference-contexts: Theorem 4 is the hardest theorem herein to prove, and the other theorems in Section 5 are proved by modifications, and/or simplifications of the proof of Theorem 4. Some of the theorems in Section 5 generalize predecessors for TxtFex 0 1 -identification <ref> [BB75, WC80, SR84, Ful85, OSW86b, Ful90a] </ref> but are much harder to prove. Some of the theorems in Section 5 are applied in the present paper and in other papers. <p> We consider herein some mathematical versions of this question. Several mathematical definitions have been given for various different notions of insensitivity to order, essentially for the case of TxtFex 0 1 -identification <ref> [BB75, WC80, SR84, Ful85, OSW86b, Ful90a] </ref>. We extend these definitions of insensitive or restricted learning functions naturally to the context of the vacillatory learning criteria of the present paper, 15 and we investigate the interesting mathematical questions of whether learning functions with these insensitivities or restrictions thereby lose learning power. <p> The next restriction defined, in effect, provides some degree of sensitivity to timing. Definition 17 (Schafer <ref> [SR84, OSW86b] </ref>, Fulk [Ful85, Ful90a]) F is called partly set-driven (synonym [Ful85, Ful90a]: rearrangement independent) , (8; t j kk = kt k ^ content () = content (t ))[F () = F (t )]. <p> Intuitively, F is set-driven (respectively, partly set-driven) iff, for each , F () depends only on the content of (respectively, depends only on the length and content of ). Schafer <ref> [SR84, OSW86b] </ref>, first, and Fulk [Ful85, Ful90a], later, independently showed that set-driven learning functions can't TxtFex 0 1 -identify some classes of languages that unrestricted learning functions can, but that partly set-driven learning functions do not restrict learning power with respect to TxtFex 0 1 -identification.
Reference: [TM87] <author> T. Toffoli and N. Margolus. </author> <title> Cellular Automata Machines. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: We have considered (among other possibilities) computable models of learning on computable data sequences. The whole universe or humanly significant portions of it may be computable and/or discrete. Such possibilities are taken seriously, for example, in <ref> [Zus69, Tof77, TM87, Fey82, Cas92b, Cas86, CRS94] </ref>.
Reference: [Tof77] <author> T. Toffoli. </author> <title> Cellular automata machines. </title> <type> Technical Report 208, </type> <institution> Comp. Comm. Sci. Dept., University of Michigan, </institution> <year> 1977. </year>
Reference-contexts: We have considered (among other possibilities) computable models of learning on computable data sequences. The whole universe or humanly significant portions of it may be computable and/or discrete. Such possibilities are taken seriously, for example, in <ref> [Zus69, Tof77, TM87, Fey82, Cas92b, Cas86, CRS94] </ref>.
Reference: [WC80] <author> K. Wexler and P. Culicover. </author> <title> Formal Principles of Language Acquisition. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass, </address> <year> 1980. </year>
Reference-contexts: Theorem 4 is the hardest theorem herein to prove, and the other theorems in Section 5 are proved by modifications, and/or simplifications of the proof of Theorem 4. Some of the theorems in Section 5 generalize predecessors for TxtFex 0 1 -identification <ref> [BB75, WC80, SR84, Ful85, OSW86b, Ful90a] </ref> but are much harder to prove. Some of the theorems in Section 5 are applied in the present paper and in other papers. <p> We consider herein some mathematical versions of this question. Several mathematical definitions have been given for various different notions of insensitivity to order, essentially for the case of TxtFex 0 1 -identification <ref> [BB75, WC80, SR84, Ful85, OSW86b, Ful90a] </ref>. We extend these definitions of insensitive or restricted learning functions naturally to the context of the vacillatory learning criteria of the present paper, 15 and we investigate the interesting mathematical questions of whether learning functions with these insensitivities or restrictions thereby lose learning power. <p> Answering many of these questions for the vacillatory criteria is much more difficult than for the TxtFex 0 1 case. 16 As noted above, we also apply some of our results in this section to help us prove results in this and other papers. Definition 16 (Wexler <ref> [WC80, OSW86b] </ref>) F is called set-driven , (8; t j content () = content (t ))[F () = F (t )]: 15 For the so-called order independence notions (Definition 19 below), in the interest of conceptual parsimony, but without loss of generality in theorems, we render them purely syntactically rather as <p> We had several painful experiences, for example, with subtly incorrect, alternative constructions to the one in the proof of Theorem 4 below. 15 <ref> [WC80] </ref> essentially notes that set driven learning functions are insensitive to time (unlike text learnability). The next restriction defined, in effect, provides some degree of sensitivity to timing.
Reference: [Wex82] <author> K. </author> <title> Wexler. On extensional learnability. </title> <journal> Cognition, </journal> <volume> 11 </volume> <pages> 89-95, </pages> <year> 1982. </year>
Reference: [Wex93] <author> K. </author> <title> Wexler. The subset principle is an intensional principle. </title> <editor> In E. Reuland and W. Abraham, editors, </editor> <booktitle> Knowledge and Language, </booktitle> <volume> volume I, </volume> <pages> pages 217-239. </pages> <publisher> Kluwer, </publisher> <year> 1993. </year>
Reference-contexts: She calls the finite sets D featured tell tales. The theorem witnesses a severe constraint called the subset principle on learning from positive data. See [Ang80, Ber85] regarding the importance of the subset princi ple for circumventing overgeneralization in learning languages from positive data. See <ref> [KLHM93, Wex93] </ref> for discussion regarding the possible connection between this subset principle and a more traditionally linguistically oriented one in [MW87]. We let 2fl = fl. Theorem 3 Suppose I 2 f Fex a b ; Bc a g. Suppose F TxtI-identifies L.
Reference: [Wie77] <author> R. Wiehagen. </author> <title> Identification of formal languages. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 53, </volume> <pages> pages 571-579. </pages> <publisher> Springer, </publisher> <year> 1977. </year>
Reference-contexts: suitably clever machine M might be able to exploit the recursiveness of texts to learn larger classes of languages than any machine required to succeed on arbitrary texts, but Corollary 1 in Section 3 below implies that this is not the case (generalizing the b = 1 case essentially from <ref> [Wie77, BB75] </ref>). We say, then, that the restriction to recursive texts is circumvented. 3 Angluin, in her seminal paper [Ang80], presents a severe constraint on TxtFex 1 -identification of classes of languages: the subset principle. <p> It is, then, interesting and important to compare learning power where success is required on all texts with the cases where it is only required on all recursive texts. Wiehagen <ref> [Wie77] </ref> essentially notes that RecTxtFex 0 1 = TxtFex 0 1 (a related result was first proved in [BB75]), and [CL82] essentially observes that (8a)[RecTxtFex a 1 = TxtFex a 1 ]. <p> Gold [Gol67] proved Corollary 8 with TxtFex 0 1 in place of TxtBc fl and was clearly concerned that his result meant that only rather puny language classes could be learned from positive data. However, Wiehagen <ref> [Wie77] </ref> presents a class of r.e. languages in TxtFex 0 1 which contains a finite variant of each r.e. language. Wiehagen's class is obviously quite hefty. Angluin presents examples natural from the perspective of formal language theory that also are in TxtFex 0 1 [Ang80, Ang82].
Reference: [WZ92] <author> R. Wiehagen and T. Zeugmann. </author> <title> Too much information can be too much for learning efficiently. </title> <editor> In K. Jantke, editor, </editor> <booktitle> Proceedings of the Third International Workshop on Analogical and Inductive Inference, volume 642 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 72-86. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Dagstuhl Castle, Germany, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: For language learning, bounding complexity of learning machines as in [DS86] or [CJS94a] largely remains to be explored. Translating relative solvability results into relative feasibility results, as in <ref> [WZ92] </ref>, would be very interesting to pursue in the context of the present paper. In Section 5 there are several results about no loss of learning power in passing from some learning function F to an insensitive or restricted learning function F 0 .
Reference: [You73] <author> P. Young. </author> <title> Easy constructions in complexity theory: Gap and speed-up theorems. </title> <booktitle> Proceedings of the AMS, </booktitle> <volume> 37 </volume> <pages> 555-563, </pages> <year> 1973. </year>
Reference-contexts: ones attention to languages which are the (pairing function coded) graphs of total functions, then it is essentially shown (the a = 0 case in [BP73] and the a &gt; 0 cases in [CS83]) that 9 See [RC94] for an example from complexity theory. 10 See also Young's version in <ref> [You73] </ref> and our Operator Recursion Theorem variant in [Smi94]. 11 Just output every n-th grammar. 10 the hierarchy again collapses.
Reference: [Zus69] <author> K. Zuse. Rechnender Raum. Vieweg, Braunshweig, </author> <year> 1969. </year> <title> Translated as Calculating Space, </title> <type> Tech. </type> <institution> Transl. AZT-70-164-GEMIT, MIT Project MAC, </institution> <year> 1970. </year> <month> 32 </month>
Reference-contexts: We have considered (among other possibilities) computable models of learning on computable data sequences. The whole universe or humanly significant portions of it may be computable and/or discrete. Such possibilities are taken seriously, for example, in <ref> [Zus69, Tof77, TM87, Fey82, Cas92b, Cas86, CRS94] </ref>.
References-found: 101

