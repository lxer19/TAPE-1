URL: http://www.cs.Helsinki.fi/research/cosco/Articles/unicom97.ps.gz
Refering-URL: 
Root-URL: 
Title: On the Accuracy of Stochastic Complexity Approximations  
Author: Petri Kontkanen, Petri Myllymaki, Tomi Silander, Henry Tirri 
Address: P.O.Box  FIN-00014 University of Helsinki, Finland  
Affiliation: Complex Systems Computation Group (CoSCo)  Department of Computer Science  
Note: Pp. 103-117 in Proceedings of the Causal Models and Statistical Learning Seminar (London, UK,  
Email: cosco@cs.Helsinki.FI,  
Phone: 26,  
Web: http://www.cs.Helsinki.FI/research/cosco/  
Date: March 1997).  
Abstract: Stochastic complexity of a data set is defined as the shortest possible code length for the data obtainable by using some fixed set of models. This measure is of great theoretical and practical importance as a tool for tasks such as determining model complexity, or performing predictive inference. Unfortunately for cases where the data has missing information, computing the stochastic complexity requires marginalizing (integrating) over the missing data, which results even in the discrete data case to computing a sum with an exponential number of terms. Therefore in most cases the stochastic complexity measure has to be approximated. In this paper we will investigate empirically the performance of some of the most common stochastic complexity approximations in an attempt to understand their small sample behavior in the incomplete data framework. In earlier empirical evaluations the problem of not knowing the actual stochastic complexity for incomplete data was circumvented either by using synthetic data, or by comparing the behavior of the stochastic complexity approximation methods to crossvalidated prediction error, approaches which both suffer from validity problems. Our comparison is based on the novel idea of using demonstrably representative small samples from real data sets, and then calculating by "brute force" the exponential sums. This allows for the first time a comparison between the true stochastic complexity and its approximations with real-world data.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Akaike. </author> <title> Information theory and an extension of the maximum likelihood principle. In B.N. </title> <editor> Petrox and F. Caski, editors, </editor> <booktitle> Proceedings of the Second International Symposium on Information Theory, </booktitle> <pages> pages 267-281, </pages> <address> Budapest, </address> <year> 1973. </year> <note> Akademiai Kiado. </note>
Reference-contexts: This is clearly an infeasible task, so in most practical situations stochastic complexity measure has to be approximated. Although several methods for computing the evidence approximately has been suggested in the literature (see e.g., <ref> [1, 3, 4, 14, 22, 25, 27] </ref>), the quality of most of these approximations is not well known, except for some asymptotic results. On the other hand, in many real life situations we are typically faced with relatively small data sets. <p> In the Akaike information criterion (AIC) <ref> [1] </ref>, the stochastic complexity is approximated by The BIC criterion can also been given a formulation in the MDL setting, as showed in [22]. The BIC (and AIC) approximations can also be used as a motivation for another approximation method.
Reference: [2] <author> J.M. Bernardo and A.F.M Smith. </author> <title> Bayesian theory. </title> <publisher> John Wiley, </publisher> <year> 1994. </year>
Reference-contexts: More importantly, for both 103 of these tasks, stochastic complexity can be shown to be an optimal criteria both in information theoretic and Bayesian probability theory frameworks (see the discussion in <ref> [2, 22] </ref>). In this paper, we focus on an incomplete data situation, where the sample data contains some missing information.
Reference: [3] <author> H. Bozdogan. </author> <title> On the information-based measure of covariance complexity and its applications to the evaluation of multivariate linear models. </title> <journal> Communications in Statistics Theory and Methods, </journal> <volume> 19(1) </volume> <pages> 221-278, </pages> <year> 1990. </year>
Reference-contexts: This is clearly an infeasible task, so in most practical situations stochastic complexity measure has to be approximated. Although several methods for computing the evidence approximately has been suggested in the literature (see e.g., <ref> [1, 3, 4, 14, 22, 25, 27] </ref>), the quality of most of these approximations is not well known, except for some asymptotic results. On the other hand, in many real life situations we are typically faced with relatively small data sets.
Reference: [4] <author> P. Cheeseman and J. Stutz. </author> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, chapter 6. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, </address> <year> 1996. </year>
Reference-contexts: This is clearly an infeasible task, so in most practical situations stochastic complexity measure has to be approximated. Although several methods for computing the evidence approximately has been suggested in the literature (see e.g., <ref> [1, 3, 4, 14, 22, 25, 27] </ref>), the quality of most of these approximations is not well known, except for some asymptotic results. On the other hand, in many real life situations we are typically faced with relatively small data sets. <p> ^ fi (D obs ) ^ fi (D obs ; ~ D mis ), we get SC (D obs j M ) = log P (D obs j M ) P (D obs ; ~ D mis j M ) ! This Cheeseman-Stutz (CS) approximation, used in the Autoclass system <ref> [4] </ref>, has in many empirical tests turned out to be quite accurate, yet a computationally efficient approximation of stochastic complexity [5, 17]. In Section 4, the above described stochastic complexity approximation methods (4)- (6) are empirically evaluated by using public domain real world data.
Reference: [5] <author> D.M. Chickering and D. Heckerman. </author> <title> Efficient approximations for the marginal likelihood of incomplete data given a Bayesian network. </title> <editor> In E. Horvitz and F. Jensen, editors, </editor> <booktitle> Proceedings of the 12th Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 158-168, </pages> <address> Portland, Oregon, August 1996. </address> <publisher> Morgan Kaufmann Publishers. </publisher> <pages> 115 </pages>
Reference-contexts: quality of the stochastic complexity approximations empirically, we encounter the following difficult methodological problem: if calculating the stochastic complexity measure exactly is not feasible for any reasonable sized data set, how do we then evaluate the approximation quality if we do not have any reference measure? In earlier empirical evaluations <ref> [5, 17] </ref>, the problem of not knowing the actual stochastic complexity for incomplete data was circumvented either by using synthetic data, or by comparing the behavior of the stochastic complexity approximation methods to crossvalidated prediction error. <p> M ) = log P (D obs j M ) P (D obs ; ~ D mis j M ) ! This Cheeseman-Stutz (CS) approximation, used in the Autoclass system [4], has in many empirical tests turned out to be quite accurate, yet a computationally efficient approximation of stochastic complexity <ref> [5, 17] </ref>. In Section 4, the above described stochastic complexity approximation methods (4)- (6) are empirically evaluated by using public domain real world data. <p> From (10) we saw that due to the exponential summation, calculating the stochastic complexity measure for a finite mixture model class M K is not feasible for any reasonable sized data set. How do we then evaluate the approximation quality if we do not have any reference measure? In <ref> [5, 17] </ref>, the problem of not knowing the actual stochastic complexity for incomplete data is circumvented by using synthetic data in a model class selection problem. <p> The amounts of data needed to represent the underlying distribution are substantial (thousands of data vectors for parameter spaces of only moderate dimensionality), which defies the whole purpose of finding out the approximation quality for small sample sizes encountered in real life. The results reported in <ref> [5, 17] </ref> clearly reflect this difficulty. Since we cannot calculate the stochastic complexity for real data sets, an alternative solution, commonly suggested for model class selection problems, is to compare the stochastic approximation methods against some other, more easily computable measure, such as the leave-one-out crossvalidation measure [26]. <p> Several observations can be made from these results. First, it is clear that the approximations are sensitive to the missing data estimate and behave very poorly with low posterior models. The results also support the observations in <ref> [5, 16] </ref> where C-S was found to outperform both AIC and BIC.
Reference: [6] <author> G. Chow. </author> <title> A comparison of the information and posterior probability criteria for model selection. </title> <journal> Journal of Econometrics, </journal> <volume> 16 </volume> <pages> 21-33, </pages> <year> 1981. </year>
Reference-contexts: In the beginning of this paper we have argued that stochastic complexity provides a principled measure for model selection and prediction tasks. However, we know that the crossvalidation measure is in fact an average value of the last term in the sequential decomposition of the actual stochastic complexity <ref> [6] </ref>, under random re-orderings of the data. Thus it will be very hard to judge the quality of other approximations based on such a coarse measure.
Reference: [7] <author> G. Cooper and E. Herskovits. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: Assuming that the parameter vectors ff and ki are independent, the joint prior distribution of all the parameters is Di ( 1 ; : : : ; K ) k=1 i=1 As shown in <ref> [7, 13] </ref>, with the above assumptions, the posterior probability of complete data (D obs ; D mis ) for a K-cluster finite mixture model class M K can be written as P (D obs ; D mis j M K ) = P (D obs ; D mis j fi; M
Reference: [8] <author> M.H. </author> <title> DeGroot. Optimal statistical decisions. </title> <publisher> McGraw-Hill, </publisher> <year> 1970. </year>
Reference-contexts: Since the family of Dirichlet densities is conjugate (see e.g. <ref> [8] </ref>) to the family of mul-tinomials, i.e., the functional form of parameter distribution remains invariant in the prior-to-posterior transformation, we assume that the prior distributions of the parameters are from this family.
Reference: [9] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: In order to be able to use the AIC, BIC,C-S, and LN methods for approximating SC, we need a method for providing estimates of the missing data. In the experiments reported here, we used the Expectation Maximization (EM) <ref> [9] </ref> algorithm for computing ~ D mis and ^ fi (D obs ; ~ D mis j M ). As EM is a locally optimal search algorithm, the algorithm was repeated in each case 20 times, starting from randomly chosen initial locations.
Reference: [10] <author> B.S. </author> <title> Everitt and D.J. Hand. Finite Mixture Distributions. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: log P (D obs ; D mis j M )dD mis Z P (D obs ; D mis j M )dD mis ; (7) where d ( ~ D mis ; D mis ) is some distance function. 4 Empirical results 4.1 The problem In the finite mixture model family <ref> [10, 29] </ref>, the probability distribution for a data vector ~ d is written as a weighted sum of mixture distributions, P ( ~ d) = k=1 P (Y = y k )P ( ~ djY = y k ) ; (8) where Y denotes a latent clustering random variable, the values
Reference: [11] <author> S. Haykin. </author> <title> Neural Networks: A Comprehensive Foundation. </title> <publisher> IEEE Press/Macmillan College Publishing Company, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: For instance, the problem of choosing the proper model complexity (in order to avoid overfitting) can be solved by using the stochastic complexity measure. This type of model selection is common in many machine learning approaches, e.g., in selecting the proper number of hidden units in feed-forward neural networks <ref> [11] </ref>, or in pruning of a decision tree [19]. Stochastic complexity offers also a theoretically solid framework for computing optimal predictive distributions, as will be shown in Section 2.
Reference: [12] <author> D. Heckerman and D. Chickering. </author> <title> A comparison of scientific and engineering criteria for bayesian model selection. </title> <booktitle> In Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 275-281, </pages> <address> Ft. Lauderdale, Florida, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: Thus it will be very hard to judge the quality of other approximations based on such a coarse measure. The relationship between the stochastic complexity measure (called "scientific criterion") and crossvalidation measure (called "engineering criterion") together with some experimental results in model class selection tasks are discussed in <ref> [12] </ref>. In an earlier study [15] we have demonstrated that for some commonly used benchmark data sets, on the average very small random samples (less than 10%) are sufficient to construct good predictive models.
Reference: [13] <author> D. Heckerman, D. Geiger, </author> <title> and D.M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20(3) </volume> <pages> 197-243, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: Assuming that the parameter vectors ff and ki are independent, the joint prior distribution of all the parameters is Di ( 1 ; : : : ; K ) k=1 i=1 As shown in <ref> [7, 13] </ref>, with the above assumptions, the posterior probability of complete data (D obs ; D mis ) for a K-cluster finite mixture model class M K can be written as P (D obs ; D mis j M K ) = P (D obs ; D mis j fi; M
Reference: [14] <author> R.E. Kass and A.E. Raftery. </author> <title> Bayes factors. </title> <type> Technical Report 254, </type> <institution> Department of Statistics, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: This is clearly an infeasible task, so in most practical situations stochastic complexity measure has to be approximated. Although several methods for computing the evidence approximately has been suggested in the literature (see e.g., <ref> [1, 3, 4, 14, 22, 25, 27] </ref>), the quality of most of these approximations is not well known, except for some asymptotic results. On the other hand, in many real life situations we are typically faced with relatively small data sets. <p> In this paper we compare empirically the four stochastic complexity approximation methods described below. The Bayesian information criterion (BIC) <ref> [25, 14] </ref>, also known as the Schwarz criterion, can be derived by expanding the logarithm of the integrand in (3) around the posterior mode ^ fi (D obs ), ^ fi (D obs ) = arg max P (fi j D obs ); which yields SC (D obs j M )
Reference: [15] <author> P. Kontkanen, P. Myllymaki, T. Silander, H. Tirri, and P. Grunwald. </author> <title> Comparing predictive inference methods for discrete domains. </title> <booktitle> In Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 311-318, </pages> <address> Ft. Lauderdale, Florida, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: However, as pointed out in Section 4.2, using either of these approaches does not necessarily provide correct information about the quality of the approximations. The key to solving this dilemma lies in an earlier study <ref> [15] </ref>, where we observed that for certain real world data sets we can obtain good predictive models already with very small samples of the full training set. In such cases we do not loose any essential modeling information by replacing the full data set by a small sample. <p> An example of such a simple model family is the Naive Bayes classifier, in which case the model family consists of a single model class (so the model class selection problem disappears), and the predictive distribution (2) can be computed extremely efficiently, as showed in <ref> [15] </ref>. Nevertheless, in many practical cases the integral is of the form which is not computationally feasible. This situation occurs even with relative simple model families, if the sample data D is incomplete, i.e., if it contains missing information. <p> The relationship between the stochastic complexity measure (called "scientific criterion") and crossvalidation measure (called "engineering criterion") together with some experimental results in model class selection tasks are discussed in [12]. In an earlier study <ref> [15] </ref> we have demonstrated that for some commonly used benchmark data sets, on the average very small random samples (less than 10%) are sufficient to construct good predictive models.
Reference: [16] <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Comparing Bayesian model class selection criteria by discrete finite mixtures. </title> <editor> In D. Dowe, K. Korb, and J. Oliver, editors, </editor> <booktitle> Information, Statistics and Induction in Science, </booktitle> <pages> pages 364-374, </pages> <booktitle> Proceedings of the ISIS'96 Conference, </booktitle> <address> Melbourne, Australia, </address> <month> August </month> <year> 1996. </year> <title> World Scientific, </title> <publisher> Singapore. </publisher>
Reference-contexts: Several observations can be made from these results. First, it is clear that the approximations are sensitive to the missing data estimate and behave very poorly with low posterior models. The results also support the observations in <ref> [5, 16] </ref> where C-S was found to outperform both AIC and BIC.
Reference: [17] <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Experimenting with the Cheeseman-Stutz evidence approximation for predictive modeling and data mining. </title> <booktitle> In Proceedings of Tenth International FLAIRS Conference (to appear), </booktitle> <address> Daytona Beach, Florida, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: quality of the stochastic complexity approximations empirically, we encounter the following difficult methodological problem: if calculating the stochastic complexity measure exactly is not feasible for any reasonable sized data set, how do we then evaluate the approximation quality if we do not have any reference measure? In earlier empirical evaluations <ref> [5, 17] </ref>, the problem of not knowing the actual stochastic complexity for incomplete data was circumvented either by using synthetic data, or by comparing the behavior of the stochastic complexity approximation methods to crossvalidated prediction error. <p> M ) = log P (D obs j M ) P (D obs ; ~ D mis j M ) ! This Cheeseman-Stutz (CS) approximation, used in the Autoclass system [4], has in many empirical tests turned out to be quite accurate, yet a computationally efficient approximation of stochastic complexity <ref> [5, 17] </ref>. In Section 4, the above described stochastic complexity approximation methods (4)- (6) are empirically evaluated by using public domain real world data. <p> From (10) we saw that due to the exponential summation, calculating the stochastic complexity measure for a finite mixture model class M K is not feasible for any reasonable sized data set. How do we then evaluate the approximation quality if we do not have any reference measure? In <ref> [5, 17] </ref>, the problem of not knowing the actual stochastic complexity for incomplete data is circumvented by using synthetic data in a model class selection problem. <p> The amounts of data needed to represent the underlying distribution are substantial (thousands of data vectors for parameter spaces of only moderate dimensionality), which defies the whole purpose of finding out the approximation quality for small sample sizes encountered in real life. The results reported in <ref> [5, 17] </ref> clearly reflect this difficulty. Since we cannot calculate the stochastic complexity for real data sets, an alternative solution, commonly suggested for model class selection problems, is to compare the stochastic approximation methods against some other, more easily computable measure, such as the leave-one-out crossvalidation measure [26].
Reference: [18] <editor> D. Michie, D.J. Spiegelhalter, and C.C. Taylor, editors. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <address> London, </address> <year> 1994. </year>
Reference-contexts: For our experiments, we have chosen nine data sets from the collection discussed in [28]. Many of these data sets appeared in the extensive comparative study performed by the StatLog-project <ref> [18] </ref>, and are standard benchmarks in the machine learning community.
Reference: [19] <author> J.R. Quinlan. </author> <title> Improved use of continuous attributes in C4.5. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 77-90, </pages> <year> 1996. </year> <month> 116 </month>
Reference-contexts: This type of model selection is common in many machine learning approaches, e.g., in selecting the proper number of hidden units in feed-forward neural networks [11], or in pruning of a decision tree <ref> [19] </ref>. Stochastic complexity offers also a theoretically solid framework for computing optimal predictive distributions, as will be shown in Section 2.
Reference: [20] <author> A. Raftery. </author> <title> Approximate Bayes factors and accounting for model uncertainty in gen-eralized linear models. </title> <type> Technical Report 255, </type> <institution> Department of Statistics, University of Washington, </institution> <year> 1993. </year>
Reference-contexts: In earlier similar studies <ref> [20, 24] </ref>, the model family used has either been too restricted for extending the results to real-world domains, or too general to allow an exact solution to be used for the comparisons.
Reference: [21] <author> J. Rissanen. </author> <title> Stochastic complexity. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 49(3) </volume> <pages> 223-239 and 252-265, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction Rissanen <ref> [21, 22] </ref> has defined the stochastic complexity of a data set as the shortest possible code length for the data obtainable by using some fixed set of models. This measure can be used as a tool for solving several difficult problems of great practical importance. <p> In this framework, fixing a specific model, i.e., the parametric form and the specific parameter values, determines a single probability distribution. Consequently, in the following we treat M as a set of models, instead of as a set of distributions. Rissanen <ref> [21] </ref> defined the stochastic complexity SC (D j M) of a dataset D relative to a set of models M as the shortest code length for D that can be obtained with the help of models M.
Reference: [22] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publishing Company, </publisher> <address> New Jersey, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Rissanen <ref> [21, 22] </ref> has defined the stochastic complexity of a data set as the shortest possible code length for the data obtainable by using some fixed set of models. This measure can be used as a tool for solving several difficult problems of great practical importance. <p> More importantly, for both 103 of these tasks, stochastic complexity can be shown to be an optimal criteria both in information theoretic and Bayesian probability theory frameworks (see the discussion in <ref> [2, 22] </ref>). In this paper, we focus on an incomplete data situation, where the sample data contains some missing information. <p> This is clearly an infeasible task, so in most practical situations stochastic complexity measure has to be approximated. Although several methods for computing the evidence approximately has been suggested in the literature (see e.g., <ref> [1, 3, 4, 14, 22, 25, 27] </ref>), the quality of most of these approximations is not well known, except for some asymptotic results. On the other hand, in many real life situations we are typically faced with relatively small data sets. <p> Rissanen [21] defined the stochastic complexity SC (D j M) of a dataset D relative to a set of models M as the shortest code length for D that can be obtained with the help of models M. In <ref> [22] </ref>, the corresponding code length was defined as SC (D j M) = log P (D j M) = log P (D j fi; M)P (fi j M)dfi; (1) where the integration goes over all the possible models fi in M. <p> In the Akaike information criterion (AIC) [1], the stochastic complexity is approximated by The BIC criterion can also been given a formulation in the MDL setting, as showed in <ref> [22] </ref>. The BIC (and AIC) approximations can also be used as a motivation for another approximation method.
Reference: [23] <author> J. Rissanen. </author> <title> Fisher information and stochastic complexity. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42(1) </volume> <pages> 40-47, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Although Rissanen derived the stochastic complexity measure by using information-theoretic arguments, from (1) we see that the stochastic complexity has a direct link to Bayesian probability theory as the code length is defined with the help of the marginal likelihood (or evidence) P (D j M). Rissanen has recently <ref> [23] </ref> introduced an alternative coding scheme for stochastic complexity, which produces for some data sets D even shorter codes than (1), but in this paper we will focus on the "old" formulation of stochastic complexity. Stochastic complexity is an interesting measure as it offers solutions to two practically important questions.
Reference: [24] <author> S. Rosenkranz. </author> <title> The Bayes factors for model evaluation in hierarchical Poisson model for area counts. </title> <type> PhD thesis, </type> <institution> Department of Biostatistics, University of Wash-ington, </institution> <year> 1992. </year>
Reference-contexts: In earlier similar studies <ref> [20, 24] </ref>, the model family used has either been too restricted for extending the results to real-world domains, or too general to allow an exact solution to be used for the comparisons.
Reference: [25] <author> G. Schwarz. </author> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics, </journal> <volume> 6 </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference-contexts: This is clearly an infeasible task, so in most practical situations stochastic complexity measure has to be approximated. Although several methods for computing the evidence approximately has been suggested in the literature (see e.g., <ref> [1, 3, 4, 14, 22, 25, 27] </ref>), the quality of most of these approximations is not well known, except for some asymptotic results. On the other hand, in many real life situations we are typically faced with relatively small data sets. <p> In this paper we compare empirically the four stochastic complexity approximation methods described below. The Bayesian information criterion (BIC) <ref> [25, 14] </ref>, also known as the Schwarz criterion, can be derived by expanding the logarithm of the integrand in (3) around the posterior mode ^ fi (D obs ), ^ fi (D obs ) = arg max P (fi j D obs ); which yields SC (D obs j M )
Reference: [26] <author> M. Stone. </author> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 36 </volume> <pages> 111-147, </pages> <year> 1974. </year>
Reference-contexts: Since we cannot calculate the stochastic complexity for real data sets, an alternative solution, commonly suggested for model class selection problems, is to compare the stochastic approximation methods against some other, more easily computable measure, such as the leave-one-out crossvalidation measure <ref> [26] </ref>. In the beginning of this paper we have argued that stochastic complexity provides a principled measure for model selection and prediction tasks.
Reference: [27] <author> L. Tierney and J. Kadane. </author> <title> Accurate approximations for posterior moments and marginal densities. </title> <journal> J. Amer. Statist. Ass., </journal> <volume> 81 </volume> <pages> 82-86, </pages> <year> 1986. </year>
Reference-contexts: This is clearly an infeasible task, so in most practical situations stochastic complexity measure has to be approximated. Although several methods for computing the evidence approximately has been suggested in the literature (see e.g., <ref> [1, 3, 4, 14, 22, 25, 27] </ref>), the quality of most of these approximations is not well known, except for some asymptotic results. On the other hand, in many real life situations we are typically faced with relatively small data sets.
Reference: [28] <author> H. Tirri, P. Kontkanen, and P. Myllymaki. </author> <title> Probabilistic instance-based learning. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 507-515. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1996. </year>
Reference-contexts: However, with the missing data introduced by model assumptions, calculating it in practice requires computing over an exponential sum. It should also be noted that although the finite mixtures are conceptually simple models, our earlier empirical results (see e.g. <ref> [28] </ref>) show very good performance in predictive inference tasks, when compared to results obtained by more complex model families, such as neural networks or decision trees. <p> Thus by using this "brute force method" we are able to calculate the stochastic complexity exactly, giving us a measure against which we can then compare the approximations for real data sets. For our experiments, we have chosen nine data sets from the collection discussed in <ref> [28] </ref>. Many of these data sets appeared in the extensive comparative study performed by the StatLog-project [18], and are standard benchmarks in the machine learning community.
Reference: [29] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1985. </year> <month> 117 </month>
Reference-contexts: log P (D obs ; D mis j M )dD mis Z P (D obs ; D mis j M )dD mis ; (7) where d ( ~ D mis ; D mis ) is some distance function. 4 Empirical results 4.1 The problem In the finite mixture model family <ref> [10, 29] </ref>, the probability distribution for a data vector ~ d is written as a weighted sum of mixture distributions, P ( ~ d) = k=1 P (Y = y k )P ( ~ djY = y k ) ; (8) where Y denotes a latent clustering random variable, the values
References-found: 29

