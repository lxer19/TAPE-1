URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/leemon/www/papers/nips94/nips94.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/leemon/www/papers/index.html
Root-URL: 
Title: Advantage Updating Applied to a Differential Game  
Author: Mance E. Harmon Leemon C. Baird III* A. Harry Klopf 
Keyword: Category: Control, Navigation, and Planning Keywords: Reinforcement Learning, Advantage Updating, Dynamic Programming, Differential Games  
Note: Avionics Circle Wright-Patterson Air Force Base,  
Address: WL/AAAT Bldg.  OH 45433-7301  
Affiliation: Wright Laboratory  Wright Laboratory  Wright Laboratory  
Email: harmonme@aa.wpafb.mil  baird@cs.usafa.af.mil  klopfah@aa.wpafb.mil  
Phone: 635 2185  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> Baird, L.C. </author> <year> (1993). </year> <title> Advantage updating Wright-Patterson Air Force Base, </title> <institution> OH. (Wright Laboratory Technical Report WL-TR-93-1146, </institution> <note> available from the Defense Technical information Center, </note> <institution> Cameron Station, </institution> <address> Alexandria, VA 22304-6145). </address>
Reference-contexts: 1 ADVANTAGE UPDATING The advantage updating algorithm <ref> (Baird, 1993) </ref> is a reinforcement learning algorithm in which two types of information are stored. For each state x, the value V (x) is stored, representing an estimate of the total discounted return expected when starting in state x and performing optimal actions. <p> If A max converges to zero in every state, the advantage function is said to be normalized. Advantage updating has been shown to learn faster than Q-learning (Watkins, 1989), especially for continuous-time problems <ref> (Baird, 1993) </ref>. If advantage updating (Baird, 1993) is used to control a deterministic system, there are two equations that are the equivalent of the Bellman equation in value iteration (Bertsekas, 1987). These are a pair of two simultaneous equations (Baird, 1993): ( ) t u ' D (3) u where a <p> If A max converges to zero in every state, the advantage function is said to be normalized. Advantage updating has been shown to learn faster than Q-learning (Watkins, 1989), especially for continuous-time problems <ref> (Baird, 1993) </ref>. If advantage updating (Baird, 1993) is used to control a deterministic system, there are two equations that are the equivalent of the Bellman equation in value iteration (Bertsekas, 1987). These are a pair of two simultaneous equations (Baird, 1993): ( ) t u ' D (3) u where a time step is of duration <p> to learn faster than Q-learning (Watkins, 1989), especially for continuous-time problems <ref> (Baird, 1993) </ref>. If advantage updating (Baird, 1993) is used to control a deterministic system, there are two equations that are the equivalent of the Bellman equation in value iteration (Bertsekas, 1987). These are a pair of two simultaneous equations (Baird, 1993): ( ) t u ' D (3) u where a time step is of duration D t, and performing action u in state x results in a reinforcement of R and a transition to state x t+ D t .
Reference: <author> Baird, L.C., & Harmon, M. E. </author> <title> (In preparation). Residual gradient algorithms Wright-Patterson Air Force Base, </title> <type> OH. (Wright Laboratory Technical report). </type>
Reference: <author> Baird, L.C., & Klopf, A. H. </author> <year> (1993). </year> <title> Reinforcement learning with high-dimensional, continuous actions Wright-Patterson Air Force Base, </title> <institution> OH. (Wright Laboratory technical report WL-TR-93-1147, </institution> <note> available from the Defense Technical information Center, </note> <institution> Cameron Station, </institution> <address> Alexandria, VA 22304-6145). </address>
Reference-contexts: 1 ADVANTAGE UPDATING The advantage updating algorithm <ref> (Baird, 1993) </ref> is a reinforcement learning algorithm in which two types of information are stored. For each state x, the value V (x) is stored, representing an estimate of the total discounted return expected when starting in state x and performing optimal actions. <p> If A max converges to zero in every state, the advantage function is said to be normalized. Advantage updating has been shown to learn faster than Q-learning (Watkins, 1989), especially for continuous-time problems <ref> (Baird, 1993) </ref>. If advantage updating (Baird, 1993) is used to control a deterministic system, there are two equations that are the equivalent of the Bellman equation in value iteration (Bertsekas, 1987). These are a pair of two simultaneous equations (Baird, 1993): ( ) t u ' D (3) u where a <p> If A max converges to zero in every state, the advantage function is said to be normalized. Advantage updating has been shown to learn faster than Q-learning (Watkins, 1989), especially for continuous-time problems <ref> (Baird, 1993) </ref>. If advantage updating (Baird, 1993) is used to control a deterministic system, there are two equations that are the equivalent of the Bellman equation in value iteration (Bertsekas, 1987). These are a pair of two simultaneous equations (Baird, 1993): ( ) t u ' D (3) u where a time step is of duration <p> to learn faster than Q-learning (Watkins, 1989), especially for continuous-time problems <ref> (Baird, 1993) </ref>. If advantage updating (Baird, 1993) is used to control a deterministic system, there are two equations that are the equivalent of the Bellman equation in value iteration (Bertsekas, 1987). These are a pair of two simultaneous equations (Baird, 1993): ( ) t u ' D (3) u where a time step is of duration D t, and performing action u in state x results in a reinforcement of R and a transition to state x t+ D t .
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic programming: Deterministic and stochastic models. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Advantage updating has been shown to learn faster than Q-learning (Watkins, 1989), especially for continuous-time problems (Baird, 1993). If advantage updating (Baird, 1993) is used to control a deterministic system, there are two equations that are the equivalent of the Bellman equation in value iteration <ref> (Bertsekas, 1987) </ref>.
Reference: <author> Bradtke, S. J. </author> <year> (1993). </year> <title> Reinforcement Learning Applied to Linear Quadratic Regulation. </title> <booktitle> Proceedings of the 5th annual Conference on Neural Information Processing Systems </booktitle> . 
Reference-contexts: It is possible to derive an algorithm that has guaranteed convergence for a quadratic function approximation system <ref> (Bradtke, 1993) </ref>, but that algorithm is specific to quadratic systems. One solution to this problem is to derive a learning algorithm to perform gradient descent on the mean squared Bellman residuals given in (5) and (6). This is called the residual gradient form of an algorithm.
Reference: <author> Isaacs, </author> <title> Rufus (1965). Differential games. </title> <address> New York: </address> <publisher> John Wiley and Sons, Inc. </publisher>
Reference-contexts: The differential game in this paper was deterministic, so this was not needed here. 3 THE SIMULATION 3.1 GAME DEFINITION We employed a linear-quadratic, differential game <ref> (Isaacs, 1965) </ref> for comparing Q-learning to advantage updating, and for comparing the algorithms in their residual gradient forms. The game has two players, a missile and a plane, as in games described by Rajan, Prasad, and Rao (1980) and Millington (1991).
Reference: <author> Millington, P. J. </author> <year> (1991). </year> <title> Associative reinforcement learning for optimal control. </title> <type> Unpublished master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference: <author> Rajan, N., Prasad, U. R., and Rao, N. J. </author> <year> (1980). </year> <title> Pursuit-evasion of two aircraft in a horizontal plane. </title> <journal> Journal of Guidance and Control. </journal> <volume> 3 (3), May-June, </volume> <pages> 261-267. </pages>
Reference: <author> Rumelhart, D., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning representations by backpropagating errors. </title> <journal> Nature. </journal> <volume> 323 , 9 October, </volume> <pages> 533-536. </pages>
Reference-contexts: , ) ( ) ( ) ( , ) max ( , ) max ( , ) a A x u W u t - As a simple, gradient-descent algorithm, equation (7) is guaranteed to converge to the correct answer for a deterministic system, in the same sense that backpropagation <ref> (Rumelhart, Hinton, Williams, 1986) </ref> is guaranteed to converge. However, if the system is nondeterministic, then it is necessary to independently generate two different possible "next states" x t+ D t for a given action u t performed in a given state x t .
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning. </booktitle>
Reference-contexts: This ensures that the weight change is an unbiased estimator of the true Bellman-residual gradient, but requires a system such as in Dyna <ref> (Sutton, 1990) </ref> to generate the second x t+ D t .
Reference: <author> Tesauro, G. </author> <year> (1990). </year> <title> Neurogammon: A neural-network backgammon program. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <pages> 3 , (pp. 33-40). </pages> <address> San Diego, CA. </address>
Reference-contexts: There are two Bellman residuals, (5) and (6), so the residual gradient algorithm must perform gradient descent on the sum of the two squared Bellman residuals. It has been found to be useful to combine reinforcement learning algorithms with function approximation systems <ref> (Tesauro, 1990 & 1992) </ref>.
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <booktitle> Machine Learning, </booktitle> <pages> 8(3/4) , 279-292. </pages>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> Doctoral thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, England. </address>
Reference-contexts: The notation A x A x u ( ) max ( , ) = (2) defines A max (x). If A max converges to zero in every state, the advantage function is said to be normalized. Advantage updating has been shown to learn faster than Q-learning <ref> (Watkins, 1989) </ref>, especially for continuous-time problems (Baird, 1993). If advantage updating (Baird, 1993) is used to control a deterministic system, there are two equations that are the equivalent of the Bellman equation in value iteration (Bertsekas, 1987).
References-found: 13

