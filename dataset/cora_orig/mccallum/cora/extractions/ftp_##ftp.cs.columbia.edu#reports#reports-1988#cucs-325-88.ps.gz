URL: ftp://ftp.cs.columbia.edu/reports/reports-1988/cucs-325-88.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1988.html
Root-URL: http://www.cs.columbia.edu
Title: A Survey of Software Fault Tolerance Techniques  
Author: Jonathan M. Smith 
Keyword: Terminology, techniques for building reliable systems, and fault tolerance  
Note: are discussed.  
Address: New York, NY 10027  
Affiliation: Computer Science Department, Columbia University,  
Abstract: While a scientific consensus on the measurement of software reliability has not been reached, software systems are sufficiently pervasive that ``software components'' of larger systems must be reliable, since dependence is placed on them. Fault tolerant systems utilize redundant components to mitigate the effects of component failures, and thus create a system which is more reliable than a single component. This idea can be applied to software systems as well. Several techniques for designing fault tolerant software systems are discussed and assessed qualitatively, where ``software fault'' refers to what is more commonly known as a bug. The assumptions, relative merits, available experimental results, and implementation experience are discussed for each technique. This leads us to some conclusions about the state of the field. 
Abstract-found: 1
Intro-found: 1
Reference: - -- 
Reference: [Anderson1981a] <author> T. Anderson and P.A. Lee, </author> <title> Fault Tolerance: </title> <booktitle> Principles and Practice, Prentice-Hall International (1981). </booktitle>
Reference-contexts: Fault tolerance stems from the observation that it is rarely possible to carry fault prevention to its logical end, perfection. Fault tolerance recognizes that faults may exist, and fault tolerance strategies attempt to prevent faults from causing system failures. Four phases of fault tolerance are identified <ref> [Anderson1981a] </ref>, Error detection: The effects of the fault presumably manifest themselves in an error which results in an observable failure, which can be detected. - -- Damage assessment: A flagged error may only be the ``tip of the iceberg''; due to information propagation, much state information other than that directly associated <p> Some practical issues of constructing such systems are being addressed by the DEDIX [Avizienis1985b] system under construction at UCLA. 5.3. Recovery Block The Recovery Block method was originated by Horning, Randell, and others [Randell1975a, Horning1974a] at the University of Newcastle upon Tyne. Others associated with that research group <ref> [Anderson1981a] </ref> have discussed recovery blocks and produced special-purpose hardware [Lee1980a] to support efficient implementation of the scheme. A compendium of this work has been compiled by Shrivastava [Shrivastava1985a]. The recovery block is a language construct for encapsulating a program segment which is to be performed reliably.
Reference: [Anderson1982a] <author> T. Anderson and P.A. Lee, </author> <title> ``Fault Tolerance Terminology Proposals,'' </title> <booktitle> in Proceedings, 12th International Symposium on Fault-Tolerant Computing, </booktitle> <address> Los Angeles, CA. </address> <month> (June </month> <year> 1982), </year> <pages> pp. 29-33. </pages>
Reference-contexts: The failure set of an implementation S is F = -x fi D:S (x fi ) S (x fi )-. Thus, by this definition, the failure set of S is . A slightly different set of definitions is given by Anderson and Lee <ref> [Anderson1982a] </ref>, including some notions from Avizienis and Kelly [Avizienis1984a]. A system consists of a set of components which interact under the control of a design. A component is simply another system.
Reference: [Anderson1986a] <author> T. Anderson, </author> <title> ``A structured decision mechanism for Diverse Software,'' </title> <booktitle> in Proceedings, IEEE 1986 Symposium on Reliability in Distributed Software and Database Systems. </booktitle> <year> (1986), </year> <pages> pp. 125-129. </pages>
Reference-contexts: Several techniques, discussed in the following sections, have been proposed as methods of combining systems and selecting results. Anderson <ref> [Anderson1986a] </ref> has proposed a framework which includes these combining and selection techniques. The combining mechanism combines the multiple software versions. The selecting mechanism selects a result from the combined multiple versions of software. These two mechanisms comprise a software fault tolerance scheme. 5.2.
Reference: [Avizienis1977a] <author> A. Avizienis and L. Chen, </author> <title> ``On the implementation of N-version programming for software fault tolerance during execution,'' </title> <booktitle> in Proceedings, COMPSAC 77, 1st IEEE-CS International Computer Software and Applications Conference, </booktitle> <address> Chicago, IL (November 8-11 1977), </address> <pages> pp. 149-155. </pages>
Reference-contexts: The combining mechanism combines the multiple software versions. The selecting mechanism selects a result from the combined multiple versions of software. These two mechanisms comprise a software fault tolerance scheme. 5.2. N-Version Programming N-version Programming is a technique originated and advocated by Avizienis, et al. <ref> [Chen1978a, Avizienis1977a, Avizienis1985a] </ref>. Much research on the technique has been published, starting in the mid-1970s. N-version programming is conceptually similar to N-modular redundancy, a hardware reliability [Avizienis1978a, Siewiorek1982a] technique.
Reference: [Avizienis1978a] <author> A. Avizienis, </author> <title> ``Fault tolerance: The survival attribute of digital systems,'' </title> <booktitle> Proceedings of the IEEE 66, </booktitle> <pages> pp. </pages> <month> 1109-1125 (October </month> <year> 1978). </year>
Reference-contexts: N-Version Programming N-version Programming is a technique originated and advocated by Avizienis, et al. [Chen1978a, Avizienis1977a, Avizienis1985a]. Much research on the technique has been published, starting in the mid-1970s. N-version programming is conceptually similar to N-modular redundancy, a hardware reliability <ref> [Avizienis1978a, Siewiorek1982a] </ref> technique.
Reference: [Avizienis1984a] <author> A. Avizienis and John P.J. Kelly, </author> <title> ``Fault Tolerance by Design Diversity: Concepts and Experiments,'' </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. </pages> <month> 67-80 (August </month> <year> 1984). </year>
Reference-contexts: Thus, by this definition, the failure set of S is . A slightly different set of definitions is given by Anderson and Lee [Anderson1982a], including some notions from Avizienis and Kelly <ref> [Avizienis1984a] </ref>. A system consists of a set of components which interact under the control of a design. A component is simply another system. <p> The basic idea behind all of fault tolerance is the use of redundancy. The idea of using redundancy to construct reliable systems from unreliable components was first described by Von Neumann [Neumann1956a] in 1956. Redundancy (multiple copies) is used to detect faults and mask failures. Avizienis and Kelly <ref> [Avizienis1984a] </ref> suggest that the different types of redundancy possible in a computation are: g repetition g replication (hardware) g logic (software) These are of course old ideas applied in a new setting; for example, the same methods are used in experimental science to gauge the reliability of results; thus science sets
Reference: [Avizienis1985a] <author> A. Avizienis, </author> <title> ``The N-Version Approach to Fault-Tolerant Software,'' </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pp. </pages> <month> 1491-1501 (December </month> <year> 1985). </year>
Reference-contexts: The combining mechanism combines the multiple software versions. The selecting mechanism selects a result from the combined multiple versions of software. These two mechanisms comprise a software fault tolerance scheme. 5.2. N-Version Programming N-version Programming is a technique originated and advocated by Avizienis, et al. <ref> [Chen1978a, Avizienis1977a, Avizienis1985a] </ref>. Much research on the technique has been published, starting in the mid-1970s. N-version programming is conceptually similar to N-modular redundancy, a hardware reliability [Avizienis1978a, Siewiorek1982a] technique.
Reference: [Avizienis1985b] <author> A. Avizienis, P. Gunningberg, J. P. J. Kelly, L. Strigini, P. J. Traverse, K. S. Tso, and U. Voges, </author> <title> ``The UCLA DEDIX system: a Distributed Testbed for Multiple-Version Software,'' </title> <booktitle> in Digest of FTCS-15, the 15th International Symposium on Fault-Tolerant Computing, </booktitle> <address> Ann Arbor, Michigan (June 1985), </address> <pages> pp. 126-134. </pages>
Reference-contexts: The number of coincident errors, ( m above ), deter mines the set of x fi D such that N-version programming will fail. Some practical issues of constructing such systems are being addressed by the DEDIX <ref> [Avizienis1985b] </ref> system under construction at UCLA. 5.3. Recovery Block The Recovery Block method was originated by Horning, Randell, and others [Randell1975a, Horning1974a] at the University of Newcastle upon Tyne.
Reference: [Brooks1975a] <author> F. P. Brooks, Jr., </author> <title> The Mythical Man-Month, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass. </address> <year> (1975). </year>
Reference-contexts: To be fair, the techniques are intuitively attractive, and once the process of using the automated verification's error detection capabilities became familiar, confidence in the techniques would improve. Structured programming techniques are disciplines, which when applied can lead to faster development times and fewer errors <ref> [Brooks1975a] </ref>, yet they do not guarantee that specifications are followed, nor that the modular decomposition done is the correct one for the problem at hand.
Reference: [Chen1978a] <author> L. Chen and A. Avizienis, </author> <title> ``N-version programming: A fault tolerance approach to reliability of software operation,'' </title> <booktitle> in Digest, 8th Annual International Conference on Fault-Tolerant Computing, </booktitle> <address> Toulouse, France (June 21-23 1978), </address> <pages> pp. 3-9. </pages>
Reference-contexts: The combining mechanism combines the multiple software versions. The selecting mechanism selects a result from the combined multiple versions of software. These two mechanisms comprise a software fault tolerance scheme. 5.2. N-Version Programming N-version Programming is a technique originated and advocated by Avizienis, et al. <ref> [Chen1978a, Avizienis1977a, Avizienis1985a] </ref>. Much research on the technique has been published, starting in the mid-1970s. N-version programming is conceptually similar to N-modular redundancy, a hardware reliability [Avizienis1978a, Siewiorek1982a] technique.
Reference: [Dahl1972a] <author> O.-J. Dahl, </author> <title> C.A.R. Hoare, and E.W. Dijkstra, Structured Programming, </title> <publisher> Academic Press, </publisher> <address> New York (1972). </address>
Reference-contexts: For example, in software, failures, and faults causing errors, can be reduced or removed by: g Verification (proofs of correctness) [Floyd1967a, Hoare1969a, Dijkstra1976a] g Precise specifications combined with testing. Prather [Prather1983a] provides an excellent overview of program testing, and includes an extensive bibliography. g Structured programming techniques <ref> [Dahl1972a] </ref> with which we combine the numerous design techniques such as Parnas' information hiding [Parnas (null)a], Ross' SADT, et cetera. However, experience has shown that these techniques are insufficient, i.e. ``bugs'' remain.
Reference: [Dijkstra1972a] <author> E.W. Dijkstra, </author> <title> ``Notes on Structured Programming,'' in Structured Programming, </title> <publisher> Academic Press, </publisher> <address> New York (1972). </address>
Reference-contexts: However, experience has shown that these techniques are insufficient, i.e. ``bugs'' remain. Intuitively, it should be clear that testing does not prove that no bugs remain, unless both the specification is complete in every detail, and the testing exhausts all the possibilities specified. To quote Dijkstra <ref> [Dijkstra1972a] </ref> : ``Program testing can be used to show the presence of bugs, but never to show their absence'' Proofs of program correctness are an elegant methodology for producing correct programs, but they suffer from the same weaknesses of any mathematical proof, that is: g What has been proved is what
Reference: [Dijkstra1976a] <author> E. W. Dijkstra, </author> <title> A Discipline of Programming, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J. </address> <year> (1976). </year>
Reference-contexts: Fault Avoidance The basic notion of fault avoidance is that the systems should be as fault-free as they can be made. For example, in software, failures, and faults causing errors, can be reduced or removed by: g Verification (proofs of correctness) <ref> [Floyd1967a, Hoare1969a, Dijkstra1976a] </ref> g Precise specifications combined with testing. Prather [Prather1983a] provides an excellent overview of program testing, and includes an extensive bibliography. g Structured programming techniques [Dahl1972a] with which we combine the numerous design techniques such as Parnas' information hiding [Parnas (null)a], Ross' SADT, et cetera.
Reference: [Eckhardt1985a] <author> Dave E. Eckhardt, Jr. and Larry D. Lee, </author> <title> ``A Theoretical Basis for the Analysis of Multiversion Software Subject to Coincident Errors,'' </title> <journal> IEEE Transactions on Software Engineering SE-11(12), </journal> <pages> pp. </pages> <month> 1511-1517 (December </month> <year> 1985). </year>
Reference-contexts: Knight and Leveson [Knight1986a, Knight1986b, Knight1985a] have carried out experimental work on the independence assumption of multiple software versions. Their results indicate that the assumption of independence with respect to the faults in the software is not upheld in practice. Eckhardt and Lee <ref> [Eckhardt1985a] </ref> provide a theoretical analysis of - -- coincident errors and their effects on some software fault tolerance schemes. Other results are discussed later in the section on the "Consensus Recovery Block".
Reference: [Feynman1963a] <author> Richard P. Feynman, Robert B. Leighton, and Matthew Sands, </author> <title> The Feynman Lectures on Physics, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA (1963). </address>
Reference-contexts: From this evidence, it seems that predicting and estimating reliability of software are difficult tasks indeed, and of course new methods make data gathered using, e.g. assembly language, somewhat obsolete. This, unfortunately, makes science hard. As Feynmann <ref> [Feynman1963a] </ref> points out, ``...the sole test of the validity of any idea is experiment'', and without metrics and gathered data, this is difficult. However, in spite of this lack of experimental data, general techniques have been proposed to increase the reliability of - -- software.
Reference: [Floyd1967a] <author> R.W. Floyd, </author> <title> ``Assigning meanings to programs,'' </title> <booktitle> in Proceedings, American Math. Society Symposium in Applied Mathematics (1967), </booktitle> <pages> pp. 19-31. </pages>
Reference-contexts: Fault Avoidance The basic notion of fault avoidance is that the systems should be as fault-free as they can be made. For example, in software, failures, and faults causing errors, can be reduced or removed by: g Verification (proofs of correctness) <ref> [Floyd1967a, Hoare1969a, Dijkstra1976a] </ref> g Precise specifications combined with testing. Prather [Prather1983a] provides an excellent overview of program testing, and includes an extensive bibliography. g Structured programming techniques [Dahl1972a] with which we combine the numerous design techniques such as Parnas' information hiding [Parnas (null)a], Ross' SADT, et cetera.
Reference: [Gerhart1976a] <author> Susan L. Gerhart and Lawrence Yelowitz, </author> <title> ``Observations of Fallibility in Applications of Modern Programming Methodologies,'' </title> <journal> IEEE Transactions on Software Engineering (September 1976). </journal>
Reference-contexts: Structured programming techniques are disciplines, which when applied can lead to faster development times and fewer errors [Brooks1975a], yet they do not guarantee that specifications are followed, nor that the modular decomposition done is the correct one for the problem at hand. Gerhart and Yelowitz <ref> [Gerhart1976a] </ref> provide a host of examples where published applications of these techniques are in error; they also attempt to describe some of the general problems which cause the errors. One design technique which has been successful is the use of High Level Languages.
Reference: [Goel1979a] <author> A.L. Goel and K. Okumoto, </author> <title> ``A time dependent error detection rate model for software reliability and other performance measures,'' </title> <journal> IEEE Transactions on Reliability, </journal> <pages> pp. </pages> <month> 206-211 (August </month> <year> 1979). </year>
Reference-contexts: Software Reliability Assessment One should of course have a firm grasp of what ``reliability'' is in the context of software, and how to measure it. Lit-tlewood [Littlewood1979a] addresses this concern in a short discourse on measurement techniques applicable to software. Several software reliability models and metrics have been suggested <ref> [Musa1975a, Musa1979a, Littlewood1979b, Goel1979a] </ref> Musa, Iannino, and Okumoto [Musa1987a] provide a detailed reference on their approach to software reliability; the survey paper by Ramamoorthy and Bastani [Ramamoorthy1982a] discusses a wide variety of models and issues.
Reference: [Hansen1983a] <author> R.C. Hansen, R.W. Peterson, and N.O. Whittington, </author> <title> ``Fault Detection and Recovery,'' </title> <journal> Bell System Technical Journal 62(1), </journal> <pages> pp. </pages> <month> 349-366 (January </month> <year> 1983). </year>
Reference-contexts: The recovery block scheme defines a method for ensuring that the changes effected to external variables be done reliably. The scheme is conceptually similar to the ``standby spare'' technique used in constructing reliable hardware systems <ref> [Toy1983a, Hansen1983a] </ref>. N alternate methods of passing an acceptance test (instances) are provided and rank-ordered according to some criterion such as observed performance. The first such instance is referred to as the primary.
Reference: [Hecht1979a] <author> H. Hecht, </author> <title> ``Fault-Tolerant Software,'' </title> <journal> IEEE Transactions on Reliability, </journal> <pages> pp. </pages> <month> 227-232 (August </month> <year> 1979). </year>
Reference-contexts: The END keyword terminates the recovery block. Note that the acceptance test is specific to this application; by its nature, this will almost always be the case. Hecht <ref> [Hecht1979a] </ref> provides a detailed discussion of the forms such acceptance tests might take. Scott, Gault, and McAllister [Scott1983a] and Scott, et al [Scott1984a]. have shown that acceptance test failures can be tolerated within a certain range, in particular failure rates f , 0.0 f 0.25.
Reference: [Hoare1969a] <author> C.A.R. Hoare, </author> <title> ``An axiomatic basis for computer programming,'' </title> <journal> Communications of the ACM 12, </journal> <pages> pp. 576-580, </pages> <month> 583 (October </month> <year> 1969). </year>
Reference-contexts: Fault Avoidance The basic notion of fault avoidance is that the systems should be as fault-free as they can be made. For example, in software, failures, and faults causing errors, can be reduced or removed by: g Verification (proofs of correctness) <ref> [Floyd1967a, Hoare1969a, Dijkstra1976a] </ref> g Precise specifications combined with testing. Prather [Prather1983a] provides an excellent overview of program testing, and includes an extensive bibliography. g Structured programming techniques [Dahl1972a] with which we combine the numerous design techniques such as Parnas' information hiding [Parnas (null)a], Ross' SADT, et cetera.
Reference: [Horning1974a] <author> J.J. Horning, H.C. Lauer, P.M. Melliar-Smith, and B. Randell, </author> <title> ``A program structure for error detection and recovery.,'' </title> <booktitle> in Proceedings, Conference on Operating Systems: Theoretical and Practical Aspects (April 1974), </booktitle> <pages> pp. 177-193. </pages>
Reference-contexts: Some practical issues of constructing such systems are being addressed by the DEDIX [Avizienis1985b] system under construction at UCLA. 5.3. Recovery Block The Recovery Block method was originated by Horning, Randell, and others <ref> [Randell1975a, Horning1974a] </ref> at the University of Newcastle upon Tyne. Others associated with that research group [Anderson1981a] have discussed recovery blocks and produced special-purpose hardware [Lee1980a] to support efficient implementation of the scheme. A compendium of this work has been compiled by Shrivastava [Shrivastava1985a].
Reference: [Knight1985a] <author> John C. Knight, Nancy G. Leveson, and Lois D. St. Jean, </author> <title> ``A Large Scale Experiment in N-Version Programming,'' </title> <booktitle> in Proceedings of the 15th Annual International Symposium on Fault-Tolerant Computing (FTCS-15), IEEE (1985), </booktitle> <pages> pp. 135-139. </pages>
Reference-contexts: This is important information if we seek to estimate reliability of the final system from observations we have made about its constituent components. Where reliability goals are set, this information can predict how many independent copies of software are necessary to meet the goal. Knight and Leveson <ref> [Knight1986a, Knight1986b, Knight1985a] </ref> have carried out experimental work on the independence assumption of multiple software versions. Their results indicate that the assumption of independence with respect to the faults in the software is not upheld in practice.
Reference: [Knight1986a] <author> J.C. Knight and N.G. Leveson, </author> <title> ``An Empirical study of failure probabilities in multi-version software.,'' </title> <booktitle> in - -- Proceedings of the 16th Annual International Symposium on Fault-Tolerant Computing (FTCS-16), </booktitle> <address> Vienna, Austria (July 1986), </address> <pages> pp. 165-170. </pages>
Reference-contexts: This is important information if we seek to estimate reliability of the final system from observations we have made about its constituent components. Where reliability goals are set, this information can predict how many independent copies of software are necessary to meet the goal. Knight and Leveson <ref> [Knight1986a, Knight1986b, Knight1985a] </ref> have carried out experimental work on the independence assumption of multiple software versions. Their results indicate that the assumption of independence with respect to the faults in the software is not upheld in practice.
Reference: [Knight1986b] <author> J.C. Knight and N.G. Leveson, </author> <title> ``An Experimental Evaluation of the Assumption of Independence in Mul-tiversion Programming,'' </title> <journal> IEEE Transactions on Software Engineering SE-12(1), </journal> <pages> pp. </pages> <month> 96-109 (January </month> <year> 1986). </year>
Reference-contexts: This is important information if we seek to estimate reliability of the final system from observations we have made about its constituent components. Where reliability goals are set, this information can predict how many independent copies of software are necessary to meet the goal. Knight and Leveson <ref> [Knight1986a, Knight1986b, Knight1985a] </ref> have carried out experimental work on the independence assumption of multiple software versions. Their results indicate that the assumption of independence with respect to the faults in the software is not upheld in practice.
Reference: [Koenig1986a] <author> Andrew Koenig, </author> <title> ``C Traps and Pitfalls,'' </title> <note> Computing Science Technical Report No. 123 (July 1, 1986). </note> <institution> AT&T Bell Laboratories </institution>
Reference-contexts: Failures, which we will define precisely in a later section, can occur due to errors in the hardware (e.g., a short circuit) or errors in the software (e.g., using '=' instead of the intended '==' <ref> [Koenig1986a] </ref> in a C comparison). Hardware fault tolerance [Siewiorek1982a] is well-understood, to the point of being an engineering discipline. There are several reasons why this is so: 1. The physics of hardware components, such as silicon, are well understood; 2.
Reference: [Laprie1985a] <author> Jean-Claude Laprie, </author> <title> ``Dependable Computing and Fault Tolerance: Concepts and Terminology,'' </title> <booktitle> in Fifteenth Annual International Symposium on Fault-Tolerant Computing, </booktitle> <address> Ann Arbor, MI (June 1985), </address> <pages> pp. 2-11. </pages>
Reference-contexts: Survey Organization We begin with a lexicon of terms in Section 2, for precision in the remaining discussion. Section 2 also outlines the role of fault tolerance in the design process, using a taxonomy developed by Laprie <ref> [Laprie1985a] </ref>. Section 3 provides a terse description of error recovery, and Section 4 discusses the nature of Software Faults, and also serves to relate software fault tolerance techniques to other methods for constructing reliable software systems, such as structured programming. <p> The only requirement is that it could be observed by a strict application of the ASR." [Lee1982a, p. 35] Laprie <ref> [Laprie1985a] </ref> also provides a discussion of terminology. In addition to bibliographic material, many examples for each term and a terminological taxonomy (reproduced in Figure 3) are given.
Reference: [Lee1980a] <author> P.A. Lee, N. Ghani, and K. Heron, </author> <title> ``A Recovery Cache for the PDP-11,'' </title> <journal> IEEE Transactions on Computers C-29(6), </journal> <pages> pp. </pages> <month> 546-549 (June </month> <year> 1980). </year>
Reference-contexts: Recovery Block The Recovery Block method was originated by Horning, Randell, and others [Randell1975a, Horning1974a] at the University of Newcastle upon Tyne. Others associated with that research group [Anderson1981a] have discussed recovery blocks and produced special-purpose hardware <ref> [Lee1980a] </ref> to support efficient implementation of the scheme. A compendium of this work has been compiled by Shrivastava [Shrivastava1985a]. The recovery block is a language construct for encapsulating a program segment which is to be performed reliably.
Reference: [Lee1982a] <author> P.A. Lee and D.E. Morgan, </author> <title> ``Fundamental Concepts of Fault Tolerant Computing, Progress Report,'' </title> <booktitle> in Proceedings, 12th International Symposium on Fault-Tolerant Computing, </booktitle> <address> Los Angeles, CA. </address> <month> (June </month> <year> 1982), </year> <pages> pp. 34-38. </pages>
Reference-contexts: Researchers in Software Fault Tolerance have suffered from the lack of common definitions, recognized the state of their field, and attempted to improve it. Morgan [Morgan1982a] points out where these problems with terminology have hindered researchers in fault-tolerant computing; efforts to remedy the problem are described. Lee and Morgan <ref> [Lee1982a] </ref> discuss the preliminary results of one such effort. In particular, they point out difficulties with definitions of the terms ``fault'', ``error'', ``failure'', ``specifications'', ``recovery'', ``system reliability'', ``hardware reliability'', and ``software reliability''. <p> The only requirement is that it could be observed by a strict application of the ASR." <ref> [Lee1982a, p. 35] </ref> Laprie [Laprie1985a] also provides a discussion of terminology. In addition to bibliographic material, many examples for each term and a terminological taxonomy (reproduced in Figure 3) are given.
Reference: [Lerner1988a] <author> Mark D. Lerner, </author> <title> ``Fault Tolerance on Massively Parallel Processors,'' </title> <type> Technical Report #CUCS-370-88, </type> <institution> Columbia University Computer Science Department (1988). </institution>
Reference-contexts: Conclusions We have focused our attention on general methods for software fault tolerance. Certain problems allow problem-specific fault-tolerance strategies to be used, e.g., where some redundant information such as an ``invariant'' is part of the problem definition. Lerner <ref> [Lerner1988a] </ref> has examined these techniques, and finds them promising, especially in massively parallel architectures. The more general algorithms, such as the ones we have discussed here, must function without regard to the problem characteristics.
Reference: [Leveson1986a] <author> Nancy G. Leveson, </author> <title> ``Software Safety: What, Why, and How,'' </title> <journal> ACM Computing Surveys 18(2), </journal> <pages> pp. </pages> <month> 125-163 (June, </month> <year> 1986). </year>
Reference-contexts: 1. Introduction As computer systems become responsible for supporting increasing numbers of human activities, there is a corresponding increase in dependence on the machine's correct function. The extremal points are life-critical systems <ref> [Leveson1986a] </ref>, where the dependence on the system could determine whether a person lives or dies. Simple systems, e.g., a traffic light controller, can be constructed using only hardware components. More complex systems are constructed using programmable components, which require software (programs) in some form.
Reference: [Littlewood1979a] <author> B. Littlewood, </author> <title> ``How to Measure Software Reliability and How Not To,'' </title> <journal> IEEE Transactions on Reliability, </journal> <pages> pp. </pages> <month> 103-110 (June </month> <year> 1979). </year>
Reference-contexts: Software Reliability Assessment One should of course have a firm grasp of what ``reliability'' is in the context of software, and how to measure it. Lit-tlewood <ref> [Littlewood1979a] </ref> addresses this concern in a short discourse on measurement techniques applicable to software.
Reference: [Littlewood1979b] <author> B. Littlewood, </author> <title> ``Software Reliability Model for Modular Program Structure,'' </title> <note> IEEE Transactions on Reliability (August 1979). </note>
Reference-contexts: Software Reliability Assessment One should of course have a firm grasp of what ``reliability'' is in the context of software, and how to measure it. Lit-tlewood [Littlewood1979a] addresses this concern in a short discourse on measurement techniques applicable to software. Several software reliability models and metrics have been suggested <ref> [Musa1975a, Musa1979a, Littlewood1979b, Goel1979a] </ref> Musa, Iannino, and Okumoto [Musa1987a] provide a detailed reference on their approach to software reliability; the survey paper by Ramamoorthy and Bastani [Ramamoorthy1982a] discusses a wide variety of models and issues.
Reference: [Melliar-Smith1979a] <author> P.M. Melliar-Smith, </author> <title> ``System Specification,'' in Computing Systems Reliability, </title> <editor> ed. B. Randell, </editor> <publisher> Cam-bridge University Press (1979), </publisher> <pages> pp. 19-65. </pages>
Reference-contexts: In particular, they point out difficulties with definitions of the terms ``fault'', ``error'', ``failure'', ``specifications'', ``recovery'', ``system reliability'', ``hardware reliability'', and ``software reliability''. They suggest, as a start, that correctness and incorrectness be defined with respect to an Authoritative System Reference (ASR), which is in essence an ideal specification <ref> [Melliar-Smith1979a] </ref>. Disagreement with the ASR then constitutes a failure.
Reference: [Morgan1982a] <author> David E. Morgan, </author> <title> ``Report of Subcommittee on Models, Fundamental Concepts, and Terminology,'' </title> <booktitle> in Proceedings, 12th International Symposium on Fault-Tolerant Computing, </booktitle> <address> Los Angeles, CA. </address> <month> (June </month> <year> 1982), </year> <pages> pp. 3-5. </pages>
Reference-contexts: From these examples, it's clear that a well-defined term can be used to measure and compare, and to share the results of these measurements. Researchers in Software Fault Tolerance have suffered from the lack of common definitions, recognized the state of their field, and attempted to improve it. Morgan <ref> [Morgan1982a] </ref> points out where these problems with terminology have hindered researchers in fault-tolerant computing; efforts to remedy the problem are described. Lee and Morgan [Lee1982a] discuss the preliminary results of one such effort.
Reference: [Musa1975a] <author> John D. Musa, </author> <title> ``A theory of software reliability and its application,'' </title> <journal> IEEE Transactions on Software Engineering SE-1, </journal> <pages> pp. </pages> <month> 312-327 (September </month> <year> 1975). </year>
Reference-contexts: Software Reliability Assessment One should of course have a firm grasp of what ``reliability'' is in the context of software, and how to measure it. Lit-tlewood [Littlewood1979a] addresses this concern in a short discourse on measurement techniques applicable to software. Several software reliability models and metrics have been suggested <ref> [Musa1975a, Musa1979a, Littlewood1979b, Goel1979a] </ref> Musa, Iannino, and Okumoto [Musa1987a] provide a detailed reference on their approach to software reliability; the survey paper by Ramamoorthy and Bastani [Ramamoorthy1982a] discusses a wide variety of models and issues.
Reference: [Musa1979a] <author> John D. Musa, </author> <title> ``Validity of Execution-Time Theory of Software Reliability,'' </title> <note> IEEE Transactions on Reliability (August 1979). </note>
Reference-contexts: Software Reliability Assessment One should of course have a firm grasp of what ``reliability'' is in the context of software, and how to measure it. Lit-tlewood [Littlewood1979a] addresses this concern in a short discourse on measurement techniques applicable to software. Several software reliability models and metrics have been suggested <ref> [Musa1975a, Musa1979a, Littlewood1979b, Goel1979a] </ref> Musa, Iannino, and Okumoto [Musa1987a] provide a detailed reference on their approach to software reliability; the survey paper by Ramamoorthy and Bastani [Ramamoorthy1982a] discusses a wide variety of models and issues.
Reference: [Musa1987a] <author> John D. Musa, Anthony Iannino, and Kazuhira Okumoto, </author> <title> Software Reliability: Measurement, Prediction, Application, </title> <publisher> McGraw-Hill (1987). </publisher>
Reference-contexts: Lit-tlewood [Littlewood1979a] addresses this concern in a short discourse on measurement techniques applicable to software. Several software reliability models and metrics have been suggested [Musa1975a, Musa1979a, Littlewood1979b, Goel1979a] Musa, Iannino, and Okumoto <ref> [Musa1987a] </ref> provide a detailed reference on their approach to software reliability; the survey paper by Ramamoorthy and Bastani [Ramamoorthy1982a] discusses a wide variety of models and issues. <p> The management of the development process requires logging of failures, their times and natures. This seems most appropriate for the domains in which it has been applied, e.g. ABM software <ref> [Musa1987a] </ref>. Data-domain testing uses a priori knowledge of the input set to determine where the programs fail, and why. This seems most appropriate for the multiple-version software approach.
Reference: [Neumann1956a] <author> John von Neumann, </author> <title> ``Probabilistic logics and the synthesis of reliable organisms from unreliable components,'' in Automata Studies, </title> <editor> ed. J. McCarthy, </editor> <publisher> Princeton University Press (1956), </publisher> <pages> pp. 43-98. </pages>
Reference-contexts: The basic idea behind all of fault tolerance is the use of redundancy. The idea of using redundancy to construct reliable systems from unreliable components was first described by Von Neumann <ref> [Neumann1956a] </ref> in 1956. Redundancy (multiple copies) is used to detect faults and mask failures.
Reference: [Parnas(null)a] <author> D. L. Parnas, </author> <title> ``On the Criteria to be used in Decomposing Systems into Modules,'' </title> <journal> Communications of the ACM 15(12). </journal>
Reference: [Prather1983a] <author> R.E. Prather, </author> <title> ``Theory of Program Testing An Overview,'' </title> <journal> The Bell System Technical Journal 62(10, </journal> <volume> part 2), </volume> <pages> pp. </pages> <month> 3073-3105 (December </month> <year> 1983). </year>
Reference-contexts: For example, in software, failures, and faults causing errors, can be reduced or removed by: g Verification (proofs of correctness) [Floyd1967a, Hoare1969a, Dijkstra1976a] g Precise specifications combined with testing. Prather <ref> [Prather1983a] </ref> provides an excellent overview of program testing, and includes an extensive bibliography. g Structured programming techniques [Dahl1972a] with which we combine the numerous design techniques such as Parnas' information hiding [Parnas (null)a], Ross' SADT, et cetera. However, experience has shown that these techniques are insufficient, i.e. ``bugs'' remain.
Reference: [Ramamoorthy1982a] <author> C. V. Ramamoorthy and Farokh B. Bastani, </author> <title> ``Software Reliability Status and Perspectives,'' </title> <journal> IEEE Transactions on Software Engineering SE-8(4), </journal> <pages> pp. </pages> <month> 354-371 (July </month> <year> 1982). </year>
Reference-contexts: Several software reliability models and metrics have been suggested [Musa1975a, Musa1979a, Littlewood1979b, Goel1979a] Musa, Iannino, and Okumoto [Musa1987a] provide a detailed reference on their approach to software reliability; the survey paper by Ramamoorthy and Bastani <ref> [Ramamoorthy1982a] </ref> discusses a wide variety of models and issues. From this evidence, it seems that predicting and estimating reliability of software are difficult tasks indeed, and of course new methods make data gathered using, e.g. assembly language, somewhat obsolete. This, unfortunately, makes science hard.
Reference: [Randell1975a] <author> B. Randell, </author> <title> ``System structure for software fault tolerance,'' </title> <journal> IEEE Transactions on Software Engineering SE-1, </journal> <pages> pp. </pages> <month> 220-232 (June </month> <year> 1975). </year>
Reference-contexts: Some practical issues of constructing such systems are being addressed by the DEDIX [Avizienis1985b] system under construction at UCLA. 5.3. Recovery Block The Recovery Block method was originated by Horning, Randell, and others <ref> [Randell1975a, Horning1974a] </ref> at the University of Newcastle upon Tyne. Others associated with that research group [Anderson1981a] have discussed recovery blocks and produced special-purpose hardware [Lee1980a] to support efficient implementation of the scheme. A compendium of this work has been compiled by Shrivastava [Shrivastava1985a]. <p> Notation The goal of the routine is to provide an output which is the square root of the numerical argument. The notation we have used in the example is that of the RB [Smith1987a] language of Smith and Maguire; the notation for specifying recovery blocks closely follows Randell <ref> [Randell1975a] </ref>. The ENSURE keyword indicates that what follows is to be used as the acceptance test for this recovery block. In this case, we have defined a macro EQUAL which defines equality in terms of a relative error measure to make the example more realistic.
Reference: [Robbins1975a] <author> Herbert Robbins and John Van Ryzin, </author> <title> Introduction to Statistics, </title> <month> SRA </month> <year> (1975). </year>
Reference-contexts: We will use the formal state machine model of Section 2 to discuss redundancy. The use of redundancy relies on statistical independence . In statistics <ref> [Robbins1975a] </ref>, independence means that given two events, A and B, from an outcome space, I,pr (A JB)=pr (A), and pr (B JA)=pr (B). Thus, pr (A B)=pr (A) . pr (B).
Reference: [Scott1983a] <author> R. Keith Scott, James W. Gault, and David F. McAllister, </author> <title> ``Modeling Fault-Tolerant Software Reliability,'' </title> <booktitle> in Proceedings, IEEE 1983 Symposium on Reliability in Distributed Software and Database Systems (1983), </booktitle> <pages> pp. 15-27. </pages>
Reference-contexts: The END keyword terminates the recovery block. Note that the acceptance test is specific to this application; by its nature, this will almost always be the case. Hecht [Hecht1979a] provides a detailed discussion of the forms such acceptance tests might take. Scott, Gault, and McAllister <ref> [Scott1983a] </ref> and Scott, et al [Scott1984a]. have shown that acceptance test failures can be tolerated within a certain range, in particular failure rates f , 0.0 f 0.25. Scott's thesis [Scott1983b] proposes a synthetic software fault tolerance method, which is discussed in the next section. 5.4.
Reference: [Scott1983b] <author> Roderick Keith Scott, </author> <title> ``Data Domain Modeling of Fault-Tolerant Software Reliability,'' </title> <type> Ph.D. Thesis, </type> <institution> North Carolina State University at Raleigh (1983). </institution>
Reference-contexts: Hecht [Hecht1979a] provides a detailed discussion of the forms such acceptance tests might take. Scott, Gault, and McAllister [Scott1983a] and Scott, et al [Scott1984a]. have shown that acceptance test failures can be tolerated within a certain range, in particular failure rates f , 0.0 f 0.25. Scott's thesis <ref> [Scott1983b] </ref> proposes a synthetic software fault tolerance method, which is discussed in the next section. 5.4. Consensus Recovery Block Scott [Scott1983b] observed the following in an analysis of the N-Version Programming and Recovery Block schemes for software fault tolerance: 1.) Recovery Blocks: The major difficulty with the Recovery Block scheme is <p> Scott's thesis <ref> [Scott1983b] </ref> proposes a synthetic software fault tolerance method, which is discussed in the next section. 5.4. Consensus Recovery Block Scott [Scott1983b] observed the following in an analysis of the N-Version Programming and Recovery Block schemes for software fault tolerance: 1.) Recovery Blocks: The major difficulty with the Recovery Block scheme is the acceptance test; analysis shows that it is the most crucial component of the scheme if reliability is to be <p> However, there remain points to be addressed. In particular, how well do the schemes perform in practice? Scott's experimental data <ref> [Scott1983b] </ref> provide a great deal of insight into the assumptions and behavior of these methodologies. First, he concludes that the independence assumption is unwarranted 6 based on a statistical analysis of many independently-developed 7 versions of a program.
Reference: [Scott1984a] <author> R. Keith Scott, James W. Gault, David F. McAllister, and Jeffrey Wiggs, </author> <title> ``Experimental Validation of Six Fault-Tolerant Software Reliability Models,'' </title> <booktitle> in Proceedings of the 14th Annual International Symposium on Fault-Tolerant Computing (1984). </booktitle> - -- 
Reference-contexts: Note that the acceptance test is specific to this application; by its nature, this will almost always be the case. Hecht [Hecht1979a] provides a detailed discussion of the forms such acceptance tests might take. Scott, Gault, and McAllister [Scott1983a] and Scott, et al <ref> [Scott1984a] </ref>. have shown that acceptance test failures can be tolerated within a certain range, in particular failure rates f , 0.0 f 0.25. Scott's thesis [Scott1983b] proposes a synthetic software fault tolerance method, which is discussed in the next section. 5.4.
Reference: [Shrivastava1985a] <author> Santosh K. Shrivastava, </author> <title> Reliable Computing Systems, </title> <publisher> Springer-Verlag (1985). </publisher>
Reference-contexts: Others associated with that research group [Anderson1981a] have discussed recovery blocks and produced special-purpose hardware [Lee1980a] to support efficient implementation of the scheme. A compendium of this work has been compiled by Shrivastava <ref> [Shrivastava1985a] </ref>. The recovery block is a language construct for encapsulating a program segment which is to be performed reliably.
Reference: [Siewiorek1982a] <author> Daniel P. Siewiorek and Robert S. Swarz, </author> <title> The Theory and Practice of Reliable System Design, </title> <publisher> Digital Press (1982). </publisher>
Reference-contexts: Failures, which we will define precisely in a later section, can occur due to errors in the hardware (e.g., a short circuit) or errors in the software (e.g., using '=' instead of the intended '==' [Koenig1986a] in a C comparison). Hardware fault tolerance <ref> [Siewiorek1982a] </ref> is well-understood, to the point of being an engineering discipline. There are several reasons why this is so: 1. The physics of hardware components, such as silicon, are well understood; 2. The complexity of large hardware designs is several orders of magnitude less than large software systems; 3. <p> N-Version Programming N-version Programming is a technique originated and advocated by Avizienis, et al. [Chen1978a, Avizienis1977a, Avizienis1985a]. Much research on the technique has been published, starting in the mid-1970s. N-version programming is conceptually similar to N-modular redundancy, a hardware reliability <ref> [Avizienis1978a, Siewiorek1982a] </ref> technique.
Reference: [Smith1987a] <author> Jonathan M. Smith and Gerald Q. Maguire,Jr., ``RB: </author> <title> Programmer Specification of Redundancy,'' </title> <type> Technical Report CUCS-269-87, </type> <institution> Columbia University Computer Science Department (1987). </institution>
Reference-contexts: Notation The goal of the routine is to provide an output which is the square root of the numerical argument. The notation we have used in the example is that of the RB <ref> [Smith1987a] </ref> language of Smith and Maguire; the notation for specifying recovery blocks closely follows Randell [Randell1975a]. The ENSURE keyword indicates that what follows is to be used as the acceptance test for this recovery block.
Reference: [Strom1983a] <author> R. E. Strom and S. Yemini, </author> <title> ``NIL: An Integrated Language and System for Distributed Programming,'' </title> <journal> ACM SIGPLAN Notices, </journal> <pages> pp. </pages> <month> 73-82 (June </month> <year> 1983). </year>
Reference-contexts: Thus, for a given task and time frame, error-free code will be produced faster using a higher level language. In addition, the tools available are very helpful in removing errors early in the process, e.g. type-checking compilers and support systems <ref> [Strom1983a] </ref>. However, the task size may be too large, or the time available too small for these methods to make the deliverable error-free, in spite of the quality of these tools.
Reference: [Toy1983a] <author> W.N. Toy and L.E. Gallaher, </author> <title> ``Overview and Architecture of the 3B20D Processor,'' </title> <journal> Bell System Technical Journal 62(1), </journal> <pages> pp. </pages> <month> 181-190 (January </month> <year> 1983). </year> - -- 
Reference-contexts: The recovery block scheme defines a method for ensuring that the changes effected to external variables be done reliably. The scheme is conceptually similar to the ``standby spare'' technique used in constructing reliable hardware systems <ref> [Toy1983a, Hansen1983a] </ref>. N alternate methods of passing an acceptance test (instances) are provided and rank-ordered according to some criterion such as observed performance. The first such instance is referred to as the primary.
References-found: 53

