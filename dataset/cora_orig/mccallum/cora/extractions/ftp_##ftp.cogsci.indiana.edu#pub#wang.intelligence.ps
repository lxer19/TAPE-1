URL: ftp://ftp.cogsci.indiana.edu/pub/wang.intelligence.ps
Refering-URL: http://www.cogsci.indiana.edu/farg/peiwang/papers.html
Root-URL: 
Email: pwang@cogsci.indiana.edu  
Title: On the Working Definition of Intelligence  
Author: Pei Wang 
Address: 510 North Fess, Bloomington, IN 47408, USA  
Affiliation: Indiana University  
Note: Center for Research on Concepts and Cognition,  Copyright 1995 All rights reserved  
Abstract: This paper is about the philosophical and methodological foundation of artificial intelligence (AI). After discussing what is a good "working definition", "intelligence" is defined as "the ability for an information processing system to adapt to its environment with insufficient knowledge and resources". Applying the definition to a reasoning system, we get the major components of Non-Axiomatic Reasoning System (NARS), which is a symbolic logic implemented in a computer system, and has many interesting properties that are closely related to intelligence. The definition also clarifies the difference and relationship between AI and other disciplines, such as computer science. Finally, the definition is compared with other popular definitions of intelligence, and its advantages are argued.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Birnbaum. Rigor mortis: </author> <title> a response to Nilsson's "Logic and artificial intelligence". </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 57-77, </pages> <year> 1991. </year>
Reference-contexts: On the contrary, traditional computer systems are Turing machines either globally (from experience to response) or locally (from question to answer). Many arguments proposed against logical AI (for example, <ref> [1, 20] </ref>), symbolic AI (for example, [6]), or AI as a whole (for example, [33]), are actually against a more specific target: pure-axiomatic systems.
Reference: [2] <author> M. Boddy and T. Dean. </author> <title> Deliberation scheduling for problem solving in time-constrained environments. </title> <journal> Artificial Intelligence, </journal> <volume> 67 </volume> <pages> 245-285, </pages> <year> 1994. </year>
Reference-contexts: from the system, no matter how good an answer has been found for it. 3.7 Chunk-based memory Because only part of the system's knowledge is used in answering each question and because it is possible for a question-answering activity to stop after any number of inference steps (like anytime algorithms <ref> [2] </ref>), it is important for the system to organize its knowledge in such a way that the more relevant and important knowledge is made more accessible. <p> A variation of this approach is that the task is provided with no deadline, but the user can interrupt the process at any time to get a best-so-far answer <ref> [2] </ref>. NARS uses a more flexible manner to decide how much time is spent on a task, and both the system and the user (environment) influence the result.
Reference: [3] <author> R. Carnap. </author> <title> Logical Foundations of Probability. </title> <publisher> The University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1950. </year>
Reference-contexts: transforming a given more or less inexact concept into an exact one or, rather, in replacing the first by the second", where the first may belong to everyday language or to a previous stage in the scientific language, and the second must be given by explicit rules for its use <ref> [3] </ref>. According to Carnap [3], the second concept, or the working definition as it is called in this paper, must fulfill the following requirements: 1. It is similar to the concept to be defined, as the latter's vagueness permits. 2. It is defined in an exact form. 3. <p> or less inexact concept into an exact one or, rather, in replacing the first by the second", where the first may belong to everyday language or to a previous stage in the scientific language, and the second must be given by explicit rules for its use <ref> [3] </ref>. According to Carnap [3], the second concept, or the working definition as it is called in this paper, must fulfill the following requirements: 1. It is similar to the concept to be defined, as the latter's vagueness permits. 2. It is defined in an exact form. 3.
Reference: [4] <author> D. Chalmers, R. French, and D. Hofstadter. </author> <title> High-level perception, representation, and analogy: </title> <journal> a critique of artificial intelligence methodology. Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 4 </volume> <pages> 185-211, </pages> <year> 1992. </year>
Reference-contexts: When specified in isolation, an implemented function is often quite different from its "natural form" that happens in the human mind. For example, to study analogy without perception leads to distorted cognitive models <ref> [4] </ref>. 3. Having a certain cognitive function is not enough to make a system intelligent. For example, problem-solving by exhaustive searching is usually not considered intelligence, and many unintelligent animals have perception.
Reference: [5] <author> C. Cherniak. </author> <title> Minimal Rationality. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: time, that encompass the whole range of human values, and in which each problem is interconnected with all the other problems in the world." [35] Cherniak's minimal rationality: "We are in the finitary predicament of having fixed limits on our cognitive resources, in particular, on memory capacity and computing time." <ref> [5] </ref> Russell and Wefald's limited rationality: "Intelligence was intimately linked to the ability to succeed as far as possible given one's limited computational and informational resources." [31] Medin and Ross even have made it so clearly: "Much of intelligent behavior can be understood in terms of strategies for coping with too
Reference: [6] <author> H. Dreyfus. </author> <title> What Computers Still Can't Do. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: They appear in human thinking, and are often recognized as related to intelligence. 3. Most of them have been discussed and produced by different AI theories and systems, but separately. 4. They are often judged as impossible by the critics of AI (for examples, see <ref> [6, 33] </ref>). The interesting point is: now all of them can be derived (to a certain extent) from the previous working definition of intelligence, and many of them have been shown, to a different extent, by an implementation of NARS [42]. <p> On the contrary, traditional computer systems are Turing machines either globally (from experience to response) or locally (from question to answer). Many arguments proposed against logical AI (for example, [1, 20]), symbolic AI (for example, <ref> [6] </ref>), or AI as a whole (for example, [33]), are actually against a more specific target: pure-axiomatic systems.
Reference: [7] <author> J. Elgot-Drapkin, M. Miller, and Perlis D. </author> <title> Memory, reason, and time: the step-logic approach. </title> <editor> In R. Cummins and J. Pollock, editors, </editor> <booktitle> Philosophy and AI, chapter 4, </booktitle> <pages> pages 79-103. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: In contrary, NARS has a "life-time of its own" <ref> [7] </ref>. When the system is experienced enough, there will be lots of tasks for the system to process. On the other hand, new input can come at any time. Consequently, the system's history is no longer like the previous loop.
Reference: [8] <author> R. </author> <title> French. Subcognition and the limits of the Turing test. </title> <journal> Mind, </journal> <volume> 99 </volume> <pages> 53-65, </pages> <year> 1990. </year>
Reference-contexts: To imitate human performance in a conversation, it has to produce the answers in a "human-way". To do this, it not only needs some cognitive facilities, but also a "human experience" <ref> [8] </ref>. Therefore, it must have a body that feels like human, it must have all human motivations (including the biological ones), and it must be treated by people as a human being | so it must simply be an "artificial human", rather than a computer system with artificial intelligence. <p> Is it necessary? As French points out, by using behaviors as evidence, the Turing test is for human intelligence, not for intelligence in general <ref> [8] </ref>. As a working definition for intelligence, such an approach can lead to good psychological models, which are valuable for many reasons, but suffer from a "human chauvinism" [11] | we have to say, according to the definition, that E.
Reference: [9] <author> C. Hempel. </author> <title> A purely syntactical definition of confirmation. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 8 </volume> <pages> 122-143, </pages> <year> 1943. </year>
Reference-contexts: We all know what a proof or a disproof of a statement is in first order predicate logic, however, as revealed by Hempel's famous "confirmation paradox" <ref> [9] </ref>, to introduce the concepts of "positive evidence" and "negative evidence" into first order language is hard, if not impossible. Fortunately, we have an alternative language family: the formal language used by term logics. Let us call it term-oriented languages.
Reference: [10] <author> G. Hinton, J. McClelland, and D. Rumelhart. </author> <title> Distributed representation. </title> <editor> In D. Rumel-hart and J. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Exploration in the Microstructure of cognition, </booktitle> <volume> Vol. 1, </volume> <booktitle> Foundations, </booktitle> <pages> pages 77-109. </pages> <publisher> The MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: the same initial urgency, will get more processing when the system is "idle" than when the system is "busy". 4.8 Distributed representation Knowledge in NARS is represented distributedly in the sense that there is no one-to-one correspondence between the input/output in the experience/response and the knowledge in 15 the memory <ref> [10] </ref>. When a piece of new knowledge is provided to the system, it is not simply inserted into the memory. Spontaneous inferences will happen, which generate derived conclusions. Moreover, the new knowledge may be revised when it is in conflict with previous knowledge.
Reference: [11] <author> D. Hofstadter. </author> <title> Godel, Escher, Bach: an Eternal Golden Braid. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: For this reason, these behaviors, with their results, are better to be attributed to the system itself, than to anyone else <ref> [11] </ref>. 4.19 Own life Traditional computer systems always repeat the following "life cycle": waiting for problems ! accepting a problem ! working on it ! getting a solution for it ! waiting for problems . In contrary, NARS has a "life-time of its own" [7]. <p> In NARS, these properties, no matter whether they are referred to as advantage or disadvantage, become inevitable "epiphenomena" <ref> [11] </ref> of a unified architecture, which is based on several simple principles. 5 What Is Unintelligent? 5.1 The need to exclude something When defining intelligence, many authors ignore the complementary question: what is unintelligent? For AI to be a branch of science, this question must be clearly answered, as pointed out <p> Actually, it is the contrary: when a problem can be solved by both of them, the unintelligent system is usually better, because it guarantees a correct solution. As Hofstadter said, for tasks like adding two numbers, a "reliable but mindless" system is better than an "intelligent but fallible" system <ref> [11] </ref>. Pure-axiomatic systems are very useful in mathematics, where the aim is to idealize knowledge and questions to such an extent that the revision of knowledge and the deadline of questions can be ignored. <p> If NARS is implemented in a von Neumann computer, can it go beyond the scope of CS? Yes, it is possible because a computer system is a hierarchy with many levels <ref> [11] </ref>. Some critics implicitly assume that because a certain level of a computer system can be captured by first order predicate logic and Turing machine, these theories also bind all the performances the system can have. This is not the case. <p> As a working definition for intelligence, such an approach can lead to good psychological models, which are valuable for many reasons, but suffer from a "human chauvinism" <ref> [11] </ref> | we have to say, according to the definition, that E. T. is not intelligent, because it will definitely fail a Turing test. <p> If we say "hard to computers", then AI becomes "whatever hasn't been done yet", which is called "Tesler's Theorem" by Hofstadter <ref> [11] </ref> and "gee whiz view" by Schank [32]. Such a definition cannot lead to a proper distinction between intelligent and unintelligent systems.
Reference: [12] <author> D. Hofstadter. </author> <title> Waking up from the Boolean dream, or, subcognition as computation. In Metamagical Themas: Questing for the Essence of Mind and Pattern, chapter 26. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1985. </year> <month> 30 </month>
Reference-contexts: What happens here has been pointed out by Hofstadter: "Something can be computational at one level, but not at another level." <ref> [12] </ref>. On the contrary, traditional computer systems are Turing machines either globally (from experience to response) or locally (from question to answer). <p> Designed as a reasoning system, but not a "logicist" one [25], NARS actually shares more philosophical opinions with the sub-symbolic, or connectionist movement <ref> [12, 14, 30, 36] </ref>, but chooses to formalize and implement these opinions in a framework that looks more close to the traditional symbolic AI tradition.
Reference: [13] <author> D. Hofstadter and M. Mitchell. </author> <title> The Copycat project: a model of mental fluidity and analogy-making. </title> <editor> In K. Holyoak and J. Barnden, editors, </editor> <booktitle> Advances in Connectionist and Neural Computation Theory, </booktitle> <volume> Volume 2: </volume> <pages> Analogical Connections, pages 31-112. </pages> <publisher> Ablex Publishing Corporation, </publisher> <address> Norwood, New Jersey, </address> <year> 1994. </year>
Reference-contexts: any time, and the question-answering activities are usually open-ended (as described above), NARS cannot answer questions one by one, but have to work on many of them in a time-sharing manner. 11 NARS use a control mechanism named controlled concurrency [42], which is similar to the parallel terraced scan strategy <ref> [13] </ref>. In NARS, the processor time is allocated according to the urgency distribution of the tasks, so different tasks are processed at different speeds. The urgency of a task is adjusted dynamically, according to the decay rate and whether a good answer is already found. <p> The relations are in turn determined by the system's experience and its processing on the experience. When a concept is involved in the processing of a task, usually only part of the knowledge associated with the concept is used. Consequently, concepts become "fluid" <ref> [13] </ref>: 1. No concept has a clear-cut boundary. Whether a concept is an instance of another concept is a matter of degree. 2 2. The membership evaluations are revisable. Therefore, what a concept actually means to the system is also variable. 3.
Reference: [14] <author> J. Holland. </author> <title> Escaping brittleness: the possibilities of general purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. Michalski, J. Carbonell, and T. Mitchell, editors, </editor> <booktitle> Machine Learning: an artificial intelligence approach, volume II, chapter 20, </booktitle> <pages> pages 593-624. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1986. </year>
Reference-contexts: Designed as a reasoning system, but not a "logicist" one [25], NARS actually shares more philosophical opinions with the sub-symbolic, or connectionist movement <ref> [12, 14, 30, 36] </ref>, but chooses to formalize and implement these opinions in a framework that looks more close to the traditional symbolic AI tradition.
Reference: [15] <author> D. Kirsh. </author> <title> Foundations of AI: the big issues. </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 3-30, </pages> <year> 1991. </year>
Reference-contexts: The debate on this issue has been going on for decades, and there is still little sign of consensus <ref> [15] </ref>. As a matter of fact, almost every one in the field has his/her own ideas about how the word "intelligence" should be used, and these ideas in turn influence the choice of research goals and methods, as well as serve as standards to judge other researchers' works.
Reference: [16] <author> P. Kugel. </author> <title> Thinking may be more than computing. </title> <journal> Cognition, </journal> <volume> 22 </volume> <pages> 137-198, </pages> <year> 1986. </year>
Reference-contexts: Of course, eventually the system will end its working for the question, but the reason is neither that a satisficing answer has been found, nor that a deadline is reached, but that the question-answering task has lost in the resources competition. As a result, like trial and error procedures <ref> [16] </ref>, NARS may provide no, one, or more than one answer (s) to a question. In the last case, a later answer is "better" than a previous one, because it is based on more knowledge, but not necessarily "closer to the objective fact".
Reference: [17] <author> D. Lenat and E. Feigenbaum. </author> <title> On the thresholds of knowledge. </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 185-250, </pages> <year> 1991. </year>
Reference-contexts: Following are some representative opinions: "Intelligence is the power to rapidly find an adequate solution in what appears a priori (to observers) to be an immense search space." (Lenat and Feigenbaum, <ref> [17] </ref>) "Artificial intelligence is the study of complex information-processing problems that often have their roots in some aspect of biological information-processing.
Reference: [18] <author> D. Marr. </author> <title> Artificial intelligence: a personal view. </title> <journal> Artificial Intelligence, </journal> <volume> 9 </volume> <pages> 37-48, </pages> <year> 1977. </year>
Reference-contexts: The goal of the subject is to identify interesting and solvable information-processing problems, and solve them." (Marr, <ref> [18] </ref>) "AI is concerned with methods of achieving goals in situations in which the information available has a certain complex character.
Reference: [19] <author> J. McCarthy. </author> <booktitle> Mathematical logic in artificial intelligence. Ddalus, </booktitle> <volume> 117(1) </volume> <pages> 297-311, </pages> <year> 1988. </year>
Reference-contexts: The methods that have to be used are related to the problem presented by the situation and are similar whether the problem solver is human, a Martian, or a computer program." (McCarthy, <ref> [19] </ref>) Intelligence usually means "the ability to solve hard problems". (Minsky, [22]) "By `general intelligent action' we wish to indicate the same scope of intelligence as we see in human action: that in any real situation behavior appropriate to the ends of the system and adaptive to the demands of the
Reference: [20] <author> D. McDermott. </author> <title> A critique of pure reason. </title> <journal> Computational Intelligence, </journal> <volume> 3 </volume> <pages> 151-160, </pages> <year> 1987. </year>
Reference-contexts: On the contrary, traditional computer systems are Turing machines either globally (from experience to response) or locally (from question to answer). Many arguments proposed against logical AI (for example, <ref> [1, 20] </ref>), symbolic AI (for example, [6]), or AI as a whole (for example, [33]), are actually against a more specific target: pure-axiomatic systems.
Reference: [21] <author> D. Medin and B. Ross. </author> <title> Cognitive Psychology. </title> <publisher> Harcourt Brace Jovanovich, </publisher> <address> Fort Worth, </address> <year> 1992. </year>
Reference-contexts: Some of them (the higher urgency or importance values) get more attention, that is, are more active or accessible, while some others are relatively forgotten. With insufficient space, some knowledge and questions will be absolutely forgotten | eliminated from the memory. Like in human memory <ref> [21] </ref>, in NARS forgetting is not a deliberate action, but a side-effect caused by resource competition. 4.7 Flexible time-consuming In traditional computing systems, how much time is spent on a task is determined by the system designer, and the user provides tasks without time requirements. <p> was intimately linked to the ability to succeed as far as possible given one's limited computational and informational resources." [31] Medin and Ross even have made it so clearly: "Much of intelligent behavior can be understood in terms of strategies for coping with too little information and too many possibilities." <ref> [21] </ref> With all of the above already said, what is new in NARS? We claim that the following makes NARS different from other AI projects, philosophically and methodologically: 1. To explicitly and unambiguously define intelligence as "adaption with insufficient knowledge and resources". 2.
Reference: [22] <author> M. Minsky. </author> <title> The Society of Mind. </title> <publisher> Simon and Schuster, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: The methods that have to be used are related to the problem presented by the situation and are similar whether the problem solver is human, a Martian, or a computer program." (McCarthy, [19]) Intelligence usually means "the ability to solve hard problems". (Minsky, <ref> [22] </ref>) "By `general intelligent action' we wish to indicate the same scope of intelligence as we see in human action: that in any real situation behavior appropriate to the ends of the system and adaptive to the demands of the environment can occur, within some limits of speed and complexity." (Newell <p> See [41] for more discussions. 18 tasks", which are actually more or less independent of the original processes, even though historically derived from them. This is the functional autonomy phenomena <ref> [22] </ref>. In the extreme form, the derived tasks may become so strong that they even prevent the input tasks from being fulfilled.
Reference: [23] <author> A. Newell. </author> <title> Unified Theories of Cognition. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1990. </year>
Reference-contexts: "a fairly sharp line between the physical and the intellectual capacities of a man" (Turing in [38]). 24 Such a working definition of intelligence asks researchers to use passing the Turing test as a sufficient and necessary condition for having intelligence, and to take psychological evidence seriously, as Soar does <ref> [23] </ref>. Such a working definition can be criticized from different directions: Is it sufficient? Searle argues that even if a computer system can pass the Turing test, it still cannot think, because it lacks the causal capacity of brain to produce intentionality, which is a biological phenomenon [33].
Reference: [24] <author> A. Newell and H. Simon. </author> <title> Computer science as empirical inquiry: symbols and search. </title> <booktitle> The Tenth Turing Lecture, </booktitle> <month> March </month> <year> 1976. </year> <note> Frist published in Communications of the Association for Computing Machinery 19. </note>
Reference-contexts: intelligent action' we wish to indicate the same scope of intelligence as we see in human action: that in any real situation behavior appropriate to the ends of the system and adaptive to the demands of the environment can occur, within some limits of speed and complexity." (Newell and Simon, <ref> [24] </ref>) "Intelligence means getting better over time." (Schank, [32]) Here we do perceive something in common among the statements, however, their difference is equally obvious. 1.2 Do we need a definition? Maybe it is too early to define intelligence.
Reference: [25] <author> N. </author> <title> Nilsson. </title> <journal> Logic and artificial intelligence. Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 31-56, </pages> <year> 1991. </year>
Reference-contexts: Many arguments proposed against logical AI (for example, [1, 20]), symbolic AI (for example, [6]), or AI as a whole (for example, [33]), are actually against a more specific target: pure-axiomatic systems. Designed as a reasoning system, but not a "logicist" one <ref> [25] </ref>, NARS actually shares more philosophical opinions with the sub-symbolic, or connectionist movement [12, 14, 30, 36], but chooses to formalize and implement these opinions in a framework that looks more close to the traditional symbolic AI tradition.
Reference: [26] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference-contexts: As a result, the evaluation is changeable and system-dependent. The weight of evidence is defined in such a way that it can be used to indicate randomness (see [39] for a comparison with Bayesian network <ref> [26] </ref>), fuzziness (see [41] for a comparison with fuzzy logic [44]), and ignorance (see [43] for a comparison with Dempster-Shafer theory [34]). Though different types of uncertainty have different origins, they usually co-exist, and are tangled with one another in practical situations.
Reference: [27] <author> C. </author> <title> Peirce. </title> <booktitle> Collected papers of Charles Sanders Peirce, </booktitle> <volume> volume 2. </volume> <publisher> Harvard University Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1931. </year>
Reference-contexts: Even deductions are no longer "truth-preserving", in the sense that a conclusion may be revised by new knowledge, even if the premises remain unchallenged. 10 A major advantage of term logics over predicate/propositional logics is: multiple types of inference can be naturally put into the format of syllogism <ref> [27, 40] </ref>.
Reference: [28] <author> G. Reeke and G. Edelman. </author> <title> Real brains and artificial intelligence. </title> <journal> Ddalus, </journal> <volume> 117(1) </volume> <pages> 143-173, </pages> <year> 1988. </year>
Reference-contexts: Such an opinion is put in its extreme form by neu-roscientists Reeke and Edelman, who argue that "the ultimate goals of AI and neuroscience are quite similar" <ref> [28] </ref>. Though it sounds reasonable to identify AI with brain model, few AI researchers exactly take such an approach. Even the "neural network" movement is "not focused on neural modeling (i.e., the modeling of neurons), but rather : : : focused on neurally inspired modeling of cognitive process" [30].
Reference: [29] <author> R. Reiter. </author> <title> Nonmonotonic reasoning. </title> <booktitle> Annual Review of Computer Science, </booktitle> <volume> 2 </volume> <pages> 147-186, </pages> <year> 1987. </year>
Reference-contexts: For example, non-monotonic logics consider the revision of defeasible conclusions (such as "Tweety can fly") caused by new evidence (such as "Tweety is a penguin"), but usually make default rules (such as "Birds normally can fly") unchangeable, and do not take time pressure into account <ref> [29] </ref>. Many learning systems attempt to improve the behaviors of a system, but still work with binary logic, and look for best solutions of problems.
Reference: [30] <author> D. Rumelhart and J. McClelland. </author> <title> PDP models and general issues in cognitive science. </title> <editor> In D. Rumelhart and J. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Exploration in the Microstructure of cognition, </booktitle> <volume> Vol. 1, </volume> <booktitle> Foundations, </booktitle> <pages> pages 110-146. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: As a result, the answering of a question is usually the cooperation of several concepts. However, like in connectionist models <ref> [30] </ref>, there is no "global plan" or "central process" that is responsible for each question. The cooperation is carried out by message-passing among chunks. <p> Designed as a reasoning system, but not a "logicist" one [25], NARS actually shares more philosophical opinions with the sub-symbolic, or connectionist movement <ref> [12, 14, 30, 36] </ref>, but chooses to formalize and implement these opinions in a framework that looks more close to the traditional symbolic AI tradition. <p> Though it sounds reasonable to identify AI with brain model, few AI researchers exactly take such an approach. Even the "neural network" movement is "not focused on neural modeling (i.e., the modeling of neurons), but rather : : : focused on neurally inspired modeling of cognitive process" <ref> [30] </ref>. Why? One obvious reason is the complexity of this approach. The current technology is still not powerful enough to simulate a huge neural network, not to mention that there are still many mysteries in brain.
Reference: [31] <author> S. Russell and E. Wefald. </author> <title> Do the Right Thing. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1991. </year>
Reference-contexts: minimal rationality: "We are in the finitary predicament of having fixed limits on our cognitive resources, in particular, on memory capacity and computing time." [5] Russell and Wefald's limited rationality: "Intelligence was intimately linked to the ability to succeed as far as possible given one's limited computational and informational resources." <ref> [31] </ref> Medin and Ross even have made it so clearly: "Much of intelligent behavior can be understood in terms of strategies for coping with too little information and too many possibilities." [21] With all of the above already said, what is new in NARS? We claim that the following makes NARS
Reference: [32] <author> R. Schank. </author> <title> Where is the AI. </title> <journal> AI Magazine, </journal> <volume> 12(4) </volume> <pages> 38-49, </pages> <year> 1991. </year>
Reference-contexts: scope of intelligence as we see in human action: that in any real situation behavior appropriate to the ends of the system and adaptive to the demands of the environment can occur, within some limits of speed and complexity." (Newell and Simon, [24]) "Intelligence means getting better over time." (Schank, <ref> [32] </ref>) Here we do perceive something in common among the statements, however, their difference is equally obvious. 1.2 Do we need a definition? Maybe it is too early to define intelligence. It is obvious that, after the decades of study, we still do not know very much about it. <p> Usually, the systems are developed by analyzing domain knowledge and expert strategy, then building them into a computer system. Though often profitable, these systems do not provide much insight about how the mind works. No wonder people ask, after knowing how such a system works, "Where's the AI?" <ref> [32] </ref> | these systems look just like ordinary computer application systems, and suffer from rigidity and brittleness (something AI wants to avoid). <p> If we say "hard to computers", then AI becomes "whatever hasn't been done yet", which is called "Tesler's Theorem" by Hofstadter [11] and "gee whiz view" by Schank <ref> [32] </ref>. Such a definition cannot lead to a proper distinction between intelligent and unintelligent systems.
Reference: [33] <author> J. Searle. </author> <title> Minds, brains, and programs. </title> <journal> The Behavioral and Brain Science, </journal> <volume> 3 </volume> <pages> 417-424, </pages> <year> 1980. </year>
Reference-contexts: semantics, the internal activities of a system have no effects on truth value and meaning of the language it uses. "Without an interpretation, a system has no access to the semantics of a formal language it uses" is the central argument in Searle's "Chinese room" thought experiment against strong AI <ref> [33] </ref>. His argument is valid for model-theoretic semantics, but not for experience-grounded semantics. <p> They appear in human thinking, and are often recognized as related to intelligence. 3. Most of them have been discussed and produced by different AI theories and systems, but separately. 4. They are often judged as impossible by the critics of AI (for examples, see <ref> [6, 33] </ref>). The interesting point is: now all of them can be derived (to a certain extent) from the previous working definition of intelligence, and many of them have been shown, to a different extent, by an implementation of NARS [42]. <p> unified architecture, which is based on several simple principles. 5 What Is Unintelligent? 5.1 The need to exclude something When defining intelligence, many authors ignore the complementary question: what is unintelligent? For AI to be a branch of science, this question must be clearly answered, as pointed out by Searle <ref> [33] </ref>. As any concept, if everything is intelligent, then this concept is empty. We need to rule out something to study the remaining objects. <p> The distinction should also be consistent with the way the word "intelligence" is used in everyday language, otherwise we would better use another word. Intuitively, normal humans are intelligent, but traditional computing systems and most animals are not, or much less intelligent. As Searle said <ref> [33] </ref>, a definition of intelligence can rule out candidates like stomach, adding machine, or telephone. <p> On the contrary, traditional computer systems are Turing machines either globally (from experience to response) or locally (from question to answer). Many arguments proposed against logical AI (for example, [1, 20]), symbolic AI (for example, [6]), or AI as a whole (for example, <ref> [33] </ref>), are actually against a more specific target: pure-axiomatic systems. <p> Such a working definition can be criticized from different directions: Is it sufficient? Searle argues that even if a computer system can pass the Turing test, it still cannot think, because it lacks the causal capacity of brain to produce intentionality, which is a biological phenomenon <ref> [33] </ref>. However, he does not demonstrate convincingly why thinking, intentionality, and intelligence cannot have a high-level (higher than biological level) description.
Reference: [34] <author> G. Shafer. </author> <title> A Mathematical Theory of Evidence. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1976. </year>
Reference-contexts: The weight of evidence is defined in such a way that it can be used to indicate randomness (see [39] for a comparison with Bayesian network [26]), fuzziness (see [41] for a comparison with fuzzy logic [44]), and ignorance (see [43] for a comparison with Dempster-Shafer theory <ref> [34] </ref>). Though different types of uncertainty have different origins, they usually co-exist, and are tangled with one another in practical situations.
Reference: [35] <author> H. Simon. </author> <title> Reason in Human Affairs. </title> <publisher> Stanford University Press, Stanford, </publisher> <address> California, </address> <year> 1983. </year>
Reference-contexts: rationality) in the following ideas: Simon's bounded rationality: "Within the behavioral model of bounded rationality, one doesn't have to make choices that are infinitely deep in time, that encompass the whole range of human values, and in which each problem is interconnected with all the other problems in the world." <ref> [35] </ref> Cherniak's minimal rationality: "We are in the finitary predicament of having fixed limits on our cognitive resources, in particular, on memory capacity and computing time." [5] Russell and Wefald's limited rationality: "Intelligence was intimately linked to the ability to succeed as far as possible given one's limited computational and informational
Reference: [36] <author> P. Smolensky. </author> <title> On the proper treatment of connectionism. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 11 </volume> <pages> 1-74, </pages> <year> 1988. </year>
Reference-contexts: Designed as a reasoning system, but not a "logicist" one [25], NARS actually shares more philosophical opinions with the sub-symbolic, or connectionist movement <ref> [12, 14, 30, 36] </ref>, but chooses to formalize and implement these opinions in a framework that looks more close to the traditional symbolic AI tradition.
Reference: [37] <author> J. Strosnider and C. Paul. </author> <title> A structured view of real-time problem solving. </title> <journal> AI Magazine, </journal> <volume> 15 </volume> <pages> 45-66, </pages> <year> 1994. </year>
Reference-contexts: Most current AI systems do not consider time constraints at run-time. Most "real time" systems only process time constraints in the form of deadline <ref> [37] </ref>. 3. Various constraints are imposed on what the system can experience. For example, only questions that can be answered by retrieval and deduction from current knowledge are acceptable, new knowledge cannot conflict with previous knowledge, and so on. <p> In this way, the reasonings are goal-directed, and the system's resources efficiency can be improved. Due to insufficient resources, the system cannot consult all relevant knowledge for each question. On the other hand, to set up a static standard for a satisficing answer <ref> [37] </ref> is too rigid a solution, because the resources may still be variable for a better answer. <p> On the other hand, many real-time systems allow users to attach a deadline to a task, and the time spent on the task is determined by the deadline <ref> [37] </ref>. A variation of this approach is that the task is provided with no deadline, but the user can interrupt the process at any time to get a best-so-far answer [2].
Reference: [38] <author> A. </author> <title> Turing. </title> <journal> Computing machinery and intelligence. Mind, </journal> <volume> LIX:433-460, </volume> <year> 1950. </year>
Reference-contexts: 1 To Define Intelligence 1.1 Retrospect The attempts of clarifying the concept "intelligence" and discussing the possibility and paths to produce it in computing machinery can be backtracked to Turing's famous article in 1950, in which he suggested an imitation test as a sufficient condition of being intelligent <ref> [38] </ref>. The debate on this issue has been going on for decades, and there is still little sign of consensus [15]. <p> In this way, we can draw "a fairly sharp line between the physical and the intellectual capacities of a man" (Turing in <ref> [38] </ref>). 24 Such a working definition of intelligence asks researchers to use passing the Turing test as a sufficient and necessary condition for having intelligence, and to take psychological evidence seriously, as Soar does [23].
Reference: [39] <author> P. Wang. </author> <title> Belief revision in probability theory. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 519-526. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: As a result, the evaluation is changeable and system-dependent. The weight of evidence is defined in such a way that it can be used to indicate randomness (see <ref> [39] </ref> for a comparison with Bayesian network [26]), fuzziness (see [41] for a comparison with fuzzy logic [44]), and ignorance (see [43] for a comparison with Dempster-Shafer theory [34]). Though different types of uncertainty have different origins, they usually co-exist, and are tangled with one another in practical situations.
Reference: [40] <author> P. Wang. </author> <title> From inheritance relation to non-axiomatic logic. </title> <type> Technical Report 84, </type> <institution> Center for Research on Concepts and Cognition, Indiana University, Bloomington, Indiana, </institution> <year> 1993. </year> <note> Available via WWW at http://www.cogsci.indiana.edu/farg/pwang papers.html. A revised version is going to appear in the International Journal of Approximate Reasoning. </note>
Reference-contexts: Because this paper is concentrated in the philosophical and methodological foundation of the NARS project, formal descriptions and detailed discussions for the components are left to other papers (such as <ref> [40, 42] </ref>). 3.1 Experience-grounded semantics The traditional model-theoretic semantics is no longer applicable to NARS. Due to AIKR, no knowledge in NARS is "true" in the sense that it corresponds to "state of affairs" in the real world. <p> The new semantics is discussed in more detail, and applied to a formal language, in <ref> [40] </ref>. The basic differences between experience-grounded semantics and model-theoretic semantics are: 1. As descriptions of an environment, the former is partial, developing in time, and not conflict-free, whereas the latter is complete, static, and consistent. 2. <p> In its simplest form, the relationship can be measured by the weight of positive and negative evidence of a statement (according to available experience). This measurement of uncertainty and its variations are discussed in detail in <ref> [40] </ref>. What makes this measurement different from other proposed measurements of uncertainty is that it compares a statement with the experience of the system rather than with a model. As a result, the evaluation is changeable and system-dependent. <p> Though widely believed to be "too restricted" and outstripped as a language for mathematical logic, term-oriented language is more suitable for an intelligent reasoning system, as shown by the practice of NARS. NARS uses a formally defined language (see <ref> [40] </ref> for a simple version), in which each sentence has the form "S P ", where S is the subject term of the sentence, and P is the predicate term of the sentence. <p> As a multi-valued extension of "&lt;", "" is transitive to a certain degree, which is measured by the truth value. For a term T , ideally there are three possibilities (a more general and formal description is in <ref> [40] </ref>): 1. <p> A term may serve as an instance for another term, and represent a property for a third term, at the same time. In this way, inferences about intensions (properties) and inference about extensions (instances) are processed similarly (see <ref> [40] </ref> for detail). 3.4 Plausible inferences Due to insufficient knowledge, the system needs to do "ampliative inferences", such as induction, abduction, and analogy. <p> Even deductions are no longer "truth-preserving", in the sense that a conclusion may be revised by new knowledge, even if the premises remain unchallenged. 10 A major advantage of term logics over predicate/propositional logics is: multiple types of inference can be naturally put into the format of syllogism <ref> [27, 40] </ref>. <p> Different types of inference have different truth value functions. Generally speaking, abductive and inductive conclusions are supported by less evidence than deductive conclusions. The functions are determined according to the semantics of the language <ref> [40] </ref>. In axiomatic logics, an inference rule is valid if it is truth-preserving. In NARS, an inference rule is valid if its conclusion summarizes correctly (according to the semantics) the experience carried by the premises. <p> On the other hand, to set up a static standard for a satisficing answer [37] is too rigid a solution, because the resources may still be variable for a better answer. What NARS does is: to report a best-so-far answer <ref> [40] </ref>, and to continue looking for better answers, permitted by the system's current resources situation [42]. 3.6 Controlled concurrency To support different types of time requirements, in NARS the "time pressure" on each inference task (forward or backward) is not represented by an absolute deadline, but by a relatively defined urgency, <p> These conflicts are normal, rather than exceptional. Actually, their existence is a major driving force of learning, and only by their solutions some types of inference, like induction and abduction, can get their results accumulated <ref> [40] </ref>. In first order predicate 13 logic, a pair of conflicting propositions imply all propositions. This does not happen in a term logic, such as NARS. 4.2 Multiple results Conventional algorithms provide a single answer to each question, then stop working on it. <p> As a field of science, we want to learn how human mind, or "mind" in general, works; as a branch 3 In addition, NARS can also be interpreted as a network by taking terms as nodes, and judgments as links <ref> [40] </ref>. The possibility of interpreting NARS both as a symbolic reasoning system and an associative network ease the comparisons between NARS and other systems. 23 of technology, we want to apply computers to domains where only the human mind works well currently.
Reference: [41] <author> P. Wang. </author> <title> The interpretation of fuzziness. </title> <type> Technical Report 86, </type> <institution> Center for Research on Concepts and Cognition, Indiana University, Bloomington, Indiana, </institution> <year> 1993. </year> <note> Available via WWW at http://www.cogsci.indiana.edu/farg/pwang papers.html. </note>
Reference-contexts: As a result, the evaluation is changeable and system-dependent. The weight of evidence is defined in such a way that it can be used to indicate randomness (see [39] for a comparison with Bayesian network [26]), fuzziness (see <ref> [41] </ref> for a comparison with fuzzy logic [44]), and ignorance (see [43] for a comparison with Dempster-Shafer theory [34]). Though different types of uncertainty have different origins, they usually co-exist, and are tangled with one another in practical situations. <p> As a result, the system's behavior will to a certain extent depend on "its own 2 Therefore, all the concepts in NARS are "fuzzy" [44], however, NARS is not a "fuzzy logic", according to the current usage of the term. See <ref> [41] </ref> for more discussions. 18 tasks", which are actually more or less independent of the original processes, even though historically derived from them. This is the functional autonomy phenomena [22].
Reference: [42] <author> P. Wang. </author> <title> Non-axiomatic reasoning system (version 2.2). </title> <type> Technical Report 75, </type> <institution> Center for Research on Concepts and Cognition, Indiana University, Bloomington, Indiana, </institution> <year> 1993. </year> <note> Available via WWW at http://www.cogsci.indiana.edu/farg/pwang papers.html. </note>
Reference-contexts: Because this paper is concentrated in the philosophical and methodological foundation of the NARS project, formal descriptions and detailed discussions for the components are left to other papers (such as <ref> [40, 42] </ref>). 3.1 Experience-grounded semantics The traditional model-theoretic semantics is no longer applicable to NARS. Due to AIKR, no knowledge in NARS is "true" in the sense that it corresponds to "state of affairs" in the real world. <p> What NARS does is: to report a best-so-far answer [40], and to continue looking for better answers, permitted by the system's current resources situation <ref> [42] </ref>. 3.6 Controlled concurrency To support different types of time requirements, in NARS the "time pressure" on each inference task (forward or backward) is not represented by an absolute deadline, but by a relatively defined urgency, which indicates the time quota the task can get by comparing it with other tasks, <p> Because new tasks can appear at any time, and the question-answering activities are usually open-ended (as described above), NARS cannot answer questions one by one, but have to work on many of them in a time-sharing manner. 11 NARS use a control mechanism named controlled concurrency <ref> [42] </ref>, which is similar to the parallel terraced scan strategy [13]. In NARS, the processor time is allocated according to the urgency distribution of the tasks, so different tasks are processed at different speeds. <p> Because in term logics, all forward and backward inferences require the premises share at least one common term, they must happen within a chunk. As a result, chunk becomes a natural unit for time and space scheduling <ref> [42] </ref>. Within a chunk, knowledge is also organized according to a relative importance evaluation. Knowledge with a higher importance value is more "accessible" for the system, that is, has a higher probability to be used to process the current tasks. <p> Only the compound concepts that correspond to repeatedly appearing patterns in the experience can survive the competition for resources, and develop into stable, full-fledged concepts. 3.9 Working routine In summary, NARS works by repeatedly carrying out inference steps, each of which consist of the following operations <ref> [42] </ref>: 1. Check for input tasks (new knowledge or question provided by environment). If there are input tasks, put them into corresponding chunks, increase the priority of the in volved chunks, and generate new chunks (if necessary). 2. Pick up a chunk according to their priority distribution. <p> The interesting point is: now all of them can be derived (to a certain extent) from the previous working definition of intelligence, and many of them have been shown, to a different extent, by an implementation of NARS <ref> [42] </ref>. <p> For instance, there are many ways that NARS can be extended from its current design <ref> [42] </ref>, though it is already a non-axiomatic system. 5.3 Intelligence and computation What is the relationship of artificial intelligence (AI) and computer science (CS)? What is the position of AI in the whole science enterprise? Traditionally, AI is referred to as a branch of CS. <p> As a virtual machine, NARS can be based on another virtual machine, which is a pure-axiomatic system <ref> [42] </ref>, and this fact does not make the system less "non-axiomatic". Obviously, with its fluid concepts, revisable knowledge, and fallible inference rules, NARS breaks the regulations of classic logics. Being context-dependent and open-ended, the question-answering activities are also no longer computations.
Reference: [43] <author> P. Wang. </author> <title> A defect in Dempster-Shafer theory. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 560-566. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1994. </year>
Reference-contexts: As a result, the evaluation is changeable and system-dependent. The weight of evidence is defined in such a way that it can be used to indicate randomness (see [39] for a comparison with Bayesian network [26]), fuzziness (see [41] for a comparison with fuzzy logic [44]), and ignorance (see <ref> [43] </ref> for a comparison with Dempster-Shafer theory [34]). Though different types of uncertainty have different origins, they usually co-exist, and are tangled with one another in practical situations.
Reference: [44] <author> L. Zadeh. </author> <title> Fuzzy sets. </title> <journal> Information and Control, </journal> <volume> 8 </volume> <pages> 338-353, </pages> <year> 1965. </year> <month> 32 </month>
Reference-contexts: As a result, the evaluation is changeable and system-dependent. The weight of evidence is defined in such a way that it can be used to indicate randomness (see [39] for a comparison with Bayesian network [26]), fuzziness (see [41] for a comparison with fuzzy logic <ref> [44] </ref>), and ignorance (see [43] for a comparison with Dempster-Shafer theory [34]). Though different types of uncertainty have different origins, they usually co-exist, and are tangled with one another in practical situations. <p> On the other hand, it is impossible for the system to always determine correctly which processes are more closely related to the original processes. As a result, the system's behavior will to a certain extent depend on "its own 2 Therefore, all the concepts in NARS are "fuzzy" <ref> [44] </ref>, however, NARS is not a "fuzzy logic", according to the current usage of the term. See [41] for more discussions. 18 tasks", which are actually more or less independent of the original processes, even though historically derived from them. This is the functional autonomy phenomena [22].
References-found: 44

