URL: http://www.eecs.umich.edu/HPS/pub/tc_design_toc.ps
Refering-URL: http://www.eecs.umich.edu/HPS/hps_tracecache.html
Root-URL: http://www.eecs.umich.edu
Email: fsanjayp, ites, pattg@eecs.umich.edu  
Phone: Tel: (313) 936-0404  
Title: Evaluation of Design Options for the Trace Cache Fetch Mechanism  
Author: Sanjay Jeram Patel, Daniel Holmes Friendly, and Yale N. Patt Fellow, IEEE 
Keyword: high bandwidth fetch mechanisms, trace cache, instruction cache, wide issue machines, speculative execution  
Address: Ann Arbor, MI 48109-2122  
Affiliation: Advanced Computer Architecture Laboratory Department of Electrical Engineering and Computer Science The University of Michigan  
Abstract: In this paper, we examine some critical design features of a trace cache fetch engine for a 16-wide issue processor and evaluate their effects on performance. We evaluate path associativity, partial matching, and inactive issue, all of which are straightforward extensions to the trace cache. We examine features such as the fill unit and branch predictor design. In our final analysis, we show that the trace cache mechanism attains a 28% performance improvement over an aggressive single block fetch mechanism and a 15% improvement over a sequential multi-block mechanism. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Burger, T. Austin, and S. Bennett, </author> <title> "Evaluating future microprocessors: The simplescalar tool set," </title> <type> Technical Report 1308, </type> <institution> University of Wisconsin - Madison Technical Report, </institution> <month> July </month> <year> 1996. </year>
Reference-contexts: The fetch mechanism stalls until the missing line arrives. 10 4 Experimental Setup A pipeline simulator that allows the modeling of wrong path effects was used as the experimental model. The simulator was implemented using the SimpleScalar 2.0 tool suite and instruction set <ref> [1] </ref>, which is a superset of the MIPS-IV ISA. In the execution model, all instructions undergo four stages of processing: fetch, issue, schedule, execute. Each stage takes at least one cycle.
Reference: [2] <author> P.-Y. Chang, E. Hao, T.-Y. Yeh, and Y. N. Patt, </author> <title> "Branch classification: A new mechanism for improving branch predictor performance," </title> <booktitle> in Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 22-31, </pages> <year> 1994. </year> <month> 26 </month>
Reference-contexts: The selection between the components is done by a 15-bit gshare-style selector. Combining a per-address predictor with a gshare predictor has been shown to be an effective way of boosting predictor accuracy <ref> [13, 2] </ref>. This fetch mechanism is similar to the one used on the Alpha 21264 [12]. We also compare the trace cache to a mechanism where the instruction cache is capable of supplying a sequential stream of instructions beyond conditional branches which are predicted to be not taken.
Reference: [3] <author> R. P. Colwell, R. P. Nix, J. J. O'Donnell, D. B. Papworth, and P. K. Rodman, </author> <title> "A VLIW architecture for a trace scheduling compiler," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 37, no. 8, </volume> <pages> pp. 967-979, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: There are several compile-time approaches to solving the fetch bottleneck problem. With Block-Structured ISAs [14, 8], superblock scheduling [9] and trace scheduling <ref> [5, 3] </ref>, the static form of the program is organized by the compiler into longer sequential units composed of 3 multiple basic blocks.
Reference: [4] <author> T. M. Conte, K. N. Menezes, P. M. Mills, and B. A. Patel, </author> <title> "Optimization of instruction fetch mechanisms for high issue rates," </title> <booktitle> in Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: We also examine some of the key components of the trace cache fetch mechanism such as the fill unit and multiple branch predictor. 2 Related Work Cache organizations for simultaneously fetching multiple blocks have been studied by Yeh et al [28], Conte et al <ref> [4] </ref> and Seznec et al [24]. By multiporting the instruction cache and/or the branch target buffer (BTB) and generating multiple fetch addresses and branch predictions per cycle, these schemes are able to overcome the single fetch block bottleneck. There are several compile-time approaches to solving the fetch bottleneck problem.
Reference: [5] <author> J. A. Fisher, </author> <title> "Trace scheduling: A technique for global microcode compaction," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-30, no. 7, </volume> <pages> pp. 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: There are several compile-time approaches to solving the fetch bottleneck problem. With Block-Structured ISAs [14, 8], superblock scheduling [9] and trace scheduling <ref> [5, 3] </ref>, the static form of the program is organized by the compiler into longer sequential units composed of 3 multiple basic blocks.
Reference: [6] <author> D. H. Friendly, S. J. Patel, and Y. N. Patt, </author> <title> "Alternative fetch and issue techniques from the trace cache fetch mechanism," </title> <booktitle> in Proceedings of the 30th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1997. </year>
Reference-contexts: They presented a thorough comparison between the trace cache scheme and several hardware-based high-bandwidth fetch schemes and showed the advantage of using a trace cache, both in performance and latency. Extensions and analysis of the trace cache mechanism were proposed by Patel et al <ref> [19, 6, 18] </ref> and trace cache implications on processor design were presented in [23]. <p> This process is referred to as partial matching <ref> [22, 6] </ref>. Alternatively, the trace cache can be designed to signal a hit only if all the blocks within the selected segment match; otherwise a miss is signaled. Figure 4 shows the performance difference between a trace cache that implements partial matching and one that does not. <p> As long as the prediction is correct, this does not impact the effective fetch rate of the processor. If the prediction is incorrect however, an opportunity to issue a greater number of correct instructions has been missed. With inactive issue <ref> [6] </ref> all blocks within a trace cache line are issued into the processor core whether or not they match the predicted path. The blocks that do not match the prediction are said to be issued inactively. <p> For our baseline multiple branch predictor, we are using a pattern history table entry composed of 3 two-bit saturating counters. This entry format was derived as a cost-effective version of the scheme used in <ref> [19, 6] </ref> where a PHT entry is composed of 7 two-bit counters. Figure 10 shows how the seven counters are used to supply three predictions per cycle.
Reference: [7] <author> D. H. Friendly, S. J. Patel, and Y. N. Patt, </author> <title> "Putting the fill unit to work: Dynamic optimizations for trace cache microprocessors," </title> <booktitle> in Proceedings of the 30th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1997. </year>
Reference-contexts: Because the dependencies within a segment are explicitly marked, the ordering of instructions carries no significance. Instructions within the cache line can be arranged to mitigate the routing required to forward instructions to functional unit reservation stations. Friendly et al <ref> [7] </ref> examine a scheme where instructions within a segment are ordered to reduce the communication delays associated with data forwarding across many functional units. 7 3.2 The Fill Unit The fill unit collects instructions as they are issued by the processor and combines them into trace segments.
Reference: [8] <author> E. Hao, P.-Y. Chang, M. Evers, and Y. N. Patt, </author> <title> "Increasing the instruction fetch rate via block-structured instruction set architectures," </title> <booktitle> in Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1996. </year>
Reference-contexts: By multiporting the instruction cache and/or the branch target buffer (BTB) and generating multiple fetch addresses and branch predictions per cycle, these schemes are able to overcome the single fetch block bottleneck. There are several compile-time approaches to solving the fetch bottleneck problem. With Block-Structured ISAs <ref> [14, 8] </ref>, superblock scheduling [9] and trace scheduling [5, 3], the static form of the program is organized by the compiler into longer sequential units composed of 3 multiple basic blocks.
Reference: [9] <author> W. W. Hwu, S. A. Mahlke, W. Y. Chen, P. P. Chang, N. J. Warter, R. A. Bringmann, R. G. Ouellette, R. E. Hank, T. Kiyohara, G. E. Haab, J. G. Holm, and D. M. Lavery, </author> <title> "The superblock: An effective technique for VLIW and superscalar compilation," </title> <journal> Journal of Supercomputing, </journal> <volume> vol. 7, no. </volume> <pages> 9-50, </pages> , <year> 1993. </year>
Reference-contexts: There are several compile-time approaches to solving the fetch bottleneck problem. With Block-Structured ISAs [14, 8], superblock scheduling <ref> [9] </ref> and trace scheduling [5, 3], the static form of the program is organized by the compiler into longer sequential units composed of 3 multiple basic blocks.
Reference: [10] <author> W. W. Hwu and Y. N. Patt, </author> <title> "Checkpoint repair for out-of-order execution machines," </title> <booktitle> in Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 18-26, </pages> <year> 1987. </year>
Reference-contexts: An ideal return address stack is modeled. The execution engine is composed of 16 functional units, each with a 32-entry reservation station. The functional units are uniform and capable of all operations. A 64KB L1 data cache was used. The model uses checkpoint repair <ref> [10] </ref> to recover from branch mispredictions and exceptions. The execution engine is capable of creating up to three checkpoints each cycle, one for each fetch block supplied. The memory scheduler waits for addresses to be generated before scheduling memory operations. <p> The blocks that do not match the prediction are said to be issued inactively. Although these inactive instructions are renamed and receive physical registers for their destination values, the changes they make to the register alias table <ref> [10] </ref> are not considered valid for subsequent issue cycles. Thus instructions along the predicted path view the speculative state of the processor exactly as if the inactive blocks had not been issued.
Reference: [11] <author> J. D. Johnson, </author> <title> "Expansion caches for superscalar microprocessors," </title> <type> Technical Report CSL--TR-94-630, </type> <institution> Stanford University, </institution> <address> Palo Alto CA, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Such a solution adds considerable logic complexity in an already critical execution path of the processor. Either cycle time will be affected or extra pipeline stages will be required. Recently proposed, the trace cache <ref> [20, 11, 22, 19] </ref> overcomes this bandwidth hurdle without requiring excessive logic complexity in the instruction delivery path. Like an instruction cache, the trace cache is accessed using the Program Counter. <p> In 1994, Peleg and Weiser filed a patent on the trace cache concept [20]. A similar concept was proposed by Johnson <ref> [11] </ref>. The concept was further investigated by Rotenberg et al [22]. They presented a thorough comparison between the trace cache scheme and several hardware-based high-bandwidth fetch schemes and showed the advantage of using a trace cache, both in performance and latency.
Reference: [12] <author> J. Keller, </author> <title> The 21264: A Superscalar Alpha Processor with Out-of-Order Execution, </title> <institution> Digital Equipment Corporation, Hudson, </institution> <address> MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: The selection between the components is done by a 15-bit gshare-style selector. Combining a per-address predictor with a gshare predictor has been shown to be an effective way of boosting predictor accuracy [13, 2]. This fetch mechanism is similar to the one used on the Alpha 21264 <ref> [12] </ref>. We also compare the trace cache to a mechanism where the instruction cache is capable of supplying a sequential stream of instructions beyond conditional branches which are predicted to be not taken.
Reference: [13] <author> S. McFarling, </author> <title> "Combining branch predictors," </title> <type> Technical Report TN-36, </type> <institution> Digital Western Research Laboratory, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Evaluation of this PHT entry organization versus several others is provided in section 5.5. The baseline configuration for the trace cache predictor uses the gshare scheme outlined by McFarling <ref> [13] </ref>. The global branch history is XORed with the current fetch address, forming an index into the PHT. This hashing better utilizes the PHT and improves prediction accuracy over other global history based schemes. <p> The selection between the components is done by a 15-bit gshare-style selector. Combining a per-address predictor with a gshare predictor has been shown to be an effective way of boosting predictor accuracy <ref> [13, 2] </ref>. This fetch mechanism is similar to the one used on the Alpha 21264 [12]. We also compare the trace cache to a mechanism where the instruction cache is capable of supplying a sequential stream of instructions beyond conditional branches which are predicted to be not taken.
Reference: [14] <author> S. Melvin and Y. Patt, </author> <title> "Enhancing instruction scheduling with a block-structured ISA," </title> <booktitle> International Journal on Parallel Processing, </booktitle> <year> 1994. </year>
Reference-contexts: By multiporting the instruction cache and/or the branch target buffer (BTB) and generating multiple fetch addresses and branch predictions per cycle, these schemes are able to overcome the single fetch block bottleneck. There are several compile-time approaches to solving the fetch bottleneck problem. With Block-Structured ISAs <ref> [14, 8] </ref>, superblock scheduling [9] and trace scheduling [5, 3], the static form of the program is organized by the compiler into longer sequential units composed of 3 multiple basic blocks.
Reference: [15] <author> S. W. Melvin and Y. N. Patt, </author> <title> "Performance benefits of large execution atomic units in dynamically scheduled machines," </title> <booktitle> in Proceedings of Supercomputing '89, </booktitle> <pages> pp. 427-432, </pages> <year> 1989. </year>
Reference-contexts: There are several compile-time approaches to solving the fetch bottleneck problem. With Block-Structured ISAs [14, 8], superblock scheduling [9] and trace scheduling [5, 3], the static form of the program is organized by the compiler into longer sequential units composed of 3 multiple basic blocks. Melvin and Patt <ref> [15] </ref> discuss the performance implications of the fill unit and the idea of dynamically combining fetch blocks into larger "execution atomic units" (EAUs) to further increase the fetch bandwidth is first proposed. In 1994, Peleg and Weiser filed a patent on the trace cache concept [20].
Reference: [16] <author> K. N. Menezes, S. W. Sathaye, and T. M. Conte, </author> <title> "Path prediction for high issue-rate processors," </title> <booktitle> in Proceedings of the 1997 ACM/IEEE Conference on Parallel Architectures and Compilation Techniques, </booktitle> <year> 1997. </year>
Reference-contexts: All three predictions are made with a single access to the PHT. The configurations evaluated in this experiment include the 7 counter scheme, the 3 counter scheme, and a scheme presented by Menezes et al <ref> [16] </ref> where each PHT entry contains the most likely path through a program subgraph containing 3 branches. In this scheme, the PHT entries are 4 bits wide: 3 bits to encode the likely path (8 paths are possible) and a fourth bit which records the likeliness of this path.
Reference: [17] <author> R. Nair and M. E. Hopkins, </author> <title> "Exploiting instruction level parallelism in processors by caching scheduled groups," </title> <booktitle> in Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 13-25, </pages> <year> 1997. </year>
Reference-contexts: Extensions and analysis of the trace cache mechanism were proposed by Patel et al [19, 6, 18] and trace cache implications on processor design were presented in [23]. A similar approach to caching dynamic instruction groups was presented in the DIF cache by Nair and Hopkins <ref> [17] </ref>. 3 The Trace Cache Fetch Mechanism We divide the trace cache fetch mechanism into four major components: a trace cache, a fill unit, a multiple branch predictor, and a conventional instruction cache.
Reference: [18] <author> S. J. Patel, M. Evers, and Y. N. Patt, </author> <title> "Improving trace cache effectiveness with branch promotion and trace packing," </title> <booktitle> in Proceedings of the 25th Annual International Symposium on Computer Architecture, </booktitle> <year> 1998. </year>
Reference-contexts: They presented a thorough comparison between the trace cache scheme and several hardware-based high-bandwidth fetch schemes and showed the advantage of using a trace cache, both in performance and latency. Extensions and analysis of the trace cache mechanism were proposed by Patel et al <ref> [19, 6, 18] </ref> and trace cache implications on processor design were presented in [23]. <p> In addition to exacerbating the block redundancy problem, splitting a basic block creates an additional block that may tax other structures such as the branch predictor. A study of effective techniques for splitting blocks was done by Patel et al <ref> [18] </ref>. Three outcomes are possible with the arrival of each new block of instructions: (1) the arriving block is merged with the unfinalized segment and the new, larger segment is not finalized. (2) the entire arriving block cannot be merged with the awaiting segment.
Reference: [19] <author> S. J. Patel, D. H. Friendly, and Y. N. Patt, </author> <title> "Critical issues regarding the trace cache fetch mechanism," </title> <type> Technical Report CSE-TR-335-97, </type> <institution> University of Michigan Technical Report, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: Such a solution adds considerable logic complexity in an already critical execution path of the processor. Either cycle time will be affected or extra pipeline stages will be required. Recently proposed, the trace cache <ref> [20, 11, 22, 19] </ref> overcomes this bandwidth hurdle without requiring excessive logic complexity in the instruction delivery path. Like an instruction cache, the trace cache is accessed using the Program Counter. <p> They presented a thorough comparison between the trace cache scheme and several hardware-based high-bandwidth fetch schemes and showed the advantage of using a trace cache, both in performance and latency. Extensions and analysis of the trace cache mechanism were proposed by Patel et al <ref> [19, 6, 18] </ref> and trace cache implications on processor design were presented in [23]. <p> For our baseline multiple branch predictor, we are using a pattern history table entry composed of 3 two-bit saturating counters. This entry format was derived as a cost-effective version of the scheme used in <ref> [19, 6] </ref> where a PHT entry is composed of 7 two-bit counters. Figure 10 shows how the seven counters are used to supply three predictions per cycle.
Reference: [20] <author> A. Peleg and U. Weiser. </author> <title> Dynamic Flow Instruction Cache Memory Organized Around Trace Segments Independant of Virtual Address Line. </title> <type> U.S. Patent Number 5,381,533, </type> <year> 1994. </year> <month> 28 </month>
Reference-contexts: Such a solution adds considerable logic complexity in an already critical execution path of the processor. Either cycle time will be affected or extra pipeline stages will be required. Recently proposed, the trace cache <ref> [20, 11, 22, 19] </ref> overcomes this bandwidth hurdle without requiring excessive logic complexity in the instruction delivery path. Like an instruction cache, the trace cache is accessed using the Program Counter. <p> Melvin and Patt [15] discuss the performance implications of the fill unit and the idea of dynamically combining fetch blocks into larger "execution atomic units" (EAUs) to further increase the fetch bandwidth is first proposed. In 1994, Peleg and Weiser filed a patent on the trace cache concept <ref> [20] </ref>. A similar concept was proposed by Johnson [11]. The concept was further investigated by Rotenberg et al [22]. They presented a thorough comparison between the trace cache scheme and several hardware-based high-bandwidth fetch schemes and showed the advantage of using a trace cache, both in performance and latency.
Reference: [21] <author> F. Pollack, </author> <title> Description of the intel microprocessor roadmap, </title> <publisher> Press briefing, </publisher> <month> October </month> <year> 1998. </year>
Reference-contexts: Much of this performance increase comes from the increase in effective fetch rate, which is twice that of the single block engine. Because it is a low-complexity technique for delivering high instruction bandwidth, the trace cache will be an important component of future microprocessors <ref> [21] </ref>. There remain many important issues which need resolving.
Reference: [22] <author> E. Rotenberg, S. Bennett, and J. E. Smith, </author> <title> "Trace cache: a low latency approach to high bandwidth instruction fetching," </title> <booktitle> in Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1996. </year>
Reference-contexts: Such a solution adds considerable logic complexity in an already critical execution path of the processor. Either cycle time will be affected or extra pipeline stages will be required. Recently proposed, the trace cache <ref> [20, 11, 22, 19] </ref> overcomes this bandwidth hurdle without requiring excessive logic complexity in the instruction delivery path. Like an instruction cache, the trace cache is accessed using the Program Counter. <p> In 1994, Peleg and Weiser filed a patent on the trace cache concept [20]. A similar concept was proposed by Johnson [11]. The concept was further investigated by Rotenberg et al <ref> [22] </ref>. They presented a thorough comparison between the trace cache scheme and several hardware-based high-bandwidth fetch schemes and showed the advantage of using a trace cache, both in performance and latency. <p> This process is referred to as partial matching <ref> [22, 6] </ref>. Alternatively, the trace cache can be designed to signal a hit only if all the blocks within the selected segment match; otherwise a miss is signaled. Figure 4 shows the performance difference between a trace cache that implements partial matching and one that does not. <p> We provide a comparison of the enhances trace cache to the current dominant techniques for fetch engine design. Rotenberg et al. <ref> [22] </ref> presented a thorough comparison of the trace cache's performance on the SPECint92 and IBS benchmarks to a few of the hardware-based multiple block fetch techniques mentioned in section 2.
Reference: [23] <author> E. Rotenberg, Q. Jacobsen, Y. Sazeides, and J. E. Smith, </author> <title> "Trace processors," </title> <booktitle> in Proceedings of the 30th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1997. </year>
Reference-contexts: Extensions and analysis of the trace cache mechanism were proposed by Patel et al [19, 6, 18] and trace cache implications on processor design were presented in <ref> [23] </ref>.
Reference: [24] <author> A. Seznec, S. Jourdan, P. Sainrat, and P. Michaud, </author> <title> "Multiple-block ahead branch predictors," </title> <booktitle> in Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1996. </year>
Reference-contexts: We also examine some of the key components of the trace cache fetch mechanism such as the fill unit and multiple branch predictor. 2 Related Work Cache organizations for simultaneously fetching multiple blocks have been studied by Yeh et al [28], Conte et al [4] and Seznec et al <ref> [24] </ref>. By multiporting the instruction cache and/or the branch target buffer (BTB) and generating multiple fetch addresses and branch predictions per cycle, these schemes are able to overcome the single fetch block bottleneck. There are several compile-time approaches to solving the fetch bottleneck problem.
Reference: [25] <author> E. Sprangle and Y. Patt, </author> <title> "Facilitating superscalar processing via a combined static/dynamic register renaming scheme," </title> <booktitle> in Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 143-147, </pages> <year> 1994. </year>
Reference-contexts: It is important to note that a complex dependency analysis across 16 instructions does not need to be performed on segments fetched from the trace cache. The concept of explicitly marking internal/external register values within a basic block was first described by Sprangle and Patt <ref> [25] </ref> and later adapted for use with the trace cache by Vajapeyam and Mitra [27]. Instructions within a segment can be arranged in an order that permits quick issue. Because the dependencies within a segment are explicitly marked, the ordering of instructions carries no significance.
Reference: [26] <author> J. Stark, P. Racunas, and Y. N. Patt, </author> <title> "Reducing the performance impact of instruction cache misses by writing instructions into the reservation stations out-of-order," </title> <booktitle> in Proceedings of the 30th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 34 - 43, </pages> <year> 1997. </year>
Reference-contexts: All experiments were performed on the SPECint95 benchmarks and on a benchmark suite consisting of several common C applications <ref> [26] </ref>. Table 2 lists the number of instructions simulated and the input set, if the input was derived from a standard input set 1 . All simulations were run until completion (except li and ijpeg, 500M instructions).
Reference: [27] <author> S. Vajapeyam and T. Mitra, </author> <title> "Improving superscalar instruction dispatch and issue by exploiting dynamic code sequences," </title> <booktitle> in Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 1-12, </pages> <year> 1997. </year>
Reference-contexts: The concept of explicitly marking internal/external register values within a basic block was first described by Sprangle and Patt [25] and later adapted for use with the trace cache by Vajapeyam and Mitra <ref> [27] </ref>. Instructions within a segment can be arranged in an order that permits quick issue. Because the dependencies within a segment are explicitly marked, the ordering of instructions carries no significance.
Reference: [28] <author> T.-Y. Yeh, D. Marr, and Y. N. Patt, </author> <title> "Increasing the instruction fetch rate via multiple branch prediction and branch address cache," </title> <booktitle> in Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pp. 67-76, </pages> <year> 1993. </year>
Reference-contexts: We also examine some of the key components of the trace cache fetch mechanism such as the fill unit and multiple branch predictor. 2 Related Work Cache organizations for simultaneously fetching multiple blocks have been studied by Yeh et al <ref> [28] </ref>, Conte et al [4] and Seznec et al [24]. By multiporting the instruction cache and/or the branch target buffer (BTB) and generating multiple fetch addresses and branch predictions per cycle, these schemes are able to overcome the single fetch block bottleneck.

References-found: 28

