URL: ftp://mancos.cs.utah.edu/papers/coma.ps.Z
Refering-URL: ftp://mancos.cs.utah.edu/papers/coma-abs.html
Root-URL: 
Title: An Argument for Simple COMA  
Author: Ashley Saulsbury, Tim Wilkinson John Carter and Anders Landin 
Address: Box 1263, S-164 28 Kista, Sweden  
Affiliation: Swedish Institute of Computer Science  
Abstract: We present design details and some initial performance results of a novel scalable shared memory multiprocessor architecture. This architecture features the automatic data migration and replication capabilities of cache-only memory architecture (COMA) machines, without the accompanying hardware complexity. A software layer manages cache space allocation at a page-granularity similarly to distributed virtual shared memory (DVSM) systems, leaving simpler hardware to maintain shared memory coherence at a cache line granularity. By reducing the hardware complexity, the machine cost and development time are reduced. We call the resulting hybrid hardware and software multiprocessor architecture Simple COMA. Preliminary results indicate that the performance of Simple COMA is comparable to that of more complex contemporary all-hardware designs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Simoni, J. Hennessy, and M. Horowitz. </author> <title> An evaluation of directory schemes for cache coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The memory system hardware must keep cached copies of data coherent throughout the machine. In a CC-NUMA this is done by the cache controller (shown in the shaded box 2 in directory-based protocol <ref> [1] </ref> and some SRAM state. In this design, data from remote nodes can only reside locally in a node's caches, never in its main (DRAM) memory. This feature restricts the total amount of data that each node can replicate from remote nodes at any given instant.
Reference: [2] <author> J-L. Baer and W-H. Wang. </author> <title> On the Inclusion Proper ties for Multi-Level Cache Hierarchies. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 73-80, </pages> <year> 1988. </year>
Reference-contexts: For each valid cache line in the first or second level caches, there must be corresponding space allocated in the node's main memory. Thus, Simple COMA's attraction memory is fully inclusive <ref> [2] </ref> with respect to the various levels of cache. Due to this, no action is required for invalid coherence units on replacement of their containing page. If the state is invalid in the attraction memory, data cannot be in a higher level cache either.
Reference: [3] <author> W.J. Bolosky, M.L. Scott, R.P. Fitzgerald, R.J. Fowler, and A.L. Cox. </author> <title> NUMA Policies and Their Relation to Memory Architecture. </title> <booktitle> In Proceedings of the 4th ASPLOS Symposium, </booktitle> <pages> pages 212-221, </pages> <year> 1991. </year>
Reference-contexts: This effect leads to pressure when designing CC-NUMAs to build very large (and expensive) second level caches. The addition of operating system managed page migration <ref> [3] </ref> allows the designated "home" of a data page (the node where the corresponding physical memory resides) to be moved. This goes a little way towards solving the problem of incorrect data partitioning. Additionally, pages can be replicated manually, but this greatly complicates coherence management.
Reference: [4] <author> H. Burkhardt, S. Frank, B. Knobe, and J. Rothnie. </author> <title> Overview of the KSR1 Computer System. </title> <type> Technical Report KSR-TR-9202001, </type> <institution> Kendall Square Research. </institution>
Reference-contexts: However, the amount of remote data that can be replicated locally, and thus accessed efficiently, is limited by the size of a node's cache, which exacerbates the need for large and expensive caches. In a cache-only memory architecture (COMA [20]), such as the SICS DDM [7] and the KSR-1 <ref> [4] </ref>, the ma-chine's memory is again distributed across the nodes in the machine. However, instead of being allocated to fixed physical addresses, each node's memory is converted into a large, slow 1 cache (or attraction memory) by additional hardware.
Reference: [5] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Im plementation and Performance of Munin. </title> <booktitle> In Thirteenth Symposium on Operating System Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: This allows greater and more flexible data replication, but requires more complex hardware and data coherence protocols. Distributed Virtual Shared Memory (DVSM) systems, such as IVY [12] and Munin <ref> [5] </ref>, forgo all hardware support for coherence management. A DVSM system can exhibit COMA-like properties by migrating and replicating pages from node to node while still keeping them coherent. <p> In addition, DVSM systems have very simple hardware requirements. and thus can operate on anything from tightly coupled distributed memory multiprocessors like the CM-5 [14] to a network of workstations <ref> [5, 12] </ref>. However, these advantages of DVSM systems come at a potentially serious cost in terms of performance. Because all coherence management is done in software, the overheads associated with DVSM are high. <p> This overhead is increasing as network bandwidths improve. The problem can be mitigated by introducing a dedicated network technology, as was done in the Meiko CS-2 [13], or using delayed consistency to reduce the number of messages sent, as was done in Munin <ref> [5] </ref>, at the cost of more hardware or more complex coherence protocols. Finally, DVSM systems are particularly sensitive to false sharing, wherein unrelated data items that happen to reside on the same page can interfere with one another because the MMU provides support only at the granularity of a page.
Reference: [6] <author> E. Hagersten. </author> <title> Toward Scalable Cache Only Memory Architectures. </title> <type> PhD thesis, </type> <institution> Royal Institute of Technology, Stockholm, </institution> <year> 1992. </year>
Reference-contexts: In addition, COMA machines can run applications that do not map well to CC-NUMA architectures <ref> [6] </ref>, such as applications that have a per-node working set larger than the size of each node's cache and applications with dynamic data access patterns in which data cannot be statically partitioned across the physical memories effectively.
Reference: [7] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> DDM A Cache-Only Memory Architecture. </title> <journal> IEEE Computer, </journal> <volume> 25(9) </volume> <pages> 44-54, </pages> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: However, the amount of remote data that can be replicated locally, and thus accessed efficiently, is limited by the size of a node's cache, which exacerbates the need for large and expensive caches. In a cache-only memory architecture (COMA [20]), such as the SICS DDM <ref> [7] </ref> and the KSR-1 [4], the ma-chine's memory is again distributed across the nodes in the machine. However, instead of being allocated to fixed physical addresses, each node's memory is converted into a large, slow 1 cache (or attraction memory) by additional hardware. <p> For a COMA such as the SICS DDM <ref> [7] </ref>, this extra hardware consists of: (i) a hashing function to map from the physical address used by the processor to the real physical address of the set of DRAM cache lines that might hold this entry, (ii) tag memory and comparators that determine which cache line in the designated set
Reference: [8] <author> E. Hagersten, A. Saulsbury, and A. Landin. </author> <title> Simple COMA node implementations. </title> <booktitle> In Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: The latter requires hardware support not available on most platforms, unless, such as in the Wisconsin Wind Tunnel [14], or special hardware, such as in Typhoon/Tempest [15]. 3 Simple COMA In this section we describe in more detail the Simple COMA idea <ref> [8, 16] </ref> and examine various aspects of the architecture that allow a simpler hardware architecture than COMA or CC-NUMA, but without compromising the performance. 3.1 Building an attraction memory In a uniprocessor, the operating system manages physical memory as a cache of the much larger virtual memory working set.
Reference: [9] <author> T. Joe and J.L. Hennessy. </author> <title> Evaluating the memory overhead required for coma architectures. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 82-93, </pages> <year> 1994. </year>
Reference-contexts: is complicated by the need to preserve the last copy of a data item; unlike a CC-NUMA there is no reserved home for a cache line that is evicted from a node's attraction memory, so care must be taken not to throw away the last copy of a cache line <ref> [9, 21] </ref>. 2.3 DVSM Distributed Virtual Shared Memory (DVSM) systems also provide a shared memory abstraction, but memory coherence is implemented entirely in software (see Figure 1c). <p> Unlike a traditional COMA, transferring a coherence unit to another node cannot force further replacement on that node <ref> [9] </ref>, since in Simple COMA that node will already have the space allocated for the coherence units if it is sharing the page.
Reference: [10] <author> R.P. LaRowe and C.S. Ellis. </author> <title> Experimental Compari son of Memory Management Policies for NUMA Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(4) </volume> <pages> 319-363, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: In general, these mechanisms are slow and complex, work only at a page granularity, and have not been shown to improve performance significantly beyond intelligent 2 In Figure 1, shading is used to highlight where each archi-tecture's control logic is situated. initial page placement <ref> [10] </ref>. 2.2 COMA In a Cache Only Memory Architectiure (COMA) machine, additional hardware is used to implement a large DRAM cache (or attraction memory) instead of the large local memory of a CC-NUMA.
Reference: [11] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <year> 1990. </year>
Reference-contexts: Northampton Square, London, UK y Permanent address: University of Utah, Department of Computer Science, 3190 MEB, Salt Lake City, UT 84112 z e-mail: fans,tim,retrac,landing@sics.se Possibly the most popular large scale shared memory multiprocessor design is the cache coherent nonuniform memory access (CC-NUMA) architecture, such as embodied by the Stanford DASH <ref> [11] </ref>. In a CC-NUMA, the machine's physical memory is distributed across the nodes in the machine, but every node can map any page of physical memory (local or remote).
Reference: [12] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: This allows greater and more flexible data replication, but requires more complex hardware and data coherence protocols. Distributed Virtual Shared Memory (DVSM) systems, such as IVY <ref> [12] </ref> and Munin [5], forgo all hardware support for coherence management. A DVSM system can exhibit COMA-like properties by migrating and replicating pages from node to node while still keeping them coherent. <p> There are two major advantages to software DVSM systems. As they are implemented entirely in software, they can employ sophisticated algorithms to manage memory and adapt to changing memory access patterns. Even early DVSM systems like IVY <ref> [12] </ref> exhibited many of the desirable properties of COMA, e.g., automatic data migration and replication. In addition, DVSM systems have very simple hardware requirements. and thus can operate on anything from tightly coupled distributed memory multiprocessors like the CM-5 [14] to a network of workstations [5, 12]. <p> In addition, DVSM systems have very simple hardware requirements. and thus can operate on anything from tightly coupled distributed memory multiprocessors like the CM-5 [14] to a network of workstations <ref> [5, 12] </ref>. However, these advantages of DVSM systems come at a potentially serious cost in terms of performance. Because all coherence management is done in software, the overheads associated with DVSM are high.
Reference: [13] <author> Meiko Limited. </author> <title> CS-2 Product Description, </title> <year> 1992. </year>
Reference-contexts: This overhead is increasing as network bandwidths improve. The problem can be mitigated by introducing a dedicated network technology, as was done in the Meiko CS-2 <ref> [13] </ref>, or using delayed consistency to reduce the number of messages sent, as was done in Munin [5], at the cost of more hardware or more complex coherence protocols.
Reference: [14] <author> S. K. Reinhardt, M. D. Hill, J. R. Larus, A. R. Lebeck, J. C. Lewis, and D. A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Even early DVSM systems like IVY [12] exhibited many of the desirable properties of COMA, e.g., automatic data migration and replication. In addition, DVSM systems have very simple hardware requirements. and thus can operate on anything from tightly coupled distributed memory multiprocessors like the CM-5 <ref> [14] </ref> to a network of workstations [5, 12]. However, these advantages of DVSM systems come at a potentially serious cost in terms of performance. Because all coherence management is done in software, the overheads associated with DVSM are high. <p> The latter requires hardware support not available on most platforms, unless, such as in the Wisconsin Wind Tunnel <ref> [14] </ref>, or special hardware, such as in Typhoon/Tempest [15]. 3 Simple COMA In this section we describe in more detail the Simple COMA idea [8, 16] and examine various aspects of the architecture that allow a simpler hardware architecture than COMA or CC-NUMA, but without compromising the performance. 3.1 Building an
Reference: [15] <author> S.K. Reinhardt, J.R. Larus, and D.A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proc. of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <year> 1994. </year>
Reference-contexts: The latter requires hardware support not available on most platforms, unless, such as in the Wisconsin Wind Tunnel [14], or special hardware, such as in Typhoon/Tempest <ref> [15] </ref>. 3 Simple COMA In this section we describe in more detail the Simple COMA idea [8, 16] and examine various aspects of the architecture that allow a simpler hardware architecture than COMA or CC-NUMA, but without compromising the performance. 3.1 Building an attraction memory In a uniprocessor, the operating system
Reference: [16] <author> A. Saulsbury. </author> <title> Supporting fine-grain shared mem ory. </title> <note> Technical Report from January 1993, now publi-cally available as SICS Tech Report number T94:06, </note> <institution> Swedish Institute of Computer Science, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: The latter requires hardware support not available on most platforms, unless, such as in the Wisconsin Wind Tunnel [14], or special hardware, such as in Typhoon/Tempest [15]. 3 Simple COMA In this section we describe in more detail the Simple COMA idea <ref> [8, 16] </ref> and examine various aspects of the architecture that allow a simpler hardware architecture than COMA or CC-NUMA, but without compromising the performance. 3.1 Building an attraction memory In a uniprocessor, the operating system manages physical memory as a cache of the much larger virtual memory working set.
Reference: [17] <author> A. Saulsbury, T. Wilkinson, J. B. Carter, and A. Landin. </author> <title> Handling replacement in simple COMA. </title> <institution> Swedish Inst. of Computer Science Report, </institution> <year> 1994. </year>
Reference-contexts: Then, the page being replaced can be evicted by the coherence hardware while the computation continues. A full description of the analytical model used in this study can be found elsewhere <ref> [17] </ref>. 5.4 Simulation results Figures 2 through 4 illustrate the estimated extra number of machine cycles required to handle first and second level cache misses and inter-node coherence-related communication for a CC-NUMA, a traditional COMA, and a Simple COMA machine.
Reference: [18] <author> J.S. Singh, W-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <institution> Stanford University, </institution> <type> Report, </type> <month> April </month> <year> 1991. </year>
Reference-contexts: information was fed into an analytical model that calculated the approximate number of machine cycles the computation is stalled for each architecture and the impact of Simple COMA's page replacement overhead on performance. 5.1 Applications The simulations in this study modeled moderately sized machines running applications chosen from the Splash <ref> [18] </ref> benchmark suite: MP3D A simple simulator for rarified gas flow over an object in a wind tunnel. Data: 80000 particles for 40 time steps using the default test geometry. OCEAN An SOR application modeling the behavior of a small section of ocean surface.
Reference: [19] <author> M. Thapar and B. Delagi. </author> <title> Stanford Distributed Directory Protocol. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 78-80, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: software must designate an "owner of last resort" for each page that cannot silently discard the page without designating a new "owner of last resort." Finally, if the list of coherence unit copies is distributed with the data in a coherence protocol design, as in the Stan-ford Distributed Directory protocol <ref> [19] </ref>, then the coherence units must be unhooked from the linked lists before the page can be reallocated.
Reference: [20] <author> D.H.D. Warren and S. Haridi. </author> <title> Data Diffusion Machine-a scalable shared virtual memory multiprocessor. </title> <booktitle> In International Conference on Fifth Generation Computer Systems 1988. </booktitle> <publisher> ICOT, </publisher> <year> 1988. </year>
Reference-contexts: However, the amount of remote data that can be replicated locally, and thus accessed efficiently, is limited by the size of a node's cache, which exacerbates the need for large and expensive caches. In a cache-only memory architecture (COMA <ref> [20] </ref>), such as the SICS DDM [7] and the KSR-1 [4], the ma-chine's memory is again distributed across the nodes in the machine. However, instead of being allocated to fixed physical addresses, each node's memory is converted into a large, slow 1 cache (or attraction memory) by additional hardware.
Reference: [21] <author> X. Zhang and Y. Yan. </author> <title> Latency analysis of cc-numa and cc-coma hierarchical ring multiprocessors. </title> <booktitle> In Proceedings of the 1994 International Conference of Parallel Processing, </booktitle> <volume> Vol. I, </volume> <pages> pages 174-181, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: is complicated by the need to preserve the last copy of a data item; unlike a CC-NUMA there is no reserved home for a cache line that is evicted from a node's attraction memory, so care must be taken not to throw away the last copy of a cache line <ref> [9, 21] </ref>. 2.3 DVSM Distributed Virtual Shared Memory (DVSM) systems also provide a shared memory abstraction, but memory coherence is implemented entirely in software (see Figure 1c).
References-found: 21

