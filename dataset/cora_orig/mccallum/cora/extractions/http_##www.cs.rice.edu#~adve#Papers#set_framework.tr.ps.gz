URL: http://www.cs.rice.edu/~adve/Papers/set_framework.tr.ps.gz
Refering-URL: http://www.cs.rice.edu/~dsystem/dhpf/overview.html
Root-URL: 
Title: An Integer Set Framework for HPF Analysis and Code Generation  
Author: Vikram Adve John Mellor-Crummey Ajay Sethi 
Address: M/S 132  6100 Main Street, Houston, TX 77005-1892.  
Affiliation: Department of Computer Science,  Rice University  
Abstract: The core of the Rice dHPF compiler for High Performance Fortran is a practical, executable, equational framework for data parallel program analysis and optimization. This framework, based on abstract operations on sets of integers, greatly simplifies the implementation of many key analysis, optimization and code generation tasks (even with a general computation partitioning model), and yet enhances their generality and flexibility. This paper describes the equational framework, tradeoffs in its implementation, the formulation and implementation of important analyses and optimizations within the framework, and a hierarchical code generation strategy to support the framework.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: First, the compiler should have the maximum possible flexibility in its computation partitioning strategies, and not restrict the partitioning to follow the widely-used owner-computes rule [25] or restrict all statements in a loop to have the same partitioning <ref> [1] </ref>. The dHPF compiler incorporates a flexible computation partitioning (CP) model, in which each statement (including control flow) has one or more computational "homes" that specify where instances of the statement will execute. <p> A few research groups have used systems of linear inequalities to provide a more general and flexible approach than patterns for compiling data parallel programs <ref> [3, 2, 5, 1] </ref>. In most cases, these have been 1 used primarily for code generation based on Fourier-Motzkin elimination (FME) [26, 3]. <p> In most cases, these have been 1 used primarily for code generation based on Fourier-Motzkin elimination (FME) [26, 3]. Amarasinghe and Lam <ref> [1] </ref> also describe how inequalities and FME can support array dataflow analysis and a few specific communication optimizations, but these primarily operate by direct manipulation of inequalities. <p> Furthermore, their representation could not represent general non-convex sets or a general set union operation, so optimizations using them are limited in scope <ref> [1, 5, 2] </ref>. (For example, their representation would not support our CP model, or optimizations such as coalescing of arbitrary affine references to an array [1].) The Rice dHPF compiler is based on a practical, executable, equational framework that enables data parallel program analyses and optimizations to be expressed in terms <p> Furthermore, their representation could not represent general non-convex sets or a general set union operation, so optimizations using them are limited in scope [1, 5, 2]. (For example, their representation would not support our CP model, or optimizations such as coalescing of arbitrary affine references to an array <ref> [1] </ref>.) The Rice dHPF compiler is based on a practical, executable, equational framework that enables data parallel program analyses and optimizations to be expressed in terms of abstract operations on integer sets. <p> Limited set representations such as Regular Section Descriptors [14] and Data Access Descriptors [4], as well as previous implementations of systems of linear inequalites supported by Fourier-Motzkin Elimination <ref> [2, 5, 1] </ref>, do not support general non-convex sets set (and consequently, do not support general set union, difference, or complement operations). Their major advantage, however, is that they can support affine expressions with symbolic (i.e., unknown constant) coefficients. <p> The previous work on using linear inequalities and Fourier-Motzkin Elimination [26] for code generation share our goal of improving the generality, the level of abstraction, and the quality of code in data-parallel compilers. Three groups (Ancourt et al. [2], the Paradigm compiler group [29] and SUIF <ref> [1] </ref>) applied these techniques to support code generation for communication and iteration sets described by linear inequalities. <p> The representations used by these groups were unable to describe general non-convex sets, which limited the scope of their techniques (e.g., it precluded addressing problems such as coalescing communication for arbitrary affine references or splitting iterations sets). The SUIF work <ref> [1] </ref> assumes that all statements in a loop have identical computation partitionings. In contrast, we support both communication analysis and code generation for a much more general computation partitioning model than previous compilers.
Reference: [2] <author> C. Ancourt, F. Coelho, F. Irigoin, and R. Keryell. </author> <title> A linear algebra framework for static HPF code distribution. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: A few research groups have used systems of linear inequalities to provide a more general and flexible approach than patterns for compiling data parallel programs <ref> [3, 2, 5, 1] </ref>. In most cases, these have been 1 used primarily for code generation based on Fourier-Motzkin elimination (FME) [26, 3]. <p> Furthermore, their representation could not represent general non-convex sets or a general set union operation, so optimizations using them are limited in scope <ref> [1, 5, 2] </ref>. (For example, their representation would not support our CP model, or optimizations such as coalescing of arbitrary affine references to an array [1].) The Rice dHPF compiler is based on a practical, executable, equational framework that enables data parallel program analyses and optimizations to be expressed in terms <p> Limited set representations such as Regular Section Descriptors [14] and Data Access Descriptors [4], as well as previous implementations of systems of linear inequalites supported by Fourier-Motzkin Elimination <ref> [2, 5, 1] </ref>, do not support general non-convex sets set (and consequently, do not support general set union, difference, or complement operations). Their major advantage, however, is that they can support affine expressions with symbolic (i.e., unknown constant) coefficients. <p> The previous work on using linear inequalities and Fourier-Motzkin Elimination [26] for code generation share our goal of improving the generality, the level of abstraction, and the quality of code in data-parallel compilers. Three groups (Ancourt et al. <ref> [2] </ref>, the Paradigm compiler group [29] and SUIF [1]) applied these techniques to support code generation for communication and iteration sets described by linear inequalities.
Reference: [3] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with do loops. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: A few research groups have used systems of linear inequalities to provide a more general and flexible approach than patterns for compiling data parallel programs <ref> [3, 2, 5, 1] </ref>. In most cases, these have been 1 used primarily for code generation based on Fourier-Motzkin elimination (FME) [26, 3]. <p> A few research groups have used systems of linear inequalities to provide a more general and flexible approach than patterns for compiling data parallel programs [3, 2, 5, 1]. In most cases, these have been 1 used primarily for code generation based on Fourier-Motzkin elimination (FME) <ref> [26, 3] </ref>. Amarasinghe and Lam [1] also describe how inequalities and FME can support array dataflow analysis and a few specific communication optimizations, but these primarily operate by direct manipulation of inequalities.
Reference: [4] <author> V. Balasundaram. </author> <title> Interactive Parallelization of Numerical Scientific Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Limited set representations such as Regular Section Descriptors [14] and Data Access Descriptors <ref> [4] </ref>, as well as previous implementations of systems of linear inequalites supported by Fourier-Motzkin Elimination [2, 5, 1], do not support general non-convex sets set (and consequently, do not support general set union, difference, or complement operations).
Reference: [5] <author> P. Banerjee, J. Chandy, M. Gupta, E. Hodges, J. Holm, A. Lain, D. Palermo, S. Ramaswamy, and E. Su. </author> <title> The Paradigm compiler for distributed-memory multicomputers. </title> <journal> IEEE Computer, </journal> <volume> 28(10) </volume> <pages> 37-47, </pages> <month> October </month> <year> 1995. </year> <month> 16 </month>
Reference-contexts: A few research groups have used systems of linear inequalities to provide a more general and flexible approach than patterns for compiling data parallel programs <ref> [3, 2, 5, 1] </ref>. In most cases, these have been 1 used primarily for code generation based on Fourier-Motzkin elimination (FME) [26, 3]. <p> Furthermore, their representation could not represent general non-convex sets or a general set union operation, so optimizations using them are limited in scope <ref> [1, 5, 2] </ref>. (For example, their representation would not support our CP model, or optimizations such as coalescing of arbitrary affine references to an array [1].) The Rice dHPF compiler is based on a practical, executable, equational framework that enables data parallel program analyses and optimizations to be expressed in terms <p> Limited set representations such as Regular Section Descriptors [14] and Data Access Descriptors [4], as well as previous implementations of systems of linear inequalites supported by Fourier-Motzkin Elimination <ref> [2, 5, 1] </ref>, do not support general non-convex sets set (and consequently, do not support general set union, difference, or complement operations). Their major advantage, however, is that they can support affine expressions with symbolic (i.e., unknown constant) coefficients.
Reference: [6] <author> S. Benkner, B. Chapman, and H. Zima. </author> <title> Vienna Fortran 90. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Program analysis and implementation techniques in current compilers, however, do not appear flexible enough to support a general class of computation partitionings and program optimizations. Most research and commercial data-parallel compilers to date <ref> [23, 6, 22, 28, 17, 10, 11, 13, 7, 12, 31] </ref> (including the Rice Fortran D compiler) perform communication analysis and code generation using pattern matching. While such approaches can provide excellent performance where they apply, they may provide poor performance for patterns they cannot handle. <p> much overlap because our compiler currently uses an MPI primitive with sender-side buffering that prevents overlap of communication and computation at run time. 6 Related Work As explained in the Introduction, most research and commercial data-parallel compilers to date use pattern-based approaches for implementing basic communication and iteration set analysis <ref> [23, 6, 22, 28, 30, 31, 10, 11, 13, 7, 12] </ref>. This is a fundamentally different approach from that taken in this paper, and its strengths and weaknesses have been discussed in the Introduction.
Reference: [7] <author> Z. Bozkus, L. Meadows, S. Nakamoto, V. Schuster, and M. Young. </author> <title> Compiling high performance fortran. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 704-709, </pages> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Program analysis and implementation techniques in current compilers, however, do not appear flexible enough to support a general class of computation partitionings and program optimizations. Most research and commercial data-parallel compilers to date <ref> [23, 6, 22, 28, 17, 10, 11, 13, 7, 12, 31] </ref> (including the Rice Fortran D compiler) perform communication analysis and code generation using pattern matching. While such approaches can provide excellent performance where they apply, they may provide poor performance for patterns they cannot handle. <p> much overlap because our compiler currently uses an MPI primitive with sender-side buffering that prevents overlap of communication and computation at run time. 6 Related Work As explained in the Introduction, most research and commercial data-parallel compilers to date use pattern-based approaches for implementing basic communication and iteration set analysis <ref> [23, 6, 22, 28, 30, 31, 10, 11, 13, 7, 12] </ref>. This is a fundamentally different approach from that taken in this paper, and its strengths and weaknesses have been discussed in the Introduction.
Reference: [8] <author> S. Chatterjee, J. Gilbert, R. Schreiber, and S. Teng. </author> <title> Optimal evaluation of array expressions on massively parallel machines. </title> <type> Technical Report CSL-92-11, </type> <institution> Xerox Corporation, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: This is a fundamentally different approach from that taken in this paper, and its strengths and weaknesses have been discussed in the Introduction. There is also a large body of work on techniques to enumerate communication sets and iteration sets in the presence of cyclic (k) distributions (e.g., <ref> [12, 8, 20] </ref>). These techniques likely provide more efficient support for cyclic (k) distributions but would be much less efficient for simpler distributions, and are much less general in the forms of references and computation partitionings they could handle.
Reference: [9] <author> M. Gerndt. </author> <title> Automatic Parallelization for Distributed-Memory Multiprocessing Systems. </title> <type> PhD thesis, </type> <institution> University of Bonn, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Nevertheless, the scalar efficiency of the gener-ated code is quite good (partly shown by the speedup of 0.94 on 1 processor). There were no opportunities for in-place communication with the (block, *) distribution, but nearly all data received was unpacked into overlap areas <ref> [9] </ref> which reduces guard overhead for using the data. The compiler also recognized and implemented two reductions. We considered these results to be acceptable before any compiler tuning, for a cluster system with relatively high communication overhead (e.g., a four-byte node-to-node message costs about 119 s).
Reference: [10] <author> M. Gupta and P. Banerjee. </author> <title> A methodology for high-level synthesis of communication for multicomputers. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Program analysis and implementation techniques in current compilers, however, do not appear flexible enough to support a general class of computation partitionings and program optimizations. Most research and commercial data-parallel compilers to date <ref> [23, 6, 22, 28, 17, 10, 11, 13, 7, 12, 31] </ref> (including the Rice Fortran D compiler) perform communication analysis and code generation using pattern matching. While such approaches can provide excellent performance where they apply, they may provide poor performance for patterns they cannot handle. <p> much overlap because our compiler currently uses an MPI primitive with sender-side buffering that prevents overlap of communication and computation at run time. 6 Related Work As explained in the Introduction, most research and commercial data-parallel compilers to date use pattern-based approaches for implementing basic communication and iteration set analysis <ref> [23, 6, 22, 28, 30, 31, 10, 11, 13, 7, 12] </ref>. This is a fundamentally different approach from that taken in this paper, and its strengths and weaknesses have been discussed in the Introduction.
Reference: [11] <author> M. Gupta, S. Midkiff, E. Schonberg, V. Seshadri, D. Shields, K. Wang, W. Ching, and T. Ngo. </author> <title> An HPF compiler for the IBM SP2. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Program analysis and implementation techniques in current compilers, however, do not appear flexible enough to support a general class of computation partitionings and program optimizations. Most research and commercial data-parallel compilers to date <ref> [23, 6, 22, 28, 17, 10, 11, 13, 7, 12, 31] </ref> (including the Rice Fortran D compiler) perform communication analysis and code generation using pattern matching. While such approaches can provide excellent performance where they apply, they may provide poor performance for patterns they cannot handle. <p> much overlap because our compiler currently uses an MPI primitive with sender-side buffering that prevents overlap of communication and computation at run time. 6 Related Work As explained in the Introduction, most research and commercial data-parallel compilers to date use pattern-based approaches for implementing basic communication and iteration set analysis <ref> [23, 6, 22, 28, 30, 31, 10, 11, 13, 7, 12] </ref>. This is a fundamentally different approach from that taken in this paper, and its strengths and weaknesses have been discussed in the Introduction.
Reference: [12] <author> S. K. S. Gupta, S. D. Kaushik, C.-H. Huang, and P. Sadayappan. </author> <title> Compiling array expressions for efficient execution on distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 32(2) </volume> <pages> 155-172, </pages> <month> 1 February </month> <year> 1996. </year>
Reference-contexts: Program analysis and implementation techniques in current compilers, however, do not appear flexible enough to support a general class of computation partitionings and program optimizations. Most research and commercial data-parallel compilers to date <ref> [23, 6, 22, 28, 17, 10, 11, 13, 7, 12, 31] </ref> (including the Rice Fortran D compiler) perform communication analysis and code generation using pattern matching. While such approaches can provide excellent performance where they apply, they may provide poor performance for patterns they cannot handle. <p> We also add an additional optimization step to reduce runtime overhead in the resulting code. This optimization was also used by Gupta et al. <ref> [12] </ref> but was based on detailed analysis of specific patterns, whereas we use a general integer-set-based algorithm, described below. <p> much overlap because our compiler currently uses an MPI primitive with sender-side buffering that prevents overlap of communication and computation at run time. 6 Related Work As explained in the Introduction, most research and commercial data-parallel compilers to date use pattern-based approaches for implementing basic communication and iteration set analysis <ref> [23, 6, 22, 28, 30, 31, 10, 11, 13, 7, 12] </ref>. This is a fundamentally different approach from that taken in this paper, and its strengths and weaknesses have been discussed in the Introduction. <p> This is a fundamentally different approach from that taken in this paper, and its strengths and weaknesses have been discussed in the Introduction. There is also a large body of work on techniques to enumerate communication sets and iteration sets in the presence of cyclic (k) distributions (e.g., <ref> [12, 8, 20] </ref>). These techniques likely provide more efficient support for cyclic (k) distributions but would be much less efficient for simpler distributions, and are much less general in the forms of references and computation partitionings they could handle.
Reference: [13] <author> J. Harris, J. Bircsak, M. R. Bolduc, J. A. Diewald, I. Gale, N. Johnson, S. Lee, C. A. Nelson, and C. Offner. </author> <title> Compiling High Performance Fortran for distributed-memory systems. </title> <journal> Digital Technical Journal of Digital Equipment Corp., </journal> <volume> 7(3) </volume> <pages> 5-23, </pages> <month> Fall </month> <year> 1995. </year>
Reference-contexts: Program analysis and implementation techniques in current compilers, however, do not appear flexible enough to support a general class of computation partitionings and program optimizations. Most research and commercial data-parallel compilers to date <ref> [23, 6, 22, 28, 17, 10, 11, 13, 7, 12, 31] </ref> (including the Rice Fortran D compiler) perform communication analysis and code generation using pattern matching. While such approaches can provide excellent performance where they apply, they may provide poor performance for patterns they cannot handle. <p> much overlap because our compiler currently uses an MPI primitive with sender-side buffering that prevents overlap of communication and computation at run time. 6 Related Work As explained in the Introduction, most research and commercial data-parallel compilers to date use pattern-based approaches for implementing basic communication and iteration set analysis <ref> [23, 6, 22, 28, 30, 31, 10, 11, 13, 7, 12] </ref>. This is a fundamentally different approach from that taken in this paper, and its strengths and weaknesses have been discussed in the Introduction.
Reference: [14] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Limited set representations such as Regular Section Descriptors <ref> [14] </ref> and Data Access Descriptors [4], as well as previous implementations of systems of linear inequalites supported by Fourier-Motzkin Elimination [2, 5, 1], do not support general non-convex sets set (and consequently, do not support general set union, difference, or complement operations).
Reference: [15] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: In the presence of a symbolic number of processors or symbolic k, we use a virtual processor (VP) model that naturally matches the semantics of templates in HPF <ref> [15] </ref>. We also add an additional optimization step to reduce runtime overhead in the resulting code. This optimization was also used by Gupta et al. [12] but was based on detailed analysis of specific patterns, whereas we use a general integer-set-based algorithm, described below.
Reference: [16] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: Early work on compiling data parallel programs describes basic compilation steps in terms of set operations <ref> [16, 22] </ref>; however, this was viewed only as a pedagogical abstraction and the corresponding compilers (Kali and Fortran D) were implemented using pattern matching techniques.
Reference: [17] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Preliminary experiences with the Fortran D compiler. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Program analysis and implementation techniques in current compilers, however, do not appear flexible enough to support a general class of computation partitionings and program optimizations. Most research and commercial data-parallel compilers to date <ref> [23, 6, 22, 28, 17, 10, 11, 13, 7, 12, 31] </ref> (including the Rice Fortran D compiler) perform communication analysis and code generation using pattern matching. While such approaches can provide excellent performance where they apply, they may provide poor performance for patterns they cannot handle.
Reference: [18] <author> W. Kelly, W. Pugh, and E. Rosser. </author> <title> Code generation for multiple mappings. </title> <booktitle> In Frontiers '95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Omega also supports generation of efficient code for multiple iteration spaces (such as loops containing multiple statements with different CPs) <ref> [18] </ref>. While the underlying algorithms have poor worst-case performance, such cases appear very unlikely to occur in practice [24]. 5 From our perspective, a more significant limitation of the Omega library is that it does not permit unknown coefficients in affine constraints. <p> To partition a single loop or loop-nest with regular computation partitionings that can be represented explicitly as integer mappings, we Omega library's algorithm for code generation with multiple iteration spaces <ref> [18] </ref>. The function reduces loop bounds and also lifts guards out of inner loops when multiple statements with non-overlapping iteration spaces exist. An important optimization we apply is to provide available information about enclosing scopes so as to simplify the resulting code.
Reference: [19] <author> Wayne Kelly, Vadim Maslov, William Pugh, Evan Rosser, Tatiana Shpeisman, and David Wonnacott. </author> <title> The Omega Library Interface Guide. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, Univ. of Maryland, College Park, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: In contrast, the Omega library from the University of Maryland (originally developed to support the Omega test for dependence analysis) provides a powerful implementation of a general class of integer set operations, including set union, using a generalization of Fourier elimination techniques <ref> [24, 19] </ref>. Omega also supports generation of efficient code for multiple iteration spaces (such as loops containing multiple statements with different CPs) [18].
Reference: [20] <author> K. Kennedy, N. Nedeljkovic, and A. Sethi. </author> <title> A linear-time algorithm for computing the memory access sequence in data-parallel programs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Loops with unknown strides are not supported by our framework, and would have to fall back on more expensive run-time techniques such as a finite-state-machine approach for computing communication and iteration sets (for example, <ref> [20] </ref>), or an inspector-executor approach. In the presence of a symbolic number of processors or symbolic k, we use a virtual processor (VP) model that naturally matches the semantics of templates in HPF [15]. We also add an additional optimization step to reduce runtime overhead in the resulting code. <p> This is a fundamentally different approach from that taken in this paper, and its strengths and weaknesses have been discussed in the Introduction. There is also a large body of work on techniques to enumerate communication sets and iteration sets in the presence of cyclic (k) distributions (e.g., <ref> [12, 8, 20] </ref>). These techniques likely provide more efficient support for cyclic (k) distributions but would be much less efficient for simpler distributions, and are much less general in the forms of references and computation partitionings they could handle.
Reference: [21] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: HPF, namely alignment of the array with a template and distribution of the template on a physical processor array (the template and processor array are each represented by a separate tuple space) <ref> [21] </ref>.
Reference: [22] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Program analysis and implementation techniques in current compilers, however, do not appear flexible enough to support a general class of computation partitionings and program optimizations. Most research and commercial data-parallel compilers to date <ref> [23, 6, 22, 28, 17, 10, 11, 13, 7, 12, 31] </ref> (including the Rice Fortran D compiler) perform communication analysis and code generation using pattern matching. While such approaches can provide excellent performance where they apply, they may provide poor performance for patterns they cannot handle. <p> Early work on compiling data parallel programs describes basic compilation steps in terms of set operations <ref> [16, 22] </ref>; however, this was viewed only as a pedagogical abstraction and the corresponding compilers (Kali and Fortran D) were implemented using pattern matching techniques. <p> This approach, based on explicit integer sets, enables us to exploit in-place communication for arbitrary communication sets, independent of data layouts and communication patterns. 2.5 Implementing Loop-Splitting For Reducing Communication Overhead Previous researchers <ref> [22, 30] </ref> have suggested iteration reordering techniques to ameliorate two types of communication overhead: the cost of referencing buffered non-local data, and the latency of communication. Both techniques involve splitting a loop to separate the iterations that access only local data from those that may access non-local data. <p> The latency of communication can be (partly) hidden by splitting because communication required for non-local iterations can be overlapped with local computation. The only implementation of this approach we know of is in Kali <ref> [22] </ref>, where the authors used set equations to explain the optimization but used pattern-based analysis to derive the iteration sets for a few special cases. <p> This approach is only practical for a small number of special cases, and may be even more limited when statements in a loop have different CPs. We extend the equations in <ref> [22] </ref> to apply to arbitrary sets of references, and any CP in our CP model, using the sets and mappings described in previous sections. <p> much overlap because our compiler currently uses an MPI primitive with sender-side buffering that prevents overlap of communication and computation at run time. 6 Related Work As explained in the Introduction, most research and commercial data-parallel compilers to date use pattern-based approaches for implementing basic communication and iteration set analysis <ref> [23, 6, 22, 28, 30, 31, 10, 11, 13, 7, 12] </ref>. This is a fundamentally different approach from that taken in this paper, and its strengths and weaknesses have been discussed in the Introduction.
Reference: [23] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Program analysis and implementation techniques in current compilers, however, do not appear flexible enough to support a general class of computation partitionings and program optimizations. Most research and commercial data-parallel compilers to date <ref> [23, 6, 22, 28, 17, 10, 11, 13, 7, 12, 31] </ref> (including the Rice Fortran D compiler) perform communication analysis and code generation using pattern matching. While such approaches can provide excellent performance where they apply, they may provide poor performance for patterns they cannot handle. <p> much overlap because our compiler currently uses an MPI primitive with sender-side buffering that prevents overlap of communication and computation at run time. 6 Related Work As explained in the Introduction, most research and commercial data-parallel compilers to date use pattern-based approaches for implementing basic communication and iteration set analysis <ref> [23, 6, 22, 28, 30, 31, 10, 11, 13, 7, 12] </ref>. This is a fundamentally different approach from that taken in this paper, and its strengths and weaknesses have been discussed in the Introduction.
Reference: [24] <author> W. Pugh. </author> <title> A practical algorithm for exact array dependence analysis. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: This requires an integer set package that supports all the key set operations including intersection, union, difference, domain, range, composition, and projection. (Appendix A defines some of these operations). Pugh et al. at the University of Maryland have recently developed algorithms for integer set problems represented by Presburger formulas <ref> [24] </ref>. They use advanced Fourier elimination techniques to provide two key capabilities: a general class of integer set operations including set union and non-convex sets, and an algorithm to generate efficient code for multiple iteration spaces [?]. These algorithms are implemented in an integer set package, the Omega library. <p> In contrast, the Omega library from the University of Maryland (originally developed to support the Omega test for dependence analysis) provides a powerful implementation of a general class of integer set operations, including set union, using a generalization of Fourier elimination techniques <ref> [24, 19] </ref>. Omega also supports generation of efficient code for multiple iteration spaces (such as loops containing multiple statements with different CPs) [18]. <p> Omega also supports generation of efficient code for multiple iteration spaces (such as loops containing multiple statements with different CPs) [18]. While the underlying algorithms have poor worst-case performance, such cases appear very unlikely to occur in practice <ref> [24] </ref>. 5 From our perspective, a more significant limitation of the Omega library is that it does not permit unknown coefficients in affine constraints.
Reference: [25] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN 17 '89 Conference on Programming Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: To achieve these levels of performance, we believe two requirements must be met. First, the compiler should have the maximum possible flexibility in its computation partitioning strategies, and not restrict the partitioning to follow the widely-used owner-computes rule <ref> [25] </ref> or restrict all statements in a loop to have the same partitioning [1]. The dHPF compiler incorporates a flexible computation partitioning (CP) model, in which each statement (including control flow) has one or more computational "homes" that specify where instances of the statement will execute.
Reference: [26] <author> A. Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> John Wiley and Sons, </publisher> <address> Chichester, Great Britain, </address> <year> 1986. </year>
Reference-contexts: A few research groups have used systems of linear inequalities to provide a more general and flexible approach than patterns for compiling data parallel programs [3, 2, 5, 1]. In most cases, these have been 1 used primarily for code generation based on Fourier-Motzkin elimination (FME) <ref> [26, 3] </ref>. Amarasinghe and Lam [1] also describe how inequalities and FME can support array dataflow analysis and a few specific communication optimizations, but these primarily operate by direct manipulation of inequalities. <p> These techniques likely provide more efficient support for cyclic (k) distributions but would be much less efficient for simpler distributions, and are much less general in the forms of references and computation partitionings they could handle. The previous work on using linear inequalities and Fourier-Motzkin Elimination <ref> [26] </ref> for code generation share our goal of improving the generality, the level of abstraction, and the quality of code in data-parallel compilers.
Reference: [27] <author> Marc Snir, Steve W. Otto, Steven Huss-Lederman, David W. Walker, , and Jack Dongarra. </author> <title> MPI: The Complete Reference. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: The exceptions are code-generation support (to generate two-version code) for finding in-place communication at runtime, and code-generation support for the virtual processor model. The dHPF compiler currently generates Fortran 77 or Fortran 90 SPMD programs using MPI <ref> [27] </ref> for communication. 13 (a) Speedups for Jacobi (b) Speedups for Tomcatv The goal of the work presented so far is primarily to demonstrate that it is practical and extremely powerful to use an integer set framework for implementing key analyses and optimizations in an HPF compiler.
Reference: [28] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating communication for array statements: Design, implementation, and evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 150-159, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Program analysis and implementation techniques in current compilers, however, do not appear flexible enough to support a general class of computation partitionings and program optimizations. Most research and commercial data-parallel compilers to date <ref> [23, 6, 22, 28, 17, 10, 11, 13, 7, 12, 31] </ref> (including the Rice Fortran D compiler) perform communication analysis and code generation using pattern matching. While such approaches can provide excellent performance where they apply, they may provide poor performance for patterns they cannot handle. <p> much overlap because our compiler currently uses an MPI primitive with sender-side buffering that prevents overlap of communication and computation at run time. 6 Related Work As explained in the Introduction, most research and commercial data-parallel compilers to date use pattern-based approaches for implementing basic communication and iteration set analysis <ref> [23, 6, 22, 28, 30, 31, 10, 11, 13, 7, 12] </ref>. This is a fundamentally different approach from that taken in this paper, and its strengths and weaknesses have been discussed in the Introduction.
Reference: [29] <author> E. Su, A. Lain, S. Ramaswamy, D. J. Palermo, E. W. Hodges IV, and P. Banerjee. </author> <title> Advanced compilation techniques in the PARADIGM compiler for distributed-memory multicomputers. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The previous work on using linear inequalities and Fourier-Motzkin Elimination [26] for code generation share our goal of improving the generality, the level of abstraction, and the quality of code in data-parallel compilers. Three groups (Ancourt et al. [2], the Paradigm compiler group <ref> [29] </ref> and SUIF [1]) applied these techniques to support code generation for communication and iteration sets described by linear inequalities.
Reference: [30] <author> C.-W. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: This approach, based on explicit integer sets, enables us to exploit in-place communication for arbitrary communication sets, independent of data layouts and communication patterns. 2.5 Implementing Loop-Splitting For Reducing Communication Overhead Previous researchers <ref> [22, 30] </ref> have suggested iteration reordering techniques to ameliorate two types of communication overhead: the cost of referencing buffered non-local data, and the latency of communication. Both techniques involve splitting a loop to separate the iterations that access only local data from those that may access non-local data. <p> The final, simplified code for the Erlebacher example loop is shown in Figure 6 (c). It is interesting to note in the Erlebacher example that the final placement of the communication code is exactly what the optimization known as vector message-pipelining aims to achieve <ref> [30] </ref>. This optimization moves pipelined shift communication occuring in boundary iterations of a partitioned loop out of the loop in order to eliminate runtime checks on the communication statements. <p> much overlap because our compiler currently uses an MPI primitive with sender-side buffering that prevents overlap of communication and computation at run time. 6 Related Work As explained in the Introduction, most research and commercial data-parallel compilers to date use pattern-based approaches for implementing basic communication and iteration set analysis <ref> [23, 6, 22, 28, 30, 31, 10, 11, 13, 7, 12] </ref>. This is a fundamentally different approach from that taken in this paper, and its strengths and weaknesses have been discussed in the Introduction.
Reference: [31] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: Program analysis and implementation techniques in current compilers, however, do not appear flexible enough to support a general class of computation partitionings and program optimizations. Most research and commercial data-parallel compilers to date <ref> [23, 6, 22, 28, 17, 10, 11, 13, 7, 12, 31] </ref> (including the Rice Fortran D compiler) perform communication analysis and code generation using pattern matching. While such approaches can provide excellent performance where they apply, they may provide poor performance for patterns they cannot handle. <p> much overlap because our compiler currently uses an MPI primitive with sender-side buffering that prevents overlap of communication and computation at run time. 6 Related Work As explained in the Introduction, most research and commercial data-parallel compilers to date use pattern-based approaches for implementing basic communication and iteration set analysis <ref> [23, 6, 22, 28, 30, 31, 10, 11, 13, 7, 12] </ref>. This is a fundamentally different approach from that taken in this paper, and its strengths and weaknesses have been discussed in the Introduction.
References-found: 31

