URL: http://www.cs.utexas.edu/users/ajohn/current.ps
Refering-URL: http://www.cs.utexas.edu/users/code/code-publications/
Root-URL: 
Email: fajohn,browneg@cs.utexas.edu  
Title: A Parallel Programming Language based on Constraints  
Author: Ajita John, J. C. Browne 
Address: Austin, TX 78712  
Affiliation: Dept. of Computer Sciences University of Texas,  
Abstract: This paper describes the first results from research on the translation of constraint systems into task level parallel programs in a compilable language. This is the only research, of which we are aware, which attempts to generate efficient parallel programs for numerical computation from constraint systems. Algorithms are expressed as constraint systems. A dependence graph is derived from the constraint system and a set of input variables. The dependence graph, which exploits the parallelism in the constraints, is mapped to the target language CODE, which represents parallel computation structures as generalized dependence graphs. Finally, parallel C programs are generated. The granularity of the derived dependence graphs depends upon the complexity of the operations represented in the constraint system. To extract parallel programs of appropriate granularity, the following features have been included: (i) modularity, (ii) operations over structured types as primitives, (iii) definition of sequential C functions. A prototype of the compiler has been implemented. The execution environment or software architecture is specified separately from the constraint system. The domain of matrix computations has been targeted for applications. Some examples have been programmed. Initial results are very encouraging. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Doug Baldwin. </author> <title> A Status Report on CONSUL. Languages and Compilers for Parallel Computing, </title> <editor> Gelernter, Nicolau and Padua (eds), </editor> <publisher> MIT Press in 1990. </publisher>
Reference-contexts: Both these features are supported through the parameterization of abstract types, modularity and use of C functions in arithmetic expressions. 4 Related Work The major pieces of work related to our research are described in this section. Consul <ref> [1, 2] </ref> : The goal of this work resembles ours in that it is to extract parallelism from constraints. But the approach is different in that local propagation is used to find satisfying values for the system of constraints. <p> This condition will be relaxed in later versions of the compiler. E.g. AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g captures the constraints A <ref> [1] </ref> == A [0] AND A [2] == A [1] AND B [2] == A [1] AND B [3] == A [2] Constraints constructed from applications of Rule 1 are referred to as simple constraints, which form the building blocks for non-simple constraints, constructed from applications of Rules 2-4. <p> This condition will be relaxed in later versions of the compiler. E.g. AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g captures the constraints A <ref> [1] </ref> == A [0] AND A [2] == A [1] AND B [2] == A [1] AND B [3] == A [2] Constraints constructed from applications of Rule 1 are referred to as simple constraints, which form the building blocks for non-simple constraints, constructed from applications of Rules 2-4. <p> This condition will be relaxed in later versions of the compiler. E.g. AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g captures the constraints A <ref> [1] </ref> == A [0] AND A [2] == A [1] AND B [2] == A [1] AND B [3] == A [2] Constraints constructed from applications of Rule 1 are referred to as simple constraints, which form the building blocks for non-simple constraints, constructed from applications of Rules 2-4. <p> An example of a construct that will not be compiled successfully is AND FOR (i 1 5) f A <ref> [1] </ref> == A [i] + B [i] g with A unknown and B known. This is because in the first iteration both the terms A [1] and A [i] are unknown whereas subsequent iterations have only A [i] as an unknown. <p> An example of a construct that will not be compiled successfully is AND FOR (i 1 5) f A <ref> [1] </ref> == A [i] + B [i] g with A unknown and B known. This is because in the first iteration both the terms A [1] and A [i] are unknown whereas subsequent iterations have only A [i] as an unknown. To extract parallelism from constraints classified as computation, the patterns of access within a constraint are studied. Throughout this discussion, the case of array accesses will be detailed.
Reference: [2] <author> Doug Baldwin. </author> <title> Consul: A Parallel Constraint Language. </title> <booktitle> IEEE Software 1989. </booktitle>
Reference-contexts: a set of constraints upon the state variables defining the solution and choosing an appropriate subset of the state variables as the input set is an attractive approach to specification of programs, but there has been little success previously in attaining efficient execution of parallel programs derived from constraint representations <ref> [2] </ref>. There are however, both motivations for continuing research in this direction and reasons for optimism concerning success. Constraint systems have attractive properties for compilation to parallel computation structures. <p> The execution environment specification can be used by the compiler to choose execution environment specific synchronization/communication mechanisms. By translating to a compilable language, we avoid the slow interpretive execution environments plaguing typical parallel/concurrent constraint languages <ref> [16, 2] </ref>. By narrowing our semantic domain, the system has the added flexibility of choosing optimal algorithms for executing operations. (iv) Scalability: It is crucial to be able to scale to larger applications and problem sizes. <p> Both these features are supported through the parameterization of abstract types, modularity and use of C functions in arithmetic expressions. 4 Related Work The major pieces of work related to our research are described in this section. Consul <ref> [1, 2] </ref> : The goal of this work resembles ours in that it is to extract parallelism from constraints. But the approach is different in that local propagation is used to find satisfying values for the system of constraints. <p> This condition will be relaxed in later versions of the compiler. E.g. AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g captures the constraints A [1] == A [0] AND A <ref> [2] </ref> == A [1] AND B [2] == A [1] AND B [3] == A [2] Constraints constructed from applications of Rule 1 are referred to as simple constraints, which form the building blocks for non-simple constraints, constructed from applications of Rules 2-4. <p> This condition will be relaxed in later versions of the compiler. E.g. AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g captures the constraints A [1] == A [0] AND A <ref> [2] </ref> == A [1] AND B [2] == A [1] AND B [3] == A [2] Constraints constructed from applications of Rule 1 are referred to as simple constraints, which form the building blocks for non-simple constraints, constructed from applications of Rules 2-4. <p> E.g. AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g captures the constraints A [1] == A [0] AND A <ref> [2] </ref> == A [1] AND B [2] == A [1] AND B [3] == A [2] Constraints constructed from applications of Rule 1 are referred to as simple constraints, which form the building blocks for non-simple constraints, constructed from applications of Rules 2-4.
Reference: [3] <author> Utpal Banerjee. </author> <title> Loop Parallelization, A Book Series on Loop Transformations for Restructuring Compilers, </title> <publisher> Kluwer, </publisher> <year> 1994. </year>
Reference-contexts: Other related work includes MATLAB and Equational systems like Unity. Technical computing environments like MATLAB [15] integrate numerical analysis and matrix computation. Unity [6] is a 4 programming notation and a logic to reason about parallel and distributed programs. We, of course, use the methods of parallel compilation <ref> [3] </ref> to derive procedural programs from constraint systems. The approach of Pandey and Browne [18, 19] expresses parallel computation as constraints on the order of execution of units of computations. We derive this ordering from the constraints and an input set of variables. <p> E.g. AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g captures the constraints A [1] == A [0] AND A [2] == A [1] AND B [2] == A [1] AND B <ref> [3] </ref> == A [2] Constraints constructed from applications of Rule 1 are referred to as simple constraints, which form the building blocks for non-simple constraints, constructed from applications of Rules 2-4.
Reference: [4] <author> B. Freeman-Benson and Alan Borning. </author> <title> The design and implementation of Kaleidoscope '90, a constraint imperative programming language. </title> <booktitle> Proc. of the IEEE Computer Society Intl. Conf. on Computer Languages, </booktitle> <pages> pages 174-180, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: This is a data-parallel programming language designed for portability and an implicitly parallel programming style with some optimization directives. In these respects, it is similar to our system. But, the difference lies in the program specification styles: procedural versus declarative. Thinglab [5] and Kaleidoscope <ref> [4] </ref> : These two systems are similar in that they transform constraints to a compilable language rather than to an interpretive execution environment. Thinglab is a constraint-oriented graphic simulation laboratory. Constraints are compiled to sequential procedural code. Kaleidoscope integrates object-oriented programming with constraints.
Reference: [5] <author> Bjorn N. Freeman-Benson. </author> <title> A Module Compiler for Thinglab II. </title> <booktitle> Proc. 1989 ACM Conf. on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <address> New Orleans, </address> <month> October </month> <year> 1989. </year> <note> ACM. </note>
Reference-contexts: This is a data-parallel programming language designed for portability and an implicitly parallel programming style with some optimization directives. In these respects, it is similar to our system. But, the difference lies in the program specification styles: procedural versus declarative. Thinglab <ref> [5] </ref> and Kaleidoscope [4] : These two systems are similar in that they transform constraints to a compilable language rather than to an interpretive execution environment. Thinglab is a constraint-oriented graphic simulation laboratory. Constraints are compiled to sequential procedural code. Kaleidoscope integrates object-oriented programming with constraints.
Reference: [6] <author> K.M. Chandy and J. Misra. </author> <title> Parallel Program Design : A Foundation, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, </address> <year> 1989. </year>
Reference-contexts: There are however, both motivations for continuing research in this direction and reasons for optimism concerning success. Constraint systems have attractive properties for compilation to parallel computation structures. A constraint system gives the minimum specification (See <ref> [6] </ref> for the benefits from postponing imposition of program structure) for a computation, thereby offering the compiler freedom of choice for derivation of control structure. Constraint systems offer some unique advantages as a representation from which parallel programs are to be derived. <p> The performance reported for the system is only comparable with commercial Prolog and Lisp systems [16]. Other related work includes MATLAB and Equational systems like Unity. Technical computing environments like MATLAB [15] integrate numerical analysis and matrix computation. Unity <ref> [6] </ref> is a 4 programming notation and a logic to reason about parallel and distributed programs. We, of course, use the methods of parallel compilation [3] to derive procedural programs from constraint systems.
Reference: [7] <author> Collins, T.S. and Browne J.C. </author> <title> MaTrix++; An Object-Oriented Environment for Parallel High-Perfomance Matrix Computations. </title> <booktitle> Proc. of the 1995 Hawaii Intl. Conf. on Systems and Software. </booktitle>
Reference-contexts: We have a built-in matrix type with its associated operations of addition, subtraction, multiplication and inverse. The matrix subtypes currently implemented in our system are lower and upper triangular and generalized matrices. We plan to extend the type system to a richer class of matrices including hierarchical matrices <ref> [7] </ref>. Specialized algorithms based on the structure of the matrix can be invoked for the matrix subtypes. <p> The approach of Pandey and Browne [18, 19] expresses parallel computation as constraints on the order of execution of units of computations. We derive this ordering from the constraints and an input set of variables. Collins <ref> [7] </ref> expresses matrix computations as hierarchical type declarations and translates to implementations which maximizes use of type information. 5 Language Description This section describes the different components of the programming system. <p> There are several promising approaches: object-oriented formulations of data structures are one possibility. A simpler and more algorithmic basis for definition of constraints over partitions of matrices is to utilize a simple version of the hierarchical type theory for matrices by Collins and Browne <ref> [7] </ref>. The hierarchical type model for 15 matrices establishes a compilable semantics for computations over hierarchical matrices. Additionally, the next steps in this research are: a) Design and implement a "block" structure in which destructive update to variables is allowed.
Reference: [8] <author> J.J. Dongarra and D.C. Sorenson. </author> <title> SCHEDULE: Tools for Developing and Analyzing Parallel Fortran Programs. </title> <institution> Argonne National Laboratory, </institution> <note> MCSD Technical Memorandum No. 86, </note> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: The next two subsections describe two examples programmed in our system. 7.1 Block Triangular Solver (BTS) The example chosen is the solution of the AX = B linear algebra problem for a known lower triangular matrix A and vector B. The parallel algorithm <ref> [8] </ref> involves dividing the matrix into blocks as shown in 12 triangular sub-matrices that are solved sequentially, and M 10 ; M 20 ; : : : M 32 represent sub-matrices that must be multiplied by the vector from above and the result subtracted from the vector from the left.
Reference: [9] <author> R. Eigenmann and W. Blume. </author> <title> An Effectiveness Study of Parallelizing Compiler Techniques. </title> <booktitle> Proc. Intl. Conf. Par. Proc., </booktitle> <year> 1991, </year> <pages> pp. II 17-25. </pages>
Reference-contexts: This approach has little hope of extracting efficient programs and offers performance only in the range of logic languages. Extension of procedural languages with directives for parallelism: Attempt to parallelize Fortran with additional specifications have not been broadly successful <ref> [9] </ref>. Declarative extensions have been added as part of High-Performance Fortran (HPF) [20]. This is a data-parallel programming language designed for portability and an implicitly parallel programming style with some optimization directives. In these respects, it is similar to our system.
Reference: [10] <author> Ian Foster. </author> <title> Designing and Building Parallel Programs. </title> <note> ISBN 0-201-57594-9,Addison-Wesley. </note>
Reference-contexts: Finally, sequential C programs and parallel C programs for shared memory machines like the CRAY J90, SparcCenter 2000, and Sequent and the distributed memory PVM [11] system can be generated. An MPI <ref> [10] </ref> backend for CODE is under development. The granularity of the derived dependence graphs depends upon the types directly represented as primitives in the constraint representation.
Reference: [11] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek and Vaidy Sun-deram. </author> <title> PVM: Parallel Virtual Machine:A Users' Guide and Tutorial for Networked Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The software architecture or execution environment to which CODE is to compile is separately specified (SMP, DSM, NOW, etc). Finally, sequential C programs and parallel C programs for shared memory machines like the CRAY J90, SparcCenter 2000, and Sequent and the distributed memory PVM <ref> [11] </ref> system can be generated. An MPI [10] backend for CODE is under development. The granularity of the derived dependence graphs depends upon the types directly represented as primitives in the constraint representation.
Reference: [12] <author> Ajita John, J. C. Browne. </author> <title> A Constraint-based Parallel Programming Language. </title> <type> Technical Report TR95-42, </type> <institution> Department of Computer Sciences, Unversity of Texas at Austin, </institution> <year> 1995, </year> <note> URL: http://www.cs.utexas.edu/users/ajohn/current.ps </note> . 
Reference: [13] <author> S. Lakshmivarahan and Sudarshan K. Dhall. </author> <title> Analysis and Design of Parallel Algorithms: Arithmetic and Matrix Problems. </title> <booktitle> McGraw-Hill Series in Supercomputing and Parallel Processing, </booktitle> <year> 1990. </year>
Reference-contexts: It is further assumed that the blocks B and C are symmetric and commute. A version of the parallel algorithm (taken from <ref> [13] </ref>) programmed in our system is shown in odd-indexed (reduced system) and another for even-indexed (eliminated system) terms. The reduction process is repeatedly applied to the reduced system. After k 1 iterations the reduced system contains the solution for a single term. <p> The reduction process is repeatedly applied to the reduced system. After k 1 iterations the reduced system contains the solution for a single term. The rest of the terms can be obtained by back-substitution. The variable names, BP, CP, dP correspond to the indexed terms B,C,d in <ref> [13] </ref>. The indexed terms for BP , CP , dP refer to computed terms during iterations of the reduction process. BP [0], CP [0] and dP [i][0] are initialized to B, C and dP i respectively. pow is a C function implementing the arithmetic power function. <p> Furthermore, it is capable of detecting the parallelism within the expression 2 fl CP [j 1] fl CP [j 1] BP [j 1] fl BP [j 1] in the computation for BP [j]. The authors in <ref> [13] </ref> have mentioned that the single-solution step is the major bottleneck in the algorithm. But, in our experiments we found the reduction phase resistant to scalability. This is due to the fact that the computation for BP [j] and CP [j] involve matrix-matrix multiplication: an O (n 3 ) operation.
Reference: [14] <author> William Leler. </author> <title> Constraint Programming Languages. </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: The textually expressed constraint system is transformed to an undirected graph representation as for example given by Leler <ref> [14] </ref>. Phase 2. A depth-first search algorithm transforms the undirected graph to a directed graph. Phase 3. With a set of input variables, the directed graph is traversed by a depth-first search to map the constraints to conditionals and computations for nodes of a generalized dependence graph. Phase 4.
Reference: [15] <institution> MATLAB from MathWorks. </institution> <note> URL: http://www.mathworks.com/matlab.html </note>
Reference-contexts: However, only a concurrent implementation and no parallel implementation has been developed. The performance reported for the system is only comparable with commercial Prolog and Lisp systems [16]. Other related work includes MATLAB and Equational systems like Unity. Technical computing environments like MATLAB <ref> [15] </ref> integrate numerical analysis and matrix computation. Unity [6] is a 4 programming notation and a logic to reason about parallel and distributed programs. We, of course, use the methods of parallel compilation [3] to derive procedural programs from constraint systems.
Reference: [16] <author> Michael Mehl, Ralf Scheidhauer, and Christian Schulte. </author> <title> An Abstract Machine for Oz. Programming Languages, Implementations, Logics and Programs, </title> <booktitle> Seventh Intl. Symposium, </booktitle> <publisher> Springer-Verlag, LNCS 982, </publisher> <address> Utrecht, The Netherlands, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: The execution environment specification can be used by the compiler to choose execution environment specific synchronization/communication mechanisms. By translating to a compilable language, we avoid the slow interpretive execution environments plaguing typical parallel/concurrent constraint languages <ref> [16, 2] </ref>. By narrowing our semantic domain, the system has the added flexibility of choosing optimal algorithms for executing operations. (iv) Scalability: It is crucial to be able to scale to larger applications and problem sizes. <p> Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in [21]. However, only a concurrent implementation and no parallel implementation has been developed. The performance reported for the system is only comparable with commercial Prolog and Lisp systems <ref> [16] </ref>. Other related work includes MATLAB and Equational systems like Unity. Technical computing environments like MATLAB [15] integrate numerical analysis and matrix computation. Unity [6] is a 4 programming notation and a logic to reason about parallel and distributed programs.
Reference: [17] <author> P. Newton and J. C. Browne. </author> <title> The Code 2.0 Graphical Parallel Programming Environment. </title> <booktitle> Proc. of the 1992 Intl. Conf. on Supercomputing. </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992, </year> <pages> pp 167-177. </pages>
Reference-contexts: A dependence graph is derived from the constraint specification and the input set of variables. The dependence graph is mapped to the target language CODE <ref> [17] </ref>, which expresses parallel structure over sequential units of computation declaratively as a generalized dependence graph. The software architecture or execution environment to which CODE is to compile is separately specified (SMP, DSM, NOW, etc). <p> Phase 4. Specifications of the execution environment are used to optimally select the communication and synchronization mechanisms to be used by CODE <ref> [17] </ref>. This phase is yet to be completely implemented. Phase 5. <p> But, there are provisions in the system to select certain program variables as shared variables in a shared memory environment. Also, some operations (e.g. matrix multiplication) can be chosen for parallel execution. Our target for executable for constraint programs is the CODE <ref> [17] </ref> parallel programming environment. CODE takes a dependence graph as its input. The form of a node in a CODE dependence graph is given in Figure 6.
Reference: [18] <author> Raju Pandey and J. C. Browne. </author> <title> Event-based Composition of Concurrent Programs. </title> <booktitle> Workshop on Languages and Compilers for Parallel Computation, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Unity [6] is a 4 programming notation and a logic to reason about parallel and distributed programs. We, of course, use the methods of parallel compilation [3] to derive procedural programs from constraint systems. The approach of Pandey and Browne <ref> [18, 19] </ref> expresses parallel computation as constraints on the order of execution of units of computations. We derive this ordering from the constraints and an input set of variables.
Reference: [19] <author> Raju Pandey and J. C. Browne. </author> <title> A Compositional Approach to Concurrent Object-oriented Programming. </title> <booktitle> IEEE International Conference on Computer Languages, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Unity [6] is a 4 programming notation and a logic to reason about parallel and distributed programs. We, of course, use the methods of parallel compilation [3] to derive procedural programs from constraint systems. The approach of Pandey and Browne <ref> [18, 19] </ref> expresses parallel computation as constraints on the order of execution of units of computations. We derive this ordering from the constraints and an input set of variables.
Reference: [20] <author> H. Richardson. </author> <title> HPF: History, Overview and Current developments. </title> <institution> Thinking Machines Corporation, </institution> <note> TMC 261, URL: http://www.crpc.rice.edu/HPFF/publications.html </note>
Reference-contexts: Extension of procedural languages with directives for parallelism: Attempt to parallelize Fortran with additional specifications have not been broadly successful [9]. Declarative extensions have been added as part of High-Performance Fortran (HPF) <ref> [20] </ref>. This is a data-parallel programming language designed for portability and an implicitly parallel programming style with some optimization directives. In these respects, it is similar to our system. But, the difference lies in the program specification styles: procedural versus declarative.
Reference: [21] <author> Vijay A. Saraswat. </author> <title> Concurrent Constraint Programming Languages. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon, Pittsburgh, 1989. School of Computer Science. </institution> <month> 17 </month>
Reference-contexts: Thinglab is a constraint-oriented graphic simulation laboratory. Constraints are compiled to sequential procedural code. Kaleidoscope integrates object-oriented programming with constraints. Neither is concerned with extraction of parallel structures, which is our major concern. Concurrent Constraint Programming: Vijay Saraswat described a family of concurrent constraint logic programming languages,the cc languages <ref> [21] </ref>. The logic and constraint portions are explicitly separated with the constraint part acting as an active data store for the logic half. <p> Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in <ref> [21] </ref>. However, only a concurrent implementation and no parallel implementation has been developed. The performance reported for the system is only comparable with commercial Prolog and Lisp systems [16]. Other related work includes MATLAB and Equational systems like Unity.
References-found: 21

