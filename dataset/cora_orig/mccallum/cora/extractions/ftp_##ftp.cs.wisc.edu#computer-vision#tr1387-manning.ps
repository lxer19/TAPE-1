URL: ftp://ftp.cs.wisc.edu/computer-vision/tr1387-manning.ps
Refering-URL: http://www.cs.wisc.edu/computer-vision/pubs.html
Root-URL: http://www.cs.wisc.edu
Title: Dynamic View Morphing  
Author: Russell A. Manning Charles R. Dyer 
Note: This work is sponsored by the National Science Foundation under Grant No. IIS-9530985 and by the Defense Advanced Research Projects Agency (DARPA) and Rome Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-97-1-0138.  
Date: September 1998  
Address: Madison, Wisconsin 53706  
Affiliation: Department of Computer Sciences University of Wisconsin  
Pubnum: Technical Report #1387  
Abstract: We present a novel technique for interpolating between two views of a dynamic scene. Our approach extends the concept of view morphing introduced in [SD96] and retains the relative advantages of that method. The interpolation will portray one possible physically-valid version of what transpired in the scene during the intervening time between views. The scene is assumed to consist of a small number of objects. Each object can undergo any motion during the time between views as long as its total movement is equivalent to a single, rigid translation. The dynamic view morphing technique can work with widely-spaced reference views, sparse point correspondences, and uncalibrated cameras. When the camera-to-camera transformation can be determined, the virtual objects can be portrayed moving along straight-line, constant-velocity trajectories. Methods are developed for determining the camera-to-camera transformation from information available in the reference views. It is shown that each moving object in a scene has a corresponding fundamental matrix and that the camera-to-camera transformation can be determined from two distinct fundamental matrices. Dynamic view morphing is developed for both pinhole and orthographic cameras, and the use of three or more reference views is discussed. Static view morphing is made more versatile with respect to occlusion, and mosaicing is combined with dynamic view morphing for the case when both reference views share the same optical center. The resulting combination of techniques can be used to fill-in missing gaps in movies, perform "view hand-offs" between cameras at different locations, create movies from still images, perform movie stabilization and compression, track objects during periods of obstruction, and related tasks. 
Abstract-found: 1
Intro-found: 1
Reference: [AS97] <author> Shai Avidan and Amnon Shashua. </author> <title> Novel view synthesis in tensor space. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 1034-1040, </pages> <year> 1997. </year>
Reference-contexts: However, mosaicing techniques can be crucial in creating dynamic view morphs from co-centered reference cameras (see Section 5.1). Another major group of image-based rendering techniques can be termed image-based modeling <ref> [MB95, AS97, SD97, TK92] </ref>. These techniques attempt to reconstruct three-dimensional scene geometry from reference views. Once the geometry is recovered, new views of the scene can be generated from arbitrary positions in space. All of these techniques make use of camera calibration and dense point correspondences in some way. <p> Once the geometry is recovered, new views of the scene can be generated from arbitrary positions in space. All of these techniques make use of camera calibration and dense point correspondences in some way. The techniques of [MB95] and <ref> [AS97] </ref> automatically recover camera calibration and dense point correspondences and work from a small number of input views (e.g., two or three). However, they will fail if the original views are not closely spaced. <p> When the two reference views are sufficiently close, some existing stereo algorithms can produce a reliable set of point correspondences (for instance [ZDFL95]). However, when the views are closely spaced it is also possible to use techniques other than view morphing to produce new virtual views <ref> [MB95, AS97] </ref>. These techniques offer more flexibility in positioning the virtual camera. They also generate dense correspondences, leading to improved image quality. View morphing is most applicable when the views are widely spaced, in which case other techniques cannot be used.
Reference: [BCS97] <author> Christopher Bregler, Michele Covell, and Malcolm Slaney. </author> <title> Video rewrite: Driving visual specch with audio. </title> <booktitle> In Proc. SIGGRAPH 97, </booktitle> <pages> pages 353-360, </pages> <year> 1997. </year>
Reference-contexts: Talisman clearly solves a different problem than dynamic view interpolation; for instance, complete scene geometry is assumed and the output is meant to be approximate rather than physically correct. Bregler <ref> [BCS97] </ref> presents a method for creating views of a novel dynamic scene from existing video footage of a closely related scene. Specifically, the technique starts with footage of a person talking and creates a new sequence showing the person saying new words with corresponding mouth and jaw movements.
Reference: [BN92] <author> Thaddeus Beier and Shawn Neely. </author> <title> Feature-based image metamorphosis. </title> <booktitle> In Proc. </booktitle> <volume> SIG-GRAPH 92, </volume> <pages> pages 35-42, </pages> <year> 1992. </year>
Reference-contexts: The linear interpolation of this small set of conjugate points produces only a small section of the desired virtual view; the remainder of the virtual view is created through standard image morphing techniques (such as <ref> [BN92] </ref>) using the interpolated points to guide the morphing process. The result is a compelling, if approximate, new view of the original scene. Since the linear interpolation of conjugate points is physically valid, increasing the density of the original correspondence leads to increased realism. <p> First, the two reference views were divided into layers corresponding to the moving objects; for example, Fig. 8 shows the layers for sequence (iii). Second, for each corresponding layer a set of conjugate points between the two views was determined. Since our implementation uses the Beier-Neely algorithm <ref> [BN92] </ref> for the morphing step, we actually determined a series of line-segment correspondences instead of point correspondences.
Reference: [Che95] <author> Shenchang Eric Chen. </author> <title> Quicktime VR | An image-based approach to virtual environment navigation. </title> <booktitle> In Proc. SIGGRAPH 95, </booktitle> <pages> pages 29-38, </pages> <year> 1995. </year>
Reference-contexts: As it turns out, some of the techniques are simply not applicable to the problem, while others require more information than we are assuming is available. One major category of image-based rendering techniques is called mosaicing and includes such work as <ref> [Sze94, Che95, Sze96, Int97, RPFRA98, SS97, Dav98] </ref>. As the name implies, these techniques seek to create large views of a scene by combining many small extant views. Most work on mosaicing is limited to static scenes, making the results not directly applicable to dynamic view interpolation.
Reference: [CW93] <author> Shenchang Eric Chen and Lance Williams. </author> <title> View interpolation for image synthesis. </title> <booktitle> In Proc. SIGGRAPH 93, </booktitle> <pages> pages 279-288, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction View interpolation <ref> [CW93] </ref> is the process of using two reference views of a scene to generate a series of virtual views which, taken together, represent a smooth, continuous, and physically accurate transition between the originals. <p> All of the methods cited for the previous category assume that the reference views are of static scenes. If the reference views contain object motion, then these techniques cannot be directly applied. A third general category consists of image-based rendering techniques specifically designed for view interpolation <ref> [CW93, SD96] </ref>. Our work belongs to this group. The methods of this category are all pure image-based rendering techniques, in that three-dimensional scene geometry is not recovered. Instead, new views are created by moving pixels around or otherwise stretching and distorting the original views. <p> As with the image-based modeling techniques but unlike mosaicing, these methods can produce new views of a scene from novel positions in space. View morphing [SD96] can work with unknown camera calibration and very sparse point correspondences. Both <ref> [CW93] </ref> and [SD96] are for static scenes and cannot be directly applied to dynamic view interpolation except in very limited 4 circumstances (see Section 2.2). A few other prominent image-based rendering techniques deserve mention with respect to the dynamic view interpolation problem.
Reference: [Dav98] <author> James Davis. </author> <title> Mosaics of scenes with moving objects. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 354-360, </pages> <year> 1998. </year>
Reference-contexts: As it turns out, some of the techniques are simply not applicable to the problem, while others require more information than we are assuming is available. One major category of image-based rendering techniques is called mosaicing and includes such work as <ref> [Sze94, Che95, Sze96, Int97, RPFRA98, SS97, Dav98] </ref>. As the name implies, these techniques seek to create large views of a scene by combining many small extant views. Most work on mosaicing is limited to static scenes, making the results not directly applicable to dynamic view interpolation. <p> As the name implies, these techniques seek to create large views of a scene by combining many small extant views. Most work on mosaicing is limited to static scenes, making the results not directly applicable to dynamic view interpolation. Davis <ref> [Dav98] </ref> worked with dynamic scenes, but his goal was to remove the motion from the dynamic scene in order to create one single, large, frozen view. Irani, et. al., [IAH95] described mosaicing of video sequences of dynamic scenes. <p> We have experimented with the following simplified camera model, given below for camera A: T UA = 6 1 0 0 0 0 1=f A 7 where f A is the focal length and R A is a rotation matrix. For another example of this model, see <ref> [Dav98] </ref>. 6 To the extent that this simplified camera matrix accurately models the reference cameras, the following theory can be used.
Reference: [FB81] <author> M. A. Fischler and R. C. Bolles. </author> <title> Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. </title> <journal> Communications of the ACM, </journal> <volume> 24(6) </volume> <pages> 381-395, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: If an object is stationary, then its conjugate points are also conjugate directions. Recall that knowledge of four conjugate directions is sufficient to determine T AB . Since many more than four conjugate points are provided, a RANSAC method <ref> [FB81] </ref> could be used to automatically calculate T AB : T AB is repeatedly estimated from random subsets of four conjugate directions (or a similar small number) and each estimate is tested against the remaining conjugate directions.
Reference: [GB95] <author> Michel Gondry and Pierre Buffin. </author> <title> Like a rolling stone. Music Video, </title> <year> 1995. </year>
Reference-contexts: Dynamic view morphing can then interpolate between the still images 31 to create a movie showing the relationship between the original viewpoints. Similar techniques have been used for artistic effect in the mass media <ref> [GB95] </ref>. Tracking from a single fixed camera: If views are captured by a single camera whose position, orientation, and internal parameters all stay fixed, then the camera-to-camera transformation T AB between any pair of views is just I.
Reference: [Gre86] <author> Ned Greene. </author> <title> Environment mapping and other applications of world projections. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 6(11) </volume> <pages> 21-29, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: For stationary layers, a panoramic mosaicing technique can be used to create the view interpolation. First, using T AB to incorporate the view from B with the view from A, an environment map <ref> [Gre86] </ref> is made relative to basis A. Then, during the process of creating a virtual view, the transformation T AV is used to convert the environment map to basis V of the virtual camera.
Reference: [Hec89] <author> Paul S. Heckbert. </author> <title> Fundamentals of texture mapping and image warping. </title> <type> Master's thesis, </type> <institution> University of California, Dept. CS, Berkeley, </institution> <address> CA, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: The postwarp is chosen to map the four interpolated corners to the corners of a rectangle (a projective transformation can be determined by its behavior on four such points <ref> [Hec89] </ref>). The rectangle can be the linear interpolation of the two bounding rectangles of the original views. Step 5 marks the end of the algorithm. We return now to a complication that can arise in step 2. <p> In the context of mosaicing, it is well known that by identifying four corresponding points in a pair of images, a projective mapping between the images can be found <ref> [Hec89, Sze94] </ref>. This projective mapping is just a transformation between the camera bases. Note that there is a precondition for calculating the projective transformation: all three-member subsets of the four conjugate directions must span &lt; 3 .
Reference: [Hor90] <author> B. K. P. Horn. </author> <title> Relative orientation. </title> <journal> Int. J. Computer Vision, </journal> <volume> 4 </volume> <pages> 59-78, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: This problem, similar in nature to the relative orientation problem of <ref> [Hor90] </ref>, can be termed the relative calibration problem since it consists of calibrating camera B in terms of camera A's basis. We call T AB the camera-to-camera transformation. In order to use the linear motion algorithm it is only necessary to find T AB up to a scalar. <p> In <ref> [Hor90] </ref> it is shown that five conjugates points is a sufficient number for finding the relative orientation; we always assume many more correspondences than this are provided anyway.
Reference: [IAH95] <author> Michal Irani, P. Anandan, and Steve Hsu. </author> <title> Mosaic based representations of video sequences and their applications. </title> <booktitle> In Proc. Fifth Int. Conf. on Computer Vision, </booktitle> <pages> pages 605-611, </pages> <year> 1995. </year>
Reference-contexts: Most work on mosaicing is limited to static scenes, making the results not directly applicable to dynamic view interpolation. Davis [Dav98] worked with dynamic scenes, but his goal was to remove the motion from the dynamic scene in order to create one single, large, frozen view. Irani, et. al., <ref> [IAH95] </ref> described mosaicing of video sequences of dynamic scenes. Their technique assumes many reference views taken from a single vantage point; it does not involve the creation of views from novel positions in space nor the creation of virtual motion for objects in the scene.
Reference: [Int97] <institution> Interactive Pictures Corporation, Inc. IPIX, </institution> <note> version 1.0, </note> <year> 1997. </year>
Reference-contexts: As it turns out, some of the techniques are simply not applicable to the problem, while others require more information than we are assuming is available. One major category of image-based rendering techniques is called mosaicing and includes such work as <ref> [Sze94, Che95, Sze96, Int97, RPFRA98, SS97, Dav98] </ref>. As the name implies, these techniques seek to create large views of a scene by combining many small extant views. Most work on mosaicing is limited to static scenes, making the results not directly applicable to dynamic view interpolation.
Reference: [MB95] <author> Leonard McMillan and Gary Bishop. </author> <booktitle> Plenoptic modeling. In Proc. SIGGRAPH 95, </booktitle> <pages> pages 39-46, </pages> <year> 1995. </year>
Reference-contexts: However, mosaicing techniques can be crucial in creating dynamic view morphs from co-centered reference cameras (see Section 5.1). Another major group of image-based rendering techniques can be termed image-based modeling <ref> [MB95, AS97, SD97, TK92] </ref>. These techniques attempt to reconstruct three-dimensional scene geometry from reference views. Once the geometry is recovered, new views of the scene can be generated from arbitrary positions in space. All of these techniques make use of camera calibration and dense point correspondences in some way. <p> These techniques attempt to reconstruct three-dimensional scene geometry from reference views. Once the geometry is recovered, new views of the scene can be generated from arbitrary positions in space. All of these techniques make use of camera calibration and dense point correspondences in some way. The techniques of <ref> [MB95] </ref> and [AS97] automatically recover camera calibration and dense point correspondences and work from a small number of input views (e.g., two or three). However, they will fail if the original views are not closely spaced. Plenoptic modeling [MB95] requires panoramic reference views and also assumes a sparse set of point <p> The techniques of <ref> [MB95] </ref> and [AS97] automatically recover camera calibration and dense point correspondences and work from a small number of input views (e.g., two or three). However, they will fail if the original views are not closely spaced. Plenoptic modeling [MB95] requires panoramic reference views and also assumes a sparse set of point correspondences is initially provided. Voxel coloring [SD97] requires accurate camera calibration and a large number of input views, but will recover dense point correspondences automatically. <p> When the two reference views are sufficiently close, some existing stereo algorithms can produce a reliable set of point correspondences (for instance [ZDFL95]). However, when the views are closely spaced it is also possible to use techniques other than view morphing to produce new virtual views <ref> [MB95, AS97] </ref>. These techniques offer more flexibility in positioning the virtual camera. They also generate dense correspondences, leading to improved image quality. View morphing is most applicable when the views are widely spaced, in which case other techniques cannot be used.
Reference: [MD98] <author> Russell A. Manning and Charles R. Dyer. </author> <title> Environment map morphing. </title> <type> Technical report, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: A modification to the basic implementation is necessary to account for this relaxed condition. 15 Alternatively, the fully unrestricted "environment map morphing" technique described in <ref> [MD98] </ref> can be used. In particular, this solves the problem with respect to the background object or any other large object that can easily contain its own vanishing point.
Reference: [Nay97] <editor> Shree K. Nayar. Catadioptric omnidirectional camera. </editor> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 482-488, </pages> <year> 1997. </year>
Reference-contexts: Dynamic view morphing can also be used to portray the object during periods of occlusion, when it would otherwise not be seen in the view. These techniques are particularly applicable when a wide-field-of-view pinhole camera such as OmniCamera <ref> [Nay97] </ref> is used to acquire the views. This is because, even when such a camera is completely unchanging, it still acquires much of the available scene information.
Reference: [RPFRA98] <author> B. Rousso, S. Peleg, I. Finci, and A. Rav-Acha. </author> <title> Universal mosaicing using pipe projection. </title> <booktitle> In Proc. 6th Int. Conf. on Computer Vision, </booktitle> <pages> pages 945-952, </pages> <year> 1998. </year>
Reference-contexts: As it turns out, some of the techniques are simply not applicable to the problem, while others require more information than we are assuming is available. One major category of image-based rendering techniques is called mosaicing and includes such work as <ref> [Sze94, Che95, Sze96, Int97, RPFRA98, SS97, Dav98] </ref>. As the name implies, these techniques seek to create large views of a scene by combining many small extant views. Most work on mosaicing is limited to static scenes, making the results not directly applicable to dynamic view interpolation.
Reference: [SD96] <author> Steven M. Seitz and Charles R. Dyer. </author> <title> View morphing. </title> <booktitle> In Proc. SIGGRAPH 96, </booktitle> <pages> pages 21-30, </pages> <year> 1996. </year>
Reference-contexts: Most work relating to view interpolation assumes the scene is static and unchanging; in this paper we introduce a method for interpolating between views of a dynamic scene. Our work is based upon an earlier technique called view morphing <ref> [SD96] </ref> which provides a method for interpolating between two widely-spaced views of a static scene (Fig. 2). The technique has several strengths that make it suitable for practical applications. First, only two reference views are assumed. <p> All of the methods cited for the previous category assume that the reference views are of static scenes. If the reference views contain object motion, then these techniques cannot be directly applied. A third general category consists of image-based rendering techniques specifically designed for view interpolation <ref> [CW93, SD96] </ref>. Our work belongs to this group. The methods of this category are all pure image-based rendering techniques, in that three-dimensional scene geometry is not recovered. Instead, new views are created by moving pixels around or otherwise stretching and distorting the original views. <p> Instead, new views are created by moving pixels around or otherwise stretching and distorting the original views. As with the image-based modeling techniques but unlike mosaicing, these methods can produce new views of a scene from novel positions in space. View morphing <ref> [SD96] </ref> can work with unknown camera calibration and very sparse point correspondences. Both [CW93] and [SD96] are for static scenes and cannot be directly applied to dynamic view interpolation except in very limited 4 circumstances (see Section 2.2). <p> As with the image-based modeling techniques but unlike mosaicing, these methods can produce new views of a scene from novel positions in space. View morphing <ref> [SD96] </ref> can work with unknown camera calibration and very sparse point correspondences. Both [CW93] and [SD96] are for static scenes and cannot be directly applied to dynamic view interpolation except in very limited 4 circumstances (see Section 2.2). A few other prominent image-based rendering techniques deserve mention with respect to the dynamic view interpolation problem. <p> By using information contained in the fundamental matrix, the two reference views are transformed into rectified views (see Appendix in <ref> [SD96] </ref>). <p> This optional last step is termed postwarping. 2.2 Trivial Dynamic Scenes The underlying theory of view morphing assumes static scenes. In practice, the algorithm handles certain types of small movement reasonably well. For instance, <ref> [SD96] </ref> gives an example depicting two reference views of a face in which the eyes change gaze direction. A smooth series of new views is generated showing a gradual change in the gaze direction of the eyes. <p> Fig. 8 demonstrates layering. A similar "tearing" effect can occur in a completely static scene due to objects being located at different depths. A monotonicty assumption was made in <ref> [SD96] </ref> to preclude this problem; however, the concept of layering can also be applied to static view morphing to deal with tearing effects (see Section 7, sequence (iv)). <p> A simple postwarp transformation is the linear interpolation of I and T AB (i.e., (1 s)I + sT AB ). In our implementation, we use a postwarp method discussed in <ref> [SD96] </ref>: The four matching corners of each original view are prewarped and then linearly interpolated. The postwarp is chosen to map the four interpolated corners to the corners of a rectangle (a projective transformation can be determined by its behavior on four such points [Hec89]). <p> Note that the edges of the tape roads were placed on the table by hand and no steps were taken to guarantee parallelism. Sequence (iv): This sequence shows a view interpolation for a static scene that violates the mono-tonicity assumption discussed in <ref> [SD96, Sei97] </ref>. The monotonicity assumption was circumvented by the use of layering: the pillar in the foreground was placed on one layer and the remaining scene elements formed the background layer.
Reference: [SD97] <author> Steven M. Seitz and Charles R. Dyer. </author> <title> Photorealistic scene reconstruction by voxel coloring. </title> <booktitle> In Proc. Image Understanding Workshop, </booktitle> <pages> pages 1067-1073, </pages> <year> 1997. </year>
Reference-contexts: However, mosaicing techniques can be crucial in creating dynamic view morphs from co-centered reference cameras (see Section 5.1). Another major group of image-based rendering techniques can be termed image-based modeling <ref> [MB95, AS97, SD97, TK92] </ref>. These techniques attempt to reconstruct three-dimensional scene geometry from reference views. Once the geometry is recovered, new views of the scene can be generated from arbitrary positions in space. All of these techniques make use of camera calibration and dense point correspondences in some way. <p> However, they will fail if the original views are not closely spaced. Plenoptic modeling [MB95] requires panoramic reference views and also assumes a sparse set of point correspondences is initially provided. Voxel coloring <ref> [SD97] </ref> requires accurate camera calibration and a large number of input views, but will recover dense point correspondences automatically.
Reference: [Sei97] <author> Steven M. Seitz. </author> <title> Image-Based Transformation of Viewpoint and Scene Appearance. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, WI, </institution> <year> 1997. </year>
Reference-contexts: Note that the edges of the tape roads were placed on the table by hand and no steps were taken to guarantee parallelism. Sequence (iv): This sequence shows a view interpolation for a static scene that violates the mono-tonicity assumption discussed in <ref> [SD96, Sei97] </ref>. The monotonicity assumption was circumvented by the use of layering: the pillar in the foreground was placed on one layer and the remaining scene elements formed the background layer.
Reference: [SS97] <author> Richard Szeliski and Heung-Yeung Shum. </author> <title> Creating full view panoramic image mosaics and environment maps. </title> <booktitle> In Proc. SIGGRAPH 97, </booktitle> <pages> pages 251-258, </pages> <year> 1997. </year>
Reference-contexts: As it turns out, some of the techniques are simply not applicable to the problem, while others require more information than we are assuming is available. One major category of image-based rendering techniques is called mosaicing and includes such work as <ref> [Sze94, Che95, Sze96, Int97, RPFRA98, SS97, Dav98] </ref>. As the name implies, these techniques seek to create large views of a scene by combining many small extant views. Most work on mosaicing is limited to static scenes, making the results not directly applicable to dynamic view interpolation.
Reference: [Sze94] <author> Richard Szeliski. </author> <title> Image mosaicing for tele-reality applications. </title> <booktitle> In Proc. Workshop Applications of Computer Vision, </booktitle> <pages> pages 44-53, </pages> <year> 1994. </year>
Reference-contexts: As it turns out, some of the techniques are simply not applicable to the problem, while others require more information than we are assuming is available. One major category of image-based rendering techniques is called mosaicing and includes such work as <ref> [Sze94, Che95, Sze96, Int97, RPFRA98, SS97, Dav98] </ref>. As the name implies, these techniques seek to create large views of a scene by combining many small extant views. Most work on mosaicing is limited to static scenes, making the results not directly applicable to dynamic view interpolation. <p> In the context of mosaicing, it is well known that by identifying four corresponding points in a pair of images, a projective mapping between the images can be found <ref> [Hec89, Sze94] </ref>. This projective mapping is just a transformation between the camera bases. Note that there is a precondition for calculating the projective transformation: all three-member subsets of the four conjugate directions must span &lt; 3 .
Reference: [Sze96] <author> Richard Szeliski. </author> <title> Video mosaics for virtual environments. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 16(2) </volume> <pages> 22-30, </pages> <year> 1996. </year>
Reference-contexts: As it turns out, some of the techniques are simply not applicable to the problem, while others require more information than we are assuming is available. One major category of image-based rendering techniques is called mosaicing and includes such work as <ref> [Sze94, Che95, Sze96, Int97, RPFRA98, SS97, Dav98] </ref>. As the name implies, these techniques seek to create large views of a scene by combining many small extant views. Most work on mosaicing is limited to static scenes, making the results not directly applicable to dynamic view interpolation.
Reference: [TK92] <author> Carlo Tomasi and Takeo Kanade. </author> <title> Shape and motion from image streams under orthography: A factorization method. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 9(2) </volume> <pages> 137-154, </pages> <year> 1992. </year>
Reference-contexts: However, mosaicing techniques can be crucial in creating dynamic view morphs from co-centered reference cameras (see Section 5.1). Another major group of image-based rendering techniques can be termed image-based modeling <ref> [MB95, AS97, SD97, TK92] </ref>. These techniques attempt to reconstruct three-dimensional scene geometry from reference views. Once the geometry is recovered, new views of the scene can be generated from arbitrary positions in space. All of these techniques make use of camera calibration and dense point correspondences in some way. <p> Plenoptic modeling [MB95] requires panoramic reference views and also assumes a sparse set of point correspondences is initially provided. Voxel coloring [SD97] requires accurate camera calibration and a large number of input views, but will recover dense point correspondences automatically. The Factorization Method <ref> [TK92] </ref> by itself is just a method for recovering scene geometry, but it can be the basis for image-based modeling techniques such as one being developed at K 2 T, Inc.
Reference: [TK96] <author> Jay Torborg and James T. Kajiya. Talisman: </author> <title> Commodity realtime 3D graphics for the PC. </title> <booktitle> In Proc. SIGGRAPH 96, </booktitle> <pages> pages 353-363, </pages> <year> 1996. </year>
Reference-contexts: Both [CW93] and [SD96] are for static scenes and cannot be directly applied to dynamic view interpolation except in very limited 4 circumstances (see Section 2.2). A few other prominent image-based rendering techniques deserve mention with respect to the dynamic view interpolation problem. Talisman <ref> [TK96] </ref> is an image-based technique for speeding-up the traditional graphics pipeline. As in traditional three-dimensional graphics systems, Talisman starts with a complete model of the scene it seeks to portray.
Reference: [WA94] <author> John Y. A. Wang and Edward H. Adelson. </author> <title> Representing moving images with layers. </title> <journal> IEEE Trans. Image Processing, </journal> <volume> 3(5) </volume> <pages> 625-638, </pages> <year> 1994. </year>
Reference-contexts: The solution is to separate unrelated scene points. We accomplish this by segmenting the scene into different layers, with each layer representing a group of related scene points. A similar technique 7 appears in <ref> [WA94] </ref>. Typically, we will have a separate layer for each moving object in the scene and one for the stationary "background" object. For instance, in the example just discussed there would be two layers: one for the truck and the other for the remaining background elements. Fig. 8 demonstrates layering.
Reference: [ZDFL95] <author> Z. Zhang, R. Deriche, O. Faugeras, and Q.-T. Luong. </author> <title> A robust technique for matching two uncalibrated images through the recovery of the unknown epipolar geometry. </title> <journal> Artificial Intelligence, </journal> <volume> 78 </volume> <pages> 87-119, </pages> <year> 1995. </year> <month> 36 </month>
Reference-contexts: We will refer to these locations as marked scene points. When the two reference views are sufficiently close, some existing stereo algorithms can produce a reliable set of point correspondences (for instance <ref> [ZDFL95] </ref>). However, when the views are closely spaced it is also possible to use techniques other than view morphing to produce new virtual views [MB95, AS97]. These techniques offer more flexibility in positioning the virtual camera. They also generate dense correspondences, leading to improved image quality.
References-found: 27

