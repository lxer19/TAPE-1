URL: http://www.cs.duke.edu/~mlittman/docs/witness-tm.ps
Refering-URL: http://www.cs.duke.edu/~mlittman/topics/pomdp-page.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: The Witness Algorithm: Solving Partially Observable Markov Decision Processes  
Author: Michael L. Littman 
Date: December 30th, 1994  
Abstract-found: 0
Intro-found: 1
Reference: [ Astrom, 1965 ] <author> Astrom, K. J. </author> <year> 1965. </year> <title> Optimal control of Markov decision processes with incomplete state estimation. </title> <journal> J. Math. Anal. Appl. </journal> <volume> 10 </volume> <pages> 174-205. </pages>
Reference-contexts: 1 Introduction Markov decision processes (MDP's) [ Bellman, 1957 ] are a mathematical formalization of problems in which a decision-maker, or agent, must choose how to act to maximize its reward over a series of interactions with its environment. Partially observable Markov decision processes (POMDP's) <ref> [ Drake, 1962, Astrom, 1965, Smallwood and Sondik, 1973 ] </ref> generalize the MDP framework to the case where the agent must make its decisions in partial ignorance of its current situation. This paper describes the POMDP framework and presents some well-known results from the field. <p> That is, knowing its belief state is sufficient information for the agent to behave optimally. More formal arguments to this effect have been put forth elsewhere <ref> [ Astrom, 1965 ] </ref> . The machinery needed to derive this formally is beyond the scope of this paper.
Reference: [ Bellman, 1957 ] <author> Bellman, Richard 1957. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey. </address>
Reference-contexts: 1 Introduction Markov decision processes (MDP's) <ref> [ Bellman, 1957 ] </ref> are a mathematical formalization of problems in which a decision-maker, or agent, must choose how to act to maximize its reward over a series of interactions with its environment. <p> Note that the same expression can be used to specify the optimal policy. 3.1 Bounding a step of value iteration The method of value iteration <ref> [ Bellman, 1957 ] </ref> is a way of approximating the optimal value function, V fl . The next few sections define value iteration and show how it can be used to produce arbitrarily good policies.
Reference: [ Bertsekas, 1987 ] <author> Bertsekas, D. P. </author> <year> 1987. </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: The function V fl is called the optimal value function and it is unique <ref> [ Bertsekas, 1987 ] </ref> . The optimal policy, fl : (S) ! A, maps belief states to actions. It is stationary [ Bertsekas, 1987 ] , meaning that the optimal action choice for belief state b is constant over time. <p> The function V fl is called the optimal value function and it is unique <ref> [ Bertsekas, 1987 ] </ref> . The optimal policy, fl : (S) ! A, maps belief states to actions. It is stationary [ Bertsekas, 1987 ] , meaning that the optimal action choice for belief state b is constant over time. <p> For now, consider these as abstract mathematical functions. 8 Section 4 describes a representation for V and V 0 that is suitable for computer manipulation. To show that applying Equation 4 leads to an improved approximation, we need the following lemma (see <ref> [ Bertsekas, 1987 ] </ref> ).
Reference: [ Cassandra et al., 1994 ] <author> Cassandra, Anthony R.; Kaelbling, Leslie Pack; and Littman, Michael L. </author> <year> 1994. </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA. </address>
Reference-contexts: Such an algorithm has eluded us thus far. Two additional points are worth making here. First, although the algorithm described in our earlier paper <ref> [ Cassandra et al., 1994 ] </ref> was not formally correct, the experimental results reported there were valid. We have rerun the same examples with the revised algorithm and found no important differences.
Reference: [ Cassandra, 1994 ] <author> Cassandra, </author> <title> Anthony 1994. Optimal policies for partially observable Markov decision processes. </title> <type> Technical Report CS-94-14, </type> <institution> Brown University, Department of Computer Science, Providence RI. </institution>
Reference-contexts: Like previous algorithms, the witness algorithm can run in time exponential in the size of V t . In what sense is the witness algorithm superior to previous algorithms for solving POMDP problems, then? For one thing, preliminary experiments indicate that the witness algorithm is faster in practice (see <ref> [ Cassandra, 1994 ] </ref> for some very informal results). The primary complexity-theoretic difference is that the witness algorithm runs in polynomial time in the number of policy trees in Q a t .
Reference: [ Cheng, 1988 ] <author> Cheng, </author> <month> Hsien-Te </month> <year> 1988. </year> <title> Algorithms for Partially Observable Markov Decision Processes. </title> <type> Ph.D. Dissertation, </type> <institution> University of British Columbia, British Columbia, Canada. </institution>
Reference-contexts: Another class of algorithms work with the set ^ V t V t and search for policy trees in V t ^ V t . That is, they build up to V t one policy tree at a time. These approaches are sometimes called relaxed region algorithms <ref> [ Cheng, 1988 ] </ref> because the vectors corresponding to the policy trees in ^ V t partition belief space into regions that constitute a relaxation (a form of approximation) of the regions corresponding to the true V t function. <p> The algorithms work by extending the set ^ V t with non-extraneous policy trees until no more exist at which point V t = ^ V t and the algorithms can terminate. The earlier algorithms that follow this approach <ref> [ Cheng, 1988, Sondik, 1971 ] </ref> have worst-case running times that are exponential in the size of V t1 . There is some reason to believe that this is a necessary feature of this problem (see Section 7.1). In this section we take a slightly different approach.
Reference: [ Drake, 1962 ] <author> Drake, A. W. </author> <year> 1962. </year> <title> Observation of a Markov Process Through a Noisy Channel. </title> <type> Ph.D. Dissertation, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts. </institution>
Reference-contexts: 1 Introduction Markov decision processes (MDP's) [ Bellman, 1957 ] are a mathematical formalization of problems in which a decision-maker, or agent, must choose how to act to maximize its reward over a series of interactions with its environment. Partially observable Markov decision processes (POMDP's) <ref> [ Drake, 1962, Astrom, 1965, Smallwood and Sondik, 1973 ] </ref> generalize the MDP framework to the case where the agent must make its decisions in partial ignorance of its current situation. This paper describes the POMDP framework and presents some well-known results from the field.
Reference: [ Eagle, 1984 ] <author> Eagle, James N. </author> <year> 1984. </year> <title> The optimal search for a moving target when the search path is constrained. </title> <journal> Operations research 32(5) </journal> <pages> 1107-1115. </pages>
Reference-contexts: The first involves gener-ating a superset of the policy trees, V + t , and then deleting those that are extraneous <ref> [ Monahan, 1982, Eagle, 1984 ] </ref> . The process for deleting extraneous trees is described in Section 7.4. Regardless of the true size of V t , algorithms based on this approach will generate an exponential (in jV t1 j) number of policy trees. <p> (value (choice (p; o)); a; o)[s] as desired. 7.4 Eliminating extraneous policy trees Given a set X of policy trees, how can we find a minimum-sized subset, V X , that represents the same value function? Monahan [ Monahan, 1982 ] describes a method that is further explored by Eagle <ref> [ Eagle, 1984 ] </ref> for eliminating extraneous policy trees from a set. Interestingly, Monahan attributes the method to Sondik [ Smallwood and Sondik, 1973 ] but the two methods are only similar at a high level. The method described here is a slight variant of the one discussed by Monahan.
Reference: [ Garey and Johnson, 1979 ] <author> Garey, M. R. and Johnson, D. S. </author> <year> 1979. </year> <title> Computers and intractability: A guide to the theory of NP-completeness. </title> <publisher> Freeman, </publisher> <address> San Francisco, CA. </address> <month> 47 </month>
Reference-contexts: This means that there is a polynomial time algorithm to solve it if and only if P = NP. This can be shown using a somewhat messy reduction to MAX-2-SAT or quadratic programming <ref> [ Garey and Johnson, 1979 ] </ref> . Our earlier algorithm claimed to solve the V -function witness problem in polynomial time. In fact, the algorithm runs in polynomial time but will not always give correct answers.
Reference: [ Heyman and Sobel, 1984 ] <author> Heyman, D. and Sobel, M. </author> <year> 1984. </year> <title> Stochastic Mod--els in Operations Research: </title> <journal> Stochastic Optimization, </journal> <volume> volume 2. </volume> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference-contexts: Now, for i from 1 to t, generate V i from V i1 using a step of value iteration. The following lemma shows that, by using a large enough value of t, we can find a value function arbitrarily close to V fl (see, e.g. <ref> [ Heyman and Sobel, 1984 ] </ref> ). <p> Theorem 3 <ref> [ Williams and Baird, 1993, Heyman and Sobel, 1984 ] </ref> tells us the result of using this criterion.
Reference: [ Monahan, 1982 ] <author> Monahan, George E. </author> <year> 1982. </year> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <booktitle> Management Science 28 </booktitle> <pages> 1-16. </pages>
Reference-contexts: The extraneous t-step policy trees that are generated can then be identified and removed using the techniques discussed in Section 7.4. This approach, first introduced by Monahan <ref> [ Monahan, 1982 ] </ref> , can be very efficient when the number of observations is small. <p> The first involves gener-ating a superset of the policy trees, V + t , and then deleting those that are extraneous <ref> [ Monahan, 1982, Eagle, 1984 ] </ref> . The process for deleting extraneous trees is described in Section 7.4. Regardless of the true size of V t , algorithms based on this approach will generate an exponential (in jV t1 j) number of policy trees. <p> (choice (p; o))[s 0 ] = R [s; a] + fl o back (value (choice (p; o)); a; o)[s] as desired. 7.4 Eliminating extraneous policy trees Given a set X of policy trees, how can we find a minimum-sized subset, V X , that represents the same value function? Monahan <ref> [ Monahan, 1982 ] </ref> describes a method that is further explored by Eagle [ Eagle, 1984 ] for eliminating extraneous policy trees from a set. Interestingly, Monahan attributes the method to Sondik [ Smallwood and Sondik, 1973 ] but the two methods are only similar at a high level.
Reference: [ Platzman, 1977 ] <author> Platzman, Loren K. </author> <year> 1977. </year> <title> Finite-memory estimation and control of finite probabilistic systems. </title> <type> Ph.D. Dissertation, </type> <institution> Massachusetts Institute of Technology. </institution>
Reference-contexts: However, this is not completely well-defined as it stands. If the agent can guarantee itself a huge reward tomorrow by doing nothing today, what's to keep it from doing nothing forever? This is sometimes called the problem of the infinitely delayed splurge <ref> [ Platzman, 1977 ] </ref> and a standard way to combat it is to use geometric discounting. This means a reward received t steps in the future is only worth fl t as much as it would be if it were received today.
Reference: [ Puterman, 1994 ] <author> Puterman, Martin X. </author> <year> 1994. </year> <title> Markov Decision Processes. </title> <publisher> unknown. </publisher>
Reference: [ Schrijver, 1986 ] <author> Schrijver, </author> <title> Alexander 1986. Theory of linear and integer programming. </title> <publisher> Wiley-Interscience. </publisher>
Reference-contexts: Note that we must assume that the number of bits of precision used in specifying the model, M, is polynomial in these quantities since the running time of linear programming is expressed as a function of the input precision <ref> [ Schrijver, 1986 ] </ref> . Proof: We would like to bound the running time of a single call to witness (Table 5). The work can be divided up as follows. <p> The total work due to solving linear programs consists of solving a polynomial number of polynomial-sized linear programs, each of which can be solved in polynomial time <ref> [ Schrijver, 1986 ] </ref> . The other work performed is primarily in calls to back and besttree, each of which is trivially implemented to run in polynomial time.
Reference: [ Smallwood and Sondik, 1973 ] <author> Smallwood, Richard D. and Sondik, Ed-ward J. </author> <year> 1973. </year> <title> The optimal control of partially observable Markov processes over a finite horizon. </title> <journal> Operations Research 21 </journal> <pages> 1071-1088. </pages>
Reference-contexts: 1 Introduction Markov decision processes (MDP's) [ Bellman, 1957 ] are a mathematical formalization of problems in which a decision-maker, or agent, must choose how to act to maximize its reward over a series of interactions with its environment. Partially observable Markov decision processes (POMDP's) <ref> [ Drake, 1962, Astrom, 1965, Smallwood and Sondik, 1973 ] </ref> generalize the MDP framework to the case where the agent must make its decisions in partial ignorance of its current situation. This paper describes the POMDP framework and presents some well-known results from the field. <p> The policy tree chooses an action for each of the t steps as a function of the history of observations. A fundamental fact about finite-horizon POMDP's is that their value functions are piecewise-linear and convex <ref> [ Smallwood and Sondik, 1973 ] </ref> . Figure 4 shows an example value function for a simple two state POMDP. A representation that makes use of these properties was introduced by Sondik [ Sondik, 1971 ] . <p> Interestingly, Monahan attributes the method to Sondik <ref> [ Smallwood and Sondik, 1973 ] </ref> but the two methods are only similar at a high level. The method described here is a slight variant of the one discussed by Monahan. <p> It repeated a classic analysis that shows that an infinite-horizon POMDP problem can be approximated arbitrarily well by a finite-horizon solution. It gave a new (to us) analysis that bounds the infinite-horizon performance of solving a sequence of finite-horizon problems approximately. The paper repeated a result of Sondik <ref> [ Smallwood and Sondik, 1973 ] </ref> that states that a finite-horizon value function is piecewise-linear and convex over belief space. It gave a novel proof of this that shows that we can interpret the components of the linear functions as the performance of a finite-horizon policy.
Reference: [ Sondik, 1971 ] <author> Sondik, E. </author> <year> 1971. </year> <title> The Optimal Control of Partially Observable Markov Processes. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University. </institution>
Reference-contexts: A fundamental fact about finite-horizon POMDP's is that their value functions are piecewise-linear and convex [ Smallwood and Sondik, 1973 ] . Figure 4 shows an example value function for a simple two state POMDP. A representation that makes use of these properties was introduced by Sondik <ref> [ Sondik, 1971 ] </ref> . Theorem 4 (Piecewise-linear and convex): The optimal t-horizon value function can be written as: V t (b) = max X b [s]ff [s] ; for some finite size collection, C t , of jSj-dimensional vectors. <p> The algorithms work by extending the set ^ V t with non-extraneous policy trees until no more exist at which point V t = ^ V t and the algorithms can terminate. The earlier algorithms that follow this approach <ref> [ Cheng, 1988, Sondik, 1971 ] </ref> have worst-case running times that are exponential in the size of V t1 . There is some reason to believe that this is a necessary feature of this problem (see Section 7.1). In this section we take a slightly different approach.
Reference: [ Watkins, 1989 ] <author> Watkins, C. J.C.H. </author> <year> 1989. </year> <title> Learning with Delayed Rewards. </title> <type> Ph.D. Dissertation, </type> <institution> Cambridge University. </institution>
Reference-contexts: In this section we take a slightly different approach. Instead of building up a representation of V t directly, we first find a collection of policy trees that represents Q a t , for each a 2 A. These Q-functions are defined analogously to the Q-functions of Watkins <ref> [ Watkins, 1989 ] </ref> |the expression Q a t (b) represents the expected reward for taking action a from belief state b and then acting optimally for the the remaining t 1 steps.
Reference: [ Williams and Baird, 1993 ] <author> Williams, Ronald J. and Baird, Leemon C. </author> <title> III 1993. Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-13, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA. </address> <month> 48 </month>
Reference-contexts: Theorem 3 <ref> [ Williams and Baird, 1993, Heyman and Sobel, 1984 ] </ref> tells us the result of using this criterion.
References-found: 18

