URL: http://www.cs.utexas.edu/users/markj/proposal.ps
Refering-URL: http://www.cs.utexas.edu/users/markj/
Root-URL: 
Email: markj@cs.utexas.edu  
Title: Non-Compacting Memory Allocation and Real-Time Garbage Collection attractiveness for many applications, garbage collection for real-time
Author: Mark S. Johnstone Advisor: Paul R. Wilson 
Degree: Dissertation Proposal  
Note: In spite of its obvious  is not popular. This is largely  
Date: January 30, 1996  
Address: Austin, Texas 78712-1188  
Affiliation: Dept. of Computer Sciences University of Texas  
Abstract: Garbage collection is the automatic reclamation of computer storage [Knu73, Coh81, Wil92, Wil95]. While in many systems, programmers must explicitly reclaim heap memory at some point in their program by using a "free" or "dispose" statement, garbage collected systems free the programmer from this burden. Most existing "real-time" garbage collectors are not in fact usefully real-time, largely due to the use of a read barrier to trigger incremental copying of data structures being traversed by the running application. This may slow down running applications unpredictably, even though individual increments of garbage collection work are small and bounded. We have developed a hard real-time garbage collector which uses a write barrier to only coordinate collection work with modifications of pointers in data structures, therefore making coordination costs cheaper and more predictable. We combine this write barrier approach with implicit non-copying reclamation ("fake copying"), which has most of the advantages of copying collection (notably avoiding both the sweep phase required by mark-sweep collectors, and the touching of garbage objects when reclaiming their space), without the disadvantage of having to actually copy the objects. It has long been believed that fragmentation will be prohibitively high in a non-copying garbage collection algorithm. We address these fragmentation issues with our garbage collector, and compare the fragmentation produced by a number of traditional memory allocation algorithms. In doing this comparison we produce some surprising results demonstrating that methodologies used to study fragmentation, dating back as far as 1961, are fundamentally unsound and biased. We have conducted sound studies that suggest that fragmentation can be kept very low for most real programs using well-known, non-copying policies. Finally, we have extended our garbage collector with generational techniques to attempt to provide excellent average-case efficiency while preserving good soft real-time responsiveness. 
Abstract-found: 1
Intro-found: 1
Reference: [AEL88] <author> Andrew W. Appel, John R. Ellis, and Kai Li. </author> <title> Real-time concurrent garbage collection on stock multiprocessors. </title> <booktitle> In Proceedings of the 1988 SIGPLAN Conference on Programming Language Design and Implementation [PLD88], </booktitle> <pages> pages 11-20. </pages>
Reference-contexts: In short, Baker's scheme will unpredictably suffer from unacceptably long garbage collection slowdowns. This problem is even worse in recent collectors which use page-wise virtual memory protection to trigger larger increments of collector work <ref> [AEL88, Det90a, Joh92, BDS91] </ref>, and is also significant on Lisp-machine style hardware. Even if the necessary checks are performed by dedicated parallel hardware, most of the available CPU time may be used up (in the worst case) by the actual trapping to copying routines and the copying itself.
Reference: [AM92] <editor> Antonio Albano and Ron Morrison, editors. </editor> <booktitle> Fifth International Workshop on Persistent Object Systems, </booktitle> <address> San Miniato, Italy, </address> <month> September </month> <year> 1992. </year> <note> Springer-Verlag. </note>
Reference: [AP87] <author> S. Abraham and J. Patel. </author> <title> Parallel garbage collection on a virtual memory system. </title> <editor> In E. Chiricozzi and A. D'Amato, editors, </editor> <booktitle> International Conference on Parallel Processing and Applications, </booktitle> <pages> pages 243-246, </pages> <address> L'Aquila, Italy, September 1987. </address> <publisher> Elsevier North-Holland. </publisher>
Reference-contexts: One strategy is to ensure that objects can never get lost, by preventing any pointers from being destroyed <ref> [AP87, Yua90] </ref>. Before overwriting a pointer, the old pointer value is immediately traversed, or saved away so that the collector can still find it and trace it later. We call this a snapshot-at-beginning algorithm because the collector's view of reachable data structures is fixed when collection begins.
Reference: [Bak78] <author> Henry G. Baker, Jr. </author> <title> List processing in real time on a serial computer. </title> <journal> Communications of the ACM, </journal> <volume> 21(4) </volume> <pages> 280-294, </pages> <month> April </month> <year> 1978. </year>
Reference-contexts: If memory fragmentation does prove to be a problem, then copying algorithms become more attractive than a non-copying algorithm. As we said above, copying routines need more coordination between the application and the collector. There are three basic approaches to providing this coordination: 1. Read-barrier only techniques <ref> [Bak78] </ref> prevent the application from ever seeing an inconsistent copy of any object by catching all references to all objects. This approach is attractive in situations where 1 Note that non-copying collection need not incur the cost of the sweep phase of a mark-sweep collector as is commonly assumed. <p> This issue is particularly complicated for garbage collected systems because the programmer no longer has direct control on when a block of memory becomes available for reuse. 5.1.1 Why Baker's Read Barrier is Not Usefully Real Time Baker's incremental copying technique <ref> [Bak78] </ref> is the best known "real-time" collection strategy, but it is actually poorly suited to real-time garbage collection on stock hardware; its close coupling between application program actions and collector actions makes it intrinsically more expensive and difficult to use for real-time applications.
Reference: [Bak91] <author> Henry G. Baker, Jr. </author> <title> The Treadmill: Real-time garbage collection without motion sickness. In OOPSLA '91 Workshop on Garbage Collection in Object-Oriented Systems [OOP91]. </title> <note> Position paper. Also appears as SIGPLAN Notices 27(3) 66-70, </note> <month> March </month> <year> 1992. </year>
Reference-contexts: This model is based on tricolor marking [DLM + 78] and is augmented with the key idea that garbage collection is really the process of marking objects and moving them from one set to another <ref> [Bak91] </ref>. In addition, this model uses two important invariants that allow us address the issues of consistency and conservatism in incremental collection. <p> This approach is attractive in situations where 1 Note that non-copying collection need not incur the cost of the sweep phase of a mark-sweep collector as is commonly assumed. In Section 5.7.1 we explain a technique known as "fake copying" [Wan89] (also known as "implicit reclamation" <ref> [Bak91] </ref>) which avoids the cost of a sweep phase. 2 Conservative pointer finding techniques are subject to erroneously keeping objects (and hence entire data structures) live by finding an integer that happens to look like a pointer into the heap. <p> The garbage objects are never examined, and their space is implicitly reclaimed. While at first these two methods of reclaiming garbage memory may seem fundamentally different, there is a way to combine them to receive many of the advantages of both <ref> [Wan89, Bak91] </ref>. This "fake copying" approach is fundamental to our real-time garbage collector implementation, and we will discuss it in detail in Section 5.5. 4.1 Real-time Garbage Collection For truly real-time applications, fine-grained incremental garbage collection appears to be necessary. <p> an incremental update write barrier can be sped up considerably at the cost of increased conservatism by always assuming that any pointer store will violate the write barrier and optimistically shading the R-value without checking the color of the L-value. 5.5 Non-Copying, Incremental Read Barrier Techniques Wang [Wan89] and Baker <ref> [Bak91] </ref> independently presented a critical insight that can be used to make a mark-sweep collector have many of the advantages of a copying collector. Their insight was that in a copying collector, the "spaces" of the collector are really just a particular implementation of sets. <p> In addition, it must be relatively easy to move an object from one set to another. Finally, it must be easy to switch the roles of the sets at the end of collection. Baker's incremental non-copying algorithm (called the treadmill algorithm) <ref> [Bak91] </ref> uses doubly-linked lists (and per-object color fields) to implement the garbage collection sets, rather than separate memory areas. These lists are linked into a cyclic structure, as shown in Figure 3. This cyclic structure is divided into four sections: the new-set, the free-set, the from-set and the to-set. <p> We expand on this idea considerably in Section 6. 5.7.1 Non-copying Implicit Reclamation The testbed garbage collector that we have implemented combines an incremental update write barrier with a generalization of Baker's non-copying implicit reclamation strategy <ref> [Bak91] </ref>, so that objects not yet reached need not be traversed to be reclaimed, as they are in the sweep phase of a mark-sweep collector.
Reference: [BC92] <editor> Yves Bekkers and Jacques Cohen, editors. </editor> <booktitle> International Workshop on Memory Management, number 637 in Lecture Notes in Computer Science, </booktitle> <address> St. Malo, France, </address> <month> September </month> <year> 1992. </year> <note> Springer-Verlag. </note>
Reference: [BDS91] <author> Hans-J. Boehm, Alan J. Demers, and Scott Shenker. </author> <title> Mostly parallel garbage collection. </title> <booktitle> In Pro ceedings of the 1991 SIGPLAN Conference on Programming Language Design and Implementation [PLD91], </booktitle> <pages> pages 157-164. </pages>
Reference-contexts: In short, Baker's scheme will unpredictably suffer from unacceptably long garbage collection slowdowns. This problem is even worse in recent collectors which use page-wise virtual memory protection to trigger larger increments of collector work <ref> [AEL88, Det90a, Joh92, BDS91] </ref>, and is also significant on Lisp-machine style hardware. Even if the necessary checks are performed by dedicated parallel hardware, most of the available CPU time may be used up (in the worst case) by the actual trapping to copying routines and the copying itself. <p> When such a pointer is created, the collector is notified 17 so that it can either trace the pointed-to object immediately, or re-examine the location in which the pointer was stored again later to find any "hidden" objects <ref> [Ste75, DLM + 78, BDS91] </ref>. For example, in Figure 2 step 2, when the pointer from object A to object E is created, a pointer to this pointer is recorded, and object E would be traversed when this pointer is reexamined.
Reference: [BL92] <author> Ball and Larus. </author> <title> Optimal profiling and tracing of programs. </title> <booktitle> In Conference Record of the Nineteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 59-70. </pages> <publisher> ACM Press, </publisher> <month> January </month> <year> 1992. </year>
Reference-contexts: This simulated program does not actually do anything with the allocated blocks, as a real program would, but it imitates the real program's request sequences exactly, which is sufficient for measuring the memory usage. Modern profiling tools <ref> [BL92, CK93] </ref> can also be used with the simulation program to determine how many instruction cycles are spent in the allocator itself. 3.7 Experimental Results An interesting result that we have produced is that small variations in policy can have large variations in fragmentation.
Reference: [Bro84] <author> Rodney A. Brooks. </author> <title> Trading data space for reduced time and code space in real-time collection on stock hardware. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on LISP and Functional Programming [LFP84], </booktitle> <pages> pages 108-113. </pages>
Reference-contexts: For other programs, it may also be feasible to rely on an occasional compacting collection with some degradation of real-time guarantees and/or overall performance <ref> [Bro84] </ref>. For still other applications, fragmentation issues will force the choice of a copying collector. Our work evaluates many of the costs associated with both copying and non-copying collection and is applicable to the entire spectrum of collection techniques. <p> Unfortunately, this approach also has systematic problems where the simple act of traversing a linked list can cause an application to miss all of its real-time deadlines. 2. Read barrier/write barrier combinations <ref> [Bro84] </ref> combine a read-barrier and a write barrier to alleviate the worst case situation that we described with a read-barrier only approach by requiring that all object references be made through a pointer indirection. <p> Others have proposed copying based algorithms that rely on a combination of a read barrier and a write barrier <ref> [Bro84] </ref>, or a write barrier only [NOPH92] to coordinate the collector's view of the graph with that of the application.
Reference: [Che70] <author> C. J. </author> <title> Cheney. A nonrecursive list compacting algorithm. </title> <journal> Communications of the ACM, </journal> <volume> 13(11):677 678, </volume> <month> November </month> <year> 1970. </year>
Reference-contexts: In a mark-sweep collector, once the live objects have been distinguished from the garbage objects, memory is exhaustively examined (swept) to find all of the garbage objects, and their space is reclaimed. An example of the second method is copying collection <ref> [FY69, Che70] </ref>. In a copy collector, the live objects are copied out of one area of memory and into another. Once all live objects have been copied out of the original memory area, the entire area is considered to be garbage and can be reclaimed in one operation.
Reference: [CK93] <author> Robert Cmelik and David Keppel. Shade: </author> <title> A fast instruction-set simulator for execution profiling. </title> <type> Technical Report UWCSE 93-06-06, </type> <institution> Dept. of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, Washington, </address> <year> 1993. </year>
Reference-contexts: This simulated program does not actually do anything with the allocated blocks, as a real program would, but it imitates the real program's request sequences exactly, which is sufficient for measuring the memory usage. Modern profiling tools <ref> [BL92, CK93] </ref> can also be used with the simulation program to determine how many instruction cycles are spent in the allocator itself. 3.7 Experimental Results An interesting result that we have produced is that small variations in policy can have large variations in fragmentation.
Reference: [Coh81] <author> Jacques Cohen. </author> <title> Garbage collection of linked data structures. </title> <journal> Computing Surveys, </journal> <volume> 13(3) </volume> <pages> 341-367, </pages> <month> September </month> <year> 1981. </year>
Reference: [Col61] <author> G. O. Collins. </author> <title> Experience in automatic storage allocation. </title> <journal> Communications of the ACM, </journal> <volume> 4(10):436 440, </volume> <month> October </month> <year> 1961. </year>
Reference-contexts: the allocation literature we discovered that virtually all past work in this field suffered from one common flaw: almost no one measured how well different allocators performed for actual programs. 29 The overwhelming majority of memory allocation studies to date have been based on a methodology developed in the 1960's <ref> [Col61] </ref>, which uses synthetic traces intended to model "typical" program behavior. This methodology has the advantage that it is easy to implement, and allows experiments to avoid quirky behavior specific to a few programs.
Reference: [DDZ93] <author> David Detlefs, Al Dosser, and Benjamin Zorn. </author> <title> Memory allocation costs in large C and C++ programs. </title> <type> Technical Report CU-CS-665-93, </type> <institution> University of Colorado at Boulder, Dept. of Computer Science, Boulder, Colorado, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: Using information from past memory requests, it chooses which free block of memory to allocate for the current request such that future requests will be able to be satisfied with a minimal amount 29 The research done by Zorn <ref> [DDZ93, ZG92, Vo95] </ref> was the only work we found that used actual programs in their studies. They have made these programs, many of which we used, available by anonymous ftp. <p> (Our suite does include one almost non-freeing program, LRUsim, which is the only non-freeing program we had that we were sure did not leak. 45 We do not believe that this is a large problem, 42 Two programs used by Zorn and Grunwald [ZG92] and by Detlefs, Dosser, and Zorn <ref> [DDZ93] </ref> have heaps that are quite small|Cfrac only uses 21.4 KB and Gawk only uses 41 KB, which are only a few pages on most modern machines.
Reference: [Det90a] <author> David L. Detlefs. </author> <title> Concurrent garbage collection for C++. </title> <type> Technical Report CMU-CS-90-119, </type> <institution> Carnegie-Mellon University, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: In short, Baker's scheme will unpredictably suffer from unacceptably long garbage collection slowdowns. This problem is even worse in recent collectors which use page-wise virtual memory protection to trigger larger increments of collector work <ref> [AEL88, Det90a, Joh92, BDS91] </ref>, and is also significant on Lisp-machine style hardware. Even if the necessary checks are performed by dedicated parallel hardware, most of the available CPU time may be used up (in the worst case) by the actual trapping to copying routines and the copying itself.
Reference: [DeT90b] <author> John DeTreville. </author> <title> Experience with concurrent garbage collectors for Modula-2+. </title> <type> Technical Re port 64, </type> <institution> Digital Equipment Corporation Systems Research Center, Palo Alto, California, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: In most programs in a variety of languages, most objects live a very short time, while a small percentage of them live much longer <ref> [LH83, Ung84, Sha88, Zor90, DeT90b, Hay91] </ref>.
Reference: [DLM + 78] <author> Edsger W. Dijkstra, Leslie Lamport, A. J. Martin, C. S. Scholten, and E. F. M. Steffens. </author> <title> On-the fly garbage collection: An exercise in cooperation. </title> <journal> Communications of the ACM, </journal> <volume> 21(11) </volume> <pages> 966-975, </pages> <month> November </month> <year> 1978. </year>
Reference-contexts: This model is based on tricolor marking <ref> [DLM + 78] </ref> and is augmented with the key idea that garbage collection is really the process of marking objects and moving them from one set to another [Bak91]. In addition, this model uses two important invariants that allow us address the issues of consistency and conservatism in incremental collection. <p> For this reason, discussions of incremental collectors typically refer to the running program as 9 the mutator <ref> [DLM + 78] </ref>. An incremental scheme must have some way of keeping track of the changes to the graph of reachable objects, perhaps re-computing parts of its traversal in the face of those changes. <p> details of the traversal algorithm. (In parallel and distributed garbage collection, a relaxed consistency model also allows more parallelism and/or less synchronization, but that is beyond the scope of this dissertation.) 5.3 Tricolor Marking The abstraction of tricolor marking is helpful in understanding coherence and conservatism in incremental garbage collection <ref> [DLM + 78] </ref>. Garbage collection algorithms can be conceptually described as a process of traversing the graph of reachable objects and coloring them. The objects are originally colored white, and as the graph is traversed, they are colored black. <p> The tricolor invariant takes on two forms which we now define: 1. The strong tricolor invariant: For all black objects in the graph which have a path to a white object, all paths from that black object to that white object will contain at least one gray object <ref> [DLM + 78] </ref>. 2. The weak tricolor invariant: For all black objects in the graph which have a path to a white object, at least one path from that black object to that white object will contain at least one gray object [Yua90]. <p> When such a pointer is created, the collector is notified 17 so that it can either trace the pointed-to object immediately, or re-examine the location in which the pointer was stored again later to find any "hidden" objects <ref> [Ste75, DLM + 78, BDS91] </ref>. For example, in Figure 2 step 2, when the pointer from object A to object E is created, a pointer to this pointer is recorded, and object E would be traversed when this pointer is reexamined.
Reference: [Ede92] <author> Daniel Ross Edelson. </author> <title> Smart pointers: They're smart, but they're not pointers. </title> <booktitle> In USENIX C++ Conference [USE92], </booktitle> <pages> pages 1-19. </pages> <note> Technical Report UCSC-CRL-92-27, </note> <institution> University of California at Santa Cruz, Baskin Center for Computer Engineering and Information Sciences, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: this interface, garbage collected objects have an associated pointer type defined in a library as a parameterized class, and client code must use these pointers rather than raw C++ pointers. (Parameterization and operator overloading make this relatively easy, although smart pointers cannot be used quite as flexibly as raw pointers <ref> [Ede92] </ref>.) The main difference between our parameterized pointers and normal pointers is that pointer assignments execute an additional few lines of code, which constitute the write barrier. In our system, each object has a hidden header field, created by our overloaded version of the C++ new operator.
Reference: [EV91] <author> Steven Engelstad and Jim Vandendorp. </author> <title> Automatic storage management for systems with real time constraints. In OOPSLA '91 Workshop on Garbage Collection in Object-Oriented Systems [OOP91]. </title> <note> Position paper. </note>
Reference-contexts: This read barrier cost is potentially high, and very unpredictable, because the cost of traversing an ordinary list is strongly dependent on whether or not the list has already been reached and copied by the collector <ref> [Nil88, EV91, Wit91] </ref>. In addition, Baker's algorithm will systematically have unacceptably poor performance at the beginning of a garbage collection cycle, because referencing any object will trigger an increment of copying. It is, in general, very difficult to predict when a garbage collection cycle will begin.
Reference: [FY69] <author> Robert R. Fenichel and Jerome C. Yochelson. </author> <title> A LISP garbage-collector for virtual-memory com puter systems. </title> <journal> Communications of the ACM, </journal> <volume> 12(11) </volume> <pages> 611-612, </pages> <month> November </month> <year> 1969. </year>
Reference-contexts: In a mark-sweep collector, once the live objects have been distinguished from the garbage objects, memory is exhaustively examined (swept) to find all of the garbage objects, and their space is reclaimed. An example of the second method is copying collection <ref> [FY69, Che70] </ref>. In a copy collector, the live objects are copied out of one area of memory and into another. Once all live objects have been copied out of the original memory area, the entire area is considered to be garbage and can be reclaimed in one operation.
Reference: [Han90] <author> David R. Hanson. </author> <title> Fast allocation and deallocation of memory based on object lifetimes. </title> <journal> Software Practice and Experience, </journal> <volume> 20(1), </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: equivalent malloc () and free () calls. 47 The input data for the compilation was the the largest source file of the compiler itself (combine.c). 48 46 Obstacks are an extension to the C language optimized for allocating and deallocating objects in stack-like ways. (A similar scheme is described in <ref> [Han90] </ref>.) 47 It is our belief that we should study the behavior of the program without hand-optimized memory allocation, because a well-designed allocator should usually be able to do as well or better than most programmers hand-optimizations.
Reference: [Hay91] <author> Barry Hayes. </author> <title> Using key object opportunism to collect old objects. </title> <booktitle> In Paepcke [Pae91], </booktitle> <pages> pages 33-46. </pages>
Reference-contexts: In most programs in a variety of languages, most objects live a very short time, while a small percentage of them live much longer <ref> [LH83, Ung84, Sha88, Zor90, DeT90b, Hay91] </ref>.
Reference: [Joh92] <author> Ralph E. Johnson. </author> <title> Reducing the latency of a real-time garbage collector. </title> <journal> ACM Letters on Pro gramming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 46-58, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In short, Baker's scheme will unpredictably suffer from unacceptably long garbage collection slowdowns. This problem is even worse in recent collectors which use page-wise virtual memory protection to trigger larger increments of collector work <ref> [AEL88, Det90a, Joh92, BDS91] </ref>, and is also significant on Lisp-machine style hardware. Even if the necessary checks are performed by dedicated parallel hardware, most of the available CPU time may be used up (in the worst case) by the actual trapping to copying routines and the copying itself.
Reference: [Kno65] <author> Kenneth C. Knowlton. </author> <title> A fast storage allocator. </title> <journal> Communications of the ACM, </journal> <volume> 8(10) </volume> <pages> 623-625, </pages> <month> October </month> <year> 1965. </year>
Reference-contexts: The policy is therefore a good fit or even a best fit policy, despite the fact that it's usually described as a variation on first fit. A.2 Buddy Systems Buddy systems <ref> [Kno65, PN77] </ref> are a variant of segregated lists that support a limited but efficient kind of splitting and coalescing. In the simple buddy schemes, the entire heap area is conceptually split into two large areas, and those areas are further split into two smaller areas, and so on. <p> Several significant variations on buddy systems have been devised: A.2.1 Binary buddies. Binary buddies are the simplest and best-known kind of buddy system <ref> [Kno65] </ref>. In this scheme, all buddy sizes are a power of two, and each size is divided into two equal parts.
Reference: [Knu73] <author> Donald E. Knuth. </author> <title> The Art of Computer Programming, volume 1: Fundamental Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1973. </year> <note> First edition published in 1968. </note>
Reference-contexts: This contrasts with traditional simulation results, where best fit usually performs well but is sometimes outperformed by next fit (e.g., in Knuth's small but influential study <ref> [Knu73] </ref>). In terms of practical application, we believe this is one of our most significant findings. <p> A.3 Sequential Fits Several classic allocator algorithms are based on having a single doubly-linked linear (or circularly linked) list of all free blocks of memory. Typically, sequential fit algorithms use Knuth's boundary tag technique to support coalescing of all adjacent free areas <ref> [Knu73] </ref>. A.3.1 First-fit A first fit policy simply searches the list from the beginning, and uses the first free block large enough to satisfy the request. If the block is larger than necessary, it is split and the remainder is put on the free list. <p> This exhaustive search means that a sequential best-fit search does not scale well to large heaps with many free blocks. A.3.4 Boundary Tags and Per-Object Overheads Sequential fit techniques are usually implemented using boundary tags to support the coalescing of free areas <ref> [Knu73] </ref>. Each block of memory has a header and a footer field, both of which record the size of the block and whether it is in use.
Reference: [LFP84] <institution> Conference Record of the 1984 ACM Symposium on LISP and Functional Programming, Austin, Texas, </institution> <month> August </month> <year> 1984. </year> <note> ACM Press. </note>
Reference: [LFP90] <institution> Conference Record of the 1990 ACM Symposium on LISP and Functional Programming, </institution> <address> Nice, France, June 1990. </address> <publisher> ACM Press. </publisher>
Reference: [LH83] <author> Henry Lieberman and Carl Hewitt. </author> <title> A real-time garbage collector based on the lifetimes of objects. </title> <journal> Communications of the ACM, </journal> <volume> 26(6) </volume> <pages> 419-429, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: The minority of objects that survive for a longer period are made exempt from most garbage collection cycles so that they may have more time to die before again being considered for garbage collection <ref> [LH83, Moo84, Ung84, Wil92] </ref>. Because generational techniques rely on a heuristic|the guess that most objects will die young, and that older objects wont die soon-they are not strictly reliable, and may degrade collector performance in the 4 worst case. Thus, for some purely hard real-time systems, they are not attractive. <p> In most programs in a variety of languages, most objects live a very short time, while a small percentage of them live much longer <ref> [LH83, Ung84, Sha88, Zor90, DeT90b, Hay91] </ref>. <p> These objects are processed at every collection, over and over, and the garbage collector spends most of its time processing the same old objects repeatedly. This is the major source of inefficiency in simple garbage collectors. Generational collection <ref> [LH83] </ref> avoids much of this repeated processing by segregating objects into multiple areas by age, and collecting areas containing older objects less often than the younger ones. Once objects have survived a small number of collections, they are "moved" to a less frequently collected area. <p> The minority of objects that survive for a longer period are made exempt from most garbage collection cycles so that they may have more time to die before again being considered for garbage collection <ref> [LH83, Moo84, Ung84, Wil92] </ref>. Because generational techniques rely on a heuristic|the guess that most objects will die young, and that older objects wont die soon-they are not strictly reliable, and may degrade collector performance in the worst case. For purely hard real-time systems, therefore, they may not be attractive.
Reference: [McC60] <author> John McCarthy. </author> <title> Recursive functions of symbolic expressions and their computation by machine. </title> <journal> Communications of the ACM, </journal> <volume> 3(4) </volume> <pages> 184-195, </pages> <month> April </month> <year> 1960. </year>
Reference-contexts: Find and reclaim all objects known to be garbage (explicit garbage reclamation). 2. Find and preserve all objects known to be live. All objects left over are garbage and can be reclaimed in one action (implicit garbage reclamation). An example of the first method is mark-sweep collection <ref> [McC60] </ref>. In a mark-sweep collector, once the live objects have been distinguished from the garbage objects, memory is exhaustively examined (swept) to find all of the garbage objects, and their space is reclaimed. An example of the second method is copying collection [FY69, Che70].
Reference: [Moo84] <author> David Moon. </author> <title> Garbage collection in a large Lisp system. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on LISP and Functional Programming [LFP84], </booktitle> <pages> pages 235-246. </pages>
Reference-contexts: The minority of objects that survive for a longer period are made exempt from most garbage collection cycles so that they may have more time to die before again being considered for garbage collection <ref> [LH83, Moo84, Ung84, Wil92] </ref>. Because generational techniques rely on a heuristic|the guess that most objects will die young, and that older objects wont die soon-they are not strictly reliable, and may degrade collector performance in the 4 worst case. Thus, for some purely hard real-time systems, they are not attractive. <p> The minority of objects that survive for a longer period are made exempt from most garbage collection cycles so that they may have more time to die before again being considered for garbage collection <ref> [LH83, Moo84, Ung84, Wil92] </ref>. Because generational techniques rely on a heuristic|the guess that most objects will die young, and that older objects wont die soon-they are not strictly reliable, and may degrade collector performance in the worst case. For purely hard real-time systems, therefore, they may not be attractive.
Reference: [Nil88] <author> Kelvin Nilsen. </author> <title> Garbage collection of strings and linked data structures in real time. </title> <journal> Software Practice and Experience, </journal> <volume> 18(7) </volume> <pages> 613-640, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: This read barrier cost is potentially high, and very unpredictable, because the cost of traversing an ordinary list is strongly dependent on whether or not the list has already been reached and copied by the collector <ref> [Nil88, EV91, Wit91] </ref>. In addition, Baker's algorithm will systematically have unacceptably poor performance at the beginning of a garbage collection cycle, because referencing any object will trigger an increment of copying. It is, in general, very difficult to predict when a garbage collection cycle will begin.
Reference: [NOPH92] <author> Scott Nettles, James O'Toole, David Pierce, and Nicholas Haines. </author> <title> Replication-based incremental copying collection. </title> <booktitle> In Bekkers and Cohen [BC92], </booktitle> <pages> pages 357-364. </pages>
Reference-contexts: While this in general doubles the cost of reading from and writing to all heap allocated objects, the costs become predictable so that hard real-time assertions can be made. 3. Write-barrier only techniques <ref> [NOPH92] </ref> provide coordination between the application and the garbage collector by maintaining two separate views of the heap: one for the garbage collector, and one for the application. The application sees the original versions of all objects until the very end of the garbage collection cycle. <p> Others have proposed copying based algorithms that rely on a combination of a read barrier and a write barrier [Bro84], or a write barrier only <ref> [NOPH92] </ref> to coordinate the collector's view of the graph with that of the application. <p> 8 Nilsen gives no indication of the parameters used in timing this worst-case, so it is impossible to evaluate this pause. 12 It may be enlightening to view these issues as a variety of coherence problems|having multiple processes attempt to share changing data, while maintaining some kind of consistent view <ref> [NOPH92] </ref>. (Readers unfamiliar with coherence problems in parallel systems should not worry too much about this terminology; the issues should become apparent as we go along.) An incremental mark-sweep traversal poses a multiple readers, single writer coherence problem|the col-lector's traversal must respond to changes, but only the mutator can change the
Reference: [NR95] <author> Bob Nimon and John Radford. </author> <title> Implementing a real-time, embedded, telecommunications switching system in smalltalk, </title> <month> October </month> <year> 1995. </year> <note> experience report at OOPSLA 95. </note>
Reference-contexts: While using these languages has made for easier computation of real-time bounds, the lack of garbage collection has caused modularity, abstraction, and design to suffer. Some real-time programmers are finding tremendous increases in productivity by moving to higher level languages, such as Smalltalk, for real-time development <ref> [NR95] </ref>. Unfortunately, in order to guarantee real-time responsiveness, these programmers often find it necessary to "defeat the garbage collector" by statically allocating all objects that will ever be live in their application.
Reference: [NS90] <author> Kelvin Nilsen and William J. Schmidt. </author> <title> A high-level overview of hardware assisted real-time garbage collection. </title> <type> Technical Report TR 90-18a, </type> <institution> Dept. of Computer Science, Iowa State University, Ames, Iowa, </institution> <year> 1990. </year>
Reference-contexts: Even if the necessary checks are performed by dedicated parallel hardware, most of the available CPU time may be used up (in the worst case) by the actual trapping to copying routines and the copying itself. Nilsen and Schmidt <ref> [NS90] </ref> argue that even if increments of garbage collection work are small, a real-time program may miss its deadlines if too many small increments add up to too much total overhead over some period of time relevant to a deadline. <p> Nilsen's proposed solution to this problem is to build special hardware that guarantees that the worst-case delay for any individual program operation is small relative to that operation's normal execution time <ref> [NS90] </ref>.
Reference: [OOP89] <editor> Conference on Object Oriented Programming Systems, </editor> <booktitle> Languages and Applications (OOPSLA '89) Proceedings, </booktitle> <address> New Orleans, Louisiana, 1989. </address> <publisher> ACM Press. </publisher> <pages> 46 </pages>
Reference: [OOP91] <institution> OOPSLA '91 Workshop on Garbage Collection in Object-Oriented Systems, </institution> <month> October </month> <year> 1991. </year> <note> Avail able for anonymous FTP from cs.utexas.edu in /pub/garbage/GC91. </note>
Reference: [OOP93] <institution> OOPSLA '93 Workshop on Memory Management and Garbage Collection, </institution> <month> October </month> <year> 1993. </year> <note> Available for anonymous FTP from cs.utexas.edu in /pub/garbage/GC93. </note>
Reference: [Pae91] <editor> Andreas Paepcke, editor. </editor> <booktitle> Conference on Object Oriented Programming Systems, Languages and Applications (OOPSLA '91), </booktitle> <address> Phoenix, Arizona, </address> <month> October </month> <year> 1991. </year> <note> ACM Press. Published as SIGPLAN Notices 26(11), </note> <month> November </month> <year> 1991. </year>
Reference: [PH86] <author> Ivor P. Page and Jeff Hagins. </author> <title> Improving the performance of buddy systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(5):441-447, </volume> <month> May </month> <year> 1986. </year>
Reference-contexts: A.2.2 Double buddies. Double buddy systems <ref> [Wis78, PH86] </ref> use a different technique to allow a closer spacing of size classes. They use two different buddy systems, with staggered sizes. <p> to the rules of that series. (This complicates the treatment of large objects.) To our knowledge, the implementation we built for the present study may actually be the only double buddy system in existence, though Page wrote a simulator that is almost an entire implementation of a double buddy allocator <ref> [PH86] </ref>. A.3 Sequential Fits Several classic allocator algorithms are based on having a single doubly-linked linear (or circularly linked) list of all free blocks of memory. Typically, sequential fit algorithms use Knuth's boundary tag technique to support coalescing of all adjacent free areas [Knu73].
Reference: [PLD88] <institution> Proceedings of the 1988 SIGPLAN Conference on Programming Language Design and Implemen tation, Atlanta, Georgia, </institution> <address> June 1988. </address> <publisher> ACM Press. </publisher>
Reference: [PLD91] <institution> Proceedings of the 1991 SIGPLAN Conference on Programming Language Design and Implemen tation, Toronto, </institution> <address> Ontario, </address> <month> June </month> <year> 1991. </year> <note> ACM Press. Published as SIGPLAN Notices 26(6), </note> <month> June </month> <year> 1992. </year>
Reference: [PN77] <author> J. L. Peterson and T. A. Norman. </author> <title> Buddy systems. </title> <journal> Communications of the ACM, </journal> <volume> 20(6) </volume> <pages> 421-431, </pages> <month> June </month> <year> 1977. </year>
Reference-contexts: The policy is therefore a good fit or even a best fit policy, despite the fact that it's usually described as a variation on first fit. A.2 Buddy Systems Buddy systems <ref> [Kno65, PN77] </ref> are a variant of segregated lists that support a limited but efficient kind of splitting and coalescing. In the simple buddy schemes, the entire heap area is conceptually split into two large areas, and those areas are further split into two smaller areas, and so on.
Reference: [PSC71] <author> P. W. Purdom, S. M. Stigler, and Tat-Ong Cheam. </author> <title> Statistical investigation of three storage allocation algorithms. </title> <journal> BIT, </journal> <volume> 11 </volume> <pages> 187-195, </pages> <year> 1971. </year>
Reference-contexts: It is a "segregated storage" algorithm in the (rather misleading) sense of Purdom, Stigler, and Cheam <ref> [PSC71] </ref>. Actual storage is not segregated, and one-word header and footer fields support boundary-tag coalescing. A set of free lists is maintained, "segregating" (indexing) free objects by approximate size so as to speed up searches.
Reference: [RK68] <author> Brian Randell and C. J. Kuehner. </author> <title> Dynamic storage allocation systems. </title> <journal> Communications of the ACM, </journal> <volume> 12(7) </volume> <pages> 297-306, </pages> <month> May </month> <year> 1968. </year>
Reference-contexts: Traditionally, fragmentation is classified as external or internal <ref> [RK68] </ref>, and is combatted by splitting and coalescing free blocks. External fragmentation arises when free blocks of memory are available for allocation, but cannot be used to hold objects of the sizes actually requested by a program.
Reference: [SES84] <institution> ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments. ACM Press, </institution> <month> April </month> <year> 1984. </year> <note> Published as ACM SIGPLAN Notices 19(5), </note> <month> May, </month> <year> 1987. </year>
Reference: [Sha88] <author> Robert A. Shaw. </author> <title> Empirical Analysis of a Lisp System. </title> <type> PhD thesis, </type> <institution> Stanford University, Palo Alto, California, </institution> <month> February </month> <year> 1988. </year> <type> Technical Report CSL-TR-88-351, </type> <institution> Stanford University Computer Systems Laboratory. </institution>
Reference-contexts: In most programs in a variety of languages, most objects live a very short time, while a small percentage of them live much longer <ref> [LH83, Ung84, Sha88, Zor90, DeT90b, Hay91] </ref>.
Reference: [SKW92] <author> Vivek Singhal, Sheetal V. Kakkad, and Paul R. Wilson. </author> <title> Texas: an efficient, portable persistent store. </title> <booktitle> In Albano and Morrison [AM92], </booktitle> <pages> pages 11-33. </pages>
Reference-contexts: Minimum object size is 8 bytes. (Originally written by Sheetal Kakkad for use in the Texas Persistent Store <ref> [SKW92] </ref>, but very similar to the widely used and venerable BSD UNIX allocator by Chris Kingsley and studied by Zorn and Grunwald.) * SegrStor2:3 is very similar to SegrStorBinary, but the size classes are closer together, to decrease internal fragmentation at a possible expense in external fragmentation.
Reference: [Ste75] <author> Guy L. Steele Jr. </author> <title> Multiprocessing compactifying garbage collection. </title> <journal> Communications of the ACM, </journal> <volume> 18(9) </volume> <pages> 495-508, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: When such a pointer is created, the collector is notified 17 so that it can either trace the pointed-to object immediately, or re-examine the location in which the pointer was stored again later to find any "hidden" objects <ref> [Ste75, DLM + 78, BDS91] </ref>. For example, in Figure 2 step 2, when the pointer from object A to object E is created, a pointer to this pointer is recorded, and object E would be traversed when this pointer is reexamined.
Reference: [Str92] <author> Bjarne Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: Even though any given pause caused by the collector is short and strictly bounded, these pauses may be clustered closely together, causing the application to miss its larger granularity deadlines. 7 We use the smart pointer idiom <ref> [Str92] </ref> to provide the necessary support for garbage collection. 11 Baker uses a read barrier (special code potentially executed at every pointer reference) to maintain consistency between the running program and the garbage collector. Thus, every reference to a pointer potentially causes an increment of garbage collection to be performed. <p> The user is free to set the number of bytes allocated per increment of collection as needed in order to meet the real-time bounds. 25 5.12 Interface to C++ Currently, our collector uses a smart pointer interface to collect C++ <ref> [Str92] </ref>.
Reference: [Ung84] <author> David M. Ungar. </author> <title> Generation scavenging: A non-disruptive high-performance storage reclamation algorithm. </title> <booktitle> In ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments [SES84], </booktitle> <pages> pages 157-167. </pages> <note> Published as ACM SIGPLAN Notices 19(5), </note> <month> May, </month> <year> 1987. </year>
Reference-contexts: The minority of objects that survive for a longer period are made exempt from most garbage collection cycles so that they may have more time to die before again being considered for garbage collection <ref> [LH83, Moo84, Ung84, Wil92] </ref>. Because generational techniques rely on a heuristic|the guess that most objects will die young, and that older objects wont die soon-they are not strictly reliable, and may degrade collector performance in the 4 worst case. Thus, for some purely hard real-time systems, they are not attractive. <p> In most programs in a variety of languages, most objects live a very short time, while a small percentage of them live much longer <ref> [LH83, Ung84, Sha88, Zor90, DeT90b, Hay91] </ref>. <p> This reduces the frequency of disruptive pauses, and for many programs without real-time deadlines, this is sufficient for acceptable interactive use. The majority of pauses are so brief (a fraction of a second) that they are unlikely to be noticed by users <ref> [Ung84] </ref>; the longer pauses for multi-generation collections can often be postponed until the system is not in use, or hidden within non-interactive compute-bound phases of program operation [WM89]. Generational techniques are often used as an acceptable substitute for more expensive incremental techniques, as well as to improve overall efficiency. <p> The minority of objects that survive for a longer period are made exempt from most garbage collection cycles so that they may have more time to die before again being considered for garbage collection <ref> [LH83, Moo84, Ung84, Wil92] </ref>. Because generational techniques rely on a heuristic|the guess that most objects will die young, and that older objects wont die soon-they are not strictly reliable, and may degrade collector performance in the worst case. For purely hard real-time systems, therefore, they may not be attractive.
Reference: [USE92] <editor> USENIX Association. </editor> <booktitle> USENIX C++ Conference, </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1992. </year>
Reference: [Vo95] <author> Kiem-Phong Vo. </author> <title> Vmalloc: A general and efficient memory allocator. </title> <journal> Software Practice and Experience, </journal> <note> 1995. To appear. </note>
Reference-contexts: Using information from past memory requests, it chooses which free block of memory to allocate for the current request such that future requests will be able to be satisfied with a minimal amount 29 The research done by Zorn <ref> [DDZ93, ZG92, Vo95] </ref> was the only work we found that used actual programs in their studies. They have made these programs, many of which we used, available by anonymous ftp.
Reference: [Wan89] <author> Thomas Wang. </author> <title> MM garbage collector for C++. </title> <type> Master's thesis, </type> <institution> California Polytechnic State University, </institution> <address> San Luis Obispo, California, </address> <month> October </month> <year> 1989. </year>
Reference-contexts: This approach is attractive in situations where 1 Note that non-copying collection need not incur the cost of the sweep phase of a mark-sweep collector as is commonly assumed. In Section 5.7.1 we explain a technique known as "fake copying" <ref> [Wan89] </ref> (also known as "implicit reclamation" [Bak91]) which avoids the cost of a sweep phase. 2 Conservative pointer finding techniques are subject to erroneously keeping objects (and hence entire data structures) live by finding an integer that happens to look like a pointer into the heap. <p> The garbage objects are never examined, and their space is implicitly reclaimed. While at first these two methods of reclaiming garbage memory may seem fundamentally different, there is a way to combine them to receive many of the advantages of both <ref> [Wan89, Bak91] </ref>. This "fake copying" approach is fundamental to our real-time garbage collector implementation, and we will discuss it in detail in Section 5.5. 4.1 Real-time Garbage Collection For truly real-time applications, fine-grained incremental garbage collection appears to be necessary. <p> the test for an incremental update write barrier can be sped up considerably at the cost of increased conservatism by always assuming that any pointer store will violate the write barrier and optimistically shading the R-value without checking the color of the L-value. 5.5 Non-Copying, Incremental Read Barrier Techniques Wang <ref> [Wan89] </ref> and Baker [Bak91] independently presented a critical insight that can be used to make a mark-sweep collector have many of the advantages of a copying collector. Their insight was that in a copying collector, the "spaces" of the collector are really just a particular implementation of sets.
Reference: [Wil88] <author> Paul R. Wilson. </author> <title> Opportunistic garbage collection. </title> <journal> SIGPLAN Notices, </journal> <volume> 23(12) </volume> <pages> 98-102, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: is associated with the garbage collection cycle of the younger generation in 28 While it is not clear how many generations a collector should have, two generations appears to give good results because it gives many of the advantages of generational collection without too many repeated traversals of the objects <ref> [Wil88] </ref>. 32 which the IGP was created. So, if an object can remain in the younger generation for, say, at most three complete garbage collection cycles, then there will be three IGP lists.
Reference: [Wil92] <author> Paul R. Wilson. </author> <title> Uniprocessor garbage collection techniques. </title> <booktitle> In Bekkers and Cohen [BC92], </booktitle> <pages> pages 1-42. </pages>
Reference-contexts: The minority of objects that survive for a longer period are made exempt from most garbage collection cycles so that they may have more time to die before again being considered for garbage collection <ref> [LH83, Moo84, Ung84, Wil92] </ref>. Because generational techniques rely on a heuristic|the guess that most objects will die young, and that older objects wont die soon-they are not strictly reliable, and may degrade collector performance in the 4 worst case. Thus, for some purely hard real-time systems, they are not attractive. <p> In addition, if a newly allocated object becomes garbage before the end of this collection cycle, it cannot be reclaimed before the end of the next garbage collection cycle. 5.4 Incremental Tracing Algorithms Two basic incremental tracing strategies are possible <ref> [Wil92] </ref>. One strategy is to ensure that objects can never get lost, by preventing any pointers from being destroyed [AP87, Yua90]. Before overwriting a pointer, the old pointer value is immediately traversed, or saved away so that the collector can still find it and trace it later. <p> The minority of objects that survive for a longer period are made exempt from most garbage collection cycles so that they may have more time to die before again being considered for garbage collection <ref> [LH83, Moo84, Ung84, Wil92] </ref>. Because generational techniques rely on a heuristic|the guess that most objects will die young, and that older objects wont die soon-they are not strictly reliable, and may degrade collector performance in the worst case. For purely hard real-time systems, therefore, they may not be attractive.
Reference: [Wil95] <author> Paul R. Wilson. </author> <title> Garbage collection. </title> <journal> Computing Surveys, </journal> <note> 1995. Expanded version of [Wil92]. Draft available via anonymous internet FTP from cs.utexas.edu as pub/garbage/bigsurv.ps. In revision, to appear. </note>
Reference-contexts: In practice, these two phases may be functionally or temporally interleaved, and the reclamation technique is strongly dependent on the garbage detection technique. 6 This paper assumes some background in garbage collection techniques. For an excellent survey of basic garbage collection techniques the interested reader should see <ref> [Wil95] </ref> . 8 In general, garbage collectors use a liveness criterion that is somewhat more conservative than those used by other systems.
Reference: [Wis78] <author> David S. Wise. </author> <title> The double buddy-system. </title> <type> Technical Report 79, </type> <institution> Computer Science Department, Indiana University, Bloomington, Indiana, </institution> <month> December </month> <year> 1978. </year>
Reference-contexts: A.2.2 Double buddies. Double buddy systems <ref> [Wis78, PH86] </ref> use a different technique to allow a closer spacing of size classes. They use two different buddy systems, with staggered sizes.
Reference: [Wit91] <author> P. T. Withington. </author> <title> How real is "real time" garbage collection? In OOPSLA '91 Workshop on Garbage Collection in Object-Oriented Systems [OOP91]. </title> <note> Position paper. </note>
Reference-contexts: This read barrier cost is potentially high, and very unpredictable, because the cost of traversing an ordinary list is strongly dependent on whether or not the list has already been reached and copied by the collector <ref> [Nil88, EV91, Wit91] </ref>. In addition, Baker's algorithm will systematically have unacceptably poor performance at the beginning of a garbage collection cycle, because referencing any object will trigger an increment of copying. It is, in general, very difficult to predict when a garbage collection cycle will begin.
Reference: [WJ93] <author> Paul R. Wilson and Mark S. Johnstone. </author> <title> Truly real-time non-copying garbage collection. In OOP SLA '93 Workshop on Memory Management and Garbage Collection [OOP93]. </title> <note> Expanded version of workshop position paper submitted for publication. </note>
Reference-contexts: write barrier essentially records very similar information to that of an incremental update write barrier, so most of the overhead should be able to serve both purposes. 6.1 Discussion Generational collection can be combined with real-time techniques, but in the general case, the marriage is not a particularly happy one <ref> [WJ93] </ref>. Typically, generational techniques improve expected performance at the expense of worst-case performance, while real-time garbage collection is oriented toward providing absolute worst case guarantees.
Reference: [WJNB95] <author> Paul R. Wilson, Mark S. Johnstone, Michael Neely, and David Boles. </author> <title> Dynamic storage allocation: A survey and critical review. </title> <booktitle> In 1995 International Workshop on Memory Management, </booktitle> <address> Kinross, Scotland, UK, 1995. </address> <publisher> Springer Verlag LNCS. </publisher>
Reference-contexts: If a page is empty, it can be made available for allocating objects in a different size class, preserving the invariant that all objects in a page are of a single size class. 35 For a much more extensive discussion on these issues, see <ref> [WJNB95] </ref> 36 A simple enhancement can support splitting: when a free list for a requested size is empty, a suitable larger block may be found by searching free lists for larger sizes.
Reference: [WM89] <author> Paul R. Wilson and Thomas G. Moher. </author> <title> Design of the Opportunistic Garbage Collector. </title> <booktitle> In Conference on Object Oriented Programming Systems, Languages and Applications (OOPSLA '89) Proceedings [OOP89], </booktitle> <pages> pages 23-35. </pages>
Reference-contexts: The majority of pauses are so brief (a fraction of a second) that they are unlikely to be noticed by users [Ung84]; the longer pauses for multi-generation collections can often be postponed until the system is not in use, or hidden within non-interactive compute-bound phases of program operation <ref> [WM89] </ref>. Generational techniques are often used as an acceptable substitute for more expensive incremental techniques, as well as to improve overall efficiency. However, they are not sufficient for hard real-time applications.
Reference: [Yua90] <author> Taichi Yuasa. </author> <title> Real-time garbage collection on general-purpose machines. </title> <journal> Journal of Systems and Software, </journal> <volume> 11 </volume> <pages> 181-198, </pages> <year> 1990. </year>
Reference-contexts: The weak tricolor invariant: For all black objects in the graph which have a path to a white object, at least one path from that black object to that white object will contain at least one gray object <ref> [Yua90] </ref>. <p> One strategy is to ensure that objects can never get lost, by preventing any pointers from being destroyed <ref> [AP87, Yua90] </ref>. Before overwriting a pointer, the old pointer value is immediately traversed, or saved away so that the collector can still find it and trace it later. We call this a snapshot-at-beginning algorithm because the collector's view of reachable data structures is fixed when collection begins.
Reference: [ZG92] <author> Benjamin Zorn and Dirk Grunwald. </author> <title> Empirical measurements of six allocation-intensive C pro grams. </title> <type> Technical Report CU-CS-604-92, </type> <institution> University of Colorado at Boulder, Dept. of Computer Science, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: Using information from past memory requests, it chooses which free block of memory to allocate for the current request such that future requests will be able to be satisfied with a minimal amount 29 The research done by Zorn <ref> [DDZ93, ZG92, Vo95] </ref> was the only work we found that used actual programs in their studies. They have made these programs, many of which we used, available by anonymous ftp. <p> from allocators which have no header overheads. (Our suite does include one almost non-freeing program, LRUsim, which is the only non-freeing program we had that we were sure did not leak. 45 We do not believe that this is a large problem, 42 Two programs used by Zorn and Grunwald <ref> [ZG92] </ref> and by Detlefs, Dosser, and Zorn [DDZ93] have heaps that are quite small|Cfrac only uses 21.4 KB and Gawk only uses 41 KB, which are only a few pages on most modern machines.
Reference: [ZG94] <author> Benjamin Zorn and Dirk Grunwald. </author> <title> Evaluating models of memory allocation. </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> 1(4) </volume> <pages> 107-131, </pages> <year> 1994. </year>
Reference: [Zor90] <author> Benjamin Zorn. </author> <title> Comparing mark-and-sweep and stop-and-copy garbage collection. </title> <booktitle> In Conference Record of the 1990 ACM Symposium on LISP and Functional Programming [LFP90], </booktitle> <pages> pages 87-98. </pages>
Reference-contexts: In most programs in a variety of languages, most objects live a very short time, while a small percentage of them live much longer <ref> [LH83, Ung84, Sha88, Zor90, DeT90b, Hay91] </ref>.
Reference: [Zor93] <author> Benjamin Zorn. </author> <title> The measured cost of conservative garbage collection. </title> <journal> Software|Practice and Experience, </journal> <volume> 23(7) </volume> <pages> 733-756, </pages> <month> July </month> <year> 1993. </year> <month> 48 </month>
Reference-contexts: Some support for this idea comes from <ref> [Zor93] </ref>, which showed that hand optimizations usually do little good compared to choosing the right allocator. 48 Because of the way the GNU C compiler is distributed, this is in fact the most common workload|people frequently down-load a new version of the compiler and compile it with an old version, then <p> a new version of the compiler and compile it with an old version, then recompile it with itself twice as a cross-check 43 * Ghost is GhostScript, a widely-used portable PostScript (page rendering language) interpreter written by Peter Deutsch, and modified by Zorn et al. to remove hand-optimized memory allocation <ref> [Zor93] </ref>. The input was manual.ps, the largest of the standard inputs available from Zorn's ftp site. <p> The input, adj.perl, formatted the words in a dictionary into filled paragraphs. Hand-optimized memory allocation was removed by Zorn <ref> [Zor93] </ref>. to ensure that the generated code does not change between self-compiles (i.e., it reaches a fixed point). 49 Note that this is not the same input set as used by Zorn et al. in their experiments; they used an unspecified combination of several programs.
References-found: 66

