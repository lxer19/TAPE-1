URL: http://www.cs.ucsd.edu/users/jenny/CCC97/CCC.ps
Refering-URL: http://www.cs.ucsd.edu/users/jenny/cv.html
Root-URL: http://www.cs.ucsd.edu
Email: jenny@cs.ucsd.edu  
Title: Cluster Computing Conference, 1997 Structural Prediction Models for High-Performance Distributed Applications  
Author: Jennifer M. Schopf 
Web: http://www.cs.ucsd.edu/users/jenny  
Address: San Diego  
Affiliation: Computer Science and Engineering Department University of California,  
Abstract: We present a structural performance model to predict an application's performance on a set of distributed resources. We decompose application performance in accordance with the structure of the application, that is, into interacting component models that correspond to component sub-tasks. Then, using the application profile and available information as guides, we select models for each component appropriately. This allows different modeling approaches for different application components as needed. As a proof-of-concept, we have implemented this approach for two distributed applications, a master-slave genetic algorithm code and a red-black stencil successive over-relaxation code. We achieve predictions within 10% while demonstrating the flexibility this framework allows. 
Abstract-found: 1
Intro-found: 1
Reference: [ASWB95] <author> Cosimo Anglano, Jennifer Schopf, Richard Wol-ski, and Fran Berman. Zoom: </author> <title> A hierarchical representation for heterogeneous applications. </title> <type> Technical Report #CS95-451, </type> <institution> University of Cal-ifornia, San Diego, Computer Science Department, </institution> <year> 1995. </year>
Reference-contexts: Then we will present a master-slave application to be used as an example throughout. 2.1 Application Characteristics Through conversations with developers <ref> [ASWB95] </ref>, we found several common characteristics for distributed parallel applications. A good prediction model should address these concerns, and we have developed our ap proach to do so. 1 Very coarse grain. <p> Often in describing an application a developer will use a graphical representation like a program dependency graph (PDG) [FOW84], a more detailed representation such as Zoom <ref> [ASWB95] </ref>, or even a visual programming language representation such as HeNCE [BDGM93], CODE 2.0 [NB92], Enterprise [SSLP93] or VPE [DN95]. From these graphical representations, defining a structural model is straightforward.
Reference: [BDGM93] <author> A. Beguelin, J. Dongarra, A. Geist, and R. Manchek. </author> <title> HeNCE: A heterogeneous network computing environment. </title> <journal> Scientific Programming, </journal> <volume> 3(1) </volume> <pages> 49-60, </pages> <year> 1993. </year>
Reference-contexts: Often in describing an application a developer will use a graphical representation like a program dependency graph (PDG) [FOW84], a more detailed representation such as Zoom [ASWB95], or even a visual programming language representation such as HeNCE <ref> [BDGM93] </ref>, CODE 2.0 [NB92], Enterprise [SSLP93] or VPE [DN95]. From these graphical representations, defining a structural model is straightforward. As a part of the AppLeS project [BW96, BWF + 96], we are building a library of structural templates to guide the structural model definition process.
Reference: [Bha96] <author> Karan Bhatia. </author> <type> Personal communication, </type> <year> 1996. </year>
Reference-contexts: As such, they are now being used by several groups of computational scientists [DTMH96, SK96, PM94] to address problems such as protein folding. Our distributed implementation uses a global population with synchronization between generations <ref> [Bha96] </ref>. It was written in C using PVM on a heterogeneous cluster of Sparc and RS6000 workstations in the Parallel Computation Laboratory (PCL) at UCSD. 5.1 Structural Model for GA This application is structured much as the master-slave example presented in Section 2.2. <p> For this application, because the networks were unloaded, the message sizes were small and several of the processors involved were slow, the application developer informed us that the majority of the computation time would be spent in the Slave i portion of the code <ref> [Bha96] </ref>. If needed, this information could be verified by implementing the code with timing routines.
Reference: [Bri87] <author> William L. Briggs. </author> <title> A Multigrid Tutorial. </title> <institution> Society for Industrial and Applied Mathematics, Lancaster Press, </institution> <year> 1987. </year>
Reference-contexts: It was written in C with PVM for the heterogeneous cluster of machines in the PCL as the GA code was. 6.1 Structural Model for SOR In our implementation, the application is divided into "red" and "black" phases, with communication and computation alternating for each <ref> [Bri87] </ref>. This repeats for a predefined number of iterations. The data decomposition for this code is depicted in Figure 5.
Reference: [BW96] <author> Fran Berman and Richard Wolski. </author> <title> Scheduling from the perspective of the application. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <year> 1996. </year>
Reference-contexts: One reason for this is the difficulty of predicting an application's execution time in this dynamic setting where only minimal information may be available. In particular, as a part of the AppLeS scheduling project <ref> [BW96, BWF + 96] </ref>, we are interested in estimating the performance of applications for use in dynamic scheduling. Because this environment combines varying non-dedicated resources and possibly dissimilar implementation paradigms, many established modeling techniques do not satisfy the needs of our scheduler. <p> From these graphical representations, defining a structural model is straightforward. As a part of the AppLeS project <ref> [BW96, BWF + 96] </ref>, we are building a library of structural templates to guide the structural model definition process. Certain classes of applications (master-slave, stencil, etc.) are common, and we believe that a library of common structural models will cover the most common applications.
Reference: [BWF + 96] <author> Fran Berman, Richard Wolski, Silvia Figueira, Jennifer Schopf, and Gary Shao. </author> <title> Application-level scheduling on distributed heterogeneous networks. </title> <booktitle> In Proceedings of SuperComputing '96, </booktitle> <year> 1996. </year>
Reference-contexts: One reason for this is the difficulty of predicting an application's execution time in this dynamic setting where only minimal information may be available. In particular, as a part of the AppLeS scheduling project <ref> [BW96, BWF + 96] </ref>, we are interested in estimating the performance of applications for use in dynamic scheduling. Because this environment combines varying non-dedicated resources and possibly dissimilar implementation paradigms, many established modeling techniques do not satisfy the needs of our scheduler. <p> From these graphical representations, defining a structural model is straightforward. As a part of the AppLeS project <ref> [BW96, BWF + 96] </ref>, we are building a library of structural templates to guide the structural model definition process. Certain classes of applications (master-slave, stencil, etc.) are common, and we believe that a library of common structural models will cover the most common applications.
Reference: [CGS95] <author> Brad Calder, Dirk Grunwald, and Amitabh Sri-vastava. </author> <title> The predictability of branches in libraries. </title> <booktitle> In Proceedings of the 28th International Symposium on Microarchitecture, </booktitle> <year> 1995. </year> <note> Also available as WRL Research Report 95.6. </note>
Reference-contexts: The number of components modeled will depend in part on the needed accuracy for the use of the model. 4.1.2 Related Work: Application Profile Various application profiles have been addressed in the literature. A detailed application profile is of common use in compiler optimizations <ref> [Wal90, CGS95] </ref>.
Reference: [Cha94] <author> K. M. Chandy. </author> <title> Concurrent program archetypes. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Such constructs are skeletons in that they have structure, but lack detail, much as the top-most structural model shows the structure of the application with respect to its constituent task implementations, but the details of the tasks themselves are supplied by individual component models. Similarly, Chandy <ref> [Cha94] </ref> addresses programming archetypes, abstractions from a class of programs with a common structure and includes class-specific design strategies and a collection of example program designs and implementations, optimized for a collection of target machines.
Reference: [Col89] <author> Murray Cole. </author> <title> Algorithmic Skeletons: Structured Management of Parallel Computation. </title> <publisher> Pitman/MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: For example, a task will be tuned to fit an architecture's specific cache size or to use an architecture-specific library routine. The structural modeling approach is similar in spirit to the use of skeletons for parallel programming <ref> [Col89, DFH + 93] </ref>. With skeletons, useful patterns of parallel computation and interactions are packaged together as a construct, and then parameterized with other pieces of code.
Reference: [CS95] <author> Robert L. Clay and Peter A. Steenkiste. </author> <title> Distributing a chemical process optimization application over a gigabit network. </title> <booktitle> In Proceedings of SuperComputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: Small number of sub-tasks. Commonly, applications are broken into a small number of sub-tasks, usually fewer than five <ref> [CS95, DS95, DTMH96, MMFM93, PRW94, WK93, YB95] </ref>. This is a manageable number to model individually. In addition, we can use an application profile to help direct our efforts to model the most important components in more detail. Implementations are different. <p> All interactions between modeled components (including overlap) are re flected in the top-most structural model. Component models represent the performance of individual components, or sub-tasks, where a subtask is some functional unit of the application. The definition of subtask varies between applications|for some it's a function <ref> [CS95] </ref>, for others an inner loop where most of the work is done [DS95], for others a subtask of their application is another application that can stand on it's own [MMFM93]. <p> One of the problems we have encountered in doing this is that many approaches to modeling assume the availability of pieces of data that we do not have for our system. Thus, we have grounded our work in the models used for current applications in this area <ref> [CS95, DS95, DTMH96, MMFM93, PRW94, WK93, YB95] </ref>. These models all take into account complexity of computation, information availability, and accuracy needed for predictions in this setting 4 Component Model Selection Our model selection is based on the principle of using the simple model that provides accurate results.
Reference: [DFH + 93] <author> J. Darlington, A. J. Field, P. G. Harrison, P. H. J. Kelly, D. W. N. Sharp, Q. Wu, and R. C. </author> <title> While. Parallel programming using skeleton functions. </title> <booktitle> In Proceedings of Parallel Architectures and Languages Europe (PARLE) '93, </booktitle> <year> 1993. </year>
Reference-contexts: For example, a task will be tuned to fit an architecture's specific cache size or to use an architecture-specific library routine. The structural modeling approach is similar in spirit to the use of skeletons for parallel programming <ref> [Col89, DFH + 93] </ref>. With skeletons, useful patterns of parallel computation and interactions are packaged together as a construct, and then parameterized with other pieces of code.
Reference: [DN95] <author> J. Dongarra and Peter Newton. </author> <title> Overview of VPE: A visual environment for message-passing parallel programming. </title> <booktitle> In Proceedings of the 4th Heterogeneous Computing Workshop, </booktitle> <year> 1995. </year>
Reference-contexts: Often in describing an application a developer will use a graphical representation like a program dependency graph (PDG) [FOW84], a more detailed representation such as Zoom [ASWB95], or even a visual programming language representation such as HeNCE [BDGM93], CODE 2.0 [NB92], Enterprise [SSLP93] or VPE <ref> [DN95] </ref>. From these graphical representations, defining a structural model is straightforward. As a part of the AppLeS project [BW96, BWF + 96], we are building a library of structural templates to guide the structural model definition process.
Reference: [DS95] <author> J. Demmel and S. L. Smith. </author> <title> Performance of a parallel global atmospheric chemical tracer model. </title> <booktitle> In Proceedings of SuperComputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: Small number of sub-tasks. Commonly, applications are broken into a small number of sub-tasks, usually fewer than five <ref> [CS95, DS95, DTMH96, MMFM93, PRW94, WK93, YB95] </ref>. This is a manageable number to model individually. In addition, we can use an application profile to help direct our efforts to model the most important components in more detail. Implementations are different. <p> Component models represent the performance of individual components, or sub-tasks, where a subtask is some functional unit of the application. The definition of subtask varies between applications|for some it's a function [CS95], for others an inner loop where most of the work is done <ref> [DS95] </ref>, for others a subtask of their application is another application that can stand on it's own [MMFM93]. We define a component to be a free-standing operational unit of the application that is not functionally split between two machines. <p> One of the problems we have encountered in doing this is that many approaches to modeling assume the availability of pieces of data that we do not have for our system. Thus, we have grounded our work in the models used for current applications in this area <ref> [CS95, DS95, DTMH96, MMFM93, PRW94, WK93, YB95] </ref>. These models all take into account complexity of computation, information availability, and accuracy needed for predictions in this setting 4 Component Model Selection Our model selection is based on the principle of using the simple model that provides accurate results.
Reference: [DTMH96] <author> D. M. Deaven, N. Tit, J. R. Morris, and K. M. Ho. </author> <title> Structural optimization of lennard-jones clusters by a genetic algorithm. </title> <journal> Chemical Physical Letters, </journal> <volume> 256:195, </volume> <year> 1996. </year>
Reference-contexts: Small number of sub-tasks. Commonly, applications are broken into a small number of sub-tasks, usually fewer than five <ref> [CS95, DS95, DTMH96, MMFM93, PRW94, WK93, YB95] </ref>. This is a manageable number to model individually. In addition, we can use an application profile to help direct our efforts to model the most important components in more detail. Implementations are different. <p> One of the problems we have encountered in doing this is that many approaches to modeling assume the availability of pieces of data that we do not have for our system. Thus, we have grounded our work in the models used for current applications in this area <ref> [CS95, DS95, DTMH96, MMFM93, PRW94, WK93, YB95] </ref>. These models all take into account complexity of computation, information availability, and accuracy needed for predictions in this setting 4 Component Model Selection Our model selection is based on the principle of using the simple model that provides accurate results. <p> Genetic algorithms were originally developed by the artificial intelligence community as an optimization technique for NP-complete and NP-hard problems. As such, they are now being used by several groups of computational scientists <ref> [DTMH96, SK96, PM94] </ref> to address problems such as protein folding. Our distributed implementation uses a global population with synchronization between generations [Bha96].
Reference: [FB96] <author> Silvia Figueira and Fran Berman. </author> <title> Modeling the effects of contention on the performance of heterogeneous applications. </title> <booktitle> In Proceedings of the Fifth IEEE Symposium on High Performance Distributed Computing, </booktitle> <year> 1996. </year>
Reference-contexts: On different resource management systems, and on distinct architectures controlled by a single resource management system, Scatter and Gather routines may be implemented differently. In addition, these routines can be affected by the computation of both the master and the slave <ref> [FB96] </ref>, so a model that provides accurate predictions in a production setting may not be intuitive. If we had the structure model in Equation 1 or 2, then we would need to examine the Scatter routine as a whole. There are three straightforward ways to do this. <p> slowdown (SD) factor to the above equations: LoadedS i = SD fl Slave i 1 LoadedS i = SD fl Slave i 2 The model for SD could be a dynamically supplied value, for example from Wolski's Network Weather Service [Wol96], or from an analytical model of contention, for example <ref> [FB96, LS93, ZY95] </ref>. 3.2.3 Related Work: Components In some sense, all modeling work grounded in practical methods can be considered related work to the component model approach. We hope to leverage off of existing modeling approaches to build the component models, and to make selections between them.
Reference: [Fig96] <author> Silvia Figueira. </author> <type> Personal communication, </type> <year> 1996. </year>
Reference-contexts: This application is a typical distributed stencil application, using an N by N grid of data divided over the processors in strips <ref> [Fig96] </ref>. It was written in C with PVM for the heterogeneous cluster of machines in the PCL as the GA code was. 6.1 Structural Model for SOR In our implementation, the application is divided into "red" and "black" phases, with communication and computation alternating for each [Bri87].
Reference: [FK97] <author> Ian Foster and Carl Kesselman. Globus: </author> <title> A metacomputing infrastructure toolkit. </title> <journal> International Journal of Supercomputer Applications, </journal> <note> (to appear) 1997. Also available at ftp://ftp.mcs.anl.gov/pub/nexus/reports/ globus.ps.Z. </note>
Reference-contexts: System specific data, such as bandwidth values, CPU capacity, memory sizes, etc., can be supplied by system databases as envisioned for most resource management systems (such as the Meta-Computing Information Service (MCIS) for the Globus project <ref> [FK97] </ref> or the Host Object Database for Legion [Kar96, GWtLt97]) or by online tools such as the Network Weather Service [Wol96] which can supply dynamic values for bandwidth, CPU usage and memory on a given system. 3.2.1 Example: Communication Components As an example of what we mean by component models, let's
Reference: [FOW84] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <booktitle> In Proceedings of the International Symposium on Programming 6th Colloquium, </booktitle> <year> 1984. </year>
Reference-contexts: Likewise with other interaction operations, such as Max below. 3.1 Structural Models We use an application developer's description of an application to construct the structural model. Often in describing an application a developer will use a graphical representation like a program dependency graph (PDG) <ref> [FOW84] </ref>, a more detailed representation such as Zoom [ASWB95], or even a visual programming language representation such as HeNCE [BDGM93], CODE 2.0 [NB92], Enterprise [SSLP93] or VPE [DN95]. From these graphical representations, defining a structural model is straightforward.
Reference: [GKM82] <author> S. Graham, P. Kessler, and M. McKusick. </author> <title> gprof: A call graph execution profiler. </title> <booktitle> In Proceedings of the SIGPLAN '82 Symposium on Compiler Construction, 1982. Also published in SIG-PLAN Notices, </booktitle> <volume> volume 17, number 6, </volume> <pages> pages 120-126. </pages>
Reference-contexts: Admittedly, this is a difficult value to define exactly, as it will depend on many factors: architecture, specific implementations of the application, load on relevant machines, problem size, etc. In practice, these values can be found through profiling tools <ref> [GKM82, Sil] </ref> or timing runs of the application, or an estimated value can be supplied by the application developer. <p> A detailed application profile is of common use in compiler optimizations [Wal90, CGS95]. For program comprehension and tuning, tools like gprof <ref> [GKM82] </ref> and pixie [Sil], can supply application profile tables of data to help determine the percentage of time spent in a function, how many times a basic block was run, how often a global variable was accesses, and even for some tools the number of memory accesses and how many misses
Reference: [GR96] <author> Jorn Gehring and Alexander Reinefeld. </author> <title> Mars - a framework for minimizing the job execution time in a metacomputing environment. </title> <booktitle> Future Generation Computer Systems, </booktitle> <month> Spring </month> <year> 1996. </year>
Reference-contexts: Input information for the models can come from several places. As a part of the AppLeS project, application developers fill out a Heterogeneous Application Template (HAT) form that contains data about the general structure, specific implementations, and the interface between two given sub-task implementations. In the MARS system <ref> [GR96] </ref> this data is obtained by instrumenting the code and using an application monitor to record the application's behavior.
Reference: [GWtLt97] <author> Andrew S. Grimshaw, William A. Wulf, </author> <title> and the Legion team. The Legion vision of a worldwide virtual computer. </title> <journal> Communications of the ACM, </journal> <volume> 40(1), </volume> <month> January </month> <year> 1997. </year>
Reference-contexts: System specific data, such as bandwidth values, CPU capacity, memory sizes, etc., can be supplied by system databases as envisioned for most resource management systems (such as the Meta-Computing Information Service (MCIS) for the Globus project [FK97] or the Host Object Database for Legion <ref> [Kar96, GWtLt97] </ref>) or by online tools such as the Network Weather Service [Wol96] which can supply dynamic values for bandwidth, CPU usage and memory on a given system. 3.2.1 Example: Communication Components As an example of what we mean by component models, let's examine the Scatter routine from the example application
Reference: [Jam87] <author> Leah Jamieson. </author> <title> The Characteristics of Parallel Algorithms, chapter 3, Characterizing Parallel Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1987. </year> <editor> eds. L. Jamieson, D. Gannon and R. </editor> <publisher> Douglass. </publisher>
Reference-contexts: Our framework provides the needed flexibility to allow for additional information this level. 4.2.1 Related Work: Selection Related work to our approach can be found in the area of algorithm and platform classifications. Jamieson <ref> [Jam87] </ref> examines the relationship between architecture and algorithm characteristics for signal processing applications. This work attempts to identify the influence of specific application characteristics on architecture characteristics.
Reference: [Kar96] <author> John F. Karpovich. </author> <title> Support for object placement in wide area heterogeneous distributed systems. </title> <type> Technical Report CS-96-03, </type> <institution> University of Virginia, Department of Computer Science, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: System specific data, such as bandwidth values, CPU capacity, memory sizes, etc., can be supplied by system databases as envisioned for most resource management systems (such as the Meta-Computing Information Service (MCIS) for the Globus project [FK97] or the Host Object Database for Legion <ref> [Kar96, GWtLt97] </ref>) or by online tools such as the Network Weather Service [Wol96] which can supply dynamic values for bandwidth, CPU usage and memory on a given system. 3.2.1 Example: Communication Components As an example of what we mean by component models, let's examine the Scatter routine from the example application
Reference: [KME89] <author> A. Kapelnikov, R. R. Muniz, and M. D. Ercego-vac. </author> <title> A modeling methodology for the analysis of concurrent systems and computations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 6 </volume> <pages> 568-597, </pages> <year> 1989. </year>
Reference-contexts: Once a structural model has been defined, individual models must be built for the constituent components. 3.1.2 Related Work: Structural Models One common approach to modeling parallel applications is to separate the performance into application characteristics and system characteristics <ref> [TB86, Moh84, KME89, ML90] </ref>. This approach may also be adequate when the underlying workstations are very similar (for example, Zhang [YZS96] considers only workstations that do floating point arithmetic identically). However, in the cluster environment this line is not easily drawn.
Reference: [LLKS85] <author> Lawler, Lenstra, Kan, and Shmoys. </author> <title> The Traveling Salesman Problem. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: This is the subject of our cur rent research. 7 5 Modeling a Genetic Algorithm As an example from the master-slave class of applications, we have developed performance models for a genetic algorithm (GA) optimization for the Traveling Salesman Problem (TSP) <ref> [LLKS85, WSF89] </ref>. Genetic algorithms were originally developed by the artificial intelligence community as an optimization technique for NP-complete and NP-hard problems. As such, they are now being used by several groups of computational scientists [DTMH96, SK96, PM94] to address problems such as protein folding.
Reference: [LS93] <author> S. Leuttenegger and X. Sun. </author> <title> Distributed computing feasibility in a non-dedicated homogeneous distributed system. </title> <type> Technical Report Te-chinical Report Number 93-65, </type> <institution> NASA - ICASE, </institution> <year> 1993. </year>
Reference-contexts: slowdown (SD) factor to the above equations: LoadedS i = SD fl Slave i 1 LoadedS i = SD fl Slave i 2 The model for SD could be a dynamically supplied value, for example from Wolski's Network Weather Service [Wol96], or from an analytical model of contention, for example <ref> [FB96, LS93, ZY95] </ref>. 3.2.3 Related Work: Components In some sense, all modeling work grounded in practical methods can be considered related work to the component model approach. We hope to leverage off of existing modeling approaches to build the component models, and to make selections between them.
Reference: [ML90] <author> V. W. Mak and S. F. Lundstrom. </author> <title> Predicting the performance of parallel computations. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 257-270, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Once a structural model has been defined, individual models must be built for the constituent components. 3.1.2 Related Work: Structural Models One common approach to modeling parallel applications is to separate the performance into application characteristics and system characteristics <ref> [TB86, Moh84, KME89, ML90] </ref>. This approach may also be adequate when the underlying workstations are very similar (for example, Zhang [YZS96] considers only workstations that do floating point arithmetic identically). However, in the cluster environment this line is not easily drawn.
Reference: [MMFM93] <author> C. R. Mechoso, C. Ma, J. D. Farrara, and R. W. Moore. </author> <title> Parallelization and distribution of a coupled ocean-atmosphere general circulation model. </title> <journal> Monthly Weather Review, </journal> <volume> 121(7), </volume> <month> July </month> <year> 1993. </year>
Reference-contexts: Small number of sub-tasks. Commonly, applications are broken into a small number of sub-tasks, usually fewer than five <ref> [CS95, DS95, DTMH96, MMFM93, PRW94, WK93, YB95] </ref>. This is a manageable number to model individually. In addition, we can use an application profile to help direct our efforts to model the most important components in more detail. Implementations are different. <p> The definition of subtask varies between applications|for some it's a function [CS95], for others an inner loop where most of the work is done [DS95], for others a subtask of their application is another application that can stand on it's own <ref> [MMFM93] </ref>. We define a component to be a free-standing operational unit of the application that is not functionally split between two machines. That is, several machines can perform the same component (for example, the data-parallel slave operation in the example). <p> One of the problems we have encountered in doing this is that many approaches to modeling assume the availability of pieces of data that we do not have for our system. Thus, we have grounded our work in the models used for current applications in this area <ref> [CS95, DS95, DTMH96, MMFM93, PRW94, WK93, YB95] </ref>. These models all take into account complexity of computation, information availability, and accuracy needed for predictions in this setting 4 Component Model Selection Our model selection is based on the principle of using the simple model that provides accurate results.
Reference: [Moh84] <author> J. Mohan. </author> <title> Performance of Parallel Programs: Model and Analyses. </title> <type> PhD thesis, </type> <institution> Carnegie Mel-lon University, </institution> <month> July </month> <year> 1984. </year>
Reference-contexts: Once a structural model has been defined, individual models must be built for the constituent components. 3.1.2 Related Work: Structural Models One common approach to modeling parallel applications is to separate the performance into application characteristics and system characteristics <ref> [TB86, Moh84, KME89, ML90] </ref>. This approach may also be adequate when the underlying workstations are very similar (for example, Zhang [YZS96] considers only workstations that do floating point arithmetic identically). However, in the cluster environment this line is not easily drawn.
Reference: [NB92] <author> P. Newton and J. C. Browne. </author> <title> The Code 2.0 graphical parallel programming language. </title> <booktitle> In Proceedings of the ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: Often in describing an application a developer will use a graphical representation like a program dependency graph (PDG) [FOW84], a more detailed representation such as Zoom [ASWB95], or even a visual programming language representation such as HeNCE [BDGM93], CODE 2.0 <ref> [NB92] </ref>, Enterprise [SSLP93] or VPE [DN95]. From these graphical representations, defining a structural model is straightforward. As a part of the AppLeS project [BW96, BWF + 96], we are building a library of structural templates to guide the structural model definition process.
Reference: [PM94] <author> Jan Pedersen and John Moult. </author> <title> Determination of the structure of small protein fragments using torsion space monte carlo and genetic algorithm methods. In Proceedings of Meeting on Critical Assessment of Techniques for Protein Structure Prediction, </title> <booktitle> Asilomar Conference Center, </booktitle> <month> December </month> <year> 1994. </year>
Reference-contexts: Genetic algorithms were originally developed by the artificial intelligence community as an optimization technique for NP-complete and NP-hard problems. As such, they are now being used by several groups of computational scientists <ref> [DTMH96, SK96, PM94] </ref> to address problems such as protein folding. Our distributed implementation uses a global population with synchronization between generations [Bha96].
Reference: [PRW94] <author> A. T. Phillips, J. B. Rosen, and V. H. Walke. </author> <title> Molecular structure determination by convex global underestimation of local energy minima. </title> <type> Technical Report Tech Report UMSI 94/126, </type> <institution> University of Minnesota Supercomputer Institute, </institution> <year> 1994. </year>
Reference-contexts: Small number of sub-tasks. Commonly, applications are broken into a small number of sub-tasks, usually fewer than five <ref> [CS95, DS95, DTMH96, MMFM93, PRW94, WK93, YB95] </ref>. This is a manageable number to model individually. In addition, we can use an application profile to help direct our efforts to model the most important components in more detail. Implementations are different. <p> One of the problems we have encountered in doing this is that many approaches to modeling assume the availability of pieces of data that we do not have for our system. Thus, we have grounded our work in the models used for current applications in this area <ref> [CS95, DS95, DTMH96, MMFM93, PRW94, WK93, YB95] </ref>. These models all take into account complexity of computation, information availability, and accuracy needed for predictions in this setting 4 Component Model Selection Our model selection is based on the principle of using the simple model that provides accurate results.
Reference: [SBSM89] <author> R. Saavedra-Barrera, A. J. Smith, and E. Miya. </author> <title> Machine characterization based on an abstract high-level language machine. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38 </volume> <pages> 1659-1679, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Jamieson [Jam87] examines the relationship between architecture and algorithm characteristics for signal processing applications. This work attempts to identify the influence of specific application characteristics on architecture characteristics. Saavedra-Barrera <ref> [SBSM89] </ref> defined micro-benchmarks to characterize machine performance, and defined a visualization technique called pershape to provide a quantitative way of measuring the performance similarities of multiple machines.
Reference: [Sil] <institution> Silicon Graphics. Pixie Man Page. </institution> <note> also http://web.cnam.fr/Docs/man/Ultrix-4.3/ pixie.1.html. </note>
Reference-contexts: Admittedly, this is a difficult value to define exactly, as it will depend on many factors: architecture, specific implementations of the application, load on relevant machines, problem size, etc. In practice, these values can be found through profiling tools <ref> [GKM82, Sil] </ref> or timing runs of the application, or an estimated value can be supplied by the application developer. <p> A detailed application profile is of common use in compiler optimizations [Wal90, CGS95]. For program comprehension and tuning, tools like gprof [GKM82] and pixie <ref> [Sil] </ref>, can supply application profile tables of data to help determine the percentage of time spent in a function, how many times a basic block was run, how often a global variable was accesses, and even for some tools the number of memory accesses and how many misses occur at each
Reference: [SK96] <author> Steffen Schulze-Kremer. </author> <note> http://www.techfak.uni-bielefeld.de/bcd/ curric/proten/proten.html, </note> <year> 1996. </year>
Reference-contexts: Genetic algorithms were originally developed by the artificial intelligence community as an optimization technique for NP-complete and NP-hard problems. As such, they are now being used by several groups of computational scientists <ref> [DTMH96, SK96, PM94] </ref> to address problems such as protein folding. Our distributed implementation uses a global population with synchronization between generations [Bha96].
Reference: [SSLP93] <author> Jonathan Schaeffer, Duane Szafron, Greg Lobe, and Ian Parsons. </author> <title> The Enterprise model fo developing distributed applications. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 1(3), </volume> <month> August </month> <year> 1993. </year>
Reference-contexts: Often in describing an application a developer will use a graphical representation like a program dependency graph (PDG) [FOW84], a more detailed representation such as Zoom [ASWB95], or even a visual programming language representation such as HeNCE [BDGM93], CODE 2.0 [NB92], Enterprise <ref> [SSLP93] </ref> or VPE [DN95]. From these graphical representations, defining a structural model is straightforward. As a part of the AppLeS project [BW96, BWF + 96], we are building a library of structural templates to guide the structural model definition process.
Reference: [SW96] <author> Jens Simon and Jens-Michael Wierum. </author> <title> Accurate performance prediction for massively parallel systems and its applications. </title> <booktitle> In Proceedings of Euro-Par '96 Parallel Processing, </booktitle> <volume> volume 2, </volume> <year> 1996. </year>
Reference-contexts: Others, like Simon and Wierum <ref> [SW96] </ref>, require complex mi-crobenchmarks to be run on each system. 4.2 Selection Based on Available Input Information We are currently basing component selection on the sources of information that are available. The basis for this approach is to use the simplest model that provides the level of needed accuracy.
Reference: [TB86] <author> A. Thomasian and P. F. </author> <title> Bay. Analysis queueing network models for parallel processing of task systems. </title> <journal> IEEE Transactions on Computers c-35, </journal> <volume> 12 </volume> <pages> 1045-1054, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Once a structural model has been defined, individual models must be built for the constituent components. 3.1.2 Related Work: Structural Models One common approach to modeling parallel applications is to separate the performance into application characteristics and system characteristics <ref> [TB86, Moh84, KME89, ML90] </ref>. This approach may also be adequate when the underlying workstations are very similar (for example, Zhang [YZS96] considers only workstations that do floating point arithmetic identically). However, in the cluster environment this line is not easily drawn.
Reference: [Wal90] <author> David W. Wall. </author> <title> Predicting program behavior using real or estimated profiles. </title> <type> Technical Report WRL TN-18, </type> <institution> Digital Western Research Laboratory, </institution> <year> 1990. </year>
Reference-contexts: The number of components modeled will depend in part on the needed accuracy for the use of the model. 4.1.2 Related Work: Application Profile Various application profiles have been addressed in the literature. A detailed application profile is of common use in compiler optimizations <ref> [Wal90, CGS95] </ref>.
Reference: [WK93] <author> Yi-Shuen Mark Wu and Aron Kupermann. </author> <title> Prediction of the effect of the geometric phase on product rotational state distributions and integral cross sections. </title> <journal> Chemical Physics Letters, </journal> <volume> 201 </volume> <pages> 178-86, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Small number of sub-tasks. Commonly, applications are broken into a small number of sub-tasks, usually fewer than five <ref> [CS95, DS95, DTMH96, MMFM93, PRW94, WK93, YB95] </ref>. This is a manageable number to model individually. In addition, we can use an application profile to help direct our efforts to model the most important components in more detail. Implementations are different. <p> One of the problems we have encountered in doing this is that many approaches to modeling assume the availability of pieces of data that we do not have for our system. Thus, we have grounded our work in the models used for current applications in this area <ref> [CS95, DS95, DTMH96, MMFM93, PRW94, WK93, YB95] </ref>. These models all take into account complexity of computation, information availability, and accuracy needed for predictions in this setting 4 Component Model Selection Our model selection is based on the principle of using the simple model that provides accurate results.
Reference: [Wol96] <author> Rich Wolski. </author> <title> Dynamically forecasting network performance using the network weather service. </title> <type> Technical Report TR-CS96-494, </type> <institution> University of California, San Diego, Computer Science and Engineering Dept., </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: values, CPU capacity, memory sizes, etc., can be supplied by system databases as envisioned for most resource management systems (such as the Meta-Computing Information Service (MCIS) for the Globus project [FK97] or the Host Object Database for Legion [Kar96, GWtLt97]) or by online tools such as the Network Weather Service <ref> [Wol96] </ref> which can supply dynamic values for bandwidth, CPU usage and memory on a given system. 3.2.1 Example: Communication Components As an example of what we mean by component models, let's examine the Scatter routine from the example application in more depth. <p> of the slave routines, we might want to add a slowdown (SD) factor to the above equations: LoadedS i = SD fl Slave i 1 LoadedS i = SD fl Slave i 2 The model for SD could be a dynamically supplied value, for example from Wolski's Network Weather Service <ref> [Wol96] </ref>, or from an analytical model of contention, for example [FB96, LS93, ZY95]. 3.2.3 Related Work: Components In some sense, all modeling work grounded in practical methods can be considered related work to the component model approach. <p> The resulting models would be: LoadedComp i = Capacity fl Comp i where capacity would need to be supplied from a dynamic source, for example <ref> [Wol96] </ref>. Refine a component model. It might be possible to capture the memory behavior as a part of the computation component. For example: BetterComp = Memory i fl Comp i where Memory i would represent the behavior of the memory access on processor i for larger problem sizes.
Reference: [WSF89] <author> D. Whitley, T. Starkweather, and D'Ann Fuquay. </author> <title> Scheduling problems and traveling salesman: The genetic edge recombination operator. </title> <booktitle> In Proceedings of International Conference on Genetic Algorithms, </booktitle> <year> 1989. </year>
Reference-contexts: This is the subject of our cur rent research. 7 5 Modeling a Genetic Algorithm As an example from the master-slave class of applications, we have developed performance models for a genetic algorithm (GA) optimization for the Traveling Salesman Problem (TSP) <ref> [LLKS85, WSF89] </ref>. Genetic algorithms were originally developed by the artificial intelligence community as an optimization technique for NP-complete and NP-hard problems. As such, they are now being used by several groups of computational scientists [DTMH96, SK96, PM94] to address problems such as protein folding.
Reference: [YB95] <author> W. Young and C. L. Brooks. </author> <title> Dynamic load balancing algorithms for replicated data molecular dynamics. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 16 </volume> <pages> 715-722, </pages> <year> 1995. </year>
Reference-contexts: Small number of sub-tasks. Commonly, applications are broken into a small number of sub-tasks, usually fewer than five <ref> [CS95, DS95, DTMH96, MMFM93, PRW94, WK93, YB95] </ref>. This is a manageable number to model individually. In addition, we can use an application profile to help direct our efforts to model the most important components in more detail. Implementations are different. <p> One of the problems we have encountered in doing this is that many approaches to modeling assume the availability of pieces of data that we do not have for our system. Thus, we have grounded our work in the models used for current applications in this area <ref> [CS95, DS95, DTMH96, MMFM93, PRW94, WK93, YB95] </ref>. These models all take into account complexity of computation, information availability, and accuracy needed for predictions in this setting 4 Component Model Selection Our model selection is based on the principle of using the simple model that provides accurate results.
Reference: [YZS96] <author> Yong Yan, Xiaodong Zhang, and Yongsheng Song. </author> <title> An effective and practical performance prediction model for parallel computing on non-dedicated heterogeneous NOW. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> October </month> <year> 1996. </year> <institution> Also University of Texas, </institution> <address> San Antonio, </address> <note> Technical Report # TR-96-0401. </note>
Reference-contexts: This approach may also be adequate when the underlying workstations are very similar (for example, Zhang <ref> [YZS96] </ref> considers only workstations that do floating point arithmetic identically). However, in the cluster environment this line is not easily drawn. We do not separate application and system characteristics at the structural level because developers address their codes as implementations|or combinations of both application and system. <p> They require information about the frequency and average execution time for each I/O operation, memory read or write, and floating point operation throughout the application. Obtaining this level of information is difficult and often time consuming if even possible. Zhang et al. <ref> [YZS96] </ref> have developed a tool to measure these basic timing results needed for their approach, but they avoid the problem of having the profile change with respect to problem size by analyzing a single problem size over a range of possible workstation networks.
Reference: [ZY95] <author> X. Zhang and Y. Yan. </author> <title> A framework of performance prediction of parallel copmuting on non-dedicated heterogeneous networks of workstations. </title> <booktitle> In Proceedings of 1995 International Conference of Parallel Processing, </booktitle> <year> 1995. </year> <month> 13 </month>
Reference-contexts: slowdown (SD) factor to the above equations: LoadedS i = SD fl Slave i 1 LoadedS i = SD fl Slave i 2 The model for SD could be a dynamically supplied value, for example from Wolski's Network Weather Service [Wol96], or from an analytical model of contention, for example <ref> [FB96, LS93, ZY95] </ref>. 3.2.3 Related Work: Components In some sense, all modeling work grounded in practical methods can be considered related work to the component model approach. We hope to leverage off of existing modeling approaches to build the component models, and to make selections between them.
References-found: 45

