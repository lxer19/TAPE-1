URL: http://www.cis.hut.fi/~aapo/ps/NIPS96.ps
Refering-URL: http://www.cis.hut.fi/~oja/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: -Aapo.Hyvarinen,Erkki.Oja-@hut.fi  
Title: One-unit Learning Rules for Independent Component Analysis  
Author: Aapo Hyvrinen and Erkki Oja 
Address: Rakentajanaukio 2 C, FIN-02150 Espoo, Finland  
Affiliation: Helsinki University of Technology Laboratory of Computer and Information Science  
Abstract: Neural one-unit learning rules for the problem of Independent Component Analysis (ICA) and blind source separation are introduced. In these new algorithms, every ICA neuron develops into a separator that tnds one of the independent components. The learning rules use very simple constrained Hebbian/anti-Hebbian learning in which decorrelating feedback may be added. To speed up the convergence of these stochastic gradient descent rules, a novel com putationally ecient txed-point algorithm is introduced.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bell, A. and Sejnowski, T. </author> <year> (1995). </year> <title> An information-maximization approach to blind separation and blind deconvolution. Neural Computation, </title> <publisher> 7:11291159. </publisher>
Reference: <author> Bell, A. and Sejnowski, T. J. </author> <year> (1996). </year> <title> Edges are the independent components of natural scenes. </title> <booktitle> In NIPS*96, </booktitle> <address> Denver, Colorado. </address>
Reference: <author> Cardoso, J.-F. and Laheld, B. H. </author> <year> (1996). </year> <title> Equivariant adaptive source separation. </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> 44(12). </volume>
Reference: <author> Comon, P. </author> <year> (1994). </year> <title> Independent component analysis a new concept? Signal Processing, </title> <publisher> 36:287314. </publisher>
Reference-contexts: The main applications of ICA are in blind source separation, feature extraction, and blind deconvolution. In the simplest form of ICA <ref> (Comon, 1994) </ref>, we observe m scalar random variables x 1 ; :::; x m which are assumed to be linear combinations of n unknown components s 1 ; :::s n that are zero-mean and mutually statistically independent. In addition, we must assume n m. <p> Whitening means that the observed vector x is linearly transformed to a vector v = Ux such that its elements v i are mutually uncorrelated and all have unit variance <ref> (Comon, 1994) </ref>. Thus the correlation matrix of v equals unity: Efvv T g = I. This transformation is always possible and can be accomplished by classical Principal Component Analysis.
Reference: <author> Delfosse, N. and Loubaton, P. </author> <year> (1995). </year> <title> Adaptive blind separation of independent sources: a deation approach. Signal Processing, </title> <publisher> 45:5983. </publisher>
Reference: <author> Donoho, D. </author> <year> (1981). </year> <title> On minimum entropy deconvolution. In Applied Time Series Analysis II. </title> <publisher> Academic Press. </publisher>
Reference-contexts: A closely related problem is blind deconvolution, in which a convolved version x (t) of a scalar i.i.d. signal s (t) is observed. The goal is then to recover the original signal s (t) without knowing the convolution kernel <ref> (Donoho, 1981) </ref>. This problem can be represented in a way similar to eq. (1), replacing the matrix A by a tlter.
Reference: <author> Hurri, J., Hyvrinen, A., Karhunen, J., and Oja, E. </author> <year> (1996). </year> <title> Image feature extraction using independent component analysis. </title> <booktitle> In Proc. </booktitle> <address> NORSIG'96, Espoo, Finland. </address>
Reference: <author> Hyvrinen, A. </author> <year> (1997). </year> <title> A family of txed-point algorithms for independent component analysis. </title> <booktitle> In Proc. </booktitle> <address> ICASSP'97, Munich, Germany. </address>
Reference-contexts: Therefore, some ways to make the learning radically faster and more reliable may be needed. The txed-point iteration algorithms are such an alternative. Based on the learning rules introduced above, we introduce here a txed-point algorithm, whose convergence is proven and analyzed in detail in <ref> (Hyvrinen and Oja, 1997) </ref>. For simplicity, we only consider the case of whitened data here. Consider the general neural learning rule trying to tnd the extrema of kurtosis. <p> This is due to the fact that the convergence of the txed point algorithm is in fact cubic, as shown in <ref> (Hyvrinen and Oja, 1997) </ref>. To estimate N ICs, we run this algorithm N times. To ensure that we estimate each time a dierent IC, we only need to add a simple projection inside the loop, which forces the solution vector w (k) to be orthogonal to the previously found solutions. <p> This is possible because the desired weight vectors are orthonormal for whitened data (Hyvrinen and Oja, 1996b; Karhunen et al., 1997). Symmetric methods of orthogonalization may also be used <ref> (Hyvrinen, 1997) </ref>. This txed-point algorithm has several advantages when compared to other suggested ICA methods. First, the convergence of our algorithm is cubic. This means very fast convergence and is rather unique among the ICA algorithms. <p> The utility of such a generalization is that one can then choose the non-linearity according to some statistical optimality criteria, such as robustness against outliers. The txed-point algorithm may also be generalized for an arbitrary non-linearity, say g. Step 2 in the txed-point algorithm then becomes (for whitened data) <ref> (Hyvrinen, 1997) </ref>: w (k) = Efvg (w (k 1) T v)g Efg 0 (w (k 1) T v)gw (k 1). 8 Experiments A visually appealing way of demonstrating how ICA algorithms work is to use them to separate images from their linear mixtures.
Reference: <author> Hyvrinen, A. and Oja, E. </author> <year> (1996a). </year> <title> Independent component analysis by general nonlinear hebbian-like learning rules. </title> <type> Technical Report A41, </type> <institution> Helsinki University of Technology, Laboratory of Computer and Information Science. </institution>
Reference-contexts: This approach can be generalized to a large class of such optimizaton criteria, called contrast functions. For the case of on-line learning rules, this approach is developed in <ref> (Hyvrinen and Oja, 1996a) </ref>, in which it is shown that the function g in the learning rules in section 4 can be, in fact, replaced by practically any non-linear function (provided that w is normalized properly).
Reference: <author> Hyvrinen, A. and Oja, E. </author> <year> (1996b). </year> <title> Simple neuron models for independent component analysis. </title> <type> Technical Report A37, </type> <institution> Helsinki University of Technology, Laboratory of Computer and Information Science. </institution>
Reference-contexts: Note that w (t) T Cw (t) in g + might also be replaced by (Ef (w (t) T x (t)) 2 g) 2 or by kw (t)k 4 to enable a simpler implementation. It can be proven <ref> (Hyvrinen and Oja, 1996b) </ref> that using the learning rules (4) and (5), the linear output converges to cs j (t) where s j (t) is one of the ICs, and c is a scalar constant. <p> Thus we can say that the neuron learns to separate (estimate) one of the independent components. It is also possible to combine these two learning rules into a single rule that separates an IC of any kurtosis; see <ref> (Hyvrinen and Oja, 1996b) </ref>. 4 One-Unit ICA Learning Rules for Whitened Data Whitening, also called sphering, is a very useful preprocessing technique. It speeds up the convergence considerably, makes the learning more stable numerically, and allows some interesting moditcations of the learning rules. <p> A discussion of such networks can be found in <ref> (Hyvrinen and Oja, 1996b) </ref>. 6 Fixed-Point Algorithm for ICA The advantage of neural on-line learning rules like those introduced above is that the inputs v (t) can be used in the algorithm at once, thus enabling faster adaptation in a non-stationary environment.
Reference: <author> Hyvrinen, A. and Oja, E. </author> <year> (1997). </year> <title> A fast txed-point algorithm for independent component analysis. Neural Computation. </title> <note> To appear. </note>
Reference-contexts: Therefore, some ways to make the learning radically faster and more reliable may be needed. The txed-point iteration algorithms are such an alternative. Based on the learning rules introduced above, we introduce here a txed-point algorithm, whose convergence is proven and analyzed in detail in <ref> (Hyvrinen and Oja, 1997) </ref>. For simplicity, we only consider the case of whitened data here. Consider the general neural learning rule trying to tnd the extrema of kurtosis. <p> This is due to the fact that the convergence of the txed point algorithm is in fact cubic, as shown in <ref> (Hyvrinen and Oja, 1997) </ref>. To estimate N ICs, we run this algorithm N times. To ensure that we estimate each time a dierent IC, we only need to add a simple projection inside the loop, which forces the solution vector w (k) to be orthogonal to the previously found solutions. <p> This is possible because the desired weight vectors are orthonormal for whitened data (Hyvrinen and Oja, 1996b; Karhunen et al., 1997). Symmetric methods of orthogonalization may also be used <ref> (Hyvrinen, 1997) </ref>. This txed-point algorithm has several advantages when compared to other suggested ICA methods. First, the convergence of our algorithm is cubic. This means very fast convergence and is rather unique among the ICA algorithms. <p> The utility of such a generalization is that one can then choose the non-linearity according to some statistical optimality criteria, such as robustness against outliers. The txed-point algorithm may also be generalized for an arbitrary non-linearity, say g. Step 2 in the txed-point algorithm then becomes (for whitened data) <ref> (Hyvrinen, 1997) </ref>: w (k) = Efvg (w (k 1) T v)g Efg 0 (w (k 1) T v)gw (k 1). 8 Experiments A visually appealing way of demonstrating how ICA algorithms work is to use them to separate images from their linear mixtures.
Reference: <author> Jutten, C. and Herault, J. </author> <year> (1991). </year> <title> Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture. Signal Processing, </title> <publisher> 24:110. </publisher>
Reference-contexts: Note the assumption of zero mean of the ICs is in fact no restriction, as this can always be accomplished by subtracting the mean from the random vector x. Note also that no order is detned between the ICs. In blind source separation <ref> (Jutten and Herault, 1991) </ref>, the observed values of x correspond to a realization of an m-dimensional discrete-time signal x (t), t = 1; 2; :::. Then the components s j (t) are called source signals. The source signals are usually original, uncorrupted signals or noise sources.
Reference: <author> Karhunen, J., Oja, E., Wang, L., Vigario, R., and Joutsensalo, J. </author> <year> (1997). </year> <title> A class of neural networks for independent component analysis. </title> <journal> IEEE Trans. on Neural Networks. </journal> <note> To appear. </note>
Reference: <author> Oja, E. </author> <year> (1995). </year> <title> The nonlinear PCA learning rule and signal separation mathematical analysis. </title> <type> Technical Report A 26, </type> <institution> Helsinki University of Technology, Laboratory of Computer and Information Science. </institution> <note> Submitted to a journal. </note>
References-found: 14

