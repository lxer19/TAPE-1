URL: http://www.cs.yale.edu/users/csuros-miklos/papers/colt97.ps
Refering-URL: http://www.cs.yale.edu/users/csuros-miklos/papers.html
Root-URL: http://www.cs.yale.edu
Email: E-mail: fangluin,csurosg@cs.yale.edu  
Title: Learning Markov chains with variable memory length from noisy output  
Author: Dana Angluin and Miklos Cs-uros 
Note: Research supported in part by the National Science Foundation, grant CCR-9213881.  
Address: P.O Box 208285, New Haven CT 06520  
Affiliation: Department of Computer Science Yale University  
Abstract: The problem of modeling complicated data sequences, such as DNA or speech, often arises in practice. Most of the algorithms select a hypothesis from within a model class assuming that the observed sequence is the direct output of the underlying generation process. In this paper we consider the case when the output passes through a memoryless noisy channel before observation. In particular, we show that in the class of Markov chains with variable memory length, learning is affected by factors, which, despite being super-polynomial, are still small in some practical cases. Markov models with variable memory length, or probabilistic finite suffix automata, were introduced in learning theory by Ron, Singer and Tishby who also described a polynomial time learning algorithm [11, 12]. We present a modification of the algorithm which uses a noise-corrupted sample and has knowledge of the noise structure. The same algorithm is still viable if the noise is not known exactly but a good estimation is available. Finally, some experimental results are presented for removing noise from corrupted English text, and to measure how the performance of the learning algorithm is affected by the size of the noisy sample and the noise rate. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Abe and M. Warmuth. </author> <title> On the computational complexity of approximating distributions by probabilistic automata. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 205-260, </pages> <year> 1992. </year>
Reference-contexts: Similar models have been studied in information theory as well [10, 13, 14, 15]. Other models in the learning theory literature include Hidden Markov Models [9] and probabilistic finite automata. They both have severe theoretical drawbacks. Abe and Warmuth <ref> [1] </ref> proved that Hidden Markov Models are not learnable in time polynomial in the alphabet size unless RP=NP. Kearns et al. [5] proved that Probabilistic Finite Automata are not learnable in the PAC-sense [2] unless noisy parity functions are learnable. We proceed as follows. <p> Probabilistic Finite Automata Definition 2.2 The model class of Probabilistic Finite Automata is the set of five-tuples hQ; ; t; fl; i: a finite set of states Q, a finite alphabet = f 1 ; : : : ; m g, a next symbol probability function fl : Qfi 7! <ref> [0; 1] </ref> ( 8q 2 Q: 2 fl (q; ) = 1), a state transition function t : Q fi 7! Q, and an initial probability distribution : Q 7! [0; 1] ( q2Q (q) = 1). <p> alphabet = f 1 ; : : : ; m g, a next symbol probability function fl : Qfi 7! <ref> [0; 1] </ref> ( 8q 2 Q: 2 fl (q; ) = 1), a state transition function t : Q fi 7! Q, and an initial probability distribution : Q 7! [0; 1] ( q2Q (q) = 1). <p> Here we consider only the case of a memoryless time-invariant channel. In particular, we analyze first a simple noise model: a memoryless symmetric channel. Each char acter of the output is altered by the same probabil ity - 2 <ref> [0; 1] </ref>.
Reference: [2] <author> A. Blumer, A. Ehrenfeucht, D. Haussler and M. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36 </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: They both have severe theoretical drawbacks. Abe and Warmuth [1] proved that Hidden Markov Models are not learnable in time polynomial in the alphabet size unless RP=NP. Kearns et al. [5] proved that Probabilistic Finite Automata are not learnable in the PAC-sense <ref> [2] </ref> unless noisy parity functions are learnable. We proceed as follows. In Section 2 we describe the framework and algorithm of Ron et al. for learning PFSA. In Section 3 we modify the algorithm for learning from noisy data with exact knowledge of the noise structure.
Reference: [3] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Thus, it is necessary to divide it by ` to obtain a distance measure independent of the prefix length considered. The relative entropy gives a bound on the L 1 dis tance, as D [ p k q ] 2 ln 2 1 : (See Cover and Thomas <ref> [3] </ref>.) Since the L 1 norm bounds the L 2 ; L 3 ; : : : ; and L 1 norms from above, a good hypothesis remains a good one for other distance functions as well. 2 2.2 Model classes of Probabilistic Finite Automata Definition 2.2 The model class of
Reference: [4] <author> S. Eddy. </author> <title> Hidden Markov models. </title> <booktitle> Current Opinion in Structural Biology, </booktitle> <volume> 6 </volume> <pages> 361-365, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Modeling complex data sequences is one of the main problems in many research areas. Examples can be taken from computational biology <ref> [4, 6] </ref> (DNA or protein sequences), from natural language processing or speech recognition [9] (phoneme or letter sequences), etc. Most of the model building methods assume that noiseless training data are available.
Reference: [5] <author> M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. E. Schapire and L. Sellie. </author> <title> On the learnability of discrete distributions. </title> <booktitle> STOC 94, </booktitle> <pages> 273-282, </pages> <year> 1994. </year>
Reference-contexts: Other models in the learning theory literature include Hidden Markov Models [9] and probabilistic finite automata. They both have severe theoretical drawbacks. Abe and Warmuth [1] proved that Hidden Markov Models are not learnable in time polynomial in the alphabet size unless RP=NP. Kearns et al. <ref> [5] </ref> proved that Probabilistic Finite Automata are not learnable in the PAC-sense [2] unless noisy parity functions are learnable. We proceed as follows. In Section 2 we describe the framework and algorithm of Ron et al. for learning PFSA. <p> Recall that by the result of Kearns et al. <ref> [5] </ref>, it is unlikely that there is an efficient learning algorithm for the model class of PFA. We therefore impose further restrictions to make efficient learning possible.
Reference: [6] <author> A. Krogh, B. Brown, I. S. Mian, K. Sjolander,, D. Haussler. </author> <title> Hidden Markov models in computational biology: applications to protein modeling. </title> <journal> Journal of Molecular Biology, </journal> <volume> 235 </volume> <pages> 1501-1531, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Modeling complex data sequences is one of the main problems in many research areas. Examples can be taken from computational biology <ref> [4, 6] </ref> (DNA or protein sequences), from natural language processing or speech recognition [9] (phoneme or letter sequences), etc. Most of the model building methods assume that noiseless training data are available.
Reference: [7] <author> P. Lancaster and M. Tismenetsky. </author> <title> The Theory of Matrices. </title> <publisher> Academic Press, </publisher> <address> Orlando, </address> <year> 1985. </year>
Reference-contexts: Proof. The recursion for the noise transformation matrices is clear from the noise model. For the inverses, use the fact that (A B) = A 1 (see, eg., <ref> [7] </ref>). <p> A compatible matrix norm is the column norm defined as kAk fl = max m X ja ij j for matrix A = [a ij ] m i;j=1 . (See <ref> [7] </ref>.) It is easy to prove that for any matrices A and B, kA Bk fl kAk fl kBk fl : Therefore, fl k fl fl 1 fl k So we have the following lemma.
Reference: [8] <author> G. Lugosi. </author> <title> Learning with an unreliable teacher. </title> <journal> Pattern Recognition, </journal> <volume> 25 </volume> <pages> 79-87, </pages> <year> 1992. </year>
Reference-contexts: In this paper we consider the case when the output of a Markov chain with variable memory length is passed through a memoryless noisy channel before observation. Among others, Lugosi <ref> [8] </ref> considered learning from noisy data and showed that consistent learning is possible in case of the binary symmetric channel even if the noise rate is not known.
Reference: [9] <author> L. R. Rabiner. </author> <title> A tutorial on Hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77 </volume> <pages> 257-285, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Modeling complex data sequences is one of the main problems in many research areas. Examples can be taken from computational biology [4, 6] (DNA or protein sequences), from natural language processing or speech recognition <ref> [9] </ref> (phoneme or letter sequences), etc. Most of the model building methods assume that noiseless training data are available. A natural approach is, however, to consider the observed data sequence as being the output of a string generation process corrupted by some noise. <p> The string generation process is often assumed to have only a finite memory, that is, the generation of the next symbol of the sequence depends on a fixed finite number of previous symbols and state transitions. Such models include Hidden Markov Models <ref> [9] </ref>, FSM sources in data compression [10], and Markov chains with variable memory length [12]. In this paper we consider the case when the output of a Markov chain with variable memory length is passed through a memoryless noisy channel before observation. <p> Similar models have been studied in information theory as well [10, 13, 14, 15]. Other models in the learning theory literature include Hidden Markov Models <ref> [9] </ref> and probabilistic finite automata. They both have severe theoretical drawbacks. Abe and Warmuth [1] proved that Hidden Markov Models are not learnable in time polynomial in the alphabet size unless RP=NP.
Reference: [10] <author> J. Rissanen. </author> <title> Complexity of strings in the class of Markov sources. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-32:526-532, </volume> <year> 1986. </year>
Reference-contexts: The string generation process is often assumed to have only a finite memory, that is, the generation of the next symbol of the sequence depends on a fixed finite number of previous symbols and state transitions. Such models include Hidden Markov Models [9], FSM sources in data compression <ref> [10] </ref>, and Markov chains with variable memory length [12]. In this paper we consider the case when the output of a Markov chain with variable memory length is passed through a memoryless noisy channel before observation. <p> Markov models with variable memory length, otherwise known as probabilistic finite suffix automata, were introduced in learning theory by Ron, Singer and Tishby who also described a polynomial time learning algorithm [11, 12]. Similar models have been studied in information theory as well <ref> [10, 13, 14, 15] </ref>. Other models in the learning theory literature include Hidden Markov Models [9] and probabilistic finite automata. They both have severe theoretical drawbacks. Abe and Warmuth [1] proved that Hidden Markov Models are not learnable in time polynomial in the alphabet size unless RP=NP. <p> We mention here that similar frameworks and algorithms can be found in the information theory literature for data compression (FSM and FSMX sources, context trees, see, eg., <ref> [10] </ref>, [13] and [14]). The per-symbol entropy D ` [ M k H ] =` equals the minimum expected number of extra bits when the data sequence generated by M is encoded using the hypothesis H instead of the optimal code based on M . <p> Willems et al. [15] outputs a distribution over PFSA's instead of one hypothesis machine. Naively using the tree building methods of <ref> [10] </ref> or [13] may result in an unduly large number of states, etc. In either way, using our approach (or a similar one) for learning from noisy output most likely causes similar effects in running time and sample size.
Reference: [11] <author> D. Ron, Y. Singer and N. Tishby. </author> <title> Learning probabilistic automata with variable memory length. </title> <booktitle> COLT 94, </booktitle> <pages> 35-46, </pages> <year> 1994. </year>
Reference-contexts: Markov models with variable memory length, otherwise known as probabilistic finite suffix automata, were introduced in learning theory by Ron, Singer and Tishby who also described a polynomial time learning algorithm <ref> [11, 12] </ref>. Similar models have been studied in information theory as well [10, 13, 14, 15]. Other models in the learning theory literature include Hidden Markov Models [9] and probabilistic finite automata. They both have severe theoretical drawbacks. <p> Thus, in order to prove property 1, just plug ` ` + 1; * * str fl str , and ffi ffi=m (`+2) into Equation (6) and notice that ln ffi 2m : 3.5 Running time and sample size The constraint of Lemma 3.2 and the constraints of <ref> [11] </ref> help us to set the threshold and precision param eters.
Reference: [12] <author> D. Ron, Y. Singer and N. Tishby. </author> <title> The power of amnesia: learning probabilistic automata with variable memory length. </title> <journal> Machine Learning, </journal> <volume> 25 </volume> <pages> 117-149, </pages> <year> 1996. </year>
Reference-contexts: Such models include Hidden Markov Models [9], FSM sources in data compression [10], and Markov chains with variable memory length <ref> [12] </ref>. In this paper we consider the case when the output of a Markov chain with variable memory length is passed through a memoryless noisy channel before observation. <p> Markov models with variable memory length, otherwise known as probabilistic finite suffix automata, were introduced in learning theory by Ron, Singer and Tishby who also described a polynomial time learning algorithm <ref> [11, 12] </ref>. Similar models have been studied in information theory as well [10, 13, 14, 15]. Other models in the learning theory literature include Hidden Markov Models [9] and probabilistic finite automata. They both have severe theoretical drawbacks. <p> In Section 5 we show that the same algorithm also works if we have a very good estimate of the noise. 2 Probabilistic Finite Suffix Automata This section is mainly a summary of the results of Ron et al. <ref> [12] </ref>. 2.1 How good is a hypothesis? A data sequence is defined as a string over a finite alphabet . For DNA sequences, the alphabet consists of the four bases, a set of sounds may serve as an alphabet in a pronunciation model, and so on. <p> The states of this binary PFA are Q = f1; 10; 00g, arrows represent the transition function with the transition probabilities. The initial probabilities are in brackets. (Example taken from <ref> [12] </ref>.) a binary alphabet. Remark. If the class C is the class of Markov models of order less than `, then the relative entropy is proportional to `. Thus, it is necessary to divide it by ` to obtain a distance measure independent of the prefix length considered. <p> In many cases, the generation of the next symbol depends on fewer than ` previously generated symbols. This justifies the term "variable memory length Markov chain". The PFSA learning algorithm of Ron, Singer and Tishby <ref> [12] </ref> uses a sample of prefixes of length ` + 1 that are independently generated by an `-PFSA M = hQ; ; t; fl; i. <p> The asymptotically optimal methods used for data compression are good candididates but their application is not trivial. For example, the very effective approach of 1 The original algorithm of <ref> [12] </ref> uses the notation * 0 , fl min , * 1 , and * 2 for our fl str , fl sym , * str , and * sym , respectively. Willems et al. [15] outputs a distribution over PFSA's instead of one hypothesis machine. <p> Note that we could have obtained an upper bound of the form * fl (`+1)(1+) ` but (`+1) is expected to be larger than (1 + ) for small 2 values of -. 2 3.4 Algorithm when noise is known We use the algorithm of Ron, Singer and Tishby <ref> [12] </ref>. The main structure of the learning process is sketched in Figure 2. From a sample of strings generated by the target automaton T , the algorithm estimates the string probabilities and outputs a hypothesis machine H.
Reference: [13] <author> M. J. Weinberger, A. Lempel, J. Ziv. </author> <title> A sequential algorithm for the universal coding of finite memory sources. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-38:1002-1014, </volume> <year> 1992. </year>
Reference-contexts: Markov models with variable memory length, otherwise known as probabilistic finite suffix automata, were introduced in learning theory by Ron, Singer and Tishby who also described a polynomial time learning algorithm [11, 12]. Similar models have been studied in information theory as well <ref> [10, 13, 14, 15] </ref>. Other models in the learning theory literature include Hidden Markov Models [9] and probabilistic finite automata. They both have severe theoretical drawbacks. Abe and Warmuth [1] proved that Hidden Markov Models are not learnable in time polynomial in the alphabet size unless RP=NP. <p> We mention here that similar frameworks and algorithms can be found in the information theory literature for data compression (FSM and FSMX sources, context trees, see, eg., [10], <ref> [13] </ref> and [14]). The per-symbol entropy D ` [ M k H ] =` equals the minimum expected number of extra bits when the data sequence generated by M is encoded using the hypothesis H instead of the optimal code based on M . <p> Willems et al. [15] outputs a distribution over PFSA's instead of one hypothesis machine. Naively using the tree building methods of [10] or <ref> [13] </ref> may result in an unduly large number of states, etc. In either way, using our approach (or a similar one) for learning from noisy output most likely causes similar effects in running time and sample size. Therefore, we proceed with studying the PFSA learning algorithm of Ron et al.
Reference: [14] <author> M. J. Weinberger, J. J. Rissanen, M. Feder. </author> <title> A universal finite memory source. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-41:643-652, </volume> <year> 1995. </year>
Reference-contexts: Markov models with variable memory length, otherwise known as probabilistic finite suffix automata, were introduced in learning theory by Ron, Singer and Tishby who also described a polynomial time learning algorithm [11, 12]. Similar models have been studied in information theory as well <ref> [10, 13, 14, 15] </ref>. Other models in the learning theory literature include Hidden Markov Models [9] and probabilistic finite automata. They both have severe theoretical drawbacks. Abe and Warmuth [1] proved that Hidden Markov Models are not learnable in time polynomial in the alphabet size unless RP=NP. <p> We mention here that similar frameworks and algorithms can be found in the information theory literature for data compression (FSM and FSMX sources, context trees, see, eg., [10], [13] and <ref> [14] </ref>). The per-symbol entropy D ` [ M k H ] =` equals the minimum expected number of extra bits when the data sequence generated by M is encoded using the hypothesis H instead of the optimal code based on M .
Reference: [15] <author> F. M. J. Willems, Y. M. Shtarkov, Tj. J. Tjalkens. </author> <title> The context-tree weighting method: basic properties. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-41:653-664, </volume> <year> 1995. </year>
Reference-contexts: Markov models with variable memory length, otherwise known as probabilistic finite suffix automata, were introduced in learning theory by Ron, Singer and Tishby who also described a polynomial time learning algorithm [11, 12]. Similar models have been studied in information theory as well <ref> [10, 13, 14, 15] </ref>. Other models in the learning theory literature include Hidden Markov Models [9] and probabilistic finite automata. They both have severe theoretical drawbacks. Abe and Warmuth [1] proved that Hidden Markov Models are not learnable in time polynomial in the alphabet size unless RP=NP. <p> For example, the very effective approach of 1 The original algorithm of [12] uses the notation * 0 , fl min , * 1 , and * 2 for our fl str , fl sym , * str , and * sym , respectively. Willems et al. <ref> [15] </ref> outputs a distribution over PFSA's instead of one hypothesis machine. Naively using the tree building methods of [10] or [13] may result in an unduly large number of states, etc.
References-found: 15

