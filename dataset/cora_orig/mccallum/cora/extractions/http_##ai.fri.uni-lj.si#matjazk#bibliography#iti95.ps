URL: http://ai.fri.uni-lj.si/matjazk/bibliography/iti95.ps
Refering-URL: http://ai.fri.uni-lj.si/matjazk/bibliography/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: fmatjaz.kukar, igor.kononenkog@fer.uni-lj.si  
Phone: tel: +386-61-1768386  
Title: Prognosing the femoral neck fracture recovery with machine learning  
Author: Matjaz Kukar Igor Kononenko Tomaz Silvester 
Keyword: explanation ability, learning from examples, learning in medicine, multiple knowledge.  
Address: SI-61001 Ljubljana, Slovenia  
Affiliation: University of Ljubljana Faculty of electrical engineering and computer science, Trzaska 25 Medical faculty, Zaloska 2  
Abstract: We compare the performance and explanation abilities of several machine learning algorithms in the problem of predicting the femoral neck fracture recovery. Among different algorithms, the semi naive Bayesian classifier and Assistant-R seem to be the most appropriate. We analyze the combination of decisions of several classifiers for solving the prediction problem and show that the combined classifier improves both the performance and explanation ability. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cestnik, B., Kononenko, I., & Bratko, I. </author> <year> (1987). </year> <title> ASSISTANT 86: A knowledge elicitation tool for sophisticated users. </title> <editor> In I. Bratko & N. Lavrac (Eds.), </editor> <booktitle> Progress in Machine Learning. </booktitle> <address> Wilmslow, England: </address> <publisher> Sigma Press. </publisher>
Reference-contexts: At the end of the paper we give suggestions for applications of machine learning in medical diagnostic and prognostic problems. 2 The compared algorithms In this paper we compare the following algorithms: * Assistant-I and Assistant-R are reimplementations of the Assistant learning system for top down induction of decision trees <ref> (Cestnik et al., 1987) </ref>, which use information gain and RELIEFF (Kononenko, 1994) as search heuristics, respectively. * K-nearest neighbours searches, when given a new instance, for k nearest training instances and classifies the instance into the most frequent class of these k instances. * The semi-naive Bayesian classifier (Kononenko, 1991) uses
Reference: <author> Dietterich, T. G. & Shavlik, J. W. </author> <year> (1990). </year> <title> Readings in machine learning. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In recent years series of machine learning algorithms have been developed. They can be used as efficient tools for analysis of databases and for extracting the classification knowledge, that can be used to solve new problems from the given problem domain <ref> (Dietterich & Shavlik, 1990) </ref>. A classifier is a system which classifies given instance in one of N possible classes.
Reference: <author> Kononenko, I. & Bratko, I. </author> <year> (1991). </year> <title> Information based evaluation criterion for classifier's performance. </title> <journal> Machine Learning, </journal> <volume> Vol. 6, </volume> <pages> 67-80. </pages>
Reference-contexts: decision trees (Cestnik et al., 1987), which use information gain and RELIEFF (Kononenko, 1994) as search heuristics, respectively. * K-nearest neighbours searches, when given a new instance, for k nearest training instances and classifies the instance into the most frequent class of these k instances. * The semi-naive Bayesian classifier <ref> (Kononenko, 1991) </ref> uses the simplified Bayesian formula to calculate the probability of each class given the values of all attributes. <p> Each algorithm used the same subsets of instances for learning and for testing in order to provide the same experimental conditions. Besides the classification accuracy, we also measured the average information score <ref> (Kononenko & Bratko, 1991) </ref>. This measure eliminates the influence of prior probabilities and appropriately treats probabilistic answers of a classifier. Experimental results The results of experiments on five- and two-class data sets are shown in Table 1. All of the compared algorithms achieve roughly the same classification accuracy.
Reference: <author> Kononenko, I. </author> <year> (1991). </year> <title> Semi-naive Bayesian classifier. </title> <editor> In Kodratoff, Y. (Ed.), </editor> <booktitle> Proc. European Working Session on Learning-91, </booktitle> <pages> (pp. 206-219)., </pages> <editor> Porto, Potrugal. </editor> <publisher> Springer-Verlag. </publisher>
Reference-contexts: decision trees (Cestnik et al., 1987), which use information gain and RELIEFF (Kononenko, 1994) as search heuristics, respectively. * K-nearest neighbours searches, when given a new instance, for k nearest training instances and classifies the instance into the most frequent class of these k instances. * The semi-naive Bayesian classifier <ref> (Kononenko, 1991) </ref> uses the simplified Bayesian formula to calculate the probability of each class given the values of all attributes. <p> Each algorithm used the same subsets of instances for learning and for testing in order to provide the same experimental conditions. Besides the classification accuracy, we also measured the average information score <ref> (Kononenko & Bratko, 1991) </ref>. This measure eliminates the influence of prior probabilities and appropriately treats probabilistic answers of a classifier. Experimental results The results of experiments on five- and two-class data sets are shown in Table 1. All of the compared algorithms achieve roughly the same classification accuracy.
Reference: <author> Kononenko, I. </author> <year> (1992). </year> <title> Combining decisions of multiple rules. </title> <editor> In du Boulayand, B. & Sgorev, V. (Eds.), </editor> <booktitle> Artificial intelligence V: metodology, systems, applications. </booktitle> <publisher> Elsevier Science Publications. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1993). </year> <title> Inductive and Bayesian learning in medical diagnosis. </title> <journal> Applied Artificial Intelligence, </journal> <volume> Vol. 7, </volume> <pages> 317-337. </pages>
Reference-contexts: This approach is very similar to the approach used by domain experts who often make decisions on the basis of previously known similar cases. Semi-naive Bayesian classifier The decisions of the semi-naive Bayesian classifier can be naturally interpreted as a sum of information gains <ref> (Kononenko, 1993) </ref> by every attribute to the conclusion that an instance does or does not belong to the certain class. Such information gains can then be listed in a table to sum up the evidence for/against the decision.
Reference: <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: Analysis and extensions of RELIEF. </title> <editor> In De Raedt, L. & Bergadano, F. (Eds.), </editor> <booktitle> Proc. European Conf. on Machine Learning, </booktitle> <pages> (pp. 171-182)., </pages> <address> Catania, Italy. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: applications of machine learning in medical diagnostic and prognostic problems. 2 The compared algorithms In this paper we compare the following algorithms: * Assistant-I and Assistant-R are reimplementations of the Assistant learning system for top down induction of decision trees (Cestnik et al., 1987), which use information gain and RELIEFF <ref> (Kononenko, 1994) </ref> as search heuristics, respectively. * K-nearest neighbours searches, when given a new instance, for k nearest training instances and classifies the instance into the most frequent class of these k instances. * The semi-naive Bayesian classifier (Kononenko, 1991) uses the simplified Bayesian formula to calculate the probability of each
Reference: <author> Kukar, M. </author> <year> (1993). </year> <title> An application of machine learning in the femoral neck fracture diagnosis. B.Sc. </title> <type> Thesis. </type> <institution> Ljubljana, Slovenia: University of Ljubljana, Faculty of electrical eng. & computer science. </institution> <note> In Slovene. </note>
Reference-contexts: Acknowledgements We thank Prof. Dr. Andrej Baraga and As. Dr. France Zupancic from the University Traumatology Clinic in Ljubljana for allowing access to the data and commenting preliminary results in <ref> (Kukar, 1993) </ref>. Assistant-I and Assistant-R were implemented by Edvard Simec, and LFC was reimplemented by Marko Robnik. This work was supported by the Slovenian Ministry of Science and Technology.
Reference: <author> Kukar, M. </author> <year> (1994). </year> <title> Multistrategy attribute learning. </title> <booktitle> In Proc. 3 rd electrotechnical and computer science conference ERK'94, Portoroz, Slovenia. In Slovene. </booktitle>
Reference: <author> Li, M. & Vitanyi, P. </author> <year> (1993). </year> <title> An introduction to Kolmogorov complexity and its applications. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: To prefer simpler theories is generally a good idea known also as the Occam razor principle and is also formally founded by the minimum description length principle <ref> (Li & Vitanyi, 1993) </ref>. The simpler among the theories with the identical performance is also the most probable and the most transparent due to its simplicity. However, the optimal prediction does not use only a single best theory but rather all possible theories. <p> However, the optimal prediction does not use only a single best theory but rather all possible theories. The final decision should be the weighted combinations of decisions of different theories where the weights correspond to probabilities of the theories given the evidence <ref> (Li & Vitanyi, 1993) </ref>. This principle of multiple explanations is useful also in the cases where the explanation of the decision may be even more important than the accuracy of the decision. Our experiments confirm the multiple explanations principle.
Reference: <author> Pirnat, V., Kononenko, I., Janc, T., & Bratko, I. </author> <year> (1989). </year> <title> Medical estimation of automatically induced decision rules. </title> <booktitle> In Proc. of 2nd Europ. Conf. on Artificial Intelligence in Medicine, </booktitle> <pages> (pp. 24-36). </pages> <address> City University. </address>
Reference: <author> Quinlan, J., Compton, P., Horn, K., & Lazarus, L. </author> <year> (1987). </year> <title> Inductive knowledge acquisition: A case study. </title>
Reference: <editor> In J. Quinlan (Ed.), </editor> <booktitle> Applications of expert systems. Turing Institute Press & Addison-Wesley. (Also in Proc. 2 nd Australian Conf. on Applications of Expert Systems, </booktitle> <address> Sydney, </address> <month> May </month> <year> 1986). </year>
Reference: <author> Ragavan, H. & Rendell, L. </author> <year> (1993). </year> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> In Proc. 10 th Intern. Conf. on Machine Learning, </booktitle> <pages> (pp. 252-259)., </pages> <address> Amherst, MA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The backpropagation learning procedure (Rumelhart & McClelland, 1986) implements a kind of a gradient descent and its modification weight elimination (Weigand et al., 1990) - forces the network to stay as small as possible, therefore preventing it to overfit the training data. * Lookahead feature construction <ref> (Ragavan & Rendell, 1993) </ref> is also an algorithm for top down induction of decision trees, which uses a limited lookahead to detect significant dependencies between attributes.
Reference: <author> Rumelhart, D. & McClelland, J. L. </author> <year> (1986). </year> <title> Parallel Distributed Processing, </title> <booktitle> volume 1: Foundations. </booktitle> <address> Cambridge: </address> <publisher> MIT Press. </publisher>
Reference-contexts: It is non-naive in the sense that it does not assume the attributes to be independent, although the dependencies can only be processed in a limited way. * Backpropagation neural network is a hierarchical network consisting of fully interconnected layers of processing units. The backpropagation learning procedure <ref> (Rumelhart & McClelland, 1986) </ref> implements a kind of a gradient descent and its modification weight elimination (Weigand et al., 1990) - forces the network to stay as small as possible, therefore preventing it to overfit the training data. * Lookahead feature construction (Ragavan & Rendell, 1993) is also an algorithm for
Reference: <author> Silvester, T. </author> <year> (1992). </year> <title> The rate of femoral head necrosis depending on the time interval from the injury to the internal fixation. </title> <journal> Medicinski razgledi 1992, </journal> <volume> Vol. 31, </volume> <pages> 293-313. </pages> <note> In Slovene. </note>
Reference-contexts: It describes a patient that was immediately examined and treated and therefore contributes to the classification in the "No complications" class. Such result has already been exposed in medical papers <ref> (Silvester, 1992) </ref>. Explanation abilities evaluated by a domain expert A domain expert, in our case a physician, evaluated all explanations, offered by the compared algorithms.
Reference: <author> Weigand, S., Huberman, A., & Rumelhart, D. E. </author> <year> (1990). </year> <title> Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> Vol. </volume> <pages> 1(3). </pages>
Reference-contexts: The backpropagation learning procedure (Rumelhart & McClelland, 1986) implements a kind of a gradient descent and its modification weight elimination <ref> (Weigand et al., 1990) </ref> - forces the network to stay as small as possible, therefore preventing it to overfit the training data. * Lookahead feature construction (Ragavan & Rendell, 1993) is also an algorithm for top down induction of decision trees, which uses a limited lookahead to detect significant dependencies between
References-found: 17

