URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/TM217.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts97.htm
Root-URL: http://www.mcs.anl.gov
Title: PCx User Guide  
Author: by Joseph Czyzyk, Sanjay Mehrotra, and Stephen J. Wright 
Note: This work was supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng 38.  
Date: March 1997  
Web: ANL/MCS-TM-217  
Address: 9700 South Cass Avenue Argonne, IL 60439  
Affiliation: ARGONNE NATIONAL LABORATORY  Mathematics and Computer Science Division  
Pubnum: Technical Memorandum No. 217  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. D. Andersen and K. D. Andersen, </author> <title> Presolving in linear programming, </title> <journal> Mathematical Programming, </journal> <volume> 71 (1995), </volume> <pages> pp. 221-245. </pages>
Reference-contexts: The scalar oe 2 <ref> [0; 1] </ref> in (12) is chosen by a complicated heuristic that is based on the ability of the pure affine-scaling step to attain large reductions in the duality measure before reaching the boundary of the positive orthant for the (x; s; r; w) components. <p> Given the affine-scaling step, we calculate the maximum step to this boundary in primal and dual variables from the definitions ff aff;P = inffff 2 <ref> [0; 1] </ref> j (x; w) + ff (x aff ; w aff ) 0g; (14a) ff aff;D = inffff 2 [0; 1] j (s; r) + ff (s aff ; r aff ) 0g: (14b) We then compute the duality measure aff at this point as aff = 2n (x + <p> Given the affine-scaling step, we calculate the maximum step to this boundary in primal and dual variables from the definitions ff aff;P = inffff 2 <ref> [0; 1] </ref> j (x; w) + ff (x aff ; w aff ) 0g; (14a) ff aff;D = inffff 2 [0; 1] j (s; r) + ff (s aff ; r aff ) 0g: (14b) We then compute the duality measure aff at this point as aff = 2n (x + ff aff;P x) T (s + ff aff;D s) + (w + ff aff;P w) T (r + ff aff;D <p> Similarly to (14), we calculate ff max;P = inf fff 2 <ref> [0; 1] </ref> j (x; w) + ff (x; w) 0g; (18a) and set where fl P and fl D are two scaling factors obtained from Mehrotra's adaptive steplength heuristic [7, p. 588]. <p> Presolvers significantly enhance the efficiency and robustness of both simplex and interior-point codes. The presolver in PCx works with the formulation (1) stored in the LPtype data structure. It makes use of techniques described by Andersen and Andersen <ref> [1] </ref>, checking the data for the following features: Infeasibility. Check that u i 0 for each upper bound u i , i 2 U , and that a zero row of A has a corresponding zero in the right-hand side vector b. Empty Rows. <p> Forced Rows. Sometimes, the linear constraint represented by row i of A forces all its variables to either their upper or lower bounds. An example would be the constraint 10x 3 4x 10 + x 12 = 4 subject to the bounds x 3 2 <ref> [0; +1); x 10 2 [0; 1] </ref>; x 12 2 [0; +1): In this case, we must have x 3 = 0, x 10 = 1 and x 12 = 0, so these three variables (and the corresponding row of A) can be eliminated. 10 The presolver makes multiple passes through <p> Sometimes, the linear constraint represented by row i of A forces all its variables to either their upper or lower bounds. An example would be the constraint 10x 3 4x 10 + x 12 = 4 subject to the bounds x 3 2 [0; +1); x 10 2 <ref> [0; 1] </ref>; x 12 2 [0; +1): In this case, we must have x 3 = 0, x 10 = 1 and x 12 = 0, so these three variables (and the corresponding row of A) can be eliminated. 10 The presolver makes multiple passes through the data, checking for each
Reference: [2] <author> A. R. Curtis and J. K. Reid, </author> <title> On the automatic scaling of matrices for Gaussian elimination, </title> <journal> J. Inst. Maths Applics, </journal> <volume> 10 (1972), </volume> <pages> pp. 118-124. </pages>
Reference-contexts: We refer the interested reader to Gondzio's paper for details. Our implementation draws not only on this paper and also on Gondzio's code HOPDM (version 2.13), in which slightly different heuristics from those described in the paper are used. Our code applies the scaling technique of Curtis and Reid <ref> [2] </ref> to the coefficient matrix A before solving.
Reference: [3] <author> J. R. Gilbert, E. Ng, and B. W. Peyton, </author> <title> An efficient algorithm to compute row and column counts for sparse cholesky factorization, </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 15 (1994), </volume> <pages> pp. 1075-1091. </pages>
Reference-contexts: Ng and Peyton's code uses a multiple minimum degree ordering strategy identical to the one in SPARSPAK. This strategy was introduced by Liu [5]. The scheme used for symbolic factorization is partly described by Liu [6] and Gilbert, Ng, and Peyton <ref> [3] </ref>. The numerical factorization is performed by a left-looking block sparse Cholesky algorithm, as described by Ng and Peyton [8].
Reference: [4] <author> J. Gondzio, </author> <title> Multiple centrality corrections in a primal-dual method for linear programming, </title> <journal> Computational Optimization and Applications, </journal> <volume> 6 (1996), </volume> <pages> pp. 137-156. </pages>
Reference-contexts: 1 Introduction PCx is a linear programming solver developed at the Optimization Technology Center at Argonne National Laboratory and Northwestern University. It implements a variant of Mehrotra's predictor-corrector algorithm [7] with the higher-order correction strategy of Gondzio <ref> [4] </ref>. This primal-dual approach has proved to be the most efficient interior-point method for general linear programs. The bulk of PCx is written in the C programming language. However, its main computational engine|the sparse Cholesky code of Ng and Peyton [8]|is coded in Fortran 77. <p> Gondzio's <ref> [4] </ref> higher-order correction strategy is used to enhance the search direction at each iteration. In this approach, additional centering/correction directions are computed by solving (9) for different right-hand sides. <p> Default: 3.0. dualfeastol fvalueg Specify a dual feasibility tolerance. Default: 10 8 . 11 history fyesg/fnog Request that a history file be written (yes) or not written (no). If yes, the file probname.log is written to the working directory (see Section 8). HOCorrections fyesg/fnog Request that Gondzio's <ref> [4] </ref> higher-order corrections be used to en hance the search direction. Default: yes. inputdirectory fnameg Give the directory here if PCx is to search for the MPS input files in some directory other than the working directory, give the directory here. Remember to include a trailing "/". <p> PCx solved these problems efficiently, as shown in Tables 3. The improvements obtained by using higher-order corrections are not too dramatic. Part of the reason is that the factorization routine is more efficient relative to the solution routine than is the case in, for example, HOPDM (see Gondzio <ref> [4] </ref>). It follows that there is less to be gained by economizing on matrix factorizations.
Reference: [5] <author> J. W.-H. Liu, </author> <title> Modification of the minimum degree algorithm by multiple elimination, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 11 (1985), </volume> <pages> pp. </pages> <month> 141-153. </month> <title> [6] , The role of elimination trees in sparse factorization, </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 11 (1990), </volume> <pages> pp. 134-172. </pages>
Reference-contexts: Ng and Peyton's code uses a multiple minimum degree ordering strategy identical to the one in SPARSPAK. This strategy was introduced by Liu <ref> [5] </ref>. The scheme used for symbolic factorization is partly described by Liu [6] and Gilbert, Ng, and Peyton [3]. The numerical factorization is performed by a left-looking block sparse Cholesky algorithm, as described by Ng and Peyton [8].
Reference: [7] <author> S. Mehrotra, </author> <title> On the implementation of a primal-dual interior point method, </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2 (1992), </volume> <pages> pp. 575-601. </pages>
Reference-contexts: 1 Introduction PCx is a linear programming solver developed at the Optimization Technology Center at Argonne National Laboratory and Northwestern University. It implements a variant of Mehrotra's predictor-corrector algorithm <ref> [7] </ref> with the higher-order correction strategy of Gondzio [4]. This primal-dual approach has proved to be the most efficient interior-point method for general linear programs. The bulk of PCx is written in the C programming language. <p> : : :; n; (8d) (x; s; r; w) 0: (8f) We stress that the PCx code actually works with the formulation (2); we use the simpler form (6) in our discussion solely to avoid creating a notational jungle in the next few sections. 3 The Algorithm Mehrotra's predictor-corrector algorithm <ref> [7] </ref> is based on Newton's method for the KKT conditions (4a)-(4e), modified to retain positivity of the (x; s; r; w) components, to incorporate a "centering" component in the search direction, and to improve the order of accuracy to which the search direction approximates the nonlinear equations (4d) and (4e). <p> Similarly to (14), we calculate ff max;P = inf fff 2 [0; 1] j (x; w) + ff (x; w) 0g; (18a) and set where fl P and fl D are two scaling factors obtained from Mehrotra's adaptive steplength heuristic <ref> [7, p. 588] </ref>. <p> If the solution file is written, it is named probname.out and is placed in the working directory (see Section 8). Default: yes. stepfactor fvalueg Specify a value in the range (0; 1) that is used in Mehrotra's adaptive steplength heuristic from <ref> [7, p. 118] </ref>. This value is a lower bound for fl P and fl D in (19). Default: 0:9. unrollinglevel fvalueg Specify the level of loop unrolling. Allowable values are 1, 2, 4, and 8. (This parameter is used only in the Ng-Peyton sparse Cholesky code.) Default: 4.
Reference: [8] <author> E. Ng and B. W. Peyton, </author> <title> Block sparse Cholesky algorithms on advanced uniprocessor computers, </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 14 (1993), </volume> <pages> pp. 1034-1056. </pages>
Reference-contexts: Section 3 describes the algorithm, including details of termination and infeasibility detection. Section 4 1 discusses the major computational issue in the code|factorization of a sparse, positive definite matrix|including the modifications to the Ng-Peyton code <ref> [8] </ref> needed in this context. Presolver capabilities are outlined in Section 5. Section 7 contains instructions for installing the code in a Unix environment, while instructions for invoking PCx as a stand-alone solver are given in Section 8. <p> These factorizations and triangular substitutions dominate the computational cost of the algorithm. The factorization is carried out with the sparse Cholesky code of Ng and Peyton <ref> [8] </ref>, modified slightly to handle the small pivot elements that frequently arise during later iterations of the interior-point method. <p> This strategy was introduced by Liu [5]. The scheme used for symbolic factorization is partly described by Liu [6] and Gilbert, Ng, and Peyton [3]. The numerical factorization is performed by a left-looking block sparse Cholesky algorithm, as described by Ng and Peyton <ref> [8] </ref>. The code exploits hierarchical memory by splitting the supernodes into blocks that fit into available cache. (Cache size is passed to the code as a parameter.) Loop unrolling is used to make better use of registers.
Reference: [9] <author> S. J. Wright, </author> <title> The Cholesky factorization in interior-point and barrier methods, </title> <type> Preprint MCS-P600-0596, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> May </month> <year> 1996. </year> <title> [10] , Primal-Dual Interior-Point Methods, </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, Pa., </address> <year> 1997. </year>
Reference-contexts: This substitution causes the off-diagonal elements in the ith column of the Cholesky factor L to be extremely small (essentially zero) and causes the ith component of the solution vector to be extremely small. Analysis of this technique has been performed by Wright <ref> [9] </ref>.
Reference: [11] <author> Y. Zhang, </author> <title> User's Guide to LIPSOL, </title> <institution> Department of Mathematics and Statistics, University of Maryland Baltimore County, Baltimore, Md., </institution> <month> July </month> <year> 1995. </year> <title> [12] , Solving large-scale linear programs by interior-point methods under the MATLAB en-viroment, </title> <type> Technical Report TR96-01, </type> <institution> Department of Mathematics and Statistics, University of Maryland Baltimore County, Baltimore, Md., </institution> <year> 1996. </year>
Reference: [13] <institution> Annual Energy Outlook 1996, Energy Information Administration, U. S. Department of Energy, </institution> <address> Washington, DC 20585, </address> <year> 1996. </year> <note> Document DOE/EIA-0383(96). 20 </note>
Reference-contexts: In two other cases, infeasibility was detected by the preprocessor, so the interior-point solver did not need to be called at all. The NEMS problems are instances of models in the National Energy Modeling System (NEMS) of the Energy Information Administration of the United States Department of Energy <ref> [13] </ref>. These problems are taken from NEMS modules which are used to model electricity capacity planning, petroleum marketing, and coal marketing. PCx solved these problems efficiently, as shown in Tables 3. The improvements obtained by using higher-order corrections are not too dramatic.
References-found: 10

