URL: ftp://ftp.cs.brown.edu/pub/techreports/94/cs94-36.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-94-36.html
Root-URL: http://www.cs.brown.edu/
Title: Real-Time Optical Flow  
Author: Ted Camus 
Degree: PhD Thesis  
Date: September 24, 1994  
Address: Providence, RI 02912, USA.  
Affiliation: Department of Computer Science Brown University,  
Abstract-found: 0
Intro-found: 1
Reference: [A87a] <author> P. Anandan, </author> <title> Measuring Visual Motion from Image Sequences, </title> <type> PhD Dissertation, COINS TR 87-21, </type> <institution> University of Massachusetts at Amherst, </institution> <year> 1987 </year>
Reference-contexts: Finally, since one optical flow vector is produced for each pixel of input (excepting the small + b-=2c border), optical flow measurement density is 100 percent. For these reasons, the basic correlation-based algorithm tends to be robust in practice, thus satisfying the first criteria for practical robotic vision. <ref> [A87a] </ref> and [G87] prefilter the images with band-pass filters and employ a coarse-to-fine matching strategy in which initial coarse estimates at one spatial scale are fed into the next higher resolution level. <p> Both sum-of-absolute-differences and sum-of-squared-differences could be implemented with no difference in computational time by using lookup tables for the absolute-value and squared-value functions (although the latter would require somewhat more memory to hold the larger aggregate patch match values). <ref> [A87a] </ref> chooses to use the SSD for easier mathematical analysis, since the derivatives of the SAD function are discontinuous at zero; [S91] follows suit. <p> This is a somewhat idealized example. level in the pyramid is coarser by a factor of 4, thus resolution decreases exponentially with each layer. [PFH87], [Uhr87] suggest the construction of "gentler" pyramids, with ratios somewhat less than a power-of-2 on each side. <ref> [A87a] </ref> suggests using overlapping pyramids to avoid missing motion lying on a pixel boundary. Resampling techniques used in computer graphics can help here ([F86], [Burt81], [Burt83]), however, such complexity greatly reduces the practicality of a hardware implementation of the traditional 3-dimensional pyramid.
Reference: [A87b] <author> P. Anandan, </author> <title> A Unified Perspective on Computational Techniques for the Measurement of Visual Motion, </title> <booktitle> Proceedings of the IEEE ICCV, </booktitle> <address> p.219-230, </address> <year> 1987 </year>
Reference-contexts: The strength of a pyramid structure with regard to optical flow is that it can efficiently capture large motions of large contiguous areas of an image ([A87a], <ref> [A87b] </ref>, [C90], [G87]), for example see Figure 5.5. However, the general requirement that the area of the moving surface be roughly proportional to the magnitude of motion does not always hold for interesting types of motion ([C93]).
Reference: [AA93] <author> D. Alpert, D. Avnon, </author> <title> Architecture of the Pentium Microprocessor, </title> <journal> p.11-21, </journal> <note> June 1993 IEEE Micro </note>
Reference-contexts: Conversely, personal computers generally make use of mostly integer arithmetic. Although multimedia applications will certainly increase the use of floating-point, personal computers will most likely emphasize their integer performance for the foreseeable future. Examples are Intel's Pentium (tm) <ref> [AA93] </ref> and IBM/Motorola's PowerPC (tm) 604 processor [Mot94b], 89 both of which contain multiple integer units but only a single floating-point unit. <p> In particular, CPU's intended for desktop personal computers such as Intel's x86 and the PowerPC series will be built with multiple integer functional units (but not necessarily multiple floating point units), the Pentium <ref> [AA93] </ref> and PowerPC 604 [Mot94b] being the first such examples.
Reference: [AB85] <author> E. Adelson, J. Bergen, </author> <title> Spatiotemporal Energy Models for the Perception of Motion, </title> <journal> Journal of the Optical Society of America, </journal> <volume> 2(2) </volume> <pages> 284-299, </pages> <year> 1985 </year>
Reference-contexts: the actual motion, since that would provide two linearly independent components of the velocity vector. [SS85] describes elaborated Reichardt detectors, in which the point receptive fields of the original Reichardt detectors are replaced with spatial filters, and stresses the need for effective voting rules in differentiating between different sensors' outputs. <ref> [AB85] </ref> describes the construction of phase-independent separable spatiotemporally oriented opponent-motion filters, and suggests using the ratios between multiple sensors for contrast-invariance; although higher contrast will increase the absolute 14 outputs of the motion sensors, the ratios of their outputs should remain constant. [AB85] also shows the relationship between energy models and <p> voting rules in differentiating between different sensors' outputs. <ref> [AB85] </ref> describes the construction of phase-independent separable spatiotemporally oriented opponent-motion filters, and suggests using the ratios between multiple sensors for contrast-invariance; although higher contrast will increase the absolute 14 outputs of the motion sensors, the ratios of their outputs should remain constant. [AB85] also shows the relationship between energy models and Reichardt detectors. [WM93] first convolves the image with a set of linear, separable spatiotemporal filters and applies the brightness constancy equation (equation 2.1) to each.
Reference: [AD91] <author> M. Allmen, C. Dyer, </author> <title> Long-Range Spatiotemporal Motion Understanding Using Spatiotemporal Flow Curves, </title> <booktitle> Proceedings of the IEEE CVPR, </booktitle> <address> p.303-309, </address> <year> 1991 </year>
Reference-contexts: option is to run the algorithm using two patch sizes; one would expect that the "quality" of the match of the larger patch would be much worse than that of the smaller patch, when both are placed at the motion boundary [LBP88]; this could run completely in parallel as well. <ref> [AD91] </ref> notes that often precise boundary detection is not necessary for accurate motion perception; for example, Johansson [J73] demonstrated that people can easily identify a walking person while only viewing lights attached to the moving person's joints (see also [J75]). <p> Given that the algorithm runs best with a temporally dense set of images (i.e. a fast frame rate), 3-dimensional spatio-temporal coherence is an excellent candidate for furthering this algorithm. Occlusion and disocclusion detection can also be handled by spatiotemporal coherence <ref> [AD91] </ref>. Detecting multiple moving objects requires an initial segmentation step. [SRT92] considers a FOE triangle created by the intersection of any three optical flow vectors on an object in segmenting out objects.
Reference: [AP93] <author> N.Ancona, T.Poggio, </author> <title> Optical Flow from 1D Correlation: </title> <booktitle> Application to a Time-To-Crash Detector,Fourth International Conference on Computer Vision,p.209-214,1993 </booktitle>
Reference-contexts: Furthermore, although matching methods such as these can do well with translating images, they often have difficulty with diverging images [BFBB93], such as will be analyzed later in this thesis (Chapter 4). Pyramid structures are discussed further in Section 5.4. 18 <ref> [AP93] </ref> computes optical flow only along a single dimension; often, the vector sum of two one-dimensional optical flow vectors is sufficiently qualitatively similar to the two-dimensional optical flow to be used in certain robotic vision tasks. <p> Using Green's theorems, the divergence of the optical flow field can be estimated by intergrating the optical flow normal to a closed contour. By using the estimated divergence of the flow field, time-to-contact is calculated with errors reported on the order of 10% <ref> [AP93] </ref>. The simplicity of this approach makes it a plausible candidate for biological implementations. [Hub94a] uses the correlation of 2-dimensional image patches along a single dimension ([AP93], [BLP89a]) to simulate the navigation of a fly in a synthetic maze using optical flow, at a rate of about one frame per second <p> It would be very difficult to locate any distinctive features in Figure 4.3, and extremely difficult if not impossible to localize them to subpixel resolution. <ref> [AP93] </ref> uses Green's theorems and a one-dimensional correlation-based optical flow technique to estimate the divergence of the optical flow field by integrating the optical flow normal to a closed contour; for the application of time-to-contact they report errors on the order of 10%. There are two issues worth mentioning here. <p> Hardware implementations could take parallelism to an even finer scale, and includes 93 implementation on special-purpose image processors (such as the PIPE or Datacube) and in VLSI [O85]. Although this algorithm would be much more difficult to implement in VLSI in its entirety than say the one-dimensional technique of <ref> [AP93] </ref>, the simple nature of the box filter makes it suitable for implementation in VLSI, Field-Programmable Gate Arrays (FPGA's), systolic arrays, or digital signal processors.
Reference: [ATYM] <institution> P.K.Allen,A.Timcenko,B.Yoshimi,P.Michelman,Automated Tracking and Grasping of a Moving Object with a Robotic Hand-Eye System, Columbia University, </institution> <note> accepted for publication IEEE Transactions on Robotics and Automation </note>
Reference-contexts: Authors rarely report the computational time needed for their algorithms; when they do, it is on the order of many minutes per frame ([WM93]), or require specialized hardware such as a Connection Machine ([BLP89a], [DN93], [SU87], [WZ91], [L88]), Datacube ([LK93], [N91]), custom image processors [DW93], or PIPE [KSL85], [WWB88], <ref> [ATYM] </ref>. Techniques which can run in real-time often impose strict restrictions on the environment; [HB88] presents a technique that can segment in real-time for tracking purposes, but requires that a textured object be moving in front of a relative textureless background.
Reference: [AWB87] <author> J. Aloimonos, I. Weiss, A. Bandyopadhyay, </author> <title> Active Vision, </title> <booktitle> Proceedings of the IEEE ICCV, </booktitle> <year> 1987 </year>
Reference-contexts: Unfortunately this may just prove to be too difficult. Conversely, a bottom-up approach would declare that the general problem is too difficult, and impose constraints on the problem at hand (Active perception can be seen as an implementation of this philosophy; see <ref> [AWB87] </ref>, [Bal90], [CF88]). Indeed, the approach taken by this author is to determine what information is available from the optical flow (in particular, quantized measurements) and verify what can be calculated from them.
Reference: [BA93] <author> S. Bhandarkar, H. Arabnia, </author> <title> The Multi-Ring Reconfigurable Network for Computer Vision, </title> <booktitle> p.180-190, Workshop on Computer Architectures for Machine Perception, </booktitle> <address> New Orleans Louisiana, </address> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alamitos CA, </address> <year> 1993 </year>
Reference-contexts: Flexible network topologies such as described in <ref> [BA93] </ref> would likely be required for a massively parallel system to be able to operate on a wide variety of computer vision problems.
Reference: [Bal90] <author> D. Ballard, </author> <title> Animate Vision, </title> <institution> University of Rochester Technical Report 329, </institution> <month> Feb. </month> <year> 1990 </year>
Reference-contexts: Unfortunately this may just prove to be too difficult. Conversely, a bottom-up approach would declare that the general problem is too difficult, and impose constraints on the problem at hand (Active perception can be seen as an implementation of this philosophy; see [AWB87], <ref> [Bal90] </ref>, [CF88]). Indeed, the approach taken by this author is to determine what information is available from the optical flow (in particular, quantized measurements) and verify what can be calculated from them.
Reference: [Br78] <author> R. Bracewell, </author> <title> The Fourier Transform and Its Applications, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1978 </year>
Reference: [B81] <author> H. Buelthoff, </author> <title> Figure-Ground Discrimination in the Visual System of Drosophila melanogaster, </title> <booktitle> Biological Cybernetics 41 p.139-145, </booktitle> <year> 1981 </year>
Reference-contexts: By accurately computing this 2-D vector field, it is in principle possible to calculate three-dimensional properties of the environment, and quantities such as time-to-contact with an an observed object [Lee76]. Biological organisms make considerable use of optical flow, such as the detection of discontinuities [PRH81], and figure-ground discrimination 7 <ref> [B81] </ref>.
Reference: [BB88] <author> A. Borst, S. Bahde, </author> <title> Spatio-Temporal Integration of Motion, </title> <address> Naturwissenschaften 75, p.265-267, </address> <year> 1988 </year>
Reference-contexts: This philosophy seems quite consistent with biological implementations, such as the spatial integration of simple correlation-type motion detectors found in the fly [EB93] (although the fly does not appear to explicitly calculate time-to-contact <ref> [BB88] </ref>). 4.4 Effects of Heuristic Temporal Antialiasing Section 3.3 discussed the temporal aliasing which can occur as a result of the space-time tradeoff of the linear-time optical flow algorithm. This section will demonstrate that for the application of time-to-contact, the heuristic described is effective in improving results.
Reference: [BBH+89] <author> P. Burt, J. Bergen, R. Hingorani, R. Kolezynski, W. Lee, A. Leung, J. Lubin, H. Shvaytser, </author> <title> Object Tracking With a Moving Camera, </title> <booktitle> IEEE 1989 Workshop on Visual Motion, </booktitle> <address> p.2-12, </address> <month> March </month> <year> 1989 </year>
Reference-contexts: In cases were more precise boundary detection is 107 translating left, with the resulting optical flow and grey-level shaded relative depth map. desired however, one option is to zoom in on boundaries using a pyramid technique such as in <ref> [BBH+89] </ref>. Figure 6.2 shows the optical flow algorithm run on the same SRI tree sequence as in Figure 3.9, except that the image is zoomed by a factor of 2, and shows the outline of the branches much more clearly.
Reference: [BBHP90] <author> J. Bergen, P. Burt, R. Hingorani, S. Peleg, </author> <title> Computing Two 118 Motions from Three frames, </title> <booktitle> Proceedings of the IEEE ICCV, </booktitle> <address> p.27-32, </address> <month> August </month> <year> 1990 </year>
Reference-contexts: These techniques can be slow to converge and 106 difficult to apply to practical problems however. <ref> [BBHP90] </ref> describes six types of basic motion: 1) single object motion; 2) two moving surfaces separated by a boundary; 3) two transparent surfaces with different motions, including moving shadows, reflections, and transparent surfaces; 4) interleaved components; 5) small moving object in front of moving background; and 6) two motions each of <p> One possibility to handle transparency is to use an iterative procedure as in <ref> [BBHP90] </ref>. Here two motions are hypothesized, p and q. They define a sequence of difference images : 108 D i I (x; y; i + 1) I p (x; y; i) where I p (x; y; 1) is image 1 transformed by the motion p.
Reference: [BFBB92] <author> J. Barron, D. Fleet, S.S. Beauchemin, T. Burkitt, </author> <title> Performance of optical Flow Techniques, </title> <booktitle> Proceedings of the IEEE CVPR, </booktitle> <address> p.236-242, </address> <year> 1992. </year>
Reference-contexts: However, for a sufficiently textured surface, the optical flow field will be arbitrarily close to the motion field. Many techniques for optical flow exist (see <ref> [BFBB92] </ref> and [LV89] for reviews and discussions of several techniques). Although these techniques can perform very well for certain sequences of images, there are very few that are currently able to support real-time performance. <p> This technique is essentially a differential method applied to phase rather than intensity <ref> [BFBB92] </ref>. The full 2-dimensional velocity field is determined by fitting a linear velocity field to the component velocities. <p> Note that this additional search is limited to the number of velocities searched over, i.e. it is still a strictly linear algorithm. In addition, we limit our look-ahead to only one frame. Although techniques that use considerable temporal support can perform quite well <ref> [BFBB92] </ref>, in real-time robotics it is not acceptable to impose a latency of a large number of images before a result is produced. The lookahead of a single image used by this algorithm is a relatively modest requirement. <p> It would be pointless to attempt to measure the performance of this particular algorithm per se using the methods used in <ref> [BFBB92] </ref> and [WM93]; since the individual optical flow vectors are quantized in magnitude and direction, the average error for a single pixel would be very poor.
Reference: [BFBB93] <author> J. Barron, D. Fleet, S.S. Beauchemin, T. Burkitt, </author> <title> Performance of Optical Flow Techniques, </title> <type> Revised Tech Report RPL-TR-9107, </type> <institution> Queen's University, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: One option for improving performance for these methods is to only calculate optical flow where the spatial derivatives are large (i.e. the gradient is steep), such as is discussed in <ref> [BFBB93] </ref>, however this can result in a sparse motion field which may be inadequate for some problems. <p> Furthermore, although matching methods such as these can do well with translating images, they often have difficulty with diverging images <ref> [BFBB93] </ref>, such as will be analyzed later in this thesis (Chapter 4). <p> In particular optical flow methods based on numerical differentiation are notoriously sensitive to noise. Although high-resolution CCD cameras are available, they can be very expensive. Optical flow methods employing extensive filtering can be very accurate but are computationally intensive <ref> [BFBB93] </ref>. Even considering the yearly exponential growth in available computational power, practical applications may still be elusive considering the cost of a quality CCD camera and high-resolution framebuffer. <p> Identify the stationary regions in the registered images, using M (t) as an initial guess, and set M (t + 1) to the new tracked region. 109 4. Update the temporally integrated image Av (t + 1), and repeat. <ref> [BFBB93] </ref>, [NG92], [VG92], [SAH91] make use of confidence measures in calculating optical flow. Conversely, the "winner-take-all" nature of this algorithm makes no use of confidence measures whatsoever, instead relying on the algorithm's inherent smoothness in producing a 100% dense output.
Reference: [BJ94] <author> M. Black, A. Jepson, </author> <title> Estimating Multiple Independent Motions in Segmented Images using Parametric Models with Local Deformations, </title> <booktitle> IEEE Workshop on Motion of Non-Rigid and Articulated Objects, </booktitle> <address> Austin, Texas, </address> <month> Nov. </month> <year> 1994 </year>
Reference-contexts: Here the least-squares solution for equation 2.2 is solved in a local N xN neighborhood, under the assumption that local velocity is locally constant. This can run at several frames per second on a Connection-Machine 2 (depending on the size of the neighborhood N ); see also [DNS92]. <ref> [BJ94] </ref> minimizes an objective function consisting of three terms : one, the standard optical flow intensity constraint equation; two, the differences with the optical flow in neighboring pixels; and three, the difference with a parametric planar patch estimate of the optical flow in regions of approximately constant intensity.
Reference: [BLP89a] <author> H. Buelthoff, J. Little, T. Poggio, </author> <title> A Parallel Algorithm for Real-time Computation of Optical Flow, </title> <booktitle> Nature 337(6207) </booktitle> <pages> 549-553, </pages> <month> 9 Feb </month> <year> 1989 </year>
Reference-contexts: As a result robotic vision researchers are frustrated by an inability to obtain reliable optical flow estimates in real-world conditions, and practical applications for optical flow algorithms remain scarce. Algorithms based on the correlation of image patches (e.g. <ref> [BLP89a] </ref>) have been shown to be robust in practice but are in general infeasible due to their computational complexity. <p> If certain assumptions are enforced, however, then the problem becomes well-posed, and can be satisfactorily solved in most cases. A relatively noise-resistant method of calculating optical flow is described in <ref> [BLP89a] </ref>, [BLP89b] and summarized here. We will assume that the maximum possible displacement for any pixel is limited to in any direction. The actual value of depends on the expected velocities of the pixels in the image plane. This is shown in Figure 2.4. <p> By using the estimated divergence of the flow field, time-to-contact is calculated with errors reported on the order of 10% [AP93]. The simplicity of this approach makes it a plausible candidate for biological implementations. [Hub94a] uses the correlation of 2-dimensional image patches along a single dimension ([AP93], <ref> [BLP89a] </ref>) to simulate the navigation of a fly in a synthetic maze using optical flow, at a rate of about one frame per second using 512x512 images [Hub94b]. <p> A genetic algorithm is used to train the artificial fly's optomotor response to turbulent air conditions to prevent collisions with the maze's walls. The independence of one pixel's chosen displacement from all other pixels' displacements motivates massive parallel implementations such as the one implemented on the connection machine <ref> [BLP89a] </ref>. But even these massively parallel implementations are only "close-to-real-time", on the order of seconds per frame. <p> Certainly, without the luxury of custom image processors, other techniques must be used for practical operation with conventional serial computers due to the search-based nature of the optical flow algorithm itself, which is described next. 20 Chapter 3 Linear-Time Optical Flow Since correlation-based optical flow algorithms (e.g. <ref> [BLP89a] </ref>) satisfy the first criteria for practical robotic vision, that of robustness, we will use it as a starting point. The second criteria for practical robotic vision is computational efficiency. <p> However, the pixel-matching technique can still be successful based on the simple argument presented in performing a degenerate form of binary feature matching, as in <ref> [BLP89a] </ref>. [ZL93] notes many of the computational advantages in using a 1-bit feature map (in their case, an edge map) over full 8-bit grey-level images. The optical flow for these images is shown in Figures 4.22-4.24. <p> Row-wise partial sums. Complete box-sums. solute value can be easily calculated by the expression : abs (x) = (x^(x&gt;>31))-(x>>31), where ^ is the standard C operator for bitwise XOR and &gt;> represents an arithmetic shift. Other schemes such as in <ref> [BLP89a] </ref> suggest the ANDing of certain features such as edges. <p> The next step is referred to as the "Excitation" stage in <ref> [BLP89a] </ref>, or the local neighborhood summation stage, as demonstrated in Figure 5.1. Here each element of the absolute difference (or squared difference) array is replaced with the sum of all elements in a local neighborhood - fl in size. In Figure 5.1, the neighborhood is 3x3 in size. <p> At a maximum, in the current implementation, all 64 rows/columns in all 9 directions (8 directions plus zero motion) for each of 20 speeds could be usefully parallelized, exploiting up to 11520 separate processing nodes. Running the algorithm by processing individual pixels on separate processors (as in <ref> [BLP89a] </ref>) is not particularly recommended on a typical modern parallel machine which uses tens or hundreds of much more powerful microprocessors instead of the 1-bit processors of a Connection Machine-2 [TM88], since I/O and communication latencies are then likely to become the bottlenecks.
Reference: [BLP89b] <author> H. Buelthoff, J. Little, T. Poggio, </author> <title> A Parallel Motion Algorithm Consistent with Psychophysics and Physiology, </title> <booktitle> IEEE 1989 Workshop on Visual Motion, </booktitle> <pages> 165-172, </pages> <month> March </month> <year> 1989 </year>
Reference-contexts: Only the motion along the direction of the gradient ( @E @x ; @E @y ) is available. This is known as the aperture problem [MU81], [NS88] and is illustrated in Figure 2.2. The left of Figure 2.2 shows an instance of the strong 9 aperture problem <ref> [BLP89b] </ref>. <p> If certain assumptions are enforced, however, then the problem becomes well-posed, and can be satisfactorily solved in most cases. A relatively noise-resistant method of calculating optical flow is described in [BLP89a], <ref> [BLP89b] </ref> and summarized here. We will assume that the maximum possible displacement for any pixel is limited to in any direction. The actual value of depends on the expected velocities of the pixels in the image plane. This is shown in Figure 2.4. <p> With a large neighborhood, ties are relatively rare and are decided arbitrarily. 17 This algorithm has many desirable properties. Due to the two-dimensional scope of the matching window, this algorithm generally does not suffer from the aperture problem except in extreme cases <ref> [BLP89b] </ref>, and tends to be very resistant to random noise. In fact the algorithm's "winner-take-all" nature does not even require that the calculated match strengths have any relation whatsoever to what their values should theoretically be, it is only necessary that their relative ordering remains correct. <p> If the contour shown extended infinitely with no end markers or distinguishing illumination changes, then we would have an instance of the strong aperture problem <ref> [BLP89b] </ref>, which in principle cannot be solved. <p> Using the technique of <ref> [BLP89b] </ref> (the same as described in Section 2.3) on a Datacube MaxVideo 200, [LK93] calculates optical flow within a limited radius of +/- 3 pixels horizontally and +/- 2 pixels vertically using a 7x7 correlation window at 10 Hz on 128x120 images. <p> We will use single-pixel-shift units as a basic measure, i.e. the comparison of a single given pixel with a single hypothesized pixel shift. Recall that in the traditional algorithm ([BLP89a], <ref> [BLP89b] </ref>) only two frames are used, and a pixel is constrained to lie within a certain neighborhood, in their case 7x5. Motion is calculated everywhere except for a border region where motion calculation would require pixels not available in the image itself.
Reference: [Bor90] <author> A. Borst, </author> <title> How Do Flies Land ?, BioScience 40(4): </title> <type> 292-299, </type> <month> April </month> <year> 1990 </year>
Reference: [Burt81] <author> P. Burt, </author> <title> Fast Filter Transforms for Image Processing, </title> <booktitle> Computer Graphics and Image Processing 16 </booktitle> <pages> 20-51, </pages> <year> 1981 </year>
Reference-contexts: Resampling techniques used in computer graphics can help here ([F86], <ref> [Burt81] </ref>, [Burt83]), however, such complexity greatly reduces the practicality of a hardware implementation of the traditional 3-dimensional pyramid. <p> For example, spatial resampling techniques can be used in the construction of pyramids [F86]. <ref> [Burt81] </ref> presents a Hierarchal Discrete Correlation technique which can very efficiently approximate a Gaussian kernel. It remains to be seen how useful antialiasing techniques such as in [SS93] are applicable to the problems described in this thesis.
Reference: [Burt83] <author> P. Burt, </author> <title> Fast Algorithms for Estimating Local Image Properties, Computer Vision, Graphics, </title> <booktitle> and Image Processing 21 </booktitle> <pages> 368-382, </pages> <year> 1983 </year>
Reference-contexts: Resampling techniques used in computer graphics can help here ([F86], [Burt81], <ref> [Burt83] </ref>), however, such complexity greatly reduces the practicality of a hardware implementation of the traditional 3-dimensional pyramid.
Reference: [C90] <author> T. Camus, </author> <title> Applications of Pyramid Structures to Multiscale Optical Flow, </title> <institution> Brown University Technical Report CS-90-09, </institution> <month> August </month> <year> 1990 </year>
Reference-contexts: The strength of a pyramid structure with regard to optical flow is that it can efficiently capture large motions of large contiguous areas of an image ([A87a], [A87b], <ref> [C90] </ref>, [G87]), for example see Figure 5.5. However, the general requirement that the area of the moving surface be roughly proportional to the magnitude of motion does not always hold for interesting types of motion ([C93]).
Reference: [C91] <author> T. Camus, </author> <title> Accelerating Optical Flow Computations in VLSI, </title> <note> unpublished report, Brown University January 1991 </note>
Reference-contexts: It is possible to perform the bulk of the computations in customized silicon chips; one such (partial) implementation has been done in CMOS <ref> [C91] </ref>. [DW93] calculates structure-from-motion using the same basic shift-match-winner-take-all algorithm implemented on the Image Understanding Architecture simulator; they report an estimated 0.54 seconds per frame using a maximum possible displacement = 20. [LK93] calculates optical flow (for the purposes of tracking) within a limited radius (+/- 2 pixels vertically, +/- 3 <p> At the finest scale, each row of an image can processed individually by a pipelined VLSI chip and fed into further processing modules, as has been shown by a partial implementation in CMOS VLSI <ref> [C91] </ref>. At a maximum, in the current implementation, all 64 rows/columns in all 9 directions (8 directions plus zero motion) for each of 20 speeds could be usefully parallelized, exploiting up to 11520 separate processing nodes.
Reference: [C93] <author> T. Camus, </author> <type> Thesis Proposal, </type> <institution> Brown AI Memo 93-1005, </institution> <month> October </month> <year> 1993 </year>
Reference-contexts: Nonetheless, <ref> [C93] </ref> proposed the thesis that optical flow could indeed be be computed in real-time using computing power that is appropriate and practical for a mobile robot, that the optical flow thus produced would be robust, and that it would be sufficiently accurate to be used for certain robotic vision tasks. <p> This algorithm has been successively used on many real and synthetic image sequences for a variety of real-time robotic vision tasks ([CB91], <ref> [C93] </ref>, [C94b]; see also Section 6.1). [Du94a] reports being able to use this optical flow algorithm to instantiate some fly-like control laws ([War88]) in a small mobile robot. <p> used throughout this thesis. 4.1 Calculating Time-to-Contact The primary example of quantitative computer vision used throughout this thesis will be the calculation of time-to-contact during a collision sequence of a small mobile robot with a sweatshirt draped over a chair as in Figure 4.1 (many qualitative examples also appear in <ref> [C93] </ref>). The sweatshirt is used so that there is large enough of a surface area, and so that the robot's camera can actually make contact with the sweatshirt safely without risk to the camera or robot. The sequence begins with frame 0 on the right of Figure 4.1.
Reference: [C94a] <author> T. Camus, </author> <title> Real-Time Optical Flow, SME Technical Paper MS94-176, </title> <booktitle> MVA/SME Applied Machine Vision 1994, </booktitle> <address> Minneapolis Minnesota, </address> <month> June </month> <year> 1994 </year>
Reference: [C94b] <author> T. Camus, </author> <title> Calculating Time-to-Collision with Real-time Optical Flow, </title> <booktitle> SPIE Visual Communications and Image Processing, </booktitle> <address> Chicago IL, </address> <month> Sept. </month> <year> 1994 </year>
Reference-contexts: This algorithm has been successively used on many real and synthetic image sequences for a variety of real-time robotic vision tasks ([CB91], [C93], <ref> [C94b] </ref>; see also Section 6.1). [Du94a] reports being able to use this optical flow algorithm to instantiate some fly-like control laws ([War88]) in a small mobile robot.
Reference: [CAMP93] <institution> Workshop on Computer Architectures for Machine Perception, </institution> <address> New Orleans Louisiana, </address> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alamitos CA, </address> <year> 1993 </year>
Reference-contexts: From [O85] : "It is only by using machines with [levels of performance in the Giga operations per second] that significant real-time image processing becomes a realistic 83 proposition." Entire workshops <ref> [CAMP93] </ref> and specialized texts [Uhr87], [O85] are devoted to custom hardware or massively parallel implementations of computer vision algorithms. For practical mobile robot vision, considerations must be given to efficiency. One factor related to performance is an efficient representation and algorithm used to analyze the image (s).
Reference: [Cas93] <author> B. </author> <title> Case, SPARC V9 Adds a Wealth of New Features, </title> <type> Microprocessor Report, </type> <month> Feb. 15 </month> <year> 1993 </year>
Reference-contexts: overflow. */ mask = (best_match - cand_match) &gt;> 31; /* arithmetic shift */ best_match = ( best_match & mask) | ( cand_match &~ mask); best_motion = (best_motion & mask) | (cand_motion &~ mask); branches by using a "conditional move" operation, such as found on the Alpha [McL93] and SPARC V9 <ref> [Cas93] </ref> architectures. Often, some tight loops are memory limited (i.e. byte loads and stores dominate). <p> Such memory accesses are generally sequential and highly predictable, and thus can take advantage of instructions that provide memory-access "hints" as are found on the Alpha [DEC92], PA-RISC [hp94], PowerPC [Mot94a], and SPARC V9 <ref> [Cas93] </ref> architectures. Thus this optical flow algorithm is well-suited for operation on common personal computers, since it does not require the floating point capabilities of the much more expensive scientific workstations, and can achieve a more balanced memory-bandwidth to computation ratio.
Reference: [CB91] <author> T. Camus, H. Buelthoff, </author> <title> Space-Time Tradeoffs for Adaptive Real-Time Tracking, Mobile Robots VI, </title> <editor> William J. Wolfe, Wendall H. Chun ed., </editor> <booktitle> Proc. SPIE 1613, </booktitle> <address> p.268-276, </address> <month> Nov. </month> <year> 1991 </year>
Reference-contexts: It works for this example, and one can argue that the potential for aliasing is only limited by the sampling rate. Therefore, it might seem that the motion B ) C was correct, based on temporal aliasing arguments. This solution was presented in <ref> [CB91] </ref>, and was a reasonable solution at the time, because available computational power limited practical motion detection to a small number of temporal delays. Now however, delays of as much as ffit = 20 frames are practical and have been successfully tested. In these cases, the "fastest first" rule [CB91] too <p> in <ref> [CB91] </ref>, and was a reasonable solution at the time, because available computational power limited practical motion detection to a small number of temporal delays. Now however, delays of as much as ffit = 20 frames are practical and have been successfully tested. In these cases, the "fastest first" rule [CB91] too often selects an incorrect faster motion over a slower, better matching motion. The Temporal Aperture Problem is an example of this phenomenon.
Reference: [CF88] <author> J. Clark, N. Ferrier, </author> <title> Modal Control of an Attentive Vision System, </title> <booktitle> Proceedings of the IEEE ICCV, </booktitle> <address> p.514-523, </address> <year> 1988 </year>
Reference-contexts: Unfortunately this may just prove to be too difficult. Conversely, a bottom-up approach would declare that the general problem is too difficult, and impose constraints on the problem at hand (Active perception can be seen as an implementation of this philosophy; see [AWB87], [Bal90], <ref> [CF88] </ref>). Indeed, the approach taken by this author is to determine what information is available from the optical flow (in particular, quantized measurements) and verify what can be calculated from them.
Reference: [D86] <author> M. Drumheller, </author> <title> Connection Machine Stereomatching, </title> <booktitle> Proceedings of the 1986 AAAI, </booktitle> <address> Philadelphia PA, p.748-753, </address> <month> August </month> <year> 1986 </year>
Reference: [DEC92] <institution> Alpha Architecture Handbook, </institution> <note> Preliminary Edition, </note> <institution> Digital Equipment Corp., Maynard, </institution> <address> MA, </address> <year> 1992 </year>
Reference-contexts: Such memory accesses are generally sequential and highly predictable, and thus can take advantage of instructions that provide memory-access "hints" as are found on the Alpha <ref> [DEC92] </ref>, PA-RISC [hp94], PowerPC [Mot94a], and SPARC V9 [Cas93] architectures. Thus this optical flow algorithm is well-suited for operation on common personal computers, since it does not require the floating point capabilities of the much more expensive scientific workstations, and can achieve a more balanced memory-bandwidth to computation ratio.
Reference: [DN93] <author> A. Del Bimbo, P. Nesi, </author> <title> Optical Flow Estimation on the Connection 119 Machine 2, </title> <booktitle> p.267-274, Workshop on Computer Architectures for Machine Perception, </booktitle> <address> New Orleans Louisiana, </address> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alamitos CA, </address> <year> 1993 </year>
Reference-contexts: Authors rarely report the computational time needed for their algorithms; when they do, it is on the order of many minutes per frame ([WM93]), or require specialized hardware such as a Connection Machine ([BLP89a], <ref> [DN93] </ref>, [SU87], [WZ91], [L88]), Datacube ([LK93], [N91]), custom image processors [DW93], or PIPE [KSL85], [WWB88], [ATYM]. <p> An estimate ffiE ffit of the temporal derivative @E @t can be provided using a forward-difference method : ffiE ffit = E 1 E 0 t 1 t 0 . The spatial derivative @E @x can be similarly estimated using the pixels adjacent to pixel p. <ref> [DN93] </ref> calculates gradient-based optical flow using the multipoint technique for solving partial differential equations. Here the least-squares solution for equation 2.2 is solved in a local N xN neighborhood, under the assumption that local velocity is locally constant.
Reference: [DNS92] <author> A. Del Bimbo, P. Nesi, J. Sanz, </author> <title> Optical Flow Computation Using Extended Constraints, </title> <institution> Department of Systems and Informatics Tech Report DSI-RT 19/92, Faculty of Engineering, University of Florence, Italy, </institution> <year> 1992 </year>
Reference-contexts: Here the least-squares solution for equation 2.2 is solved in a local N xN neighborhood, under the assumption that local velocity is locally constant. This can run at several frames per second on a Connection-Machine 2 (depending on the size of the neighborhood N ); see also <ref> [DNS92] </ref>. [BJ94] minimizes an objective function consisting of three terms : one, the standard optical flow intensity constraint equation; two, the differences with the optical flow in neighboring pixels; and three, the difference with a parametric planar patch estimate of the optical flow in regions of approximately constant intensity.
Reference: [Du94a] <author> A. Duchon, </author> <title> Robot Navigation from a Gibsonian Viewpoint. </title> <booktitle> 1994 IEEE International Conference on Systems, Man and Cybernetics, </booktitle> <address> San Antonio, Texas. </address> <month> October 2-5, </month> <year> 1994. </year> <booktitle> IEEE, </booktitle> <address> Piscataway, NJ, </address> <pages> pp 2272-2277. </pages>
Reference-contexts: This algorithm has been successively used on many real and synthetic image sequences for a variety of real-time robotic vision tasks ([CB91], [C93], [C94b]; see also Section 6.1). <ref> [Du94a] </ref> reports being able to use this optical flow algorithm to instantiate some fly-like control laws ([War88]) in a small mobile robot.
Reference: [Du94b] <author> A. </author> <title> Duchon, </title> <journal> Ecological Robotics. </journal> <note> in preparation, </note> <year> 1994. </year>
Reference-contexts: With a frame rate of 4-5 frames per second and the robot moving at 4-5 cm per second, the robot is able to 25 boundary, its intensity is "split" between the two adjacent pixels in the image. successfully maneuver through an unmodified office environment. Recently, <ref> [Du94b] </ref> has also been able to use the algorithm to play the game of tag with a cardboard target, and even with a slow-moving person, using only optical flow for pursuit, docking, and escape. 3.2 Temporal Aperture Problem The standard aperture problem has already been discussed in Section 2.1.
Reference: [DW93] <author> R. Dutta. C. Weems, </author> <title> Parallel Dense Depth from Motion on the Image Understanding Architecture, </title> <booktitle> Proceedings of the IEEE CVPR, </booktitle> <address> p.154-159, </address> <year> 1993 </year>
Reference-contexts: Authors rarely report the computational time needed for their algorithms; when they do, it is on the order of many minutes per frame ([WM93]), or require specialized hardware such as a Connection Machine ([BLP89a], [DN93], [SU87], [WZ91], [L88]), Datacube ([LK93], [N91]), custom image processors <ref> [DW93] </ref>, or PIPE [KSL85], [WWB88], [ATYM]. Techniques which can run in real-time often impose strict restrictions on the environment; [HB88] presents a technique that can segment in real-time for tracking purposes, but requires that a textured object be moving in front of a relative textureless background. <p> It is possible to perform the bulk of the computations in customized silicon chips; one such (partial) implementation has been done in CMOS [C91]. <ref> [DW93] </ref> calculates structure-from-motion using the same basic shift-match-winner-take-all algorithm implemented on the Image Understanding Architecture simulator; they report an estimated 0.54 seconds per frame using a maximum possible displacement = 20. [LK93] calculates optical flow (for the purposes of tracking) within a limited radius (+/- 2 pixels vertically, +/- 3 pixels <p> It should also be noted that even hardware implementations such as described in <ref> [DW93] </ref> could benefit from the linear-time space-time tradeoff described in this thesis since only a linear number of pixel shifts would be calculated instead of a quadratic number.
Reference: [EB93] <author> M. Egelhaaf, A. Borst, </author> <title> Motion Computation and Visual Orientation in Flies, </title> <journal> Comp. Biochem. Physiol., </journal> <volume> 104A(4):659-673, </volume> <year> 1993 </year>
Reference-contexts: Although contrast-sensitive elementary motion detectors may be sufficient for very small (and lightweight) insects such as the fly ([Bor90], <ref> [EB93] </ref>) they are clearly insufficient for mobile robots. In addition, note that the image is badly out of focus close to the target. Actual contact with the sweatshirt was made somewhere between frames 141 and 142 (nominally 141.5). <p> This philosophy seems quite consistent with biological implementations, such as the spatial integration of simple correlation-type motion detectors found in the fly <ref> [EB93] </ref> (although the fly does not appear to explicitly calculate time-to-contact [BB88]). 4.4 Effects of Heuristic Temporal Antialiasing Section 3.3 discussed the temporal aliasing which can occur as a result of the space-time tradeoff of the linear-time optical flow algorithm.
Reference: [F86] <author> K. Fant, </author> <title> A Nonaliasing, Real-Time Spatial Transform Technique, </title> <journal> IEEE Computer Graphics and Applications 6 </journal> <pages> 71-80, </pages> <year> 1986 </year>
Reference-contexts: For example, spatial resampling techniques can be used in the construction of pyramids <ref> [F86] </ref>. [Burt81] presents a Hierarchal Discrete Correlation technique which can very efficiently approximate a Gaussian kernel. It remains to be seen how useful antialiasing techniques such as in [SS93] are applicable to the problems described in this thesis.
Reference: [Fl92] <author> D. </author> <title> Fleet, Measurement of Image Velocity, </title> <publisher> Kluwer Academic Publ., </publisher> <address> Norwell MA, </address> <year> 1992 </year>
Reference-contexts: The resulting over-determined set of equations is then solved using a total least squares or orthogonal regression technique. <ref> [Fl92] </ref> describes a phase-based method which describes component velocity in terms of the motion of level phase contours in the output of band-pass velocity-tuned filters. This technique is essentially a differential method applied to phase rather than intensity [BFBB92].
Reference: [FL93] <author> D. Fleet, K. Langley, </author> <title> Recursive Filters for Optical Flow, </title> <institution> RPL-TR-9308, Robotics and Perception Lab, Queen's University, </institution> <address> Ontario May 1993 </address>
Reference: [FvD+90] <author> J. Foley, A. vanDam, S. Feiner, J. Hughes, </author> <title> Computer Graphics Principles and Practice, </title> <publisher> Addison-Wesley, </publisher> <address> Reading MA, </address> <year> 1990 </year>
Reference-contexts: When viewing such a subsampled image sequence, it is clear that considerable aliasing is occuring (see <ref> [FvD+90] </ref> for details on the problem of aliasing). In fact the aliasing is so bad one wonders how the algorithm can work at all. Figure 3.4 provides an example of this problem. On the left, a dark pixel moves between two CCD sensors. <p> In this case, each "individual" measurement is in fact the weighted averaged of the four nearest pixels to that actual real-coordinate point in the image (Figure 4.7. This weighted averaging method has many of the desirable properties of the "weighted area with overlap" sampling filter described in <ref> [FvD+90] </ref> (p.132 and p.617) but has the advantage of being extremely fast to compute (only a few floating point subtracts, adds and conversions to integer).
Reference: [G87] <author> F. Glazer, </author> <title> Hierarchal Motion Detection, </title> <type> PhD Dissertation, </type> <institution> COINS TR 87-02 University of Massachusetts at Amherst, </institution> <year> 1987 </year>
Reference-contexts: For these reasons, the basic correlation-based algorithm tends to be robust in practice, thus satisfying the first criteria for practical robotic vision. [A87a] and <ref> [G87] </ref> prefilter the images with band-pass filters and employ a coarse-to-fine matching strategy in which initial coarse estimates at one spatial scale are fed into the next higher resolution level. <p> The strength of a pyramid structure with regard to optical flow is that it can efficiently capture large motions of large contiguous areas of an image ([A87a], [A87b], [C90], <ref> [G87] </ref>), for example see Figure 5.5. However, the general requirement that the area of the moving surface be roughly proportional to the magnitude of motion does not always hold for interesting types of motion ([C93]).
Reference: [GG84] <author> S. Geman, D. Geman, </author> <title> Stochastic Relaxation, Gibbs distributions, and the Bayesian Restoration of Images, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-741, </pages> <month> Nov. </month> <year> 1984 </year>
Reference-contexts: One possibility with this algorithm is to calculate an initial FOE, then calculate the deviation from the hypothesized FOE at all points in the image. <ref> [GG84] </ref> uses a Markov Random Field model incorporating "line processes" to separate motions across occluding boundaries.
Reference: [Hil84] <author> E. Hildreth, </author> <title> The Measurement of Visual Motion, </title> <publisher> MIT Press, </publisher> <address> Cambridge MA </address>
Reference-contexts: In contrast to finding dense flow using a smoothness constraint, <ref> [Hil84] </ref> proposes calcu 10 to a translating straight contour may be determined. If there are curvature or intensity variations, then we have the weak aperture problem, which can be satisfactorily solved if certain assumptions are enforced. lating optical flow at points along the contours in the image. <p> Other schemes such as in [BLP89a] suggest the ANDing of certain features such as edges. Edges are used in some methods of motion detection (as in <ref> [Hil84] </ref>, [P90], [WWB88], [ZL93]) but are not used here because the resulting optical flow density would then depend on the density of the features in the image sequence, which might be too low for subsequent processing, or in any event would vary from one image sequence to another. <p> If dense measurements are required, this fact may be viewed as a strength; if only sparse measurements are acceptable, it may be viewed as a weakness. In fact, there would be little computational savings in calculating optical flow only at areas of high contrast such as edges <ref> [Hil84] </ref> or corners ([Nag87]) since the pipelined nature of the box-filter generally assumes that optical flow is calculated at all points in the image to be efficient (see Section 5.1 for details).
Reference: [Horn86] <author> B. Horn, </author> <title> Robotic Vision, </title> <publisher> MIT press, </publisher> <address> Cambridge MA, </address> <year> 1986 </year>
Reference: [H81] <author> P. Huber, </author> <title> Robust Statistics, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1981 </year>
Reference-contexts: By calculating an estimate of time-to-contact along the circumferences of circles of multiple radii, we can compute several independent estimates, and take some best-fit measure among them. In this case a robust maximum-likelihood estimate is used <ref> [H81] </ref>. This form of regression, unlike the standard least-squares estimate case, has the desirable property that outliers have only a minimal (non-linear) affect the final result. <p> One way to approach this problem is in two separate steps, outlier rejection followed by a classical estimation technique. <ref> [H81] </ref> argues against this approach, noting 1) it can be difficult to cleanly separate these two steps, given that the definition of outliers depends on some initial estimate in the first place, 2) the cleaned data will not be normal (statistically, some good data will have been eliminated and some bad <p> The location of this sudden drop could possibly be used to adaptively set value of the lower-bound threshold as well. (Although <ref> [H81] </ref> argues against an explicit outlier rejection step before maximum-likelihood estimation, this situation differs somewhat in that there is prior knowledge of the nature of the outliers in this case.) 50 1 Radius from FOE 31 1 Radius from FOE 31 255 1 TTC 1 Radius from FOE 31 1 Radius <p> For normal observations, s n is 12% more efficient than d n . However, in this example all errors are not normally distributed, some observations with probability " will have their errors multiplied by a factor of 3. To analyze this case, <ref> [H81] </ref> defines a ratio asymptotic relative efficiency (ARE) : ARE (") = lim n!1 var (d n )=(Ed n ) 2 Results from [H81] summarized in the following table show that for this example only 2 bad observations in 1000 are enough to offset the 12% advantage of s n in <p> To analyze this case, <ref> [H81] </ref> defines a ratio asymptotic relative efficiency (ARE) : ARE (") = lim n!1 var (d n )=(Ed n ) 2 Results from [H81] summarized in the following table show that for this example only 2 bad observations in 1000 are enough to offset the 12% advantage of s n in favor of d n . Therefore, the standard classic least-squares measure is clearly inappropriate when there are outliers present. <p> Therefore, the standard classic least-squares measure is clearly inappropriate when there are outliers present. A procedure for robust maximum-likelihood estimation (M-estimation) is derived in <ref> [H81] </ref> and repeated here using the notation of [HS93]. The goal is to minimize a function of the form: min n X (x i T k ) (A:1) where T k is the parameter to be estimated. <p> : n X (x i T k ) = 0 (A:2) Huber proposed as an object function the following : (x) = 0:5x 2 ; jxj a; ajxj 0:5a 2 ; o.w. which naturally results in : (x) = &lt; a; if x &lt; a; a; if x &gt; a <ref> [H81] </ref> recommends setting the tuning constant a to some value between 1 and 2; the time-to-contact algorithm seems extremely insensitive to the exact value of tuning constant a.
Reference: [HB88] <author> I. Horswill, R.Brooks, </author> <title> Situated Vision in a Dynamic World: Chasing Objects, </title> <booktitle> Seventh National Conference on Artificial Intelligence, </booktitle> <address> p.796-800,1988 </address>
Reference-contexts: Techniques which can run in real-time often impose strict restrictions on the environment; <ref> [HB88] </ref> presents a technique that can segment in real-time for tracking purposes, but requires that a textured object be moving in front of a relative textureless background.
Reference: [Heg87] <author> D. </author> <title> Heger, Optical Flow from Spatiotemporal Filters, </title> <booktitle> Proceedings of the IEEE ICCV, </booktitle> <address> p.181-190, </address> <year> 1987 </year>
Reference-contexts: From <ref> [Heg87] </ref>, two-dimensional patterns translating in the image plane occupy a plane in the spatio-temporal frequency domain : ! t = u! x + v! y where ! t , ! x , ! y are the temporal and spatial frequencies of the motion respectively.
Reference: [HG91] <author> J. Huber, V. Graefe, </author> <title> Quantitative Interpretation of Image Velocities in Real-Time, </title> <booktitle> IEEE 1991 Workshop on Visual Motion, </booktitle> <address> p.211-216, </address> <month> Oct. </month> <year> 1991 </year>
Reference-contexts: when measured over a 10x10 optical flow vectors, with a standard deviation of 2.8 time steps. [TS91] simplifies the calculation of time-to-impact by using velocity represented in a polar coordinate system, and computes a coarse hazard map using computational time on the order of one minute on a Sparcstation 1. <ref> [HG91] </ref> reports accuracy of better than 1% at distances of 15 meters down to 2 meters from the target using recursive filtering of subpixel measurements of features in the image, using a real-time image processor.
Reference: [Hil91] <author> E. Hildreth, </author> <title> Recovering Heading for Visually-Guided Navigation, </title> <booktitle> Vision Research 32(6) </booktitle> <pages> 1177-1192, </pages> <year> 1992 </year>
Reference-contexts: This is true regardless of rotational motion, since the latter adds the same motion component regardless of the depth 38 variations in the image [LP80]. There is psychophysical evidence supporting this differential motion method of separating the rotational and translational components of the flow field [WH89]. <ref> [Hil91] </ref> extends the approach of [RL85] by considering moving objects in the image as well. Since the chair in Figure 4.1 used in the time-to-contact experiments represents a flat surface, techniques based on differential motion cannot be used for this example.
Reference: [HP90] <author> J. Hennessy, D. Patterson, </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA, </address> <year> 1990 </year>
Reference-contexts: Flexible network topologies such as described in [BA93] would likely be required for a massively parallel system to be able to operate on a wide variety of computer vision problems. Amdahl's Law (attributed to Gene Amdahl) <ref> [HP90] </ref> states that as sections of a program that are parallelizable are increasingly parallelized, the remaining serial sections of the program will become the bottleneck. In the case of this algorithm, if the shift and match stage were completely parallelized, the temporal antialiasing stage would then become the bottleneck.
Reference: [hp94] <institution> PA-RISC 1.1 Architecture and Instruction Set Reference Manual, third edition, Hewlett Packard, </institution> <month> Feb. </month> <year> 1994 </year>
Reference-contexts: Once the base addresses of corresponding shifted rows are calculated, a single index register can be updated with each new pixel in architectures such as SPARC [Sun87], PA-RISC <ref> [hp94] </ref>, and PowerPC [Mot94a]. Architectures with only base + displacement addressing, such as Alpha [SW94] and MIPS [PH94], require an explicit address add. <p> Such memory accesses are generally sequential and highly predictable, and thus can take advantage of instructions that provide memory-access "hints" as are found on the Alpha [DEC92], PA-RISC <ref> [hp94] </ref>, PowerPC [Mot94a], and SPARC V9 [Cas93] architectures. Thus this optical flow algorithm is well-suited for operation on common personal computers, since it does not require the floating point capabilities of the much more expensive scientific workstations, and can achieve a more balanced memory-bandwidth to computation ratio.
Reference: [HS81] <author> B. Horn, P. Schunck, </author> <title> Determining Optical Flow, </title> <booktitle> Artificial Intelligence 17 </booktitle> <pages> 185-203, </pages> <month> August </month> <year> 1981 </year> <month> 120 </month>
Reference-contexts: For small motions, constant ambient illumination, these assumptions are more or less true except for pathological situations such as occluding boundaries. While occluding boundaries is an important application for optical flow, for the moment we will only consider the general case. From <ref> [HS81] </ref>, assume that the brightness of a given point in an image is constant : dE = 0: (2:1) And we wish to find the velocity v = (u; w) = ( dx dt ; dy dt ). <p> The problem can then be formulated as minimizing the sum of the errors in the equation for the rate of change of image brightness and the deviation from smoothness in the optical flow <ref> [HS81] </ref>. The optical flow field is then determined by minimizing a cost functional L ([HS81], [KWM89]): Z Z @ _x ) 2 + ( @y @ _y ) 2 + ( @y [KWM89] proposes implementing such a smoothness constraint in a neural network and relates it to the primate's visual system. <p> In practice for robustness, [UGVT88] implements two variants of regularization techniques to create a smooth optical flow field. Smoothing the image can help attenuate the effects of noise, however smoothing the basic optical flow measurements by imposing a smoothness constraint or regularization such as in <ref> [HS81] </ref> can smooth over discontinuities in the motion 13 field and introduce inaccuracies, as well as increase the computational cost.
Reference: [HS93] <author> R. Haralick, L. Shapiro, </author> <title> Computer and Robot Vision II, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1993 </year>
Reference-contexts: Both applications can make use of various methods for correspondence (issues of correspondence and image matching are discussed in <ref> [HS93] </ref>), however tracking across space is not equivalent to tracking across time. In particular, the former can make use of the epipolar constraint in matching across space. The latter's matching across space and time 111 can make use of the velocity equation in matching across both space and time. <p> Therefore, the standard classic least-squares measure is clearly inappropriate when there are outliers present. A procedure for robust maximum-likelihood estimation (M-estimation) is derived in [H81] and repeated here using the notation of <ref> [HS93] </ref>. The goal is to minimize a function of the form: min n X (x i T k ) (A:1) where T k is the parameter to be estimated. In this form, is called the object function, 115 and T k is called an M-estimate.
Reference: [Hub94a] <author> S. Huber, </author> <title> video presentation, From Animals to Animats: </title> <booktitle> Third International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Brighton, G.B., </address> <month> 8-12 August 94 </month>
Reference-contexts: By using the estimated divergence of the flow field, time-to-contact is calculated with errors reported on the order of 10% [AP93]. The simplicity of this approach makes it a plausible candidate for biological implementations. <ref> [Hub94a] </ref> uses the correlation of 2-dimensional image patches along a single dimension ([AP93], [BLP89a]) to simulate the navigation of a fly in a synthetic maze using optical flow, at a rate of about one frame per second using 512x512 images [Hub94b].
Reference: [Hub94b] <author> S. Huber, </author> <type> personal communication. </type>
Reference-contexts: this approach makes it a plausible candidate for biological implementations. [Hub94a] uses the correlation of 2-dimensional image patches along a single dimension ([AP93], [BLP89a]) to simulate the navigation of a fly in a synthetic maze using optical flow, at a rate of about one frame per second using 512x512 images <ref> [Hub94b] </ref>. A genetic algorithm is used to train the artificial fly's optomotor response to turbulent air conditions to prevent collisions with the maze's walls. The independence of one pixel's chosen displacement from all other pixels' displacements motivates massive parallel implementations such as the one implemented on the connection machine [BLP89a].
Reference: [HW91] <author> N. Hatsopoulos, W. Warren Jr., </author> <title> Visual Navigation with a Neural Network, </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 303-317, </pages> <year> 1991 </year>
Reference-contexts: Since the optical flow vectors at various points in the image can be treated independently, heading determination can be calculated with a two-layer linear neural network <ref> [HW91] </ref>. [RL85] computes the FOE in cases where there is a rotational component present by calculating the difference vectors k ij u k of the optical flow vectors in a local area to a given vector.
Reference: [IRP92] <author> M. Irani, B. Rousso, S. Peleg, </author> <title> Detecting and Tracking Multiple Moving Objects Using Temporal Integration, </title> <booktitle> Proceedings of the ECCV, </booktitle> <address> p.282-287, </address> <year> 1992 </year>
Reference-contexts: This is repeated, and they report convergence within a few iterations in most cases, regardless of the initial estimate for p 0 . Case 5), a small moving object in front of moving background, could make use of some form of temporal integration such as in <ref> [IRP92] </ref>.
Reference: [J73] <author> G. Johansson, </author> <title> Visual Perception of Biological Motion and a Model for its Analysis, </title> <journal> Perception and Psychophysics, </journal> <volume> 14 </volume> <pages> 201-211, </pages> <year> 1973 </year>
Reference-contexts: the match of the larger patch would be much worse than that of the smaller patch, when both are placed at the motion boundary [LBP88]; this could run completely in parallel as well. [AD91] notes that often precise boundary detection is not necessary for accurate motion perception; for example, Johansson <ref> [J73] </ref> demonstrated that people can easily identify a walking person while only viewing lights attached to the moving person's joints (see also [J75]). These 105 a lighter shade means a better match. "Moving Light Displays" do however sweep out a spatio-temporal path whose temporal coherence can be exploited.
Reference: [J75] <author> G. Johansson, </author> <title> Visual Motion Perception, </title> <publisher> Scientific American 232:6 p.76-88, </publisher> <month> June </month> <year> 1975 </year>
Reference-contexts: motion boundary [LBP88]; this could run completely in parallel as well. [AD91] notes that often precise boundary detection is not necessary for accurate motion perception; for example, Johansson [J73] demonstrated that people can easily identify a walking person while only viewing lights attached to the moving person's joints (see also <ref> [J75] </ref>). These 105 a lighter shade means a better match. "Moving Light Displays" do however sweep out a spatio-temporal path whose temporal coherence can be exploited.
Reference: [K81] <author> D. Knuth, </author> <booktitle> The Art of Computer Programming, Volume 2: Seminumerical Algorithms, </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1981 </year>
Reference-contexts: We are using the same exact optical flow algorithm, just on very poor data. Figures 4.25, 4.26 demonstrates the algorithm's performance with 2, 4, 6, 8, 10 and 12 additive Gaussian and evenly distributed random noise, respectively. The Gaussian (normal) distribution was produced using the standard formula from <ref> [K81] </ref>, with mean zero and variance 2 .
Reference: [K86] <author> J.J. Koenderink, </author> <title> Optic Flow, </title> <booktitle> Vision Research 26(1) </booktitle> <pages> 161-180, </pages> <year> 1986 </year>
Reference-contexts: Biological organisms make considerable use of optical flow, such as the detection of discontinuities [PRH81], and figure-ground discrimination 7 [B81]. Motion detection is useful for autonomous mobile robot navigation [PBF89] as well as undersea navigation [NSY91]. <ref> [K86] </ref>, [KvD78] discusses the decomposition of optical flow into curl, divergence and shear for the purposes of determining local shape of an object, and [Reg86] presents evidence that the human visual system possesses specific detectors for similar types of basic motion. [TR93] describes an application for image coding. [MB90] covers experiments
Reference: [KG92] <author> C. Kambhamettu, D. Goldgof, </author> <title> Point Correspondence Recovery in Non-rigid Motion, </title> <booktitle> Proceedings of the IEEE CVPR, </booktitle> <address> p.222-227, </address> <year> 1992 </year>
Reference-contexts: Performance of the algorithm does improve after this point.) In addition, the sweatshirt used as a target, although solid-colored, is not as textureless as many objects found in a typical office, such as a black filing cabinet. Subsequent work will examine these issues in turn. Non-rigid motion (for example <ref> [KG92] </ref>) would seem to require more precise measurements than is possible from this algorithm without some form of interpolation. Since the basic assumption of the patch-matching approach is local rigidity, it is likely that major modifications would be needed to make the algorithm suitable for this application.
Reference: [KMB+91] <author> C. Koch, A. Moore, W. Bair, T. Horiuchi, B. Bishofberger, J. Lazzaro, </author> <title> Computing Motion Using Analog VLSI Vision Chips, An Experimental Comparison Among Four Approaches, </title> <booktitle> IEEE Workshop on Visual Motion, </booktitle> <address> p.312-324, </address> <year> 1991 </year>
Reference-contexts: Although this approach can in theory give very precise measurements, one major problem is that numerical differentiation of this sort is very sensitive to noise in the input (for example cf. <ref> [KMB+91] </ref>). This is particularly true if the spatial derivative is small (i.e. the slope is flat) at the point in question, in which case a constant amount of noise has a greater detrimental effect on the numerical differentiation.
Reference: [KSL85] <author> E. Kent, M. Shneier, R. Lumia, </author> <title> PIPE: Pipelined Image Processing Engine, </title> <journal> Journal Parallel and Distributed Computing 2, </journal> <volume> P. </volume> <pages> 50-78, </pages> <year> 1985 </year>
Reference-contexts: Authors rarely report the computational time needed for their algorithms; when they do, it is on the order of many minutes per frame ([WM93]), or require specialized hardware such as a Connection Machine ([BLP89a], [DN93], [SU87], [WZ91], [L88]), Datacube ([LK93], [N91]), custom image processors [DW93], or PIPE <ref> [KSL85] </ref>, [WWB88], [ATYM]. Techniques which can run in real-time often impose strict restrictions on the environment; [HB88] presents a technique that can segment in real-time for tracking purposes, but requires that a textured object be moving in front of a relative textureless background.
Reference: [KvD78] <author> J.J. Koenderink, A.J. van Doorn, </author> <title> How an Ambulant Observer can Construct a Model of the Environment from the Geometrical Structure of the Visual Flow, Kybernetic 1978, </title> <type> Oldenbourg, </type> <institution> Muenchen </institution>
Reference-contexts: Biological organisms make considerable use of optical flow, such as the detection of discontinuities [PRH81], and figure-ground discrimination 7 [B81]. Motion detection is useful for autonomous mobile robot navigation [PBF89] as well as undersea navigation [NSY91]. [K86], <ref> [KvD78] </ref> discusses the decomposition of optical flow into curl, divergence and shear for the purposes of determining local shape of an object, and [Reg86] presents evidence that the human visual system possesses specific detectors for similar types of basic motion. [TR93] describes an application for image coding. [MB90] covers experiments in
Reference: [KWM89] <author> C. Koch, H.T. Wang, B. Mathur, </author> <title> Computing Motion in the Primate's Visual System, </title> <journal> Journal of Experimental Biology, </journal> <volume> 146 </volume> <year> 115-139,1989 </year>
Reference-contexts: The problem can then be formulated as minimizing the sum of the errors in the equation for the rate of change of image brightness and the deviation from smoothness in the optical flow [HS81]. The optical flow field is then determined by minimizing a cost functional L ([HS81], <ref> [KWM89] </ref>): Z Z @ _x ) 2 + ( @y @ _y ) 2 + ( @y [KWM89] proposes implementing such a smoothness constraint in a neural network and relates it to the primate's visual system. <p> The optical flow field is then determined by minimizing a cost functional L ([HS81], <ref> [KWM89] </ref>): Z Z @ _x ) 2 + ( @y @ _y ) 2 + ( @y [KWM89] proposes implementing such a smoothness constraint in a neural network and relates it to the primate's visual system. In contrast to finding dense flow using a smoothness constraint, [Hil84] proposes calcu 10 to a translating straight contour may be determined.
Reference: [Lee76] <author> D. Lee, </author> <title> A Theory of Visual Control of Braking Based on Information about Time-to-Collision, </title> <booktitle> Perception 5 </booktitle> <pages> 437-459, </pages> <year> 1976 </year>
Reference-contexts: An example of optical flow is shown in Figure 2.1. By accurately computing this 2-D vector field, it is in principle possible to calculate three-dimensional properties of the environment, and quantities such as time-to-contact with an an observed object <ref> [Lee76] </ref>. Biological organisms make considerable use of optical flow, such as the detection of discontinuities [PRH81], and figure-ground discrimination 7 [B81]. <p> have accurate optical flow to perform such functions as obstacle avoidance ([NA89]), Chapter 4 demonstrates that despite these deficiences remarkable accuracy may still be achieved in the context of calculating time-to-collision. 33 Chapter 4 Time-to-Contact One application of optical flow is time-to-contact, sometimes pessimistically referred to as time-to-collision or time-to-crash <ref> [Lee76] </ref>, [T90]. Using only optical measurements, and without knowing one's own velocity or distance from a surface, it is possible to determine when contact with the surface will be made. <p> low contrast of the sequence. 37 Since P is immobile, set _ Y = 0 and substituting (yZ) for Y : _y = y ( Z Finally divide by y and take the reciprocals of both sides : y = _ Z The quantity t is known as the time-to-contact <ref> [Lee76] </ref>. Note that the left hand side contains purely optical quantities, and that knowledge of t does not give any information about distance or velocity per se, but only of their ratio.
Reference: [L88] <author> J.Little, </author> <title> Integrating Vision Modules on a Fine-Grained Parallel Machine, in Machine Vision, </title> <publisher> Academic Press, </publisher> <address> p.57-96, </address> <year> 1988 </year>
Reference-contexts: Authors rarely report the computational time needed for their algorithms; when they do, it is on the order of many minutes per frame ([WM93]), or require specialized hardware such as a Connection Machine ([BLP89a], [DN93], [SU87], [WZ91], <ref> [L88] </ref>), Datacube ([LK93], [N91]), custom image processors [DW93], or PIPE [KSL85], [WWB88], [ATYM]. <p> In this case the updates can easily be performed directly using shift and mask operations, as shown in Figure 5.1. On some parallel machines such as on the Connection Machine the match values could be calculated independently and combined using built-in MINIMUM calculating hardware ([D86], [LBC87], <ref> [L88] </ref>). <p> Companies such as Thinking Machines has recognized the advantages of using moderate numbers of commercial microprocessors in their latest offering (the CM-5) in favor of previous models which are composed of up to tens of thousands of 1-bit custom processors ([D86], [LBC87], <ref> [L88] </ref>). [Uhr87] notes that the overhead for communication in a MIMD (multiple-instruction, multiple data) computer may 1 Note that this only considers the raw amount of work done, and not whether or not the work done is particularly efficient or useful.
Reference: [LBC87] <author> J. Little, G. Blelloch, T. Cass, </author> <title> Parallel Algorithms for Computer Vision on the Connection Machine, </title> <booktitle> Proceedings of the IEEE ICCV, </booktitle> <address> p.587-591, </address> <year> 1987 </year>
Reference-contexts: In this case the updates can easily be performed directly using shift and mask operations, as shown in Figure 5.1. On some parallel machines such as on the Connection Machine the match values could be calculated independently and combined using built-in MINIMUM calculating hardware ([D86], <ref> [LBC87] </ref>, [L88]). <p> Companies such as Thinking Machines has recognized the advantages of using moderate numbers of commercial microprocessors in their latest offering (the CM-5) in favor of previous models which are composed of up to tens of thousands of 1-bit custom processors ([D86], <ref> [LBC87] </ref>, [L88]). [Uhr87] notes that the overhead for communication in a MIMD (multiple-instruction, multiple data) computer may 1 Note that this only considers the raw amount of work done, and not whether or not the work done is particularly efficient or useful.
Reference: [LBP88] <author> J. Little, H. Buelthoff, T. Poggio, </author> <title> Parallel Optical Flow Using Local Voting, </title> <booktitle> Second International Conference on Computer Vision, </booktitle> <address> p.454-9, </address> <year> 1988 </year>
Reference-contexts: One option is to run the algorithm using two patch sizes; one would expect that the "quality" of the match of the larger patch would be much worse than that of the smaller patch, when both are placed at the motion boundary <ref> [LBP88] </ref>; this could run completely in parallel as well. [AD91] notes that often precise boundary detection is not necessary for accurate motion perception; for example, Johansson [J73] demonstrated that people can easily identify a walking person while only viewing lights attached to the moving person's joints (see also [J75]).
Reference: [LK93] <author> J. Little, J. Kahn, </author> <title> A Smart Buffer for Tracking Using Motion Data, </title> <booktitle> p.257-266, Workshop on Computer Architectures for Machine Perception, </booktitle> <address> New Orleans Louisiana, </address> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alamitos CA, </address> <year> 1993 </year>
Reference-contexts: the bulk of the computations in customized silicon chips; one such (partial) implementation has been done in CMOS [C91]. [DW93] calculates structure-from-motion using the same basic shift-match-winner-take-all algorithm implemented on the Image Understanding Architecture simulator; they report an estimated 0.54 seconds per frame using a maximum possible displacement = 20. <ref> [LK93] </ref> calculates optical flow (for the purposes of tracking) within a limited radius (+/- 2 pixels vertically, +/- 3 pixels horizontally) using a 7x7 correlation window at 10 Hz on 128x120 images using the Datacube MaxVideo 200. 19 Computational issues are discussed in detail in Chapter 5. <p> Using the technique of [BLP89b] (the same as described in Section 2.3) on a Datacube MaxVideo 200, <ref> [LK93] </ref> calculates optical flow within a limited radius of +/- 3 pixels horizontally and +/- 2 pixels vertically using a 7x7 correlation window at 10 Hz on 128x120 images.
Reference: [LP80] <author> H. Longuet-Higgins, K. Prazdny, </author> <title> The Interpretation of a Moving Retinal Image, </title> <journal> Proc. of the Royal Society of London, </journal> <volume> B 208 </volume> <pages> 385-397, </pages> <year> 1980 </year>
Reference-contexts: A least-squares fit is performed for those vector differences above a certain threshold. This is true regardless of rotational motion, since the latter adds the same motion component regardless of the depth 38 variations in the image <ref> [LP80] </ref>. There is psychophysical evidence supporting this differential motion method of separating the rotational and translational components of the flow field [WH89]. [Hil91] extends the approach of [RL85] by considering moving objects in the image as well.
Reference: [LPK93] <author> C-C Lin, V. Prasanna, A. Khokhar, </author> <title> Scalable Parallel Extraction of Linear Features on MP-2, </title> <booktitle> p.352-361, Workshop on Computer Architectures for Machine Perception, </booktitle> <address> New Orleans Louisiana, </address> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alamitos CA, </address> <year> 1993 </year>
Reference-contexts: In practice on a serial machine it is convenient to compare the current best match value with each new one as it is calculated, using a traditional IF-THEN construct. In a massively parallel processor it may be undesirable to require a data-dependent conditional construct in the algorithm (e.g. <ref> [LPK93] </ref>), since this means that some processors will be executing different instructions depending on the data being processed; this may be difficult or impossible with SIMD (single-instruction-multiple-data) hardware. In this case the updates can easily be performed directly using shift and mask operations, as shown in Figure 5.1.
Reference: [LV89] <author> J. Little, A. Verri, </author> <title> Analysis of Differential and Matching Methods for Optical Flow, </title> <booktitle> IEEE 1989 Workshop on Visual Motion, </booktitle> <pages> 173-180, </pages> <month> March </month> <year> 1989 </year>
Reference-contexts: However, for a sufficiently textured surface, the optical flow field will be arbitrarily close to the motion field. Many techniques for optical flow exist (see [BFBB92] and <ref> [LV89] </ref> for reviews and discussions of several techniques). Although these techniques can perform very well for certain sequences of images, there are very few that are currently able to support real-time performance. <p> For now we will ignore this issue.) The spatial derivatives @E @x and @E @y and the temporal derivative at an image point @E @t can be estimated using two or more images by using for example finite or central difference methods <ref> [LV89] </ref>. This leaves two unknowns u and w, motion along the X and Y axes respectively, with only one constraint, equation 2.2. Only the motion along the direction of the gradient ( @E @x ; @E @y ) is available.
Reference: [Mor77] <author> H. Moravec, </author> <title> Towards Automatic Visual Obstacle Avoidance, </title> <booktitle> 5th IJCAI, </booktitle> <address> p.584, </address> <year> 1977 </year>
Reference-contexts: In these cases 16 rigid-body assumption. In this figure, - = 3. a lower value indicates a better match. Moravec <ref> [Mor77] </ref> uses a variant of normalized cross correlation in stereo matching. Although this has the advantage of insensitivity to contrast, the sum of absolute differences measure used in most examples in this thesis was found to be very insensitive to the image contrast.
Reference: [MBLB91] <author> H. Mallot, H. Buelthoff, J. Little, S. Bohrer, </author> <title> Inverse Perspective Mapping Simplifies Optical Flow Computation and Obstacle Detection, </title> <journal> Biological Cybernetics, </journal> <volume> 64 </volume> <year> 177-185,1991 </year>
Reference-contexts: A key part of this stage is known as a box filter. This single operation is so important to real-time operation that it will be described in some detail here. 84 The first step of the patch-shift-match operation is the shifting stage ([BLP89a], <ref> [MBLB91] </ref>). In a massively parallel machine such as the Connection Machine 2, it might be desirable to implement this as an explicit image shift so that corresponding pixels are located in the same processing node. <p> But note that optical flow is only valuable when used in the context of a specific task, such as obstacle avoidance <ref> [MBLB91] </ref> or time-to-contact, and the performance of this algorithm when applied to the latter has been shown to be exceptionally good. However, it remains to be seen if such performance can be equaled for other robotic vision tasks.
Reference: [M82] <author> D. Marr, </author> <title> Vision, </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1982 </year>
Reference-contexts: This particular application demonstrates that, at least for this application, the potential limitations of non-real-valued optical flow measurements do not manifest themselves. 81 82 Chapter 5 Real-Time Performance and Hardware Considerations <ref> [M82] </ref> defines three levels at which an information processing device can be understood : 1) Computational theory 2) Representation and algorithm 3) Hardware implementation The first level describes the goal of the computation and the logic by which it is carried out.
Reference: [MB90] <author> D. Murray, B. Buxton, </author> <title> Experiments in the Machine Interpretation of Visual Motion, </title> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1990 </year>
Reference-contexts: navigation [NSY91]. [K86], [KvD78] discusses the decomposition of optical flow into curl, divergence and shear for the purposes of determining local shape of an object, and [Reg86] presents evidence that the human visual system possesses specific detectors for similar types of basic motion. [TR93] describes an application for image coding. <ref> [MB90] </ref> covers experiments in segmentation, structure from motion, tracking and qualitative shape analysis. [NA89] discusses obstacle avoidance using only qualitative optical flow. In general the optical flow will not be the same as the true 2-D projection of the 3-D motion field [VP87].
Reference: [McL93] <author> E. McLellan, </author> <title> The Alpha AXP Architecture and 21064 Processor, </title> <journal> p.36-47, </journal> <note> June 1993 IEEE Micro </note>
Reference-contexts: not be an arithmetic overflow. */ mask = (best_match - cand_match) &gt;> 31; /* arithmetic shift */ best_match = ( best_match & mask) | ( cand_match &~ mask); best_motion = (best_motion & mask) | (cand_motion &~ mask); branches by using a "conditional move" operation, such as found on the Alpha <ref> [McL93] </ref> and SPARC V9 [Cas93] architectures. Often, some tight loops are memory limited (i.e. byte loads and stores dominate).
Reference: [MH80] <author> D. Marr, E. Hildreth, </author> <title> Theory of Edge Detection, </title> <journal> Proceedings of the Royal Society of London, B:207 p.187-217, </journal> <year> 1980 </year>
Reference-contexts: If there are curvature or intensity variations, then we have the weak aperture problem, which can be satisfactorily solved if certain assumptions are enforced. lating optical flow at points along the contours in the image. Here contours are extracted from the image using Marr and Hildreth's edge detector <ref> [MH80] </ref> and required that the velocity field be smooth only along a contour and not across it.
Reference: [Mot94a] <institution> PowerPC 603 RISC Microprocessor User's Manual, part# MPC603UM/AD, Motorola, </institution> <year> 1994 </year>
Reference-contexts: Once the base addresses of corresponding shifted rows are calculated, a single index register can be updated with each new pixel in architectures such as SPARC [Sun87], PA-RISC [hp94], and PowerPC <ref> [Mot94a] </ref>. Architectures with only base + displacement addressing, such as Alpha [SW94] and MIPS [PH94], require an explicit address add. <p> Such memory accesses are generally sequential and highly predictable, and thus can take advantage of instructions that provide memory-access "hints" as are found on the Alpha [DEC92], PA-RISC [hp94], PowerPC <ref> [Mot94a] </ref>, and SPARC V9 [Cas93] architectures. Thus this optical flow algorithm is well-suited for operation on common personal computers, since it does not require the floating point capabilities of the much more expensive scientific workstations, and can achieve a more balanced memory-bandwidth to computation ratio.
Reference: [Mot94b] <institution> PowerPC 604 RISC Microprocessor Technical Summary, part# MPC604/D, Motorola, </institution> <year> 1994 </year>
Reference-contexts: Conversely, personal computers generally make use of mostly integer arithmetic. Although multimedia applications will certainly increase the use of floating-point, personal computers will most likely emphasize their integer performance for the foreseeable future. Examples are Intel's Pentium (tm) [AA93] and IBM/Motorola's PowerPC (tm) 604 processor <ref> [Mot94b] </ref>, 89 both of which contain multiple integer units but only a single floating-point unit. <p> In particular, CPU's intended for desktop personal computers such as Intel's x86 and the PowerPC series will be built with multiple integer functional units (but not necessarily multiple floating point units), the Pentium [AA93] and PowerPC 604 <ref> [Mot94b] </ref> being the first such examples.
Reference: [MU81] <author> D. Marr, S. Ullman, </author> <title> Directional Sensitivity and Its Use in Early Visual Processing, </title> <journal> Proc.R.Soc.Lond. </journal> <volume> B 211, p.151-180, </volume> <year> 1981 </year>
Reference-contexts: This leaves two unknowns u and w, motion along the X and Y axes respectively, with only one constraint, equation 2.2. Only the motion along the direction of the gradient ( @E @x ; @E @y ) is available. This is known as the aperture problem <ref> [MU81] </ref>, [NS88] and is illustrated in Figure 2.2. The left of Figure 2.2 shows an instance of the strong 9 aperture problem [BLP89b].
Reference: [Nag87] <author> H-H Nagel, </author> <title> On the Estimation of Optical Flow: Relations between Different Approaches and Some New Results, </title> <booktitle> Artificial Intelligence 33 </booktitle> <pages> 299-324, </pages> <year> 1987 </year>
Reference-contexts: total temporal derivative of the spatial gradient is zero : drE = 0 Expanding this equation leads to two constraints : @ 2 E @ 2 E v 2 + @x@ t @ 2 E v 1 + @y 2 v 2 + @y@ t This approach is taken in <ref> [Nag87] </ref>, [UGVT88], and assumes that the displacements being measured are less than one half of a cycle of the highest spatial frequencies in the image, otherwise there will be aliasing. <p> Figure 4.3 shows the image of the chair at frames 12, 41, 96, 126, 134, and 139. Note the very low contrast of the sequence; gradient and especially second-derivative techniques such as <ref> [Nag87] </ref> which suggest calculating flow at grey-level "corners" would not find any such high-contrast locations in these images, once the chair fills the field of view.
Reference: [N91] <editor> R.Nelson,Qualitative Detection of Motion by a Moving Observer, </editor> <booktitle> Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <address> p.173-178,1991 </address>
Reference-contexts: Authors rarely report the computational time needed for their algorithms; when they do, it is on the order of many minutes per frame ([WM93]), or require specialized hardware such as a Connection Machine ([BLP89a], [DN93], [SU87], [WZ91], [L88]), Datacube ([LK93], <ref> [N91] </ref>), custom image processors [DW93], or PIPE [KSL85], [WWB88], [ATYM]. Techniques which can run in real-time often impose strict restrictions on the environment; [HB88] presents a technique that can segment in real-time for tracking purposes, but requires that a textured object be moving in front of a relative textureless background.
Reference: [NA89] <author> R. Nelson, J. Aloimonos, </author> <title> Obstacle Avoidance Using Flow Field Divergence, </title> <journal> IEEE PAMI-11 no. </journal> <volume> 10, p.1102-1106, </volume> <month> October </month> <year> 1987 </year>
Reference-contexts: and shear for the purposes of determining local shape of an object, and [Reg86] presents evidence that the human visual system possesses specific detectors for similar types of basic motion. [TR93] describes an application for image coding. [MB90] covers experiments in segmentation, structure from motion, tracking and qualitative shape analysis. <ref> [NA89] </ref> discusses obstacle avoidance using only qualitative optical flow. In general the optical flow will not be the same as the true 2-D projection of the 3-D motion field [VP87].
Reference: [Nal93] <author> V. Nalwa, </author> <title> A Guided Tour of Computer Vision, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1993 </year>
Reference-contexts: Although the linear set of velocities may seem more intuitive than the harmonic series, in fact the latter is often much more suited to the types of motion found in real vision problems. One example is shown in Figure 3.8 <ref> [Nal93] </ref>.
Reference: [NG92] <author> S. Negahdaripour, V. Ganesan, </author> <title> Simple Direct Computation of the FOE, </title> <booktitle> Proceedings of the IEEE CVPR, </booktitle> <address> p.228-235, </address> <year> 1992 </year>
Reference-contexts: Since the chair in Figure 4.1 used in the time-to-contact experiments represents a flat surface, techniques based on differential motion cannot be used for this example. It is possible to calculate the FOE directly without computing the intervening optical flow. <ref> [NG92] </ref> discusses a method for calculating the FOE using the assumption that calculated depth should always be positive. <p> Several such FOE estimates can be made, with the actual FOE taken to be their intersection. For robustness, <ref> [NG92] </ref> uses only the sign of the depth values and not their magnitudes. This algorithm can take advantage of the X-Y component representation of the optical flow vectors. <p> This method is very simple and extremely fast, and produces visually accurate results for most cases, especially with images with low noise. Unlike gradient-based approaches (such as <ref> [NG92] </ref>), the patch-matching nature of this algorithm is largely insensitive to local contrast. The use of the entire dense optical flow field is supported by [WH89] which presents psychophysical evidence that observer performance in heading estimation decreases with a decrease in the density of the optical flow information available. <p> Identify the stationary regions in the registered images, using M (t) as an initial guess, and set M (t + 1) to the new tracked region. 109 4. Update the temporally integrated image Av (t + 1), and repeat. [BFBB93], <ref> [NG92] </ref>, [VG92], [SAH91] make use of confidence measures in calculating optical flow. Conversely, the "winner-take-all" nature of this algorithm makes no use of confidence measures whatsoever, instead relying on the algorithm's inherent smoothness in producing a 100% dense output.
Reference: [NH87] <author> S. Negahdaripour, B. Horn, </author> <title> Direct Passive Navigation, </title> <journal> IEEE PAMI, </journal> <volume> 9:1, </volume> <month> Jan. </month> <year> 1991 </year>
Reference-contexts: It is possible to calculate the FOE directly without computing the intervening optical flow. [NG92] discusses a method for calculating the FOE using the assumption that calculated depth should always be positive. The image brightness constraint equation proposed by <ref> [NH87] </ref> reduces in the purely translational motion case to : c + Z where c is the temporal derivative of the image intensity function E (x; y; t), t = (t 1 ; t 2 ; t 3 ) is the instantaneous translational velocity of the camera relative to a stationary <p> Thus are an infinite number of f ^ t; ^ Zg solutions to equation 4.2. <ref> [NH87] </ref> shows that for an incorrectly calculated FOE, there is a corresponding cluster of mostly negative depth values along the "FOE constraint line" which connects the actual FOE with the computed FOE.
Reference: [NS88] <author> K. Nakayama, G. Silverman, </author> <title> The Aperture Problem-I. </title> <booktitle> Vision Research 28(6) </booktitle> <pages> 739-746, </pages> <year> 1988 </year>
Reference-contexts: This leaves two unknowns u and w, motion along the X and Y axes respectively, with only one constraint, equation 2.2. Only the motion along the direction of the gradient ( @E @x ; @E @y ) is available. This is known as the aperture problem [MU81], <ref> [NS88] </ref> and is illustrated in Figure 2.2. The left of Figure 2.2 shows an instance of the strong 9 aperture problem [BLP89b].
Reference: [NSY91] <author> S. Negahdaripour, A. Shokrollahi, C.H. Yu, </author> <title> Optical Sensing for Undersea Robotic Vehicles, </title> <booktitle> Robotics and Autonomous Systems 7 </booktitle> <pages> 151-163, </pages> <year> 1991 </year>
Reference-contexts: Biological organisms make considerable use of optical flow, such as the detection of discontinuities [PRH81], and figure-ground discrimination 7 [B81]. Motion detection is useful for autonomous mobile robot navigation [PBF89] as well as undersea navigation <ref> [NSY91] </ref>. [K86], [KvD78] discusses the decomposition of optical flow into curl, divergence and shear for the purposes of determining local shape of an object, and [Reg86] presents evidence that the human visual system possesses specific detectors for similar types of basic motion. [TR93] describes an application for image coding. [MB90] covers
Reference: [O85] <editor> VLSI Image Processing, R. Offen ed., </editor> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1985 </year>
Reference-contexts: However, for computer vision to ever become practical, some consideration must be given to performance issues. The vast computational requirements of real-time computer vision make implementational issues more important than they are for many other fields of computer science. From <ref> [O85] </ref> : "It is only by using machines with [levels of performance in the Giga operations per second] that significant real-time image processing becomes a realistic 83 proposition." Entire workshops [CAMP93] and specialized texts [Uhr87], [O85] are devoted to custom hardware or massively parallel implementations of computer vision algorithms. <p> From <ref> [O85] </ref> : "It is only by using machines with [levels of performance in the Giga operations per second] that significant real-time image processing becomes a realistic 83 proposition." Entire workshops [CAMP93] and specialized texts [Uhr87], [O85] are devoted to custom hardware or massively parallel implementations of computer vision algorithms. For practical mobile robot vision, considerations must be given to efficiency. One factor related to performance is an efficient representation and algorithm used to analyze the image (s). <p> Hardware implementations could take parallelism to an even finer scale, and includes 93 implementation on special-purpose image processors (such as the PIPE or Datacube) and in VLSI <ref> [O85] </ref>. Although this algorithm would be much more difficult to implement in VLSI in its entirety than say the one-dimensional technique of [AP93], the simple nature of the box filter makes it suitable for implementation in VLSI, Field-Programmable Gate Arrays (FPGA's), systolic arrays, or digital signal processors.
Reference: [OLT93] <author> T. Olsen, R. Lockwood, J. Taylor, </author> <title> Programming a Pipelined Image Processor, </title> <booktitle> p.93-100, Workshop on Computer Architectures for Machine Perception, </booktitle> <address> New Orleans Louisiana, </address> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alamitos CA, </address> <year> 1993 </year>
Reference: [P90] <author> J. Perrone, </author> <title> Simple Technique for Optical Flow Estimation, </title> <journal> Journal of the Optical Society of America, </journal> <volume> 7(2) </volume> <pages> 264-278, </pages> <year> 1990 </year> <month> 122 </month>
Reference-contexts: For a given velocity of 1=ffit pixel/frames, we assume that motion is constant for t frames in order to register a cumulative motion of one pixel. Failure of this assumption can result in temporal aliasing, discussed in more detail later. <ref> [P90] </ref> calculates the normal velocity of edges using temporal-delay sensors which are summed in a later stage to determine actual motion. <p> Other schemes such as in [BLP89a] suggest the ANDing of certain features such as edges. Edges are used in some methods of motion detection (as in [Hil84], <ref> [P90] </ref>, [WWB88], [ZL93]) but are not used here because the resulting optical flow density would then depend on the density of the features in the image sequence, which might be too low for subsequent processing, or in any event would vary from one image sequence to another.
Reference: [P92] <author> J. Perrone, </author> <title> Model for the Computation of Self-Motion in Biological Systems, </title> <journal> Journal of the Optical Society of America, </journal> <volume> 9(2) </volume> <pages> 177-194 </pages>
Reference-contexts: vectors, but due to measurement errors this would be inaccurate, and in the case of this particular algorithm it certainly would not work, since the velocity measurements are limited to only eight directions (and zero). [TGS91] uses a least-squares approximation to the intersection of all the available optical flow vectors. <ref> [P92] </ref> uses several direction-selective and speed-tuned sensors without assuming that accurate velocity information is available.
Reference: [PBF89] <author> J-M Pichon, C. Blanes, N. Franceschini, </author> <title> Visual Guidance of a Mobile Robot Equipped with a Network of Self-Motion Sensors, </title> <booktitle> SPIE Vol. 1195 Mobile Robots IV, </booktitle> <address> p.44-53, </address> <year> 1989 </year>
Reference-contexts: Biological organisms make considerable use of optical flow, such as the detection of discontinuities [PRH81], and figure-ground discrimination 7 [B81]. Motion detection is useful for autonomous mobile robot navigation <ref> [PBF89] </ref> as well as undersea navigation [NSY91]. [K86], [KvD78] discusses the decomposition of optical flow into curl, divergence and shear for the purposes of determining local shape of an object, and [Reg86] presents evidence that the human visual system possesses specific detectors for similar types of basic motion. [TR93] describes an
Reference: [PFH87] <author> S. Peleg, O. Federbush, R. Hummel, Custom-made Pyramids, p. </author> <month> 125-146, </month> <title> in Parallel Computer Vision, </title> <editor> L. Uhr ed., </editor> <publisher> Academic Press, </publisher> <address> Orlando Florida, </address> <year> 1987 </year>
Reference-contexts: One basic problem is that each 97 16x16, 8x8, and 4x4. at different spatial scales. This is a somewhat idealized example. level in the pyramid is coarser by a factor of 4, thus resolution decreases exponentially with each layer. <ref> [PFH87] </ref>, [Uhr87] suggest the construction of "gentler" pyramids, with ratios somewhat less than a power-of-2 on each side. [A87a] suggests using overlapping pyramids to avoid missing motion lying on a pixel boundary. <p> The concept of characterizing a scene in increasingly abstract descriptions is of course attractive, however it would be unwise to fix any particular description in hardware until visual processing is much better understood. For example, <ref> [PFH87] </ref> describes pyramids using "gentler" scale ratios than the usual power-of-two pyramid structures. This reduces the normally very large detail differences found between pyramid layers.
Reference: [PH94] <author> D. Patterson, J. Hennessy, </author> <title> Computer Organization and Design: the Hardware/Software Interface, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1994 </year>
Reference-contexts: For example, [WM93] quotes 4 minutes on a Sun workstation and 10 seconds on a 128-processor Thinking Machines CM5 supercomputer. If computer processing power continues to grow at approximately 54% per year <ref> [PH94] </ref>, then such an algorithm would take approximately 16 years for the 1000-fold increase in performance necessary for real-time rates on an ordinary workstation. In addition, many such algorithms depend on the computation of first and in some cases higher numerical derivatives, which are notoriously sensitive to noise. <p> Once the base addresses of corresponding shifted rows are calculated, a single index register can be updated with each new pixel in architectures such as SPARC [Sun87], PA-RISC [hp94], and PowerPC [Mot94a]. Architectures with only base + displacement addressing, such as Alpha [SW94] and MIPS <ref> [PH94] </ref>, require an explicit address add. The former two architectures have existing superscalar implementations, however such implementations with multiple integer or load/store units may require special coding to take full advantage of their abilities (for example, to process two rows in parallel). <p> In particular, the algorithms discussed in this thesis are now usable by anyone with a computer (and in real-time by anyone with a modern, low-end workstation). Finally, general-purpose computers have experienced an exponential growth for the last several years which will likely continue for several more years. <ref> [PH94] </ref> gives a 54% yearly increase for the years 1987-1992. A software algorithm can trivially make use of a new computer generation with only a recompile (see also Section 6.1).
Reference: [PRH81] <author> T. Poggio, W. Reichardt, K. Hausen, </author> <title> A Neuronal Circuitry for Relative Movement Discrimination by the Visual System of the Fly, </title> <address> Naturwissenschaften 68, p.443-446, </address> <year> 1981 </year>
Reference-contexts: By accurately computing this 2-D vector field, it is in principle possible to calculate three-dimensional properties of the environment, and quantities such as time-to-contact with an an observed object [Lee76]. Biological organisms make considerable use of optical flow, such as the detection of discontinuities <ref> [PRH81] </ref>, and figure-ground discrimination 7 [B81].
Reference: [PVT91] <author> T. Poggio, A. Verri, V. Torre, </author> <title> Green Theorems and Qualitative Properties of the Optical Flow, </title> <booktitle> AI Memo 1289, Art. Int. </booktitle> <address> Lab, </address> <publisher> MIT 1991 </publisher>
Reference-contexts: In general however, motion may not continue into the future for several frames, so we do not extend the temporal continuity further than one frame. 4.5 Rotation examples Algorithms and techniques that work for crash detection are often easily modifiable to apply for rotation computation as well (for example see <ref> [PVT91] </ref>). Recall equation 4.1 : y = _ Z For a given approach to an object, at any instant there will be a corresponding time-to-contact t , and a point a fixed distance y from the FOE will have velocity y=t .
Reference: [R75] <author> I. Rock, </author> <title> An Introduction to Perception, </title> <publisher> Macmillan Publishing Co., </publisher> <address> New York 1975 </address>
Reference-contexts: The process of correspondence in optical flow can be similar to that of binocular stereo, except that in the former case the matching is across time, and in the latter the matching is across space <ref> [R75] </ref>. Both applications can make use of various methods for correspondence (issues of correspondence and image matching are discussed in [HS93]), however tracking across space is not equivalent to tracking across time. In particular, the former can make use of the epipolar constraint in matching across space.
Reference: [RA86] <author> V. Ramachandran, S. Anstis, </author> <title> The Perception of Apparent Motion, Scientific American, June 1986; also in compilation The Perceptual World, </title> <publisher> p.139-151, </publisher>
Reference-contexts: Movement in images can be divided into real motion across the retina, and apparent motion, the perception of motion that arises when an object or pattern appears to instantaneously move from one place to another <ref> [RA86] </ref>. In the case of the latter, the correspondence problem must be solved [U79], that of matching points or features in one image with that of another. <p> degrees, the machinery for time-to-contact calculations can be easily reused for the calculation of rotational velocity. (note that the full resolution image was used to enhance visibility). (Points just off the actual image are calculated as having moved because the background is perfectly textureless and thus the matching window "captures" <ref> [RA86] </ref> some off-image points). For this sequence, the images were rotated clockwise by .5 degrees per frame.
Reference: [Reg86] <author> D. Regan, </author> <title> Visual Processing of Four Kinds of Relative Motion, </title> <booktitle> Vision Research </booktitle> (26)1:127-145, 1986 
Reference-contexts: Motion detection is useful for autonomous mobile robot navigation [PBF89] as well as undersea navigation [NSY91]. [K86], [KvD78] discusses the decomposition of optical flow into curl, divergence and shear for the purposes of determining local shape of an object, and <ref> [Reg86] </ref> presents evidence that the human visual system possesses specific detectors for similar types of basic motion. [TR93] describes an application for image coding. [MB90] covers experiments in segmentation, structure from motion, tracking and qualitative shape analysis. [NA89] discusses obstacle avoidance using only qualitative optical flow.
Reference: [RL85] <author> J. Rieger, D. Lawton, </author> <title> Processing Differential Image Motion, </title> <journal> Journal of the Optical Society of America, </journal> <volume> 2(2) </volume> <pages> 354-359 </pages>
Reference-contexts: Since the optical flow vectors at various points in the image can be treated independently, heading determination can be calculated with a two-layer linear neural network [HW91]. <ref> [RL85] </ref> computes the FOE in cases where there is a rotational component present by calculating the difference vectors k ij u k of the optical flow vectors in a local area to a given vector. <p> There is psychophysical evidence supporting this differential motion method of separating the rotational and translational components of the flow field [WH89]. [Hil91] extends the approach of <ref> [RL85] </ref> by considering moving objects in the image as well. Since the chair in Figure 4.1 used in the time-to-contact experiments represents a flat surface, techniques based on differential motion cannot be used for this example.
Reference: [S91] <author> A. Singh, </author> <title> Optic Flow Computation A Unified Perspective, </title> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alamitos CA, </address> <year> 1991 </year>
Reference-contexts: in computational time by using lookup tables for the absolute-value and squared-value functions (although the latter would require somewhat more memory to hold the larger aggregate patch match values). [A87a] chooses to use the SSD for easier mathematical analysis, since the derivatives of the SAD function are discontinuous at zero; <ref> [S91] </ref> follows suit. However, for this algorithm the sum-of-absolute-differences has been used since it appears less sensitive to large deviations such as edges produced by shadows, and high-frequency noise; the discussion of robust measures in Appendix A reinforces this view as well.
Reference: [SAH91] <author> E. P. Simoncelli, E. H. Adelson, D. J. Heeger, </author> <title> Probability Distributions of Optical flow, </title> <booktitle> Proceedings of the IEEE CVPR, </booktitle> <address> p.310-315, </address> <month> May </month> <year> 1991 </year>
Reference-contexts: This method does not blur the optical flow field at discontinuities, but only gives sparse measurements (only along the contours) and may confuse object boundary contours with other kinds of edges, such as those due to changes in shape or reflectance. <ref> [SAH91] </ref> represents the problem probabilistically to account for the confounding effects of image noise, low contrast regions, multiple motions, lighting changes. A two-dimensional probability distribution is used that reflects directional uncertainty in the motion estimates. <p> Identify the stationary regions in the registered images, using M (t) as an initial guess, and set M (t + 1) to the new tracked region. 109 4. Update the temporally integrated image Av (t + 1), and repeat. [BFBB93], [NG92], [VG92], <ref> [SAH91] </ref> make use of confidence measures in calculating optical flow. Conversely, the "winner-take-all" nature of this algorithm makes no use of confidence measures whatsoever, instead relying on the algorithm's inherent smoothness in producing a 100% dense output.
Reference: [SHBV87] <author> D. Schaefer, P. Ho, J. Boyd, C. Vallejos, The GAM Pyramid, p. </author> <month> 15-42, </month> <title> in Parallel Computer Vision, </title> <editor> L. Uhr ed., </editor> <publisher> Academic Press, </publisher> <address> Orlando Florida, </address> <year> 1987 </year>
Reference-contexts: Pyramids can be excellent tools for hierarchal binary morphology (such as the ANDing of various features and region growing), segmentation, and the calculation of various geometrical concepts such as extrema [TLL87], <ref> [SHBV87] </ref>, [St87]. The strength of a pyramid structure with regard to optical flow is that it can efficiently capture large motions of large contiguous areas of an image ([A87a], [A87b], [C90], [G87]), for example see Figure 5.5. <p> Solutions to this problem may be performed in software, but would be more difficult in hardware; see text for details. some sort connecting these levels, if they are physically implemented similar to their logical layout (e.g. <ref> [SHBV87] </ref>). Conversely, current VLSI (Very Large Scale Integration) chips are largely flat devices. A typical chip may include several metal or polysilicate "layers", but they are basically 2-dimensional devices; there is a strong practical incentive in keeping such devices two-dimensional. This is both the pyramid's greatest strength and weakness.
Reference: [SPL92] <author> L. Stewart, A. Payne, T. Levergood, </author> <title> Are DSP Chips Obsolete?, </title> <institution> DEC Cambridge Research Lab, </institution> <type> technical report CRL 92/10, </type> <month> Nov. </month> <year> 1992 </year>
Reference-contexts: In addition, DSP instruction sets generally vary from one generation to another, so they must be reprogrammed, usually in assembly language, with each new generation <ref> [SPL92] </ref>. One attractive feature of DSP's is low overhead looping, which is ideal for the box-filter stage of the basic algorithm described in Section 5.1. However, implementations of zero-cycle branching and branch prediction are commonly found in modern CPU architectures, negating this advantage, since such loops are highly predictable. <p> When combined with faster cycle times, a modern general-purpose processor is usually as fast or faster than a DSP, while remaining much easier to program and debug <ref> [SPL92] </ref>. Thus, while a DSP would be appropriate for a low-cost (or low-power) implementation of a fully developed system, it is not necessary to resort to them to achieve real-time performance. 90 A common measure of performance for a computer vision algorithm is its frame rate, in frames per second.
Reference: [SRT92] <author> M. Shah, K. Rangarajan, P-S Tsai, </author> <title> Motion Trajectories, </title> <booktitle> in Proceedings of the IEEE CVPR, </booktitle> <address> p.839-841, </address> <year> 1992 </year>
Reference-contexts: Occlusion and disocclusion detection can also be handled by spatiotemporal coherence [AD91]. Detecting multiple moving objects requires an initial segmentation step. <ref> [SRT92] </ref> considers a FOE triangle created by the intersection of any three optical flow vectors on an object in segmenting out objects.
Reference: [SS85] <author> J. van Santen, G. Sperling, </author> <title> Elaborated Reichardt Detectors, </title> <journal> Journal of the Optical Society of America, </journal> <volume> 2(2) </volume> <pages> 300-321, </pages> <year> 1985 </year>
Reference-contexts: In theory, the output of any two remaining sensors (within 90 degrees of the actual motion) is sufficient to determine the actual motion, since that would provide two linearly independent components of the velocity vector. <ref> [SS85] </ref> describes elaborated Reichardt detectors, in which the point receptive fields of the original Reichardt detectors are replaced with spatial filters, and stresses the need for effective voting rules in differentiating between different sensors' outputs. [AB85] describes the construction of phase-independent separable spatiotemporally oriented opponent-motion filters, and suggests using the ratios <p> Special-purpose MPEG encoders/decoders, or special-purpose instructions found on general-purpose CPU's (as noted in Section 5.2) are the first examples of this. The relationship between the time-space tradeoff and spatio-temporal models of motion ([AB85], <ref> [SS85] </ref>, [WA85]) is an area of future research. Issues such as lighting and image stabilization (particularly when the camera is mounted on a small, unstable mobile robot) are common problems with machine vision, and it is desirable to develop techniques to deal with these issues.
Reference: [SS93] <author> A. Schilling, W. Strasser, </author> <title> EXACT: Algorithm and Hardware Architecture for an Improved A-Buffer, </title> <booktitle> ACM SIGGRAPH, </booktitle> <address> p.85-91, </address> <year> 1993 </year>
Reference-contexts: For example, spatial resampling techniques can be used in the construction of pyramids [F86]. [Burt81] presents a Hierarchal Discrete Correlation technique which can very efficiently approximate a Gaussian kernel. It remains to be seen how useful antialiasing techniques such as in <ref> [SS93] </ref> are applicable to the problems described in this thesis. In addition, there would be a tremendous potential for using commercial graphics hardware to assist computer vision algorithms should a common ground be found.
Reference: [St87] <author> Q. Stout, </author> <title> Pyramid Algorithms Optimal for the Worst Case, </title> <publisher> p.147-168 </publisher>
Reference-contexts: Pyramids can be excellent tools for hierarchal binary morphology (such as the ANDing of various features and region growing), segmentation, and the calculation of various geometrical concepts such as extrema [TLL87], [SHBV87], <ref> [St87] </ref>. The strength of a pyramid structure with regard to optical flow is that it can efficiently capture large motions of large contiguous areas of an image ([A87a], [A87b], [C90], [G87]), for example see Figure 5.5.
Reference: [SU87] <editor> A.Spoerri,S.Ullman,The Early Detection of Motion Boundaries, </editor> <booktitle> International Conference on Computer Vision, </booktitle> <address> p.209-218, </address> <year> 1987 </year>
Reference-contexts: Authors rarely report the computational time needed for their algorithms; when they do, it is on the order of many minutes per frame ([WM93]), or require specialized hardware such as a Connection Machine ([BLP89a], [DN93], <ref> [SU87] </ref>, [WZ91], [L88]), Datacube ([LK93], [N91]), custom image processors [DW93], or PIPE [KSL85], [WWB88], [ATYM]. <p> These values could be used for interpolation of velocities. Precise boundary detection from motion remains as future work <ref> [SU87] </ref>.
Reference: [Sun87] <institution> The SPARC Architecture Manual, part# 800-1399-07, Sun Microsystems, Mountain View CA, </institution> <year> 1987 </year>
Reference-contexts: In the case of a serial processor however, this can be implemented at negligible cost by using normal addressing arithmetic on certain microprocessors. Once the base addresses of corresponding shifted rows are calculated, a single index register can be updated with each new pixel in architectures such as SPARC <ref> [Sun87] </ref>, PA-RISC [hp94], and PowerPC [Mot94a]. Architectures with only base + displacement addressing, such as Alpha [SW94] and MIPS [PH94], require an explicit address add.
Reference: [SW94] <author> J. Smith, S. Weiss, </author> <title> PowerPC 601 and Alpha 21064: A Tale of Two RISC's, </title> <journal> p.46-58, </journal> <note> June 1994 IEEE Computer 123 </note>
Reference-contexts: Once the base addresses of corresponding shifted rows are calculated, a single index register can be updated with each new pixel in architectures such as SPARC [Sun87], PA-RISC [hp94], and PowerPC [Mot94a]. Architectures with only base + displacement addressing, such as Alpha <ref> [SW94] </ref> and MIPS [PH94], require an explicit address add. The former two architectures have existing superscalar implementations, however such implementations with multiple integer or load/store units may require special coding to take full advantage of their abilities (for example, to process two rows in parallel).
Reference: [T90] <author> J. Tresilian, </author> <title> Perceptual Information for the Timing of Interceptive Action, </title> <booktitle> Perception 19 </booktitle> <pages> 223-239, </pages> <year> 1990 </year>
Reference-contexts: accurate optical flow to perform such functions as obstacle avoidance ([NA89]), Chapter 4 demonstrates that despite these deficiences remarkable accuracy may still be achieved in the context of calculating time-to-collision. 33 Chapter 4 Time-to-Contact One application of optical flow is time-to-contact, sometimes pessimistically referred to as time-to-collision or time-to-crash [Lee76], <ref> [T90] </ref>. Using only optical measurements, and without knowing one's own velocity or distance from a surface, it is possible to determine when contact with the surface will be made. <p> want the robot to be able to move in any direction it can, particularly arbitrary rotational motion (for the purposes of flexibility only; it is only the translational compo 103 nent of motion, and not the rotational component, which provided information about the environment and the observer's relation to it <ref> [T90] </ref>. In addition, rotational movement of an active camera head on a mobile robot will generally be known, since this is generally programmed explicitly). At this particular moment however, this functionality is not available.
Reference: [T91] <author> J. Tresilian, </author> <title> Empirical and Theoretical Issues in the Perception of Time to Contact, Journal of Experimental Psychology: Human Perception and Performance, </title> <address> 17:3 p.865-876, </address> <year> 1991 </year>
Reference-contexts: Although the former has the advantage of being independent of any coordinate system, the latter more accurately characterizes a machine vision system. See also <ref> [T91] </ref>, Appendix). The motion of a point in an image for a moving camera with line of sight orthogonal to the direction of motion is inversely proportional to a point's distance from the focus of projection, and can be used to determine depth; for example see Figure 3.9.
Reference: [TGS91] <author> M. Tistarelli, E. Grosso, G. </author> <title> Sandini, Dynamic Stereo in Visual Navigation, </title> <booktitle> Proceedings of the IEEE CVPR, </booktitle> <address> p.186-193, </address> <year> 1991 </year>
Reference-contexts: Ideally, we could simply take the intersection of any two optical flow vectors, but due to measurement errors this would be inaccurate, and in the case of this particular algorithm it certainly would not work, since the velocity measurements are limited to only eight directions (and zero). <ref> [TGS91] </ref> uses a least-squares approximation to the intersection of all the available optical flow vectors. [P92] uses several direction-selective and speed-tuned sensors without assuming that accurate velocity information is available. <p> Ultimately, it is likely that accurate and robust structure-from-motion will make use of both depth from stereo as well as depth from motion, as in <ref> [TGS91] </ref>. The process of correspondence in optical flow can be similar to that of binocular stereo, except that in the former case the matching is across time, and in the latter the matching is across space [R75].
Reference: [TLL87] <author> S. Tanimoto, T. Ligocki, R. Ling, </author> <title> A Prototype Pyramid Machine for Hierarchal Cellular Logic, p.43-83, in Parallel Computer Vision, </title> <editor> L. Uhr ed., </editor> <address> Orlando Florida, </address> <publisher> Academic Press, </publisher> <year> 1987 </year>
Reference-contexts: Pyramids can be excellent tools for hierarchal binary morphology (such as the ANDing of various features and region growing), segmentation, and the calculation of various geometrical concepts such as extrema <ref> [TLL87] </ref>, [SHBV87], [St87]. The strength of a pyramid structure with regard to optical flow is that it can efficiently capture large motions of large contiguous areas of an image ([A87a], [A87b], [C90], [G87]), for example see Figure 5.5.
Reference: [TM88] <institution> Thinking Machine technical report HA87-4, </institution> <month> April </month> <year> 1987 </year>
Reference-contexts: Running the algorithm by processing individual pixels on separate processors (as in [BLP89a]) is not particularly recommended on a typical modern parallel machine which uses tens or hundreds of much more powerful microprocessors instead of the 1-bit processors of a Connection Machine-2 <ref> [TM88] </ref>, since I/O and communication latencies are then likely to become the bottlenecks.
Reference: [To92a] <author> S. Toelg, </author> <title> Gaze Control for an Active Camera System by Modeling Human Pursuit Eye Movements, </title> <booktitle> SPIE Intelligent Robots and Computer Vision XI, </booktitle> <address> p.585-598, </address> <month> Nov. </month> <year> 1992 </year>
Reference-contexts: Methods based on second or higher derivatives are potentially even more susceptible to noise as the problems associated with numerical differentiation are even worse. <ref> [To92a] </ref> calculates velocity at areas of sufficient contrast in the image using the technique of [UGVT88] at about 2-3 frames per second on a Sun 4/330. For his tracking application, dense measurements are not needed, nor is it necessary for highly accurate measurements.
Reference: [To92b] <author> S. Toelg, </author> <type> personal communication. </type>
Reference-contexts: For his tracking application, dense measurements are not needed, nor is it necessary for highly accurate measurements. Any less than a full 8 bits of information per pixel may make second-derivative techniques unsuitable however <ref> [To92b] </ref>. In practice for robustness, [UGVT88] implements two variants of regularization techniques to create a smooth optical flow field. <p> Clearly, with only 2 grey levels to use, techniques based on numerical differentiation cannot be used (and are undesirable with less than 8 bits precision anyway <ref> [To92b] </ref>).
Reference: [TR93] <author> S. Tubaro, F. Rocca, </author> <title> Motion Field Estimators and Their Application to Image Interplation, in Motion Analysis and Image Sequence Processing, 1993, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Norwell MA </address>
Reference-contexts: robot navigation [PBF89] as well as undersea navigation [NSY91]. [K86], [KvD78] discusses the decomposition of optical flow into curl, divergence and shear for the purposes of determining local shape of an object, and [Reg86] presents evidence that the human visual system possesses specific detectors for similar types of basic motion. <ref> [TR93] </ref> describes an application for image coding. [MB90] covers experiments in segmentation, structure from motion, tracking and qualitative shape analysis. [NA89] discusses obstacle avoidance using only qualitative optical flow. In general the optical flow will not be the same as the true 2-D projection of the 3-D motion field [VP87].
Reference: [TS91] <author> M. Tistarelli, G. </author> <title> Sandini, Direct Estimation of Time-to-impact from Optical Flow, </title> <booktitle> IEEE 1991 Workshop on Visual Motion, </booktitle> <address> p.226-233, </address> <month> Oct. </month> <year> 1991 </year>
Reference-contexts: reported in the literature. [UGVT88] calculates time-to-contact in a real image sequence using a second-derivative method and reports an error of 2.2 time steps at a distance of 31.1 time steps away from contact when measured over a 10x10 optical flow vectors, with a standard deviation of 2.8 time steps. <ref> [TS91] </ref> simplifies the calculation of time-to-impact by using velocity represented in a polar coordinate system, and computes a coarse hazard map using computational time on the order of one minute on a Sparcstation 1. [HG91] reports accuracy of better than 1% at distances of 15 meters down to 2 meters from
Reference: [TWR94] <author> D. Touretzky, H. Wan, A. </author> <title> Redish, Neural Representation of Space in Rats and Robots, </title> <editor> in J.M. Zurada and R.J. Marks, eds., </editor> <booktitle> Computational Intelligence: Imitating Life. Proceedings of the symposium at the 1994 IEEE World Congress on Computational Intelligence, </booktitle> <publisher> IEEE Press, </publisher> <year> 1994 </year>
Reference: [U79] <author> S. Ullman, </author> <title> The Interpretation of Visual Motion, </title> <publisher> The MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1979 </year>
Reference-contexts: Movement in images can be divided into real motion across the retina, and apparent motion, the perception of motion that arises when an object or pattern appears to instantaneously move from one place to another [RA86]. In the case of the latter, the correspondence problem must be solved <ref> [U79] </ref>, that of matching points or features in one image with that of another.
Reference: [UGVT88] <author> S. Uras, F. Girosi, A. Verri, V. Torre, </author> <title> A Computational Approach to Motion Perception,Biological Cybernetics, </title> <booktitle> 60 </booktitle> <pages> 79-87, </pages> <year> 1988 </year>
Reference-contexts: Since ffit is often a significant fraction of a second, it is questionable to ignore these terms, thus yielding second-derivative methods such as <ref> [UGVT88] </ref>, which we will mention later. For now we will ignore this issue.) The spatial derivatives @E @x and @E @y and the temporal derivative at an image point @E @t can be estimated using two or more images by using for example finite or central difference methods [LV89]. <p> temporal derivative of the spatial gradient is zero : drE = 0 Expanding this equation leads to two constraints : @ 2 E @ 2 E v 2 + @x@ t @ 2 E v 1 + @y 2 v 2 + @y@ t This approach is taken in [Nag87], <ref> [UGVT88] </ref>, and assumes that the displacements being measured are less than one half of a cycle of the highest spatial frequencies in the image, otherwise there will be aliasing. <p> Methods based on second or higher derivatives are potentially even more susceptible to noise as the problems associated with numerical differentiation are even worse. [To92a] calculates velocity at areas of sufficient contrast in the image using the technique of <ref> [UGVT88] </ref> at about 2-3 frames per second on a Sun 4/330. For his tracking application, dense measurements are not needed, nor is it necessary for highly accurate measurements. Any less than a full 8 bits of information per pixel may make second-derivative techniques unsuitable however [To92b]. In practice for robustness, [UGVT88] <p> <ref> [UGVT88] </ref> at about 2-3 frames per second on a Sun 4/330. For his tracking application, dense measurements are not needed, nor is it necessary for highly accurate measurements. Any less than a full 8 bits of information per pixel may make second-derivative techniques unsuitable however [To92b]. In practice for robustness, [UGVT88] implements two variants of regularization techniques to create a smooth optical flow field. <p> Clearly, integrating across space and time more than compensates for the non-real-valued 53 measurements of the linear-time optical flow algorithm. These results compare favorably with other techniques reported in the literature. <ref> [UGVT88] </ref> calculates time-to-contact in a real image sequence using a second-derivative method and reports an error of 2.2 time steps at a distance of 31.1 time steps away from contact when measured over a 10x10 optical flow vectors, with a standard deviation of 2.8 time steps. [TS91] simplifies the calculation of
Reference: [Uhr87] <author> L. Uhr, </author> <title> Highly Parallel, Hierarchal, Recognition Cone Perceptual Structures, p.249-292, in Parallel Computer Vision, </title> <editor> L. Uhr ed., </editor> <publisher> Academic Press, </publisher> <address> Orlando Florida, </address> <year> 1987 </year>
Reference-contexts: From [O85] : "It is only by using machines with [levels of performance in the Giga operations per second] that significant real-time image processing becomes a realistic 83 proposition." Entire workshops [CAMP93] and specialized texts <ref> [Uhr87] </ref>, [O85] are devoted to custom hardware or massively parallel implementations of computer vision algorithms. For practical mobile robot vision, considerations must be given to efficiency. One factor related to performance is an efficient representation and algorithm used to analyze the image (s). <p> This results in 3*3*10*56*56*4.5 = 1270080 single-pixel-shift units per second. Thus the workstation implementation is only 3.5 times slower than the implementation on the dedicated image processor. 1 By early 1995, this gap is expected to be closed; see Section 6.1. <ref> [Uhr87] </ref> argues strongly for massively parallel processing systems for computer vision : "Perception is a massively parallel process... <p> Companies such as Thinking Machines has recognized the advantages of using moderate numbers of commercial microprocessors in their latest offering (the CM-5) in favor of previous models which are composed of up to tens of thousands of 1-bit custom processors ([D86], [LBC87], [L88]). <ref> [Uhr87] </ref> notes that the overhead for communication in a MIMD (multiple-instruction, multiple data) computer may 1 Note that this only considers the raw amount of work done, and not whether or not the work done is particularly efficient or useful. <p> One basic problem is that each 97 16x16, 8x8, and 4x4. at different spatial scales. This is a somewhat idealized example. level in the pyramid is coarser by a factor of 4, thus resolution decreases exponentially with each layer. [PFH87], <ref> [Uhr87] </ref> suggest the construction of "gentler" pyramids, with ratios somewhat less than a power-of-2 on each side. [A87a] suggests using overlapping pyramids to avoid missing motion lying on a pixel boundary.
Reference: [VG92] <author> J.A. Vlontzos, D. Geiger, </author> <title> A MRF Approach to Optical Flow Estimation, </title> <booktitle> Proceedings of the IEEE CVPR, </booktitle> <address> p.853-856, </address> <year> 1992 </year>
Reference-contexts: Identify the stationary regions in the registered images, using M (t) as an initial guess, and set M (t + 1) to the new tracked region. 109 4. Update the temporally integrated image Av (t + 1), and repeat. [BFBB93], [NG92], <ref> [VG92] </ref>, [SAH91] make use of confidence measures in calculating optical flow. Conversely, the "winner-take-all" nature of this algorithm makes no use of confidence measures whatsoever, instead relying on the algorithm's inherent smoothness in producing a 100% dense output.
Reference: [VP87] <author> A. Verri, T. Poggio, </author> <title> Against Quantitative Optical Flow, </title> <booktitle> Proceedings of the IEEE ICCV, </booktitle> <address> p.171-180, </address> <year> 1987 </year>
Reference-contexts: In general the optical flow will not be the same as the true 2-D projection of the 3-D motion field <ref> [VP87] </ref>. For example, a rotating, perfectly featureless sphere will not induce any optical flow, however the 2-D projection of its motion field is non-zero everywhere on the sphere except at the occluding boundaries.
Reference: [W93] <author> C. Weems, Jr., </author> <title> The Second Generation Image Understanding Architecture and Beyond, </title> <booktitle> p.276-285, Workshop on Computer Architectures for Machine Perception, </booktitle> <address> New Orleans Louisiana, </address> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alamitos CA, </address> <year> 1993 </year>
Reference-contexts: such a custom pyramid in hardware however would be much more difficult than the usual 4-to-1 child-to-parent ratio found in most pyramids. [Uhr87]'s suggestion of a massively parallel lowest level augmented with a number of much more powerful processors appears to be a practical alternative to the typical hardware pyramid. <ref> [W93] </ref> describes UMASS's second-generation Image Understanding Architecture, consisting of a low-level reconfigurable array of bit-serial processors for sensory data, an intermediate level of digital signal processors to execute grouping of tokens and graph matching, and a high level of RISC processors for knowledge-based processing.
Reference: [WA85] <author> A. Watson, A. Ahumada Jr, </author> <title> Model of Human Motion Sensing, </title> <journal> Journal of the Optical Society of America, </journal> <volume> 2(2) </volume> <pages> 322-342, </pages> <year> 1985 </year>
Reference-contexts: This approach assumes that several points in spatio-temporal space can be detected in the moving surface. If, however, the aperture problem exists, then the points in spatiotemporal space occupy a line, and the plane is not fully determined. To solve this problem, <ref> [WA85] </ref> combines ten oriented sensors into opponent-motion pairs, discarding the smaller response of the pair. <p> Special-purpose MPEG encoders/decoders, or special-purpose instructions found on general-purpose CPU's (as noted in Section 5.2) are the first examples of this. The relationship between the time-space tradeoff and spatio-temporal models of motion ([AB85], [SS85], <ref> [WA85] </ref>) is an area of future research. Issues such as lighting and image stabilization (particularly when the camera is mounted on a small, unstable mobile robot) are common problems with machine vision, and it is desirable to develop techniques to deal with these issues.
Reference: [War88] <author> W. Warren Jr., </author> <title> Action Modes and Laws of Control for the Visual Guidance of Action, in Complex Movement Behavior: The motor-action controversy, </title> <editor> p.339-380, O. Meijer and K. Roth eds., </editor> <publisher> Elsevier Science Pubs., B.V. (North-Holland), </publisher> <year> 1988 </year>
Reference: [WD93] <author> S. White, S. Dhawan, POWER2: </author> <title> Next Generation of the RISC System/6000 Family, in RISC System/6000 Technology: Volume II, </title> <institution> p.2-12, IBM, </institution> <year> 1993 </year> <month> 124 </month>
Reference-contexts: Alternatively, it could run at 4 frames per second calculating 16 speeds per frame, or possibly 32 frames per second calculating only 2 speeds per frame. (Trademarks are the property of their respective owners.) For computer architectures such as the PowerPC, which is designed for fast looping across arrays <ref> [WD93] </ref>, an optimized program could yield even higher "magic numbers".
Reference: [WH89] <author> W. Warren Jr., D. </author> <title> Hannon, Eye Movements and Optical Flow, </title> <journal> Journal of the Optical Society of America, </journal> <volume> 7(1) </volume> <pages> 160-169, </pages> <year> 1989 </year>
Reference-contexts: This is true regardless of rotational motion, since the latter adds the same motion component regardless of the depth 38 variations in the image [LP80]. There is psychophysical evidence supporting this differential motion method of separating the rotational and translational components of the flow field <ref> [WH89] </ref>. [Hil91] extends the approach of [RL85] by considering moving objects in the image as well. Since the chair in Figure 4.1 used in the time-to-contact experiments represents a flat surface, techniques based on differential motion cannot be used for this example. <p> Unlike gradient-based approaches (such as [NG92]), the patch-matching nature of this algorithm is largely insensitive to local contrast. The use of the entire dense optical flow field is supported by <ref> [WH89] </ref> which presents psychophysical evidence that observer performance in heading estimation decreases with a decrease in the density of the optical flow information available. The optical flow for at several points in the collision sequence are shown in Figures 4.4-4.6 along with the calculated FOE.
Reference: [WM93] <author> J. Weber, J. Malik, </author> <title> Robust Computation of Optical Flow in a Multi-Scale Differential Framework, </title> <booktitle> Fourth International Conference on Computer Vision, </booktitle> <address> p.12-20, </address> <year> 1993 </year>
Reference-contexts: In particular, many current optical flow algorithms are not suited for practical applications such as segmentation and structure-from-motion because they either require highly specialized hardware or up to several minutes on a scientific workstation. For example, <ref> [WM93] </ref> quotes 4 minutes on a Sun workstation and 10 seconds on a 128-processor Thinking Machines CM5 supercomputer. <p> In addition, many such algorithms depend on the computation of first and in some cases higher numerical derivatives, which are notoriously sensitive to noise. In fact the current trend in optical flow research ([BFBB92], <ref> [WM93] </ref>) is to stress accuracy under ideal conditions (either no noise or at best, modeling the noise as Gaussian, a questionable assumption), and not to consider computational resource requirements or resistance to noise, which are essential for real-time robotics. <p> construction of phase-independent separable spatiotemporally oriented opponent-motion filters, and suggests using the ratios between multiple sensors for contrast-invariance; although higher contrast will increase the absolute 14 outputs of the motion sensors, the ratios of their outputs should remain constant. [AB85] also shows the relationship between energy models and Reichardt detectors. <ref> [WM93] </ref> first convolves the image with a set of linear, separable spatiotemporal filters and applies the brightness constancy equation (equation 2.1) to each. <p> This technique is essentially a differential method applied to phase rather than intensity [BFBB92]. The full 2-dimensional velocity field is determined by fitting a linear velocity field to the component velocities. Although these methods can produce accurate results ([BFBB93], <ref> [WM93] </ref>), methods based on velocity-tuned or spatiotemporal filters can be extremely computationally intensive, requiring up to several minutes or even hours on a scientific workstation. <p> It would be pointless to attempt to measure the performance of this particular algorithm per se using the methods used in [BFBB92] and <ref> [WM93] </ref>; since the individual optical flow vectors are quantized in magnitude and direction, the average error for a single pixel would be very poor.
Reference: [WWB88] <author> A. Waxman, J. Wu, F. Bergholm, </author> <title> Convected Activation Profiles: Receptive fields for Real-Time Measurement of Short-Range Visual Motion, </title> <booktitle> Proceedings of the IEEE CVPR, </booktitle> <address> p.717-723, </address> <year> 1988 </year>
Reference-contexts: Authors rarely report the computational time needed for their algorithms; when they do, it is on the order of many minutes per frame ([WM93]), or require specialized hardware such as a Connection Machine ([BLP89a], [DN93], [SU87], [WZ91], [L88]), Datacube ([LK93], [N91]), custom image processors [DW93], or PIPE [KSL85], <ref> [WWB88] </ref>, [ATYM]. Techniques which can run in real-time often impose strict restrictions on the environment; [HB88] presents a technique that can segment in real-time for tracking purposes, but requires that a textured object be moving in front of a relative textureless background. <p> Robust error norms are used to reject outliers in each error term. <ref> [WWB88] </ref> calculates the motion of edges by differentiating their Gaussian-convolved 12 spatio-temporal activation profiles with the PIPE pipelined image processor at 15 frames per second. This method is very fast and can be robust, but only gives normal flow at edges, not the true flow. <p> Other schemes such as in [BLP89a] suggest the ANDing of certain features such as edges. Edges are used in some methods of motion detection (as in [Hil84], [P90], <ref> [WWB88] </ref>, [ZL93]) but are not used here because the resulting optical flow density would then depend on the density of the features in the image sequence, which might be too low for subsequent processing, or in any event would vary from one image sequence to another.
Reference: [WZ91] <author> J. Woodfill, R. </author> <title> Zabih, </title> <booktitle> An Algorithm for Real-Time Tracking of Non-Rigid Objects,Ninth National Conference on Artificial Intelligence, </booktitle> <address> p.718-723,1991 </address>
Reference-contexts: Authors rarely report the computational time needed for their algorithms; when they do, it is on the order of many minutes per frame ([WM93]), or require specialized hardware such as a Connection Machine ([BLP89a], [DN93], [SU87], <ref> [WZ91] </ref>, [L88]), Datacube ([LK93], [N91]), custom image processors [DW93], or PIPE [KSL85], [WWB88], [ATYM].
Reference: [ZL93] <author> A. Zakhor, F. Lari, </author> <title> Edge Based 3-D Camera Motion Estimation with Application to Video Coding, in Motion Analysis and Image Sequence Processing, 1993, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Norwell MA 125 </address>
Reference-contexts: However, the pixel-matching technique can still be successful based on the simple argument presented in performing a degenerate form of binary feature matching, as in [BLP89a]. <ref> [ZL93] </ref> notes many of the computational advantages in using a 1-bit feature map (in their case, an edge map) over full 8-bit grey-level images. The optical flow for these images is shown in Figures 4.22-4.24. <p> Other schemes such as in [BLP89a] suggest the ANDing of certain features such as edges. Edges are used in some methods of motion detection (as in [Hil84], [P90], [WWB88], <ref> [ZL93] </ref>) but are not used here because the resulting optical flow density would then depend on the density of the features in the image sequence, which might be too low for subsequent processing, or in any event would vary from one image sequence to another.
References-found: 143

