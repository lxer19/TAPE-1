URL: http://www.iiia.csic.es/~cerquide/publ/Disc.ps.gz
Refering-URL: http://www.iiia.csic.es/~mantaras/
Root-URL: 
Email: fcerquide,mantarasg@iiia.csic.es  
Title: Proposal and Empirical Comparison of a Parallelizable Distance-Based Discretization Method  
Author: Jesus Cerquides and Ramon Lopez de Mantaras 
Address: 08193, Bellaterra, Barcelona, Spain  
Affiliation: Artificial Intelligence Research Institute, IIIA Spanish Council for Scientific Research, CSIC  
Abstract: Many classification algorithms are designed to work with datasets that contain only discrete attributes. Discretization is the process of converting the continuous attributes of the dataset into discrete ones in order to apply some classification algorithm. In this paper we first review previous work in discretization, then we propose a new discretization method based on a distance proposed by Lopez de Mantaras and show that it can be easily implemented in parallel, with a high improvement in its complexity. Finally we empirically show that our method has an excellent performance compared with other state-of-the-art methods. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Catlett, J. </author> <year> 1991. </year> <title> On Changing Continuous Attributes into Ordered Discrete Attributes. </title> <editor> In Kodratoff, Y., ed., </editor> <booktitle> Proceedings of the Eu-ropean Working Session on Learning, </booktitle> <pages> 164-178. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Entropy is a supervised, incremental, top-down method described in (Fayyad & Irani 1992),(Fayyad & Irani 1993). Entropy discretization recursively selects the cutpoints minimizing entropy until a stopping criterion based on the Minimum Description Length criterion ends the recursion. D-2 is a supervised, incremental, top-down method described in <ref> (Catlett 1991) </ref>. D-2 recursively selects the cutpoints maximizing Quinlan's Gain until a stopping criterion based on a set of heuristic rules ends the recursion. Distance-Based discretization Our algorithm, based on Mantaras distance between partitions (Lopez de Mantaras 1991), is global, supervised, static and Top-Down incremental.
Reference: <author> Cerquides, J. </author> <year> 1997. </year> <title> Mantaras Distance for Discretization. Proposal and Empirical Comparison of a New Parallelizable Discretization Method. </title> <type> Technical report, </type> <institution> IIIA-97-03. </institution>
Reference-contexts: Concretely, when having N processors, the time is O (k p 2 logN ), which is clearly better than the time found for the sequential procedure. A parallel version of the method has been implemented in MPI and its code can be examined in <ref> (Cerquides 1997) </ref>. Empirical comparison Comparison design We will use the accuracy of two classification algorithms to measure the discretization goodness. The two algorithms will be ID3 (Quinlan 1986) (with no pruning) and Naive-Bayes (Langley, Iba, & Thomp-son 1992). <p> We take the average of the 50 runs as a measure of performance. We also keep the results of the 50 runs to make two statistical significance tests: Rank and Signed Rank. In <ref> (Cerquides 1997) </ref>,(Gibbons 1971) one can find a complete explanation of this tests. Comparison results Average accuracies comparison For each dataset and classification algorithm we rank the 6 discretiza-tion methods, from the first place (the most accurate) to the sixth one. <p> Significance test comparison For each dataset and each classification algorithm we have performed the pairwise comparison of accuracies for the six dis-cretization methods. This has given us a partial ordering of the methods for each dataset. The full results are in <ref> (Cerquides 1997) </ref>. We will try to extract the most important conclusions in a few statements: * Rank and Signed Rank tests differ only in one over twenty of the comparisons.
Reference: <author> Dougherty, J.; Kohavi, R.; and Sahami, M. </author> <year> 1995. </year> <title> Supervised and Unsupervised Discretization of Continuous Features. </title> <editor> In Prieditis, A., and Rusell, S., eds., </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference. </booktitle>
Reference-contexts: Copyright 1997, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. Discretization methods classification In <ref> (Dougherty, Kohavi, & Sahami 1995) </ref> three different axis (Global vs. Local, Supervised vs. Unsupervised and Static vs. Dynamic) are used to make a classification of discretization methods. We will add two more axis: Direct vs.
Reference: <author> Fayyad, U. M., and Irani, K. B. </author> <year> 1992. </year> <title> On the Hadling of Continuous-Valued Attributes in Decision Tree Generation. </title> <booktitle> Machine Learning 8 </booktitle> <pages> 87-102. </pages>
Reference-contexts: ChiMerge is a supervised, incremental, bottom-up method described in (Kerber 1992). ChiMerge uses 2 statistic to determine the independence of the class from the two adjacent intervals, combining them if it is independent, and allowing them to be separate otherwise. Entropy is a supervised, incremental, top-down method described in <ref> (Fayyad & Irani 1992) </ref>,(Fayyad & Irani 1993). Entropy discretization recursively selects the cutpoints minimizing entropy until a stopping criterion based on the Minimum Description Length criterion ends the recursion. D-2 is a supervised, incremental, top-down method described in (Catlett 1991).
Reference: <author> Fayyad, U. M., and Irani, K. B. </author> <year> 1993. </year> <title> Multi-Interval Discretiza-tion of Continuous-Valued Attributes for Classification Learning. </title> <booktitle> In 13th International Joint Conference of Artificial Intelligence, </booktitle> <pages> 1022-1027. </pages>
Reference-contexts: The stopping criterion We needed a heuristic to evaluate improvement. We developed a stopping criterion based on the Minimum Description Length Principle (MDLP). The development followed to apply MDLP to our problem is parallel to that in <ref> (Fayyad & Irani 1993) </ref>. The problem that needs to be solved is a communication problem. We have to communicate a classifier method, that allows the receiver to determine the class of each example.
Reference: <author> Gibbons, J. </author> <year> 1971. </year> <title> Nonparametric statistical inference. McGraw-Hill series in probability and statistics. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Kerber, R. </author> <year> 1992. </year> <title> ChiMerge: Discretization of Numeric Attributes. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 123-128. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: Equal frequency is another unsupervised direct method. It counts the number of values we have from the attribute that we are trying to discretize and partitions it into intervals containing the same number of examples. ChiMerge is a supervised, incremental, bottom-up method described in <ref> (Kerber 1992) </ref>. ChiMerge uses 2 statistic to determine the independence of the class from the two adjacent intervals, combining them if it is independent, and allowing them to be separate otherwise. Entropy is a supervised, incremental, top-down method described in (Fayyad & Irani 1992),(Fayyad & Irani 1993).
Reference: <author> Langley, P.; Iba, W.; and Thompson, K. </author> <year> 1992. </year> <title> An Analysis of Bayesian Classifiers. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 223-228. </pages> <publisher> AAAI Press and MIT Press. </publisher>
Reference-contexts: Empirical comparison Comparison design We will use the accuracy of two classification algorithms to measure the discretization goodness. The two algorithms will be ID3 (Quinlan 1986) (with no pruning) and Naive-Bayes <ref> (Langley, Iba, & Thomp-son 1992) </ref>. We will run each algorithm in 9 different domains with different characteristics (see Table 1). For each learning algorithm, discretization method and dataset we do 50 runs, each one with 70% of the examples as training set and the remainder 30% as test set.
Reference: <author> Lopez de Mantaras, R. </author> <year> 1991. </year> <title> A Distance Based Attribute Selection Measure for Decision Tree Induction. </title> <booktitle> Machine Learning 6 </booktitle> <pages> 81-92. </pages>
Reference-contexts: D-2 is a supervised, incremental, top-down method described in (Catlett 1991). D-2 recursively selects the cutpoints maximizing Quinlan's Gain until a stopping criterion based on a set of heuristic rules ends the recursion. Distance-Based discretization Our algorithm, based on Mantaras distance between partitions <ref> (Lopez de Mantaras 1991) </ref>, is global, supervised, static and Top-Down incremental. This means that it is required to have two main components, a cut-point selection criterion and a stopping criterion. <p> For more details see <ref> (Lopez de Mantaras 1991) </ref>. Once T A is found, the next step is checking whether the cutpoint improvement is significant enough to accept it or if otherwise no further cutpoints are considered necessary for the discretization. The stopping criterion We needed a heuristic to evaluate improvement.
Reference: <author> Quinlan, J. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 81-106. </pages>
Reference-contexts: A parallel version of the method has been implemented in MPI and its code can be examined in (Cerquides 1997). Empirical comparison Comparison design We will use the accuracy of two classification algorithms to measure the discretization goodness. The two algorithms will be ID3 <ref> (Quinlan 1986) </ref> (with no pruning) and Naive-Bayes (Langley, Iba, & Thomp-son 1992). We will run each algorithm in 9 different domains with different characteristics (see Table 1).
References-found: 10

