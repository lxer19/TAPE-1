URL: http://www.eecs.berkeley.edu/~varaiya/papers_ps.dir/andrea.5.ps
Refering-URL: http://www.eecs.berkeley.edu/~varaiya/
Root-URL: 
Title: Capacity, Mutual Information, and Coding for Finite-State Markov Channels  
Author: Andrea J. Goldsmith, Member, IEEE and Pravin P. Varaiya, Fellow, IEEE A. Goldsmith P. Varaiya 
Keyword: Index Terms: Finite-State Markov Channels, Capacity, Mutual Information, Decision-Feedback Maximum-Likelihood Decoding.  
Address: Pasadena, CA 91125.  Berkeley, CA 94720.  
Affiliation: Studies, University of California, Berkeley.  Department of Electrical Engineering, California Institute of Technology,  Department of Electrical Engineering and Computer Science, University of California,  
Note: Work supported in part by an IBM graduate fellowship, and in part by the PATH program, Institute of Transportation  is with the  is with the  
Abstract: The Finite-State Markov Channel (FSMC) is a discrete-time varying channel whose variation is determined by a finite-state Markov process. These channels have memory due to the Markov channel variation. We obtain the FSMC capacity as a function of the conditional channel state probability. We also show that for i.i.d. channel inputs, this conditional probability converges weakly, and the channel's mutual information is then a closed-form continuous function of the input distribution. We next consider coding for FSMCs. In general, the complexity of maximum-likelihood decoding grows exponentially with the channel memory length. Therefore, in practice, interleaving and memoryless channel codes are used. This technique results in some performance loss relative to the inherent capacity of channels with memory. We propose a maximum-likelihood decision-feedback decoder with complexity that is independent of the channel memory. We calculate the capacity and cutoff rate of our technique, and show that it preserves the capacity of certain FSMCs. We also compare the performance of the decision-feedback decoder with that of interleaving and memoryless channel coding on a fading channel with 4PSK modulation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Mushkin and I. Bar-David, </author> <title> "Capacity and coding for the Gilbert-Elliot channels,". </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol. IT-35, No. 6, </volume> <pages> pp. 1277-1290, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: Thus, a FSMC where c k indexes a set of BSCs with different crossover probabilities is a uniformly symmetric variable noise channel. Therefore, both proposition 4 of <ref> [1] </ref> and the capacity formula obtained in [5] are corollaries of Theorem 5.1. VI Decision-Feedback Decoder A block diagram for a system with decision-feedback decoding is depicted in Figure 3. <p> Thus, the difference between the initial and final values of C j and R j indicate the performance improvement of the decision-feedback decoder over conventional techniques. 15 For this two-state model, the channel memory can be quantified by the parameter 4 1 g b, since for 2 fG; Bg <ref> [1] </ref>, p (S n = jS 0 = ) p (S n = jS 0 6= ) = n : (54) In Figure 8 we show the decision-feedback decoder's capacity and cutoff rates (C df and R df respectively) as functions of . <p> Proof of Lemma 4.2 We first note that the conditional entropy H (W jV ) = E log p (wjv), where the log function is concave on <ref> [0; 1] </ref>. To show the first inequality in (25), let f denote any concave function.
Reference: [2] <author> A.J. Goldsmith, </author> <title> "The Capacity of Time-Varying Multipath Channels," </title> <type> Masters Thesis, </type> <institution> Dept. of Electrical Engineering and Computer Science, University of California at Berkeley, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: We refer to the channel model as a finite-state Markov channel (FSMC). If the transmitter and receiver have perfect state information, then the capacity of the FSMC is just the statistical average over all states of the corresponding channel capacity <ref> [2] </ref>. On the other hand, with no information about the channel state or its transition structure, capacity is reduced to that of the Arbitrarily Varying Channel [3]. We consider the intermediate case, where the channel transition structure of the FSMC is known.
Reference: [3] <author> I. Csiszar and J. Korner, </author> <title> Information Theory: Coding Theorems for Discrete Memoryless Channels. </title> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: On the other hand, with no information about the channel state or its transition structure, capacity is reduced to that of the Arbitrarily Varying Channel <ref> [3] </ref>. We consider the intermediate case, where the channel transition structure of the FSMC is known. The memory of the FSMC comes from the dependence of the current channel state on past inputs and outputs.
Reference: [4] <author> A.J. Viterbi and J.K. Omura, </author> <title> Principles of Digital Communication and Coding. </title> <publisher> McGraw-Hill, </publisher> <year> 1979. </year>
Reference-contexts: Thus, a common strategy for channels with memory is to disperse the memory using an interleaver: if the span of the interleaver is long, then the cascade of the interleaver, channel, and deinterleaver can be considered memoryless, and coding techniques for memoryless channels may be used <ref> [4] </ref>. However, this cascaded channel has a lower inherent Shannon capacity than the original channel, since coding is restricted to memoryless channel codes. <p> In general, the output of an AWN channel is quantized to the nearest symbol in a finite output alphabet: we call this the quantized AWN (Q-AWN) channel. If the Q-AWN channel has a symmetric multiphase input alphabet of constant amplitude and output phase quantization <ref> [4, page 80] </ref>, then it is easily checked that p k (yjx) depends only on p k (jy xj), which in turn depends only on the noise density n k . Thus, it is a variable noise channel 3 . <p> Applying Lemma 4.6 to lim n!1 H (Y n jX n ; n ) completes the proof. 2 The BSC is equivalent to a binary input Q-AWN channel with binary quantization <ref> [4] </ref>. Thus, a FSMC where c k indexes a set of BSCs with different crossover probabilities is a uniformly symmetric variable noise channel. Therefore, both proposition 4 of [1] and the capacity formula obtained in [5] are corollaries of Theorem 5.1. <p> Thus, the decision-feedback decoder preserves the inherent capacity of such channels. Although capacity gives the maximum data rate for any ML encoding scheme, established coding techniques generally operate at or below the channel cutoff rate <ref> [4] </ref>.
Reference: [5] <author> H.S. Wang and N. Moayeri, </author> <title> "Modeling, capacity, and joint source/channel coding for Rayleigh fading channels," </title> <type> Technical Report WINLAB-TR-32, </type> <institution> Wireless Information Network Laboratory, Rutgers University, </institution> <month> May </month> <year> 1992. </year> <title> Also "Finite-state Markov channel a useful model for radio communication channels," </title> <journal> IEEE Trans. Vehic. Technol., </journal> <volume> Vol. VT-44, No. 1, </volume> <pages> pp. 163-171, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: We will calculate the capacity penalty of the decision-feedback decoder for general FSMCs (ignoring error propagation), and show that this penalty vanishes for a certain class of FSMCs. The most common example of a FSMC is a correlated fading channel. In <ref> [5] </ref>, a FSMC model for Rayleigh fading is proposed, where the channel state varies over binary symmetric channels with different crossover probabilities. Our recursive capacity formula is a generalization of the capacity found in [5], and we also prove the convergence of their recursive algorithm. <p> The most common example of a FSMC is a correlated fading channel. In <ref> [5] </ref>, a FSMC model for Rayleigh fading is proposed, where the channel state varies over binary symmetric channels with different crossover probabilities. Our recursive capacity formula is a generalization of the capacity found in [5], and we also prove the convergence of their recursive algorithm. Since capacity is generally unachievable for any practical coding scheme, the channel cutoff rate indicates the practical achievable information rate of a channel with coding. <p> Thus, a FSMC where c k indexes a set of BSCs with different crossover probabilities is a uniformly symmetric variable noise channel. Therefore, both proposition 4 of [1] and the capacity formula obtained in <ref> [5] </ref> are corollaries of Theorem 5.1. VI Decision-Feedback Decoder A block diagram for a system with decision-feedback decoding is depicted in Figure 3. The system is composed of a conventional (block or convolutional) encoder for memoryless channels, block interleaver, FSMC, decision-feedback decoder, and deinterleaver.
Reference: [6] <author> K. Leeuwin-Boulle and J.C. Belfiore, </author> <title> "The cutoff rate of time correlated fading channels," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol. IT-39, No. 2, </volume> <pages> pp. 612-617, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Since capacity is generally unachievable for any practical coding scheme, the channel cutoff rate indicates the practical achievable information rate of a channel with coding. The cutoff rate for correlated fading channels with MPSK inputs, assuming channel state information at the receiver, was obtained in <ref> [6] </ref>: we obtain the same cutoff rate on this channel using decision-feedback decoding. Most coding techniques for fading channels rely on built-in time diversity in the code to mitigate the fading effect. Code designs of this type can be found in [7, 8, 9] and the references therein.
Reference: [7] <author> N. Seshadri and C.-E. W. Sundberg, </author> <title> "Coded modulations for fading channels an overview," </title> <journal> European Trans. Telecommun. and Related Technol., </journal> <volume> Vol. ET-4, No. 3, </volume> <pages> pp. 309-324, </pages> <month> May-June </month> <year> 1993. </year>
Reference-contexts: Most coding techniques for fading channels rely on built-in time diversity in the code to mitigate the fading effect. Code designs of this type can be found in <ref> [7, 8, 9] </ref> and the references therein. These codes use the same time-diversity idea as interleaving and memoryless channel encoding, except that the diversity is implemented with the code metric instead of the interleaver.
Reference: [8] <author> L.-F. Wei, </author> <title> "Coded M-DPSK with built-in time diversity for fading channels," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol. IT-39, No. 6, </volume> <pages> pp. 1820-1839, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Most coding techniques for fading channels rely on built-in time diversity in the code to mitigate the fading effect. Code designs of this type can be found in <ref> [7, 8, 9] </ref> and the references therein. These codes use the same time-diversity idea as interleaving and memoryless channel encoding, except that the diversity is implemented with the code metric instead of the interleaver.
Reference: [9] <editor> D. Divsalar and M.K. Simon, </editor> <title> "The design of trellis coded MPSK for fading channels: Set partitioning for optimum code design," </title> <journal> IEEE Trans. Commun., </journal> <volume> Vol. COM-36, No. 9, </volume> <pages> pp. 1013-1021, </pages> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: Most coding techniques for fading channels rely on built-in time diversity in the code to mitigate the fading effect. Code designs of this type can be found in <ref> [7, 8, 9] </ref> and the references therein. These codes use the same time-diversity idea as interleaving and memoryless channel encoding, except that the diversity is implemented with the code metric instead of the interleaver.
Reference: [10] <author> W.C. </author> <title> Dam and D.P. Taylor, "An adaptive maximum likelihood receiver for correlated Rayleigh-fading channels," </title> <journal> IEEE Trans. Commun., </journal> <volume> Vol. COM-42, No. 9, </volume> <pages> pp. 2684-2692, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Thus, as with interleaving and memoryless channel encoding, channel correlation information is ignored with these coding schemes. Maximum-likelihood sequence estimation for fading channels without coding has been examined in <ref> [10, 11] </ref>. However, it is difficult to implement coding with these schemes due to the code delays. In our scheme, coding delays do not result in state decision delays, since the decisions are based on estimates of the coded bits.
Reference: [11] <author> J.H. </author> <title> Lodge and M.L. Moher, "Maximum-likelihood sequence estimation of CPM signals transmitted over Rayleigh flat-fading channels," </title> <journal> IEEE Trans. Commun., </journal> <volume> Vol. COM-38, No. 6, </volume> <pages> pp. 787-794, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Thus, as with interleaving and memoryless channel encoding, channel correlation information is ignored with these coding schemes. Maximum-likelihood sequence estimation for fading channels without coding has been examined in <ref> [10, 11] </ref>. However, it is difficult to implement coding with these schemes due to the code delays. In our scheme, coding delays do not result in state decision delays, since the decisions are based on estimates of the coded bits.
Reference: [12] <author> P. Billingsley. </author> <title> Probability and Measure. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1986 </year>
Reference-contexts: We also obtain some additional properties of the entropy and mutual information when the channel inputs are i.i.d. By definition, the Markov chain S n is aperiodic and irreducible over a finite state space, so the effect of its initial state dies away exponentially with time <ref> [12] </ref>. Thus, the FSMC is an 1 Note that B (y n ) has an implicit dependence on the distribution of x n . 6 indecomposable channel. <p> Proof of Lemmas A2.3 and A2.5 We must show that for all m ; 2 P (X), if m ! , then m ! and - m ! - . We first show the convergence of - m . From <ref> [12, page 346] </ref>, in order to show that - m ! - , it suffices to show that f- m g is a tight sequence of probability measures 5 , and that any subsequence of - m which converges weakly converges to - . <p> Finally, for fixed y and , f (y; fi) and p (yjfi) are linear in fi, so (f (y; fi))p (yjfi) is a bounded continuous function of fi. Thus, (80) converges to zero by the weak convergence of - k to (Theorem 25.8 of <ref> [12] </ref>). 2 Since the f m g sequence is also tight, the proof that m ! follows if the limit of any convergent subsequence of f m g is the invariant distribution for under (12). <p> n+1 ; x n ; y n ])jx n+1 ; x n 2 ) = Ef (p [y n+1 jx n+1 ; x n ; y n ]); (84) where a follows from the stationarity of the channel and the inputs, b and d follow from properties of conditional expectation <ref> [12] </ref>, and c is a consequence of Jensen's inequality. 25 The second inequality in (25) results from the fact that conditioning on an additional random variable, in this case the initial state S 0 , always reduces the entropy [14]. <p> (27), we have Ef (p [y n jy n1 ]) = Ef (p [y n+1 jy n b 2 )) EE (f (p [y n+1 jy n ])jy n d where a follows from the stationarity of the inputs and channel, b and d follow from properties of conditional expectation <ref> [12] </ref>, and c is a consequence of Jensen's inequality. The second inequality results from the fact that conditioning on an additional random variable reduces entropy. <p> The sixth equality follows from the definition of -n and the stationarity of the channel inputs. The last equality follows from the weak convergence of n and the fact that the entropy is continuous in and is bounded by log jYj (Theorem 25.8 of <ref> [12] </ref>). The limiting conditional entropy H (Y n jX n ; n ) is obtained with a similar argument. Let n denote the distribution of n and denote the corresponding limit distribution.
Reference: [13] <author> R.G. Gallager, </author> <title> Information Theory and Reliable Communication. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1968. </year>
Reference-contexts: Thus, the FSMC is an 1 Note that B (y n ) has an implicit dependence on the distribution of x n . 6 indecomposable channel. The capacity of an indecomposable channel is independent of its initial state, and is given by Theorem 4.6.4 in <ref> [13] </ref>: C = lim max 1 I (X n ; Y n ); (17) where I (; ) denotes mutual information and P (X n ) denotes the set of all input distributions on X n . <p> Using Lemma 4.1, we can also express the capacity as C = lim max 1 n X [H (Y i j i ) H (Y i jX i ; i )] : (24) Although Theorem 4.6.4 in <ref> [13] </ref> guarantees the convergence of (24), the random vectors n and n do not necessarily converge in distribution for general input distributions. We proved this convergence in xIII for i.i.d. inputs. We now derive some additional properties of the entropy and mutual information under this input restriction. <p> Thus, it is a variable noise channel 3 . We show in Appendix 6 that variable noise Q-AWN channels with the same input and 2 Symmetric channels, defined in <ref> [13, p. 94] </ref>, are a more general class of memoryless channels; an output symmetric channel is a symmetric channel with a single output partition. 3 If the input alphabet of a Q-AWN channel is not symmetric or the input symbols have different amplitudes, then the distribution of Z = jY Xj <p> The result then follows from the definition of R j . 2 Corollary An independent input distribution achieves the maximum of R df . Lemma A9.2 For a fixed input distribution p (X J ), the J corresponding -output channels are all symmetric <ref> [13, page 94] </ref>. <p> Moreover, R df = lim R j (119) Proof From Lemma A9.2, the maximizing distribution for R df is independent. Moreover, from Lemma A9.2, each of the -output channels are symmetric, therefore from <ref> [13, page 144] </ref>, a uniform distribution for p (X j ) maximizes R j for all j, and therefore it maximizes R df . By Lemma A9.3, R j is monotonically increasing in j for i.i.d. uniform inputs.
Reference: [14] <author> T.M. Cover and J.A. Thomas, </author> <title> Elements of Information Theory. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: It is easily shown <ref> [14] </ref> that H (Y n ) = i=1 and n X H (Y i jX i ; Y i1 ; X i1 ): (20) The following lemma, proved in Appendix 3, allows the mutual information to be written in terms of n and n . <p> the inputs, b and d follow from properties of conditional expectation [12], and c is a consequence of Jensen's inequality. 25 The second inequality in (25) results from the fact that conditioning on an additional random variable, in this case the initial state S 0 , always reduces the entropy <ref> [14] </ref>. <p> Proof of Lemma 5.1 From <ref> [14] </ref>, H (Y n j n ) H (Y n ) log jYj and similarly H (Y n j i n ) H (Y n ) log jYj for any i.
Reference: [15] <author> P.R. Kumar and P. Varaiya, </author> <title> Stochastic Systems: Estimation, Identification, and Adaptive Control. </title> <publisher> Prentice-Hall, </publisher> <year> 1986. </year>
Reference: [16] <author> J.G. Proakis, </author> <title> Digital Communications. 2nd Ed. </title> <address> New York: </address> <publisher> McGraw-Hill, </publisher> <year> 1989. </year> <month> 34 </month>
Reference-contexts: Unfortunately, since block or convolutional decoding introduces delay, the post-decoding decisions cannot be fed back to the decision-feedback decoder to update the b j value. This is exactly the difficulty faced by an adaptive decision-feedback equalizer (DFE), where decoding decisions are used to update the DFE tap coefficients <ref> [16] </ref>. New methods to combine DFEs and coding have recently been proposed, and several of these methods can be used to obtain some coding gain in the estimate of x j fed back through our decision-feedback decoder.
Reference: [17] <author> M.V. Eyuboglu, </author> <title> "Detection of coded modulation signals on linear, severely distorted channels using decision-feedback noise prediction with interleaving," </title> <journal> IEEE Trans. Commun., </journal> <volume> Vol. COM-36, No. 4, </volume> <pages> pp. 401-409, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: In particular, the structure of our decision-feedback decoder already includes the interleaver/deinterleaver pair proposed by Eyuboglu for DFEs with coding <ref> [17] </ref>. In his method, this pair introduced a periodic delay in the received bits such that delayed reliable decisions can be used for feedback. Applying this idea to our system effectively combines the decision-feedback decoder, deinterleaver, and decoder.
Reference: [18] <author> J.C.S. Cheung and R. Steele, </author> <title> "Soft-decision feedback equalizer for continuous phase modulated signals in wideband mobile radio channels," </title> <journal> IEEE Trans. Commun., </journal> <volume> Vol. COM-42, No. 2/3/4, </volume> <pages> pp. 1628-1638, </pages> <month> Feb.-April </month> <year> 1994. </year>
Reference-contexts: Another approach to implement coding gain uses soft decisions on the received symbols to update n , then later corrects this initial n estimate if the decoded symbols differ from their initial estimates <ref> [18] </ref>. This method truncates the number of symbols affected by an incorrect decision, at a cost of increased complexity to recalculate and update the n values. Finally, decision-feedback decoding can be done in parallel, where each parallel path corresponds to a different estimate of the received symbol.
Reference: [19] <author> A. Duel-Hallen and C. Heegard, </author> <title> "Delayed decision-feedback sequence estimation," </title> <journal> IEEE Trans. Commun., </journal> <volume> Vol. COM-37, No. 5, </volume> <pages> pp. 428-436, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Finally, decision-feedback decoding can be done in parallel, where each parallel path corresponds to a different estimate of the received symbol. The number of parallel paths will grow exponentially in this case, however we may be able to apply some of the methods outlined in <ref> [19] </ref> and [20] to reduce the number of paths sustained through the trellis. VII Two-State Variable Noise Channel We now compute the capacity and cutoff rates of a two-state Q-AWN channel with variable SNR, Gaussian noise, and 4PSK modulation.
Reference: [20] <author> M.V. Eyuboglu and S.U.H Qureshi, </author> <title> "Reduced-state sequence estimation with set partitioning and decision feedback," </title> <journal> IEEE Trans. Commun., </journal> <volume> Vol. COM-36, No. 1, </volume> <pages> pp. 13-20, </pages> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: Finally, decision-feedback decoding can be done in parallel, where each parallel path corresponds to a different estimate of the received symbol. The number of parallel paths will grow exponentially in this case, however we may be able to apply some of the methods outlined in [19] and <ref> [20] </ref> to reduce the number of paths sustained through the trellis. VII Two-State Variable Noise Channel We now compute the capacity and cutoff rates of a two-state Q-AWN channel with variable SNR, Gaussian noise, and 4PSK modulation.

References-found: 20

