URL: http://www.ai.mit.edu/people/gina/papers/aaai.ps
Refering-URL: http://www.ai.mit.edu/people/gina/gina.html
Root-URL: 
Email: gina@ai.mit.edu  
Title: Detecting Spoken Corrections through Decision Tree Methods  
Author: Gina-Anne Levow 
Keyword: Content Areas: natural language processing, spoken language understanding, decision-tree learning  
Address: Room 769, 545 Technology Sq Cambridge, MA 02139  
Affiliation: MIT AI Laboratory  
Note: Submitted to AAAI-98 1  
Abstract: Miscommunication in speech recognition systems is unavoidable, but a detailed characterization of user corrections will enable speech systems to identify when a correction is taking place and to more accurately recognize the content of correction utterances. In this paper we exploit the acoustic adaptations made by users during error correction interactions with spoken language systems in the design of decision trees to identify corrections in contrast with original input. Analysis of more than 300 pairs of original and repeat correction utterances revealed significant increases in total utterance and pause durations for all correction types, and increases in pitch variability for corrections of misrecog-nition errors. We achieve accuracy rates of 75% in distinguishing corrections of misrecognitions from original inputs. Overall success rates using the trained decision tree classifiers for separating corrections from originals fall between 67-70%. Features based on duration and speaking rate measures proved most useful in the classification. The success at identifying corrections related to misrecognitions is particularly encouraging since misrecognition errors, which produce a recognition result, albeit an incorrect one, are the most difficult for current systems to detect in free-form voice interfaces, often requiring inference. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bear, J.; Dowding, J.; and Shriberg, E. </author> <year> 1992. </year> <title> Integrating multiple knowledge sources for detection and correction of repairs in human-computer dialog. </title> <booktitle> In Proceedings of the ACL, </booktitle> <pages> 56-63. </pages>
Reference-contexts: Two areas of related research that have been investigated are the identification of self-repairs and disfluencies, where the speaker self-interrupts to change an utterance in progress, and some preliminary efforts in the study of corrections in speech input. In analyzing and identifying self-repairs, <ref> (Bear, Dowding, & Shriberg 1992) </ref> and (Heeman & Allen 1994) found that the most effective methods relied on identifying shared textual regions between the reparan-dum and the repair.
Reference: <author> Breiman, L.; Friedman, J.; Olshen, R.; and Stone, C. </author> <year> 1984. </year> <title> Classification and Regression Trees. Belmont: </title> <publisher> Wadsworth. </publisher>
Reference: <author> Colton, D. </author> <year> 1995. </year> <title> Course manual for CSE 553 speech recognition laboratory. </title> <type> Technical Report CSLU-007-95, </type> <institution> Center for Spoken Language Understanding, Ore-gon Graduate Institute. </institution>
Reference-contexts: Duration The basic duration measure is total utterance duration. This value is obtained through a two-step procedure. First we perform an automatic forced alignment of the utterance to the verbatim transcription text using the OGI CSLU CSLUsh Toolkit <ref> (Colton 1995) </ref>. Then the alignment is inspected and, if necessary, adjusted by hand to correct for any errors, such as those caused by extraneous background noise or non-speech sounds. A typical alignment appears in Figure 1.
Reference: <author> Heeman, P., and Allen, J. </author> <year> 1994. </year> <title> Detecting and correcting speech repairs. </title> <booktitle> In Proceedings of the ACL, </booktitle> <pages> 295-302. </pages>
Reference-contexts: Two areas of related research that have been investigated are the identification of self-repairs and disfluencies, where the speaker self-interrupts to change an utterance in progress, and some preliminary efforts in the study of corrections in speech input. In analyzing and identifying self-repairs, (Bear, Dowding, & Shriberg 1992) and <ref> (Heeman & Allen 1994) </ref> found that the most effective methods relied on identifying shared textual regions between the reparan-dum and the repair.
Reference: <author> Nakatani, C., and Hirschberg, J. </author> <year> 1994. </year> <title> A corpus-based study of repair cues in spontaneous speech. </title> <journal> Journal of the Acoustic Society of America 95(3) </journal> <pages> 1603-1616. </pages>
Reference-contexts: However, these techniques are limited to those instances where a reliable recognition string is available; in general, that is not the case for most speech recognition systems currently available. Alternative approaches described in <ref> (Nakatani & Hirschberg 1994) </ref> and (Shriberg, Bates, & Stolcke 1997), have emphasized acoustic-prosodic cues, including duration, pitch, and amplitude as discriminating features.
Reference: <author> Ostendorf, M.; Byrne, B.; Bacchiani, M.; Finke, M.; Gunawardana, A.; Ross, K.; Roweis, S.; </author> <title> Talkin, </title> <publisher> E. </publisher>
Reference: <author> S. D.; Waibel, A.; Wheatley, B.; and Zeppenfeld, T. </author> <year> 1996. </year> <title> Modeling systematic variations in pronunciation via a language-dependent hidden speaking mode. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing. supplementary paper. </booktitle>
Reference: <author> Oviatt, S.; Levow, G.; MacEarchern, M.; and Kuhn, K. </author> <year> 1996. </year> <title> Modeling hyperarticulate speech during human-computer error resolution. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <volume> volume 2, </volume> <pages> 801-804. </pages>
Reference-contexts: Alternative approaches described in (Nakatani & Hirschberg 1994) and (Shriberg, Bates, & Stolcke 1997), have emphasized acoustic-prosodic cues, including duration, pitch, and amplitude as discriminating features. The few studies that have focussed on spoken corrections of computer misrecognitions, <ref> (Oviatt et al. 1996) </ref> and (Swerts & Ostendorf 1995), also found significant effects of duration, and in Oviatt et al., pause insertion and lengthening played a role. <p> SYSTEM SAID: Old message 49 From Randy Lewis. SYSTEM SAID: Subject Q2 Commissions Information. In total, there were 302 of these original-repeat pairs: 196 resulting from rejections, and 106 from misrecogni-tions. Following <ref> (Oviatt et al. 1996) </ref>, (Shriberg, Bates, & Stolcke 1997), and (Ostendorf et al. 1996), we coded a set of acoustic-prosodic features to describe the utterances. These features fall into four main groups: dura-tional, pause, pitch, and amplitude. Duration The basic duration measure is total utterance duration.
Reference: <author> Quinlan, J. </author> <year> 1992. </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Decision Tree Experiments The next step was to develop predictive classifiers of original vs repeat corrections and CME's vs CRE's informed by the descriptive analysis above. We chose to implement these classifiers with decision trees (using Quinlan's <ref> (Quinlan 1992) </ref> C4.5) trained on a subset of the original-repeat pair data. Decision trees have two features which make them desirable for this task.
Reference: <author> Secrest, B. G., and Doddington, G. R. </author> <title> An integrated pitch tracking algorithm for speech systems. </title> <booktitle> In ICASSP 1993. </booktitle>
Reference: <author> Shriberg, E.; Bates, R.; and Stolcke, A. </author> <year> 1997. </year> <title> A prosody-only decision-tree model for disfluency detection. </title> <booktitle> In Eurospeech '97. </booktitle>
Reference-contexts: However, these techniques are limited to those instances where a reliable recognition string is available; in general, that is not the case for most speech recognition systems currently available. Alternative approaches described in (Nakatani & Hirschberg 1994) and <ref> (Shriberg, Bates, & Stolcke 1997) </ref>, have emphasized acoustic-prosodic cues, including duration, pitch, and amplitude as discriminating features. <p> SYSTEM SAID: Old message 49 From Randy Lewis. SYSTEM SAID: Subject Q2 Commissions Information. In total, there were 302 of these original-repeat pairs: 196 resulting from rejections, and 106 from misrecogni-tions. Following (Oviatt et al. 1996), <ref> (Shriberg, Bates, & Stolcke 1997) </ref>, and (Ostendorf et al. 1996), we coded a set of acoustic-prosodic features to describe the utterances. These features fall into four main groups: dura-tional, pause, pitch, and amplitude. Duration The basic duration measure is total utterance duration. This value is obtained through a two-step procedure.
Reference: <author> Swerts, M., and Ostendorf, M. </author> <year> 1995. </year> <title> Discourse prosody in human-machine interactions. </title> <booktitle> In Proceedings of the ECSA Tutorial and Research Workshop on Spoken Dialog Systems Theories and Applications. </booktitle>
Reference-contexts: Alternative approaches described in (Nakatani & Hirschberg 1994) and (Shriberg, Bates, & Stolcke 1997), have emphasized acoustic-prosodic cues, including duration, pitch, and amplitude as discriminating features. The few studies that have focussed on spoken corrections of computer misrecognitions, (Oviatt et al. 1996) and <ref> (Swerts & Ostendorf 1995) </ref>, also found significant effects of duration, and in Oviatt et al., pause insertion and lengthening played a role.
Reference: <author> Yankelovich, N.; Levow, G.; and Marx, M. </author> <year> 1995. </year> <title> Designing SpeechActs: Issues in speech user interfaces. </title> <booktitle> In CHI '95 Conference on Human Factors in Computing Systems. </booktitle>
Reference-contexts: System, Subjects, and Errors The utterances used in the design and analysis of the decision tree classifiers were drawn from approximately 60 hours of user interactions in a field trial of the Sun Microsystems SpeechActs system <ref> (Yankelovich, Levow, & Marx 1995) </ref>. SpeechActs provides a voice-only interface to common desktop applications such as e-mail, calendar, weather reports, stock quotes, and time and currency conversions. The system allows conversational interactions with speech recognition provided by BBN's HARK recognizer and speech synthesis through Centigram's TruVoice system. <p> The remainder of the errors were due to system crashes or parser errors. The probability of experiencing a recognition failure after a correct recognition was 16%, but 1 Designing SpeechActs: Issues in Speech User Interface Design <ref> (Yankelovich, Levow, & Marx 1995) </ref> p. 2 immediately after an incorrect recognition it was 44%, 2.75 times greater. This increase in error likelihood suggests a change in speaking style which diverges from the recognizer's model.
References-found: 13

