URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-357.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: bobick pinhanez@media.mit.edu  
Title: Divide and Conquer: Using Approximate World Models to Control View-Based Algorithms  
Author: Aaron F. Bobick and Claudio S. Pinhanez 
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 357 Submitted to CVPR'96 October 1995 Abstract Most view-based vision algorithms are based on strong assumptions about the disposition of the objects in the image. To safely apply those algorithms in real world image sequences, we are proposing that a vision system should be divided into two components. The first component contains an approximate world model of the scene | a low accuracy, coarse description of the objects and actions in the world. Approximate world models are constructed and updated by simple vision routines and by the use of contextual information. The second component employs view-based algorithms to perform required perceptual tasks; the selection and control of the view-based methods are determined by the information provided by the approximate world model. We demonstrate the approximate world model approach in a project to control cameras in a TV studio. In our Intelligent Studio automatic cameras respond to verbal requests for shots from the TV director. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. F. Allen, </author> <title> "Towards a General Theory of Action and Time," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 23, </volume> <pages> pp. 123-154, </pages> <year> 1984. </year>
Reference-contexts: Thus, a non-timed version of the script would, theoretically, give most of the information needed. Some research has been done in recognizing human movements <ref> [24, 18, 1, 16, 8] </ref> and in action recognition [10], though most methods were developed for situations much more constrained than those found in normal TV studios.
Reference: [2] <author> J. Y. Aloimonos, </author> <title> "Purposive and Qualitative Active Vision," </title> <booktitle> Proc. of Image Understanding Workshop, </booktitle> <address> Pittsburgh, Pennsylvania, </address> <pages> pp. 816-828, </pages> <month> September </month> <year> 1990. </year>
Reference: [3] <author> D. J. Beymer, </author> <title> "Face Recognition Under Varying Pose," </title> <booktitle> Proc. of CVPR'94, </booktitle> <address> Seattle, Washington, </address> <month> June 21-23, </month> <pages> pp. 756-761, </pages> <year> 1994. </year>
Reference: [4] <author> A. F. Bobick and R. C. Bolles, </author> <title> "The Representation Space Paradigm of Concurrent Evolving Object Descriptions," </title> <journal> IEEE PAMI, </journal> <volume> vol. 14(2), </volume> <pages> pp. 146-156, </pages> <month> January </month> <year> 1992. </year>
Reference: [5] <author> A. F. Bobick and C. Pinhanez, </author> <title> "Using Approximate Models as Source of Contextual Information for Vision Processing," </title> <booktitle> in Proc. of the ICCV'95 Workshop on Context-Based Vision, </booktitle> <address> Cambridge, Massachusetts, </address> <pages> pp. 13-21, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: If the accuracy requirements are relaxed, as it is in the case of approximate world models, context can be used to predict possible positions and attitudes of objects as we will show in a later section (see also <ref> [5] </ref>). Previous Work Coarse and/or hierarchical descriptions have been used before in computer vision ([4, 11]). Particularly, Bobick and Bolles ([4]) employed a multi-level representational system where different queries were answered by different representations of the same object.
Reference: [6] <author> B. A. Draper, R. T. Collins, J. Brolio, J. Griffith, A. R. Hanson, E. M. Riseman, </author> <title> "Tools and Experiments in the Knowledge-Directed Interpretation of Road Scenes", </title> <booktitle> in Proc. of the DARPA Image Understanding Workshop, </booktitle> <address> Los Angeles, </address> <publisher> Califor-nia, </publisher> <pages> pp. 178-193, </pages> <month> February </month> <year> 1987. </year>
Reference: [7] <author> C. Fermuller, </author> <title> "Global 3-D Motion Estimation," </title> <booktitle> Proc. of CVPR'93, </booktitle> <address> New York City, New York, </address> <month> June 15-17, </month> <pages> pp. 415-421, </pages> <year> 1993. </year>
Reference: [8] <author> D. Israel, J. Perry, and S. Tutiya, </author> <title> "Actions and Movements," </title> <booktitle> 12th IJCAI, </booktitle> <address> Sydney, Australia, </address> <month> August 24-30, </month> <pages> pp. 1060-1065, </pages> <year> 1991. </year>
Reference-contexts: Thus, a non-timed version of the script would, theoretically, give most of the information needed. Some research has been done in recognizing human movements <ref> [24, 18, 1, 16, 8] </ref> and in action recognition [10], though most methods were developed for situations much more constrained than those found in normal TV studios.
Reference: [9] <author> R. C. Jain and T. O. Binford, </author> <title> "Ignorance, </title> <booktitle> Myopia, and Naivete in Computer Vision Systems," CVGIP: Image Understanding, </booktitle> <volume> vol. 53(1), </volume> <pages> pp. 112-117, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The second component contains view-based algorithms with enough accuracy to satisfy the task requirements, though sensitive to particular situations in the real world. In the long run, our hope is to be able to cope with one of the recurring criticisms (e.g. <ref> [9] </ref>) of much of computer vision, that many of the developed techniques are brittle, functioning well only if some set of restrictive assumptions about the situation are true.
Reference: [10] <author> Y. Kuniyoshi and H. Inoue, </author> <title> "Qualitative Recognition of Ongoing Human Action Sequences," </title> <booktitle> Proc. of IJCAI-93, </booktitle> <pages> pp. 1600-1609, </pages> <year> 1993. </year>
Reference-contexts: Thus, a non-timed version of the script would, theoretically, give most of the information needed. Some research has been done in recognizing human movements [24, 18, 1, 16, 8] and in action recognition <ref> [10] </ref>, though most methods were developed for situations much more constrained than those found in normal TV studios. However, we believe that the use of approximate models can significantly facilitate the provision of the contextual information which is essential for action recognition.
Reference: [11] <author> D. Marr and H. K. Nishihara, </author> <title> "Representation and Recognition of the Spatial Organization of Three-Dimensional Shapes," </title> <journal> in Proc. R. Soc. Lond. B, </journal> <volume> vol. 200, </volume> <pages> pp. 269-294, </pages> <year> 1978. </year>
Reference: [12] <author> B. Moghaddam and A. Pentland, </author> <title> "Face Recognition using View-Based and Modular Eigenspaces," Automatic Systems for the Identification and Inspection of Humans, </title> <booktitle> SPIE vol. </booktitle> <volume> 2277, </volume> <month> July </month> <year> 1994. </year>
Reference: [13] <author> D. Newtson, G. Engquist, and J. Bois, </author> <title> "The Objective Basis of Behavior Units," </title> <journal> Journal of Personality and Social Psychology, </journal> <volume> vol. 35(12), </volume> <pages> pp. 847-862, </pages> <month> December </month> <year> 1977. </year>
Reference: [14] <author> C. Pinhanez and A. F. Bobick, </author> <title> "Intelligent Studios: Using Computer Vision to Control TV Cameras," </title> <booktitle> Proc. of IJCAI'95 Workshop on Entertainment and AI/Alife, </booktitle> <address> Montreal, Canada, </address> <pages> pp. 69-76, </pages> <month> August </month> <year> 1995. </year>
Reference: [15] <author> C. Pinhanez and A. F. Bobick, </author> <title> "Scripts in Machine Understanding of Image Sequences," </title> <booktitle> to appear in Proc. of AAAI Fall Symposium on Computational Models for Integrating Language and Vision, </booktitle> <address> Cambridge, Massachusetts, </address> <month> Novem-ber </month> <year> 1995. </year>
Reference-contexts: For example, if it is known from the script that a bowl is being used by a chef, the system can position the 3-D model of the bowl near the position of the hands (for more details, see <ref> [15] </ref>). The approximate world manager also updates the corresponding states of the symbolic representation of the bowl. Information about the size and position of objects, available from the approximate world model, also enables our vision system to deal more easily with adaptive parameters.
Reference: [16] <author> R. Polana and R. Nelson, </author> <title> "Low Level Recognition of Human Motion," </title> <booktitle> Proc. of IEEE Workshop on Motion of Non-Rigid and Articulated Objects, </booktitle> <address> Austin, Texas, </address> <month> November 11-12, </month> <pages> pp. 77-82, </pages> <year> 1994. </year>
Reference-contexts: Thus, a non-timed version of the script would, theoretically, give most of the information needed. Some research has been done in recognizing human movements <ref> [24, 18, 1, 16, 8] </ref> and in action recognition [10], though most methods were developed for situations much more constrained than those found in normal TV studios.
Reference: [17] <author> P. N. Prokopowicz, M. J. Swain, and R. E. Kahn, </author> <title> "Task and Environment-Sensitive Tracking," </title> <booktitle> Proc. of the Workshop on Visual Behaviors, </booktitle> <address> Seattle, Washington, </address> <month> June 19. </month> <pages> pp. 73-78, </pages> <year> 1994. </year>
Reference-contexts: Another interesting direction is to design a language which describes the pre-conditions and the outputs of vision routines in a domain-independent way, enabling the easy incorporation of new routines to the system and the re-use of vision routines in the case of completely new domains. Prokopowicz et. al. <ref> [17] </ref> examines some typical characteristics of such a language. However, their work lacks representations for the objects of the world that allows the derivation of view-dependent properties. We believe that our approximate world models provide a more adequate framework to supply the essential view-dependent in formation. 7
Reference: [18] <author> K. Rohr, </author> <title> "Towards Model-Based Recognition of Human Movements in Image Sequences," </title> <booktitle> CGVIP: Image Understanding, </booktitle> <volume> vol. 59(1), </volume> <pages> pp. 94-15, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: In our gate monitoring example, it seems plausible to construct a vision system that monitors the flow of people, and determines the position and attitude of them with relatively low accuracy (using techniques as those described in <ref> [18, 20] </ref>). And, as computer vision progresses, more domains are likely to be suitable for the computation of low accuracy representations. These coarse descriptions of the main elements of a scene are called approximate models. <p> Thus, a non-timed version of the script would, theoretically, give most of the information needed. Some research has been done in recognizing human movements <ref> [24, 18, 1, 16, 8] </ref> and in action recognition [10], though most methods were developed for situations much more constrained than those found in normal TV studios.
Reference: [19] <author> T. M. Strat and M. A. Fischler, </author> <title> "Context-Based Vision: Recognizing Objects Using Information from Both 2-D and 3-D Imagery," </title> <journal> IEEE PAMI, </journal> <volume> vol. 13(10), </volume> <pages> pp. 1050-1065, </pages> <month> October </month> <year> 1991. </year>
Reference: [20] <author> G. D. Sullivan, A. D. Worrall, and J. M. Fer-ryman, </author> <title> "Visual Object Recognition Using De-formable Models of Vehicles," </title> <booktitle> in Proc. of the ICCV'95 Workshop on Context-Based Vision, </booktitle> <address> Cambridge, Massachusetts, </address> <pages> pp. 75-86, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: In our gate monitoring example, it seems plausible to construct a vision system that monitors the flow of people, and determines the position and attitude of them with relatively low accuracy (using techniques as those described in <ref> [18, 20] </ref>). And, as computer vision progresses, more domains are likely to be suitable for the computation of low accuracy representations. These coarse descriptions of the main elements of a scene are called approximate models.
Reference: [21] <author> M. J. Tarr and M. J. Black, </author> <title> "A Computational and Evolutionary Perspective of the Role of Representation in Vision," CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> vol. 60(1), </volume> <pages> pp. 65-73, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Reconstructionist vs. Purposive Vision It is interesting to situate our scheme in the ongoing debate about reconstructionist vs. purposive vision (see <ref> [21] </ref> and the replies in the same issue). Our proposal falls between the strict reconstructionist and purely purposive strategies. We are arguing that reconstruction should exist at the approximate level, guiding the purposive vision routines of the view-based level.
Reference: [22] <author> R. </author> <title> Thibadeau, </title> <journal> "Artificial Perception of Actions," Cognitive Science, </journal> <volume> vol. 10, </volume> <pages> pp. 117-149, </pages> <year> 1986. </year>
Reference: [23] <author> J. K. Tsotsos, </author> <title> "Knowledge Organization and its Role in Representation and Interpretation of Time-Varying Data: The ALVEN System," </title> <journal> Computational Intelligence, </journal> <volume> vol. 1, </volume> <pages> pp. 16-32, </pages> <year> 1985. </year>
Reference: [24] <author> J. K. Tsotsos, J. Mylopoulos, H. D. Covvey, and S. W. Zucker, </author> <title> "A Framework for Visual Motion Understanding," </title> <journal> IEEE-PAMI, </journal> <volume> vol. 2(6), </volume> <pages> pp. 563-573, </pages> <month> November </month> <year> 1980. </year>
Reference-contexts: Thus, a non-timed version of the script would, theoretically, give most of the information needed. Some research has been done in recognizing human movements <ref> [24, 18, 1, 16, 8] </ref> and in action recognition [10], though most methods were developed for situations much more constrained than those found in normal TV studios.
Reference: [25] <author> H. Zettl, </author> <title> Television Production Handbook, 4th Edition, </title> <publisher> Wadsworth Publishing, </publisher> <address> Belmont, Cali-fornia, </address> <year> 1984. </year> <month> 8 </month>
Reference-contexts: Framing is more difficult than object localization and requires more information. For instance, a call for a close-up of a subject demands not only the information of the subject head's position and size, but also the subject's direction of sight and the position of the eyes (see <ref> [25] </ref>, pp. 111-122, for simple rules of framing). Framing also requires knowledge about the current actions; we illustrate this in a later example. The basic architecture of a SmartCam is shown in fig. 2.
References-found: 25

