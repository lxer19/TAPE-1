URL: http://L2R.cs.uiuc.edu/~danr/Papers/linearJ.ps.gz
Refering-URL: http://L2R.cs.uiuc.edu/~danr/publications.html
Root-URL: http://www.cs.uiuc.edu
Title: Linearizable Read/Write Objects  
Author: Marios Mavronicolas Dan Roth 
Date: June 1992  
Address: Cambridge, MA 02138.  
Affiliation: Aiken Computation Laboratory, Harvard University,  
Abstract: fl A preliminary version of this paper appeared in the Proceedings of the 29th annual Allerton Conference on Communication, Control and Computing (Allerton, October 1991), pp. 683-692, under the title "Sequential Consistency and Linearizability: Read/Write Objects". y Supported by ONR contract N00014-91-J-1981. E-mail address: mavronic@das.harvard.edu z Supported by NSF grant CCR-89-02500. E-mail address: danr@das.harvard.edu 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Afek, H. Attiya, D. Dolev, E. Gafni, M. Merritt and N. Shavit, </author> <title> "Atomic Snapshots of Shared Memory," </title> <booktitle> in Proceedings of the 9th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 1-14, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: and message-driven? What is the tightest coefficient of d in jRj + jW j for sequentially consistent or linearizable implementations of read/write objects in this case? Also, it will be very interesting to obtain bounds on the worst-case response times of implementing other memory objects like, e.g., atomic snapshots (cf. <ref> [1] </ref>) under sequential consistency and linearizability.
Reference: [2] <author> H. Attiya, </author> <title> "Implementing FIFO Queues and Stacks," </title> <booktitle> in Proceedings of the 5th Workshop on Distributed Algorithms, </booktitle> <address> Delfi, Greece, </address> <year> 1991. </year>
Reference-contexts: This paper continues the study of the cost of implementing memory objects, under various correctness conditions and timing assumptions, for shared-memory multiprocessor systems, initiated in <ref> [3, 2, 6] </ref>. Although our model ignores several important practical issues, like, e.g., limitations on local memory size, clock drift and "hot spots", we believe that our algorithms can be adapted to work in more realistic systems. <p> Acknowledgments: We would like to thank Hagit Attiya for making early versions of [3] and <ref> [2] </ref> available to us and for helpful discussions, and Martha Kosa and Yishay Mansour for their comments on earlier drafts of our paper. We owe special thanks to Harry Lewis for conjecturing Theorem 3.1. 28
Reference: [3] <author> H. Attiya and J. Welch, </author> <title> "Sequential Consistency versus Linearizability," </title> <booktitle> in Proceedings of the 3rd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 304-315, </pages> <year> 1991. </year>
Reference-contexts: In this paper, we continue this study and present upper bounds on the same cost for new, efficient linearizable implementations in order to further enlighten the advantages of linearizability over other, seemingly "cheaper", correctness conditions, such as sequential consistency. We follow <ref> [3] </ref> and consider a model consisting of a collection of application programs running concurrently and communicating through virtual shared memory, which consists of a collection of read/write objects. <p> A read operation simply returns after time fid, while a write operation returns after time (1 fi)d. Our implementations naturally generalize those in Theorems 3.2 and 3.3 of <ref> [3] </ref>, which are the special cases where fi = 0 and fi = 1, respectively. <p> Each computation step is specified by comp (i), for some i 2 [n]. In this computation 4 Our definitions closely follow those in <ref> [3] </ref>. 7 step, the process p i , based on its local state, changes its local state, performs some set S of send actions and possibly interacts with P i by a response event response (i). <p> This paper continues the study of the cost of implementing memory objects, under various correctness conditions and timing assumptions, for shared-memory multiprocessor systems, initiated in <ref> [3, 2, 6] </ref>. Although our model ignores several important practical issues, like, e.g., limitations on local memory size, clock drift and "hot spots", we believe that our algorithms can be adapted to work in more realistic systems. <p> Most obviously, it would be interesting to see if our bounds on worst-case response times in the imperfect clocks model can be further improved. Alternatively, it might be possible to improve the lower bounds of u=4 and u=2 for read and write operations, respectively, shown in <ref> [3] </ref>. Some steps in this direction have already been taken in [9]. <p> Acknowledgments: We would like to thank Hagit Attiya for making early versions of <ref> [3] </ref> and [2] available to us and for helpful discussions, and Martha Kosa and Yishay Mansour for their comments on earlier drafts of our paper. We owe special thanks to Harry Lewis for conjecturing Theorem 3.1. 28
Reference: [4] <author> M. Herlihy and J. Wing, </author> <title> "Linearizability: A Correctness Condition for Concurrent Objects," </title> <journal> in ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol 12, No. 3, </volume> <month> July </month> <year> 1990, </year> <pages> pp. 463-492. </pages>
Reference-contexts: If, in addition, this order is required to respect the order of non-overlapping operations at processes, the consistency mechanism is said to guarantee linearizability (cf. <ref> [4] </ref>); otherwise, it is said to guarantee sequential consistency (cf. [5]). Clearly, linearizability implies sequential consistency. It has been argued quite convincingly ([4]) that linearizability is the correctness condition that best guarantees "acceptable" concurrent behavior; indeed, linearizability enjoys a number of nice properties, like, e.g., locality 1 , and this has <p> Each object X may attain values from a domain, a set V of values, which includes a special "undefined" value, ?; a total order &lt; V , known to the processes, is defined on V. Each object X has a serial specification (cf. <ref> [4] </ref>) which defines its behavior in the absence of concurrency and failures. Formally, it defines: * A set of operations on X , Op (X ), which are ordered pair of call and response events. <p> Definition 2.2 A timed execution ff is linearizable (cf. <ref> [4] </ref>) if there exists an admissible sequence of operations t which respects ff and, in addition, it is ff-linearizable: that is, for any operations op 1 and op 2 in ff, whenever the response for operation op 1 precedes the call for operation op 2 in ops (ff), then op 1
Reference: [5] <author> L. Lamport, </author> <title> "How to Make a Multiprocessor Computer that Correctly Executes Multipro-cess Programs," </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-28, </volume> <month> No.9 (September </month> <year> 1979), </year> <pages> pp. 690-691. </pages>
Reference-contexts: If, in addition, this order is required to respect the order of non-overlapping operations at processes, the consistency mechanism is said to guarantee linearizability (cf. [4]); otherwise, it is said to guarantee sequential consistency (cf. <ref> [5] </ref>). Clearly, linearizability implies sequential consistency. <p> Formally, we define: Definition 2.1 A timed execution ff is sequentially consistent (cf. <ref> [5] </ref>) if there exists an admissible sequence of operations t which respects ff: that is, for each process p i , the restriction of ops (ff) to operations of p i is equal to the restriction of t to operations of p i .
Reference: [6] <author> R. Lipton and J. Sandberg, </author> <title> "A Scalable Shared Memory," </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: The dependence of our upper bound on d is minimal: the sum of the worst-case response times for read and write operations contains only a single additive term of d which, by <ref> [6] </ref>, is inherent. Furthermore, although the analysis of our implementation is technically challenging, the implementation itself is fairly simple, it does not use complicated control mechanisms, and it is message-economical. <p> is at most u. 6 Our result for the imperfect clocks model, in particular, the upper bound of d + O (u) on the sum of the worst-case response times for read and write operations in a linearizable implementation, along with the lower bound of d on this sum (cf. <ref> [6] </ref>), may suggest that sequential consistency and linearizability are actually "closer" than thought before. The rest of the paper is organized as follows. Section 2 presents the formal model, defines the memory objects, the correctness conditions, and the costs for their distributed implementations. <p> This paper continues the study of the cost of implementing memory objects, under various correctness conditions and timing assumptions, for shared-memory multiprocessor systems, initiated in <ref> [3, 2, 6] </ref>. Although our model ignores several important practical issues, like, e.g., limitations on local memory size, clock drift and "hot spots", we believe that our algorithms can be adapted to work in more realistic systems.
Reference: [7] <author> J. Lundelius and N. Lynch, </author> <title> "An Upper and Lower Bound for Clock Synchronization," </title> <journal> Information and Control, </journal> <volume> Vol. 62, </volume> <pages> Nos. </pages> <month> 2/3 (August/September </month> <year> 1984), </year> <pages> pp. 190-204. </pages>
Reference-contexts: We remark that, by the results of Lundelius and Lynch in <ref> [7] </ref>, an accuracy of (1 1 n )u is achievable in the imperfect clocks model. This accuracy is slightly better than u, but we chose to present and use the slightly weaker one since it can be achieved by a much simpler algorithm than the one in [7], which might also <p> and Lynch in <ref> [7] </ref>, an accuracy of (1 1 n )u is achievable in the imperfect clocks model. This accuracy is slightly better than u, but we chose to present and use the slightly weaker one since it can be achieved by a much simpler algorithm than the one in [7], which might also be of independent interest. Furthermore, the better accuracy achieved in [7] does not seem to considerably improve our subsequent results, if it improves them at all. <p> This accuracy is slightly better than u, but we chose to present and use the slightly weaker one since it can be achieved by a much simpler algorithm than the one in <ref> [7] </ref>, which might also be of independent interest. Furthermore, the better accuracy achieved in [7] does not seem to considerably improve our subsequent results, if it improves them at all. We conclude this subsection with a simple observation encompassing the "common knowledge" acquired by the processes as a result of running Synch.
Reference: [8] <author> M. Mavronicolas and D. Roth, </author> <title> "Sequential Consistency and Linearizability: Read/Write Objects," </title> <booktitle> in Proceedings of the 29th annual Allerton Conference on Communication, Control and Computing, Allerton, </booktitle> <month> October </month> <year> 1991, </year> <pages> pp. 683-692. </pages>
Reference: [9] <author> M. Mavronicolas and D. Roth, </author> <title> "Efficient, Strongly Consistent Implementations of Shared Memory," </title> <type> Technical Report TR-05-92, </type> <institution> Aiken Computation Laboratory, Harvard University, </institution> <year> 1992. </year>
Reference-contexts: Alternatively, it might be possible to improve the lower bounds of u=4 and u=2 for read and write operations, respectively, shown in [3]. Some steps in this direction have already been taken in <ref> [9] </ref>. <p> Our new major results are reported in <ref> [9] </ref>.
Reference: [10] <author> C. Papadimitriou, </author> <title> "The Serializability of Concurrent Database Updates," </title> <journal> Journal of the ACM, </journal> <volume> Vol. 26, No. </volume> <month> 4 (October </month> <year> 1979), </year> <pages> pp. 631-653. 29 </pages>
Reference-contexts: An MCS is a sequentially consistent implementation of X if every timed execution of the MCS is sequentially consistent; similarly, an MCS is a linearizable implementation of X if every timed execution of the MCS is linearizable. 5 Linearizability may be viewed as a special case of strict serializability (cf. <ref> [10] </ref>), a basic correctness condition for concurrent computations on databases, where transactions are restricted to consist of a single operation on a single object. 10 2.4 Cost Measures In general, the efficiency of an implementation of X is measured by the worst-case response time for any operation on an object X
References-found: 10

