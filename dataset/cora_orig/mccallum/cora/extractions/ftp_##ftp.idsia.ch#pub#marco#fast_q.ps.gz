URL: ftp://ftp.idsia.ch/pub/marco/fast_q.ps.gz
Refering-URL: http://www.idsia.ch/journals.html
Root-URL: 
Email: marco@idsia.ch  juergen@idsia.ch  
Title: Fast Online Q()  
Author: MARCO WIERING J URGEN SCHMIDHUBER Editor: Leslie Pack Kaelbling 
Keyword: Reinforcement learning, Q-learning, TD(), online Q(), lazy learning  
Address: Corso Elvezia 36, 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Note: Machine Learning, 1-11 (1998, in press) c 1998, in press Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: Q()-learning uses TD()-methods to accelerate Q-learning. The update complexity of previous online Q() implementations based on lookup-tables is bounded by the size of the state/action space. Our faster algorithm's update complexity is bounded by the number of actions. The method is based on the observation that Q-value updates may be postponed until they are needed. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Albus, J. S. </author> <year> (1975). </year> <title> A new approach to manipulator control: The cerebellar model articulation controller (CMAC). Dynamic Systems, </title> <booktitle> Measurement and Control, </booktitle> <volume> 97 </volume> <pages> 220-227. </pages>
Reference: <author> Atkeson, C. G., Schaal, S., & Moore, A. W. </author> <year> (1997). </year> <title> Locally weighted learning. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11 </volume> <pages> 11-73. </pages>
Reference-contexts: Our Q () variant is truly online and more efficient than others because its update complexity does not depend on the number of states. The method can also be used for speeding up tabular TD (). It uses "lazy learning" <ref> (introduced in memory-based learning, e.g., Atkeson, Moore and Schaal 1997) </ref> to postpone updates until they are needed. Outline. Section 2 reviews Q () and describes Peng and William's Q ()-algorithm (PW). Section 3 presents our more efficient algorithm. Section 4 concludes. 2.
Reference: <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, SMC-13:834-846. </journal>
Reference: <author> Bertsekas, D. P. & Tsitsiklis, J. N. </author> <year> (1996). </year> <title> Neuro-dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA. </address>
Reference: <author> Caironi, P. V. C. & Dorigo, M. </author> <year> (1994). </year> <title> Training Q-agents. </title> <type> Technical Report IRIDIA-94-14, </type> <institution> Universite Libre de Bruxelles. </institution>
Reference: <author> Cichosz, P. </author> <year> (1995). </year> <title> Truncating temporal differences: On the efficient implementation of TD() for reinforcement learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 287-318. </pages>
Reference: <author> Fritzke, B. </author> <year> (1994). </year> <title> Supervised learning with growing cell structures. </title> <editor> In Cowan, J., Tesauro, G., & Alspector, J., (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 255-262. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Fast Q (), however, can also improve matters in case of "local" function ap-proximators (FAs) consisting of seperate building blocks, such as Kohonen networks (Kohonen, 1988), CMACs (Albus, 1975; Sutton, 1996), locally weighted learning (Atkeson, Moore and Schaal 1996), and neural gas <ref> (Fritzke, 1994) </ref>. Such FAs work as follows: there are jSj possible state space "features". There are Q-values for all possible feature/action pairs, just like there are Q-values of state/action pairs in the case of tabular representation. Q-values of I jSj features are combined to evaluate an input (query).
Reference: <author> Koenig, S. & Simmons, R. G. </author> <year> (1996). </year> <title> The effect of representation and knowledge on goal-directed exploration with reinforcement learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 228-250. </pages>
Reference-contexts: Online RL's advantage can be huge. For instance, online methods that punish actions (to prevent repetitive selection of identical actions) can discover certain environments' goal states in polynomial time <ref> (Koenig and Simmons, 1996) </ref>, while o*ine RL requires exponential search time (Whitehead, 1992). Previous Q () implementations. To speed up Q-learning, Watkins (1989) suggested combining it with TD () learning. His approach resets eligibility traces once exploratory actions are executed, while Peng and Williams' variant (1996) does not require this.
Reference: <author> Kohonen, T. </author> <year> (1988). </year> <title> Self-Organization and Associative Memory. </title> <publisher> Springer, second edition. </publisher>
Reference-contexts: His approach resets eligibility traces once exploratory actions are executed, while Peng and Williams' variant (1996) does not require this. Typical online Q () implementations based on lookup-tables or other local approximators such as CMACs (Albus 1975; Sutton, 1996) or self-organizing maps <ref> (Kohonen, 1988) </ref>, however, are unnecessarily time-consuming. Their update complexity depends on the values of and discount factor fl, and is proportional to the number of SAPs (state/action pairs) which have occurred. <p> Extension to function approximators. Tabular representations do not allow for generalizing from previous experiences, which is necessary in case of large state spaces. Fast Q (), however, can also improve matters in case of "local" function ap-proximators (FAs) consisting of seperate building blocks, such as Kohonen networks <ref> (Kohonen, 1988) </ref>, CMACs (Albus, 1975; Sutton, 1996), locally weighted learning (Atkeson, Moore and Schaal 1996), and neural gas (Fritzke, 1994). Such FAs work as follows: there are jSj possible state space "features".
Reference: <author> Lin, L-J. </author> <year> (1993). </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh. </institution>
Reference: <author> Peng, J. & Williams, R. </author> <year> (1996). </year> <title> Incremental multi-step Q-learning. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 283-290. </pages>
Reference: <author> Rummery, G. & Niranjan, M. </author> <year> (1994). </year> <title> On-line Q-learning using connectionist sytems. </title> <type> Technical Report CUED/F-INFENG-TR 166, </type> <institution> Cambridge University, UK. </institution> <note> FAST ONLINE Q() 11 Singh, </note> <author> S. & Sutton, R. </author> <year> (1996). </year> <title> Reinforcement learning with replacing eligibility traces. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 123-158. </pages>
Reference-contexts: Without explicit trial boundaries, o*ine RL does not make sense at all. But even where applicable, o*ine RL tends to get outperformed by online RL, which uses experience earlier and therefore more efficiently <ref> (Rummery and Niranjan, 1994) </ref>. Online RL's advantage can be huge. For instance, online methods that punish actions (to prevent repetitive selection of identical actions) can discover certain environments' goal states in polynomial time (Koenig and Simmons, 1996), while o*ine RL requires exponential search time (Whitehead, 1992). Previous Q () implementations. <p> The sarsa algorithm <ref> (Rummery and Niranjan, 1994) </ref> replaces the right hand side in lines (1) and (2) by (r t + flQ (s t+1 ; a t+1 ) Q (s t ; a t )). 2.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference-contexts: Q ()-learning. Q () uses TD ()-methods <ref> (Sutton, 1988) </ref> to accelerate Q-learning. First note that Q-learning's update at time t + 1 may change V (s t+1 ) in the def inition of e 0 t .
Reference: <author> Sutton, R. S. </author> <year> (1996). </year> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <editor> In Touretzky, D.S., Mozer, M.C, & Hasselmo, M. E., (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 1038-1045. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference-contexts: The sarsa algorithm (Rummery and Niranjan, 1994) replaces the right hand side in lines (1) and (2) by (r t + flQ (s t+1 ; a t+1 ) Q (s t ; a t )). 2. For replacing eligibility traces <ref> (Singh and Sutton, 1996) </ref>, step 5 should be: 8a : l (s t ; a) 0; l (s t ; a t ) 1. 3. <p> ) 7) Q (s t ; a t ) Q (s t ; a t ) + ff k (s t ; a t )e 0 8) l 0 (s t ; a t ) l 0 (s t ; a t ) + 1= t For replacing eligibility traces <ref> (Singh and Sutton, 1996) </ref>, step 8 should be changed as follows: 8a : l 0 (s t ; a) 0; l 0 (s t ; a t ) 1= t . Machine precision problem and solution.
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <editor> In Lippman, D. S., Moody, J. E., & Touretzky, D. S., (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 259-266. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Thrun, S. </author> <year> (1992). </year> <title> Efficient exploration in reinforcement learning. </title> <type> Technical Report CMU-CS-92-102, </type> <institution> Carnegie-Mellon University. </institution>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, England. </address>
Reference-contexts: Q-learning. Given (s t ; a t ; r t ; s t+1 ), standard one-step Q-learning updates just a single Q-value Q (s t ; a t ) as follows <ref> (Watkins, 1989) </ref>: Q (s t ; a t ) Q (s t ; a t ) + ff k (s t ; a t )e 0 FAST ONLINE Q () 3 Here the temporal difference or TD (0)-error e 0 t is given by: e 0 where the value function V
Reference: <author> Watkins, C. J. C. H. & Dayan, P. </author> <year> (1992). </year> <title> Technical note: </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292. </pages>
Reference: <author> Whitehead, S. </author> <year> (1992). </year> <title> Reinforcement Learning for the adaptive control of perception and action. </title> <type> PhD thesis, </type> <institution> University of Rochester. </institution>
Reference-contexts: Online RL's advantage can be huge. For instance, online methods that punish actions (to prevent repetitive selection of identical actions) can discover certain environments' goal states in polynomial time (Koenig and Simmons, 1996), while o*ine RL requires exponential search time <ref> (Whitehead, 1992) </ref>. Previous Q () implementations. To speed up Q-learning, Watkins (1989) suggested combining it with TD () learning. His approach resets eligibility traces once exploratory actions are executed, while Peng and Williams' variant (1996) does not require this.
Reference: <author> Wiering, M. A. & Schmidhuber, J. </author> <year> (1998). </year> <title> Speeding up Q()-learning. </title> <editor> In Nedellec, C. & Rou-veirol, C., editors, </editor> <booktitle> Machine Learning: Proceedings of the Tenth European Conference. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
References-found: 20

