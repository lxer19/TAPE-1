URL: http://polaris.cs.uiuc.edu/reports/1460.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Email: Email: fkuba, maiki, cdpg@csrd.uiuc.edu  
Title: Practical Parallelization of Molecular Dynamics on Shared and Distributed Memory Machines some manual exchanges are
Author: Masayuki Kuba Marie-Christine Brunet, Constantine D. Polychronopoulos 
Note: Although  Co., LTD., Fuji, 416, Japan  
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: This paper discusses the comprehensive performance profiling, improvement, and benchmarking of a Molecular Dynamics code, one of the most parallelizable applications, on both shared and distributed multiprocessor systems. In our analysis, we consider language, compiler, architecture, and algorithmic changes for each parallel portion and discuss the respective bottlenecks. We also investigate whether existing parallelizing compilers can find parallelism in the original code after reconstructing data for parallel processing, and parallelized code. We give examples of the existence of alternatives for parallelization and, then, compare the peformances of each, discussing their advantages and disadvantages. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. J. Alder and T. E. Wainwright. </author> <title> Phase transition for a hard sphere system. </title> <journal> J. chem. Phys., </journal> <volume> 27 </volume> <pages> 1208-1209, </pages> <year> 1957. </year>
Reference-contexts: This 5 technique has been used since the advent of general purpose computers. Alder and Wain--wright <ref> [1] </ref> were the pioneers of this field. In 1957, they published a paper using the hard sphere model. Rahman and Stillinger [21] were the first to apply MD in order to investigate the movement of a real molecule, water.
Reference: [2] <author> Stuart L. Anderson. </author> <title> Random number generators on vector supercomputers and other advanced architectures. </title> <journal> SIAM Review, </journal> <volume> 32(2) </volume> <pages> 221-251, </pages> <year> 1990. </year>
Reference-contexts: Although our target is MD, we use random numbers in order to simulate Brownian dynamics. There is much current research in this field. Marsaglia [13] and Niederreiter [14] surveyed the pseudo-random generator in general. <ref> [2] </ref>, [6], [25], and [19] proposed different methods for parallel pseudo-random number generation respectively.
Reference: [3] <author> R. B. Bird et al. </author> <title> Dynamics of Polymeric Liquid. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: easily derive the equation for the acceleration r i from F i in Eq. (1) as follows: r i = F i =m (6) Velocity and Position: In order to compute _ r i and r i from r i , we take the velocity form of the Verlet algorithm <ref> [3] </ref>. They are solved by iterating the following steps: At the beginning of each loop iteration, velocity _ r i (t) and position r i (t) are known. 8 1.
Reference: [4] <author> M. Born and T. </author> <title> Von Karman. Uber Schwingungen in Raumgittern. </title> <journal> Physik. Z., </journal> <volume> 13 </volume> <pages> 297-309, </pages> <year> 1912. </year>
Reference-contexts: For example, we have to compute 10 22 monomers per one gram of polymer. In order to make the computing target small, we employ the minimum distance image method <ref> [4] </ref>. A portion of the whole bulk is put in a cell, called an original cell. Periodic images of that original cell, called imaginary cells, are generated (see 2) Although we simulate in 3-dimensions, the figures in this section are shown in 2-dimensions. 11 the original cell.
Reference: [5] <author> Cray Research Inc. </author> <title> Cray MPP Fortran Reference Manual. </title>
Reference-contexts: When we port our parallelized code on these machines, all loops parallelized by polymers remain parallel while the other parallel loops are changed back to serial. We explicitly designate the data communication between processors using the Parallel Virtual Machine (PVM) [9] for the SP2, and shmem <ref> [5] </ref> for the T3D. 40 Total execution time Time for communication Total execution time Time for communication Total execution time Challenge 41 6.1 Performance Results Figures 20 (a), 21 (a), and 22 show the execution times on the SP2, the T3D, and the Power Challenge.
Reference: [6] <author> Masanori Fushimi. </author> <title> Random numbers with the recusion x(t)=x(t-3p)+x(t-3q). </title> <journal> J of Comp and Applied Math, </journal> <volume> 31 </volume> <pages> 105-118, </pages> <year> 1990. </year>
Reference-contexts: Although our target is MD, we use random numbers in order to simulate Brownian dynamics. There is much current research in this field. Marsaglia [13] and Niederreiter [14] surveyed the pseudo-random generator in general. [2], <ref> [6] </ref>, [25], and [19] proposed different methods for parallel pseudo-random number generation respectively.
Reference: [7] <author> S. Goedecker and L. Colombo. </author> <title> Tight binding molecular dynamics. </title> <booktitle> In Supercomputing '94, </booktitle> <pages> pages 670-672, </pages> <year> 1994. </year>
Reference-contexts: employed in our MD code. 2 Related Work This paper covers the parallelizing compiler and the parallelizing technique to compute the MD application, including the pseudo-random number generator. 4 Performance evaluation of the MD application on multiprocessor systems is done on both an HP9000 cluster and an NEC SX/3 in <ref> [7] </ref>, and on a CRAY T3D, nCube2, Intel iPSC/860 and Intel Paragon in [16]. The parallel pseudo-random number generator is a crucial to the parallelization, especially when using the Monte Carlo Method. Although our target is MD, we use random numbers in order to simulate Brownian dynamics.
Reference: [8] <author> M. R. Haghighat. </author> <title> Symbolic Analysis for Parallelizing Compilers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1994. </year>
Reference-contexts: The program is shown in Figure 10. The figure corresponding to Figure 12 (a) is shown in Figure 12 (b). The umbalance has been eliminated. There are other ways to balance the load of the loop in Figure 10. One method is Balanced Chunk Scheduling <ref> [8] </ref>. We do not employ this method for two reasons. One is data locality. A certain monomer computed by a processor in Module FORCE might be computed by another processor in other modules. This breaks the data locality, resulting in cache miss.
Reference: [9] <author> IBM Inc. </author> <title> XL Fortran for AIX. Language Reference. </title>
Reference-contexts: When we port our parallelized code on these machines, all loops parallelized by polymers remain parallel while the other parallel loops are changed back to serial. We explicitly designate the data communication between processors using the Parallel Virtual Machine (PVM) <ref> [9] </ref> for the SP2, and shmem [5] for the T3D. 40 Total execution time Time for communication Total execution time Time for communication Total execution time Challenge 41 6.1 Performance Results Figures 20 (a), 21 (a), and 22 show the execution times on the SP2, the T3D, and the Power Challenge.
Reference: [10] <author> G. Grest K. Kremer. </author> <title> Dynamics of entangled linear polymer melts: A molecular-dynamics simulation. </title> <journal> J. Chem. Phys., </journal> <volume> 92(15) </volume> <pages> 5057-5086, </pages> <month> 8 </month> <year> 1990. </year>
Reference-contexts: Using the MD technique, we can understand the movement of each particle and estimate macro values, such as diffusion coefficients which are difficult to obtain through experimental measurements. The MD code presented here is based on <ref> [10] </ref>. The basic algorithm for a molecular dynamics application is shown in Figure 1. In this paper, we simulate the movement of polymers. <p> The total potential between monomers i and j is: U ij = U LJ ij (3) where U LJ ij is the non-bonding potential, and U ch ij is the bonding potential. Bonding Potential: For bonding potential, we take the Finitely Extensible Nonlinear Elastic (FENE) potential <ref> [10] </ref> which is expressed as follows: U ch 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; : 0 logf1 ( r ij R o ) 2 g; (Monomers are bonded and r ij R 0 ) 1; (Monomers are bonded and r ij &gt; R 0 ) 0; (Monomers are
Reference: [11] <author> D. E. Knuth. </author> <title> The Art of Computer Programming Vol.2: Seminumerical Algorithms, Second edition. </title> <publisher> Addison-Wesley, </publisher> <address> Reading Massachusetts, </address> <year> 1981. </year>
Reference: [12] <author> M. Kuba. </author> <title> On the parallelization of a CFD code. </title> <type> Technical Report 1412, </type> <institution> Center for Supercomputing Research and Development, </institution> <year> 1995. </year>
Reference-contexts: Pointers fiptr (i)g are allocated in consecutive memory (even though they are used by different processors), which gives rise to possible false sharing. The same is true for Array ix. On some multiprocessor systems, false sharing may result in extremely poor performance <ref> [12] </ref>. In order to avoid false sharing, we can employ either of the following two methods: (i) using local variables, or (ii) maintaining a large stride between adjacent entries in shared memory.
Reference: [13] <author> G. Marsaglia. </author> <title> A current view of random number generators. </title> <booktitle> In 16th Symposium on the Interface, </booktitle> <pages> pages 3-10. </pages> <institution> Computer Science and Statistics, </institution> <year> 1985. </year>
Reference-contexts: The parallel pseudo-random number generator is a crucial to the parallelization, especially when using the Monte Carlo Method. Although our target is MD, we use random numbers in order to simulate Brownian dynamics. There is much current research in this field. Marsaglia <ref> [13] </ref> and Niederreiter [14] surveyed the pseudo-random generator in general. [2], [6], [25], and [19] proposed different methods for parallel pseudo-random number generation respectively.
Reference: [14] <author> Halold Niederreiter. </author> <title> New developments in uniform pseudorandom number and vector generation. </title> <type> Technical report, </type> <institution> Institute for Information Processing, Austrian Academy of Science, </institution> <year> 1995. </year> <month> 48 </month>
Reference-contexts: The parallel pseudo-random number generator is a crucial to the parallelization, especially when using the Monte Carlo Method. Although our target is MD, we use random numbers in order to simulate Brownian dynamics. There is much current research in this field. Marsaglia [13] and Niederreiter <ref> [14] </ref> surveyed the pseudo-random generator in general. [2], [6], [25], and [19] proposed different methods for parallel pseudo-random number generation respectively.
Reference: [15] <author> David A. Padua and Rudolf Eigenmann. </author> <title> Polaris: A new generation parallelizing com-piler for mpp's. </title> <type> Technical report, </type> <institution> Center for Supercomputing Research and Development, </institution> <year> 1993. </year>
Reference-contexts: The experiments are performed on an SGI Power Challenge. This is a 16-processor MIMD system using MIPS 8000; 8 processors are used to execute parallelized codes 6) . 4.2 Automatic Parallelizing Compiler We first apply three parallelizing compilers, Power Fortran Accelerator (PFA) [22], Parafrase-2 [17], and Polaris <ref> [15] </ref>, to our MD code and evaluate their resulting codes. We turn on all optimization switches for PFA and employ the recommended passes or switches for Parafrase-2 and Polaris respectively. 6) We explicitly specify when the number of processors we have used is other than 8 processors. 18 parallelizing compilers.
Reference: [16] <author> Steve Plimpton. </author> <title> Fast parallel algorithm for short-range molecular dynamics. </title> <journal> Journal of Computational Physics, </journal> <volume> 117 </volume> <pages> 1-19, </pages> <year> 1995. </year>
Reference-contexts: compiler and the parallelizing technique to compute the MD application, including the pseudo-random number generator. 4 Performance evaluation of the MD application on multiprocessor systems is done on both an HP9000 cluster and an NEC SX/3 in [7], and on a CRAY T3D, nCube2, Intel iPSC/860 and Intel Paragon in <ref> [16] </ref>. The parallel pseudo-random number generator is a crucial to the parallelization, especially when using the Monte Carlo Method. Although our target is MD, we use random numbers in order to simulate Brownian dynamics. There is much current research in this field.
Reference: [17] <author> C. D. Polychronopoulos et al. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing and scheduling programs on multiprocessors. </title> <booktitle> In International Conference for Parallel Processing, </booktitle> <year> 1989. </year>
Reference-contexts: The experiments are performed on an SGI Power Challenge. This is a 16-processor MIMD system using MIPS 8000; 8 processors are used to execute parallelized codes 6) . 4.2 Automatic Parallelizing Compiler We first apply three parallelizing compilers, Power Fortran Accelerator (PFA) [22], Parafrase-2 <ref> [17] </ref>, and Polaris [15], to our MD code and evaluate their resulting codes.
Reference: [18] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transaction on computers, </journal> <volume> C-36(12):1425-1439, </volume> <year> 1989. </year>
Reference-contexts: We have to accumulate all np's and divide each chunk as shown in Figure 11. In this figure, nchnk (iproc) and npes indicate the first monomer for the iproc-th chunk and the number of processors respectively. The second loop is serial. Another method is Guided Self-Scheduling <ref> [18] </ref>, which also breaks the data locality. 4.4 Parallelization of FORCE 24 doall j (for all polymers) (Compute fb (h (j))) f (h (j)) = fb (h (j)) (Compute fb (i)) f (k,i) = - fb (i-1) + fb (i) enddo f (h (j+1)-1) = -fb (h (j+1)-2) enddoall Computing Bonding
Reference: [19] <author> D. V. Pryor, S. A. Cuccaro, M. Mascagni, </author> <title> and M.L. Robinson. Implementtion of a portable and reproducible parallel pseudorandom number generator. </title> <booktitle> In Supercomputing '94, </booktitle> <pages> pages 311-319, </pages> <year> 1994. </year>
Reference-contexts: Although our target is MD, we use random numbers in order to simulate Brownian dynamics. There is much current research in this field. Marsaglia [13] and Niederreiter [14] surveyed the pseudo-random generator in general. [2], [6], [25], and <ref> [19] </ref> proposed different methods for parallel pseudo-random number generation respectively. We employed Pryor's [19] method in our code, which is described in the Appendix. 3 Background of the MD Model and Application Molecular dynamics (MD) is a computational technique used to study the dynamic property of many particle systems by applying <p> There is much current research in this field. Marsaglia [13] and Niederreiter [14] surveyed the pseudo-random generator in general. [2], [6], [25], and <ref> [19] </ref> proposed different methods for parallel pseudo-random number generation respectively. We employed Pryor's [19] method in our code, which is described in the Appendix. 3 Background of the MD Model and Application Molecular dynamics (MD) is a computational technique used to study the dynamic property of many particle systems by applying Newton's classical equations of motion. <p> Among the several proposed generators discussed in Section 2, we employed the parallelized lagged-Fibonacci pseudo-random number generator <ref> [19] </ref>, which is given by: x n = x nk + x nl (mod 2 m ) (25) In the parallel version, the n-th random number on the id-th processor, x n;id , becomes as 30 follows: x n;id = x nk;id + x nl;id (mod 2 m ) (26) In
Reference: [20] <author> B. Quentrec and C. Brot. </author> <title> New method for searching for neighbors in mole cular dynamics computations. </title> <journal> J. Comp. Phys., </journal> <volume> 13 </volume> <pages> 430-432, </pages> <year> 1975. </year>
Reference-contexts: Because monomers are moving in the system, this list needs to be updated. To avoid updating it every time step, we employ a mixed method of the Verlet neighbor list [24] and the cell index method <ref> [20] </ref>, which are discussed in detail in the following subsections. As a result, the time required to compute the non-bonding forces fF LJ i g is reduced to O (N ). We also discuss this in detail in the following subsections. The neighbor list is used for computing non-bonding forces. <p> We consider how to reduce this time in the next section. 3) In our experiments, we determined the value for this thickness by trial and error until we found an effective performance for each kind of polymer and hardware system. 14 3.6.2 The Cell Index Method The cell index method <ref> [20] </ref> is used to reduce the execution time for making a neighbor list. In this method, the original cell is divided into a lattice of M fi M fi M sub-cells as shown in Figure 4.
Reference: [21] <author> A. Rahman and F. H. Stillinger. </author> <title> Molecular dynamics study of liquid water. </title> <journal> J. chem. Phys., </journal> <volume> 55 </volume> <pages> 3336-3359, </pages> <year> 1971. </year>
Reference-contexts: This 5 technique has been used since the advent of general purpose computers. Alder and Wain--wright [1] were the pioneers of this field. In 1957, they published a paper using the hard sphere model. Rahman and Stillinger <ref> [21] </ref> were the first to apply MD in order to investigate the movement of a real molecule, water. Using the MD technique, we can understand the movement of each particle and estimate macro values, such as diffusion coefficients which are difficult to obtain through experimental measurements.
Reference: [22] <author> Silicon Graphics Inc. </author> <title> POWER Fortran Accelerator User's Guide. online. </title>
Reference-contexts: The experiments are performed on an SGI Power Challenge. This is a 16-processor MIMD system using MIPS 8000; 8 processors are used to execute parallelized codes 6) . 4.2 Automatic Parallelizing Compiler We first apply three parallelizing compilers, Power Fortran Accelerator (PFA) <ref> [22] </ref>, Parafrase-2 [17], and Polaris [15], to our MD code and evaluate their resulting codes.
Reference: [23] <author> D. J. Tildesley. </author> <title> Computer Simulation of Liquids. </title> <publisher> Oxford University Press, </publisher> <year> 1989. </year>
Reference: [24] <author> L. Verlet. </author> <title> Computer `experiments' on classical fluids. I. </title> <booktitle> Thermodynamical properties of Lennard-Jones molecules, </booktitle> <pages> pages 98-103. </pages> ?, <year> 1968. </year>
Reference-contexts: Instead, a list is built containing all pairs of monomers within a certain distance of each other. Because monomers are moving in the system, this list needs to be updated. To avoid updating it every time step, we employ a mixed method of the Verlet neighbor list <ref> [24] </ref> and the cell index method [20], which are discussed in detail in the following subsections. As a result, the time required to compute the non-bonding forces fF LJ i g is reduced to O (N ). We also discuss this in detail in the following subsections.
Reference: [25] <author> Pei-Chi Wu. </author> <title> Multiplicative congruential random number generators with multiplier +-2**(k1)+-2**(k2) and modulus 2**p-1. </title> <type> Technical report, </type> <institution> Department of Computer Science and Information Engineering National Chiao Tung University, Taiwan, </institution> <year> 1995. </year>
Reference-contexts: Although our target is MD, we use random numbers in order to simulate Brownian dynamics. There is much current research in this field. Marsaglia [13] and Niederreiter [14] surveyed the pseudo-random generator in general. [2], [6], <ref> [25] </ref>, and [19] proposed different methods for parallel pseudo-random number generation respectively.
References-found: 25

