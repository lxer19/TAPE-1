URL: http://www.cfar.umd.edu/~fejes/papers/mot.ps.gz
Refering-URL: http://www.umiacs.umd.edu/users/sharat/papers/p3_19.htm
Root-URL: 
Email: email: fejes@umiacs.umd.edu, lsd@umiacs.umd.edu  
Title: What can projections of flow fields tell us about the visual motion  
Author: Sandor Fejes and Larry S. Davis 
Keyword: Egomotion estimation, Detection of moving objects, Robust line fitting, Spatio-temporal integration.  
Address: College Park, MD 20742-3275  
Affiliation: Center for Automation Research University of Maryland  
Abstract: The dimensionality of visual motion analysis can be reduced by analyzing projections of flow vector fields. In contrast to motion vector fields, these projections exhibit simple geometric properties which are invariant to the scene structure and depend only on the camera motion. Using these properties, structure and motion can be either completely or partially decoupled. We estimate motion parameters from projections of flow fields by using robust techniques, implemented in a recursive observer model. The model is applicable to general camera motion and to large field of view and requires no point correspondence. We demonstrate our projection method on the problem of detecting independently moving objects from a moving camera. Using the projection approach, the problem can be reduced to a one-dimensional optimization process which involves robust line-fitting and outlier detection. Instantaneous detection measurements are integrated temporally using tracking and spatially applying grouping of coherently moving points.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Adiv. </author> <title> Determining three-dimensional motion and structure from optical flow generated by several moving objects. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 7 </volume> <pages> 384-401, </pages> <year> 1985. </year>
Reference-contexts: A human uses a combination of visual cues to solve this problem, such as geometry; shape; illumination; color; ordinal depth from stereo, structure and occlusion; knowledge about the scene; etc. As in many other studies <ref> [1, 7, 6, 9, 13, 12] </ref> related to this problem, we use only the geometry cue to solve this problem, and assume no domain knowledge (which, of course, would be useful in practical applications). <p> restricting the allowable camera motion [13, 9], using a small FOV [6] or limiting the allowable clutter in the 8 Intuitively, twice as big translational motion in a given scene results in perceiving only half scaled scene depth. 13 scene [7, 6], and often they use techniques requiring point correspondence <ref> [1, 6, 7, 13, 12] </ref>. It is clear that any restriction of camera motion imposes limitations on allowable operational conditions, while the use of narrow FOV may restrict surveillance capabilities. <p> camera motion we can increase our detection capabilities by purposive relocation of the FOE so that the moving object can be then placed within the region of detectability in the image FOV. 5.2 Detection algorithm Detecting moving objects requires identifying regions of actual independent motion and excluding regions of "outliers" <ref> [1] </ref>. Our detection algorithm is based on the estimation of the (projected or complete) motion parameters of the camera; the motion estimation process is designed to tolerate inconsistencies in some of the bands of the projected flow field.
Reference: [2] <author> K. Daniilidis and I. Thomas. </author> <title> Decoupling the 3d motion space by fixation. </title> <booktitle> In Proceedings of the Europian Conference on Computer Vision, </booktitle> <pages> pages 685-696, </pages> <address> Cambridge, UK, </address> <year> 1996. </year> <month> 17 </month>
Reference-contexts: One way to reduce dimensionality is to use projections. One can consider only specific components of the flow field (in one or more directions) and use them to extract information about the visual motion. Such an approach was first introduced in [3, 4, 8] and then further developed in <ref> [2, 9, 11] </ref>. Before describing our approach we first introduce our basic notation and definitions. Let v (x; y) be a flow vector field (e.g. optic flow) in the image plane and p be the (unit) projection vector. <p> In <ref> [2, 3, 11] </ref> only the linearity property of orthogonal restrictions of projected flow fields is used to constrain the location of the FOE and estimate the rotational parameters. In those approaches, only those orthogonal restrictions are analyzed which pass through the image center (x = 0).
References-found: 2

