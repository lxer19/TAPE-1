URL: http://www.cs.ucsb.edu/~tyang/papers/JPDCradiosity.ps
Refering-URL: http://www.cs.ucsb.edu/Research/rapid_sweb/RAPID.html
Root-URL: http://www.cs.ucsb.edu
Title: Parallel Progressive Radiosity with Adaptive Meshing  
Author: Yizhou Yu Oscar H. Ibarra Tao Yang 
Keyword: Parallel Progressive Radiosity, Adaptive Meshing, Load balancing, Octree, Processor assignment of irregular objects.  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California  
Abstract: Progressive radiosity is widely used for realistic image synthesis in computer graphics applications. High-quality image generation usually requires radiosity with adaptive patch refinement to account for global illumination effects from irregular objects in a 3D space. Parallelizing such an algorithm with adaptive refinement is difficult since computation cost for each object varies from one iteration to another depending on the location of dynamically selected shooting patches. Dynamic load balancing is required but its overhead is high for distributed memory systems. This paper presents an efficient parallel algorithm for progressive radiosity, which adopts a static processor assignment strategy to take advantages of hierarchical computation structure in this problem, minimize communication and balance dynamic load. Our experiments on a Meiko CS-2 distributed memory machine show that this algorithm has achieved good performance for the tested cases.
Abstract-found: 1
Intro-found: 1
Reference: [BW90] <author> D.R.Baum, and J.M.Winget, </author> <title> "Real Time Radiosity Through Parallel Processing and Hardware Acceleration," </title> <journal> Computer Graphics, </journal> <volume> 24(2), </volume> <year> 1990, </year> <month> pp.67-75. </month>
Reference-contexts: There have been several parallel algorithms proposed in <ref> [BW90, BP94, PC94, RGG90] </ref> but speedups for progressive radiosity with adaptive meshing are still low because the computation associated with patches in a 3D space is unstructured and the cost varies during iterations. Load balancing is the main challenge in achieving high speedups [PC94]. <p> In the radiosity algorithm, choosing a coarse grain parallelism would not allow exploitation of data locality. Therefore, a tradeoff between selecting a grain size and exploiting the data locality has to be decided. Several parallel radiosity algorithms have been proposed in the literature, e.g. <ref> [BW90, BP94, PC94, RGG90] </ref>. Baum et. al.[BW90] have obtained interesting results on a shared memory machine with 8 processors supplied with a hardware Z buffer; however, scalability results on a larger of numbers of processors have not been demonstrated.
Reference: [BL+86] <author> Bergman, Larry, H.Fuchs, E.Grant and S.Spach, </author> <title> "Image Rendering by Adaptive Refinement," </title> <journal> Proceedings of SIGGRAPH'86, Computer Graphics, </journal> <volume> 20(4), </volume> <year> 1986, </year> <month> pp.29-37. </month>
Reference: [BP94] <author> K.Bouatouch and T.Priol, </author> <title> "Data Management Scheme for Parallel Radiosity," </title> <booktitle> Computer-Aided Design, </booktitle> <address> Vol.26, No.12, </address> <year> 1994, </year> <month> pp.876-882. </month>
Reference-contexts: There have been several parallel algorithms proposed in <ref> [BW90, BP94, PC94, RGG90] </ref> but speedups for progressive radiosity with adaptive meshing are still low because the computation associated with patches in a 3D space is unstructured and the cost varies during iterations. Load balancing is the main challenge in achieving high speedups [PC94]. <p> Load balancing is the main challenge in achieving high speedups [PC94]. The previous research <ref> [BP94, PC94] </ref> improves parallel performance by concurrentizing several iterations on different processors. This is called to exploit control parallelism. In this scheme, synchronization is needed when several iterations modify the same patch object and communication overhead is high on distributed memory machines. <p> In the radiosity algorithm, choosing a coarse grain parallelism would not allow exploitation of data locality. Therefore, a tradeoff between selecting a grain size and exploiting the data locality has to be decided. Several parallel radiosity algorithms have been proposed in the literature, e.g. <ref> [BW90, BP94, PC94, RGG90] </ref>. Baum et. al.[BW90] have obtained interesting results on a shared memory machine with 8 processors supplied with a hardware Z buffer; however, scalability results on a larger of numbers of processors have not been demonstrated. <p> But still the master processor needs to collect a new update from each slave processor, selects the shooting patch and broadcast related new updates to all slave processors at each iteration. Communication overhead and memory requirement are high. Another approach is to evenly distribute data among all nodes <ref> [PC94, BP94] </ref>. The question is how to exploit the parallelism. Since data structure in the 3D space is irregular, control-oriented parallelization is used in [PC94, BP94]. In this scheme, several iterations, corresponding the while loop in Figure 1, can be started concurrently in different processors. <p> Communication overhead and memory requirement are high. Another approach is to evenly distribute data among all nodes <ref> [PC94, BP94] </ref>. The question is how to exploit the parallelism. Since data structure in the 3D space is irregular, control-oriented parallelization is used in [PC94, BP94]. In this scheme, several iterations, corresponding the while loop in Figure 1, can be started concurrently in different processors. Notice that the parallel algorithm result is slightly different from the sequential algorithm since unsynchronized radiosity updating is used. <p> Since a processor may need non-local data, managing communications between processors is important [PC94]. Also since concurrent iterations may update the radiosity of the same patch, these updates must be synchronized. The implementation in <ref> [BP94] </ref> relies on the shared virtual memory support support, which leads to excessive operating system level overhead in maintaining data consistency. The other problems which are not well addressed are the load balancing and adaptive meshing for high quality image synthesis. <p> Adaptive meshing required at each iteration of radiosity creates an additional difficulty in designing an efficient parallel solution. This is because adaptive refinement of patches may increase the number of patches on some processors dynamically, consequently it worsens the load balance. It is indicated in <ref> [BP94] </ref> that the form-factor calculation and the radiosity update require an access to the information regarding all patches. This is due to the use of hemicube-based algorithms. Actually, hemicube is seldomly used in recent sequential algorithms because the hemicube method has aliasing effects. <p> By using a larger error tolerance for those, we can make computation more efficient without losing necessary accuracy. A larger error tolerance also means smaller subdivision trees and smaller difference between different subdivision trees, which smoothes the computation and makes it more balanced. 4.3 Form-factor calculation In <ref> [BP94] </ref>, the parallel algorithm needs to access information regarding all patches. That is actually not necessary. We only need the radiosity and geometrical information of the shooting patch, the albedo and geometrical information of the receiving patches and occluding information implied in the octree partitioning. <p> Scalability. The parallel time speedup for each test scene is shown in Figure 9. This result is comparable to the recent results for progressive radiosity <ref> [BP94] </ref> without adaptive meshing, and is much better than previous results for both adaptive and non-adaptive progressive radiosity [PC94]. As we already mentioned in Section 3, some difficulties arise from adaptive meshing, the above experimental result shows that our algorithm deals with irregular and adaptive load variation very well.
Reference: [CG+86] <author> M.F.Cohen, D.P.Greenberg, D.S.Immel and P.J.Brock, </author> <title> "An Efficient Radiosity Approach for Realistic Image Synthesis," </title> <journal> IEEE CG&A, </journal> <volume> 6(2), </volume> <year> 1986, </year> <month> pp.26-35. </month>
Reference-contexts: A further improvement to increase 1 the image quality of radiosity in such an iterative algorithm is to incorporate adaptive subdivision <ref> [CG+86, PJ94] </ref>. It is important to speedup the radiosity time since it still takes a few hours to complete all iterations for progressive radiosity and even if a user wants to view intermediate results, it takes minutes to see the obvious improvement from one view to another. <p> The direct method for this linear system using Gaussian elimination takes O (N 3 ). Thus it is time-consuming to generate a realistic image with a large number of patches. Since the radiosity matrix is diagonally dominating, the Gauss-Seidel method <ref> [CG+86] </ref> has been used to find an approximate solution, which converges in less than N iterations in practice. However the form-factor calculation is still expensive, which slows down the entire process since the partial result cannot be displayed until the form-factor calculation has completed. <p> That is actually not true for each patch with a large area. To achieve a good accuracy, adaptive meshing was integrated with radiosity in <ref> [CG+86, HSA91, LTG92, LTG93, PJ94, YP95] </ref>. In this approach, each patch derived from the initial subdivision (meshing) might be further refined during the course of the computation if the accuracy of each patch radiosity does not meet the standard.
Reference: [CC+88] <author> M.F.Cohen, S.E.Chen, J.R.Wallace, D.P.Greenberg, </author> <title> "A Progressive Refinement Approach to Fast Radiosity Image Generation," </title> <journal> Computer Graphics, </journal> <volume> 22(4), </volume> <year> 1988, </year> <month> pp.75-84. </month>
Reference-contexts: For example, the synthesis of an image with an average number of objects may take a few hours on a Sparc 10 workstation. A compromise solution to reduce the radiosity time is to use progressive illumination calculation <ref> [CC+88, LTG92] </ref>. The basic idea is to provide low-quality images at the beginning, then the algorithm conducts a sequence of refinement iterations to improve the image quality. <p> However the form-factor calculation is still expensive, which slows down the entire process since the partial result cannot be displayed until the form-factor calculation has completed. This difficulty was addressed using the progressive radiosity approach <ref> [CC+88] </ref>. This method still uses a sequence of numerical iterative refinements but only N form-factors are needed to conduct the refinement at each iteration. Thus the complexity of the form-factor calculation is distributed among those iterations and the partial result can be displayed sooner than the Gauss-Seidel based method.
Reference: [HSA91] <author> P.Hanrahan, D.Salzman and L.Aupperle, </author> <title> "A Rapid Hierarchical Radiosity Algorithm," </title> <journal> Computer Graphics, </journal> <volume> 25(4), </volume> <year> 1991, </year> <month> pp.197-206. </month>
Reference-contexts: That is actually not true for each patch with a large area. To achieve a good accuracy, adaptive meshing was integrated with radiosity in <ref> [CG+86, HSA91, LTG92, LTG93, PJ94, YP95] </ref>. In this approach, each patch derived from the initial subdivision (meshing) might be further refined during the course of the computation if the accuracy of each patch radiosity does not meet the standard.
Reference: [KPC93] <author> J.K.Kawai, J.S.Painter, and M.F.Cohen, </author> " <title> Radioptimization-Goal Based Rendering," </title> <booktitle> Computer Graphics Proceedings, Annual Conference Series, </booktitle> <year> 1993, </year> <month> pp.147-154. </month>
Reference-contexts: 1 Introduction The main application of realistic image synthesis is to create simulated scenes of photo realistic quality. Ray tracing or radiosity algorithms are widely used for computing global illumination effects in image synthesis. Recently radiosity method becomes increasingly popular for determining global illumination in architectural modeling and virtual reality <ref> [SD+93, KPC93] </ref> because it accurately portrays illumination effects such as shadows and interreflections in diffuse environments. Furthermore, once a radiosity solution is calculated for a fixed geometry, a user can interactively walk through this model to view 3D objects using virtual reality techniques on a graphics workstation.
Reference: [LTG92] <author> D.Lischinski, F.Tampieri and D.P.Greenberg, </author> <title> "Discontinuity Meshing for Accurate Radiosity," </title> <journal> IEEE CG&A, </journal> <volume> 12(6), </volume> <year> 1992, </year> <month> pp.25-39. </month>
Reference-contexts: For example, the synthesis of an image with an average number of objects may take a few hours on a Sparc 10 workstation. A compromise solution to reduce the radiosity time is to use progressive illumination calculation <ref> [CC+88, LTG92] </ref>. The basic idea is to provide low-quality images at the beginning, then the algorithm conducts a sequence of refinement iterations to improve the image quality. <p> That is actually not true for each patch with a large area. To achieve a good accuracy, adaptive meshing was integrated with radiosity in <ref> [CG+86, HSA91, LTG92, LTG93, PJ94, YP95] </ref>. In this approach, each patch derived from the initial subdivision (meshing) might be further refined during the course of the computation if the accuracy of each patch radiosity does not meet the standard.
Reference: [LTG93] <author> D.Lischinski, F.Tampieri and D.P.Greenberg, </author> <title> "Combining Hierarchical Radiosity and Discontinuity Meshing," </title> <booktitle> Computer Graphics Proceedings, Annual Conference Series, </booktitle> <year> 1993, </year> <month> pp.199-208. </month>
Reference-contexts: That is actually not true for each patch with a large area. To achieve a good accuracy, adaptive meshing was integrated with radiosity in <ref> [CG+86, HSA91, LTG92, LTG93, PJ94, YP95] </ref>. In this approach, each patch derived from the initial subdivision (meshing) might be further refined during the course of the computation if the accuracy of each patch radiosity does not meet the standard.
Reference: [MB90] <author> J.D.MacDonald and K.S.Booth, </author> <title> "Heuristics for ray tracing using space subdivision," Visual Computer, </title> <publisher> (1990)6, pp.153-166. </publisher>
Reference-contexts: It is done by casting rays from the current receiving patch to some sample points on the shooting patch. This process can be accelerated by letting the rays go through some leaf nodes in the octree between the receiving patch and the shooting patch <ref> [MB90] </ref>. In this way, occlusion test is done only against those surfaces intersecting these leaf nodes. Since a shooting patch is chosen from the initial mesh, it may contain many tiny patches.
Reference: [MC+94] <author> S.Molnar, M.Cox, David Ellsworth and H.Fuchs, </author> <title> "A Sorting Classification of Parallel Rendering," </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> No.4, </volume> <year> 1994, </year> <month> pp.23-32. </month>
Reference-contexts: And a larger patch takes the average radiosity of the smaller ones. 4.4 Rendering and display The image rendering and display usually cost a few seconds in a workstation, which is acceptable with most of computer aided design applications. There is some research work on parallel rendering <ref> [MC+94, OHA93, W94] </ref> for larger data sets, which we have not considered in this paper. 5 Processor Assignment for Superpatches Our goal is to distribute the amount of dynamic computation evenly among all processors. <p> The stage of image rendering can also be parallelized to get a faster rate of interaction and currently we can make use of some of the existing parallel polygon rendering algorithms <ref> [MC+94, OHA93, W94] </ref>. 18
Reference: [OHA93] <author> F.A.Ortega, C.D.Hansen and J.P.Ahrens, </author> <title> "Fast Data Parallel Polygon Rendering," </title> <booktitle> proceedings of Supercomputing'93, </booktitle> <address> pp.709-718. </address>
Reference-contexts: And a larger patch takes the average radiosity of the smaller ones. 4.4 Rendering and display The image rendering and display usually cost a few seconds in a workstation, which is acceptable with most of computer aided design applications. There is some research work on parallel rendering <ref> [MC+94, OHA93, W94] </ref> for larger data sets, which we have not considered in this paper. 5 Processor Assignment for Superpatches Our goal is to distribute the amount of dynamic computation evenly among all processors. <p> The stage of image rendering can also be parallelized to get a faster rate of interaction and currently we can make use of some of the existing parallel polygon rendering algorithms <ref> [MC+94, OHA93, W94] </ref>. 18
Reference: [PC94] <author> D.Paddon and A.Chalmers, </author> <title> "Parallel processing of the radiosity method," </title> <booktitle> Computer-Aided Design, </booktitle> <address> Vol.26, No.12, </address> <year> 1994, </year> <month> pp.917-927. </month>
Reference-contexts: There have been several parallel algorithms proposed in <ref> [BW90, BP94, PC94, RGG90] </ref> but speedups for progressive radiosity with adaptive meshing are still low because the computation associated with patches in a 3D space is unstructured and the cost varies during iterations. Load balancing is the main challenge in achieving high speedups [PC94]. <p> Load balancing is the main challenge in achieving high speedups <ref> [PC94] </ref>. The previous research [BP94, PC94] improves parallel performance by concurrentizing several iterations on different processors. This is called to exploit control parallelism. In this scheme, synchronization is needed when several iterations modify the same patch object and communication overhead is high on distributed memory machines. <p> Load balancing is the main challenge in achieving high speedups [PC94]. The previous research <ref> [BP94, PC94] </ref> improves parallel performance by concurrentizing several iterations on different processors. This is called to exploit control parallelism. In this scheme, synchronization is needed when several iterations modify the same patch object and communication overhead is high on distributed memory machines. <p> In the radiosity algorithm, choosing a coarse grain parallelism would not allow exploitation of data locality. Therefore, a tradeoff between selecting a grain size and exploiting the data locality has to be decided. Several parallel radiosity algorithms have been proposed in the literature, e.g. <ref> [BW90, BP94, PC94, RGG90] </ref>. Baum et. al.[BW90] have obtained interesting results on a shared memory machine with 8 processors supplied with a hardware Z buffer; however, scalability results on a larger of numbers of processors have not been demonstrated. <p> But still the master processor needs to collect a new update from each slave processor, selects the shooting patch and broadcast related new updates to all slave processors at each iteration. Communication overhead and memory requirement are high. Another approach is to evenly distribute data among all nodes <ref> [PC94, BP94] </ref>. The question is how to exploit the parallelism. Since data structure in the 3D space is irregular, control-oriented parallelization is used in [PC94, BP94]. In this scheme, several iterations, corresponding the while loop in Figure 1, can be started concurrently in different processors. <p> Communication overhead and memory requirement are high. Another approach is to evenly distribute data among all nodes <ref> [PC94, BP94] </ref>. The question is how to exploit the parallelism. Since data structure in the 3D space is irregular, control-oriented parallelization is used in [PC94, BP94]. In this scheme, several iterations, corresponding the while loop in Figure 1, can be started concurrently in different processors. Notice that the parallel algorithm result is slightly different from the sequential algorithm since unsynchronized radiosity updating is used. <p> Notice that the parallel algorithm result is slightly different from the sequential algorithm since unsynchronized radiosity updating is used. Since a processor may need non-local data, managing communications between processors is important <ref> [PC94] </ref>. Also since concurrent iterations may update the radiosity of the same patch, these updates must be synchronized. The implementation in [BP94] relies on the shared virtual memory support support, which leads to excessive operating system level overhead in maintaining data consistency. <p> Scalability. The parallel time speedup for each test scene is shown in Figure 9. This result is comparable to the recent results for progressive radiosity [BP94] without adaptive meshing, and is much better than previous results for both adaptive and non-adaptive progressive radiosity <ref> [PC94] </ref>. As we already mentioned in Section 3, some difficulties arise from adaptive meshing, the above experimental result shows that our algorithm deals with irregular and adaptive load variation very well. Distribution of computation time, idle time and communication overhead. <p> The communication cost is extremely low, used for selecting the shooting patch and broadcasting the data for the shooting patch. The load imbalance factor is within 10%. That is probably the best we can achieve for a static assignment in responding to the dynamically-changing workload. The previous work <ref> [PC94] </ref> using dynamic balancing does not have a good speedup because of high communication overhead. Effectiveness of the hierarchical calculation strategy. We have examined the impact of our hierarchical calculation method which is discussed in Section 4.2. <p> Speedup with is the speedup using our method. The improvement is around 20% and that is another reason we can obtain a much better performance compared to <ref> [PC94] </ref>. Scene 1 Scene 2 Scene 3 Improvement ratio 22% 16% 24% Speedup of NH 18.6 22.4 21.7 Table 2: Our hierarchical method vs. a non-hierarchical method (NH). Effectiveness of the processor assignment strategy. We have also examined the impact of our processor assignment strategy.
Reference: [PJ94] <author> M.Paulin and J.Jessel, </author> <title> "Adaptive mesh generation for progressive radiosity: A ray-tracing based algorithm," </title> <publisher> Eurographics'94, pp.C421-C432. </publisher>
Reference-contexts: A further improvement to increase 1 the image quality of radiosity in such an iterative algorithm is to incorporate adaptive subdivision <ref> [CG+86, PJ94] </ref>. It is important to speedup the radiosity time since it still takes a few hours to complete all iterations for progressive radiosity and even if a user wants to view intermediate results, it takes minutes to see the obvious improvement from one view to another. <p> That is actually not true for each patch with a large area. To achieve a good accuracy, adaptive meshing was integrated with radiosity in <ref> [CG+86, HSA91, LTG92, LTG93, PJ94, YP95] </ref>. In this approach, each patch derived from the initial subdivision (meshing) might be further refined during the course of the computation if the accuracy of each patch radiosity does not meet the standard.
Reference: [RGG90] <author> R.J.Recker, D.W.George and D.P.Greenberg, </author> <title> "Acceleration Techniques for Progressive Refinement Radiosity," </title> <journal> Computer Graphics, </journal> <volume> 24(2), </volume> <year> 1990, </year> <month> pp.59-66. </month>
Reference-contexts: There have been several parallel algorithms proposed in <ref> [BW90, BP94, PC94, RGG90] </ref> but speedups for progressive radiosity with adaptive meshing are still low because the computation associated with patches in a 3D space is unstructured and the cost varies during iterations. Load balancing is the main challenge in achieving high speedups [PC94]. <p> In the radiosity algorithm, choosing a coarse grain parallelism would not allow exploitation of data locality. Therefore, a tradeoff between selecting a grain size and exploiting the data locality has to be decided. Several parallel radiosity algorithms have been proposed in the literature, e.g. <ref> [BW90, BP94, PC94, RGG90] </ref>. Baum et. al.[BW90] have obtained interesting results on a shared memory machine with 8 processors supplied with a hardware Z buffer; however, scalability results on a larger of numbers of processors have not been demonstrated. <p> Baum et. al.[BW90] have obtained interesting results on a shared memory machine with 8 processors supplied with a hardware Z buffer; however, scalability results on a larger of numbers of processors have not been demonstrated. In this paper, we focus on parallel algorithms on distributed memory machines. In <ref> [RGG90] </ref>, a master-slave approach is proposed. At each computation stage, each processor obtains a part of the work to process. Since patch distribution is not fixed, data re-distribution is needed. To avoid excessive communication cost for data-redistribution, they replicate all geometrical data to all processors.
Reference: [SD+93] <author> C.Schoeneman, J.Dorsey, B.Smits, J.Arvo, and D.Greenberg, </author> " <title> Painting with Light," </title> <booktitle> Computer Graphics Proceedings, Annual Conference Series, </booktitle> <year> 1993, </year> <month> pp.143-146. </month>
Reference-contexts: 1 Introduction The main application of realistic image synthesis is to create simulated scenes of photo realistic quality. Ray tracing or radiosity algorithms are widely used for computing global illumination effects in image synthesis. Recently radiosity method becomes increasingly popular for determining global illumination in architectural modeling and virtual reality <ref> [SD+93, KPC93] </ref> because it accurately portrays illumination effects such as shadows and interreflections in diffuse environments. Furthermore, once a radiosity solution is calculated for a fixed geometry, a user can interactively walk through this model to view 3D objects using virtual reality techniques on a graphics workstation.
Reference: [SHG95] <author> J.P.Singh, J.L.Hennessy, and A. Gupta, </author> <title> "Implications of Hierarchical N-Body Methods for Multiprocessor Architectures," </title> <journal> ACM Trans. on Computer Systems, Vol.13, No.2, 1995, pp.141-202. </journal> <volume> 19 </volume>
Reference-contexts: We have not seen any publications using heuristics to do processor assignment for this problem, either. 9 The problem of partitioning irregular data space has been studied in other scientific application areas, for example n-body simulations <ref> [SHG95] </ref>. Usually the quadtree partitioning method is used and neighboring leafs are directly mapped to the same processor. This is because neighboring particles have intensive communications. We call this method the quadtree-based block mapping. This method can be generalized to a 3D space. We call it octree based block mapping.
Reference: [WEH89] <author> J.R.Wallace, K.A.Elmquist and E.A.Haines, </author> <title> "A Ray Tracing Algorithm for Progressive Radios--ity," </title> <journal> Computer Graphics, </journal> <volume> 23(3), </volume> <year> 1989, </year> <month> pp.315-324. </month>
Reference-contexts: That is why form-factor calculation is the main concern in radiosity algorithms. Given area A 1 in the receiving patch and A 2 in the shooting patch, as shown in Figure 3, <ref> [WEH89] </ref> proposed a disk formula for approximating the form-factor from an area A 2 to a differential area dA 1 : dF dA 1 A 2 = dA 1 r 2 + A 2 The accuracy can be improved by ray tracing more sample points on A 2 .
Reference: [W94] <author> S.Whitman, </author> <title> "Dynamic Load Balancing for Parallel Polygon Rendering," </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> No.4, </volume> <year> 1994, </year> <month> pp.41-48. </month>
Reference-contexts: And a larger patch takes the average radiosity of the smaller ones. 4.4 Rendering and display The image rendering and display usually cost a few seconds in a workstation, which is acceptable with most of computer aided design applications. There is some research work on parallel rendering <ref> [MC+94, OHA93, W94] </ref> for larger data sets, which we have not considered in this paper. 5 Processor Assignment for Superpatches Our goal is to distribute the amount of dynamic computation evenly among all processors. <p> The stage of image rendering can also be parallelized to get a faster rate of interaction and currently we can make use of some of the existing parallel polygon rendering algorithms <ref> [MC+94, OHA93, W94] </ref>. 18
Reference: [YP95] <author> Y.Yu and Q.Peng, </author> <title> "Multiresolution B-spline Radiosity," </title> <journal> Eurographics'95, pp.C285-C298. </journal> <volume> 20 </volume>
Reference-contexts: That is actually not true for each patch with a large area. To achieve a good accuracy, adaptive meshing was integrated with radiosity in <ref> [CG+86, HSA91, LTG92, LTG93, PJ94, YP95] </ref>. In this approach, each patch derived from the initial subdivision (meshing) might be further refined during the course of the computation if the accuracy of each patch radiosity does not meet the standard.
References-found: 20

