URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/Neural-Computation-94.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Title: On the Convergence of Stochastic Iterative Dynamic Programming Algorithms  
Author: Tommi Jaakkola Michael I. Jordan Satinder P. Singh 
Address: Amherst  
Affiliation: Department of Brain and Cognitive Sciences Massachusetts Institute of Technology  Department of Computer Science University of Massachusetts at  
Abstract: This project was supported in part by a grant from the McDonnell-Pew Foundation, by a grant from ATR Human Information Processing Research Laboratories, by a grant from Siemens Corporation, and by grant N00014-90-J-1942 from the Office of Naval Research. The project was also supported by NSF grant ASC-9217041 in support of the Center for Biological and Computational Learning at MIT, including funds provided by DARPA under the HPCC program. Michael I. Jordan is a NSF Presidential Young Investigator. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aoki, M. </author> <year> (1967). </year> <title> Optimization of Stochastic Systems. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <year> (1993). </year> <title> Learning to act using real-time dynamic programming. </title> <note> Submitted to: AI Journal. </note>
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C.J.C.H. </author> <year> (1990). </year> <title> Sequential decision problems and neural networks. </title> <editor> In D. Touretzky (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 2, </volume> <pages> pp. 686-693. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Bertsekas, D. P., & Tsitsiklis, J. N. </author> <year> (1989). </year> <title> Parallel and Distributed Computation: Numerical Methods. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher> <address> 29 Dayan, P. </address> <year> (1992). </year> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8, </booktitle> <pages> 341-362. </pages>
Reference-contexts: When the future costs are not discounted (fl = 1) but the chain is absorbing and all policies lead to the 12 terminal state w.p.1 there still exists a weighted maximum norm with respect to which T is a contraction mapping <ref> (see e.g. Bertsekas & Tsitsiklis, 1989) </ref> thereby forcing the contraction of EfF t (i; u)g.
Reference: <author> Dayan, P., & Sejnowski, T. J. </author> <year> (1993). </year> <title> TD() converges with probability 1. </title> <booktitle> CNL, The Salk Institute, </booktitle> <address> San Diego, CA. </address>
Reference: <author> Dvoretzky, A. </author> <year> (1956). </year> <title> On stochastic approximation. </title> <booktitle> Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability. </booktitle> <publisher> University of California Press. </publisher>
Reference-contexts: Except for the appearance of fi n (x) this is a standard result. With the above definitions convergence follows directly from Dvoretzky's extended theorem <ref> (Dvoretzky, 1956) </ref>. Lemma 2 Consider a stochastic iteration X n+1 (x) = G n (X n ; Y n ; x) where G n is a sequence of functions and Y n is a random process.
Reference: <author> Peng J., & Williams R. J. </author> <year> (1993). </year> <title> TD() converges with probability 1. </title> <institution> Department of Computer Science preprint, Northeastern University. </institution>
Reference: <author> Robbins, H., & Monro, S. </author> <year> (1951). </year> <title> A stochastic approximation model. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22, </volume> <pages> 400-407. </pages>
Reference: <author> Ross, S. M. </author> <year> (1970). </year> <title> Applied Probability Models with Optimization Applications. </title> <address> San Francisco: </address> <publisher> Holden-Day. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: Note that the proof covers both the on-line and batch versions. 2 The TD () algorithm The TD () <ref> (Sutton, 1988) </ref> is also a DP-based learning algorithm that is naturally defined in a Markov environment. Unlike Q-learning, however, TD does not involve decision-making tasks but rather predictions about the future costs of an evolving system.
Reference: <author> Tsitsiklis J. N. </author> <year> (1993). </year> <title> Asynchronous stochastic approximation and Q-learning. </title> <note> Submitted to: Machine Learning. </note>
Reference: <author> Watkins, C.J.C.H. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> PhD Thesis, </type> <institution> University of Cambridge, </institution> <address> England. </address> <note> 30 Watkins, </note> <author> C.J.C.H, & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 279-292. </pages>
Reference-contexts: i t + flc i t+1 + fl 2 c i t+2 + : : : + fl n1 c i t+n1 + fl n V t (i t+n ) (13) The expected value of this can be shown to be a strictly better estimate than the current estimate is <ref> (Watkins, 1989) </ref>. In the undiscounted case this holds only when n is larger than some chain-dependent constant. <p> Furthermore, we have introduced a new parameter which affects the trade-off between the bias and variance of the estimate <ref> (Watkins, 1989) </ref>. An increase in puts more weight on less biased estimates with higher variances and thus the bias in V t decreases at the expense of a higher variance. The mathematical convenience of using the geometric average can be seen as follows.
Reference: <author> Werbos, P. </author> <year> (1992). </year> <title> Approximate dynamic programming for real-time control and neural modeling. </title> <editor> In D. A. White and D. A. Sofge, (Eds.), </editor> <booktitle> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, </booktitle> <pages> pp. 493-525. </pages> <address> New York: </address> <publisher> Van Nostrand Reinhold. </publisher> <pages> 31 </pages>
References-found: 14

