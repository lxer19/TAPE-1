URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-89-854/CS-TR-89-854.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-89-854/
Root-URL: http://www.cs.wisc.edu
Title: Chained Declustering: A New Availability Strategy for Multiprocssor Database machines  
Author: Hui-I Hsiao David J. DeWitt 
Note: This research was partially supported by the Defense Advanced Research Projects Agency under contract N00039-86-C-0578, by the National Science Foundation under grants DCR-8512862, MCS82-01870, and MCS81-05904, and by a Digital Equipment Corporation External Research Grant.  
Address: Wisconsin  
Affiliation: Computer Sciences Department University of  
Abstract-found: 0
Intro-found: 1
Reference: [Anon85] <editor> Anon et. al, </editor> <title> "A Measure of Transaction Processing Power," </title> <type> TR# 85.1, </type> <institution> Tandem Computer, </institution> <address> Cuper-tino, CA, </address> <year> 1985. </year> <note> hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 16 Except for RAID. 29 </note>
Reference: [Bitt88] <author> Bitton, D. and J. Gray, </author> <title> "Disk Shadowing," </title> <booktitle> Proceedings of VLDB, </booktitle> <address> Los Angeles, </address> <month> August </month> <year> 1988. </year>
Reference: [Borr81] <author> Borr, A., </author> <title> "Transaction Monitoring in Encompass [TM]: </title> <booktitle> Reliable Distributed Transaction Processing," Proceedings of VLDB, </booktitle> <year> 1981. </year>
Reference-contexts: 1. Introduction While a number of solutions have been proposed for increasing the availability and reliability of computer systems, the most commonly used technique involves the replication of processors and mass storage <ref> [Borr81, Jone83, Kast83] </ref>. Some systems go one step further replicating, not only hardware components, but also software modules, so that when a hardware or software module fails, the redundant software modules can continue running the application software [Borr81]. <p> Some systems go one step further replicating, not only hardware components, but also software modules, so that when a hardware or software module fails, the redundant software modules can continue running the application software <ref> [Borr81] </ref>. The result is that application programs are isolated from almost all forms of failures. For database applications, the availability of disk-resident data files is perhaps the major concern. One approach for obtaining high availability is to replicate data items on separate disks attached to separate processors [Borr 81, Tera85]. <p> Our conclusions and future research directions are contained in Section 6. 2. Related Availability Strategies In this section, we briefly describe several existing techniques for improving data availability including the use of mirrored disks <ref> [Borr81] </ref>, data clustering [Tera85], and disk arrays with redundant check information [Patt88]. The mirrored disk and declustering strategies employed, respectively, by Tandem and Teradata, both maintain two identical copies of each relation. <p> In the sections below, we will describe Tandem's mirrored disk scheme, Teradata's data clustering scheme, and RAID's data placement scheme in additional detail. 2.1. Tandem's Mirrored Disks Architecture The hardware structure of a Tandem system <ref> [Borr81] </ref> consists of one or more clusters that are linked together by a token ring. Each cluster contains 2 to 16 processors with multiple disk drives. The processors within a cluster are interconnected with a dual high speed (20 Mbyte/sec) bus.
Reference: [Care85] <author> Carey, M., Livny, M., and H. Lu, </author> <title> "Dynamic Task Allocation in a Distributed Database System," </title> <booktitle> Proceedings of the 5th International Conference on Distributed Computer Systems, </booktitle> <address> Denver, </address> <month> May </month> <year> 1985. </year>
Reference: [Care86] <author> Carey, M. and H. Lu, </author> <title> "Load Balancing in a Locally Distributed Database System," </title> <booktitle> Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <year> 1986. </year>
Reference: [Cope88] <author> Copeland, G., Alexander, W., Boughter, E., and T. Keller, </author> <title> "Data Placement in Bubba," </title> <booktitle> Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Chicago, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: The mirrored disk and declustering strategies employed, respectively, by Tandem and Teradata, both maintain two identical copies of each relation. Bubba <ref> [Cope88] </ref> also employs two copies of each relation but stores the second copy as a combination of inverted indices over the first copy plus a remainder file containing the uninverted attributes.
Reference: [DeWi86] <author> DeWitt, D., Gerber, R., Graefe, G., Heytens, M., Kumar, K., and M. Muralikrishna, </author> <title> "GAMMA-A High Performance Dataflow Database Machine," </title> <booktitle> Proceedings of the 1986 VLDB Conference, </booktitle> <address> Japan, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: In the normal mode of operation, the application of both intra and inter query parallelism has proven successful at improving the performance of database management system software <ref> [Tera85, DeWi86, Livn87, Tand87, DeWi88] </ref>. However, when a failure occurs, balancing the workload among the remaining processors and disks can become difficult as one or more nodes 2 (processor/disk pairs) must pick up the workload of the component that has failed. <p> In addition, the overall throughput of the system may be drastically reduced as a bottleneck may form. For multiprocessor, shared-nothing database machines with replicated data, the application of horizontal partitioning (ie. declustering) techniques <ref> [Ries78, Tera85, DeWi86, Livn87] </ref> facilitates the successful application of inter and intra query parallelism in the normal mode of operation. None of the existing availability techniques, however, are able to fully 3 balance the workload when failures occur. <p> The chained declustering strategy maintains two physical copies of each relation. The first copy (termed the primary copy) is declustered over all disks in a relation-cluster using one of Gamma's three partitioning strategies (hashed, range, or round-robin) <ref> [DeWi86] </ref>. The second copy (termed the backup copy) is declustered using the same partitioning strategy but the corresponding fragments from both copies are stored on different nodes. <p> In reality, queries cannot simply access an arbitrary fraction of a data fragment because data may be clustered on certain attribute values, indices may exist, and the query optimizers may generate different access plans. For example, Gamma <ref> [DeWi86] </ref> provides the user with three declustering alternatives: range, hash, and round-robin partitioning.
Reference: [DeWit88] <author> DeWitt, D., Ghandeharizadeh, S., and D. Schneider, </author> <title> "A Performance Analysis of the Gamma Database Machine," </title> <booktitle> Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Chicago, </address> <month> May </month> <year> 1988. </year>
Reference: [Eage86] <author> Eager, D., Lazowska, E., and J. Zahorjan, </author> <title> "Adaptive Load Sharing in Homogeneous Distributed Systems," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. SE-12, No. 5, </volume> <month> May </month> <year> 1986. </year>
Reference: [Gibs89] <author> Gibson, G, Hellerstein, L., Karp, R., Katz, R., and D. Patterson, </author> <title> "Failure Correction Techniques for Large Disk Arrays" Proceedings of ASPLOS III, </title> <address> Boston, MA., </address> <month> March </month> <year> 1989. </year>
Reference-contexts: RAID's Data Storage Scheme In the RAID data storage scheme <ref> [Patt88, Gibs89] </ref>, an array of small, inexpensive disks is used as a single storage unit termed a group. Instead of replicating data on different drives, this strategy stores check (parity) bytes for recovering from both random single byte failures and single disk failures.
Reference: [Gray78] <author> Gray, J., </author> <booktitle> "Notes on Database Operating Systems," In Operating Systems: An Advanced Course, </booktitle> <volume> Vol. 60, </volume> <booktitle> Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1978. </year>
Reference: [Jone83] <author> Jones, S., </author> <title> "The Synapse Approach to High System and Database Availability," </title> <journal> Database Engineering, </journal> <volume> Vol. 6, No. 2, </volume> <month> June </month> <year> 1983. </year>
Reference-contexts: 1. Introduction While a number of solutions have been proposed for increasing the availability and reliability of computer systems, the most commonly used technique involves the replication of processors and mass storage <ref> [Borr81, Jone83, Kast83] </ref>. Some systems go one step further replicating, not only hardware components, but also software modules, so that when a hardware or software module fails, the redundant software modules can continue running the application software [Borr81].
Reference: [Kast83] <author> Kastner, </author> <title> P.C., "A Fault-Tolerant Transaction Processing Environment," </title> <journal> Database Engineering, </journal> <volume> Vol. 6, No. 2, </volume> <month> June </month> <year> 1983. </year>
Reference-contexts: 1. Introduction While a number of solutions have been proposed for increasing the availability and reliability of computer systems, the most commonly used technique involves the replication of processors and mass storage <ref> [Borr81, Jone83, Kast83] </ref>. Some systems go one step further replicating, not only hardware components, but also software modules, so that when a hardware or software module fails, the redundant software modules can continue running the application software [Borr81].
Reference: [Katz78] <author> Katzman, J., </author> <title> "A Fault-Tolerant Computing System," </title> <booktitle> Proceedings of the 11th Hawaii Conference on System Sciences, </booktitle> <month> January </month> <year> 1978. </year>
Reference: [Kim86] <author> Kim, M., </author> <title> "Synchronized Disk Interleaving," </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-35, No. 11, </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: An alternative approach is to use an array of disks and store the data and the redundant error detection and correction information (usually, parity bytes) on different disk drives <ref> [Kim86, Patt88] </ref>. When errors are discovered, the redundant information can be used to restore the data and the application program can continue using the data with minimum interruption. The advantage of the first approach is higher availability and better performance for transaction-oriented database applications. <p> Bubba [Cope88] also employs two copies of each relation but stores the second copy as a combination of inverted indices over the first copy plus a remainder file containing the uninverted attributes. RAID and SDI <ref> [Kim86] </ref> employ error correcting techniques that can be used to rebuild the database in the event of a disk failure. Each of these strategies is able to sustain a single disk failure and thus provides resiliency to disk failures. <p> If the two bits match, the failed bit is a 0. Otherwise, it is a 1. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 6 [Patt88] presents 5 levels of RAID. The scheme described here is the level 5 RAID. Except for the end-of-fragment parity byte, level 3 RAID is the same as the SDI <ref> [Kim86] </ref>. 8 C 1 (containing Data and Checks) 5 Disks 54321 block 0 block 1 block 2 block 3 block 4 block 5 S 0,1 S 0,2 S 0,3 S 0,4 One potential concern with the RAID scheme is its inherent reliability.
Reference: [Lind86] <author> Lindsay, B., P. Seilinger, C. Galtieri, J. Gray, R. Lorie, T. Price, F. Putzulo, I. Traiger, and B. Wade, </author> <title> "Notes on Distributed Databases," In Distributed Databases, </title> <editor> Drattan and Poole, Eds., </editor> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1980. </year>
Reference: [Livn82] <author> Livny, M. and M. Melman, </author> <title> "Load Balancing in Homogeneous Broadcast Distributed Systems," </title> <booktitle> Proceedings of ACM Computer Network Performance Symposium, </booktitle> <month> April </month> <year> 1982. </year>
Reference: [Livn87] <author> Livny, M., S. Khoshafian, and H. Boral, </author> <title> "Multi-Disk Management," </title> <booktitle> Proceedings of ACM SIG-METRICS conference, </booktitle> <address> Alberta, Canada, </address> <year> 1987. </year>
Reference-contexts: In the normal mode of operation, the application of both intra and inter query parallelism has proven successful at improving the performance of database management system software <ref> [Tera85, DeWi86, Livn87, Tand87, DeWi88] </ref>. However, when a failure occurs, balancing the workload among the remaining processors and disks can become difficult as one or more nodes 2 (processor/disk pairs) must pick up the workload of the component that has failed. <p> In addition, the overall throughput of the system may be drastically reduced as a bottleneck may form. For multiprocessor, shared-nothing database machines with replicated data, the application of horizontal partitioning (ie. declustering) techniques <ref> [Ries78, Tera85, DeWi86, Livn87] </ref> facilitates the successful application of inter and intra query parallelism in the normal mode of operation. None of the existing availability techniques, however, are able to fully 3 balance the workload when failures occur.
Reference: [Patt88] <author> Patterson, D. A., G. Gibson, and R. H. Katz, </author> <title> "A Case for Redundant Arrays of Inexpensive Disks (RAID)," </title> <booktitle> Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Chicago, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: An alternative approach is to use an array of disks and store the data and the redundant error detection and correction information (usually, parity bytes) on different disk drives <ref> [Kim86, Patt88] </ref>. When errors are discovered, the redundant information can be used to restore the data and the application program can continue using the data with minimum interruption. The advantage of the first approach is higher availability and better performance for transaction-oriented database applications. <p> Our conclusions and future research directions are contained in Section 6. 2. Related Availability Strategies In this section, we briefly describe several existing techniques for improving data availability including the use of mirrored disks [Borr81], data clustering [Tera85], and disk arrays with redundant check information <ref> [Patt88] </ref>. The mirrored disk and declustering strategies employed, respectively, by Tandem and Teradata, both maintain two identical copies of each relation. <p> RAID's Data Storage Scheme In the RAID data storage scheme <ref> [Patt88, Gibs89] </ref>, an array of small, inexpensive disks is used as a single storage unit termed a group. Instead of replicating data on different drives, this strategy stores check (parity) bytes for recovering from both random single byte failures and single disk failures. <p> The resulting byte is compared bitwise with the corresponding byte from the check sector for the block. If the two bits match, the failed bit is a 0. Otherwise, it is a 1. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 6 <ref> [Patt88] </ref> presents 5 levels of RAID. The scheme described here is the level 5 RAID.
Reference: [Ries78] <author> Ries, D. and Epstein, R., </author> <title> "Evaluation of Distribution Criteria for Distributed Database Systems," </title> <type> UCB/ERL Technical Report M78/22, </type> <institution> UC Berkeley, </institution> <month> May </month> <year> 1978. </year>
Reference-contexts: In addition, the overall throughput of the system may be drastically reduced as a bottleneck may form. For multiprocessor, shared-nothing database machines with replicated data, the application of horizontal partitioning (ie. declustering) techniques <ref> [Ries78, Tera85, DeWi86, Livn87] </ref> facilitates the successful application of inter and intra query parallelism in the normal mode of operation. None of the existing availability techniques, however, are able to fully 3 balance the workload when failures occur.
Reference: [Sale84] <author> Salem, K. and H. Garcia-Molina, </author> <title> "Disk Striping," </title> <type> EECS TR# 332, </type> <institution> Princeton University, Prince-ton, NJ, </institution> <month> December </month> <year> 1984. </year>
Reference: [Schu88] <author> Schulze, M, G. Gibson, R. Katz, and D. Patterson, </author> <title> "How Reliable Is A RAID?" Unpublished Technical Report, </title> <year> 1988. </year>
Reference-contexts: When large numbers of disks are connected to form a single system, the increased number of disks and support hardware (cables, controllers, etc.) will increase the probability of a component failure which in turn increases the probability of unavailable data. This concern is supported by the results in <ref> [Schu88] </ref>, where for a 56-disk RAID system, the probability of data being unavailable as the result of a non-media failure was shown to be as much as a factor of 30 higher than if only media failures are considered. 3.
Reference: [Ston75] <author> Stonebraker, M., </author> <title> "Implementation of Integrity Constraints and views by Query Modification," </title> <booktitle> Proceedings of the SIGMOD Workshop on Management of Data, </booktitle> <address> San Jose, Calif., </address> <month> May </month> <year> 1975. </year> <month> 30 </month>
Reference-contexts: The problem now is to design a load balancing algorithm for the chained declustering mechanism that handles all possible combinations of partitioning methods, storage organizations, and access plans. The keys to the solution are the notion of a responsible range for indexed attributes, the use of query modification techniques <ref> [Ston75] </ref>, and the availability of an extent map for relations stored as a heap. In the following sections, we will describe these techniques in more detail and illustrate how they are used. 4.1.
Reference: [Ston86] <author> Stonebraker, M., </author> <title> "The Case for Shared Nothing," </title> <journal> Database Engineering, </journal> <volume> Vol. 9, No. 1, </volume> <year> 1986. </year>
Reference-contexts: While the identical copy scheme (which is employed by both Tandem and Teradata in their systems) has been used commercially, the suitability of the second approach for database applications is still under investigation. Throughout this paper, we focus our attention on multiprocessor database machines that employ a "shared-nothing" architecture <ref> [Ston86] </ref>. In such systems, each processor has its own main memory, processors communicate through an interconnection network, and one or more disks are connected to each processor.
Reference: [Tand87] <author> Tandem Database Group, </author> <title> "NonStop SQL, A Distributed, High-Performance, High-Reliability Implementation of SQL," </title> <booktitle> Workshop on High Performance Transaction Systems, Asilomar, </booktitle> <address> CA, </address> <month> SEP </month> <year> 1987. </year>
Reference-contexts: In the normal mode of operation, the application of both intra and inter query parallelism has proven successful at improving the performance of database management system software <ref> [Tera85, DeWi86, Livn87, Tand87, DeWi88] </ref>. However, when a failure occurs, balancing the workload among the remaining processors and disks can become difficult as one or more nodes 2 (processor/disk pairs) must pick up the workload of the component that has failed.
Reference: [Tera85] <author> Teradata, </author> <title> "DBC/1012 Database Computer System Manual Release 2.0," Document No. </title> <institution> C10-0001-02, Teradata Corp., </institution> <month> NOV </month> <year> 1985. </year>
Reference-contexts: The result is that application programs are isolated from almost all forms of failures. For database applications, the availability of disk-resident data files is perhaps the major concern. One approach for obtaining high availability is to replicate data items on separate disks attached to separate processors <ref> [Borr 81, Tera85] </ref>. When one copy fails, the other copy can continue to be used. Unless both copies fail at the same time, the failure of a single copy will be transparent to users and no interruption of service will occur. <p> In the normal mode of operation, the application of both intra and inter query parallelism has proven successful at improving the performance of database management system software <ref> [Tera85, DeWi86, Livn87, Tand87, DeWi88] </ref>. However, when a failure occurs, balancing the workload among the remaining processors and disks can become difficult as one or more nodes 2 (processor/disk pairs) must pick up the workload of the component that has failed. <p> In addition, the overall throughput of the system may be drastically reduced as a bottleneck may form. For multiprocessor, shared-nothing database machines with replicated data, the application of horizontal partitioning (ie. declustering) techniques <ref> [Ries78, Tera85, DeWi86, Livn87] </ref> facilitates the successful application of inter and intra query parallelism in the normal mode of operation. None of the existing availability techniques, however, are able to fully 3 balance the workload when failures occur. <p> Our conclusions and future research directions are contained in Section 6. 2. Related Availability Strategies In this section, we briefly describe several existing techniques for improving data availability including the use of mirrored disks [Borr81], data clustering <ref> [Tera85] </ref>, and disk arrays with redundant check information [Patt88]. The mirrored disk and declustering strategies employed, respectively, by Tandem and Teradata, both maintain two identical copies of each relation. <p> If P2 is already fully utilized when the failure occurs, the response time for queries that need to access data on either pair of drives may double if the system is CPU bound. 6 2.2. Teradata's Data Clustering Scheme In the Teradata database machine <ref> [Tera85] </ref>, the processors (and the disk drives attached to them) are subdivided into clusters. <p> The tension between these two objectives is illustrated by Figures 14 and 15 (which have been extracted from <ref> [Tera85] </ref>). Figure 14 illustrates that, for a system containing 32 nodes, as the cluster size is increased from 2 to 32 processors the mean time between cluster failures (MTTF c decreases dramatically. In other words, an increase in the cluster size reduces the probability that all data will be available.
References-found: 26

