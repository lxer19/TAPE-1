URL: http://www.gia.ist.utl.pt/~pedrod/msl96.ps.gz
Refering-URL: http://www.gia.ist.utl.pt/~pedrod/
Root-URL: 
Email: pedrod@ics.uci.edu  
Title: From Instances to Rules: A Comparison of Biases  
Author: Pedro Domingos 
Web: http://www.ics.uci.edu/~pedrod  
Address: Irvine, California 92717, U.S.A.  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract: RISE is an algorithm that combines rule induction and instance-based learning (IBL). It has been empirically verified to achieve higher accuracy than state-of-the-art representatives of its parent approaches in a large number of benchmark problems. This paper investigates the conditions under which RISE's bias will be more appropriate than that of the pure approaches, through experiments in carefully controlled artificial domains. RISE's advantage compared to pure rule induction increases with increasing concept specificity. RISE's advantage compared to pure IBL is greater when the relevance of features is context-dependent (i.e., when some of the features used to describe examples are relevant only given other features' values). The paper also reports lesion studies and other empirical observations showing that RISE's good performance is indeed due to its combination of rule induction and IBL, and not to the presence of either component alone. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., and Bankert, R. L. </author> <year> 1994. </year> <title> Feature selection for case-based classification of cloud types: An empirical comparison. </title> <booktitle> In Proceedings of the 1994 AAAI Workshop on Case-Based Reasoning, </booktitle> <pages> 106-112. </pages> <address> Seattle, WA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Many variations of these exist (e.g., <ref> (Aha & Bankert 1994) </ref>). Their use can have a large positive impact on accuracy. However, all of these algorithms have the common characteristic that they ignore the fact that some features may be relevant only in context (i.e., given the values of other features).
Reference: <author> Aha, D. W.; Kibler, D.; and Albert, M. K. </author> <year> 1991. </year> <title> Instance-based learning algorithms. </title> <booktitle> Machine Learning 6 </booktitle> <pages> 37-66. </pages>
Reference-contexts: Introduction Rule induction (either performed directly (Michal-ski 1983) or by means of decision trees (Quinlan 1993a)) and instance-based learning <ref> (Aha, Kibler, & Albert 1991) </ref> (forms of which are also known as case-based, memory-based, exemplar-based, lazy, local, and nearest-neighbor learning) constitute two of the leading approaches to concept and classification learning. Rule-based methods discard the individual training examples, and remember only abstractions formed from them.
Reference: <author> Branting, L. K., and Porter, B. W. </author> <year> 1991. </year> <title> Rules and precedents as complementary warrants. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> 3-9. </pages> <address> Anaheim, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Clark, P., and Boswell, R. </author> <year> 1991. </year> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proceedings of the Sixth European Working Session on Learning, </booktitle> <pages> 151-163. </pages> <address> Porto, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: that of pure rule inducers and that of pure instance-based learners, has been observed to lead to improvements in accuracy in a large number of domains from the UCI repository (Murphy & Aha 1995), resulting in significantly better overall results than either "parent" bias (with C4.5RULES (Quinlan 1993a) and CN2 <ref> (Clark & Boswell 1991) </ref> being used as representatives of rule induction, and PEBLS (Cost & Salzberg 1993) as a representative of IBL). RISE is described in greater detail in the next section.
Reference: <author> Cost, S., and Salzberg, S. </author> <year> 1993. </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <booktitle> Machine Learning 10 </booktitle> <pages> 57-78. </pages>
Reference-contexts: observed to lead to improvements in accuracy in a large number of domains from the UCI repository (Murphy & Aha 1995), resulting in significantly better overall results than either "parent" bias (with C4.5RULES (Quinlan 1993a) and CN2 (Clark & Boswell 1991) being used as representatives of rule induction, and PEBLS <ref> (Cost & Salzberg 1993) </ref> as a representative of IBL). RISE is described in greater detail in the next section. <p> The essential idea behind VDM-type metrics is that two values should be considered similar if they make similar class predictions, and dissimilar if their predictions diverge. This has been found to give good results in several domains <ref> (Cost & Salzberg 1993) </ref>. Notice that, in particular, SV DM (x i ; x j ) is always 0 if i = j.
Reference: <author> DeGroot, M. H. </author> <year> 1986. </year> <title> Probability and Statistics. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, 2nd edition. </publisher>
Reference-contexts: The second line considers only those domains in which the observed difference is significant at the 5% level or lower. The third line shows the global significance levels obtained by applying a Wilcoxon signed-ranks test <ref> (DeGroot 1986) </ref> to the 30 accuracy differences observed. The average accuracy across all domains is a measure of debatable significance, but it is often reported, and is shown on the last line. The first specific question addressed was whether Table 4: Summary of lesion study results.
Reference: <author> Devijver, P. A., and Kittler, J. </author> <year> 1982. </year> <title> Pattern Recognition: A Statistical Approach. </title> <address> Englewood Cliffs, N.J.: Prentice/Hall. </address>
Reference-contexts: Several algorithms have been proposed for this purpose (see (Kittler 1986) for a survey), of which two of the 3 See the previous footnote regarding this test. most widely known are forward sequential search (FSS) and backward sequential search (BSS) <ref> (Devijver & Kit-tler 1982) </ref>. Many variations of these exist (e.g., (Aha & Bankert 1994)). Their use can have a large positive impact on accuracy.
Reference: <author> Domingos, P. </author> <year> 1995a. </year> <title> The RISE 2.0 system: A case study in multistrategy learning. </title> <type> Technical Report 95-2, </type> <institution> Department of Information and Computer Science, University of California at Irvine, </institution> <address> Irvine, CA. </address>
Reference: <author> Domingos, P. </author> <year> 1995b. </year> <title> Rule induction and instance-based learning: A unified approach. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1226-1232. </pages> <address> Montreal, Canada: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A rule's extension, like an instance's, then becomes the set of examples that it is the most similar rule to, and thus there is also no necessary semantic distinction between a rule and an instance. The RISE algorithm <ref> (Domingos 1995b) </ref> is a practical, computationally efficient realization of this idea. 1 1 Obviously, it is not the only possible approach to uni RISE starts with a rule base that is simply the train-ing set itself, and gradually generalizes each rule to cover neighboring instances, as long as this does not <p> With this optimization, RISE's worst-case time complexity has been shown to be quadratic in the number of examples and the number of attributes, which is comparable to that of commonly-used rule induction algorithms <ref> (Domingos 1995b) </ref>. Classification At performance time, classification of each test example is performed by finding the nearest rule to it, and assigning the example to the rule's class.
Reference: <author> Golding, A. R., and Rosenbloom, P. S. </author> <year> 1991. </year> <title> Improving rule-based systems through case-based reasoning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> 22-27. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Holte, R. C.; Acker, L. E.; and Porter, B. W. </author> <year> 1989. </year> <title> Concept learning and the problem of small disjuncts. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> 813-818. </pages> <address> De-troit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Rule induction systems often succeed in identifying small sets of highly predictive features, and, crucially, these features can vary from example to example. However, these methods can have trouble recognizing exceptions, or in general small, low-frequency sections of the space; this is known as the "small disjuncts problem" <ref> (Holte, Acker, & Porter 1989) </ref>.
Reference: <author> Kittler, J. </author> <year> 1986. </year> <title> Feature selection and extraction. </title>
Reference-contexts: RISE as IBL High sensitivity to irrelevant features has long been recognized as IBL's main problem. A natural solution is identifying the irrelevant features, and discarding them before storing the examples for future use. Several algorithms have been proposed for this purpose (see <ref> (Kittler 1986) </ref> for a survey), of which two of the 3 See the previous footnote regarding this test. most widely known are forward sequential search (FSS) and backward sequential search (BSS) (Devijver & Kit-tler 1982). Many variations of these exist (e.g., (Aha & Bankert 1994)).
Reference: <editor> In Young, T. Y., and Fu, K. S., eds., </editor> <booktitle> Handbook of Pattern Recognition and Image Processing. </booktitle> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference: <author> Michalski, R. S.; Mozetic, I.; Hong, J.; and Lavrac, N. </author> <year> 1986. </year> <title> The multi-purpose incremental learning system AQ15 and its testing application to three medical domains. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> 1041-1045. </pages> <address> Philadel-phia, PA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: second observation is that rules can be matched approximately, as instances are in an instance-based classifier (i.e., a rule can match an example if it is the closest one to it according to some similarity-computing procedure, even if the example does not logically satisfy all of the rule's preconditions; see <ref> (Michalski et al. 1986) </ref>). A rule's extension, like an instance's, then becomes the set of examples that it is the most similar rule to, and thus there is also no necessary semantic distinction between a rule and an instance.
Reference: <author> Michalski, R. S. </author> <year> 1983. </year> <title> A theory and methodology of inductive learning. </title> <booktitle> Artificial Intelligence 20 </booktitle> <pages> 111-161. </pages>
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1995. </year> <title> UCI repository of machine learning databases. Machine-readable data repository, </title> <institution> Department of Information and Computer Science, University of California at Irvine, </institution> <address> Irvine, CA. </address>
Reference-contexts: RISE's bias, which is in effect intermediate between that of pure rule inducers and that of pure instance-based learners, has been observed to lead to improvements in accuracy in a large number of domains from the UCI repository <ref> (Murphy & Aha 1995) </ref>, resulting in significantly better overall results than either "parent" bias (with C4.5RULES (Quinlan 1993a) and CN2 (Clark & Boswell 1991) being used as representatives of rule induction, and PEBLS (Cost & Salzberg 1993) as a representative of IBL). <p> Thus rules with high apparent accuracy are favored only if they also have high statistical support, i.e., if that apparent accuracy is not simply the result of a small sample. Lesion Studies Lesion studies were conducted using 30 datasets from the UCI repository <ref> (Murphy & Aha 1995) </ref>. Several aspects of the algorithm's performance were also measured. The results are shown in Table 3. Superscripts indicate significance levels for the accuracy differences between systems, using a one-tailed paired t Table 3: Results of lesion studies, and performance monitoring.
Reference: <author> Niblett, T. </author> <year> 1987. </year> <title> Constructing decision trees in noisy domains. </title> <booktitle> In Proceedings of the Second Euro-pean Working Session on Learning, </booktitle> <pages> 67-78. </pages> <address> Bled, Yu-goslavia: Sigma. </address>
Reference-contexts: When two or more rules are equally close to a test example, the rule that was most accurate on the training set wins. So as to not unduly favor more specific rules, the Laplace-corrected accuracy is used <ref> (Niblett 1987) </ref>: LAcc (R) = N corr (R) + 1 N won (R) + C where R is any rule, C is the number of classes, N won (R) is the total number or examples won by R, N corr (R) is the number of examples among those that R correctly
Reference: <author> Quinlan, J. R. </author> <year> 1993a. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Introduction Rule induction (either performed directly (Michal-ski 1983) or by means of decision trees <ref> (Quinlan 1993a) </ref>) and instance-based learning (Aha, Kibler, & Albert 1991) (forms of which are also known as case-based, memory-based, exemplar-based, lazy, local, and nearest-neighbor learning) constitute two of the leading approaches to concept and classification learning. Rule-based methods discard the individual training examples, and remember only abstractions formed from them. <p> in effect intermediate between that of pure rule inducers and that of pure instance-based learners, has been observed to lead to improvements in accuracy in a large number of domains from the UCI repository (Murphy & Aha 1995), resulting in significantly better overall results than either "parent" bias (with C4.5RULES <ref> (Quinlan 1993a) </ref> and CN2 (Clark & Boswell 1991) being used as representatives of rule induction, and PEBLS (Cost & Salzberg 1993) as a representative of IBL). RISE is described in greater detail in the next section. <p> A good operational measure of it is the average length of the rules comprising the correct description: rules with more conditions imply a more specific concept. The dependent variables are the out-of-sample accuracies of RISE and of a "divide and conquer" algorithm; C4.5RULES <ref> (Quinlan 1993a) </ref> was used as the latter. Concepts defined as Boolean functions in disjunctive normal form were used as targets. The datasets were composed of 100 examples described by 16 attributes. The average number of literals C in each disjunct comprising the concept was varied from 1 to 16.
Reference: <author> Quinlan, J. R. </author> <year> 1993b. </year> <title> Combining instance-based and model-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 236-243. </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Stanfill, C., and Waltz, D. </author> <year> 1986. </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM 29 </journal> <pages> 1213-1228. </pages>
Reference-contexts: The distance measure used is a combination of Euclidean distance for numeric attributes, and a simplified version of Stan-fill and Waltz's value difference metric for symbolic attributes <ref> (Stanfill & Waltz 1986) </ref>. Let E = (e 1 ; e 2 ; . . . ; e A ; c E ) be an example with value e i for the ith attribute and class c E .
References-found: 20

