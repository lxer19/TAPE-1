URL: http://www.ultimode.com/papers/ensemble.ps.Z
Refering-URL: http://www.ultimode.com/stevew/papers.html
Root-URL: 
Title: Speech, Handwriting, and Signal Processing section. Poster or oral presentation acceptable. Ensemble Methods for Phoneme Classification  
Author: Steve Waterhouse Gary Cook 
Keyword: Speech Recognition, Boosting, Mixtures of Experts.  
Address: Cambridge CB2 1PZ, England,  
Affiliation: Cambridge University Engineering Department  
Note: Submitted to Neural Information Processing Systems 9, to  
Email: Email: srw1001@eng.cam.ac.uk, gdc@eng.cam.ac.uk  
Phone: Tel: [+44] 1223 332754  
Abstract: In this paper we investigate a number of ensemble methods for improving the performance of phoneme classification for use in a speech recognition system. We discuss boosting and mixtures of experts, both in isolation and in combination. We present results on an isolated word database. The results show that principled ensemble methods such as boosting and mixtures provide superior performance to more naive ensemble methods. When used in combination, boosting and mixtures provide a further improvement in performance.
Abstract-found: 1
Intro-found: 1
Reference: <author> Cook, G. & Robinson, A. </author> <year> (1996), </year> <title> Boosting the performance of connectionist large -vocabulary speech recognition, </title> <booktitle> in `International Conference on Spoken Language Processing'. </booktitle>
Reference-contexts: It is therefore paramount that well principled ensemble methods are developed to effectively exploit these data sets. Previous work has shown how a novel boosting procedure based on utterance selection can be used to increase the performance of the recurrent network acoustic model <ref> (Cook & Robinson 1996) </ref>. In this work we investigate a combined boosting and mixtures-of-experts approach to improving the performance of MLP acoustic models. Results are presented for phonetic classification on a small isolated digit database. <p> In addition, by combining the techniques via bootstrapping mixtures using the boosted networks we improve the performance of the models further. Previous work has presented results for the recurrent network at the word level which out perform the baseline system <ref> (Cook & Robinson 1996) </ref>. We plan to extend the work done here on the multi-layer network to the larger Wall Street Journal Corpus.
Reference: <author> Drucker, H., Cortes, C., Jackel, L., LeCun, Y. & Vapnik, V. </author> <year> (1994), </year> <title> `Boosting and Other Ensemble Methods', </title> <booktitle> Neural Computation 6, </booktitle> <pages> 1289-1301. </pages>
Reference-contexts: Further experiments have shown that boosting also results in a significant reduction in error rate compared to an ensemble of networks, each trained on the same data <ref> (Drucker, Cortes, Jackel, LeCun & Vapnik 1994) </ref>. The boosting procedure is as follows: train a network on a randomly chosen subset of the available training data. This network is then used to filter the remaining training data to produce a training set for a second network. Flip a fair coin.
Reference: <author> Drucker, H., Schapire, R. & Simard, P. </author> <year> (1993), </year> <title> Improving Performance in Neural Networks Using a Boosting Algorithm, </title> <editor> in S. Hanson, J. Cowan & C. Giles, eds, </editor> <booktitle> `Neural Information Processing Systems', </booktitle> <publisher> Morgan Kauffmann, </publisher> <pages> pp. 42-49. </pages>
Reference-contexts: The expectation of ensemble methods is that the member networks pick out different properties present in the data, thus improving the performance when their outputs are combined. The two techniques described here, boosting <ref> (Drucker, Schapire & Simard 1993) </ref> and mixtures of experts (Jacobs, Jordan, Nowlan & Hinton 1991), differ from simple ensemble methods. In boosting, each member of the ensemble is trained on patterns that have been filtered by previously trained members of the ensemble. <p> This has the advantage that only data that is likely to result in improved generalization performance is used for training. Boosting Neural Networks The first practical application of a boosting procedure was for the optical character recognition task <ref> (Drucker et al. 1993) </ref>. An ensemble of feedforward neural networks was trained using supervised learning. Using boosting the authors reported a reduction in error rate on ZIP codes from the United States Postal Service of 28% compared to a single network.
Reference: <author> Hansen, L. & Salamon, P. </author> <year> (1990), </year> <title> `Neural Network Ensembles', </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 12, </journal> <pages> 993-1001. </pages>
Reference-contexts: INTRODUCTION There is now considerable interest in using ensembles or committees of learning machines to improve the performance of the system over that of a single learning machine. In most neural network ensembles, the ensemble members are trained on either the same data <ref> (Hansen & Salamon 1990) </ref> or different subsets of the data (Per-rone & Cooper 1993). The ensemble members typically have different initial conditions and/or different architectures. The subsets of the data may be chosen at random, with prior knowledge or by some principled approach e.g. clustering.
Reference: <author> Hochberg, M., Renals, S., Robinson, A. & Cook, G. </author> <year> (1995), </year> <title> Recent Improvements to the ABBOT Large Vocabulary CSR System, </title> <booktitle> in `ICASSP', </booktitle> <volume> Vol. I, </volume> <pages> pp. 69-72. </pages>
Reference-contexts: Given new acoustic data and the connectionist-HMM framework, the maximum a posteriori word sequence is then extracted using a single pass, start synchronous decoder. A more complete description of the system can be found in <ref> (Hochberg, Renals, Robinson & Cook 1995) </ref>. Acoustic models are trained to classify frames of parameterised speech into 1 of N phonetic categories. Training data sets of the order of 15 million frames are now available and this figure is likely to rise as research into speech recognition develops.
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J. & Hinton, G. E. </author> <year> (1991), </year> <title> `Adaptive mixtures of local experts', </title> <booktitle> Neural Computation 3(1), </booktitle> <pages> 79-87. </pages>
Reference-contexts: The expectation of ensemble methods is that the member networks pick out different properties present in the data, thus improving the performance when their outputs are combined. The two techniques described here, boosting (Drucker, Schapire & Simard 1993) and mixtures of experts <ref> (Jacobs, Jordan, Nowlan & Hinton 1991) </ref>, differ from simple ensemble methods. In boosting, each member of the ensemble is trained on patterns that have been filtered by previously trained members of the ensemble. <p> The approach we use to take account of this temporal nature is to window the input. When selecting data randomly we then take account of the frames adjacent to the input frame. MIXTURES OF EXPERTS The mixture of experts <ref> (Jacobs et al. 1991) </ref> is a different type of ensemble to the two considered so far. The ensemble members or experts are trained with data which is stochastically selected by a gate. The gate in turn learns how to best combine the experts given the data.
Reference: <author> Perrone, M. P. & Cooper, L. N. </author> <year> (1993), </year> <title> When networks disagree: Ensemble methods for hybird neural networks, in `Neural Networks for Speech and Image Processing', </title> <publisher> Chapman-Hall. </publisher>
Reference: <author> Waterhouse, S., Kershaw, D. & Robinson, T. </author> <year> (1996), </year> <title> Smoothed local adaptation of connectionist systems, </title> <booktitle> in `International Conference on Spoken Language Processing'. </booktitle>
Reference-contexts: By using the posterior probabilities to weight the experts and provide targets for the gate, we allow the effective data sets used to train each expert to overlap. This technique has already proved useful in speech recognition using mixtures of recurrent networks <ref> (Waterhouse, Kershaw & Robinson 1996) </ref>. In this paper we consider the use of mixtures of MLPs for phoneme classification. PHONEME ERROR RESULTS ON BELLCORE DIGITS We report the results of experiments on the Bellcore isolated digits database.
References-found: 8

