URL: http://www.wi.leidenuniv.nl/home/joost/esann96.ps.gz
Refering-URL: http://www.wi.leidenuniv.nl/home/joost/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Phone: 3  4  
Title: Constraining of Weights using Regularities  
Author: Joost N. Kok Elena Marchiori ;; Massimo Marchiori Claudio Rossi 
Address: P.O. Box 9512, 2300 RA Leiden, The Netherlands 2 CWI, P.O. Box 94079, 1090 GB Amsterdam, The Netherlands  via Belzoni 7, 35131 Padova, Italy  Venezia, via Torino 155, 30173 Mestre-Venezia, Italy  
Affiliation: 1 Univ. of Leiden,  Univ. of Padova,  Univ. of  
Abstract: In this paper we study how global optimization methods (like genetic algorithms) can be used to train neural networks. We introduce the notion of regularity, for studying properties of the error function that expand the search space in an artificial way. Regularities are used to generate constraints on the weights of the network. In order to find a satisfiable set of constraints we use a constraint logic programming system. Then the training of the network becomes a constrained optimization problem. We also relate the notion of regularity to so-called network transformations.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.N. Kok, E. Marchiori, M. Marchiori, and C. Rossi. </author> <title> Evolutionary training of CLP-constrained neural networks. </title> <booktitle> In Proc. </booktitle> <address> PACT'96. </address>
Reference-contexts: Note that the previous algorithms generate strict orderings on some weights. However, one can replace &lt; by in those cases where the resulting constraints are not satisfiable. We give now some results on the training of neural networks with or without constraints. For this we use a framework <ref> [1] </ref> which integrates the constraint logic programming ECL i PS e by ECRC for the generation of constraints, and GENOCOP [2] as a global optimizer. We did two series of experiments. <p> The neural network is used to predict the next value. In the experiments we used a recurrent network with one input, four hidden units and one output unit: the transfer function is the sigmoid on <ref> [1; 1] </ref> for the hidden units, and a linear one for the output. A typical run of the system with standard error function is shown in Figure 2. We also see here that the "regularities" constraints improve the convergence.
Reference: [2] <author> Z. Michalewicz. </author> <title> Genetic Algorithms + Data Structures = Evolution Programs. </title> <publisher> Springer, </publisher> <year> 1994. </year>
Reference-contexts: We give now some results on the training of neural networks with or without constraints. For this we use a framework [1] which integrates the constraint logic programming ECL i PS e by ECRC for the generation of constraints, and GENOCOP <ref> [2] </ref> as a global optimizer. We did two series of experiments. The first series on a standard data set about Iris flowers, a classic pattern recognition problem with four parameters describing an Iris flower.
Reference: [3] <author> H.J. Sussmann. </author> <title> Uniqueness of the weights for minimal feedforward nets with a given input-output map. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 589-593, </pages> <year> 1992. </year>
Reference: [4] <author> D. Thierens, J. Suykens, J. Vanderwalle, and B. De Moor. </author> <title> Genetic weight optimization of a feedforward neural network controller. </title> <booktitle> In Proc. of the Conf. on Artificial Neural Nets and Genetic Algorithms, </booktitle> <pages> pages 658-663. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
References-found: 4

