URL: http://www.isi.edu/~gil/papers/gil-mlbook3.ps
Refering-URL: http://www.isi.edu/~gil/papers/papers-ml.html
Root-URL: 
Title: Learning by Experimentation: The Operator Refinement Method  
Author: Jaime G. Carbonell and Yolanda Gil 
Note: 1 theory or correcting a potentially inaccurate one.  
Date: 19 July 1996  
Address: Pittsburgh PA, 15213, USA  
Affiliation: Computer Science Department Carnegie-Mellon University  
Abstract: Autonomous systems require the ability to plan effective courses of action under potentially uncertain or unpredictable contingencies. Planning requires knowledge of the environment that is accurate enough to allow reasoning about actions. If the environment is too complex or very dynamic, goal-driven learning with reactive feedback becomes a necessity. This chapter addresses the issue of learning by experimentation as an integral component of PRODIGY. PRODIGY is a flexible planning system that encodes its domain knowledge as declarative operators, and applies the operator refinement method to acquire additional preconditions or postconditions when observed consequences diverge from internal expectations. When multiple explanations for the observed divergence are consistent with the existing domain knowledge, experiments to discriminate among these explanations are generated. The experimentation process isolates the deficient operator and inserts the discriminant condition or unforeseen side-effect to avoid similar impasses in future planning. Thus, experimentation is demand-driven and exploits both the internal state of the planner and any external feedback received. A detailed example of integrated experiment formulation in presented as the basis for a systematic approach to extending an incomplete domain 
Abstract-found: 1
Intro-found: 1
Reference: [Blythe and Mitchell 89] <author> Blythe, J. and Mitchell, T. M., </author> <title> "On Becoming Reactive," </title> <booktitle> in Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Ithaca, New York, </address> <year> 1989. </year>
Reference-contexts: There are a number of systems that use different techniques to learn in the context of planning and interacting with an external environment. Robo-Soar [Laird et al. 89] is a system implemented in Soar that learns control knowledge from outside guidance. The Theo-Agent <ref> [Blythe and Mitchell 89] </ref> is an autonomous robot that starts out building plans to solve new problems and learns rules that allow it to have a reactive behavior. 12 4.
Reference: [Carbonell 83] <author> Carbonell, J. G., </author> <title> "Learning by Analogy: Formulating and Generalizing Plans from Past Experience," </title> <editor> in R. S. Michalski, J. G. Carbonell and T. M. Mitchell (eds.), </editor> <booktitle> Machine Learning, An Artificial Intelligence Approach, </booktitle> <publisher> Tioga Press, </publisher> <address> Palo Alto, CA, </address> <year> 1983. </year>
Reference-contexts: 1. Introduction: The Need for Reactive Experimentation Learning in the context of problem solving can occur in multiple ways, ranging from macro-operator formation [Fikes 71, Cheng and Carbonell 86] and generalized chunking [Laird et al. 86], to analogical transfer of problem solving strategies <ref> [Carbonell 83, Carbonell, 1986 86] </ref> and pure analytical or explanation-driven techniques [Mitchell et al 86, DeJong and Mooney 86, Minton & Carbonell 87]. All of these techniques, however, focus on the acquisition of control knowledge to solve problems faster, more effectively, and to avoid pitfalls encountered in similar situations.
Reference: [Carbonell et al. 89] <author> Carbonell, J., Gross, K., Hood, G., Shell, P., and Tallis, H., </author> <title> The World Modeling System User Guide, </title> <institution> Department of Computer Science, Carnegie Mellon University, Pittsburgh, PA, </institution> <type> Technical Report, </type> <year> 1989. </year> <title> Internal document </title>
Reference-contexts: Moreover, we assume environmental feedback is correct and not deterministic. Clearly, not every domain permits such a limited manipulation and interchange of information as the one used to describe the operator refinement method. In other work on experimentation we connected PRODIGY to a full 3D newtonian kinematics robotic simulator <ref> [Carbonell and Hood 86, Carbonell et al. 89] </ref> for more realistic environmental feedback [Carbonell et al. ng]. The MAX system (a PRODIGY progeny) exhibits a richer communication channel [--- 90]. Finally, we assume that the environment is only affected by the actions of our system.
Reference: [Carbonell et al. 90] <author> Carbonell, J. G., Knoblock, C. A., and Minton, S., </author> <title> "PRODIGY: An Integrated Architecture for Planning and Learning," </title> <editor> in Kurt VanLehn (ed.), </editor> <booktitle> Architectures for Intelligence, </booktitle> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1990. </year>
Reference-contexts: The work described here is a method for refining the specifications of operators, and it has been implemented in a version of the PRODIGY system augmented with capabilities for execution monitoring and dynamic replanning. 2. The Role of Experimentation in PRODIGY The PRODIGY system <ref> [Minton et al. 89, --- 89, Carbonell et al. 90] </ref> is a general purpose problem solver designed to provide an underlying basis for machine learning research. The appendix presents an overview of the basic architecture and the different learning mechanisms in the system.
Reference: [Carbonell et al. ng] <author> Carbonell, J. G., Gil, Y., and Rementeria, S., </author> <title> Experimentation in PRODIGY: Acquiring Domain and State Knowledge, </title> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, </institution> <type> Technical Report, Forthcoming. 14 </type>
Reference-contexts: Our goal is to develop learning methods and experimentation strategies to acquire missing domain knowledge in general. This paper focuses on one central approach, the operator refinement method, to acquire missing pre and post conditions of operators in the domain theory. In a forthcoming paper <ref> [Carbonell et al. ng] </ref>, we describe other techniques for learning by experimentation in the context of problem solving that address other types of incompleteness in the domain theory. 3 All descriptions of a real robotic environment, for instance, are necessarily partial -- as are all computational models of a complex external <p> In essence, plan execution failures trigger the experimentation and replanning process. Thus, each method is indexed by the failure condition to which it applies, encoded as differences between expected and observed outcomes. The first two cases are the focus of the current chapter. A forthcoming paper <ref> [Carbonell et al. ng] </ref> expands the method to address the last case on the table. <p> The experimentation methods discussed here focused only on operator refinement (both preconditions and consequences), but not on acquiring new operators, new features of the state or domain, or new meta-level control 13 structures. In a forthcoming paper <ref> [Carbonell et al. ng] </ref>, we present other techniques for learning by experimentation in the context of problem solving. That paper describes how PRODIGY can acquire knowledge about the state, such as values of attributes not known when needed to expand the current plan. <p> Another method allows the systen to learn multiple more specific versions of overgeneral operators that failed to predict outcomes consistently. The methods described here and in <ref> [Carbonell et al. ng] </ref> apply when PRODIGY is given a correct but incomplete domain theory, and learning is always incremental: the initial knowledge is monotonically augmented. <p> In other work on experimentation we connected PRODIGY to a full 3D newtonian kinematics robotic simulator [Carbonell and Hood 86, Carbonell et al. 89] for more realistic environmental feedback <ref> [Carbonell et al. ng] </ref>. The MAX system (a PRODIGY progeny) exhibits a richer communication channel [--- 90]. Finally, we assume that the environment is only affected by the actions of our system.
Reference: [Carbonell and Hood 86] <author> Carbonell, J. G. and Hood, G., </author> <title> "The World Modelers Project: Learning in a Reactive Environment," </title> <editor> in Mitchell, T. M., Carbonell, J. G. and Michalski, R. S. (eds.), </editor> <booktitle> Machine Learning: A Guide to Current Research, </booktitle> <pages> pp. 29-34, </pages> <publisher> Kluwer Academic Press, </publisher> <year> 1986. </year>
Reference-contexts: Moreover, we assume environmental feedback is correct and not deterministic. Clearly, not every domain permits such a limited manipulation and interchange of information as the one used to describe the operator refinement method. In other work on experimentation we connected PRODIGY to a full 3D newtonian kinematics robotic simulator <ref> [Carbonell and Hood 86, Carbonell et al. 89] </ref> for more realistic environmental feedback [Carbonell et al. ng]. The MAX system (a PRODIGY progeny) exhibits a richer communication channel [--- 90]. Finally, we assume that the environment is only affected by the actions of our system.
Reference: [Carbonell and Veloso 88] <author> Carbonell, J.G. and Veloso, </author> <title> M.M., "Integrating Derivational Analogy into a General Problem-Solving Architecture," </title> <booktitle> in Proceedings of the First Workshop on Case-Based Reasoning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Tampa, FL, </address> <month> May </month> <year> 1988. </year>
Reference: [Carbonell, 1986 86] <author> Carbonell, J. G., </author> <title> "Derivational Analogy: A Theory of Reconstructive Problem Solving and Expertise Acquisition," </title> <editor> in Michalski, R. S., Carbonell, J. G. and Mitchell, T. M. (eds.), </editor> <booktitle> Machine Learning, An Artificial Intelligence Approach, Volume II, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: 1. Introduction: The Need for Reactive Experimentation Learning in the context of problem solving can occur in multiple ways, ranging from macro-operator formation [Fikes 71, Cheng and Carbonell 86] and generalized chunking [Laird et al. 86], to analogical transfer of problem solving strategies <ref> [Carbonell 83, Carbonell, 1986 86] </ref> and pure analytical or explanation-driven techniques [Mitchell et al 86, DeJong and Mooney 86, Minton & Carbonell 87]. All of these techniques, however, focus on the acquisition of control knowledge to solve problems faster, more effectively, and to avoid pitfalls encountered in similar situations.
Reference: [Cheng and Carbonell 86] <author> Cheng, P. W. and Carbonell, J. G., </author> <title> "Inducing Iterative Rules from Experience: The FERMI Experiment," </title> <booktitle> in Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <address> Philadelphia, PA, </address> <year> 1986. </year>
Reference-contexts: 1. Introduction: The Need for Reactive Experimentation Learning in the context of problem solving can occur in multiple ways, ranging from macro-operator formation <ref> [Fikes 71, Cheng and Carbonell 86] </ref> and generalized chunking [Laird et al. 86], to analogical transfer of problem solving strategies [Carbonell 83, Carbonell, 1986 86] and pure analytical or explanation-driven techniques [Mitchell et al 86, DeJong and Mooney 86, Minton & Carbonell 87].
Reference: [DeJong and Mooney 86] <author> DeJong, G. F. and Mooney, R., </author> <title> "Explanation-Based Learning: An Alternative View," </title> <journal> Machine Learning 1, </journal> <volume> (2), </volume> <year> 1986, </year> <pages> 145-176. </pages>
Reference: [Etzioni 90] <author> Etzioni, O., </author> <title> A Structural Theory of Search Control, </title> <type> Ph.D. thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1990. </year> <note> In preparation </note>
Reference-contexts: The appendix presents an overview of the basic architecture and the different learning mechanisms in the system. PRODIGY can improve its performance, by learning search control rules <ref> [Minton88 88, Etzioni 90] </ref>, by storing and replaying derivational traces in an analogy/case-based reasoning mode [Veloso and Carbonell 89], by learning useful abstractions for hierarchical planning [Knoblock 89], and by acquiring knowledge from domain experts via graphically-oriented static and dynamic knowledge acquisition interfaces [Joseph 89].
Reference: [Fikes 71] <author> Fikes, R. E. and Nilsson, N. J., </author> <title> "STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving," </title> <booktitle> Artificial Intelligence 2, </booktitle> <year> 1971, </year> <pages> 189-208. </pages>
Reference-contexts: 1. Introduction: The Need for Reactive Experimentation Learning in the context of problem solving can occur in multiple ways, ranging from macro-operator formation <ref> [Fikes 71, Cheng and Carbonell 86] </ref> and generalized chunking [Laird et al. 86], to analogical transfer of problem solving strategies [Carbonell 83, Carbonell, 1986 86] and pure analytical or explanation-driven techniques [Mitchell et al 86, DeJong and Mooney 86, Minton & Carbonell 87].
Reference: [Georgeff and Lansky 87] <author> Georgeff, M. P. and Lansky, A. L., </author> <title> "Reactive Reasoning and Planning," </title> <booktitle> in Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <year> 1987. </year>
Reference-contexts: Finally, LIVE keeps no history of its past behavior, retaining only the current set of operators, objects and features. There is a significant amount of work on recovery from planning failures, both in the context of case-based reasoning and of reactive systems ( <ref> [Hammond 89, Schoppers 87, Georgeff and Lansky 87, Kaelbling 86] </ref>, and others). However our work is more focused on the techniques for learning from these failures rather than the process of plan recovery itself.
Reference: [Gross 88] <author> Gross, K. P., </author> <title> "Incremental Multiple Concept Learning Using Experiments," </title> <booktitle> in Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <address> Ann Arbor, MI, </address> <year> 1988. </year>
Reference-contexts: Thus, we characterize our work as purposeful and task-driven experimentation. Experiments are always directed at overcoming a current impasse in the planning processes. 11 3. Related Work Experimentation techniques have been used in recent work on various areas of Machine Learning, including learning from examples <ref> [Gross 88] </ref> and discovery programs [Langley et al 87, Nordhausen et al. 89]. Kulkarni and Simon [Kulkarni and Simon 89, Kulkarni 88] developed a system called KEKADA that simulates the reasoning process followed by scientists when they encounter surprising phenomena.
Reference: [Hammond 89] <author> Hammond, K., </author> <title> Case-based planning: Viewing planning as a Memory Task, </title> <publisher> Academic Press, </publisher> <year> 1989. </year>
Reference-contexts: Finally, LIVE keeps no history of its past behavior, retaining only the current set of operators, objects and features. There is a significant amount of work on recovery from planning failures, both in the context of case-based reasoning and of reactive systems ( <ref> [Hammond 89, Schoppers 87, Georgeff and Lansky 87, Kaelbling 86] </ref>, and others). However our work is more focused on the techniques for learning from these failures rather than the process of plan recovery itself.
Reference: [Joseph 89] <author> Joseph, </author> <title> R.L., "Graphical Knowledge Acquisition," </title> <booktitle> in Proceedings of the 4th Workshop on Knowledge Acquisition For Knowledge-Based Systems, </booktitle> <address> Banff, Canada, </address> <year> 1989. </year>
Reference-contexts: its performance, by learning search control rules [Minton88 88, Etzioni 90], by storing and replaying derivational traces in an analogy/case-based reasoning mode [Veloso and Carbonell 89], by learning useful abstractions for hierarchical planning [Knoblock 89], and by acquiring knowledge from domain experts via graphically-oriented static and dynamic knowledge acquisition interfaces <ref> [Joseph 89] </ref>. Our work is focused on the acquisition of the domain theory through external feedback from targeted actions: execution monitoring of plans as they unfold and targeted experiments to resolve apparent indeterminacies in the environment.
Reference: [Kaelbling 86] <author> Kaelbling, L., </author> <title> An Architecture for Intelligent Reactive Systems, </title> <booktitle> Artificial Intelligence Center, SRI International, </booktitle> <address> Menlo Park, CA, </address> <note> Technical Report Technical Note 400, </note> <year> 1986. </year>
Reference-contexts: Finally, LIVE keeps no history of its past behavior, retaining only the current set of operators, objects and features. There is a significant amount of work on recovery from planning failures, both in the context of case-based reasoning and of reactive systems ( <ref> [Hammond 89, Schoppers 87, Georgeff and Lansky 87, Kaelbling 86] </ref>, and others). However our work is more focused on the techniques for learning from these failures rather than the process of plan recovery itself.
Reference: [Knoblock 89] <author> Knoblock, </author> <title> C.A., "Learning Hierarchies of Abstraction Spaces," </title> <booktitle> in Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Ithaca, NY, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: PRODIGY can improve its performance, by learning search control rules [Minton88 88, Etzioni 90], by storing and replaying derivational traces in an analogy/case-based reasoning mode [Veloso and Carbonell 89], by learning useful abstractions for hierarchical planning <ref> [Knoblock 89] </ref>, and by acquiring knowledge from domain experts via graphically-oriented static and dynamic knowledge acquisition interfaces [Joseph 89].
Reference: [Kulkarni 88] <author> Kulkarni, D. S., </author> <title> The Process of Scientific Research: The Strategy of Experimentation, </title> <type> Ph.D. thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1988. </year>
Reference-contexts: Related Work Experimentation techniques have been used in recent work on various areas of Machine Learning, including learning from examples [Gross 88] and discovery programs [Langley et al 87, Nordhausen et al. 89]. Kulkarni and Simon <ref> [Kulkarni and Simon 89, Kulkarni 88] </ref> developed a system called KEKADA that simulates the reasoning process followed by scientists when they encounter surprising phenomena. In essence, they developed a set of heuristics to propose experiments to confirm, magnify and elaborate the extent of a previously unexpected observation.
Reference: [Kulkarni and Simon 89] <author> Kulkarni, D. and Simon, H. A., </author> <title> "The Role of Experimentation in Scientific Theory Revision," </title> <booktitle> in Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Ithaca, New York, </address> <year> 1989. </year>
Reference-contexts: Related Work Experimentation techniques have been used in recent work on various areas of Machine Learning, including learning from examples [Gross 88] and discovery programs [Langley et al 87, Nordhausen et al. 89]. Kulkarni and Simon <ref> [Kulkarni and Simon 89, Kulkarni 88] </ref> developed a system called KEKADA that simulates the reasoning process followed by scientists when they encounter surprising phenomena. In essence, they developed a set of heuristics to propose experiments to confirm, magnify and elaborate the extent of a previously unexpected observation.
Reference: [--- 90] <author> Kuokka, D. R., </author> <title> The Deliberate Integration of Planning, Execution and Learning, </title> <type> Ph.D. thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1990. </year>
Reference: [Laird et al. 86] <author> Laird. J. E., Rosenbloom, P. S., and Newell, A., </author> <title> "Chunking in SOAR: The Anatomy of a General Learning Mechanism," </title> <journal> Machine Learning 1, </journal> <volume> (1), </volume> <year> 1986, </year> <pages> 11-46. </pages>
Reference-contexts: 1. Introduction: The Need for Reactive Experimentation Learning in the context of problem solving can occur in multiple ways, ranging from macro-operator formation [Fikes 71, Cheng and Carbonell 86] and generalized chunking <ref> [Laird et al. 86] </ref>, to analogical transfer of problem solving strategies [Carbonell 83, Carbonell, 1986 86] and pure analytical or explanation-driven techniques [Mitchell et al 86, DeJong and Mooney 86, Minton & Carbonell 87].
Reference: [Laird et al. 89] <author> Laird, J. E., Yager, E. S., and Tuck, C. M., </author> <title> "Learning in Tele-autonomous Systems using Soar," </title> <booktitle> in Proceedings of the NASA Conference on Space Telerobotics, </booktitle> <address> Pasadena, CA, </address> <year> 1989. </year>
Reference-contexts: However our work is more focused on the techniques for learning from these failures rather than the process of plan recovery itself. There are a number of systems that use different techniques to learn in the context of planning and interacting with an external environment. Robo-Soar <ref> [Laird et al. 89] </ref> is a system implemented in Soar that learns control knowledge from outside guidance. The Theo-Agent [Blythe and Mitchell 89] is an autonomous robot that starts out building plans to solve new problems and learns rules that allow it to have a reactive behavior. 12 4.
Reference: [Langley et al 87] <author> Langley, P., Simon, H. A., Bradshaw, G. L., and Zytkow, J. M., </author> <title> Scientific Discovery: Computational Explorations of the Creative Processes, </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Experiments are always directed at overcoming a current impasse in the planning processes. 11 3. Related Work Experimentation techniques have been used in recent work on various areas of Machine Learning, including learning from examples [Gross 88] and discovery programs <ref> [Langley et al 87, Nordhausen et al. 89] </ref>. Kulkarni and Simon [Kulkarni and Simon 89, Kulkarni 88] developed a system called KEKADA that simulates the reasoning process followed by scientists when they encounter surprising phenomena.
Reference: [Michalski, Carbonell and Mitchell 83] <author> Michalski, R. S., Carbonell, J. G., and Mitchell, T. M. (Eds), </author> <title> Machine Learning, An Artificial Intelligence Approach, </title> <publisher> Tioga Press, </publisher> <address> Palo Alto, CA, </address> <year> 1983. </year>
Reference-contexts: Thus, experimentation may be guided towards making more effective use of existing domain knowledge. 2 The reader is referred to the two previous machine learning books <ref> [Michalski, Carbonell and Mitchell 83, Michalski, Carbonell and Mitchell 86] </ref> and other chapters of this book for several good examples of inductive methodologies and systems built upon them. 2 Experimentation to acquire or correct knowledge about the external state of the world.
Reference: [Michalski, Carbonell and Mitchell 86] <editor> Michalski, R. S., Carbonell, J. G., and Mitchell, T. M. (Eds), </editor> <booktitle> Machine Learning, An Artificial Intelligence Approach, Volume II, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1986. </year> <month> 15 </month>
Reference-contexts: Thus, experimentation may be guided towards making more effective use of existing domain knowledge. 2 The reader is referred to the two previous machine learning books <ref> [Michalski, Carbonell and Mitchell 83, Michalski, Carbonell and Mitchell 86] </ref> and other chapters of this book for several good examples of inductive methodologies and systems built upon them. 2 Experimentation to acquire or correct knowledge about the external state of the world.
Reference: [Minton & Carbonell 87] <author> Minton, S. and Carbonell, J.G., </author> <title> "Strategies for Learning Search Control Rules: An Explanation-Based Approach," </title> <booktitle> in Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Milan, Italy, </address> <year> 1987. </year>
Reference-contexts: The present version of PRODIGY is capable of producing such proofs in failure-driven EBL mode <ref> [Minton & Carbonell 87] </ref>.
Reference: [Minton et al. 89] <author> Minton, S., Knoblock, C. A., Kuokka, D. R., Gil, Y., Joseph, R. L., Carbonell, J. G., </author> <title> PRODIGY 2.0: The Manual and Tutorial, </title> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, </institution> <type> Technical Report CMU-CS-89-146, </type> <year> 1989. </year>
Reference-contexts: The work described here is a method for refining the specifications of operators, and it has been implemented in a version of the PRODIGY system augmented with capabilities for execution monitoring and dynamic replanning. 2. The Role of Experimentation in PRODIGY The PRODIGY system <ref> [Minton et al. 89, --- 89, Carbonell et al. 90] </ref> is a general purpose problem solver designed to provide an underlying basis for machine learning research. The appendix presents an overview of the basic architecture and the different learning mechanisms in the system.
Reference: [Minton87a 87] <author> Minton, S., Carbonell, J.G., Etzioni, O, Knoblock, </author> <title> C.A., Kuokka, D.R., "Acquiring Effective Search Control Rules: Explanation-Based Learning in the PRODIGY System," </title> <booktitle> in Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Irvine, CA, </address> <year> 1987. </year>
Reference-contexts: Newly acquired control knowledge may be encoded as preferred operator sequences (chunks and macrooperators), improved heuristic left-hand sides on problem solving operators (as in LEX [Mitchell et al 83]), or explicit search-control rules (as in PRODIGY <ref> [Minton87a 87, --- 89] </ref>). However important the acquisition of search control knowledge may be, the problem of acquiring factual domain knowledge and representing it effectively for problem solving is of at least equal significance.
Reference: [Minton88 88] <author> Minton, S., </author> <title> Learning Search Control Knowledge: An Explanation-based Approach, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Massachusetts, </address> <year> 1988. </year> <note> Limited availability as Carnegie-Mellon CS Tech. Report CMU-CS-88-133 </note>
Reference-contexts: The appendix presents an overview of the basic architecture and the different learning mechanisms in the system. PRODIGY can improve its performance, by learning search control rules <ref> [Minton88 88, Etzioni 90] </ref>, by storing and replaying derivational traces in an analogy/case-based reasoning mode [Veloso and Carbonell 89], by learning useful abstractions for hierarchical planning [Knoblock 89], and by acquiring knowledge from domain experts via graphically-oriented static and dynamic knowledge acquisition interfaces [Joseph 89].
Reference: [Mitchell et al 86] <author> Mitchell, T. M., Keller, R. M. and Kedar-Cabelli, S. T., </author> <title> "Explanation-Based Generalization: A Unifying View," </title> <journal> Machine Learning 1, </journal> <volume> (1), </volume> <year> 1986. </year>
Reference: [Mitchell et al 83] <author> Mitchell, T. M., Utgoff, P. E. and Banerji, R. B., </author> <title> "Learning by Experimentation: Acquiring and Refining Problem-Solving Heuristics," </title> <editor> in R. S. Michalski, J. G. Carbonell and T. M. Mitchell (eds.), </editor> <booktitle> Machine Learning, An Artificial Intelligence Approach, </booktitle> <publisher> Tioga Press, </publisher> <address> Palo Alto, CA, </address> <year> 1983. </year>
Reference-contexts: Newly acquired control knowledge may be encoded as preferred operator sequences (chunks and macrooperators), improved heuristic left-hand sides on problem solving operators (as in LEX <ref> [Mitchell et al 83] </ref>), or explicit search-control rules (as in PRODIGY [Minton87a 87, --- 89]). However important the acquisition of search control knowledge may be, the problem of acquiring factual domain knowledge and representing it effectively for problem solving is of at least equal significance.
Reference: [Nordhausen et al. 89] <author> Nordhaussen, B. and Langley, P., </author> <title> An Integrated Approach to Empirical Discovery, </title> <institution> Department of Information and Computer Science, University of California, Irvine, CA, </institution> <type> Technical Report 89-20, </type> <year> 1989. </year>
Reference-contexts: Experiments are always directed at overcoming a current impasse in the planning processes. 11 3. Related Work Experimentation techniques have been used in recent work on various areas of Machine Learning, including learning from examples [Gross 88] and discovery programs <ref> [Langley et al 87, Nordhausen et al. 89] </ref>. Kulkarni and Simon [Kulkarni and Simon 89, Kulkarni 88] developed a system called KEKADA that simulates the reasoning process followed by scientists when they encounter surprising phenomena.
Reference: [--- 89] <author> Minton, S., Carbonell, J.G., Knoblock, C.A., Kuokka, D.R., Etzioni, O., and Gil, Y., </author> <title> "Explanation-Based Learning: A Problem-Solving Perspective," </title> <booktitle> Artificial Intelligence 40, </booktitle> <pages> (1-3), </pages> <year> 1989, </year> <pages> 63-118. </pages>
Reference: [Rajamoney 86] <author> Rajamoney, S., </author> <title> Automated Design of Experiments for Refining Theories, M. S. </title> <type> Thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <year> 1986. </year>
Reference-contexts: In essence, they developed a set of heuristics to propose experiments to confirm, magnify and elaborate the extent of a previously unexpected observation. In similar spirit to the work reported here, Rajamoney focused on on the problem of refining incorrect theories of qualitative physics in the ADEPT system <ref> [Rajamoney 86] </ref>. When a contradiction arises in the process of explaining an observation, ADEPT proposes hypotheses, and experimentation is used to confirm or reject a single hypothesis at a time. Several kinds of experiments are proposed to test these hypotheses.
Reference: [Rajamoney 88] <author> Rajamoney, S. A., </author> <title> Explanation-Based Theory Revision: An Approach to the Problems of Incomplete and Incorrect Theories, </title> <type> Ph.D. thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1988. </year>
Reference-contexts: When a contradiction arises in the process of explaining an observation, ADEPT proposes hypotheses, and experimentation is used to confirm or reject a single hypothesis at a time. Several kinds of experiments are proposed to test these hypotheses. In COAST <ref> [Rajamoney 88] </ref>, experimentation-based hypothesis refutation is also used to revise an incorrect theory. Experiments are designed using the predictions made by the current hypothesized theory, and their results are used to reject possible theories.
Reference: [Schoppers 87] <author> Schoppers, M. J., </author> <title> "Universal Plans for Reactive Robots in Unpredictable Environments," </title> <booktitle> in Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Milan, Italy, </address> <year> 1987. </year>
Reference-contexts: Finally, LIVE keeps no history of its past behavior, retaining only the current set of operators, objects and features. There is a significant amount of work on recovery from planning failures, both in the context of case-based reasoning and of reactive systems ( <ref> [Hammond 89, Schoppers 87, Georgeff and Lansky 87, Kaelbling 86] </ref>, and others). However our work is more focused on the techniques for learning from these failures rather than the process of plan recovery itself.
Reference: [Shen 89] <author> Shen, W. M., </author> <title> Learning from the Environment Based on Percepts and Actions, </title> <type> Ph.D. thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1989. </year>
Reference-contexts: In contrast with these systems, our work is focused on learning by experimentation to improve the domain theory of a planning system, and more specifically to overcome impasses when external reality differs from planning expectations. The LIVE system, by Shen and Simon <ref> [Shen and Simon 89, Shen 89] </ref> shares some of our objectives. LIVE acquires new operators and refines old ones by interacting with the environment in order to formulate indirectly observable features of objects in the domain, and uses these features in creating new preconditions to split overgeneral operators.
Reference: [Shen and Simon 89] <author> Shen, W. M. and Simon, H. A., </author> <title> "Rule Creation and Rule Learning through Environmental Exploration," </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Detroit, Michigan, </address> <year> 1989. </year>
Reference-contexts: In contrast with these systems, our work is focused on learning by experimentation to improve the domain theory of a planning system, and more specifically to overcome impasses when external reality differs from planning expectations. The LIVE system, by Shen and Simon <ref> [Shen and Simon 89, Shen 89] </ref> shares some of our objectives. LIVE acquires new operators and refines old ones by interacting with the environment in order to formulate indirectly observable features of objects in the domain, and uses these features in creating new preconditions to split overgeneral operators.
Reference: [Sussman 73] <author> Sussman, G. J., </author> <title> A computational model of skill acquisition, </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1973. </year>
Reference-contexts: In fact, this knowledge is acquired later in the example, as an unfortunate side effect of attempting to make a flat mirror into a parabolic one by grinding it. 6 Sussman would call it a "clobber-brother-subgoal" interaction in HACKER <ref> [Sussman 73] </ref>. 6 PRODIGY tries again to produce a telescope mirror. The system succeeds in producing a mirror, but now needs to make it parabolic. The only operator to make IS-PARABOLIC true is GRIND-CONCAVE. Its only precondition is that the object be solid, and so it applies.

References-found: 40

