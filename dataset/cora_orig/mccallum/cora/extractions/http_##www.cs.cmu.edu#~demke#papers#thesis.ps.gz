URL: http://www.cs.cmu.edu/~demke/papers/thesis.ps.gz
Refering-URL: http://www.cs.cmu.edu/~demke/
Root-URL: http://www.cs.cmu.edu
Title: Automatic I/O Prefetching for Out-of-Core Applications  
Author: by Angela K. Demke 
Degree: A thesis submitted in conformity with the requirements for the degree of Master of Science  
Note: c Copyright by Angela K. Demke 1997  
Address: Toronto  
Affiliation: Graduate Department of Computer Science University of  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Arunachalam, A. Choudhary, and B. Rullman. </author> <title> A prefetching prototype for the parallel file system on the Paragon. </title> <booktitle> In Proceedings of the 1995 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 321-323, </pages> <month> May </month> <year> 1995. </year> <note> Extended Abstract. </note>
Reference-contexts: Kimbrel et al. show that the best behavior depends on the amount of contention of the disks, but that for reasonable data layouts on more than four disks contention is rarely a problem. Prefetching in file systems by automatically detecting file access patterns has been well studied <ref> [1, 11, 12, 13, 16, 17, 19] </ref>. Kroeger and Long look at using the compression technique known as prediction by partial match to detect access patterns and to decide what to prefetch [19]. Griffioen and Appleton construct a probability graph based on Chapter 5.
Reference: [2] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-91-002, </type> <institution> NASA Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: Our experimental results demonstrate that our scheme effectively hides the I/O latency in out-of-core versions of the entire NAS Parallel benchmark suite <ref> [2] </ref>, thus resulting in speedups of roughly twofold for the majority of these applications, and over threefold in one case. 1.4 Organization of Thesis This thesis is organized as follows. <p> 1 ; and (iii) processor speeds have increased more rapidly than disk speeds, and hence the importance of tolerating I/O latency has increased in modern systems. 4.1.2 Benchmark Applications To evaluate the effectiveness of our approach, we measured its impact on the performance of the entire NAS Parallel benchmark suite <ref> [2] </ref>. We chose these applications because they represent a variety of different scientific workloads, their data sets can easily be scaled up to out-of-core sizes, and they have not been written to manage I/O explicitly. <p> a computation is performed using the center of the wavefront and the 27 nearest neighboring points (i.e. 6 points that differ by one in only one index, 12 points that differ by one in exactly two of the indices, and 8 points that differ by one in all three indices) <ref> [2] </ref>. Although the references are all regular, and the pattern is detectable by the compiler, it would be extremely difficult to identify the pattern dynamically in the operating system. Fft solves three-dimensional partial differential equations using both forward and inverse fast-Fourier transforms.
Reference: [3] <author> R. Bordawekar, A. Choudhary, and J. Ramanujam. </author> <title> Automatic optimization of communication in out-of-core stencil codes. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Compiling for out-of-core codes tends to focus on three areas. The first area is reordering computation to improve data reuse and reduce the total I/O required <ref> [3] </ref>. The second area is inserting explicit I/O calls into array codes [6, 14, 23, 30]. In general, the compilers are aided by extensions to the source code that indicate particular structures are out-of-core. In addition, some of the work specifically targets I/O performance for parallel applications [3], while we have <p> total I/O required <ref> [3] </ref>. The second area is inserting explicit I/O calls into array codes [6, 14, 23, 30]. In general, the compilers are aided by extensions to the source code that indicate particular structures are out-of-core. In addition, some of the work specifically targets I/O performance for parallel applications [3], while we have achieved impressive speedups for even single-threaded applications. The third compilation approach is to take programs that already contain explicit I/O calls, move them to an earlier program point, and change them to Chapter 5. Related Work 69 asynchronous I/O calls instead.
Reference: [4] <author> Pei Cao, E. W. Felten, A. R. Karlin, and K. Li. </author> <title> A study of integrated prefetching and caching strategies. </title> <type> Technical report, </type> <month> November </month> <year> 1995. </year>
Reference-contexts: Finding the best combined prefetching and caching strategy (i.e. the one that gives the shortest execution time) for a fully known sequence of accesses has been studied by Cao et al. <ref> [4] </ref> for the case of a single disk and by Kimbrel et al. [15] for varying numbers of disks.
Reference: [5] <author> P. M. Chen, E. K. Lee, G. A. Gibson, R. H. Katz, and D. A. Patterson. </author> <title> RAID: high-performance, reliable secondary storage. </title> <journal> ACM Computing Surveys, </journal> <volume> 26(2) </volume> <pages> 145-185, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Fortunately, we can construct cost-effective, high-bandwidth I/O systems by harnessing the aggregate bandwidth of multiple disks <ref> [5, 18, 29] </ref>. Roughly speaking, one can always increase the I/O bandwidth by purchasing additional disks.
Reference: [6] <author> T. H. Cormen and A. Colvin. </author> <title> ViC*: A preprocessor for virtual-memory C*. </title> <type> Technical Report PCS-TR94-243, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> Novem-ber </month> <year> 1994. </year>
Reference-contexts: Compiling for out-of-core codes tends to focus on three areas. The first area is reordering computation to improve data reuse and reduce the total I/O required [3]. The second area is inserting explicit I/O calls into array codes <ref> [6, 14, 23, 30] </ref>. In general, the compilers are aided by extensions to the source code that indicate particular structures are out-of-core. In addition, some of the work specifically targets I/O performance for parallel applications [3], while we have achieved impressive speedups for even single-threaded applications.
Reference: [7] <author> P. E. Crandall, R. A. Aydt, A. A. Chien, and D. A. Reed. </author> <title> Input/output characteristics of scalable parallel applications. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year> <note> 72 BIBLIOGRAPHY 73 </note>
Reference-contexts: Introduction Many of the important computational challenges facing scientists and engineers today involve solving problems with very large data sets. For example, global climate modeling, computational physics and chemistry, and many engineering problems (e.g., aircraft simulation) can easily involve data sets that are too large to fit in main memory <ref> [7, 9, 26] </ref>. For such applications (which are commonly referred to as "out-of-core" applications), main memory simply constitutes an intermediate stage in the memory hierarchy, and the bulk of the data must reside on disk or other secondary storage.
Reference: [8] <author> K. M. Curewitz, P. Krishnan, and J. S. Vitter. </author> <title> Practical prefetching via data compression. </title> <booktitle> In Proceedings of the 1993 ACM-SIGMOD Conference on Management of Data, </booktitle> <pages> pages 257-266, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Other work has also been done in the area of prefetching for paged virtual memory systems, however this research generally depends on the operating system being able to detect patterns to initiate prefetching. Curewitz, Krishnan, and Vitter investigate using techniques developed for data compression to predict what to prefetch <ref> [8] </ref>. Their target environments are object-oriented databases and hypertext system, and they assume that the CPU will be mostly idle, allowing the operating system to perform complicated calculations without affecting a user's perceived response time. Song and Cho use the fault history to detect patterns and initiate prefetching [28].
Reference: [9] <author> J. M. del Rosario and A. Choudhary. </author> <title> High performance I/O for massively parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Introduction Many of the important computational challenges facing scientists and engineers today involve solving problems with very large data sets. For example, global climate modeling, computational physics and chemistry, and many engineering problems (e.g., aircraft simulation) can easily involve data sets that are too large to fit in main memory <ref> [7, 9, 26] </ref>. For such applications (which are commonly referred to as "out-of-core" applications), main memory simply constitutes an intermediate stage in the memory hierarchy, and the bulk of the data must reside on disk or other secondary storage.
Reference: [10] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: If so, then the reference has spatial reuse. Group Reuse For reuse among different array references, Gannon et al. observe that data reuse is exploitable only if the references are uniformly generated; that is, references whose array index expressions differ in at most the constant term <ref> [10] </ref>. For example, references B [j][0] and B [j+1][0] in Figure 3.2 are uniformly generated, while references C [i] and C [j] in by B [j+1][0] during the previous j iteration, making it very likely that this reuse will result in locality.
Reference: [11] <author> J. Griffioen and R. Appleton. </author> <title> Reducing file system latency using a predictive approach. </title> <booktitle> In Conference Proceedings of the USENIX Summer 1994 Technical Conference, </booktitle> <pages> pages 197-208, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Kimbrel et al. show that the best behavior depends on the amount of contention of the disks, but that for reasonable data layouts on more than four disks contention is rarely a problem. Prefetching in file systems by automatically detecting file access patterns has been well studied <ref> [1, 11, 12, 13, 16, 17, 19] </ref>. Kroeger and Long look at using the compression technique known as prediction by partial match to detect access patterns and to decide what to prefetch [19]. Griffioen and Appleton construct a probability graph based on Chapter 5.
Reference: [12] <author> A. S. Grimshaw and Edmond C. Loyot, Jr. </author> <title> ELFS: object-oriented extensible file systems. </title> <booktitle> In Proceedings of the First International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> page 177, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Kimbrel et al. show that the best behavior depends on the amount of contention of the disks, but that for reasonable data layouts on more than four disks contention is rarely a problem. Prefetching in file systems by automatically detecting file access patterns has been well studied <ref> [1, 11, 12, 13, 16, 17, 19] </ref>. Kroeger and Long look at using the compression technique known as prediction by partial match to detect access patterns and to decide what to prefetch [19]. Griffioen and Appleton construct a probability graph based on Chapter 5.
Reference: [13] <author> J. Huber, C. L. Elford, D. A. Reed, A. A. Chien, and D. S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Kimbrel et al. show that the best behavior depends on the amount of contention of the disks, but that for reasonable data layouts on more than four disks contention is rarely a problem. Prefetching in file systems by automatically detecting file access patterns has been well studied <ref> [1, 11, 12, 13, 16, 17, 19] </ref>. Kroeger and Long look at using the compression technique known as prediction by partial match to detect access patterns and to decide what to prefetch [19]. Griffioen and Appleton construct a probability graph based on Chapter 5.
Reference: [14] <author> K. Kennedy, C. Koelbel, and M. Paleczny. </author> <title> Scalable I/O for out-of-core structures. </title> <type> Technical Report CRPC-TR93357-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> November </month> <year> 1993. </year> <note> Updated August, </note> <year> 1994. </year>
Reference-contexts: Compiling for out-of-core codes tends to focus on three areas. The first area is reordering computation to improve data reuse and reduce the total I/O required [3]. The second area is inserting explicit I/O calls into array codes <ref> [6, 14, 23, 30] </ref>. In general, the compilers are aided by extensions to the source code that indicate particular structures are out-of-core. In addition, some of the work specifically targets I/O performance for parallel applications [3], while we have achieved impressive speedups for even single-threaded applications.
Reference: [15] <author> T. Kimbrel, A. Tomkins, R.H. Patterson, B. Bershad, P. Cao, E. Felten, G. Gibson, A. Karlin, and K. Li. </author> <title> A trace-driven comparison of algroithms for parallel prefetching and caching. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 19-34, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Finding the best combined prefetching and caching strategy (i.e. the one that gives the shortest execution time) for a fully known sequence of accesses has been studied by Cao et al. [4] for the case of a single disk and by Kimbrel et al. <ref> [15] </ref> for varying numbers of disks. Kimbrel et al. show that the best behavior depends on the amount of contention of the disks, but that for reasonable data layouts on more than four disks contention is rarely a problem.
Reference: [16] <author> D. Kotz and C. Schlatter Ellis. </author> <title> Practical prefetching techniques for multiprocessor file systems. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1) </volume> <pages> 33-51, </pages> <month> January </month> <year> 1993. </year> <note> BIBLIOGRAPHY 74 </note>
Reference-contexts: Kimbrel et al. show that the best behavior depends on the amount of contention of the disks, but that for reasonable data layouts on more than four disks contention is rarely a problem. Prefetching in file systems by automatically detecting file access patterns has been well studied <ref> [1, 11, 12, 13, 16, 17, 19] </ref>. Kroeger and Long look at using the compression technique known as prediction by partial match to detect access patterns and to decide what to prefetch [19]. Griffioen and Appleton construct a probability graph based on Chapter 5.
Reference: [17] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Prefetching in file systems for MIMD multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(2) </volume> <pages> 218-230, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Kimbrel et al. show that the best behavior depends on the amount of contention of the disks, but that for reasonable data layouts on more than four disks contention is rarely a problem. Prefetching in file systems by automatically detecting file access patterns has been well studied <ref> [1, 11, 12, 13, 16, 17, 19] </ref>. Kroeger and Long look at using the compression technique known as prediction by partial match to detect access patterns and to decide what to prefetch [19]. Griffioen and Appleton construct a probability graph based on Chapter 5.
Reference: [18] <author> O. Krieger and M. Stumm. </author> <title> HFS: A performance-oriented flexible file system based on building-block compositions. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 95-108, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Fortunately, we can construct cost-effective, high-bandwidth I/O systems by harnessing the aggregate bandwidth of multiple disks <ref> [5, 18, 29] </ref>. Roughly speaking, one can always increase the I/O bandwidth by purchasing additional disks. <p> (Buk) in Section 4.2.5. 4.1 Experimental Framework We now describe our experimental platform including the hardware platform and the software infrastructure used, and the applications which we study in our experiments. 4.1.1 Hardware and Software Infrastructure The experimental platform used to evaluate our scheme is the Hurricane File System (HFS) <ref> [18] </ref> and Hurricane operating system [34] running on the Hector shared-memory multiprocessor [35]. Hurricane is a hierarchically clustered, micro-kernel based 43 Chapter 4. Evaluation of Proposed System 44 Table 4.1: Experimental platform characteristics. <p> Most of HFS is implemented outside of the micro-kernel as a user-level server. HFS implements files using building blocks to specify the structure of the file and the file system policies applied to a file <ref> [18] </ref>. Applications are allowed to specify the structure of the file (for instance, the layout of data across the disks) at creation time, and to dynamically change the policies applied when using a file (for ex ample, for replicated files, the application can specify which replica should be used). <p> The basic characteristics of our experimental platform (with the instrumentation disabled) are shown in Table 4.1, and more detailed descriptions of the platform can be found in earlier publications <ref> [18, 34, 35] </ref> Our experiments are performed on a 16-processor Hector prototype with seven Conner CP3200 disks attached to it. Each disk is directly attached to a different processor Chapter 4.
Reference: [19] <author> T. M. Kroeger and D. D. E. </author> <title> Long. Predicting file system actions from prior events. </title> <booktitle> In Proceedings of the USENIX 1996 Annual Technical Conference, </booktitle> <pages> pages 319-328, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Kimbrel et al. show that the best behavior depends on the amount of contention of the disks, but that for reasonable data layouts on more than four disks contention is rarely a problem. Prefetching in file systems by automatically detecting file access patterns has been well studied <ref> [1, 11, 12, 13, 16, 17, 19] </ref>. Kroeger and Long look at using the compression technique known as prediction by partial match to detect access patterns and to decide what to prefetch [19]. Griffioen and Appleton construct a probability graph based on Chapter 5. <p> Prefetching in file systems by automatically detecting file access patterns has been well studied [1, 11, 12, 13, 16, 17, 19]. Kroeger and Long look at using the compression technique known as prediction by partial match to detect access patterns and to decide what to prefetch <ref> [19] </ref>. Griffioen and Appleton construct a probability graph based on Chapter 5. Related Work 68 prior file system accesses. Both approaches attempt to improve the performance of the overall file system by predicting which files are likely to be referenced next when a particular file is opened.
Reference: [20] <author> M. Malkawi and J. Patel. </author> <title> Compiler directed management policy for numerical programs. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 97-106, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: For instance, in Fft these schemes would incur page faults whenever the application moves between forward and inverse FFT phases. Using application-specific knowledge to assist memory management replacement policies was studied by Malkawi and Patel <ref> [20] </ref>, and by Park, Scott, and Sechrest [24], however, these schemes only consider retaining needed pages in memory and do not attempt to prefetch. Although the amount of I/O needed can be reduced by intelligent memory management, the latency of the remaining I/O operations is still a serious concern.
Reference: [21] <author> T. C. Mowry. </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: There are two reasons why existing read/write I/O interfaces are unacceptable for our purposes. First, for the compiler to successfully move prefetches back far enough to hide the large latency of I/O, it is essential that prefetches be non-binding <ref> [21] </ref>. The non-binding property means that when a given reference is prefetched, the data value seen by that reference is bound at reference time; in contrast, with a binding prefetch, the value is bound at prefetch time. <p> "MADV DONTNEED" hints to the madvise () interface can potentially be used to implement prefetch and release in UNIX.) 2.2.2 Minimizing Prefetch Overhead Earlier studies on compiler-based prefetching to hide cache-to-memory latency have demonstrated the importance of avoiding the overhead of unnecessarily prefetching data that already resides in the cache <ref> [21, 22] </ref>. To address this problem, compiler algorithms have been developed for inserting prefetches only for those references that are likely to suffer misses. <p> the three layers of our system|the compiler, the operating system, and the run-time layer|we now discuss each layer in more detail. 2.3 Compiler Support The bulk of our compiler algorithm is a straightforward extension of an algorithm that was developed earlier for prefetching cache-to-memory misses in dense-matrix and sparse-matrix codes <ref> [21, 22] </ref>. Conceptually, prefetching from disks into main memory entails moving down one level in the memory hierarchy. <p> Chapter 3 The Compiler Algorithm In this chapter we provide a more detailed description of our compiler algorithm for generating prefetch and release requests. The bulk of this algorithm is a straightforward extension of one that was developed earlier for prefetching cache-to-memory misses in dense-matrix and sparse-matrix codes <ref> [21, 22] </ref>. Essentially, we simply move down one level in the memory hierarchy to prefetch data from disks into main memory. <p> Hence, we begin in Section 3.1 with a description of how the compiler uses locality analysis to predict when misses (i.e. page faults) are likely to occur. Most of the work in this section has been presented previously in Mowry's thesis on cache prefetching <ref> [21] </ref> and is reproduced here for completeness and ease of reference. <p> Chapter 3. The Compiler Algorithm 33 3.2 Scheduling Prefetches Most of the changes made to the original compiler algorithm for prefetching <ref> [21] </ref> are related to how loops are split and how prefetch and release operations are scheduled. <p> The Compiler Algorithm 41 early enough to be used in the prefetch of the indirect reference, rather than in the indirect reference itself. More details on prefetching indirect references are given in Mowry's thesis on cache prefetching <ref> [21] </ref>. A More Detailed Example that it is able to prefetch the indirect a [b [i]] reference as well as the dense b [i] and c [i][j] references).
Reference: [22] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> volume 27, </volume> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: "MADV DONTNEED" hints to the madvise () interface can potentially be used to implement prefetch and release in UNIX.) 2.2.2 Minimizing Prefetch Overhead Earlier studies on compiler-based prefetching to hide cache-to-memory latency have demonstrated the importance of avoiding the overhead of unnecessarily prefetching data that already resides in the cache <ref> [21, 22] </ref>. To address this problem, compiler algorithms have been developed for inserting prefetches only for those references that are likely to suffer misses. <p> the three layers of our system|the compiler, the operating system, and the run-time layer|we now discuss each layer in more detail. 2.3 Compiler Support The bulk of our compiler algorithm is a straightforward extension of an algorithm that was developed earlier for prefetching cache-to-memory misses in dense-matrix and sparse-matrix codes <ref> [21, 22] </ref>. Conceptually, prefetching from disks into main memory entails moving down one level in the memory hierarchy. <p> Chapter 3 The Compiler Algorithm In this chapter we provide a more detailed description of our compiler algorithm for generating prefetch and release requests. The bulk of this algorithm is a straightforward extension of one that was developed earlier for prefetching cache-to-memory misses in dense-matrix and sparse-matrix codes <ref> [21, 22] </ref>. Essentially, we simply move down one level in the memory hierarchy to prefetch data from disks into main memory. <p> First, Trivedi's compiler analysis was restricted to programs in which blocking could be performed whereas previous studies on prefetching for caches have shown that many programs which can be prefetched cannot be blocked <ref> [22] </ref>. Thus, our approach is much more widely applicable. Second, we introduce the idea of a run-time layer to filter the prefetches inserted by the compiler, thus allowing the compiler to be much more aggressive about adding prefetches when the analysis cannot be performed perfectly.
Reference: [23] <author> M. Paleczny, K. Kennedy, and C. Koelbel. </author> <title> Compiler support for out-of-core arrays on data parallel machines. </title> <booktitle> In Proceedings of the Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 110-118, </pages> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Compiling for out-of-core codes tends to focus on three areas. The first area is reordering computation to improve data reuse and reduce the total I/O required [3]. The second area is inserting explicit I/O calls into array codes <ref> [6, 14, 23, 30] </ref>. In general, the compilers are aided by extensions to the source code that indicate particular structures are out-of-core. In addition, some of the work specifically targets I/O performance for parallel applications [3], while we have achieved impressive speedups for even single-threaded applications.
Reference: [24] <author> Yoonho Park, Ridgway Scott, and Stuart Sechrest. </author> <title> Virtual memory versus file interfaces for large, </title> <booktitle> memory-intensive scientific applications. In Proceedings of Supercomputing 96, </booktitle> <pages> pages 17-22, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: Recently, Park, Scott and Sechrest have shown that customizing the replacement policy on a per-application basis can significantly improve the performance of out-of-core applications <ref> [24] </ref>. An additional concern from a resource management perspective is that the memory manager may allocate more memory to a process than it actually needs in order to delay making these replacement decisions. This may have a negative impact on competing processes. <p> For instance, in Fft these schemes would incur page faults whenever the application moves between forward and inverse FFT phases. Using application-specific knowledge to assist memory management replacement policies was studied by Malkawi and Patel [20], and by Park, Scott, and Sechrest <ref> [24] </ref>, however, these schemes only consider retaining needed pages in memory and do not attempt to prefetch. Although the amount of I/O needed can be reduced by intelligent memory management, the latency of the remaining I/O operations is still a serious concern.
Reference: [25] <author> R. H. Patterson, G. A. Gibson, E. Ginting, D. Stodolsky, and J. Zelenka. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 79-95, </pages> <month> December </month> <year> 1995. </year> <note> BIBLIOGRAPHY 75 </note>
Reference-contexts: Another technique implemented in file systems is to support prefetching based on information supplied explicitly by the application <ref> [25, 27, 31] </ref>. Of these approaches, the TIP system developed by Patterson et al. [25] is most relevant to our work in that hints provided by the application level are used by the operating system to optimize file prefetching and replacement. <p> Another technique implemented in file systems is to support prefetching based on information supplied explicitly by the application [25, 27, 31]. Of these approaches, the TIP system developed by Patterson et al. <ref> [25] </ref> is most relevant to our work in that hints provided by the application level are used by the operating system to optimize file prefetching and replacement. In fact, the cost model employed by TIP might be very useful for our memory manager.
Reference: [26] <author> J. T. Poole. </author> <title> Preliminary survey of I/O intensive applications. </title> <type> Technical Report CCSF-38, </type> <institution> Scalable I/O Initiative, Caltech Concurrent Supercomputing Facilities, Caltech, </institution> <year> 1994. </year>
Reference-contexts: Introduction Many of the important computational challenges facing scientists and engineers today involve solving problems with very large data sets. For example, global climate modeling, computational physics and chemistry, and many engineering problems (e.g., aircraft simulation) can easily involve data sets that are too large to fit in main memory <ref> [7, 9, 26] </ref>. For such applications (which are commonly referred to as "out-of-core" applications), main memory simply constitutes an intermediate stage in the memory hierarchy, and the bulk of the data must reside on disk or other secondary storage.
Reference: [27] <author> T.P. Singh and A. Choudhary. ADOPT: </author> <title> A dynamic scheme for optimal prefetching in parallel file systems. </title> <type> Technical report, </type> <institution> NPAC, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Another technique implemented in file systems is to support prefetching based on information supplied explicitly by the application <ref> [25, 27, 31] </ref>. Of these approaches, the TIP system developed by Patterson et al. [25] is most relevant to our work in that hints provided by the application level are used by the operating system to optimize file prefetching and replacement.
Reference: [28] <author> I. Song and Y. Cho. </author> <title> Page prefetching based on fault history. </title> <booktitle> In USENIX Mach III symposium proceedings, </booktitle> <pages> pages 203-213, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Their target environments are object-oriented databases and hypertext system, and they assume that the CPU will be mostly idle, allowing the operating system to perform complicated calculations without affecting a user's perceived response time. Song and Cho use the fault history to detect patterns and initiate prefetching <ref> [28] </ref>. These techniques all suffer from the fact that some number of faults are required to establish patterns before prefetching can begin, and when the patterns change unnecessary prefetches will occur.
Reference: [29] <author> A. Sweeney, D. Doucette, W. Hu, C. Anderson, M. Nishimoto, , and G. Peck. </author> <title> Scalability in the XFS file system. </title> <booktitle> In USENIX Technical Conference, </booktitle> <pages> pages 1-14. </pages> <publisher> Usenix, </publisher> <month> January </month> <year> 1996. </year>
Reference-contexts: Fortunately, we can construct cost-effective, high-bandwidth I/O systems by harnessing the aggregate bandwidth of multiple disks <ref> [5, 18, 29] </ref>. Roughly speaking, one can always increase the I/O bandwidth by purchasing additional disks.
Reference: [30] <author> R. Thakur, R. Bordawekar, and A. Choudhary. </author> <title> Compilation of out-of-core data parallel programs for distributed memory machines. </title> <booktitle> In IPPS '94 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 54-72. </pages> <institution> Syracuse University, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Compiling for out-of-core codes tends to focus on three areas. The first area is reordering computation to improve data reuse and reduce the total I/O required [3]. The second area is inserting explicit I/O calls into array codes <ref> [6, 14, 23, 30] </ref>. In general, the compilers are aided by extensions to the source code that indicate particular structures are out-of-core. In addition, some of the work specifically targets I/O performance for parallel applications [3], while we have achieved impressive speedups for even single-threaded applications.
Reference: [31] <author> R. Thakur, R. Bordawekar, A. Choudhary, R. Ponnusamy, and T. Singh. </author> <title> PASSION runtime library for parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 119-128, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Another technique implemented in file systems is to support prefetching based on information supplied explicitly by the application <ref> [25, 27, 31] </ref>. Of these approaches, the TIP system developed by Patterson et al. [25] is most relevant to our work in that hints provided by the application level are used by the operating system to optimize file prefetching and replacement.
Reference: [32] <author> S. W. K. Tjiang and J. L. Hennessy. Sharlit: </author> <title> A tool for building optimizers. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1992. </year>
Reference-contexts: This instrumentation is also used to produce the detailed statistics shown in subsequent sections. We implemented our prefetching algorithm as a pass in the SUIF (Stanford University Intermediate Format) compiler <ref> [32] </ref>. The output from the SUIF compiler is C code containing prefetch and release calls (as illustrated earlier in Figure 3.2 (b)).
Reference: [33] <author> K.S. Trivedi. </author> <title> On the paging performance of array algorithms. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-26(10):938-947, </volume> <month> October </month> <year> 1977. </year>
Reference-contexts: The idea of using a compiler to extract access patterns from an application and passing this information to the operating system to improve virtual memory performance is not a new one. The first study in this area was conducted nearly twenty years ago by Trivedi <ref> [33] </ref>, who looked at the use of application access patterns extracted by a compiler to implement "prepaging". Although the interface between the compiler and the operating system is nearly identical to that which we propose, there are some significant differences.
Reference: [34] <author> R. C. Unrau, O. Krieger, B. Gamsa, and M. Stumm. </author> <title> Hierarchical clustering: A structure for scalable multiprocessor operating system design. </title> <journal> Journal of Supercomputing, </journal> 9(1/2):105-134, 1995. 
Reference-contexts: Experimental Framework We now describe our experimental platform including the hardware platform and the software infrastructure used, and the applications which we study in our experiments. 4.1.1 Hardware and Software Infrastructure The experimental platform used to evaluate our scheme is the Hurricane File System (HFS) [18] and Hurricane operating system <ref> [34] </ref> running on the Hector shared-memory multiprocessor [35]. Hurricane is a hierarchically clustered, micro-kernel based 43 Chapter 4. Evaluation of Proposed System 44 Table 4.1: Experimental platform characteristics. <p> The basic characteristics of our experimental platform (with the instrumentation disabled) are shown in Table 4.1, and more detailed descriptions of the platform can be found in earlier publications <ref> [18, 34, 35] </ref> Our experiments are performed on a 16-processor Hector prototype with seven Conner CP3200 disks attached to it. Each disk is directly attached to a different processor Chapter 4.
Reference: [35] <author> Z. G. Vranesic, M. Stumm, R. White, and D. Lewis. </author> <title> The Hector Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 24(1), </volume> <month> January </month> <year> 1991. </year> <note> BIBLIOGRAPHY 76 </note>
Reference-contexts: platform including the hardware platform and the software infrastructure used, and the applications which we study in our experiments. 4.1.1 Hardware and Software Infrastructure The experimental platform used to evaluate our scheme is the Hurricane File System (HFS) [18] and Hurricane operating system [34] running on the Hector shared-memory multiprocessor <ref> [35] </ref>. Hurricane is a hierarchically clustered, micro-kernel based 43 Chapter 4. Evaluation of Proposed System 44 Table 4.1: Experimental platform characteristics. <p> The basic characteristics of our experimental platform (with the instrumentation disabled) are shown in Table 4.1, and more detailed descriptions of the platform can be found in earlier publications <ref> [18, 34, 35] </ref> Our experiments are performed on a 16-processor Hector prototype with seven Conner CP3200 disks attached to it. Each disk is directly attached to a different processor Chapter 4.
Reference: [36] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: We represent the shape of the set of iterations that use the same data by a reuse vector space <ref> [36] </ref>. The remainder of this subsection describes how this mathematical representation is used to compute temporal, spatial, and group reuse. Chapter 3.
Reference: [37] <author> D. Womble, D. Greenberg, R. Riesen, and S. Wheat. </author> <title> Out of core, out of mind: Practical parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-16, </pages> <institution> Mississippi State University, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: While this approach does yield a logically correct answer, the resulting performance is typically so poor that it is not considered a viable technique for solving out-of-core problems <ref> [37] </ref>. In practice, scientific programmers who wish to solve out-of-core problems typically write a separate version of the program with explicit I/O calls for the sake of achieving reasonable performance. <p> Writing an out-of-core version of a program is a formidable task|it is not simply a matter of inserting a few I/O read or write statements, but often involves significant restructuring of the code, and in some cases can have a negative impact on the numerical stability of the algorithm <ref> [37] </ref>. Thus the burden of writing a second version of the program (and ensuring that it behaves correctly) presents a 1 Chapter 1. Introduction 2 significant barrier to solving large scientific problems.
References-found: 37

