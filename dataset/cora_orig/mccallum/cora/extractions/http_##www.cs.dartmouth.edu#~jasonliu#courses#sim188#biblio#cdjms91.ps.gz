URL: http://www.cs.dartmouth.edu/~jasonliu/courses/sim188/biblio/cdjms91.ps.gz
Refering-URL: http://www.cs.dartmouth.edu/~jasonliu/courses/sim188/
Root-URL: http://www.cs.dartmouth.edu
Title: The Efficient Simulation of Parallel Computer Systems  
Author: R. G. Covington S. Dwarkadas, J. R. Jump, and J. B. Sinclair S. Madala 
Keyword: Index Terms Architecture models, efficiency, parallel computers, parallel programs, performance, simulation, testbed, validation.  
Address: MS 125-233 4800 Oak Grove Drive Pasadena, CA 91109  Houston, Texas 77251  1000 Bay Area Blvd., Suite 206 Houston, TX 77058  
Affiliation: Jet Propulsion Laboratory,  Department of Electrical and Computer Engineering Rice University  Coherent Systems, Inc.  
Date: 31-58 (1991)  
Note: Reprinted from International Journal in Computer Simulation, Vol. 1, pp.  This research was supported by Texas Instruments Grant No. TI720845RJ, NSF/ONR Grant No. N00014-87-K 0324, SDSU/SDIO Contract No. N66001-85-D-0203, NASA Grant No. NAG 0-208, and a Shell Doctoral Fellowship.  
Abstract: An ongoing research project involves the design and evaluation of a software system for simulating parallel computers. A major goal in the development of this system was to avoid the high overhead associated with the conventional instruction-level simulation of sequential computers, but to retain the accuracy of that technique derived from its use of the execution of real programs. The resulting system is program-driven, but the overhead is significantly reduced by profiling the program to get timing estimates for its basic blocks, which are then used at run time to generate process execution times dynamically while avoiding a detailed emulation of each instruction's execution. A number of experiments dealing with message-passing computer systems have been performed in order to determine the level of accuracy that can be expected from its performance predictions and to measure its overhead. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. So, F. Darema-Rogers, D. A. George, V. A. Norton and G. F. Pfister, "PSIMUL: </author> <title> A System for Parallel Simulation of the Execution of Parallel Programs," </title> <institution> Research Report RC11674, IBM T. J. Watson Research Center, </institution> <month> Jan. </month> <year> 1986 </year>
Reference-contexts: It is not uncommon to use hundreds of host instructions just to emulate one target instruction <ref> [1] </ref>. When we consider the simulation of parallel computing systems composed of several interacting processors, the amount of overhead that would be incurred with instruction-level simulation can become intolerable.
Reference: [2] <author> T. Axelrod, P. Dubois and P. Eltgroth, </author> <title> "A Simulator for MIMD Performance Prediction: Application to the S-1 MkIIa Multiprocessor," </title> <journal> Parallel Computing, </journal> <volume> Vol. 1, </volume> <pages> pp. 237-274, </pages> <year> 1984. </year>
Reference-contexts: Parallel computing systems with over 100 nontrivial processors are currently available, and systems with over 1000 are proposed. Although instruction-level simulation has been used for parallel systems <ref> [2, 3] </ref>, there is a need for simulation techniques that can produce accurate performance predictions without the prohibitively large overhead of instruction-level simulation. One way to reduce the complexity of instruction-level simulation of parallel computers is to use probabilistic models to represent the behavior of a program [4].
Reference: [3] <author> J. M. Butler and A. Y. Oruc, </author> <title> "A Facility for Simulating Multiprocessors," </title> <journal> IEEE Micro, </journal> <volume> Vol. 6, No. 5, </volume> <pages> pp. 32-44, </pages> <month> Oct. </month> <year> 1986. </year>
Reference-contexts: Parallel computing systems with over 100 nontrivial processors are currently available, and systems with over 1000 are proposed. Although instruction-level simulation has been used for parallel systems <ref> [2, 3] </ref>, there is a need for simulation techniques that can produce accurate performance predictions without the prohibitively large overhead of instruction-level simulation. One way to reduce the complexity of instruction-level simulation of parallel computers is to use probabilistic models to represent the behavior of a program [4].
Reference: [4] <author> J. Archibald and J.-L. Baer, </author> <title> "Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model," </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> Vol. 4, No. 4, </volume> <pages> pp. 273-298, </pages> <year> 1986. </year>
Reference-contexts: One way to reduce the complexity of instruction-level simulation of parallel computers is to use probabilistic models to represent the behavior of a program <ref> [4] </ref>. We call this approach distribution-driven simulation. It eliminates entirely the high cost of emulating target processor instructions.
Reference: [5] <author> Y.-B. Lin, J.-L. Baer and E. D. Lazowska, </author> <title> "Tailoring a Parallel TraceDriven Simulation Technique to Specific Multiprocessor Cache Coherence Protocols," </title> <booktitle> in Proc. 1989 SCS Multiconference on Distributed Simulation, </booktitle> <year> 1989. </year>
Reference: [6] <author> S. J. Eggers, E. D. Lazowska and Y.-B. Lin, </author> <title> "Techniques for the TraceDriven Simulation of Cache Performance," </title> <booktitle> in Proc. 1989 Winter Simulation Conference, </booktitle> <month> Dec. </month> <year> 1989, </year> <pages> pp. 1042-1046. </pages>
Reference: [7] <author> M. Dubois, F. A. Briggs, I. Patil and M. Balakrishnan, </author> <title> "Trace-Driven Simulations of Parallel and Distributed Algorithms in Multiprocessors," </title> <booktitle> in Proc. 1986 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1986, </year> <pages> pp. 909-915. </pages>
Reference-contexts: Although there has been recent work in the efficient parallel simulation of multiprocessor caches using address traces [5][6], one problem is that 2 variations in timings and delays in the system can affect the ordering of events and even which events occur, so that the traces are no longer valid <ref> [7] </ref>. Execution-driven simulation is a relatively new simulation technique that can be used to produce performance predictions that are nearly as accurate as those obtained with instruction-level simulation at a fraction of the overhead of that method [8]. It combines some of the properties of tracedriven and instruction-level simulation. <p> Similar techniques based on the basic concept exploited by execution-driven simulation have also been proposed in references <ref> [7, 9, 10, 11] </ref>. A principle disadvantage of execution-driven simulation is that some of the timing details of instruction execution are lost since individual instructions are not emulated. As a result, the time required to execute a sequence of instructions can not be predicted as accurately as with instruction-level simulation. <p> They used measured user code execution times to drive the simulation. An execution-driven approach was also proposed by <ref> [7] </ref> as a means of dynamically generating and interleaving multiprocessor traces. They discuss several methods for trace interleaving in uniprocessor simulations of multiprocessor systems and refer to their approach to execution-driven simulation as on-the-fly tracing.
Reference: [8] <author> R. G. Covington, S. Madala, V. Mehta, J. R. Jump and J. B. Sinclair, </author> <title> "The Rice Parallel Processing Testbed," </title> <booktitle> in Proc. 1988 ACM SIGMETRICS Conf. on Measurement and Modeling of Computer Systems, </booktitle> <address> Santa Fe, NM, </address> <month> May </month> <year> 1988, </year> <pages> pp. 4-11. </pages>
Reference-contexts: Execution-driven simulation is a relatively new simulation technique that can be used to produce performance predictions that are nearly as accurate as those obtained with instruction-level simulation at a fraction of the overhead of that method <ref> [8] </ref>. It combines some of the properties of tracedriven and instruction-level simulation. As with both of these, it is program-driven in that the execution of a real program is used to drive the simulation. <p> A major result of this effort is the Rice Parallel Processing Testbed (RPPT), a sophisticated and powerful simulation system that can be used to investigate a number performance issues concerning the execution of concurrent programs on parallel computing systems <ref> [8] </ref>. RPPT simulation models for architectures can be written at different levels of detail, permitting the user to trade off accuracy against computational efficiency.
Reference: [9] <author> R. M. Fujimoto, </author> <title> "The Simon Simulation and Development System," </title> <booktitle> in Proc. 1985 Summer Computer Simulation Conference, </booktitle> <month> July </month> <year> 1985, </year> <pages> pp. 123-128. </pages>
Reference-contexts: Similar techniques based on the basic concept exploited by execution-driven simulation have also been proposed in references <ref> [7, 9, 10, 11] </ref>. A principle disadvantage of execution-driven simulation is that some of the timing details of instruction execution are lost since individual instructions are not emulated. As a result, the time required to execute a sequence of instructions can not be predicted as accurately as with instruction-level simulation. <p> When the simulation model is executed, the timing estimates provided by the profiled program reflect the program's execution time on the target processor rather than the simulation host. 7 One of the first simulators to utilize execution-driven simulation was Fujimoto's Simon simulation system <ref> [9] </ref>. It uses a form of execution-driven simulation called native mode execution to allow efficient evaluation of the performance of parallel programs executing on multiprocessors. Fujimoto also developed a technique for simulating systems where the host and target processors have different instruction sets [14].
Reference: [10] <author> M. J. Flynn, C. L. Mitchell and J. M. </author> <title> Mulder, "And Now a Case for More Complex Instruction Sets," </title> <journal> Computer, </journal> <volume> Vol. 20, No. 9, </volume> <pages> pp. 71-83, </pages> <month> Sept. </month> <year> 1987. </year>
Reference-contexts: Similar techniques based on the basic concept exploited by execution-driven simulation have also been proposed in references <ref> [7, 9, 10, 11] </ref>. A principle disadvantage of execution-driven simulation is that some of the timing details of instruction execution are lost since individual instructions are not emulated. As a result, the time required to execute a sequence of instructions can not be predicted as accurately as with instruction-level simulation. <p> An execution-driven approach was also proposed by [7] as a means of dynamically generating and interleaving multiprocessor traces. They discuss several methods for trace interleaving in uniprocessor simulations of multiprocessor systems and refer to their approach to execution-driven simulation as on-the-fly tracing. The computer architect's workbench <ref> [10] </ref> uses execution-driven simulation techniques to study the relative performances of a variety of uniprocessors.
Reference: [11] <author> E. Williams and F. Bobrowicz, </author> <title> "Speedup Predictions for Large Scientific Parallel Programs on Cray X-MP-Like Architectures," </title> <booktitle> in Proc. 1985 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1985, </year> <pages> pp. 541-543. </pages>
Reference-contexts: Similar techniques based on the basic concept exploited by execution-driven simulation have also been proposed in references <ref> [7, 9, 10, 11] </ref>. A principle disadvantage of execution-driven simulation is that some of the timing details of instruction execution are lost since individual instructions are not emulated. As a result, the time required to execute a sequence of instructions can not be predicted as accurately as with instruction-level simulation. <p> Yet another approach for dealing with heterogeneous host and target processor instruction sets, similar to the one utilized in the RPPT, is described in [15]. The basic concept inherent in execution-driven simulation was also used by Williams and Bobrowicz <ref> [11] </ref> to simulate an N-processor Cray X-MP with a simulator running on only one of the processors of a Cray X-MP/24. They used measured user code execution times to drive the simulation. An execution-driven approach was also proposed by [7] as a means of dynamically generating and interleaving multiprocessor traces.
Reference: [12] <author> R. G. Covington, </author> <title> "Validation of Rice Parallel Processing Testbed Applications," </title> <type> Ph.D. Dissertation, </type> <institution> Electrical and Computer Engineering Department, Rice University, </institution> <month> Dec. </month> <year> 1988. </year>
Reference-contexts: Since it is not always possible or convenient to use a host processor that has the same instruction set as the target processor, a cross-profiling technique was developed <ref> [12] </ref>. In this approach, compilers for a common source language for both the target and host machines are required. The target compiler can be a cross-compiler (i.e., it can run on the host machine) or a native compiler (running on the target machine). <p> The first three algorithms are numerically intensive, while the latter two include little arithmetic other than simple integer comparisons. Details of these algorithms can be found in the following theses <ref> [12, 24, 25] </ref>. 14 The eigenvalue/eigenvector computation (Eigen) is characterized by single-packet, single-hop communication patterns, although occasionally multi-hop transmissions are used. The program is broken into three components. The first, a tridiagonal reduction, splits the data into p (= the number of processors) parts to be processed.
Reference: [13] <author> R. G. Covington, J. R. Jump and J. B. Sinclair, </author> <title> "Cross-Profiling as an Efficient Technique in Simulating Parallel Computer Systems," </title> <booktitle> in Proc. IEEE 13th Ann. Int. Computer Software and Applications Conf., </booktitle> <address> Orlando, FL, </address> <month> Jan. </month> <year> 1989, </year> <pages> pp. 75-80. </pages>
Reference-contexts: These experiments showed errors due to mismatched basic blocks after block reduction that ranged between 0.044% and 0.85% <ref> [13] </ref>. The cross-profiling technique is illustrated in Fig. 2. The first step is to detect the basic blocks in the HLL source program and mark each with a unique label. Then the source program is compiled twice, once into the TALR and once into the HALR.
Reference: [14] <author> R. M. Fujimoto and W. B. Campbell, </author> <title> "Direct Execution Models of Processor Behavior and Performance," </title> <booktitle> in Proc. 1987 Winter Simulation Conference, </booktitle> <month> Dec. </month> <year> 1987. </year> <month> 24 </month>
Reference-contexts: It uses a form of execution-driven simulation called native mode execution to allow efficient evaluation of the performance of parallel programs executing on multiprocessors. Fujimoto also developed a technique for simulating systems where the host and target processors have different instruction sets <ref> [14] </ref>. Yet another approach for dealing with heterogeneous host and target processor instruction sets, similar to the one utilized in the RPPT, is described in [15].
Reference: [15] <author> M. Huguet, T. Lang and Y. Tamir, </author> <title> "A Block-and-Actions Generator as an Alternative to a Simulator for Collecting Architecture Measurements," </title> <booktitle> in Proc. SIGPLAN '87 Symposium on Interpreters and Interpretive Techniques, </booktitle> <address> St. Paul, MN, </address> <month> July </month> <year> 1987, </year> <pages> pp. 14-25. </pages>
Reference-contexts: Fujimoto also developed a technique for simulating systems where the host and target processors have different instruction sets [14]. Yet another approach for dealing with heterogeneous host and target processor instruction sets, similar to the one utilized in the RPPT, is described in <ref> [15] </ref>. The basic concept inherent in execution-driven simulation was also used by Williams and Bobrowicz [11] to simulate an N-processor Cray X-MP with a simulator running on only one of the processors of a Cray X-MP/24. They used measured user code execution times to drive the simulation.
Reference: [16] <author> B. A. Delagi, N. Saraiya, S. Nishimura and G. Byrd, </author> <title> "An Instrumented Architectural Simulation System," </title> <journal> Rept. </journal> <volume> No. </volume> <pages> KSL 86-36, </pages> <institution> Knowledge Systems Laboratory, Computer Science Department, Stanford University, </institution> <month> Jan. </month> <year> 1987 </year>
Reference-contexts: SIMPLE/CARE, developed at the Knowledge Systems Laboratory at Stanford, is an event-based simulation system for studying the performance of multiprocessors or multicomputers with 100-1000 processors, with a primary focus on parallel expert systems <ref> [16] </ref>. LAMINA is an applications programming interface for SIMPLE/CARE, built upon Zetalisp [17]. Execution objects in LAMINA applications can either be assigned fixed execution times or use the simulation host clock to dynamically determine them. 3.
Reference: [17] <author> B. A. Delagi, N. Saraiya, S. Nishimura and G. Byrd, "LAMINA: </author> <title> CARE Applications Interface," </title> <institution> Knowledge Systems Laboratory, Computer Science Department, Stanford University, </institution> <month> Jan. </month> <year> 1987 </year>
Reference-contexts: SIMPLE/CARE, developed at the Knowledge Systems Laboratory at Stanford, is an event-based simulation system for studying the performance of multiprocessors or multicomputers with 100-1000 processors, with a primary focus on parallel expert systems [16]. LAMINA is an applications programming interface for SIMPLE/CARE, built upon Zetalisp <ref> [17] </ref>. Execution objects in LAMINA applications can either be assigned fixed execution times or use the simulation host clock to dynamically determine them. 3.
Reference: [18] <author> R. G. Covington and J. R. </author> <title> Jump, "CSIM 2.0 Users Guide," </title> <type> Tech. </type> <institution> Rpt. #TR8501, Electrical and Computer Engineering Department, Rice University, </institution> <month> Feb. </month> <year> 1986 </year>
Reference-contexts: structure and how the processor and memory modules are interconnected. (The type of processor, which also can be thought of as part of the architecture specification, is used to select an appropriate profiler and hence becomes part of the program module.) UserSend is written in a simulation language called CSIM <ref> [18] </ref>. This is a general purpose discrete-event simulation language that uses a process-interaction approach and is implemented as an extension of Concurrent C (see the following section) with a set of procedures for event queue manipulation and statistics collection.
Reference: [19] <author> S. Dwarkadas, J. R. Jump and J. B. Sinclair, </author> <title> "Efficient Simulation of Cache Memories," </title> <booktitle> in Proceedings of the1989 Winter Simulation Conference, </booktitle> <address> Washington, D.C., </address> <month> Dec. </month> <year> 1989, </year> <pages> pp. 1032-1041. </pages>
Reference-contexts: This can significantly reduce the efficiency gains derived from the execution-driven approach. As a result, we have concentrated initially on message-based systems that do not have global memory. A discussion of some of the simulation problems due to global memory and some proposed solutions can be found in reference <ref> [19] </ref>. 9 3.2. RPPT Program Modules and Process Mappings Parallel programs for the RPPT are usually written in a language called Concurrent C (CC) [20] (Presto [21] and a parallel version of Fortran based on a message-passing paradigm have also been used).
Reference: [20] <author> S. Madala, </author> <title> "Concurrent C Users Manual," </title> <type> Tech. </type> <institution> Rept. #8701, Electrical and Computer Engineering Department, Rice University, </institution> <month> Oct. </month> <year> 1987 </year>
Reference-contexts: A discussion of some of the simulation problems due to global memory and some proposed solutions can be found in reference [19]. 9 3.2. RPPT Program Modules and Process Mappings Parallel programs for the RPPT are usually written in a language called Concurrent C (CC) <ref> [20] </ref> (Presto [21] and a parallel version of Fortran based on a message-passing paradigm have also been used). Concurrent C is an extension of the C programming language that provides primitives for process control and communication.
Reference: [21] <author> B. N. Bershad, E. D. Lazowska and H. M. Levy, </author> <title> "PRESTO: A System For ObjectOriented Parallel Programming," </title> <type> Tech. </type> <institution> Rept. #87-09-01, Computer Science Department, University of Washington, </institution> <month> Sept. </month> <year> 1987 </year>
Reference-contexts: A discussion of some of the simulation problems due to global memory and some proposed solutions can be found in reference [19]. 9 3.2. RPPT Program Modules and Process Mappings Parallel programs for the RPPT are usually written in a language called Concurrent C (CC) [20] (Presto <ref> [21] </ref> and a parallel version of Fortran based on a message-passing paradigm have also been used). Concurrent C is an extension of the C programming language that provides primitives for process control and communication.
Reference: [22] <author> P. J. Weinberger, </author> <title> "Cheap Dynamic Instruction Counting," </title> <journal> Bell Systems Technical J., </journal> <volume> Vol. 63, No. 8, </volume> <pages> pp. 1815-1826, </pages> <month> Oct. </month> <year> 1984. </year>
Reference-contexts: These profilers are based on the bb profiler developed at Bell Labs <ref> [22] </ref>. A separate cross-profiler is needed for each target instruction set that is different from that of the simulation host. If we are to use the processor of the RPPT host as a target processor, we will also need a standalone profiler for it.
Reference: [23] <author> D. R. Cheriton, </author> <title> "The V Kernel: A Software Base for Distributed Systems," </title> <journal> IEEE Software, </journal> <volume> Vol. 1, No. 2, </volume> <pages> pp. 19-42, </pages> <month> Apr. </month> <year> 1984. </year>
Reference-contexts: These were a 16-node Intel iPSC hypercube and the Loosely-Coupled Multiprocessor (LCMP), a set of Sun workstations interconnected by an Ethernet and running the distributed V system kernel <ref> [23] </ref>. Neither system is ideal for this purpose. A major 13 problem that they have in common is the lack of a high-resolution clock.
Reference: [24] <author> S. C. Ingels, </author> <title> "Validation of the Rice Parallel Processing Testbed Using Sorting Algorithms," M. S. </title> <type> Thesis, </type> <institution> Electrical and Computer Engineering Department, Rice University, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: The first three algorithms are numerically intensive, while the latter two include little arithmetic other than simple integer comparisons. Details of these algorithms can be found in the following theses <ref> [12, 24, 25] </ref>. 14 The eigenvalue/eigenvector computation (Eigen) is characterized by single-packet, single-hop communication patterns, although occasionally multi-hop transmissions are used. The program is broken into three components. The first, a tridiagonal reduction, splits the data into p (= the number of processors) parts to be processed.
Reference: [25] <author> V. Mehta, </author> <title> "Performance Prediction of Fast Fourier Transform Algorithms on Loosely Coupled Multiprocessors," M. S. </title> <type> Thesis, </type> <institution> Electrical and Computer Engineering Department, Rice University, </institution> <month> May </month> <year> 1988. </year>
Reference-contexts: The first three algorithms are numerically intensive, while the latter two include little arithmetic other than simple integer comparisons. Details of these algorithms can be found in the following theses <ref> [12, 24, 25] </ref>. 14 The eigenvalue/eigenvector computation (Eigen) is characterized by single-packet, single-hop communication patterns, although occasionally multi-hop transmissions are used. The program is broken into three components. The first, a tridiagonal reduction, splits the data into p (= the number of processors) parts to be processed.
Reference: [26] <author> G. M. </author> <title> Lauderdale, "Performance Prediction of PacketSwitched Multistage Interconnection Networks in an Execution-Driven Environment," M. S. </title> <type> Thesis, </type> <institution> Electrical and Computer Engineering Department, Rice University, </institution> <month> Sept. </month> <year> 1989. </year>
Reference-contexts: Validation - FFT (LCMP; 4 Processors). In addition to the Radix-2 FFT and GDLS algorithms described above, we used a fast Hadamard transform (FHT) <ref> [26] </ref>. The FHT utilizes floating-point adds extensively and has 19 relatively little communication, like the FFT. (The LCMP is best suited for algorithms with this very coarse-grain type of parallelism.) The results for the FFT and FHT are very similar (Figures 13 and 14).
References-found: 26

