URL: ftp://ftp.gmd.de/gmd/concorde/mb_hans.NNonMIMD.ps.gz
Refering-URL: http://www.first.gmd.de/concorde/publications.html
Root-URL: 
Phone: 0.03 0.05  
Title: Figure 8: time complexity of unit parallelism measured on MANNA theoretical prediction #nodes N time
Date: 4  
Note: 5 Conclusions and Further Work 0.01  (a) References  
Pubnum: 16  
Abstract: Our experience showed us that exibility in expressing a parallel algorithm for simulating neural networks is desirable even if it is not possible then to obtain the most efficient solution for any single training algorithm. We believe that the advantages of a clear and easy to understand program predominates the disadvantages of approaches allowing only for a specific machine or neural network algorithm. We currently investigate if other neural network models are worth while being parallelized, and how the resulting parallel algorithms can be composed of a few common basic building blocks and the logarithmic tree as efficient communication structure. 1 2 4 8 2 500 connections 40 000 connections [1] D. Ackley, G. Hinton, T. Sejnowski: A Learning Algorithm for Boltzmann Machines, Cognitive Science 9, pp. 147-169, 1985 [2] B. M. Forrest et al.: Implementing Neural Network Models on Parallel Computers, The computer Journal, vol. 30, no. 5, 1987 [3] W. Giloi: Latency Hiding in Message Passing Architectures, International Parallel Processing Symposium, April 1994, Cancun, Mexico, IEEE Computer Society Press [4] T. Nordstrm, B. Svensson: Using And Designing Massively Parallel Computers for Artificial Neural Networks, Journal Of Parallel And Distributed Computing, vol. 14, pp. 260-285, 1992 [5] A. Kramer, A. Vincentelli: Efficient parallel learning algorithms for neural networks, Advances in Neural Information Processing Systems I, D. Touretzky (ed.), pp. 40-48, 1989 [6] T. Kohonen: Self-Organization and Associative Memory, Springer-Verlag, Berlin, 1988 [7] D. A. Pomerleau, G. L. Gusciora, D. L. Touretzky, H. T. Kung: Neural Network Simulation at Warp Speed: How We Got 17 Million Connections Per Second, IEEE Intern. Conf. Neural Networks, July 1988 [8] A. Rbel: Dynamic selection of training patterns for neural networks: A new method to control the generalization, Technical Report 92-39, Technical University of Berlin, 1993 [9] D. E. Rumelhart, D. E. Hinton, R. J. Williams: Learning internal representations by error propagation, Rumelhart & McClelland (eds.), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. I, pp. 318-362, Bradford Books/MIT Press, Cambridge, MA, 1986 [10] W. Schiffmann, M. Joost, R. Werner: Comparison of optimized backpropagation algorithms, Proc. of the European Symposium on Artificial Neural Networks, ESANN '93, Brussels, pp. 97-104, 1993 [11] J. Schmidhuber: Accelerated Learning in BackPropagation Nets, Connectionism in perspective, Elsevier Science Publishers B.V. (North-Holland), pp 439-445,1989 [12] M. Taylor, P. Lisboa (eds.): Techniques and Applications of Neural Networks, Ellis Horwood, 1993 [13] M. Witbrock, M. Zagha: An implementation of backpropagation learning on GF11, a large SIMD parallel computer, Parallel Computing, vol. 14, pp. 329-346, 1990 [14] X. Zhang, M. Mckenna, J. P. Mesirov, D. L. Waltz: The backpropagation algorithm on grid and hypercube architectures, Parallel Computing, vol. 14, pp. 317-327, 1990 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Ackley, G. Hinton, T. Sejnowski: </author> <title> A Learning Algorithm for Boltzmann Machines, </title> <booktitle> Cognitive Science 9, </booktitle> <pages> pp. 147-169, </pages> <year> 1985 </year>
Reference: [2] <editor> B. M. Forrest et al.: </editor> <title> Implementing Neural Network Models on Parallel Computers, </title> <journal> The computer Journal, </journal> <volume> vol. 30, no. 5, </volume> <year> 1987 </year>
Reference: [3] <author> W. Giloi: </author> <title> Latency Hiding in Message Passing Architectures, </title> <booktitle> International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1994, </year> <title> Cancun, </title> <publisher> Mexico, IEEE Computer Society Press </publisher>
Reference: [4] <author> T. Nordstrm, B. Svensson: </author> <title> Using And Designing Massively Parallel Computers for Artificial Neural Networks, </title> <journal> Journal Of Parallel And Distributed Computing, </journal> <volume> vol. 14, </volume> <pages> pp. 260-285, </pages> <year> 1992 </year>
Reference: [5] <author> A. Kramer, A. Vincentelli: </author> <title> Efficient parallel learning algorithms for neural networks, </title> <booktitle> Advances in Neural Information Processing Systems I, </booktitle> <address> D. </address> <publisher> Touretzky (ed.), </publisher> <pages> pp. 40-48, </pages> <year> 1989 </year>
Reference: [6] <author> T. Kohonen: </author> <title> Self-Organization and Associative Memory, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1988 </year>
Reference: [7] <author> D. A. Pomerleau, G. L. Gusciora, D. L. Touretzky, H. T. Kung: </author> <title> Neural Network Simulation at Warp Speed: How We Got 17 Million Connections Per Second, </title> <booktitle> IEEE Intern. Conf. Neural Networks, </booktitle> <month> July </month> <year> 1988 </year>
Reference: [8] <author> A. Rbel: </author> <title> Dynamic selection of training patterns for neural networks: A new method to control the generalization, </title> <type> Technical Report 92-39, </type> <institution> Technical University of Berlin, </institution> <year> 1993 </year>
Reference: [9] <author> D. E. Rumelhart, D. E. Hinton, R. J. Williams: </author> <title> Learning internal representations by error propagation, </title> <editor> Rumelhart & McClelland (eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> vol. I, </volume> <pages> pp. 318-362, </pages> <publisher> Bradford Books/MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986 </year>
Reference: [10] <author> W. Schiffmann, M. Joost, R. Werner: </author> <title> Comparison of optimized backpropagation algorithms, </title> <booktitle> Proc. of the European Symposium on Artificial Neural Networks, </booktitle> <address> ESANN '93, Brussels, </address> <pages> pp. 97-104, </pages> <year> 1993 </year>
Reference: [11] <author> J. Schmidhuber: </author> <title> Accelerated Learning in BackPropagation Nets, Connectionism in perspective, </title> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <pages> pp 439-445,1989 </pages>
Reference: [12] <author> M. Taylor, P. Lisboa (eds.): </author> <title> Techniques and Applications of Neural Networks, </title> <publisher> Ellis Horwood, </publisher> <year> 1993 </year>
Reference: [13] <author> M. Witbrock, M. Zagha: </author> <title> An implementation of backpropagation learning on GF11, a large SIMD parallel computer, </title> <journal> Parallel Computing, </journal> <volume> vol. 14, </volume> <pages> pp. 329-346, </pages> <year> 1990 </year>

References-found: 13

