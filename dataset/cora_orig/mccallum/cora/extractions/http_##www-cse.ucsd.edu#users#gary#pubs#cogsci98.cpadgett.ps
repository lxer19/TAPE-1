URL: http://www-cse.ucsd.edu/users/gary/pubs/cogsci98.cpadgett.ps
Refering-URL: http://www.cse.ucsd.edu/users/gary/
Root-URL: 
Email: fcpadgett,garyg@cs.ucsd.edu  
Title: A Simple Neural Network Models Categorical Perception of Facial Expressions  
Author: Curtis Padgett and Garrison W. Cottrell 
Address: La Jolla, CA 92093-0114  
Affiliation: Computer Science Engineering 0114 University of California, San Diego  
Abstract: The performance of a neural network that categorizes facial expressions is compared with human subjects over a set of experiments using interpolated imagery. The experiments for both the human subjects and neural networks make use of interpolations of facial expressions from the Pictures of Facial Affect Database [Ekman and Friesen, 1976]. The only difference in materials between those used in the human subjects experiments [Young et al., 1997] and our materials are the manner in which the interpolated images are constructed - image-quality morphs versus pixel averages. Nevertheless, the neural network accurately captures the categorical nature of the human responses, showing sharp transitions in labeling of images along the interpolated sequence. Crucially for a demonstration of categorical perception [Harnad, 1987], the model shows the highest discrimination between transition images at the crossover point. The model also captures the shape of the reaction time curves of the human subjects along the sequences. Finally, the network matches human subjects' judgements of which expressions are being mixed in the images. The main failing of the model is that there are intrusions of neutral responses in some transitions, which are not seen in the human subjects. We attribute this difference to the difference between the pixel average stimuli and the image quality morph stimuli. These results show that a simple neural network classifier, with no access to the biological constraints that are presumably imposed on the human emotion processor, and whose only access to the surrounding culture is the category labels placed by American subjects on the facial expressions, can nevertheless simulate fairly well the human responses to emotional expressions. 
Abstract-found: 1
Intro-found: 1
Reference: [Adolphs et al., 1998] <author> Adolphs, R., Padgett, C., Logan, C., and Cottrell, G. </author> <year> (1998). </year> <title> Categorical perception of emotional facial expressions: Computer models and human performance. </title> <note> In Preparation. </note>
Reference-contexts: The best network correctly recognized 86.2% of the expressions displayed in novel face images [Padgett and Cottrell, 1997]. We then used this model to predict human responses to constructed images that dissolve 1 from one facial expression image to another <ref> [Padgett et al., 1996, Adolphs et al., 1998] </ref>. When tested on pixel-averaged transitions between facial expressions of the same subject, the model predicted that some transitions would be less categorical than others, with shallower transition curves [Padgett et al., 1996]. <p> When tested on pixel-averaged transitions between facial expressions of the same subject, the model predicted that some transitions would be less categorical than others, with shallower transition curves [Padgett et al., 1996]. Human responses on the same pixel-averaged stimuli show similar variations <ref> [Adolphs et al., 1998] </ref>.
Reference: [Anderson and Rosenfeld, 1988] <author> Anderson, J. and Rosen-feld, E., </author> <title> editors (1988). Neurocomputing: </title> <booktitle> Foundations of Research. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference: [Beale and Keil, 1992] <author> Beale, J. and Keil, F. </author> <year> (1992). </year> <title> Categorical effects in the perception of faces. </title> <journal> Cognition, </journal> <volume> 57 </volume> <pages> 217-239. </pages>
Reference: [Beale and Keil, 1995] <author> Beale, J. and Keil, F. </author> <year> (1995). </year> <title> Categorical perception as an acquired phenomenon: What are the implications? In Smith, </title> <editor> L. and Hancock, P., editors, </editor> <booktitle> Neural Computation and Psychology: Workshops in Computing Series, </booktitle> <pages> pages 176-187, </pages> <address> London. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Thus the region of ambiguity is shortened. However, different exemplars give different results. Easily identified emotions, as in the JJ images, give rise to steeper response changes than morphs between other subjects whose portrayals are not as pronounced. This is in agreement with other studies <ref> [Beale and Keil, 1995] </ref> that show familiarity with the endpoints determines the steepness of the transition in human subjects. In future work, we intend to show that our model provides a nearly complete account of the perception-classification process in that it learns to classify emotions. <p> In future work, we intend to show that our model provides a nearly complete account of the perception-classification process in that it learns to classify emotions. This is of interest because recent work has shown that, in the case of identity <ref> [Beale and Keil, 1995] </ref> (but not shown so far for emotions) perception of identity is non-categorical for unfamiliar stimuli, but is categorical for familiar stimuli.
Reference: [Busey, 1997] <author> Busey, T. </author> <year> (1997). </year> <title> Where are morphed faces in multi-dimensional face space? under review. </title>
Reference-contexts: We believe that this difference is due to the way in which we constructed our faces; simple pixel averages are more likely to be like neutral images than true morphs, which do not fall on straight lines between the endpoints in perceptual space <ref> [Busey, 1997] </ref>. We plan to verify this conjecture by applying our model to image-quality morphs in future work.
Reference: [Calder et al., 1996] <author> Calder, A., Young, A., Perrett, D., Et-coff, N., and Rowland, D. </author> <year> (1996). </year> <title> Categorical perception of morphed facial expressions. </title> <journal> Visual Cognition, </journal> <volume> 3 </volume> <pages> 81-117. </pages>
Reference-contexts: Human subject studies making use of the Ekman and Friesen prototypes also show categorical responses using morph sequences of line drawings extracted from the Ekman and Friesen images [Etcoff and Magee, 1992], and using image-quality morph sequences that appear as natural as the original images <ref> [Calder et al., 1996, Young et al., 1997] </ref>. In one of the most extensive studies with human subjects, Young et al. (1997, henceforth Megamix) show that image-quality morph sequences between six emotional expressions (Happy, Sad, Afraid, Angry, Surprised, and Disgusted) and neutral expressions exhibit categorical behavior.
Reference: [Ekman and Friesen, 1976] <author> Ekman, P. and Friesen, W. </author> <year> (1976). </year> <title> Pictures of facial affect. </title>
Reference-contexts: In previous work we evaluated several types of image features in terms of their efficacy as inputs to neural network models of emotion recognition. The facial expression images we used were from the Pictures of Facial Affect (PFA) database <ref> [Ekman and Friesen, 1976] </ref>. The categorization rates of human subjects in a six-way forced choice labeling of the images [Ekman and Friesen, 1977] (provided with the PFA) were used by the model as targets for the emotion categories.
Reference: [Ekman and Friesen, 1977] <author> Ekman, P. and Friesen, W. </author> <year> (1977). </year> <title> Facial Action Coding System. </title> <publisher> Consulting Psychologists, </publisher> <address> Palo Alto, CA. </address>
Reference-contexts: The facial expression images we used were from the Pictures of Facial Affect (PFA) database [Ekman and Friesen, 1976]. The categorization rates of human subjects in a six-way forced choice labeling of the images <ref> [Ekman and Friesen, 1977] </ref> (provided with the PFA) were used by the model as targets for the emotion categories. The best network correctly recognized 86.2% of the expressions displayed in novel face images [Padgett and Cottrell, 1997].
Reference: [Etcoff and Magee, 1992] <author> Etcoff, N. and Magee, J. </author> <year> (1992). </year> <title> Categorical perception of facial expressions. </title> <journal> Cognition, </journal> <volume> 44 </volume> <pages> 227-240. </pages>
Reference-contexts: Human responses on the same pixel-averaged stimuli show similar variations [Adolphs et al., 1998]. Human subject studies making use of the Ekman and Friesen prototypes also show categorical responses using morph sequences of line drawings extracted from the Ekman and Friesen images <ref> [Etcoff and Magee, 1992] </ref>, and using image-quality morph sequences that appear as natural as the original images [Calder et al., 1996, Young et al., 1997].
Reference: [Harnad, 1987] <author> Harnad, S. R. </author> <year> (1987). </year> <title> Categorical perception: the groundwork of cognition. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, NY. </address>
Reference-contexts: The studies have consistently reported categorical transitions in the sequences. Categorical perception (CP) typically involves demonstrating a boundary region where responses by subjects change rapidly and where the subjects show a correspondingly greater ability to discriminate the stimuli <ref> [Liberman et al., 1957, Harnad, 1987] </ref>. In previous work we evaluated several types of image features in terms of their efficacy as inputs to neural network models of emotion recognition. The facial expression images we used were from the Pictures of Facial Affect (PFA) database [Ekman and Friesen, 1976].
Reference: [Keeping, 1995] <author> Keeping, E. S. </author> <year> (1995). </year> <title> Introduction to Statistical Inference. </title> <publisher> Dover Publications, </publisher> <address> New York. </address>
Reference-contexts: The model also showed that the mean discriminability score of 0.45 (0.31) for transitions (90-70,30-10) near prototypes was significantly different (z=26.0,p&lt;.01) using a normal test for different means <ref> [Keeping, 1995] </ref> than the score of 0.69 (0.29), for transitions far from the prototype (70-50,50-30).
Reference: [Liberman et al., 1957] <author> Liberman, A., Harris, K., Hoffman, H., and Griffith, B. </author> <year> (1957). </year> <title> The discrimination of speech sounds within and across phoneme boundaries. </title> <journal> Journal of Experimental Psychology, </journal> <volume> 54 </volume> <pages> 358-368. </pages>
Reference-contexts: The studies have consistently reported categorical transitions in the sequences. Categorical perception (CP) typically involves demonstrating a boundary region where responses by subjects change rapidly and where the subjects show a correspondingly greater ability to discriminate the stimuli <ref> [Liberman et al., 1957, Harnad, 1987] </ref>. In previous work we evaluated several types of image features in terms of their efficacy as inputs to neural network models of emotion recognition. The facial expression images we used were from the Pictures of Facial Affect (PFA) database [Ekman and Friesen, 1976].
Reference: [Padgett and Cottrell, 1997] <author> Padgett, C. and Cottrell, G. </author> <year> (1997). </year> <title> Representing face images for classifying emotions. </title> <booktitle> In Advances in Neural Information Processing Systems 9, </booktitle> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: The categorization rates of human subjects in a six-way forced choice labeling of the images [Ekman and Friesen, 1977] (provided with the PFA) were used by the model as targets for the emotion categories. The best network correctly recognized 86.2% of the expressions displayed in novel face images <ref> [Padgett and Cottrell, 1997] </ref>. We then used this model to predict human responses to constructed images that dissolve 1 from one facial expression image to another [Padgett et al., 1996, Adolphs et al., 1998]. <p> A sequence thus consisted of 9 dissolve images (not including the prototypes) at 10% mix intervals for the subject JJ. Figure 1 shows examples of the transitions. We used the same images of JJ from the database as were used in Megamix for the endpoints. In previous work <ref> [Padgett and Cottrell, 1997] </ref>, we determined that extracting features from the eye and mouth regions, rather than whole-face eigenfaces gives the best generalization performance for emotion recognition. The features we used were the principal components of 32x32 pixel patches randomly sampled from the face images.
Reference: [Padgett et al., 1996] <author> Padgett, C., Cottrell, G., and Adolphs, R. </author> <year> (1996). </year> <title> Categorical perception in facial emotion classification. </title> <booktitle> In Proceedings of Cognitive Science Conference. </booktitle>
Reference-contexts: The best network correctly recognized 86.2% of the expressions displayed in novel face images [Padgett and Cottrell, 1997]. We then used this model to predict human responses to constructed images that dissolve 1 from one facial expression image to another <ref> [Padgett et al., 1996, Adolphs et al., 1998] </ref>. When tested on pixel-averaged transitions between facial expressions of the same subject, the model predicted that some transitions would be less categorical than others, with shallower transition curves [Padgett et al., 1996]. <p> When tested on pixel-averaged transitions between facial expressions of the same subject, the model predicted that some transitions would be less categorical than others, with shallower transition curves <ref> [Padgett et al., 1996] </ref>. Human responses on the same pixel-averaged stimuli show similar variations [Adolphs et al., 1998]. <p> On the contrary, all emotion pairs showed categorical behavior with few intrusions from other categories [Young et al., 1997]. For this study, we are interested in comparing the reported results of the human subject experiments in Megamix to the neural network model used in our previous study <ref> [Padgett et al., 1996] </ref>. The data used in their experiments consisted of morphed imagery from PFA. A single subject in database (JJ) served as the endpoints for the transition sequences.
Reference: [Rumelhart et al., 1986] <author> Rumelhart, D., Hinton, G., and Williams, R. </author> <year> (1986). </year> <title> Learning representations by back-propagating errors. </title> <journal> Nature, </journal> <volume> 323 </volume> <pages> 533-536. </pages> <editor> Reprinted in [Anderson and Rosenfeld, </editor> <year> 1988]. </year>
Reference-contexts: All units except the inputs are standard logistic functions. We trained the networks with back propagation and the mean-squared error cost function <ref> [Rumelhart et al., 1986] </ref>. The teaching signal was a 1 for the putative expression being portrayed, and 0 for the other six outputs.
Reference: [Russell, 1980] <author> Russell, J. A. </author> <year> (1980). </year> <title> A circomplex model of affect. </title> <journal> Journal of Personality and Social Psychology, </journal> <volume> 39 </volume> <pages> 1161-1178. </pages>
Reference-contexts: We use this term to distinguish our linear pixel-average transitions from image-quality morphing, an inherently nonlinear process. Sad Happy Anger Neutral Surprise Disgust Anger Fear shown here. The image sequences are linearly interpolated between the two database images at each extreme. tions <ref> [Russell, 1980] </ref> based on a multi-dimensional scaling (MDS) of similarity ratings of emotion categories do not adequately account for the observed boundary behavior between emotions. MDS results in a circumplex of emotions, a two-dimensional scaling solution where emotions are arranged around a circle in the scaling space.
Reference: [Seidenberg and McClelland, 1989] <author> Seidenberg, M. and Mc-Clelland, J. </author> <year> (1989). </year> <title> A distributed, developmental model of word recognition and naming. </title> <journal> Psychological Review, </journal> <volume> 96 </volume> <pages> 523-568. </pages>
Reference-contexts: We can also extract response times from our model. A standard measure of reaction time of a feed-forward neural network is to assume that it is proportional to the output error <ref> [Seidenberg and McClelland, 1989] </ref>. In our case, since there is no predetermined correct response to the dissolve imagery, we simply use the difference between the maximum output (corresponding to the network's response), and the maximum possible output (1.0).
Reference: [Young et al., 1997] <author> Young, A., Rowland, D., Calder, A., Et-coff, N., Seth, A., and Perrett, D. </author> <year> (1997). </year> <title> Facial expression megamix: Tests of dimensional and category accounts of emotion recognition. </title> <journal> Cognition, </journal> <volume> 63 </volume> <pages> 271-313. </pages>
Reference-contexts: Human subject studies making use of the Ekman and Friesen prototypes also show categorical responses using morph sequences of line drawings extracted from the Ekman and Friesen images [Etcoff and Magee, 1992], and using image-quality morph sequences that appear as natural as the original images <ref> [Calder et al., 1996, Young et al., 1997] </ref>. In one of the most extensive studies with human subjects, Young et al. (1997, henceforth Megamix) show that image-quality morph sequences between six emotional expressions (Happy, Sad, Afraid, Angry, Surprised, and Disgusted) and neutral expressions exhibit categorical behavior. <p> Accounts based on this would suggest morphing between pairs of emotions on opposite sides of the circumplex would pass through a neutral space in the center. On the contrary, all emotion pairs showed categorical behavior with few intrusions from other categories <ref> [Young et al., 1997] </ref>. For this study, we are interested in comparing the reported results of the human subject experiments in Megamix to the neural network model used in our previous study [Padgett et al., 1996]. The data used in their experiments consisted of morphed imagery from PFA. <p> An example of the average responses for the 50 ensemble networks are presented at the top of Figure 3. The average response of 40 human subjects to the same sequence of emotion transitions are reproduced from the Megamix study in the middle graph <ref> [Young et al., 1997] </ref>. The most striking feature found in both the ensemble model and the subjects' responses is very sharp transition regions from emotion to emotion across the sequence. This is true for all human transitions including those not shown. For the model, the transition behavior was also sharp. <p> This too was significant in the Megamix study <ref> [Young et al., 1997] </ref>. 4 In fact, in some sense, we are putting our worst foot forward here, in that we also modeled their Experiment 1 (data not shown) which was a six-way forced choice not including neutral.
References-found: 18

