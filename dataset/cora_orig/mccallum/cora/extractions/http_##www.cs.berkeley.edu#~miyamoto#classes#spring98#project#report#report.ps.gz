URL: http://www.cs.berkeley.edu/~miyamoto/classes/spring98/project/report/report.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~miyamoto/classes/spring98/project/
Root-URL: 
Title: Dynamic Procedure Placement Through Cache Windowing  
Author: Carleton Miyamoto 
Address: Berkeley  
Affiliation: University of California,  
Date: 252/265 Spring 98  
Pubnum: CS  
Abstract: The relative slowdown of DRAMs with respect to processor speeds and the widespread use of SMP machines have bolstered the reliance on processor caches to provide good performance. As a result, optimizing machines and software for caches have recently received more attention. In addition, with the popularity of extensible computing, which includes the object oriented programming style, shared libraries, and Java based computing, creating effective compilers has become more challenging, with an increased reliance on more dynamic techniques, such as profiling and runtime code generation. This paper proposes a dynamic optimization method called cache windowing to reduce conflict misses in L1 instruction caches. Using a combination of hardware and software support, cache windowing integrates a RollCache (a direct-mapped cache enhanced to support dynamic cache configuration) and a software implemented FIFO caching policy. Together, both allow a program to reposition procedures, dynamically and efficiently, to eliminate cache conflicts. Experiments show that this type of caching scheme can achieve miss rates competitive to a 2-way set associative cache for various programs. Currently, a high software overhead exists to support a software caching policy, though different compiler optimizations, such as inlining, may help to reduce this. Such a system provides a more robust runtime architecture that, potentially, may adapt better to a wider variety of environments. 
Abstract-found: 1
Intro-found: 1
Reference: [CK94] <author> B. Cmelik, D. Keppel, Shade: </author> <title> A Fast InstructionSet Simulator for Execution Profiling, </title> <booktitle> Proceedings of the 1994 ACM SIGMETRICS Conference on the Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1994, </year> <pages> pp. 128-137. </pages>
Reference-contexts: A real RollCache simulator provided a cache simulator for the Titanium compiler. 4.3 Shade Two distinct tools were created using Shade <ref> [CK94] </ref> (V5.33A, V9 SPARC Solaris). The first monitored the CALL (direct procedure call) and JMPL (indirect procedure call) instructions. A statistics gatherer was built as a filter on top of this to generate information about procedure calls. The second tool monitored all branch and call instructions.
Reference: [CL97] <author> J. Chen, B. Leupen, </author> <title> Improving Instruction Locality with Just-In-Time Code Layout, </title> <booktitle> USENIX Windows NT Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: It prevents more conflicts between procedures by leaving holes in the executable and attempting to avoid conflicts between functions that were placed further apart. Looking into more of a dynamic method, Chen and Leupen proposed a method for just-in-time code layout <ref> [CL97] </ref>. They lazily laid out code upon first access in a linear fashion.
Reference: [DH98] <author> K. Driesen, U. Holzle, </author> <title> Accurate Indirect Branch Prediction, </title> <institution> TRCS97-19, University of California, Santa Barbara, </institution> <month> March, </month> <year> 1998. </year>
Reference-contexts: The different access patterns of programs in different languages and runtime environments may show various levels of improvement or degradation using windowing. 8 Related Work The growing number of indirect branches affects other processor components than caches. Driesen and Holzle <ref> [DH98] </ref> investigate the effects on branch prediction. Current processors seem to have high misprediction rates for indirect branches. So, they analyzed different types of branch predictors in an attempt improve the hit rate.
Reference: [GH95] <author> M. Garland, P. Heckbert, </author> <title> Fast Polygonal Approximation of Terrains and Height Fields, </title> <institution> CMU-CS-95-181, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> September, </month> <year> 1995. </year>
Reference-contexts: The Quantify tool was then used to measure execution times. Figure 6 shows the overhead for several programs. Added here is the surface program, which is a surface modeling program that uses an incremental Delaunay Triangulation algorithm <ref> [GH95] </ref>. This measurement is only looking at software overhead. The effects of the windowing cache policy are not accounted for in these measurements. For all except Em3d , the overhead is substantial in the optimized case.
Reference: [HK97] <author> A. Hashemi, D. Kaeli, B. Calder, </author> <title> Efficient Procedure Mapping Using Cache Line Coloring, </title> <booktitle> ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June, </month> <year> 1997. </year>
Reference-contexts: Though optimality has been shown to be NP complete, various heuristics have been developed. The most common is to place procedures close together based upon the total number of calls between them. A variation on this, called cache line coloring <ref> [HK97] </ref>, uses this combined with a technique similar to register coloring and was shown to be effective on various programs. Some more dynamic methods, such as just-in-time code layout have also been researched.
Reference: [ML92] <author> T. Mowry, M. Lam, A. Gupta, </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching, </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October, </month> <year> 1992. </year>
Reference-contexts: In order to ease the burden on the hardware, various compiler, runtime, and language techniques have been developed that improve the effectiveness of these L1 caches. For L1 data caches, prefetching allows the hardware to hide much of the memory latency in a cache miss <ref> [ML92] </ref>. For numerical applications, especially where matrix transformations are required, tiling methods increase the locality of the transform operation [WL91]. It, for example, can provide a large performance boost to dense matrix multiplication algorithms.
Reference: [Mu97] <author> S. Muchnick, </author> <title> Advanced Compiler Design and Implementation, ISBN 1-55860-320-4, </title> <publisher> (c) 1997 Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, Ch. </address> <month> 20, </month> <pages> pp. 669-704. </pages>
Reference-contexts: Tiling methods effectively reduce capacity misses in the cache and lower the required bandwidth to memory, which is especially important on many shared memory architectures. Compilers also employ various loop transformations, such as loop fusion and permutation, to increase locality when accessing data structures <ref> [Mu97] </ref>. Some of these data methods require hardware support. Prefetching, for example, requires special CPU instructions and a cache that can support multiple, simultaneous outstanding memory requests. For some applications, these compiler optimizations greatly enhance the scalability of the problems they can solve.
Reference: [PH90] <author> K. Pettis, R. Hansen, </author> <title> Profile Guided Code Positioning, </title> <booktitle> ACM SIGPLAN'90 Conference on Programming Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June, </month> <year> 1990. </year>
Reference-contexts: They also looked into the call patterns of object oriented style programs and compare them to traditional programming styled ones. There have also been investigations into various different procedure sorting methods. One by Pettis and Hansen <ref> [PH90] </ref> describes a "closest is best" method. Procedures are sorted depending on the weight of the edges connecting them in the call graph. The edge weights are determined by the number of calls detected between them (usually during a profiling run).
Reference: [WL91] <author> M. Wolf, M. Lam, </author> <title> A Data Locality Optimizing Algorithm, </title> <booktitle> ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1991. </year>
Reference-contexts: For L1 data caches, prefetching allows the hardware to hide much of the memory latency in a cache miss [ML92]. For numerical applications, especially where matrix transformations are required, tiling methods increase the locality of the transform operation <ref> [WL91] </ref>. It, for example, can provide a large performance boost to dense matrix multiplication algorithms. Tiling methods effectively reduce capacity misses in the cache and lower the required bandwidth to memory, which is especially important on many shared memory architectures.
Reference: [YS98] <author> K. Yelick, L. Semenzato, G. Pike, C. Miyamoto, B. Liblit, A. Krishnamurthy, P. Hilfinger, S. Graham, D. Gay, P. Colella, A. Aiken, Titanium: </author> <title> A High Performance Java Dialect, </title> <booktitle> ACM 1998 Workshop on Java for High-Performance Network Computing, </booktitle> <address> Stanford, California, </address> <month> February, </month> <year> 1998. </year>
Reference-contexts: The second tool monitored all branch and call instructions. Filters were written to take this information and interface with the cache simulators previously described. 4.4 The Titanium Compiler Titanium is a SPMD Java-based language for which a reference compiler has been developed <ref> [YS98] </ref>. It uses the Java runtime library with custom native routines, along with some additional routines, to form its runtime library. This compiler was taken and modified to gather information on the instruction working sets of various Titanium programs, currently most of which are numerical applications.
References-found: 10

