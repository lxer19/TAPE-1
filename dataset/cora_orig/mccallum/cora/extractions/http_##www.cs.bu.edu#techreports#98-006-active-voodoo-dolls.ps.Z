URL: http://www.cs.bu.edu/techreports/98-006-active-voodoo-dolls.ps.Z
Refering-URL: http://cs-www.bu.edu/techreports/Home.html
Root-URL: 
Email: jisidoro@bu.edu, sclaroff@bu.edu  
Title: Active Voodoo Dolls: A Vision Based Input Device for Nonrigid Control  
Author: John Isidoro and Stan Sclaroff 
Date: June 1998.  
Address: Animation, Philadelphia, PA,  Boston University  
Affiliation: Computer  Computer Science Department  
Note: BU CS TR98-006. To appear in Proc.  
Abstract: A vision based technique for nonrigid control is presented that can be used for animation and video game applications. The user grasps a soft, squishable object in front of a camera that can be moved and deformed in order to specify motion. Active Blobs, a nonrigid tracking technique is used to recover the position, rotation and nonrigid deformations of the object. The resulting transformations can be applied to a texture mapped mesh, thus allowing the user to control it interactively. Our use of texture mapping hardware in tracking makes the system responsive enough for interactive animation and video game character control. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Azarbayejani, T. Starner, B. Horowitz, </author> <title> and A.P. Pent-land. Visually controlled graphics. </title> <journal> PAMI, </journal> <volume> 15(6):602605, </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: Williams [26] tracked iridescent points placed on the face of the user. The points were used to pull regions of a triangular mesh of a human head. This approach has the disadvantage that the dots must be painted or pasted onto the user's face. In contrast, Azarbayejani, et al. <ref> [1] </ref> used a feature finding algorithm to track facial points and converted them into three-dimensional estimates using an extended Kalman Filter. Motion estimates were then used to drive a rigid graphics model of the user's head. Contour-based approaches have also been proposed.
Reference: [2] <author> M. Black and Y. Yacoob. </author> <title> Tracking and recognizing rigid and non-rigid facial motions using local parametric models of image motion. </title> <booktitle> Proc. </booktitle> <address> ICCV, </address> <year> 1995. </year>
Reference-contexts: Furthermore, correlation-based methods are not well-suited to tracking moving objects that undergo scaling, rotation, or nonrigid deformation, since tracking is accomplished via correlation with a translating template. Larger deformations can be accomodated if the problem is posed in terms of deformable image registration <ref> [2, 23] </ref>. In particular, La Cascia, et al. [6], use this approach to register a texture-mapped polygonal model of a person's head with an incoming video sequence. <p> Perhaps the simplest warping functions to be used are those of a 2D affine model or an eight parameter projective model [24]. Unfortunately, these functions are only suitable for approximating the rigid motion of a planar patch. The functions can be extended to include linear and quadratic polynomials <ref> [2] </ref>; however, the extended formulation cannot model general nonrigid motion. A more general parameterization of nonrigid motion can be obtained via the use of the modal representation [19]. In the modal representation, deformation is represented in terms of eigenvectors of a finite element (FE) model.
Reference: [3] <author> M.J. Black and A. Rangarajan. </author> <title> On the unification of line processes, outlier rejection, and robust statistics with applications in early vision. </title> <address> IJCV, 19(1):5791, </address> <year> 1996. </year>
Reference-contexts: robustness to outliers can be further improved via the use of a Lorentzian influence function: (e i ; ) = log (1 + i This norm replaces the traditional quadratic norm found in least squares, and is equivalent to the incorporation of an analog outlier process in our objective function <ref> [3] </ref>. The formulation results in better robustness to specular highlights and occlusions. For efficiency, the log function can be implemented via table look-up. Equation 8 includes a data term only; thus it only enforces the recovered model's fidelity to the image measurements.
Reference: [4] <author> A. Blake and M. Isard. </author> <title> 3D position, attitude and shape input using video tracking of hands and lips. </title> <booktitle> Proc. SIGGRAPH, </booktitle> <year> 1994. </year>
Reference-contexts: Contour-based approaches have also been proposed. For instance, Terzopoulos and Waters [25] used snakes to follow lines drawn on the user's face. The snakes drive an intricate face model where muscle and skin are physically simulated. In a similar approach, Blake and Isard employed contours in gesture tracking <ref> [4] </ref>, allowing the user to use hand motion as a three-dimensional mouse. Rigid motion supplies the position of the mouse while nonrigid motion supplies button pressing and releasing actions. Other researchers [8, 10] have used optic flow to drive the motion of an anatomically-motivated polyhedral model of the face.
Reference: [5] <author> A. Bobick. </author> <title> Movement, activity, and action: The role of knowledge in the perception of motion. </title> <journal> In Proc. Royal Soc.: Special Issue on Knowledge-based Vision in Man and Machine, </journal> <year> 1997. </year> <title> 6 (a) (c) in row (a). The deformable region used for tracking is in row (b). In row (c) the deformation parameters are applied to two dimensions of a three-dimensional animal cracker box. </title>
Reference-contexts: Other researchers have used simple low-level image processing techniques such as normalized image correlation [7], or background subtraction and thresholding <ref> [5, 15, 16, 27] </ref> for tracking moving body parts, facial features, and hand gestures. The resulting tracking output can be used to drive the animation of a face model [9], to drive manipulation of virtual objects [15], or to interact with animated creatures in a virtual world [27, 16, 5]. <p> The resulting tracking output can be used to drive the animation of a face model [9], to drive manipulation of virtual objects [15], or to interact with animated creatures in a virtual world <ref> [27, 16, 5] </ref>. Such low-level methods are surprisingly useful for qualitative measures (e.g., person is standing, object is bending) but are not 1 a. b. c. (a), and a deformable color region on the object used for tracking (b).
Reference: [6] <author> M. La Cascia, J. Isidoro, and S. Sclaroff. </author> <title> Head tracking via robust registration in texture map images. </title> <booktitle> Proc. </booktitle> <address> CVPR, </address> <note> 1998 (to appear). </note>
Reference-contexts: Larger deformations can be accomodated if the problem is posed in terms of deformable image registration [2, 23]. In particular, La Cascia, et al. <ref> [6] </ref>, use this approach to register a texture-mapped polygonal model of a person's head with an incoming video sequence.
Reference: [7] <author> T. Darrell and A. Pentland. </author> <title> Space-time gestures. </title> <booktitle> Proc. </booktitle> <address> CVPR, </address> <year> 1993. </year>
Reference-contexts: In a different approach, histograms of optic flow direction can be used directly in hand gesture recognition [11], allowing television viewers to freely use their hand like a mouse to make menu selections. Other researchers have used simple low-level image processing techniques such as normalized image correlation <ref> [7] </ref>, or background subtraction and thresholding [5, 15, 16, 27] for tracking moving body parts, facial features, and hand gestures.
Reference: [8] <author> D. DeCarlo and D. Metaxas. </author> <title> The integration of optical flow and deformable models: Applications to human face shape and motion estimation. </title> <booktitle> Proc. </booktitle> <address> CVPR, </address> <year> 1996. </year>
Reference-contexts: In a similar approach, Blake and Isard employed contours in gesture tracking [4], allowing the user to use hand motion as a three-dimensional mouse. Rigid motion supplies the position of the mouse while nonrigid motion supplies button pressing and releasing actions. Other researchers <ref> [8, 10] </ref> have used optic flow to drive the motion of an anatomically-motivated polyhedral model of the face. Optic flow has also been used to capture motion of more general nonrigid models like deformable su-perquadrics [17, 18].
Reference: [9] <author> I. Essa, T. Darrell, and A. Pentland. </author> <title> Tracking facial motion. MIT Media Lab, Vision and Modeling TR 272, </title> <year> 1994. </year>
Reference-contexts: Other researchers have used simple low-level image processing techniques such as normalized image correlation [7], or background subtraction and thresholding [5, 15, 16, 27] for tracking moving body parts, facial features, and hand gestures. The resulting tracking output can be used to drive the animation of a face model <ref> [9] </ref>, to drive manipulation of virtual objects [15], or to interact with animated creatures in a virtual world [27, 16, 5].
Reference: [10] <author> I. Essa and A. Pentland. </author> <title> Coding, analysis, interpretation, and recognition of facial expressions. </title> <journal> PAMI, </journal> <volume> 19(7):757 763, </volume> <year> 1997. </year>
Reference-contexts: In a similar approach, Blake and Isard employed contours in gesture tracking [4], allowing the user to use hand motion as a three-dimensional mouse. Rigid motion supplies the position of the mouse while nonrigid motion supplies button pressing and releasing actions. Other researchers <ref> [8, 10] </ref> have used optic flow to drive the motion of an anatomically-motivated polyhedral model of the face. Optic flow has also been used to capture motion of more general nonrigid models like deformable su-perquadrics [17, 18].
Reference: [11] <author> W. Freeman and C. Weissman. </author> <title> Television control by hand gestures. </title> <booktitle> In Proc. Intl. Workshop on Automatic Face- and Gesture- Recognition, </booktitle> <year> 1995. </year>
Reference-contexts: Optic flow has also been used to capture motion of more general nonrigid models like deformable su-perquadrics [17, 18]. In a different approach, histograms of optic flow direction can be used directly in hand gesture recognition <ref> [11] </ref>, allowing television viewers to freely use their hand like a mouse to make menu selections. Other researchers have used simple low-level image processing techniques such as normalized image correlation [7], or background subtraction and thresholding [5, 15, 16, 27] for tracking moving body parts, facial features, and hand gestures.
Reference: [12] <author> M. Gleicher. </author> <title> Projective registration with difference decomposition. </title> <booktitle> Proc. </booktitle> <address> CVPR, </address> <year> 1997. </year>
Reference-contexts: The penalties can be derived directly from the modal stiffness obtained in modal analysis [19]. 5.2 Difference Decomposition To minimize the energy (Equation 10) a difference decomposition approach <ref> [12] </ref> is used. The approach offers the benefit that it requires the equivalent O (1) image gradient calculations and O (N ) image products per iteration. In the difference decomposition, we define a basis of difference images generated by adding small changes to each of the blob parameters.
Reference: [13] <author> F. Hampel, E. Ronchetti, P. Rousseeuw, and W. Stehel. </author> <title> Robust Statistics: The Approach Based on Influence Functions. </title> <publisher> John Wiley and Sons, </publisher> <year> 1986. </year>
Reference-contexts: The process can be made less sensitive to outliers if we replace the quadratic error norm with a robust error norm <ref> [13] </ref>: E image = 1 n X (e i ; ); (8) where is a scale parameter that is determined based on expected image noise.
Reference: [14] <author> R. Jain, R. Kasturi, and B. Shunck. </author> <title> Machine Vision. </title> <publisher> McGraw-Hill, </publisher> <year> 1995. </year>
Reference-contexts: Alternatively, the user can define a bounding contour for a region via a sketch interface. In general, the number of contour segments must be reduced. We utilize the tolerance band approach, where the merging stage can be iteratively alternated with recursive subdivision <ref> [14] </ref>. In practice, a single merging pass is sufficient for a user-sketched boundary. The triangles are then generated using an adaptation of Ruppert's Delaunay refinement algorithm [21]. The algorithm accepts two parameters that control angle and triangle size constraints.
Reference: [15] <author> M. Krueger. </author> <title> Virtual Reality II. </title> <publisher> Addison Wesley, </publisher> <year> 1990. </year>
Reference-contexts: Other researchers have used simple low-level image processing techniques such as normalized image correlation [7], or background subtraction and thresholding <ref> [5, 15, 16, 27] </ref> for tracking moving body parts, facial features, and hand gestures. The resulting tracking output can be used to drive the animation of a face model [9], to drive manipulation of virtual objects [15], or to interact with animated creatures in a virtual world [27, 16, 5]. <p> The resulting tracking output can be used to drive the animation of a face model [9], to drive manipulation of virtual objects <ref> [15] </ref>, or to interact with animated creatures in a virtual world [27, 16, 5]. Such low-level methods are surprisingly useful for qualitative measures (e.g., person is standing, object is bending) but are not 1 a. b. c. (a), and a deformable color region on the object used for tracking (b).
Reference: [16] <author> P. Maes, T. Darrell, B. Blumberg, and A. Pentland. </author> <title> The alive system: Wireless, full-body interaction with autonomous agents. Multimedia Systems, </title> <address> 5(2):105112, </address> <year> 1997. </year>
Reference-contexts: Other researchers have used simple low-level image processing techniques such as normalized image correlation [7], or background subtraction and thresholding <ref> [5, 15, 16, 27] </ref> for tracking moving body parts, facial features, and hand gestures. The resulting tracking output can be used to drive the animation of a face model [9], to drive manipulation of virtual objects [15], or to interact with animated creatures in a virtual world [27, 16, 5]. <p> The resulting tracking output can be used to drive the animation of a face model [9], to drive manipulation of virtual objects [15], or to interact with animated creatures in a virtual world <ref> [27, 16, 5] </ref>. Such low-level methods are surprisingly useful for qualitative measures (e.g., person is standing, object is bending) but are not 1 a. b. c. (a), and a deformable color region on the object used for tracking (b).
Reference: [17] <author> D. Metaxas and D. Terzopoulos. </author> <title> Shape and nonrigid motion estimation through physics-based synthesis. </title> <journal> PAMI, </journal> <volume> 15(6):580 591, </volume> <year> 1993. </year>
Reference-contexts: Other researchers [8, 10] have used optic flow to drive the motion of an anatomically-motivated polyhedral model of the face. Optic flow has also been used to capture motion of more general nonrigid models like deformable su-perquadrics <ref> [17, 18] </ref>. In a different approach, histograms of optic flow direction can be used directly in hand gesture recognition [11], allowing television viewers to freely use their hand like a mouse to make menu selections.
Reference: [18] <author> A. Pentland and B. Horowitz. </author> <title> Recovery of non-rigid motion and structure. </title> <journal> PAMI, </journal> <volume> 13(7):730742, </volume> <year> 1991. </year>
Reference-contexts: Other researchers [8, 10] have used optic flow to drive the motion of an anatomically-motivated polyhedral model of the face. Optic flow has also been used to capture motion of more general nonrigid models like deformable su-perquadrics <ref> [17, 18] </ref>. In a different approach, histograms of optic flow direction can be used directly in hand gesture recognition [11], allowing television viewers to freely use their hand like a mouse to make menu selections.
Reference: [19] <author> A. Pentland and J. Williams. </author> <title> Good vibrations : Modal dynamics for graphics and animation. </title> <booktitle> Proc. SIGGRAPH, </booktitle> <address> 23(4):215222, </address> <year> 1989. </year>
Reference-contexts: The functions can be extended to include linear and quadratic polynomials [2]; however, the extended formulation cannot model general nonrigid motion. A more general parameterization of nonrigid motion can be obtained via the use of the modal representation <ref> [19] </ref>. In the modal representation, deformation is represented in terms of eigenvectors of a finite element (FE) model. Taken together, modes form an orthogonal basis set for describing nonrigid shape deformation. <p> The penalties can be derived directly from the modal stiffness obtained in modal analysis <ref> [19] </ref>. 5.2 Difference Decomposition To minimize the energy (Equation 10) a difference decomposition approach [12] is used. The approach offers the benefit that it requires the equivalent O (1) image gradient calculations and O (N ) image products per iteration.
Reference: [20] <author> A. Rosenfeld and A. Kak. </author> <title> Digital Picture Processing. </title> <publisher> Academic Press, </publisher> <year> 1976. </year>
Reference-contexts: The bounding contour (s) for a support region can be extracted via a standard 4-connected contour following algorithm <ref> [20] </ref>. Alternatively, the user can define a bounding contour for a region via a sketch interface. In general, the number of contour segments must be reduced. We utilize the tolerance band approach, where the merging stage can be iteratively alternated with recursive subdivision [14].
Reference: [21] <author> J. Ruppert. </author> <title> A Delaunay refinement algorithm for quality 2-dimensional mesh generation. </title> <journal> J. of Algorithms, </journal> <volume> 18(3):548 585, </volume> <year> 1995. </year>
Reference-contexts: We utilize the tolerance band approach, where the merging stage can be iteratively alternated with recursive subdivision [14]. In practice, a single merging pass is sufficient for a user-sketched boundary. The triangles are then generated using an adaptation of Ruppert's Delaunay refinement algorithm <ref> [21] </ref>. The algorithm accepts two parameters that control angle and triangle size constraints. To satisfy these constraints, additional interior vertices may be added to the original polygon during mesh generation. The source code is available from http://www.netlib.org/voronoi/.
Reference: [22] <author> S. Sclaroff. </author> <title> Modal Matching: A Method for Describing, Comparing, and Manipulating Digital Signals. </title> <type> PhD thesis, </type> <institution> MIT Media Lab, </institution> <year> 1995. </year>
Reference-contexts: The FE interpolation matrix, H, is formulated using Gaussian interpolants as specified in <ref> [22] </ref>. To give the further control over deformation, we allow the user to specify that only a subset of the observed deformation parameters is to be applied to the graphics model. <p> Texture map interpolation and rendering were accomplished using OpenGL. Given a triangle mesh, the FE model can be initialized using Gaussian interpolants with finite support. Due to space limitations, readers are directed to <ref> [22] </ref> for the formulation. The generalized eigenvectors and eigenval-ues are computed using code from the EISPACK library: http://www.netlib.org/eispack/. Finally, difference decomposition basis vectors are pre-computed. In practice, four basis vectors per model param eter are sufficient.
Reference: [23] <author> S. Sclaroff and J. Isidoro. </author> <title> Active blobs. </title> <booktitle> Proc. </booktitle> <address> ICCV, </address> <year> 1998. </year>
Reference-contexts: Furthermore, correlation-based methods are not well-suited to tracking moving objects that undergo scaling, rotation, or nonrigid deformation, since tracking is accomplished via correlation with a translating template. Larger deformations can be accomodated if the problem is posed in terms of deformable image registration <ref> [2, 23] </ref>. In particular, La Cascia, et al. [6], use this approach to register a texture-mapped polygonal model of a person's head with an incoming video sequence. <p> In this case, the object is simply a piece of foam rubber painted with a colored grid pattern. For tracking purposes, we build a two-dimensional de-formable texture mapped model of a region on the squishy object. We model the region using the active blob formulation <ref> [23] </ref>. Tracking of the voodoo doll's deformation is modeled in terms of a deformable, triangular mesh that captures object shape plus a color texture map that captures object appearance. <p> By taking advantage of texture mapping hardware, active blobs can track nonrigidly deforming shapes at speeds approaching video frame rate. We will now give an overview of the active blobs formulation. For a more detailed formulation, readers are refered to <ref> [23] </ref>. 4.1 Blob Warping In the active blobs formulation, nonrigid deformation is controlled by parametric functions. <p> The difference template D is computed using the same formula. Finally, the formulation can be extended to include a and a regularizing term that enforces the priors on the model parameters as described in <ref> [23] </ref>. 6 Using Observed Deformations to Animate Objects As the user manipulates the voodoo doll input device, we want corresponding deformations to be applied to the graphics model. The appropriate amounts of deformations to be applied are determined through tracking the deforming active blob as describe above.
Reference: [24] <author> R. Szeliski. </author> <title> Video mosaics for virtual environments. </title> <journal> IEEE CG&A, </journal> <volume> 16(2):2230, </volume> <year> 1996. </year>
Reference-contexts: Perhaps the simplest warping functions to be used are those of a 2D affine model or an eight parameter projective model <ref> [24] </ref>. Unfortunately, these functions are only suitable for approximating the rigid motion of a planar patch. The functions can be extended to include linear and quadratic polynomials [2]; however, the extended formulation cannot model general nonrigid motion.
Reference: [25] <author> D. Terzopoulos and K. Waters. </author> <title> Analysis and synthesis of facial image sequences using physical and anatomical models. </title> <journal> PAMI, </journal> <volume> 15(6), </volume> <year> 1993. </year>
Reference-contexts: Motion estimates were then used to drive a rigid graphics model of the user's head. Contour-based approaches have also been proposed. For instance, Terzopoulos and Waters <ref> [25] </ref> used snakes to follow lines drawn on the user's face. The snakes drive an intricate face model where muscle and skin are physically simulated. In a similar approach, Blake and Isard employed contours in gesture tracking [4], allowing the user to use hand motion as a three-dimensional mouse.
Reference: [26] <author> L. Williams. </author> <title> Performance-Driven Facial Animation. </title> <booktitle> Proc. SIGGRAPH, </booktitle> <address> 24(4):235242, </address> <year> 1990. </year>
Reference-contexts: Most previous systems that use vision to control graphics have tracked face, hand, or body gestures. Some previous approaches are based on tracking feature points across a sequence of video frames. Williams <ref> [26] </ref> tracked iridescent points placed on the face of the user. The points were used to pull regions of a triangular mesh of a human head. This approach has the disadvantage that the dots must be painted or pasted onto the user's face.
Reference: [27] <author> C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland. Pfinder: </author> <title> Real-time tracking of the human body. </title> <journal> PAMI, </journal> <volume> 19(7):780785, </volume> <year> 1997. </year>
Reference-contexts: Other researchers have used simple low-level image processing techniques such as normalized image correlation [7], or background subtraction and thresholding <ref> [5, 15, 16, 27] </ref> for tracking moving body parts, facial features, and hand gestures. The resulting tracking output can be used to drive the animation of a face model [9], to drive manipulation of virtual objects [15], or to interact with animated creatures in a virtual world [27, 16, 5]. <p> The resulting tracking output can be used to drive the animation of a face model [9], to drive manipulation of virtual objects [15], or to interact with animated creatures in a virtual world <ref> [27, 16, 5] </ref>. Such low-level methods are surprisingly useful for qualitative measures (e.g., person is standing, object is bending) but are not 1 a. b. c. (a), and a deformable color region on the object used for tracking (b).
References-found: 27

