URL: http://www.cs.cmu.edu/~yx/papers/ISCV95.ps.Z
Refering-URL: http://www.ius.cs.cmu.edu/afs/cs/usr/yx/www/HomePage.html
Root-URL: 
Email: yx@cs.cmu.edu sas@cs.cmu.edu  
Title: Dense Structure From A Dense Optical Flow Sequence  
Author: Yalin Xiong Steven A. Shafer 
Address: Pittsburgh, PA 15213  
Affiliation: The Robotics Institute Carnegie Mellon University  
Abstract: This paper presents a structure-from-motion system which delivers dense structural information from a sequence of dense optical flows. Most traditional feature-based approaches cannot be extended to compute dense structure due to impractical computational complexity. We demonstrate that by decomposing uncertainty information into independent and correlated parts we can decrease these complexities from O(N 2 ) to O(N ), where N is the number of pixels in the images. We also show that this dense structure-from-motion system requires only local optical flows, i.e. image matchings between two adjacent frames, instead of the tracking of features over a long sequence. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Golad Adiv. </author> <title> Determining three-dimensional motion and structure from optical flow generated by several moving objects. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(4) </volume> <pages> 384-401, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: While stripping a full-resolution image to a handful of features may greatly simplify the algorithm and the computation, most of information contained in the image is lost. Traditional flow-based methods, such as reported by Bruss & Horn [4], Heeger & Jepson [5], Adiv <ref> [1] </ref>, have concentrated on either solving the problem of recovering motion and structure from a single optical flow field or using very low resolution optical flows. As far as we know, little has been done to achieve a dense structure-from-motion system except Heel's work in [6].
Reference: [2] <author> A. Azarbayejani and Alex Pentland. </author> <title> Recursive estimation of motion, structure, and focal length. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <note> 1995. to appear. </note>
Reference-contexts: Feature-based methods compute the relative structure information among features by analyzing their 2D motions in images. Examples of such systems were reported by Tomasi & Kanade [10], Broida et al [3] and Azarbayejani & Pentland <ref> [2] </ref> . Because the whole analysis is limited to features which usually number not more than a hundred, the results from those systems yield very sparse shape information.
Reference: [3] <author> Ted J. Broida, S. Chandrashekhar, and Rama Chellappa. </author> <title> Recursive estimation of 3D motion from a monocular image sequence. </title> <journal> IEEE Transactions on Aerosp. Electron. Syst., </journal> <volume> 26(4) </volume> <pages> 639-656, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Feature-based methods compute the relative structure information among features by analyzing their 2D motions in images. Examples of such systems were reported by Tomasi & Kanade [10], Broida et al <ref> [3] </ref> and Azarbayejani & Pentland [2] . Because the whole analysis is limited to features which usually number not more than a hundred, the results from those systems yield very sparse shape information. <p> This paper shows our attempt to overcome these difficulties. We demonstrate a system which incrementally accumulates dense structural information from a sequence of optical flows. The system has the following features: 1. The system is based on EKF (extended Kalman filtering) as proposed in <ref> [3] </ref>. 2. The formulation of the structure from motion uses separate independent and correlated structural uncertainties. By employing the separation and other mathematical techniques such as Sherman-Morrison-Woodbury inversion and principal component analysis, we can achieve an O (N ) numerical algorithm to compute Kalman filtering. 3.
Reference: [4] <author> Anna R. Bruss and Berthold K. P. Horn. </author> <title> Passive navigation. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 21 </volume> <pages> 3-20, </pages> <year> 1983. </year>
Reference-contexts: While stripping a full-resolution image to a handful of features may greatly simplify the algorithm and the computation, most of information contained in the image is lost. Traditional flow-based methods, such as reported by Bruss & Horn <ref> [4] </ref>, Heeger & Jepson [5], Adiv [1], have concentrated on either solving the problem of recovering motion and structure from a single optical flow field or using very low resolution optical flows.
Reference: [5] <author> David J. Heeger and Allan D. Jepson. </author> <title> Subspace methods for recovering rigid motion I: Algorithm and implementation. </title> <journal> International Journal of Computer Vision, </journal> <volume> 7(2) </volume> <pages> 95-117, </pages> <year> 1992. </year>
Reference-contexts: While stripping a full-resolution image to a handful of features may greatly simplify the algorithm and the computation, most of information contained in the image is lost. Traditional flow-based methods, such as reported by Bruss & Horn [4], Heeger & Jepson <ref> [5] </ref>, Adiv [1], have concentrated on either solving the problem of recovering motion and structure from a single optical flow field or using very low resolution optical flows. As far as we know, little has been done to achieve a dense structure-from-motion system except Heel's work in [6].
Reference: [6] <author> Joachim Heel. </author> <title> Temporally integrated surface reconstruction. </title> <booktitle> In Proceedings of International Conference on Computer Vision, </booktitle> <pages> pages 292-295, </pages> <year> 1990. </year>
Reference-contexts: As far as we know, little has been done to achieve a dense structure-from-motion system except Heel's work in <ref> [6] </ref>. <p> Therefore, we need to transform the previous depth map into the current camera coordinate system and resample the depth map according to the current sensor grid. Though the depth map and its independent uncertainty can be easily interpolated as in <ref> [8, 6] </ref>, the interpolation of correlated uncertainty is a new problem.
Reference: [7] <author> H. C. Longuet-Higgins and K. Prazdny. </author> <title> The interpretation of a moving retinal image. </title> <journal> Proc. R. Soc. Lond. B, </journal> <volume> 208 </volume> <pages> 385-397, </pages> <year> 1980. </year>
Reference-contexts: of the camera with respect to the scene is composed of a translation velocity (U; V; W ) and a rotation velocity (A; B; C), we have the following relation between the flow velocity (v x ; v y ) and the depth Z of pixel location (x; y) from <ref> [7] </ref>: v x = Z v y = Z If we designate the camera motion parameterization as ~ M 0 = (U; V; W; A; B; C) T and the flow velocity as ~v = (v x ; v y ) T , the above equation can be expressed as ~v
Reference: [8] <author> Larry Matthies, Richard Szeliski, and Takeo Kanade. </author> <title> Kalman filter-based algorithms for estimating depth from image sequences. </title> <journal> International Journal of Computer Vision, </journal> <volume> 3 </volume> <pages> 209-236, </pages> <year> 1989. </year>
Reference-contexts: Therefore, we need to transform the previous depth map into the current camera coordinate system and resample the depth map according to the current sensor grid. Though the depth map and its independent uncertainty can be easily interpolated as in <ref> [8, 6] </ref>, the interpolation of correlated uncertainty is a new problem.
Reference: [9] <author> J. Inigo Thomas and J. Oliensis. </author> <title> Incorporating motion error in multi-frame structure from motion. </title> <type> Technical Report COINS TR91-36, </type> <institution> Computer and Information Science, University of Mas-sachusetts at Amherst, </institution> <year> 1991. </year>
Reference-contexts: Unfortunately, if we apply this EKF scheme directly to the dense structure recovery problem, the computation and memory requirements are insurmountable. As pointed out in <ref> [9] </ref>, the uncertainties of the depth values Z i ; i = 1; 2; :::; N are correlated due to uncertain motion. Thus the covariance matrix P is a full N fi N matrix.
Reference: [10] <author> Carlo Tomasi. </author> <title> Shape and motion from image streams: A factorization method. </title> <type> Technical Report CMU-CS-91-172, </type> <institution> The School of Computer Science, Carnegie Mellon University, </institution> <year> 1991. </year>
Reference-contexts: Most approaches proposed in the literature can be classified according to whether they are based on features or optical flows. Feature-based methods compute the relative structure information among features by analyzing their 2D motions in images. Examples of such systems were reported by Tomasi & Kanade <ref> [10] </ref>, Broida et al [3] and Azarbayejani & Pentland [2] . Because the whole analysis is limited to features which usually number not more than a hundred, the results from those systems yield very sparse shape information.
Reference: [11] <author> Yalin Xiong and Steven A. Shafer. </author> <title> Dense structure from a dense optical flow sequence. </title> <type> Technical Report CMU-RI-TR-95-10, </type> <institution> The Robotics Institute, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: Because the flow uncertainties of different pixels are independent, C d is an N fi N diagonal matrix. Inverting the N fi N matrix in O (N) time will be explained in the next section. More details on minimizing the objective function could be found in <ref> [11] </ref>. We introduce the concept of "dynamic motion parameterization" to equalize sensitivities of motion parameters. <p> Concatenation in multiplication: The multiplication of two matrices can still be represented in the same form: (C 1 +U 1 V T 2 ) = C 3 +U 3 V T Using the above tools as detailed in <ref> [11] </ref>, once the a priori covariance P can be represented in the format of Eq. 10, the posteriori covariance P + can also be represented in the same format as P + = C mp C T C pp (C 4 + U 4 V T where the only difference is <p> In general, there are l = 6k + 6 non-zero eigenvalues and corresponding eigenvectors, which can be computed easily as in <ref> [11] </ref>. Because the outer product represents covariance which must be symmetric, it can be decomposed by SVD (Singular Value Decomposition), in which columns of orthogonal matrices are eigenvectors and the elements of the diagonal matrix are eigenval-ues. <p> Though the depth map and its independent uncertainty can be easily interpolated as in [8, 6], the interpolation of correlated uncertainty is a new problem. As explained in <ref> [11] </ref> (Appendix B), since a correlated un certainty is always a positive definite symmetric outer product, it can be represented as UV T = BB T ; where B is an N fik matrix just like U. <p> We designate this process as the "forward transform". Basically the forward transform involves only linear transformations such as 3D translation and rotation. More details on trans forming uncertainty matrices could be found in <ref> [11] </ref>. 6 Experiments We tested our system on real image sequences taken by a Sony XC-75 video camera. The relative camera movements in all the sequences involve both rotation and translation. 6.1 Ambiguities It is well known that there are intrinsic ambiguities in recovering structure from motion. <p> It clearly shows the converging shape resulting from recursively combining information from multiple frames. Due to limited space, we are unable to show more experiments on real image sequences, which could be found in <ref> [11] </ref>.
Reference: [12] <author> Yalin Xiong and Steven A. Shafer. </author> <title> Hypergeo-metric filters for optical flow and affine matching. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 771-776, </pages> <year> 1995. </year>
Reference-contexts: The rotations and translations of the camera with respect to the straw hat were discontinuous. The camera had about an 11 ffi field of view. The optical flow and its uncertainty were computed using hypergeometric filters <ref> [12] </ref>. computed after the corresponding frames. It clearly shows the converging shape resulting from recursively combining information from multiple frames. Due to limited space, we are unable to show more experiments on real image sequences, which could be found in [11].
References-found: 12

