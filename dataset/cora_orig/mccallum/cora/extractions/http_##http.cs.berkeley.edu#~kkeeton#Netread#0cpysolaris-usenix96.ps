URL: http://http.cs.berkeley.edu/~kkeeton/Netread/0cpysolaris-usenix96.ps
Refering-URL: http://http.cs.berkeley.edu/~kkeeton/Netread/netread.html
Root-URL: 
Title: Abstract  
Abstract: This paper describes a new feature in Solaris that uses virtual memory remapping combined with checksumming support from the networking hardware, to eliminate data-touching overhead from the TCP/IP protocol stack. By implementing page remapping operations at the right level of the operating system, and caching MMU mappings to take advantage of locality of reference, significant performance gain is attained on certain hardware platforms. Nevertheless, the performance improvement over CPU copying varies, depending on the host memory cache architecture, MMU design, and application behavior. We begin by comparing different zero-copy schemes, and explain our preference for page remapping and copy-on-write (COW) techniques. We then describe our implementation, and present its performance characteristics under a number of different parameters. We conclude with ideas for future improvements. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Accetta, R. Baron, D. Golub, R. Rashid, A. Tevanian, and M. Young. </author> <title> Mach: A New Kernel Foundation for UNIX Development, </title> <booktitle> Proceedings of the Summer 1986 USENIX Technical Conference and Exhibition, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: Further, for broader market appeal we felt it unwise to rely on specialized hardware, as some of the cited examples in literature did. We turned to virtual memory remapping and copy-on-write technique, both being widely adopted in operating system design to avoid copying <ref> [1, 2, 8, 16] </ref>. Although these operations are not without expense, they often can be applied transparently, and so fit our goal of minimizing software module change. Networking adaptors used are Suns SBus-based ATM interface cards, capable of both 155 and 622 Mb/s, OC-3 and OC-12 respectively.
Reference: [2] <author> D. R. Cheriton. </author> <title> The V distributed system, </title> <journal> Communications of the ACM, vol.31, </journal> <volume> no.3, </volume> <month> March </month> <year> 1988. </year>
Reference-contexts: Further, for broader market appeal we felt it unwise to rely on specialized hardware, as some of the cited examples in literature did. We turned to virtual memory remapping and copy-on-write technique, both being widely adopted in operating system design to avoid copying <ref> [1, 2, 8, 16] </ref>. Although these operations are not without expense, they often can be applied transparently, and so fit our goal of minimizing software module change. Networking adaptors used are Suns SBus-based ATM interface cards, capable of both 155 and 622 Mb/s, OC-3 and OC-12 respectively.
Reference: [3] <author> D. D. Clark, V. Jacobson, J. Romkey, and H. Sal-wen. </author> <title> An Analysis of TCP Processing Overhead, </title> <journal> IEEE Communications Magazine, </journal> <pages> 23-29, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Networking software or hardware is often a major bottleneck in this respect, and considerable research focuses on delivering the vast bandwidth effortlessly. Analysis of components of networking software reveals that data copy and checksum overhead dominates processing time for high throughput applications <ref> [3, 10, 17] </ref>. Older generation networking software and hardware often required multiple data copy and separate data checksum operations on each byte of a data packet. The past few years have seen a number of successful implementations (such as in Solaris release 2.4) introducing single-copy (CPU copy).
Reference: [4] <author> E. Cooper, P. Steenkiste, R. Sansom, and B. Zill. </author> <title> Protocol Implementation on the Nectar Communication Processor, </title> <booktitle> Proceedings of SIG-COMM 90 Conference on Comm. Architectures, Protocols and Applications, </booktitle> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: Limited interface memory could pose a serious resource problem. Memory hogs or bug-ridden applications with memory leaks can easily deplete the interface memory available for use [15]. A variant of this approach that uses a dedicated coprocessor for protocol processing is described by Cooper et al <ref> [4] </ref>. 3.2.2 Kernel-network shared memory To alleviate the resource problem described above, this scheme lets the operating system kernel manage the interface memory, and uses direct memory access (DMA), or program I/O (PIO, i.e. CPU copy), to move data between interface memory and application buffers.
Reference: [5] <author> C. Dalton, G. Watson, D. Banks, C. Calamvokis, A. Edwards, and J. Lumley. </author> <title> Afterburner - A network-independent card provides architectural support for high-performance protocols, </title> <journal> IEEE Network, </journal> <month> July </month> <year> 1993. </year> ***. <title> Due to a buffer alignment issue described in Section 3.2.4, zero-copy is currently not supported in NFS. </title>
Reference-contexts: A further advantage lies in existing applications not being required to undergo modifications, since the sockets copy semantic is fully maintained by this approach. Afterburner, a classic example in this category, is described by Dalton et al. in <ref> [5] </ref>. The software to support this scheme can be complicated. Kernel networking buffer management code must be enhanced to support this special pool of memory from the network interface, which it co-manages with the device driver.
Reference: [6] <author> P. Druschel and L Peterson. Fbufs: </author> <title> A High--Bandwidth Cross-Domain Transfer Facility, </title> <booktitle> Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: One proposal in this category is called fast buffers (fbufs) described by Druschel and Peterson in <ref> [6] </ref>. It uses a per-process buffer pool that is pre-mapped in both the user and kernel address spaces, thus eliminating the user-kernel data copy. Ideally, each data byte crosses the memory bus exactly once, so overhead is low and no special, pro-cessor-addressable interface memory is needed.
Reference: [7] <author> P. Druschel, L. Peterson, and B. Davie. </author> <title> Experiences with a High-Speed Network Adaptor: A Software Perspective, </title> <booktitle> Proceedings of SIG-COMM94 Conference on Comm. Architectures, Protocols and Applications, </booktitle> <month> Aug. </month> <year> 1994. </year>
Reference: [8] <author> R. Fitzgerald and R. F. Rashid. </author> <title> The Integration of Virtual Memory Management and Interpro-cess Communication in Accent, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4.2, </volume> <month> May </month> <year> 1986. </year>
Reference-contexts: Further, for broader market appeal we felt it unwise to rely on specialized hardware, as some of the cited examples in literature did. We turned to virtual memory remapping and copy-on-write technique, both being widely adopted in operating system design to avoid copying <ref> [1, 2, 8, 16] </ref>. Although these operations are not without expense, they often can be applied transparently, and so fit our goal of minimizing software module change. Networking adaptors used are Suns SBus-based ATM interface cards, capable of both 155 and 622 Mb/s, OC-3 and OC-12 respectively.
Reference: [9] <author> R. Gingell, J. Moran and W. Shannon. </author> <title> Virtual Memory Architecture in SunOS, </title> <booktitle> Proceedings of the USENIX Conference, </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: We therefore conclude that such a reduction in protection is not unreasonable. 4. Design and Implementation In this section, we outline our design goals, constraints, and briey discuss our implementation. 4.1. Design Goals Fit the design into Solaris VM architecture <ref> [9, 12] </ref>. Build the new remapping + copy-on-write func tionality on top of the existing VM base. Implement the new functionality outside of VM segment layer and vnode layer. Thus it can work transparently on any type of memory objects applications choose to use. <p> If so, simply reuse its kernel address. Otherwise, map in the user page. 3. Change the user protection to read-only. Mark the user page as copy-on-write page. When the transport is finished with the user page - 1. Unlock the page. . Hardware Address Translation. See <ref> [9] </ref>. **. Solaris networking subsystem uses AT&T STREAMS framework. 2. Restore the user protection back to read-write. Tear down the copy-on-write state. 3. Do NOT unmap and deallocate the MMU resources of the kernel mapping.
Reference: [10] <author> J. Kay and J. Pasquale. </author> <title> Measurement, Analysis, and Improvement of UDP/IP Throughput for the DECstation 5000, </title> <booktitle> Proceedings of the Winter USENIX Conference, </booktitle> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Networking software or hardware is often a major bottleneck in this respect, and considerable research focuses on delivering the vast bandwidth effortlessly. Analysis of components of networking software reveals that data copy and checksum overhead dominates processing time for high throughput applications <ref> [3, 10, 17] </ref>. Older generation networking software and hardware often required multiple data copy and separate data checksum operations on each byte of a data packet. The past few years have seen a number of successful implementations (such as in Solaris release 2.4) introducing single-copy (CPU copy).
Reference: [11] <author> K. Kleinpaste, P. Steenkiste, and B. Zill. </author> <title> Software Support for Outboard Buffering and Checksumming, </title> <booktitle> Proceedings of SIGCOMM95 Conference on Comm. Architectures, Protocols and Applications, </booktitle> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: If this doesnt happen in time, the transmit side will time-out and generate unnecessary retransmissions. If DMA is used, it requires pinning and unpinning of user pages, and possibly mapping from the kernel context, which add to the costs <ref> [11] </ref>. Applications can no longer hog the interface memory directly, but TCP retransmit buffers still reside in the interface memory.
Reference: [12] <author> J. Moran. </author> <title> SunOS Virtual Memory Implementation, </title> <booktitle> Proceedings of EUUG Conference. </booktitle> <address> London England, </address> <month> Spring </month> <year> 1988. </year>
Reference-contexts: We therefore conclude that such a reduction in protection is not unreasonable. 4. Design and Implementation In this section, we outline our design goals, constraints, and briey discuss our implementation. 4.1. Design Goals Fit the design into Solaris VM architecture <ref> [9, 12] </ref>. Build the new remapping + copy-on-write func tionality on top of the existing VM base. Implement the new functionality outside of VM segment layer and vnode layer. Thus it can work transparently on any type of memory objects applications choose to use.
Reference: [13] <author> J. Smith and C. Brendan S. Traw. </author> <title> Giving Applications Access to Gb/s Networking, </title> <journal> IEEE Network, </journal> <month> July </month> <year> 1993. </year>
Reference-contexts: Different Zero Copy Schemes Several zero-copy schemes have been proposed in the literature. Smith et al. <ref> [13] </ref> outlines a variety of approaches to host interface design and supporting software. Steenkiste [14] presents a taxonomy of host interfaces and their numbers and types of data movement across a memory bus.
Reference: [14] <author> P. Steenkiste. </author> <title> A Systematic Approach to Host Interface Design for High-Speed Networks, </title> <booktitle> IEEE Computer, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: Different Zero Copy Schemes Several zero-copy schemes have been proposed in the literature. Smith et al. [13] outlines a variety of approaches to host interface design and supporting software. Steenkiste <ref> [14] </ref> presents a taxonomy of host interfaces and their numbers and types of data movement across a memory bus.
Reference: [15] <author> B. Traw. </author> <title> Applying Architectural Parallelism to High Performance Network Subsystems, </title> <type> Ph.D. Dissertation, </type> <institution> University of Pennsylvania, </institution> <year> 1995. </year>
Reference-contexts: Software compatibility and portability do not exist. Limited interface memory could pose a serious resource problem. Memory hogs or bug-ridden applications with memory leaks can easily deplete the interface memory available for use <ref> [15] </ref>.
Reference: [16] <author> S.-Y. Tzou and D. P. Anderson. </author> <title> The Performance of Message-passing using Restricted Virtual Memory Remapping, </title> <journal> Software - Practice and Experience vol. </journal> <volume> 21, </volume> <month> March </month> <year> 1991. </year>
Reference-contexts: Further, for broader market appeal we felt it unwise to rely on specialized hardware, as some of the cited examples in literature did. We turned to virtual memory remapping and copy-on-write technique, both being widely adopted in operating system design to avoid copying <ref> [1, 2, 8, 16] </ref>. Although these operations are not without expense, they often can be applied transparently, and so fit our goal of minimizing software module change. Networking adaptors used are Suns SBus-based ATM interface cards, capable of both 155 and 622 Mb/s, OC-3 and OC-12 respectively.

References-found: 16

