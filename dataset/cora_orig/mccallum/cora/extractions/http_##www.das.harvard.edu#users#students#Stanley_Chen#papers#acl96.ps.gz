URL: http://www.das.harvard.edu/users/students/Stanley_Chen/papers/acl96.ps.gz
Refering-URL: http://www.das.harvard.edu/users/students/Stanley_Chen/Stanley_Chen.html
Root-URL: 
Email: sfc@eecs.harvard.edu  goodman@eecs.harvard.edu  
Title: An Empirical Study of Smoothing Techniques for Language Modeling  
Author: Stanley F. Chen Joshua Goodman 
Address: 33 Oxford St. Cambridge, MA 02138  33 Oxford St. Cambridge, MA 02138  
Affiliation: Harvard University Aiken Computation Laboratory  Harvard University Aiken Computation Laboratory  
Date: June 1996  
Note: To appear in Proceedings of the 34th Annual Meeting of the ACL,  
Abstract: We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which out perform existing methods.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bahl, Lalit R., Frederick Jelinek, </author> <title> and Robert L. </title>
Reference: <author> Mercer. </author> <year> 1983. </year> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5(2):179-190, </volume> <month> March. </month>
Reference-contexts: 1 Introduction Smoothing is a technique essential in the construction of n-gram language models, a staple in speech recognition <ref> (Bahl, Jelinek, and Mercer, 1983) </ref> as well as many other domains (Church, 1988; Brown et al., 1990; Kernighan, Church, and Gale, 1990).
Reference: <author> Brown, Peter F., John Cocke, Stephen A. DellaPi-etra, Vincent J. DellaPietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. </author> <year> 1990. </year> <title> A statistical approach to machine translation. </title> <journal> Computational Linguistics, </journal> <volume> 16(2) </volume> <pages> 79-85, </pages> <month> June. </month>
Reference: <author> Brown, Peter F., Stephen A. DellaPietra, </author> <note> Vincent J. </note>
Reference: <author> DellaPietra, Jennifer C. Lai, and Robert L. Mercer. </author> <year> 1992. </year> <title> An estimate of an upper bound for the entropy of English. </title> <journal> Computational Linguistics, </journal> <volume> 18(1) </volume> <pages> 31-40, </pages> <month> March. </month>
Reference: <author> Chen, Stanley F. </author> <year> 1996. </year> <title> Building Probabilistic Models for Natural Language. </title> <type> Ph.D. thesis, </type> <institution> Harvard University. </institution> <note> In preparation. </note>
Reference: <author> Church, Kenneth. </author> <year> 1988. </year> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143. </pages>
Reference-contexts: In addition, it would be interesting to see whether these results extend to fields other than language modeling where smoothing is used, such as prepositional phrase attachment (Collins and Brooks, 1995), part-of-speech tagging <ref> (Church, 1988) </ref>, and stochastic parsing (Magerman, 1994). Acknowledgements The authors would like to thank Stuart Shieber and the anonymous reviewers for their comments on previous versions of this paper.
Reference: <author> Church, Kenneth W. and William A. Gale. </author> <year> 1991. </year> <title> A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 19-54. </pages>
Reference: <author> Collins, Michael and James Brooks. </author> <year> 1995. </year> <title> Prepositional phrase attachment through a backed-off model. </title> <editor> In David Yarowsky and Kenneth Church, editors, </editor> <booktitle> Proceedings of the Third Workshop on Very Large Corpora, </booktitle> <pages> pages 27-38, </pages> <address> Cambridge, MA, </address> <month> June. </month>
Reference-contexts: In addition, it would be interesting to see whether these results extend to fields other than language modeling where smoothing is used, such as prepositional phrase attachment <ref> (Collins and Brooks, 1995) </ref>, part-of-speech tagging (Church, 1988), and stochastic parsing (Magerman, 1994). Acknowledgements The authors would like to thank Stuart Shieber and the anonymous reviewers for their comments on previous versions of this paper.
Reference: <author> Gale, William A. and Kenneth W. Church. </author> <year> 1990. </year> <title> Estimation procedures for language context: poor estimates are worse than none. </title> <booktitle> In COMP-STAT, Proceedings in Computational Statistics, 9th Symposium, </booktitle> <pages> pages 69-74, </pages> <address> Dubrovnik, Yu-goslavia, </address> <month> September. </month>
Reference: <author> Gale, William A. and Kenneth W. Church. </author> <year> 1994. </year>
Reference: <editor> What's wrong with adding one? In N. Oostdijk and P. de Haan, editors, </editor> <booktitle> Corpus-Based Research into Language. </booktitle> <address> Rodolpi, Amsterdam. </address>
Reference: <author> Gale, William A. and Geoffrey Sampson. </author> <year> 1995. </year> <title> Good-Turing frequency estimation without tears. </title> <journal> Journal of Quantitative Linguistics, </journal> <note> 2(3). To appear. </note>
Reference: <author> Good, I.J. </author> <year> 1953. </year> <title> The population frequencies of species and the estimation of population parameters. </title> <journal> Biometrika, </journal> <volume> 40(3 and </volume> 4):237-264. 
Reference-contexts: Gale and Church (1990; 1994) have argued that this method generally performs poorly. The Good-Turing estimate <ref> (Good, 1953) </ref> is central to many smoothing techniques. It is not used directly for n-gram smoothing because, like additive smoothing, it does not perform the interpolation of lower- and higher-order models essential for good performance.
Reference: <author> Jeffreys, H. </author> <year> 1948. </year> <title> Theory of Probability. </title> <publisher> Clarendon Press, </publisher> <address> Oxford, </address> <note> second edition. </note>
Reference: <author> Jelinek, Frederick and Robert L. Mercer. </author> <year> 1980. </year> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Proceedings of the Workshop on Pattern Recognition in Practice, </booktitle> <address> Amster-dam, The Netherlands: </address> <publisher> North-Holland, </publisher> <month> May. </month>
Reference-contexts: For ex ample, we can take P interp (w i jw i1 ) = P ML (w i jw i1 ) + (1 )P ML (w i ) getting the behavior that bigrams involving common words are assigned higher probabilities <ref> (Jelinek and Mercer, 1980) </ref>. 2 Previous Work The simplest type of smoothing used in practice is additive smoothing (Lidstone, 1920; Johnson, 1932; Jeffreys, 1948), where we take P add (w i jw i1 c (w i c (w i1 in+1 ) + ffijV j (2) and where Lidstone and Jeffreys advocate
Reference: <author> Johnson, W.E. </author> <year> 1932. </year> <title> Probability: deductive and inductive problems. </title> <journal> Mind, </journal> <volume> 41 </volume> <pages> 421-423. </pages>
Reference: <author> Katz, Slava M. </author> <year> 1987. </year> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(3):400-401, </volume> <month> March. </month>
Reference-contexts: Referring to equation (2), we fix ffi = 1 in plus-one smoothing. In plus-delta, we consider any ffi. 4.2.3 Katz Smoothing (katz) While the original paper <ref> (Katz, 1987) </ref> uses a single parameter k, we instead use a different k for each n &gt; 1, k n .
Reference: <author> Kernighan, M.D., K.W. Church, and W.A. Gale. </author> <year> 1990. </year> <title> A spelling correction program based on a noisy channel model. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Computational Linguistics, </booktitle> <pages> pages 205-210. </pages>
Reference: <author> Lidstone, G.J. </author> <year> 1920. </year> <title> Note on the general case of the Bayes-Laplace formula for inductive or a posteriori probabilities. </title> <journal> Transactions of the Faculty of Actuaries, </journal> <volume> 8 </volume> <pages> 182-192. </pages>
Reference: <author> MacKay, David J. C. and Linda C. Peto. </author> <year> 1995. </year> <title> A hierarchical Dirichlet language model. </title> <booktitle> Natural Language Engineering, </booktitle> <volume> 1(3) </volume> <pages> 1-19. </pages>
Reference: <author> Magerman, David M. </author> <year> 1994. </year> <title> Natural Language Parsing as Statistical Pattern Recognition. </title> <type> Ph.D. thesis, </type> <institution> Stanford University, </institution> <month> February. </month>
Reference-contexts: In addition, it would be interesting to see whether these results extend to fields other than language modeling where smoothing is used, such as prepositional phrase attachment (Collins and Brooks, 1995), part-of-speech tagging (Church, 1988), and stochastic parsing <ref> (Magerman, 1994) </ref>. Acknowledgements The authors would like to thank Stuart Shieber and the anonymous reviewers for their comments on previous versions of this paper.
Reference: <author> Nadas, Arthur. </author> <year> 1984. </year> <title> Estimation of probabilities in the language model of the IBM speech recognition system. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-32(4):859-861, </volume> <month> August. </month>
Reference: <author> Press, W.H., B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling. </author> <year> 1988. </year> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference-contexts: In each run except as noted below, optimal values for the parameters of the given technique were searched for using Powell's search algorithm as realized in Numerical Recipes in C <ref> (Press et al., 1988, pp. 309-317) </ref>. Parameters were chosen to optimize the cross-entropy of one of the development test sets associated with the given training set.
References-found: 24

