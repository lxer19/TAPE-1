URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3599/3599.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Fuzzy Finite-state Automata Can Be Deterministically Encoded into Recurrent Neural Networks  
Author: Christian W. Omlin a Karvel K. Thornber a C. Lee Giles a;b 
Address: Princeton, NJ 08540  College Park, MD 20742  
Affiliation: a NEC Research Institute,  b UMIACS, U. of Maryland,  
Pubnum: Technical Report CS-TR-3599 and UMIACS-96-12  
Abstract: There has been an increased interest in combining fuzzy systems with neural networks because fuzzy neural systems merge the advantages of both paradigms. On the one hand, parameters in fuzzy systems have clear physical meanings and rule-based and linguistic information can be incorporated into adaptive fuzzy systems in a systematic way. On the other hand, there exist powerful algorithms for training various neural network models. However, most of the proposed combined architectures are only able to process static input-output relationships, i.e. they are not able to process temporal input sequences of arbitrary length. Fuzzy finite-state automata (FFAs) can model dynamical processes whose current state depends on the current input and previous states. Unlike in the case of deterministic finite-state automata (DFAs), FFAs are not in one particular state, rather each state is occupied to some degree defined by a membership function. Based on previous work on encoding DFAs in discrete-time, second-order recurrent neural networks, we propose an algorithm that constructs an augmented recurrent neural network that encodes a FFA and recognizes a given fuzzy regular language with arbitrary accuracy. We then empirically verify the encoding methodology by measuring string recognition performance of recurrent neural networks which encode large randomly generated FFAs. In particular, we examine how the networks' performance varies as a function of synaptic weight strength.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon, A. Dewdney, and T. Ott, </author> <title> "Efficient simulation of finite automata by neural nets," </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> vol. 38, no. 2, </volume> <pages> pp. 495-514, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Alternatively, a DFA M can also be considered a generator which generates the regular language L (M ). 2.2 Network Construction Various methods have been proposed for implementing DFAs in recurrent neural networks <ref> [1, 2, 12, 13, 22, 28, 32] </ref>. <p> 3.3 A fuzzy finite-state automaton (FFA) f M is a 6-tuple f M =&lt; ; Q; Z; e R; ffi; ! &gt; where , Q, and q 0 are the same as in DFAs; Z is a finite output alphabet, e R is the fuzzy initial state, ffi : fiQfi <ref> [0; 1] </ref> ! Q is the fuzzy transition map and ! : Q ! Z is the output map. <p> greatly simplifies the encoding of FFAs in recurrent networks with continuous discriminant functions: Theorem 3.2 Given a regular fuzzy automaton f M , there exists a deterministic finite-state automaton M with output alphabet Z f : is a production weightg [ f0g which computes the membership function : fl ! <ref> [0; 1] </ref> of the language L ( f M ). The constructive proof can be found in [44]. <p> On the other hand, FFAs can be in several states at any given time with different degrees of vagueness; vagueness is specified by a real number from the interval <ref> [0; 1] </ref>. Theorem 3.2 enables us to transform any FFA into a deterministic automaton which computes the same membership function : fl ! [0; 1]. We just need to demonstrate how to implement the computation of with continuous discriminant functions. 12 with weighted state transitions. <p> other hand, FFAs can be in several states at any given time with different degrees of vagueness; vagueness is specified by a real number from the interval <ref> [0; 1] </ref>. Theorem 3.2 enables us to transform any FFA into a deterministic automaton which computes the same membership function : fl ! [0; 1]. We just need to demonstrate how to implement the computation of with continuous discriminant functions. 12 with weighted state transitions. State 1 is the automaton's start state; accepting states are drawn with double circles.
Reference: [2] <author> R. Alquezar and A. Sanfeliu, </author> <title> "An algebraic framework to represent finite state machines in single-layer recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 7, no. 5, </volume> <editor> p. </editor> <volume> 931, </volume> <year> 1995. </year>
Reference-contexts: Alternatively, a DFA M can also be considered a generator which generates the regular language L (M ). 2.2 Network Construction Various methods have been proposed for implementing DFAs in recurrent neural networks <ref> [1, 2, 12, 13, 22, 28, 32] </ref>.
Reference: [3] <author> H. Berenji and P. Khedkar, </author> <title> "Learning and fine tuning fuzzy logic controllers through reinforcement," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 5, </volume> <pages> pp. 724-740, </pages> <year> 1992. </year>
Reference-contexts: Neural network representations of fuzzy logic interpolation have also been used within the context of reinforcement learning <ref> [3] </ref>. A typical fuzzy neural network used for intelligent control is shown in figure 1.
Reference: [4] <author> J. Bezdek, </author> <title> ed., </title> <journal> IEEE Transactions on Neural Networks Special Issue on Fuzzy Logic and Neural Networks, </journal> <volume> vol. 3. </volume> <booktitle> IEEE Neural Networks Council, </booktitle> <year> 1992. </year>
Reference-contexts: 1 Introduction 1.1 Fuzzy Systems and Neural Networks There has been an increased interest in combining artificial neural networks and fuzzy systems (see <ref> [4] </ref> for a collection of papers). Fuzzy logic [55] provides a mathematical foundation for approximate reasoning; fuzzy 1 logic controllers have proven very successful in a variety of applications [6, 23, 24, 34].
Reference: [5] <author> M. Casey, </author> <title> Computation in discrete-time dynamical systems. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, University of California at San Diego, La Jolla, </institution> <address> CA, </address> <year> 1995. </year>
Reference: [6] <author> S. Chiu, S. Chand, D. Moore, and A. Chaudhary, </author> <title> "Fuzzy logic for control of roll and moment for a flexible wing aircraft," </title> <journal> IEEE Control Systems Magazine, </journal> <volume> vol. 11, no. 4, </volume> <pages> pp. 42-48, </pages> <year> 1991. </year>
Reference-contexts: Fuzzy logic [55] provides a mathematical foundation for approximate reasoning; fuzzy 1 logic controllers have proven very successful in a variety of applications <ref> [6, 23, 24, 34] </ref>. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values. Furthermore, rule-based information can be incorporated into fuzzy systems in a systematic way.
Reference: [7] <author> A. Cleeremans, D. Servan-Schreiber, and J. McClelland, </author> <title> "Finite state automata and simple recurrent recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 3, </volume> <pages> pp. 372-381, </pages> <year> 1989. </year> <month> 17 </month>
Reference: [8] <author> G. Cybenko, </author> <title> "Approximation by superpositions of a sigmoidal function," </title> <journal> Mathematics of Control, Sig--nals, and Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 303-314, </pages> <year> 1989. </year>
Reference-contexts: Artificial neural networks have become valuable computational tools in their own right for tasks such as pattern recognition, control, and forecasting. Fuzzy systems and multilayer perceptrons are computationally equivalent, i.e. they are both universal approximators <ref> [8, 50] </ref>. Recurrent neural networks have been shown to be computationally equivalent with Turing machines [43]; whether or not recurrent fuzzy systems are also Turing equivalent remains an open question. While the methodologies underlying fuzzy systems and neural networks are quite different, their functional forms are often similar.
Reference: [9] <author> S. Das and M. Mozer, </author> <title> "A unified gradient-descent/clustering architecture for finite state machine induction," </title> <booktitle> in Advances in Neural Information Processing Systems 6 (J. </booktitle> <editor> Cowan, G. Tesauro, and J. Alspector, eds.), </editor> <address> (San Francisco, CA), </address> <pages> pp. 19-26, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference: [10] <author> D. Dubois and H. Prade, </author> <title> Fuzzy sets and systems: </title> <journal> theory and applications, </journal> <volume> vol. </volume> <booktitle> 144 of Mathematics in Science and Engineering, </booktitle> <pages> pp. 220-226. </pages> <publisher> Academic Press, </publisher> <year> 1980. </year>
Reference-contexts: Any fuzzy automaton as described in definition 3.3 is equivalent to a restricted fuzzy automaton <ref> [10] </ref>. Notice that a FFA reduces to a conventional DFA by restricting the transition weights to 1. As in the case of DFAs and regular grammars, there exist a correspondence between FFAs and fuzzy regular grammars [10]: Theorem 3.1 For a given fuzzy grammar e G, there exists a fuzzy automaton <p> fuzzy automaton as described in definition 3.3 is equivalent to a restricted fuzzy automaton <ref> [10] </ref>. Notice that a FFA reduces to a conventional DFA by restricting the transition weights to 1. As in the case of DFAs and regular grammars, there exist a correspondence between FFAs and fuzzy regular grammars [10]: Theorem 3.1 For a given fuzzy grammar e G, there exists a fuzzy automaton f M such that L ( e G) = L ( f M ). 11 Our goal is to use only continuous (sigmoidal and linear) discriminant functions for the neural network implementation of FFAs.
Reference: [11] <author> J. Elman, </author> <title> "Finding structure in time," </title> <journal> Cognitive Science, </journal> <volume> vol. 14, </volume> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference: [12] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda, "Representation of finite state automata in recurrent radial basis function networks," </title> <booktitle> Machine Learning, </booktitle> <year> 1995. </year> <note> In press. </note>
Reference-contexts: Alternatively, a DFA M can also be considered a generator which generates the regular language L (M ). 2.2 Network Construction Various methods have been proposed for implementing DFAs in recurrent neural networks <ref> [1, 2, 12, 13, 22, 28, 32] </ref>.
Reference: [13] <author> P. Frasconi, M. Gori, and G. </author> <title> Soda, "Injecting nondeterministic finite state automata into recurrent networks," </title> <type> tech. rep., </type> <institution> Dipartimento di Sistemi e Informatica, Universita di Firenze, Italy, Florence, Italy, </institution> <year> 1993. </year>
Reference-contexts: Alternatively, a DFA M can also be considered a generator which generates the regular language L (M ). 2.2 Network Construction Various methods have been proposed for implementing DFAs in recurrent neural networks <ref> [1, 2, 12, 13, 22, 28, 32] </ref>.
Reference: [14] <author> B. Gaines and L. Kohout, </author> <title> "The logic of automata," </title> <journal> International Journal of General Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 191-208, </pages> <year> 1976. </year>
Reference-contexts: Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of X-rays [35], in digital circuit design [27], and in the design of intelligent human-computer interfaces [41]. The fundamentals of FFAs have been in discussed in <ref> [14, 40, 54] </ref> without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [18, 19, 25, 49]. The synthesis method proposed in [18] uses digital design technology to implement fuzzy representations of states and outputs.
Reference: [15] <author> C. Giles, C. Miller, D. Chen, H. Chen, G. Sun, and Y. Lee, </author> <title> "Learning and extracting finite state automata with second-order recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <editor> p. </editor> <volume> 380, </volume> <year> 1992. </year>
Reference: [16] <author> P. Goode and M. Chow, </author> <title> "A hybrid fuzzy/neural systems used to extract heuristic knowledge from a fault detection problem," </title> <booktitle> in Proceedings of the Third IEEE Conference on Fuzzy Systems, </booktitle> <volume> vol. III, </volume> <pages> pp. 1731-1736, </pages> <year> 1994. </year>
Reference-contexts: which adopted some learning algorithms; e.g. there exists a backpropagation training algorithms for fuzzy logic systems which are similar to the training algorithms for neural networks [17, 51]. 1.2 Fuzzy Knowledge Representation in Neural Networks In some cases, neural networks can be structured based on the principles of fuzzy logic <ref> [16, 36] </ref>. Neural network representations of fuzzy logic interpolation have also been used within the context of reinforcement learning [3]. A typical fuzzy neural network used for intelligent control is shown in figure 1. <p> The extraction of fuzzy if-then-rules from trained multilayer perceptrons has also been investigated <ref> [16, 20, 29, 30] </ref>.
Reference: [17] <author> V. Gorrini and H. Bersini, </author> <title> "Recurrent fuzzy systems," </title> <booktitle> in Proceedings of the Third IEEE Conference on Fuzzy Systems, </booktitle> <volume> vol. I, </volume> <pages> pp. 193-198, </pages> <year> 1994. </year>
Reference-contexts: The development of powerful learning algorithms for neural networks has been beneficial to the field of fuzzy systems which adopted some learning algorithms; e.g. there exists a backpropagation training algorithms for fuzzy logic systems which are similar to the training algorithms for neural networks <ref> [17, 51] </ref>. 1.2 Fuzzy Knowledge Representation in Neural Networks In some cases, neural networks can be structured based on the principles of fuzzy logic [16, 36]. Neural network representations of fuzzy logic interpolation have also been used within the context of reinforcement learning [3].
Reference: [18] <author> J. Grantner and M. Patyra, </author> <title> "Synthesis and analysis of fuzzy logic finite state machine models," </title> <booktitle> in Proceedings of the Third IEEE Conference on Fuzzy Systems, </booktitle> <volume> vol. I, </volume> <pages> pp. 205-210, </pages> <year> 1994. </year>
Reference-contexts: The fundamentals of FFAs have been in discussed in [14, 40, 54] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature <ref> [18, 19, 25, 49] </ref>. The synthesis method proposed in [18] uses digital design technology to implement fuzzy representations of states and outputs. <p> The fundamentals of FFAs have been in discussed in [14, 40, 54] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [18, 19, 25, 49]. The synthesis method proposed in <ref> [18] </ref> uses digital design technology to implement fuzzy representations of states and outputs. In [49], the implementation of a Moore machine with fuzzy inputs and states is realized by training a feedforward network explicitly on the state transition table using a modified backpropagation algorithm.
Reference: [19] <author> J. Grantner and M. Patyra, </author> <title> "VLSI implementations of fuzzy logic finite state machines," </title> <booktitle> in Proceedings of the Fifth IFSA Congress, </booktitle> <pages> pp. 781-784, </pages> <year> 1993. </year>
Reference-contexts: The fundamentals of FFAs have been in discussed in [14, 40, 54] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature <ref> [18, 19, 25, 49] </ref>. The synthesis method proposed in [18] uses digital design technology to implement fuzzy representations of states and outputs.
Reference: [20] <author> Y. Hayashi and A. Imura, </author> <title> "Fuzzy neural expert system with automated extraction of fuzzy if-then rules from a trained neural network," </title> <booktitle> in Proceedings of the First IEEE Conference on Fuzzy Systems, </booktitle> <pages> pp. 489-494, </pages> <year> 1990. </year>
Reference-contexts: The extraction of fuzzy if-then-rules from trained multilayer perceptrons has also been investigated <ref> [16, 20, 29, 30] </ref>.
Reference: [21] <author> J. Hopcroft and J. Ullman, </author> <title> Introduction to Automata Theory, </title> <booktitle> Languages, and Computation. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1979. </year>
Reference-contexts: Most of this can be found in detail in [31] and briefly with experimental verification in [33]. 4 2.1 Deterministic Finite-state Automata Regular languages represent the smallest class of formal languages in the Chomsky hierarchy <ref> [21] </ref>. Regular languages are generated by regular grammars.
Reference: [22] <author> B. Horne and D. Hush, </author> <title> "Bounds on the complexity of recurrent neural network implementations of finite state machines," </title> <booktitle> in Advances in Neural Information Processing Systems 6, </booktitle> <pages> pp. 359-366, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Alternatively, a DFA M can also be considered a generator which generates the regular language L (M ). 2.2 Network Construction Various methods have been proposed for implementing DFAs in recurrent neural networks <ref> [1, 2, 12, 13, 22, 28, 32] </ref>.
Reference: [23] <author> W. J. M. Kickert and H. van Nauta Lemke, </author> <title> "Application of a fuzzy controller in a warm water plant," </title> <journal> Automatica, </journal> <volume> vol. 12, no. 4, </volume> <pages> pp. 301-308, </pages> <year> 1976. </year>
Reference-contexts: Fuzzy logic [55] provides a mathematical foundation for approximate reasoning; fuzzy 1 logic controllers have proven very successful in a variety of applications <ref> [6, 23, 24, 34] </ref>. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values. Furthermore, rule-based information can be incorporated into fuzzy systems in a systematic way.
Reference: [24] <author> C. Lee, </author> <title> "Fuzzy logic in control systems: fuzzy logic controller," </title> <journal> IEEE Transactions on Man, Systems, and Cybernetics, </journal> <volume> vol. SMC-20, no. 2, </volume> <pages> pp. 404-435, </pages> <year> 1990. </year>
Reference-contexts: Fuzzy logic [55] provides a mathematical foundation for approximate reasoning; fuzzy 1 logic controllers have proven very successful in a variety of applications <ref> [6, 23, 24, 34] </ref>. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values. Furthermore, rule-based information can be incorporated into fuzzy systems in a systematic way.
Reference: [25] <author> S. Lee and E. Lee, </author> <title> "Fuzzy neural networks," </title> <journal> Mathematical Biosciences, </journal> <volume> vol. 23, </volume> <pages> pp. 151-177, </pages> <year> 1975. </year>
Reference-contexts: The fundamentals of FFAs have been in discussed in [14, 40, 54] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature <ref> [18, 19, 25, 49] </ref>. The synthesis method proposed in [18] uses digital design technology to implement fuzzy representations of states and outputs.
Reference: [26] <author> T. Ludermir, </author> <title> "Logical networks capable of computing weighted regular languages," </title> <booktitle> in Proceedings of the International Joint Conference on Neural Networks 1991, </booktitle> <volume> vol. II, </volume> <pages> pp. 1687-1692, </pages> <year> 1991. </year>
Reference-contexts: The fuzzification of inputs and states reduces the memory size that is required to implement the automaton in a microcontroller, e.g. antilock braking systems. In related work, an algorithm for implementing weighted regular languages in neural networks with probabilistic logic nodes was discussed in <ref> [26] </ref>. A general synthesis method for synchronous fuzzy sequential circuits has been discussed in [52].
Reference: [27] <author> S. Mensch and H. Lipp, </author> <title> "Fuzzy specification of finite state machines," </title> <booktitle> in Proceedings of the European Design Automation Conference, </booktitle> <pages> pp. 622-626, </pages> <year> 1990. </year>
Reference-contexts: Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of X-rays [35], in digital circuit design <ref> [27] </ref>, and in the design of intelligent human-computer interfaces [41]. The fundamentals of FFAs have been in discussed in [14, 40, 54] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [18, 19, 25, 49].
Reference: [28] <author> M. Minsky, </author> <title> Computation: Finite and Infinite Machines, </title> <journal> ch. </journal> <volume> 3, </volume> <pages> pp. 32-66. </pages> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, Inc., </publisher> <year> 1967. </year>
Reference-contexts: Alternatively, a DFA M can also be considered a generator which generates the regular language L (M ). 2.2 Network Construction Various methods have been proposed for implementing DFAs in recurrent neural networks <ref> [1, 2, 12, 13, 22, 28, 32] </ref>.
Reference: [29] <author> S. Mitra and S. Pal, </author> <title> "Fuzzy multilayer perceptron, inferencing and rule generation," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 6, no. 1, </volume> <pages> pp. 51-63, </pages> <year> 1995. </year>
Reference-contexts: The extraction of fuzzy if-then-rules from trained multilayer perceptrons has also been investigated <ref> [16, 20, 29, 30] </ref>.
Reference: [30] <author> T. Nishina, M. Hagiwara, and M. Nakagawa, </author> <title> "Fuzzy inference neural networks which automatically partition a pattern space and extract fuzzy if-then rules," </title> <booktitle> in Proceedings of the Third IEEE Conference on Fuzzy Systems, </booktitle> <volume> vol. II, </volume> <pages> pp. 1314-1319, </pages> <year> 1994. </year>
Reference-contexts: The extraction of fuzzy if-then-rules from trained multilayer perceptrons has also been investigated <ref> [16, 20, 29, 30] </ref>.
Reference: [31] <author> C. Omlin and C. Giles, </author> <title> "Constructing deterministic finite-state automata in recurrent neural networks," </title> <journal> Journal of the ACM. </journal> <note> Accepted for publication. A revised version of U. of Maryland TR UMIACS-TR-95-50. </note>
Reference-contexts: We briefly review deterministic, finite-state automata and their implementation in recurrent neural networks in section 2. The extension of DFAs to FFAs is discussed in section 3. In section 4, we show how FFAs can be implemented in recurrent networks based on previous work on the encoding of DFAs <ref> [32, 31, 33] </ref>. In particular, our results show that FFAs can be encoded into recurrent networks such that a constructed network assigns membership grades to strings of arbitrary length with arbitrary accuracy. Notice that we do not claim that such a representation can be learned. <p> A summary and directions for future work in section 6 conclude this paper. 2 Finite-state Automata and Recurrent Neural Networks Here we discuss the relationship between finite-state automata and recurrent neural networks necessary to mapping fuzzy automata into recurrent networks. Most of this can be found in detail in <ref> [31] </ref> and briefly with experimental verification in [33]. 4 2.1 Deterministic Finite-state Automata Regular languages represent the smallest class of formal languages in the Chomsky hierarchy [21]. Regular languages are generated by regular grammars. <p> We have recently proven that DFAs can be encoded in discrete-time, second-order recurrent neural networks with sigmoidal discriminant functions such that the DFA and constructed network accept the same regular language <ref> [31] </ref>. <p> In the remainder of this section, we state results which establish that stability of the internal representation can be achieved. The proofs of these results can be found in <ref> [31] </ref>. The terms principal and residual inputs will be useful for the following discussion: Definition 2.3 Let S i be a neuron with low output signal S t i and S j be a neuron with high output signal S t j . <p> The graphs of the function f (x; r) are shown in figure 2 for different values of the parameter r. The function f (x; r) has some desirable properties <ref> [31] </ref>: Lemma 2.2 For any H &gt; 0, the function f (x; r) has at least one fixed point 0 f .
Reference: [32] <author> C. Omlin and C. Giles, </author> <title> "Constructing deterministic finite-state automata in sparse recurrent neural networks," </title> <booktitle> in IEEE International Conference on Neural Networks (ICNN'94), </booktitle> <pages> pp. 1732-1737, </pages> <year> 1994. </year>
Reference-contexts: We briefly review deterministic, finite-state automata and their implementation in recurrent neural networks in section 2. The extension of DFAs to FFAs is discussed in section 3. In section 4, we show how FFAs can be implemented in recurrent networks based on previous work on the encoding of DFAs <ref> [32, 31, 33] </ref>. In particular, our results show that FFAs can be encoded into recurrent networks such that a constructed network assigns membership grades to strings of arbitrary length with arbitrary accuracy. Notice that we do not claim that such a representation can be learned. <p> Alternatively, a DFA M can also be considered a generator which generates the regular language L (M ). 2.2 Network Construction Various methods have been proposed for implementing DFAs in recurrent neural networks <ref> [1, 2, 12, 13, 22, 28, 32] </ref>.
Reference: [33] <author> C. Omlin and C. Giles, </author> <title> "Stable encoding of large finite-state automata in recurrent neural networks with sigmoid discriminants," </title> <journal> Neural Computation, </journal> <volume> vol. 8, no. 4, </volume> <year> 1996. </year>
Reference-contexts: We briefly review deterministic, finite-state automata and their implementation in recurrent neural networks in section 2. The extension of DFAs to FFAs is discussed in section 3. In section 4, we show how FFAs can be implemented in recurrent networks based on previous work on the encoding of DFAs <ref> [32, 31, 33] </ref>. In particular, our results show that FFAs can be encoded into recurrent networks such that a constructed network assigns membership grades to strings of arbitrary length with arbitrary accuracy. Notice that we do not claim that such a representation can be learned. <p> Most of this can be found in detail in [31] and briefly with experimental verification in <ref> [33] </ref>. 4 2.1 Deterministic Finite-state Automata Regular languages represent the smallest class of formal languages in the Chomsky hierarchy [21]. Regular languages are generated by regular grammars. <p> As such, they represent worst cases, i.e. the finite-state dynamics of a given neural network implementation may remain stable for smaller values of H even for very large networks <ref> [33] </ref>.
Reference: [34] <author> C. Pappis and E. Mamdani, </author> <title> "A fuzzy logic controller for a traffic junction," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. SMC-7, no. 10, </volume> <pages> pp. 707-717, </pages> <year> 1977. </year> <month> 19 </month>
Reference-contexts: Fuzzy logic [55] provides a mathematical foundation for approximate reasoning; fuzzy 1 logic controllers have proven very successful in a variety of applications <ref> [6, 23, 24, 34] </ref>. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values. Furthermore, rule-based information can be incorporated into fuzzy systems in a systematic way.
Reference: [35] <author> A. Pathak and S. Pal, </author> <title> "Fuzzy grammars in syntactic recognition of skeletal maturity from x-rays," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 16, no. 5, </volume> <pages> pp. 657-667, </pages> <year> 1986. </year>
Reference-contexts: Thus, it is only natural to ask whether recurrent neural networks can also represent fuzzy finite-state automata (FFAs) and thus be used to implement recognizers of fuzzy regular grammars. Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of X-rays <ref> [35] </ref>, in digital circuit design [27], and in the design of intelligent human-computer interfaces [41]. The fundamentals of FFAs have been in discussed in [14, 40, 54] without presenting a systematic method for machine synthesis.
Reference: [36] <author> C. Perneel, J.-M. Renders, J.-M. Themlin, and M. Acheroy, </author> <title> "Fuzzy reasoning and neural networks for decision making problems in uncertain environments," </title> <booktitle> in Proceedings of the Third IEEE Conference on Fuzzy Systems, </booktitle> <volume> vol. II, </volume> <pages> pp. 1111-1125, </pages> <year> 1994. </year>
Reference-contexts: which adopted some learning algorithms; e.g. there exists a backpropagation training algorithms for fuzzy logic systems which are similar to the training algorithms for neural networks [17, 51]. 1.2 Fuzzy Knowledge Representation in Neural Networks In some cases, neural networks can be structured based on the principles of fuzzy logic <ref> [16, 36] </ref>. Neural network representations of fuzzy logic interpolation have also been used within the context of reinforcement learning [3]. A typical fuzzy neural network used for intelligent control is shown in figure 1.
Reference: [37] <author> J. Pollack, </author> <title> "The induction of dynamical recognizers," </title> <journal> Machine Learning, </journal> <volume> vol. 7, </volume> <pages> pp. 227-252, </pages> <year> 1991. </year>
Reference: [38] <author> M. Rabin, </author> <title> "Probabilistic automata," </title> <journal> Information and Control, </journal> <volume> vol. 6, </volume> <pages> pp. 230-245, </pages> <year> 1963. </year>
Reference-contexts: the minimum weight of the productions used: G (x) = G (S ) x) = max min [ G (S ! ff 1 ); G (ff 1 ! ff 2 ); : : : ; G (ff m ! x)] This is akin to the definition of stochastic regular languages <ref> [38] </ref> where the min-and max-operators are replaced by the product- and sum-operators, respectively. Both fuzzy and stochastic regular languages are examples of weighted regular languages [39].
Reference: [39] <author> A. Salommaa, </author> <title> "Probabilistic and weighted grammars," </title> <journal> Information and Control, </journal> <volume> vol. 15, </volume> <pages> pp. 529-544, </pages> <year> 1969. </year>
Reference-contexts: Both fuzzy and stochastic regular languages are examples of weighted regular languages <ref> [39] </ref>.
Reference: [40] <author> E. Santos, </author> <title> "Maximin automata," </title> <journal> Information and Control, </journal> <volume> vol. 13, </volume> <pages> pp. 363-377, </pages> <year> 1968. </year>
Reference-contexts: Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of X-rays [35], in digital circuit design [27], and in the design of intelligent human-computer interfaces [41]. The fundamentals of FFAs have been in discussed in <ref> [14, 40, 54] </ref> without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [18, 19, 25, 49]. The synthesis method proposed in [18] uses digital design technology to implement fuzzy representations of states and outputs.
Reference: [41] <author> H. Senay, </author> <title> "Fuzzy command grammars for intelligent interface design," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 22, no. 5, </volume> <pages> pp. 1124-1131, </pages> <year> 1992. </year>
Reference-contexts: Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of X-rays [35], in digital circuit design [27], and in the design of intelligent human-computer interfaces <ref> [41] </ref>. The fundamentals of FFAs have been in discussed in [14, 40, 54] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [18, 19, 25, 49].
Reference: [42] <author> J. Si and A. Michel, </author> <title> "Analysis and synthesis of a class of discrete-time neural networks with multilevel threshold neurons," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 6, no. 1, </volume> <editor> p. </editor> <volume> 105, </volume> <year> 1995. </year>
Reference-contexts: A general synthesis method for synchronous fuzzy sequential circuits has been discussed in [52]. A synthesis method for a class of discrete-time neural networks with multilevel threshold neurons with applications to gray level image processing has been proposed in <ref> [42] </ref>. 1.3 Outline of Paper The purpose of this paper is to show that recurrent networks that can represent DFAs can be easily modified to accommodate FFAs. We briefly review deterministic, finite-state automata and their implementation in recurrent neural networks in section 2. <p> This suggests the use of continuous multilevel threshold neurons <ref> [42] </ref> which also have the potential for stable internal DFA state representations. Whether training such networks is feasible remains an open question.
Reference: [43] <author> H. Siegelmann and E. Sontag, </author> <title> "Turing compatability with neural nets," </title> <journal> Applied Mathematics Letters, </journal> <volume> vol. 4, no. 6, </volume> <pages> pp. 77-80, </pages> <year> 1991. </year>
Reference-contexts: Fuzzy systems and multilayer perceptrons are computationally equivalent, i.e. they are both universal approximators [8, 50]. Recurrent neural networks have been shown to be computationally equivalent with Turing machines <ref> [43] </ref>; whether or not recurrent fuzzy systems are also Turing equivalent remains an open question. While the methodologies underlying fuzzy systems and neural networks are quite different, their functional forms are often similar.
Reference: [44] <author> M. Thomason and P. Marinos, </author> <title> "Deterministic acceptors of regular fuzzy languages," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> no. 3, </volume> <pages> pp. 228-230, </pages> <year> 1974. </year>
Reference-contexts: The constructive proof can be found in <ref> [44] </ref>.
Reference: [45] <author> K. Thornber, </author> <title> "The fidelity of fuzzy-logic inference," </title> <journal> IEEE Transactions on Fuzzy Systems, </journal> <volume> vol. 1, no. 4, </volume> <pages> pp. 288-297, </pages> <year> 1993. </year>
Reference-contexts: : : from properties of fuzzy sets A 1 ; A 2 ; : : : with the help of an inference scheme A 1 ; A 2 ; : : : ! B 1 ; B 2 ; : : : which is governed by a set of rules <ref> [45, 46] </ref>. 3 A large class of problems where the current state depends on both the current input and the previous state can be modeled by finite-state automata or their equivalent grammars.
Reference: [46] <author> K. Thornber, </author> <title> "A key to fuzzy-logic inference," </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> vol. 8, </volume> <pages> pp. 105-121, </pages> <year> 1993. </year>
Reference-contexts: : : from properties of fuzzy sets A 1 ; A 2 ; : : : with the help of an inference scheme A 1 ; A 2 ; : : : ! B 1 ; B 2 ; : : : which is governed by a set of rules <ref> [45, 46] </ref>. 3 A large class of problems where the current state depends on both the current input and the previous state can be modeled by finite-state automata or their equivalent grammars.
Reference: [47] <author> P. Tino, B. Horne, and C.L.Giles, </author> <title> "Finite state machines and recurrent neural networks automata and dynamical systems approaches," </title> <type> Tech. Rep. </type> <institution> UMIACS-TR-95-1, Institute for Advance Computer Studies, University of Maryland, College Park, MD 20742, </institution> <year> 1995. </year>
Reference: [48] <author> P. Tino and J. Sajda, </author> <title> "Learning and extracting initial mealy machines with a modular neural network model," </title> <journal> Neural Computation, </journal> <volume> vol. 7, no. 4, </volume> <pages> pp. 822-844, </pages> <year> 1995. </year>
Reference: [49] <author> F. Unal and E. Khan, </author> <title> "A fuzzy finite state machine implementation based on a neural fuzzy system," </title> <booktitle> in Proceedings of the Third International Conference on Fuzzy Systems, </booktitle> <volume> vol. 3, </volume> <pages> pp. 1749-1754, </pages> <year> 1994. </year> <month> 20 </month>
Reference-contexts: The fundamentals of FFAs have been in discussed in [14, 40, 54] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature <ref> [18, 19, 25, 49] </ref>. The synthesis method proposed in [18] uses digital design technology to implement fuzzy representations of states and outputs. <p> Neural network implementations of fuzzy automata have been proposed in the literature [18, 19, 25, 49]. The synthesis method proposed in [18] uses digital design technology to implement fuzzy representations of states and outputs. In <ref> [49] </ref>, the implementation of a Moore machine with fuzzy inputs and states is realized by training a feedforward network explicitly on the state transition table using a modified backpropagation algorithm.
Reference: [50] <author> L.-X. Wang, </author> <title> Adaptive Fuzzy Systems and Control: Design and Stability Analysis. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1994. </year>
Reference-contexts: Artificial neural networks have become valuable computational tools in their own right for tasks such as pattern recognition, control, and forecasting. Fuzzy systems and multilayer perceptrons are computationally equivalent, i.e. they are both universal approximators <ref> [8, 50] </ref>. Recurrent neural networks have been shown to be computationally equivalent with Turing machines [43]; whether or not recurrent fuzzy systems are also Turing equivalent remains an open question. While the methodologies underlying fuzzy systems and neural networks are quite different, their functional forms are often similar.
Reference: [51] <author> L.-X. Wang, </author> <title> "Fuzzy systems are universal approximators," </title> <booktitle> in Proceedings of the First International Conference on Fuzzy Systems, </booktitle> <pages> pp. 1163-1170, </pages> <year> 1992. </year>
Reference-contexts: The development of powerful learning algorithms for neural networks has been beneficial to the field of fuzzy systems which adopted some learning algorithms; e.g. there exists a backpropagation training algorithms for fuzzy logic systems which are similar to the training algorithms for neural networks <ref> [17, 51] </ref>. 1.2 Fuzzy Knowledge Representation in Neural Networks In some cases, neural networks can be structured based on the principles of fuzzy logic [16, 36]. Neural network representations of fuzzy logic interpolation have also been used within the context of reinforcement learning [3].
Reference: [52] <author> T. Watanabe, M. Matsumoto, and M. Enokida, </author> <title> "Synthesis of synchronous fuzzy sequential circuits," </title> <booktitle> in Proceedings of the Third IFSA World Congress, </booktitle> <pages> pp. 288-291, </pages> <year> 1989. </year>
Reference-contexts: In related work, an algorithm for implementing weighted regular languages in neural networks with probabilistic logic nodes was discussed in [26]. A general synthesis method for synchronous fuzzy sequential circuits has been discussed in <ref> [52] </ref>.
Reference: [53] <author> R. Watrous and G. Kuhn, </author> <title> "Induction of finite-state languages using second-order recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <editor> p. </editor> <volume> 406, </volume> <year> 1992. </year>
Reference: [54] <author> W. Wee and K. Fu, </author> <title> "A formulation of fuzzy automata and its applications as a model of learning systems," </title> <journal> IEEE Transactions on System Science and Cybernetics, </journal> <volume> vol. 5, </volume> <pages> pp. 215-223, </pages> <year> 1969. </year>
Reference-contexts: Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of X-rays [35], in digital circuit design [27], and in the design of intelligent human-computer interfaces [41]. The fundamentals of FFAs have been in discussed in <ref> [14, 40, 54] </ref> without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [18, 19, 25, 49]. The synthesis method proposed in [18] uses digital design technology to implement fuzzy representations of states and outputs.
Reference: [55] <author> L. Zadeh, </author> <title> "Fuzzy sets," </title> <journal> Information and Control, </journal> <volume> vol. 8, </volume> <pages> pp. 338-353, </pages> <year> 1965. </year>
Reference-contexts: 1 Introduction 1.1 Fuzzy Systems and Neural Networks There has been an increased interest in combining artificial neural networks and fuzzy systems (see [4] for a collection of papers). Fuzzy logic <ref> [55] </ref> provides a mathematical foundation for approximate reasoning; fuzzy 1 logic controllers have proven very successful in a variety of applications [6, 23, 24, 34]. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values.
Reference: [56] <author> Z. Zeng, R. Goodman, and P. Smyth, </author> <title> "Learning finite state machines with self-clustering recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 5, no. 6, </volume> <pages> pp. 976-990, </pages> <year> 1993. </year> <month> 21 </month>
References-found: 56

