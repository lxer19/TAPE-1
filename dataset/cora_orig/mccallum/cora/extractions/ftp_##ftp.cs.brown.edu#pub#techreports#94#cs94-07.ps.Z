URL: ftp://ftp.cs.brown.edu/pub/techreports/94/cs94-07.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-94-07.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> Baum, L. E. </author> <title> An inequality and associated maximization technique in statistical estimation for probabilistic functions of a Markov process. </title> <booktitle> Inequalities 3 (1972), </booktitle> <pages> 1-8. </pages>
Reference: 2. <author> Black, E., Jelinek, F., Lafferty, J., Magerman, D., Mercer, R. and Roukos, S. </author> <title> Towards history-based grammars: using richer models for probabilistic parsing. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics. </booktitle> <year> 1993, </year> <pages> 31-37. </pages>
Reference-contexts: Note that this goal is quite different from other uses of context sensitive statistics such as improving the speed of parsing [14] or improving the probability of the correct parse <ref> [2] </ref>. While we believe our statistics could be adapted to these purposes, our own interest lies in the area of language models. A language model is a distribution over strings of (English) words, and a good model should accurately reflect the true distribution of English strings.
Reference: 3. <author> Boggess, L., Agarwal, R. and Davis, R. </author> <title> Disambiguation of prepositional phrases in automatically labeled technical text. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <year> 1991, </year> <pages> 155-159. </pages>
Reference: 4. <author> Briscoe, T. and Waegner, N. </author> <title> Robust stochastic parsing using the inside-ouside algorithm. </title> <booktitle> In Workshop Notes, Statistically-Based NLP Techniques. AAAI, </booktitle> <year> 1992, </year> <pages> 30-53. </pages>
Reference-contexts: Adding non-terminals is a particularly sticky problem for grammer learners, as the unconstrained space is too large to search. What is usually done is to fix the number of non-terminals in some other way, either using outside sources of information <ref> [4] </ref>, or, as we do, via a restricted formalism [5]. Another approach, suggested in [13] is to use a CNF grammar, simply guess an upper bound on the number of non-terminals, and deploy a grammar minimization procedure periodically during the grammar training.
Reference: 5. <author> Carroll, G. and Charniak, E. </author> <title> Learning probabilistic dependency grammars from labeled text. </title> <booktitle> In Working Notes, Fall Symposium Series. AAAI, </booktitle> <year> 1992, </year> <pages> 25-32. </pages>
Reference-contexts: Adding non-terminals is a particularly sticky problem for grammer learners, as the unconstrained space is too large to search. What is usually done is to fix the number of non-terminals in some other way, either using outside sources of information [4], or, as we do, via a restricted formalism <ref> [5] </ref>. Another approach, suggested in [13] is to use a CNF grammar, simply guess an upper bound on the number of non-terminals, and deploy a grammar minimization procedure periodically during the grammar training.
Reference: 6. <author> Charniak, E. </author> <title> Statistical Language Learning . MIT Press, </title> <address> Cambridge, </address> <year> 1993. </year>
Reference-contexts: With a grammar-based model, one first parses the sentences using the grammar, and then uses the parse information to assign the probabilities to the actual words (See <ref> [6] </ref>.). We make the standard assumption that sentences occur independently of each other, and thus, if w 1;N are the words of the l sentences s 1;l , l Y P (s i ) (1) This assumption allows us to focus on individual sentences and their parses.
Reference: 7. <author> Charniak, E., Hendrickson, C., Jacobson, N. and Perkowitz, M. </author> <title> Equations for part-of-speech tagging. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <year> 1993, </year> <pages> 784-789. </pages>
Reference: 8. <author> Church, K. W. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Second Conference on Applied Natural Language Processing . ACL, </booktitle> <year> 1988, </year> <pages> 136-143. </pages>
Reference: 9. <author> DeRose, S. J. </author> <title> Grammatical category disambiguation by statistical optimization. </title> <booktitle> Computational Linguistics 14 (1988), </booktitle> <pages> 31-39. </pages>
Reference: 10. <author> Dunning, T. </author> <title> Accurate methods for the statistics of surprise and coincidence. </title> <booktitle> Computational Linguistics 121 (1993), </booktitle> <pages> 61-74. </pages>
Reference-contexts: We estimate significant difference using a likelihood ratio analysis described in <ref> [10] </ref>. <p> (10) In the latter case, we have H (P (R t j t); P (R t j t); C 1 ; C 2 ) (11) since in this case P (R t j s; t) = P (R t j :s; t) = P (R t j t) Finally, following <ref> [10] </ref> we consider the quantity log (s; t) = log H (P (R t j t); P (R t j t); C 1 ; C 2 ) We lack space to show an exact form for H and log (s; t) (but see [10] for details). <p> P (R t j t) Finally, following <ref> [10] </ref> we consider the quantity log (s; t) = log H (P (R t j t); P (R t j t); C 1 ; C 2 ) We lack space to show an exact form for H and log (s; t) (but see [10] for details). Intuitively, however, this is a measure of how likely it is that the context sensitive probabilities for the rules given s; t are really just the context-free probabilities.
Reference: 11. <author> Francis, W. N. and Ku cera, H. </author> <title> Frequency Analysis of English Usage: Lexicon and Grammar. </title> <publisher> Houghton Mi*in, </publisher> <address> Boston, </address> <year> 1982. </year>
Reference-contexts: We derived our context-sensitive grammar from a PCFG developed from related work on grammar induction. This latter grammar was learned on the basis of a 300,000 word training corpus, consisting of all sentences in the tagged Brown Corpus <ref> [11] </ref> of length less than 23, and not containing certain terminals we wished to ignore (most notably parentheses, foreign words and titles). We built the context sensitive version of this grammar by applying Equation 7 and training over the same corpus from which the PCFG was learned.
Reference: 12. <author> Jelinek, F. </author> <title> Markov source modeling of text generation. In The Impact of Processing Techniques on Communications, </title> <editor> J. K. Skwirzinski, Ed. </editor> <publisher> Nijhoff, </publisher> <address> Dordrecht, </address> <year> 1985. </year>
Reference: 13. <author> Lari, K. and Young, S. J. </author> <title> The estimation of stochastic context-free grammars using the Inside-Outside algorithm. </title> <booktitle> In Computer Speech and Language. </booktitle> <volume> vol. 4, </volume> <year> 1990, </year> <pages> 35-56. </pages>
Reference-contexts: What is usually done is to fix the number of non-terminals in some other way, either using outside sources of information [4], or, as we do, via a restricted formalism [5]. Another approach, suggested in <ref> [13] </ref> is to use a CNF grammar, simply guess an upper bound on the number of non-terminals, and deploy a grammar minimization procedure periodically during the grammar training.
Reference: 14. <author> Magerman, D. M. and Weir, C. </author> <title> Efficiency, robustness and accuracy in Picky chart parsing. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics. </booktitle> <year> 1992, </year> <pages> 40-47. 15 </pages>
Reference-contexts: In this paper we investigate a scheme for introducing context sensitive statistics into stochastic parsing, with the aim of improving a grammar-based language model for English. Note that this goal is quite different from other uses of context sensitive statistics such as improving the speed of parsing <ref> [14] </ref> or improving the probability of the correct parse [2]. While we believe our statistics could be adapted to these purposes, our own interest lies in the area of language models.
References-found: 14

