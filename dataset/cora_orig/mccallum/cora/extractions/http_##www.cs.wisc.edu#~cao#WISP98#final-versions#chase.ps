URL: http://www.cs.wisc.edu/~cao/WISP98/final-versions/chase.ps
Refering-URL: http://www.cs.wisc.edu/~cao/WISP98-program.html
Root-URL: http://www.cs.wisc.edu
Email: fgadde,chaseg@cs.duke.edu  misha@research.att.com  
Title: A Taste of Crispy Squid  
Author: Syam Gadde, Jeff Chase Michael Rabinovich 
Address: Durham NC, 27708  108 Park Avenue Florham Park, NJ 07932  
Affiliation: Dept. of Computer Science Duke University  AT&T Labs Research  
Abstract: Distributed proxy caches are in use throughout the world to reduce access latency and bandwidth demands for Internet object transfer. The CRISP project seeks to build more effective distributed Web caches by exploring alternatives to the hierarchical structure and multicast handling of probes common to the most popular distributed Web cache systems. CRISP caches are structured as a collective of autonomous Web proxy servers sharing their cache directories through a common mapping service that can be queried with at most one message exchange. Individual servers may be configured to replicate all or part of the global map in order to balance access cost, overhead and hit ratio, depending on the size and geographic dispersion of the collective cache. We have prototyped several CRISP cache structures in Crispy Squid, an extension to the Squid Internet Object Cache. We are evaluating these cache structures using Proxycizer, a full-featured package for replaying traces of observed request traffic into Web cache prototypes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jussara Almeida and Pei Cao. </author> <note> Wiscon-sin Proxy Benchmark 1.0. Available at http://www.cs.wisc.edu/~cao/wpb1.0.html. </note>
Reference-contexts: Proxycizer also implements several proxy simulator classes, including those for Harvest and the CRISP cache architectures described in Section 2. Several other useful packages have recently been made available, each addressing different aspects of Internet server simulation and benchmarking. While Proxycizer is purely trace-driven, the Wis-consin Proxy Benchmark (WPB) <ref> [1] </ref> uses synthetic workloads modeled to emulate typical temporal locality patterns to test proxies under load. hbench:Web [14] is a benchmark for Web servers that is not explicitly trace-driven, but does preprocess Web server logs and automatically generates an appropriate traffic model for driving a Web server.
Reference: [2] <author> Gaurav Banga and Peter Druschel. </author> <title> Measuring the capacity of a Web server. </title> <booktitle> In Proceedings of The USENIX Symposium on Internet Technologies and Systems (USITS). </booktitle> <institution> Department of Computer Science, Rice University, </institution> <month> December </month> <year> 1997. </year>
Reference-contexts: S-Clients <ref> [2] </ref> introduces a scalable mechanism for driving Web servers to overload conditions.
Reference: [3] <author> Anawat Chankhunthod, Peter Danzig, Chuck Neerdaels, Michael F. Schwartz, and Kurt J. Worrell. </author> <title> A hierarchical Internet object cache. </title> <booktitle> In Proceedings of the USENIX 1996 Annual Technical Conference, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Hierarchical Web proxy caches descended from the Harvest project <ref> [3] </ref> are now in wide use. Popular Harvest descendents include the commercial product NetCache [4] and the public-domain Squid caches [20]. <p> Proxycizer automati-cally senses the log format and selects the correct filter. The currently supported log formats are Squid access logs [20], Crispy Squid access logs [8], Harvest access and hierarchy logs <ref> [3] </ref>, UC Berkeley Home IP traces [11], DEC traces [12], and simclient logs. Proxycizer also implements several proxy simulator classes, including those for Harvest and the CRISP cache architectures described in Section 2.
Reference: [4] <institution> Internet Middleware Corporation. Harvest cached-3.0 user's manual. </institution> <address> San Jose, California. </address>
Reference-contexts: 1 Introduction Hierarchical Web proxy caches descended from the Harvest project [3] are now in wide use. Popular Harvest descendents include the commercial product NetCache <ref> [4] </ref> and the public-domain Squid caches [20]. Hierarchical Web caches are built as a collective of independent proxy caching servers that share their cache contents through a simple multi-cast query mechanism using the Internet Cache Protocol (ICP) [19].
Reference: [5] <author> Brad Duska, David Marwood, and Michael J. Feeley. </author> <title> The measured access characteristics of World-Wide-Web client proxy caches. </title> <booktitle> In Proceedings of The USENIX Symposium on Internet Technologies and Systems (USITS), </booktitle> <month> De-cember </month> <year> 1997. </year>
Reference-contexts: S-Clients [2] introduces a scalable mechanism for driving Web servers to overload conditions. Squid Proxy Analysis (SPA) <ref> [5] </ref> provides trace-driven simulations that are similar in nature to our simulation programs, but are tailored to emulate the replacement behavior of Squid caches. 5 Conclusion Distributed proxy caches are an important tool for reducing Internet congestion and access latency.
Reference: [6] <author> Li Fan, Pei Cao, Jussara Almeida, and Andrei Broder. </author> <title> Summary Cache: A scalable wide-area Web cache sharing protocol. </title> <booktitle> In Proceedings of ACM SIGCOMM98, </booktitle> <month> September </month> <year> 1998. </year>
Reference-contexts: However, our experiments have shown that hierarchical caches have scalability and coverage problems that manifest themselves in large cache configurations [9]. Other researchers have confirmed these observations <ref> [17, 6] </ref>. The CRISP project is exploring alternative Web cache structures that provide more scalable and efficient structures for large, geographically dispersed collective Web caches. <p> One problem with this structure is that map replicas increase the storage requirements for the cache. If the size of the map becomes cumbersome, a map compression scheme such as Summary Cache <ref> [6] </ref> may be used, although our prototype does not support this. 2.2.3 Replicated Partial Directory Replicated Partial Directory seeks to further reduce the cost of replicating the cache directory, but without compromising hit ratios significantly.
Reference: [7] <author> Syam Gadde. </author> <title> The Proxy-cizer Web proxy tool suite. </title> <address> http://www.cs.duke.edu/ari/Proxycizer/. </address>
Reference-contexts: We later added tools to simulate the behavior of proxy caches under workloads driven by various proxy traces. We have consolidated these programs into a carefully-designed suite of tools and reusable libraries called Proxycizer. <ref> [7] </ref> Proxycizer is written in C++, will compile with any recent version of g++ (as well as many vendor-specific compilers) and has been tested on platforms running FreeBSD, Digital Unix, and Solaris.
Reference: [8] <author> Syam Gadde, Jeff Chase, and Michael Ra-binovich. Crispy Squid. </author> <note> Available at http://www.cs.duke.edu/ari/crisp/. </note>
Reference-contexts: Proxycizer automati-cally senses the log format and selects the correct filter. The currently supported log formats are Squid access logs [20], Crispy Squid access logs <ref> [8] </ref>, Harvest access and hierarchy logs [3], UC Berkeley Home IP traces [11], DEC traces [12], and simclient logs. Proxycizer also implements several proxy simulator classes, including those for Harvest and the CRISP cache architectures described in Section 2.
Reference: [9] <author> Syam Gadde, Jeff Chase, and Michael Rabi-novich. </author> <title> Directory structures for scalable Internet caches. </title> <type> Technical Report CS-1997-18, </type> <institution> Department of Computer Science, Duke University, </institution> <month> November </month> <year> 1997. </year>
Reference-contexts: However, our experiments have shown that hierarchical caches have scalability and coverage problems that manifest themselves in large cache configurations <ref> [9] </ref>. Other researchers have confirmed these observations [17, 6]. The CRISP project is exploring alternative Web cache structures that provide more scalable and efficient structures for large, geographically dispersed collective Web caches. <p> It also places unnecessary load on high-level servers that field the cache misses from all of their de scendents. * Multicasting can increase the probe load on all caching servers to unmanageable levels. Our studies <ref> [9] </ref> showed that a three-level hierarchy of caches using ICP can send an order of magnitude more query traffic through the network than a CRISP cache of equivalent size equipped with a central mapping server. * Lost ICP messages or busy neighbor caches increase miss latencies.
Reference: [10] <author> Syam Gadde, Michael Rabinovich, and Jeff Chase. </author> <title> Reduce, Reuse, Recycle: An approach to building large Internet caches. </title> <booktitle> In Proceedings of The Sixth Workshop on Hot Topics in Operating Systems (HOTOS-VI), </booktitle> <pages> pages 93-98, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: The key design issue for a CRISP cache is the structure of the mapping service. Several factors influence the placement of proxies for a particular environment; likewise, different proxy organizations dictate different structures for the mapping service. The first CRISP prototype <ref> [10] </ref> was based on a set of independent mapping servers, each maintaining an exclusive portion of the global directory. <p> CRISP mapping server on a 266 MHz AlphaStation 500 system with 100 Mb/s Ethernet yielded an average query latency of less than 3 ms at a request rate corresponding to approximately 85,000 clients (estimated by determining the average per-client access rate seen at an AT&T proxy during peak demand periods) <ref> [10] </ref>. In fact, the mapping server has little effect on access latency since it merely performs a hash lookup on an in-memory directory.
Reference: [11] <author> Steven D. Gribble. </author> <title> UC Berkeley Home IP HTTP traces, </title> <month> July </month> <year> 1997. </year> <note> Available at http://www.acm.org/sigcomm/ITA/. </note>
Reference-contexts: Proxycizer automati-cally senses the log format and selects the correct filter. The currently supported log formats are Squid access logs [20], Crispy Squid access logs [8], Harvest access and hierarchy logs [3], UC Berkeley Home IP traces <ref> [11] </ref>, DEC traces [12], and simclient logs. Proxycizer also implements several proxy simulator classes, including those for Harvest and the CRISP cache architectures described in Section 2. Several other useful packages have recently been made available, each addressing different aspects of Internet server simulation and benchmarking.
Reference: [12] <author> Thomas M. Kroeger, Jeff Mogul, and Carlos Maltzahn. </author> <title> Digital's Web proxy traces. </title> <note> Available at ftp://ftp.digital.com /pub/DEC/traces/proxy/webtraces.html. </note>
Reference-contexts: Proxycizer automati-cally senses the log format and selects the correct filter. The currently supported log formats are Squid access logs [20], Crispy Squid access logs [8], Harvest access and hierarchy logs [3], UC Berkeley Home IP traces [11], DEC traces <ref> [12] </ref>, and simclient logs. Proxycizer also implements several proxy simulator classes, including those for Harvest and the CRISP cache architectures described in Section 2. Several other useful packages have recently been made available, each addressing different aspects of Internet server simulation and benchmarking.
Reference: [13] <author> Carlos Maltzahn, Kathy J. Richardson, and Dirk Grunwald. </author> <title> Performance issues of enterprise level Web proxies. </title> <booktitle> In Proceedings of The ACM SIGMETRICS Conference on Measurement and Modeling of Compter Systems, </booktitle> <pages> pages 13-23, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Our approach to evaluating performance of Web cache structures is rooted in the wide range of proxy traces available today. Because these traces are indicative of current real-life proxy usage patterns, they are a powerful basis for predicting performance of collective cache structures in actual use. <ref> [13, 16] </ref> Used in conjunction with continuous online trace gathering, trace-based performance studies can track the effects of Web usage patterns as they evolve. However, we have found that the tools available to analyze these traces are inadequate for our needs.
Reference: [14] <author> Stephen Manley, Michael Courage, and Margo Seltzer. </author> <title> A self-scaling and self-configuring benchmark for Web servers. </title> <note> Available at http://www.eecs.harvard.edu/ ~vino/web/hbench-web/. </note>
Reference-contexts: Several other useful packages have recently been made available, each addressing different aspects of Internet server simulation and benchmarking. While Proxycizer is purely trace-driven, the Wis-consin Proxy Benchmark (WPB) [1] uses synthetic workloads modeled to emulate typical temporal locality patterns to test proxies under load. hbench:Web <ref> [14] </ref> is a benchmark for Web servers that is not explicitly trace-driven, but does preprocess Web server logs and automatically generates an appropriate traffic model for driving a Web server. S-Clients [2] introduces a scalable mechanism for driving Web servers to overload conditions.
Reference: [15] <author> Karin Petersen, Mike J. Spreitzer, Douglas B. Terry, Marvin M. Theimer, and Alan J. De-mers. </author> <title> Flexible update propagation for weakly consistent replication. </title> <booktitle> In Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 288-301, </pages> <month> October </month> <year> 1997. </year>
Reference-contexts: Asynchronous propagation of directory updates enables the collective CRISP cache to cover larger network distances without affecting miss latency. For example, proxies may batch updates to the map and propagate them with periodic anti-entropy exchanges using an efficient stream protocol <ref> [15] </ref>. Our current prototype batches updates into TCP streams sent to a central distribution node at (configurable) periodic intervals, in order to amortize network connection costs over a group of updates. On typical traces, propagation intervals of up to an hour cause hit ratios to drop by at most 5%.
Reference: [16] <author> Alex Rousskov. </author> <title> On performance of caching proxies. </title> <note> Available at http://www.cs.ndsu.nodak.edu/ rousskov /research/cache/squid/profiling/papers/. </note>
Reference-contexts: Our approach to evaluating performance of Web cache structures is rooted in the wide range of proxy traces available today. Because these traces are indicative of current real-life proxy usage patterns, they are a powerful basis for predicting performance of collective cache structures in actual use. <ref> [13, 16] </ref> Used in conjunction with continuous online trace gathering, trace-based performance studies can track the effects of Web usage patterns as they evolve. However, we have found that the tools available to analyze these traces are inadequate for our needs.
Reference: [17] <author> Renu Tewari, Michael Dahlin, Harrick Vin, and John Kay. </author> <title> Beyond hierarchies: Design considerations for distributed caching on the Internet. </title> <type> Technical Report TR98-04, </type> <institution> Department of Computer Sciences, University of Texas at Austin, </institution> <month> February </month> <year> 1998. </year>
Reference-contexts: However, our experiments have shown that hierarchical caches have scalability and coverage problems that manifest themselves in large cache configurations [9]. Other researchers have confirmed these observations <ref> [17, 6] </ref>. The CRISP project is exploring alternative Web cache structures that provide more scalable and efficient structures for large, geographically dispersed collective Web caches. <p> Tewari et al. <ref> [17] </ref> show that this leads to remarkably high latencies in some configurations, motivating shallower and "bushier" hierarchies. It also places unnecessary load on high-level servers that field the cache misses from all of their de scendents. * Multicasting can increase the probe load on all caching servers to unmanageable levels.
Reference: [18] <author> Vinod Valloppillil and Keith W. Ross. </author> <title> Cache Array Routing Protocol v1.0. </title> <type> Internet-Draft, </type> <month> June </month> <year> 1997. </year> <note> Available at http://www.etext.org/Internet/Internet-Drafts/draft-vinod-carp-v1-03.txt. </note>
Reference-contexts: describe the three basic directory configurations we have prototyped with Crispy Squid. 2.2.1 Partitioned Synchronous Directory In this structure, the mapping service consists of several mapping servers, each assigned some exclusive subset of the URL-space using a hash function in a manner similar to the Cache Array Routing Protocol (CARP) <ref> [18] </ref>. Caching servers synchronously query the assigned mapping server when a client's request results in a local cache miss, and notify the appropriate mapping server when an object is added to or evicted from the local cache.
Reference: [19] <author> Duane Wessels and K Claffy. </author> <title> Internet cache protocol (ICP), </title> <type> version 2. Internet-Draft, </type> <month> May </month> <year> 1997. </year> <note> Available at http://ircache.nlanr.net/Cache/ICP/. </note>
Reference-contexts: Popular Harvest descendents include the commercial product NetCache [4] and the public-domain Squid caches [20]. Hierarchical Web caches are built as a collective of independent proxy caching servers that share their cache contents through a simple multi-cast query mechanism using the Internet Cache Protocol (ICP) <ref> [19] </ref>. The National Laboratory for Applied Network Research has engineered a structure of Squid caches spanning the United States, and is coordinating with participants across the world to build a "global mesh" of Web proxy caches.
Reference: [20] <author> Duane Wessels et al. </author> <title> Squid Internet Object Cache. </title> <address> http://squid.nlanr.net/. </address>
Reference-contexts: 1 Introduction Hierarchical Web proxy caches descended from the Harvest project [3] are now in wide use. Popular Harvest descendents include the commercial product NetCache [4] and the public-domain Squid caches <ref> [20] </ref>. Hierarchical Web caches are built as a collective of independent proxy caching servers that share their cache contents through a simple multi-cast query mechanism using the Internet Cache Protocol (ICP) [19]. <p> Proxycizer automati-cally senses the log format and selects the correct filter. The currently supported log formats are Squid access logs <ref> [20] </ref>, Crispy Squid access logs [8], Harvest access and hierarchy logs [3], UC Berkeley Home IP traces [11], DEC traces [12], and simclient logs. Proxycizer also implements several proxy simulator classes, including those for Harvest and the CRISP cache architectures described in Section 2.
References-found: 20

