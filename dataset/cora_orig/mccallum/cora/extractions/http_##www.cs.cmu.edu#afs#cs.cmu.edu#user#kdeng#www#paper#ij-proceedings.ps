URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/kdeng/www/paper/ij-proceedings.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/kdeng/www/papers.html
Root-URL: 
Abstract: Instance-based learning methods explicitly remember all the data that they receive. They usually have no training phase, and only at prediction time do they perform computation. Then, they take a query, search the database for similar datapoints and build an on-line local model (such as a local average or local regression) with which to predict an output value. In this paper we review the advantages of instance based methods for autonomous systems, but we also note the ensuing cost: hopelessly slow computation as the database grows large. We present and evaluate a new way of structuring a database and a new algorithm for accessing it that maintains the advantages of instance-based learning. Earlier attempts to combat the cost of instance-based learning have sacrificed the explicit retention of all data, or been applicable only to instance-based predictions based on a small number of near neighbors or have had to re-introduce an explicit training phase in the form of an interpolative data structure. Our approach builds a multiresolution data structure to summarize the database of experiences at all resolutions of interest simultaneously. This permits us to query the database with the same exibility as a conventional linear search, but at greatly reduced computational cost.
Abstract-found: 1
Intro-found: 1
Reference: [Kibler and Aha, 1988] <author> D. Kibler and D. W. Aha, </author> <title> Comparing Instance Averaging and Instance Filtering Learning Algorithms, </title> <booktitle> Proceedings of 3rd European Working Session on Learning, </booktitle> <publisher> Pitman, </publisher> <year> 1988 </year>
Reference-contexts: Another solution to the cost of instance-based learning is editing (or prototypes): most datapoints are forgotten and only particularly representative ones are used (e.g. <ref> [Kibler and Aha, 1988; Skalak, 1994] </ref>). Kibler and Aha extended this idea further by allowing datapoints to represent local averages of sets of previously-observed datapoints. This can be effective, and unlike range-searching can be applicable even for wide kernel widths.
Reference: [Atkeson, 1989] <author> C. G. Atkeson, </author> <title> Using Local Models to Control Movement, </title> <booktitle> Proceedings of Neural Information Processing Systems Conference, </booktitle> <year> 1989 </year>
Reference-contexts: For practical use in function approximation, much better instance-based methods than nearest neighbor are available that form local linear models, and compute weighted averages of data to remove the noise from predictions (e.g, see <ref> [Atkeson, 1989; Grosse, 1989] </ref>). Learning parameters need not be fixed in advance There are many learning parameters in instance-based algorithms. One of the most important concerns the extent to which the smoothing of noise is traded against goodness of fit.
Reference: [Franke, 1982] <author> R. Franke, </author> <title> Scattered Data Interpolation: Tests of Some Methods, </title> <journal> Mathematics of Computation, </journal> <volume> Vol 38, No 157, </volume> <month> January </month> <year> 1982. </year>
Reference-contexts: Kernel Regression The approach taken in this paper can be applied to a wide variety of instance-based algorithms, but here we will concentrate on one of the most well known examples: Kernel Regression (see, for example, <ref> [Franke, 1982] </ref>). Assume that datapoints consist of -input, output pairs such as (x 1 ,y 1 ), (x 2 ,y 2 ),. . . (x N ,y N ) where the inputs are d-element real-valued vectors and the outputs are scalars, and to date we have observed N datapoints.
Reference: [Grosse, 1989] <author> E. Grosse, LOESS: </author> <title> Multivariate Smoothing by Moving Least Squares, Approximation Theory VI, Edited by C. </title> <editor> K. Chul, L. L. Schumaker and J. D. Ward, </editor> <publisher> Academic Press, </publisher> <year> 1989 </year>
Reference-contexts: For practical use in function approximation, much better instance-based methods than nearest neighbor are available that form local linear models, and compute weighted averages of data to remove the noise from predictions (e.g, see <ref> [Atkeson, 1989; Grosse, 1989] </ref>). Learning parameters need not be fixed in advance There are many learning parameters in instance-based algorithms. One of the most important concerns the extent to which the smoothing of noise is traded against goodness of fit. <p> As the database grows large it becomes increasingly expensive to make predictions. Each prediction involves searching the database to find similar earlier datapoints. This can mean hopelessly slow performance after merely tens of thousands of predictions. Various researchers have attempted to deal with this problem <ref> [Aha et al., 1991; Grosse, 1989; Moore, 1990; Skalak, 1994] </ref>, but, as we will see in Section 7, none in a manner that avoids sacrificing at least one of the benefits of instance-based methods described above. <p> There are interesting parallels between prototypes and Multires. The intermediate nodes of the kd-tree can be thought of as fabricated prototypes, summarizing all the data below them. Decision trees and kd-trees have been previously used to cache local mappings in the tree leaves <ref> [Grosse, 1989; Moore, 1990; Omohundro, 1991; Quinlan, 1993] </ref>. These algorithms provide fast access once the tree is built, but a new structure needs to be built each time new learning parameters are required. Furthermore, unlike the multires-olution method, the resulting predictions from the tree have substantial discontinuities between boundaries. <p> These algorithms provide fast access once the tree is built, but a new structure needs to be built each time new learning parameters are required. Furthermore, unlike the multires-olution method, the resulting predictions from the tree have substantial discontinuities between boundaries. Only in <ref> [Grosse, 1989] </ref> is continuity enforced, but at the cost of tree-size, tree-building-cost and prediction-cost all being exponential in the number of input variables. Dimensionality is a weakness of Multires. Diminishing returns set in above approximately 10 dimensions if the data is distributed uniformly.
Reference: [Maron and Moore, 1994] <author> O. Maron and A. W. Moore, </author> <title> Hoeffding Races: Accelerating Model Selection Search, </title> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <year> 1994 </year>
Reference-contexts: The right hand graph shows a corresponding decrease in computational cost. computational cost of multires KR against t--the cutoff threshold. Real datasets In another experiment, we ran multires KR on data from several real-world and robot-learning datasets. Further details of the datasets can be found in <ref> [Maron and Moore, 1994] </ref>. They include an industrial packaging process for which the slowness of prediction had been a reasonable cause for concern. Encouragingly, multires KR speeds up prediction by a factor of 100 with no discernible difference in prediction quality between multires and regular KR.
Reference: [Moore, 1990] <author> A. W. Moore, </author> <title> Acquisition of Dynamic Control Knowledge for a Robotic Manipulator, </title> <booktitle> Proceedings of the 7th International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990 </year>
Reference-contexts: As the database grows large it becomes increasingly expensive to make predictions. Each prediction involves searching the database to find similar earlier datapoints. This can mean hopelessly slow performance after merely tens of thousands of predictions. Various researchers have attempted to deal with this problem <ref> [Aha et al., 1991; Grosse, 1989; Moore, 1990; Skalak, 1994] </ref>, but, as we will see in Section 7, none in a manner that avoids sacrificing at least one of the benefits of instance-based methods described above. <p> There are interesting parallels between prototypes and Multires. The intermediate nodes of the kd-tree can be thought of as fabricated prototypes, summarizing all the data below them. Decision trees and kd-trees have been previously used to cache local mappings in the tree leaves <ref> [Grosse, 1989; Moore, 1990; Omohundro, 1991; Quinlan, 1993] </ref>. These algorithms provide fast access once the tree is built, but a new structure needs to be built each time new learning parameters are required. Furthermore, unlike the multires-olution method, the resulting predictions from the tree have substantial discontinuities between boundaries.
Reference: [Moore et al., 1992] <author> A. W. Moore and D. J. Hill and M. P. Johnson, </author> <title> An Empirical Investigation of Brute Force to choose Features, Smoothers and Function Approximators, </title> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Volume 3, </volume> <editor> Edited by S. Hanson and S. Judd and T. Petsche, </editor> <publisher> MIT Press, </publisher> <year> 1992 </year>
Reference-contexts: This is of immense use in an autonomous system that is both making new predictions online and tuning its learning parameters online as new data is arriving <ref> [Moore et al., 1992] </ref>. In contrast, a non-instance-based method must choose a parameter set and then train with it.
Reference: [Omohundro, 1991] <author> S. M. Omohundro, </author> <title> Bumptrees for Efficient Function, Constraint, and Classification Learning, </title> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <year> 1991 </year>
Reference-contexts: Constructing the kernel regression tree To construct a tree from a batch of training instances we use a top-down recursive procedure. This is the most standard way of constructing kd-trees, described, for example, in <ref> [Preparata et al., 1985; Omohundro, 1991] </ref>. In this work we use the common variation of splitting a hypercube in the center of the widest dimension instead of at the median point. <p> There are interesting parallels between prototypes and Multires. The intermediate nodes of the kd-tree can be thought of as fabricated prototypes, summarizing all the data below them. Decision trees and kd-trees have been previously used to cache local mappings in the tree leaves <ref> [Grosse, 1989; Moore, 1990; Omohundro, 1991; Quinlan, 1993] </ref>. These algorithms provide fast access once the tree is built, but a new structure needs to be built each time new learning parameters are required. Furthermore, unlike the multires-olution method, the resulting predictions from the tree have substantial discontinuities between boundaries.
Reference: [Preparata et al., 1985] <author> F. P. Preparata and M. Shamos, </author> <title> Computational Geometry, </title> <publisher> Springer-Verlag, </publisher> <year> 1985 </year>
Reference-contexts: But how can such a set of groupings be obtained? And how can we ensure that all weights within each grouping are very similar? We can use a kd-tree <ref> [Preparata et al., 1985] </ref>, enhanced with extra cached information, as an implementation of this grouping idea. A kd-tree is a binary tree that recursively splits the whole input space into partitions, in a manner similar to a decision tree acting on real-valued inputs. <p> Constructing the kernel regression tree To construct a tree from a batch of training instances we use a top-down recursive procedure. This is the most standard way of constructing kd-trees, described, for example, in <ref> [Preparata et al., 1985; Omohundro, 1991] </ref>. In this work we use the common variation of splitting a hypercube in the center of the widest dimension instead of at the median point. <p> No other software method, to our knowledge, does. There is, however, the simple hardware alternative of using a fast enough computer. The standard kernel regression method can be parallelized with full efficiency. Kd-trees have frequently been used in instance-based learning methods for nearest neighbor searching or for range searching <ref> [Preparata et al., 1985] </ref>.
Reference: [Quinlan, 1993] <author> J. R. Quinlan, </author> <title> Combining Instance-Based and Model-Based Learning, </title> <booktitle> Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <year> 1993 </year>
Reference-contexts: There are interesting parallels between prototypes and Multires. The intermediate nodes of the kd-tree can be thought of as fabricated prototypes, summarizing all the data below them. Decision trees and kd-trees have been previously used to cache local mappings in the tree leaves <ref> [Grosse, 1989; Moore, 1990; Omohundro, 1991; Quinlan, 1993] </ref>. These algorithms provide fast access once the tree is built, but a new structure needs to be built each time new learning parameters are required. Furthermore, unlike the multires-olution method, the resulting predictions from the tree have substantial discontinuities between boundaries.
Reference: [Skalak, 1994] <author> D. B. Skalak, </author> <title> Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithms, </title> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <year> 1994 </year>
Reference-contexts: As the database grows large it becomes increasingly expensive to make predictions. Each prediction involves searching the database to find similar earlier datapoints. This can mean hopelessly slow performance after merely tens of thousands of predictions. Various researchers have attempted to deal with this problem <ref> [Aha et al., 1991; Grosse, 1989; Moore, 1990; Skalak, 1994] </ref>, but, as we will see in Section 7, none in a manner that avoids sacrificing at least one of the benefits of instance-based methods described above. <p> Another solution to the cost of instance-based learning is editing (or prototypes): most datapoints are forgotten and only particularly representative ones are used (e.g. <ref> [Kibler and Aha, 1988; Skalak, 1994] </ref>). Kibler and Aha extended this idea further by allowing datapoints to represent local averages of sets of previously-observed datapoints. This can be effective, and unlike range-searching can be applicable even for wide kernel widths.
Reference: [Stanfill et al., 1986] <author> C. Stanfill and D. Waltz, </author> <title> Towards Memory-Based Reasoning, </title> <journal> Communications of the ACM, </journal> <volume> 29(12), </volume> <year> 1986 </year>
References-found: 12

