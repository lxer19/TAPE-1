URL: http://www.neci.nj.nec.com/homepages/mann/papers/eccv96.ps.gz
Refering-URL: http://www.neci.nj.nec.com/homepages/mann/papers-available.html
Root-URL: http://www.neci.nj.nec.com
Title: Computational Perception of Scene Dynamics  
Author: Richard Mann, Allan Jepson and Jeffrey Mark Siskind 
Address: Toronto, Ontario M5S 1A4 CANADA  
Affiliation: Department of Computer Science, University of Toronto  
Abstract: Understanding observations of interacting objects requires one to reason about qualitative scene dynamics. For example, on observing a hand lifting a can, we may infer that an `active' hand is applying an upwards force (by grasping) to lift a `passive' can. We present an implemented computational theory that derives such dynamic descriptions directly from camera input. Our approach is based on an analysis of the Newtonian mechanics of a simplified scene model. Interpretations are expressed in terms of assertions about the kinematic and dynamic properties of the scene. The feasibility of interpretations can be determined relative to Newtonian mechanics by a reduction to linear programming. Finally, to select plausible interpretations, multiple feasible solutions are compared using a preference hierarchy. We provide computational examples to demonstrate that our model is sufficiently rich to describe a wide variety of image sequences. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Norman I. Badler. </author> <title> Temporal scene analysis: Conceptual descriptions of object movements. </title> <type> Technical Report 80, </type> <institution> University of Toronto Department of Computer Science, </institution> <month> February </month> <year> 1975. </year>
Reference-contexts: The use of domain knowledge by a vision system has been studied extensively for both static and motion domains. Many prior systems have attempted to extract event or conceptual descriptions from image sequences based on spatio-temporal features of the input <ref> [1, 23, 18, 4, 14] </ref>. A number of other systems have attempted to represent structure in static and dynamic scenes using qualitative physical models or rule-based systems [6, 8, 13, 20, 22, 5].
Reference: 2. <author> David Baraff. </author> <title> Interactive simulation of solid rigid bodies. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 15(3) </volume> <pages> 63-75, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Given a scene with convex polygonal object parts, we can represent the forces between contacting parts by a set of forces acting on the vertices of the convex hull of their contact region <ref> [7, 2] </ref>. Under this simplification, the equations of motion for each object can be written as a set of equality constraints which relate the forces and torques at each contact point to the object masses and accelerations.
Reference: 3. <author> M. Blum, A. K. Griffith, and B. Neumann. </author> <title> A stability test for configurations of blocks. </title> <editor> A. I. </editor> <volume> Memo 188, </volume> <editor> M. I. T. </editor> <booktitle> Artificial Intelligence Laboratory, </booktitle> <month> February </month> <year> 1970. </year>
Reference-contexts: A number of other systems have used physically-based representations. In particular, Ikeuchi and Suehiro [10] and Siskind [21] propose representions of events based on changing kinematic relations in time-varying scenes. Also, closer to our approach, Blum et. al. <ref> [3] </ref> propose a representation of forces in static scenes. Our system extends these approaches to consider both kinematic and dynamic properties in time-varying scenes containing rigid objects. coke 30 37 48 60 cars 10 26 28 35 arch 35 49 55 65 21 31 35 39 Fig. 1.
Reference: 4. <author> Gary C. Borchardt. </author> <title> Event calculus. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 524-527, </pages> <address> Los Angeles, CA, </address> <month> August </month> <year> 1985. </year>
Reference-contexts: The use of domain knowledge by a vision system has been studied extensively for both static and motion domains. Many prior systems have attempted to extract event or conceptual descriptions from image sequences based on spatio-temporal features of the input <ref> [1, 23, 18, 4, 14] </ref>. A number of other systems have attempted to represent structure in static and dynamic scenes using qualitative physical models or rule-based systems [6, 8, 13, 20, 22, 5].
Reference: 5. <author> Matthew Brand, Lawrence Birnbaum, and Paul Cooper. </author> <title> Sensible scenes: Visual understanding of complex scenes through causal analysis. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artifical Intelligence, </booktitle> <pages> pages 588-593, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Many prior systems have attempted to extract event or conceptual descriptions from image sequences based on spatio-temporal features of the input [1, 23, 18, 4, 14]. A number of other systems have attempted to represent structure in static and dynamic scenes using qualitative physical models or rule-based systems <ref> [6, 8, 13, 20, 22, 5] </ref>. In contrast to both ? Also at Canadian Institute for Advanced Research. ?? Current address: Department of Electrical Engineering, Technion, Haifa 32000, ISRAEL of these approaches, our system uses an explicit physically-based representation based on Newtonian physics.
Reference: 6. <author> Scott Elliott Fahlman. </author> <title> A planning system for robot construction tasks. </title> <journal> Artificial Intelligence, </journal> <volume> 5(1) </volume> <pages> 1-49, </pages> <year> 1974. </year>
Reference-contexts: Many prior systems have attempted to extract event or conceptual descriptions from image sequences based on spatio-temporal features of the input [1, 23, 18, 4, 14]. A number of other systems have attempted to represent structure in static and dynamic scenes using qualitative physical models or rule-based systems <ref> [6, 8, 13, 20, 22, 5] </ref>. In contrast to both ? Also at Canadian Institute for Advanced Research. ?? Current address: Department of Electrical Engineering, Technion, Haifa 32000, ISRAEL of these approaches, our system uses an explicit physically-based representation based on Newtonian physics.
Reference: 7. <author> Roy Featherstone. </author> <title> Robot Dynamics Algorithms. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1987. </year>
Reference-contexts: Given a scene with convex polygonal object parts, we can represent the forces between contacting parts by a set of forces acting on the vertices of the convex hull of their contact region <ref> [7, 2] </ref>. Under this simplification, the equations of motion for each object can be written as a set of equality constraints which relate the forces and torques at each contact point to the object masses and accelerations.
Reference: 8. <author> Brian V. Funt. </author> <title> Problem-solving with diagrammatic representations. </title> <journal> Artificial Intelligence, </journal> <volume> 13(3) </volume> <pages> 201-230, </pages> <month> May </month> <year> 1980. </year>
Reference-contexts: Many prior systems have attempted to extract event or conceptual descriptions from image sequences based on spatio-temporal features of the input [1, 23, 18, 4, 14]. A number of other systems have attempted to represent structure in static and dynamic scenes using qualitative physical models or rule-based systems <ref> [6, 8, 13, 20, 22, 5] </ref>. In contrast to both ? Also at Canadian Institute for Advanced Research. ?? Current address: Department of Electrical Engineering, Technion, Haifa 32000, ISRAEL of these approaches, our system uses an explicit physically-based representation based on Newtonian physics.
Reference: 9. <author> Herbert Goldstein. </author> <title> Classical Mechanics. </title> <publisher> Addison-Wesley, </publisher> <address> second edition, </address> <year> 1980. </year>
Reference-contexts: This test is valid for both two and three dimensional scene models. For rigid bodies under continuous motion, the dynamics are described by the Newton-Euler equations of motion <ref> [9] </ref> which relate the total applied force and torque to the observed accelerations of the objects.
Reference: 10. <author> Katsushi Ikeuchi and T. Suehiro. </author> <title> Towards an assembly plan from observation, part i: Task recognition with polyhedral objects. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10(3) </volume> <pages> 368-385, </pages> <year> 1994. </year>
Reference-contexts: A number of other systems have used physically-based representations. In particular, Ikeuchi and Suehiro <ref> [10] </ref> and Siskind [21] propose representions of events based on changing kinematic relations in time-varying scenes. Also, closer to our approach, Blum et. al. [3] propose a representation of forces in static scenes.
Reference: 11. <author> Michael Jenkin and Allan D. Jepson. </author> <title> Detecting floor anomalies. </title> <booktitle> In Proceedings of the British Machine Vision Conference, </booktitle> <pages> pages 731-740, </pages> <address> York, UK, </address> <year> 1994. </year>
Reference-contexts: As described in x2.1, we model the scene as a set of two-dimensional convex polygons. To obtain estimates for the object motions we use a view-based tracking algorithm similar to the optical flow and stereo disparity algorithms described in <ref> [12, 11] </ref>. The input to the tracker consists of the image sequence, a set of object template images (including a polygonal outline for each object), and an estimate for the object positions in the first frame of the sequence.
Reference: 12. <author> Allan D. Jepson and Michael J. Black. </author> <title> Mixture models for optical flow. </title> <booktitle> In Proceedings of the IEEE Conference of Computer Vision and Pattern Recognition, </booktitle> <pages> pages 760-761, </pages> <year> 1993. </year>
Reference-contexts: As described in x2.1, we model the scene as a set of two-dimensional convex polygons. To obtain estimates for the object motions we use a view-based tracking algorithm similar to the optical flow and stereo disparity algorithms described in <ref> [12, 11] </ref>. The input to the tracker consists of the image sequence, a set of object template images (including a polygonal outline for each object), and an estimate for the object positions in the first frame of the sequence.
Reference: 13. <author> Leo Joskowicz and Elisha P. Sacks. </author> <title> Computational kinematics. </title> <journal> Artificial Intelligence, </journal> <volume> 51(1-3):381-416, </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: Many prior systems have attempted to extract event or conceptual descriptions from image sequences based on spatio-temporal features of the input [1, 23, 18, 4, 14]. A number of other systems have attempted to represent structure in static and dynamic scenes using qualitative physical models or rule-based systems <ref> [6, 8, 13, 20, 22, 5] </ref>. In contrast to both ? Also at Canadian Institute for Advanced Research. ?? Current address: Department of Electrical Engineering, Technion, Haifa 32000, ISRAEL of these approaches, our system uses an explicit physically-based representation based on Newtonian physics.
Reference: 14. <author> Yasuo Kuniyoshi and Hirochika Inoue. </author> <title> Qualitative recognition of ongoing human action sequences. </title> <booktitle> In IJCAI93, </booktitle> <pages> pages 1600-1609, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: The use of domain knowledge by a vision system has been studied extensively for both static and motion domains. Many prior systems have attempted to extract event or conceptual descriptions from image sequences based on spatio-temporal features of the input <ref> [1, 23, 18, 4, 14] </ref>. A number of other systems have attempted to represent structure in static and dynamic scenes using qualitative physical models or rule-based systems [6, 8, 13, 20, 22, 5].
Reference: 15. <author> Vladimir Lifschitz. </author> <title> Computing circumscription. </title> <booktitle> In IJCAI85, </booktitle> <pages> pages 121-127, </pages> <year> 1985. </year>
Reference-contexts: Finally, in the case that the assertions at the highest priority are the same in both interpretations, then we check the assertions at the next lower priority, and so on. This approach, based upon prioritised ordering of elementary preference relations, is similar to prioritised circumscription <ref> [15] </ref>. To find maximally-preferred models, we search the space of possible interpretations. We perform a breadth-first search, starting with the empty set of assertions, incrementally adding new assertions to this set. Each branch of the search terminates upon finding a minimal set of assertions required for feasible force balancing.
Reference: 16. <author> Richard Mann. </author> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Toronto. </institution> <note> To appear. </note>
Reference-contexts: This property justifies the algorithm above where we set all of the lower priority assertions to the most permissive settings during each stage of the minimization. In general we refer to this property as monotonicity <ref> [16] </ref>. 5 Examples We have applied our system to several image sequences taken from a desktop environment (see Figure 1). The sequences were taken from a video camera attached to a SunVideo imaging system.
Reference: 17. <author> Richard Mann, Allan Jepson, and Jeffrey Mark Siskind. </author> <title> The compuational perception of scene dynamics. </title> <note> 1996. Submitted. </note>
Reference-contexts: Since we can approximate these constraints by a set of linear equations and inequalities, dynamic feasibility can be tested using linear programming (see <ref> [17] </ref> for details). 4 Preferences Given a fairly rich ontology, it is common for there to be multiple feasible interpretations for a given scene configuration. For example, for the lifting phase of the coke sequence in Figure 1 there are five feasible interpretations, as shown in Figure 2.
Reference: 18. <author> Bernd Neumann and Hans-Joachim Novak. </author> <title> Event models for recognition and natural language description of events in real-world image sequences. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 724-726, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: The use of domain knowledge by a vision system has been studied extensively for both static and motion domains. Many prior systems have attempted to extract event or conceptual descriptions from image sequences based on spatio-temporal features of the input <ref> [1, 23, 18, 4, 14] </ref>. A number of other systems have attempted to represent structure in static and dynamic scenes using qualitative physical models or rule-based systems [6, 8, 13, 20, 22, 5].
Reference: 19. <author> Whitman Richards, Allan D. Jepson, and Jacob Feldman. </author> <title> Priors, preferences and categorical percepts. </title> <editor> In Whitman Richards and David Knill, editors, </editor> <title> Perception as Bayesian Inference. </title> <institution> Cambridge University Press. </institution> <note> To appear. </note>
Reference-contexts: The absence of an assertion denotes its negation. Rather than searching for all interpretations, we seek interpretations that require, in some specified sense, the weakest properties of the various objects. We use model preference relations, as discussed by Richards, Jepson, and Feld-man <ref> [19] </ref>, to express a suitable ordering on the various interpretations. The basic idea is to compare pairs of interpretations using a prioritised set of elementary preference relations.
Reference: 20. <author> Jeffrey Mark Siskind. </author> <title> Naive Physics, Event Perception, Lexical Semantics, and Language Acquisition. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cam-bridge, MA, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: Many prior systems have attempted to extract event or conceptual descriptions from image sequences based on spatio-temporal features of the input [1, 23, 18, 4, 14]. A number of other systems have attempted to represent structure in static and dynamic scenes using qualitative physical models or rule-based systems <ref> [6, 8, 13, 20, 22, 5] </ref>. In contrast to both ? Also at Canadian Institute for Advanced Research. ?? Current address: Department of Electrical Engineering, Technion, Haifa 32000, ISRAEL of these approaches, our system uses an explicit physically-based representation based on Newtonian physics.
Reference: 21. <author> Jeffrey Mark Siskind. </author> <title> Axiomatic support for event perception. </title> <editor> In Paul McKe-vitt, editor, </editor> <booktitle> Proceedings of the AAAI-94 Workshop on the Integration of Natural Language and Vision Processing, </booktitle> <pages> pages 153-160, </pages> <address> Seattle, WA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: A number of other systems have used physically-based representations. In particular, Ikeuchi and Suehiro [10] and Siskind <ref> [21] </ref> propose representions of events based on changing kinematic relations in time-varying scenes. Also, closer to our approach, Blum et. al. [3] propose a representation of forces in static scenes.
Reference: 22. <author> Jeffrey Mark Siskind. </author> <title> Grounding language in perception. </title> <journal> Artificial Intelligence Review, </journal> <volume> 8 </volume> <pages> 371-391, </pages> <year> 1995. </year>
Reference-contexts: Many prior systems have attempted to extract event or conceptual descriptions from image sequences based on spatio-temporal features of the input [1, 23, 18, 4, 14]. A number of other systems have attempted to represent structure in static and dynamic scenes using qualitative physical models or rule-based systems <ref> [6, 8, 13, 20, 22, 5] </ref>. In contrast to both ? Also at Canadian Institute for Advanced Research. ?? Current address: Department of Electrical Engineering, Technion, Haifa 32000, ISRAEL of these approaches, our system uses an explicit physically-based representation based on Newtonian physics.
Reference: 23. <author> John K. Tsotsos, John Mylopoulos, H. Dominic Covvey, and Steven W. Zucker. </author> <title> A framework for visual motion understanding. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 2(6) </volume> <pages> 563-573, </pages> <month> November </month> <year> 1980. </year> <title> This article was processed using the L a T E X macro package with ECCV'96 style </title>
Reference-contexts: The use of domain knowledge by a vision system has been studied extensively for both static and motion domains. Many prior systems have attempted to extract event or conceptual descriptions from image sequences based on spatio-temporal features of the input <ref> [1, 23, 18, 4, 14] </ref>. A number of other systems have attempted to represent structure in static and dynamic scenes using qualitative physical models or rule-based systems [6, 8, 13, 20, 22, 5].
References-found: 23

