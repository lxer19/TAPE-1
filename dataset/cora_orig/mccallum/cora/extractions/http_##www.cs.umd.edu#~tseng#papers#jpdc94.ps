URL: http://www.cs.umd.edu/~tseng/papers/jpdc94.ps
Refering-URL: http://www.cs.umd.edu/~tseng/papers.html
Root-URL: 
Email: seema@cs.rice.edu ken@cs.rice.edu  tseng@cs.stanford.edu  
Title: Evaluating Compiler Optimizations For Fortran D  
Author: Seema Hiranandani Ken Kennedy Chau-Wen Tseng 
Address: Houston, TX 77251-1892  Stanford, CA 94305  
Affiliation: Department of Computer Science Rice University  Computer Systems Laboratory Stanford University  
Abstract: fl This research was supported by the Center for Research on Parallel Computation, a Science and Technology Center funded by NSF through Cooperative Agreement Number CCR-9120008. Use of the Intel iPSC/860 was provided by the Center for Research on Parallel Computation under NSF Cooperative Agreement Nos. CCR-8809615 and CDA-8619893 with support from the Keck Foundation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Allen, J. R., and Kennedy, K. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems 9, </journal> <month> 4 (Oct. </month> <year> 1987), </year> <pages> 491-542. </pages>
Reference-contexts: Detailed examples of these compiler optimizations are discussed elsewhere [28, 46]. We note that several compiler optimizations utilize program transformations such as loop interchange, fusion, distribution, alignment, and strip-mining. Their legality is determined in exactly the same manner as for shared-memory parallelizing compilers <ref> [1, 32, 35] </ref>, since transformations must preserve the meaning of the original program. <p> In the next section we describe optimizations that try to hide T copy and T transit by overlapping communication with computation. 3.1.1 Message Vectorization Message vectorization is a loop-based optimization fundamental to the Fortran D compilation process [5, 19]. It uses the results of data dependence analysis <ref> [1, 35] </ref> to extract communication from within loops, combining element messages per loop iteration with one vectorized message preceding the loop. Message vectorization first calculates commlevel, the level of the deepest loop-carried true dependence or loop enclosing a loop-independent true dependence.
Reference: [2] <author> Amarasinghe, S., and Lam, M. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Program Language Design and Implementation (Albuquerque, </booktitle> <address> NM, </address> <month> June </month> <year> 1993). </year>
Reference-contexts: Aspar [29] and P 3 C [18] perform simple dependence analysis to extract communication from parallel loops, and rely on portable run-time libraries to support collective communication and reductions. The Suif compiler uses precise data-flow information for arrays from last-write-trees to avoid over-communication <ref> [2] </ref>. 32 Few optimizations have been evaluated either analytically or through empirical measurements to determine their effectiveness. Rogers & Pingali performed experiments on the Intel iPSC/2 that showed message pipelining is needed to extract parallelism from a cyclically distributed Gauss-Seidel computation [42].
Reference: [3] <institution> Applied Parallel Research. </institution> <note> Forge 90 Distributed Memory Parallelizer: User's Guide, version 8.0 ed. </note> <institution> Placerville, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: The Fortran D compiler is similar to other owner computes systems such as Adapt [38], Adaptor [8], Aspar [29], Forge90 <ref> [3] </ref>, P 3 C [18], and Vienna Fortran [12], but performs more compile-time optimization and requires less run-time support. As previously stated, most optimizations in this paper have been discovered by other researchers. Message vectorization was first developed by Gerndt [19] and Balasundaram et al. [5].
Reference: [4] <author> Balasundaram, V. </author> <title> Translating control parallelism to data parallelism. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing (Houston, </booktitle> <address> TX, </address> <month> Mar. </month> <year> 1991). </year>
Reference-contexts: Computing the result on the processor owning the rhs and then sending the result to the owner of the lhs could reduce the amount of data communicated. This optimization is a simple application of the "owner stores" rule proposed by Balasundaram <ref> [4] </ref>.
Reference: [5] <author> Balasundaram, V., Fox, G., Kennedy, K., and Kremer, U. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference (Charleston, </booktitle> <address> SC, </address> <month> Apr. </month> <year> 1990). </year>
Reference-contexts: In the next section we describe optimizations that try to hide T copy and T transit by overlapping communication with computation. 3.1.1 Message Vectorization Message vectorization is a loop-based optimization fundamental to the Fortran D compilation process <ref> [5, 19] </ref>. It uses the results of data dependence analysis [1, 35] to extract communication from within loops, combining element messages per loop iteration with one vectorized message preceding the loop. <p> In these cases dynamic data decomposition may be used to temporarily change the ownership of data during program execution, exposing parallelism by internalizing cross-processor dependences <ref> [5] </ref>. For instance, dynamic data decomposition can internalize the computation wavefront in both phases of ADI integration, allowing processors to execute in parallel without communication [33]. However, dynamic data decomposition is only applicable when there are full dimensions of parallelism available in the computation. <p> As previously stated, most optimizations in this paper have been discovered by other researchers. Message vectorization was first developed by Gerndt [19] and Balasundaram et al. <ref> [5] </ref>. Most distributed-memory compilers implicitly perform message vectorization by extracting messages out of user-specified parallel regions, usually performing message coalescing and aggregation as well. <p> Preliminary experiences show it can produce highly optimized code for stencil programs that is competitive with hand-optimized code [46]. Ongoing work to provide environmental support for automatic data decomposition and static performance estimation will also enhance the usefulness of the Fortran D compiler <ref> [5, 6, 25] </ref>. 10 Acknowledgements The authors wish to thank Joel Saltz, Robert Schnabel and Robert Weaver for several enlightening discussions. We are also grateful to the ParaScope research group for providing the software infrastructure for the Fortran D compiler. 34
Reference: [6] <author> Balasundaram, V., Fox, G., Kennedy, K., and Kremer, U. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (Williamsburg, </booktitle> <address> VA, </address> <month> Apr. </month> <year> 1991). </year>
Reference-contexts: More accurate analytical models can be developed, but may be hindered by unpredictable system discontinuities. For instance, communication cost increases abruptly past 100 bytes on the iPSC/860 [7]. The Fortran D compiler will employ a flexible and precise approach using training sets to estimate communication and computation costs <ref> [6, 25, 30] </ref>. Accurate static estimates of communication and computation are also needed by the compiler to calculate block sizes for coarse-grain pipelining. 6 Scalability The scalability of an optimization describes whether its effectiveness increases, decreases, or remains constant in proportion to some characteristic. <p> Preliminary experiences show it can produce highly optimized code for stencil programs that is competitive with hand-optimized code [46]. Ongoing work to provide environmental support for automatic data decomposition and static performance estimation will also enhance the usefulness of the Fortran D compiler <ref> [5, 6, 25] </ref>. 10 Acknowledgements The authors wish to thank Joel Saltz, Robert Schnabel and Robert Weaver for several enlightening discussions. We are also grateful to the ParaScope research group for providing the software infrastructure for the Fortran D compiler. 34
Reference: [7] <author> Bokhari, S. </author> <title> Complete exchange on the iPSC-860. </title> <type> ICASE Report 91-4, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: For most MIMD distributed-memory machines, the cost to send the first byte is significantly higher than the cost for additional bytes. For instance, the Intel iPSC/860 requires approximately 95 sec to send one byte versus .4 sec for each additional byte <ref> [7] </ref>. The following optimizations seek to reduce T start by eliminating messages, reducing the total number of messages sent. <p> However, when communication takes place between groups of processors in regular patterns, message overhead can be reduced by utilizing fast collective communication routines instead of generating individual messages <ref> [7, 36] </ref>. These routines can significantly reduce the number of messages required for global communications without affecting parallelism. For instance, computing the global sum of an array on N processors can be reduced from O (N 2 ) to O (NlogN) messages. <p> Fortunately this is relatively true for the small block sizes that are selected. More accurate analytical models can be developed, but may be hindered by unpredictable system discontinuities. For instance, communication cost increases abruptly past 100 bytes on the iPSC/860 <ref> [7] </ref>. The Fortran D compiler will employ a flexible and precise approach using training sets to estimate communication and computation costs [6, 25, 30].
Reference: [8] <author> Brandes, T. </author> <title> Efficient data parallel programming without explicit message passing for distributed memory multiprocessors. </title> <type> Internal Report AHR-92-4, </type> <institution> High Performance Computing Center, GMD, </institution> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: The Fortran D compiler is similar to other owner computes systems such as Adapt [38], Adaptor <ref> [8] </ref>, Aspar [29], Forge90 [3], P 3 C [18], and Vienna Fortran [12], but performs more compile-time optimization and requires less run-time support. As previously stated, most optimizations in this paper have been discovered by other researchers. Message vectorization was first developed by Gerndt [19] and Balasundaram et al. [5].
Reference: [9] <author> Bromley, M., Heller, S., McNerney, T., and Steele, Jr., G. </author> <title> Fortran at ten gigaflops: The Connection Machine convolution compiler. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation (Toronto, </booktitle> <address> Canada, </address> <month> June </month> <year> 1991). </year>
Reference-contexts: These kernels, shown in Figure 6, contain stencil computations and reductions, techniques commonly used by scientific programmers to solve partial differential equations (PDEs) <ref> [9, 17] </ref>. Details of how optimizations were applied are discussed elsewhere [28, 46]. Most optimizations were performed by hand, simulating algorithms we later implemented in the compiler. <p> else insert buffered messages endif else ffl must be pipelined computation flg select efficient granularity for pipelining apply strip-mining & loop iterchange vectorize, coalesce, and aggregate messages insert buffered messages endif if insufficient storage is available then apply storage optimizations endif enddo (Oxygen [45]), or parallel array operations (CM Fortran <ref> [9] </ref>, Paragon [13]). Other compilers identify parallelism and vectorize messages automatically using the single assignment semantics of their high-level functional languages (Crystal [36], Id Nouveau [42]). <p> The CM Fortran compiler extracts communications from array operations and handles reductions expressed as array intrinsics. Bromley et al. develop optimization techniques for CM-2 stencils that avoid unnecessary intra-processor data motion, insert communication only for nonlocal data, and improve register usage, resulting in significant improvements in execution speed <ref> [9] </ref>. Except for optimizations to improve register reuse, these techniques appear to be subsumed by standard Fortran D optimizations. Burns et al. developed techniques for guiding the use of unbuffered messages on the Alliant CAMPUS/800 using data dependence information [10].
Reference: [10] <author> Burns, C., Kuhn, R., and Werme, E. </author> <title> Low copy message passing on the Alliant CAMPUS/800. </title> <booktitle> In Proceedings of Supercomputing '92 (Minneapolis, </booktitle> <address> MN, </address> <month> Nov. </month> <year> 1992). </year>
Reference-contexts: Except for optimizations to improve register reuse, these techniques appear to be subsumed by standard Fortran D optimizations. Burns et al. developed techniques for guiding the use of unbuffered messages on the Alliant CAMPUS/800 using data dependence information <ref> [10] </ref>. They show that unbuffered messages improve overall performance for a collection of hand-parallelized scientific programs. Olander & Schnabel show that Dino programs can be significantly improved through iteration reordering and pipelin-ing [41]. Their studies validate the effectiveness of selected compiler optimizations for complete programs.
Reference: [11] <author> Callahan, D., and Kennedy, K. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <booktitle> Journal of Supercomputing 2 (October 1988), </booktitle> <pages> 151-169. </pages>
Reference-contexts: The first step is partitioning the data and computation across processors. The second is introducing communication to maintain the semantics of the program. The Fortran D compiler partitions computation across processors using the owner computes rule|where each processor only computes values of data it owns <ref> [11, 42, 50] </ref>. <p> The compiler will decide at each point which optimizations are actually worth performing. 8 Related Work The Fortran D compiler is a second-generation distributed-memory compiler that emphasizes compile-time analysis and optimization instead of language extensions and run-time support. Unlike earlier systems such as Callahan & Kennedy <ref> [11] </ref> and Superb [19, 50], it applies analysis and optimization up front before code generation, rather than first inserting guards and element-wise messages then optimizing via program transformations and partial evaluation.
Reference: [12] <author> Chapman, B., Mehrotra, P., and Zima, H. </author> <title> Programming in Vienna Fortran. </title> <booktitle> Scientific Programming 1, 1 (Fall 1992), </booktitle> <pages> 31-50. </pages>
Reference-contexts: The Fortran D compiler is similar to other owner computes systems such as Adapt [38], Adaptor [8], Aspar [29], Forge90 [3], P 3 C [18], and Vienna Fortran <ref> [12] </ref>, but performs more compile-time optimization and requires less run-time support. As previously stated, most optimizations in this paper have been discovered by other researchers. Message vectorization was first developed by Gerndt [19] and Balasundaram et al. [5].
Reference: [13] <author> Chase, C., Cheung, A., Reeves, A., and Smith, M. </author> <title> Paragon: A parallel programming environment for scientific applications using communication structures. </title> <journal> Journal of Parallel and 35 Distributed Computing 16, </journal> <month> 2 (Oct. </month> <year> 1992), </year> <pages> 79-91. </pages>
Reference-contexts: buffered messages endif else ffl must be pipelined computation flg select efficient granularity for pipelining apply strip-mining & loop iterchange vectorize, coalesce, and aggregate messages insert buffered messages endif if insufficient storage is available then apply storage optimizations endif enddo (Oxygen [45]), or parallel array operations (CM Fortran [9], Paragon <ref> [13] </ref>). Other compilers identify parallelism and vectorize messages automatically using the single assignment semantics of their high-level functional languages (Crystal [36], Id Nouveau [42]).
Reference: [14] <author> Chatterjee, S., Blelloch, G., and Zagha, M. </author> <title> Scan primitives for vector computers. </title> <booktitle> In Proceedings of Supercomputing '90 (New York, </booktitle> <address> NY, </address> <month> Nov. </month> <year> 1990). </year>
Reference-contexts: Scans are similar but perform parallel-prefix operations instead. A sum scan would return the sums of all the prefixes of an array. Scans are used to solve a number of computations in scientific codes, including linear recurrences and tridiagonal systems <ref> [14, 34] </ref>. If a reduction or scan accesses data in a manner that sequentializes computation across processors, it may be parallelized by relaxing the owner computes rule and providing methods to combine partial results. Reductions are parallelized by allowing each processor to compute in parallel, later accumulating the partial results.
Reference: [15] <author> Choudhary, A., Fox, G., Hiranandani, S., Kennedy, K., Koelbel, C., Ranka, S., and Tseng, C. </author> <title> Compiling Fortran 77D and 90D for MIMD distributed-memory machines. </title> <booktitle> In Frontiers '92: The 4th Symposium on the Frontiers of Massively Parallel Computation (McLean, </booktitle> <address> VA, </address> <month> Oct. </month> <year> 1992). </year>
Reference-contexts: The design and implementation of the prototype compiler are described elsewhere. They include algorithms to partition data and computation [26]; internal representations, program analysis, pipelining and code generation algorithms [27]; interprocedural compilation issues [21]; and integrating partitioning and scalarization optimizations for Fortran 90D <ref> [15] </ref>. In order to be successful, the Fortran D compiler must be able to generate highly efficient programs. This paper presents our efforts to develop compiler optimizations and evaluate their impact on performance. <p> In comparison, the efficiency of pipelining improves with larger problem sizes. On the other hand, dynamic data decomposition becomes more effective as the number of processors increases, because it may be performed concurrently <ref> [15] </ref>. It may also eliminate communication in other phases of the computation not part of the pipelined computation. As a result, determining when dynamic data decomposition should be performed remains an open research area. 5.2.2 Fine-grain Pipelining Consider the simple examples presented in Figure 8.
Reference: [16] <author> Fox, G., Hiranandani, S., Kennedy, K., Koelbel, C., Kremer, U., Tseng, C., and Wu, M. </author> <title> Fortran D language specification. </title> <type> Tech. Rep. </type> <institution> TR90-141, Dept. of Computer Science, Rice University, </institution> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: Each dimension of the decomposition is distributed in a block, cyclic, or block-cyclic manner. Irregular and dynamic data decomposition are also supported. The complete language is described in detail elsewhere <ref> [16, 46] </ref>. There are two major steps in compiling Fortran D for MIMD distributed-memory machines. The first step is partitioning the data and computation across processors. The second is introducing communication to maintain the semantics of the program.
Reference: [17] <author> Fox, G., Johnson, M., Lyzenga, G., Otto, S., Salmon, J., and Walker, D. </author> <title> Solving Problems on Concurrent Processors, </title> <journal> vol. </journal> <volume> 1. </volume> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: After the copy is performed, both buffered and unbuffered messages can overlap communication and computation. 3.3 Exploiting Parallelism 3.3.1 Partitioning Computation Most scientific applications are completely parallel in either a synchronous or loosely synchronous manner <ref> [17] </ref>. In these computations all processors execute SPMD programs in a loose lockstep, alternating between phases of local computation and synchronous global communication. These problems achieve good load balance because all processors are utilized during the computation phase. For instance, Jacobi and Red-black SOR are loosely synchronous computations. <p> These kernels, shown in Figure 6, contain stencil computations and reductions, techniques commonly used by scientific programmers to solve partial differential equations (PDEs) <ref> [9, 17] </ref>. Details of how optimizations were applied are discussed elsewhere [28, 46]. Most optimizations were performed by hand, simulating algorithms we later implemented in the compiler.
Reference: [18] <author> Gabber, E., Averbuch, A., and Yehudai, A. </author> <title> Experience with a portable parallelizing Pascal compiler. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing (St. </booktitle> <address> Charles, IL, </address> <month> Aug. </month> <year> 1991). </year>
Reference-contexts: The Fortran D compiler is similar to other owner computes systems such as Adapt [38], Adaptor [8], Aspar [29], Forge90 [3], P 3 C <ref> [18] </ref>, and Vienna Fortran [12], but performs more compile-time optimization and requires less run-time support. As previously stated, most optimizations in this paper have been discovered by other researchers. Message vectorization was first developed by Gerndt [19] and Balasundaram et al. [5]. <p> Id Nouveau applies message vectorization, message pipelining, and recognizes reductions. Kali performs iteration reordering for individual parallel loops and suggests using array transpo sition as a form of dynamic data decomposition for ADI integration [33]. Aspar [29] and P 3 C <ref> [18] </ref> perform simple dependence analysis to extract communication from parallel loops, and rely on portable run-time libraries to support collective communication and reductions.
Reference: [19] <author> Gerndt, M. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice & Experience 2, </journal> <month> 3 (Sept. </month> <year> 1990), </year> <pages> 171-193. </pages>
Reference-contexts: In the next section we describe optimizations that try to hide T copy and T transit by overlapping communication with computation. 3.1.1 Message Vectorization Message vectorization is a loop-based optimization fundamental to the Fortran D compilation process <ref> [5, 19] </ref>. It uses the results of data dependence analysis [1, 35] to extract communication from within loops, combining element messages per loop iteration with one vectorized message preceding the loop. <p> The compiler will decide at each point which optimizations are actually worth performing. 8 Related Work The Fortran D compiler is a second-generation distributed-memory compiler that emphasizes compile-time analysis and optimization instead of language extensions and run-time support. Unlike earlier systems such as Callahan & Kennedy [11] and Superb <ref> [19, 50] </ref>, it applies analysis and optimization up front before code generation, rather than first inserting guards and element-wise messages then optimizing via program transformations and partial evaluation. <p> As previously stated, most optimizations in this paper have been discovered by other researchers. Message vectorization was first developed by Gerndt <ref> [19] </ref> and Balasundaram et al. [5]. Most distributed-memory compilers implicitly perform message vectorization by extracting messages out of user-specified parallel regions, usually performing message coalescing and aggregation as well.
Reference: [20] <author> Gupta, M., and Banerjee, P. </author> <title> A methodology for high-level synthesis of communication for multicomputers. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing (Washington, </booktitle> <address> DC, </address> <month> July </month> <year> 1992). </year>
Reference-contexts: Other optimizations are less common. Crystal pioneered the strategy of identifying collective communication opportunities through syntactic analysis. Gupta & Banerjee describe a sophisticated framework for analyzing collective communication and estimating pipelining costs in Parafrase-2 <ref> [20] </ref>. Id Nouveau applies message vectorization, message pipelining, and recognizes reductions. Kali performs iteration reordering for individual parallel loops and suggests using array transpo sition as a form of dynamic data decomposition for ADI integration [33].
Reference: [21] <author> Hall, M. W., Hiranandani, S., Kennedy, K., and Tseng, C. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '92 (Minneapolis, </booktitle> <address> MN, </address> <month> Nov. </month> <year> 1992). </year>
Reference-contexts: The design and implementation of the prototype compiler are described elsewhere. They include algorithms to partition data and computation [26]; internal representations, program analysis, pipelining and code generation algorithms [27]; interprocedural compilation issues <ref> [21] </ref>; and integrating partitioning and scalarization optimizations for Fortran 90D [15]. In order to be successful, the Fortran D compiler must be able to generate highly efficient programs. This paper presents our efforts to develop compiler optimizations and evaluate their impact on performance.
Reference: [22] <author> Hatcher, P., and Quinn, M. </author> <title> Data-parallel Programming on MIMD Computers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Most distributed-memory compilers implicitly perform message vectorization by extracting messages out of user-specified parallel regions, usually performing message coalescing and aggregation as well. Compilers that take this approach require the user to specify parallel loops (Al [47], Arf [48], Kali [33]), parallel functions (C* [43], Dataparallel C <ref> [22] </ref>, Dino [44]), parallel code blocks 31 partition data across processors partition computation using owner computes rule detect and parallelize reductions & scans compute cross-processor loops for each loop nest L do if L is fully parallel (i.e., no cross-processor loops) then vectorize, coalesce, and aggregate messages select and insert collective
Reference: [23] <author> Havlak, P., and Kennedy, K. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 2, </journal> <month> 3 (July </month> <year> 1991), </year> <pages> 350-360. </pages>
Reference-contexts: Message vectorization first calculates commlevel, the level of the deepest loop-carried true dependence or loop enclosing a loop-independent true dependence. This level determines the outermost loop where element messages resulting from the same array reference may be legally combined. Regular section descriptors (RSDs), compact representations of rectangular array sections <ref> [23] </ref>, are built for vectorized nonlocal accesses and stored at the loop at commlevel.
Reference: [24] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Tech. Rep. CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, TX, </institution> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Fortran D has also contributed to the development of High Performance Fortran (HPF), a new proposed Fortran standard <ref> [24] </ref>. Our goal with Fortran D is to provide a simple yet efficient machine-independent parallel programming model by shifting the burden of machine-dependent optimization to the compiler. To evaluate the Fortran D programming model, we are implementing a prototype compiler for MIMD distributed-memory machines.
Reference: [25] <author> Hiranandani, S., Kennedy, K., Koelbel, C., Kremer, U., and Tseng, C. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Languages and Compilers for Parallel Computing, Fourth International Workshop (Santa Clara, </booktitle> <address> CA, </address> <month> Aug. </month> <year> 1991), </year> <editor> U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, Eds., </editor> <publisher> Springer-Verlag. </publisher> <pages> 36 </pages>
Reference-contexts: More accurate analytical models can be developed, but may be hindered by unpredictable system discontinuities. For instance, communication cost increases abruptly past 100 bytes on the iPSC/860 [7]. The Fortran D compiler will employ a flexible and precise approach using training sets to estimate communication and computation costs <ref> [6, 25, 30] </ref>. Accurate static estimates of communication and computation are also needed by the compiler to calculate block sizes for coarse-grain pipelining. 6 Scalability The scalability of an optimization describes whether its effectiveness increases, decreases, or remains constant in proportion to some characteristic. <p> Preliminary experiences show it can produce highly optimized code for stencil programs that is competitive with hand-optimized code [46]. Ongoing work to provide environmental support for automatic data decomposition and static performance estimation will also enhance the usefulness of the Fortran D compiler <ref> [5, 6, 25] </ref>. 10 Acknowledgements The authors wish to thank Joel Saltz, Robert Schnabel and Robert Weaver for several enlightening discussions. We are also grateful to the ParaScope research group for providing the software infrastructure for the Fortran D compiler. 34
Reference: [26] <author> Hiranandani, S., Kennedy, K., and Tseng, C. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. In Languages, Compilers, and Run-Time Environments for Distributed Memory Machines, </title> <editor> J. Saltz and P. Mehrotra, Eds. </editor> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: To evaluate the Fortran D programming model, we are implementing a prototype compiler for MIMD distributed-memory machines. The design and implementation of the prototype compiler are described elsewhere. They include algorithms to partition data and computation <ref> [26] </ref>; internal representations, program analysis, pipelining and code generation algorithms [27]; interprocedural compilation issues [21]; and integrating partitioning and scalarization optimizations for Fortran 90D [15]. In order to be successful, the Fortran D compiler must be able to generate highly efficient programs.
Reference: [27] <author> Hiranandani, S., Kennedy, K., and Tseng, C. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM 35, </journal> <month> 8 (Aug. </month> <year> 1992), </year> <pages> 66-80. </pages>
Reference-contexts: To evaluate the Fortran D programming model, we are implementing a prototype compiler for MIMD distributed-memory machines. The design and implementation of the prototype compiler are described elsewhere. They include algorithms to partition data and computation [26]; internal representations, program analysis, pipelining and code generation algorithms <ref> [27] </ref>; interprocedural compilation issues [21]; and integrating partitioning and scalarization optimizations for Fortran 90D [15]. In order to be successful, the Fortran D compiler must be able to generate highly efficient programs. This paper presents our efforts to develop compiler optimizations and evaluate their impact on performance. <p> The Fortran D compiler can distinguish pipelined computations from fully par Time P 4 P 3 P 2 P 1 P 1 P 2 P 3 P 4 14 allel computations by discovering cross-processor loops|loops that cause computation wavefronts to sweep across processor boundaries <ref> [27] </ref>. 3.3.6 Fine-grain Pipelining We present two optimizations to exploit pipeline parallelism. The key observation we make is that the amount of pipeline parallelism is determined by the amount of computation C enclosed by the send and recv primitives inserted around cross-processor loops.
Reference: [28] <author> Hiranandani, S., Kennedy, K., and Tseng, C. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing (Washington, </booktitle> <address> DC, </address> <month> July </month> <year> 1992). </year>
Reference-contexts: In these examples, we assume arrays are distributed block-wise onto a four processor machine, and that the two dimensional arrays in Figure 4 are distributed by columns (i.e., block-wise in the second dimension). Detailed examples of these compiler optimizations are discussed elsewhere <ref> [28, 46] </ref>. We note that several compiler optimizations utilize program transformations such as loop interchange, fusion, distribution, alignment, and strip-mining. Their legality is determined in exactly the same manner as for shared-memory parallelizing compilers [1, 32, 35], since transformations must preserve the meaning of the original program. <p> These kernels, shown in Figure 6, contain stencil computations and reductions, techniques commonly used by scientific programmers to solve partial differential equations (PDEs) [9, 17]. Details of how optimizations were applied are discussed elsewhere <ref> [28, 46] </ref>. Most optimizations were performed by hand, simulating algorithms we later implemented in the compiler. At this point the prototype Fortran D compiler performs message vectorization, coalescing, aggregation, vector message pipelining, unbuffered messages, reductions, broadcasts, fine-grain pipelining, and coarse-grain pipelining (with a preset granularity).
Reference: [29] <author> Ikudome, K., Fox, G., Kolawa, A., and Flower, J. </author> <title> An automatic and symbolic paralleliza-tion system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference (Charleston, </booktitle> <address> SC, </address> <month> Apr. </month> <year> 1990). </year>
Reference-contexts: The Fortran D compiler is similar to other owner computes systems such as Adapt [38], Adaptor [8], Aspar <ref> [29] </ref>, Forge90 [3], P 3 C [18], and Vienna Fortran [12], but performs more compile-time optimization and requires less run-time support. As previously stated, most optimizations in this paper have been discovered by other researchers. Message vectorization was first developed by Gerndt [19] and Balasundaram et al. [5]. <p> Id Nouveau applies message vectorization, message pipelining, and recognizes reductions. Kali performs iteration reordering for individual parallel loops and suggests using array transpo sition as a form of dynamic data decomposition for ADI integration [33]. Aspar <ref> [29] </ref> and P 3 C [18] perform simple dependence analysis to extract communication from parallel loops, and rely on portable run-time libraries to support collective communication and reductions.
Reference: [30] <author> Kennedy, K., McIntosh, N., and M c Kinley, K. S. </author> <title> Static performance estimation in a par-allelizing compiler. </title> <type> Tech. Rep. </type> <institution> TR91-174, Dept. of Computer Science, Rice University, </institution> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: More accurate analytical models can be developed, but may be hindered by unpredictable system discontinuities. For instance, communication cost increases abruptly past 100 bytes on the iPSC/860 [7]. The Fortran D compiler will employ a flexible and precise approach using training sets to estimate communication and computation costs <ref> [6, 25, 30] </ref>. Accurate static estimates of communication and computation are also needed by the compiler to calculate block sizes for coarse-grain pipelining. 6 Scalability The scalability of an optimization describes whether its effectiveness increases, decreases, or remains constant in proportion to some characteristic.
Reference: [31] <author> Kennedy, K., and M c Kinley, K. S. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing (Washington, </booktitle> <address> DC, </address> <month> July </month> <year> 1992). </year>
Reference-contexts: Because loop interchange may affect usage of registers and cache, the Fortran D compiler restores the original innermost loop after strip-mining. More sophisticated approaches would permute inner loops into memory order to exploit data locality for the local computation <ref> [31] </ref>. 15 3.4 Reducing Storage 3.4.1 Partitioning Data Most optimizations increase the amount of temporary storage required by the program. Storage optimizations seek to reduce storage requirements. Compile-time partitioning of the data so that each processor allocates memory only for array sections owned locally is fundamental.
Reference: [32] <author> Kennedy, K., M c Kinley, K. S., and Tseng, C. </author> <title> Analysis and transformation in an interactive parallel programming tool. </title> <journal> Concurrency: Practice & Experience 5, </journal> <month> 7 (Oct. </month> <year> 1993), </year> <pages> 575-602. </pages>
Reference-contexts: Detailed examples of these compiler optimizations are discussed elsewhere [28, 46]. We note that several compiler optimizations utilize program transformations such as loop interchange, fusion, distribution, alignment, and strip-mining. Their legality is determined in exactly the same manner as for shared-memory parallelizing compilers <ref> [1, 32, 35] </ref>, since transformations must preserve the meaning of the original program.
Reference: [33] <author> Koelbel, C., and Mehrotra, P. </author> <title> Programming data parallel algorithms on distributed memory machines using Kali. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing (Cologne, </booktitle> <address> Germany, </address> <month> June </month> <year> 1991). </year>
Reference-contexts: However, a combination of compile-time analysis and run-time processing can be applied to optimize communication. If no loop-carried true dependences are present, inspectors and executors may be created at compile-time during code generation to combine messages at run-time <ref> [33, 39] </ref>. The inspector performs the equivalent of message coalescing and aggregation at run-time. The executor then utilizes collective communication specialized for irregular computations. <p> This optimization is a simple application of the "owner stores" rule proposed by Balasundaram [4]. In particular, it may be desirable to partition loops amongst processors so that each loop iteration is executed on a single processor, such as in Kali <ref> [33] </ref> and Arf [48], in order to eliminate the need for guards and improve load balance. 3.1.7 Replicate Computation The Fortran D compiler considers scalar variables to be replicated. All processors thus perform computations involving assignments to scalar variables. <p> It may be used to hide communication cost by separating loop iterations accessing only local data and placing them between send and recv statements, hiding T transit <ref> [33] </ref>. 11 3.2.4 Unbuffered Messages The Fortran D compiler generally uses buffered messages, such as the csend () and crecv () routines from the Intel NX/2 message-passing library. <p> For instance, dynamic data decomposition can internalize the computation wavefront in both phases of ADI integration, allowing processors to execute in parallel without communication <ref> [33] </ref>. However, dynamic data decomposition is only applicable when there are full dimensions of parallelism available in the computation. For instance, it cannot be used to exploit parallelism for SOR or Livermore 23, because the computation wavefront crosses both spatial dimensions. <p> Most distributed-memory compilers implicitly perform message vectorization by extracting messages out of user-specified parallel regions, usually performing message coalescing and aggregation as well. Compilers that take this approach require the user to specify parallel loops (Al [47], Arf [48], Kali <ref> [33] </ref>), parallel functions (C* [43], Dataparallel C [22], Dino [44]), parallel code blocks 31 partition data across processors partition computation using owner computes rule detect and parallelize reductions & scans compute cross-processor loops for each loop nest L do if L is fully parallel (i.e., no cross-processor loops) then vectorize, coalesce, <p> Id Nouveau applies message vectorization, message pipelining, and recognizes reductions. Kali performs iteration reordering for individual parallel loops and suggests using array transpo sition as a form of dynamic data decomposition for ADI integration <ref> [33] </ref>. Aspar [29] and P 3 C [18] perform simple dependence analysis to extract communication from parallel loops, and rely on portable run-time libraries to support collective communication and reductions. <p> The optimizations described in this paper have been targeted towards regular computation on dense matrices. The same principles apply, but in a different form, for irregular or adaptive computations on sparse matrices. Compilers such as Arf [48] and Kali <ref> [33] </ref> support irregular computations by creating inspectors and executors for each loop nest to detect and combine messages for nonlocal accesses at run-time [39]. This approach may be viewed as a sophisticated version of message vectorization, coalescing, and aggregation.
Reference: [34] <author> Kogge, P., and Stone, H. </author> <title> A parallel algorithm for the efficient solution of a general class of recurrence equations. </title> <journal> IEEE Transactions on Computers C-22, </journal> <month> 8 (Aug. </month> <year> 1973), </year> <pages> 786-793. </pages>
Reference-contexts: Scans are similar but perform parallel-prefix operations instead. A sum scan would return the sums of all the prefixes of an array. Scans are used to solve a number of computations in scientific codes, including linear recurrences and tridiagonal systems <ref> [14, 34] </ref>. If a reduction or scan accesses data in a manner that sequentializes computation across processors, it may be parallelized by relaxing the owner computes rule and providing methods to combine partial results. Reductions are parallelized by allowing each processor to compute in parallel, later accumulating the partial results.
Reference: [35] <author> Kuck, D., Kuhn, R., Padua, D., Leasure, B., and Wolfe, M. J. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages (Williamsburg, </booktitle> <address> VA, </address> <month> Jan. </month> <year> 1981). </year>
Reference-contexts: Detailed examples of these compiler optimizations are discussed elsewhere [28, 46]. We note that several compiler optimizations utilize program transformations such as loop interchange, fusion, distribution, alignment, and strip-mining. Their legality is determined in exactly the same manner as for shared-memory parallelizing compilers <ref> [1, 32, 35] </ref>, since transformations must preserve the meaning of the original program. <p> In the next section we describe optimizations that try to hide T copy and T transit by overlapping communication with computation. 3.1.1 Message Vectorization Message vectorization is a loop-based optimization fundamental to the Fortran D compilation process [5, 19]. It uses the results of data dependence analysis <ref> [1, 35] </ref> to extract communication from within loops, combining element messages per loop iteration with one vectorized message preceding the loop. Message vectorization first calculates commlevel, the level of the deepest loop-carried true dependence or loop enclosing a loop-independent true dependence.
Reference: [36] <author> Li, J., and Chen, M. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 2, </journal> <month> 3 (July </month> <year> 1991), </year> <pages> 361-376. </pages>
Reference-contexts: However, when communication takes place between groups of processors in regular patterns, message overhead can be reduced by utilizing fast collective communication routines instead of generating individual messages <ref> [7, 36] </ref>. These routines can significantly reduce the number of messages required for global communications without affecting parallelism. For instance, computing the global sum of an array on N processors can be reduced from O (N 2 ) to O (NlogN) messages. <p> Other compilers identify parallelism and vectorize messages automatically using the single assignment semantics of their high-level functional languages (Crystal <ref> [36] </ref>, Id Nouveau [42]). In comparison, compil ers that explicitly use data dependence information (such as the Fortran D compiler) can extract communication even out of sequential regions such as those found in ADI or SOR. Other optimizations are less common.
Reference: [37] <author> McMahon, F. </author> <title> The Livermore Fortran Kernels: A computer test of the numerical performance range. </title> <type> Tech. Rep. </type> <institution> UCRL-53745, Lawrence Livermore National Laboratory, </institution> <year> 1986. </year>
Reference-contexts: This reduces the buffer space required by a factor of n=B at the expense of additional messages. 4 Empirical Performance Evaluation To evaluate the usefulness of each compiler optimization, we applied them where appropriate to a small selection of scientific program kernels adapted from the Livermore Kernels <ref> [37] </ref> and finite-difference algorithms. These kernels, shown in Figure 6, contain stencil computations and reductions, techniques commonly used by scientific programmers to solve partial differential equations (PDEs) [9, 17]. Details of how optimizations were applied are discussed elsewhere [28, 46].
Reference: [38] <author> Merlin, J. </author> <title> ADAPTing Fortran-90 array programs for distributed memory architectures. </title> <booktitle> In First International Conference of the Austrian Center for Parallel Computation (Salzburg, </booktitle> <address> Austria, </address> <month> Sept. </month> <year> 1991). </year> <month> 37 </month>
Reference-contexts: The Fortran D compiler is similar to other owner computes systems such as Adapt <ref> [38] </ref>, Adaptor [8], Aspar [29], Forge90 [3], P 3 C [18], and Vienna Fortran [12], but performs more compile-time optimization and requires less run-time support. As previously stated, most optimizations in this paper have been discovered by other researchers.
Reference: [39] <author> Mirchandaney, R., Saltz, J., Smith, R., Nicol, D., and Crowley, K. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing (St. </booktitle> <address> Malo, France, </address> <month> July </month> <year> 1988). </year>
Reference-contexts: However, a combination of compile-time analysis and run-time processing can be applied to optimize communication. If no loop-carried true dependences are present, inspectors and executors may be created at compile-time during code generation to combine messages at run-time <ref> [33, 39] </ref>. The inspector performs the equivalent of message coalescing and aggregation at run-time. The executor then utilizes collective communication specialized for irregular computations. <p> The same principles apply, but in a different form, for irregular or adaptive computations on sparse matrices. Compilers such as Arf [48] and Kali [33] support irregular computations by creating inspectors and executors for each loop nest to detect and combine messages for nonlocal accesses at run-time <ref> [39] </ref>. This approach may be viewed as a sophisticated version of message vectorization, coalescing, and aggregation. Finally, two groups have analyzed the scalability of optimizations to exploit pipeline parallelism. Naik explores parallelization and optimization of pipelined computations in the context of CFD applications [40].
Reference: [40] <author> Naik, V. </author> <title> Scalability issues for a class of CFD applications. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference (Williamsburg, </booktitle> <address> VA, </address> <month> Apr. </month> <year> 1992). </year>
Reference-contexts: This approach may be viewed as a sophisticated version of message vectorization, coalescing, and aggregation. Finally, two groups have analyzed the scalability of optimizations to exploit pipeline parallelism. Naik explores parallelization and optimization of pipelined computations in the context of CFD applications <ref> [40] </ref>. Results show that in addition to increases in problem size and processor speed, 33 improvements in algorithms and data partitioning are also necessary to yield scalable parallelism.
Reference: [41] <author> Olander, D., and Schnabel, R. </author> <title> Preliminary experience in developing a parallel thin-layer Navier Stokes code and implications for parallel language design. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference (Williamsburg, </booktitle> <address> VA, </address> <month> Apr. </month> <year> 1992). </year>
Reference-contexts: They show that unbuffered messages improve overall performance for a collection of hand-parallelized scientific programs. Olander & Schnabel show that Dino programs can be significantly improved through iteration reordering and pipelin-ing <ref> [41] </ref>. Their studies validate the effectiveness of selected compiler optimizations for complete programs. Ruhl performed studies on a variety of parallel architectures, demonstrating excellent speedups for the Oxygen compiler [45]. The optimizations described in this paper have been targeted towards regular computation on dense matrices.
Reference: [42] <author> Rogers, A., and Pingali, K. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation (Portland, </booktitle> <address> OR, </address> <month> June </month> <year> 1989). </year>
Reference-contexts: The first step is partitioning the data and computation across processors. The second is introducing communication to maintain the semantics of the program. The Fortran D compiler partitions computation across processors using the owner computes rule|where each processor only computes values of data it owns <ref> [11, 42, 50] </ref>. <p> These optimizations can also hide or eliminate T copy , the message copy time, by using unbuffered messages. 3.2.1 Message Pipelining Message pipelining inserts a send for each nonlocal reference as soon as it is defined <ref> [42] </ref>. The recv is placed immediately before the value is used. Any computation performed between the definition and use of the value can then help hide T transit . Unfortunately, message pipelining prevents optimizations such as message vectorization, resulting in significantly greater total communication cost. <p> Other compilers identify parallelism and vectorize messages automatically using the single assignment semantics of their high-level functional languages (Crystal [36], Id Nouveau <ref> [42] </ref>). In comparison, compil ers that explicitly use data dependence information (such as the Fortran D compiler) can extract communication even out of sequential regions such as those found in ADI or SOR. Other optimizations are less common. Crystal pioneered the strategy of identifying collective communication opportunities through syntactic analysis. <p> Rogers & Pingali performed experiments on the Intel iPSC/2 that showed message pipelining is needed to extract parallelism from a cyclically distributed Gauss-Seidel computation <ref> [42] </ref>. Strip-mining the computation by hand to increase the granularity of pipelining also improved performance. However, the Id Nouveau compiler does not distinguish between parallel and pipelined computations. It is thus unable to determine when message pipelining is needed to exploit pipeline parallelism.
Reference: [43] <author> Rose, J., and Steele, Jr., G. </author> <title> C fl : An extended C language for data parallel programming. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing (Santa Clara, </booktitle> <address> CA, </address> <month> May </month> <year> 1987), </year> <editor> L. Kartashev and S. Kartashev, </editor> <publisher> Eds. </publisher>
Reference-contexts: Most distributed-memory compilers implicitly perform message vectorization by extracting messages out of user-specified parallel regions, usually performing message coalescing and aggregation as well. Compilers that take this approach require the user to specify parallel loops (Al [47], Arf [48], Kali [33]), parallel functions (C* <ref> [43] </ref>, Dataparallel C [22], Dino [44]), parallel code blocks 31 partition data across processors partition computation using owner computes rule detect and parallelize reductions & scans compute cross-processor loops for each loop nest L do if L is fully parallel (i.e., no cross-processor loops) then vectorize, coalesce, and aggregate messages select
Reference: [44] <author> Rosing, M., Schnabel, R., and Weaver, R. </author> <title> The DINO parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing 13, </journal> <month> 1 (Sept. </month> <year> 1991), </year> <pages> 30-42. </pages>
Reference-contexts: Most distributed-memory compilers implicitly perform message vectorization by extracting messages out of user-specified parallel regions, usually performing message coalescing and aggregation as well. Compilers that take this approach require the user to specify parallel loops (Al [47], Arf [48], Kali [33]), parallel functions (C* [43], Dataparallel C [22], Dino <ref> [44] </ref>), parallel code blocks 31 partition data across processors partition computation using owner computes rule detect and parallelize reductions & scans compute cross-processor loops for each loop nest L do if L is fully parallel (i.e., no cross-processor loops) then vectorize, coalesce, and aggregate messages select and insert collective communications if
Reference: [45] <author> Ruhl, R. </author> <title> Evaluation of compiler-generated parallel programs on three multicomputers. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing (Washington, </booktitle> <address> DC, </address> <month> July </month> <year> 1992). </year>
Reference-contexts: apply vector message pipelining insert unbuffered messages else insert buffered messages endif else ffl must be pipelined computation flg select efficient granularity for pipelining apply strip-mining & loop iterchange vectorize, coalesce, and aggregate messages insert buffered messages endif if insufficient storage is available then apply storage optimizations endif enddo (Oxygen <ref> [45] </ref>), or parallel array operations (CM Fortran [9], Paragon [13]). Other compilers identify parallelism and vectorize messages automatically using the single assignment semantics of their high-level functional languages (Crystal [36], Id Nouveau [42]). <p> Olander & Schnabel show that Dino programs can be significantly improved through iteration reordering and pipelin-ing [41]. Their studies validate the effectiveness of selected compiler optimizations for complete programs. Ruhl performed studies on a variety of parallel architectures, demonstrating excellent speedups for the Oxygen compiler <ref> [45] </ref>. The optimizations described in this paper have been targeted towards regular computation on dense matrices. The same principles apply, but in a different form, for irregular or adaptive computations on sparse matrices.
Reference: [46] <author> Tseng, C. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Each dimension of the decomposition is distributed in a block, cyclic, or block-cyclic manner. Irregular and dynamic data decomposition are also supported. The complete language is described in detail elsewhere <ref> [16, 46] </ref>. There are two major steps in compiling Fortran D for MIMD distributed-memory machines. The first step is partitioning the data and computation across processors. The second is introducing communication to maintain the semantics of the program. <p> The second is introducing communication to maintain the semantics of the program. The Fortran D compiler partitions computation across processors using the owner computes rule|where each processor only computes values of data it owns [11, 42, 50]. Details of the Fortran D compiler are discussed elsewhere <ref> [46] </ref>: 5 1) Reduce Communication Overhead * Message Vectorization * Message Coalescing * Message Aggregation * Collective Communication * Run-time Processing * Relax Owner Computes Rule * Replicate Computation 2) Hide Communication Overhead * Message Pipelining * Vector Message Pipelining * Iteration Reordering * Unbuffered Messages 3) Exploit Parallelism * Partition <p> In these examples, we assume arrays are distributed block-wise onto a four processor machine, and that the two dimensional arrays in Figure 4 are distributed by columns (i.e., block-wise in the second dimension). Detailed examples of these compiler optimizations are discussed elsewhere <ref> [28, 46] </ref>. We note that several compiler optimizations utilize program transformations such as loop interchange, fusion, distribution, alignment, and strip-mining. Their legality is determined in exactly the same manner as for shared-memory parallelizing compilers [1, 32, 35], since transformations must preserve the meaning of the original program. <p> These kernels, shown in Figure 6, contain stencil computations and reductions, techniques commonly used by scientific programmers to solve partial differential equations (PDEs) [9, 17]. Details of how optimizations were applied are discussed elsewhere <ref> [28, 46] </ref>. Most optimizations were performed by hand, simulating algorithms we later implemented in the compiler. At this point the prototype Fortran D compiler performs message vectorization, coalescing, aggregation, vector message pipelining, unbuffered messages, reductions, broadcasts, fine-grain pipelining, and coarse-grain pipelining (with a preset granularity). <p> We have incorporated many of the optimizations described in this paper in the prototype Fortran D compiler. Preliminary experiences show it can produce highly optimized code for stencil programs that is competitive with hand-optimized code <ref> [46] </ref>. Ongoing work to provide environmental support for automatic data decomposition and static performance estimation will also enhance the usefulness of the Fortran D compiler [5, 6, 25]. 10 Acknowledgements The authors wish to thank Joel Saltz, Robert Schnabel and Robert Weaver for several enlightening discussions.
Reference: [47] <author> Tseng, P.-S. </author> <title> A parallelizing compiler for distributed memory parallel computers. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation (White Plains, </booktitle> <address> NY, </address> <month> June </month> <year> 1990). </year>
Reference-contexts: Message vectorization was first developed by Gerndt [19] and Balasundaram et al. [5]. Most distributed-memory compilers implicitly perform message vectorization by extracting messages out of user-specified parallel regions, usually performing message coalescing and aggregation as well. Compilers that take this approach require the user to specify parallel loops (Al <ref> [47] </ref>, Arf [48], Kali [33]), parallel functions (C* [43], Dataparallel C [22], Dino [44]), parallel code blocks 31 partition data across processors partition computation using owner computes rule detect and parallelize reductions & scans compute cross-processor loops for each loop nest L do if L is fully parallel (i.e., no cross-processor
Reference: [48] <author> Wu, J., Saltz, J., Hiranandani, S., and Berryman, H. </author> <title> Runtime compilation methods for mul-ticomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing (St. </booktitle> <address> Charles, IL, </address> <month> Aug. </month> <year> 1991). </year>
Reference-contexts: This optimization is a simple application of the "owner stores" rule proposed by Balasundaram [4]. In particular, it may be desirable to partition loops amongst processors so that each loop iteration is executed on a single processor, such as in Kali [33] and Arf <ref> [48] </ref>, in order to eliminate the need for guards and improve load balance. 3.1.7 Replicate Computation The Fortran D compiler considers scalar variables to be replicated. All processors thus perform computations involving assignments to scalar variables. <p> Most distributed-memory compilers implicitly perform message vectorization by extracting messages out of user-specified parallel regions, usually performing message coalescing and aggregation as well. Compilers that take this approach require the user to specify parallel loops (Al [47], Arf <ref> [48] </ref>, Kali [33]), parallel functions (C* [43], Dataparallel C [22], Dino [44]), parallel code blocks 31 partition data across processors partition computation using owner computes rule detect and parallelize reductions & scans compute cross-processor loops for each loop nest L do if L is fully parallel (i.e., no cross-processor loops) then <p> The optimizations described in this paper have been targeted towards regular computation on dense matrices. The same principles apply, but in a different form, for irregular or adaptive computations on sparse matrices. Compilers such as Arf <ref> [48] </ref> and Kali [33] support irregular computations by creating inspectors and executors for each loop nest to detect and combine messages for nonlocal accesses at run-time [39]. This approach may be viewed as a sophisticated version of message vectorization, coalescing, and aggregation.
Reference: [49] <author> Yeung, D., and Agarwal, A. </author> <title> Experience with fine-grain synchronization in MIMD machines for preconditioned conjugate gradient. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (San Diego, </booktitle> <address> CA, </address> <month> May </month> <year> 1993). </year>
Reference-contexts: Yeung & Agarwal examine the scalability of fine and coarse-grain pipelining for pipelined computations in the context of the forward and backward substitution stages of a preconditioned conjugate gradient solver <ref> [49] </ref>. They find that coarse-grain pipelining is not scalable if the granularity of pipelining is proportional to the problem size divided by the number of processors.

References-found: 49

