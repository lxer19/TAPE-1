URL: http://www.cs.umn.edu/Users/dept/users/du/advanced.networking/atmperf.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/du/advanced.networking/
Root-URL: http://www.cs.umn.edu
Title: Distributed Network Computing over Local ATM Networks  of ATM LANs: Implementations and Experiences with an Emerging  
Author: Mengjou Lin, Jenwei Hsieh, Joseph P. Thomas and David H.C. Du James A. MacDonald 
Keyword: Distributed Network Computing, Asynchronous Transfer Mode (ATM), Application Pro gramming Interface, Performance Measurement.  
Note: To appear in IEEE Journal on Selected Areas in Communications Special Issue  Technology (Early '95).  
Affiliation: Computer Science Department Minnesota Supercomputer Center Inc. University of Minnesota Minneapolis, Minnesota  Army High Performance Computing Research Center University of Minnesota  
Abstract: Communication between processors has long been the bottleneck of distributed network computing. However, recent progress in switch-based high-speed Local Area Networks (LANs) may be changing this situation. Asynchronous Transfer Mode (ATM) is one of the most widely-accepted and emerging high-speed network standards which can potentially satisfy the communication needs of distributed network computing. In this paper, we investigate distributed network computing over local ATM networks. We first study the performance characteristics involving end-to-end communication in an environment that includes several types of workstations interconnected via a Fore Systems' ASX-100 ATM Switch. We then compare the communication performance of four different Application Programming Interfaces (APIs). The four APIs were Fore Systems ATM API, BSD socket programming interface, Sun's Remote Procedure Call (RPC), and the Parallel Virtual Machine (PVM) message passing library. Each API represents distributed programming at a different communication protocol layer. We evaluated two popular distributed applications, parallel Matrix Multiplication and parallel Partial Differential Equations, over the local ATM network. The experimental results show that network computing is very promising over local ATM networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Goscinski. </author> <title> Distributed Operating Systems: The Logical Design. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Sun RPC/XDR uses a remote procedure call to invoke remote services. Basically, both message passing and RPC can provide similar communication capability. For a comprehensive comparison of message passing and RPC mechanism, refer to Chapter 5.3.14 of Goscinsk's book <ref> [1] </ref>. Each API discussed in this paper represents a distributed programming environment over a different communication layer in the protocol hierarchy. A distributed program using an API in a lower layer, like Fore's API, can take advantage of better communication performance.
Reference: [2] <author> S.G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: One of the parallel algorithms which uses a 2-d mesh topology is briefly described below. For a detailed description, refer to <ref> [2] </ref>. One class of the PDE problems can be represented by a uniform mesh of n + 1 horizontal and n + 1 vertical lines over the unit square as shown in Figure 14 (a), where n is a positive number. The intersections of these lines are called mesh points.
Reference: [3] <author> ANSI X3T9.3. </author> <title> Fiber Channel Physical and Signaling Interface (FC-PH), </title> <address> 4.2 edition, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: The OC-3 (155.52 Mbits/sec) and OC-12 (622.08 Mbits/sec) have been designated as the customer access rates in B-ISDN. The Block Coded transmission sub-layer is based on the physical layer technology developed for Fiber Channel Standard <ref> [3] </ref>. Most of the functions of this sub-layer involve generating and processing the overhead and ATM cell header.
Reference: [4] <author> A.S. Tanenbaum, R. van Renesse. </author> <title> Distributed Operating System. </title> <journal> Computing Surveys, </journal> <volume> 17(4), </volume> <year> 1985. </year>
Reference-contexts: It was first introduced in the 4.2BSD Unix operating system. RPC is a popular client/server paradigm for IPC between processes in different computers across a network. It is widely used as a communication mechanism in distributed systems, such as the V kernel [11] and the Amoeba distributed operating system <ref> [4] </ref>. PVM was developed at Oak Ridge National Laboratory. It is a software package that allows a heterogeneous network of parallel, serial, and vector computers to appear as a single computational resource. PVM was adopted as the communication primitives for the Cray T3D massive parallel supercomputer [8].
Reference: [5] <author> ATM Forum. </author> <title> ATM User-Network Interface Specification, </title> <address> 3.0 edition, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: ATM could become a good candidate for providing the kind of communication capability needed by distributed network computing. The multicasting capability of ATM can be utilized by PVM multicasting subroutines. The ATM signaling protocol Q.93B <ref> [5] </ref> or Fore's SPANS could be used to maintain application specific topologies such as 2-D mesh, hypercube, or tree. PVM over Fore's API has the potential to become an appropriate API for running distributed programs over a local ATM network.
Reference: [6] <author> Bellcore. </author> <title> Network Compatible ATM for Local Network Applications, </title> <address> 1.0 edition, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Although ATM network technology was originally developed for public telecommunication networks over metropolitan and wide areas, recent interest has focused on applying this technology to interconnect computing resources within a local area <ref> [6] </ref>. In this paper, we investigate the feasibility of performing network computing over ATM in local area. 2.2 Network Environment The experiments described were performed over a variety of host and network architectures. Different host architectures tested include the Sparc 1+, Sparc 2, and 4/690, all from Sun Microsystems.
Reference: [7] <author> J.D. Cavanaugh and T.J. Salo. </author> <title> Internetworking with ATM WANs. In Advances in Local and Metropolitan Area Networks. </title> <publisher> William Stallings, IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: ATM, proposed by international standards organizations, uses small 53 bytes cells to transmit data in multiples of OC-1 rates (51.84 Mbits/sec). Popular data transfer rates for ATM are OC-3 (155.52 Mbits/sec) and OC-12 (622.08 Mbits/sec) <ref> [7, 10] </ref>. ATM was initially developed as a standard for wide-area broadband networks. The fact that Local ATM networks are appearing in advance of long-haul ATM networks, makes ATM an attractive alternative to traditional LANs. ATM networks are characterized by their switch-based network architecture. <p> Finally, we present a description of the four different APIs. 4 2.1 Asynchronous Transfer Mode ATM is a method for transporting information by using fixed-length cells (53 octets) <ref> [7, 10] </ref>. It is based on virtual circuit-oriented packet (or cell) switching. A cell includes a 5-byte header and a 48-byte information payload. A connection identifier, which consists of virtual circuit identifier (VCI) and virtual path identifier (VPI), is placed in each cell header.
Reference: [8] <author> Cray Research. </author> <title> PVM and HeNCE Programmer's Manual, </title> <note> SR-2501 version 3.0. </note>
Reference-contexts: PVM was developed at Oak Ridge National Laboratory. It is a software package that allows a heterogeneous network of parallel, serial, and vector computers to appear as a single computational resource. PVM was adopted as the communication primitives for the Cray T3D massive parallel supercomputer <ref> [8] </ref>. For interprocess communication, any of the four APIs can be chosen. However, the performance of the application may be affected by the decision made. Each API may also represent communicating in a different protocol layer.
Reference: [9] <author> J. Crowcroft, I. Wakeman, Z. Wang, and D. </author> <title> Sirovica. </title> <journal> Is Layering Harmful? IEEE Network, </journal> <volume> 6(1) </volume> <pages> 20-24, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: The results are presented in Figure 8 (b). This is a known TCP timing abnormality which has been reported by Crowcroft <ref> [9] </ref> and others. We have observed the same timing abnormality in our local ATM network. However, after enabling TCP NODELAY, this timing abnormality disappears. We performed experiments with the same TCP echo program, over both FDDI and Ethernet for message sizes less than 16 Kbytes.
Reference: [10] <author> C.T. Lea. </author> <title> What Should Be the Goal for ATM. </title> <journal> IEEE Network, </journal> <month> September </month> <year> 1992. </year>
Reference-contexts: ATM, proposed by international standards organizations, uses small 53 bytes cells to transmit data in multiples of OC-1 rates (51.84 Mbits/sec). Popular data transfer rates for ATM are OC-3 (155.52 Mbits/sec) and OC-12 (622.08 Mbits/sec) <ref> [7, 10] </ref>. ATM was initially developed as a standard for wide-area broadband networks. The fact that Local ATM networks are appearing in advance of long-haul ATM networks, makes ATM an attractive alternative to traditional LANs. ATM networks are characterized by their switch-based network architecture. <p> Finally, we present a description of the four different APIs. 4 2.1 Asynchronous Transfer Mode ATM is a method for transporting information by using fixed-length cells (53 octets) <ref> [7, 10] </ref>. It is based on virtual circuit-oriented packet (or cell) switching. A cell includes a 5-byte header and a 48-byte information payload. A connection identifier, which consists of virtual circuit identifier (VCI) and virtual path identifier (VPI), is placed in each cell header.
Reference: [11] <author> D.R. Cheriton. </author> <title> The V Distributed System. </title> <journal> Communications of the ACM, </journal> <volume> 31(3), </volume> <year> 1988. </year>
Reference-contexts: It was first introduced in the 4.2BSD Unix operating system. RPC is a popular client/server paradigm for IPC between processes in different computers across a network. It is widely used as a communication mechanism in distributed systems, such as the V kernel <ref> [11] </ref> and the Amoeba distributed operating system [4]. PVM was developed at Oak Ridge National Laboratory. It is a software package that allows a heterogeneous network of parallel, serial, and vector computers to appear as a single computational resource.
Reference: [12] <author> E. Biagioni, E. Coope, and R. Samsom. </author> <title> Designing a Practical ATM LAN. </title> <journal> IEEE Network, </journal> <pages> pages 32-39, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: The host adapters included a Series-100 and Series-200 interface for the Sun SBus. The physical media for both the Series-100 and Series-200 adapters is the 100 Mbits/sec TAXI interface (FDDI fiber plant and signal encoding scheme). The local area switch was a Fore ASX-100. The SBA-100 interface (Figure 2) <ref> [12] </ref> was Fore's first-generation host adapter and interfaced to the host at the cell level. The Series-100 adapter is capable of performing the ATM cell header CRC generation/verification and the AAL 3/4 CRC generation/verification. <p> The i960 uses local memory to manage pointers to packets, and uses DMA (Direct Memory Access) to move cells out of and into host memory. Cells are never stored in adapter memory. The ASX-100 local ATM switch (Figure 4) <ref> [12] </ref> is based on a 2.4 Gbits/sec (gigabit per second) switch fabric and a RISC control processor. The switch supports four network modules with each module supporting up to 622 Mbits/sec.
Reference: [13] <institution> Fore Systems, Inc. </institution> <note> ForeRunner SBA-200 ATM SBus Adapter User's Manual, </note> <year> 1993. </year>
Reference-contexts: In our test environment, several workstations were interconnected via a Fore's ASX-100 ATM Switch. The details of this local ATM environment will be discussed in Section 2. There are at least four possible APIs available: * Fore's API <ref> [13] </ref>, * BSD socket programming interface [20, 21], * Sun's Remote Procedure Call (RPC) [21], and * the Parallel Virtual Machine (PVM) message passing library [14].
Reference: [14] <author> G.A. Geist, A.Beguelin, J.Dongarra, W.Jiang, R.Manchek, V.Sunderam. </author> <title> PVM 3 User's Guide and Reference Manual. </title> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The details of this local ATM environment will be discussed in Section 2. There are at least four possible APIs available: * Fore's API [13], * BSD socket programming interface [20, 21], * Sun's Remote Procedure Call (RPC) [21], and * the Parallel Virtual Machine (PVM) message passing library <ref> [14] </ref>. Fore's API provides several capabilities which are not normally available in other APIs, such as guaranteed bandwidth reservation, selection of different ATM Adaptation Layers (AAL), multicasting, and other ATM specific features. The BSD socket interface provides facilities for Interprocess Communication (IPC).
Reference: [15] <author> Z. Haas. </author> <title> A Protocol Structure for High-Speed Communication over Broadband ISDN. </title> <journal> IEEE Networks, </journal> <volume> 5(1) </volume> <pages> 64-70, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Several recent papers have found a significant portion of communication overhead occurring due to these interactions <ref> [15, 22, 17] </ref>. Our primary goal was to study the performance tradeoffs of choosing different APIs in a local ATM environment. In our test environment, several workstations were interconnected via a Fore's ASX-100 ATM Switch. The details of this local ATM environment will be discussed in Section 2.
Reference: [16] <author> Intel Insight i960. </author> <title> Fore Systems 200-Series ATM Adapters for High-Speed Workstations and PCs, </title> <booktitle> 3rd Quarter 1993. </booktitle>
Reference-contexts: These tasks are CPU intensive and thus the network throughput becomes bounded by the CPU performance of the host. In contrast, the Series-200 host adapter (Figure 3) <ref> [16] </ref> is Fore's second generation interface and uses an Intel i960 as an onboard processor. The i960 takes over most of the AAL and cell related tasks including the SAR functions for AAL 3/4 and AAL 5, and cell multiplexing.
Reference: [17] <author> M. Zitterbart. </author> <title> High-Speed Transport Components. </title> <journal> IEEE Network, </journal> <volume> 5(1) </volume> <pages> 54-63, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Several recent papers have found a significant portion of communication overhead occurring due to these interactions <ref> [15, 22, 17] </ref>. Our primary goal was to study the performance tradeoffs of choosing different APIs in a local ATM environment. In our test environment, several workstations were interconnected via a Fore's ASX-100 ATM Switch. The details of this local ATM environment will be discussed in Section 2.
Reference: [18] <author> Minnesota Supercomputer Center Inc. </author> <title> An Overview of the MAGIC Project, </title> <year> 1993. </year>
Reference-contexts: Where possible, each architecture was tested with a variety of network interfaces, and in the case of ATM, with and without the presence of a local area switch. The ATM environment was provided by the MAGIC (Multidimensional Applications and Gigabit Internetwork Consortium) <ref> [18] </ref> project. Fore Systems, Inc. host adapters and local area switches were used. The host adapters included a Series-100 and Series-200 interface for the Sun SBus. The physical media for both the Series-100 and Series-200 adapters is the 100 Mbits/sec TAXI interface (FDDI fiber plant and signal encoding scheme).
Reference: [19] <author> Request for Comment 1483. </author> <title> Multiprotocol Encapsulation over ATM Adaptation Layer 5, </title> <month> July </month> <year> 1993. </year>
Reference-contexts: Five service class are being standardized to provide these services. The CCITT recommendation for ATM specifies five AAL protocols <ref> [19] </ref> which are listed as follows: 1. AAL Type 1 provides constant bit rate services, such as traditional voice transmis sion. 2. AAL Type 2 transports variable bit rate video and audio information, and keeps the timing relation between source and destination. 3.
Reference: [20] <author> S.Le*er, M.McKusick, M.Karels, J.Quarterman. </author> <title> The Design and Implementation of the 4.3BSD Unix Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: In our test environment, several workstations were interconnected via a Fore's ASX-100 ATM Switch. The details of this local ATM environment will be discussed in Section 2. There are at least four possible APIs available: * Fore's API [13], * BSD socket programming interface <ref> [20, 21] </ref>, * Sun's Remote Procedure Call (RPC) [21], and * the Parallel Virtual Machine (PVM) message passing library [14]. <p> Once connected, the descriptors for the sockets can be read from or written to by user processes similar to regular file operations. The transparency of sockets allows the kernel to redirect the output of one process to the input of another process residing on another machine <ref> [20] </ref>. All sockets are typed according to their communications semantics. Socket types are defined by the subset of properties a socket supports. These properties are in-order delivery of data, unduplicated delivery of data, reliable delivery of data, preservation of message boundaries, support for out-of-band messages, and connection-oriented communication.
Reference: [21] <author> Sun Microsystems. </author> <title> Network Programming Guide, </title> <month> March </month> <year> 1990. </year>
Reference-contexts: In our test environment, several workstations were interconnected via a Fore's ASX-100 ATM Switch. The details of this local ATM environment will be discussed in Section 2. There are at least four possible APIs available: * Fore's API [13], * BSD socket programming interface <ref> [20, 21] </ref>, * Sun's Remote Procedure Call (RPC) [21], and * the Parallel Virtual Machine (PVM) message passing library [14]. <p> The details of this local ATM environment will be discussed in Section 2. There are at least four possible APIs available: * Fore's API [13], * BSD socket programming interface [20, 21], * Sun's Remote Procedure Call (RPC) <ref> [21] </ref>, and * the Parallel Virtual Machine (PVM) message passing library [14]. Fore's API provides several capabilities which are not normally available in other APIs, such as guaranteed bandwidth reservation, selection of different ATM Adaptation Layers (AAL), multicasting, and other ATM specific features. <p> BSD socket supports the Unix domain, the Internet domain, and the NS domain. In our environment, we are limited to use the Internet domain for communication over local ATM networks. In the Internet domain, stream sockets and datagram sockets use TCP/IP and UDP/IP <ref> [21] </ref> as the underlying protocols, respectively. <p> BSD Sockets are a well-accepted interprocess communication protocol. However, it does not provide machine transparent access. Sun Microsystems has claimed that their RPC library will use the Transport Layer Interface (TLI) API <ref> [21] </ref> instead of sockets in future operating system releases. Sun RPC/XDR is suitable for client/server applications such as remote database access, remote file access, and transaction processing.
Reference: [22] <author> T. von Eicken, D.E. Culler, S.C. Goldstein, and K.E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In The 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Several recent papers have found a significant portion of communication overhead occurring due to these interactions <ref> [15, 22, 17] </ref>. Our primary goal was to study the performance tradeoffs of choosing different APIs in a local ATM environment. In our test environment, several workstations were interconnected via a Fore's ASX-100 ATM Switch. The details of this local ATM environment will be discussed in Section 2.
Reference: [23] <author> C.A. Thekkath and H.M. Levy. </author> <title> Limits to Low-Latency Communication on High-Speed Networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Several related works have studied point-to-point ATM connections in this regard. Wolman [25] discussed the performance of TCP/IP and showed that a point-to-point ATM connection has approximately a twofold performance increase over Ethernet LANs. Thekkath <ref> [23] </ref> showed that the overhead of a lightweight RPC is only 170 sec. However, both measurements did not include the overhead incurred within ATM switches.
Reference: [24] <author> C.A. Thekkath, H.M. Levy, and E.D. Lazowska. </author> <title> Efficient Support for Multicomputing on ATM Networks. </title> <type> Technical Report TR 93-04-03, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> April </month> <year> 1993. </year> <month> 32 </month>
Reference-contexts: Thekkath [23] showed that the overhead of a lightweight RPC is only 170 sec. However, both measurements did not include the overhead incurred within ATM switches. Thekkath <ref> [24] </ref> also implemented a distributed shared memory system over ATM and reported that it took 37 sec to perform a remote write operation through an ATM switch. This result is "raw performance". It is not clear what the end-to-end, application-to-application overhead will be. This paper is organized as follows.
Reference: [25] <author> Alec Wolman, Geoff Voelker, and Chandramohan A. Thekkath. </author> <title> Latency Analysis of TCP on an ATM Network. </title> <type> Technical Report TR 93-03-03, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> March </month> <year> 1993. </year> <month> 33 </month>
Reference-contexts: Our goal is to provide general programming guidelines that programmers can use in developing distributed applications for local ATM networks. Several related works have studied point-to-point ATM connections in this regard. Wolman <ref> [25] </ref> discussed the performance of TCP/IP and showed that a point-to-point ATM connection has approximately a twofold performance increase over Ethernet LANs. Thekkath [23] showed that the overhead of a lightweight RPC is only 170 sec. However, both measurements did not include the overhead incurred within ATM switches.
References-found: 25

