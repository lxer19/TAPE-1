URL: ftp://ftp.ics.uci.edu/pub/machine-learning-papers/publications/Ali-TAI-94-MultipleModels.ps.Z
Refering-URL: http://www.ics.uci.edu/AI/ML/MLAbstracts.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: On Learning Multiple Descriptions of a Concept  
Author: Kamal Ali, Clifford Brunk and Michael Pazzani 
Keyword: Academic Track. Area: Machine Learning  
Address: Irvine, CA, 92717  
Affiliation: Department of Information and Computer Science, University of California,  
Email: fali,brunk,pazzanig@ics.uci.edu  
Phone: 714-725-3491, 714-856-5888  
Date: June 30, 1994  
Abstract: In sparse data environments, greater classification accuracy can be achieved by learning several concept descriptions of the data and combining their classifications. Stochastic search is a general tool which can be used to generate many good concept descriptions (rule sets) for each class in the data. Bayesian probability theory offers an optimal strategy for combining classifications of the individual concept descriptions, and here we use an approximation of that theory. This strategy is most useful when additional data is difficult to obtain and every increase in classification accuracy is important. The primary result of this paper is that multiple concept descriptions are particularly helpful in "flat" hypothesis spaces in which there are many equally good ways to grow a rule, each having similar gain. Another result is experimental evidence that learning multiple rule sets yields more accurate classifications than learning multiple rules for some domains. To demonstrate these behaviors, we learn multiple concept descriptions by adapting HYDRA, a noise-tolerant relational learning algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ali K. and Pazzani M. </author> <year> (1992). </year> <title> Reducing the small disjuncts problem by learning probabilistic concept descriptions (Technical Report ICS-92-111). </title> <address> Irvine CA: </address> <institution> University of California at Irvine, Department of Information & Computer Sciences. </institution> <note> To appear in Petsche T., </note> <editor> Judd S. and Hanson S. (ed.s), </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Vol. </volume> <pages> 3. </pages> <address> Cambridge, Massachusetts. </address> <publisher> MIT Press. </publisher>
Reference: <author> Ali K. and Pazzani M. </author> <year> (1993). </year> <title> HYDRA: A Noise-tolerant Relational Concept Learning Algorithm. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence. </booktitle> <address> Cham-bery, France: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: When there are no literals that cause an increase in ls-content or if the clause no longer covers any negative examples, HYDRA moves onto the next clause. HYDRA learns more accurate rule sets using ls-content than it does using information gain <ref> (Ali and Pazzani, 1993) </ref>. After all the clauses have been learned, HYDRA forms an estimate of the likelihood ratio ls ij associated with clause ij of concept i (this time, it uses the entire training data, not just the examples uncovered by previous clauses).
Reference: <author> Buntine W. </author> <year> (1990). </year> <title> A Theory of Learning Classification Rules. </title> <type> Doctoral dissertation. </type> <institution> School of Computing Science, University of Technology, </institution> <address> Sydney, Australia. </address>
Reference-contexts: The document block classification domain requires learning a concept description containing recursion. Although previous work has shown that learning multiple concept descriptions increases accuracy for other kinds of concept descriptions <ref> (e.g. decision trees, Buntine 1990) </ref> there has been no prior work in learning multiple concept descriptions for relational problems or in learning multiple concept descriptions, each one being a rule set. <p> We will compare HYDRA-MM using ls-content with HYDRA-MM using the Bayes gain function which we now define. As Bayesian theory prescribes voting among concept descriptions with weight proportional to the posterior probability of the concept description, it makes sense to search for concept descriptions with high posterior probability <ref> (Buntine, 1990) </ref>. This implies that the function pr 2 (equation 2) should be used as a literal selection metric for learning clauses.
Reference: <author> Dolsak B. and Muggleton S. </author> <year> (1991). </year> <title> The application of inductive logic programming to finite element mesh design. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning. </booktitle> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In order to learn these concepts, we use the relational learning algorithm HYDRA (Ali & Pazzani, 1993; Ali & Pazzani, 1992). We present results on the following relational problems: predicting finite-element mesh structure <ref> (Dolsak and Muggleton, 1991) </ref>, learning the King-Rook-King concept (Muggleton et al., 1989), classifying a document block (Esposito et al., 1993) and predicting whether a person is required to make payments on their student loan (Pazzani and Brunk, 1991). The document block classification domain requires learning a concept description containing recursion.
Reference: <author> Duda R., Gaschnig J. and Hart P. </author> <year> (1979). </year> <title> Model design in the Prospector consultant system for mineral exploration. </title> <editor> In D. Michie (ed.), </editor> <booktitle> Expert systems in the micro-electronic age. </booktitle> <address> Edinburgh, England. </address> <publisher> Edinburgh University Press. </publisher>
Reference-contexts: Evidence combination methods previously used include naive Bayesian combination (Kononenko, Smyth) and combination according to the posterior probability of the concept description (Buntine). In this paper we will compare these two methods. We will use the odds 3 form of naive Bayesian combination because HYDRA associates likelihood odds ratios <ref> (Duda et al., 1979) </ref> with rules rather 3 The odds of a proposition with probability p are p=(1 p). 6 than class probability vectors. 3 Theoretical background The Bayesian approach postulates that assuming we want to maximize some utility function u that depends on some action and an incompletely known state <p> For rule set M ij of Class i , let ls ij denote the LS of the clause with the highest LS value (taken only over clauses satisfied 12 by the current test example). We combine these LS values using the odds form of Bayes rule <ref> (Duda et al., 1979) </ref> in which the prior odds of a class are multiplied by the LS's of all satisfied clauses.
Reference: <author> Dzeroski S. and Bratko I. </author> <year> (1992). </year> <title> Handling noise in Inductive Logic Programming. </title> <booktitle> In Proceedings of the International Workshop on Inductive Logic Programming. </booktitle> <address> Tokyo, Japan. </address> <publisher> ICOT. </publisher>
Reference-contexts: Bayes-p is a simplification in which concept descriptions vote using just the accuracy of a clause. These 2 variants used the same learned description. We tested HYDRA-MM with the ls-content and the Bayesian gain metrics on the following well-known relational domains: finite element mesh structure with 5 objects <ref> (Dzeroski and Bratko, 1992) </ref>, King-Rook-King illegality (Muggleton et al., 1989), document block classification (Esposito et al., 1993) and student loan prediction (Brunk and Pazzani, 1991). In addition, we also tested it on 14 the following domains traditionally regarded as "attribute-value domains".
Reference: <author> Esposito F., Malerba D., Semeraro G. and Pazzani M. </author> <year> (1993). </year> <title> A machine learning approach to document understanding. </title> <booktitle> In Proceedings of the Second international workshop on multistrategy learning. </booktitle> <address> Harpers Ferry, WV. </address>
Reference-contexts: In order to learn these concepts, we use the relational learning algorithm HYDRA (Ali & Pazzani, 1993; Ali & Pazzani, 1992). We present results on the following relational problems: predicting finite-element mesh structure (Dolsak and Muggleton, 1991), learning the King-Rook-King concept (Muggleton et al., 1989), classifying a document block <ref> (Esposito et al., 1993) </ref> and predicting whether a person is required to make payments on their student loan (Pazzani and Brunk, 1991). The document block classification domain requires learning a concept description containing recursion. <p> These 2 variants used the same learned description. We tested HYDRA-MM with the ls-content and the Bayesian gain metrics on the following well-known relational domains: finite element mesh structure with 5 objects (Dzeroski and Bratko, 1992), King-Rook-King illegality (Muggleton et al., 1989), document block classification <ref> (Esposito et al., 1993) </ref> and student loan prediction (Brunk and Pazzani, 1991). In addition, we also tested it on 14 the following domains traditionally regarded as "attribute-value domains".
Reference: <author> Fayyad U. and Irani K. </author> <year> (1992). </year> <title> The Attribute Selection Problem in Decision Tree Generation. </title> <booktitle> In AAAI-92: Proceedings, Tenth National Conference on Artificial Intelligence, </booktitle> <address> Menlo Park: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: In the first, 1 positive and 0 negative examples go to the left branch and 10 positive and 2 negative go the right branch (denoted ((1; 0); (10; 2))). According to the C-SEP family of measures <ref> (Fayyad and Irani, 1992) </ref> one should prefer the ((4; 0); (7; 2)) over this one. However, the Bayes gain function gives a gain of 5:8 fi 10 4 to the first split and 5:5 fi 10 4 to the second split.
Reference: <author> Gams M. </author> <year> (1989). </year> <title> New Measurements Highlight the Importance of Redundant Knowledge. </title> <booktitle> In European Working Session on Learning (4th : 1989 : Montpeiller, </booktitle> <address> France). </address> <publisher> Pitman. </publisher>
Reference-contexts: Previous methods for generating multiple concept descriptions include beam search (Smyth et al., 1992), stochastic search (Kononenko et al., 1992; Bun-tine, 1990), user-intervention (Kwok and Carter, 1990) and n-fold cross-validation <ref> (Gams, 1989) </ref>. In the approach of Kwok and Carter, the user examines the best candidate splits for growing a decision tree and can then choose to use some of them to grow separate trees.
Reference: <author> Holte R., Acker L. and Porter B. </author> <year> (1989). </year> <title> Concept Learning and the Problem of Small Disjuncts. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence. </booktitle> <address> Detroit, MI. </address> <publisher> Morgan Kaufmann. </publisher> <editor> 23 Kononenko I. and Kovacic M. </editor> <year> (1992). </year> <title> Learning as Optimization: Stochastic Generation of Multiple Knowledge. </title> <booktitle> In Machine Learning: Proceedings of the Ninth International Workshop. </booktitle> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We also use the King-Rook King-Pawn (KRKP) domain which has been demonstrated to have small disjuncts <ref> (Holte et al., 1989) </ref>. Representation- For the attribute-value domains we created tuples whose length matched the number of attributes. This minimized the need to introduce attributes through literals involving existentially quantified variables a relatively expensive process. For the relational domains, we used the same representation used by previous authors.
Reference: <author> Kwok S. and Carter C. </author> <year> (1990). </year> <title> Multiple decision trees. </title> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <volume> 4, </volume> <pages> 327-335. </pages>
Reference-contexts: Previous methods for generating multiple concept descriptions include beam search (Smyth et al., 1992), stochastic search (Kononenko et al., 1992; Bun-tine, 1990), user-intervention <ref> (Kwok and Carter, 1990) </ref> and n-fold cross-validation (Gams, 1989). In the approach of Kwok and Carter, the user examines the best candidate splits for growing a decision tree and can then choose to use some of them to grow separate trees.
Reference: <author> Lloyd J.W. </author> <year> (1984). </year> <title> Foundations of Logic Programming. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: After learning, the task is to take a novel test example and find which class it belongs to. This framework is a generalization of the work of Quinlan (1990) on learning relational descriptions. In this paper we will refer to first-order Horn clauses <ref> (e.g. Lloyd, 1984) </ref> as rules. A first-order Horn clause such as class-a (X,Y) b (X); c (Y ) consists of a head (class-a (X,Y)) and a body which is a conjunction of literals (b (X); c (Y )) 2 . The arrow signifies logical implication.
Reference: <author> Muggleton S., Bain M., Hayes-Michie J. and Michie D. </author> <year> (1989). </year> <title> An experimental comparison of human and machine-learning formalisms. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning. </booktitle> <address> Ithaca, NY. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In order to learn these concepts, we use the relational learning algorithm HYDRA (Ali & Pazzani, 1993; Ali & Pazzani, 1992). We present results on the following relational problems: predicting finite-element mesh structure (Dolsak and Muggleton, 1991), learning the King-Rook-King concept <ref> (Muggleton et al., 1989) </ref>, classifying a document block (Esposito et al., 1993) and predicting whether a person is required to make payments on their student loan (Pazzani and Brunk, 1991). The document block classification domain requires learning a concept description containing recursion. <p> These 2 variants used the same learned description. We tested HYDRA-MM with the ls-content and the Bayesian gain metrics on the following well-known relational domains: finite element mesh structure with 5 objects (Dzeroski and Bratko, 1992), King-Rook-King illegality <ref> (Muggleton et al., 1989) </ref>, document block classification (Esposito et al., 1993) and student loan prediction (Brunk and Pazzani, 1991). In addition, we also tested it on 14 the following domains traditionally regarded as "attribute-value domains".
Reference: <author> Muggleton S. and Feng C. </author> <year> (1990). </year> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Learning Theory. </booktitle> <address> Tokyo. </address> <publisher> Ohmsha Press. </publisher>
Reference: <author> Muggleton S., Srinivasan A. and Bain M. </author> <year> (1992). </year> <title> Compression, Significance and Accuracy. </title> <booktitle> In Machine Learning: Proceedings of the Ninth International Workshop. </booktitle> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pazzani M. and Brunk C. </author> <year> (1991). </year> <title> Detecting and correcting errors in rule-based expert systems: an integration of empirical and explanation-based learning. </title> <journal> Knowledge Acquisition, </journal> <volume> 3, </volume> <pages> 157-173. </pages>
Reference-contexts: We present results on the following relational problems: predicting finite-element mesh structure (Dolsak and Muggleton, 1991), learning the King-Rook-King concept (Muggleton et al., 1989), classifying a document block (Esposito et al., 1993) and predicting whether a person is required to make payments on their student loan <ref> (Pazzani and Brunk, 1991) </ref>. The document block classification domain requires learning a concept description containing recursion. <p> Examples covered by the clause are removed from the training set and the process continues to learn on the remaining examples, terminating when no more positive examples are left. 5 HYDRA HYDRA is derived from the inductive learning component of FOCL <ref> (Pazzani and Kibler, 1991) </ref> which added to FOIL the ability to learn using intensionally defined prior background knowledge. HYDRA differs from FOCL in three important ways. 9 1. HYDRA learns a set of rules for each class so that each set can compete to classify test examples. 2. <p> We tested HYDRA-MM with the ls-content and the Bayesian gain metrics on the following well-known relational domains: finite element mesh structure with 5 objects (Dzeroski and Bratko, 1992), King-Rook-King illegality (Muggleton et al., 1989), document block classification (Esposito et al., 1993) and student loan prediction <ref> (Brunk and Pazzani, 1991) </ref>. In addition, we also tested it on 14 the following domains traditionally regarded as "attribute-value domains". In the breast cancer recurrence prediction domain and the lymphography domain we added the relation equal-attr (X,Y) which allows attribute-attribute comparisons in addition to attribute-value comparisons.
Reference: <author> Pazzani M. and Kibler D. </author> <year> (1991). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9, 1, </volume> <pages> 57-94. </pages>
Reference-contexts: We present results on the following relational problems: predicting finite-element mesh structure (Dolsak and Muggleton, 1991), learning the King-Rook-King concept (Muggleton et al., 1989), classifying a document block (Esposito et al., 1993) and predicting whether a person is required to make payments on their student loan <ref> (Pazzani and Brunk, 1991) </ref>. The document block classification domain requires learning a concept description containing recursion. <p> Examples covered by the clause are removed from the training set and the process continues to learn on the remaining examples, terminating when no more positive examples are left. 5 HYDRA HYDRA is derived from the inductive learning component of FOCL <ref> (Pazzani and Kibler, 1991) </ref> which added to FOIL the ability to learn using intensionally defined prior background knowledge. HYDRA differs from FOCL in three important ways. 9 1. HYDRA learns a set of rules for each class so that each set can compete to classify test examples. 2. <p> We tested HYDRA-MM with the ls-content and the Bayesian gain metrics on the following well-known relational domains: finite element mesh structure with 5 objects (Dzeroski and Bratko, 1992), King-Rook-King illegality (Muggleton et al., 1989), document block classification (Esposito et al., 1993) and student loan prediction <ref> (Brunk and Pazzani, 1991) </ref>. In addition, we also tested it on 14 the following domains traditionally regarded as "attribute-value domains". In the breast cancer recurrence prediction domain and the lymphography domain we added the relation equal-attr (X,Y) which allows attribute-attribute comparisons in addition to attribute-value comparisons.
Reference: <author> Quinlan R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 3. </pages>
Reference-contexts: Light lines show the coverage of rules learned by the 3 algorithms. lines indicate learned rules that try to approximate the underlying disjuncts. The leftmost figure illustrates what is learned using a separate and conquer technique <ref> (e.g. FOIL, Quinlan, 1990) </ref> which learns an approximation (rule) to the first disjunct and then removes training examples covered by that rule in order to learn subsequent rules. The middle figure illustrates what may be learned by a multiple rules approach told to learn 6 rules. <p> compute the gain of adding a test to a tree (or a literal to a clause) then, one computes the difference between the probability after addition and the probability before addition (using equation 2). 4 Separate and Conquer Strategy HYDRA uses a separate and conquer control strategy based on FOIL <ref> (Quinlan, 1990) </ref> in which a rule is learned and then the training examples covered by that rule are separated from the uncovered examples for which further rules are learned. A rule for a given class such as class-a (V1,V2) is learned by a greedy search strategy. <p> It starts with an empty clause body, which in logic is equivalent to the rule class-a (V1,V2) true and so covers all remaining positive and negative examples. Next, the strategy considers all literals that it can add to the clause body and ranks each by the information gained <ref> (Quinlan, 1990) </ref> by adding that literal. Briefly, the information gain measure favors the literal whose addition to the clause body would cover many positive examples and exclude many negative examples.
Reference: <author> Smyth P. and Goodman R. </author> <year> (1992). </year> <title> Rule Induction Using Information Theory. </title> <editor> In G. Piatetsky-Shapiro (ed.) </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press, MIT Press. </publisher>
Reference-contexts: Note that this meets both of the objectives outlined above: each disjunct is modeled multiple times and attempts to model the small disjunct are less influenced the major disjunct. Previous methods for generating multiple concept descriptions include beam search <ref> (Smyth et al., 1992) </ref>, stochastic search (Kononenko et al., 1992; Bun-tine, 1990), user-intervention (Kwok and Carter, 1990) and n-fold cross-validation (Gams, 1989).
Reference: <author> Towell G., Shavlik J and Noordewier M. </author> <year> (1990). </year> <title> Refinement of Approximate Domain Theories by Knowledge-Based Artificial Neural Networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI-90). </booktitle> <address> Boston, MA. </address> <publisher> AAAI. </publisher> <pages> 24 </pages>
Reference-contexts: that use of multiple concept descriptions led to increased accuracy in this domain. 6.4 Comparison to previous work On the promoters domain, as far as we know, the 92.5% accuracy obtained here is higher than for any symbolic method (ID3 or C4.5) and equal to that of standard neural-net back-propagation <ref> (Towell et al., 1990) </ref>. KBANN (Towell et al., 1990) which can take advantage of prior knowledge gets 96%. <p> descriptions led to increased accuracy in this domain. 6.4 Comparison to previous work On the promoters domain, as far as we know, the 92.5% accuracy obtained here is higher than for any symbolic method (ID3 or C4.5) and equal to that of standard neural-net back-propagation <ref> (Towell et al., 1990) </ref>. KBANN (Towell et al., 1990) which can take advantage of prior knowledge gets 96%.
References-found: 20

