URL: http://www.csc.ncsu.edu/eos/users/b/billy/www/Publications/PS_files/Philippe.ps
Refering-URL: http://www.csc.ncsu.edu/eos/users/b/billy/www/MARCA_Models/MM_descriptions/qnatm_desc.html
Root-URL: http://www.csc.ncsu.edu
Title: Numerical Methods in Markov Chain Modelling  
Author: Bernard Philippe Youcef Saad and William J. Stewart 
Keyword: Key words: Markov chain models; Homogeneous linear systems; Direct methods; Successive Overrelaxation; Preconditioned power iterations; Arnoldi's method; GMRES.  
Date: August 9, 1996  
Abstract: This paper describes and compares several methods for computing stationary probability distributions of Markov chains. The main linear algebra problem consists of computing an eigenvector of a sparse, non-symmetric, matrix associated with a known eigenvalue. It can also be cast as a problem of solving a homogeneous, singular linear system. We present several methods based on combinations of Krylov subspace techniques, single vector power iteration/relaxation procedures and acceleration techniques. We compare the performance of these methods on some realistic problems. y IRISA, Rennes, France. Research supported by CNRS (87:N 920070). fl Research Institute for Advanced Computer Science, NASA Ames Research Center. Mof-fett Field CA 94035. Research supported by Cooperative Agreement NCC 2-387 between the National Aeronautics and Space Administration (NASA) and the Universities Space Research Association (USRA). + North Carolina State University, Computer Science. Research supported in part by NSF (DDM 8906248 & INT-8613332), by IRISA, France and by by Cooperative Agreement NCC 2-387 between the National Aeronautics and Space Administration (NASA) and the Universities Space Research Association (USRA). 
Abstract-found: 1
Intro-found: 1
Reference: <author> Arnoldi, W.E. </author> <year> 1951. </year> <title> The principle of minimized iteration in the solution of the matrix eigenvalue problem. </title> <journal> Quart. Appl. Math., </journal> <volume> 9, </volume> <pages> 17-29. </pages>
Reference: <author> Axelsson, O., and V.A. Barker. </author> <year> 1984. </year> <title> Finite Element Solution of Boundary Value Problems. </title> <publisher> Academic Press, </publisher> <address> Orlando, Florida. </address>
Reference: <author> Belady, L.A., and C.J. Kuehner. </author> <year> 1969. </year> <title> Dynamic Space Sharing in Computer Systems. </title> <journal> Comm. ACM, </journal> <volume> 12, 5, </volume> <pages> 282-288. </pages>
Reference-contexts: The parameter q may be represented as the Belady-Kuehner lifetime function, <ref> (Belady and Kuehner, 1969) </ref>, which for a process executing in memory space m is given by q = ff (m) k , where ff depends on the processing speed as well as on program characteristics, and k depends both on program locality and on the memory management strategy.
Reference: <author> Boyer, P., A. Dupuis and A. Khelladi. </author> <year> 1988. </year> <title> A Simple Model for Repeated Calls due to Time-Outs. </title> <editor> CNET, LAA/SLC/EVP, Route de Tregastel, </editor> <address> 22301 Lannion, France. </address>
Reference-contexts: Note that all the methods now work, for all cases, and it becomes extremely difficult to choose a best method. 5.2 Example 2: A Telecommunications Model. The model in the figure below has been used to determine the effect of impatient telephone customers on a computerized telephone exchange <ref> (Boyer et al., 1988) </ref>. In this model a request is made by a customer for service. The customer is prepared to wait for a certain period of time to get a reply.
Reference: <author> Cao, W.L., and W.J. Stewart. </author> <year> 1985. </year> <title> Iterative Aggregation/Disaggregation Techniques for Nearly Uncoupled Markov Chains. </title> <journal> JACM, </journal> <volume> 32, 3, </volume> <pages> 702-719. </pages>
Reference: <author> Chatelin, F. </author> <year> 1984. </year> <title> Spectral Approximation of Linear Operators. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: The condition number for the eigenvector y i involves the reduced resolvent S ( i ), defined as the inverse of the restriction of P i I to fz i g ? , the subspace orthogonal to the left eigenspace associated with i , <ref> (Chatelin, 1984, p. 17) </ref>: c (y i ) = kS ( i )k: Though not apparent from the definition, the condition number for the eigenvector is implicitly related to that for the eigenvalues of P , (Wilkinson, 1965). <p> Moreover, it is easy to show <ref> (Chatelin, 1984) </ref> from the definition that c (y i ) max 1 : A consequence of the above inequality is that a poor separation of the unit eigenvalue from the other eigenvalues will cause poor conditioning for the associated eigenvectors.
Reference: <author> Chatelin, F., and W.L. Miranker. </author> <year> 1984. </year> <title> Aggregation/Disaggregation for the Eigenvalue Problem. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 21, </volume> <pages> 17-47. </pages>
Reference-contexts: The condition number for the eigenvector y i involves the reduced resolvent S ( i ), defined as the inverse of the restriction of P i I to fz i g ? , the subspace orthogonal to the left eigenspace associated with i , <ref> (Chatelin, 1984, p. 17) </ref>: c (y i ) = kS ( i )k: Though not apparent from the definition, the condition number for the eigenvector is implicitly related to that for the eigenvalues of P , (Wilkinson, 1965). <p> Moreover, it is easy to show <ref> (Chatelin, 1984) </ref> from the definition that c (y i ) max 1 : A consequence of the above inequality is that a poor separation of the unit eigenvalue from the other eigenvalues will cause poor conditioning for the associated eigenvectors.
Reference: <author> Courtois, P.J. </author> <year> 1977. </year> <title> Decomposability; Queueing and Computer System Applications. </title> <publisher> Academic Press, </publisher> <address> Orlando, Florida. </address>
Reference-contexts: Unfortunately, in Markov chain models, situations frequently arise in which there is a cluster of eigenvalues very close to the unit eigenvalue. This happens, for example, when the system is nearly completely decomposable, <ref> (Courtois, 1977) </ref>. An added difficulty caused by the poor separation of the unit eigenvalue is a slow rate of convergence. This may render standard matrix iterative methods intolerably slow. In this paper we discuss a variety of methods of computing the stationary probability vector of large Markov chains.
Reference: <author> Cullum, J., and R. Willoughby. </author> <year> 1984. </year> <title> A Lanczos procedure for the modal analysis of very large non-symmetric matrices. </title> <booktitle> In Proceedings of the 23rd Conference on Decision and Control, </booktitle> <address> Las Vegas. </address>
Reference: <author> Duff, I.S., A.M. Erisman, and J.K. Reid. </author> <year> 1986. </year> <title> Direct Methods for Sparse Matrices. </title> <publisher> Claren-don Press, Oxford. </publisher>
Reference-contexts: Since the matrices involved are usually large and sparse, the savings made by such schemes can be considerable. One such sparse storage scheme, and the one used in implementing the iterative procedures in this study, is the compressed sparse row format <ref> (Duff, 1986) </ref>. In this scheme only the non-zero elements, their column indices and an index to the beginning of each row is kept. <p> One of the most commonly used storage schemes for sparse matrices is the row sparse compact storage, sometimes referred to as the a; ja; ia scheme <ref> (Duff, 1986) </ref>. <p> If the non-zero elements lie close to the diagonal, then bandwidth schemes may be effective, <ref> (Duff, 1986) </ref>. When applying direct equation solving methods such as Gaussian elimination, it is usually assumed that the complete set of linear equations has already been derived and that the entire coefficient matrix is stored somewhere in the computer memory, albeit in a compact form. <p> Specifically, we implemented a sparse inverse iteration algorithm called GE (for Gaussian Elimination). This program accepts the transpose of a transition rate matrix which is stored in the usual row compact form <ref> (Duff, 1986) </ref>. It extracts each row of Q T one at a time, expands this row into a vector of length n and performs reductions on it by adding multiples of previously reduced rows.
Reference: <author> Elman, H.C. </author> <year> 1982. </year> <title> Iterative Methods for Large Sparse Nonsymmetric Systems of Linear Equations. </title> <type> PhD thesis, </type> <institution> Yale University, Computer Science Dpt., </institution> <address> New Haven, CT. </address>
Reference-contexts: As a result one can stop as soon as the desired accuracy is achieved. GMRES is theoretically equivalent to GCR <ref> (Elman, 1982) </ref> and to ORTHODIR (Hageman and Young, 1981) but is less costly both in terms of storage and arithmetic (Saad and Schultz, 1986).
Reference: <author> Funderlic, R.E., and R.J. Plemmons. </author> <year> 1984. </year> <title> A Combined Direct-Iterative Method for Certain M-Matrix Linear Systems. </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 5, 1, </volume> <pages> 32-42. </pages>
Reference-contexts: In our experiments, this incomplete factorization is referred to as ILUK. Although the above three ILU factorizations are the only ones we considered, there are other possibilities. For example, some ILU-based methods make use of the symmetric zero structure of a matrix <ref> (Funderlic and Plemmons, 1984) </ref>. In other words, LU is the exact decomposition of the symmetric non-zero portion of the matrix.
Reference: <author> Funderlic, R.E., M. Neumann and R.J. Plemmons. </author> <year> 1982. </year> <title> LU decompositions of Generalized Diagonally Dominant Matrices. </title> <journal> Numer. Math., </journal> <volume> 40, </volume> <pages> 57-69. </pages>
Reference: <author> Grassmann, W.K., M.I. Taskar and D.P.Heyman. </author> <year> 1985. </year> <title> Regenerative Analysis and Steady State Distributions for Markov Chains. </title> <journal> Operations Research, </journal> <volume> 33, 5, </volume> <pages> 1107-1116. </pages>
Reference: <author> Hageman, A.L., and D.M. Young. </author> <year> 1981. </year> <title> Applied Iterative Methods. </title> <publisher> Academic Press, </publisher> <address> New York. </address> <note> 32 Jennings, </note> <author> A., and W.J. Stewart. </author> <year> 1981. </year> <title> A simultaneous iteration algorithm for real matrices. </title> <journal> ACM, Trans. of Math. Software, </journal> <volume> 7, </volume> <pages> 184-198. </pages>
Reference-contexts: As a result one can stop as soon as the desired accuracy is achieved. GMRES is theoretically equivalent to GCR (Elman, 1982) and to ORTHODIR <ref> (Hageman and Young, 1981) </ref> but is less costly both in terms of storage and arithmetic (Saad and Schultz, 1986).
Reference: <author> Kleinrock, L. </author> <year> 1975. </year> <title> Queueing Systems. Volume 1: Theory. </title> <publisher> Wiley Interscience, John Wiley and Sons. </publisher>
Reference-contexts: Often this time is taken to be sufficiently long that all influence of the initial starting state has been erased. The probabilities thus obtained are referred to as the long-run or stationary probabilities. For a homogeneous, continuous time Markov chain (CTMC) with n states, it may be shown <ref> (Kleinrock, 1975, for example) </ref> that, Q = 0; (1) where Q is an n fi n matrix whose elements q ij denote the rate of transition of the chain from state i to state j and , a vector of length n, is the stationary probability vector; i.e. i is the
Reference: <author> Koury, J.R., D.F. McAllister and W.J. Stewart. </author> <year> 1984. </year> <title> Iterative methods for Computing Stationary Distributions of Nearly Completely Decomposable Markov Chains. </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 5, 2, </volume> <pages> 64-186. </pages>
Reference: <author> Meijerink, J.A., and H.A. van der Vorst. </author> <year> 1977. </year> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix. </title> <journal> Math. Comp., </journal> <volume> 31, 137, </volume> <pages> 148-162. </pages>
Reference-contexts: Do i = 1; n If (i; j) 2 N Z (Q) then * Compute s = q ij P min (i;j)1 * If (i j) then l ij = s * If (i&lt;j) then u ij = s=l ii ILU (0) is known to exist for non-singular M -matrices <ref> (Meijerink and van der Vorst, 1977) </ref>. It may also be shown to exist for the matrix Q (and Q T ), by trivially extending the results in Axelsson and Barker (1984), page 42. The second incomplete factorization that we studied is a threshold based scheme.
Reference: <author> Mitra, D., and P. Tsoucas. </author> <year> 1988. </year> <title> Relaxations for the Numerical Solutions of Some Stochastic Problems. Stochastic Models, </title> <type> 4, 3. </type>
Reference-contexts: Some limited information is available on the effect of the ordering of the state space on the convergence of these iterative methods. Examples are available in which Gauss-Seidel works extremely well for one ordering but not at all for an opposing ordering, <ref> (Mitra, 1988) </ref>. In these examples the magnitude of the non-zero elements appears to have little effect on the speed of convergence. It appears that an ordering that in some sense preserves the direction of probability flow works best, (Mitra, 1988). 3.3 SSOR Iteration The Symmetric Successive Overrelaxation method (SSOR) (Young, 1971) <p> well for one ordering but not at all for an opposing ordering, <ref> (Mitra, 1988) </ref>. In these examples the magnitude of the non-zero elements appears to have little effect on the speed of convergence. It appears that an ordering that in some sense preserves the direction of probability flow works best, (Mitra, 1988). 3.3 SSOR Iteration The Symmetric Successive Overrelaxation method (SSOR) (Young, 1971) consists of following a relaxation sweep from top down by a relaxation sweep from bottom up.
Reference: <author> Ng, E. </author> <year> 1989. </year> <title> Comparison of Direct Sparse Solvers. Presented at the SIAM Conference on Sparse Matrices, </title> <address> Glenenden Beach, Oregon. </address>
Reference-contexts: The reason is that GE was designed uniquely for Markov chain problems, while MA28 was designed as a general purpose sparse linear equation solver. In a wide variety of other problems, MA28 has been used very successfully <ref> (Ng, 1989) </ref>.
Reference: <author> Parlett, B.N. ,and D.R. Taylor and Z.S. Liu. </author> <year> 1985. </year> <title> A look-ahead Lanczos algorithm for non-symmetric matrices. </title> <journal> Mathematics of Computation, </journal> <volume> 44, </volume> <pages> 105-124. </pages>
Reference: <author> Perros, H.G. </author> <year> 1990. </year> <title> Private Communication. </title>
Reference: <author> Radicati, G., and Y. Robert. </author> <year> 1987. </year> <title> A Comparison of Conjugate Gradient-like Algorithms for Solving Sparse Nonsymmetric Linear Systems. A Vector and Parallel Implementation. </title> <type> Technical Report Laboratoire TIM3, </type> <institution> Institut National Polytechnique de Grenoble, France. </institution>
Reference-contexts: We did not cover every possible solution method in this paper. Simultaneous iteration methods were not included because our experience over several years indicates that this is inferior to Arnoldi. The Bi-Conjugate Gradient method and the Conjugate gradient squared method, methods which have had much success in other domains, <ref> (Radicati and Robert, 1987) </ref>. were not included. In fact in our initial study, we programmed both these methods but found them unsatisfactory. Both failed to converge when applied to NCD problems and in other cases they performed less well than the methods examined in this report.
Reference: <author> Saad, Y. </author> <year> 1984. </year> <title> Chebyshev acceleration techniques for solving non-symmetric eigenvalue problems. </title> <journal> Mathematics of Computation, </journal> <volume> 42, </volume> <pages> 567-588. </pages>
Reference-contexts: Both failed to converge when applied to NCD problems and in other cases they performed less well than the methods examined in this report. Potentially competitive alternatives include the techniques based on polynomial acceleration of Arnoldi's method such as a hybrid Chebyshev-Arnoldi algorithm <ref> (Saad, 1984) </ref>. As a general rule however, we observe that the preconditioner makes a bigger difference than the acceleration procedure itself. Thus, in many cases there is hardly any difference between the performance of GMRES and PCARN when ILUTH is used with a small tolerance.
Reference: <author> Saad, Y. </author> <year> 1980. </year> <title> Variations on Arnoldi's method for computing eigenelements of large unsym-metric matrices. </title> <journal> Lin. Alg. Appl., </journal> <volume> 34, </volume> <pages> 269-295. </pages>
Reference: <author> Saad, Y., and M.H. Schultz. </author> <year> 1985. </year> <title> Conjugate Gradient-like algorithms for solving nonsymmetric linear systems. </title> <journal> Mathematics of Computation, </journal> <volume> 44, </volume> <pages> 417-424. </pages>
Reference-contexts: spanfAV m g 21 which means that we are solving Affi = r 0 with a projection process with K = spanfr 0 ; Ar 0 ; ; A m1 r 0 g and L = AK = spanfAr 0 ; Ar 0 ; ; A m r 0 g: <ref> (Saad and Schultz, 1985) </ref>. A brief description of the GMRES algorithm follows. Details can be found in Saad and Schultz (1986). Algorithm: GMRES 1. Start: Choose x 0 and a dimension m of the Krylov subspaces. 2.
Reference: <author> Saad, Y., and M.H. Schultz. </author> <year> 1986. </year> <title> GMRES: A generalized minimal residual algorithm for solving non-symmetric linear systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7, </volume> <pages> 856-869. </pages>
Reference-contexts: In this case a common alternative is to define projection methods that attempt to minimize the 2-norm of the residual vector, b Ax, instead of the A-norm of the error. One 17 such method is the GMRES algorithm <ref> (Saad and Schultz, 1986) </ref> which will be described in Section 5.4. Projection methods for the eigenvalue problem Ax = x can be defined similarly. <p> In practice one may compute progressively the QR factorization of the matrix H j for j = 1; : : : m of stage 2 and this allows to obtain the residual norm of the corresponding approximate solution x k at every step without having to compute x k , <ref> (Saad and Schultz, 1986) </ref>. As a result one can stop as soon as the desired accuracy is achieved. GMRES is theoretically equivalent to GCR (Elman, 1982) and to ORTHODIR (Hageman and Young, 1981) but is less costly both in terms of storage and arithmetic (Saad and Schultz, 1986). <p> having to compute x k , <ref> (Saad and Schultz, 1986) </ref>. As a result one can stop as soon as the desired accuracy is achieved. GMRES is theoretically equivalent to GCR (Elman, 1982) and to ORTHODIR (Hageman and Young, 1981) but is less costly both in terms of storage and arithmetic (Saad and Schultz, 1986). Moreover, it can be shown that, in exact arithmetic, the method cannot break down 22 although it may be very slow or even stagnate in cases when the symmetric part of the matrix is not positive definite.
Reference: <author> Schweitzer, P.J., and K.W. Kindle. </author> <year> 1986. </year> <title> An Iterative Aggregation/Disaggregation Algorithm for Solving Linear Equations. </title> <journal> Applied Mathematics and Computation, </journal> <volume> 18, </volume> <pages> 313-353. </pages>
Reference: <author> Sheskin, T.J. </author> <year> 1985. </year> <title> A Markov Chain Partitioning Algorithm for Computing Steady State Probabilities. </title> <journal> Operations Research, </journal> <volume> 33, 1, </volume> <pages> 228-235. </pages> <address> 33 Stewart, G.W. </address> <year> 1973. </year> <title> Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Stewart, G.W. </author> <year> 1991. </year> <title> On the sensitivity of nearly uncoupled Markov chains. In The Numerical Solution of Markov Chains, </title> <editor> William J. Stewart, ed. </editor> <publisher> Marcel Dekker, Inc. </publisher> <address> New York. </address>
Reference-contexts: There is a great number and variety of these methods as evidenced by the different approaches presented at a recent workshop on the numerical solution of Markov chains, <ref> (Stewart, 1991) </ref>.
Reference: <author> Stewart, G.W. </author> <year> 1976. </year> <title> Simultaneous iteration for computing invariant subspaces of non-Hermitian matrices. </title> <journal> Numer. Mat., </journal> <volume> 25, </volume> <pages> 123-136. </pages>
Reference: <author> Stewart, W.J. </author> <year> 1991. </year> <title> The Numerical Solution of Markov Chains. </title> <publisher> Marcel Dekker, Inc. </publisher> <address> New York. </address>
Reference-contexts: There is a great number and variety of these methods as evidenced by the different approaches presented at a recent workshop on the numerical solution of Markov chains, <ref> (Stewart, 1991) </ref>.
Reference: <author> Takahashi, Y. </author> <year> 1975. </year> <title> A Lumping Method for Numerical Calculation of Stationary Distributions of Markov Chains. Research Report Number B-18, </title> <institution> Department of Information Sciences, Tokyo Institute of Technology, </institution> <address> Tokyo, Japan. </address>
Reference: <author> Vantilborgh, H. </author> <year> 1985. </year> <title> Aggregation with an error of O(* 2 ). J.ACM, </title> <booktitle> 32, </booktitle> <pages> 162-190. </pages>
Reference: <author> Young, D.M. </author> <year> 1971. </year> <title> Iterative solution of large linear systems. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: The resulting optimal convergence rate can be a considerably improvement over Gauss-Seidel. The choice of an optimal, or even a reasonable, value for ! has been the subject of much study, especially for problems arising in the numerical solution of partial differential equations <ref> (Young, 1971) </ref>. Although some results have been obtained for certain classes of matrices, little is available at present for arbitrary non-symmetric linear systems. <p> In these examples the magnitude of the non-zero elements appears to have little effect on the speed of convergence. It appears that an ordering that in some sense preserves the direction of probability flow works best, (Mitra, 1988). 3.3 SSOR Iteration The Symmetric Successive Overrelaxation method (SSOR) <ref> (Young, 1971) </ref> consists of following a relaxation sweep from top down by a relaxation sweep from bottom up.

References-found: 35

