URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/douglas-craig/Preprints/pub37tr.ps.gz
Refering-URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/douglas-craig/ccd-preprints.html
Root-URL: http://www.cs.yale.edu
Title: GEMMW: A PORTABLE LEVEL 3 BLAS WINOGRAD VARIANT OF STRASSEN'S MATRIX-MATRIX MULTIPLY ALGORITHM  
Author: CRAIG C. DOUGLAS MICHAEL HEROUX GORDON SLISHMAN AND ROGER M. SMITH 
Keyword: Key words. Level 3 BLAS, matrix multiplication, Winograd's variant of Strassen's algorithm, multilevel algorithms AMS(MOS) subject classifications. Numerical Analysis: Numerical Linear Algebra  
Abstract: Matrix-matrix multiplication is normally computed using one of the BLAS or a reinvention of part of the BLAS. Unfortunately, the BLAS were designed with small matrices in mind. When huge, well conditioned matrices are multiplied together, the BLAS perform like the blahs, even on vector machines. For matrices where the coefficients are well conditioned, Winograd's variant of Strassen's algorithm offers some relief, but is rarely available in a quality form on most computers. We reconsider this method and offer a highly portable solution based on the Level 3 BLAS interface. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aho, J. Hopcroft, and J. Ullman, </author> <title> The Design and Analysis of Computer Algorithms, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: 1. Preliminaries. Matrix-matrix multiplication is a very basic computer oper ation. A very clear description of how to do it can be found in many textbooks, e.g., <ref> [1] </ref>. Suppose we want to multiply two matrices A : M fi K and B : K fi N; where the elements of A and B are real or complex numbers and M , K, and N are natural numbers. <p> In other words, caveat emptor for either class of matrix-matrix multiplication in real codes. Stability discussions are contained in [2] and [5] along with their references. An interesting application to solving linear systems of equations is contained in <ref> [1] </ref> and [3]. <p> Complex Strassen-Winograd. Provided with gemmw is a specialized version of the classical matrix multiplication algorithm for complex matrices. Let P , Q, R, and S be real matrices. A well known trick <ref> [1] </ref> calculates (P + Qi) (R + Si) using the formula [P (R S) + (P Q) S)] + [(P + Q) R P (R S)]i: Note that there are only 3 matrix multiplies instead of the usual 4. When applying this trick to Strassen-Winograd there are two options: 1.
Reference: [2] <author> D. H. Baily, </author> <title> Extra high speed matrix multiplication on the Cray-2, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9 (1988), </volume> <pages> pp. 603-607. </pages>
Reference-contexts: Hence, for matrices A and B, we assume the coefficients are "well conditioned" enough so that both methods get acceptable answers. In other words, caveat emptor for either class of matrix-matrix multiplication in real codes. Stability discussions are contained in <ref> [2] </ref> and [5] along with their references. An interesting application to solving linear systems of equations is contained in [1] and [3].
Reference: [3] <author> D. H. Baily, K. Lee, and H. Simon, </author> <title> Using Strassen's algorithm to accelerate the solution of linear systems, </title> <journal> J. Supercomp., </journal> <volume> 4 (1990), </volume> <pages> pp. 357-371. </pages>
Reference-contexts: In other words, caveat emptor for either class of matrix-matrix multiplication in real codes. Stability discussions are contained in [2] and [5] along with their references. An interesting application to solving linear systems of equations is contained in [1] and <ref> [3] </ref>.
Reference: [4] <author> N. Carriero and D. Gelernter, </author> <title> Linda in context, </title> <journal> Comm. ACM, </journal> <volume> 32 (1989), </volume> <pages> pp. 444-458. </pages>
Reference-contexts: The user stores submatrices of A and B into the parallel data base. In our example code, the Linda system <ref> [4] </ref> was used. The computation continues with a call to the parallel matrix multiplication routine, matmulp, with seven parameters. These 8 include the original sizes (M , K, and N ), the partitioning (M, K, and N ), and the number of processors (p) to use.
Reference: [5] <author> N. J. Higham, </author> <title> Exploiting fast matrix multiplication within the Level 3 BLAS, </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16 (1990), </volume> <pages> pp. 352-368. </pages>
Reference-contexts: Hence, for matrices A and B, we assume the coefficients are "well conditioned" enough so that both methods get acceptable answers. In other words, caveat emptor for either class of matrix-matrix multiplication in real codes. Stability discussions are contained in [2] and <ref> [5] </ref> along with their references. An interesting application to solving linear systems of equations is contained in [1] and [3].
Reference: [6] <author> J. Madsen, </author> <title> Volume 3: UNICOS Math and Scientific Library Reference Manual (SR-2081), </title> <institution> Cray Research, Inc., </institution> <note> version 6.0 ed., </note> <year> 1990. </year>
Reference-contexts: The code is flexible enough that by modifying the macro definitions in one header file, essentially any library can be substituted for the default ones. In this manner, it is trivial to make the code work with the BLAS, Cray Scientific Library <ref> [6] </ref>, ESSL [7], NAG [8], or any other library the user chooses. We used the BLAS distributed with LAPACK. A list of the macros and the routines that are actually called is contained in Table 2. <p> The first option actually runs slightly faster ( 2 3%) than the second, but uses 2.5 times as much storage in the process. Only a hardware manufacturer (e.g., [7] or <ref> [6] </ref>) would ever consider delivering the first approach without testing the second approach first. Note that by using the second option and a Level 3 BLAS routine (e.g., dgemm or sgemm) that requires no extra storage, the storage requirement given in (3) is still valid.
Reference: [7] <author> L. Mason and M. E. Sliva, </author> <title> Engineering and Scientific Subroutine Library: Guide and Reference, </title> <type> Version 2, </type> <institution> IBM Corporation, Kingston, NY, 1.0 ed., </institution> <year> 1992. </year>
Reference-contexts: The code is flexible enough that by modifying the macro definitions in one header file, essentially any library can be substituted for the default ones. In this manner, it is trivial to make the code work with the BLAS, Cray Scientific Library [6], ESSL <ref> [7] </ref>, NAG [8], or any other library the user chooses. We used the BLAS distributed with LAPACK. A list of the macros and the routines that are actually called is contained in Table 2. <p> The first option actually runs slightly faster ( 2 3%) than the second, but uses 2.5 times as much storage in the process. Only a hardware manufacturer (e.g., <ref> [7] </ref> or [6]) would ever consider delivering the first approach without testing the second approach first. Note that by using the second option and a Level 3 BLAS routine (e.g., dgemm or sgemm) that requires no extra storage, the storage requirement given in (3) is still valid. <p> Needless to say, gemmw and winos use the second option in the complex cases. Unlike at least one commercially available Strassen-Winograd matrix multiplication routine for complex data <ref> [7] </ref>, our routines do not attempt to re-order the matrices op (A) and op (B) in order to achieve stride 1 vectors. We note that the matrices may not always be in a part of memory that allows writes or is quick to do so. <p> However, since ZGEMMS uses 2.5 times as much memory as ZGEMMW, it quickly runs out of memory or into paging situations that are painful. There are two cases where ZGEMMS fails to perform at all (the first is noted in <ref> [7] </ref> and the second is an undocumented bug). First, when A, B, or C overlap with one or more of the other matrices, ZGEMMS calls ZGEMM instead, resulting in terrible performance (see Table 5).

References-found: 7

