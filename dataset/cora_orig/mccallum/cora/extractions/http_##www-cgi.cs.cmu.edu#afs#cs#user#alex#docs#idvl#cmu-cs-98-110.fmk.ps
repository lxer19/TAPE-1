URL: http://www-cgi.cs.cmu.edu/afs/cs/user/alex/docs/idvl/cmu-cs-98-110.fmk.ps
Refering-URL: 
Root-URL: 
Phone: 2  
Title: IMPROVING ACOUSTIC MODELS BY WATCHING TELEVISION  
Author: Michael J. Witbrock , and Alexander G. Hauptmann 
Note: This work was first presented at the 1997 AAAI Spring Symposium,  
Address: Pittsburgh, PA 15213-3890 USA  4616 Henry St, Pittsburgh PA 15213 USA  Palo Alto, CA.,  
Affiliation: 1 School of Computer Science, Carnegie Mellon University,  Justresearch (Justsystem Pittsburgh Research Center),  
Date: March 19 th 1998  March 1997.  
Pubnum: CMU-CS-98-110  
Abstract: 3 The work described in this paper was done while the first author was an employee of Carnegie Mellon University Abstract Obtaining sufficient labelled training data is a persistent difficulty for speech recognition research. Although well transcribed data is expensive to produce, there is a constant stream of challenging speech data and poor transcription broadcast as closed-captioned television. We describe a reliable unsupervised method for identifying accurately transcribed sections of these broadcasts, and show how these segments can be used to train a recognition system. Starting from acoustic models trained on the Wall Street Journal database, a single iteration of our training method reduced the word error rate on an independent broadcast television news test set from 62.2 % to 59.5%. This paper is based on work supported by the National Science Foundation, DARPA and NASA under NSF Cooperative agreement No. IRI-9411299. We thank Justsystem Corporation for supporting the preparation of the paper. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cox, S.J., and Bridle, J.S. </author> <title> Simultaneous speaker normali-sation and utterance labelling using Bayesian/neural net techniques. </title> <booktitle> 1990. In Proceedings of the 1990 IEEE International Conference on Acoustics, Speech, and Signal Processing. </booktitle> <volume> Vol. 1. </volume> <pages> pp 161-4. </pages>
Reference-contexts: Previous work on automatic learning in speech recognition has focussed chiefly on unsupervised adaptation schemes. Cox and Bridles connectionist RECNORM system <ref> (Cox & Bridle 1990) </ref>, for e xample, improved recognition accuracy by simply training the recognition network to more confidently output its existing classification decisions. The HTK recogniser described in (Woodland et al. 1994) also used unsupervised speaker adaptation to improve accuracy.
Reference: <author> Hauptmann, A. and Witbrock, M. </author> <year> 1996, </year> <title> Informedia: News-on-Demand Multimedia Information Acquisition and Retrieval, </title> <editor> In Maybury, M, ed, </editor> <booktitle> Intelligent Multimedia Information Retrieval, </booktitle> <publisher> AAAI Press, Forthcoming. </publisher>
Reference: <author> Hwang, M., Rosenfeld, R., Thayer, E., Mosur, R., Chase, L., Weide, R., Huang, X., and Alleva, F., </author> <year> 1994, </year> <title> Improving Speech Recognition Performance via Phone-Dependent VQ Codebooks and Adaptive Language Models in SPHINX-II. </title> <journal> ICASSP-94, </journal> <volume> vol. I, </volume> <pages> pp. 549-552. </pages>
Reference: <author> Kershaw, D.J. Robinson, A.J. and Renals, S.J., </author> <year> 1996, </year> <title> The 1995 Abbot Hybrid Connectionist-HMM Large-Vocabulary Recognition System. </title> <booktitle> In Notes from the 1996 ARPA Speech Recognition Workshop, </booktitle> <address> Arden House, Harriman NY, </address> <month> Feb </month> <year> 1996. </year>
Reference: <author> Nye, H., </author> <year> 1984, </year> <title> The Use of a One-Stage Dynamic Programming Algorithm for Connected Word Recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> Vol AASP-32, No 2, </volume> <pages> pp 263-271 Rudnicky, </pages> <address> A.I., </address> <year> 1996, </year> <title> Language Modeling with Limited Domain Data, </title> <booktitle> In Proceeding of the 1995 ARPA Workshop on Spoken Language Technology. CMU Speech Group, </booktitle> <year> 1997, </year> <note> URL: http://www.speech.cs.cmu.edu/speech Sphinx III Training, 1997, http://www.cs.cmu.edu/~eht/s3_train/s3_train.html Ravishankar, </note> <author> M. K., </author> <year> 1996, </year> <title> Efficient Algorithms for Speech Recognition, </title> <type> PhD diss. </type> <institution> Carnegie Mellon University. </institution> <note> Technical Report CMU-CS-96-143. </note>
Reference-contexts: Since the errors made by the captioning service and those made by Sphinx are lar gely independent, we can be confident that extended sections over which the captions and the Sphinx transcript correspond have been correctly transcribed.The process of finding correspondences is rather straightforw ard: a dynamic programming alignment <ref> (Nye 1984) </ref> is performed between the tw o text strings, with a distance metric between words that is zero if they match exactly, one if they don't match at all, and which increases with the number of mismatched letters in the case of partial matches.
Reference: <author> Placeway, P. and Lafferty, J., </author> <year> 1996, </year> <title> Cheating with Imperfect Transcripts, </title> <booktitle> In Proceedings of ICSLP 1996. </booktitle>
Reference-contexts: On this set, segmented into ninety utterance chunks, the recognition word error rate (substitutions+insertions+deletions) was 62.2%. Analysis of the recognizer errors sho ws that even with a trigram language model derived from a correct transcript, there is a significant error rate <ref> (Placeway & Lafferty 1996) </ref>. This leads to the conclusion that poor acoustic modeling is the major source of error for the broadcast tele vision data.
Reference: <author> Woodland, </author> <title> P.C. Leggetter, C.J. Odell, J.J., Valtchev, </title> <publisher> V. </publisher>
Reference: <author> Young, S.J., </author> <year> 1995, </year> <title> The 1994 HTK large vocabulary speech recognition system, </title> <booktitle> In Proceedings of the 1995 IEEE International Conference on Acoustics, Speech, and Signal Processing. </booktitle> <volume> Vol. 1. </volume> <pages> pp 73-76. </pages>
References-found: 8

