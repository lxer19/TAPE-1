URL: http://www.cs.berkeley.edu/~dandre/cs281/dhbl_cs281.ps
Refering-URL: http://www.cs.berkeley.edu/~dandre/cs281/project.html
Root-URL: 
Email: dandre@cs.berkeley.edu  
Title: Learning Hierarchical Behaviors  
Author: David Andre 
Web: http://www.cs.berkeley.edu/~dandre  
Address: 387 Soda Hall, #1776 Berkeley, CA 94720-1776  
Affiliation: Computer Science Division, University of California at Berkeley  
Note: 05/23/98 David Andre 1  
Abstract: In the last few years, many researchers have begun to study how to introduce hierarchy into reinforcement learning methods. Generally, this work has pushed the envelope in one of several important directions, but it typically has avoided the question of how to learn systems of hierarchical behavior from experience. We present a preliminary system for learning simple hierarchical systems using a constrained language for behavior specification. The system learns do-until macros by choosing a subgoal from and using its past experience with respect to this subgoal to build a constrained macro behavior that achieves it. The system can either be run offline and used for new but similar problems, or can be used during learning. We present an algorithm that combines macro learning with asynchronous value iteration methods such as prioritized sweeping. Macros learned by the system for a simple environment are discussed. Additionally, we present the related literature and sketch out the future work for this project. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Andre, D., Friedman, N., Parr, R. </author> <year> 1998. </year> <title> Generalized Prioritized Sweeping. </title> . <booktitle> In Advances in Neural Information Processing Systems. </booktitle> <volume> Vol 10. </volume> <publisher> MIT Press: </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In prioritized sweeping (Moore and Atkeson, 1993), states that are likely to undergo large changes in value are chosen to be updated. Peng and Williams (1993) present a similar approach for use with the DYNA architecture. Generalized prioritized sweeping <ref> (Andre, Friedman, and Parr, 1998) </ref> extends the prioritized sweeping model by noticing and utilizing the fact that priorities of the states are ordered based on the gradient of the Bellman equation. <p> This might suggest techniques for modifying macros to improve the abstraction. Also, we intend to combine this approach with the state of the art methods for exploration (Friedman et al.s Bayesian exploration, 1998), model representation <ref> (Andre et al.s generalized prioritized sweeping, 1998) </ref>, model learning (Friedmans SEM algorithm, 1997), and function approximation (various methods presented in Bertsekas and Tsitsiklis, 1996). Finally, we intend to attempt to extend this technique to partially observable domains. Conclusions This paper has presented a simple method for learning constrained hierarchical behaviors.
Reference: <author> Benson, S., and Nilsson, N., </author> <year> 1995. </year> <title> Reacting, planning, and learning in an autonomous agent. </title> <editor> In Furukawa, K, Michie, D., and Muggleton, S., editors, </editor> <booktitle> Machine Intelligence 14. </booktitle> <publisher> Oxford University Press: Oxford. </publisher>
Reference: <author> Bertsekas, D.C., and Tsitsiklis, J.N. </author> <year> 1996. </year> <title> Neuro-dynamic programming. </title> <publisher> Athena Scientific: </publisher> <address> Belmont, Mass. </address>
Reference-contexts: The study of function approximation techniques falls in this camp to some degree. In that case, the values of the states are stored and updated using an approximation function <ref> (e.g., see Bertsekas & Tsitsiklis, 1996) </ref>. These techniques can be hierarchical in a fashion, as they represent the modular structure of a state space. Although often these technique use explicit hard coding of the aggregation, many researchers have investigated adaptive aggregation algorithms (e.g. Moores Parti-Game algorithm). <p> Also, we intend to combine this approach with the state of the art methods for exploration (Friedman et al.s Bayesian exploration, 1998), model representation (Andre et al.s generalized prioritized sweeping, 1998), model learning (Friedmans SEM algorithm, 1997), and function approximation <ref> (various methods presented in Bertsekas and Tsitsiklis, 1996) </ref>. Finally, we intend to attempt to extend this technique to partially observable domains. Conclusions This paper has presented a simple method for learning constrained hierarchical behaviors.
Reference: <author> Bertsekas, D.P., and Tsitsiklis, J.N. </author> <year> 1989. </year> <title> Parallel and distributed computation. </title> <publisher> Prentice Hall:Englewood Cliffs. </publisher>
Reference-contexts: The algorithms differ in which states undergo a value update after each step in the world. In the classic asynchronous value iteration model <ref> (Bertsekas and Tsitsiklis, 1989) </ref>, all states undergo several value updates (until significant changes cease to take place). In DYNA (Sutton, 1990), a small number of random states undergo updates.
Reference: <author> Dayan, P., and Hinton, G.E. </author> <year> 1993. </year> <title> Feudal reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <publisher> MIT Press: </publisher> <address> Cambridge, MA. </address> <pages> 271-278. </pages>
Reference: <author> Dean, T., and Lin, S.-H. </author> <year> 1995. </year> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> Proceedings of the fourteenth international joint conference on artificial intelligence. </booktitle> <publisher> Morgan Kaufmann. </publisher> <pages> 1121-1127. </pages>
Reference: <author> Dietterich, T.G. </author> <year> 1997. </year> <title> Hierarchical reinforcement learning with maxq value function decomposition. </title> <type> Technical report, </type> <institution> Computer Science Dept, Oregon State University. </institution>
Reference: <author> Fikes, R.E., Hart, P.E., and Nilsson, N.J. </author> <year> 1972. </year> <title> Learning and executing generalized robot plans. </title> <booktitle> Artificial Intelligence 3 </booktitle> <pages> 251-288. </pages>
Reference: <author> Friedman, N. </author> <year> 1998. </year> <title> The Bayesian structural EM algorithm. </title> <booktitle> In Proceedings of the 14 th conference on uncertainty in artificial intelligence UAI98. 05/23/98 David Andre 10 Friedman, </booktitle> <editor> N., Dearden, R., and Russell, S.J. </editor> <booktitle> 1998. Bayesian Q-Learning. In Proceedings of the 15 th national conference on artificial intelligence AAAI98. </booktitle>
Reference-contexts: In prioritized sweeping (Moore and Atkeson, 1993), states that are likely to undergo large changes in value are chosen to be updated. Peng and Williams (1993) present a similar approach for use with the DYNA architecture. Generalized prioritized sweeping <ref> (Andre, Friedman, and Parr, 1998) </ref> extends the prioritized sweeping model by noticing and utilizing the fact that priorities of the states are ordered based on the gradient of the Bellman equation. <p> This might suggest techniques for modifying macros to improve the abstraction. Also, we intend to combine this approach with the state of the art methods for exploration <ref> (Friedman et al.s Bayesian exploration, 1998) </ref>, model representation (Andre et al.s generalized prioritized sweeping, 1998), model learning (Friedmans SEM algorithm, 1997), and function approximation (various methods presented in Bertsekas and Tsitsiklis, 1996). Finally, we intend to attempt to extend this technique to partially observable domains.
Reference: <author> Gordon, G.J. </author> <year> 1995. </year> <title> Sequential update of Bayesian network structure. </title> <booktitle> In Proc. 13 th Conf. On Uncertainty in AI. </booktitle>
Reference: <author> Kaelbling, </author> <title> L.P. 1993. Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Proceedings of the tenth international conference on machine learning ICML93. </booktitle> <publisher> Morgan Kaufmann:San Mateo, CA. </publisher> <pages> 167-173. </pages>
Reference: <author> Kaelbling, </author> <title> L.P., Littman, M.L., and Moore, </title> <publisher> A.W. </publisher> <year> 1996. </year> <title> Reinforcement learning: a survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285. </pages>
Reference-contexts: Finally, we conclude with discussion, description of future work, and a brief conclusion. Background on Markov Decision Processes and Prioritized Sweeping We assume the standard Markov Decision Process (MDP) framework for reinforcement learning <ref> (Kaelbling et. al, 1996) </ref>. We denote by S the set of possible environment states and by A the set of possible actions.
Reference: <author> Laird, J.E., Rosenbloom, P.S., Newell, A. </author> <year> 1986. </year> <title> Chunking in SOAR: The anatomy of a general learning mechanism. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 11-46. </pages>
Reference: <author> Lin, L.J. </author> <year> 1993. </year> <title> Reinforcement learning for Robots using neural networks. </title> <type> PhD Thesis. </type> <institution> Computer Science Department, Carnegie Mellon University. </institution>
Reference: <author> McCallum, A.K. </author> <year> 1995. </year> <title> Reinforcement Learning with Selective Perception and Hidden State. </title> <type> PhD Thesis. </type> <institution> University of Rochester. </institution>
Reference: <author> McGovern, A., Sutton, R.S., and Fagg, A.H. </author> <year> 1997. </year> <title> Roles of macro-actions in accelerating reinforcement learning. </title> <note> In Grace Hopper Celebration of Women in Computing,13-18. </note>
Reference: <author> Moore, A.W. </author> <year> 1994. </year> <title> The part-game algorithm for variable resolution reinforcement learning in muli-dimensional spaces, </title> <booktitle> Advances in Neural Processing Systems 7 </booktitle> <pages> 711-718, </pages> <publisher> MIT Press:Cambridge, </publisher> <address> MA. </address>
Reference: <author> Moore, A.W., and Atkeson, C.G. </author> <year> 1993. </year> <title> Prioritized sweeping reinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 12 </volume> <pages> 103-130. </pages>
Reference-contexts: In the classic asynchronous value iteration model (Bertsekas and Tsitsiklis, 1989), all states undergo several value updates (until significant changes cease to take place). In DYNA (Sutton, 1990), a small number of random states undergo updates. In prioritized sweeping <ref> (Moore and Atkeson, 1993) </ref>, states that are likely to undergo large changes in value are chosen to be updated. Peng and Williams (1993) present a similar approach for use with the DYNA architecture.
Reference: <author> Nilsson, N.J. </author> <year> 1973. </year> <title> Hierarchical robot planning and execution system. SRI AI Center Technical Note 76, SRI International, </title> <publisher> Inc., </publisher> <address> Menlo Park, CA. </address>
Reference: <author> Nilsson, N.J. </author> <year> 1994. </year> <title> Teleo-reactive programs for agent control. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 139-158. </pages>
Reference: <author> Parr, R. </author> <year> 1998. </year> <title> Hierarchical control and learning for Markov decision processes, </title> <type> PhD Thesis, </type> <institution> Computer Science Division, University of California at Berkeley. </institution>
Reference-contexts: In prioritized sweeping (Moore and Atkeson, 1993), states that are likely to undergo large changes in value are chosen to be updated. Peng and Williams (1993) present a similar approach for use with the DYNA architecture. Generalized prioritized sweeping <ref> (Andre, Friedman, and Parr, 1998) </ref> extends the prioritized sweeping model by noticing and utilizing the fact that priorities of the states are ordered based on the gradient of the Bellman equation.
Reference: <author> Parr, R., and Russell, S.J. </author> <year> 1998. </year> <title> Reinforcement learning with hierarchies of machines. </title> <booktitle> In Advances in Neural Information Processing Systems. </booktitle> <volume> Vol 10. </volume> <publisher> MIT Press: </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In prioritized sweeping (Moore and Atkeson, 1993), states that are likely to undergo large changes in value are chosen to be updated. Peng and Williams (1993) present a similar approach for use with the DYNA architecture. Generalized prioritized sweeping <ref> (Andre, Friedman, and Parr, 1998) </ref> extends the prioritized sweeping model by noticing and utilizing the fact that priorities of the states are ordered based on the gradient of the Bellman equation.
Reference: <author> Precup, D., and Sutton, R.S., </author> <year> 1998. </year> <title> Multi-time models for temporally abstract planning. </title> <booktitle> In Advances in Neural Information Processing System. </booktitle> <volume> Vol 10. </volume> <publisher> MIT Press: </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In many applications, one would like to have the system determine the appropriate subgoals and learn the hierarchy of behaviors automatically. Although some recent work <ref> (Sutton, Precup, and Singh, 1998) </ref> addresses the issue of modifying parts of learned macros (called options in their work), their method of learning options is somewhat undesirable because it still requires the user to specify explicit subgoals.
Reference: <author> Precup, D., Sutton, R. S., Singh, S. </author> <year> 1998. </year> <title> Theoretical results on reinforcement learning with temporally abstract options. </title> <booktitle> Proceedings of the Tenth European Conference on Machine Learning, </booktitle> <publisher> Springer Verlag. </publisher>
Reference-contexts: In many applications, one would like to have the system determine the appropriate subgoals and learn the hierarchy of behaviors automatically. Although some recent work <ref> (Sutton, Precup, and Singh, 1998) </ref> addresses the issue of modifying parts of learned macros (called options in their work), their method of learning options is somewhat undesirable because it still requires the user to specify explicit subgoals.
Reference: <author> Precup, D., Sutton, R.S., Singh, S.P. </author> <year> 1997. </year> <title> Planning with closed-loop macro actions. </title> <booktitle> Working notes of the 1997 AAAI Fall Symposium on Model-directed Autonomous Systems, </booktitle> <pages> pp. 70-76. </pages>
Reference: <author> Singh, S. </author> <year> 1992. </year> <title> Scaling reinforcement learning by learning variable temporal resolution models. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning ICML92. </booktitle> <publisher> Morgan Kaufmann:San Mateo, CA. </publisher> <pages> 521-539. </pages>
Reference: <author> Singh, S.P. </author> <year> 1992. </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine learning, </journal> <volume> 8(3). </volume> <month> May. </month>
Reference: <author> Sutton, R. S., Precup, D., Singh, S. </author> <year> 1998 </year> <month> (submitted). </month> <title> Between MDPs and semi-MDPs: Learning, planning, and representing knowledge at multiple temporal scales. </title> <type> JAIR. </type>
Reference-contexts: In many applications, one would like to have the system determine the appropriate subgoals and learn the hierarchy of behaviors automatically. Although some recent work <ref> (Sutton, Precup, and Singh, 1998) </ref> addresses the issue of modifying parts of learned macros (called options in their work), their method of learning options is somewhat undesirable because it still requires the user to specify explicit subgoals.
Reference: <author> Sutton, R. S., Precup, D., Singh, S. </author> <year> 1998. </year> <title> Intra-option learning about temporally abstract actions. </title> <booktitle> Proceedings of the 15th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In many applications, one would like to have the system determine the appropriate subgoals and learn the hierarchy of behaviors automatically. Although some recent work <ref> (Sutton, Precup, and Singh, 1998) </ref> addresses the issue of modifying parts of learned macros (called options in their work), their method of learning options is somewhat undesirable because it still requires the user to specify explicit subgoals.
Reference: <author> Sutton, R.S. </author> <year> 1990. </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Machine Learning: Proc. 7 th Int. Conf. </booktitle>
Reference-contexts: The algorithms differ in which states undergo a value update after each step in the world. In the classic asynchronous value iteration model (Bertsekas and Tsitsiklis, 1989), all states undergo several value updates (until significant changes cease to take place). In DYNA <ref> (Sutton, 1990) </ref>, a small number of random states undergo updates. In prioritized sweeping (Moore and Atkeson, 1993), states that are likely to undergo large changes in value are chosen to be updated. Peng and Williams (1993) present a similar approach for use with the DYNA architecture.
Reference: <author> Thrun, T., Schwartz, A. </author> <year> 1995. </year> <title> Finding structure in reinforcement learning. </title> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Uther, W., and Veloso, M.M. </author> <year> 1997. </year> <title> Generalizing Adversarial Reinforcement Learning. </title> <booktitle> AAAI Fall symposium on Model directed autonomous systems. </booktitle>
Reference: <author> Watkins, C.J. </author> <year> 1989. </year> <title> Models of delayed reinforcement learning. </title> <type> PhD Thesis. </type> <institution> Psych. dept, Cambridge University. </institution>
Reference-contexts: There are two main types of methods, model-based and model-free methods. Model-free 05/23/98 David Andre 3 methods, such as Q-learning <ref> (Watkins, 1989) </ref>, will not be considered in detail here. Model based methods utilize the model of the environment to perform value propagation steps (planning) during learning.
Reference: <author> Weiring, M., Schmidhuber, J. </author> <year> 1997. </year> <title> HQ-Learning. Adaptive Behavior. </title> <type> 6(2). </type>
Reference: <author> Y. Takahashi, M. Asada and K. Hosoda. </author> <year> 1996. </year> <title> Reasonable Performance in Less Learning Time by Real Robot Based on Incremental State Space Segmentation. </title> <booktitle> Proc. of IEEE/RSJ International Conference on Intelligent Robots and Systems. </booktitle> <pages> 1518-1524. </pages>
References-found: 35

