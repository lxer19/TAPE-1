URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/sparcle.ps.Z
Refering-URL: http://cag-www.lcs.mit.edu/~kubitron/papers/index.html
Root-URL: 
Title: Sparcle: An Evolutionary Processor Design for Large-Scale Multiprocessors  
Author: Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, Godfrey D'Souza, and Mike Parkin 
Date: 12 March 1993  
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: Sparcle is a processor chip developed jointly by MIT, LSI Logic, and SUN Microsystems, by evolving an existing RISC architecture towards a processor suited for large-scale multiprocessors. Sparcle supports three multiprocessor mechanisms: fast context switching, fast, user-level message handling, and fine-grain synchronization. The Sparcle effort demonstrates that RISC architectures coupled with a communications and memory management unit do not require major architectural changes to support multiprocessing efficiently.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak Ordering ANew Definition. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <address> New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Thus, systems must implement dynamic pipelines into the network in which the hardware ensures that multiple, previously issued memory operations have completed before issuing operations that depend on their completion. Context switching is one mechanism for dynamic pipelining. Other methods for dynamic pipelining including prefetching and weak ordering <ref> [9, 1, 13] </ref>.
Reference: [2] <author> Anant Agarwal, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Kubi-atowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, Dan Nussbaum, Mike Parkin, and Donald Yeung. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: Indeed, Sparcle is derived from the SPARC [17] architecture, and is being integrated into Alewife <ref> [3, 2] </ref>, a large-scale multiprocessor system being developed at MIT. Sparcle tolerates long communication and synchronization latencies by rapidly switching to other threads of computation. The current implementation of Sparcle can switch to another thread of computation in 14 cycles. <p> Sparcle also supports DMA for larger messages. The mechanics of sending and receiving messages are described in Section 4. 3 Overview of the Alewife Machine Interfaces The Sparcle chip is part of a complete multiprocessing system. It serves as the CPU for the Alewife machine <ref> [2] </ref> a distributed shared-memory multiprocessor with up to 512 nodes and hardware-supported cache coherence. Figure 4 depicts the Alewife machine as a set of processing nodes connected in a mesh topology.
Reference: [3] <author> Anant Agarwal, Beng-Hong Lim, David A. Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <address> New York, </address> <month> June </month> <year> 1990. </year> <pages> 21 22 </pages>
Reference-contexts: Indeed, Sparcle is derived from the SPARC [17] architecture, and is being integrated into Alewife <ref> [3, 2] </ref>, a large-scale multiprocessor system being developed at MIT. Sparcle tolerates long communication and synchronization latencies by rapidly switching to other threads of computation. The current implementation of Sparcle can switch to another thread of computation in 14 cycles.
Reference: [4] <institution> Alpha Architecture Reference Manual. Digital Press, </institution> <year> 1992. </year>
Reference-contexts: This could include widening of internal processor registers and use of special full/empty bit synchronization instructions which are sandwiched between Alpha-style <ref> [4] </ref> load-locked/store-conditional synchronization instructions. 5.3 Fast Message Handling Fast messaging in Alewife relies on a number of features in the CMMU. All of the network queuing and DMA mechanisms are a part of this chip.
Reference: [5] <author> Gail Alverson, Robert Alverson, and David Callahan. </author> <title> Exploiting Heterogeneous Parallelism on a Multithreaded Multiprocessor. </title> <booktitle> In Workshop on Multithreaded Computers, Proceedings of Supercomputing '91. ACM Sigraph & IEEE, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: This implies that we need to associate two bits of state with each J-structure element: whether the element is full or empty, and whether the wait queue is locked or not. Other architectures implement these two state bits directly in hardware by having multiple state bits per memory location <ref> [5, 14] </ref>. Instead of providing an additional hardware bit, we take advantage of SPARC's atomic register-memory swap operation.
Reference: [6] <author> Arvind, R. S. Nikhil, and K. K. Pingali. I-Structures: </author> <title> Data Structures for Parallel Computing. </title> <booktitle> In Proceedings of the Workshop on Graph Reduction, (Springer-Verlag Lecture Notes in Computer Science 279), </booktitle> <month> September/October </month> <year> 1986. </year>
Reference-contexts: At the programming language level, we provide parallel do-loops to express data-level parallelism, and J-structure and L-structure arrays to express fine-grain data-level synchronization. A J-structure is a data structure for producer-consumer style synchronization and was inspired by Arvind et al.'s I-structures <ref> [6] </ref>. It is like an array, but each element has additional state: full or empty. The initial state of a J-structure element is empty. A reader of an element waits until the element's state is full before returning the value.
Reference: [7] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: It also supports the LimitLESS cache-coherence protocol <ref> [7] </ref>, which maintains a few pointers per memory block in hardware (up to five in Alewife) and emulates additional pointers in software when needed. Through this protocol, all of the caches in the system maintain a coherent view of global memory.
Reference: [8] <author> William J. Dally et al. </author> <title> The J-Machine: A Fine-Grain Concurrent Computer. </title> <booktitle> In IFIP Congress, </booktitle> <year> 1989. </year>
Reference-contexts: The particular SPARC design that we modified has eight overlapping register windows. Rather than using the register windows as a register stack, we used them in pairs to represent four independent, non-overlapping contexts. We use one as a context for trap handlers, as in <ref> [8, 15] </ref> and the other three for user threads. The SPARC Current Window Pointer (CWP) serves as the context pointer. Further, the Window Invalid Mask (WIM) is used to indicate which contexts are disabled and which are active.
Reference: [9] <author> Michel Dubois, Christoph Scheurich, and Faye A. Briggs. </author> <title> Synchronization, coherence, and event ordering in multiprocessors. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 9-21, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Thus, systems must implement dynamic pipelines into the network in which the hardware ensures that multiple, previously issued memory operations have completed before issuing operations that depend on their completion. Context switching is one mechanism for dynamic pipelining. Other methods for dynamic pipelining including prefetching and weak ordering <ref> [9, 1, 13] </ref>.
Reference: [10] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory; Early Experience. </title> <booktitle> In Conference on Principles and Practice of Parallel Programming. ACM, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: In Alewife, with a fast message, a thread can be created on a remote processor in 7sec. Restricting ourselves to shared-memory operations, remote thread creation takes 24sec. See <ref> [10] </ref> for a study on the importance of an efficient message interface in a shared-memory setting.
Reference: [11] <author> John Kubiatowicz, David Chaiken, and Anant Agarwal. </author> <title> Closing the Window of Vulnerability in Multiphase Memory Transactions. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V). ACM, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: Further, individual load and store instructions have varied semantics with respect to the full/empty bit: some cause test-and-set like operations; others invoke traps. This places some data processing logic within the first-level cache. For modern processors 3 Some interesting forward-progress issues appear, however. See <ref> [11] </ref>. 17 stio Header, N0 I D E M M W stio Data, N1 I | | D E M M W ipilaunch 1, 1 I | | | | D E W Q1 Q2 I | | D E W Cache Bus: I I I N0 N0 I N1 N1
Reference: [12] <author> Kiyoshi Kurihara, David Chaiken, and Anant Agarwal. </author> <title> Latency Tolerance through Mul-tithreading in Large-Scale Multiprocessors. </title> <booktitle> In Proceedings International Symposium on Shared Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: By maintaining a separate PC and PSR for each context, a more aggressive processor design could switch contexts much faster. However, even with 14 cycles of overhead and four processor-resident contexts, multithreading can significantly improve system performance <ref> [18, 12] </ref>. 4.2 Support for Fine-Grain Synchronization As described in Section 2.1, fine-grain data-level synchronization is expressed with J- and L-structures and implemented using new instructions that interact with full/empty bits in memory. 11 The new load, store and swap instructions are implemented in Sparcle using the SPARC alternate address space
Reference: [13] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <address> New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Thus, systems must implement dynamic pipelines into the network in which the hardware ensures that multiple, previously issued memory operations have completed before issuing operations that depend on their completion. Context switching is one mechanism for dynamic pipelining. Other methods for dynamic pipelining including prefetching and weak ordering <ref> [9, 1, 13] </ref>.
Reference: [14] <author> G. M. Papadopoulos and D.E. Culler. Monsoon: </author> <title> An Explicit Token-Store Architecture. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <address> New York, </address> <month> June </month> <year> 1990. </year> <note> IEEE. </note>
Reference-contexts: This implies that we need to associate two bits of state with each J-structure element: whether the element is full or empty, and whether the wait queue is locked or not. Other architectures implement these two state bits directly in hardware by having multiple state bits per memory location <ref> [5, 14] </ref>. Instead of providing an additional hardware bit, we take advantage of SPARC's atomic register-memory swap operation.
Reference: [15] <author> Charles L. Seitz. </author> <title> Mosaic Project Update, March 1991. </title> <booktitle> Darpa VLSI woskshop. </booktitle>
Reference-contexts: The particular SPARC design that we modified has eight overlapping register windows. Rather than using the register windows as a register stack, we used them in pairs to represent four independent, non-overlapping contexts. We use one as a context for trap handlers, as in <ref> [8, 15] </ref> and the other three for user threads. The SPARC Current Window Pointer (CWP) serves as the context pointer. Further, the Window Invalid Mask (WIM) is used to indicate which contexts are disabled and which are active.
Reference: [16] <author> B.J. Smith. </author> <title> Architecture and Applications of the HEP Multiprocessor Computer System. </title> <booktitle> SPIE, </booktitle> <volume> 298 </volume> <pages> 241-248, </pages> <year> 1981. </year>
Reference-contexts: A software prefetch instruction is also provided. The modifications to a modern RISC microprocessor to achieve fast context switching are described in Section 4.1. Sparcle supports fine-grain data-level synchronization through the use of full/empty bits, as in the HEP <ref> [16] </ref>. With full/empty bits, the probe of a lock and access of the data word protected by the lock can be accomplished in one operation. If the synchronization attempt fails, then the synchronization trap is invoked to handle the fault. <p> Thus, an L-structure allows mutually exclusive access to each of its elements, and allows multiple non-locking readers. J- and L-structures, as well as other types of fine-grain data-level synchronization, are supported by Sparcle with per-word, full/empty bits in memory <ref> [16] </ref>. Sparcle provides new load and store instructions that interact with the full/empty bits. An extra synchronous trap line has also been added to deliver the full/empty trap. This extra line allows the trap to be immediately identified. <p> For synchronization faults, the trap handler might also choose to retry the request immediately (spin). Processors that switch rapidly between multiple threads of computation are called multi-threaded architectures. The prototypical multithreaded machine is the HEP <ref> [16] </ref>. In the HEP, the processor switches every cycle between eight processor-resident threads. Cycle-by-cycle interleaving of threads is termed fine multithreading.
Reference: [17] <author> SPARC Architecture Manual, </author> <year> 1988. </year> <institution> SUN Microsystems, Mountain View, California. </institution>
Reference-contexts: Indeed, Sparcle is derived from the SPARC <ref> [17] </ref> architecture, and is being integrated into Alewife [3, 2], a large-scale multiprocessor system being developed at MIT. Sparcle tolerates long communication and synchronization latencies by rapidly switching to other threads of computation. The current implementation of Sparcle can switch to another thread of computation in 14 cycles.
Reference: [18] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Exploring the Benefits of Multiple Hardware Contexts in a Multiprocessor Architecture: Preliminary Results. </title> <booktitle> In Proceedings 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 273-280, </pages> <address> New York, </address> <month> June </month> <year> 1989. </year> <month> 23 </month>
Reference-contexts: By maintaining a separate PC and PSR for each context, a more aggressive processor design could switch contexts much faster. However, even with 14 cycles of overhead and four processor-resident contexts, multithreading can significantly improve system performance <ref> [18, 12] </ref>. 4.2 Support for Fine-Grain Synchronization As described in Section 2.1, fine-grain data-level synchronization is expressed with J- and L-structures and implemented using new instructions that interact with full/empty bits in memory. 11 The new load, store and swap instructions are implemented in Sparcle using the SPARC alternate address space
References-found: 18

