URL: http://www.daimi.aau.dk/~depenau/PUB/thesis.ps
Refering-URL: http://www.daimi.aau.dk/~depenau/PUB/pub.html
Root-URL: http://www.daimi.aau.dk
Title: Automated Design of Neural Network Architecture for Classification  
Degree: Ph.D. Thesis by  
Address: Ny Munkegade, Bldg. 540, DK-8000 Aarhus C  
Affiliation: and DAIMI, Computer Science Department, Aarhus University,  
Note: TERMA Elektronik AS, Hovmarken 4, DK-8520 Lystrup  Danish Academy of Technical Sciences Industrial Research Education Ph.D. Thesis EF-448  
Date: Jan Depenau  August 1995  
Abstract-found: 0
Intro-found: 1
Reference: <institution> []GENERAL DESCRIPTION OF NEURAL NETWORKS </institution>
Reference: [DARPA 88] <institution> DARPA Neural Network Study. </institution> <year> (1988), </year> <note> AFCEA International Press. </note>
Reference-contexts: Although they might look different and are trained differently, they are all feed-forward networks. A further geometric description and interpretation of some of these networks and other types can be found in [Wieland and Leighton 87], [Lippmann 87], [Huang and Lippmann 87], <ref> [DARPA 88] </ref> (chapter 6), [Gibson and Cowan 90] and [Huang and Huang 91]. 2.6.
Reference: [Funahashi 89] <author> Funahashi, K. </author> <year> (1989), </year> <title> On the Approximate Realization of Continuous Mappings by Neural Networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2, </volume> <pages> 183-192 </pages>
Reference: [Gibson and Cowan 90] <author> Gibson, G.J. and Cowan, C.F.N. </author> <year> (1990), </year> <title> On the Decision Regions of Multilayer Perceptrons, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol 78, No. 10, </volume> <month> October </month> <year> 1990. </year>
Reference-contexts: Although they might look different and are trained differently, they are all feed-forward networks. A further geometric description and interpretation of some of these networks and other types can be found in [Wieland and Leighton 87], [Lippmann 87], [Huang and Lippmann 87], [DARPA 88] (chapter 6), <ref> [Gibson and Cowan 90] </ref> and [Huang and Huang 91]. 2.6.
Reference: [Hebb 49] <author> Hebb, D. </author> <year> (1949), </year> <title> The Organization of Behavior, </title> <address> New York: </address> <publisher> John Wiley and Sons </publisher>
Reference-contexts: Early work in the field was indeed inspired by biological studies of the brain function, and [McCulloch and Pitts 43] and <ref> [Hebb 49] </ref> described systems of neurons and learning rules derived from biological models. Today most neural networks ~ bear only a passing resemblance to natural brains or nervous systems.
Reference: [Hertz et al. 91] <author> Hertz, J., Krogh, A. and Palmer, R. </author> <year> (1991), </year> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison Wesley. </publisher>
Reference-contexts: DEFINITION OF GENERALISATION (ERROR) 31 p 6 4 2 654321 growth function when P VC dim , 4 (P ) 1:5P VC dim =VC dim !. counts for all linear classifiers. Mathematical proofs of the bounding function in the case of general linear classifiers can be found in <ref> [Hertz et al. 91] </ref>, [Cover 65] and [Nilsson 89]. From this description one could get the idea that the capacity is simply equal to the number of adjustable weights and as such proportionally related to the complexity. <p> Inspired by the Bernoulli theorem, Vapnik and Chervonenkis (in their most famous paper [Vapnik and Chervonenskis 71]) and Vapnik ([Vapnik 82], [Vapnik 92]) defined the uniform convergence in the following way: 4 For those who would like to know more about the statistical physics approach, <ref> [Hertz et al. 91] </ref>, [Tishby 95] including references, provide a good introduction. 4.5. <p> As this equation shows weights that are not continually updated by the Gradient Descent will gradually decay, hence the name Weight Decay. A drawback to this simple Weight Decay method is that the decay rate is equal for all weights. <ref> [Hertz et al. 91] </ref> gives a review of methods very similar to the simple Weight Decay that try to use different decay rates, so e.g. small weights will decay more rapidly. 1 Note the similarity with equation (4.7) in the paragraph on the VC theory. 5.2.
Reference: [Huang and Lippmann 87] <author> Huang, W.H. and Lippmann, R.P. </author> <year> (1987), </year> <title> Neural net and traditional classifiers, </title> <booktitle> In Neural Information Processing Systems (Denver 1987). </booktitle>
Reference-contexts: Although they might look different and are trained differently, they are all feed-forward networks. A further geometric description and interpretation of some of these networks and other types can be found in [Wieland and Leighton 87], [Lippmann 87], <ref> [Huang and Lippmann 87] </ref>, [DARPA 88] (chapter 6), [Gibson and Cowan 90] and [Huang and Huang 91]. 2.6.
Reference: [Kohonen 84] <author> Kohonen, T. </author> <title> (1984) Self-Organization and Associative Memory, </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: [Lapedes and Farber 88] <author> Lapedes, A. and Farber R. </author> <year> (1988), </year> <title> How Neural Nets Work. </title> <booktitle> In Neural Information Processing Systems, </booktitle> <editor> ed. D.Z. </editor> <booktitle> Ander-son, </booktitle> <pages> pp 442-456. </pages> <address> New York: </address> <publisher> American Institute of Physics. </publisher>
Reference: [Le Cun 93a] <author> Le Cun, Y. </author> <year> (1993), </year> <title> Efficient Learning & Second-Order Methods ATutorial at NIPS fl 93. </title> <booktitle> Notes from NIPS 93, </booktitle> <address> Denver MARRIOT City Center Denver, Colorado, Novem-ber 29, </address> <year> 1993. </year> <pages> 76 pages. </pages>
Reference-contexts: T anh (U li ) is often the best choice, a further explanation is given in <ref> [Le Cun 93a] </ref>. <p> How long it takes before the minimum is reached will depend on how large the step is. The size of the step is normally called the step-size or learning-rate. A nice illustration of how to determine the learning-rate can be found in <ref> [Le Cun 93a] </ref>. An alternative would be to calculate where the second derivative is zero and move x immediately to that value. <p> The normalisation was done to ensure that the input elements are equally weighted and that the dynamism of the hidden units in the first layer is explored. Further description and alternative ideas of normalisation can be found in <ref> [Le Cun 93a] </ref>. Although this seems reasonable from a neural network point of view, one should be aware that it might make the task even harder.
Reference: [Lippmann 87] <author> Lippmann, R.P. </author> <year> (1987), </year> <title> An Introduction to Computing with Neural Nets. </title> <journal> IEEE ASSP Magazine, </journal> <month> April </month> <year> 1987, </year> <pages> pp. 4-22. </pages>
Reference-contexts: Although they might look different and are trained differently, they are all feed-forward networks. A further geometric description and interpretation of some of these networks and other types can be found in [Wieland and Leighton 87], <ref> [Lippmann 87] </ref>, [Huang and Lippmann 87], [DARPA 88] (chapter 6), [Gibson and Cowan 90] and [Huang and Huang 91]. 2.6.
Reference: [Lutzy et al. 93] <author> Lutzy, O. and Dengel, A. </author> <year> (1993), </year> <title> A Comparison of Neural Net Simulators IEEE Expert, </title> <month> August </month> <year> 1993, </year> <pages> pp. 43-51. 103 104 BIBLIOGRAPHY </pages>
Reference-contexts: 13 V 1n V L1 V Lm Multi-layer perceptron, c) Cascade-Correlation, and d) GLOCAL The implementation of the first three types of network was done in the very popular SNNS simulator [SNNS 94], probably one of the best simulators at the moment 2 . 2 In 1993 Lutzy et al. <ref> [Lutzy et al. 93] </ref> made an evaluation of some of the most popular simulators, PlaNet v5.6, Pygmalion v2.0, Rochesster Connectionist Simulator v4.2. In their conclusions they recom mend the use of the SNNS. 82 CHAPTER 7.
Reference: [McCulloch and Pitts 43] <author> McCulloch, W. and Pitts, W. </author> <title> (1943) A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics. </journal> <volume> 7, </volume> <pages> pp. 115-133. </pages>
Reference-contexts: Early work in the field was indeed inspired by biological studies of the brain function, and <ref> [McCulloch and Pitts 43] </ref> and [Hebb 49] described systems of neurons and learning rules derived from biological models. Today most neural networks ~ bear only a passing resemblance to natural brains or nervous systems.
Reference: [Michie et al. 94] <editor> Michie, D., Spiegelhalter, D.J. and Taylor, </editor> <booktitle> C.C. </booktitle> <year> (1994), </year> <title> Machine Learning, Neural and Statistical Classification, </title> <booktitle> Ellis Horwood series in Artificial Intelligence, </booktitle> <pages> 289 pages. </pages>
Reference: [Minsky and Papert 69] <author> Minsky, M. and Papert, S. </author> <year> (1969), </year> <title> Perceptrons, </title> <address> Cambridge MA: </address> <publisher> MIT Press. </publisher> <pages> 258 pages. </pages>
Reference-contexts: approximation a neural network can implement has often been asked, and the answer is that both MLP and RBF networks can approximate any function, see [Hornik et al. 89], [Leshno et al. 93] and [Hartman et al. 90], while a simple perceptron can solve only linearly separable ~ classification problems <ref> [Minsky and Papert 69] </ref>. 2.6 Networks Used in this Thesis Besides the three networks mentioned a number of network types will be used in this thesis. In figure 2.4 there is an illustration of these networks and how they perform a classification task in two dimensions. <p> Gallant argues that such units would be feature detectors and using enough of these units the task will eventually be solved. In theory it can be shown that this is true (see <ref> [Minsky and Papert 69] </ref> for the boolean case and [Hornik et al. 89] for the general case), but it may unfortunately require an exponential large number of hidden units. Gallant does not specify the features, they are just some arbitrary features.
Reference: [Nilsson 89] <author> Nilsson, N.J. </author> <year> (1965), </year> <title> Learning Machines Foundations of trainable pattern-classifying systems. </title> <publisher> McGraw-Hill Book Company. </publisher> <pages> 137 pages. </pages>
Reference-contexts: Mathematical proofs of the bounding function in the case of general linear classifiers can be found in [Hertz et al. 91], [Cover 65] and <ref> [Nilsson 89] </ref>. From this description one could get the idea that the capacity is simply equal to the number of adjustable weights and as such proportionally related to the complexity.
Reference: [Pao 89] <author> Pao, H. </author> <year> (1989), </year> <title> Adaptive Pattern Recognition and Neural Networks. 1 - 327. </title> <publisher> Addison Wesley. </publisher>
Reference-contexts: Gallant does not specify the features, they are just some arbitrary features. If more regular features were used, like multiplication of each single input, this approach would be similar to what is known as the functional link, invented by Pao <ref> [Pao 89] </ref>. The principle of the Distributed Construction algorithm and its corresponding architecture are shown in figure 6.3. 6.3.
Reference: [Prechelt 94] <author> Prechelt, L. </author> <year> (1994), </year> <title> Proben 1 A Set of Neural Network Benchmark Problems and Benchmarking Rules. Found on connectionist ftp list. </title>
Reference: [Rosenblatt 62] <author> Rosenblatt, F.(1962), </author> <booktitle> Principles of Neurodynamics. </booktitle> <address> New York: </address> <publisher> Spartan. </publisher>
Reference-contexts: For a simple perceptron with output units having threshold transfer functions there is a special learning method named after the perceptron. It is called the perceptron learning rule ~ <ref> [Rosenblatt 62] </ref>. This method is, however, restricted to being only valid if the problem is linearly separable. An extension of this method, known as the pocket algorithm ~ [Gallant 86], is able to handle nonlinearly separable problems.
Reference: [Ripley 94] <author> Ripley, B.D. </author> <year> (1994), </year> <title> Neural Networks and Related Methods for Classification. </title> <journal> J. R. Statist. Soc B(1994) 56, </journal> <volume> No. 3, </volume> <pages> pp. 409-456. </pages>
Reference-contexts: Another question left open is how to deal with higher dimensionality in the input space, which also has influence on the metric to be used. Discussions and illustrations of these problems can be found in [Duda and Hart 73], [Schalkoff 92], [4], [6] and <ref> [Ripley 94] </ref>. It should be stressed that the answers are not trivial, but indeed very important for the overall performance, perhaps the most important.
Reference: [Rumelhart et al. 86] <editor> Rumelhart, D. E. and McClelland, J. L. </editor> <booktitle> and the PDP Research Group (1986), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations. </booktitle> <pages> pp. 1 - 547, </pages> <address> Cambridge: </address> <publisher> MIT Press. </publisher>
Reference-contexts: A method which allows calculation of the derivative of the error between actual output and target with respect to all weights in the network was derived independently by several people, see for example <ref> [Rumelhart et al. 86] </ref> (chapter 8). The method is known as the Error Propagation ~ or Back-Propagation method ~ , and is central to much current work on learning in neural networks.
Reference: [Rumelhart et al. 86] <author> Rumelhart, D.E., Hinton, G.E. and Williams, R.J. </author> <year> (1986), </year> <title> Learning Internal Representations by Error Propagation, </title> <booktitle> Nature 323, </booktitle> <pages> pp. 533-536. </pages>
Reference-contexts: A method which allows calculation of the derivative of the error between actual output and target with respect to all weights in the network was derived independently by several people, see for example <ref> [Rumelhart et al. 86] </ref> (chapter 8). The method is known as the Error Propagation ~ or Back-Propagation method ~ , and is central to much current work on learning in neural networks.
Reference: [Scalettar and Zee 88] <author> Scalettar, R. and Zee, A. </author> <year> (1988), </year> <title> Emergence of Grandmother Memory in Feed Forward Networks: Learning with Noise and Forgetfulness. In Connectionist Models and Their Implications: </title> <booktitle> Readings from Cognitive Science, </booktitle> <editor> eds. D. Waltz and J.A. Feldman, </editor> <volume> Chapter 11, </volume> <pages> pp 309-332. </pages> <address> Nor-wood: </address> <publisher> Ablex. </publisher>
Reference: [Schalkoff 92] <author> Schalkoff R. J. </author> <year> (1992), </year> <title> Pattern Recognition: Statistical, Structural and Neural Approaches, </title> <publisher> John Wiley and Sons, Inc. </publisher> <year> 1992. </year> <note> BIBLIOGRAPHY 105 </note>
Reference-contexts: There are several different iterative methods that perform the task of clustering ~ , see e.g. [Duda and Hart 73](Chapter 6), [Fukunaga 90](chapter 11), [Dasarathy 90] and <ref> [Schalkoff 92] </ref>(chapter 5). Many of these cluster algorithms including the very popular "K-mean algorithm ~ " are based on the neighbourhood properties among the patterns. The problem with using one of these methods is that in general they do not take the class membership into consideration. <p> Another question left open is how to deal with higher dimensionality in the input space, which also has influence on the metric to be used. Discussions and illustrations of these problems can be found in [Duda and Hart 73], <ref> [Schalkoff 92] </ref>, [4], [6] and [Ripley 94]. It should be stressed that the answers are not trivial, but indeed very important for the overall performance, perhaps the most important. <p> Each row indicates how the elements from the class at the left are classified. 1 An alternative way to reduce the number of features is to use the Karhunen-Loeve transformation also known as Principal Component Analysis, see e.g. <ref> [Schalkoff 92] </ref>(pp. 306-308) 7.4. NEURAL NETWORKS FOR CLASSIFICATION OF ICE 81 7.4 Neural Networks for Classification of Ice 7.4.1 Neural Network Architecture In order to explore neural networks' capability to classify ice types, several different networks were implemented. <p> Another question is how to deal with higher dimensionality, which also has a certain influence on the metric to be used. Discussions on and illustrations of these problems can be found in [Duda and Hart 73], <ref> [Schalkoff 92] </ref>, [4] and [6]. It should be stressed that the answers are not trivial, but indeed very important for the overall performance, perhaps the most important. Measurement of the generalisation ability. One of the everlasting questions is how to measure the generalisation ability of different learning machines.
Reference: [Sejnowski and Rosenberg 87] <author> Sejnowski, T.J and Rosenberg, C.R. </author> <year> (1987), </year> <title> Parallel networks that learn to pronounce English text, </title> <journal> Complex Systems, </journal> <volume> Vol. 1, </volume> <pages> pp. 145-168. </pages>
Reference: [SOM 95] <author> Kohonen, T. et al. </author> <year> (1995), </year> <title> Manual for the Self-Organised Map Program Package (SOM-PAK), </title> <note> version 3.1 (April 7, 1995), </note> <institution> Helsinki University of Technology, Laboratory of Computer and Information Science. </institution> <note> Available from the Internet site cochlea.hut.fi in directory /pub/som pak. </note>
Reference-contexts: An exception is the Mean backscatter feature. Self-Organised-Map: ~ The last analysis was also an attempt to measure how close the data were to each other. An un-supervised clustering method belonging to a class of neural networks called Self-Organised-Map <ref> [SOM 95] </ref> was used. Several different networks (number of output units and thereby possible clusters) were constructed and more than 20,000 different Maps were produced. In figure 7.5 the best performing network is shown. Best performing means the map with the lowest quantisation error.
Reference: [SNNS 94] <author> Zell, A. et al. </author> <year> (1994), </year> <title> Stuttgart Neural Network Simulator User Manual Version 3.3 Report No. 3/94 from University of Stuttgart, Institute for Parallel and Distributed High Performance System (IPVP). </title> <note> Available from the Internet site ftp.infomatik.uni-stuttgart.de in directory /pub/SNNS. </note>
Reference-contexts: a V 11 V 12 V 13 V 1n V L1 V Lm c V 11 V 12 V 13 V 1n V L1 V Lm Multi-layer perceptron, c) Cascade-Correlation, and d) GLOCAL The implementation of the first three types of network was done in the very popular SNNS simulator <ref> [SNNS 94] </ref>, probably one of the best simulators at the moment 2 . 2 In 1993 Lutzy et al. [Lutzy et al. 93] made an evaluation of some of the most popular simulators, PlaNet v5.6, Pygmalion v2.0, Rochesster Connectionist Simulator v4.2.
Reference: [Thrun et al. 91] <author> Thrun, </author> <title> S.B. and 23 co-authors (1991), The MONK's Problems A performance comparison of different learning algorithms, </title> <institution> CMU-CS-91-197 Carnegie-Mellon U. Department of Computer Science Tech Report. </institution>
Reference-contexts: So it seems there is no theoretical evidence that one method will improve the generalisation more than the other. In order to get an idea of which method is the best the three methods were tested on the benchmark MONK's problems described in a report made by <ref> [Thrun et al. 91] </ref>. In this report they also described 3 fully connected neural networks trained by a Back-Propagation with Weight Decay (BPWD) that outperformed all other approaches (network and rule-based systems) on these problems in an extensive machine learning competition. <p> The goal here was to find how many weights could be eliminated by the different methods while still performing as well as <ref> [Thrun et al. 91] </ref>. The result from these experiments is shown in table 5.1 and the architecture which the different methods produce on the MONK 1 problem is shown in figure 5.2.
Reference: [Widrow and Stearns 85] <author> Widrow, B. and Stearns, S.D. </author> <year> (1985), </year> <title> Adaptive Signal Processing, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address> <booktitle> []RADIAL BASIS FUNCTION NETWORKS </booktitle>
Reference: [Broomhead and Lowe 88] <author> Broomhead, D. and Lowe, D. </author> <year> (1988), </year> <title> Multi-variable Interpolation and Adaptive Networks. RSRE Memo No. </title> <type> 4148. </type> <institution> Royal Signals and Radar Establishment, Malvern. </institution>
Reference-contexts: The use of such units is similar to the technique of radial basis functions for function approximation <ref> [Broomhead and Lowe 88] </ref>. <p> In earlier works like <ref> [Broomhead and Lowe 88] </ref> the parameters and patterns (input and output) were considered as a system of equations, normally overdetermined, and they were solved by methods like the Singular Value Decomposition method (SVD) [Golub and Loan 83]. The Gradient Descent methods can also be applied.
Reference: [Hartman et al. 90] <author> Hartman, E.J., Keeler, J.D. and Kowalski, J.M. </author> <year> (1990), </year> <title> Layered Neural Networks with Gaussian Hidden Units as Universal Approximations. </title> <booktitle> Neural Computation 2, </booktitle> <pages> pp. 210-215. </pages>
Reference-contexts: The question of which approximation a neural network can implement has often been asked, and the answer is that both MLP and RBF networks can approximate any function, see [Hornik et al. 89], [Leshno et al. 93] and <ref> [Hartman et al. 90] </ref>, while a simple perceptron can solve only linearly separable ~ classification problems [Minsky and Papert 69]. 2.6 Networks Used in this Thesis Besides the three networks mentioned a number of network types will be used in this thesis.
Reference: [Moody and Darken 89] <author> Moody J.E. and Darken C.J. </author> <year> (1989), </year> <title> Fast learning in network of locally-tuned processing units, </title> <booktitle> Neural Computation 1, </booktitle> <pages> pp. 281-294. </pages>
Reference: [Musavi et al. 92] <author> Musavi, M.T., Ahmed, W., Chan, K.H., Faris, K.B. and Hummels, D.M. </author> <year> (1992), </year> <title> On the Training of Radial Basis Function Classifiers, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol 5, </volume> <pages> pp. 595-603, </pages> <year> 1992. </year>
Reference: [Poggio and Girosi 90a] <author> Poggio, T. and Girosi, F. </author> <year> (1990), </year> <title> Networks for Approximation and Learning. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> Vol 78, No. 9, </volume> <month> September </month> <year> 1990, </year> <pages> pp. </pages> <address> 1481 -1497. 106 BIBLIOGRAPHY </address>
Reference: [Poggio and Girosi 90b] <author> Poggio, T. and Girosi, F. </author> <year> (1990), </year> <title> Extensions of a Theory of Networks for Approximation and Learning: dimensionality reduction and clustering. </title> <journal> A.I. </journal> <note> Memo No.1167, C.B.I.P. Paper No. 44. </note> <institution> Massachusetts Institute of Technology. </institution> <month> April </month> <year> 1990. </year> <pages> 17 pages. </pages>
Reference-contexts: One of the advantages of this approach is that all available data can be used for training. This approach has been used in connection with RBF networks ([Poggio and Girosi 90a] <ref> [Poggio and Girosi 90b] </ref>) which will be shown in chapter 5 in connection with optimisation of the architecture. References to descriptions of alternative approaches can be found in the bibliography under the heading GENERALISATION including a Bayesian approach by [MacKay 92] 34 CHAPTER 4.
Reference: [Powell 87] <author> Powell M.J.D (1987). </author> <title> Radial basis functions for multivari-ant interpolation: a review. In Algorithms for Approximation (eds. J.C. </title> <editor> Mason and M.G. </editor> <booktitle> Cox), </booktitle> <pages> pp. 143-167. </pages> <publisher> Oxford Clarendon Press. []LEARNING AND ERROR FUNCTIONS </publisher>
Reference: [Battiti 92] <author> Battiti, R.(1992), </author> <title> First and Second-Order Methods for Learning: between Steepest descent and Newton's Method, </title> <journal> Neural Computation, </journal> <volume> Vol. 4 (2), </volume> <pages> pp. 141-167. </pages>
Reference-contexts: Several second order methods (using 2nd order Taylor approximation) have been developed (or adapted) for use with neural networks <ref> [Battiti 92] </ref>. Methods like the Quick-Prop 2 developed by [Fahlman 89] and the Conjugate Gradient are becoming increasingly popular. The Scaled Conjugate Gradient algorithm developed by [Moller 93a] is a variation of a standard Conjugate Gradient ~ algorithm.
Reference: [Buntine and Weigend 91a] <author> Buntine, W. and Weigend, A. </author> <year> (1991), </year> <title> Calculating Second Derivatives on Feed-Forward Networks, </title> <note> submitted to IEEE Transactions on Neural Networks. </note>
Reference: [Buntine and Weigend 91b] <author> Buntine, W.L. and Weigend, A.S. </author> <year> (1991), </year> <title> Bayesian BackPropagation, </title> <journal> Complex Systems, </journal> <volume> Vol. 5, </volume> <pages> pp. 603-643. </pages>
Reference-contexts: But when the training set is small, this approximation can be poor <ref> [Buntine and Weigend 91b] </ref>, and it is necessary to impose constraints on the network solutions. This is in a Bayesian perspective the same as choosing appropriate priors which is also strongly related to penalty terms or regularisers in statistical literature, see chapter 5. <p> In [MacKay 92], [MacKay 91] and <ref> [Buntine and Weigend 91b] </ref> a Bayesian approach was used to come up with various rules for changing the weights, including changing the decay rate dynamically during learning. 5.2.2 Early Stopping Another way of constraining the parameters is to stop the training before the network has made use of many of its
Reference: [Depenau and Moller 95] <author> Depenau, J. and Moller, M. </author> <year> (1995), </year> <title> The LMS-oe Error Function, </title> <booktitle> Proceedings from the World Congress on Neural Networks, </booktitle> <volume> Vol I, </volume> <pages> pp. 614-617, </pages> <address> Washington 1995. </address>
Reference: [Hampshire 92] <author> Hampshire, J.B. </author> <year> (1992), </year> <title> A Differential Theory of Learning for Statistical Pattern Recognition with Connectionist Models, </title> <type> Ph.D. Thesis, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: In order to illustrate a problem with the mean square error function, consider a network with two output units having outputs between 0 and 1. The outputs are mapped onto an V L1 -axis and a V L2 -axis respectively as shown by the simple illustration <ref> [Hampshire 92] </ref> in figure 3.3. If the desired target pattern is (1 0) then all outputs to the right of the line V L2 = V L1 in figure 3.3. can be considered to be correct. <p> The main idea is to incorporate appropriate constraints into the error function, so that the weights are constrained away from bad regions in weight space. A way to avoid suboptimal solutions is to strictly minimise the number of misclas-sifications. Hampshire defines such an approach that works for classification problems <ref> [Hampshire 92] </ref>. A more general approach that involves a soft minimisation of misclassi-fications is presented here.
Reference: [Hampshire and Waibel 90] <author> Hampshire, J.B. and Waibel, A.H. </author> <year> (1990), </year> <title> A Novel Objective Function for Improved Phoneme Recognition Using Time-Delay Neural Networks, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> Vol. 1, No. 2, </volume> <pages> pp. 216-228. </pages>
Reference: [Hornik et al. 89] <author> Hornik, K., Stinchcombe, M. and White, H. </author> <year> (1989). </year> <title> Multilayer Feedforward Networks Are Universal Approximators. </title> <booktitle> Neural Networks 2, </booktitle> <pages> pp. 359-366. </pages>
Reference-contexts: The question of which approximation a neural network can implement has often been asked, and the answer is that both MLP and RBF networks can approximate any function, see <ref> [Hornik et al. 89] </ref>, [Leshno et al. 93] and [Hartman et al. 90], while a simple perceptron can solve only linearly separable ~ classification problems [Minsky and Papert 69]. 2.6 Networks Used in this Thesis Besides the three networks mentioned a number of network types will be used in this thesis. <p> Gallant argues that such units would be feature detectors and using enough of these units the task will eventually be solved. In theory it can be shown that this is true (see [Minsky and Papert 69] for the boolean case and <ref> [Hornik et al. 89] </ref> for the general case), but it may unfortunately require an exponential large number of hidden units. Gallant does not specify the features, they are just some arbitrary features.
Reference: [Johansson et al. 91] <author> Johansson, E. M., Dowla, F.U. and Goodman D.M. </author> <year> (1991), </year> <title> Backpropagation Learning for Multi-Layer Feed-Forward Neural Networks Using the Conjugate Gradient Method, </title> <journal> International Journal of Neural Systems, </journal> <volume> Vol. 2, No. 4, </volume> <pages> pp. 291-301. BIBLIOGRAPHY 107 </pages>
Reference: [Kocabas 91] <author> Kocabas, S. </author> <year> (1991), </year> <title> A review of learning, </title> <journal> The Knowledge Engineering Review, </journal> <volume> Vol 6:3, </volume> <year> 1991, </year> <pages> pp. 195-222. </pages>
Reference-contexts: NEURAL NETWORKS Chapter 3 Learning Training is the process where the network's weights are adjusted in order to establish the desired input-output function. Training is one example of "machine learning", for a survey of other machine learning techniques see <ref> [Kocabas 91] </ref>. This chapter includes a description of different methods/techniques used in connection with learning for the three feed-forward neural networks: simple perceptron, RBF network and MLP.
Reference: [Korning 94] <author> Korning, P.G. </author> <title> (1994) Training of Neural Networks by means of Genetic Algorithms Working on very long Chromosomes Daimi PB-486, </title> <institution> Computer Science Department, Aarhus University. </institution>
Reference-contexts: They also conclude that GAs are only able to work with less complex networks (&lt; 50 units). Recent work at the Computer Science Department at Aarhus University by <ref> [Korning 94] </ref> has confirmed this conclusion, but [Schmidt and Stidsen 94] have also shown that by constraining the value of the weights and using weight sharing it is possible to get good performances. 3.3 Soft-Monotonic Error Function Error functions like mean square are known to be Bayes optimal ~ in the
Reference: [Leshno et al. 93] <author> Leshno, M., Lin, V.Y., Pinkus, A. and Schocken, S. </author> <year> (1993), </year> <title> Multilayer Feedforward Networks With a Nonpolynomial Activation Function Can Approximate Any Function. </title> <booktitle> Neural Networks, </booktitle> <volume> Vol 6, </volume> <pages> pp. 861-867. </pages>
Reference-contexts: The question of which approximation a neural network can implement has often been asked, and the answer is that both MLP and RBF networks can approximate any function, see [Hornik et al. 89], <ref> [Leshno et al. 93] </ref> and [Hartman et al. 90], while a simple perceptron can solve only linearly separable ~ classification problems [Minsky and Papert 69]. 2.6 Networks Used in this Thesis Besides the three networks mentioned a number of network types will be used in this thesis.
Reference: [Makram-Ebeid et al. 89] <author> Mackram-Ebeid, S., Surat, J.A. and Viola, J. </author> <year> (1989), </year> <title> A Rationalized Backpropagation Learning Algorithm, </title> <booktitle> In proceedings of the International Joint Conference on Neural Networks, Washington 1989, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 373-380, </pages> <address> New York: </address> <publisher> IEEE. </publisher>
Reference: [Morgan and Bourlard 90] <author> Morgan, N. and Bourlard, H. </author> <year> (1990), </year> <title> Generalization and parameter estimation in feedforward nets: some experiments, In Neural Information Processing Systems II, </title> <editor> Ed. D.S. Touretsky, </editor> <publisher> Morgan Kaufmann, NIPS II, </publisher> <pages> pp. 630-637. </pages>
Reference-contexts: This philosophy is often referred to as Ockham's Razor ~ . 7 This seems to be plausible and agrees with an often used rule of thumb, called Uncle Bernie's rule in <ref> [Morgan and Bourlard 90] </ref> after Bernard Widrow [Widrow 87]. 42 CHAPTER 4. GENERALISATION Chapter 5 Regularisation and Pruning Improving Predetermined Architectures This chapter describes several methods to improve the generalisation ability of a neural network with a predetermined architecture containing many parameters.
Reference: [Muselli 95] <author> Muselli, M. </author> <year> (1995), </year> <title> Is Pocket Algorithm Optimal, </title> <booktitle> Lecture Notes in Artificial Intelligence 904, Computational Learning Theory, Second European Conference, </booktitle> <address> EuroCOLT'95. Barcelona, Spain, </address> <month> March </month> <year> 1995. </year> <title> Proceedings. </title> <publisher> Springer. Paul Vitanyi (Ed). </publisher> <pages> pp. 287-297. </pages>
Reference-contexts: In <ref> [Muselli 95] </ref> it is shown that this only is true under certain conditions. 3.2. TRAINING OF FEED-FORWARD NETWORK 15 the variables from one point in the parameter space to another and eventually end up at a point where the function is at its minimum.
Reference: [Moller 90a] <author> Moller, M. </author> <year> (1990), </year> <title> CM Algoritmen, </title> <type> Masters Thesis in Danish, </type> <institution> Daimi IR-95, Computer Science Department, Aarhus University. </institution>
Reference: [Moller 90b] <author> Moller, M. </author> <year> (1990), </year> <title> Learning by Conjugate Gradients, </title> <booktitle> In Proceedings of the Sixth International Meeting of Young Computer Scientist, </booktitle> <publisher> LNCS 464, Springer Verlag, </publisher> <address> New York, </address> <pages> pp. 184-195. </pages>
Reference: [Moller 93a] <author> Moller, M. </author> <year> (1993), </year> <title> A Scaled Conjugate Gradient Algorithm for Fast Supervised Learning, Neural Networks, </title> <journal> June, </journal> <volume> Vol. 6, No. 4, </volume> <pages> pp. 525-533. </pages>
Reference-contexts: Several second order methods (using 2nd order Taylor approximation) have been developed (or adapted) for use with neural networks [Battiti 92]. Methods like the Quick-Prop 2 developed by [Fahlman 89] and the Conjugate Gradient are becoming increasingly popular. The Scaled Conjugate Gradient algorithm developed by <ref> [Moller 93a] </ref> is a variation of a standard Conjugate Gradient ~ algorithm. The major idea of Conjugate Gradient algorithms is that up to second order they produce non-interfering directions of search. <p> SCG exhibits second order convergence properties and contains no problem dependent parameters. See <ref> [Moller 93a] </ref> for a detailed description of SCG. 3.2.7 Training by Genetic Algorithms Genetic Algorithms (GAs) ~ are general search techniques, based on biological principles ([Goldberg 89] [Holland 75]). In many cases GAs have shown to be effective adaptive techniques for optimisation. <p> An MLP with two hidden layers containing 32 units in the first layer and 16 in the second, i.e. 322 weights plus 25 thresholds was designed. The training was done by the Scaled Conjugate Gradient method, see <ref> [Moller 93a] </ref>. Two similar experiments were performed with the only difference that the error functions were the Soft-Monotonic error function and the M S oe error function described in chapter 3. The classification errors were 10.3% and 10.9% which is only a minor difference.
Reference: [Moller 93b] <author> Moller, M. </author> <year> (1993), </year> <title> Supervised Learning on Large Redundant Training sets, </title> <journal> International Journal of Neural Systems, </journal> <volume> Vol. 4, No. 1, </volume> <pages> pp. 15-25. 108 BIBLIOGRAPHY </pages>
Reference: [Moller 93c] <author> Moller, M. </author> <year> (1993), </year> <title> Exact Calculation of the Product of the Hessian Matrix of Feed-Forward Network Error Functions and a Vector in O(N ) Time, </title> <type> Technical Report, </type> <institution> Daimi PB-432, Computer Science Department, Aarhus University. </institution>
Reference: [Moller 93d] <author> Moller, M. </author> <year> (1993), </year> <title> Adaptive Preconditioning of the Hessian Matrix, </title> <note> submitted to Neural Computation. </note>
Reference: [Moller 93e] <author> Moller, M. </author> <year> (1993), </year> <title> Efficient Training of Feed-Forward Neural Networks. </title> <type> Ph.D. Thesis, </type> <institution> Daimi PB-464, Computer Science Department, Aarhus University. </institution>
Reference-contexts: This is especially true of problems where the targets only have to be approximated such as classification problems. A nice theoretical discussion about issues concerning on-line and off-line techniques can be found in <ref> [Moller 93e] </ref>(pp. 44-52). In practice there is sometimes a problem with the on-line methods when patterns from different classes are very differently distributed. <p> LEARNING in the error function. One possible approach is to define an error function that penalises errors of large magnitude. The first approach is called the Exponential Error function (see e.g. [Moller et al. 94] or <ref> [Moller 93e] </ref> where it is first described), defined as: E ( w) = 2 p;j j t p j +fiV p where ff and fi are positive parameters.
Reference: [Moller and Depenau 94] <author> Moller, M. and Depenau, J. </author> <year> (1994), </year> <title> Soft-Monotonic Error Functions, </title> <booktitle> Proceedings from the World Congress on Neural Networks, </booktitle> <volume> Vol 3, </volume> <pages> pp. 444-449, </pages> <address> San Diego 1994. </address>
Reference: [Schiffmann et al.92] <author> Schiffmann, W. Joost, M. and Werner R. </author> <year> (1992), </year> <title> Synthesis and Performance Analysis of Multilayer Neural Network Architectures. </title> <institution> Technical Report 16/1992 from University of Koblenz, Institute fur Physics. </institution>
Reference-contexts: GAs are suitable for finding "good areas" in the weight space, because GAs are global methods. A review of some of the work done by using GAs to adjust the weights can be found in e.g. <ref> [Schiffmann et al.92] </ref>. They also conclude that GAs are only able to work with less complex networks (&lt; 50 units).
Reference: [Schmidt and Stidsen 94] <author> Schmidt, M. and Stidsen, T. </author> <year> (1995), </year> <title> Using Genetic Algorithms to Train Neural Networks using weight sharing, weight pruning and unit pruning, </title> <institution> Computer Science Department, Aarhus University. </institution>
Reference-contexts: They also conclude that GAs are only able to work with less complex networks (&lt; 50 units). Recent work at the Computer Science Department at Aarhus University by [Korning 94] has confirmed this conclusion, but <ref> [Schmidt and Stidsen 94] </ref> have also shown that by constraining the value of the weights and using weight sharing it is possible to get good performances. 3.3 Soft-Monotonic Error Function Error functions like mean square are known to be Bayes optimal ~ in the sense that minimisation with these functions produces
Reference: [Seber and Wild 89] <author> Seber, G.A.F. and Wild, C.J. </author> <year> (1989), </year> <title> Non-linear Regression, </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference-contexts: This gives a sort of balanced distribution of errors. For regression problems it is well known in statistics that a balanced set of errors can yield better generalisation, this is often referred to as variance heterogeneity <ref> [Seber and Wild 89] </ref>. It is an open question whether this is also true in connection with classification problems. When ff increases to infinity, the exponential error function is monotonic.
Reference: [Solla et al. 88] <author> Solla, S.A., Levin, E. and Fleisher, M. </author> <year> (1988), </year> <title> Accelerated Learning in Layered Neural Networks, </title> <journal> Complex Systems, </journal> <volume> Vol. 2, </volume> <pages> pp. </pages> <month> 625-639. </month> <title> []NEURAL NETWORK AND THE QUESTION OF GENERALISATION </title>
Reference: [Baum and Haussler 89] <author> Baum, E.B. and Haussler, D. </author> <year> (1989), </year> <title> What Size Net Gives Valid Generalization? Neural Computation 1, </title> <journal> pp. </journal> <pages> 151-160. </pages>
Reference-contexts: The weights between the new unit and the input are then frozen (never changed) and the output from the new unit is connected to the output unit. A few examples are illustrated in figure 4.7. 6 The estimation of the growth function 4 F (P ) in <ref> [Baum and Haussler 89] </ref> is set equal to the product of all 4 F i (P ) from each hidden unit, 4 F (P ) = Q N 1 i=1 4 F i (P ).
Reference: [Blumer et al. 86] <author> Blumer, A., Ehrenfeucht, A., Haussler, D. and Warmuth, M. </author> <year> (1986,) </year> <title> Learnability and the Vapnik-Chervonenkis Dimension. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 36, No. 4, </volume> <month> October </month> <year> 1989, </year> <pages> pp. 929-965. </pages>
Reference-contexts: For a VC dim smaller than the optimal VC dim the resources of the network will not be sufficient for learning the training set. Although the confidence interval is small, indicating a good correlation between the 5 In [Vapnik 82]and <ref> [Blumer et al. 86] </ref> there is an alternative and often used exposition of inequality (4.7) given by: P ( sup E G (W) E L (W) E G (W) where 4 () is the growth function. 36 CHAPTER 4. <p> Valiant's work was defined in very broad terms and restricted to the Boolean functions (B N ! B where B = f1; 1g). It was later extended to learning functions, taken from a class of functions defined by regions in the Euclidean n-dimensional space by <ref> [Blumer et al. 86] </ref>.
Reference: [Bottou et al. 94] <author> Bottou, L., Cortes, C. and Vapnik, V. </author> <year> (1994). </year> <title> On the Effective VC-Dimension. Found on connectionist ftp list. </title> <type> 12 pages. BIBLIOGRAPHY 109 </type>
Reference-contexts: The field is slowly growing, but the big breakthrough is still to come. One of the newest ideas of making the VC theory more useful is an attempt to measure the VC dim [Vapnik et al. 94] and <ref> [Bottou et al. 94] </ref> from experiments. So far without convincing results.
Reference: [Cover 65] <author> Cover T.M. </author> <year> (1965), </year> <title> Geometrical and Statistical Properties of Linear Inequalites with Applications in Pattern Recognition, </title> <journal> IEEE Transactions on Electronic Computers 14, </journal> <pages> pp. 326-334. </pages>
Reference-contexts: Mathematical proofs of the bounding function in the case of general linear classifiers can be found in [Hertz et al. 91], <ref> [Cover 65] </ref> and [Nilsson 89]. From this description one could get the idea that the capacity is simply equal to the number of adjustable weights and as such proportionally related to the complexity. <p> Since * 2 is equal 4 times the confidence interval it must be small in order to get a good generalisation ability, so the number of training patterns needed can be expected to be high. 4.6. BOUNDS TO THE VC-DIMENSION 39 In <ref> [Cover 65] </ref> it is further shown that if the transfer function is changed from a threshold function to a pulse transfer function (RBF unit) the VC dim will be N 0 + 2 (number of weights including thresholds and radius). 4.6.2 V C dim for a Two-Layer Feed-Forward Network The exact
Reference: [Denker et al. 87] <author> Denker, J., Schwartz, D., Wittner, B., Solla, S., Howard, R., Jackel, L. and Hopfield, J. </author> <year> (1987), </year> <title> Large Automatic Learning, Rule Extraction, and Generalization, </title> <booktitle> Complex Systems 1, </booktitle> <pages> pp. 877-922. </pages>
Reference-contexts: So far nothing is said about the values of these parameters. It seems likely that there will be several weight constellations w i from w that could implement the desired function. This was confirmed by Denker et al.(1988) <ref> [Denker et al. 87] </ref> in what they call a perturbation analysis. They took a well working network (E L = 0), and perturbed the network, moving the weights to a new point in the weight space, and re-trained it.
Reference: [Fogel 91] <author> Fogel, D.B. </author> <year> (1991), </year> <title> An Information Criterion for Optimal Neural Network Selection. </title> <journal> IEEE Transactions on Neural Network, </journal> <volume> Vol. 2. No. 3. </volume> <month> September </month> <year> 1991. </year>
Reference-contexts: References to descriptions of alternative approaches can be found in the bibliography under the heading GENERALISATION including a Bayesian approach by [MacKay 92] 34 CHAPTER 4. GENERALISATION [MacKay 91], Akaike's Final Prediction Error by <ref> [Fogel 91] </ref>, [Rasmussen 93], Expected Test Set Error and Risk Minimisation by [Moody 92],[Moody 94], [Fogel 91] and Minimum Description Length Approach by [Rissanen 84]. 4.5 Theoretical Analysis of Generalisation In the field of theoretical analysis of generalisation there are only two approaches that have made a significant impact to date: <p> References to descriptions of alternative approaches can be found in the bibliography under the heading GENERALISATION including a Bayesian approach by [MacKay 92] 34 CHAPTER 4. GENERALISATION [MacKay 91], Akaike's Final Prediction Error by <ref> [Fogel 91] </ref>, [Rasmussen 93], Expected Test Set Error and Risk Minimisation by [Moody 92],[Moody 94], [Fogel 91] and Minimum Description Length Approach by [Rissanen 84]. 4.5 Theoretical Analysis of Generalisation In the field of theoretical analysis of generalisation there are only two approaches that have made a significant impact to date: the approach based on statistical physics 4 and the approach based on computational learning theory.
Reference: [Geman et al. 92] <author> Geman, S., Bienenstock, E. and R. </author> <month> Doursat </month> <year> (1992), </year> <title> Neural Networks and the Bias/Variance Dilemma. </title> <booktitle> Neural Computation 4, </booktitle> <pages> pp. 1-58. </pages>
Reference-contexts: CAPACITY AND COMPLEXITY 29 and weights, and thereby the network's architecture. It is therefore not surprising that the network's architecture has an influence on the generalisation ability. There are many references with a similar approach to explain generalisation, e.g. <ref> [Geman et al. 92] </ref> providing an excellent link between the intuitive approach and a more mathematical description from a statistical viewpoint. 4.2 Capacity and Complexity In the previous paragraph the connection between the performance of a neural network and its complexity and capacity was indicated. <p> Neural nets do not take into consideration or make any assumption about the statistical properties of the data, that is one of their strengths 4 . 4 Some would argue that this is not the case, see for example <ref> [Geman et al. 92] </ref> 7.4. NEURAL NETWORKS FOR CLASSIFICATION OF ICE 85 Therefore it makes no sense not to use the data in a better way. <p> On the other hand, neural networks have shown some difficulties in extrapolating information, and it might be necessary to enforce some bias <ref> [Geman et al. 92] </ref>. Also, the 9 x 9 window in general represents a much smaller number of pixels than the original segments. For each of the two network types P ERCEP and M LP (25) about 20 different networks were implemented.
Reference: [Girard 89] <author> Girard, D.A. </author> <year> (1989), </year> <title> A Fast 'Monte-Carlo Cross-Validation' Procedure for Large Least Squares Problems with Noisy Data, </title> <journal> Numer. Math., </journal> <volume> Vol. 56, </volume> <pages> pp. 1-23. </pages>
Reference: [Hinton 89] <author> Hinton, G. </author> <year> (1989), </year> <title> Connectionist Learning Procedures, </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 40, </volume> <pages> pp. 185-234. </pages>
Reference: [Judd 87] <author> Judd, J.S. </author> <title> (1987),Complexity of connectionist learning with various node functions, </title> <type> COINS Technical Report 87-60, </type> <institution> University of Amherst, </institution> <address> Amherst, MA. </address>
Reference: [Le Cun 89] <author> Le Cun, Y. </author> <year> (1989). </year> <title> Generalization and Network Design Strategies, In Connectionism in Perspective, </title> <editor> Eds. R. Pfeifer, Z. Schleter, F. Fogelmann and L. Steels, </editor> <address> Zurich, </address> <publisher> Elsevier. </publisher>
Reference: [MacKay 91b] <author> MacKay, D.J.C. </author> <year> (1991), </year> <title> A Practical Bayesian Framework for Back-Prop Networks, </title> <journal> Neural Computation, </journal> <volume> Vol. 4, No. 3, </volume> <pages> pp. 448-472. </pages>
Reference: [MacKay 92] <author> MacKay, D.J.C. </author> <year> (1992), </year> <title> Information-Based Objective Functions for Active Data Selection, </title> <journal> Neural Computation, </journal> <volume> Vol. 4, </volume> <pages> pp. 590-604. </pages>
Reference-contexts: References to descriptions of alternative approaches can be found in the bibliography under the heading GENERALISATION including a Bayesian approach by <ref> [MacKay 92] </ref> 34 CHAPTER 4. <p> For networks with sigmoid transfer functions, the use of simple Weight Decay is equivalent to incorporation of prior knowledge of the best possible choice of network is a network working as a linear model (small weights make the units work in their linear areas). In <ref> [MacKay 92] </ref>, [MacKay 91] and [Buntine and Weigend 91b] a Bayesian approach was used to come up with various rules for changing the weights, including changing the decay rate dynamically during learning. 5.2.2 Early Stopping Another way of constraining the parameters is to stop the training before the network has made
Reference: [MacKay 91] <author> MacKay, D.J.C. </author> <year> (1991), </year> <title> Bayesian Interpolation, </title> <journal> Neural Computation, </journal> <volume> Vol. 4, No. 3, </volume> <pages> pp. 415-447. </pages>
Reference-contexts: References to descriptions of alternative approaches can be found in the bibliography under the heading GENERALISATION including a Bayesian approach by [MacKay 92] 34 CHAPTER 4. GENERALISATION <ref> [MacKay 91] </ref>, Akaike's Final Prediction Error by [Fogel 91], [Rasmussen 93], Expected Test Set Error and Risk Minimisation by [Moody 92],[Moody 94], [Fogel 91] and Minimum Description Length Approach by [Rissanen 84]. 4.5 Theoretical Analysis of Generalisation In the field of theoretical analysis of generalisation there are only two approaches that <p> For networks with sigmoid transfer functions, the use of simple Weight Decay is equivalent to incorporation of prior knowledge of the best possible choice of network is a network working as a linear model (small weights make the units work in their linear areas). In [MacKay 92], <ref> [MacKay 91] </ref> and [Buntine and Weigend 91b] a Bayesian approach was used to come up with various rules for changing the weights, including changing the decay rate dynamically during learning. 5.2.2 Early Stopping Another way of constraining the parameters is to stop the training before the network has made use of
Reference: [Moody 92] <author> Moody, J.E. </author> <year> (1992), </year> <title> The effective number of parameters: an analysis of generalization and regularization in nonlinear learning systems, </title> <booktitle> In Neural Information Processing Systems, </booktitle> <editor> Ed. Cowan and Giles, </editor> <publisher> Morgan Kaufmann, Vol. </publisher> <address> 4. 110 BIBLIOGRAPHY </address>
Reference-contexts: References to descriptions of alternative approaches can be found in the bibliography under the heading GENERALISATION including a Bayesian approach by [MacKay 92] 34 CHAPTER 4. GENERALISATION [MacKay 91], Akaike's Final Prediction Error by [Fogel 91], [Rasmussen 93], Expected Test Set Error and Risk Minimisation by <ref> [Moody 92] </ref>,[Moody 94], [Fogel 91] and Minimum Description Length Approach by [Rissanen 84]. 4.5 Theoretical Analysis of Generalisation In the field of theoretical analysis of generalisation there are only two approaches that have made a significant impact to date: the approach based on statistical physics 4 and the approach based on
Reference: [Moody 94] <author> Moody, J.E. </author> <year> (1994), </year> <title> Prediction Risk and Architecture Selection for Neural Networks, Found on connectionist ftp list. To Appear in From Statistics to Neural Networks: Theory and Pattern Recognition Applications, </title> <editor> V. Cherkassky, J.H. Friedman and H. Wechsler (eds.), </editor> <booktitle> NATO ASI Series F, </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: [Tishby et al. 88] <author> Tishby, N., Levin, E. and Solla, S.A. </author> <year> (1989), </year> <title> Consistent Inference of Probabilities in Layered Neural Networks: Predictions and Generalization, </title> <booktitle> IJCNN International Joint Conference on Neural Networks. </booktitle> <volume> Vol II. </volume> <pages> pp. 403-409. </pages>
Reference: [Rasmussen 93] <author> Rasmussen, C.E.. </author> <year> (1993), </year> <title> Generalisation in Neural Networks. </title> <institution> Master Thesis from Electronics Institute, Technical University of Denmark, Lyngby, </institution> <year> 1993. </year> <pages> 83 pages 83. </pages>
Reference-contexts: References to descriptions of alternative approaches can be found in the bibliography under the heading GENERALISATION including a Bayesian approach by [MacKay 92] 34 CHAPTER 4. GENERALISATION [MacKay 91], Akaike's Final Prediction Error by [Fogel 91], <ref> [Rasmussen 93] </ref>, Expected Test Set Error and Risk Minimisation by [Moody 92],[Moody 94], [Fogel 91] and Minimum Description Length Approach by [Rissanen 84]. 4.5 Theoretical Analysis of Generalisation In the field of theoretical analysis of generalisation there are only two approaches that have made a significant impact to date: the approach
Reference: [Rissanen 84] <author> Rissanen, J. </author> <year> (1984), </year> <title> Universal Coding, Information , Prediction, and Estimation, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 30, No. 4, </volume> <pages> pp. 629-636. </pages>
Reference-contexts: GENERALISATION [MacKay 91], Akaike's Final Prediction Error by [Fogel 91], [Rasmussen 93], Expected Test Set Error and Risk Minimisation by [Moody 92],[Moody 94], [Fogel 91] and Minimum Description Length Approach by <ref> [Rissanen 84] </ref>. 4.5 Theoretical Analysis of Generalisation In the field of theoretical analysis of generalisation there are only two approaches that have made a significant impact to date: the approach based on statistical physics 4 and the approach based on computational learning theory.
Reference: [Sporrin 95] <author> Sporring, J. </author> <year> (1995), </year> <title> Statistical Aspects of Generalisation in Neural Networks. </title> <institution> Master Thesis from Department of Computer Science, University of Copenhagen, </institution> <address> Denmark. 54 pages4. </address>
Reference: [Tishby 95] <author> Tishby, N. </author> <year> (1995), </year> <title> Statistical Physics Models of Supervised Learning, The Mathematics of Generalisation, Proceedings of the SFI/CNLS Workshop on Formal Approaches to Supervised Learning, </title> <editor> Ed. D.H. </editor> <booktitle> Wolpert. </booktitle> <pages> pp. 215-242. </pages> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Inspired by the Bernoulli theorem, Vapnik and Chervonenkis (in their most famous paper [Vapnik and Chervonenskis 71]) and Vapnik ([Vapnik 82], [Vapnik 92]) defined the uniform convergence in the following way: 4 For those who would like to know more about the statistical physics approach, [Hertz et al. 91], <ref> [Tishby 95] </ref> including references, provide a good introduction. 4.5.
Reference: [Valiant 84] <author> L.G. </author> <title> Valiant (1994) A Theory of the Learnable. </title> <journal> Communications of the AMC. </journal> <volume> Vol. 27. No. 11, </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: THEORETICAL ANALYSIS OF GENERALISATION 37 perturbation. In fact, it moved in some other direction and settled on a new point w i in the weight space. 4.5.2 Probably Approximately Correct Learning Theory The PAC ~ learning theory was introduced by Valiant <ref> [Valiant 84] </ref> in 1984, where he defines a notation of learnability.
Reference: [Vapnik and Chervonenskis 71] <author> Vapnik, V.N. and A.YA. Chervonenkis, A.YA. </author> <year> (1971), </year> <title> On the Uniform Convergence of Realative Frequencies of Events to Their Probabilities. </title> <journal> Theory Prob. Appl. </journal> <volume> 16. </volume> <pages> pp. 264-280. </pages>
Reference-contexts: So E G (W) is the probability of error, and E L (W) is the frequency of error on the training set. Inspired by the Bernoulli theorem, Vapnik and Chervonenkis (in their most famous paper <ref> [Vapnik and Chervonenskis 71] </ref>) and Vapnik ([Vapnik 82], [Vapnik 92]) defined the uniform convergence in the following way: 4 For those who would like to know more about the statistical physics approach, [Hertz et al. 91], [Tishby 95] including references, provide a good introduction. 4.5.
Reference: [Vapnik 82] <author> Vapnik, V.N. </author> <year> (1982). </year> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference-contexts: The figure shows that the growth function 4 (P ) = 2 P will be valid until the number of patterns exceeds the VC dim whereupon it becomes polynomial. In <ref> [Vapnik 82] </ref> it is shown that a bounding function for the growth function when P VC dim is: 4 (P ) 1:5 VC dim ! The graph of the growth function is not only valid for the two input classifiers, but 3 This example is the well-known XOR problem, which cannot <p> It includes a description of necessary and sufficient conditions as well as bounds to the rate of convergence <ref> [Vapnik 82] </ref> (chapter 6). These bounds, which are independent of the distribution function P (V 0 ; T), are based on a quantitative measure of the capacity of the set of functions implemented by the network: the VC-dimension of the set. <p> For a VC dim smaller than the optimal VC dim the resources of the network will not be sufficient for learning the training set. Although the confidence interval is small, indicating a good correlation between the 5 In <ref> [Vapnik 82] </ref>and [Blumer et al. 86] there is an alternative and often used exposition of inequality (4.7) given by: P ( sup E G (W) E L (W) E G (W) where 4 () is the growth function. 36 CHAPTER 4.
Reference: [Vapnik 92] <author> Vapnik, V.N. </author> <year> (1992). </year> <title> Principles of Risk Minimization for Learning Theory Advances in Neural Information Processing Systems IV (Denver 1992). </title> <editor> ed. J.E.Moody et al., </editor> <address> 831-838. San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: So E G (W) is the probability of error, and E L (W) is the frequency of error on the training set. Inspired by the Bernoulli theorem, Vapnik and Chervonenkis (in their most famous paper [Vapnik and Chervonenskis 71]) and Vapnik ([Vapnik 82], <ref> [Vapnik 92] </ref>) defined the uniform convergence in the following way: 4 For those who would like to know more about the statistical physics approach, [Hertz et al. 91], [Tishby 95] including references, provide a good introduction. 4.5.
Reference: [Vapnik et al. 94] <author> Vapnik, V., Levin, E. and LeCun, Y. </author> <year> (1994). </year> <title> Measuring the VC-Dimension of a Learning Machine, </title> <journal> Neural Computation, </journal> <volume> Vol. 6, </volume> <pages> pp. 851-876. BIBLIOGRAPHY 111 </pages>
Reference-contexts: The field is slowly growing, but the big breakthrough is still to come. One of the newest ideas of making the VC theory more useful is an attempt to measure the VC dim <ref> [Vapnik et al. 94] </ref> and [Bottou et al. 94] from experiments. So far without convincing results.
Reference: [Wan 90] <author> Wan, E.A. </author> <year> (1990), </year> <title> Neural Network Classification: A Bayesian Interpretation, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> Vol. 1 (4), </volume> <pages> pp. 303-305. </pages>
Reference: [Wieland and Leighton 87] <author> Wieland, A. and Leighton, R. </author> <year> (1987), </year> <title> Geometric analysis of neural networks capabilities In proc. </title> <booktitle> 1st Int. Conf. on Neural Nets, IEEE, </booktitle> <address> San Diego, Ca, </address> <month> June </month> <year> 1987, </year> <note> Vol. 2. </note>
Reference-contexts: Although they might look different and are trained differently, they are all feed-forward networks. A further geometric description and interpretation of some of these networks and other types can be found in <ref> [Wieland and Leighton 87] </ref>, [Lippmann 87], [Huang and Lippmann 87], [DARPA 88] (chapter 6), [Gibson and Cowan 90] and [Huang and Huang 91]. 2.6.
Reference: [Widrow 87] <editor> Widrow B (1987), ADALINE and MADALINE, </editor> <booktitle> Plenary Speech, Vol 1 Proc. IEEE 1st Intl. Conf. on Neural Networks, </booktitle> <address> San Diego, Ca, </address> <pages> pp. 143-158. </pages>
Reference-contexts: This philosophy is often referred to as Ockham's Razor ~ . 7 This seems to be plausible and agrees with an often used rule of thumb, called Uncle Bernie's rule in [Morgan and Bourlard 90] after Bernard Widrow <ref> [Widrow 87] </ref>. 42 CHAPTER 4. GENERALISATION Chapter 5 Regularisation and Pruning Improving Predetermined Architectures This chapter describes several methods to improve the generalisation ability of a neural network with a predetermined architecture containing many parameters.
Reference: [Wolpert 95] <author> Wolpert, D. </author> <year> (1995), </year> <title> The Mathematics of Generalization. Proceedings of the SFI/CNLS Workshop on Formal Approaches to Supervised Learning. </title> <publisher> Addison Wesley: </publisher> <pages> pp. 115 - 162. </pages> <institution> []NETWORK ARCHITECTURES </institution>
Reference: [Alpaydin 91] <author> Alpaydin, E. </author> <year> (1991): </year> <title> GAL : Networks that grow when they learn and shrink when they forget, </title> <type> TR 91-032, </type> <institution> International Computer Science Institute. </institution> <month> May </month> <year> 1991. </year>
Reference: [Arentoft 95] <author> Arentoft, M. and Flink, M. </author> <year> (1995), </year> <title> Optimising of net-structure in feed-forward network by means of genetic algorithms. Report to Neural Network Studygroup (in Danish), </title> <institution> Computer Science Department, Aarhus University. </institution>
Reference-contexts: GAs have shown promising results, but similar to the case where they were used for training the network, see paragraph 3.2.7, their practicability is limited to smaller problems. Recent work carried out at the Computer Science Department of Aarhus University by <ref> [Arentoft 95] </ref> has confirmed this tendency. 74 CHAPTER 6. CONSTRUCTION ALGORITHMS Chapter 7 Classification of Ice The estimation of ice type concentrations from Synthetic Aperture Radar (SAR) images has been investigated for several years, see e.g. [Skriver 94].
Reference: [Carpenter and Grossberg] <author> Carpenter, G. and Grossberg S. </author> <year> (1987) </year> <month> "ART2: </month> <title> Self-organization of stable category recognition codes for analog input patterns" Applied Optics, </title> <booktitle> 26, </booktitle> <pages> 4919-4930 </pages>
Reference: [Cooper et al. 88] <author> Cooper, L.N., Reilly D.E. and Elbaum, C. </author> <year> (1988). </year> <title> Neural Network Systems, An Introduction for Managers, Decision-Makers and Strategists Nestor, Inc. One Rich-mond Square, </title> <journal> Providence, </journal> <volume> Ri 02906 (401) 331-9640. </volume> <pages> 31 pages. </pages>
Reference-contexts: Today it seems to be one of the really successful networks, as it is the heart of a learning system from the American company Nestor Inc., formed by the three inventors of the method, D.L Reilly, L.N. Cooper and C. Elbaum <ref> [Cooper et al. 88] </ref>. Many of the more recent construction algorithms using RBF units are very similar to the RCE network/methods as they are also based on the idea of category (cluster) learning, see e.g. [1] and [4].
Reference: [Depenau and Moller 94] <author> Depenau, J. and Moller, M. </author> <year> (1994), </year> <title> Aspects of Generalization and Pruning, </title> <booktitle> Proceedings from the World Congress on Neural Networks, </booktitle> <volume> Vol 3, </volume> <pages> pp. 464-469, </pages> <address> San Diego 1994. </address>
Reference: [Depenau 95] <author> Depenau, J. </author> <year> (1995), </year> <title> A Global-Local Learning Algorithm, </title> <booktitle> Proceedings from the World Congress on Neural Networks, </booktitle> <volume> Vol I, </volume> <pages> pp. 587-590, </pages> <address> Washington 1995. </address>
Reference-contexts: Time = 1 Time = End -1 Time = End evolution. 6.7 Global-Local Learning Algorithm Inspired by the previously described methods Depenau proposed in '95 a new method which applied a Global-Local approach, combining units with sigmoid and Gaussian transfer functions in the hidden layer to solve classification tasks, see <ref> [Depenau 95] </ref> (Appendix F). The Global-Local approach, or simply GLOCAL, is another supervised learning method for dynamically building and training neural networks.
Reference: [Depenau 94] <author> Depenau, J. </author> <year> (1994), </year> <title> Evaluation of the Cascade-Correlation Algorithm, </title> <type> Technical Report. 112 BIBLIOGRAPHY </type>
Reference: [Fahlman and Lebiere 90] <author> Fahlman, S.E. and C. </author> <booktitle> Lebiere (1990), The Cascade-Correlation Learning Architecture. In Advances in Neural Information Processing Systems II (Denver 1989), </booktitle> <editor> ed. D.S. Touretzky, </editor> <address> 524-532. San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In one particular case it is very important to avoid saturation: when a network is trained and built at the same time by the Cascade-Correlation Algorithm (CCA) <ref> [Fahlman and Lebiere 90] </ref>. A detailed description of the CCA can be found in chapter 6. Experiments with the CCA in Appendix E using the SCG as the learning algorithm showed that SCG combined with MS-oe error function performs much better than when combined with MSE without an offset. <p> If the network's performance is satisfying after retraining, go to 6. Otherwise: Go to 3. 6. Stop: The algorithm stops. The CCA has been reported to be very efficient in the task of learning almost any classification problem <ref> [Fahlman and Lebiere 90] </ref>. The constructed networks' ability to generalise has, however, not shown the same convincing results. The problem is that the CCA often inserts a lot of unnecessary hidden units. The reasons are among other things: 62 CHAPTER 6. <p> The output units were also trained by BP with j = 0:001 and ff = 0:9. For the sake of comparison the CCA was tested as well. The original code made by Fahlman <ref> [Fahlman and Lebiere 90] </ref> was used with the following parameters: j = 0:1, offset = 0:1, the "patience factor" = 0 and poolsize = 1. The training set contained 525 patterns and the test set 174 patterns. No validation set was used.
Reference: [Fahlman 89] <author> Fahlman, S.E. </author> <year> (1989), </year> <title> Fast Learning Variations on Backpropagation: An Empirical Study, </title> <booktitle> In proceedings of the 1988 Connectionist Models Summer School, </booktitle> <editor> Eds. D.S. Touretzky, G. Hinton and T. </editor> <booktitle> Sejnowski, </booktitle> <pages> pp. 38-51, </pages> <address> San Mateo: </address> <publisher> Morgan Kauffmann. </publisher>
Reference-contexts: Several second order methods (using 2nd order Taylor approximation) have been developed (or adapted) for use with neural networks [Battiti 92]. Methods like the Quick-Prop 2 developed by <ref> [Fahlman 89] </ref> and the Conjugate Gradient are becoming increasingly popular. The Scaled Conjugate Gradient algorithm developed by [Moller 93a] is a variation of a standard Conjugate Gradient ~ algorithm. The major idea of Conjugate Gradient algorithms is that up to second order they produce non-interfering directions of search. <p> This means that a flat spot area is reached. 3.4.1 Preventing Saturation (or Eliminating "Flat Spots") In order to prevent saturation and speed up the learning procedure, Fahlman <ref> [Fahlman 89] </ref> introduced a small offset, oe &gt; 0, that is added to the derivative.
Reference: [Finnoff et al. 93] <author> Finnoff, W., Hergert, F. and Zimmermann, H.G. </author> <title> (1993) Improving Model Selection by Nonconvergent Methods. </title> <booktitle> Neural Networks. </booktitle> <volume> Vol. 6, </volume> <pages> pp. 771-783. </pages>
Reference-contexts: The question still remains open whether one could benefit from combining the different methods. The experiment showed that Weight Decay along with pruning methods that require training and retraining is a good idea. Several successful tests are shown in <ref> [Finnoff et al. 93] </ref> where a combination of Early Stopping is used in connection with various regularisation and pruning methods, including Weight Decay and OBD. <p> In order to answer these questions further work will be needed in the areas of: Combining different methods to improve predetemined architectures. The experiment showed that Weight Decay along with pruning methods that require training and retraining is a good idea. Several successful tests are shown in <ref> [Finnoff et al. 93] </ref> where a combination of Early Stopping is used in connection with various regularisation and pruning methods, including Weight Decay and OBD. Optimal stop criteria for pruning methods.
Reference: [Frean 90] <author> Frean, M. </author> <year> (1990), </year> <title> The Upstart Algorithm: A Method for Constructing and Training Feedforward Neural Networks, </title> <journal> Neural Computation, </journal> <volume> Vol. 2, </volume> <pages> pp. 198-209. </pages>
Reference-contexts: Many of the later derived construction algorithms ~ may be interpreted as extensions of Gallant's ideas. The more popular methods derived from Gallant's algorithms include algorithms such as the Upstart algorithm <ref> [Frean 90] </ref> and the Tiling algorithm [Mezard et al. 89]. 1 Before reading this chapter it might be a good idea to take a second look at the different transfer functions in chapter 2. 53 54 CHAPTER 6.
Reference: [Fritzke 94] <author> Fritzke, B. </author> <year> (1994), </year> <title> Growing Cell Structures A Self-Organizing Network for Unsupervised and Supervised Learning, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 7, </volume> <pages> pp. 1441-1460, </pages> <year> 1994. </year>
Reference: [Gallant 86] <author> Gallant S.I. </author> <year> (1986), </year> <title> Three Constructing algorithms for Network learning, </title> <booktitle> Proc. 8th Annual Conf. of Cognitive Science Society, </booktitle> <pages> 652-660. </pages>
Reference-contexts: It is called the perceptron learning rule ~ [Rosenblatt 62]. This method is, however, restricted to being only valid if the problem is linearly separable. An extension of this method, known as the pocket algorithm ~ <ref> [Gallant 86] </ref>, is able to handle nonlinearly separable problems. <p> A survey of some constructive algorithms can be found in [1]. Gallant was one of the first to come up with several supervised learning methods for dynamically building and training neural networks using units with threshold transfer functions <ref> [Gallant 86] </ref>. Many of the later derived construction algorithms ~ may be interpreted as extensions of Gallant's ideas. <p> data can be handled. * Analytic tractability: Analytic bounds to scaling and generalisation ability can be derived in many cases, thanks to the simplicity and modular structure of the underlying model, see chapter 4. 6.2 Simple Construction Algorithms Three of the most simple construction algorithms were proposed by Gallant in <ref> [Gallant 86] </ref>. The following description of these methods serves as an introduction to the field because they are easy to understand and contain many of the terms and ideas which are common to many of the later algorithms in the field. <p> Otherwise: go to step 5. 5. Stop: The algorithm stops. The goal of these two methods is to make the error on the training set become as low as possible. Gallant uses his own Pocket Algorithm ~ <ref> [Gallant 86] </ref> for training.
Reference: [Gallant 86] <author> Gallant S.I. </author> <year> (1986), </year> <title> Optimal Linear Discriminants, </title> <booktitle> In Eighth International Conference on Pattern Recognition (Paris 1986), </booktitle> <pages> pp. 849-852. </pages> <address> New York: </address> <publisher> IEEE. </publisher>
Reference-contexts: It is called the perceptron learning rule ~ [Rosenblatt 62]. This method is, however, restricted to being only valid if the problem is linearly separable. An extension of this method, known as the pocket algorithm ~ <ref> [Gallant 86] </ref>, is able to handle nonlinearly separable problems. <p> A survey of some constructive algorithms can be found in [1]. Gallant was one of the first to come up with several supervised learning methods for dynamically building and training neural networks using units with threshold transfer functions <ref> [Gallant 86] </ref>. Many of the later derived construction algorithms ~ may be interpreted as extensions of Gallant's ideas. <p> data can be handled. * Analytic tractability: Analytic bounds to scaling and generalisation ability can be derived in many cases, thanks to the simplicity and modular structure of the underlying model, see chapter 4. 6.2 Simple Construction Algorithms Three of the most simple construction algorithms were proposed by Gallant in <ref> [Gallant 86] </ref>. The following description of these methods serves as an introduction to the field because they are easy to understand and contain many of the terms and ideas which are common to many of the later algorithms in the field. <p> Otherwise: go to step 5. 5. Stop: The algorithm stops. The goal of these two methods is to make the error on the training set become as low as possible. Gallant uses his own Pocket Algorithm ~ <ref> [Gallant 86] </ref> for training.
Reference: [Gallant 90] <author> Gallant, S. </author> <year> (1990), </year> <title> Perceptron-Based Learning Algorithms, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> Vol. 1, No. 2, </volume> <pages> pp. 179-191. </pages>
Reference-contexts: If after training the connections to the output unit the performance of the network is not as desired, new hidden units with randomly chosen and frozen weights are added. A newer version allowing short cuts (connections between input and output) was proposed in <ref> [Gallant 90] </ref>. The algorithm is thus changed and will start with a simple perceptron. If the performance of the perceptron after training is not as desired, new hidden units with randomly chosen and frozen weights are added. <p> The extension to the Distributed method is simply to add the desired number of output units, because the hidden units are randomly chosen feature detectors. A more detailed description of the Tower and Distributed methods including experiments can be found in <ref> [Gallant 90] </ref>. The networks are all limited to have units with threshold functions as transfer functions, which makes it possible to use the Pocket Algorithm. However, units with continuous transfer function like the sigmoid can also be applied as long as the training algorithm function also is changed.
Reference: [Hansen and Salamon 92] <author> Hansen, L.K. and Salamon, P. </author> <year> (1992), </year> <title> Neural Network Ensembles. </title> <journal> IEEE Trans. Pattern Analysis and Machine In-tell., </journal> <pages> pp. 993 - 1001. </pages>
Reference: [Harp et al. 90] <author> Harp, S.A., Samad, T. and Guha, A.(1990), </author> <title> Designing Application-Specific Neural Networks Using the Genetic Algorithms. </title> <booktitle> Advances in Neural Information Processing Systems II (Denver 1989). </booktitle> <editor> ed. D.S. </editor> <booktitle> Touretzky, </booktitle> <pages> pp. 447-454. </pages> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It should be stressed that the answers are not trivial, but indeed very important for the overall performance, perhaps the most important. An alternative approach to building neural network is to use Genetic Algorithms, as suggested in [Miller et al. 89], <ref> [Harp et al. 90] </ref> and [Whitley et al. 90]. GAs have shown promising results, but similar to the case where they were used for training the network, see paragraph 3.2.7, their practicability is limited to smaller problems.
Reference: [Hassibi and Stork 93] <author> Hassibi, B., and Stork, D. </author> <year> (1993), </year> <title> Second order derivatives for network pruning: Optimal Brain Surgeon. </title> <booktitle> Advances in Neural Information Processing Systems V (Denver 1993). </booktitle> <editor> ed. S.J. Hanson et al., </editor> <address> 164-171. San Mateo: </address> <publisher> Morgan Kauf-mann. BIBLIOGRAPHY 113 </publisher>
Reference-contexts: Retrain the network 8. If a certain stop criterion is fulfilled go to step 9 else go to step 3 9. Restore the weights saved in step 3 and stop 48 REGULARISATION AND PRUNING 5.3.3 Optimal Brain Surgeon The Optimal Brain Surgeon (OBS) <ref> [Hassibi and Stork 93] </ref> can be considered as an extension of OBD. It is also based on the Taylor expansion, but contrary to OBD it does not assume that the off-diagonal of the Hessian is zero. Instead it reformulates the goal. <p> way: H 1 p p X (p+1) X (p+1)T H 1 P + X (p+1)T H 1 p X (p+1) with H 1 0 = ff 1 I and H 1 where P is the number of patterns and X is a vector containing information of the second derivative, see <ref> [Hassibi and Stork 93] </ref> for a detailed description. The OBS algorithm is: 1. Choose a reasonable network architecture 2. Train the network until a reasonable solution is obtained 3. Store the weights 4. Compute the inversion of the second derivatives H 1 5.
Reference: [Huang and Huang 91] <author> Huang, S,C. and Huang Y.F. </author> <year> (1991), </year> <title> Bounds on the Number of Hidden Neurons in Multilayer Perceptrons IEEE Transactions on Neural Networks, </title> <journal> Vol. </journal> <volume> 2, No. 1, </volume> <pages> pp. 47-55. </pages>
Reference-contexts: A further geometric description and interpretation of some of these networks and other types can be found in [Wieland and Leighton 87], [Lippmann 87], [Huang and Lippmann 87], [DARPA 88] (chapter 6), [Gibson and Cowan 90] and <ref> [Huang and Huang 91] </ref>. 2.6.
Reference: [Hwang et al. 94] <author> Hwang, J. You, S., Lay. S. and Jou, I. </author> <year> (1994), </year> <title> What's Wrong with A Cascade-Correlation Learning Network: A Projection Pursuit Learning Perspective. Found on connectionist ftp list. </title>
Reference: [Knudsen and Vlk 94] <author> Knudsen, S.L. and Vlk, L.H. </author> <year> (1994), </year> <title> Ockham's Razor Drawn against Overfitted Neural Nets: A Quest for the Glorious Features, </title> <type> Masters Thesis, </type> <institution> Daimi IR-121, Computer Science Department, Aarhus University. </institution>
Reference-contexts: Further experiments with similar results can be found in <ref> [Knudsen and Vlk 94] </ref>. 5.3.5 Remarks These experiments confirmed that no single method was the best all the time and that there were several optimal solutions to the same problem. The experiments did, however, show that OBS was by far the most stable and robust method.
Reference: [Le Cun et al. 90] <author> Le Cun, Y., Denker, J.S. and Solla, S.A. </author> <year> (1990), </year> <title> Optimal Brain Damage. </title> <booktitle> In Advances in Neural Information Processing Systems II (Denver 1989). </booktitle> <editor> ed. D.S. Touretzky, </editor> <address> 598-605. San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: If no groups are removed the original MAG should take over. 5.3. PRUNING 47 5.3.2 Optimal Brain Damage A more mathematical approach using information from the second order derivatives of the error function to perform pruning has been derived by LeCun in a method called Optimal Brain Damage (OBD) <ref> [Le Cun et al. 90] </ref>.
Reference: [Marchand et al. 90] <author> Marchand, M.M., Golea, M. and Rujan, P. </author> <year> (1990), </year> <title> A Convergence Theorem for Sequential Learning in Two-Layer Perceptron, </title> <journal> Europhysics Letters 11, </journal> <pages> pp. 487-492, </pages> <year> 1990. </year>
Reference: [Mezard and Nadal 89] <author> Mezard, M. and Nadal, J.P. </author> <year> (1989), </year> <title> Learning in Feedfor-ward Layered Networks: The Tiling Algorithm, </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> Vol. 22, </volume> <pages> pp. 2191-2203. </pages>
Reference: [Miller et al. 89] <author> Miller, G.F., Todd, P.M. and Hegde, S.U. </author> <year> (1989), </year> <title> Designing Neural Networks Using Genetic Algorithms. </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <address> ICGA'89. Arlington 1989), </address> <publisher> Ed. J.D. Shaffer, </publisher> <pages> pp. 319-384. </pages> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It should be stressed that the answers are not trivial, but indeed very important for the overall performance, perhaps the most important. An alternative approach to building neural network is to use Genetic Algorithms, as suggested in <ref> [Miller et al. 89] </ref>, [Harp et al. 90] and [Whitley et al. 90]. GAs have shown promising results, but similar to the case where they were used for training the network, see paragraph 3.2.7, their practicability is limited to smaller problems.
Reference: [Pedersen 95] <author> Pedersen H.C.(1995), </author> <title> Clustering. Report to Neural Network Studygroup (in Danish), </title> <institution> Computer Science Department, Aarhus University. </institution>
Reference-contexts: GLOBAL-LOCAL LEARNING ALGORITHM 71 of the cluster elements to belong to another output class. This idea was explored and described in <ref> [Pedersen 95] </ref> who found through a number of experiments that the number of inserted RBF units was reduced, but the generalisation ability (test set error) was not improved. The final example was the idea of only inserting an RBF unit if the cluster contained more than 2 elements. <p> Several similar tests on other real life problems taken from the PROBEN1 database showing the performance of both GLOCAL and a version of GLOCAL where the cluster was allowed to contain a percentage of members from another class can be found in <ref> [Pedersen 95] </ref>.
Reference: [Platt 91] <author> Platt, J.C. </author> <year> (1991), </year> <title> Learning by Combining memorization and Gradient Descent, </title> <booktitle> in NIPS 3, </booktitle> <editor> Editors D. Touretzky, M. </editor> <publisher> Kaufmann Publishers, Inc., </publisher> <address> Denver, Colorado, </address> <pages> pp. 714-720. </pages>
Reference: [Reed 93] <author> Reed, R. </author> <year> (1993), </year> <title> Pruning Algorithms A Survey, </title> <journal> IEEE Transactions on Information Neural Networks, </journal> <volume> Vol. 4, No. 5, </volume> <pages> pp. 740-747. </pages>
Reference-contexts: The number of different pruning methods is gradually increasing. A survey of some of the earlier pruning methods can be found in <ref> [Reed 93] </ref>. In the following paragraphs three popular pruning methods will be described. 5.3.1 Magnitude Based Pruning One of the simplest methods for reducing the complexity of a neural network is called Magnitude Based pruning (MAG).
Reference: [Reilly et al. 82] <author> Reilly D.E., Cooper, L.N. and Elbaum, C. </author> <year> (1982). </year> <title> A Neural Model for Category Learning. </title> <journal> Biological Cybernetics, </journal> <volume> Vol 45, </volume> <pages> pp. 35-41, </pages> <publisher> Springer-Verlag Berlin-Heidelberg. 114 BIBLIOGRAPHY </publisher>
Reference-contexts: CONSTRUCTION ALGORITHMS The class of networks using RBF units in the hidden layer have for several years shown good performances and are well suited for real life applications implemented in hardware, see RCE <ref> [Reilly et al. 82] </ref> and RAN [8]. This class can be dated back to at least 1982 where the Restricted Coulomb Energy 2 (RCE) network was introduced as a neural model for Category Learning. <p> CCA, including mathematical foundation, description of some major problems and an attempt to solve them, can be found in Appendix E. 6.5 Restricted Coulomb Energy Network The Restricted Coulomb Energy (RCE) network is the name of the architecture that comes up when the algorithm for automatically building networks proposed in <ref> [Reilly et al. 82] </ref> is used. This method will therefore be called the RCE method. The RCE network is basically what today is called a Radial Basis Function network, see chapter 2.
Reference: [Weigend et al. 90] <author> Weigend, A.S., Huberman, B.A. and Rumelhart, D.E. </author> <year> (1990), </year> <title> Predicting the Future: A Connectionist Approach, </title> <journal> International Journal of Neural Systems, </journal> <volume> Vol. 1, </volume> <month> pp.193-209. </month>
Reference: [Whitley et al. 90] <author> Whitley D., Starkweather T. and Bogart C. </author> <year> (1990), </year> <title> Genetic algorithms and neural networks: optimizing connections and connectivity. </title> <booktitle> Parallel Computing 14, </booktitle> <year> 1990, </year> <pages> pp 347-361. </pages> <publisher> Elsevier Science Publishers B.V. (North-Holland). []RELATED TOPICS </publisher>
Reference-contexts: It should be stressed that the answers are not trivial, but indeed very important for the overall performance, perhaps the most important. An alternative approach to building neural network is to use Genetic Algorithms, as suggested in [Miller et al. 89], [Harp et al. 90] and <ref> [Whitley et al. 90] </ref>. GAs have shown promising results, but similar to the case where they were used for training the network, see paragraph 3.2.7, their practicability is limited to smaller problems.
Reference: [Dasarathy 90] <author> Dasarathy, B. V. </author> <year> (1990), </year> <title> Nearest Neighbor(NN) Norms: NN Pattern Classification Techniques. </title> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: There are several different iterative methods that perform the task of clustering ~ , see e.g. [Duda and Hart 73](Chapter 6), [Fukunaga 90](chapter 11), <ref> [Dasarathy 90] </ref> and [Schalkoff 92](chapter 5). Many of these cluster algorithms including the very popular "K-mean algorithm ~ " are based on the neighbourhood properties among the patterns. The problem with using one of these methods is that in general they do not take the class membership into consideration.
Reference: [Duda and Hart 73] <author> Duda, R.O. and Hart, P.E. </author> <year> (1973), </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: Used in connection with estimation of the parameters of a classifier it will, given an asymptotically larger and statistically independent training set, produce an output that equals an optimal Bayesian discriminator <ref> [Duda and Hart 73] </ref> (pp. 154-155). The next paragraphs all assume the use of the MSE, because it is mathematically manageable and the methods will still be valid for other error functions. There are several alternative error functions such as the relative entropy. <p> There are several different iterative methods that perform the task of clustering ~ , see e.g. <ref> [Duda and Hart 73] </ref>(Chapter 6), [Fukunaga 90](chapter 11), [Dasarathy 90] and [Schalkoff 92](chapter 5). Many of these cluster algorithms including the very popular "K-mean algorithm ~ " are based on the neighbourhood properties among the patterns. <p> It may be calculated or trained, but the general idea is to adjust some radii Ndim in order to control the overlap between different clusters. NfiNdim is the covariance matrix that determines the shape 68 CHAPTER 6. CONSTRUCTION ALGORITHMS of the cluster, see e.g. <ref> [Duda and Hart 73] </ref>(pp.22-32). The most simple way to calculate the NfiNdim is to set the diagonal elements of NfiNdim ( p oe ii ) equal to the distance between the centre of the cluster and the farthest member and zero otherwise. <p> Another question left open is how to deal with higher dimensionality in the input space, which also has influence on the metric to be used. Discussions and illustrations of these problems can be found in <ref> [Duda and Hart 73] </ref>, [Schalkoff 92], [4], [6] and [Ripley 94]. It should be stressed that the answers are not trivial, but indeed very important for the overall performance, perhaps the most important. <p> The problem is that points (parameter) that were originally easy to separate because the distance between them was large, may now be placed close to each other. A nice illustration of this problem can be found in <ref> [Duda and Hart 73] </ref>(pp. 213-217). <p> This would open the way to the use of a large number of well-known cluster methods. Another question is how to deal with higher dimensionality, which also has a certain influence on the metric to be used. Discussions on and illustrations of these problems can be found in <ref> [Duda and Hart 73] </ref>, [Schalkoff 92], [4] and [6]. It should be stressed that the answers are not trivial, but indeed very important for the overall performance, perhaps the most important. Measurement of the generalisation ability.
Reference: [Fletcher 75] <author> Fletcher, R. </author> <year> (1975), </year> <title> Practical Methods of Optimization, </title> <journal> Vol. </journal> <volume> 1, </volume> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: Many learning algorithms have problems with handling such functions and a general solution is not known. There are, however, some ways to remedy this problem, as shown later. A further description of the Optimisation Theory can be found in e.g. <ref> [Fletcher 75] </ref> or [Gill et al. 81]. 3.2.2 Gradient Descent Using a 1st order Taylor approximation, with respect to the weights w, the change in error with respect to the output unit's value can be expressed as: E (w + 4w) E (w) + 4w T E (w): (3.2) 16 CHAPTER
Reference: [Fukunaga 90] <author> Fukunaga, K. </author> <year> (1990), </year> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> ACADEMIC PRESS, </publisher> <address> New York. </address>
Reference-contexts: This process is continued until each training pattern has been used as test pattern. The average of all test errors is then used as an estimate for the generalisation error. An excellent description of the method and mathematical proof can be found in <ref> [Fukunaga 90] </ref> (page 220). A big drawback of this method is that the network has to undergo a full training phase for all training sets. A way to soften this problem is simply to take larger blocks of patterns for testing. <p> There are several different iterative methods that perform the task of clustering ~ , see e.g. [Duda and Hart 73](Chapter 6), <ref> [Fukunaga 90] </ref>(chapter 11), [Dasarathy 90] and [Schalkoff 92](chapter 5). Many of these cluster algorithms including the very popular "K-mean algorithm ~ " are based on the neighbourhood properties among the patterns.
Reference: [Gill et al. 81] <author> Gill, P.E., Murray, W. and Wright, M.H. </author> <year> (1981), </year> <title> Practical Optimization, </title> <publisher> Academic Press. </publisher>
Reference-contexts: Many learning algorithms have problems with handling such functions and a general solution is not known. There are, however, some ways to remedy this problem, as shown later. A further description of the Optimisation Theory can be found in e.g. [Fletcher 75] or <ref> [Gill et al. 81] </ref>. 3.2.2 Gradient Descent Using a 1st order Taylor approximation, with respect to the weights w, the change in error with respect to the output unit's value can be expressed as: E (w + 4w) E (w) + 4w T E (w): (3.2) 16 CHAPTER 3.
Reference: [Goldberg 89] <author> Goldberg, D.E. </author> <year> (1989), </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning, </title> <publisher> Addison-Wesley. </publisher>
Reference: [Golub and Loan 83] <author> Golub, G.H. and van Loan, C.F. </author> <year> (1983), </year> <title> Matrix Computations, </title> <publisher> The John Hopkins University Press. </publisher>
Reference-contexts: In earlier works like [Broomhead and Lowe 88] the parameters and patterns (input and output) were considered as a system of equations, normally overdetermined, and they were solved by methods like the Singular Value Decomposition method (SVD) <ref> [Golub and Loan 83] </ref>. The Gradient Descent methods can also be applied. Consider the case where output units V Li = g j=1 w L j have differentiable transfer functions and the hidden units are Gaussian which delimits a hyper-ellipsoidal in the input space. <p> The ffiw indicates how all weights should be adjusted, according to the elimination of a weight. This means that the network does not demand retraining. The only "learning" parameter involved is an ff which comes from initialising an iterative method that together with the Sherman-Morrison formula (see e.g. <ref> [Golub and Loan 83] </ref>) are employed to calculate the Inverse Hessian matrix in the following way: H 1 p p X (p+1) X (p+1)T H 1 P + X (p+1)T H 1 p X (p+1) with H 1 0 = ff 1 I and H 1 where P is the number
Reference: [Haralick 71] <author> Haralick, R.M. </author> <year> (1971,) </year> <title> On a Texture-Context Feature Extraction Algorithm for Remotely Sensed Imagery, </title> <booktitle> IEEE Decision and Control Conference, </booktitle> <pages> pp. 650-657. </pages>
Reference: [Haralick 79] <author> Haralick, R.M. </author> <year> (1979), </year> <title> Statistical and Structural Approaches to Texture, </title> <booktitle> Proc. IEEE 67, </booktitle> <pages> pp. 786-804. </pages>
Reference-contexts: For each of these areas (segments) 16 features are calculated. These features are listed below. The definition and further explanation of these features can be found both in <ref> [Haralick 79] </ref> and [Skriver 94]. 1. Mean value of the backscatter coefficient in dB 2. Mean intensity, e.g. the pixel value is squared before the averaging 3. Standard deviation of intensity 4. Maximum intensity 5. Minimum intensity 6.
Reference: [Haralick et al. 73] <author> Haralick, R.M., Shanmugan, K.S. and Dinstein, I. </author> <year> (1973), </year> <title> Textural Feature for Image Classification, </title> <journal> IEEE Trans. Syst., Man, Cybern., </journal> <volume> SMC-3, </volume> <pages> pp. 610-621. </pages>
Reference: [Holland 75] <author> Holland, J.H. </author> <year> (1975), </year> <booktitle> Adaption in Natural and Artificial Systems. </booktitle> <publisher> University of Michigan Press. BIBLIOGRAPHY 115 </publisher>
Reference-contexts: SCG exhibits second order convergence properties and contains no problem dependent parameters. See [Moller 93a] for a detailed description of SCG. 3.2.7 Training by Genetic Algorithms Genetic Algorithms (GAs) ~ are general search techniques, based on biological principles ([Goldberg 89] <ref> [Holland 75] </ref>). In many cases GAs have shown to be effective adaptive techniques for optimisation. GAs have been applied in conjunction with neural nets to provide a learning algorithm and to find the optimum architecture of the network in terms of number of hidden units and their connectivity.
Reference: [Horn and Johnson 85] <author> Horn, R.H. and Johnson, C.A. </author> <year> (1985), </year> <title> Matrix Analysis, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference: [Knuth 81] <author> Knuth, D.E. </author> <year> (1981), </year> <booktitle> The Art of Computer Programming, </booktitle> <volume> Vol. 2, </volume> <booktitle> Semi-Numerical Algorithms, </booktitle> <publisher> Addison-Wesley Publishing Company. </publisher>
Reference: [Kreyszig 88] <author> Kreyszig, E. </author> <year> (1988), </year> <institution> Advanced Engineering Mathematics, </institution> <address> 6th edition, </address> <publisher> John Wiley and Sons, Inc. </publisher>
Reference: [Leamer 79] <institution> Leamer E.E (1979), Specification searcher. </institution> <address> New York: </address> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: The second method of stopping training can also be interpreted as a form of regularisation but in fact it seems to be one of the true innovations resulting from neural network research. The last method can be viewed as a more established technique called specification search <ref> [Leamer 79] </ref>. 43 44 REGULARISATION AND PRUNING 5.2 Regularisation Regularisation comes from statistical mathematics and is based on the idea that the generalisation error is a sum of two components: the error on the training set and an additional term depending on the network's complexity.
Reference: [Luenberger 84] <author> Luenberger, D.G. </author> <year> (1984), </year> <title> Linear and Nonlinear Programming, </title> <publisher> Addison-Wesley Publishing Company, Inc. </publisher>
Reference: [Niblack 85] <author> Niblack, W. </author> <year> (1985), </year> <title> An Introduction to Digital Image Processing, </title> <publisher> Strandsberg Publishing Company, </publisher> <address> Birkerod, </address> <month> Den-mark </month> <year> 1985. </year>
Reference-contexts: CONSTRUCTION ALGORITHMS Chapter 7 Classification of Ice The estimation of ice type concentrations from Synthetic Aperture Radar (SAR) images has been investigated for several years, see e.g. [Skriver 94]. The classification estimation has been performed by training a Bayesian Maximum Likelihood Classifier (BM LC) <ref> [Niblack 85] </ref> with a classification rate about 80%. This chapter describes the work done in connection with a preliminary investigation of a neural network's capability to classify ice types.
Reference: [Skriver 95] <author> Skriver, H. </author> <year> (1995), </year> <title> Private communication. </title>
Reference-contexts: Pruning often removed all weights to a single input unit, thereby telling that this input could be cancelled. Different inputs were cancelled from one network to another, but in general the Minimum intensity and the Normalised 4th order moment were cancelled. This result was confirmed by <ref> [Skriver 95] </ref>.
Reference: [Skriver 94] <author> Skriver, H. </author> <title> (1994) On the accuracy of estimation of ice type concentration from ERS-1 SAR images from EARSEL 14th Symposium, </title> <address> Gothenburg, </address> <year> 1994). </year>
Reference-contexts: Recent work carried out at the Computer Science Department of Aarhus University by [Arentoft 95] has confirmed this tendency. 74 CHAPTER 6. CONSTRUCTION ALGORITHMS Chapter 7 Classification of Ice The estimation of ice type concentrations from Synthetic Aperture Radar (SAR) images has been investigated for several years, see e.g. <ref> [Skriver 94] </ref>. The classification estimation has been performed by training a Bayesian Maximum Likelihood Classifier (BM LC) [Niblack 85] with a classification rate about 80%. This chapter describes the work done in connection with a preliminary investigation of a neural network's capability to classify ice types. <p> A number of features, which is of great importance for research and application purposes, may be derived from the radar data. This applies to ice concentration, ice motion, lead structure, flow size distribution, etc. <ref> [Skriver 94] </ref>. The basis of feature retrieval of any kind is the ability to discriminate between the targets in question. In SAR images the basis of discrimination is the properties of the backscatter coefficient, i.e. the mean backscatter coefficient and the spatial variations in the backscatter coefficient, also called texture. <p> The latter is used and a fine segmentation algorithm has been developed for that purpose, see <ref> [Skriver 94] </ref>. In this chapter the techniques and results from previous work done by Skriver using the classical Bayesian Maximum Likelihood Classifier method are shortly reviewed ([Skriver 94], [Skriver 89]). <p> For each of these areas (segments) 16 features are calculated. These features are listed below. The definition and further explanation of these features can be found both in [Haralick 79] and <ref> [Skriver 94] </ref>. 1. Mean value of the backscatter coefficient in dB 2. Mean intensity, e.g. the pixel value is squared before the averaging 3. Standard deviation of intensity 4. Maximum intensity 5. Minimum intensity 6. <p> This is the same as reducing the influence of speckle in the image, which is normally done by simply using as large areas in the image as possible. In <ref> [Skriver 94] </ref> the PMR is only included in the classification for segments with more than 1000 pixels, because the PMR is not unbiased for smaller segments, as mentioned above. For small segments only the backscatter coefficient, which is unbiased, is used in the classification. <p> The above-mentioned large number of manually classified segments is in practical applications unrealistic, because it is a very time-consuming and difficult task. Therefore only 30 of the largest segments were used for the training set in <ref> [Skriver 94] </ref>. This is of course unfortunate because the calculation of the model's parameters only is based on larger segments (more than 1000 pixels), but it is to be used for all segments.
Reference: [Skriver 89] <author> Skriver, H. </author> <year> (1989), </year> <title> Extraction of Sea Ice Parameters from Synthetic Aperture Radar Images, </title> <type> Ph.D. </type> <institution> Thesis from Electromagnetic Institute, Technical University of Denmark, Lyngby, </institution> <year> 1989. </year> <note> 116 BIBLIOGRAPHY </note>
Reference-contexts: The latter is used and a fine segmentation algorithm has been developed for that purpose, see [Skriver 94]. In this chapter the techniques and results from previous work done by Skriver using the classical Bayesian Maximum Likelihood Classifier method are shortly reviewed ([Skriver 94], <ref> [Skriver 89] </ref>). After that results from using a neural network on the same problem are given. 7.2 What is the Problem? The European satellite ERS-1 orbits around the earth with an interval of 3 days. <p> Open water (WA) Class 6. Ice mixture (MIX) Each ice type is an indication of how old the ice is. 7.2.1 Segmentation and Features At Electromagnetics Institute (EMI), Technical University of Denmark (DTU) a program which is able to segment the images has been developed <ref> [Skriver 89] </ref>. The segmentation algorithm can be briefly described as: 1) Edge detection, 2) Centre point determination, 3) Segment border determination, and 4) Segment merging. For each of these areas (segments) 16 features are calculated. These features are listed below. <p> The segment's feature is strongly disturbed by noise because the backscatter coefficient inherits the so-called speckle noise in the SAR image. A statistical analysis of the features made by Skriver in <ref> [Skriver 89] </ref> showed that: 1. The distribution of the backscatter coefficient was a gamma distribution and the difference between the 5% and 95% fractiles was as large as 17 dB (about 50 times difference), when individual pixels were considered. 2.
References-found: 143

