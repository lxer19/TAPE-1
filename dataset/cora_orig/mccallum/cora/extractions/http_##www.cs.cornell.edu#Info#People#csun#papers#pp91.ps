URL: http://www.cs.cornell.edu/Info/People/csun/papers/pp91.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/csun/sun.html
Root-URL: 
Title: Multifrontal Factorization Using Clique Trees 1  
Author: Alex Pothen and Chunguang Sun 
Note: Distributed  
Abstract: This paper appears in Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, J. Dongarra, K. Kennedy, P. Messina, D. C. Sorensen and R. G. Voigt, eds., SIAM, Philadelphia, 1992, pp.34-40. Abstract We describe the issues involved in the design and implementation of an efficient parallel multifrontal algorithm for computing the Cholesky factorization of a sparse matrix on distributed memory multiprocessors. The clique tree data structure is used to organize the numerical factorization. A crucial issue is the computation of good mappings of the computational tasks on processors to achieve load balance and low communication costs. Good speedups have been obtained for many practical problems on an iPSC/2. Both the arithmetic complexity and the communication complexity of our algorithm on grid graphs are asymptotically optimal. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. C. Ashcraft, </author> <title> The domain/segment partition for the factorization of sparse symmetric positive definite matrices, </title> <type> Tech. Report ECA-TR-148, </type> <institution> Boeing computer Services, </institution> <address> Seattle, Washington, </address> <month> Nov </month> <year> 1990. </year>
Reference-contexts: The algorithm of Lucas, Blank, and Tiemann [7] requires the use of the nested dissection ordering, and also requires extra storage since a column in a separator is stored on more than one processor. Ashcraft, Eisenstat, Liu, and Sherman <ref> [1, 2] </ref> have compared the distributed multifrontal method with two variants of column-oriented distributed sparse factorization algorithms, and have shown that the multifrontal approach outperforms the other algorithms on an iPSC/2 for the model grid problem. <p> A new task partitioning and mapping strategy for general sparse problems is proposed and compared with existing schemes. Previous works on distributed sparse Cholesky factorizations report experimental results on only two classes of problems|grids and L-shaped problems <ref> [1, 2, 3] </ref>. We have experimented with a collection of problems with no regular sparsity structure from the Boeing-Harwell test set and have obtained good speedups. 2. Application of clique trees to the multifrontal method. We assume that the reader is familiar with the graph model of sparse Cholesky factorization.
Reference: [2] <author> C. C. Ashcraft, S. C. Eisenstat, J. W. Liu, and A. H. Sherman, </author> <title> A comparison of three column-based distributed sparse factorization schemes, </title> <type> Tech. Report CS-90-09, </type> <institution> Department of Computer Science, York University, </institution> <address> North York, Ontario, Canada, </address> <month> Aug </month> <year> 1990. </year>
Reference-contexts: The algorithm of Lucas, Blank, and Tiemann [7] requires the use of the nested dissection ordering, and also requires extra storage since a column in a separator is stored on more than one processor. Ashcraft, Eisenstat, Liu, and Sherman <ref> [1, 2] </ref> have compared the distributed multifrontal method with two variants of column-oriented distributed sparse factorization algorithms, and have shown that the multifrontal approach outperforms the other algorithms on an iPSC/2 for the model grid problem. <p> A new task partitioning and mapping strategy for general sparse problems is proposed and compared with existing schemes. Previous works on distributed sparse Cholesky factorizations report experimental results on only two classes of problems|grids and L-shaped problems <ref> [1, 2, 3] </ref>. We have experimented with a collection of problems with no regular sparsity structure from the Boeing-Harwell test set and have obtained good speedups. 2. Application of clique trees to the multifrontal method. We assume that the reader is familiar with the graph model of sparse Cholesky factorization.
Reference: [3] <author> G. A. Geist and E. G. Ng, </author> <title> Task scheduling for parallel sparse Cholesky factorization, </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18 (1989), </volume> <pages> pp. 291-314. </pages>
Reference-contexts: A new task partitioning and mapping strategy for general sparse problems is proposed and compared with existing schemes. Previous works on distributed sparse Cholesky factorizations report experimental results on only two classes of problems|grids and L-shaped problems <ref> [1, 2, 3] </ref>. We have experimented with a collection of problems with no regular sparsity structure from the Boeing-Harwell test set and have obtained good speedups. 2. Application of clique trees to the multifrontal method. We assume that the reader is familiar with the graph model of sparse Cholesky factorization. <p> This process is recursively applied to each subtree. Since the distribution of the arithmetic work in a clique tree is ignored, this scheme is only effective for those problems with balanced tree structure and balanced workload distribution such as the grids. Geist and Ng <ref> [3] </ref> have proposed a bin-pack mapping scheme for mapping arbitrary trees. This is the only mapping strategy reported in the literature that is applicable to general problems. We have adapted their scheme, which was described with respect to the fan-out algorithm, to the multifrontal context.
Reference: [4] <author> J. A. George, J. W. Liu, and E. G. Ng, </author> <title> Communication results for parallel sparse Cholesky factorization on a hypercube, </title> <booktitle> Parallel Computing, 10 (1989), </booktitle> <pages> pp. 287-298. </pages>
Reference-contexts: Since the factorization of the given sparse matrix is formulated in terms of partial factorizations of dense submatrices, vectorization on the processors can be fully exploited. 3 3. Mapping schemes. George, Liu, and Ng <ref> [4] </ref> have proposed the subtree-to-subcube mapping for the model grid problem ordered by nested dissection. They proposed their scheme with respect to the elimination tree, but we apply this scheme here to the clique tree of the grid. The root clique is partitioned among all processors.
Reference: [5] <author> J. G. Lewis, B. W. Peyton, and A. Pothen, </author> <title> A fast algorithm for reordering sparse matrices for parallel factorization, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10 (1989), </volume> <pages> pp. 1146-1173. </pages>
Reference-contexts: The set of maximal cliques of the chordal graph G can be organized into a tree structure called the clique tree. A discussion of clique trees and their applications in sparse matrix algorithms may be found in <ref> [5, 9] </ref>. The adjacency graph of a 6 fi 6 Cholesky factor and a corresponding clique tree are shown in Fig. 1. Note that in the figure, each maximal clique is drawn as a circle, and the vertices of the clique are listed in the circle. <p> Every vertex in anc (K) belongs also to some ancestor clique of K, and vertices in new (K) do not belong to any ancestor clique. An important property of clique trees computed from the algorithm in <ref> [5] </ref> is that anc (K) P , where P is the parent of K, and P is the only clique satisfying this relation. Let s = jnew (K)j, t = janc (K)j, and r = s + t.
Reference: [6] <author> J. W. Liu, </author> <title> The multifrontal method for sparse matrix solution: theory and practice, </title> <type> Tech. Report 90-04, </type> <institution> Computer Science, York University, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: 1. Introduction. The multifrontal method has proved to be attractive in solving large sparse systems of equations efficiently in many different computational environments (see Liu <ref> [6] </ref> for a survey). We describe a parallel multifrontal sparse Cholesky factorization algorithm for distributed-memory multiprocessors. We show that the clique tree is a natural data structure in formulating the multifrontal method in that the parallel algorithm is easily described and efficiently implemented in terms of it.
Reference: [7] <author> R. Lucas, T. Blank, and J. Tiemann, </author> <title> A parallel solution method for large sparse systems of equations, </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <month> CAD-6 </month> <year> (1987), </year> <pages> pp. 981-991. </pages>
Reference-contexts: We show that the clique tree is a natural data structure in formulating the multifrontal method in that the parallel algorithm is easily described and efficiently implemented in terms of it. Two groups have described previous implementations of the distributed multifrontal method. The algorithm of Lucas, Blank, and Tiemann <ref> [7] </ref> requires the use of the nested dissection ordering, and also requires extra storage since a column in a separator is stored on more than one processor.
Reference: [8] <author> A. Pothen and P. Raghavan, </author> <title> Sparse orthogonal factorization on a distributed-memory multiprocessor. </title> <note> In preparation, </note> <year> 1991. </year>
Reference-contexts: This scheme may be viewed as a generalization of the subtree-to-subcube mapping in which both the structure of the tree and the distribution of work in the tree are considered in mapping tasks. Proportional mapping has also been considered in the context of distributed sparse orthogonal factorization <ref> [8] </ref>. 4. Distributed numerical factorization. All three mapping schemes discussed above partition the numerical factorization into a local phase and a distributed phase. In the local phase, processors work in parallel on independent subtrees and no communication among processors is necessary.
Reference: [9] <author> A. Pothen and C. Sun, </author> <title> Compact clique tree data structures in sparse matrix factorizations, in Large-Scale Numerical Optimization, </title> <booktitle> Proceedings in Applied Mathematics, </booktitle> <volume> vol. </volume> <pages> 46, </pages> <institution> Society for Industrial and Applied Mathematics, </institution> <month> Dec </month> <year> 1990, </year> <pages> pp. </pages> <month> 180-204. </month> <title> [10] , An efficient distributed multifrontal algorithm. </title> <note> In preparation, 1991. 9 </note>
Reference-contexts: The set of maximal cliques of the chordal graph G can be organized into a tree structure called the clique tree. A discussion of clique trees and their applications in sparse matrix algorithms may be found in <ref> [5, 9] </ref>. The adjacency graph of a 6 fi 6 Cholesky factor and a corresponding clique tree are shown in Fig. 1. Note that in the figure, each maximal clique is drawn as a circle, and the vertices of the clique are listed in the circle.
References-found: 9

