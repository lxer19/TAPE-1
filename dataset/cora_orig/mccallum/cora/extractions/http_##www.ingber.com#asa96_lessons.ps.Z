URL: http://www.ingber.com/asa96_lessons.ps.Z
Refering-URL: http://www.ingber.com/
Root-URL: 
Email: ingber@alumni.caltech.edu  
Title: %A L. Ingber %T Adaptive simulated annealing (ASA): Lessons learned %J Control and Cybernetics Annealing
Author: %D Lester Ingber Lester Ingber 
Address: P.O.B. 857 McLean, VA 22101 U.S.A.  
Affiliation: Research  
Note: %P (to be published) This is an invited paper to a special issue of the Polish Journal Control and Cybernetics on Simulated  associated  The author's ASA code has been publicly available for over two years. During this time the author has volunteered to help  algorithms, are described.  
Date: 1995  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> BINDER, K. AND STA UFFER, D., </author> <title> (1985) A simple introduction to Monte Carlo simulations and some specialized topics, In: Applications of the Monte Carlo Method in Statistical Physics, </title> <editor> K. Binder (Ed.), </editor> <publisher> Berlin, Springer-Verlag. </publisher>
Reference-contexts: This essentially is the Boltzmann distribution contributing to the statistical mechanical partition function of the system <ref> (Binder and Stauffer, 1985) </ref>.
Reference: <author> BRO WN, R., RULKOV, </author> <title> N.F., AND TUFILLARO, N.B., (1994) Synchronization of chaotic systems: the effects of additive noise and drift in the dynamics and driving, </title> <journal> Phys. </journal> <note> Rev. E (to be published). </note>
Reference: <author> CERNY, V., </author> <title> (1982) A thermodynamical approach to the travelling salesman problem: An efficient simulation algorithm, </title> <type> Report, </type> <institution> Bratislava, Czechoslovakia, Comenius University. </institution>
Reference: <author> COHEN, B., </author> <title> (1994) Training synaptic delays in a recurrent neural network, M.S. </title> <type> Thesis, </type> <institution> Tel-Aviv, Israel, Tel-Aviv University. </institution>
Reference: <author> CORANA, A., MARCHESI, M., MARTINI, C., AND RIDELLA, S., </author> <title> (1987) Minimizing multimodal functions of continuous variables with the "simulated annealing" algorithm, </title> <journal> ACM Trans. Mathl. </journal> <volume> Software 13, 3, </volume> <pages> 262-279. </pages>
Reference-contexts: Only ASA regularly attained the global minimum, and it was more efficient in attaining regular minima at each comparable number of generated states than BA or FA. The ASA_TEST problem contains 10 20 local minima with a parameter dimension of 4 <ref> (Corana et al, 1987) </ref>, and it typically takes 2000-3000 generated states, or about 2 CPU-sec on a Sun SPARC-II, to find the global minimum. 4.2.3.
Reference: <author> CVIJOVIC , D. AND KLINOWSKI, J., </author> <title> (1995) Taboo search: An approach to the multiple minima problem, </title> <booktitle> Science 267, </booktitle> <volume> 5198, </volume> <pages> 664-666. </pages>
Reference-contexts: The Traveling Salesman (TSP) Problem is an excellent example of such a situation; the best search algorithms for the TSP problem are hand-tailored for it (Reinelt, 1994) . 4.2.5. Shubert problem In <ref> (Cvijovic and Klinowski, 1995) </ref> some strong claims about the superiority of taboo search over genetic algorithms and simulated annealing were made.
Reference: <author> DE JONG, K.A., </author> <title> (1992) Genetic algorithms are NOT function optimizers, </title> <booktitle> In: Foundations of Genetic Algorithms: Proceedings 24-29 July 1992, </booktitle> <editor> D. Whitley (Ed.), </editor> <address> Vail, </address> <publisher> CO, Morgan Kaufman. </publisher>
Reference-contexts: In each case, ASA outperformed the GA problem. It should be clear that GA is a class of algorithms that are interesting in their own right. GA was not originally developed as an optimization algorithm <ref> (De Jong, 1992) </ref>, and basic GA does not offer any statistical guarantee of global convergence to an optimal point (Forrest, 1993). Nevertheless, it should be expected that GA may be better suited for some problems than SA. 4.2.2.
Reference: <editor> FORREST, S., </editor> <booktitle> (1993) Genetic algorithms: Principles of natural selection applied to computation, Science 261, </booktitle> <volume> 5123, </volume> <pages> 872-878. </pages>
Reference-contexts: It should be clear that GA is a class of algorithms that are interesting in their own right. GA was not originally developed as an optimization algorithm (De Jong, 1992), and basic GA does not offer any statistical guarantee of global convergence to an optimal point <ref> (Forrest, 1993) </ref>. Nevertheless, it should be expected that GA may be better suited for some problems than SA. 4.2.2.
Reference: <author> FROST, R., </author> <title> (1993) Ensemble Based Simulated Annealing (EBSA), </title> <address> ftp.sdsc.edu:/pub/sdsc/math/Ebsa, La Jolla, CA, University of California San Diego. </address>
Reference-contexts: Parallel code It is quite difficult to directly parallelize an SA algorithm (Ingber, 1993b), e.g., without incurring very restrictive constraints on temperature schedules (Kimura and Taki, 1991), or violating an associated sampling proof <ref> (Frost, 1993) </ref>. However, the fat tail of ASA permits parallelization of developing generated states prior to subjecting them to the acceptance test (Ingber, 1992). The ASA_PARALLEL OPTIONS provide parameters to easily parallelize the code, using various implementations, e.g., PVM, shared memory, etc.
Reference: <author> GEMAN, S. AND GEMAN, D., </author> <title> (1984) Stochastic relaxation, Gibbs distribution and the Bayesian restoration in images, </title> <journal> IEEE Trans. Patt. Anal. Mac. Int. </journal> <volume> 6, 6, </volume> <pages> 721-741. </pages>
Reference-contexts: A sufficiency proof was then shown to put an lower bound on that schedule as 1/ log (t), where t is an artificial time measure of the annealing schedule <ref> (Geman and Geman, 1984) </ref>. However, independent credit usually goes to several other authors for independently developing the algorithm that is now recognized as simulated annealing (Cerny, 1982; Pincus, 1970). 2.1.1. <p> Given g (Dx), it has been proven <ref> (Geman and Geman, 1984) </ref> that it suffices to obtain a global minimum of E (x) if T is selected to be not faster than T (k) = ln k with T 0 large enough.
Reference: <author> INDIVERI, G., NATERI, G., RAFFO, L., AND CAVIGLIA, D., </author> <title> (1993) A neural network architecture for defect detection through magnetic inspection, </title> <type> Report, </type> <institution> Genova, Italy, University of Genova. </institution>
Reference: <author> INGBER, L., </author> <title> (1989) Very fast simulated re-annealing, </title> <journal> Mathl. Comput. Modelling 12, </journal> <volume> 8, </volume> <pages> 967-973. </pages> <note> ASA Lessons Learned -25- Lester Ingber INGBER, </note> <author> L., </author> <title> (1990) Statistical mechanical aids to calculating term structure models, </title> <journal> Phys. Rev. A 42, </journal> <volume> 12, </volume> <pages> 7057-7064. </pages>
Reference-contexts: 1. INTRODUCTION Adaptive simulated annealing (ASA) is a global optimization algorithm that relies on randomly importance-sampling the parameter space, i.e., in contrast to utilizing deterministic approaches often used by OR and mathematical programming people. Since the public release of the very fast simulated reannealing (VFSR) code <ref> (Ingber, 1989) </ref>, now called ASA (Ingber, 1993a), I have volunteered to help users via e-mail on code-specific problems they might encounter. <p> However, this does not affect the annealing proof of ASA, and so this may used without damaging the (weak) ergodicity property. 2.2.3. ASA applications The above defines this method of adaptive simulated annealing (ASA), previously called very fast simulated reannealing (VFSR) <ref> (Ingber, 1989) </ref> only named such to contrast it the previous method of fast annealing (FA) (Szu and Hartley, 1987). The annealing schedules for the temperatures T i decrease exponentially in annealing-time k, i.e., T i = T i0 exp (-c i k 1/D ).
Reference: <author> INGBER, L., </author> <title> (1991) Statistical mechanics of neocortical interactions: A scaling paradigm applied to electroencephalography, </title> <journal> Phys. Rev. A 44, </journal> <volume> 6, </volume> <pages> 4017-4060. </pages>
Reference-contexts: I hav e used ASA in several systems, ranging from combat analysis (Ingber, Fujio, and Wehner, 1991; Ingber and Sworder, 1991), to finance (Ingber, 1990; Ingber, Wehner et al, 1991), to neuroscience <ref> (Ingber, 1991) </ref>, to a set of test problems (Ingber and Rosen, 1992), to a new technique combining the ASA Lessons Learned -12- Lester Ingber power of SA with the physics of large-scale systems (Ingber, 1992).
Reference: <author> INGBER, L., </author> <title> (1992) Generic mesoscopic neural networks based on statistical mechanics of neocortical interactions, </title> <journal> Phys. </journal> <note> Rev. A 45, 4, R2183-R2186. </note>
Reference-contexts: I hav e used ASA in several systems, ranging from combat analysis (Ingber, Fujio, and Wehner, 1991; Ingber and Sworder, 1991), to finance (Ingber, 1990; Ingber, Wehner et al, 1991), to neuroscience (Ingber, 1991), to a set of test problems <ref> (Ingber and Rosen, 1992) </ref>, to a new technique combining the ASA Lessons Learned -12- Lester Ingber power of SA with the physics of large-scale systems (Ingber, 1992). <p> Wehner, 1991; Ingber and Sworder, 1991), to finance (Ingber, 1990; Ingber, Wehner et al, 1991), to neuroscience (Ingber, 1991), to a set of test problems (Ingber and Rosen, 1992), to a new technique combining the ASA Lessons Learned -12- Lester Ingber power of SA with the physics of large-scale systems <ref> (Ingber, 1992) </ref>. An optimization algorithm typically is just one tool used in a major project, and many users of ASA use this code as just one such tool. <p> However, the fat tail of ASA permits parallelization of developing generated states prior to subjecting them to the acceptance test <ref> (Ingber, 1992) </ref>. The ASA_PARALLEL OPTIONS provide parameters to easily parallelize the code, using various implementations, e.g., PVM, shared memory, etc. The scale of parallelization afforded by ASA, without violating its sampling proof, is given by a typical ratio of the number of generated to accepted states. <p> In ASA, if annealing is forsaken, and QUENCHing turned on, voiding the proof of sampling, remarkable increases of speed can be obtained, apparently sometimes even greater than other `greedy' algorithms. For example, in <ref> (Ingber and Rosen, 1992) </ref> along with 5 GA test problems from the UCSD GA archive, another harder problem (the ASA_TEST problem that comes with the ASA code) was used. In (Ingber, 1993b) QUENCHing was applied to this harder problem.
Reference: <author> INGBER, L., </author> <title> (1993a) Adaptive Simulated Annealing (ASA), </title> <address> [ftp.alumni.caltech.edu: /pub/ingber/ASA-shar, ASA-shar.Z, ASA.tar.Z, ASA.tar.gz, ASA.zip], McLean, VA, </address> <note> Lester Ingber Research. </note>
Reference-contexts: Since the public release of the very fast simulated reannealing (VFSR) code (Ingber, 1989), now called ASA <ref> (Ingber, 1993a) </ref>, I have volunteered to help users via e-mail on code-specific problems they might encounter. <p> Section 4 addresses some comparison tests and controversial claims, emphasizing that in fact there are many shades of SA. Section 5 gives a brief conclusion. ASA source code in C-language is publicly available via anonymous ftp <ref> (Ingber, 1993a) </ref>, or via the world wide web (WWW) at http://www.alumni.caltech.edu/~ingber/. Problems with the code can be addressed to ingber@alumni.caltech.edu. Requests to be placed on the ASA mailing list should be addressed to asa-request@alumni.caltech.edu. 2. ASA ALGORITHM ASA Lessons Learned -3- Lester Ingber 2.1. <p> These are among several considerations that gav e rise to Adaptive Simulated Annealing (ASA). Full details are available by obtaining the publicly available source code <ref> (Ingber, 1993a) </ref>.
Reference: <author> INGBER, L., </author> <title> (1993b) Simulated annealing: Practice versus theory, </title> <journal> Mathl. Comput. Modelling 18, </journal> <volume> 11, </volume> <pages> 29-57. </pages>
Reference-contexts: A previous paper has described SA in the context of its use across many disciplines, and the variation in use in actual practice versus the algorithm offered in theoretical papers <ref> (Ingber, 1993b) </ref>. Section 2 gives a brief mathematical description of the ASA algorithm. Section 3 describes some of the options in the code, and explains why they are useful. Section 4 addresses some comparison tests and controversial claims, emphasizing that in fact there are many shades of SA. <p> While perhaps someday less stringent necessary conditions may be developed for the Boltzmann algorithm, this is not now the state of affairs. The question arises, what is the value of this clear misuse of the claim to use SA to help solve these problems/systems <ref> (Ingber, 1993b) </ref>? Below, a variant of SA, adaptive simulated annealing (ASA) (Ingber, 1989; Ingber, 1993a), in fact does justify an exponential annealing schedule, but only if a particular distribution is used for the generating function. 2.1.3. <p> Reannealing (a) Parameter temperatures. For the parameter temperatures, the tangents (or any other alternative functions that might be defined by the user) are used as a relative measure of the steepness of each dimension the most recent best saved state. As demonstrated for the ASA_TEST problem <ref> (Ingber, 1993b) </ref>, this feature can enhance the efficiency of the search. (b) Cost temperature. <p> Quenching An SA algorithm loses much of its authority if the search cheats by trying to anneal at rates faster than permitted by its associated proof, e.g., simulated quenching (SQ). However, this can be useful in a number of circumstances <ref> (Ingber, 1993b) </ref>. When the dimension of a parameter space, each parameter having a continuous or large integral set of values, reaches 15 to 20, the volume of search typically becomes quite large and this can severely tax most present-day workstations. <p> Parallel code It is quite difficult to directly parallelize an SA algorithm <ref> (Ingber, 1993b) </ref>, e.g., without incurring very restrictive constraints on temperature schedules (Kimura and Taki, 1991), or violating an associated sampling proof (Frost, 1993). However, the fat tail of ASA permits parallelization of developing generated states prior to subjecting them to the acceptance test (Ingber, 1992). <p> For example, in (Ingber and Rosen, 1992) along with 5 GA test problems from the UCSD GA archive, another harder problem (the ASA_TEST problem that comes with the ASA code) was used. In <ref> (Ingber, 1993b) </ref> QUENCHing was applied to this harder problem. The resulting SQ code was shown to speed up the search by as much as as factor of 86 (without ev en attempting to see if this could be increased further with more extreme quenching).
Reference: <author> INGBER, L., FUJIO, H., AND WEHNER, M.F., </author> <title> (1991) Mathematical comparison of combat computer models to exercise data, </title> <journal> Mathl. Comput. Modelling 15, </journal> <volume> 1, </volume> <pages> 65-90. </pages>
Reference-contexts: I hav e used ASA in several systems, ranging from combat analysis (Ingber, Fujio, and Wehner, 1991; Ingber and Sworder, 1991), to finance (Ingber, 1990; Ingber, Wehner et al, 1991), to neuroscience <ref> (Ingber, 1991) </ref>, to a set of test problems (Ingber and Rosen, 1992), to a new technique combining the ASA Lessons Learned -12- Lester Ingber power of SA with the physics of large-scale systems (Ingber, 1992).
Reference: <author> INGBER, L. AND ROSEN, B., </author> <title> (1992) Genetic algorithms and very fast simulated reannealing: A comparison, </title> <journal> Mathl. Comput. Modelling 16, </journal> <volume> 11, </volume> <pages> 87-100. </pages>
Reference-contexts: I hav e used ASA in several systems, ranging from combat analysis (Ingber, Fujio, and Wehner, 1991; Ingber and Sworder, 1991), to finance (Ingber, 1990; Ingber, Wehner et al, 1991), to neuroscience (Ingber, 1991), to a set of test problems <ref> (Ingber and Rosen, 1992) </ref>, to a new technique combining the ASA Lessons Learned -12- Lester Ingber power of SA with the physics of large-scale systems (Ingber, 1992). <p> Wehner, 1991; Ingber and Sworder, 1991), to finance (Ingber, 1990; Ingber, Wehner et al, 1991), to neuroscience (Ingber, 1991), to a set of test problems (Ingber and Rosen, 1992), to a new technique combining the ASA Lessons Learned -12- Lester Ingber power of SA with the physics of large-scale systems <ref> (Ingber, 1992) </ref>. An optimization algorithm typically is just one tool used in a major project, and many users of ASA use this code as just one such tool. <p> However, the fat tail of ASA permits parallelization of developing generated states prior to subjecting them to the acceptance test <ref> (Ingber, 1992) </ref>. The ASA_PARALLEL OPTIONS provide parameters to easily parallelize the code, using various implementations, e.g., PVM, shared memory, etc. The scale of parallelization afforded by ASA, without violating its sampling proof, is given by a typical ratio of the number of generated to accepted states. <p> In ASA, if annealing is forsaken, and QUENCHing turned on, voiding the proof of sampling, remarkable increases of speed can be obtained, apparently sometimes even greater than other `greedy' algorithms. For example, in <ref> (Ingber and Rosen, 1992) </ref> along with 5 GA test problems from the UCSD GA archive, another harder problem (the ASA_TEST problem that comes with the ASA code) was used. In (Ingber, 1993b) QUENCHing was applied to this harder problem.
Reference: <author> INGBER, L. AND SWORDER, </author> <title> D.D., (1991) Statistical mechanics of combat with human factors, </title> <journal> Mathl. Comput. Modelling 15, </journal> <volume> 11, </volume> <pages> 99-127. </pages>
Reference-contexts: I hav e used ASA in several systems, ranging from combat analysis (Ingber, Fujio, and Wehner, 1991; Ingber and Sworder, 1991), to finance (Ingber, 1990; Ingber, Wehner et al, 1991), to neuroscience <ref> (Ingber, 1991) </ref>, to a set of test problems (Ingber and Rosen, 1992), to a new technique combining the ASA Lessons Learned -12- Lester Ingber power of SA with the physics of large-scale systems (Ingber, 1992).
Reference: <author> INGBER, L., WEHNER, M.F., JABBOUR, G.M., AND BARNHILL, </author> <title> T.M., (1991) Application of statistical mechanics methodology to term-structure bond-pricing models, </title> <journal> Mathl. Comput. Modelling 15, </journal> <volume> 11, </volume> <pages> 77-98. </pages>
Reference-contexts: I hav e used ASA in several systems, ranging from combat analysis (Ingber, Fujio, and Wehner, 1991; Ingber and Sworder, 1991), to finance (Ingber, 1990; Ingber, Wehner et al, 1991), to neuroscience <ref> (Ingber, 1991) </ref>, to a set of test problems (Ingber and Rosen, 1992), to a new technique combining the ASA Lessons Learned -12- Lester Ingber power of SA with the physics of large-scale systems (Ingber, 1992).
Reference: <author> KIMURA, K. AND TAKI, K., </author> <title> (1991) Time-homogeneous parallel annealing algorithm, </title> <type> Report TR-673, </type> <institution> Tokyo, Japan, Institute for New Generation Computer Technology. </institution>
Reference-contexts: Parallel code It is quite difficult to directly parallelize an SA algorithm (Ingber, 1993b), e.g., without incurring very restrictive constraints on temperature schedules <ref> (Kimura and Taki, 1991) </ref>, or violating an associated sampling proof (Frost, 1993). However, the fat tail of ASA permits parallelization of developing generated states prior to subjecting them to the acceptance test (Ingber, 1992).
Reference: <author> KIRKPATRICK, S., GELATT, C.D., JR., AND VECCHI, </author> <title> M.P., (1983) Optimization by simulated annealing, </title> <booktitle> Science 220, </booktitle> <volume> 4598, </volume> <pages> 671-680. </pages>
Reference-contexts: ASA ALGORITHM ASA Lessons Learned -3- Lester Ingber 2.1. Standard Simulated Annealing (SA) The Metropolis Monte Carlo integration algorithm (Metropolis et al, 1953) was generalized by the Kirkpatrick algorithm to include a temperature schedule for efficient searching <ref> (Kirkpatrick et al, 1983) </ref>. A sufficiency proof was then shown to put an lower bound on that schedule as 1/ log (t), where t is an artificial time measure of the annealing schedule (Geman and Geman, 1984). <p> This method was generalized to fitting non-convex cost-functions arising in a variety of problems, e.g., finding the optimal wiring for a densely wired computer chip <ref> (Kirkpatrick et al, 1983) </ref>. The choices of probability distributions described in this section are generally specified as Boltzmann annealing (BA) (Szu and Hartley, 1987). <p> Rather, this methodology can be readily extended to any problem for which a reasonable probability density h (Dx) can be formulated <ref> (Kirkpatrick et al, 1983) </ref>. 2.1.2. Simulated quenching (SQ) Many researchers have found it very attractive to take advantage of the ease of coding and implementing SA, utilizing its ability to handle quite complex cost functions and constraints. <p> Of course, as stated above, the use of Monte Carlo techniques for performing integrals (Metropolis et al, 1953) is generally credited to be the origin of the development of SA <ref> (Kirkpatrick et al, 1983) </ref>. However, importance sampling with the fastest permitted temperature schedules often can lead to quite poor resolutions of local minima which may substantially contribute to integrals. Then, the rates of annealing must be slowed down, e.g., using inverse QUENCHing, to get better resolution.
Reference: <author> MA, S.-K., </author> <title> (1985) Statistical Mechanics, </title> <address> Philadelphia, </address> <publisher> World Scientific. </publisher>
Reference-contexts: The problem then reduces to finding T (k) to satisfy Eq. (10). Note that, given a very large space to sample, often at best only a weak ergodicity can be assumed for this proof, and any such ergodicity even for well-understood physical systems is an open area of research <ref> (Ma, 1985) </ref>.
Reference: <author> MATHEWS, J. AND WALKER, </author> <title> R.L., </title> <booktitle> (1970) Mathematical Methods of Physics, 2nd ed., </booktitle> <address> New York, NY, </address> <publisher> Benjamin. </publisher>
Reference-contexts: of probability distributions p (x); and the energy distribution per state d ((e (x))), giving an aggregate energy E, x The principle of maximizing the entropy, S, S = - S p (x) ln [ p (x)/ p (x)] , (3) where x represents a reference state, using Lagrange multipliers <ref> (Mathews and Walker, 1970) </ref> to constrain the energy to average value T , leads to the most likely Gibbs distribution G (x), G (x) = Z in terms of the normalizing partition function Z , and the Hamiltonian H operator as the energy function, Z = S exp ((-H (x)/T ))
Reference: <author> METROPOLIS, N., ROSENBLUTH, A.W., ROSENBLUTH, M.N., TELLER, A.H., AND TELLER, E., </author> <title> (1953) Equation of state calculations by fast computing machines, </title> <journal> J. Chem. Phys. </journal> <volume> 21, 6, </volume> <month> 1087-1092. </month> <title> ASA Lessons Learned -26- Lester Ingber PENNA, T.J.P., (1994) Traveling salesman problem and Tsallis statistics, </title> <journal> Phys. Rev. </journal> <volume> E 50, 6, </volume> <month> R1-R3. </month>
Reference-contexts: Problems with the code can be addressed to ingber@alumni.caltech.edu. Requests to be placed on the ASA mailing list should be addressed to asa-request@alumni.caltech.edu. 2. ASA ALGORITHM ASA Lessons Learned -3- Lester Ingber 2.1. Standard Simulated Annealing (SA) The Metropolis Monte Carlo integration algorithm <ref> (Metropolis et al, 1953) </ref> was generalized by the Kirkpatrick algorithm to include a temperature schedule for efficient searching (Kirkpatrick et al, 1983). <p> Boltzmann annealing (BA) Credit for the first simulated annealing is generally given to a Monte Carlo importance-sampling technique for doing large-dimensional path integrals arising in statistical physics problems <ref> (Metropolis et al, 1953) </ref>. This method was generalized to fitting non-convex cost-functions arising in a variety of problems, e.g., finding the optimal wiring for a densely wired computer chip (Kirkpatrick et al, 1983). <p> For BA, if T (k) is selected to be Eq. (8), then Eq. (7) gives ASA Lessons Learned -6- Lester Ingber S g k k=k 0 S 1/k = . (11) Although there are sound physical principles underlying the choices of Eqs. (7) and (1) <ref> (Metropolis et al, 1953) </ref>, it was noted that this method of finding the global minimum in x-space was not limited to physics examples requiring bona fide temperatures and energies. <p> ASA sampling Since ASA accomplishes its fit by importance sampling the space of parameters, it would seem that this process should provide a good sampling technique for other purposes, e.g., performing integrals. Of course, as stated above, the use of Monte Carlo techniques for performing integrals <ref> (Metropolis et al, 1953) </ref> is generally credited to be the origin of the development of SA (Kirkpatrick et al, 1983). However, importance sampling with the fastest permitted temperature schedules often can lead to quite poor resolutions of local minima which may substantially contribute to integrals.
Reference: <author> PINCUS, M., </author> <title> (1970) A Monte Carlo method for the approximate solution of certain types of constrained optimization problems, </title> <journal> Oper. Res. </journal> <volume> 18, </volume> <pages> 1225-1228. </pages>
Reference: <author> REINELT, G., </author> <title> (1994) The Traveling Salesman, computational solutions for TSP applications, </title> <publisher> Berling, Springer. </publisher>
Reference-contexts: The Traveling Salesman (TSP) Problem is an excellent example of such a situation; the best search algorithms for the TSP problem are hand-tailored for it <ref> (Reinelt, 1994) </ref> . 4.2.5. Shubert problem In (Cvijovic and Klinowski, 1995) some strong claims about the superiority of taboo search over genetic algorithms and simulated annealing were made.
Reference: <author> ROSEN, B., </author> <title> (1992) Function optimization based on advanced simulated annealing, </title> <booktitle> IEEE Workshop on Physics and Computation - PhysComp '92 289-293. </booktitle>
Reference-contexts: I hav e used ASA in several systems, ranging from combat analysis (Ingber, Fujio, and Wehner, 1991; Ingber and Sworder, 1991), to finance (Ingber, 1990; Ingber, Wehner et al, 1991), to neuroscience (Ingber, 1991), to a set of test problems <ref> (Ingber and Rosen, 1992) </ref>, to a new technique combining the ASA Lessons Learned -12- Lester Ingber power of SA with the physics of large-scale systems (Ingber, 1992). <p> Nevertheless, it should be expected that GA may be better suited for some problems than SA. 4.2.2. Comparing BA, FA, and ASA A study was made comparing the performance among Boltzmann annealing (BA), fast annealing (FA), and ASA, using the difficult test problem that comes with the ASA code <ref> (Rosen, 1992) </ref>. Only ASA regularly attained the global minimum, and it was more efficient in attaining regular minima at each comparable number of generated states than BA or FA. <p> In ASA, if annealing is forsaken, and QUENCHing turned on, voiding the proof of sampling, remarkable increases of speed can be obtained, apparently sometimes even greater than other `greedy' algorithms. For example, in <ref> (Ingber and Rosen, 1992) </ref> along with 5 GA test problems from the UCSD GA archive, another harder problem (the ASA_TEST problem that comes with the ASA code) was used. In (Ingber, 1993b) QUENCHing was applied to this harder problem.
Reference: <author> SCHRAUDOLPH, N.N. AND GREFENSTETTE, J.J., </author> <title> (1991) A Users Guide to GAUCSD 1.2, </title> <type> Report, </type> <institution> La Jolla, CA, University of California at San Diego. </institution>
Reference-contexts: Examples of Comparisons 4.2.1. Genetic algorithms (GA) A direct comparison was made between ASA/VFSR and a publicly available genetic algorithm (GA) code, using a test suite already adapted and adopted for GA <ref> (Schraudolph and Grefenstette, 1991) </ref>. In each case, ASA outperformed the GA problem. It should be clear that GA is a class of algorithms that are interesting in their own right.
Reference: <author> SZU, H. AND HARTLEY, R., </author> <title> (1987) Fast simulated annealing, </title> <journal> Phys. Lett. </journal> <volume> A 122, </volume> <pages> 3-4, 157-162. </pages>
Reference-contexts: This method was generalized to fitting non-convex cost-functions arising in a variety of problems, e.g., finding the optimal wiring for a densely wired computer chip (Kirkpatrick et al, 1983). The choices of probability distributions described in this section are generally specified as Boltzmann annealing (BA) <ref> (Szu and Hartley, 1987) </ref>. <p> For the purposes of this paper, a heuristic demonstration follows, to show that Eq. (8) will suffice to give a global minimum of E (x) <ref> (Szu and Hartley, 1987) </ref>. <p> Specifically, it was noted that the Cauchy distribution has some definite advantages over the Boltzmann form <ref> (Szu and Hartley, 1987) </ref>. The Cauchy distribution they define is g (Dx) = (Dx 2 + T 2 ) (D+1)/2 which has a fatter tail than the Gaussian form of the Boltzmann distribution, permitting easier access to test local minima in the search for the desired global minimum. <p> The method of FA is thus seen to have an annealing schedule exponentially faster than the method of BA. This method has been tested in a variety of problems <ref> (Szu and Hartley, 1987) </ref>. ASA Lessons Learned -8- Lester Ingber 2.2. Adaptive Simulated Annealing (ASA) In a variety of physical problems we have a D-dimensional parameter-space. Different parameters have different finite ranges, fixed by physical considerations, and different annealing-time-dependent sensitivities, measured by the curvature of the cost-function at local minima. <p> ASA applications The above defines this method of adaptive simulated annealing (ASA), previously called very fast simulated reannealing (VFSR) (Ingber, 1989) only named such to contrast it the previous method of fast annealing (FA) <ref> (Szu and Hartley, 1987) </ref>. The annealing schedules for the temperatures T i decrease exponentially in annealing-time k, i.e., T i = T i0 exp (-c i k 1/D ).
Reference: <author> TANG, X.Z., TRACY, E.R., BOOZER, A.D., DEBRAUW, A., AND BRO WN, R., </author> <title> (1995) Symbol sequence statistics in noisy chaotic signal reconstruction, </title> <journal> Phys. Rev. </journal> <volume> E 51, 4, </volume> <publisher> (in press). </publisher>
Reference: <author> VAN LAARHOVEN, P.J.M. AND AARTS, E.H.L., </author> <title> (1987) Simulated Annealing: Theory and Applications, </title> <address> Dordrecht, The Netherlands, D. </address> <publisher> Reidel. </publisher>
Reference-contexts: Fast annealing (FA) Although there are many variants and improvements made on the standard Boltzmann algorithm described above, many textbooks finish just about at this point without going into more detail about other algorithms that depart from this explicit algorithm <ref> (van Laarhoven and Aarts, 1987) </ref>. Specifically, it was noted that the Cauchy distribution has some definite advantages over the Boltzmann form (Szu and Hartley, 1987).
Reference: <author> WOFSEY, M., </author> <title> (1993) Technology: Shortcut tests validity of complicated formulas, </title> <journal> The Wall Street Journal 222, </journal> <volume> 60, </volume> <month> B1. </month>
Reference-contexts: The popularity of the code, roughly measured by the size of the ASA_list of people specifically requesting to be placed on a monthly list of updatesnow at about 500 namesincreased about a factor of two after an article in The Wall Street Journal <ref> (Wofsey, 1993) </ref>. I have had contact with thousands of users via e-mail, a few via other channels of communication, and it is clear that these are just a small fraction of people that have at least tried to use the code. <p> Some published acknowledgments to use of the code are in the asa_papers file of the ASA archive; the disciplines range from physics (Brown et al, 1994; Tang et al, 1995) to neural networks (Cohen, 1994; Indiveri et al, 1993) to difficult imaging problems (Wu and Levine, 1993) to finance <ref> (Wofsey, 1993) </ref>, where most of the latter uses are kept proprietary. 3. ASA OPTIONS 3.1.
Reference: <author> WOLPERT, D.H. AND MACREADY, </author> <title> W.G., (1995) No free lunch theorems for search, </title> <type> Report, </type> <institution> Santa Fe, NM, Santa Fe Institute. </institution>
Reference-contexts: Most likely, the researcher must work to learn more about the system to best apply a given algorithm. There is a formal proof that all algorithms should perform the same over all problems <ref> (Wolpert and Macready, 1995) </ref>. This obviously is of little practical help for a given particular problem, albeit the authors claim that their approach offers some guidelines for specific algorithms. <p> The use of TSP as a test for comparisons among SA techniques seems quite inappropriate. To quote another source <ref> (Wolpert and Macready, 1995) </ref>: As an example of this, it is well known that generic methods (like simulated annealing and genetic algorithms) are unable to compete with carefully hand-crafted solutions for specific search problems.
Reference: <author> WU, K. AND LEVINE, </author> <title> M.D., (1993) 3-D object representation using parametric geons, </title> <institution> TR-CIM-93-13, Montreal, Canada, Center for Intelligent Machines, McGill University. </institution>
Reference-contexts: Some published acknowledgments to use of the code are in the asa_papers file of the ASA archive; the disciplines range from physics (Brown et al, 1994; Tang et al, 1995) to neural networks (Cohen, 1994; Indiveri et al, 1993) to difficult imaging problems <ref> (Wu and Levine, 1993) </ref> to finance (Wofsey, 1993), where most of the latter uses are kept proprietary. 3. ASA OPTIONS 3.1.
References-found: 35

