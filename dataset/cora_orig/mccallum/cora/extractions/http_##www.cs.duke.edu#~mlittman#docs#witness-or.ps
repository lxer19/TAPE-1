URL: http://www.cs.duke.edu/~mlittman/docs/witness-or.ps
Refering-URL: http://www.cs.duke.edu/~mlittman/topics/pomdp-page.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Efficient dynamic-programming updates in partially observable Markov decision processes  
Author: Michael L. Littman Anthony R. Cassandra Leslie Pack Kaelbling 
Date: December 12, 1995  
Abstract: We examine the problem of performing exact dynamic-programming updates in partially observable Markov decision processes (pomdps) from a computational complexity viewpoint. Dynamic-programming updates are a crucial operation in a wide range of pomdp solution methods and we find that it is intractable to perform these updates on piecewise-linear convex value functions for general pomdps. We offer a new algorithm, called the witness algorithm, which can compute updated value functions efficiently on a restricted class of pomdps in which the number of linear facets is not too great. We compare the witness algorithm to existing algorithms analytically and empirically and find that it is the fastest algorithm over a wide range of pomdp sizes.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: Although it is known that solving multi-stage pomdps is computationally intractable [7], we have found that we are often able to compute exact answers or reasonable approximations to larger pomdps than have been reported elsewhere <ref> [1, 4] </ref>. 20 Nevertheless, these exact algorithms do not appear to be practical for moderately large problems, and even small problems can exhibit explosive growth of the number of vectors in successive iterations.
Reference: [2] <author> Hsien-Te Cheng. </author> <title> Algorithms for Partially Observable Markov Decision Processes. </title> <type> PhD thesis, </type> <institution> University of British Columbia, British Columbia, Canada, </institution> <year> 1988. </year>
Reference-contexts: Problem Statement Given a set of vectors representing a value function v 2 V X , compute a minimal set of vectors 0 representing Hv. Although many algorithms for solving this problem have been proposed over the last twenty years <ref> [11, 6, 3, 2, 15] </ref>, the algorithm described in this work is the first developed with close attention to issues of computational complexity and is currently the most efficient exact algorithm for performing dynamic-programming updates over a wide range of pomdp sizes. We will now define the dynamic-programming operator H. <p> Thus, even if a vector could be identified as useful in constant time, the running times of these algorithms are at least exponential in jZj, making them of little use for solving pomdps with anything but the smallest observation sets. Cheng's linear support and relaxed region algorithms <ref> [2] </ref> make use of special-purpose routines that enumerate the vertices of each linear region of the value function. For polynomially action-output-bounded pomdps, the number of linear regions is guaranteed to be small, but the number of vertices of each region can be quite large. <p> The witness approach to computing a bears a resemblance to Cheng's linear support algorithm <ref> [2] </ref> and Lark's algorithm [15]. The fundamental difference is that it does not exhaustively enumerate vectors or vertices. <p> In particular, are typical pomdps polynomially action-output bounded? Also, the analyses ignore low-order terms, which might dominate the running time for small pomdps. In this section we empirically compare the witness algorithm to Cheng's linear support algorithm <ref> [2, 5] </ref> and Lark's enumeration algorithm [15]. We chose the linear support algorithm because it has been shown to outperform Sondik's one-pass algorithm and Cheng's relaxed region algorithm on a variety of pomdps [2] and appears to be the leading exact algorithm. <p> In this section we empirically compare the witness algorithm to Cheng's linear support algorithm [2, 5] and Lark's enumeration algorithm [15]. We chose the linear support algorithm because it has been shown to outperform Sondik's one-pass algorithm and Cheng's relaxed region algorithm on a variety of pomdps <ref> [2] </ref> and appears to be the leading exact algorithm. We chose Lark's enumeration algorithm because it seems to be the leading enumeration algorithm; in principle it should always outperform Monahan's algorithm, and did so in all our early experimental trials. Our comparisons use CPU time as the measure of performance.
Reference: [3] <author> James N. Eagle. </author> <title> The optimal search for a moving target when the search path is constrained. </title> <journal> Operations Research, </journal> <volume> 32(5) </volume> <pages> 1107-1115, </pages> <year> 1984. </year>
Reference-contexts: Problem Statement Given a set of vectors representing a value function v 2 V X , compute a minimal set of vectors 0 representing Hv. Although many algorithms for solving this problem have been proposed over the last twenty years <ref> [11, 6, 3, 2, 15] </ref>, the algorithm described in this work is the first developed with close attention to issues of computational complexity and is currently the most efficient exact algorithm for performing dynamic-programming updates over a wide range of pomdp sizes. We will now define the dynamic-programming operator H. <p> Proof: Several algorithms for finding 0 work by enumerating the vectors in G and then identifying which of these vectors is useful: Monahan's algorithm [6] was the first and later Eagle <ref> [3] </ref> and Lark [15] provided improvements. However, all these algorithms, regardless of their details, build G, the size of which is jAjjj jZj . <p> Since fl k <ref> [3] </ref> = 0, for all g 2 T , X P (z; a ) g (z)] = x : (2) The a 0 action yields an immediate reward of zero and then causes a transition to system state 1, 2, or 3, under observation z, weighted according to the appropriate vector.
Reference: [4] <author> Michael Littman, Anthony Cassandra, and Leslie Pack Kaelbling. </author> <title> Learning policies for partially observable environments: Scaling up. </title> <editor> In Ar-mand Prieditis and Stuart Russell, editors, </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 362-370, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Although it is known that solving multi-stage pomdps is computationally intractable [7], we have found that we are often able to compute exact answers or reasonable approximations to larger pomdps than have been reported elsewhere <ref> [1, 4] </ref>. 20 Nevertheless, these exact algorithms do not appear to be practical for moderately large problems, and even small problems can exhibit explosive growth of the number of vectors in successive iterations. <p> Our current work tries to overcome these difficulties using approximate representations of the infinite-horizon value function <ref> [4, 8] </ref>. A Appendix A.1 Proof of Lemma 1 Given an information state x and a set of vectors F , let fl fl be the lexicographic maximum vector in F such that x fl fl = max fl2F (x fl).
Reference: [5] <author> William S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observable Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28 </volume> <pages> 47-66, </pages> <year> 1991. </year>
Reference-contexts: of the algorithms supports these analyses and shows that the witness algorithm is able to solve pomdps of a wider range of sizes; however, all exact solution methods are intractable for most large practical problems. 2 Problem Formulation The reader is referred to the excellent surveys by Monahan [6], Lovejoy <ref> [5] </ref>, and White [15] for a thorough introduction to solving pomdps algorithmically. This section briefly presents notation and background results that enable us to define the problem addressed in this work. Our notation follows that of White. <p> In particular, are typical pomdps polynomially action-output bounded? Also, the analyses ignore low-order terms, which might dominate the running time for small pomdps. In this section we empirically compare the witness algorithm to Cheng's linear support algorithm <ref> [2, 5] </ref> and Lark's enumeration algorithm [15]. We chose the linear support algorithm because it has been shown to outperform Sondik's one-pass algorithm and Cheng's relaxed region algorithm on a variety of pomdps [2] and appears to be the leading exact algorithm.
Reference: [6] <author> George E. Monahan. </author> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28 </volume> <pages> 1-16, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: empirical study of the algorithms supports these analyses and shows that the witness algorithm is able to solve pomdps of a wider range of sizes; however, all exact solution methods are intractable for most large practical problems. 2 Problem Formulation The reader is referred to the excellent surveys by Monahan <ref> [6] </ref>, Lovejoy [5], and White [15] for a thorough introduction to solving pomdps algorithmically. This section briefly presents notation and background results that enable us to define the problem addressed in this work. Our notation follows that of White. <p> Problem Statement Given a set of vectors representing a value function v 2 V X , compute a minimal set of vectors 0 representing Hv. Although many algorithms for solving this problem have been proposed over the last twenty years <ref> [11, 6, 3, 2, 15] </ref>, the algorithm described in this work is the first developed with close attention to issues of computational complexity and is currently the most efficient exact algorithm for performing dynamic-programming updates over a wide range of pomdp sizes. We will now define the dynamic-programming operator H. <p> Proof: Several algorithms for finding 0 work by enumerating the vectors in G and then identifying which of these vectors is useful: Monahan's algorithm <ref> [6] </ref> was the first and later Eagle [3] and Lark [15] provided improvements. However, all these algorithms, regardless of their details, build G, the size of which is jAjjj jZj .
Reference: [7] <author> Christos H. Papadimitriou and John N. Tsitsiklis. </author> <title> The complexity of Markov decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3) </volume> <pages> 441-450, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: We have applied our algorithm in the context of a value-iteration procedure for approximating optimal infinite-horizon policies. We have solved problems in a variety of domains, taken from the operations research, artificial intelligence, and machine learning literatures. Although it is known that solving multi-stage pomdps is computationally intractable <ref> [7] </ref>, we have found that we are often able to compute exact answers or reasonable approximations to larger pomdps than have been reported elsewhere [1, 4]. 20 Nevertheless, these exact algorithms do not appear to be practical for moderately large problems, and even small problems can exhibit explosive growth of the
Reference: [8] <author> Ronald Parr and Stuart Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: Our current work tries to overcome these difficulties using approximate representations of the infinite-horizon value function <ref> [4, 8] </ref>. A Appendix A.1 Proof of Lemma 1 Given an information state x and a set of vectors F , let fl fl be the lexicographic maximum vector in F such that x fl fl = max fl2F (x fl).
Reference: [9] <author> Katsushige Sawaki and Akira Ichikawa. </author> <title> Optimal control for partially observable Markov decision processes over an infinite horizon. </title> <journal> Journal of the Operations Research Society of Japan, </journal> <volume> 21(1) </volume> <pages> 1-14, </pages> <month> March </month> <year> 1978. </year>
Reference-contexts: We will define H more carefully in the next section, but it is worth noting here its critical role in a host of pomdp solution methods. Dynamic-programming updates can be used for solving finite-horizon pomdps [11], finding approximate solutions to infinite-horizon discounted pomdps by value iteration <ref> [9] </ref>, performing policy improvement in infinite-horizon algorithms [13], computing optimal average-reward policies [12], and accelerating convergence of successive approximation methods via reward revision [16].
Reference: [10] <author> Alexander Schrijver. </author> <title> Theory of linear and integer programming. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, NY, </address> <year> 1986. </year>
Reference-contexts: total number of iterations in H a is 1 + jZj (jj 1)j a j + j a j: The statements in the loop in H a can all be implemented to run in poly-nomial time; this includes inR, since polynomial-time algorithms for linear programming with polynomial-precision rational numbers exist <ref> [10] </ref>. The total running time of H a is therefore bounded by a polynomial in jSj, jZj, jAj, jj and j a j. The H routine calls H a for each a 2 A and then calls inR once for each vector found.
Reference: [11] <author> Richard D. Smallwood and Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 1071-1088, </pages> <year> 1973. </year>
Reference-contexts: We will define H more carefully in the next section, but it is worth noting here its critical role in a host of pomdp solution methods. Dynamic-programming updates can be used for solving finite-horizon pomdps <ref> [11] </ref>, finding approximate solutions to infinite-horizon discounted pomdps by value iteration [9], performing policy improvement in infinite-horizon algorithms [13], computing optimal average-reward policies [12], and accelerating convergence of successive approximation methods via reward revision [16]. <p> Problem Statement Given a set of vectors representing a value function v 2 V X , compute a minimal set of vectors 0 representing Hv. Although many algorithms for solving this problem have been proposed over the last twenty years <ref> [11, 6, 3, 2, 15] </ref>, the algorithm described in this work is the first developed with close attention to issues of computational complexity and is currently the most efficient exact algorithm for performing dynamic-programming updates over a wide range of pomdp sizes. We will now define the dynamic-programming operator H. <p> Smallwood and Sondik <ref> [11] </ref> showed that H a and H preserve piecewise linearity and convexity and therefore that Hv can be represented in a convenient form if v is piecewise linear and convex.
Reference: [12] <author> Edward Sondik. </author> <title> The Optimal Control of Partially Observable Markov Processes. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1971. </year>
Reference-contexts: Dynamic-programming updates can be used for solving finite-horizon pomdps [11], finding approximate solutions to infinite-horizon discounted pomdps by value iteration [9], performing policy improvement in infinite-horizon algorithms [13], computing optimal average-reward policies <ref> [12] </ref>, and accelerating convergence of successive approximation methods via reward revision [16]. Most of these approaches treat H as a primitive operation, so any improvement in the speed with which H can be computed immediately translates into speed-ups for a wide range of advanced pomdp algorithms.
Reference: [13] <author> Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Research, </journal> <volume> 26(2), </volume> <year> 1978. </year>
Reference-contexts: Dynamic-programming updates can be used for solving finite-horizon pomdps [11], finding approximate solutions to infinite-horizon discounted pomdps by value iteration [9], performing policy improvement in infinite-horizon algorithms <ref> [13] </ref>, computing optimal average-reward policies [12], and accelerating convergence of successive approximation methods via reward revision [16].
Reference: [14] <author> L. G. Valiant and V. V. Vazirani. </author> <title> NP is as easy as detecting unique solutions. </title> <journal> Theoretical Computer Science, </journal> <volume> 47(1) </volume> <pages> 85-93, </pages> <year> 1986. </year>
Reference-contexts: A satisfying assignment maps each of the variables to either "true" or "false" so the entire formula evaluates to "true." There is a corollary, proved by Valiant and Vazirani <ref> [14] </ref>, that implies that there exists a polynomial-time algorithm for finding a satisfying assignment for a formula that is guaranteed to have at most one satisfying assignment only if RP = N P, a long-open problem. 3 We can show that a polynomial-time algorithm for solving polyno-mially output-bounded pomdps could be
Reference: [15] <author> Chelsea C. White, III. </author> <title> Partially observed Markov decision processes: A survey. </title> <journal> Annals of Operations Research, </journal> <volume> 32, </volume> <year> 1991. </year>
Reference-contexts: supports these analyses and shows that the witness algorithm is able to solve pomdps of a wider range of sizes; however, all exact solution methods are intractable for most large practical problems. 2 Problem Formulation The reader is referred to the excellent surveys by Monahan [6], Lovejoy [5], and White <ref> [15] </ref> for a thorough introduction to solving pomdps algorithmically. This section briefly presents notation and background results that enable us to define the problem addressed in this work. Our notation follows that of White. <p> Problem Statement Given a set of vectors representing a value function v 2 V X , compute a minimal set of vectors 0 representing Hv. Although many algorithms for solving this problem have been proposed over the last twenty years <ref> [11, 6, 3, 2, 15] </ref>, the algorithm described in this work is the first developed with close attention to issues of computational complexity and is currently the most efficient exact algorithm for performing dynamic-programming updates over a wide range of pomdp sizes. We will now define the dynamic-programming operator H. <p> Proof: Several algorithms for finding 0 work by enumerating the vectors in G and then identifying which of these vectors is useful: Monahan's algorithm [6] was the first and later Eagle [3] and Lark <ref> [15] </ref> provided improvements. However, all these algorithms, regardless of their details, build G, the size of which is jAjjj jZj . <p> The witness approach to computing a bears a resemblance to Cheng's linear support algorithm [2] and Lark's algorithm <ref> [15] </ref>. The fundamental difference is that it does not exhaustively enumerate vectors or vertices. <p> In particular, are typical pomdps polynomially action-output bounded? Also, the analyses ignore low-order terms, which might dominate the running time for small pomdps. In this section we empirically compare the witness algorithm to Cheng's linear support algorithm [2, 5] and Lark's enumeration algorithm <ref> [15] </ref>. We chose the linear support algorithm because it has been shown to outperform Sondik's one-pass algorithm and Cheng's relaxed region algorithm on a variety of pomdps [2] and appears to be the leading exact algorithm.
Reference: [16] <author> Chelsea C. White, III and William T. Scherer. </author> <title> Solution procedures for partially observed Markov decision processes. </title> <journal> Operations Research, </journal> <volume> 37(5) </volume> <pages> 791-797, </pages> <month> September-October </month> <year> 1989. </year> <month> 31 </month>
Reference-contexts: Dynamic-programming updates can be used for solving finite-horizon pomdps [11], finding approximate solutions to infinite-horizon discounted pomdps by value iteration [9], performing policy improvement in infinite-horizon algorithms [13], computing optimal average-reward policies [12], and accelerating convergence of successive approximation methods via reward revision <ref> [16] </ref>. Most of these approaches treat H as a primitive operation, so any improvement in the speed with which H can be computed immediately translates into speed-ups for a wide range of advanced pomdp algorithms.
References-found: 16

