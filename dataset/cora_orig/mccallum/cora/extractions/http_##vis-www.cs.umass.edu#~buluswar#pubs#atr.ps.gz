URL: http://vis-www.cs.umass.edu/~buluswar/pubs/atr.ps.gz
Refering-URL: http://vis-www.cs.umass.edu/~buluswar/
Root-URL: 
Email: buluswar@cs.umass.edu draper@cs.colostate.edu  
Title: Color Recognition by Learning: ATR in Color Images  
Author: Shashi D. Buluswar Bruce A. Draper 
Address: Amherst, MA. U.S.A. Ft. Collins, CO, U.S.A.  
Affiliation: Dept. of Computer Science Dept. of Computer Science University of Massachusetts Colorado State University  
Abstract: Traditional methods for ATR (Automatic Target Recognition) use infrared (IR) sensors for detecting heat emanating from targets. IR-based ATR techniques are susceptible to sensor-induced errors; for instance, targets may not be detected if they are cold (when vehicle engines are turned off), or when the background is hot (on a hot day). This work presents an approach to real-time color-based ATR which uses multivariate decision trees for recursive non-parametric function approximation to learn the color of a target from training samples, and then detects targets by classifying pixels based on the approximated function. Tests of the color-based system, sanctioned by the U.S. Defense Advanced Research Projects Agency Unmanned Ground Vehicle Project (DARPA-UGV), have resulted in a 90% target detection rate (compared to the 45% detection rate of the IR-based system developed for the same tests). When the color system was used in conjunction with the IR-based system, 100% of the targets were detected.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.R. Beveridge, D. Panda and T. Yachik, </author> <title> November 1993 Fort Carson RSTA Data Collection, </title> <institution> Colorado State University Technical Report CSS-94-118, </institution> <year> 1994. </year>
Reference-contexts: This method has been implemented in a system for ATR of camouflaged military vehicles in real-time, and has been tested in a DARPA-sanctioned study [19] on the Ft. Carson data set <ref> [1] </ref> and at the DARPA UGV Demo-C [11]. In each test, over 90% of the targets were detected (compared to a 45% detection rate by the IR-based system). <p> Finally, overlapping bounding rectangles are merged, to produce a region-of-interest image, with the boxes drawn around the targets; figure 4 shows the result of grouping and extracting target regions from the corresponding binary image. 6 Results The Ft. Carson data set <ref> [1] </ref>, collected in a DARPA-sanctioned study, consists of about 150 color and IR images of camouflaged military vehicles under conditions that vary from bright (and hot) daytime to dark (and cool) dusk; the distance to the targets ranges from 100 to about 500 meters.
Reference: [2] <author> C.E. Brodley and P.E. Utgoff, </author> <title> "Multivariate decision trees", </title> <booktitle> Machine Learning, </booktitle> <year> 1995, </year> <pages> pp 45-57. </pages>
Reference-contexts: Multivariate decision trees create piecewise-linear approximations to surfaces in feature space by recursively dividing feature space with hyperplanes, and have been shown to produce good classification results from relatively few training samples. Multivariate decision trees <ref> [2] </ref> recursively subdivide the feature space by linear threshold units (LTU's). Each LTU is a binary test represented by linear combinations of feature values and associated weights. Each division attempts to separate, in a set of known instances (the training set), target instances from non-targets. <p> Thus, each node in a decision tree is either a decision or a class. Figure 3 (left) shows a decision-tree operating in a three-dimensional feature space, where the two classes being separated are the '+'s and the '-'s; Brodley <ref> [2] </ref> describes further details. The LTU weights are approximated using the Recursive Least Squares (RLS) algorithm, which minimizes the mean squared error between the estimated y i and true y i values, (y i y i ) 2 of the selected features over a number of training instances. <p> If the (right). leaf-node results in better performance, the subtree is replaced by it <ref> [2] </ref>. 5 ATR system using MDT A decision tree for the camouflaged targets is built by providing sample pixels of the targets and background (e.g., vegetation, sky, rocks, etc.) from images taken under various conditions.
Reference: [3] <author> G. Buchsbaum, </author> <title> "A Spatial Processor Model for Object Colour Perception", </title> <journal> Journal of the Franklin Institute, </journal> <pages> 310 pp 1-26, </pages> <year> 1980. </year>
Reference-contexts: in two spatially distinct parts of the image, and that the unknown illumination falls within the gamut of known artificial illuminants; Ohta [16] assumes artificial illumination constrained by the CIE model to reduce performance errors; Novak and Shafer [15, 18] assume a point light source and pure specular reflection; Buchsbaum <ref> [3] </ref> assumes that the surface reflectance averaged over the entire image is grey; Maloney's work [12] is a refinement of Buchsbaum's but has been applied only under the constraints of an indoor world with Munsell color chips.
Reference: [4] <author> S. Buluswar, </author> <title> Trichromatic model of Daylight Variation, </title> <institution> University of Mas-sachusetts Computer Science Department, </institution> <type> technical report, </type> <institution> UM-CS-1995-012. </institution>
Reference-contexts: In RGB space, the parabola stretches out into a thin curved surface <ref> [4] </ref>. The effect of illumination geometry and viewing geometry depend on the reflectance of the surface. Most realistic surfaces have reflectances that have a mixture lambertian and specular components.
Reference: [5] <author> J. Crisman and C. Thorpe, </author> <title> "Color Vision for Road Following", Vision and Navigation: </title> <publisher> The Carnegie Mellon NAVLAB, Kluwer, </publisher> <year> 1990. </year>
Reference-contexts: To recognize targets in outdoor scenes, we therefore need to select a classification scheme that performs well on arbitrarily shaped clusters in feature space. By definition, parametric classifiers (such as minimum-distance classifiers, as used by Crisman <ref> [5] </ref>) can be ruled out, since the underlying equations are unknown. Based on their success in other areas of non-parametric approximation, neural networks (i.e., feed-forward back-propagation nets) and multivariate decision trees were considered.
Reference: [6] <author> G.D. Finlayson, B.V. Funt and K. Barnard, </author> <title> "Color Constancy Under Varying Illumination", </title> <booktitle> Proceedings of the Fifth International Conference on Computer Vision, </booktitle> <pages> pp 720-725, </pages> <year> 1995. </year>
Reference-contexts: The color of daylight changes significantly due to the sun-angle and weather conditions, and the position and orientation of the target are also expected to vary. Consequently, the apparent color of a target varies under realistic conditions. Previous methods in computational color recognition, such as color constancy algorithms <ref> [18, 7, 6] </ref>, have dealt with varying color in highly constrained environments, and are generally not applicable to outdoor imagery. It will be shown that as imaging conditions vary, the apparent color of objects forms characteristic types of clusters in color RGB space, depending on the surface properties. <p> Unfortunately, in order to separate illumination conditions from surface reflectance effects, most color constancy algorithms make strong assumptions about the nature of the world. For example, Forsyth [7] assumes a Mondrian world with constant illumination without inter-reflections or multiple light sources; Finlayson <ref> [6] </ref> assumes that surfaces with the same reflectance have been identified in two spatially distinct parts of the image, and that the unknown illumination falls within the gamut of known artificial illuminants; Ohta [16] assumes artificial illumination constrained by the CIE model to reduce performance errors; Novak and Shafer [15, 18]
Reference: [7] <author> D. </author> <title> Forsyth."A Novel Approach for Color Constancy", </title> <journal> International Journal of Computer Vision, </journal> <pages> 5 pp 5-36, </pages> <year> 1990. </year>
Reference-contexts: The color of daylight changes significantly due to the sun-angle and weather conditions, and the position and orientation of the target are also expected to vary. Consequently, the apparent color of a target varies under realistic conditions. Previous methods in computational color recognition, such as color constancy algorithms <ref> [18, 7, 6] </ref>, have dealt with varying color in highly constrained environments, and are generally not applicable to outdoor imagery. It will be shown that as imaging conditions vary, the apparent color of objects forms characteristic types of clusters in color RGB space, depending on the surface properties. <p> An illuminant-invariant measure of surface reflectance is recovered by first determining the properties of the illuminant from variations across images. Unfortunately, in order to separate illumination conditions from surface reflectance effects, most color constancy algorithms make strong assumptions about the nature of the world. For example, Forsyth <ref> [7] </ref> assumes a Mondrian world with constant illumination without inter-reflections or multiple light sources; Finlayson [6] assumes that surfaces with the same reflectance have been identified in two spatially distinct parts of the image, and that the unknown illumination falls within the gamut of known artificial illuminants; Ohta [16] assumes artificial <p> While many of these constancy algorithms are quite sophisticated and perform impressively within the specified constraints, Forsyth <ref> [7] </ref> aptly states, "Experimental results for [color constancy] algorithms running on real images are not easily found in the literature: : : Some work exists on the processes which can contribute to real world lightness constancy, but very little progress has been made in this area." 3.3 The nature of the
Reference: [8] <author> R. Gershon, A. Jepson and J. Tsotsos, </author> <title> The Effects of Ambient Illumination on the Structure of Shadows in Chromatic Images. </title> <institution> RBCV-TR-86-9, Dept. of Computer Science, University of Toronto, </institution> <year> 1986. </year>
Reference-contexts: Viewing geometry, i.e., the position and orientation of the camera with respect to the surface, determines the amount and composition of the light reaching the camera, depending on the specular content of the surface. Shadows and inter-reflections also affect the color of the light incident upon a surface <ref> [8] </ref>. Shadowing occurs either when the surface is facing away from the sun (self-shadowing), or when a second object blocks the sunlight. Inter-reflections are caused when other surfaces reflect light incident upon them, onto the surface in question.
Reference: [9] <author> B.K.P. Horn, </author> <title> Robot Vision, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Additionally, there are other problems inherent to outdoor color imagery that further complicate color-based recognition. The apparent color of an object is a function of the color of the incident light, surface reflection, illumination geometry, viewing geometry and imaging parameters <ref> [9] </ref>. Each of these factors can vary in outdoor conditions; in addition, the effect of a host of unmodeled phenomena, such as shadows and inter-reflections, is unpredictable. <p> in the literature: : : Some work exists on the processes which can contribute to real world lightness constancy, but very little progress has been made in this area." 3.3 The nature of the variation of apparent object color in outdoor scenes According to the standard model of image formation <ref> [9] </ref>, the observed color of objects in images is a function of (i) the color of the incident light (daylight), (ii) the reflectance properties of the surface of the object (iii) the illumination geometry, (iv) the viewing geometry, and (v) the imaging parameters. <p> Theoretical parametric models exist for the various phases of the image formation process <ref> [9, 10, 13, 18] </ref>, although these models appear too restrictive to be used in unconstrained imagery; still, they provide an approximate qualitative description of the variation of apparent color. <p> In RGB space, the parabola stretches out into a thin curved surface [4]. The effect of illumination geometry and viewing geometry depend on the reflectance of the surface. Most realistic surfaces have reflectances that have a mixture lambertian and specular components. Existing reflectance models of mixed reflection surfaces <ref> [9, 13, 18] </ref> are yet be applied to unconstrained imagery in the context of color-based recognition.
Reference: [10] <author> D. Judd, D. MacAdam and G. Wyszecki, </author> <title> "Spectral Distribution of Typical Daylight as a Function of Correlated Color Temperature", </title> <journal> Journal of the Optical Society of America, </journal> <volume> 54(8) </volume> <pages> 1031-1040, </pages> <year> 1964. </year>
Reference-contexts: Theoretical parametric models exist for the various phases of the image formation process <ref> [9, 10, 13, 18] </ref>, although these models appear too restrictive to be used in unconstrained imagery; still, they provide an approximate qualitative description of the variation of apparent color. <p> Theoretical parametric models exist for the various phases of the image formation process [9, 10, 13, 18], although these models appear too restrictive to be used in unconstrained imagery; still, they provide an approximate qualitative description of the variation of apparent color. The CIE model <ref> [10] </ref> states that the color of daylight varies along a characteristic curve, defined by the following equation in the CIE chromaticity space (of which RGB is a linear transform). y = 2:87x 3:0x 2 0:275; (1) where 0:25 &lt;= x &lt;= 0:38.
Reference: [11] <institution> Lockheed-Martin Corp., from DARPA UGV DEMO-C, </institution> <year> 1995. </year>
Reference-contexts: This method has been implemented in a system for ATR of camouflaged military vehicles in real-time, and has been tested in a DARPA-sanctioned study [19] on the Ft. Carson data set [1] and at the DARPA UGV Demo-C <ref> [11] </ref>. In each test, over 90% of the targets were detected (compared to a 45% detection rate by the IR-based system). <p> The IR-based ATR system used at the DARPA UGV Demo-C is based on double-window detection <ref> [11, 17] </ref>. Using this method on 25 randomly chosen images from the Ft. Carson IR data set, only 22 out of 50 targets were detected, with 5 false alarms; in addition, the only civilian vehicle in the image set was mistaken for a target. <p> In addition, the system was tested live at the UGV Demo-C, with similar results (the exact numbers from Demo-C are not available). By comparison, the IR-based system <ref> [11] </ref> detected 22 of the 50 targets, with 5 false alarms. Four of the false alarms were from background foliage, and one was a civilian vehicle.
Reference: [12] <author> L.T. Maloney and B.A. Wandell, </author> <title> "Color Constancy: A Method for Recovering Surface Spectral Reflectance", </title> <journal> Journal of the Optical Society of America, </journal> <volume> A3, </volume> <pages> pp 29-33, </pages> <year> 1986. </year>
Reference-contexts: the gamut of known artificial illuminants; Ohta [16] assumes artificial illumination constrained by the CIE model to reduce performance errors; Novak and Shafer [15, 18] assume a point light source and pure specular reflection; Buchsbaum [3] assumes that the surface reflectance averaged over the entire image is grey; Maloney's work <ref> [12] </ref> is a refinement of Buchsbaum's but has been applied only under the constraints of an indoor world with Munsell color chips.
Reference: [13] <author> S.K. Nayar, K. Ikeuchi, and T. Kanade, </author> <title> "Determining shape and reflectance of hybrid surfaces by photometric sampling", </title> <journal> IEEE Transactions on Robotics and Automation pp 418-431, </journal> <year> 1990. </year>
Reference-contexts: Theoretical parametric models exist for the various phases of the image formation process <ref> [9, 10, 13, 18] </ref>, although these models appear too restrictive to be used in unconstrained imagery; still, they provide an approximate qualitative description of the variation of apparent color. <p> In RGB space, the parabola stretches out into a thin curved surface [4]. The effect of illumination geometry and viewing geometry depend on the reflectance of the surface. Most realistic surfaces have reflectances that have a mixture lambertian and specular components. Existing reflectance models of mixed reflection surfaces <ref> [9, 13, 18] </ref> are yet be applied to unconstrained imagery in the context of color-based recognition.
Reference: [14] <author> C. Novak, S. Shafer and R. Wilson, </author> <title> "Obtaining Accurate Color Images for Machine Vision Research", </title> <booktitle> Proceedings of the SPIE, v 1250, </booktitle> <year> 1990. </year>
Reference-contexts: A number of imaging parameters cause further color shifts. For instance, wavelength-dependent displacement of light rays by the camera lens onto the image plane due to chromatic aberration can cause color mixing and blurring <ref> [14] </ref>. Nonlinear camera response and digitization errors can skew the ratio of the values in the three color bands (red, green and blue), and the dynamic range of intensity in outdoor scenes accentuates the possibility of blooming and clipping [14]. 3.2 Previous approaches to color vision under varying illumination In the <p> plane due to chromatic aberration can cause color mixing and blurring <ref> [14] </ref>. Nonlinear camera response and digitization errors can skew the ratio of the values in the three color bands (red, green and blue), and the dynamic range of intensity in outdoor scenes accentuates the possibility of blooming and clipping [14]. 3.2 Previous approaches to color vision under varying illumination In the past, color recognition under varying illumination has generally been addressed as a color constancy problem, where the goal is to match object colors under varying illumination without knowing the spectral composition of the incident light or surface reflectance. <p> The goal of imaging systems is to preserve the color of objects as they appear in the scene, depending on a few imaging parameters (focal length, response function, etc.). Unfortunately, phenomena such as clipping, blooming and nonlinearities will introduce distortions to the appearance of objects in color space <ref> [14] </ref>.
Reference: [15] <author> C. Novak and S. Shafer, </author> <title> A Method for Estimating Scene Parameters from Color Histograms, </title> <institution> Carnegie Mellon University School of Computer Science, </institution> <type> technical report, </type> <institution> CMU-CS-93-177, </institution> <year> 1993. </year>
Reference-contexts: Finlayson [6] assumes that surfaces with the same reflectance have been identified in two spatially distinct parts of the image, and that the unknown illumination falls within the gamut of known artificial illuminants; Ohta [16] assumes artificial illumination constrained by the CIE model to reduce performance errors; Novak and Shafer <ref> [15, 18] </ref> assume a point light source and pure specular reflection; Buchsbaum [3] assumes that the surface reflectance averaged over the entire image is grey; Maloney's work [12] is a refinement of Buchsbaum's but has been applied only under the constraints of an indoor world with Munsell color chips.
Reference: [16] <author> Y. Ohta and Y. Hayashi, </author> <title> "Recovery of Illuminant and Surface Colors from Images Based on the CIE Daylight", </title> <booktitle> Proceedings of the Third European Conference on Computer Vision, </booktitle> <pages> pp 235-246, </pages> <year> 1994. </year>
Reference-contexts: example, Forsyth [7] assumes a Mondrian world with constant illumination without inter-reflections or multiple light sources; Finlayson [6] assumes that surfaces with the same reflectance have been identified in two spatially distinct parts of the image, and that the unknown illumination falls within the gamut of known artificial illuminants; Ohta <ref> [16] </ref> assumes artificial illumination constrained by the CIE model to reduce performance errors; Novak and Shafer [15, 18] assume a point light source and pure specular reflection; Buchsbaum [3] assumes that the surface reflectance averaged over the entire image is grey; Maloney's work [12] is a refinement of Buchsbaum's but has
Reference: [17] <author> B.J. Schachter, </author> <title> "A Survey and Evaluation of FLIR Target Detection/Segmentation Algorithms", </title> <booktitle> DARPA Image Understanding Workshop, </booktitle> <year> 1982. </year>
Reference-contexts: One approach to this problem is to develop more sophisticated target recognition algorithms for IR images (Schachter <ref> [17] </ref> contains a review of several methods). It is our belief, however, that the gains possible through this line of research are limited due to problems inherent to the data. <p> They offer the clear advantage of being useful at any time, day or night, and can be used in many types of smoke and fog. Most IR-based ATR systems assume that the targets are warmer than the background <ref> [17] </ref> (or have characteristic heat signatures w.r.t. the background), and can therefore fail when the target heat, relative to the background, varies unpredictably. <p> The IR-based ATR system used at the DARPA UGV Demo-C is based on double-window detection <ref> [11, 17] </ref>. Using this method on 25 randomly chosen images from the Ft. Carson IR data set, only 22 out of 50 targets were detected, with 5 false alarms; in addition, the only civilian vehicle in the image set was mistaken for a target. <p> Carson IR data set, only 22 out of 50 targets were detected, with 5 false alarms; in addition, the only civilian vehicle in the image set was mistaken for a target. A representative result is shown in figure 1. While other IR-based techniques have been proposed <ref> [17] </ref>, there is no strong evidence to indicate that these techniques can overcome the problems inherent to IR data. 3 Color imagery for ATR This paper advocates using color to enhance ATR systems.
Reference: [18] <author> S.A. Shafer, </author> <title> "Using Color to Separate Reflection Components", </title> <booktitle> Color Research Application, </booktitle> <pages> 10 pp 210-218, </pages> <year> 1985. </year>
Reference-contexts: The color of daylight changes significantly due to the sun-angle and weather conditions, and the position and orientation of the target are also expected to vary. Consequently, the apparent color of a target varies under realistic conditions. Previous methods in computational color recognition, such as color constancy algorithms <ref> [18, 7, 6] </ref>, have dealt with varying color in highly constrained environments, and are generally not applicable to outdoor imagery. It will be shown that as imaging conditions vary, the apparent color of objects forms characteristic types of clusters in color RGB space, depending on the surface properties. <p> Finlayson [6] assumes that surfaces with the same reflectance have been identified in two spatially distinct parts of the image, and that the unknown illumination falls within the gamut of known artificial illuminants; Ohta [16] assumes artificial illumination constrained by the CIE model to reduce performance errors; Novak and Shafer <ref> [15, 18] </ref> assume a point light source and pure specular reflection; Buchsbaum [3] assumes that the surface reflectance averaged over the entire image is grey; Maloney's work [12] is a refinement of Buchsbaum's but has been applied only under the constraints of an indoor world with Munsell color chips. <p> Theoretical parametric models exist for the various phases of the image formation process <ref> [9, 10, 13, 18] </ref>, although these models appear too restrictive to be used in unconstrained imagery; still, they provide an approximate qualitative description of the variation of apparent color. <p> In RGB space, the parabola stretches out into a thin curved surface [4]. The effect of illumination geometry and viewing geometry depend on the reflectance of the surface. Most realistic surfaces have reflectances that have a mixture lambertian and specular components. Existing reflectance models of mixed reflection surfaces <ref> [9, 13, 18] </ref> are yet be applied to unconstrained imagery in the context of color-based recognition.
Reference: [19] <author> T. Yachik, </author> <title> "Status of Evaluation, </title> <booktitle> RSTA Workshop", DARPA Image Understanding Workshop, </booktitle> <year> 1995. </year>
Reference-contexts: The RGB representation of color makes it possible to use a lookup table for real-time classification on standard hardware. This method has been implemented in a system for ATR of camouflaged military vehicles in real-time, and has been tested in a DARPA-sanctioned study <ref> [19] </ref> on the Ft. Carson data set [1] and at the DARPA UGV Demo-C [11]. In each test, over 90% of the targets were detected (compared to a 45% detection rate by the IR-based system).
References-found: 19

