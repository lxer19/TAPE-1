URL: http://www.eecs.umich.edu/techreports/cse/1997/CSE-TR-335-97.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse97.html
Root-URL: http://www.eecs.umich.edu
Title: Critical Issues Regarding the Trace Cache Fetch Mechanism  
Author: Sanjay Jeram Patel, Daniel Holmes Friendly, and Yale N. Patt 
Keyword: high bandwidth fetch mechanisms, instruction cache, wide issue machines, speculative execution  
Address: Ann Arbor, MI 48109-2122  
Affiliation: Advanced Computer Architecture Laboratory Department of Electrical Engineering and Computer Science The University of Michigan  
Pubnum: Technical Report  
Email: fsanjayp, ites, pattg@eecs.umich.edu  
Phone: Tel: (313) 936-0404  
Abstract: In order to meet the demands of wider issue processors, fetch mechanisms will need to fetch multiple basic blocks per cycle. The trace cache supplies several basic blocks each cycle by storing logically contiguous instructions in physically contiguous storage. When a particular basic block is requested, the trace cache can potentially respond with the requested block along with several blocks that followed it when the block was last encountered. In this technical report, we examine some critical features of a trace cache mechanism designed for a 16-wide issue processor and evaluate their effects on performance. We examine features such as cache associativity, storage partitioning, branch predictor design, instruction cache design, and fill unit design. We compare the performance of our trace cache mechanism with that of the design presented by Rotenberg et al [19] and show a 23% improvement in performance. In our final analysis, we compare our trace cache mechanism with an aggressive single basic block fetch mechanism and show that the trace cache mechanism attains a 24% improvement in performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.-Y. Chang, M. Evers, and Y. N. Patt, </author> <title> "Improving branch prediction accuracy by reducing pattern history table interference," </title> <booktitle> in Proceedings of the 1996 ACM/IEEE Conference on Parallel Architectures and Compilation Techniques, </booktitle> <year> 1996. </year>
Reference-contexts: The technology for predicting a single branch in one cycle is more mature than the technology for predicting multiple branches in one cycle. As the techniques and lessons learned with single branch predictors, techniques such as reducing negative interference <ref> [1] </ref> and combining branch predictors [13], are integrated into multiple branch predictors, the performance of the trace cache mechanism will continue to increase.
Reference: [2] <author> P.-Y. Chang, E. Hao, T.-Y. Yeh, and Y. N. Patt, </author> <title> "Branch classification: A new mechanism for improving branch predictor performance," </title> <booktitle> in Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 22-31, </pages> <year> 1994. </year> <month> 31 </month>
Reference-contexts: The selection between the components is done by a 15-bit gshare-style selector. Combining a per-address predictor with a gshare predictor was first proposed by McFarling [13]. Using a two-level mechanism to select between the two was proposed by Chang et al <ref> [2] </ref>. A similar version of this predictor is implemented in the DECChip 21264 [8]. While the trace cache predictor is roughly twice the size (64KB) of the single branch predictor (32KB), the access times are roughly equivalent. Both predictors have 32K entries in their pattern history tables.
Reference: [3] <author> R. P. Colwell, R. P. Nix, J. J. O'Donnell, D. B. Papworth, and P. K. Rodman, </author> <title> "A VLIW architecture for a trace scheduling compiler," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 37, no. 8, </volume> <pages> pp. 967-979, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Similar fetch rate increasing techniques are employed for statically scheduled machines through compiler techniques such as superblock scheduling [10] and trace scheduling <ref> [6, 3] </ref>. A precursor to the trace cache was first introduced by Melvin, Shebanow and Patt [16]. They proposed the fill unit to compact a basic block's worth of instructions into an entry in a decoded instruction cache.
Reference: [4] <author> T. M. Conte, K. N. Menezes, P. M. Mills, and B. A. Patel, </author> <title> "Optimization of instruction fetch mechanisms for high issue rates," </title> <booktitle> in Proceedings of the 22st Annual International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: As the focus of our current research in uniprocessor design has concentrated on wide issue machines (eg. 16 wide), the design of high-bandwidth fetch engines is extremely important. Cache organizations for simultaneously fetching multiple basic blocks have been studied by Yeh et al [27], Conte et al <ref> [4] </ref> and Seznec et al [20]. By multiporting the instruction cache and/or the branch target buffer (BTB) and generating multiple fetch addresses and branch predictions per cycle, these schemes are theoretically able to overcome the single basic block fetch bottleneck.
Reference: [5] <author> S. Dutta and M. Franklin, </author> <title> "Control flow prediction with tree-like subgraphs for superscalar processors," </title> <booktitle> in Proceedings of the 28th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 258-263, </pages> <year> 1995. </year>
Reference-contexts: In addition to work on instruction caching techniques, there has also been serious investigation into predicting multiple branches per cycle. Fetching multiple blocks and predicting multiple branches go hand-in-hand. Dutta and Franklin <ref> [5] </ref> proposed subgraph oriented branch prediction mechanisms which use local subgraph history (akin to per-address history) to form a prediction. In addition to proposing a caching structure, Seznec et al [20] also presented a multiple branch predictor capable of predicting two branches per cycle.
Reference: [6] <author> J. A. Fisher, </author> <title> "Trace scheduling: A technique for global microcode compaction," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-30, no. 7, </volume> <pages> pp. 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: Similar fetch rate increasing techniques are employed for statically scheduled machines through compiler techniques such as superblock scheduling [10] and trace scheduling <ref> [6, 3] </ref>. A precursor to the trace cache was first introduced by Melvin, Shebanow and Patt [16]. They proposed the fill unit to compact a basic block's worth of instructions into an entry in a decoded instruction cache.
Reference: [7] <author> M. Franklin and M. Smotherman, </author> <title> "A fill-unit approach to multiple instruction issue," </title> <booktitle> in Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 162-171, </pages> <year> 1994. </year>
Reference-contexts: In [15] the performance implications of the fill unit are discussed and the idea of dynamically combining basic blocks into larger "execution atomic units" (EAUs) to further increase the fetch bandwidth is first proposed. Two other extensions of the original schemes were presented by Smotherman and Franklin. In <ref> [7] </ref>, they applied the original fill unit ideas to dynamically create VLIW instructions out of RISC-type operations. They reworked the fill unit finalization strategy by restricting the type of instruction dependencies allowed in a fill unit line and by filling both paths beyond a conditional branch.
Reference: [8] <author> L. Gwennap, </author> <title> "Digital 21264 sets new standard," Microprocessor Report, </title> <journal> pp. </journal> <volume> 11 - 16, </volume> <month> October </month> <year> 1996. </year>
Reference-contexts: Since branch targets tend to exhibit spatial locality [24], direct-mapped trace caches suffer a significant penalty due to conflict misses. Implementing associativity will increase the cache access latency. However, several recent techniques such as set prediction <ref> [29, 8] </ref> and remap caches [18] allow caches to exhibit both lower conflict misses and lower access times. <p> Combining a per-address predictor with a gshare predictor was first proposed by McFarling [13]. Using a two-level mechanism to select between the two was proposed by Chang et al [2]. A similar version of this predictor is implemented in the DECChip 21264 <ref> [8] </ref>. While the trace cache predictor is roughly twice the size (64KB) of the single branch predictor (32KB), the access times are roughly equivalent. Both predictors have 32K entries in their pattern history tables.
Reference: [9] <author> E. Hao, P.-Y. Chang, M. Evers, and Y. N. Patt, </author> <title> "Increasing the instruction fetch rate via block-structured instruction set architectures," </title> <booktitle> in Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1996. </year>
Reference-contexts: By multiporting the instruction cache and/or the branch target buffer (BTB) and generating multiple fetch addresses and branch predictions per cycle, these schemes are theoretically able to overcome the single basic block fetch bottleneck. A compile-time approach to solving the fetch bottleneck problem is the Block-Structured ISA <ref> [14, 9] </ref>. The static form of the program is organized into enlarged atomic units composed of multiple basic blocks by the compiler, which, in that case, plays a central role in attaining higher fetch bandwidth.
Reference: [10] <author> W. W. Hwu, S. A. Mahlke, W. Y. Chen, P. P. Chang, N. J. Warter, R. A. Bringmann, R. G. Ouellette, R. E. Hank, T. Kiyohara, G. E. Haab, J. G. Holm, and D. M. Lavery, </author> <title> "The superblock: An effective technique for VLIW and superscalar compilation," </title> <journal> Journal of Supercomputing, </journal> <volume> vol. 7, no. </volume> <pages> 9-50, </pages> , <year> 1993. </year>
Reference-contexts: Similar fetch rate increasing techniques are employed for statically scheduled machines through compiler techniques such as superblock scheduling <ref> [10] </ref> and trace scheduling [6, 3]. A precursor to the trace cache was first introduced by Melvin, Shebanow and Patt [16]. They proposed the fill unit to compact a basic block's worth of instructions into an entry in a decoded instruction cache.
Reference: [11] <author> Pentium Processor User's Manual Volume 1: </author> <title> Pentium Processor Data Book, </title> <publisher> Intel Corporation, </publisher> <year> 1993. </year>
Reference-contexts: While troublesome, this problem is overcome with straightforward techniques such as fetching two adjacent cache lines and realigning the instructions <ref> [11] </ref>. For processors capable of executing six or more instructions per cycle, the need to fetch beyond control instructions arises. As the focus of our current research in uniprocessor design has concentrated on wide issue machines (eg. 16 wide), the design of high-bandwidth fetch engines is extremely important.
Reference: [12] <author> D. Kaeli and P. Emma, </author> <title> "Branch history table predictions of moving target branches due to subroutine returns," </title> <booktitle> in Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <year> 1991. </year>
Reference-contexts: This field encodes the number and directions of branches in the segment and includes bits to identify whether a segment ends in a branch and whether that branch is a return from subroutine instruction. In the case of a return instruction, the return address stack <ref> [12] </ref> provides the next fetch address. For reasons mentioned below, this information is stored in the tag store. The total size of a line is around 97 bytes for a typical architecture: 5x16 bytes of instructions, 4x4 bytes of target addresses, and 1 byte of path information.
Reference: [13] <author> S. McFarling, </author> <title> "Combining branch predictors," </title> <type> Technical Report TN-36, </type> <institution> Digital Western Research Laboratory, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Although other methods for indexing the PHT are examined in section 5.8, the baseline configuration for the trace cache predictor uses the gshare scheme outlined by McFarling <ref> [13] </ref>. The global branch history is XORed with the current fetch address, forming an index into the PHT. Gshare has been shown to be effective in reducing negative interference in commonly accessed PHT entries, such as entries accessed by strongly biased branches. <p> The single branch predictor is a hybrid predictor, consisting of two components: a 15-bit PAs predictor and a 15-bit gshare predictor. The selection between the components is done by a 15-bit gshare-style selector. Combining a per-address predictor with a gshare predictor was first proposed by McFarling <ref> [13] </ref>. Using a two-level mechanism to select between the two was proposed by Chang et al [2]. A similar version of this predictor is implemented in the DECChip 21264 [8]. <p> The technology for predicting a single branch in one cycle is more mature than the technology for predicting multiple branches in one cycle. As the techniques and lessons learned with single branch predictors, techniques such as reducing negative interference [1] and combining branch predictors <ref> [13] </ref>, are integrated into multiple branch predictors, the performance of the trace cache mechanism will continue to increase.
Reference: [14] <author> S. Melvin and Y. Patt, </author> <title> "Enhancing instruction scheduling with a block-structured ISA," </title> <booktitle> International Journal on Parallel Processing, </booktitle> <year> 1994. </year>
Reference-contexts: By multiporting the instruction cache and/or the branch target buffer (BTB) and generating multiple fetch addresses and branch predictions per cycle, these schemes are theoretically able to overcome the single basic block fetch bottleneck. A compile-time approach to solving the fetch bottleneck problem is the Block-Structured ISA <ref> [14, 9] </ref>. The static form of the program is organized into enlarged atomic units composed of multiple basic blocks by the compiler, which, in that case, plays a central role in attaining higher fetch bandwidth.
Reference: [15] <author> S. W. Melvin and Y. N. Patt, </author> <title> "Performance benefits of large execution atomic units in dynamically scheduled machines," </title> <booktitle> in Proceedings of Supercomputing '89, </booktitle> <pages> pp. 427-432, </pages> <year> 1989. </year>
Reference-contexts: The trace cache is a new paradigm for caching instructions and directly deals with lost bandwidth due to partial fetches. The trace cache stores logically contiguous instruction sequences in physically contiguous storage, a concept first proposed by Melvin <ref> [15] </ref>. <p> A hit in the decoded instruction cache results in a larger atomic unit of work than would be possible if the individual instructions were fetched and decoded one per cycle. In <ref> [15] </ref> the performance implications of the fill unit are discussed and the idea of dynamically combining basic blocks into larger "execution atomic units" (EAUs) to further increase the fetch bandwidth is first proposed. Two other extensions of the original schemes were presented by Smotherman and Franklin. <p> The trace cache supplies multiple basic blocks of instructions each cycle by storing logically contiguous instruction sequences in physically contiguous storage | a concept first proposed by Melvin et al <ref> [15] </ref>. We have shown that a large trace cache assisted by a small instruction cache outperforms a small trace cache acting as an assist to a large instruction cache.
Reference: [16] <author> S. W. Melvin, M. C. Shebanow, and Y. N. Patt, </author> <title> "Hardware support for large atomic units in dynamically scheduled machines," </title> <booktitle> in Proceedings of the 21st Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 60-63, </pages> <year> 1988. </year>
Reference-contexts: Similar fetch rate increasing techniques are employed for statically scheduled machines through compiler techniques such as superblock scheduling [10] and trace scheduling [6, 3]. A precursor to the trace cache was first introduced by Melvin, Shebanow and Patt <ref> [16] </ref>. They proposed the fill unit to compact a basic block's worth of instructions into an entry in a decoded instruction cache.
Reference: [17] <author> Y. Patt, W. Hwu, and M. Shebanow, "HPS, </author> <title> a new microarchitecture: Rationale and introduction," </title> <booktitle> in Proceedings of the 18th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 103-107, </pages> <year> 1985. </year>
Reference-contexts: The fetch mechanism stalls until the missing line arrives. 10 4 Experimental Setup The trace cache was added as the fetch mechanism of a 16-wide issue processor implementing the HPS model of execution <ref> [17] </ref>. To isolate the effects of the fetch engine, many of the bottlenecks in the execution core have been removed.
Reference: [18] <author> P. B. Racunas and Y. N. Patt, </author> <title> "Achieving full associativity with direct-mapped access times using a remap cache," </title> <type> Unpublised manuscript, </type> <year> 1997. </year>
Reference-contexts: Since branch targets tend to exhibit spatial locality [24], direct-mapped trace caches suffer a significant penalty due to conflict misses. Implementing associativity will increase the cache access latency. However, several recent techniques such as set prediction [29, 8] and remap caches <ref> [18] </ref> allow caches to exhibit both lower conflict misses and lower access times.
Reference: [19] <author> E. Rotenberg, S. Bennett, and J. E. Smith, </author> <title> "Trace cache: a low latency approach to high bandwidth instruction fetching," </title> <booktitle> in Proceedings of the 29th Annual ACM/IEEE International Symposium on Mi-croarchitecture, </booktitle> <year> 1996. </year> <month> 32 </month>
Reference-contexts: Fetching beyond conditional branches that are not taken provides a boost of approximately two instruction per cycle <ref> [19] </ref>. While branch mispredictions and cache misses result in many cycles during which no useful instructions are fetched, they occur relatively infrequently. The loss due to partial fetches will be incurred almost every cycle instructions are fetched from the cache. <p> We present performance data on a wide set of fetch mechanism organizations and attempt to isolate those parameters 2 that strongly affect performance. We also extend and test some of the ideas mentioned by Rotenberg et al <ref> [19] </ref> in their study of trace caches. 2 Related Work The loss in fetch bandwidth due to partial fetches affects all superscalar processors. <p> In addition to proposing a caching structure, Seznec et al [20] also presented a multiple branch predictor capable of predicting two branches per cycle. Recently, Wallace and Bagherzadeh extended Seznec's scheme [25]. The trace cache concept was investigated by Rotenberg et al <ref> [19] </ref>. They presented a thorough comparison between the trace cache scheme to the current hardware-based high-bandwidth fetch schemes, clearly showing the advantage of using a trace cache, both in performance and latency. They also present a multiple branch predictor capable of making three predictions per cycle. <p> At the end of the cycle, the segment is matched with the prediction. Since the predictor selects B to follow A but D to follow B, only A and B are supplied for execution. This is called partial matching and was briefly discussed by Rotenberg <ref> [19] </ref>. Its performance implications will be examined in section 5.4. A T NT D C Trace Cache Multiple Branch Predictor address of A 3 T/NT/T selection logic A B C Predictions shows the control flow from block A. The predictor selects path ABD. The trace cache only contains ABC. <p> 2.0 3.0 4.0 5.0 6.0 Instructions per Cycle 64KB TCache/4KB ICache 32KB TCache/32KB ICache 4KB TCache/64KB ICache com gcc go ijpeg li m88ksim perl vortex 0.0 1.0 2.0 3.0 4.0 5.0 6.0 Instructions per Cycle 256KB TCache/4KB ICache 128KB TCache/128KB ICache 4KB TCache/256KB ICache 16 5.3 Path Associativity Path associativity <ref> [19] </ref> relaxes the constraint that different segments starting from the same basic block cannot be stored in the trace cache at the same time. <p> This process is referred to as partial matching <ref> [19] </ref>. Alternatively, the trace cache can be designed to signal a hit only if all the blocks within the selected segment match. that does not. <p> An assessment of the mechanism would not be complete without a comparison to the current dominant techniques for fetch engine design. Rotenberg et al. <ref> [19] </ref> presented a thorough comparison of the trace cache's performance on the SPECint92 and IBS benchmarks to a few of the hardware-based multiple block fetch techniques mentioned in section 2. Here we present a comparison of our baseline configuration with an aggressive single block fetch mechanism.
Reference: [20] <author> A. Seznec, S. Jourdan, P. Sainrat, and P. Michaud, </author> <title> "Multiple-block ahead branch predictors," </title> <booktitle> in Pro--ceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1996. </year>
Reference-contexts: Cache organizations for simultaneously fetching multiple basic blocks have been studied by Yeh et al [27], Conte et al [4] and Seznec et al <ref> [20] </ref>. By multiporting the instruction cache and/or the branch target buffer (BTB) and generating multiple fetch addresses and branch predictions per cycle, these schemes are theoretically able to overcome the single basic block fetch bottleneck. A compile-time approach to solving the fetch bottleneck problem is the Block-Structured ISA [14, 9]. <p> Fetching multiple blocks and predicting multiple branches go hand-in-hand. Dutta and Franklin [5] proposed subgraph oriented branch prediction mechanisms which use local subgraph history (akin to per-address history) to form a prediction. In addition to proposing a caching structure, Seznec et al <ref> [20] </ref> also presented a multiple branch predictor capable of predicting two branches per cycle. Recently, Wallace and Bagherzadeh extended Seznec's scheme [25]. The trace cache concept was investigated by Rotenberg et al [19].
Reference: [21] <author> J. E. Smith, </author> <title> "A study of branch prediction strategies," </title> <booktitle> in Proceedings of the 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 135-148, </pages> <year> 1981. </year>
Reference-contexts: Here, we explore other indexing options. In figure 16, the conditional branch misprediction rates for the go benchmark are shown for four different PHT index methods. The first method is to use only bits of the next fetch address. This technique is similar to the 2-bit counter technique <ref> [21] </ref> used for single-level branch predictors. The second method is to use only bits of the global history, or GAg. The third method is to use a combination of the next fetch address and the global history, or GAs 2 .
Reference: [22] <author> M. Smotherman and M. Franklin, </author> <title> "Improving cisc instruction decoding performance using a fill unit," </title> <booktitle> in Proceedings of the 28th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 219-229, </pages> <year> 1995. </year>
Reference-contexts: In [7], they applied the original fill unit ideas to dynamically create VLIW instructions out of RISC-type operations. They reworked the fill unit finalization strategy by restricting the type of instruction dependencies allowed in a fill unit line and by filling both paths beyond a conditional branch. In <ref> [22] </ref>, they demonstrated how a fill unit could help overcome the decoder bottleneck of a Pentium Pro type processor. In addition to work on instruction caching techniques, there has also been serious investigation into predicting multiple branches per cycle. Fetching multiple blocks and predicting multiple branches go hand-in-hand.
Reference: [23] <author> E. Sprangle and Y. Patt, </author> <title> "Facilitating superscalar processing via a combined static/dynamic register renaming scheme," </title> <booktitle> in Proceedings of the 27th Annual ACM/IEEE International Symposium on Mi-croarchitecture, </booktitle> <pages> pp. 143-147, </pages> <year> 1994. </year>
Reference-contexts: A complex dependency analysis of 16 instructions does not need to be performed on the fetched segment. Sprangle and Patt <ref> [23] </ref> demonstrated that pre-analyzed fetch packets require fewer read and write ports to the register renaming structures and the register file. Finally, instructions can be stored in an order that permits quick issue.
Reference: [24] <author> G. S. Tyson, </author> <title> "The effects of predication on branch prediction," </title> <booktitle> in Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 196-206, </pages> <year> 1994. </year>
Reference-contexts: With a direct-mapped 128KB trace cache, blocks which are 2048 instructions apart will map to the same line. Since branch targets tend to exhibit spatial locality <ref> [24] </ref>, direct-mapped trace caches suffer a significant penalty due to conflict misses. Implementing associativity will increase the cache access latency. However, several recent techniques such as set prediction [29, 8] and remap caches [18] allow caches to exhibit both lower conflict misses and lower access times.
Reference: [25] <author> S. Wallace and N. Bagherzadeh, </author> <title> "Multiple branch and block prediction," </title> <booktitle> in Proceedings of the 1997 ACM/IEEE Conference on High Performance Computer Architecture, </booktitle> <year> 1997. </year>
Reference-contexts: In addition to proposing a caching structure, Seznec et al [20] also presented a multiple branch predictor capable of predicting two branches per cycle. Recently, Wallace and Bagherzadeh extended Seznec's scheme <ref> [25] </ref>. The trace cache concept was investigated by Rotenberg et al [19]. They presented a thorough comparison between the trace cache scheme to the current hardware-based high-bandwidth fetch schemes, clearly showing the advantage of using a trace cache, both in performance and latency.
Reference: [26] <author> S. Wilton and N. Jouppi, </author> <title> "An enhanced access and cycle time model for on-chip caches," </title> <institution> in DEC Western Research Lab. </institution> <type> Technical Report 93/5, </type> <year> 1994. </year>
Reference-contexts: Both predictors have 32K entries in their pattern history tables. The trace cache predictor has 16 bits in each entry, whereas the single branch predictor has three tables, each of two bits. The magnitudes of and differences in the widths of these entries make the access times very similar <ref> [26] </ref>. 28 cache mechanism outperforms the single block scheme across all but one of the benchmarks simulated. On the benchmark compress, the difference is 22%, on gcc 5%, on ijpeg 20%, on li 39%, on m88ksim 51%, on perl 32%, and on vortex 30%.
Reference: [27] <author> T.-Y. Yeh, D. Marr, and Y. N. Patt, </author> <title> "Increasing the instruction fetch rate via multiple branch prediction and branch address cache," </title> <booktitle> in Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pp. 67-76, </pages> <year> 1993. </year>
Reference-contexts: As the focus of our current research in uniprocessor design has concentrated on wide issue machines (eg. 16 wide), the design of high-bandwidth fetch engines is extremely important. Cache organizations for simultaneously fetching multiple basic blocks have been studied by Yeh et al <ref> [27] </ref>, Conte et al [4] and Seznec et al [20]. By multiporting the instruction cache and/or the branch target buffer (BTB) and generating multiple fetch addresses and branch predictions per cycle, these schemes are theoretically able to overcome the single basic block fetch bottleneck.
Reference: [28] <author> T.-Y. Yeh and Y. N. Patt, </author> <title> "Two-level adaptive branch prediction," </title> <booktitle> in Proceedings of the 24th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 51-61, </pages> <year> 1991. </year>
Reference-contexts: In the case of our trace cache mechanism, three predictions per cycle are required. Two level adaptive branch prediction has been demonstrated to achieve high prediction accuracy over a wide set of applications <ref> [28] </ref>. In a two level scheme, the first level of history records the outcomes of the most recently executed branches. The second level of history records the most likely outcome when a particular pattern in the first level history is encountered.
Reference: [29] <author> R. Yung, </author> <title> "Design decisions influencing the ultrasparc's instruction fetch architecture," </title> <booktitle> in Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1996. </year> <month> 33 </month>
Reference-contexts: Since branch targets tend to exhibit spatial locality [24], direct-mapped trace caches suffer a significant penalty due to conflict misses. Implementing associativity will increase the cache access latency. However, several recent techniques such as set prediction <ref> [29, 8] </ref> and remap caches [18] allow caches to exhibit both lower conflict misses and lower access times.
References-found: 29

