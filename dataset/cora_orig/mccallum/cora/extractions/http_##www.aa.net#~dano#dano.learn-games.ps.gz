URL: http://www.aa.net/~dano/dano.learn-games.ps.gz
Refering-URL: http://www.aa.net/~dano/
Root-URL: 
Title: Learning to Play Games From Experience: An Application of Artificial Neural Networks and Temporal Difference Learning  
Author: Daniel Kenneth Olson 
Date: December, 1993  
Abstract-found: 0
Intro-found: 1
Reference: <author> Barto, A. G., Sutton, R. S., and Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning and sequential decision making. </title> <type> Technical Report COINS 89-95, </type> <institution> Dept. of Computer and Information Science, University of Mas-sachusetts, </institution> <address> Amherst MA 01003. </address> <note> Available via FTP from archive.ohio-state.edu:/pub/neuroprose. </note>
Reference-contexts: These kinds of processes can be referred to collectively as sequential decision tasks <ref> (Barto et al., 1989) </ref>. Whether driving a car to work, or buttering a piece of toast, there is a goal in mind and a sequence of actions for attempting to reach it.
Reference: <author> Boyan, J. A. </author> <year> (1992). </year> <title> Modular neural networks for learning. </title> <type> Master's thesis, </type> <institution> University of Cambridge. </institution> <note> Available via FTP from archive.ohio-state.edu:/pub/neuroprose. </note>
Reference-contexts: Justin Boyan <ref> (Boyan, 1992) </ref> extended Tesauro's ideas for his Master's thesis by creating a modular TD network which also played backgammon. His resulting program was entered into the 1992 Computer Games Olympiad under the name MAESTRO 1.0 and proceeded to take second place. <p> Due to its lack of speed, the ability to try alternative network topologies and parameter settings was very limited. In fact, a learning trial of 13,600 games of Tic-Tac-Toe with raw state inputs (see Section 6.8) took about 5 days to complete. In contrast, Boyan <ref> (Boyan, 1992) </ref> reports training times of about 2 hours for a comparable learning trial on DEC 3100 workstation. 41 Chapter 4 Maze Mapping In this example, capabilities of the TD learning procedure are tested by using a fairly simple sequential decision problem. <p> A remedy which worked quite well was to force the neural player to repeat the same bad game until it was "fixed". After 10 games, the problem was solved. Final testing over 1000 games resulted in just 1 loss and a score of -0.001. Although Boyan's setup for Tic-Tac-Toe <ref> (Boyan, 1992) </ref> was slightly different, the number of training games to reach the best play took about the same time. In contrast, Kolen and Goel's (Kolen and Goel, 1991) Tic-Tac-Toe player learned to draw its opponent much more rapidly, usually by the 100th game. <p> Modularity Another methods for improving playing capability is modularity. Instead of trying to pack all the the knowledge for a static evaluator into one monolithic network perhaps several neural player's policies could be combined in a voting system (which could be another neural network). Boyan <ref> (Boyan, 1992) </ref> also demonstrated that using separate neural networks for different phases of the game can be a tremendous help. Even without augmenting the basic approach, it does appear that more complicated problems might be solved; Tesauro's success at Backgammon seems to indicate this.
Reference: <author> Bramer, M. A., </author> <title> editor (1983). Computer game-playing: Theory and practice. </title> <publisher> Ellis Horwood Limited, Market Cross House, </publisher> <address> Cooper Street, Chichester, West Sussex, PO19 1EB, England. </address>
Reference-contexts: Looking back, evidence of this fascination can be traced to the 1890's. It was during this period that an electro-mechanical device created by the inventor Torres y Quevedo entertained World Fair goers by playing the chess endgame King and Rook against King <ref> (Bramer, 1983) </ref>. From artificial intelligence's (AI) beginnings, researchers have been using games as a testbed for search and planning algorithms, and continue to do so. In many cases, programs based upon these algorithms compete at and beyond human playing abilities.
Reference: <author> Carpenter, G. and Grossberg, S. </author> <year> (1987). </year> <title> Invariant pattern recognition and recall by an attentive self-organizing ART architecture in a nonstationary world. </title> <booktitle> In Proceedings of the IEEE First International Conference on Neural Networks, volume II, p 737. </booktitle>
Reference-contexts: Today, there are many different architectures to chose from, an artifact of the attitude in late 80's in which neural network research was equated with inventing new paradigms. Examples of of the most common paradigms 18 are Adaptive Resonance Theory (ART) <ref> (Carpenter and Grossberg, 1987) </ref>, Hopfield Net (Hopfield, 1982), and the Adaptive Vector Quantizer (Kohonen, 1981). Another popular one, known as Backpropagation, is used for most of the work in this thesis. The next section provides an overview for this popular multi-layer feedforward network 3 and its accompanying training procedure.
Reference: <author> Fahlman, S. E. </author> <year> (1988). </year> <title> An empirical study of learning speed in backpropagation networks. </title> <type> Technical Report CMU-CS-88-162, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213. </address>
Reference-contexts: In response, many enhancements have been suggested that help speed up learning and are more likely to converge to good solutions. Some examples are: * Momentum which attempts to dampen radical weight changes. * QuickProp <ref> (Fahlman, 1988) </ref> which uses second-order derivatives to greatly decrease training time. * and many others : : : Since there are so many variations, confusion can arise when someone reports "I did such and such with backprop".
Reference: <author> Fahlman, S. E. and Lebiere, C. </author> <year> (1990). </year> <title> The Cascade-Correlation learning architecture. </title> <type> Technical Report CMU-CS-90-100, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213. </address>
Reference-contexts: The neural networks can be configured with optional features such as momentum, skip-layer connections, and variable rates for each unit. In addition, the Cascade Correlation Architecture <ref> (Fahlman and Lebiere, 1990) </ref> which "grows" a network topology is being modified to work with the TD method, but at the time of this writing is not yet ready. It would be interesting to compare different options but this is currently beyond the scope of this thesis. <p> With small problems, trial and error can suffice; with complex mappings (larger state space, bizarre nonlinearities), trial and error leads to despair. Larger state spaces typically require more units which tends to paralyze the vanilla backpropagation learning rule. The Cascade Correlation Architecture (CCA) <ref> (Fahlman and Lebiere, 1990) </ref> automatically finds "good" topologies by incrementally adding hidden units. In addition, since CCA adds one unit at a time and freezes its weights, there are less units adapting which greatly increases the ability to learning difficult mappings.
Reference: <author> Feldman, J. and Ballard, D. </author> <year> (1982). </year> <title> Connectionist models and their properties. </title> <booktitle> Cognitive Science, 6,205. 79 Hebb, </booktitle> <address> D. </address> <year> (1949). </year> <title> Organization of behavior. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: The field is rich with interesting neural architectures and training procedures, each designed to tackle specific kinds of problems. Unfortunately, it is also rich in vocabulary that refers to similar concepts. For example, the terms connectionist networks <ref> (Feldman and Ballard, 1982) </ref>, parallel distributed processors (Rumelhart and McClelland, 1986), and neurocomputing (Hecht-Nielsen, 1989) all refer to artificial neural networks. A very good summary can be found in (Simpson, 1988).
Reference: <author> Hecht-Nielsen, R. </author> <year> (1989). </year> <title> Neurocomputing. </title> <publisher> Addison-Wesley Publishing Company, Inc. </publisher>
Reference: <author> Holland, J. H. </author> <year> (1983). </year> <title> Escaping brittleness. </title> <booktitle> In Proceedings of the International Machine Learning Workshop, </booktitle> <pages> pp 92-95. </pages>
Reference-contexts: The ideas that Sutton describes are not entirely new, but mostly a refinement and formalization of temporal difference methods used by previous learning systems. Examples are Samuel's checker player (Samuel, 1959) (yes, he had it from the beginning), Holland's Bucket Brigade <ref> (Holland, 1983) </ref> for Genetic Algorithms, and Sutton's earlier work with the Adaptive Heuristic Critic network (Sutton, 1984). Incremental TD () Sutton developed an incremental real-time learning procedure for neural networks known as TD ().
Reference: <author> Hopfield, J. </author> <year> (1982). </year> <title> Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> In Proceedings of the National Academy of Sciences USA, </booktitle> <volume> volume 79, </volume> <pages> pp 2554-2558. </pages>
Reference-contexts: Today, there are many different architectures to chose from, an artifact of the attitude in late 80's in which neural network research was equated with inventing new paradigms. Examples of of the most common paradigms 18 are Adaptive Resonance Theory (ART) (Carpenter and Grossberg, 1987), Hopfield Net <ref> (Hopfield, 1982) </ref>, and the Adaptive Vector Quantizer (Kohonen, 1981). Another popular one, known as Backpropagation, is used for most of the work in this thesis. The next section provides an overview for this popular multi-layer feedforward network 3 and its accompanying training procedure.
Reference: <author> Kohonen, T. </author> <year> (1981). </year> <title> Automatic formation of topological maps in a self-organizing system. </title> <booktitle> In Proceedings of the 2nd Scandinavian Conference on Image Analysis, </booktitle> <pages> pp 214-220. </pages>
Reference-contexts: Examples of of the most common paradigms 18 are Adaptive Resonance Theory (ART) (Carpenter and Grossberg, 1987), Hopfield Net (Hopfield, 1982), and the Adaptive Vector Quantizer <ref> (Kohonen, 1981) </ref>. Another popular one, known as Backpropagation, is used for most of the work in this thesis. The next section provides an overview for this popular multi-layer feedforward network 3 and its accompanying training procedure.
Reference: <author> Kolen, J. F. and Goel, A. K. </author> <year> (1991). </year> <title> Learning in parallel distributed processing networks: Computational complexity and information content. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, 21(2),359-367. </journal>
Reference-contexts: To prevent such functions, Samuel added the piece advantage to the static evaluator's output. Kolen and Goel's Neural Tic-Tac-Toe In a study by John F. Kolen and Ashok K. Goel <ref> (Kolen and Goel, 1991) </ref> into the limitations of backpropagation learning, they used Tic-Tac-Toe as an example problem. <p> Final testing over 1000 games resulted in just 1 loss and a score of -0.001. Although Boyan's setup for Tic-Tac-Toe (Boyan, 1992) was slightly different, the number of training games to reach the best play took about the same time. In contrast, Kolen and Goel's <ref> (Kolen and Goel, 1991) </ref> Tic-Tac-Toe player learned to draw its opponent much more rapidly, usually by the 100th game. Apparently their heuristic player was deterministic, meaning, faced with the same board position, it selected the same move.
Reference: <author> Lee, K.-F. and Mahajan, S. </author> <year> (1988). </year> <title> A pattern classification approach to evaluation function learning. </title> <journal> Artificial Intelligence, 36,1-25. </journal>
Reference-contexts: A Bayesian Othello Player (alternative to ANN) Researchers Kai-Fu Lee and Sanjoy Mahajan have been evolving a series of highly successful Othello playing programs known as BILL. The latest and greatest version, BILL 3.0 <ref> (Lee and Mahajan, 1988) </ref>, was taught using super vised learning much like Tesauro's NeuroGammon. Instead of using a neural network, though, they relied on a classic pattern recognition technique, a Bayesian classifier.
Reference: <author> McCulloch, W. and Pitts, W. </author> <year> (1943). </year> <title> A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics, 7,115-133. </journal>
Reference-contexts: Brief History Although many believe artificial neural networks to be a recent development, their roots extend at least 50 years back. The earliest work is generally considered to have occurred in the 1940's with McCulloch and Pitts' <ref> (McCulloch and Pitts, 1943) </ref> model for neurons. Their work introduced the concept of the binary threshold activation unit and showed that any logic function could be configured with networks of such processors.
Reference: <author> McKinsey, J. C. </author> <year> (1952). </year> <title> Introduction to the theory of games. The RAND Series. </title> <publisher> McGraw-Hill Book Company, Inc. </publisher>
Reference-contexts: At their core are the data structures and mathematics of Game Theory <ref> (McKinsey, 1952) </ref> which provided the game tree, static evaluator, and min-max theorem to analyze game outcomes.
Reference: <author> Minsky, M. and Papert, S. </author> <year> (1969). </year> <title> Perceptrons. </title> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference-contexts: They also shared a limitation: their learning rules only worked for two layer networks. This problem was later amplified when Minksy and Papert produced the book "Perceptrons" <ref> (Minsky and Papert, 1969) </ref> which 21 pointed out that two layer networks could only implement linearly separa-ble classification functions and that there were no obvious extensions of the perceptron training rule which allowed it to work with hidden layers.
Reference: <author> Morehead, A. H. and Mott-Smith, G., </author> <title> editors (1963). Hoyle's rules of games. The New American Library, </title> <publisher> Inc., </publisher> <address> 1633 Broadway, New York, New York 10019. </address>
Reference-contexts: 41.0 42.6 45.6 41.3 * Randy | Selects hit or stand at random. * Hold | Always stands with the first two cards dealt. * Dealer | Use the dealer's policy of standing when sum is 17 or greater. * Hoyle | A policy suggested by Hoyle's Rules of Games <ref> (Morehead and Mott-Smith, 1963) </ref>. Stand on 17 or greater, unless dealer's face up card is a 2 through 6, in which case stand on 13 or greater.
Reference: <author> Pearl, J. </author> <year> (1983). </year> <title> Game-searching theory: Survey of recent results. </title> <editor> In Bramer, M. A., editor, </editor> <booktitle> Computer Game-Playing: Theory and Practice, chapter 20. </booktitle> <publisher> Ellis Horwood Limited, Market Cross House, </publisher> <address> Cooper Street, Chichester, West Sussex, PO19 1EB, England. </address> <note> 80 Rosenblatt, </note> <author> F. </author> <year> (1962). </year> <title> Principals of neurodynamics. </title> <publisher> Spartan Books, </publisher> <editor> Wash--ington. Rumelhart and McClelland, editors (1986). </editor> <booktitle> Parallel distributed processing. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: To improve the utility of the imperfect static evaluator, a limited game tree can be generated to some reasonable depth. Each leaf node is evaluated and the results backed-up using the min-max procedure. It should be noted, though, that according to Perl <ref> (Pearl, 1983) </ref> no theoretical model supports the fact that the backed-up evaluation is any betterfor decision making than that of the current node.
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representation by error propagation. </title> <booktitle> In Parallel Distributed Processing, </booktitle> <volume> volume 1, chapter 8. </volume> <publisher> MIT Press. </publisher>
Reference-contexts: The field is rich with interesting neural architectures and training procedures, each designed to tackle specific kinds of problems. Unfortunately, it is also rich in vocabulary that refers to similar concepts. For example, the terms connectionist networks (Feldman and Ballard, 1982), parallel distributed processors <ref> (Rumelhart and McClelland, 1986) </ref>, and neurocomputing (Hecht-Nielsen, 1989) all refer to artificial neural networks. A very good summary can be found in (Simpson, 1988). <p> This little annoyance is known as the structural credit assignment problem, a hurdle that helped stopped neural network research dead in its tracks in the 1960's. Without its solution, only linearly separable 4 functions could be learned. Generalized Delta Rule A welcome enhancement known as the generalized delta rule <ref> (Rumelhart et al., 1986) </ref> allows gradient descent to be applied to weights in hidden layers.
Reference: <author> Samuel, A. L. </author> <year> (1959). </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal on Research and Development, 3,210-229. </journal>
Reference-contexts: Pioneering ideas were put forth in the 1950's by A. L. Samuel <ref> (Samuel, 1959) </ref> with a Checkers program that could improve just by playing itself, but until recently there have been few advances. Now during the 1980's, learning techniques based upon statistics, genetics, and neural networks have become popular and are showing a capacity for solving real world problems. <p> This is why it's called a temporal difference method since it attempts to minimize the difference between temporally successive predictions. The ideas that Sutton describes are not entirely new, but mostly a refinement and formalization of temporal difference methods used by previous learning systems. Examples are Samuel's checker player <ref> (Samuel, 1959) </ref> (yes, he had it from the beginning), Holland's Bucket Brigade (Holland, 1983) for Genetic Algorithms, and Sutton's earlier work with the Adaptive Heuristic Critic network (Sutton, 1984). Incremental TD () Sutton developed an incremental real-time learning procedure for neural networks known as TD (). <p> Since his talk at the Internation Joint Conference on Neural Networks in July 1991, the approach has been adopted by others; several such case will be covered. Samuel's checker player Samuel's work 6 with a checker playing program <ref> (Samuel, 1959) </ref> has probably been reference more than any other game learning research. Using a standard game tree search architecture, he investigated several different ideas for teaching the static evaluator. One method he toyed with during the 50's was what he called generalization learning, an early temporal difference method.
Reference: <editor> Reprinted in E. A. Feigenbaum & J. Feldman (Eds.), </editor> <booktitle> Computers and thought. </booktitle> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Samuel, A. L. </author> <year> (1967). </year> <title> Some studies in machine learning using the game of checkers, II. </title> <journal> IBM Journal on Research and Development, 11,601-617. </journal>
Reference-contexts: Using a standard game tree search architecture, he investigated several different ideas for teaching the static evaluator. One method he toyed with during the 50's was what he called generalization learning, an early temporal difference method. He also investigated other techniques such as rote learning and nonlinear signature tables <ref> (Samuel, 1967) </ref>. The static evaluator was the usual linear combination of features as in equation 2.12. The coefficients for the features were adjusted during play by comparing the evaluation of the current state against the backed-up value provided by a min-max search.
Reference: <author> Shannon, C. E. </author> <year> (1950). </year> <title> Programming a computer for playing chess. </title> <journal> Philosophy Magazine, 41,256-275. </journal>
Reference-contexts: Three words are emphasized because they indicate key problems in building such an architecture. Fortunately, solutions have been devised for each that can be readily built upon. Game Playing techniques for computers are well established, having a history that dates back to at least 1950 <ref> (Shannon, 1950) </ref>. As such, there is a ready library of approaches including game trees and various search algorithms (min-max, alpha-beta pruning, etc.). 5 Machine learning has been one of the more difficult aspects of AI (are any easy?), especially as applied to games.
Reference: <author> Simpson, P. K. </author> <year> (1988). </year> <title> A review of artificial neural systems II: Paradigms, applications, and implementations. </title> <type> Technical Report MZ 7202-K, </type> <institution> General Dynamics, Electronics Division, </institution> <address> P.O. Box 85310, San Diego, CA 92138. </address>
Reference-contexts: Unfortunately, it is also rich in vocabulary that refers to similar concepts. For example, the terms connectionist networks (Feldman and Ballard, 1982), parallel distributed processors (Rumelhart and McClelland, 1986), and neurocomputing (Hecht-Nielsen, 1989) all refer to artificial neural networks. A very good summary can be found in <ref> (Simpson, 1988) </ref>.
Reference: <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal credit assignment in reinforcement learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Amherst. </institution>
Reference-contexts: Examples are Samuel's checker player (Samuel, 1959) (yes, he had it from the beginning), Holland's Bucket Brigade (Holland, 1983) for Genetic Algorithms, and Sutton's earlier work with the Adaptive Heuristic Critic network <ref> (Sutton, 1984) </ref>. Incremental TD () Sutton developed an incremental real-time learning procedure for neural networks known as TD (). It is essentially a modification of the delta rule which 23 allows it to work in a forward only fashion.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. Machine Learning, </title> <publisher> 3,9-44. </publisher>
Reference-contexts: When the outcome is not discovered until some time after a move, it becomes a difficult temporal reinforcement problem. A promising solution which will be adopted is the method of Temporal Difference (TD) <ref> (Sutton, 1988) </ref>. <p> This has lead to a steady increase of new concepts and applications, still growing today. 5 Backpropagation was actually discovered much earlier than the 80's, but wasn't well known until the PDP group rediscovered it. 22 2.2 Temporal Difference Methods The Temporal Difference (TD) procedure of Richard Sutton <ref> (Sutton, 1988) </ref> provides a method for solving the temporal credit assignment problem. Its goal is to teach a controller to accurately predict future events (rewards) by just observing a sequential decision task over and over. <p> Learning Cumulative Cost A slight twist on the TD methods must be presented since this problem involves learning to predict a cumulative outcome (the number of moves) as opposed to a final outcome. The general TD learning rule can be changed so 44 that it accumulates a cost function <ref> (Sutton, 1988) </ref>. w t = ff (c t+1 + P t+1 P t ) k=1 @w The only difference is that a new term c t+1 |the cost of traversing from state s t to s t+1 |is added to the previous prediction.
Reference: <author> Tesauro, G. J. </author> <year> (1991). </year> <title> Practical issues in temporal difference learning. </title> <type> Technical Report RC 17223 (#76307), </type> <institution> IBM Research Division, T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY 10598. </address>
Reference-contexts: The architecture was similar to that of NeuroGammon except that instead of learning from a database of moves, it now learned while playing itself. To begin with, several problem areas were addressed in <ref> (Tesauro, 1991) </ref> which could present road blocks to TD success. * Will learning during self play turn into self consistent yet non-optimal play? Answer: No, if trained long enough. * Will a TD method work when learning to predict and control simulta neously? Answer: Yes. * Sutton's convergence proof is for <p> These two values are then combined into a single score as P win P lose providing an evaluation from -1 to +1. This use of separate outputs to represent different game outcomes was inspired by Tesauro's backgammon player <ref> (Tesauro, 1991) </ref>. Encoding the State Most of the networks trained in this section encode the state into a set of 8 features related to the singlet and doublet idea used by Huey.
Reference: <author> Tesauro, G. J. </author> <year> (1993). </year> <title> TD-gammon, a self-teaching backgammon program, achieves master-level play. </title> <note> Available via FTP from archive.ohio-state.edu:/pub/neuroprose. 81 Tesauro, </note> <author> G. J. and Sejnowski, T. J. </author> <year> (1989). </year> <title> A parallel network that learns to play backgammon. </title> <journal> Artificial Intelligence, </journal> <volume> 39(3). </volume>
Reference-contexts: When the outcome is not discovered until some time after a move, it becomes a difficult temporal reinforcement problem. A promising solution which will be adopted is the method of Temporal Difference (TD) (Sutton, 1988). Its power has been demonstrated by the recent success of Gerald Tesauro's backgammon program <ref> (Tesauro, 1993) </ref> which is perhaps the best computer player to date. 1.2 Ob jectives The somewhat general architecture that will be proposed might be best described as a pattern classification system which learns to associate a given game situation with its likelihood of being on a path to success or failure. <p> In addition, a network was able to learn only using the raw board as input and performed as well as NeuroGammon (which used features). Even better, Tesauro reported in June 1993 <ref> (Tesauro, 1993) </ref> that with additional training (1,500,000 games total) and a 2-ply search, TD-Gammon has actually achieved master-level play 7 . Other TD Game Players Papers have been written by at least two other individuals on using Sut-ton's TD method for game learning.
Reference: <author> Thrun, S. B. and Moller, K. </author> <year> (1991). </year> <title> On planning and exploration in non-discrete environments. </title> <note> Available via FTP from archive.ohio-state.edu:/pub/neuroprose. </note>
Reference-contexts: The process of selecting an action consists of three functions: action/next state generation, state encoding, and state evaluation (see Figure 3.2). Two distinct methods for selecting an action are direct and indirect <ref> (Thrun and Moller, 1991) </ref>. Direct | A direct selection scheme uses a neural network to map a state to an action without any search. Advantage: It selects an action quickly and and in constant time.
Reference: <author> Walker, S. </author> <year> (1992). </year> <title> Untitled undergraduate thesis. Copy received via direct communication with the author. </title>
Reference-contexts: Other TD Game Players Papers have been written by at least two other individuals on using Sut-ton's TD method for game learning. Steven Walker's undergraduate thesis <ref> (Walker, 1992) </ref> describes learning experiments using the game Othello. His work mirrors Tesauro's approach, trying both supervised and TD method trained networks. Likewise, he also discovered that TD produced a superior player. <p> The alternate raw encoding is quite simple. Each of the 9 squares has two inputs assigned to it: one for the presence of an X the other for an O yielding a total of 18 inputs. A bi-polar encoding may also work as used by Walker in <ref> (Walker, 1992) </ref> although more compact encodings tend to increase learning time. 1 Earlier attempts with Tic-Tac-Toe used a network with a single output P win . This caused problems when deciding what utility to assign a draw.
Reference: <author> Werbos, P. J. </author> <year> (1989). </year> <title> Backpropagation and neurocontrol: A review and prospectus. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pp 209-216. </pages>
Reference-contexts: An adaptive control system that can learn to perform sequential decision tasks might find application in many domains such as controlling a manufacturing process, autonomous vehicles, avionics, and aids for the disabled. As noted by Paul Werbos <ref> (Werbos, 1989) </ref>, work towards solving these problems involves a large share of the U. S. Gross National Product (GNP). Games, which are sequential decision tasks, provide convenient testbeds for the study of such learning. Their goals and rules 3 are well defined simplifying the modeling and simulation process.
Reference: <author> Widrow, B. and Hoff, M. </author> <year> (1960). </year> <title> Adaptive switching circuits. </title> <booktitle> In 1960 IRE WESCON Convention Record, </booktitle> <volume> volume part 4, </volume> <pages> pp 96-104. 82 </pages>
Reference-contexts: Delta Rule The delta rule, or Least Mean Squares (LMS) training procedure <ref> (Widrow and Hoff, 1960) </ref>, uses gradient descent to minimize the error E in Equation 2.2. <p> The late 1950's began the first wave of neural network popularity with such systems as Frank Rosenblatt's Perceptron (Rosenblatt, 1962) and Bernard Widrow's Adaline (which included the delta rule) <ref> (Widrow and Hoff, 1960) </ref>. These two systems were quite similar in that they used a layered feedfor-ward topology along with a supervised learning rule for adapting connection weights. They also shared a limitation: their learning rules only worked for two layer networks.
References-found: 32

