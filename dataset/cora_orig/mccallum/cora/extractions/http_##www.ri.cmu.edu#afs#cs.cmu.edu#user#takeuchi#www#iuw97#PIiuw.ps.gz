URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/takeuchi/www/iuw97/PIiuw.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/takeuchi/www/research.html
Root-URL: 
Title: Abstract  
Abstract: The Carnegie Mellon University MURI project sponsored by ONR performs multidisciplinary research in integrating vision algorithms with sensing technology for lowpower, lowlatency, compact adaptive vision systems. These are crucial features necessary for augmenting the human sensory system and enabling sensory driven information delivery. The project spans four subareas ranging from low to high level of vision: (1) smart filters, based on the AcoustoOptic Tunable Filter (AOTF) technology; (2) computational sensor methodology, which integrates raw sensing and computation by means of VLSI technology; (3) neural-network based saliency identification techniques for identifying the most useful information for extraction and display; and (4) visual learning methods for automatic signaltosymbol mapping. 
Abstract-found: 1
Intro-found: 1
Reference: [Allport, 1989] <author> Allport, A. </author> <title> Visual Attention, Foundation of Cognitive Science, </title> <editor> M. Posner (ed.), </editor> <publisher> MIT Press, </publisher> <year> 1989, </year> <pages> pp. 631682. </pages>
Reference-contexts: Second, it has been shown that the visual attention improves performance, and is needed for maintaining coherent behavior while interacting with the environment (i.e. attentionforaction) <ref> [Allport, 1989] </ref>. Location of such attention must be maintained in the environmental coordinates, thus maintaining coherence under ocular and head motion [Milanese, 1993]. Unlike eye movement (i.e., overt shifts), the attention shifts (i.e. covert shifts) do not require any motor action, but occur internally on a fixed retinal image.
Reference: [Aloimonos, 1992] <author> Aloimonos, J. (ed.), </author> <title> Special Issue on Purposive, Qualitative, Active Vision, CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> Vol. 56, No. 1, </volume> <year> 1992. </year>
Reference-contexts: For example, the concept of active vision proposes to control the geometric parameters of the camera (e.g., pan, tilt, etc.) to improve the reliability of the perception <ref> [Aloimonos, 1992] </ref>. It has been shown that initially illposed problems can be solved after the topdown adaptation of the cameras pose has acquired new, more appropriate image data. However, adjusting geometric parameters is only one level at which adaptation can take place.
Reference: [Brajovic and Kanade, 1994] <author> Brajovic, V. and T. Kanade, </author> <title> Computational Sensors for Global Operations, </title> <booktitle> IUS Proceedings, </booktitle> <pages> pp. 621-630, </pages> <year> 1994. </year>
Reference-contexts: Our recent work has been concerned with efficient implementation of global operations over large groups of image data using a computational sensor paradigm <ref> [Brajovic and Kanade, 1994] </ref>.
Reference: [Brajovic and Kanade, 1996] <author> V. Brajovic and T. Kanade: </author> <title> A Sorting Image Sensor: An Example of Massively Parallel Intensity-to-Time Processing for Low-Latency Computational Sensors, </title> <booktitle> Proceedings of the 1996 IEEE International Conference on Robotics and Automation, </booktitle> <address> Minneapo-lis, MN, </address> <month> April </month> <year> 1996, </year> <pages> pp. 16381643. </pages>
Reference-contexts: The main difficulty with implementing global operations comes from the necessity to bring together all or most of the data in the input data set. We have formulated two mechanisms for implementing global operations in computational sensors: (1) sensory attention [Brajovic and Kanade, 1997], and (2) intensitytotime processing paradigm <ref> [Brajovic and Kanade, 1996] </ref>. The sensory attention is based on the premise that salient features within the retinal image represent important global features of the entire image. This premise is attractive for two reasons.
Reference: [Brajovic and Kanade, 1997] <author> Brajovic, V and T. Kanade: </author> <title> Computational Sensors for Low Latency Adaptive Vision, </title> <booktitle> in IUW Proc., </booktitle> <year> 1997. </year>
Reference-contexts: Our research further develops these emerging technologies, and brings these visions closer to reality. The CMU MURI project performs multidisciplinary research spanning all levels of vision and sensing: dynamically tunable acoustooptic multispectral imaging <ref> [Brajovic and Kanade, 1997] </ref>; VLSIbased computational sensors [Brajovic and Kanade, 1997]; neural network saliency detection [Pomerleau, 1997]; automatic visual acquisition of object models [Hebert et al., 1997]; domainindependent evolutionbased learning for signaltosymbol mapping [Glickman and Sycara, 1997]; and learning coordination among multiple signal-to-symbol mapping agents [Teller and Veloso, 1997b]. <p> Our research further develops these emerging technologies, and brings these visions closer to reality. The CMU MURI project performs multidisciplinary research spanning all levels of vision and sensing: dynamically tunable acoustooptic multispectral imaging <ref> [Brajovic and Kanade, 1997] </ref>; VLSIbased computational sensors [Brajovic and Kanade, 1997]; neural network saliency detection [Pomerleau, 1997]; automatic visual acquisition of object models [Hebert et al., 1997]; domainindependent evolutionbased learning for signaltosymbol mapping [Glickman and Sycara, 1997]; and learning coordination among multiple signal-to-symbol mapping agents [Teller and Veloso, 1997b]. <p> The main difficulty with implementing global operations comes from the necessity to bring together all or most of the data in the input data set. We have formulated two mechanisms for implementing global operations in computational sensors: (1) sensory attention <ref> [Brajovic and Kanade, 1997] </ref>, and (2) intensitytotime processing paradigm [Brajovic and Kanade, 1996]. The sensory attention is based on the premise that salient features within the retinal image represent important global features of the entire image. This premise is attractive for two reasons.
Reference: [Carlsson, 1996] <author> Carlsson, S. </author> <title> Combinatorial Geometry for Shape Representation and Indexing, </title> <booktitle> Proc. International Workshop on Object Representation for Computer Vision. </booktitle> <address> Cambridge, England, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: An object model is built by extracting features from a collec tion of observations. The most significant features are extracted for the entire set and are used in the model representation. Extensions to generic object recognition were presented recently <ref> [Carlsson, 1996] </ref>. Other recent approaches use the images directly to extract a small set of characteristic object images which are compared with observed views at recognition time. For example, the eigen-images techniques are based on this idea.
Reference: [Brajovic and Kanade, 1997] <author> Denes, l.J., M. Gott-lieb, B.Kaminsky, P. Metes, Z.K. Kun, M. Capizzi, J. Hibner, D. Purta, </author> <title> and A.M. Guzman: Multi-Spectral Imaging Filters, </title> <booktitle> in IUW Proc., </booktitle> <year> 1997. </year>
Reference-contexts: Our research further develops these emerging technologies, and brings these visions closer to reality. The CMU MURI project performs multidisciplinary research spanning all levels of vision and sensing: dynamically tunable acoustooptic multispectral imaging <ref> [Brajovic and Kanade, 1997] </ref>; VLSIbased computational sensors [Brajovic and Kanade, 1997]; neural network saliency detection [Pomerleau, 1997]; automatic visual acquisition of object models [Hebert et al., 1997]; domainindependent evolutionbased learning for signaltosymbol mapping [Glickman and Sycara, 1997]; and learning coordination among multiple signal-to-symbol mapping agents [Teller and Veloso, 1997b]. <p> Our research further develops these emerging technologies, and brings these visions closer to reality. The CMU MURI project performs multidisciplinary research spanning all levels of vision and sensing: dynamically tunable acoustooptic multispectral imaging <ref> [Brajovic and Kanade, 1997] </ref>; VLSIbased computational sensors [Brajovic and Kanade, 1997]; neural network saliency detection [Pomerleau, 1997]; automatic visual acquisition of object models [Hebert et al., 1997]; domainindependent evolutionbased learning for signaltosymbol mapping [Glickman and Sycara, 1997]; and learning coordination among multiple signal-to-symbol mapping agents [Teller and Veloso, 1997b]. <p> The main difficulty with implementing global operations comes from the necessity to bring together all or most of the data in the input data set. We have formulated two mechanisms for implementing global operations in computational sensors: (1) sensory attention <ref> [Brajovic and Kanade, 1997] </ref>, and (2) intensitytotime processing paradigm [Brajovic and Kanade, 1996]. The sensory attention is based on the premise that salient features within the retinal image represent important global features of the entire image. This premise is attractive for two reasons.
Reference: [Glickman and Sycara, 1997] <author> Glickman, M. and K. Sycara: </author> <title> Adaptive Acquisition of Search Control Knowledge in the Evolution of Face Recognition Neural Networks, </title> <booktitle> in IUW Proc., </booktitle> <year> 1997. </year>
Reference-contexts: MURI project performs multidisciplinary research spanning all levels of vision and sensing: dynamically tunable acoustooptic multispectral imaging [Brajovic and Kanade, 1997]; VLSIbased computational sensors [Brajovic and Kanade, 1997]; neural network saliency detection [Pomerleau, 1997]; automatic visual acquisition of object models [Hebert et al., 1997]; domainindependent evolutionbased learning for signaltosymbol mapping <ref> [Glickman and Sycara, 1997] </ref>; and learning coordination among multiple signal-to-symbol mapping agents [Teller and Veloso, 1997b]. We believe that the tight integration of vision algorithms and sensing technology will result in lowpower, low latency, compact, adaptive vision systems crucial for effective human sensory augmentation. 1.1.
Reference: [Gross et al.] <author> P. Gros, O. Bournez and E. Boyer. </author> <title> Using Local Planar Geometric Invariants to Match and Model Images of Line Segments. </title> <note> To appear in Int. </note> <editor> J. </editor> <booktitle> of Computer Vision and Image Understanding. </booktitle>
Reference-contexts: The information most relevant to recognition is extracted from the collection of raw images and used as the model for recognition. This process is often referred to as visual learning. Progress has been made recently in developing such approaches. For example, in object modeling <ref> [Gross et al.] </ref>, 2D or 3D model of objects are built for recognition applications. An object model is built by extracting features from a collec tion of observations. The most significant features are extracted for the entire set and are used in the model representation. <p> The main goal of the work presented here was to explore the use of tools and methods in the field of image retrieval when applied to the problem of landmark recognition. It is clear that the global architecture of the system is close to that of object recognition systems <ref> [Gross et al.] </ref>: a training stage in which 3D shape, 2D aspects, or groups, are characterized is followed by a recognition stage in which this information is used to recognize the models, objects or groups in new images.
Reference: [Hebert et al., 1997] <author> Hebert, M., K. Ikeuchi, Y. Takeuchi, P. Gros, </author> <title> Visual Learning for Landmark Recognition, </title> <booktitle> in IUW Proc., </booktitle> <year> 1997. </year>
Reference-contexts: The CMU MURI project performs multidisciplinary research spanning all levels of vision and sensing: dynamically tunable acoustooptic multispectral imaging [Brajovic and Kanade, 1997]; VLSIbased computational sensors [Brajovic and Kanade, 1997]; neural network saliency detection [Pomerleau, 1997]; automatic visual acquisition of object models <ref> [Hebert et al., 1997] </ref>; domainindependent evolutionbased learning for signaltosymbol mapping [Glickman and Sycara, 1997]; and learning coordination among multiple signal-to-symbol mapping agents [Teller and Veloso, 1997b].
Reference: [Kanade and Bajcsy, 1993] <author> Kanade, T. and R. </author> <title> Bajcsy, Computational Sensors: A Report from DARPA workshop, </title> <booktitle> IUS Proceedings, </booktitle> <year> 1993. </year>
Reference-contexts: By using an appropriate threshold, the target image alone is displayed against a black background. Tests of laboratory scenes give encouragingly good results. 3. Computational Sensors for LowLatency Adaptive Vision Contributors: Vladimir Brajovic and Takeo Kanade The computational sensor paradigm <ref> [Kanade and Bajcsy, 1993] </ref> has the potential to greatly reduce latency and provide topdown sensory adaptation to the vision system.
Reference: [Lamiroy and Gros, 1996] <author> B.Lamiroy and P.Gros. </author> <title> Rapid Object Indexing and Recognition Using Enhanced Geometric Hashing. </title> <booktitle> Proc. of the 4th European Conf. on Computer Vision, </booktitle> <address> Cambridge, England, </address> <booktitle> pages 59--70, </booktitle> <volume> Vol. 1, </volume> <month> April </month> <year> 1996. </year>
Reference-contexts: A similar problem, although in a completely different context, is encountered in image indexing, where the main problem is to store and organize images to facilitate their retrieval <ref> [Lamiroy and Gros, 1996] </ref> [Schmid and Mohr, 1996]. The emphasis in this case is on the kind of features used and the types of requests that can be made by the user. For image retrieval, actual systems (QBIC, JACOB, Virage...) are closer to smart browsing than to image recognition.
Reference: [Milanese, 1993] <author> R. Milanese, </author> <title> Detecting Salient regions in an Image: from Biological evidence to computer implementation, </title> <type> Ph.D. Thesis, </type> <institution> Dept. of Computer Science, U. of Genova, Switzerland, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Second, it has been shown that the visual attention improves performance, and is needed for maintaining coherent behavior while interacting with the environment (i.e. attentionforaction) [Allport, 1989]. Location of such attention must be maintained in the environmental coordinates, thus maintaining coherence under ocular and head motion <ref> [Milanese, 1993] </ref>. Unlike eye movement (i.e., overt shifts), the attention shifts (i.e. covert shifts) do not require any motor action, but occur internally on a fixed retinal image. For this reason, attention shifts are faster and play an important role in lowlatency vision systems.
Reference: [Najm et al., 1995] <author> Najm, W., Mironer, M. and Fraser, L. </author> <title> (1995) Analysis of Target Crashes and ITS Countermeasure Actions. </title> <booktitle> Proc. of 1995 ITS America Annual Meeting, </booktitle> <pages> pp. 931-940. </pages>
Reference-contexts: In the near term, we will begin interfacing some of our computational sensors with smart AOTF filters. 4. Visibility Estimation from a Moving Vehicle Contributor: Dean Pomerleau Reduced visibility caused by fog, rain, snow, darkness and glare is a frequent contributing factor to traffic accidents <ref> [Najm et al., 1995] </ref>. In fact, some of the most serious of all highway incidents, sometimes involving dozens or even hundreds of vehi-Figure 2: Sequence of images of indices computed by the sorting sensors. cles, occur when reduced visibility conditions result in a chain reaction of crashes.
Reference: [National Weather Service 1996] <institution> Surface Weather Observations and Reports (1996) Federal Meteorological Handbook, </institution> <note> 5th Edition, National Weather Service Publication FMH-1. </note>
Reference-contexts: Technologies typically employed to estimate visibility include: transmissometers, which measure the transmittance of the atmosphere over a baseline distance; and nephelometers, which measure the scattering coefficient caused by suspended particles in an air sample <ref> [National Weather Service 1996] </ref>. Unfortunately, these systems suffer from several drawbacks as they are not always estimating visibility from the drivers point of view.
Reference: [Pomerleau and Jochem, 1996] <author> Pomerleau, D. and Jochem, T. </author> <title> (1996) Rapidly Adapting Machine Vision for Automated Vehicle Steering. </title> <journal> IEEE Expert, </journal> <volume> Vol. 11, No. 2. </volume> <pages> pp. 19-27. </pages>
Reference-contexts: Even the features that are supposed to be consistent on a roadway, the lane markings, vary greatly in their appearance, and are in fact frequently missing or obscured. The Rapidly Adapting Lateral Position Handler (RALPH) system <ref> [Pomerleau and Jochem, 1996] </ref> overcomes this difficulty when detecting the position and curvature of the road ahead in camera images by utilizing whatever features are visible on the roadway.
Reference: [Pomerleau, 1997] <author> Pomerleau, D. </author> <title> Visibility Estimation from a Moving Vehicle Using the RALPH Vision System, </title> <booktitle> in IUW Proc., </booktitle> <year> 1997. </year>
Reference-contexts: Our research further develops these emerging technologies, and brings these visions closer to reality. The CMU MURI project performs multidisciplinary research spanning all levels of vision and sensing: dynamically tunable acoustooptic multispectral imaging [Brajovic and Kanade, 1997]; VLSIbased computational sensors [Brajovic and Kanade, 1997]; neural network saliency detection <ref> [Pomerleau, 1997] </ref>; automatic visual acquisition of object models [Hebert et al., 1997]; domainindependent evolutionbased learning for signaltosymbol mapping [Glickman and Sycara, 1997]; and learning coordination among multiple signal-to-symbol mapping agents [Teller and Veloso, 1997b].
Reference: [Schmid and Mohr, 1996] <author> C. Schmid and R. Mohr. </author> <title> Combining Greyvalue Invariants with Local Constraints for Object Recognition. </title> <booktitle> Proceedings of the Conference on Computer Vision and Pattern Recognition, </booktitle> <address> San Francisco, California, USA. pages 872--877, </address> <month> June </month> <year> 1996 </year>
Reference-contexts: A similar problem, although in a completely different context, is encountered in image indexing, where the main problem is to store and organize images to facilitate their retrieval [Lamiroy and Gros, 1996] <ref> [Schmid and Mohr, 1996] </ref>. The emphasis in this case is on the kind of features used and the types of requests that can be made by the user. For image retrieval, actual systems (QBIC, JACOB, Virage...) are closer to smart browsing than to image recognition.
Reference: [Teller and Veloso, 1997b] <author> Teller, A. and M. Veloso, </author> <title> Learning Better Classification Teams, </title> <booktitle> in IUW Proc., </booktitle> <year> 1997. </year>
Reference-contexts: dynamically tunable acoustooptic multispectral imaging [Brajovic and Kanade, 1997]; VLSIbased computational sensors [Brajovic and Kanade, 1997]; neural network saliency detection [Pomerleau, 1997]; automatic visual acquisition of object models [Hebert et al., 1997]; domainindependent evolutionbased learning for signaltosymbol mapping [Glickman and Sycara, 1997]; and learning coordination among multiple signal-to-symbol mapping agents <ref> [Teller and Veloso, 1997b] </ref>. We believe that the tight integration of vision algorithms and sensing technology will result in lowpower, low latency, compact, adaptive vision systems crucial for effective human sensory augmentation. 1.1. <p> This insurance of mutual suitability is the orchestration problem. Evolutionary computation is a natural machine learning environment in which to find many, behaviorally distinct models. We focus on PADO, a evolutionary computation framework designed specifically for signal classification (e.g., <ref> [Teller and Veloso, 1997b] </ref>). As a process of divide and conquer, PADO evolves multiple pools of sub-solutions and then orchestrates one or more learned models from each pool. <p> The question we investigate is: What opportunities are there for learning in the orchestration process and how much improvement can this learning provide? While answering this question, our research demonstrated several things <ref> [Teller and Veloso, 1997b] </ref>. First, specific experiments on distinct signals demonstrated the feasibility of PADOs divide and conquer strategy; the failure of the evolved orchestration procedure suggested PADOs preferability to unconstrained learning.
References-found: 19

