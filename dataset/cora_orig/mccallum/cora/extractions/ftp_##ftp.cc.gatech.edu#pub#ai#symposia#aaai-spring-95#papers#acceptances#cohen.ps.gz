URL: ftp://ftp.cc.gatech.edu/pub/ai/symposia/aaai-spring-95/papers/acceptances/cohen.ps.gz
Refering-URL: ftp://ftp.cc.gatech.edu/pub/ai/symposia/aaai-spring-95/papers/finals.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A Representation and Learning Mechanisms for Mental States  
Author: Paul Cohen, Marc Atkin, Tim Oates, Dawn Gregory 
Note: A Motivating Example  
Address: Amherst, MA 01003-4610  
Affiliation: Experimental Knowledge Systems Laboratory Computer Science Department, LGRC University of Massachusetts  
Abstract: We want to build an agent that plans by imagining sequences of future states. Subjectively, these states seem very rich and detailed. Providing an agent with sufficiently rich knowledge about its world is an impediment to studying this kind of planning, so we have developed mechanisms for an agent to learn about its world. One mechanism learns dependencies between synchronous "snapshots" of the world; the other learns about processes and their relationships. Imagine an old kitchen cabinet, recently removed from a kitchen wall, six feet long, with doors but no back, nails sticking out of odd places, splintered where the crowbar did its work. This cabinet rests on the basement floor, but you want to attach it to the basement wall. It weighs about 50 lbs and it's very cumbersome. Your first thought is to attach a batten to the back of the cabinet along its length, then drill screw holes in the basement wall, then drill through the batten at locations that correspond to the holes in the wall. You intend to lift the cabinet four feet off the ground, register the batten holes with the screw holes, and screw the cabinet to the wall. Running through this plan in your mind, you realize it won't work, because you cannot hold a 50 lb., six-foot, structurally unsound cabinet four feet off the ground with one hand, while you screw it to the wall with the other. You need another person to help you, or you must build some sort of scaffolding to hold the cabinet in place. Suppose neither option is feasible. After thinking about it for a while, you suddenly come up with a new plan: Attach three or four L-brackets to the basement wall, hoist the cabinet onto the L-brackets, and then secure it to the wall. As you run through this plan mentally you recognize several hazards: the doors will swing and get in the way; the nails are dangerous and must be removed; you must not grasp the cabinet where the wood is splintered. Subjectively, each hazard seems to be "read" from a mental movie of sorts: You imagine hoisting the cabinet, having it lean slightly toward you, and a door swinging open and knocking your spectacles off your nose. You imagine holding the cabinet against the wall with your shoulder (now that its weight is supported by the L-brackets) leaving two hands free to drive in the screws, but then you realize that if you drop a screw, you can't bend down to pick it up, so you modify your plan and put a bunch of screws in your shirt pocket. Or if you don't have a pocket, you hold them in your teeth. You can almost feel the metal chinking against your teeth. The most striking thing about this example is how much you think about, and how rich your mental images seem. Another characteristic of the example is the "functional plasticity" of its components: Your shoulder becomes something to brace against the cabinet; your mouth becomes something to hold screws; the cabinet doors become something that hit you in the face; the nails, which once functioned as fasteners, now tear your flesh. A third characteristic of the example is that once you have a skeletal plan (e.g., lift the cabinet onto the L-brackets and attach it to the wall) you seem to fill in the details by imagining executing the plan, by visualization and forward simulation. Indeed, this is how you discovered that the original plan wouldn't work. Perhaps crude plans can be generated by conventional, propositional AI algorithms, but checking a plan seems to require some ability to imagine or visualize oneself executing it. Subjectively, the frame problem and problems of relevance don't seem to arise (McCarthy and Hayes, 1969). Suppose that in an attempt to have the screws near at hand, you place them on one of the shelves of the cabinet before you lift it. Where are the screws when the cabinet has been hoisted into place? Subjectively, in your mind's eyes and ears, you can see and hear the screws as they roll off the shelf and fall on the floor. Similarly, the relevance of swinging doors seems to emerge as you imagine hoisting the cabinet. Subjectively it isn't difficult to envision future states, nor are we troubled by the impossibility of knowing precisely how the world will look after an action. We know enough about processes such as hoisting cabinets to support planning by visualization. The focus of this paper is how we learn about processes. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> Computational Intelligence. </institution> <year> 1994. </year> <title> The Imagery Debate Revisited. </title> <journal> Special issue of Computational Intelligence, </journal> <volume> Vol. 9, No. </volume> <pages> 4. </pages>
Reference: <author> Gibson, J .J. </author> <year> 1979. </year> <title> The Ecological Approach to Visual Perception. </title> <address> Boston: Houghton-Mi*in. </address>
Reference: <author> Hammond., K. </author> <year> 1986. </year> <title> CHEF: A Model of Case-based Planning. </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence. </booktitle> <pages> pp. 261-271. </pages>
Reference-contexts: In fact, we don't know whether relationships between fluents are different in kind from the knowledge we use to imagine activities. The structure of fluents (and relationships among them) seems minimal compared with the rich structures in, say, the case-based planning literature <ref> (e.g., Hammond, 1986) </ref>. We do not yet know how Baby "carves up" its experiences. We tend to think of processes in terms of a beginning, middle, and end, and there's considerable consensus about which activities are which.
Reference: <author> A. E. Howe and P. R. Cohen. </author> <title> Understanding Planner Behavior. </title> <note> To appear in AI Journal, </note> <year> 1995. </year>
Reference: <author> J. M. Mandler. </author> <title> How to Build a Baby: II. Conceptual Primitives. </title> <journal> Psychological Review, 1992, </journal> <volume> Vol. 99, No. 4, </volume> <pages> pp. 587-604. </pages>
Reference: <author> J. McCarthy and P. J. Hayes. </author> <year> 1969. </year> <title> Some Philosophical Problems from the Standpoint of Artificial Intelligence. </title> <editor> In B. Meltzer and D. Michie, </editor> <booktitle> Machine Intelligence IV. </booktitle> <publisher> Elsevier. </publisher>
Reference: <author> T. Oates, D. E. Gregory and P. R. Cohen. </author> <title> Detecting Complex Dependencies in Data. </title> <booktitle> To appear in Proceedings of the Fifth International Workshop on AI and Statistics, </booktitle> <year> 1995. </year>
Reference: <author> Z. Zheng. </author> <title> A benchmark for classifier learning. </title> <institution> Technical Report from Basser Department of Computer Science, University of Sydney, NSW. </institution> <year> 1994. </year> <title> Acknowledgments We thank two anonymous reviewers for their insightful and encouraging comments. The work on msdd was supported by arpa/rl Contract F30602-93-C-0100. </title>
References-found: 8

