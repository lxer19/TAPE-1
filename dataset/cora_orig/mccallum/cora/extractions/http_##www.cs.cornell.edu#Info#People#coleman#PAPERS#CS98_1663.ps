URL: http://www.cs.cornell.edu/Info/People/coleman/PAPERS/CS98_1663.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/coleman/papers.html
Root-URL: 
Title: Automatic Differentiation and MATLAB Interface Toolbox  
Author: Thomas F. Coleman Arun Verma 
Date: January 9, 1998  
Note: ADMIT-1  
Abstract: Cornell Computer Science Technical Report TR 98-1663 Abstract ADMIT-1 enables you to compute sparse Jacobian and Hessian matrices, using automatic differentiation technology, from a MATLAB environment. You need only supply a function to be differentiated and ADMIT-1 will exploit sparsity if present to yield sparse derivative matrices (in sparse MATLAB form). A generic AD tool, subject to some functionality requirements, can be plugged into ADMIT-1; examples include ADOL-C [19] (C/C++ target functions) and ADMAT [9] (MATLAB target functions). ADMIT-1 also allows for the calculation of gradients and has several other related functions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. M. Averick, J. J. More, C. H. Bischof, A. Carle, and A. Griewank, </author> <title> Computing large sparse Jacobian matrices using automatic differentiation, </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 15 (1994), </volume> <pages> pp. 285-294. </pages>
Reference-contexts: Sparse finite differencing techniques were first introduced by Curtis, Powell and Reid [15]; Coleman and More [8, 7, 3] and Newsam and Ramsdell [20] further developed these ideas using graph-theoretic interpretations. Recently, related methods were developed to be used in conjunction with AD tools instead of finite differencing <ref> [14, 1, 2] </ref>. 2.1 Computation of a sparse Jacobian One way to approach the problem of estimating a sparse Jacobian matrix of a mapping F : &lt; n ! &lt; m , is in the following terms : given a sparse m by n matrix J , obtain vectors d 1 <p> Bi-coloring 1-sided Coloring Direct Substitution column row 337 270 1753 452 Table 1 Totals for LP Collection Also, similar to the results reported in <ref> [1] </ref> for forward-mode direct determination, the Jacobian matrices determined by our bi-coloring/AD approach are significantly and uniformly more accurate than the finite-difference approximations. This is true for both direct determination and the substitution approach. Second, the direct approach is uniformly more accurate than the substitution method.
Reference: [2] <author> C. H. Bischof, A. Bouaricha, P. M. Khademi, and J. J. </author> <title> More, Computing gradients in large-scale optimization using automatic differentiation, </title> <type> Preprint MCS-P488-0195, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Sparse finite differencing techniques were first introduced by Curtis, Powell and Reid [15]; Coleman and More [8, 7, 3] and Newsam and Ramsdell [20] further developed these ideas using graph-theoretic interpretations. Recently, related methods were developed to be used in conjunction with AD tools instead of finite differencing <ref> [14, 1, 2] </ref>. 2.1 Computation of a sparse Jacobian One way to approach the problem of estimating a sparse Jacobian matrix of a mapping F : &lt; n ! &lt; m , is in the following terms : given a sparse m by n matrix J , obtain vectors d 1
Reference: [3] <author> T. F. Coleman and J. Y. Cai, </author> <title> The cyclic coloring problem and estimation of sparse Hessian matrices, </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 7 (1986), </volume> <pages> pp. 221-235. 18 </pages>
Reference-contexts: The ADMIT-1 software and related information can be accessed online on WWW at the URL : http://www.cs.cornell.edu/home/verma/AD/research.html. 2 Computation of sparse Jacobian and Hessian Matrices In this section we review the techniques for computing sparse Jacobian and Hessian matrices. For details on this subject refer to <ref> [14, 8, 7, 3] </ref>. Sparse finite differencing techniques were first introduced by Curtis, Powell and Reid [15]; Coleman and More [8, 7, 3] and Newsam and Ramsdell [20] further developed these ideas using graph-theoretic interpretations. <p> For details on this subject refer to [14, 8, 7, 3]. Sparse finite differencing techniques were first introduced by Curtis, Powell and Reid [15]; Coleman and More <ref> [8, 7, 3] </ref> and Newsam and Ramsdell [20] further developed these ideas using graph-theoretic interpretations. <p> The algorithms that we have implemented are based on the work of Powell and Toint [21], and Coleman and More <ref> [3, 7, 4] </ref>. These authors consider direct and indirect (substitution) methods; indirect methods usually require fewer function (or gradient) evaluations while direct methods produce more accurate approximations to the Hessian matrix. For a complete review on this subject, refer to Coleman and Cai [3]. <p> These authors consider direct and indirect (substitution) methods; indirect methods usually require fewer function (or gradient) evaluations while direct methods produce more accurate approximations to the Hessian matrix. For a complete review on this subject, refer to Coleman and Cai <ref> [3] </ref>. In summary, there are basically three different ways to compute a sparse Hessian : 1. Ignoring the symmetry : This is exactly like single-sided Jacobian problem: symmetry is ignored. So H i;j and H j;i are computed independently. <p> Direct exploiting symmetry : This is the path-coloring method as described in [7]. The path coloring chromatic number is denoted by (G). 3. Substitution exploiting symmetry : This is the cyclic-coloring method as described in <ref> [3] </ref>. The cyclic coloring chromatic number is denoted by 0 (G). <p> This method requires cyclic coloring of the adjacency graph of the Hessian matrix. The algorithm involved can be found in detail in Coleman and Cai <ref> [3] </ref>. In summary, the algorithm involves finding a permutation , such that columns of L (the lower triangular part of H (; )) in the same group do not intersect in the same row position. <p> When the Hessian matrix is computed, sparsity is exploited (using graph-coloring techniques, etc. <ref> [7, 3] </ref>) Synopsis v=evalH (fun,x) v=evalH (fun,x,Extra) [v,grad]=evalH (fun,x,Extra) [v,grad,H]=evalH (fun,x,Extra,HPI) [v,grad,H]=evalH (fun,x,Extra,HPI,verb) [v,grad,H]=evalH (fun,x,Extra,HPI,verb,fdstep) Description [v,grad]=evalH (fun,x,Extra) Determine the (scalar) value and gradient (dense vector) of fun at the input argument x. The first input argument, fun, is an integer handle identifying the user function. <p> You can provide a full matrix, Extra, to be used by your target function (if required). HPI= getHPI (fun, n,Extra,method) Overrides the default coloring. method = 'i-a': The default, ignore the symmetry. Compute exactly using AD. method = 'd-a': direct method [7], using AD. method = 's-a': substitution method <ref> [3] </ref> using AD. method = 'i-f': ignore the symmetry and use finite differences (FD) method = 'd-f': direct method [7] with FD. method = 's-f': substitution method [3] with FD. HPI= getHPI (fun, n,Extra,method,SPH) You can supply SPH, a sparse MATLAB matrix representing the sparsity structure of the Hessian matrix. <p> Compute exactly using AD. method = 'd-a': direct method [7], using AD. method = 's-a': substitution method <ref> [3] </ref> using AD. method = 'i-f': ignore the symmetry and use finite differences (FD) method = 'd-f': direct method [7] with FD. method = 's-f': substitution method [3] with FD. HPI= getHPI (fun, n,Extra,method,SPH) You can supply SPH, a sparse MATLAB matrix representing the sparsity structure of the Hessian matrix.
Reference: [4] <author> T. F. Coleman, B. S. Garbow, and J. J. </author> <title> More, Software for estimating sparse Jacobian matrices, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 (1984), </volume> <pages> pp. </pages> <month> 329-345. </month> <title> [5] , Software for estimating sparse Hessian matrices, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 11 (1985), </volume> <pages> pp. 363-377. </pages>
Reference-contexts: This approach is called the one-sided column approach for computing a sparse Jacobian <ref> [8, 4, 15] </ref>. Finite differences can be used to approximate the products J d; automatic differentiation in the forward mode can be used to compute the products J d exactly. <p> The algorithms that we have implemented are based on the work of Powell and Toint [21], and Coleman and More <ref> [3, 7, 4] </ref>. These authors consider direct and indirect (substitution) methods; indirect methods usually require fewer function (or gradient) evaluations while direct methods produce more accurate approximations to the Hessian matrix. For a complete review on this subject, refer to Coleman and Cai [3]. <p> Two columns are said to "intersect" if they both have a nonzero in the same row position. The order in which the candidate columns are searched for is unspecified in the algorithm given above. Ordering based on graph coloring heuristics have proven to be effective <ref> [4] </ref>. One such ordering, smallest degree ordering is the default ordering used in ADMIT-1.
Reference: [6] <author> T. F. Coleman and G. Jonsson, </author> <title> The efficient calculation of structured gradients using automatic differentiation, </title> <journal> SIAM Journal on Scientific Computing, </journal> <note> (to appear. Also appeared as Technical Report CTC97TR272, </note> <institution> Cornell Theory Center, Cornell University, 1997.). </institution>
Reference-contexts: It is also possible to compute gradients (special case of Jacobians) of structured computations by exposing the sparsity in extended Jacobian matrix <ref> [6] </ref>. The software for structure computations is presented as a separate MATLAB toolbox, ADMIT-2 [10], which is built on top of the ADMIT-1 toolbox. This paper is outlined as follows: In x2, we review the sparsity exploiting techniques to compute the sparse Jacobian and Hessian matrices.
Reference: [7] <author> T. F. Coleman and J. J. </author> <title> More, Estimation of sparse Hessian matrices and graph coloring problems, </title> <journal> Math. Programming, </journal> <volume> 28 (1984), </volume> <pages> pp. </pages> <month> 243-270. </month> <title> [8] , Estimation of sparse Jacobian matrices and graph coloring problems, </title> <journal> SIAM J. on Numerical Analysis, </journal> <volume> 20 (1984), </volume> <pages> pp. 187-209. </pages>
Reference-contexts: The ADMIT-1 software and related information can be accessed online on WWW at the URL : http://www.cs.cornell.edu/home/verma/AD/research.html. 2 Computation of sparse Jacobian and Hessian Matrices In this section we review the techniques for computing sparse Jacobian and Hessian matrices. For details on this subject refer to <ref> [14, 8, 7, 3] </ref>. Sparse finite differencing techniques were first introduced by Curtis, Powell and Reid [15]; Coleman and More [8, 7, 3] and Newsam and Ramsdell [20] further developed these ideas using graph-theoretic interpretations. <p> For details on this subject refer to [14, 8, 7, 3]. Sparse finite differencing techniques were first introduced by Curtis, Powell and Reid [15]; Coleman and More <ref> [8, 7, 3] </ref> and Newsam and Ramsdell [20] further developed these ideas using graph-theoretic interpretations. <p> The algorithms that we have implemented are based on the work of Powell and Toint [21], and Coleman and More <ref> [3, 7, 4] </ref>. These authors consider direct and indirect (substitution) methods; indirect methods usually require fewer function (or gradient) evaluations while direct methods produce more accurate approximations to the Hessian matrix. For a complete review on this subject, refer to Coleman and Cai [3]. <p> So H i;j and H j;i are computed independently. The minimum number of groups needed to compute the Hessian via this method is indicated by (G 2 ). 2. Direct exploiting symmetry : This is the path-coloring method as described in <ref> [7] </ref>. The path coloring chromatic number is denoted by (G). 3. Substitution exploiting symmetry : This is the cyclic-coloring method as described in [3]. The cyclic coloring chromatic number is denoted by 0 (G). <p> When the Hessian matrix is computed, sparsity is exploited (using graph-coloring techniques, etc. <ref> [7, 3] </ref>) Synopsis v=evalH (fun,x) v=evalH (fun,x,Extra) [v,grad]=evalH (fun,x,Extra) [v,grad,H]=evalH (fun,x,Extra,HPI) [v,grad,H]=evalH (fun,x,Extra,HPI,verb) [v,grad,H]=evalH (fun,x,Extra,HPI,verb,fdstep) Description [v,grad]=evalH (fun,x,Extra) Determine the (scalar) value and gradient (dense vector) of fun at the input argument x. The first input argument, fun, is an integer handle identifying the user function. <p> The default coloring corresponds to direct determination. You can provide a full matrix, Extra, to be used by your target function (if required). HPI= getHPI (fun, n,Extra,method) Overrides the default coloring. method = 'i-a': The default, ignore the symmetry. Compute exactly using AD. method = 'd-a': direct method <ref> [7] </ref>, using AD. method = 's-a': substitution method [3] using AD. method = 'i-f': ignore the symmetry and use finite differences (FD) method = 'd-f': direct method [7] with FD. method = 's-f': substitution method [3] with FD. <p> Compute exactly using AD. method = 'd-a': direct method <ref> [7] </ref>, using AD. method = 's-a': substitution method [3] using AD. method = 'i-f': ignore the symmetry and use finite differences (FD) method = 'd-f': direct method [7] with FD. method = 's-f': substitution method [3] with FD. HPI= getHPI (fun, n,Extra,method,SPH) You can supply SPH, a sparse MATLAB matrix representing the sparsity structure of the Hessian matrix.

Reference: [15] <author> A. R. Curtis, M. J. D. Powell, and J. K. Reid, </author> <title> On the estimation of sparse Jacobian matrices, </title> <journal> J. Inst. Math. Appl., </journal> <volume> 13 (1974), </volume> <pages> pp. 117-119. </pages>
Reference-contexts: For details on this subject refer to [14, 8, 7, 3]. Sparse finite differencing techniques were first introduced by Curtis, Powell and Reid <ref> [15] </ref>; Coleman and More [8, 7, 3] and Newsam and Ramsdell [20] further developed these ideas using graph-theoretic interpretations. <p> This approach is called the one-sided column approach for computing a sparse Jacobian <ref> [8, 4, 15] </ref>. Finite differences can be used to approximate the products J d; automatic differentiation in the forward mode can be used to compute the products J d exactly.
Reference: [16] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and intractability, A guide to the theory of NP-completeness, W.H. </title> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: A graph-theoretic interpretation of both the direct determination problem and determination by substitution can be constructed based directly on Jacobian structure. The associated graph coloring problems are shown to be NP-complete <ref> [14, 16] </ref>; therefore, heuristic schemes are considered to construct the "bi-partition". For more insight into the problem involved and algorithmic details, refer to x6.
Reference: [17] <author> A. Griewank, </author> <title> Direct calculation of Newton steps without accumulating Jacobians, in Large-Scale Numerical Optimization, </title> <editor> T. F. Coleman and Y. Li, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1990, </year> <pages> pp. </pages> <month> 115-137. </month> <title> [18] , Some bounds on the complexity of gradients, Jacobians, and Hessians, in Complexity in Nonlinear Optimization, </title> <editor> P. Pardalos, ed., </editor> <publisher> World Scientific Publishers, </publisher> <year> 1993, </year> <pages> pp. 128-161. </pages>
Reference-contexts: Department of Energy under grant DE-FG02-97ER25013. 1 2 variables of functions defined by a high-level language computer program. For a basic review of automatic differentiation, we refer the readers to <ref> [17, 18] </ref>. Large scale nonlinear problems often exhibit structure, e.g., partial separability, composite functions, discrete time optimal control problems, and inverse problems. The derivative matrices of these structured computations are typically dense; however, it is possible to define sparse extended derivative matrices [12, 11] which can be computed using ADMIT-1. <p> Given an arbitrary n-by-t V matrix V , product J V can be directly calculated using automatic differentiation in the "forward mode"; given an arbitrary m-by-t W matrix W , the product W T J can be calculated using automatic differentiation in the "reverse mode", e.g., <ref> [17, 18] </ref>. The motivation for taking this 2-sided view comes from the following observations. The one-sided column solution based on a column partition defines a matrix V such that J can be determined from the product J V .
Reference: [19] <author> A. Griewank, D. Juedes, and J. Utke, ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++, </title> <journal> ACM Trans. On Math. Software, </journal> <volume> 22 (1996), </volume> <pages> pp. 131-167. </pages>
Reference-contexts: Note that gradient computation is a special case of these requirements, since computing the gradient is equivalent to a reverse product with W = 1, a scalar, the reverse product = rf (x) = J T . ADOL-C <ref> [19] </ref> and ADMAT [9] satisfy these requirements. 5 An example Here is simple example illustrating how to use ADMIT-1 to calculate the Jacobian of the function y = F (x); F : &lt; n ! &lt; n where y (1) = 2x (1) 2 + 1 y (i) = x (i)
Reference: [20] <author> G. N. Newsam and J. D. Ramsdell, </author> <title> Estimation of sparse jacobian matrices, </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 4 (1983), </volume> <pages> pp. 404-417. </pages>
Reference-contexts: For details on this subject refer to [14, 8, 7, 3]. Sparse finite differencing techniques were first introduced by Curtis, Powell and Reid [15]; Coleman and More [8, 7, 3] and Newsam and Ramsdell <ref> [20] </ref> further developed these ideas using graph-theoretic interpretations.
Reference: [21] <author> M. J. D. Powell and P. L. Toint, </author> <title> On the estimation of sparse Hessian matrices, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 16 (1979), </volume> <pages> pp. 1060-1074. 19 </pages>
Reference-contexts: The algorithms that we have implemented are based on the work of Powell and Toint <ref> [21] </ref>, and Coleman and More [3, 7, 4]. These authors consider direct and indirect (substitution) methods; indirect methods usually require fewer function (or gradient) evaluations while direct methods produce more accurate approximations to the Hessian matrix. For a complete review on this subject, refer to Coleman and Cai [3]. <p> heuristic M N CO can be changed to address this problem by simply changing the conditional (LB) to: if w 1 (J T R ); nnz (c)): 6.2 Algorithms for computing sparse Hessians The algorithms we have implemented for this step are based on the work of Powell and Toint <ref> [21] </ref> and Coleman and More [5]. 1. Ignoring the symmetry : Given the sparsity pattern of Hessian, SP H, subroutine ignhess (called by getHPI) determines a permutation p and a partition of the columns of H, consistent with the determination of all nonzeros H directly and independently.
References-found: 12

