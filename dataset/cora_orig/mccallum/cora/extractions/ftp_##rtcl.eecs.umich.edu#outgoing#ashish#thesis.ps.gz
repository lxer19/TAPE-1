URL: ftp://rtcl.eecs.umich.edu/outgoing/ashish/thesis.ps.gz
Refering-URL: http://www.eecs.umich.edu/~ashish/
Root-URL: http://www.cs.umich.edu
Title: STRUCTURING HOST COMMUNICATION SOFTWARE FOR QUALITY OF SERVICE GUARANTEES  
Author: by Ashish Mehra 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Science and Engineering) in The  Doctoral Committee: Professor Kang G. Shin, Chair Associate Professor Farnam Jahanian Assistant Professor Sugih Jamin Dr. Dilip Kandlur Professor Toby Teorey Assistant Professor Kimberly Wasserman  
Date: 1997  
Affiliation: University of Michigan  
Abstract-found: 0
Intro-found: 1
Reference: <institution> 220 BIBLIOGRAPHY </institution>
Reference: [1] <author> M. B. Abbott and L. L. Peterson, </author> <title> "Increasing network throughput by integrating protocol layers," </title> <journal> IEEE/ACM Trans. Networking, </journal> <volume> vol. 5, no. 1, </volume> <pages> pp. 600-610, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: The techniques employed to improve the delivered throughput and latency of communication protocols include [61] * minimization of data-copying within the communication subsystem and across the API [52, 181], * optimization of protocol implementations, such as hand-optimized critical paths [40], 27 and integrated layer processing <ref> [1, 41] </ref>, * appropriate network interface design for high performance [161], and, * exploitation of concurrency and parallelism within the communication subsystem [152]. <p> A detailed study of factors affecting end-to-end communication latency in LAN environments is described in [170]. ILP, first proposed in [41] and subsequently implemented and evaluated in <ref> [1] </ref>, reduces the number of accesses to network data by effectively collapsing protocol layers and executing them in an integration fashion for each data word accessed. Several recent efforts have also focused on optimizing the protocol processing latency in TCP/IP protocol stacks [16, 129, 176]. <p> The negative impact of data-touching overheads such as checksumming has also been studied extensively, and a number of techniques devised to improve data-copying performance <ref> [1, 97] </ref>. Similarly, much attention has been focused recently on appropriate buffer management for data copy elimination [25, 27, 130]. In contrast, our goal is to explicitly account for any copying cost incurred during data movement to/from applications, and measure this cost via appropriate profiling.
Reference: [2] <author> T. Abdelzaher, E. Atkins, and K. Shin, </author> <title> "QoS negotiation in real-time systems and its application to automated flight control," </title> <booktitle> in Proc. Real-Time Technology and Applications Symposium, </booktitle> <pages> pp. 228-238, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: However, leaving resource management entirely up to the application may not suffice for QoS-sensitive management of resources. Alternately, applications and operating systems can be designed to cooperate with one another to perform QoS negotiation and adaptation to maximize service accessibility and continuity <ref> [2, 33, 104] </ref>. A novel way to realize application-specific policies in networking is via installing of application code directly in the kernel [64]. <p> This system, which is implemented in RT-Mach [171], reserves processing capacity based on average processing usage of applications and provides notifications to applications to adjust QoS levels. QoS negotiation and adaptation support has also been developed for real-time applications <ref> [2] </ref>, which provides support for specification of QoS compromises and supports graceful QoS degradation under overload or failure conditions. The negative effects of the scheduling variability introduced by the UNIX operating system on audio playback is highlighted in [102]. <p> While we do not consider dynamic QoS negotiation and adaptation, QoS-aware APIs may also allow applications to specify a range of desired QoS levels and receive dynamic notifications regarding overload conditions <ref> [2, 104] </ref>. The communication subsystem must interface to the applications via appropriate mechanisms such as application libraries, maintain additional state regarding applications and active connections, and move data to/from applications as per the associated QoS requirements. <p> In order to realize generic components, the service model must be decoupled from the service architecture and its components. This in turn facilitates extension of the service architecture to more relaxed QoS models such as statistical guarantees, and QoS negotiation and adaptation <ref> [2] </ref>. To realize this service we adopt the real-time channel paradigm and base the service architecture on the architectural mechanisms and extensions described in Chapters 3 and 4. 5.2.2 Service Architecture subset of this architecture also applies to intermediate nodes.
Reference: [3] <author> R. Ahuja, S. Keshav, and H. Saran, </author> <title> "Design, implementation, and performance of a native mode ATM transport layer," </title> <booktitle> in Proc. IEEE INFOCOM, </booktitle> <pages> pp. 206-214, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: We have designed and implemented an RSVP-based QoS architecture supporting integrated services in TCP/IP protocol stacks, running on legacy (e.g., Token Ring and Ethernet) LAN interfaces as well as high-speed ATM networks [13], as described in Chapter 7. A native-mode ATM transport layer has been designed and implemented in <ref> [3] </ref>, and enhanced with QoS support for a user space implementation [75]. Similar to our RSVP-based QoS architecture, traffic policing and shaping is performed while copying application data into kernel buffers; the application is blocked if it is violating its traffic specification.
Reference: [4] <author> D. P. Anderson, S. Y. Tzou, R. Wahbe, R. Govindan, and M. Andrews, </author> <title> "Support for continuous media in the DASH system," </title> <booktitle> in Proc. Int'l Conf. on Distributed Computing Systems, </booktitle> <pages> pp. 54-61, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Related work in each of these areas is described below. Network and protocol support for QoS: The DASH project <ref> [4, 5] </ref> explored provision of performance guarantees for unicast sessions in IP networks. It argued that bounded processing delays and overheads within the communication subsystem can be achieved through appropriate priority-based scheduling [6]. <p> When the application requests that the channel be destroyed, all resources allocated for the channel are released by the network and the communication subsystems at the source and destination hosts. Traffic and QoS Specification: Traffic generation on real-time channels is based on a linear bounded arrival process <ref> [4, 44] </ref> characterized by three parameters: maximum message size (M max bytes), maximum message rate (R max messages/second), and maximum burst size (B max messages). In any interval of length ffi, the number of messages generated is bounded by B max +ffiR max .
Reference: [5] <author> D. P. Anderson, </author> <title> "Metascheduling for continuous media," </title> <journal> ACM Trans. Computer Systems, </journal> <volume> vol. 11, no. 3, </volume> <pages> pp. 226-252, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Related work in each of these areas is described below. Network and protocol support for QoS: The DASH project <ref> [4, 5] </ref> explored provision of performance guarantees for unicast sessions in IP networks. It argued that bounded processing delays and overheads within the communication subsystem can be achieved through appropriate priority-based scheduling [6].
Reference: [6] <author> D. P. Anderson, L. Delgrossi, and R. G. Herrtwich, </author> <title> "Structure and scheduling in real-time protocol implementations," </title> <type> Technical Report TR-90-021, </type> <institution> International Computer Science Institute, Berkeley, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: In addition, this allows protocol processing to be scheduled via thread scheduling schemes distinct from those developed for application threads, especially for multimedia applications <ref> [6, 70] </ref>. In recent years, resource management for computation has moved towards giving more control to the applications, e.g., user-level threads [113], scheduler activations [7], and application-kernel coordination to dynamically vary the degree of active parallelism in scientific computing applications [174]. <p> Network and protocol support for QoS: The DASH project [4, 5] explored provision of performance guarantees for unicast sessions in IP networks. It argued that bounded processing delays and overheads within the communication subsystem can be achieved through appropriate priority-based scheduling <ref> [6] </ref>. The Tenet real-time protocol suite [12] is one of the first implementations of real-time channels [63] on wide-area networks (WANs), and supports both deterministic and statistical service guarantees. <p> We take these costs into account when developing the admission control extensions described in Chapter 4. OS support for QoS-sensitive communication: The need for scheduling protocol processing at priority levels consistent with those of the communicating application was highlighted in <ref> [6] </ref>, and some implementation strategies demonstrated in [70]. More recently, processor capacity reserves in RT-Mach [123] have been combined with user-level protocol processing [112] for predictable protocol processing inside hosts [107].
Reference: [7] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy, </author> <title> "Scheduler activations: Effective kernel support for the user-level management of parallelism," </title> <journal> ACM Trans. Computer Systems, </journal> <volume> vol. 10, no. 1, </volume> <pages> pp. 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: In addition, this allows protocol processing to be scheduled via thread scheduling schemes distinct from those developed for application threads, especially for multimedia applications [6, 70]. In recent years, resource management for computation has moved towards giving more control to the applications, e.g., user-level threads [113], scheduler activations <ref> [7] </ref>, and application-kernel coordination to dynamically vary the degree of active parallelism in scientific computing applications [174]. Providing applications with similar control over thread management and scheduling within the communication subsystem can potentially improve performance by exploiting locality, improving resource utilization, and reducing context switching and scheduling overheads.
Reference: [8] <author> C. M. Aras, J. F. Kurose, D. S. Reeves, and H. Schulzrinne, </author> <title> "Real-time communication in packet-switched networks," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 82, no. 1, </volume> <pages> pp. 122-139, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: A QoS-sensitive operating system, for example, would ensure that applications exchanging QoS-sensitive data are scheduled for execution so as to satisfy the desired end-to-end QoS guarantees on communication. There have been numerous proposals for QoS-sensitive provisioning of network resources such as transmission bandwidth and buffers <ref> [8, 188] </ref>, and host computation 1 resources such as CPU cycles [71, 165, 179]. However, not much attention has been paid to the interface between the applications and the network, i.e., the communication subsystem at end hosts, for provision of QoS guarantees. <p> In general, provision of QoS guarantees on end-to-end communication requires appropriate support in the network routers and end hosts to prevent uncontrolled resource contention and maintain per-connection QoS guarantees <ref> [8] </ref>. The application provides a per-connection QoS specification, which defines its QoS requirements for each QoS parameter, and also provides a traffic specification, which defines its traffic generation characteristics on that connection against which the QoS guarantees are to be given. <p> While policing ascertains whether the traffic is conformant or not, shaping buffers non-conformant traffic until conformance as per the traffic specification. Each node also implements a QoS-sensitive queueing policy which determines the nature of queueing at the node, and a service discipline <ref> [8, 188] </ref> which determines the order in which packets are scheduled for service, e.g., processing and transmission to the next node along the chosen route. Example service disciplines include Weighted Fair Queueing [50], also known as Packet-by-Packet Generalized Processor Sharing [144], and Rate-Controlled Static-Priority Queueing [187]. <p> These extensions are then refined for hosts simultaneously engaged in transmission as well as reception of QoS-sensitive traffic. The issues of simultaneous management of CPU and link bandwidth for real-time communication are of wide-ranging interest. The above-mentioned extensions are applicable to other proposals for real-time communication and QoS guarantees <ref> [8, 188] </ref>, and to other host and network architectures. In particular, Internet servers running TCP/IP protocol stacks supporting integrated services, especially the guaranteed service class [159], can also benefit from these extensions. <p> Strict deterministic QoS guarantees on bandwidth, delay, delay jitter, and packet loss may be necessary for hard real-time applications and multimedia applications such as medical imaging. This necessitates new policies and approaches for network design <ref> [8] </ref> in which messages or packets have deadlines associated with them. A message arriving at its destination after its deadline has expired is practically useless to the application; such messages are considered lost and are discarded. <p> By providing additional buffers and adaptively controlling the rate at which packets are consumed at the receiver, these 18 applications can adapt to existing network conditions <ref> [8, 39, 145] </ref>. The destination buffers serve to restore the spacing between successive data samples and buffer out variations in network delay. A study on packet video transport [91] illustrated the effectiveness of using network feedback about load conditions to modulate the frame transmission rate of video sources. <p> Our primary focus is on the architectural mechanisms for run-time traffic management within the communication subsystem to satisfy the QoS requirements of all connections, without undue degradation in the performance of best-effort traffic. While the proposed architecture is applicable to other proposals for guaranteed-QoS connections <ref> [8, 188] </ref>, we focus on real-time channels, a paradigm for guaranteed-QoS communication services in packet-switched networks [63, 92]. Consider the problem of servicing several guaranteed-QoS and best-effort connections engaged in network input/output at a host. <p> Host communication resources include CPU bandwidth for protocol processing, link bandwidth for packet transmissions and reception, and buffer space. A QoS-sensitive communication subsystem must provide services for the two related aspects of guaranteed-QoS communication <ref> [8] </ref>, namely, traffic specification and resource management. Figure 3.1 illustrates a generic software architecture for guaranteed-QoS communication services at the host. The components constituting this architecture are briefly discussed below. <p> Best-effort traffic should not be unduly penalized by non-conformant real-time traffic, especially under work-conserving processing. 3.2.2 Real-Time Channels: A Model for Guaranteed-QoS Communica tion Several models have been proposed for guaranteed-QoS communication in packet-switched networks <ref> [8] </ref>. While the architectural mechanisms proposed in this paper are applicable to most of the proposed models, we focus on real-time channels [63, 92], using the model proposed and analyzed in [92]. <p> If d is the desired end-to-end delay bound for a channel, message m i generated at the source is guaranteed to be delivered at the sink by time `(m i ) + d. See [92] for more details. Resource Management: As with other proposals for guaranteed-QoS communication <ref> [8] </ref>, there are two related aspects to resource management for real-time channels: admission control and (run-time) scheduling. Admission control for real-time channels is provided by Algorithm D order [92], which uses fixed-priority scheduling for computing the worst-case delay experienced by a channel at a link. <p> The issues of simultaneous management of CPU and link bandwidth for real-time communication are of wide-ranging interest. Our present work is applicable to other proposals for real-time communication and QoS guarantees <ref> [8, 188] </ref>. Further, the proposed admission control extensions are general and applicable to other host and network architectures. In particular, Internet servers running TCP/IP protocol stacks supporting Integrated Services [22, 65], especially the guaranteed service class [159], can also benefit from these extensions. <p> The analysis and extensions developed in this chapter are applicable to other proposals for guaranteed real-time communication in packet-switched networks <ref> [8, 188] </ref>. Our main conclusions can be summarized as follows. In order to best utilize CPU and link bandwidth, one must account for implementation overheads that reduce useful resource capacity. Further, one must also consider the implications of the implementation paradigm adopted to manage CPU and link bandwidth.
Reference: [9] <author> ARMADA Homepage. </author> <note> http://www.eecs.umich.edu/RTCL/armada/. </note>
Reference-contexts: These results also reveal several deficiencies of a server-based implementation especially at the receiving host, that could be largely resolved by collocating the server in the kernel. The guaranteed-QoS service described in this chapter is being utilized in the ARMADA project <ref> [9] </ref>, a collaborative effort between the Real-Time Computing Laboratory at the University of Michigan and the Honeywell Technology Center. The project aims to develop 155 and demonstrate an integrated set of techniques and software tools necessary to realize embedded fault-tolerant and real-time applications on distributed, evolving computing platforms.
Reference: [10] <author> E. A. Arnould, F. J. Bitz, E. C. Cooper, H. T. Kung, R. D. Sansom, and P. A. Steenkiste, </author> <title> "The design of Nectar: A network backplane for heterogeneous multicom-puters," </title> <booktitle> in Proc. Int'l Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 205-216, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: More importantly, it permits management of the communication subsystem by a separate executive designed for communication-related operations, such as the x-kernel [76]. The VMP NAB [90] and Nectar CAB [120] are two examples of this functional partitioning. For example, the Nectar CAB <ref> [10, 120] </ref> off-loads all protocol processing functions from the host processor, freeing it from adapter handshake overheads and permitting greater overlap between useful computation and communication processing. 31 Multiprocessor front-ends: Multiprocessor front-ends may be designed using special--purpose or general-purpose hardware.
Reference: [11] <author> M. L. Bailey, B. Gopal, M. A. Pagels, L. L. Peterson, and P. Sarkar, "PATHFINDER: </author> <title> A pattern-based packet classifier," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 115-123, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Successive implementations of packet filters have reduced classification overhead significantly, e.g., BPF [114], MPF [186], and PATHFINDER <ref> [11] </ref>. While all of these filter designs perform classification via interpretation, more recently dynamic code generation techniques have been applied to realize very efficient packet filters, as in DPF [59]. Packet filters are necessary for packet classification in connectionless networks, such as the IPv4-based Internet. <p> Packet filters are necessary for packet classification in connectionless networks, such as the IPv4-based Internet. Further, they may also be needed in native ATM networks that aggregate multiple end-to-end connections over virtual circuits <ref> [11] </ref>. Receive livelock elimination: Besides optimizing the receive data path through the communication subsystem, efforts have also been made to address another key problem associated with data reception, namely, receive livelock [151].
Reference: [12] <author> A. Banerjea, D. Ferrari, B. Mah, M. Moran, D. C. Verma, and H. Zhang, </author> <title> "The Tenet real-time protocol suite: Design, implementation, and experiences," </title> <journal> IEEE/ACM Trans. Networking, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 1-11, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Network and protocol support for QoS: The DASH project [4, 5] explored provision of performance guarantees for unicast sessions in IP networks. It argued that bounded processing delays and overheads within the communication subsystem can be achieved through appropriate priority-based scheduling [6]. The Tenet real-time protocol suite <ref> [12] </ref> is one of the first implementations of real-time channels [63] on wide-area networks (WANs), and supports both deterministic and statistical service guarantees. In addition to providing protocol support for end-to-end signalling, support is provided for QoS-sensitive packet scheduling at hosts and routers via Rate-Controlled Static-Priority Queueing [187].
Reference: [13] <author> T. Barzilai, D. Kandlur, A. Mehra, D. Saha, and S. Wise, </author> <title> "Design and implementation of an RSVP-based quality of service architecture for integrated services Internet," </title> <booktitle> in Proc. Int'l Conf. on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: J. Watson Research Center, we have developed an RSVP-based QoS architecture for TCP/IP protocol stacks supporting an integrated services Internet <ref> [13] </ref>. This architecture represents a major functional enhancement to the traditional sockets-based communication subsystem [108]. <p> We have designed and implemented an RSVP-based QoS architecture supporting integrated services in TCP/IP protocol stacks, running on legacy (e.g., Token Ring and Ethernet) LAN interfaces as well as high-speed ATM networks <ref> [13] </ref>, as described in Chapter 7. A native-mode ATM transport layer has been designed and implemented in [3], and enhanced with QoS support for a user space implementation [75]. <p> We note that extensions to the sockets API have been proposed to support the requirements of QoS-sensitive traffic, most of these realized via new socket options. The architecture described in Chapter 7 adopts a unique control socket approach to realize a QoS-aware sockets layer <ref> [13] </ref>. Signalling Routines rtcRegisterPort, rtcUnregisterPort: An application indicates its intent to receive signalling requests for real-time channels by registering a signalling port with RTCOP via rtcRegisterPort. The application also specifies an agent function that the service must invoke in response to incoming signalling requests. <p> Note that a clip is similar to traditional Unix socket that has been extended with the necessary reservation states, as in <ref> [13] </ref>. However, compared to a socket a clip has more state and communication resources associated with it, for both incoming and outgoing traffic. <p> In collaboration with researchers at the IBM T. J. Watson Research Center, we have designed and implemented architectural extensions to the sockets-based communication subsystem that enable RSVP-based integrated services infrastructure in the Internet <ref> [13] </ref>. One of the primary goals of our service architecture is to blend the QoS support with the existing TCP/IP stack and socket API, preserving the structure of the Unix networking subsystem. <p> Additional details on the internals of the architecture can be found in <ref> [13] </ref>. Figure 7.3 shows the software architecture of an rsvp enabled host. In this example, a number of applications are using rsvp signaling for resource reservation. The applications use an rsvp api (rapi) library to communicate with the rsvp daemon running on the host. <p> If the attached nic does not support QoS functions, as in legacy Ethernet and Token Ring networks, ndd extensions are required to support per-connection QoS via packet queueing and scheduling <ref> [13] </ref>. path through QOSMGR. For a packet associated with a reservation, the socket layer obtains a buffer by calling qos alloc (), which transfers control to the QOSMGR. It first checks whether the application has enabled policing for that reservation (by setting the police flag) (step 1). <p> Coupled with an appropriate buffer management policy, the variations in the scheduling delay can also be accommodated. As mentioned in Section 3.3, for legacy nics with appropriate queueing and scheduling supported by the ndd, datalink-level shaping serves as an alternative to session-level shaping <ref> [13] </ref>. With datalink-level scheduling, the packet scheduler operates in a non work-conserving fashion, injecting compliant packets into the network either in the context of an executing application thread, or on receipt of a transmission-complete interrupt notification for the previous packet transmission.
Reference: [14] <author> M. Bjorkman and P. Gunningberg, </author> <title> "Locking effects in multiprocessor implementations of protocols," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 74-83, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Special-purpose designs facilitate efficient interaction with the host and the network interface unit [82, 83], while general-purpose designs must explicitly coordinate accesses to these interfaces <ref> [14, 138, 182] </ref>. More processing power in the front-end can improve the quality of service provided to the applications, by reducing queuing delays within the communication subsystem. Network interfaces for multicomputers: Network interface design for multicomputer environments presents unique opportunities and cost-performance tradeoffs. <p> In horizontal architectures [182], each process implements a specific layer of a protocol graph; at most two processes can be assigned to each layer, one each for transmission and reception. In vertical process architectures <ref> [14, 76, 83] </ref>, on the other hand, processes are assigned to active entities such as connections or messages and each process implements one path through the protocol graph. This approach significantly reduces context switches and message buffering that are unavoidable in horizontal process architectures. <p> Various combinations of these approaches are also possible. Fine-grain techniques provide more scalable parallelism and hence potentially greater performance gains. In practice, though, synchronization constraints, resource contention, and load balancing requirements limit the speedups, and hence the message throughputs, observed. Overheads of managing parallelism: The techniques used in <ref> [14] </ref> to parallelize x-kernel [76, 141] and the protocols, including the locking mechanisms used and dedication of special x-kernel functions to specific processors, suggest that special synchronization paradigms and processor allocation mechanisms are needed to manage communication parallelism effectively. <p> Synchronization and contention overheads may seriously limit the amount of parallelism that can be exploited effectively in practice. As shown in <ref> [14] </ref>, parallel implementations of contemporary transport layer protocols like TCP are synchronization-limited because these protocols retain a significant amount of connection state that must be maintained consistently. In comparison, the results obtained for UDP show higher speedups since UDP maintains significantly less state than TCP. <p> Our approach of statically partitioning the processing resources is similar to multiprocessor front-ends [138], except that a set of processors within the host are dedicated for protocol processing and communication-related functions, as in <ref> [14] </ref>. Accordingly, the architecture described in Chapter 3 is based on the process-per-connection model of protocol processing. 2.5 QoS-Sensitive Communication and Computation The preceding section focused on techniques that improve the average performance of communication subsystems, and hence the application-level throughput and latency realized. <p> We note that at least two other efforts have employed such artificial sources and sinks of data, namely, 68 the virtual network device in <ref> [14] </ref> that resides on a separate processor, and the "in-memory" network device used in [134]. 3.5 Experimental Evaluation We evaluate the efficacy of the proposed architecture in isolating real-time channels from each other and from best-effort traffic.
Reference: [15] <author> D. L. Black, R. D. Smith, S. J. Sears, and R. W. Dean, "FLIPC: </author> <title> A low latency messaging system for distributed real-time environments," </title> <booktitle> in Proc. USENIX Winter Conference, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: More importantly, the architecture outlined in [185] does not consider provision of signaling and resource management services within the communication subsystem. Communication support for efficient messaging in distributed real-time environments is provided in FLIPC <ref> [15] </ref>. FLIPC exploits programmable communication controllers to realize low latency message transfer. Similar to our approach, arriving messages are not processed in interrupt context, but processed by a thread that is in turn scheduled for execution. However, FLIPC does not provide any explicit QoS guarantees on the end-to-end message delivery.
Reference: [16] <author> T. Blackwell, </author> <title> "Speeding up protocols for small messages," </title> <booktitle> in Proc. of ACM SIG-COMM, </booktitle> <pages> pp. 85-95, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: While data copying primarily impacts data transfer throughput, protocol processing latency is significantly affected by the complex, protocol-specific cache behavior of network protocols <ref> [16, 133] </ref>. Finally, protocol stack execution performance is significantly affected by operating system overheads such as context switching and device interrupts [163]. <p> Several recent efforts have also focused on optimizing the protocol processing latency in TCP/IP protocol stacks <ref> [16, 129, 176] </ref>. User-level protocol processing: Several research efforts have focused on increasing communication subsystem throughput via user-level handling of network data [57, 112, 167]. In 28 addition to data copy minimization compared to a server-based implementation, user-level protocol processing offers significant flexibility in developing and debugging communication protocols. <p> More refined experiments are necessary to select accurate values for C p , C sw and C cm . It has been shown that the cache behavior of network protocols is protocol-specific and that cache misses play a significant role in protocol stack execution latency <ref> [16, 133] </ref>. For partitioned caches, cache behavior can be made more predictable under control of the operating system [110]. <p> Recall that the output communication handler fragments packets and shepherds them down the protocol stack in a single loop. The difference in overhead between the first and other packets can be partly attributed to cache effects, which have been shown to affect protocol stack execution latency significantly <ref> [16, 133] </ref>. For the receive path, on the other hand, we distinguish between the last packet of a message and the other packets. <p> Similarly, the importance of cache performance for small messages such as those 163 found in typical signalling protocols is highlighted in <ref> [16] </ref>. This has significant implications for system parameterization since it highlights the difficulty in measuring various processing overheads accurately. Cache predictability may be improved via appropriate protocol implementation and compilation techniques [129], or via cache partitioning and appropriate OS support [110]. <p> The scalability of the QoS architecture is, therefore, contingent in part upon the accuracy with which an application specifies its run-time communication behavior. Protocol stack performance and optimizations of existing implementations has been the 184 subject of numerous research articles, including some very recent ones <ref> [16, 129, 176] </ref>. How--ever, all of these studies focus on the traditional best-effort data path. Our study assumes significance in that it quantifies the performance penalty imposed by new data-handling components in the protocol stack, and their impact on the best-effort data path.
Reference: [17] <author> G. Blair, A. Campbell, G. Coulson, F. Garcia, D. Hutchison, A. Scott, and D. Shepherd, </author> <title> "A network interface unit to support continuous media," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 264-275, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Demultiplex-ing incoming packets early and absorbing bursts in distinct per-connection queues is an attractive way to prevent receive livelock, an observation also made in the context of paths in Scout [128]. Our architectural approach facilitates provision of QoS guarantees while preventing receive livelock. Network adapter design: The MNI <ref> [17] </ref> is a network interface unit designed specifically for continuous media applications. It supports functions for full end-to-end multimedia communication in addition to protocol processing, and is capable of moving data directly between I/O devices and the network via device-to-device communication.
Reference: [18] <author> M. A. Blumrich, C. Dubnicki, E. W. Felten, and K. Li, </author> <title> "Protected, user-level DMA for the SHRIMP network interface," </title> <booktitle> in Proc. International Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: For example, the Hamlyn [30] network interface performs sender-controlled data transfer, in which the sender controls the memory location at the receiver where data will be deposited. Similarly, the network interface for the SHRIMP multicomputer <ref> [18] </ref> realizes direct writes to remote memory via virtual memory mapped communication [56]. This requires specialized network interface hardware that maintains virtual to physical addressing mappings. Implications for QoS-sensitive transmission/reception Neither of these approaches are feasible for QoS-sensitive communication between workstation-type clients and servers across WANs.
Reference: [19] <author> G. Bollella and K. Jeffay, </author> <title> "Supporting co-resident operating systems," </title> <booktitle> in Proc. Real-Time Technology and Applications Symposium, </booktitle> <pages> pp. 4-14, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The functionality provided by our CORDS-based server cannot be truly effective without additional support from the underlying operating system. Such support could be in the form of capacity reserves for the service [123] or appropriate system partitioning <ref> [19] </ref>, and would require proper integration of the QoS-sensitive communication subsystem with the operating system for provision of application-level QoS guarantees. <p> A similar approach that partitions the CPU to support co-resident general-purpose and real-time operating systems has been proposed and analyzed in <ref> [19] </ref>. In this approach, a certain capacity is set aside for execution of the real-time operating system and the remaining capacity is allocated to the general-purpose operating system. Schedulability tests for real-time tasks are suitably modified to consider only the available real-time capacity.
Reference: [20] <author> M. Borden, E. Crawley, B. Davie, and S. Batsell, </author> <title> "Integration of real-time services in an IP-ATM network architecture," Request for Comments RFC 1821, </title> <month> August </month> <year> 1995. </year> <title> Bay Networks, Bellcore, </title> <publisher> NRL. </publisher>
Reference-contexts: communication is also evidenced by the rapid proliferation of on-line multimedia content on the WWW, the shortcomings of the "best-effort" service provided by today's Internet, and the efforts by the Internet Engineering Task Force (IETF) to address these shortcomings by developing protocols and standards for Integrated Services on the Internet <ref> [20, 22, 65] </ref>. To support such applications, all hardware and software components involved in transferring application data from one host to another across the network must be designed to be QoS-sensitive, i.e., to provide QoS guarantees. <p> Towards that end, the IETF is developing a set of protocols and standards for integrated services <ref> [20, 65, 149, 189] </ref>. In the IETF's vision, applications request and reserve resources, both in the network and at the attached hosts (clients or servers) using an end-to-end receiver-initiated Resource ReSerVation Protocol (RSVP) [23, 189]. <p> In contrast to RSVP, which initiates reservation setup at the receiver, an alternative approach 6 to signalling is adopted by the ST-II protocol [49], which initiates reservation setup at the sender. The issues involved in providing QoS support in IP-over-ATM networks [149] are also being explored <ref> [20] </ref>. 1.2 Dissertation Focus and Problem Statement As mentioned, the primary focus of this dissertation is host communication subsystem design to request and obtain per-connection QoS, assuming availability of appropriate network support.
Reference: [21] <author> A. Braccini, A. D. Bimbo, and E. Vicario, </author> <title> "Interprocess communication dependency on network load," </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> vol. 17, no. 4, </volume> <pages> pp. 357-369, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The delivered end-to-end delay and throughput for TCP and UDP traffic is measured against background load for a range of message sizes. The effect of background Ethernet load on TCP and UDP performance is studied in <ref> [21] </ref>. More recently, the effect of background load on the performance of SunOS inter-process communication (IPC) mechanisms and TCP/IP protocol stack is studied in [143], for an artificial CPU-intensive workload as well as a real distributed application.
Reference: [22] <author> R. Braden, D. Clark, and S. Shenker, </author> <title> "Integrated services in the Internet architecture: An overview," Request for Comments RFC 1633, </title> <month> July </month> <year> 1994. </year> <note> Xerox PARC. </note>
Reference-contexts: communication is also evidenced by the rapid proliferation of on-line multimedia content on the WWW, the shortcomings of the "best-effort" service provided by today's Internet, and the efforts by the Internet Engineering Task Force (IETF) to address these shortcomings by developing protocols and standards for Integrated Services on the Internet <ref> [20, 22, 65] </ref>. To support such applications, all hardware and software components involved in transferring application data from one host to another across the network must be designed to be QoS-sensitive, i.e., to provide QoS guarantees. <p> Proposals for predictive (or best-effort) real-time communication, such as FIFO+ [39] and Hop-Laxity [156], fall in this category. 1.1.2 Integrated Services on the Internet Significant efforts are being made by the IETF to enhance the service model of the Internet to support integrated services for voice, video, and data transport <ref> [22, 39] </ref>. <p> the general scenario described above, to support integrated services on the Internet, the network routers and end hosts need to be enhanced to perform traffic classification on a per-flow basis, create and maintain flow-specific soft reservation states, and handle data packets from different flows in accordance with their QoS requirements <ref> [22] </ref>. Towards that end, the IETF is developing a set of protocols and standards for integrated services [20, 65, 149, 189]. <p> In the IETF's vision, applications request and reserve resources, both in the network and at the attached hosts (clients or servers) using an end-to-end receiver-initiated Resource ReSerVation Protocol (RSVP) [23, 189]. Resource management is performed via per-flow traffic shaping and scheduling for various classes of service <ref> [22] </ref>, such as guaranteed service [159] that provides guaranteed delay, and controlled load service [183] that has more relaxed QoS requirements. The guaranteed service is intended for applications requiring firm guarantees on loss-less on-time datagram delivery. <p> Our present work is applicable to other proposals for real-time communication and QoS guarantees [8, 188]. Further, the proposed admission control extensions are general and applicable to other host and network architectures. In particular, Internet servers running TCP/IP protocol stacks supporting Integrated Services <ref> [22, 65] </ref>, especially the guaranteed service class [159], can also benefit from these extensions. Similarly, Internet routers can apply these extensions when incoming packets must 78 be fragmented before forwarding in order to reconcile the different MTUs of the attached networks. <p> Significant efforts are being made by the Internet Engineering Task Force (IETF) to enhance the service model of the Internet to support integrated services for voice, video, and data transport <ref> [22] </ref>. This in turn implies that the existing communication subsystems running TCP/IP protocol stacks in Internet hosts be modified for QoS-sensitive traffic handling according to integrated services standards.
Reference: [23] <author> R. Braden, L. Zhang, S. Berson, S. Herzog, and S. Jamin, </author> <title> "Resource ReSerVation Protocol (RSVP) version 1 functional specification," Internet Draft draft-ietf-rsvp-spec-12.txt, </title> <month> May </month> <year> 1996. </year> <month> ISI/PARC/USC. </month>
Reference-contexts: Towards that end, the IETF is developing a set of protocols and standards for integrated services [20, 65, 149, 189]. In the IETF's vision, applications request and reserve resources, both in the network and at the attached hosts (clients or servers) using an end-to-end receiver-initiated Resource ReSerVation Protocol (RSVP) <ref> [23, 189] </ref>. Resource management is performed via per-flow traffic shaping and scheduling for various classes of service [22], such as guaranteed service [159] that provides guaranteed delay, and controlled load service [183] that has more relaxed QoS requirements. <p> Finally, we conclude with a summary of the main contributions of the chapter. 7.2 RSVP and Integrated Services: An Overview Below we present a brief overview of the RSVP protocol and the service classes under discussion in the IETF. A complete description of RSVP is provided in <ref> [23] </ref>, while details on different service classes can be found in [159, 183]. 7.2.1 RSVP: An End-to-End View run RSVP daemons that exchange RSVP messages (PATH and RESV) on behalf of their hosts. <p> Hence, a reservation can be established before or any time after the data flow starts. Additional details of refreshing PATH and RESV states and handling route changes are provided in <ref> [23] </ref>. An RSVP flow is uniquely identified by the five-tuple &lt;protocol,src address,src port,dst address,dst port&gt;. Filters are set up at routers and hosts to classify packets belonging to an RSVP flow and treat them in accordance with the reservation made on the flow.
Reference: [24] <author> A. Brown and M. Seltzer, </author> <title> "Operating system benchmarking in the wake of lmbench: A case study of the performance of NetBSD on the Intel x86 architecture," </title> <booktitle> in Proc. of ACM SIGMETRICS, </booktitle> <pages> pp. 214-224, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: The first such study focused on the reasons behind the failure of operating system performance improvements to track performance improvements in hardware technology [142]. More recently, efforts have focused on developing a suite of portable operating system benchmarks for cross-platform performance comparisons [115] as well as detailed system analysis <ref> [24] </ref>. In all these efforts, however, the performance profiling is geared towards performance comparisons and the impact of OS-hardware interactions on operating system primitives. <p> For example, for our platform timestamps using the real-time clock cost 15 s. While this overhead is relatively high, it can be reduced drastically by using hardware performance counters provided in several modern processors <ref> [24] </ref>. Hardware cycle counters provide timestamps with resolutions of the order of just a few nanoseconds. The kernel-level profiling described in Chapter 7 is performed using such hardware counters. For platforms with significant timestamp cost, the generated profile sample must be adjusted appropriately. <p> The processing performed on the collected samples may include calculation of simple averages and additional statistics such as the standard deviation. Often it is more accurate to discard some of the highest and the lowest values before computing parameter statistics <ref> [24] </ref>. This is true for us since the unpredictability of the underlying operating system may introduce occasional preemption between the prologue and epilogue processing in each module.
Reference: [25] <author> J. C. Brustoloni and P. Steenkiste, </author> <title> "Effects of buffering semantics on I/O performance," </title> <booktitle> in Proc. USENIX Symp. on Operating Systems Design and Implementation, </booktitle> <pages> pp. 277-291, </pages> <month> October </month> <year> 1996. </year> <month> 222 </month>
Reference-contexts: Contention can occur for resources such as processing capacity, network buffers, and network links, and may occur inside the sending as well receiving hosts. We discuss each of the above-mentioned factors in detail next. 2.3.1 API Semantics The location and data passing semantics <ref> [25] </ref> of the API affects communication performance in a variety of ways. Typically the API is implemented by the kernel (e.g. BSD Sockets [108]), or by a trusted server at user level, and the overhead of crossing protection domains (e.g., the user-kernel boundary) during data transfer degrades communication throughput. <p> Support for in-kernel device-to-device data transfers for multimedia applications is described in [60, 93]. An extensive taxonomy of the software and hardware tradeoffs involved in data passing and performance comparison of different buffering semantics is provided in <ref> [25] </ref>, and two copy avoidance techniques evaluated in [27]. Network adapter support for checksummed, multiple-packet communication in an ATM network is explored in [26]. Protocol stack optimization: A number of research efforts have focused on the optimization of protocol stack execution latency. <p> We note that while the RTC API ANCHOR overheads are relatively high, these measurements are for an unoptimized implementation and can be improved substantially with careful performance optimizations. With appropriate buffer management and API buffering semantics <ref> [25, 130] </ref> it may even be possible to completely eliminate the copying of data within RTC API ANCHOR. However, more immediately we are concerned with ensuring that the overheads incurred in RTC API ANCHOR do not result in QoS-insensitive handling of data. <p> The negative impact of data-touching overheads such as checksumming has also been studied extensively, and a number of techniques devised to improve data-copying performance [1, 97]. Similarly, much attention has been focused recently on appropriate buffer management for data copy elimination <ref> [25, 27, 130] </ref>. In contrast, our goal is to explicitly account for any copying cost incurred during data movement to/from applications, and measure this cost via appropriate profiling.
Reference: [26] <author> J. C. Brustoloni and P. Steenkiste, </author> <title> "Copy emulation in checksummed, multiple-packet communication," </title> <booktitle> in Proc. IEEE INFOCOM, </booktitle> <month> November </month> <year> 1997. </year>
Reference-contexts: An extensive taxonomy of the software and hardware tradeoffs involved in data passing and performance comparison of different buffering semantics is provided in [25], and two copy avoidance techniques evaluated in [27]. Network adapter support for checksummed, multiple-packet communication in an ATM network is explored in <ref> [26] </ref>. Protocol stack optimization: A number of research efforts have focused on the optimization of protocol stack execution latency.
Reference: [27] <author> J. C. Brustoloni and P. Steenkiste, </author> <title> "Evaluation of data passing and scheduling avoidance," </title> <booktitle> in Proc. Intl. Workshop on Network and Operating System Support for Digital Audio and Video, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: Support for in-kernel device-to-device data transfers for multimedia applications is described in [60, 93]. An extensive taxonomy of the software and hardware tradeoffs involved in data passing and performance comparison of different buffering semantics is provided in [25], and two copy avoidance techniques evaluated in <ref> [27] </ref>. Network adapter support for checksummed, multiple-packet communication in an ATM network is explored in [26]. Protocol stack optimization: A number of research efforts have focused on the optimization of protocol stack execution latency. <p> The negative impact of data-touching overheads such as checksumming has also been studied extensively, and a number of techniques devised to improve data-copying performance [1, 97]. Similarly, much attention has been focused recently on appropriate buffer management for data copy elimination <ref> [25, 27, 130] </ref>. In contrast, our goal is to explicitly account for any copying cost incurred during data movement to/from applications, and measure this cost via appropriate profiling.
Reference: [28] <author> V. Buch, T. von Eicken, A. Basu, and W. Vogels, "U-Net: </author> <title> A user-level network interface for parallel and distributed computing," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <pages> pp. 40-53, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: With appropriate support in the network interface and the operating system, it is possible to grant applications full access to the network interface, thereby bypassing the operating system completely <ref> [28, 55] </ref>. Application-specific networking: Recently there has been much interest in OS extension technologies, in which trusted application code is installed for execution within the OS. This allows operating systems to be customized or "extended" to implement application-specific policies.
Reference: [29] <author> A. Burns, K. Tindell, and A. Wellings, </author> <title> "Effective analysis for engineering real-time fixed priority schedulers," </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> vol. 21, no. 5, </volume> <pages> pp. 475-480, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Moreover, the issues and tradeoffs involved in realizing QoS-sensitive communication subsystems are not considered. Modeling of multimedia/real-time operating systems: A number of recent efforts have attempted to bridge the gap between theory and practice for real-time systems and multimedia computing <ref> [29, 69, 95, 98] </ref>. The admission control extensions developed in Chapter 4 are geared towards the real-time communication needs of distributed systems, and hence complement these efforts.
Reference: [30] <author> G. Buzzard, D. Jacobson, M. Mackey, S. Marovich, and J. Wilkes, </author> <title> "An implementation of the Hamlyn sender-managed interface architecture," </title> <booktitle> in Proc. USENIX Symp. on Operating Systems Design and Implementation, </booktitle> <pages> pp. 245-259, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Network interfaces for multicomputers: Network interface design for multicomputer environments presents unique opportunities and cost-performance tradeoffs. In dedicated multicomputers, network interface designs are typically tightly coupled with the processor, and as such can exploit attributes of such a tight coupling. For example, the Hamlyn <ref> [30] </ref> network interface performs sender-controlled data transfer, in which the sender controls the memory location at the receiver where data will be deposited. Similarly, the network interface for the SHRIMP multicomputer [18] realizes direct writes to remote memory via virtual memory mapped communication [56].
Reference: [31] <author> L. F. Cabrera, E. Hunter, M. J. Karels, and D. A. </author> <title> Mosher, "User-process communication performance in networks of computers," </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> vol. 14, no. 1, </volume> <pages> pp. 38-53, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Experimental and analytical studies Several studies have attempted to study the effects of multiprogramming on communication performance experimentally as well as analytically. An in depth study of the effect of background load on communication performance for the 4.2 BSD TCP/IP protocol stack is reported in <ref> [31] </ref>. The delivered end-to-end delay and throughput for TCP and UDP traffic is measured against background load for a range of message sizes. The effect of background Ethernet load on TCP and UDP performance is studied in [21].
Reference: [32] <author> A. T. Campbell, C. Aurrecoechea, and L. Hauw, </author> <title> "A review of QoS architectures," </title> <journal> Multimedia Systems Journal, </journal> <year> 1996. </year>
Reference-contexts: An extensive survey of QoS architectures is provided in <ref> [32] </ref>, which provides a comprehensive view of the state of the art in the provisioning of end-to-end QoS. In the discussion below, we highlight a subset of these approaches, focusing on enhancements and the associated implications for end hosts.
Reference: [33] <author> A. T. Campbell and G. Coulson, </author> <title> "QoS adaptive transports: Delivering scalable media to the desktop," </title> <journal> IEEE Network Magazine, </journal> <pages> pp. 18-27, </pages> <month> March/April </month> <year> 1997. </year>
Reference-contexts: However, leaving resource management entirely up to the application may not suffice for QoS-sensitive management of resources. Alternately, applications and operating systems can be designed to cooperate with one another to perform QoS negotiation and adaptation to maximize service accessibility and continuity <ref> [2, 33, 104] </ref>. A novel way to realize application-specific policies in networking is via installing of application code directly in the kernel [64]. <p> The AQUA system [104] is one such effort which has developed QoS negotiation and adaptation support for allocation of CPU and network resources. Similarly, a QoS adaptive transport system is described in <ref> [33] </ref> that incorporates a QoS-aware API and mechanisms to assist applications to adapt to fluctuations in the delivered network QoS. A scheme for adaptive rate-controlled scheduling is presented in [184]. A dynamic QoS control scheme using optimistic processor reservation and application feedback is described in [135].
Reference: [34] <author> A. T. Campbell, G. Coulson, and D. Hutchison, </author> <title> "A quality of service architecture," </title> <journal> Computer Communication Review, </journal> <month> April </month> <year> 1994. </year>
Reference-contexts: The primary focus of OMEGA is development of an integrated framework for the specification and translation of application QoS requirements and allocation of the necessary resources. QoS-A <ref> [34] </ref> is a layered architecture focusing on provision of QoS within the communication subsystem and the network. It provides features such as end-to-end admission control, resource reservation, QoS translation between layers, and QoS monitoring and maintenance. QoS-A specifies a functionally rich and general architecture supporting networked multimedia applications.
Reference: [35] <author> C. E. Catlett, </author> <title> "In Search of Gigabit Applications," </title> <journal> IEEE Communication Magazine, </journal> <pages> pp. 42-51, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Networked multimedia applications integrating various media such as audio, video, and text, exhibit a wide range of QoS requirements on communication [62]. Examples of such applications include high-definition television, medical imaging, scientific visualization, full-motion video, multiparty video conferencing, and collaborative workspace <ref> [35, 145, 148] </ref>. The QoS requirements of multimedia applications differ significantly from those of tra 17 ditional data transfer applications such as remote login, electronic mail, and file transfer.
Reference: [36] <author> D. R. Cheriton and C. L. Williamson, </author> <title> "VMTP as the transport layer for high-performance distributed systems," </title> <journal> IEEE Communication Magazine, </journal> <pages> pp. 37-44, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: This model has been used in a tool to compare different protocols and protocol implementations for varying levels of background load. In particular, it has been used to evaluate the performance of VMTP <ref> [36, 140] </ref>, FTP and SunRPC [72], and to compare the performance of OSI TP4 and TCP [160]. This tool also allows specification of variable message sizes and the delay between successive messages sent on a given connection.
Reference: [37] <author> G. Chesson, </author> <title> "XTP/PE overview," </title> <booktitle> in Proc. Conference on Local Computer Networks, </booktitle> <month> October </month> <year> 1988. </year>
Reference-contexts: Network adapters either facilitate flexibility in supporting different protocols through simple designs [46, 103, 120], or restrict flexibility, and hence improve performance, by supporting specific communication protocols and resource management strategies <ref> [37, 90] </ref>. In general, the overhead of host-adapter interaction and unnecessary data movement across the system bus can limit the amount of useful concurrency between the software and hardware portions of the protocol stack, thus affecting communication subsystem performance.
Reference: [38] <author> D. D. Clark, </author> <title> "The structuring of systems using upcalls," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <pages> pp. 171-180, </pages> <year> 1985. </year>
Reference-contexts: However, implementation overheads and constraints are not incorporated in the resource management policies. Moreover, no performance impact of supporting QoS in communication is reported. Real-time upcalls (RTUs) [68] are a mechanism to schedule protocol processing for net-worked multimedia applications via event-based upcalls <ref> [38] </ref>. Protocol processing activities are scheduled via an extended version of the rate monotonic (RM) scheduling policy [111]. Similar to our approach, delayed preemption adopted to reduce the number of context switches.
Reference: [39] <author> D. D. Clark, S. Shenker, and L. Zhang, </author> <title> "Supporting real-time applications in an integrated services packet network: Architecture and mechanism," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 14-26, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Finally, instead of deterministic or probabilistic guarantees, even looser forms of QoS may be provisioned for applications that can adapt to variations in the delivered QoS. Proposals for predictive (or best-effort) real-time communication, such as FIFO+ <ref> [39] </ref> and Hop-Laxity [156], fall in this category. 1.1.2 Integrated Services on the Internet Significant efforts are being made by the IETF to enhance the service model of the Internet to support integrated services for voice, video, and data transport [22, 39]. <p> Proposals for predictive (or best-effort) real-time communication, such as FIFO+ [39] and Hop-Laxity [156], fall in this category. 1.1.2 Integrated Services on the Internet Significant efforts are being made by the IETF to enhance the service model of the Internet to support integrated services for voice, video, and data transport <ref> [22, 39] </ref>. <p> By providing additional buffers and adaptively controlling the rate at which packets are consumed at the receiver, these 18 applications can adapt to existing network conditions <ref> [8, 39, 145] </ref>. The destination buffers serve to restore the spacing between successive data samples and buffer out variations in network delay. A study on packet video transport [91] illustrated the effectiveness of using network feedback about load conditions to modulate the frame transmission rate of video sources.
Reference: [40] <author> D. D. Clark, V. Jacobson, J. Romkey, and H. Salwen, </author> <title> "An analysis of TCP processing overhead," </title> <journal> IEEE Trans. Communications, </journal> <pages> pp. 23-29, </pages> <month> June </month> <year> 1989. </year> <month> 223 </month>
Reference-contexts: The techniques employed to improve the delivered throughput and latency of communication protocols include [61] * minimization of data-copying within the communication subsystem and across the API [52, 181], * optimization of protocol implementations, such as hand-optimized critical paths <ref> [40] </ref>, 27 and integrated layer processing [1, 41], * appropriate network interface design for high performance [161], and, * exploitation of concurrency and parallelism within the communication subsystem [152]. <p> Network adapter support for checksummed, multiple-packet communication in an ATM network is explored in [26]. Protocol stack optimization: A number of research efforts have focused on the optimization of protocol stack execution latency. These include protocol-specific optimizations for TCP <ref> [40] </ref> and UDP [146], improving data touching overheads of checksumming for UDP/IP stacks [97], and exploring the non-data touching processing and related operating system overheads in TCP/IP stacks [96]. A detailed study of factors affecting end-to-end communication latency in LAN environments is described in [170].
Reference: [41] <author> D. D. Clark and D. L.Tennenhouse, </author> <title> "Architectural considerations for a new generation of communication protocols," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 200-208, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The techniques employed to improve the delivered throughput and latency of communication protocols include [61] * minimization of data-copying within the communication subsystem and across the API [52, 181], * optimization of protocol implementations, such as hand-optimized critical paths [40], 27 and integrated layer processing <ref> [1, 41] </ref>, * appropriate network interface design for high performance [161], and, * exploitation of concurrency and parallelism within the communication subsystem [152]. <p> A detailed study of factors affecting end-to-end communication latency in LAN environments is described in [170]. ILP, first proposed in <ref> [41] </ref> and subsequently implemented and evaluated in [1], reduces the number of accesses to network data by effectively collapsing protocol layers and executing them in an integration fashion for each data word accessed. <p> We note that the proposed architecture and the admission control extensions described in Chapter 4 are also applicable to application-level framing <ref> [41] </ref> and user-level protocol processing architectures explored in recent efforts [55, 112, 167] to improve data transfer throughput in high-speed networks. In our design approach we have not made any specific assumptions about the location of the protocol stack, which could reside in the kernel or in user space. <p> Note that in our architecture, applications output packets to the socket layer. Thus, our architecture is naturally suited to techniques such as application-level framing <ref> [41] </ref> and can be used with protocols such as RTP [155]. For example, it supports user-level fragmentation and protocol processing performed in the application's address space. Accordingly, it is well-suited to multi-threaded multimedia applications that have distinct data generator and data exporter threads.
Reference: [42] <author> G. Coulson, G. S. Blair, and P. Robin, </author> <title> "Micro-kernel support for continuous media in distributed systems," </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> vol. 26, </volume> <pages> pp. 1323-1341, </pages> <year> 1994. </year>
Reference-contexts: Key abstractions proposed and implemented were a split-level scheduling architecture featuring an EDF [111] kernel scheduler and an application-level scheduler for efficient real-time scheduling. The application-level and kernel-level schedulers communication with each other via shared memory. Support for continuous media in the Chorus microkernel is described in <ref> [42] </ref> which utilizes the IPC and scheduling techniques proposed in [70]. Nemesis [109] is a multimedia operating system design from scratch. While many aspects of operating system design have been considered, not much attention has been given to QoS issues in communication subsystem design.
Reference: [43] <author> G. Coulson, A. Campbell, P. Robin, G. S. Blair, M. Papathomous, and D. Shepherd, </author> <title> "The design of a QoS-controlled ATM-based communications system in Chorus," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 13, no. 4, </volume> <pages> pp. 686-699, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Further, whether shaping is performed on traffic associated with a reservation is determined from the service class that reservation belongs to. Support for QoS-sensitive communication: The design of a QoS-controlled communications system for ATM networks is described in <ref> [43] </ref>. However, implementation overheads and constraints are not incorporated in the resource management policies. Moreover, no performance impact of supporting QoS in communication is reported. Real-time upcalls (RTUs) [68] are a mechanism to schedule protocol processing for net-worked multimedia applications via event-based upcalls [38].
Reference: [44] <author> R. L. Cruz, </author> <title> A Calculus for Network Delay and a Note on Topologies of Interconnection Networks, </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> July </month> <year> 1987. </year> <note> available as technical report UILU-ENG-87-2246. </note>
Reference-contexts: When the application requests that the channel be destroyed, all resources allocated for the channel are released by the network and the communication subsystems at the source and destination hosts. Traffic and QoS Specification: Traffic generation on real-time channels is based on a linear bounded arrival process <ref> [4, 44] </ref> characterized by three parameters: maximum message size (M max bytes), maximum message rate (R max messages/second), and maximum burst size (B max messages). In any interval of length ffi, the number of messages generated is bounded by B max +ffiR max .
Reference: [45] <author> H. Custer, </author> <title> Inside Windows NT, Microsoft Press, One Microsoft Way, </title> <address> Redmond, Washington 98052-6399, </address> <year> 1993. </year>
Reference-contexts: Thus, these policies must be extended to make them useful in practice. ffi Guaranteed-QoS communication services on microkernel operating systems: Microkernel operating systems play an increasing important role in today's PC, workstation, and server markets, as evidenced by the growing popularity of Windows NT <ref> [45] </ref>. Realization of guaranteed-QoS communication services on contemporary microkernel operating systems presents additional challenges that affect the structuring and performance of communication software. <p> This chapter describes the design, implementation, and evaluation of one such service for a microkernel operating system. Microkernel operating systems continue to play an important role in operating system design <ref> [45, 100] </ref>, and are being extended to support real-time and multimedia applications [162]. With the continued upsurge in the demand for networked multimedia applications on the WWW, it is important, therefore, to examine realization of QoS-sensitive communication subsystems on contemporary microkernel operating systems.
Reference: [46] <author> C. Dalton, G. Watson, D. Banks, C. Calamvokis, A. Edwards, and J. Lumley, </author> <title> "Afterburner," </title> <journal> IEEE Network Magazine, </journal> <pages> pp. 36-43, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Since the network adapter defines the communication primitives available to the communication software, flexible support for network I/O necessitates a careful division of functionality between the adapter and the host processor [89, 164]. Network adapters either facilitate flexibility in supporting different protocols through simple designs <ref> [46, 103, 120] </ref>, or restrict flexibility, and hence improve performance, by supporting specific communication protocols and resource management strategies [37, 90].
Reference: [47] <author> P. B. Danzig, </author> <title> "An analytical model of operating system protocol processing including effects of multiprogramming," </title> <booktitle> in Proc. of ACM SIGMETRICS, </booktitle> <pages> pp. 11-20, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Useful insights into the subtle effects of multiprogramming and network input load on packet reception time are provided by Danzig <ref> [47] </ref>. He constructs a performance model of protocol processing in UNIX [108] to capture the effects of multiprogramming and limited socket buffers.
Reference: [48] <author> B. Davie, </author> <title> "The architecture and implementation of a high-speed host interface," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 228-239, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The design of high-speed network adapters, their performance characteristics, and implications for protocol stacks in uniprocessor workstation environments has received significant attention recently, for FDDI [151] as well as ATM <ref> [48, 173] </ref> networks. Uniprocessor front-ends and network adapters: With dedicated network front-ends, the network adapter operates under control of a front-end communication processor which handles all communication-related functions including network interrupts.
Reference: [49] <author> L. Delgrossi and L. Berger, </author> <title> "Internet stream protocol version 2 (ST-2) protocol specification version ST2+," Request for Comments RFC 1819, </title> <month> August </month> <year> 1995. </year> <institution> ST2 Working Group. </institution>
Reference-contexts: RSVP, in particular, has been designed to support multicast communication between heterogeneous receivers. In contrast to RSVP, which initiates reservation setup at the receiver, an alternative approach 6 to signalling is adopted by the ST-II protocol <ref> [49] </ref>, which initiates reservation setup at the sender.
Reference: [50] <author> A. Demers, S. Keshav, and S. Shenker, </author> <title> "Analysis and simulation of a fair queueing algorthm," </title> <booktitle> Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 3-12, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Example service disciplines include Weighted Fair Queueing <ref> [50] </ref>, also known as Packet-by-Packet Generalized Processor Sharing [144], and Rate-Controlled Static-Priority Queueing [187]. In addition to servicing (QoS-sensitive) traffic according to the associated QoS guarantees, the service discipline also determines the treatment of best-effort traffic at the node.
Reference: [51] <author> J. Dolter, S. Daniel, A. Mehra, J. Rexford, W. Feng, and K. Shin, "SPIDER: </author> <title> Flexible and efficient communication support for point-to-point distributed systems," </title> <booktitle> in Proc. Int'l Conf. on Distributed Computing Systems, </booktitle> <pages> pp. 574-580, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: We have examined the impact of adapter characteristics on the ability to support real-time communication effectively [77], and have explored QoS support on network adapters for point-to-point and shared networks [80]. We have designed SPIDER <ref> [51] </ref>, an adapter supporting real-time communication in point-to-point networks.
Reference: [52] <author> P. Druschel, M. B. Abbott, M. Pagels, and L. L. Peterson, </author> <title> "Network subsystem design: A case for an integrated data path," </title> <journal> IEEE Network Magazine, </journal> <pages> pp. 8-17, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The techniques employed to improve the delivered throughput and latency of communication protocols include [61] * minimization of data-copying within the communication subsystem and across the API <ref> [52, 181] </ref>, * optimization of protocol implementations, such as hand-optimized critical paths [40], 27 and integrated layer processing [1, 41], * appropriate network interface design for high performance [161], and, * exploitation of concurrency and parallelism within the communication subsystem [152].
Reference: [53] <author> P. Druschel and G. Banga, </author> <title> "Lazy receiver processing (LRP): A network subsystem architecture for server systems," </title> <booktitle> in Proc. USENIX Symp. on Operating Systems Design and Implementation, </booktitle> <pages> pp. 261-275, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Lazy receiver processing (LRP) <ref> [53] </ref>, while not completely eliminating it, significantly reduces the likelihood of receive livelock even under high input load. In LRP, an incoming packet is classified and enqueued, but not processed, until the application receives the data. <p> The CORDS path abstraction [172], which is similar to Scout paths [128], provides a rich framework for development of real-time communication services for distributed applications, as demonstrated with the guaranteed-QoS communication service in Chapter 5. As mentioned earlier, while LRP <ref> [53] </ref> works well for receive-livelock elimination for best-effort traffic, the architectural approach outlined in this dissertation is needed to extend it to accommodate QoS-sensitive traffic. Similar to LRP, our approach also utilizes early demultiplexing and channel-specific queueing of incoming packets. <p> Our architecture, therefore, facilitates provision of QoS guarantees while preventing receive livelock. We note that an alternative approach is outlined in <ref> [53] </ref>, in which receive livelock is prevented by deferring protocol processing of a received packet until the receiving application is scheduled for execution. <p> Protocol processing for transmission is performed at system call time, while protocol processing for reception is triggered via an upcall 209 into the application's address space. Type IV: Lazy receiver processing (LRP) <ref> [53] </ref> is similar to the traditional in-kernel configuration, except that the processing of receiving packets is performed mostly in the context of, and at the priority of, the receiving application when it actually receives the data.
Reference: [54] <author> P. Druschel and L. L. Peterson, "Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <pages> pp. 189-202, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Proposals to achieve high-bandwidth data transfer across protection domains (e.g., between the kernel and an application) include restricted virtual memory remapping [175], container shipping [147], and Fbufs <ref> [54] </ref>. Support for in-kernel device-to-device data transfers for multimedia applications is described in [60, 93]. An extensive taxonomy of the software and hardware tradeoffs involved in data passing and performance comparison of different buffering semantics is provided in [25], and two copy avoidance techniques evaluated in [27].
Reference: [55] <author> P. Druschel, L. L. Peterson, and B. S. Davie, </author> <title> "Experiences with a high-speed network adaptor: A software perspective," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 2-13, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: With appropriate support in the network interface and the operating system, it is possible to grant applications full access to the network interface, thereby bypassing the operating system completely <ref> [28, 55] </ref>. Application-specific networking: Recently there has been much interest in OS extension technologies, in which trusted application code is installed for execution within the OS. This allows operating systems to be customized or "extended" to implement application-specific policies. <p> We note that the proposed architecture and the admission control extensions described in Chapter 4 are also applicable to application-level framing [41] and user-level protocol processing architectures explored in recent efforts <ref> [55, 112, 167] </ref> to improve data transfer throughput in high-speed networks. In our design approach we have not made any specific assumptions about the location of the protocol stack, which could reside in the kernel or in user space. <p> Protocol processing is thus triggered via IPC between the application and the trusted server on the one hand, and between the server and the kernel on the other. Type III: also corresponds to a user-level protocol processing configuration, on a micro-kernel operating system [112, 168] or otherwise <ref> [55] </ref>, with the difference that data transmission and reception is performed by threads executing in communication subsystem libraries linked with the application; control operations such as setting up connections are performed by the user-level trusted server or the kernel.
Reference: [56] <author> C. Dubnicki, L. Iftode, E. W. Felten, and K. Li, </author> <title> "Software support for virtual memory-mapped communication," </title> <booktitle> in Proc. International Conference on Parallel Processing, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: For example, the Hamlyn [30] network interface performs sender-controlled data transfer, in which the sender controls the memory location at the receiver where data will be deposited. Similarly, the network interface for the SHRIMP multicomputer [18] realizes direct writes to remote memory via virtual memory mapped communication <ref> [56] </ref>. This requires specialized network interface hardware that maintains virtual to physical addressing mappings. Implications for QoS-sensitive transmission/reception Neither of these approaches are feasible for QoS-sensitive communication between workstation-type clients and servers across WANs. For example, unlike these approaches, clients and servers do not trust one another.
Reference: [57] <author> A. Edwards, G. Watson, J. Lumley, D. Banks, C. Clamvokis, and C. Dalton, </author> <title> "User-space protocols deliver high performance to applications on a low-cost Gb/s LAN," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 14-24, </pages> <address> London, UK, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Several recent efforts have also focused on optimizing the protocol processing latency in TCP/IP protocol stacks [16, 129, 176]. User-level protocol processing: Several research efforts have focused on increasing communication subsystem throughput via user-level handling of network data <ref> [57, 112, 167] </ref>. In 28 addition to data copy minimization compared to a server-based implementation, user-level protocol processing offers significant flexibility in developing and debugging communication protocols.
Reference: [58] <author> Edwin F. Menze III and F. Travostino, </author> <title> The CORDS Book, </title> <institution> OSF Research Institute, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: The most obvious is the ease of development and debugging, resulting in a shorter development cycle. Another important reason is the infeasibility of the other alternative (pure in-kernel development) available for implementation. CORDS allows an x-kernel protocol graph to span address space (and protection) boundaries via proxies <ref> [58] </ref>. Thus, our implementation could span user and kernel spaces, with portions of our implementation developed in user space and moved into the kernel after debugging and testing.
Reference: [59] <author> D. Engler and M. F. Kaashoek, "DPF: </author> <title> Fast, flexible message demultiplexing using dynamic code generation," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 53-59, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Successive implementations of packet filters have reduced classification overhead significantly, e.g., BPF [114], MPF [186], and PATHFINDER [11]. While all of these filter designs perform classification via interpretation, more recently dynamic code generation techniques have been applied to realize very efficient packet filters, as in DPF <ref> [59] </ref>. Packet filters are necessary for packet classification in connectionless networks, such as the IPv4-based Internet. Further, they may also be needed in native ATM networks that aggregate multiple end-to-end connections over virtual circuits [11]. <p> In the absence of such support, the receiving host must rely upon efficient packet filters <ref> [59, 186] </ref> to perform the packet classification. Note that this overhead is not incurred at the sending host if the API directly associates outgoing messages with channels. <p> The traffic classification function is thus a 189 statically compiled packet filter, as opposed to a dynamically generated one <ref> [59] </ref>. 7.3.2 Data Transfer During data transfer on a QoS connection, the socket layer interacts with QOSMGR to obtain a buffer for each packet generated by the application.
Reference: [60] <author> K. Fall and J. Pasquale, </author> <title> "Exploiting in-kernel data paths to improve I/O throughput and CPU availability," </title> <booktitle> in Proc. USENIX Winter Conference, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: Proposals to achieve high-bandwidth data transfer across protection domains (e.g., between the kernel and an application) include restricted virtual memory remapping [175], container shipping [147], and Fbufs [54]. Support for in-kernel device-to-device data transfers for multimedia applications is described in <ref> [60, 93] </ref>. An extensive taxonomy of the software and hardware tradeoffs involved in data passing and performance comparison of different buffering semantics is provided in [25], and two copy avoidance techniques evaluated in [27]. Network adapter support for checksummed, multiple-packet communication in an ATM network is explored in [26].
Reference: [61] <author> D. C. Feldmeier, </author> <title> "A survey of high performance protocol implementation techniques," in High Performance Networks: Technology and Protocols, </title> <editor> A. N. Tantawy, </editor> <booktitle> editor, </booktitle> <pages> pp. 29-50, </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference-contexts: The techniques employed to improve the delivered throughput and latency of communication protocols include <ref> [61] </ref> * minimization of data-copying within the communication subsystem and across the API [52, 181], * optimization of protocol implementations, such as hand-optimized critical paths [40], 27 and integrated layer processing [1, 41], * appropriate network interface design for high performance [161], and, * exploitation of concurrency and parallelism within the
Reference: [62] <author> D. Ferrari, </author> <title> "Client requirements for real-time communication services," </title> <journal> IEEE Communication Magazine, </journal> <pages> pp. 65-72, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: A given application may exhibit varying degrees of tolerance to only a single QoS parameter or a combination of multiple QoS parameters. Networked multimedia applications integrating various media such as audio, video, and text, exhibit a wide range of QoS requirements on communication <ref> [62] </ref>. Examples of such applications include high-definition television, medical imaging, scientific visualization, full-motion video, multiparty video conferencing, and collaborative workspace [35, 145, 148]. The QoS requirements of multimedia applications differ significantly from those of tra 17 ditional data transfer applications such as remote login, electronic mail, and file transfer.
Reference: [63] <author> D. Ferrari and D. C. Verma, </author> <title> "A scheme for real-time channel establishment in wide-area networks," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 8, no. 3, </volume> <pages> pp. 368-379, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: While this architecture is applicable to a wide variety of service disciplines for providing QoS guarantees, it is validated by implementing real-time channels <ref> [63, 92] </ref>, a paradigm for guaranteed-QoS communication in packet-switched networks proposed by other researchers. <p> It argued that bounded processing delays and overheads within the communication subsystem can be achieved through appropriate priority-based scheduling [6]. The Tenet real-time protocol suite [12] is one of the first implementations of real-time channels <ref> [63] </ref> on wide-area networks (WANs), and supports both deterministic and statistical service guarantees. In addition to providing protocol support for end-to-end signalling, support is provided for QoS-sensitive packet scheduling at hosts and routers via Rate-Controlled Static-Priority Queueing [187]. <p> While the proposed architecture is applicable to other proposals for guaranteed-QoS connections [8, 188], we focus on real-time channels, a paradigm for guaranteed-QoS communication services in packet-switched networks <ref> [63, 92] </ref>. Consider the problem of servicing several guaranteed-QoS and best-effort connections engaged in network input/output at a host. <p> While the architectural mechanisms proposed in this paper are applicable to most of the proposed models, we focus on real-time channels <ref> [63, 92] </ref>, using the model proposed and analyzed in [92]. A real-time channel is a simplex, fixed-route, virtual con 50 nection between a source and destination host, with sequenced messages and associated performance guarantees on message delivery. It therefore conforms to the connection semantics mentioned earlier. <p> This would then provide effective support for adaptive applications which are built 75 with a certain degree of tolerance for QoS violations. Various elements of this architecture can also be utilized to realize statistical real-time channels <ref> [63] </ref>. Statistical QoS guarantees can potentially be useful to a large class of distributed multimedia applications. Finally, we have extended this architecture to realize a QoS-sensitive communication subsystem for shared-memory multiprocessor multimedia servers [119].
Reference: [64] <author> M. E. Fiuczynski and B. N. Bershad, </author> <title> "An extensible protocol architecture for application-specific networking," </title> <booktitle> in Proc. USENIX Winter Conference, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: Alternately, applications and operating systems can be designed to cooperate with one another to perform QoS negotiation and adaptation to maximize service accessibility and continuity [2, 33, 104]. A novel way to realize application-specific policies in networking is via installing of application code directly in the kernel <ref> [64] </ref>. <p> Application-specific networking: Recently there has been much interest in OS extension technologies, in which trusted application code is installed for execution within the OS. This allows operating systems to be customized or "extended" to implement application-specific policies. Plexus <ref> [64] </ref> and application-specific handlers in the Aegis kernel [180] are two examples of approaches to realize application-specific networking via OS extensions. Packet classification: Packet filters provide general and flexible classification (i.e., demul-tiplexing) of incoming packets to application end-points, at the lowest layer of the protocol stack.
Reference: [65] <author> S. Floyd and V. Jacobson, </author> <title> "Link-sharing and resource management models for packet networks," </title> <journal> IEEE/ACM Trans. Networking, </journal> <volume> vol. 3, no. 4, </volume> <pages> pp. 365-386, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: communication is also evidenced by the rapid proliferation of on-line multimedia content on the WWW, the shortcomings of the "best-effort" service provided by today's Internet, and the efforts by the Internet Engineering Task Force (IETF) to address these shortcomings by developing protocols and standards for Integrated Services on the Internet <ref> [20, 22, 65] </ref>. To support such applications, all hardware and software components involved in transferring application data from one host to another across the network must be designed to be QoS-sensitive, i.e., to provide QoS guarantees. <p> Towards that end, the IETF is developing a set of protocols and standards for integrated services <ref> [20, 65, 149, 189] </ref>. In the IETF's vision, applications request and reserve resources, both in the network and at the attached hosts (clients or servers) using an end-to-end receiver-initiated Resource ReSerVation Protocol (RSVP) [23, 189]. <p> In the context of integrated services, the expected QoS requirements of applications and issues involved in sharing link bandwidth across multiple classes of traffic are explored in <ref> [65, 158] </ref>. Much support being provided on the Internet is geared towards realizing efficient multicast communication and accommodating the heterogeneity of receivers. RSVP, in particular, has been designed to support multicast communication between heterogeneous receivers. <p> A software-based implementation also enables experimentation with a variety of link sharing policies, especially if multiple service classes are supported. For example, alternative approaches such as setting aside a certain minimum CPU and link bandwidth for best-effort traffic can be explored <ref> [65] </ref>. The architecture can also be extended to networks providing explicit support for QoS guarantees, such as ATM. However, the communication software may need to track adapter buffer usage in order to schedule the transfer of outgoing packets to the adapter. <p> Our present work is applicable to other proposals for real-time communication and QoS guarantees [8, 188]. Further, the proposed admission control extensions are general and applicable to other host and network architectures. In particular, Internet servers running TCP/IP protocol stacks supporting Integrated Services <ref> [22, 65] </ref>, especially the guaranteed service class [159], can also benefit from these extensions. Similarly, Internet routers can apply these extensions when incoming packets must 78 be fragmented before forwarding in order to reconcile the different MTUs of the attached networks.
Reference: [66] <author> B. Ford and S. Susarla, </author> <title> "CPU inheritance scheduling," </title> <booktitle> in Proc. USENIX Symp. on Operating Systems Design and Implementation, </booktitle> <pages> pp. 91-105, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Our design approach decouples the 41 protocol processing priority from that of the application, allowing the former to be derived from the traffic characteristics and run-time behavior of the application. QoS-sensitive CPU scheduling: Recently several QoS-sensitive CPU scheduling policies have been proposed recently <ref> [66, 71, 165, 179] </ref>. Scheduling algorithms for integrated scheduling of multimedia soft real-time computation and traditional hard real-time tasks on a multiprocessor multimedia server are proposed and evaluated in [94].
Reference: [67] <author> A. Garg, </author> <title> "Parallel STREAMS: A multiprocessor implementation," </title> <booktitle> in Winter 1990 USENIX Conference, </booktitle> <pages> pp. 163-176, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: In this model, protocol processing is treated as work strictly local to each processor, resulting in an implicit sharing between the computation and communication subsystems. An alternative approach 34 treats protocol processing as global work that can be scheduled uniformly on any available processor <ref> [67, 99] </ref>; this results in explicit sharing between the two subsystems. Implications for QoS-sensitive protocol processing These approaches to exploiting communication parallelism may not suffice for QoS-sensitive protocol processing since they introduce unpredictability in the availability and allocation of processing resources, and complicate global coordination for network access.
Reference: [68] <author> R. Gopalakrishnan and G. M. Parulkar, </author> <title> "A real-time upcall facility for protocol processing with QoS guarantees," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <address> p. 231, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Support for QoS-sensitive communication: The design of a QoS-controlled communications system for ATM networks is described in [43]. However, implementation overheads and constraints are not incorporated in the resource management policies. Moreover, no performance impact of supporting QoS in communication is reported. Real-time upcalls (RTUs) <ref> [68] </ref> are a mechanism to schedule protocol processing for net-worked multimedia applications via event-based upcalls [38]. Protocol processing activities are scheduled via an extended version of the rate monotonic (RM) scheduling policy [111]. Similar to our approach, delayed preemption adopted to reduce the number of context switches. <p> This suggests that the data exporter threads, i.e., those performing fragmentation and other processing of network data, be scheduled for execution using fine-grain preemption. 205 Such fine-grain multiplexing of communication threads has been adopted and analyzed in <ref> [68, 69] </ref> for RM scheduling, and in [116, 117] for EDF scheduling, respectively. The architecture described in [116], which was presented in Chapter 3 decouples protocol processing priority from application priority, deriving the former from the traffic and QoS specification on each connection. <p> Potential load-dependent variations in the actual shaping latency can be accommodated via appropriate adaptation and 206 buffer management, or largely eliminated via integration with QoS-sensitive CPU scheduling policies. Our work complements recent work on QoS-sensitive CPU scheduling of applications [71, 165, 179] and protocol processing <ref> [68, 69, 116, 117] </ref> at end hosts. While these efforts focus on CPU scheduling, our primary focus is on the QoS support architecture exported to sockets based applications. With appropriate CPU scheduling support, our QoS architecture enables new and legacy applications to utilize end-to-end QoS on communication.
Reference: [69] <author> R. Gopalakrishnan and G. M. Parulkar, </author> <title> "Bringing real-time scheduling theory and practice closer for multimedia computing," </title> <booktitle> in Proc. of ACM SIGMETRICS, </booktitle> <pages> pp. 1-12, </pages> <month> May </month> <year> 1996. </year> <month> 225 </month>
Reference-contexts: Moreover, the issues and tradeoffs involved in realizing QoS-sensitive communication subsystems are not considered. Modeling of multimedia/real-time operating systems: A number of recent efforts have attempted to bridge the gap between theory and practice for real-time systems and multimedia computing <ref> [29, 69, 95, 98] </ref>. The admission control extensions developed in Chapter 4 are geared towards the real-time communication needs of distributed systems, and hence complement these efforts. <p> This is particularly true for preemption caused by external events such as network interrupts. One can account for the cache miss penalty due to preemption via careful schedulability analysis [105], but frequent preemption still degrades available CPU capacity, as also observed in <ref> [69] </ref>. Moreover, an important implication of arbitrary preemption for the proposed architecture is that a handler may get preempted just before initiating transmission, even though it had finished preparing a packet for transmission, thus idling the link. <p> This suggests that the data exporter threads, i.e., those performing fragmentation and other processing of network data, be scheduled for execution using fine-grain preemption. 205 Such fine-grain multiplexing of communication threads has been adopted and analyzed in <ref> [68, 69] </ref> for RM scheduling, and in [116, 117] for EDF scheduling, respectively. The architecture described in [116], which was presented in Chapter 3 decouples protocol processing priority from application priority, deriving the former from the traffic and QoS specification on each connection. <p> Potential load-dependent variations in the actual shaping latency can be accommodated via appropriate adaptation and 206 buffer management, or largely eliminated via integration with QoS-sensitive CPU scheduling policies. Our work complements recent work on QoS-sensitive CPU scheduling of applications [71, 165, 179] and protocol processing <ref> [68, 69, 116, 117] </ref> at end hosts. While these efforts focus on CPU scheduling, our primary focus is on the QoS support architecture exported to sockets based applications. With appropriate CPU scheduling support, our QoS architecture enables new and legacy applications to utilize end-to-end QoS on communication.
Reference: [70] <author> R. Govindan and D. P. Anderson, </author> <title> "Scheduling and IPC mechanisms for continuous media," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <pages> pp. 68-80, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In addition, this allows protocol processing to be scheduled via thread scheduling schemes distinct from those developed for application threads, especially for multimedia applications <ref> [6, 70] </ref>. In recent years, resource management for computation has moved towards giving more control to the applications, e.g., user-level threads [113], scheduler activations [7], and application-kernel coordination to dynamically vary the degree of active parallelism in scientific computing applications [174]. <p> Commercial and research operating systems: Govindan and Anderson <ref> [70] </ref> first proposed scheduling and IPC mechanisms for operating systems supporting continuous media. Key abstractions proposed and implemented were a split-level scheduling architecture featuring an EDF [111] kernel scheduler and an application-level scheduler for efficient real-time scheduling. The application-level and kernel-level schedulers communication with each other via shared memory. <p> The application-level and kernel-level schedulers communication with each other via shared memory. Support for continuous media in the Chorus microkernel is described in [42] which utilizes the IPC and scheduling techniques proposed in <ref> [70] </ref>. Nemesis [109] is a multimedia operating system design from scratch. While many aspects of operating system design have been considered, not much attention has been given to QoS issues in communication subsystem design. <p> We take these costs into account when developing the admission control extensions described in Chapter 4. OS support for QoS-sensitive communication: The need for scheduling protocol processing at priority levels consistent with those of the communicating application was highlighted in [6], and some implementation strategies demonstrated in <ref> [70] </ref>. More recently, processor capacity reserves in RT-Mach [123] have been combined with user-level protocol processing [112] for predictable protocol processing inside hosts [107].
Reference: [71] <author> P. Goyal, X. Guo, and H. M. Vin, </author> <title> "A hierarchical CPU scheduler for multimedia operating systems," </title> <booktitle> in Proc. USENIX Symp. on Operating Systems Design and Implementation, </booktitle> <pages> pp. 107-121, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: There have been numerous proposals for QoS-sensitive provisioning of network resources such as transmission bandwidth and buffers [8, 188], and host computation 1 resources such as CPU cycles <ref> [71, 165, 179] </ref>. However, not much attention has been paid to the interface between the applications and the network, i.e., the communication subsystem at end hosts, for provision of QoS guarantees. <p> Our design approach decouples the 41 protocol processing priority from that of the application, allowing the former to be derived from the traffic characteristics and run-time behavior of the application. QoS-sensitive CPU scheduling: Recently several QoS-sensitive CPU scheduling policies have been proposed recently <ref> [66, 71, 165, 179] </ref>. Scheduling algorithms for integrated scheduling of multimedia soft real-time computation and traditional hard real-time tasks on a multiprocessor multimedia server are proposed and evaluated in [94]. <p> The scheduling delay associated with session-level shaping can be largely eliminated (or at least made predictable) by employing QoS-sensitive CPU scheduling policies, as discussed next. 7.6.2 QoS-Sensitive CPU Scheduling Recently several QoS-sensitive scheduling policies such as stride scheduling [179], proportional share [165], and hierarchical scheduling <ref> [71] </ref> have been proposed. These policies ensure that the CPU is allocated to individual threads in the order of their associated QoS requirements and at the granularity of a certain quantum, i.e., each thread executes at the most for a quantum each time it is selected to run. <p> Potential load-dependent variations in the actual shaping latency can be accommodated via appropriate adaptation and 206 buffer management, or largely eliminated via integration with QoS-sensitive CPU scheduling policies. Our work complements recent work on QoS-sensitive CPU scheduling of applications <ref> [71, 165, 179] </ref> and protocol processing [68, 69, 116, 117] at end hosts. While these efforts focus on CPU scheduling, our primary focus is on the QoS support architecture exported to sockets based applications. <p> Another approach is to realize the entire communication subsystem as a class with an appropriate share in the recently proposed QoS-sensitive scheduling policies <ref> [71, 139, 165, 179] </ref>. These policies ensure that threads execute on the CPU in the order of their associated 214 QoS requirements and at the granularity of a certain quantum. Each thread executes for at most a quantum each time it is selected to run.
Reference: [72] <author> P. Gunningberg, M. Bjorkman, E. Nordmark, S. Pink, P. Sjodin, and J.-E. Stromquist, </author> <title> "Application protocols and performance benchmarks," </title> <journal> IEEE Communication Magazine, </journal> <pages> pp. 30-36, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: This model has been used in a tool to compare different protocols and protocol implementations for varying levels of background load. In particular, it has been used to evaluate the performance of VMTP [36, 140], FTP and SunRPC <ref> [72] </ref>, and to compare the performance of OSI TP4 and TCP [160]. This tool also allows specification of variable message sizes and the delay between successive messages sent on a given connection.
Reference: [73] <author> O. Hagsand and P. Sjodin, </author> <title> "Workstation support for real-time multimedia communication," </title> <booktitle> in Winter USENIX Conference, </booktitle> <pages> pp. 133-142, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: More recently, processor capacity reserves in RT-Mach [123] have been combined with user-level protocol processing [112] for predictable protocol processing inside hosts [107]. Operating system support for multimedia communication is explored in <ref> [73] </ref>, where the focus is on provision of preemption points and EDF scheduling in the kernel, and in [177], which also focuses on the scheduling architecture. None of these approachees provide support for traffic enforcement or decoupling of protocol processing priority from application priority.
Reference: [74] <author> T.-Y. Huang, J. W. Liu, and D. Hull, </author> <title> "A method for bounding the effect of DMA I/O interference on program execution time," </title> <booktitle> in Proc. 17th Real-Time Systems Symposium, </booktitle> <pages> pp. 275-285, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: Therefore, while the absolute performance observed may not be entirely accurate, the observed trends and performance comparisons reported continue to be valid. Also, it may be possible to extend the message service time computation to accurately account for the potential perturbation caused by the DMA transfers via careful analysis <ref> [74] </ref>.
Reference: [75] <author> J.-F. Huard, "kStack: </author> <title> A user space native-mode ATM transport layer with QoS support," </title> <type> Technical Report CU/CTR TR 463-96-29, </type> <institution> Center for Telecommunications Research, Columbia University, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: A native-mode ATM transport layer has been designed and implemented in [3], and enhanced with QoS support for a user space implementation <ref> [75] </ref>. Similar to our RSVP-based QoS architecture, traffic policing and shaping is performed while copying application data into kernel buffers; the application is blocked if it is violating its traffic specification. However, our design is applicable to general TCP/IP protocol stacks, including legacy LAN and ATM interfaces.
Reference: [76] <author> N. C. Hutchinson and L. L. Peterson, </author> <title> "The x-Kernel: An architecture for implementing network protocols," </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> vol. 17, no. 1, </volume> <pages> pp. 1-13, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: More importantly, it permits management of the communication subsystem by a separate executive designed for communication-related operations, such as the x-kernel <ref> [76] </ref>. The VMP NAB [90] and Nectar CAB [120] are two examples of this functional partitioning. <p> The pursuit of application-level gigabits-per-second has prompted recent efforts to develop faster communication subsystems through the exploitation of parallelism. In general, communication subsystems employ one or more processes (or threads) to implement a protocol graph <ref> [76] </ref>. Depending upon the allocation of work to these processes, communication subsystems can be classified into horizontal or vertical process architectures [152]. <p> In horizontal architectures [182], each process implements a specific layer of a protocol graph; at most two processes can be assigned to each layer, one each for transmission and reception. In vertical process architectures <ref> [14, 76, 83] </ref>, on the other hand, processes are assigned to active entities such as connections or messages and each process implements one path through the protocol graph. This approach significantly reduces context switches and message buffering that are unavoidable in horizontal process architectures. <p> Fine-grain techniques provide more scalable parallelism and hence potentially greater performance gains. In practice, though, synchronization constraints, resource contention, and load balancing requirements limit the speedups, and hence the message throughputs, observed. Overheads of managing parallelism: The techniques used in [14] to parallelize x-kernel <ref> [76, 141] </ref> and the protocols, including the locking mechanisms used and dedication of special x-kernel functions to specific processors, suggest that special synchronization paradigms and processor allocation mechanisms are needed to manage communication parallelism effectively. <p> We have developed a prototype implementation of the proposed architecture using a communication executive derived from x-kernel 3.1 <ref> [76] </ref> that exercises complete control over a Motorola 68040 CPU. This configuration avoids any interference from computation or other operating system activities on the host, allowing us to focus on the communication subsystem. <p> send message on the specified real-time channel rtc recv receiving task receive message from real-time message queue rtc close sending task close specified real-time channel Table 3.1: Routines constituting the real-time channel API. 3.4 Prototype Implementation We have implemented the proposed architecture using a communication executive derived from x-kernel (v3.1) <ref> [76] </ref> that exercises complete control over a 25 MHz Motorola 68040 CPU. <p> Our implementation approach is to utilize and extend the functionality and facilities provided in OSF's CORDS environment. CORDS is based on the x-kernel object-oriented networking framework originally developed at the University of Arizona <ref> [76] </ref>, with some significant extensions for controlled allocation of system resources. The primary advantage of using CORDS for our prototype implementation is the support for paths. As discussed in Section 5.4.2 below, paths play a significant role in the realization of our service architecture on this platform. <p> Another advantage of using CORDS is the ease of composing protocol stacks in the x-kernel networking framework, in which a communication subsystem is implemented as a configurable graph of protocol objects. More details on the x-kernel can be found in <ref> [76, 141] </ref>. 5.4.2 OSF Path Framework: Implications and Extensions While preserving the structure and functionality of the original x-kernel, CORDS adds two abstractions, paths and allocators, to provide path-specific reservation/allocation of system resources.
Reference: [77] <author> A. Indiresan, A. Mehra, and K. Shin, </author> <title> "Design tradeoffs in implementing real-time channels on bus-based multiprocessor hosts," </title> <type> Technical Report CSE-TR-238-95, </type> <institution> University of Michigan, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: Appropriate queueing and scheduling support is needed on network interfaces to support QoS-sensitive traffic. We have studied the implications of network adapter characteristics for real-time communication on a Fibre 32 Channel adapter manufactured by Ancor Communications <ref> [77] </ref>. <p> The Credit Net ATM network interface [101] provides appropriate buffer management and flow scheduling 39 support suitable for connections engaged in guaranteed-QoS communication. We have examined the impact of adapter characteristics on the ability to support real-time communication effectively <ref> [77] </ref>, and have explored QoS support on network adapters for point-to-point and shared networks [80]. We have designed SPIDER [51], an adapter supporting real-time communication in point-to-point networks. <p> C x includes the cost of setting up DMA transfer operations, if any. Our experience with adapter design and the implications for packet transmission time are highlighted in <ref> [77] </ref>. 61 Routines Invoked By Function Performed rtc init receiving task create local queue to receive messages rtc create sending task create real-time channel with given parameters to remote end point (queue); return channel ID rtc send sending task send message on the specified real-time channel rtc recv receiving task receive <p> This in turn involves experimentally determining the latency-throughput characteristics of the adapter. Using our implementation, a parameterization of the networking hardware available to us revealed significant performance-related deficiencies such as poor data transfer throughput and high, unpredictable packet transmission time <ref> [77] </ref>. Since these deficiences were due to adapter design, they severely limited our ability to demonstrate the capabilities of our architecture and implementation. Given our primary focus on unidirectional data transfer, it suffices to ensure that transmission of a packet of size s takes L x (s) time units.
Reference: [78] <author> A. Indiresan, A. Mehra, and K. Shin, </author> <title> "The END: An E mulated N etwork Device for evaluating adapter design," </title> <booktitle> in Proc. 3rd Intl. Workshop on Performability Modeling of Computer and Communication Systems (PMCCS3), </booktitle> <pages> pp. 90-94, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: Chapter 8 examines the issues involved in realizing such an integration. The proposed architecture can be extended in several directions. We have extended the null device into a sophisticated network device emulator, called END, providing link bandwidth management <ref> [78] </ref>. Using END, we can explore additional issues involved when interfacing to adapters with support for QoS guarantees. In addition to the nature of QoS guarantees, various alternatives for adapter support for buffer management can be explored [101, 130]. <p> The nature of adapter support depends on whether the receiving host employs interrupt mode or polled mode handling for packet input, with interrupt mode requiring comparatively less adapter intelligence. These assumptions can be validated easily using the END <ref> [78] </ref>, a sophisticated network device emulator that was evolved from the null device described in Chapter 3.
Reference: [79] <author> A. Indiresan, A. Mehra, and K. G. Shin, </author> <title> "The END: A network adapter design tool," </title> <type> RTCL Technical Report, </type> <institution> University of Michigan, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: These assumptions can be validated easily using the END [78], a sophisticated network device emulator that was evolved from the null device described in Chapter 3. We have used END as a tool to study design improvements in existing adapters <ref> [79] </ref>, and explore adapter-based strategies for receive livelock elimination [81], the details of which are beyond the scope of this dissertation.
Reference: [80] <author> A. Indiresan, A. Mehra, and K. G. Shin, </author> <title> "Exploring QoS support in adapters via an Emulated Network Device," </title> <note> Submitted for publication, </note> <month> May </month> <year> 1997. </year>
Reference-contexts: We have examined the impact of adapter characteristics on the ability to support real-time communication effectively [77], and have explored QoS support on network adapters for point-to-point and shared networks <ref> [80] </ref>. We have designed SPIDER [51], an adapter supporting real-time communication in point-to-point networks. <p> The link scheduling overhead can be reduced further by pre-selecting the packet to transmit next (thus overlapping with ongoing packet transmission) and recovering from any potential priority inversions. Moving the link scheduling function to the adapter improves performance significantly <ref> [80] </ref> by facilitating greater overlap between packet processing and packet selection, and reducing context switches and cache perturbation, thus increasing channel admissibility for both O1 and O2. 4.3.2 Estimating Wait Time To compute the total message wait time, we first consider the time spent waiting for a lower-priority handler to relinquish
Reference: [81] <author> A. Indiresan, A. Mehra, and K. G. Shin, </author> <title> "Receive livelock elimination via dynamic interrupt rate control," </title> <type> RTCL Technical Report, </type> <institution> University of Michigan, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: In LRP, an incoming packet is classified and enqueued, but not processed, until the application receives the data. We have studied adapter support for 29 receive livelock elimination using the END <ref> [81] </ref>. "Paths" through the communication subsystem: The Path abstraction realized in the Open Software Foundation's CORDS framework [172] provides a rich framework for development of real-time communication services for distributed applications. <p> These assumptions can be validated easily using the END [78], a sophisticated network device emulator that was evolved from the null device described in Chapter 3. We have used END as a tool to study design improvements in existing adapters [79], and explore adapter-based strategies for receive livelock elimination <ref> [81] </ref>, the details of which are beyond the scope of this dissertation.
Reference: [82] <author> M. R. Ito, L. Takeuchi, and G. Neufeld, </author> <title> "A multiprocessor approach for meeting the processing requirements for OSI," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 220-227, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Special-purpose designs facilitate efficient interaction with the host and the network interface unit <ref> [82, 83] </ref>, while general-purpose designs must explicitly coordinate accesses to these interfaces [14, 138, 182]. More processing power in the front-end can improve the quality of service provided to the applications, by reducing queuing delays within the communication subsystem.
Reference: [83] <author> N. Jain, M. Schwartz, and T. R. Bashkow, </author> <title> "Transport protocol processing at GBPS rates," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 188-199, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Special-purpose designs facilitate efficient interaction with the host and the network interface unit <ref> [82, 83] </ref>, while general-purpose designs must explicitly coordinate accesses to these interfaces [14, 138, 182]. More processing power in the front-end can improve the quality of service provided to the applications, by reducing queuing delays within the communication subsystem. <p> With the advent of high-speed networks, uniprocessor protocol implementations (on the host or the network interface) are often unable to keep up with, or fully utilize, the network <ref> [83] </ref>, resulting in performance bottlenecks within the communication subsystem. This performance degradation is all the more pronounced for multiprocessor hosts due to a higher rate of message generation and consumption. The pursuit of application-level gigabits-per-second has prompted recent efforts to develop faster communication subsystems through the exploitation of parallelism. <p> In horizontal architectures [182], each process implements a specific layer of a protocol graph; at most two processes can be assigned to each layer, one each for transmission and reception. In vertical process architectures <ref> [14, 76, 83] </ref>, on the other hand, processes are assigned to active entities such as connections or messages and each process implements one path through the protocol graph. This approach significantly reduces context switches and message buffering that are unavoidable in horizontal process architectures. <p> Relatively fine-grain techniques include message parallelism and packet parallelism. With message parallelism a processor is associated with each message generated or consumed at the host. Packet parallelism, on the other hand, associates a processor with each arriving or departing packet <ref> [83] </ref>. Various combinations of these approaches are also possible. Fine-grain techniques provide more scalable parallelism and hence potentially greater performance gains. In practice, though, synchronization constraints, resource contention, and load balancing requirements limit the speedups, and hence the message throughputs, observed.
Reference: [84] <author> R. Jain, </author> <title> "Congestion control in computer networks: Issues and trends," </title> <journal> IEEE Network Magazine, </journal> <pages> pp. 24-30, </pages> <month> May </month> <year> 1990. </year> <month> 226 </month>
Reference-contexts: An application transferring large amounts of data may be blocked if enough socket buffers are not available. Similarly, the amount of buffer space available is used by protocols such as TCP to invoke flow control and congestion control mechanisms <ref> [84] </ref>. Existing APIs such as BSD Sockets have traditionally provide applications no control over resource management for communication-related activities. Thus, for the application the communication subsystem is a "black box," accepting requests for data transmission or reception and informing the application upon completion.
Reference: [85] <author> S. Jamin, P. Danzig, S. Shenker, and L. Zhang, </author> <title> "A measurement-based admission con-trol algorithm for integrated services packet networks," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 2-13, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: In addition to the above services, looser forms of QoS may also be provided via services such as predictive service <ref> [85] </ref>, which performs measurement-based admission control. In the context of integrated services, the expected QoS requirements of applications and issues involved in sharing link bandwidth across multiple classes of traffic are explored in [65, 158]. <p> The feasibility of adapting frame transmission rates to the available network bandwidth was also demonstrated in [86]. For such applications it suffices to provide statistical performance bounds on end-to-end communication delay and packet loss, which could also be computed using the measured performance of the network <ref> [85] </ref>. In general, adaptive applications allow the network to trade off strict QoS guarantees for simpler network design, higher network utilization, and looser QoS guarantees supplemented with additional intelligence and resources at the senders and receivers.
Reference: [86] <author> K. Jeffay, D. L. Stone, T. Talley, and F. D. Smith, </author> <title> "Adaptive best-effort delivery of digital audio and video across packet-switched networks," </title> <booktitle> in Lecture Notes in Computer Science, </booktitle> <volume> volume 712, </volume> <pages> pp. 3-14, </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: A study on packet video transport [91] illustrated the effectiveness of using network feedback about load conditions to modulate the frame transmission rate of video sources. The feasibility of adapting frame transmission rates to the available network bandwidth was also demonstrated in <ref> [86] </ref>. For such applications it suffices to provide statistical performance bounds on end-to-end communication delay and packet loss, which could also be computed using the measured performance of the network [85].
Reference: [87] <author> M. Jones, P. Leach, Joseph Barrera III, and R. Draves, </author> <title> "Support for user-centric modular real-time resource management in the Rialto operating system," </title> <booktitle> in Proc. Intl. Workshop on Network and Operating System Support for Digital Audio and Video, </booktitle> <pages> pp. 55-65, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Nemesis [109] is a multimedia operating system design from scratch. While many aspects of operating system design have been considered, not much attention has been given to QoS issues in communication subsystem design. The Rialto operating system <ref> [87, 88] </ref> combines minimum-laxity and fair scheduling policies to realize integrated scheduling of real-time and non real-time processes. The primary unit of execution in Rialto is an activity, which is a time-constrained section of code which can adapt to overload conditions.
Reference: [88] <author> M. B. Jones, Joseph S. Barrera III, A. Forin, P. J. Leach, D. la Rosu, and M.-C. Rosu, </author> <title> "An overview of the Rialto real-time architecture," </title> <booktitle> in ACM SIGOPS European Workshop on System Support for Worldwide Applications, </booktitle> <month> September </month> <year> 1996. </year>
Reference-contexts: Nemesis [109] is a multimedia operating system design from scratch. While many aspects of operating system design have been considered, not much attention has been given to QoS issues in communication subsystem design. The Rialto operating system <ref> [87, 88] </ref> combines minimum-laxity and fair scheduling policies to realize integrated scheduling of real-time and non real-time processes. The primary unit of execution in Rialto is an activity, which is a time-constrained section of code which can adapt to overload conditions.
Reference: [89] <author> H. Kanakia, </author> <title> "Host interface architecture: Key principles," </title> <booktitle> in Proc. of the IFIP TC6 Int'l Conf. on Local Area Networks (INDOLAN), </booktitle> <pages> pp. 79-97, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: By migrating frequent and time-consuming communication functions into hardware, the available network bandwidth can be utilized effectively. Since the network adapter defines the communication primitives available to the communication software, flexible support for network I/O necessitates a careful division of functionality between the adapter and the host processor <ref> [89, 164] </ref>. Network adapters either facilitate flexibility in supporting different protocols through simple designs [46, 103, 120], or restrict flexibility, and hence improve performance, by supporting specific communication protocols and resource management strategies [37, 90].
Reference: [90] <author> H. Kanakia and D. R. Cheriton, </author> <title> "The VMP network adapter board (NAB): high-performance network communication for multiprocessors," </title> <booktitle> Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 175-187, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Network adapters either facilitate flexibility in supporting different protocols through simple designs [46, 103, 120], or restrict flexibility, and hence improve performance, by supporting specific communication protocols and resource management strategies <ref> [37, 90] </ref>. In general, the overhead of host-adapter interaction and unnecessary data movement across the system bus can limit the amount of useful concurrency between the software and hardware portions of the protocol stack, thus affecting communication subsystem performance. <p> Not only does this permit special tuning of the functionality desired and significantly reduce the number of interrupts delivered to the host, it also permits exploitation of available concurrency through pipelining of the transmission and reception datapaths <ref> [90] </ref>. More importantly, it permits management of the communication subsystem by a separate executive designed for communication-related operations, such as the x-kernel [76]. The VMP NAB [90] and Nectar CAB [120] are two examples of this functional partitioning. <p> reduce the number of interrupts delivered to the host, it also permits exploitation of available concurrency through pipelining of the transmission and reception datapaths <ref> [90] </ref>. More importantly, it permits management of the communication subsystem by a separate executive designed for communication-related operations, such as the x-kernel [76]. The VMP NAB [90] and Nectar CAB [120] are two examples of this functional partitioning.
Reference: [91] <author> H. Kanakia, P. P. Mishra, and A. Reibman, </author> <title> "An adaptive congestion control scheme for real-time packet video transport," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 20-31, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: The destination buffers serve to restore the spacing between successive data samples and buffer out variations in network delay. A study on packet video transport <ref> [91] </ref> illustrated the effectiveness of using network feedback about load conditions to modulate the frame transmission rate of video sources. The feasibility of adapting frame transmission rates to the available network bandwidth was also demonstrated in [86].
Reference: [92] <author> D. D. Kandlur, K. G. Shin, and D. Ferrari, </author> <title> "Real-time communication in multi-hop networks," </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 5, no. 10, </volume> <pages> pp. 1044-1056, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: While this architecture is applicable to a wide variety of service disciplines for providing QoS guarantees, it is validated by implementing real-time channels <ref> [63, 92] </ref>, a paradigm for guaranteed-QoS communication in packet-switched networks proposed by other researchers. <p> While the proposed architecture is applicable to other proposals for guaranteed-QoS connections [8, 188], we focus on real-time channels, a paradigm for guaranteed-QoS communication services in packet-switched networks <ref> [63, 92] </ref>. Consider the problem of servicing several guaranteed-QoS and best-effort connections engaged in network input/output at a host. <p> While the architectural mechanisms proposed in this paper are applicable to most of the proposed models, we focus on real-time channels <ref> [63, 92] </ref>, using the model proposed and analyzed in [92]. A real-time channel is a simplex, fixed-route, virtual con 50 nection between a source and destination host, with sequenced messages and associated performance guarantees on message delivery. It therefore conforms to the connection semantics mentioned earlier. <p> While the architectural mechanisms proposed in this paper are applicable to most of the proposed models, we focus on real-time channels [63, 92], using the model proposed and analyzed in <ref> [92] </ref>. A real-time channel is a simplex, fixed-route, virtual con 50 nection between a source and destination host, with sequenced messages and associated performance guarantees on message delivery. It therefore conforms to the connection semantics mentioned earlier. <p> If d is the desired end-to-end delay bound for a channel, message m i generated at the source is guaranteed to be delivered at the sink by time `(m i ) + d. See <ref> [92] </ref> for more details. Resource Management: As with other proposals for guaranteed-QoS communication [8], there are two related aspects to resource management for real-time channels: admission control and (run-time) scheduling. Admission control for real-time channels is provided by Algorithm D order [92], which uses fixed-priority scheduling for computing the worst-case delay <p> See <ref> [92] </ref> for more details. Resource Management: As with other proposals for guaranteed-QoS communication [8], there are two related aspects to resource management for real-time channels: admission control and (run-time) scheduling. Admission control for real-time channels is provided by Algorithm D order [92], which uses fixed-priority scheduling for computing the worst-case delay experienced by a channel at a link. <p> Link bandwidth is managed via multi-class non-preemptive EDF scheduling with link packet queues organized similar to CPU run queues. Link scheduling is non-work-conserving to avoid stressing resources at downstream hosts; in general, the link is allowed to "work ahead" in a limited fashion, as determined by the link horizon <ref> [92] </ref>. Overload protection Per-channel traffic enforcement is performed when new messages are inserted into the message queue, and again when packets are inserted into the link packet queues. <p> Admission control extensions for receiving hosts, including hosts engaged in simultaneous data transmission and reception, are developed subsequently. Finally, we conclude the chapter with a summary of the key contributions and implications. 4.2 Managing CPU and Link Bandwidth As mentioned earlier, Algorithm D order <ref> [92] </ref> computes the worst-case response time for a message. This response time has two components: the time spent waiting for resources and the time spent using resources. <p> The following discussion focuses on the sending host, but, much of the discussion is also applicable to the receiving host. Specific issues affecting receiving hosts are discussed in Section 4.5. The real-time channel model presented in <ref> [92] </ref> accounts for non-preemptive packet transmissions, but assumes an ideal preemption model for CPU access, i.e., the CPU can be allocated to a waiting higher-priority handler immediately at no extra cost. <p> Subsequently, throughput climbs because link utilization improves and CPU requirements continue to decrease. This effect is analyzed in Section 4.3. 4.3 Worst-Case Service and Wait Times For a channel requesting admission, D order can compute the worst-case message response time (the system time requirement in <ref> [92] </ref>) by accounting for three components: * the worst-case waiting time (T w ) due to lower-priority handlers or packets, * the worst-case service time for the message (T s ), and * the worst-case waiting time due to message arrivals on all existing higher-priority channels (T hp We show below
Reference: [93] <author> D. D. Kandlur, D. Saha, and M. Willebeek-LeMair, </author> <title> "Protocol architecture for multimedia applications over ATM networks," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 14, no. 7, </volume> <pages> pp. 1349-1359, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: Proposals to achieve high-bandwidth data transfer across protection domains (e.g., between the kernel and an application) include restricted virtual memory remapping [175], container shipping [147], and Fbufs [54]. Support for in-kernel device-to-device data transfers for multimedia applications is described in <ref> [60, 93] </ref>. An extensive taxonomy of the software and hardware tradeoffs involved in data passing and performance comparison of different buffering semantics is provided in [25], and two copy avoidance techniques evaluated in [27]. Network adapter support for checksummed, multiple-packet communication in an ATM network is explored in [26].
Reference: [94] <author> H. Kaneko, J. A. Stankovic, S. Sen, and K. Ramamritham, </author> <title> "Integrated scheduling of multimedia and hard real-time tasks," </title> <booktitle> in Proc. 17th Real-Time Systems Symposium, </booktitle> <pages> pp. 206-217, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: QoS-sensitive CPU scheduling: Recently several QoS-sensitive CPU scheduling policies have been proposed recently [66, 71, 165, 179]. Scheduling algorithms for integrated scheduling of multimedia soft real-time computation and traditional hard real-time tasks on a multiprocessor multimedia server are proposed and evaluated in <ref> [94] </ref>.
Reference: [95] <author> D. Katcher, H. Arakawa, and J. K. Strosnider, </author> <title> "Engineering and analysis of fixed priority schedulers," </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> vol. 19, no. 9, </volume> <pages> pp. 920-934, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Moreover, the issues and tradeoffs involved in realizing QoS-sensitive communication subsystems are not considered. Modeling of multimedia/real-time operating systems: A number of recent efforts have attempted to bridge the gap between theory and practice for real-time systems and multimedia computing <ref> [29, 69, 95, 98] </ref>. The admission control extensions developed in Chapter 4 are geared towards the real-time communication needs of distributed systems, and hence complement these efforts.
Reference: [96] <author> J. Kay and J. Pasquale, </author> <title> "The importance of non-data touching processing overheads in TCP/IP," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 259-268, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: These include protocol-specific optimizations for TCP [40] and UDP [146], improving data touching overheads of checksumming for UDP/IP stacks [97], and exploring the non-data touching processing and related operating system overheads in TCP/IP stacks <ref> [96] </ref>. A detailed study of factors affecting end-to-end communication latency in LAN environments is described in [170]. <p> More germane to our focus, several recent efforts have also focused on experimentally quantifying, and identifying factors that influence, protocol stack performance. A detailed study of the non-data touching processing overheads in TCP/IP protocol stacks is presented in <ref> [96] </ref>. The factors contributing to these overheads include network buffer management, protocol-specific processing, operating system functions, data structure manipulations, and error checking. This study reports an extensive breakdown of the overheads incurred at each layer of the protocol stack for a DECstation 5000/200 running the Ultrix 4.2a operating system.
Reference: [97] <author> J. Kay and J. Pasquale, </author> <title> "Measurement, analysis, and improvement of UDP/IP throughput for the DECstation 5000," </title> <booktitle> in Proc. USENIX Winter Conference, </booktitle> <pages> pp. 249-258, </pages> <month> January </month> <year> 1993. </year> <month> 227 </month>
Reference-contexts: Protocol stack optimization: A number of research efforts have focused on the optimization of protocol stack execution latency. These include protocol-specific optimizations for TCP [40] and UDP [146], improving data touching overheads of checksumming for UDP/IP stacks <ref> [97] </ref>, and exploring the non-data touching processing and related operating system overheads in TCP/IP stacks [96]. A detailed study of factors affecting end-to-end communication latency in LAN environments is described in [170]. <p> The negative impact of data-touching overheads such as checksumming has also been studied extensively, and a number of techniques devised to improve data-copying performance <ref> [1, 97] </ref>. Similarly, much attention has been focused recently on appropriate buffer management for data copy elimination [25, 27, 130]. In contrast, our goal is to explicitly account for any copying cost incurred during data movement to/from applications, and measure this cost via appropriate profiling.
Reference: [98] <author> K. A. Kettler, D. I. Katcher, and J. K. Strosnider, </author> <title> "A modeling methodology for real-time/multimedia operating systems," </title> <booktitle> in Proc. of the Real-Time Technology and Applications Symposium, </booktitle> <pages> pp. 15-26, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Moreover, the issues and tradeoffs involved in realizing QoS-sensitive communication subsystems are not considered. Modeling of multimedia/real-time operating systems: A number of recent efforts have attempted to bridge the gap between theory and practice for real-time systems and multimedia computing <ref> [29, 69, 95, 98] </ref>. The admission control extensions developed in Chapter 4 are geared towards the real-time communication needs of distributed systems, and hence complement these efforts.
Reference: [99] <author> S. Khanna, M. Sebree, and J. Zolnowsky, </author> <title> "Realtime scheduling in SunOS 5.0," </title> <booktitle> in Winter USENIX Conference, </booktitle> <pages> pp. 375-390, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: In this model, protocol processing is treated as work strictly local to each processor, resulting in an implicit sharing between the computation and communication subsystems. An alternative approach 34 treats protocol processing as global work that can be scheduled uniformly on any available processor <ref> [67, 99] </ref>; this results in explicit sharing between the two subsystems. Implications for QoS-sensitive protocol processing These approaches to exploiting communication parallelism may not suffice for QoS-sensitive protocol processing since they introduce unpredictability in the availability and allocation of processing resources, and complicate global coordination for network access. <p> Modifying an existing operating system to support multimedia and real-time applications is typically a major undertaking and prone to subtle timing issues. Commercial operating systems such as Solaris have been enhanced with some support for real-time applications, such as fixed priorities and priority inheritance <ref> [99] </ref>. However, such mechanisms have been found to be insufficient in a dynamic execution environment [104]. Extensions to Windows NT to support dynamic real-time applications are described in [162]. Support for real-time processes, admission control, and detecting and managing overrun is provided.
Reference: [100] <author> D. G. Korn, </author> <title> "Porting UNIX to windows NT," </title> <booktitle> in Proc. USENIX Winter Conference, </booktitle> <month> January </month> <year> 1997. </year>
Reference-contexts: This chapter describes the design, implementation, and evaluation of one such service for a microkernel operating system. Microkernel operating systems continue to play an important role in operating system design <ref> [45, 100] </ref>, and are being extended to support real-time and multimedia applications [162]. With the continued upsurge in the demand for networked multimedia applications on the WWW, it is important, therefore, to examine realization of QoS-sensitive communication subsystems on contemporary microkernel operating systems.
Reference: [101] <author> C. Kosak, D. Eckhardt, T. Mummert, P. Steenkiste, and A. Fisher, </author> <title> "Buffer management and flow control in the Credit Net ATM host interface," </title> <booktitle> in Proc. Conference on Local Computer Networks, </booktitle> <pages> pp. 370-378, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: It supports functions for full end-to-end multimedia communication in addition to protocol processing, and is capable of moving data directly between I/O devices and the network via device-to-device communication. The Credit Net ATM network interface <ref> [101] </ref> provides appropriate buffer management and flow scheduling 39 support suitable for connections engaged in guaranteed-QoS communication. We have examined the impact of adapter characteristics on the ability to support real-time communication effectively [77], and have explored QoS support on network adapters for point-to-point and shared networks [80]. <p> Using END, we can explore additional issues involved when interfacing to adapters with support for QoS guarantees. In addition to the nature of QoS guarantees, various alternatives for adapter support for buffer management can be explored <ref> [101, 130] </ref>. While we have focused on per-channel QoS guarantees, this architecture can be easily extended to allow aggregation of multiple connections on the QoS "pipe" provided by a channel.
Reference: [102] <author> I. Kouvelas and V. Hardman, </author> <title> "Overcoming workstation scheduling problems in a real-time audio tool," </title> <booktitle> in Proc. USENIX Winter Conference, </booktitle> <month> January </month> <year> 1997. </year>
Reference-contexts: The negative effects of the scheduling variability introduced by the UNIX operating system on audio playback is highlighted in <ref> [102] </ref>. Application-level support to adapt to such QoS variations and its realization in a real-time audio tool is also described. While we do not consider dynamic QoS negotiation and adaptation in this dissertation, most of the architectural mechanisms and enhancements can be utilized for such scenarios.
Reference: [103] <author> A. Krishnakumar and K. Sabnani, </author> <title> "VLSI implementations of communication protocols a survey," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 7, no. 7, </volume> <pages> pp. 1082-1090, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Since the network adapter defines the communication primitives available to the communication software, flexible support for network I/O necessitates a careful division of functionality between the adapter and the host processor [89, 164]. Network adapters either facilitate flexibility in supporting different protocols through simple designs <ref> [46, 103, 120] </ref>, or restrict flexibility, and hence improve performance, by supporting specific communication protocols and resource management strategies [37, 90].
Reference: [104] <author> L. Krishnamurthy, </author> <title> AQUA: An Adaptive Quality of Service Architecture for Distributed Multimedia Applications, </title> <type> PhD thesis, </type> <institution> University of Kentucky, </institution> <year> 1997. </year>
Reference-contexts: However, leaving resource management entirely up to the application may not suffice for QoS-sensitive management of resources. Alternately, applications and operating systems can be designed to cooperate with one another to perform QoS negotiation and adaptation to maximize service accessibility and continuity <ref> [2, 33, 104] </ref>. A novel way to realize application-specific policies in networking is via installing of application code directly in the kernel [64]. <p> The AQUA system <ref> [104] </ref> is one such effort which has developed QoS negotiation and adaptation support for allocation of CPU and network resources. Similarly, a QoS adaptive transport system is described in [33] that incorporates a QoS-aware API and mechanisms to assist applications to adapt to fluctuations in the delivered network QoS. <p> Commercial operating systems such as Solaris have been enhanced with some support for real-time applications, such as fixed priorities and priority inheritance [99]. However, such mechanisms have been found to be insufficient in a dynamic execution environment <ref> [104] </ref>. Extensions to Windows NT to support dynamic real-time applications are described in [162]. Support for real-time processes, admission control, and detecting and managing overrun is provided. However, as stated by the authors, they have not yet accounted for implementation overheads and constraints. <p> While we do not consider dynamic QoS negotiation and adaptation, QoS-aware APIs may also allow applications to specify a range of desired QoS levels and receive dynamic notifications regarding overload conditions <ref> [2, 104] </ref>. The communication subsystem must interface to the applications via appropriate mechanisms such as application libraries, maintain additional state regarding applications and active connections, and move data to/from applications as per the associated QoS requirements. <p> Moreover, due to the requirement of capturing constituent system overheads accurately, our on-line profiling operates at a finer time scale and may be disabled once appropriate measurements have been completed. Several other efforts have also focused on operating system support for resource monitoring and application adaptation <ref> [104, 122, 135] </ref>. Such support becomes necessary in order to accommodate inaccurate or changing estimates of application resource requirements, and is geared primary towards adaptive multimedia applications.
Reference: [105] <author> C.-G. Lee, J. Hahn, Y.-M. Seo, S. L. Min, R. Ha, S. Hong, C. Y. Park, M. Lee, and C. S. Kim, </author> <title> "Analysis of cache-related preemption delay in fixed-priority preemptive scheduling," </title> <booktitle> in Proc. 17th Real-Time Systems Symposium, </booktitle> <pages> pp. 264-274, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: This is particularly true for preemption caused by external events such as network interrupts. One can account for the cache miss penalty due to preemption via careful schedulability analysis <ref> [105] </ref>, but frequent preemption still degrades available CPU capacity, as also observed in [69]. Moreover, an important implication of arbitrary preemption for the proposed architecture is that a handler may get preempted just before initiating transmission, even though it had finished preparing a packet for transmission, thus idling the link.
Reference: [106] <author> C. Lee, R. Rajkumar, and C. Mercer, </author> <title> "Experiences with processor reservation and dynamic QOS in Real-Time Mach," </title> <booktitle> in Proc. of Multimedia Japan, </booktitle> <month> March </month> <year> 1996. </year>
Reference-contexts: Reserves are independent of application threads and can account for execution of a thread in multiple protection domains. While capacity reserves are useful for applications with a pri 40 ori knowledge resource requirements, they are also useful in environments requiring dynamic QoS control <ref> [106] </ref>. Virtual memory management for interactive multimedia applications has also been realized in RT-Mach [136]. Modifying an existing operating system to support multimedia and real-time applications is typically a major undertaking and prone to subtle timing issues.
Reference: [107] <author> C. Lee, K. Yoshida, C. Mercer, and R. Rajkumar, </author> <title> "Predictable communication protocol processing in Real-Time Mach," </title> <booktitle> in Proc. Real-Time Technology and Applications Symposium, </booktitle> <pages> pp. 220-229, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: More recently, processor capacity reserves in RT-Mach [123] have been combined with user-level protocol processing [112] for predictable protocol processing inside hosts <ref> [107] </ref>. Operating system support for multimedia communication is explored in [73], where the focus is on provision of preemption points and EDF scheduling in the kernel, and in [177], which also focuses on the scheduling architecture. <p> For end-to-end guarantees, resource management within the communication subsystem must be integrated with that for applications. The architecture proposed and analyzed in this chapter is directly applicable if a portion of the host processing capacity can be reserved for communication-related activities <ref> [107, 123] </ref>. The proposed architectural extensions can be realized as a server with appropriate capacity reserves and/or execution priority. Our prototype implementation is indeed such a server executing in a standalone configuration. More importantly, our approach decouples protocol processing priority from that of the application. <p> While we only consider management of communication resources, the present work can be extended to incorporate application scheduling as well. Our analysis is directly applicable if a portion of the host processing capacity can be reserved for communication-related activities, e.g., via capacity reserves <ref> [107, 123] </ref>. Chapter 8 discusses applicability of these admission control extensions to application-level framing and user-level protocol processing architectures. The rest of the chapter is organized as follows. <p> Once developed and debugged, the server can be placed within the kernel to improve performance and hence relax conservative cost estimates. We note that a server-based configuration is regarded as a reasonable approach to build predictable communication services on microkernel operating systems <ref> [107] </ref>. Chapter ?? compares and contrasts various protocol processing architectures regarding the extent to which they facilitate per-connection QoS guarantees. The functionality provided by our CORDS-based server cannot be truly effective without additional support from the underlying operating system. <p> This is because since protocol processing is performed, and data transmission initiated, during the application's currently allocated quantum, the overlap between protocol processing and link transmission still exists and can be exploited. Further, on data reception, the communication threads in the application library are immediately scheduled for execution <ref> [107] </ref>; thus, the overlap between data reception and protocol processing can be exploited as well with appropriate scheduling of these communication threads. <p> Note that a Type III protocol processing architecture can be used in conjunction with processor capacity reserves [123] to realize predictable protocol processing <ref> [107] </ref>. Unlike our approach, the approach outlined in [107] does not derive the protocol processing priority from a connection's QoS and traffic specifications, nor does it exploit the overlap between 211 protocol processing and link transmission/reception. <p> Note that a Type III protocol processing architecture can be used in conjunction with processor capacity reserves [123] to realize predictable protocol processing <ref> [107] </ref>. Unlike our approach, the approach outlined in [107] does not derive the protocol processing priority from a connection's QoS and traffic specifications, nor does it exploit the overlap between 211 protocol processing and link transmission/reception.
Reference: [108] <author> S. J. Le*er, M. K. McKusick, M. J. Karels, and J. S. Quarterman, </author> <title> The Design and Implementation of the 4.3BSD Unix Operating System, </title> <publisher> Addison Wesley, </publisher> <month> May </month> <year> 1989. </year>
Reference-contexts: J. Watson Research Center, we have developed an RSVP-based QoS architecture for TCP/IP protocol stacks supporting an integrated services Internet [13]. This architecture represents a major functional enhancement to the traditional sockets-based communication subsystem <ref> [108] </ref>. One of the key features of this architecture is the transparent accommodation of network interfaces with differing QoS capabilities, ranging from traditional LANs, such as Token Ring, to ATM networks with a high degree of QoS support. <p> We discuss each of the above-mentioned factors in detail next. 2.3.1 API Semantics The location and data passing semantics [25] of the API affects communication performance in a variety of ways. Typically the API is implemented by the kernel (e.g. BSD Sockets <ref> [108] </ref>), or by a trusted server at user level, and the overhead of crossing protection domains (e.g., the user-kernel boundary) during data transfer degrades communication throughput. <p> Useful insights into the subtle effects of multiprogramming and network input load on packet reception time are provided by Danzig [47]. He constructs a performance model of protocol processing in UNIX <ref> [108] </ref> to capture the effects of multiprogramming and limited socket buffers. The model accounts for protocol processing overhead and the variability introduced by interrupt servicing, scheduling latency, and memory contention, in the time taken by a destination process to receive arriving packets. <p> The bottom half interfaces to RTCOP for signalling (i.e., connection setup and teardown), and to CLIPS for QoS-sensitive data transfer. As described in Section 5.3, the design of the API has been significantly influenced by the structure of the sockets API in BSD Unix <ref> [108] </ref> and its variants. Signalling via RTCOP: End-to-end signalling and resource reservation is performed by RTCOP to establish and teardown guaranteed-QoS connections across the communicating hosts, possibly via multiple network nodes. <p> Architectural Overview Table 8.1 lists the protocol processing architectures proposed in the literature. Type I: corresponds to traditional in-kernel communication subsystem present in BSD Unix <ref> [108] </ref> and its derivatives. Protocol processing in this architecture is performed at system call time or subsequently in response to a network device or timer interrupt. The architectural enhancement described in Chapter 7 were applied to a Type I architecture.
Reference: [109] <author> I. Leslie, D. McAuley, R. Black, T. Roscoe, P. Barham, D. Evers, R. Faribairns, and E. Hyden, </author> <title> "The design and implementation of an operating system to support distributed multimedia applications," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 14, no. 7, </volume> <pages> pp. 1280-1297, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: The application-level and kernel-level schedulers communication with each other via shared memory. Support for continuous media in the Chorus microkernel is described in [42] which utilizes the IPC and scheduling techniques proposed in [70]. Nemesis <ref> [109] </ref> is a multimedia operating system design from scratch. While many aspects of operating system design have been considered, not much attention has been given to QoS issues in communication subsystem design.
Reference: [110] <author> J. Liedtke, H. Hartig, and M. Hohmuth, </author> <title> "OS-controlled cache predictability for real-time systems," </title> <booktitle> in Proc. Real-Time Technology and Applications Symposium, </booktitle> <pages> pp. 213-223, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Not only does this result in loss of CPU capacity to unnecessary context switches, it also increases the likelihood of disturbing the footprint in the cache [124], unless the cache is suitably partitioned <ref> [110] </ref>. This is particularly true for preemption caused by external events such as network interrupts. One can account for the cache miss penalty due to preemption via careful schedulability analysis [105], but frequent preemption still degrades available CPU capacity, as also observed in [69]. <p> It has been shown that the cache behavior of network protocols is protocol-specific and that cache misses play a significant role in protocol stack execution latency [16, 133]. For partitioned caches, cache behavior can be made more predictable under control of the operating system <ref> [110] </ref>. We examine some of the issues involved and the difficulty of accurate parameterization in Chapter 6. 4.4 Channel Admissibility In this section, we demonstrate that the tradeoff between resource capacity and channel admissibility is influenced significantly by P, the number of packets between preemptions, and S, the packet size. <p> This has significant implications for system parameterization since it highlights the difficulty in measuring various processing overheads accurately. Cache predictability may be improved via appropriate protocol implementation and compilation techniques [129], or via cache partitioning and appropriate OS support <ref> [110] </ref>. Any worst-case processing estimates are likely to be overly conservative. We note that this problem relates to memory subsystem design for modern processors, and is not related to the actual mechanism employed to profile communication subsystems.
Reference: [111] <author> C. Liu and J. Layland, </author> <title> "Scheduling algorithms for multiprogramming in hard real-time environment," </title> <journal> Journal of the ACM, </journal> <volume> vol. 1, no. 20, </volume> <pages> pp. 46-61, </pages> <month> January </month> <year> 1973. </year>
Reference-contexts: Moreover, no performance impact of supporting QoS in communication is reported. Real-time upcalls (RTUs) [68] are a mechanism to schedule protocol processing for net-worked multimedia applications via event-based upcalls [38]. Protocol processing activities are scheduled via an extended version of the rate monotonic (RM) scheduling policy <ref> [111] </ref>. Similar to our approach, delayed preemption adopted to reduce the number of context switches. Our approach differs from RTUs in that we use thread-based execution model for protocol processing, schedule threads via a modified earliest-deadline-first (EDF) policy [111], and extend resource management policies within the communication subsystem 38 to account <p> scheduled via an extended version of the rate monotonic (RM) scheduling policy <ref> [111] </ref>. Similar to our approach, delayed preemption adopted to reduce the number of context switches. Our approach differs from RTUs in that we use thread-based execution model for protocol processing, schedule threads via a modified earliest-deadline-first (EDF) policy [111], and extend resource management policies within the communication subsystem 38 to account for a number of implementation overheads and constraints. Similar to our approach, rate-based flow control of multimedia streams via kernel-based communication threads is also proposed in [185]. <p> Commercial and research operating systems: Govindan and Anderson [70] first proposed scheduling and IPC mechanisms for operating systems supporting continuous media. Key abstractions proposed and implemented were a split-level scheduling architecture featuring an EDF <ref> [111] </ref> kernel scheduler and an application-level scheduler for efficient real-time scheduling. The application-level and kernel-level schedulers communication with each other via shared memory. Support for continuous media in the Chorus microkernel is described in [42] which utilizes the IPC and scheduling techniques proposed in [70]. <p> The total end-to-end delay, however, can exceed the message inter-arrival time of the channel. Contrary to the approach for admission control, run-time link scheduling is governed by a variation of the multi-class earliest-deadline-first (EDF) policy <ref> [111] </ref>. The above approach only accounts for management of link bandwidth at the host. <p> Similarly, real-time operating systems typically provide support for periodic fixed-priority and dynamic priority CPU scheduling, such as rate monotonic (RM) and earliest deadline first (EDF), respectively <ref> [111] </ref>. While such QoS-sensitive CPU scheduling policies suffice for computation activities, the granularity of the CPU scheduling quantum may be too coarse for accurate scheduling and traffic shaping of application data exporter threads.
Reference: [112] <author> C. Maeda and B. N. Bershad, </author> <title> "Protocol service decomposition for high-performance networking," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <pages> pp. 244-255, </pages> <month> December </month> <year> 1993. </year> <month> 228 </month>
Reference-contexts: Splitting the API into data transfer and control parts, with the data transfer part implemented as a library at user level, was shown to improve throughput substantially <ref> [112] </ref>. The amount of buffering available at the API also affects the protocol dynamics of end-to-end protocols. An application transferring large amounts of data may be blocked if enough socket buffers are not available. <p> Several recent efforts have also focused on optimizing the protocol processing latency in TCP/IP protocol stacks [16, 129, 176]. User-level protocol processing: Several research efforts have focused on increasing communication subsystem throughput via user-level handling of network data <ref> [57, 112, 167] </ref>. In 28 addition to data copy minimization compared to a server-based implementation, user-level protocol processing offers significant flexibility in developing and debugging communication protocols. <p> OS support for QoS-sensitive communication: The need for scheduling protocol processing at priority levels consistent with those of the communicating application was highlighted in [6], and some implementation strategies demonstrated in [70]. More recently, processor capacity reserves in RT-Mach [123] have been combined with user-level protocol processing <ref> [112] </ref> for predictable protocol processing inside hosts [107]. Operating system support for multimedia communication is explored in [73], where the focus is on provision of preemption points and EDF scheduling in the kernel, and in [177], which also focuses on the scheduling architecture. <p> We note that the proposed architecture and the admission control extensions described in Chapter 4 are also applicable to application-level framing [41] and user-level protocol processing architectures explored in recent efforts <ref> [55, 112, 167] </ref> to improve data transfer throughput in high-speed networks. In our design approach we have not made any specific assumptions about the location of the protocol stack, which could reside in the kernel or in user space. <p> These three components together ensure QoS-sensitive handling of network traffic at sending and receiving hosts. We realize this service architecture as a user-level CORDS server running on the OSF MK 7.2 microkernel. Even though server-based protocol stacks perform poorly compared to user-level protocol libraries or in-kernel implementations <ref> [112, 168] </ref>, we choose a server configuration for several reasons. A server configuration considerably eases software development and debugging, particularly the location and correction of timing-related bugs. <p> The final reason is related to performance. A server-based implementation is natural for a microkernel operating system, but may perform poorly compared to user-level protocol libraries due to excessive data copying and context switching <ref> [112, 168] </ref>. As mentioned in Section 5.1, it seems appropriate to be conservative when building a guaranteed-QoS communication service. It follows that in the worst case, compared to user-level libraries a server configuration only suffers from additional context switches. <p> Protocol processing is thus triggered via IPC between the application and the trusted server on the one hand, and between the server and the kernel on the other. Type III: also corresponds to a user-level protocol processing configuration, on a micro-kernel operating system <ref> [112, 168] </ref> or otherwise [55], with the difference that data transmission and reception is performed by threads executing in communication subsystem libraries linked with the application; control operations such as setting up connections are performed by the user-level trusted server or the kernel.
Reference: [113] <author> B. D. Marsh, T. J. LeBlanc, M. L. Scott, and E. P. Markatos, </author> <title> "First-class user-level threads," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <pages> pp. 110-121, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In addition, this allows protocol processing to be scheduled via thread scheduling schemes distinct from those developed for application threads, especially for multimedia applications [6, 70]. In recent years, resource management for computation has moved towards giving more control to the applications, e.g., user-level threads <ref> [113] </ref>, scheduler activations [7], and application-kernel coordination to dynamically vary the degree of active parallelism in scientific computing applications [174].
Reference: [114] <author> S. McCanne and V. Jacobson, </author> <title> "The BSD packet filter: A new architecture for user-level packet capture," </title> <booktitle> in Proc. USENIX Winter Conference, </booktitle> <pages> pp. 259-269, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: They were first proposed primarily to enable user-level network capture for developing new protocols without kernel modifications [127], but are increasingly being utilized for realizing protocols such as UDP and TCP as user-level libraries. Successive implementations of packet filters have reduced classification overhead significantly, e.g., BPF <ref> [114] </ref>, MPF [186], and PATHFINDER [11]. While all of these filter designs perform classification via interpretation, more recently dynamic code generation techniques have been applied to realize very efficient packet filters, as in DPF [59]. Packet filters are necessary for packet classification in connectionless networks, such as the IPv4-based Internet.
Reference: [115] <author> L. McVoy and C. Staelin, "lmbench: </author> <title> Portable tools for performance analysis," </title> <booktitle> in Proc. USENIX Winter Conference, </booktitle> <pages> pp. 279-295, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Over and above the fixed overhead, the time spent in this routine increases with message size. This is because, as mentioned in Section 5.5, RTC API ANCHOR copies application data into path-specific message buffers in order to preserve application data integrity in the worst case. For our platform, lmbench <ref> [115] </ref> reports a memory copy bandwidth of 40 MB/second; the actual copy bandwidth would be somewhat lower due to the overheads imposed by the x-kernel copy routine. In any case, the increase in C a can be largely attributed to the time to copy in application data. <p> The first such study focused on the reasons behind the failure of operating system performance improvements to track performance improvements in hardware technology [142]. More recently, efforts have focused on developing a suite of portable operating system benchmarks for cross-platform performance comparisons <ref> [115] </ref> as well as detailed system analysis [24]. In all these efforts, however, the performance profiling is geared towards performance comparisons and the impact of OS-hardware interactions on operating system primitives.
Reference: [116] <author> A. Mehra, A. Indiresan, and K. Shin, </author> <title> "Resource management for real-time communication: Making theory meet practice," </title> <booktitle> in Proc. of 2nd Real-Time Technology and Applications Symposium, </booktitle> <pages> pp. 130-138, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Protocol processing can be work-conserving or non-work-conserving, with best-effort traffic given processing and transmission priority over "work ahead" real-time traffic. The architectural framework adopted utilizes an abstraction of the underlying communication subsystem in terms of various processing costs, overheads, and policies, which are used for admission control <ref> [116] </ref> and run-time resource management. We have developed a prototype implementation of the proposed architecture using a communication executive derived from x-kernel 3.1 [76] that exercises complete control over a Motorola 68040 CPU. <p> Contrary to the approach for admission control, run-time link scheduling is governed by a variation of the multi-class earliest-deadline-first (EDF) policy [111]. The above approach only accounts for management of link bandwidth at the host. As shown in <ref> [116] </ref>, and discussed in the next chapter, it cannot be applied directly to CPU bandwidth management. 3.2.3 Performance Related Considerations To provide deterministic QoS guarantees on communication, all processing costs and overheads involved in managing and using resources must be accounted for. <p> multi-class earliest-deadline-first Handler execution cooperative preemption with configurable number of packets between preemptions Link scheduling multi-class EDF (options 1, 2 and 3) Overload protection block handler, decay handler deadline, enforce I min , drop overflow messages Table 3.2: Available policies in the prototype implementation. available resource capacity and channel admissibility <ref> [116] </ref>, as discussed in the next chapter. To use the model of packet transmission time presented in Section 3.3.2, C x and B x must be determined for a given network adapter and host architecture. This in turn involves experimentally determining the latency-throughput characteristics of the adapter. <p> This suggests that the data exporter threads, i.e., those performing fragmentation and other processing of network data, be scheduled for execution using fine-grain preemption. 205 Such fine-grain multiplexing of communication threads has been adopted and analyzed in [68, 69] for RM scheduling, and in <ref> [116, 117] </ref> for EDF scheduling, respectively. The architecture described in [116], which was presented in Chapter 3 decouples protocol processing priority from application priority, deriving the former from the traffic and QoS specification on each connection. Accurate traffic shaping is realized via EDF scheduling of protocol processing threads. <p> The architecture described in <ref> [116] </ref>, which was presented in Chapter 3 decouples protocol processing priority from application priority, deriving the former from the traffic and QoS specification on each connection. Accurate traffic shaping is realized via EDF scheduling of protocol processing threads. <p> Potential load-dependent variations in the actual shaping latency can be accommodated via appropriate adaptation and 206 buffer management, or largely eliminated via integration with QoS-sensitive CPU scheduling policies. Our work complements recent work on QoS-sensitive CPU scheduling of applications [71, 165, 179] and protocol processing <ref> [68, 69, 116, 117] </ref> at end hosts. While these efforts focus on CPU scheduling, our primary focus is on the QoS support architecture exported to sockets based applications. With appropriate CPU scheduling support, our QoS architecture enables new and legacy applications to utilize end-to-end QoS on communication.
Reference: [117] <author> A. Mehra, A. Indiresan, and K. Shin, </author> <title> "Structuring communication software for quality of service guarantees," </title> <booktitle> in Proc. 17th Real-Time Systems Symposium, </booktitle> <pages> pp. 144-154, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: Selecting a packet for transmission incurs some overhead in addition to that of initiating transmission on the link. Additional overhead may be involved if the link scheduler must transfer packets between link packet queues <ref> [117] </ref>. In O1, the scheduler is frequently invoked from the interrupt service routine (ISR) announcing completion of packet transmission. <p> As part of handling the interrupt, the ISR examines packet headers (possibly via a packet filter) to classify the packet, i.e., determine the channel it corresponds to, and enqueues the packet in the channel's input queue for subsequent processing by the corresponding channel handler <ref> [117] </ref>. Note that the handler performs message reassembly after processing the last packet of a message, and enqueues the message in the channel message queue for subsequent 98 retrieval by the application. <p> This suggests that the data exporter threads, i.e., those performing fragmentation and other processing of network data, be scheduled for execution using fine-grain preemption. 205 Such fine-grain multiplexing of communication threads has been adopted and analyzed in [68, 69] for RM scheduling, and in <ref> [116, 117] </ref> for EDF scheduling, respectively. The architecture described in [116], which was presented in Chapter 3 decouples protocol processing priority from application priority, deriving the former from the traffic and QoS specification on each connection. Accurate traffic shaping is realized via EDF scheduling of protocol processing threads. <p> Potential load-dependent variations in the actual shaping latency can be accommodated via appropriate adaptation and 206 buffer management, or largely eliminated via integration with QoS-sensitive CPU scheduling policies. Our work complements recent work on QoS-sensitive CPU scheduling of applications [71, 165, 179] and protocol processing <ref> [68, 69, 116, 117] </ref> at end hosts. While these efforts focus on CPU scheduling, our primary focus is on the QoS support architecture exported to sockets based applications. With appropriate CPU scheduling support, our QoS architecture enables new and legacy applications to utilize end-to-end QoS on communication.
Reference: [118] <author> A. Mehra, J. Rexford, H.-S. Ang, and F. Jahanian, </author> <title> "Design and implementation of a window-consistent replication service," </title> <booktitle> in Proc. Real-Time Technology and Applications Symposium, </booktitle> <pages> pp. 182-191, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Accordingly, the service described in this chapter would be utilized by one or more middleware services such as real-time caching and real-time primary-backup replication <ref> [118] </ref>. In the context of the ARMADA project, numerous research avenues are being explored towards extending the guaranteed-QoS communication service to other models of real-time communication, including QoS negotiation and adaptation. Acknowledgements Several individuals have contributed to the realization of the guaranteed-QoS communication service described in this chapter.
Reference: [119] <author> A. Mehra and K. Shin, </author> <title> "QoS-sensitive protocol processing in shared-memory multiprocessor multimedia servers," </title> <booktitle> in Proc. of 3rd IEEE Workshop on Architecture and Implementation of High-Performance Communication Subsystems, </booktitle> <pages> pp. 163-169, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Accordingly, the process-per-connection model, with appropriate extensions, seems the most suitable candidate for QoS-sensitive protocol processing. We have proposed an architecture for QoS-sensitive protocol processing on SMMPs that maps and schedules protocol processing for different guaranteed-QoS connections on a dedicated set of protocol processors <ref> [119] </ref>, with each connection handler mapped to exactly one processor. Our approach of statically partitioning the processing resources is similar to multiprocessor front-ends [138], except that a set of processors within the host are dedicated for protocol processing and communication-related functions, as in [14]. <p> Various elements of this architecture can also be utilized to realize statistical real-time channels [63]. Statistical QoS guarantees can potentially be useful to a large class of distributed multimedia applications. Finally, we have extended this architecture to realize a QoS-sensitive communication subsystem for shared-memory multiprocessor multimedia servers <ref> [119] </ref>. However, numerous issues involved in managing parallelism while providing QoS guarantees need to be explored. 76 CHAPTER 4 ADMISSION CONTROL EXTENSIONS FOR END HOSTS 4.1 Introduction The previous chapter focused on architectural components within the communication subsystem to realize QoS guarantees.
Reference: [120] <author> O. Menzilcioglu and S. Schlick, "Nectar CAB: </author> <title> A high-speed network processor," </title> <booktitle> in Proc. Int'l Conf. on Distributed Computing Systems, </booktitle> <pages> pp. 508-515, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Since the network adapter defines the communication primitives available to the communication software, flexible support for network I/O necessitates a careful division of functionality between the adapter and the host processor [89, 164]. Network adapters either facilitate flexibility in supporting different protocols through simple designs <ref> [46, 103, 120] </ref>, or restrict flexibility, and hence improve performance, by supporting specific communication protocols and resource management strategies [37, 90]. <p> More importantly, it permits management of the communication subsystem by a separate executive designed for communication-related operations, such as the x-kernel [76]. The VMP NAB [90] and Nectar CAB <ref> [120] </ref> are two examples of this functional partitioning. <p> More importantly, it permits management of the communication subsystem by a separate executive designed for communication-related operations, such as the x-kernel [76]. The VMP NAB [90] and Nectar CAB [120] are two examples of this functional partitioning. For example, the Nectar CAB <ref> [10, 120] </ref> off-loads all protocol processing functions from the host processor, freeing it from adapter handshake overheads and permitting greater overlap between useful computation and communication processing. 31 Multiprocessor front-ends: Multiprocessor front-ends may be designed using special--purpose or general-purpose hardware.
Reference: [121] <author> C. W. Mercer and H. Tokuda, </author> <booktitle> "Preemptibility in real-time operating systems," in Proc. Real-Time Systems Symposium, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: The admission control extensions developed in Chapter 4 are geared towards the real-time communication needs of distributed systems, and hence complement these efforts. The implications of priority inversion due to non-preemptible critical sections was studied in <ref> [121] </ref>; however, preemption costs such as context switches and cache misses, and the resulting degradation in useful resource capacity, were not considered. We take these costs into account when developing the admission control extensions described in Chapter 4.
Reference: [122] <author> C. W. Mercer and R. Rajkumar, </author> <title> "An interactive interface and RT-Mach support for monitoring and controlling resource management," </title> <booktitle> in Proc. Real-Time Technology and Applications Symposium, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Moreover, due to the requirement of capturing constituent system overheads accurately, our on-line profiling operates at a finer time scale and may be disabled once appropriate measurements have been completed. Several other efforts have also focused on operating system support for resource monitoring and application adaptation <ref> [104, 122, 135] </ref>. Such support becomes necessary in order to accommodate inaccurate or changing estimates of application resource requirements, and is geared primary towards adaptive multimedia applications.
Reference: [123] <author> C. W. Mercer, S. Savage, and H. Tokuda, </author> <title> "Processor capacity reserves for multimedia operating systems," </title> <booktitle> in Proc. of the IEEE International Conference on Multimedia Computing and Systems, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: The focus of Rialto is primarily on QoS-sensitive application scheduling, and issues in QoS-sensitive communication subsystem design are not considered. Real-Time Mach (RT-Mach) [171] provides processor capacity reserves <ref> [123] </ref>, a mechanism to allow applications to reserve processing capacity for execution of its threads. Reserves are independent of application threads and can account for execution of a thread in multiple protection domains. <p> OS support for QoS-sensitive communication: The need for scheduling protocol processing at priority levels consistent with those of the communicating application was highlighted in [6], and some implementation strategies demonstrated in [70]. More recently, processor capacity reserves in RT-Mach <ref> [123] </ref> have been combined with user-level protocol processing [112] for predictable protocol processing inside hosts [107]. <p> For end-to-end guarantees, resource management within the communication subsystem must be integrated with that for applications. The architecture proposed and analyzed in this chapter is directly applicable if a portion of the host processing capacity can be reserved for communication-related activities <ref> [107, 123] </ref>. The proposed architectural extensions can be realized as a server with appropriate capacity reserves and/or execution priority. Our prototype implementation is indeed such a server executing in a standalone configuration. More importantly, our approach decouples protocol processing priority from that of the application. <p> While we only consider management of communication resources, the present work can be extended to incorporate application scheduling as well. Our analysis is directly applicable if a portion of the host processing capacity can be reserved for communication-related activities, e.g., via capacity reserves <ref> [107, 123] </ref>. Chapter 8 discusses applicability of these admission control extensions to application-level framing and user-level protocol processing architectures. The rest of the chapter is organized as follows. <p> The functionality provided by our CORDS-based server cannot be truly effective without additional support from the underlying operating system. Such support could be in the form of capacity reserves for the service <ref> [123] </ref> or appropriate system partitioning [19], and would require proper integration of the QoS-sensitive communication subsystem with the operating system for provision of application-level QoS guarantees. <p> Even with LRP, the architectural mechanisms provided by our architecture would be required to determine the order in which the application processes data received on multiple sockets. Note that a Type III protocol processing architecture can be used in conjunction with processor capacity reserves <ref> [123] </ref> to realize predictable protocol processing [107]. Unlike our approach, the approach outlined in [107] does not derive the protocol processing priority from a connection's QoS and traffic specifications, nor does it exploit the overlap between 211 protocol processing and link transmission/reception. <p> There are two approaches to capacity allocation between the two subsystems. In one approach, a portion of the host processing capacity can be reserved for the entire communication subsystem via capacity reserves <ref> [123] </ref>. Note that the reserves model proposed in [123] associates reserves with individual threads and allows multiple threads to subscribe to the same reserve. <p> There are two approaches to capacity allocation between the two subsystems. In one approach, a portion of the host processing capacity can be reserved for the entire communication subsystem via capacity reserves <ref> [123] </ref>. Note that the reserves model proposed in [123] associates reserves with individual threads and allows multiple threads to subscribe to the same reserve.
Reference: [124] <author> J. Mogul and A. Borg, </author> <title> "The effect of context switches on cache performance," </title> <booktitle> in Proc. Int'l Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 75-85, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Further, since the communication subsystem typically shares processing resources with application threads, additional scheduling and context switching overheads, and associated cache misses <ref> [124] </ref>, may be incurred. Additional sources of overhead include buffer management, timer management, and error-recovery mechanisms such as retransmissions. As highlighted in the remaining chapters of this dissertation, support for QoS-sensitive handling of data imposes new overheads and demands on communication subsystem performance. <p> This is because preemption due to unrelated, even lower priority, activities can occur frequently and at arbitrary instants. Not only does this result in loss of CPU capacity to unnecessary context switches, it also increases the likelihood of disturbing the footprint in the cache <ref> [124] </ref>, unless the cache is suitably partitioned [110]. This is particularly true for preemption caused by external events such as network interrupts.
Reference: [125] <author> J. Mogul and K. K. Ramakrishnan, </author> <title> "Eliminating receive livelock in an interrupt-driven kernel," </title> <booktitle> in Winter USENIX Conference, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: Receive livelock elimination: Besides optimizing the receive data path through the communication subsystem, efforts have also been made to address another key problem associated with data reception, namely, receive livelock [151]. Receive livelock has been addressed at length in <ref> [125] </ref> via a combination of techniques (such as limiting interrupt arrival rates, fair polling, processing packets to completion, and regulating CPU usage for protocol processing) to avoid receive livelock and maintain system throughput near the maximum system input capacity under high load. <p> Polled-mode packet input, an alternative mechanism in which the host periodically polls the network adapter for arrived packets, can be effective in preventing receive livelock <ref> [125] </ref>. Packets can also be input using a hybrid mechanism combining interrupts (under low network input load) with polling (under high network input load). <p> Third, a persistent burst of packet arrivals at the receiver can result in receive live-lock <ref> [125, 151] </ref>, as explained in Chapter 2. Receive livelock can occur even in a point-to-point network with QoS-sensitive forwarding, especially when there is low real-time traffic (which is subjected to admission control) but high, persistent best-effort traffic. <p> Polled mode handling is an alternative to interrupt mode handling that can be effective in preventing receive livelock <ref> [125] </ref>. Fourth, in order to keep the analysis tractable, we must make additional assumptions about the support available in the adapter for QoS guarantees, as outlined in the discussion below. <p> Polling, which is sometimes preferred over interrupts [173], helps amortize interrupt overhead over multiple packets per polling cycle at the expense of increased packet reception latency. As mentioned before, polling can be effective in preventing receive livelock <ref> [125] </ref>. We assume that polling is realized via clocked interrupts [173] such that a polling timer expires periodically, and at each polling instant inputs up to a certain number of packets (the quota in [125]). <p> As mentioned before, polling can be effective in preventing receive livelock <ref> [125] </ref>. We assume that polling is realized via clocked interrupts [173] such that a polling timer expires periodically, and at each polling instant inputs up to a certain number of packets (the quota in [125]). The quota is determined by the host processing capacity allocated to servicing the polling timer and inputing packets. We focus below on admission control extensions when packet reception is performed purely via clocked interrupts with quotas. <p> We focus below on admission control extensions when packet reception is performed purely via clocked interrupts with quotas. Our analysis can be easily extended to the case where polling is performed by an explicitly-scheduled polling thread, as described in <ref> [125] </ref>. Suppose the host configures the polling timer to expire every I poll time units, and the quota is q packets; that is, each time the polling timer expires, up to q packets are input from the network adapter.
Reference: [126] <author> J. C. Mogul, </author> <title> "Network locality at the scale of processes," </title> <journal> ACM Trans. Computer Systems, </journal> <volume> vol. 10, no. 2, </volume> <pages> pp. 81-109, </pages> <month> May </month> <year> 1992. </year> <month> 229 </month>
Reference-contexts: While we describe possible approaches to QoS-sensitive CPU scheduling in Section 2.5, here we describe the results from an interesting study done by Mogul <ref> [126] </ref>. He argues that communication delays in a multiprogrammed system may be reduced through "dallying".
Reference: [127] <author> J. C. Mogul, R. F. Rashid, and M. J. Accetta, </author> <title> "The packet filter: An efficient mecha-nism for user-level network code," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <pages> pp. 39-51, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Packet classification: Packet filters provide general and flexible classification (i.e., demul-tiplexing) of incoming packets to application end-points, at the lowest layer of the protocol stack. They were first proposed primarily to enable user-level network capture for developing new protocols without kernel modifications <ref> [127] </ref>, but are increasingly being utilized for realizing protocols such as UDP and TCP as user-level libraries. Successive implementations of packet filters have reduced classification overhead significantly, e.g., BPF [114], MPF [186], and PATHFINDER [11].
Reference: [128] <author> D. Mosberger and L. L. Peterson, </author> <title> "Making paths explicit in the Scout operating system," </title> <booktitle> in Proc. USENIX Symp. on Operating Systems Design and Implementation, </booktitle> <pages> pp. 153-167, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Similarly, the Scout operating system from the University of Arizona proposes the use of explicit paths as an important abstraction in operating system design to improve performance <ref> [128] </ref>. However, while paths in [172] are envisioned primarily as a static, relatively coarse-grain mechanism, paths in [128] are not associated with communication resources and assigned deadlines or priorities via admission control. <p> Similarly, the Scout operating system from the University of Arizona proposes the use of explicit paths as an important abstraction in operating system design to improve performance <ref> [128] </ref>. However, while paths in [172] are envisioned primarily as a static, relatively coarse-grain mechanism, paths in [128] are not associated with communication resources and assigned deadlines or priorities via admission control. <p> Moreover, these optimizations continue to be applicable in our proposed QoS-sensitive architecture as well as the enhancements made to TCP/IP stacks for integrated services. In constrast, we examine the overheads imposed by new data-handling components in the protocol stack. We adapt and extend the notion of paths in <ref> [128, 172] </ref> to obtain QoS guarantees and communication resources via admission control, and perform fine-grain path-based resource management to maintain QoS guarantees. While packet filters were designed primarily to classify incoming packets, they can also be used to classify outgoing traffic at network routers. <p> Similar to our approach, arriving messages are not processed in interrupt context, but processed by a thread that is in turn scheduled for execution. However, FLIPC does not provide any explicit QoS guarantees on the end-to-end message delivery. The CORDS path abstraction [172], which is similar to Scout paths <ref> [128] </ref>, provides a rich framework for development of real-time communication services for distributed applications, as demonstrated with the guaranteed-QoS communication service in Chapter 5. <p> Demultiplex-ing incoming packets early and absorbing bursts in distinct per-connection queues is an attractive way to prevent receive livelock, an observation also made in the context of paths in Scout <ref> [128] </ref>. Our architectural approach facilitates provision of QoS guarantees while preventing receive livelock. Network adapter design: The MNI [17] is a network interface unit designed specifically for continuous media applications.
Reference: [129] <author> D. Mosberger, L. L. Peterson, P. G. Bridges, and S. O'Malley, </author> <title> "Analysis of techniques to improve protocol processing latency," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 73-84, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Several recent efforts have also focused on optimizing the protocol processing latency in TCP/IP protocol stacks <ref> [16, 129, 176] </ref>. User-level protocol processing: Several research efforts have focused on increasing communication subsystem throughput via user-level handling of network data [57, 112, 167]. In 28 addition to data copy minimization compared to a server-based implementation, user-level protocol processing offers significant flexibility in developing and debugging communication protocols. <p> This has significant implications for system parameterization since it highlights the difficulty in measuring various processing overheads accurately. Cache predictability may be improved via appropriate protocol implementation and compilation techniques <ref> [129] </ref>, or via cache partitioning and appropriate OS support [110]. Any worst-case processing estimates are likely to be overly conservative. We note that this problem relates to memory subsystem design for modern processors, and is not related to the actual mechanism employed to profile communication subsystems. <p> The scalability of the QoS architecture is, therefore, contingent in part upon the accuracy with which an application specifies its run-time communication behavior. Protocol stack performance and optimizations of existing implementations has been the 184 subject of numerous research articles, including some very recent ones <ref> [16, 129, 176] </ref>. How--ever, all of these studies focus on the traditional best-effort data path. Our study assumes significance in that it quantifies the performance penalty imposed by new data-handling components in the protocol stack, and their impact on the best-effort data path.
Reference: [130] <author> B. Murphy, S. Zeadally, and C. J. Adams, </author> <title> "An analysis of process and memory models to support high-speed networking in a UNIX environment," </title> <booktitle> in Proc. USENIX Winter Conference, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: Using END, we can explore additional issues involved when interfacing to adapters with support for QoS guarantees. In addition to the nature of QoS guarantees, various alternatives for adapter support for buffer management can be explored <ref> [101, 130] </ref>. While we have focused on per-channel QoS guarantees, this architecture can be easily extended to allow aggregation of multiple connections on the QoS "pipe" provided by a channel. <p> We note that while the RTC API ANCHOR overheads are relatively high, these measurements are for an unoptimized implementation and can be improved substantially with careful performance optimizations. With appropriate buffer management and API buffering semantics <ref> [25, 130] </ref> it may even be possible to completely eliminate the copying of data within RTC API ANCHOR. However, more immediately we are concerned with ensuring that the overheads incurred in RTC API ANCHOR do not result in QoS-insensitive handling of data. <p> The negative impact of data-touching overheads such as checksumming has also been studied extensively, and a number of techniques devised to improve data-copying performance [1, 97]. Similarly, much attention has been focused recently on appropriate buffer management for data copy elimination <ref> [25, 27, 130] </ref>. In contrast, our goal is to explicitly account for any copying cost incurred during data movement to/from applications, and measure this cost via appropriate profiling.
Reference: [131] <author> K. Nahrstedt and J. M. Smith, </author> <title> "The QoS broker," </title> <journal> IEEE Multimedia, </journal> <volume> vol. 2, no. 1, </volume> <pages> pp. 53-67, </pages> <month> Spring </month> <year> 1995. </year>
Reference-contexts: QoS architectures: The OMEGA [132] end point architecture provides support for end-to-end QoS guarantees. In this architecture, application QoS requirements are translated to network QoS requirements by the QoS Broker <ref> [131] </ref>, which negotiates for the necessary 37 host and network resources. The OMEGA approach assumes appropriate support from the operating system for QoS-sensitive application execution, and the network subsystem for provision of transport-to-transport layer guarantees.
Reference: [132] <author> K. Nahrstedt and J. M. Smith, </author> <title> "Design, implementation and experiences of the OMEGA end-point architecture," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 14, no. 7, </volume> <pages> pp. 1263-1279, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: We note that the architectural approach, mechanisms, and extensions developed in this dissertation are applicable to unicast as well as multicast sessions, for both sender-initiated and receiver-initiated signalling. QoS architectures: The OMEGA <ref> [132] </ref> end point architecture provides support for end-to-end QoS guarantees. In this architecture, application QoS requirements are translated to network QoS requirements by the QoS Broker [131], which negotiates for the necessary 37 host and network resources.
Reference: [133] <author> E. Nahum, D. Yates, J. Kurose, and D. Towsley, </author> <title> "Cache behavior of network protocols," </title> <booktitle> in Proc. of ACM SIGMETRICS, </booktitle> <pages> pp. 169-180, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: While data copying primarily impacts data transfer throughput, protocol processing latency is significantly affected by the complex, protocol-specific cache behavior of network protocols <ref> [16, 133] </ref>. Finally, protocol stack execution performance is significantly affected by operating system overheads such as context switching and device interrupts [163]. <p> More refined experiments are necessary to select accurate values for C p , C sw and C cm . It has been shown that the cache behavior of network protocols is protocol-specific and that cache misses play a significant role in protocol stack execution latency <ref> [16, 133] </ref>. For partitioned caches, cache behavior can be made more predictable under control of the operating system [110]. <p> Recall that the output communication handler fragments packets and shepherds them down the protocol stack in a single loop. The difference in overhead between the first and other packets can be partly attributed to cache effects, which have been shown to affect protocol stack execution latency significantly <ref> [16, 133] </ref>. For the receive path, on the other hand, we distinguish between the last packet of a message and the other packets. <p> A faster CPU or higher memory copy bandwidth would reduce protocol processing latency. Similarly, different processor and cache architectures would generate different context switching overheads and cache miss penalties, respectively. The cache performance of communication subsystems is also determined in large part by the composition of the protocol stack <ref> [133] </ref>. The I/O bus bandwidth in part determines the available DMA bandwidth to/from system memory. Since this affects the time spent moving data between memory buffers and the network interface, it also determines, along with link bandwidth, the transmission time for outgoing packets. <p> A study of the cache behavior of network protocols such as TCP and UDP reports widely variable effects on processing latency, depending on whether the cache is cold or hot <ref> [133] </ref>. Similarly, the importance of cache performance for small messages such as those 163 found in typical signalling protocols is highlighted in [16]. This has significant implications for system parameterization since it highlights the difficulty in measuring various processing overheads accurately.
Reference: [134] <author> E. M. Nahum, D. J. Yates, J. F. Kurose, and D. Towsley, </author> <title> "Performance issues in parallelized network protocols," </title> <booktitle> in Proc. USENIX Symp. on Operating Systems Design and Implementation, </booktitle> <pages> pp. 125-137, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Increased parallelism is likely to be a feature of the next generation of communication protocols [150, 191]. Recent results have shown that connectional parallelism delivers comparatively more scalable performance than message parallelism <ref> [134, 153, 154] </ref>. Protocol processing in SMMPs: Similar problems arise when implementing protocol processing in shared-memory multiprocessors (SMMPs), where the approaches adopted essentially lie on two extremes. In one approach, each processor executing a process also performs protocol processing for messages transmitted by that process [169]. <p> We note that at least two other efforts have employed such artificial sources and sinks of data, namely, 68 the virtual network device in [14] that resides on a separate processor, and the "in-memory" network device used in <ref> [134] </ref>. 3.5 Experimental Evaluation We evaluate the efficacy of the proposed architecture in isolating real-time channels from each other and from best-effort traffic. The evaluation is conducted for a subset of the policies listed in Table 3.2, under varying degrees of traffic load and traffic specification violations.
Reference: [135] <author> T. Nakajima, </author> <title> "A dynamic QOS control based on optimistic processor reservation," </title> <booktitle> in Proc. Intl. Conf. on Multimedia Computing and Systems, </booktitle> <year> 1996. </year>
Reference-contexts: A scheme for adaptive rate-controlled scheduling is presented in [184]. A dynamic QoS control scheme using optimistic processor reservation and application feedback is described in <ref> [135] </ref>. This system, which is implemented in RT-Mach [171], reserves processing capacity based on average processing usage of applications and provides notifications to applications to adjust QoS levels. <p> Moreover, due to the requirement of capturing constituent system overheads accurately, our on-line profiling operates at a finer time scale and may be disabled once appropriate measurements have been completed. Several other efforts have also focused on operating system support for resource monitoring and application adaptation <ref> [104, 122, 135] </ref>. Such support becomes necessary in order to accommodate inaccurate or changing estimates of application resource requirements, and is geared primary towards adaptive multimedia applications.
Reference: [136] <author> T. Nakajima and H. Tezuka, </author> <title> "Virtual memory management for interactive continuous media applications," </title> <booktitle> in Proc. Intl. Conf. on Multimedia Computing and Systems, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: While capacity reserves are useful for applications with a pri 40 ori knowledge resource requirements, they are also useful in environments requiring dynamic QoS control [106]. Virtual memory management for interactive multimedia applications has also been realized in RT-Mach <ref> [136] </ref>. Modifying an existing operating system to support multimedia and real-time applications is typically a major undertaking and prone to subtle timing issues. Commercial operating systems such as Solaris have been enhanced with some support for real-time applications, such as fixed priorities and priority inheritance [99].
Reference: [137] <author> Netperf Homepage. </author> <note> http://www.cup.hp.com/netperf/NetperfPage.html. </note>
Reference-contexts: These experiments stress the functional aspects of the system by exercising the various data paths through the QOSMGR. For our experiments we have extended the netperf <ref> [137] </ref> program to interact with the QOSMGR and create QoS sessions. We have 193 instrumented netperf to permit the creation of sessions with different options for local traffic control collect user-level statistics for packet transmission time.
Reference: [138] <author> A. N. Netravali, W. D. Roome, and K. Sabnani, </author> <title> "Design and implementation of a high-speed transport protocol," </title> <journal> IEEE Trans. Communications, </journal> <volume> vol. 38, no. 11, </volume> <pages> pp. 2010-2024, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Special-purpose designs facilitate efficient interaction with the host and the network interface unit [82, 83], while general-purpose designs must explicitly coordinate accesses to these interfaces <ref> [14, 138, 182] </ref>. More processing power in the front-end can improve the quality of service provided to the applications, by reducing queuing delays within the communication subsystem. Network interfaces for multicomputers: Network interface design for multicomputer environments presents unique opportunities and cost-performance tradeoffs. <p> Our approach of statically partitioning the processing resources is similar to multiprocessor front-ends <ref> [138] </ref>, except that a set of processors within the host are dedicated for protocol processing and communication-related functions, as in [14].
Reference: [139] <author> J. Nieh and M. S. Lam, </author> <title> "The design, implementation and evaluation of SMART: A scheduler for multimedia applications," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <month> October </month> <year> 1997. </year>
Reference-contexts: Another approach is to realize the entire communication subsystem as a class with an appropriate share in the recently proposed QoS-sensitive scheduling policies <ref> [71, 139, 165, 179] </ref>. These policies ensure that threads execute on the CPU in the order of their associated 214 QoS requirements and at the granularity of a certain quantum. Each thread executes for at most a quantum each time it is selected to run.
Reference: [140] <author> E. Nordmark and D. R. Cheriton, </author> <title> "Experiences from VMTP: How to achieve low response time," in Protocols for High-Speed Networks, </title> <editor> H. Rudin and R. Williamson, </editor> <booktitle> editors, </booktitle> <pages> pp. 43-54, </pages> <publisher> North-Holland, </publisher> <year> 1989. </year>
Reference-contexts: This model has been used in a tool to compare different protocols and protocol implementations for varying levels of background load. In particular, it has been used to evaluate the performance of VMTP <ref> [36, 140] </ref>, FTP and SunRPC [72], and to compare the performance of OSI TP4 and TCP [160]. This tool also allows specification of variable message sizes and the delay between successive messages sent on a given connection.
Reference: [141] <author> S. W. O'Malley and L. L. Peterson, </author> <title> "A dynamic network architecture," </title> <journal> ACM Trans. Computer Systems, </journal> <volume> vol. 10, no. 2, </volume> <pages> pp. 110-143, </pages> <month> May </month> <year> 1992. </year> <month> 230 </month>
Reference-contexts: Fine-grain techniques provide more scalable parallelism and hence potentially greater performance gains. In practice, though, synchronization constraints, resource contention, and load balancing requirements limit the speedups, and hence the message throughputs, observed. Overheads of managing parallelism: The techniques used in [14] to parallelize x-kernel <ref> [76, 141] </ref> and the protocols, including the locking mechanisms used and dedication of special x-kernel functions to specific processors, suggest that special synchronization paradigms and processor allocation mechanisms are needed to manage communication parallelism effectively. <p> Another advantage of using CORDS is the ease of composing protocol stacks in the x-kernel networking framework, in which a communication subsystem is implemented as a configurable graph of protocol objects. More details on the x-kernel can be found in <ref> [76, 141] </ref>. 5.4.2 OSF Path Framework: Implications and Extensions While preserving the structure and functionality of the original x-kernel, CORDS adds two abstractions, paths and allocators, to provide path-specific reservation/allocation of system resources.
Reference: [142] <author> J. K. Ousterhout, </author> <title> "Why aren't operating systems getting faster as fast as hardware?," </title> <booktitle> in Summer USENIX Conference, </booktitle> <pages> pp. 1-10, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Excessive data copying has been identified as one of the main reasons behind the relatively smaller improvements in the performance of networking software compared to the advances made in networking hardware <ref> [142] </ref>. While data copying primarily impacts data transfer throughput, protocol processing latency is significantly affected by the complex, protocol-specific cache behavior of network protocols [16, 133]. Finally, protocol stack execution performance is significantly affected by operating system overheads such as context switching and device interrupts [163]. <p> The first such study focused on the reasons behind the failure of operating system performance improvements to track performance improvements in hardware technology <ref> [142] </ref>. More recently, efforts have focused on developing a suite of portable operating system benchmarks for cross-platform performance comparisons [115] as well as detailed system analysis [24]. In all these efforts, however, the performance profiling is geared towards performance comparisons and the impact of OS-hardware interactions on operating system primitives.
Reference: [143] <author> C. Papadopoulos and G. M. Parulkar, </author> <title> "Experimental evaluation of SUNOS IPC and TCP/IP protocol implementation," </title> <journal> IEEE/ACM Trans. Networking, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 199-216, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: The effect of background Ethernet load on TCP and UDP performance is studied in [21]. More recently, the effect of background load on the performance of SunOS inter-process communication (IPC) mechanisms and TCP/IP protocol stack is studied in <ref> [143] </ref>, for an artificial CPU-intensive workload as well as a real distributed application.
Reference: [144] <author> A. K. Parekh and R. G. Gallager, </author> <title> "A generalized processor sharing approach to flow control in integrated services networks the single node case," </title> <booktitle> in Proc. IEEE INFO-COM, </booktitle> <pages> pp. 915-924, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Example service disciplines include Weighted Fair Queueing [50], also known as Packet-by-Packet Generalized Processor Sharing <ref> [144] </ref>, and Rate-Controlled Static-Priority Queueing [187]. In addition to servicing (QoS-sensitive) traffic according to the associated QoS guarantees, the service discipline also determines the treatment of best-effort traffic at the node. Note that the service discipline may also determine the admission control procedure employed at the node.
Reference: [145] <author> C. Partridge, </author> <title> Gigabit Networking, </title> <publisher> Addison Wesley, </publisher> <address> One Jacob Way, Reading, Mas-sachusetts 01867, </address> <year> 1994. </year>
Reference-contexts: Networked multimedia applications integrating various media such as audio, video, and text, exhibit a wide range of QoS requirements on communication [62]. Examples of such applications include high-definition television, medical imaging, scientific visualization, full-motion video, multiparty video conferencing, and collaborative workspace <ref> [35, 145, 148] </ref>. The QoS requirements of multimedia applications differ significantly from those of tra 17 ditional data transfer applications such as remote login, electronic mail, and file transfer. <p> For example, full-motion video requires guaranteed high bandwidth which may be highly variable, but may tolerate limited amounts of packet loss; audio, on the other hand, requires low delay jitter and is sensitive to packet loss <ref> [145] </ref>. Similarly, multiparty video-conferencing and collaborative workspace applications require multicast capabilities with temporal synchronization between the audio and video streams; this necessitates additional inter-stream QoS dependencies in addition to the above-mentioned intra-stream QoS requirements. <p> By providing additional buffers and adaptively controlling the rate at which packets are consumed at the receiver, these 18 applications can adapt to existing network conditions <ref> [8, 39, 145] </ref>. The destination buffers serve to restore the spacing between successive data samples and buffer out variations in network delay. A study on packet video transport [91] illustrated the effectiveness of using network feedback about load conditions to modulate the frame transmission rate of video sources. <p> For example, for adaptive applications that can adjust to existing network load conditions, the delivered QoS may determine the amount of buffering needed for acceptable performance. Recent studies using voice conferencing indicate that the delivered QoS determines the degree and exact nature of adaptivity required in the application <ref> [145] </ref>. The controlled load service [183] being standardized by the IETF for an integrated services Internet is intended for such adaptive real-time applications. 2.2 Communication Subsystem Overview Before discussing the factors affecting communication subsystem performance, we present a brief overview of typical host communication subsystems. <p> With slower networks, packets (or messages) were generated and consumed at a slower rate, and therefore the host was generally not the bottleneck. Gigabit networks have shifted the bottleneck from the transmission media to the end hosts <ref> [145, 161] </ref>. For the designers of the communication subsystem, therefore, the primary challenge has been realization of high-throughput and low-latency communication.
Reference: [146] <author> C. Partridge and S. Pink, </author> <title> "A faster UDP," </title> <journal> IEEE/ACM Trans. Networking, </journal> <volume> vol. 1, no. 4, </volume> <pages> pp. 429-440, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Network adapter support for checksummed, multiple-packet communication in an ATM network is explored in [26]. Protocol stack optimization: A number of research efforts have focused on the optimization of protocol stack execution latency. These include protocol-specific optimizations for TCP [40] and UDP <ref> [146] </ref>, improving data touching overheads of checksumming for UDP/IP stacks [97], and exploring the non-data touching processing and related operating system overheads in TCP/IP stacks [96]. A detailed study of factors affecting end-to-end communication latency in LAN environments is described in [170].
Reference: [147] <author> J. Pasquale, E. Anderson, and P. Muller, </author> <title> "Container shipping: Operating system support for I/O-intensive applications," </title> <journal> IEEE Computer, </journal> <volume> vol. 27, no. 3, </volume> <pages> pp. 84-93, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Proposals to achieve high-bandwidth data transfer across protection domains (e.g., between the kernel and an application) include restricted virtual memory remapping [175], container shipping <ref> [147] </ref>, and Fbufs [54]. Support for in-kernel device-to-device data transfers for multimedia applications is described in [60, 93]. An extensive taxonomy of the software and hardware tradeoffs involved in data passing and performance comparison of different buffering semantics is provided in [25], and two copy avoidance techniques evaluated in [27].
Reference: [148] <editor> B. Pehrson, P. Gunningberg, and S. Pink, </editor> <title> "Distributed multimedia applications on gigabit networks," </title> <journal> IEEE Network Magazine, </journal> <pages> pp. 26-35, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Networked multimedia applications integrating various media such as audio, video, and text, exhibit a wide range of QoS requirements on communication [62]. Examples of such applications include high-definition television, medical imaging, scientific visualization, full-motion video, multiparty video conferencing, and collaborative workspace <ref> [35, 145, 148] </ref>. The QoS requirements of multimedia applications differ significantly from those of tra 17 ditional data transfer applications such as remote login, electronic mail, and file transfer.
Reference: [149] <author> M. Perez, F. Liaw, A. Mankin, E. Hoffman, D. Grossman, and A. Malis, </author> <title> "ATM signaling support for IP over ATM," Request for Comments RFC 1755, </title> <month> February </month> <year> 1995. </year> <note> ISI, Fore, Motoral Codex, Ascom Timeplex. </note>
Reference-contexts: Towards that end, the IETF is developing a set of protocols and standards for integrated services <ref> [20, 65, 149, 189] </ref>. In the IETF's vision, applications request and reserve resources, both in the network and at the attached hosts (clients or servers) using an end-to-end receiver-initiated Resource ReSerVation Protocol (RSVP) [23, 189]. <p> In contrast to RSVP, which initiates reservation setup at the receiver, an alternative approach 6 to signalling is adopted by the ST-II protocol [49], which initiates reservation setup at the sender. The issues involved in providing QoS support in IP-over-ATM networks <ref> [149] </ref> are also being explored [20]. 1.2 Dissertation Focus and Problem Statement As mentioned, the primary focus of this dissertation is host communication subsystem design to request and obtain per-connection QoS, assuming availability of appropriate network support.
Reference: [150] <author> T. F. L. Porta and M. Schwartz, </author> <title> "The MultiStream Protocol: A highly flexible high-speed transport protocol," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 4, </volume> <pages> pp. 519-530, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Large connection state and the requirement of in-order delivery of packets to applications are two examples of features that limit performance in parallel implementations. Increased parallelism is likely to be a feature of the next generation of communication protocols <ref> [150, 191] </ref>. Recent results have shown that connectional parallelism delivers comparatively more scalable performance than message parallelism [134, 153, 154]. Protocol processing in SMMPs: Similar problems arise when implementing protocol processing in shared-memory multiprocessors (SMMPs), where the approaches adopted essentially lie on two extremes.
Reference: [151] <author> K. K. Ramakrishnan, </author> <title> "Performance considerations in designing network interfaces," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 203-219, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The number of context switches incurred during protocol processing are a function of the protocol architecture employed [152], while the number of interrupts is a function of the design of the network adapter and the packet input mechanism adopted <ref> [151, 173] </ref>. Further, since the communication subsystem typically shares processing resources with application threads, additional scheduling and context switching overheads, and associated cache misses [124], may be incurred. Additional sources of overhead include buffer management, timer management, and error-recovery mechanisms such as retransmissions. <p> For example, the receiving host may be unable to keep up with the packet arrival rate, which is now higher because of a bursty sender. A persistent burst of packet arrivals at the receiver can result in receive livelock <ref> [151] </ref>. Receive livelock is a phenomenon in which, under network input overload, a host or router is swamped with processing and discarding arriving packets to the extent that the effective throughput of the system falls to zero. We highlight strategies to prevent or eliminate receive livelock in Section 2.4. <p> Further, they may also be needed in native ATM networks that aggregate multiple end-to-end connections over virtual circuits [11]. Receive livelock elimination: Besides optimizing the receive data path through the communication subsystem, efforts have also been made to address another key problem associated with data reception, namely, receive livelock <ref> [151] </ref>. <p> The design of high-speed network adapters, their performance characteristics, and implications for protocol stacks in uniprocessor workstation environments has received significant attention recently, for FDDI <ref> [151] </ref> as well as ATM [48, 173] networks. Uniprocessor front-ends and network adapters: With dedicated network front-ends, the network adapter operates under control of a front-end communication processor which handles all communication-related functions including network interrupts. <p> In addition, the mechanism adopted for packet input must also be considered, since it affects performance and hence channel admissibility significantly. In interrupt-mode packet input, the adapter interrupts the host on each packet arrival. This may result in excessive overheads and leave the host susceptible to receive livelock <ref> [151] </ref>, a scenario in which the host is continuously receiving and discarding arriving packets (due to queue overflow, say) such that effective system throughput falls to zero. <p> Third, a persistent burst of packet arrivals at the receiver can result in receive live-lock <ref> [125, 151] </ref>, as explained in Chapter 2. Receive livelock can occur even in a point-to-point network with QoS-sensitive forwarding, especially when there is low real-time traffic (which is subjected to admission control) but high, persistent best-effort traffic.
Reference: [152] <author> D. C. Schmidt and T. Suda, </author> <title> "Transport system architecture services for high-performance communications systems," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 4, </volume> <pages> pp. 489-506, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: During protocol processing and data transfer, the protocols may invoke various operating system resource management functions such as timer services, buffer management, and device management (i.e., interrupt servicing and low-level device operations), as shown in Figure 2.2. Depending upon the protocol architecture, kernel architecture, and process architecture <ref> [152] </ref>, other resource management functions such as process management and scheduling may also occur during data transfer. On multiprocessor hosts, processor allocation may also occur in addition to the above. <p> On multiprocessor hosts, processor allocation may also occur in addition to the above. An extensive survey of transport system architecture services and implementation strategies in a variety of operating systems such as System V UNIX, BSD UNIX, x-kernel, and Choices is presented in <ref> [152] </ref>. 20 2.3 Factors Affecting Performance As mentioned, communication subsystems provide applications with a range of communication services through a variety of communication protocols, management of the necessary communication resources, and interaction with network devices for data transmission and reception. <p> Finally, protocol stack execution performance is significantly affected by operating system overheads such as context switching and device interrupts [163]. The number of context switches incurred during protocol processing are a function of the protocol architecture employed <ref> [152] </ref>, while the number of interrupts is a function of the design of the network adapter and the packet input mechanism adopted [151, 173]. Further, since the communication subsystem typically shares processing resources with application threads, additional scheduling and context switching overheads, and associated cache misses [124], may be incurred. <p> of data-copying within the communication subsystem and across the API [52, 181], * optimization of protocol implementations, such as hand-optimized critical paths [40], 27 and integrated layer processing [1, 41], * appropriate network interface design for high performance [161], and, * exploitation of concurrency and parallelism within the communication subsystem <ref> [152] </ref>. We discuss each of these techniques and their compatibility with, and implications for, QoS-sensitive communication subsystem design. 2.4.1 Performance optimizations for efficient data transfer A wide variety of performance optimizations have been applied to communication subsystems. The most significant of these are briefly described below. <p> In general, communication subsystems employ one or more processes (or threads) to implement a protocol graph [76]. Depending upon the allocation of work to these processes, communication subsystems can be classified into horizontal or vertical process architectures <ref> [152] </ref>. In horizontal architectures [182], each process implements a specific layer of a protocol graph; at most two processes can be assigned to each layer, one each for transmission and reception. <p> This approach significantly reduces context switches and message buffering that are unavoidable in horizontal process architectures. Grains of parallelism: Several grains of parallelism can be exploited within both these approaches <ref> [152] </ref>. These grains roughly correspond to the mapping between the process architecture and the available processing resources. Finer grains of parallelism can be exploited for vertical process architectures. A relatively coarse-grain technique, connectional 33 parallelism, associates a processor with each active connection. <p> In the next chapter we study the extent to which these factors affect admissibility in the context of real-time channels. 3.3 QoS-Sensitive Communication Subsystem Architecture In the process-per-message model <ref> [152] </ref>, a process or thread shepherds a message through the protocol stack. Besides eliminating extraneous context switches encountered in the process-per-protocol model [152], it also facilitates protocol processing to be scheduled according to a variety of policies, as opposed to the software-interrupt level processing in BSD Unix. <p> the next chapter we study the extent to which these factors affect admissibility in the context of real-time channels. 3.3 QoS-Sensitive Communication Subsystem Architecture In the process-per-message model <ref> [152] </ref>, a process or thread shepherds a message through the protocol stack. Besides eliminating extraneous context switches encountered in the process-per-protocol model [152], it also facilitates protocol processing to be scheduled according to a variety of policies, as opposed to the software-interrupt level processing in BSD Unix. However, the process-per-message model introduces additional complexity for supporting per-channel QoS guarantees. <p> Since QoS guarantees are specified on a per-channel basis, it suffices to have a single thread coordinate access to resources for all messages on a given channel. We employ a process-per-channel model, which is a QoS-sensitive extension of the process-per-connection model <ref> [152] </ref>. In the process-per-channel model, protocol processing on each channel is coordinated by a unique channel handler , a lightweight thread created on successful establishment of the channel.
Reference: [153] <author> D. C. Schmidt and T. Suda, </author> <title> "Measuring the performance of parallel message-based process architectures," </title> <booktitle> in Proc. IEEE INFOCOM, </booktitle> <pages> pp. 624-633, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Increased parallelism is likely to be a feature of the next generation of communication protocols [150, 191]. Recent results have shown that connectional parallelism delivers comparatively more scalable performance than message parallelism <ref> [134, 153, 154] </ref>. Protocol processing in SMMPs: Similar problems arise when implementing protocol processing in shared-memory multiprocessors (SMMPs), where the approaches adopted essentially lie on two extremes. In one approach, each processor executing a process also performs protocol processing for messages transmitted by that process [169].
Reference: [154] <author> D. C. Schmidt and T. Suda, </author> <title> "The performance of alternative threading architectures for parallel communication subystems," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1997. To appear. </note>
Reference-contexts: Increased parallelism is likely to be a feature of the next generation of communication protocols [150, 191]. Recent results have shown that connectional parallelism delivers comparatively more scalable performance than message parallelism <ref> [134, 153, 154] </ref>. Protocol processing in SMMPs: Similar problems arise when implementing protocol processing in shared-memory multiprocessors (SMMPs), where the approaches adopted essentially lie on two extremes. In one approach, each processor executing a process also performs protocol processing for messages transmitted by that process [169].
Reference: [155] <author> H. Schulzrinne, S. Casner, R. Frederick, and V. Jacobson, "RTP: </author> <title> A transport protocol for real-time applications," Request for Comments RFC 1889, </title> <month> January </month> <year> 1996. </year> <note> GMD/Precept Software/PARC/LBNL. </note>
Reference-contexts: Note that in our architecture, applications output packets to the socket layer. Thus, our architecture is naturally suited to techniques such as application-level framing [41] and can be used with protocols such as RTP <ref> [155] </ref>. For example, it supports user-level fragmentation and protocol processing performed in the application's address space. Accordingly, it is well-suited to multi-threaded multimedia applications that have distinct data generator and data exporter threads.
Reference: [156] <author> H. Schulzrinne, J. Kurose, and D. Towsley, </author> <title> "An evaluation of scheduling mechanisms for providing best-effort, real-time communication in wide-area networks," </title> <booktitle> in Proc. IEEE INFOCOM, </booktitle> <month> June </month> <year> 1994. </year> <month> 231 </month>
Reference-contexts: Finally, instead of deterministic or probabilistic guarantees, even looser forms of QoS may be provisioned for applications that can adapt to variations in the delivered QoS. Proposals for predictive (or best-effort) real-time communication, such as FIFO+ [39] and Hop-Laxity <ref> [156] </ref>, fall in this category. 1.1.2 Integrated Services on the Internet Significant efforts are being made by the IETF to enhance the service model of the Internet to support integrated services for voice, video, and data transport [22, 39].
Reference: [157] <author> M. Seltzer and C. </author> <title> Small, "Self-monitoring and self-adapting operating systems," </title> <booktitle> in Proc. Workshop on HoTOS, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: As such, there is no need to equip the operating system with information regarding the performance of individual components and primitives, nor is the profiling geared towards on-line system parameterization. Self-parameterizing protocol stacks come closest in flavor to the idea of self-monitoring and self-adapting operating systems <ref> [157] </ref>, although the two differ greatly in goals, scope and the approach adopted. It is proposed in [157] to perform continuous monitoring of operating system activity to construct a database of performance statistics, classify this data appropriately, and perform off-line analysis to construct a characterization of the system under normal behavior <p> Self-parameterizing protocol stacks come closest in flavor to the idea of self-monitoring and self-adapting operating systems <ref> [157] </ref>, although the two differ greatly in goals, scope and the approach adopted. It is proposed in [157] to perform continuous monitoring of operating system activity to construct a database of performance statistics, classify this data appropriately, and perform off-line analysis to construct a characterization of the system under normal behavior and detect anomalous behavior.
Reference: [158] <author> S. Shenker, D. Clark, and L. Zhang, </author> <title> "A scheduling service model and a scheduling architecture for an integrated services packet network," </title> <note> Working Paper, August 1993. Xerox PARC. </note>
Reference-contexts: In the context of integrated services, the expected QoS requirements of applications and issues involved in sharing link bandwidth across multiple classes of traffic are explored in <ref> [65, 158] </ref>. Much support being provided on the Internet is geared towards realizing efficient multicast communication and accommodating the heterogeneity of receivers. RSVP, in particular, has been designed to support multicast communication between heterogeneous receivers.
Reference: [159] <author> S. Shenker, C. Partridge, and R. Guerin, </author> <title> "Specification of guaranteed quality of service," Internet Draft draft-ietf-intserv-guaranteed-svc-05.txt, </title> <month> June </month> <year> 1996. </year> <month> Xe-rox/BBN/IBM. </month>
Reference-contexts: Resource management is performed via per-flow traffic shaping and scheduling for various classes of service [22], such as guaranteed service <ref> [159] </ref> that provides guaranteed delay, and controlled load service [183] that has more relaxed QoS requirements. The guaranteed service is intended for applications requiring firm guarantees on loss-less on-time datagram delivery. <p> The above-mentioned extensions are applicable to other proposals for real-time communication and QoS guarantees [8, 188], and to other host and network architectures. In particular, Internet servers running TCP/IP protocol stacks supporting integrated services, especially the guaranteed service class <ref> [159] </ref>, can also benefit from these extensions. <p> Prior resource reservation with specification of the desired QoS guarantee and traffic characteristics is required to isolate the needed resources for these applications. The guaranteed service <ref> [159] </ref> being standardized by the IETF for an integrated services Internet supports deterministic guarantees on loss-less on-time data delivery. <p> Further, the proposed admission control extensions are general and applicable to other host and network architectures. In particular, Internet servers running TCP/IP protocol stacks supporting Integrated Services [22, 65], especially the guaranteed service class <ref> [159] </ref>, can also benefit from these extensions. Similarly, Internet routers can apply these extensions when incoming packets must 78 be fragmented before forwarding in order to reconcile the different MTUs of the attached networks. <p> A complete description of RSVP is provided in [23], while details on different service classes can be found in <ref> [159, 183] </ref>. 7.2.1 RSVP: An End-to-End View run RSVP daemons that exchange RSVP messages (PATH and RESV) on behalf of their hosts. <p> Honoring the reservations requires, among other things, resource and traffic management at the hosts and routers. The resource and traffic management mechanisms depend heavily on the service classes supported. 7.2.2 Service Classes Two important service classes currently under standardization by IETF are (i) guaranteed service <ref> [159] </ref>, and (ii) controlled load [183] service. Guaranteed services is targeted at provid 186 ing applications with a mathematically provable end-to-end delay bound using appropriate buffer and bandwidth reservation at all network elements. <p> The sender still overruns the receiver since the excess best-effort traffic is being sent to the receiver over the default best-effort VC. Experiment IV: In this experiment, both traffic policing and traffic shaping are enabled in the QOSMGR. This corresponds to the recommended behavior for the guaranteed service <ref> [159] </ref>, and may also be applied to the controlled load service. In this case, the QOSMGR performs packet level policing and blocks the application thread whenever it tries to transmit data in excess of the associated reservation.
Reference: [160] <author> P. Sjodin, P. Gunningberg, E. Nordmark, and S. Pink, </author> <title> "Towards protocol benchmarks," in Protocols for High-Speed Networks, </title> <editor> H. Rudin and R. Williamson, </editor> <booktitle> editors, </booktitle> <pages> pp. 57-67, </pages> <publisher> North-Holland, </publisher> <year> 1989. </year>
Reference-contexts: In contrast, we propose that traditional protocol stacks be extended to dynamically determine the performance of various components and maintain this information in terms of well-defined system parameters. 6.3.2 Protocol Benchmarking The notion of protocol benchmarks was proposed in <ref> [160] </ref> for a comparative evaluation of different implementations of communication protocols and protocol stacks. To capture the communication behavior of real applications, a three-level model is proposed that comprises basic operations, basic applications, and compound applications. <p> This model has been used in a tool to compare different protocols and protocol implementations for varying levels of background load. In particular, it has been used to evaluate the performance of VMTP [36, 140], FTP and SunRPC [72], and to compare the performance of OSI TP4 and TCP <ref> [160] </ref>. This tool also allows specification of variable message sizes and the delay between successive messages sent on a given connection. The primary focus of such protocol benchmarks and the above-mentioned tool is to compare the end-to-end performance of different protocols.
Reference: [161] <author> J. M. Smith and C. B. S. Traw, </author> <title> "Giving applications access to Gb/s networking," </title> <journal> IEEE Network Magazine, </journal> <pages> pp. 44-52, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: With slower networks, packets (or messages) were generated and consumed at a slower rate, and therefore the host was generally not the bottleneck. Gigabit networks have shifted the bottleneck from the transmission media to the end hosts <ref> [145, 161] </ref>. For the designers of the communication subsystem, therefore, the primary challenge has been realization of high-throughput and low-latency communication. <p> the delivered throughput and latency of communication protocols include [61] * minimization of data-copying within the communication subsystem and across the API [52, 181], * optimization of protocol implementations, such as hand-optimized critical paths [40], 27 and integrated layer processing [1, 41], * appropriate network interface design for high performance <ref> [161] </ref>, and, * exploitation of concurrency and parallelism within the communication subsystem [152]. We discuss each of these techniques and their compatibility with, and implications for, QoS-sensitive communication subsystem design. 2.4.1 Performance optimizations for efficient data transfer A wide variety of performance optimizations have been applied to communication subsystems.
Reference: [162] <author> S. Sommer and J. Potter, </author> <title> "Operating system extensions for dynamic real-time applications," </title> <booktitle> in Proc. 17th Real-Time Systems Symposium, </booktitle> <pages> pp. 45-50, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: However, such mechanisms have been found to be insufficient in a dynamic execution environment [104]. Extensions to Windows NT to support dynamic real-time applications are described in <ref> [162] </ref>. Support for real-time processes, admission control, and detecting and managing overrun is provided. However, as stated by the authors, they have not yet accounted for implementation overheads and constraints. Moreover, the issues and tradeoffs involved in realizing QoS-sensitive communication subsystems are not considered. <p> This chapter describes the design, implementation, and evaluation of one such service for a microkernel operating system. Microkernel operating systems continue to play an important role in operating system design [45, 100], and are being extended to support real-time and multimedia applications <ref> [162] </ref>. With the continued upsurge in the demand for networked multimedia applications on the WWW, it is important, therefore, to examine realization of QoS-sensitive communication subsystems on contemporary microkernel operating systems.
Reference: [163] <author> P. Steenkiste, </author> <title> "Analyzing communication latency using the Nectar communication processor," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 199-209, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: While data copying primarily impacts data transfer throughput, protocol processing latency is significantly affected by the complex, protocol-specific cache behavior of network protocols [16, 133]. Finally, protocol stack execution performance is significantly affected by operating system overheads such as context switching and device interrupts <ref> [163] </ref>. The number of context switches incurred during protocol processing are a function of the protocol architecture employed [152], while the number of interrupts is a function of the design of the network adapter and the packet input mechanism adopted [151, 173].
Reference: [164] <author> P. A. Steenkiste, </author> <title> "A systematic approach to host interface design for high-speed networks," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 47-57, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: By migrating frequent and time-consuming communication functions into hardware, the available network bandwidth can be utilized effectively. Since the network adapter defines the communication primitives available to the communication software, flexible support for network I/O necessitates a careful division of functionality between the adapter and the host processor <ref> [89, 164] </ref>. Network adapters either facilitate flexibility in supporting different protocols through simple designs [46, 103, 120], or restrict flexibility, and hence improve performance, by supporting specific communication protocols and resource management strategies [37, 90].
Reference: [165] <author> I. Stoica, H. Abdel-Wahab, K. Jeffay, S. K. Baruah, J. E. Gehrke, and C. G. Plax-ton, </author> <title> "A proportional share resource allocation algorithm for real-time time-shared systems," </title> <booktitle> in Proc. 17th Real-Time Systems Symposium, </booktitle> <pages> pp. 288-299, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: There have been numerous proposals for QoS-sensitive provisioning of network resources such as transmission bandwidth and buffers [8, 188], and host computation 1 resources such as CPU cycles <ref> [71, 165, 179] </ref>. However, not much attention has been paid to the interface between the applications and the network, i.e., the communication subsystem at end hosts, for provision of QoS guarantees. <p> Our design approach decouples the 41 protocol processing priority from that of the application, allowing the former to be derived from the traffic characteristics and run-time behavior of the application. QoS-sensitive CPU scheduling: Recently several QoS-sensitive CPU scheduling policies have been proposed recently <ref> [66, 71, 165, 179] </ref>. Scheduling algorithms for integrated scheduling of multimedia soft real-time computation and traditional hard real-time tasks on a multiprocessor multimedia server are proposed and evaluated in [94]. <p> The scheduling delay associated with session-level shaping can be largely eliminated (or at least made predictable) by employing QoS-sensitive CPU scheduling policies, as discussed next. 7.6.2 QoS-Sensitive CPU Scheduling Recently several QoS-sensitive scheduling policies such as stride scheduling [179], proportional share <ref> [165] </ref>, and hierarchical scheduling [71] have been proposed. <p> Potential load-dependent variations in the actual shaping latency can be accommodated via appropriate adaptation and 206 buffer management, or largely eliminated via integration with QoS-sensitive CPU scheduling policies. Our work complements recent work on QoS-sensitive CPU scheduling of applications <ref> [71, 165, 179] </ref> and protocol processing [68, 69, 116, 117] at end hosts. While these efforts focus on CPU scheduling, our primary focus is on the QoS support architecture exported to sockets based applications. <p> Another approach is to realize the entire communication subsystem as a class with an appropriate share in the recently proposed QoS-sensitive scheduling policies <ref> [71, 139, 165, 179] </ref>. These policies ensure that threads execute on the CPU in the order of their associated 214 QoS requirements and at the granularity of a certain quantum. Each thread executes for at most a quantum each time it is selected to run.
Reference: [166] <institution> The Real-Time Group, OSF RI MK7.2 Release Notes, OSF Research Institute, </institution> <month> Oc-tober </month> <year> 1996. </year>
Reference-contexts: The issues and techniques explored in this chapter are also relevant to contemporary monolithic operating systems such as UNIX and its variants. In this chapter we present our experiences in designing and implementing a guaranteed 107 QoS communication service, also based on real-time channels, on OSF MK 7.2 <ref> [166] </ref>, a microkernel operating system based on CMU Mach. Besides giving us access to full source code, OSF MK 7.2 allows us to exploit the Path abstraction provided in OSF's x-kernel-based CORDS framework [172]. <p> Each PC runs the MK 7.2 microkernel operating system from the Open Software Foundation (OSF) Research Institute <ref> [166] </ref>. While not a full-fledged real-time OS, MK 7.2 includes several features that facilitate provision of QoS guarantees. Specifically, the 124 7.2 release includes the CORDS (Communication Object for Real-time Dependable Systems) protocol implementation environment [172] in which our implementation resides.
Reference: [167] <author> C. A. Thekkath, T. D. Nguyen, E. Moy, and E. Lazowska, </author> <title> "Implementing network protocols at user level," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 64-73, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Several recent efforts have also focused on optimizing the protocol processing latency in TCP/IP protocol stacks [16, 129, 176]. User-level protocol processing: Several research efforts have focused on increasing communication subsystem throughput via user-level handling of network data <ref> [57, 112, 167] </ref>. In 28 addition to data copy minimization compared to a server-based implementation, user-level protocol processing offers significant flexibility in developing and debugging communication protocols. <p> We note that the proposed architecture and the admission control extensions described in Chapter 4 are also applicable to application-level framing [41] and user-level protocol processing architectures explored in recent efforts <ref> [55, 112, 167] </ref> to improve data transfer throughput in high-speed networks. In our design approach we have not made any specific assumptions about the location of the protocol stack, which could reside in the kernel or in user space.
Reference: [168] <author> C. A. Thekkath, T. D. Nguyen, E. Moy, and E. Lazowska, </author> <title> "Implementing network protocols at user level," </title> <journal> IEEE/ACM Trans. Networking, </journal> <volume> vol. 1, no. 5, </volume> <pages> pp. 554-565, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: These three components together ensure QoS-sensitive handling of network traffic at sending and receiving hosts. We realize this service architecture as a user-level CORDS server running on the OSF MK 7.2 microkernel. Even though server-based protocol stacks perform poorly compared to user-level protocol libraries or in-kernel implementations <ref> [112, 168] </ref>, we choose a server configuration for several reasons. A server configuration considerably eases software development and debugging, particularly the location and correction of timing-related bugs. <p> The final reason is related to performance. A server-based implementation is natural for a microkernel operating system, but may perform poorly compared to user-level protocol libraries due to excessive data copying and context switching <ref> [112, 168] </ref>. As mentioned in Section 5.1, it seems appropriate to be conservative when building a guaranteed-QoS communication service. It follows that in the worst case, compared to user-level libraries a server configuration only suffers from additional context switches. <p> Protocol processing is thus triggered via IPC between the application and the trusted server on the one hand, and between the server and the kernel on the other. Type III: also corresponds to a user-level protocol processing configuration, on a micro-kernel operating system <ref> [112, 168] </ref> or otherwise [55], with the difference that data transmission and reception is performed by threads executing in communication subsystem libraries linked with the application; control operations such as setting up connections are performed by the user-level trusted server or the kernel.
Reference: [169] <author> C. A. Thekkath, D. L. Eager, E. D. Lazowska, and H. M. Levy, </author> <title> "A performance analysis of network I/O in shared-memory multiprocessors," </title> <institution> Computer Science and Engineering Technical Report 92-04-04, University of Washington, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: Protocol processing in SMMPs: Similar problems arise when implementing protocol processing in shared-memory multiprocessors (SMMPs), where the approaches adopted essentially lie on two extremes. In one approach, each processor executing a process also performs protocol processing for messages transmitted by that process <ref> [169] </ref>. In this model, protocol processing is treated as work strictly local to each processor, resulting in an implicit sharing between the computation and communication subsystems.
Reference: [170] <author> C. A. Thekkath and H. M. Levy, </author> <title> "Limits to low-latency communication on high-speed networks," </title> <journal> ACM Trans. Computer Systems, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A detailed study of factors affecting end-to-end communication latency in LAN environments is described in <ref> [170] </ref>. ILP, first proposed in [41] and subsequently implemented and evaluated in [1], reduces the number of accesses to network data by effectively collapsing protocol layers and executing them in an integration fashion for each data word accessed.
Reference: [171] <author> H. Tokuda et al., </author> <title> "Real-Time Mach: Towards a predictable real-time system," </title> <booktitle> in Proc. USENIX Mach Workshop, </booktitle> <pages> pp. 73-82, </pages> <year> 1993. </year>
Reference-contexts: A scheme for adaptive rate-controlled scheduling is presented in [184]. A dynamic QoS control scheme using optimistic processor reservation and application feedback is described in [135]. This system, which is implemented in RT-Mach <ref> [171] </ref>, reserves processing capacity based on average processing usage of applications and provides notifications to applications to adjust QoS levels. <p> The primary unit of execution in Rialto is an activity, which is a time-constrained section of code which can adapt to overload conditions. The focus of Rialto is primarily on QoS-sensitive application scheduling, and issues in QoS-sensitive communication subsystem design are not considered. Real-Time Mach (RT-Mach) <ref> [171] </ref> provides processor capacity reserves [123], a mechanism to allow applications to reserve processing capacity for execution of its threads. Reserves are independent of application threads and can account for execution of a thread in multiple protection domains.
Reference: [172] <author> F. Travostino, E. Menze, and F. Reynolds, </author> <title> "Paths: Programming with system resources in support of real-time distributed applications," </title> <booktitle> in Proc. IEEE Workshop on Object-Oriented Real-Time Dependable Systems, </booktitle> <month> February </month> <year> 1996. </year> <month> 232 </month>
Reference-contexts: The communication subsystem constituting this service features a new service architecture that makes extensive use of OSF's CORDS framework <ref> [172] </ref>. This service architecture integrates three components: an API for specifying and obtaining QoS guarantees, a sender-based reliable end-to-end signalling protocol, and enhanced mechanisms and policies for admission control, resource reservation, and run-time resource management. <p> In LRP, an incoming packet is classified and enqueued, but not processed, until the application receives the data. We have studied adapter support for 29 receive livelock elimination using the END [81]. "Paths" through the communication subsystem: The Path abstraction realized in the Open Software Foundation's CORDS framework <ref> [172] </ref> provides a rich framework for development of real-time communication services for distributed applications. Using this abstraction, unique paths can be defined through the communication subsystem, and path-specific allocation performed for resources such as packet buffers, input packet queues, and input shepherd threads. <p> Similarly, the Scout operating system from the University of Arizona proposes the use of explicit paths as an important abstraction in operating system design to improve performance [128]. However, while paths in <ref> [172] </ref> are envisioned primarily as a static, relatively coarse-grain mechanism, paths in [128] are not associated with communication resources and assigned deadlines or priorities via admission control. <p> Moreover, these optimizations continue to be applicable in our proposed QoS-sensitive architecture as well as the enhancements made to TCP/IP stacks for integrated services. In constrast, we examine the overheads imposed by new data-handling components in the protocol stack. We adapt and extend the notion of paths in <ref> [128, 172] </ref> to obtain QoS guarantees and communication resources via admission control, and perform fine-grain path-based resource management to maintain QoS guarantees. While packet filters were designed primarily to classify incoming packets, they can also be used to classify outgoing traffic at network routers. <p> Similar to our approach, arriving messages are not processed in interrupt context, but processed by a thread that is in turn scheduled for execution. However, FLIPC does not provide any explicit QoS guarantees on the end-to-end message delivery. The CORDS path abstraction <ref> [172] </ref>, which is similar to Scout paths [128], provides a rich framework for development of real-time communication services for distributed applications, as demonstrated with the guaranteed-QoS communication service in Chapter 5. <p> Such support may be available directly from the adapter (e.g., the VCI in ATM network adapters), or the necessary information may be carried in link-level packet headers <ref> [172] </ref>, in which case the host does not incur any significant packet classification overhead. In the absence of such support, the receiving host must rely upon efficient packet filters [59, 186] to perform the packet classification. <p> Besides giving us access to full source code, OSF MK 7.2 allows us to exploit the Path abstraction provided in OSF's x-kernel-based CORDS framework <ref> [172] </ref>. <p> While not a full-fledged real-time OS, MK 7.2 includes several features that facilitate provision of QoS guarantees. Specifically, the 124 7.2 release includes the CORDS (Communication Object for Real-time Dependable Systems) protocol implementation environment <ref> [172] </ref> in which our implementation resides. Our implementation approach is to utilize and extend the functionality and facilities provided in OSF's CORDS environment. CORDS is based on the x-kernel object-oriented networking framework originally developed at the University of Arizona [76], with some significant extensions for controlled allocation of system resources. <p> System resources associated with paths include dynamically allocated memory, input packet buffers, and input threads that shepherd messages up a protocol stack <ref> [172] </ref>. Paths, coupled with allocators, provide a capability for reserving and allocating resources 125 at any protocol stack layer on behalf of a particular class of messages.
Reference: [173] <author> C. B. S. Traw and J. M. Smith, </author> <title> "Hardware/software organization of a high--performance ATM host interface," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 240-253, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The number of context switches incurred during protocol processing are a function of the protocol architecture employed [152], while the number of interrupts is a function of the design of the network adapter and the packet input mechanism adopted <ref> [151, 173] </ref>. Further, since the communication subsystem typically shares processing resources with application threads, additional scheduling and context switching overheads, and associated cache misses [124], may be incurred. Additional sources of overhead include buffer management, timer management, and error-recovery mechanisms such as retransmissions. <p> The design of high-speed network adapters, their performance characteristics, and implications for protocol stacks in uniprocessor workstation environments has received significant attention recently, for FDDI [151] as well as ATM <ref> [48, 173] </ref> networks. Uniprocessor front-ends and network adapters: With dedicated network front-ends, the network adapter operates under control of a front-end communication processor which handles all communication-related functions including network interrupts. <p> Polling, which is sometimes preferred over interrupts <ref> [173] </ref>, helps amortize interrupt overhead over multiple packets per polling cycle at the expense of increased packet reception latency. As mentioned before, polling can be effective in preventing receive livelock [125]. We assume that polling is realized via clocked interrupts [173] such that a polling timer expires periodically, and at each <p> Polling, which is sometimes preferred over interrupts <ref> [173] </ref>, helps amortize interrupt overhead over multiple packets per polling cycle at the expense of increased packet reception latency. As mentioned before, polling can be effective in preventing receive livelock [125]. We assume that polling is realized via clocked interrupts [173] such that a polling timer expires periodically, and at each polling instant inputs up to a certain number of packets (the quota in [125]). The quota is determined by the host processing capacity allocated to servicing the polling timer and inputing packets.
Reference: [174] <author> A. Tucker and A. Gupta, </author> <title> "Process control and scheduling issues for multiprogrammed shared-memory multiprocessors," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <pages> pp. 159-166, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: In recent years, resource management for computation has moved towards giving more control to the applications, e.g., user-level threads [113], scheduler activations [7], and application-kernel coordination to dynamically vary the degree of active parallelism in scientific computing applications <ref> [174] </ref>. Providing applications with similar control over thread management and scheduling within the communication subsystem can potentially improve performance by exploiting locality, improving resource utilization, and reducing context switching and scheduling overheads. In general, the API can be extended to allow application-specific resource management.
Reference: [175] <author> S.-Y. Tzou and D. P. Anderson, </author> <title> "The performance of message-passing using restricted virtual memory remapping," </title> <journal> Software Practice and Experience, </journal> <volume> vol. 21, no. 3, </volume> <pages> pp. 251-267, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Buffer management for data copy elimination: Elimination of unnecessary data copying via appropriate buffer management has been the focus of numerous research efforts in recent years. Proposals to achieve high-bandwidth data transfer across protection domains (e.g., between the kernel and an application) include restricted virtual memory remapping <ref> [175] </ref>, container shipping [147], and Fbufs [54]. Support for in-kernel device-to-device data transfers for multimedia applications is described in [60, 93].
Reference: [176] <author> R. van Renesse, </author> <title> "Masking the overhead of protocol layering," </title> <booktitle> in Proc. of ACM SIG-COMM, </booktitle> <pages> pp. 96-104, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Several recent efforts have also focused on optimizing the protocol processing latency in TCP/IP protocol stacks <ref> [16, 129, 176] </ref>. User-level protocol processing: Several research efforts have focused on increasing communication subsystem throughput via user-level handling of network data [57, 112, 167]. In 28 addition to data copy minimization compared to a server-based implementation, user-level protocol processing offers significant flexibility in developing and debugging communication protocols. <p> The scalability of the QoS architecture is, therefore, contingent in part upon the accuracy with which an application specifies its run-time communication behavior. Protocol stack performance and optimizations of existing implementations has been the 184 subject of numerous research articles, including some very recent ones <ref> [16, 129, 176] </ref>. How--ever, all of these studies focus on the traditional best-effort data path. Our study assumes significance in that it quantifies the performance penalty imposed by new data-handling components in the protocol stack, and their impact on the best-effort data path.
Reference: [177] <author> C. Vogt, R. G. Herrtwich, and R. Nagarajan, "HeiRAT: </author> <title> The Heidelberg resource administration technique design philosophy and goals," </title> <institution> Research Report 43.9213, IBM Research Division, IBM European Networking Center, </institution> <address> Heidelberg, Germany, </address> <year> 1992. </year>
Reference-contexts: Operating system support for multimedia communication is explored in [73], where the focus is on provision of preemption points and EDF scheduling in the kernel, and in <ref> [177] </ref>, which also focuses on the scheduling architecture. None of these approachees provide support for traffic enforcement or decoupling of protocol processing priority from application priority.
Reference: [178] <author> I. Wakeman, A. Ghosh, J. Crowcroft, V. Jacobson, and S. Floyd, </author> <title> "Implementing real-time packet forwarding policies using Streams," </title> <booktitle> in Proc. USENIX Winter Conference, </booktitle> <year> 1995. </year>
Reference-contexts: While packet filters were designed primarily to classify incoming packets, they can also be used to classify outgoing traffic at network routers. However, traffic classification for provision of QoS guarantees imposes additional requirements on the classification function, and as such may require more elaborate functionality <ref> [178] </ref>. Regarding receive livelock elimination, while approaches such as LRP work well for best-effort traffic, appropriate OS support is needed to ensure the application is scheduled to run in a QoS-sensitive fashion.
Reference: [179] <author> C. Waldspurger, </author> <title> Lottery and Stride Scheduling: Flexible Proportional-Share Resource Management, </title> <type> PhD thesis, Technical Report, </type> <institution> MIT/LCS/TR-667, Laboratory for CS, MIT, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: There have been numerous proposals for QoS-sensitive provisioning of network resources such as transmission bandwidth and buffers [8, 188], and host computation 1 resources such as CPU cycles <ref> [71, 165, 179] </ref>. However, not much attention has been paid to the interface between the applications and the network, i.e., the communication subsystem at end hosts, for provision of QoS guarantees. <p> Our design approach decouples the 41 protocol processing priority from that of the application, allowing the former to be derived from the traffic characteristics and run-time behavior of the application. QoS-sensitive CPU scheduling: Recently several QoS-sensitive CPU scheduling policies have been proposed recently <ref> [66, 71, 165, 179] </ref>. Scheduling algorithms for integrated scheduling of multimedia soft real-time computation and traditional hard real-time tasks on a multiprocessor multimedia server are proposed and evaluated in [94]. <p> The scheduling delay associated with session-level shaping can be largely eliminated (or at least made predictable) by employing QoS-sensitive CPU scheduling policies, as discussed next. 7.6.2 QoS-Sensitive CPU Scheduling Recently several QoS-sensitive scheduling policies such as stride scheduling <ref> [179] </ref>, proportional share [165], and hierarchical scheduling [71] have been proposed. <p> Potential load-dependent variations in the actual shaping latency can be accommodated via appropriate adaptation and 206 buffer management, or largely eliminated via integration with QoS-sensitive CPU scheduling policies. Our work complements recent work on QoS-sensitive CPU scheduling of applications <ref> [71, 165, 179] </ref> and protocol processing [68, 69, 116, 117] at end hosts. While these efforts focus on CPU scheduling, our primary focus is on the QoS support architecture exported to sockets based applications. <p> Another approach is to realize the entire communication subsystem as a class with an appropriate share in the recently proposed QoS-sensitive scheduling policies <ref> [71, 139, 165, 179] </ref>. These policies ensure that threads execute on the CPU in the order of their associated 214 QoS requirements and at the granularity of a certain quantum. Each thread executes for at most a quantum each time it is selected to run.
Reference: [180] <author> D. Wallach, D. Engler, and M. F. Kaashoek, "ASHs: </author> <title> Application-specific handlers for high-performance messaging," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 40-52, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Application-specific networking: Recently there has been much interest in OS extension technologies, in which trusted application code is installed for execution within the OS. This allows operating systems to be customized or "extended" to implement application-specific policies. Plexus [64] and application-specific handlers in the Aegis kernel <ref> [180] </ref> are two examples of approaches to realize application-specific networking via OS extensions. Packet classification: Packet filters provide general and flexible classification (i.e., demul-tiplexing) of incoming packets to application end-points, at the lowest layer of the protocol stack.
Reference: [181] <author> R. W. Watson and S. A. Mamrak, </author> <title> "Gaining efficiency in transport services by appropriate design and implementation choices," </title> <journal> ACM Trans. Computer Systems, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 97-120, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: The techniques employed to improve the delivered throughput and latency of communication protocols include [61] * minimization of data-copying within the communication subsystem and across the API <ref> [52, 181] </ref>, * optimization of protocol implementations, such as hand-optimized critical paths [40], 27 and integrated layer processing [1, 41], * appropriate network interface design for high performance [161], and, * exploitation of concurrency and parallelism within the communication subsystem [152].
Reference: [182] <author> C. M. Woodside and R. G. Franks, </author> <title> "Alternative software architectures for parallel protocol execution with synchronous IPC," </title> <journal> IEEE/ACM Trans. Networking, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 178-186, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Special-purpose designs facilitate efficient interaction with the host and the network interface unit [82, 83], while general-purpose designs must explicitly coordinate accesses to these interfaces <ref> [14, 138, 182] </ref>. More processing power in the front-end can improve the quality of service provided to the applications, by reducing queuing delays within the communication subsystem. Network interfaces for multicomputers: Network interface design for multicomputer environments presents unique opportunities and cost-performance tradeoffs. <p> In general, communication subsystems employ one or more processes (or threads) to implement a protocol graph [76]. Depending upon the allocation of work to these processes, communication subsystems can be classified into horizontal or vertical process architectures [152]. In horizontal architectures <ref> [182] </ref>, each process implements a specific layer of a protocol graph; at most two processes can be assigned to each layer, one each for transmission and reception.
Reference: [183] <author> J. Wroclawski, </author> <title> "Specification of controlled-load network element service," Internet Draft draft-ietf-intserv-ctrl-load-svc-03.txt, </title> <address> June 1996. </address> <publisher> MIT. </publisher>
Reference-contexts: Resource management is performed via per-flow traffic shaping and scheduling for various classes of service [22], such as guaranteed service [159] that provides guaranteed delay, and controlled load service <ref> [183] </ref> that has more relaxed QoS requirements. The guaranteed service is intended for applications requiring firm guarantees on loss-less on-time datagram delivery. <p> Recent studies using voice conferencing indicate that the delivered QoS determines the degree and exact nature of adaptivity required in the application [145]. The controlled load service <ref> [183] </ref> being standardized by the IETF for an integrated services Internet is intended for such adaptive real-time applications. 2.2 Communication Subsystem Overview Before discussing the factors affecting communication subsystem performance, we present a brief overview of typical host communication subsystems. <p> A complete description of RSVP is provided in [23], while details on different service classes can be found in <ref> [159, 183] </ref>. 7.2.1 RSVP: An End-to-End View run RSVP daemons that exchange RSVP messages (PATH and RESV) on behalf of their hosts. <p> The resource and traffic management mechanisms depend heavily on the service classes supported. 7.2.2 Service Classes Two important service classes currently under standardization by IETF are (i) guaranteed service [159], and (ii) controlled load <ref> [183] </ref> service. Guaranteed services is targeted at provid 186 ing applications with a mathematically provable end-to-end delay bound using appropriate buffer and bandwidth reservation at all network elements. <p> However, the goodput still does not match the packet transmission rate at the sender. Experiment III: In this experiment, the same reservation as in Experiment II is created, but with traffic policing enabled in the QOSMGR. This corresponds to the recommended behavior for the controlled load service <ref> [183] </ref>. In this case, the QOSMGR performs packet level policing for the session and transmits all excess (i.e., non-compliant) traffic as best effort, i.e., it does not shape non-compliant traffic.
Reference: [184] <author> D. K. Y. Yau and S. S. Lam, </author> <title> "Adaptive rate-controlled scheduling for multimedia applications," </title> <booktitle> in Proc. of ACM Multimedia, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: Similarly, a QoS adaptive transport system is described in [33] that incorporates a QoS-aware API and mechanisms to assist applications to adapt to fluctuations in the delivered network QoS. A scheme for adaptive rate-controlled scheduling is presented in <ref> [184] </ref>. A dynamic QoS control scheme using optimistic processor reservation and application feedback is described in [135]. This system, which is implemented in RT-Mach [171], reserves processing capacity based on average processing usage of applications and provides notifications to applications to adjust QoS levels.
Reference: [185] <author> D. K. Y. Yau and S. S. Lam, </author> <title> "An architecture towards efficient OS support for distributed multimedia," </title> <booktitle> in Proc. IST/SPIE Multimedia Computing and Networking, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: Similar to our approach, rate-based flow control of multimedia streams via kernel-based communication threads is also proposed in <ref> [185] </ref>. In contrast to our notion of per-connection threads, however, a coarser notion of per-process kernel threads is adopted. This scheme is clearly not suitable for an application with multiple QoS connections, each with different QoS requirements and traffic characteristics. <p> This scheme is clearly not suitable for an application with multiple QoS connections, each with different QoS requirements and traffic characteristics. Mechanisms for scheduling multiple communication threads, and the issues involved in reception side processing, are not considered. More importantly, the architecture outlined in <ref> [185] </ref> does not consider provision of signaling and resource management services within the communication subsystem. Communication support for efficient messaging in distributed real-time environments is provided in FLIPC [15]. FLIPC exploits programmable communication controllers to realize low latency message transfer.
Reference: [186] <author> M. Yuhara, B. N. Bershad, C. Maeda, and J. E. B. Moss, </author> <title> "Efficient packet demulti-plexing for multiple endpoints and large messages," </title> <booktitle> in Proc. USENIX Winter Conference, </booktitle> <month> January </month> <year> 1994. </year> <month> 233 </month>
Reference-contexts: They were first proposed primarily to enable user-level network capture for developing new protocols without kernel modifications [127], but are increasingly being utilized for realizing protocols such as UDP and TCP as user-level libraries. Successive implementations of packet filters have reduced classification overhead significantly, e.g., BPF [114], MPF <ref> [186] </ref>, and PATHFINDER [11]. While all of these filter designs perform classification via interpretation, more recently dynamic code generation techniques have been applied to realize very efficient packet filters, as in DPF [59]. Packet filters are necessary for packet classification in connectionless networks, such as the IPv4-based Internet. <p> In the absence of such support, the receiving host must rely upon efficient packet filters <ref> [59, 186] </ref> to perform the packet classification. Note that this overhead is not incurred at the sending host if the API directly associates outgoing messages with channels.
Reference: [187] <author> H. Zhang and D. Ferrari, </author> <title> "Rate-controlled static-priority queueing," </title> <booktitle> in Proc. IEEE INFOCOM, </booktitle> <pages> pp. 227-236, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Example service disciplines include Weighted Fair Queueing [50], also known as Packet-by-Packet Generalized Processor Sharing [144], and Rate-Controlled Static-Priority Queueing <ref> [187] </ref>. In addition to servicing (QoS-sensitive) traffic according to the associated QoS guarantees, the service discipline also determines the treatment of best-effort traffic at the node. Note that the service discipline may also determine the admission control procedure employed at the node. <p> In addition to providing protocol support for end-to-end signalling, support is provided for QoS-sensitive packet scheduling at hosts and routers via Rate-Controlled Static-Priority Queueing <ref> [187] </ref>. However, the Tenet project does not consider incorporation of protocol processing overheads into the network-level resource management policies. In particular, it has not addressed the problem of QoS-sensitive protocol processing inside hosts.
Reference: [188] <author> H. Zhang, </author> <title> "Service disciplines for guaranteed performance service in packet-switching networks," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 83, no. 10, </volume> , <month> October </month> <year> 1995. </year>
Reference-contexts: A QoS-sensitive operating system, for example, would ensure that applications exchanging QoS-sensitive data are scheduled for execution so as to satisfy the desired end-to-end QoS guarantees on communication. There have been numerous proposals for QoS-sensitive provisioning of network resources such as transmission bandwidth and buffers <ref> [8, 188] </ref>, and host computation 1 resources such as CPU cycles [71, 165, 179]. However, not much attention has been paid to the interface between the applications and the network, i.e., the communication subsystem at end hosts, for provision of QoS guarantees. <p> While policing ascertains whether the traffic is conformant or not, shaping buffers non-conformant traffic until conformance as per the traffic specification. Each node also implements a QoS-sensitive queueing policy which determines the nature of queueing at the node, and a service discipline <ref> [8, 188] </ref> which determines the order in which packets are scheduled for service, e.g., processing and transmission to the next node along the chosen route. Example service disciplines include Weighted Fair Queueing [50], also known as Packet-by-Packet Generalized Processor Sharing [144], and Rate-Controlled Static-Priority Queueing [187]. <p> These extensions are then refined for hosts simultaneously engaged in transmission as well as reception of QoS-sensitive traffic. The issues of simultaneous management of CPU and link bandwidth for real-time communication are of wide-ranging interest. The above-mentioned extensions are applicable to other proposals for real-time communication and QoS guarantees <ref> [8, 188] </ref>, and to other host and network architectures. In particular, Internet servers running TCP/IP protocol stacks supporting integrated services, especially the guaranteed service class [159], can also benefit from these extensions. <p> Our primary focus is on the architectural mechanisms for run-time traffic management within the communication subsystem to satisfy the QoS requirements of all connections, without undue degradation in the performance of best-effort traffic. While the proposed architecture is applicable to other proposals for guaranteed-QoS connections <ref> [8, 188] </ref>, we focus on real-time channels, a paradigm for guaranteed-QoS communication services in packet-switched networks [63, 92]. Consider the problem of servicing several guaranteed-QoS and best-effort connections engaged in network input/output at a host. <p> The issues of simultaneous management of CPU and link bandwidth for real-time communication are of wide-ranging interest. Our present work is applicable to other proposals for real-time communication and QoS guarantees <ref> [8, 188] </ref>. Further, the proposed admission control extensions are general and applicable to other host and network architectures. In particular, Internet servers running TCP/IP protocol stacks supporting Integrated Services [22, 65], especially the guaranteed service class [159], can also benefit from these extensions. <p> The analysis and extensions developed in this chapter are applicable to other proposals for guaranteed real-time communication in packet-switched networks <ref> [8, 188] </ref>. Our main conclusions can be summarized as follows. In order to best utilize CPU and link bandwidth, one must account for implementation overheads that reduce useful resource capacity. Further, one must also consider the implications of the implementation paradigm adopted to manage CPU and link bandwidth.
Reference: [189] <author> L. Zhang, S. Deering, D. Estrin, S. Shenker, and D. Zappala, "RSVP: </author> <title> A new resource ReSerVation Protocol," </title> <journal> IEEE Network, </journal> <pages> pp. 8-18, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Towards that end, the IETF is developing a set of protocols and standards for integrated services <ref> [20, 65, 149, 189] </ref>. In the IETF's vision, applications request and reserve resources, both in the network and at the attached hosts (clients or servers) using an end-to-end receiver-initiated Resource ReSerVation Protocol (RSVP) [23, 189]. <p> Towards that end, the IETF is developing a set of protocols and standards for integrated services [20, 65, 149, 189]. In the IETF's vision, applications request and reserve resources, both in the network and at the attached hosts (clients or servers) using an end-to-end receiver-initiated Resource ReSerVation Protocol (RSVP) <ref> [23, 189] </ref>. Resource management is performed via per-flow traffic shaping and scheduling for various classes of service [22], such as guaranteed service [159] that provides guaranteed delay, and controlled load service [183] that has more relaxed QoS requirements.
Reference: [190] <author> X. Zhang, Z. Wang, N. Gloy, J. B. Chen, and M. D. Smith, </author> <title> "Operating system support for automatic profiling and optimization," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <month> October </month> <year> 1997. </year>
Reference-contexts: We note that while our primary focus is on provision of deterministic guarantees, self-parameterizing protocol stacks are desirable for provision of looser forms of QoS guarantees as well. System support for automatic profiling and optimization of applications is described in <ref> [190] </ref>, where the focus is primarily on improving application execution performance via statistical profiling and profile-based optimizations.
Reference: [191] <author> M. Zitterbart, B. Stiller, and A. N. Tantawy, </author> <title> "A model for flexible high-performance communication subsystems," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 4, </volume> <pages> pp. 507-518, </pages> <month> May </month> <year> 1993. </year> <month> 234 </month>
Reference-contexts: Large connection state and the requirement of in-order delivery of packets to applications are two examples of features that limit performance in parallel implementations. Increased parallelism is likely to be a feature of the next generation of communication protocols <ref> [150, 191] </ref>. Recent results have shown that connectional parallelism delivers comparatively more scalable performance than message parallelism [134, 153, 154]. Protocol processing in SMMPs: Similar problems arise when implementing protocol processing in shared-memory multiprocessors (SMMPs), where the approaches adopted essentially lie on two extremes.
References-found: 192

