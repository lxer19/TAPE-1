URL: http://www.cis.upenn.edu/~daes/papers/eurocolt95.ps.gz
Refering-URL: http://www.cis.upenn.edu/~daes/papers.html
Root-URL: 
Email: dale@cs.toronto.edu  
Title: Characterizing Rational versus Exponential Learning Curves  
Author: Dale Schuurmans 
Date: March 1995.  
Note: Appears in Proceedings of the Second European Conference on Computational Learning Theory (EuroCOLT-95), Barcelona, Spain,  
Address: Toronto, Ontario M5S 1A4 CANADA  
Affiliation: Department of Computer Science University of Toronto  
Abstract: We consider the standard problem of learning a concept from random examples. Here a learning curve can be defined to be the expected error of a learner's hypotheses as a function of training sample size. Haussler, Littlestone and Warmuth have shown that, in the distribution free setting, the smallest expected error a learner can achieve in the worst case over a concept class C converges rationally to zero error (i.e., fi(1=t) for training sample size t). However, recently Cohn and Tesauro have demonstrated how exponential convergence can often be observed in experimental settings (i.e., average error decreasing as e fi(t) ). By addressing a simple non-uniformity in the original analysis, this paper shows how the dichotomy between rational and exponential worst case learning curves can be recovered in the distribution free theory. These results support the experimental findings of Cohn and Tesauro: for finite concept classes, any consistent learner achieves exponential convergence, even in the worst case; but for continuous concept classes, no learner can exhibit sub-rational convergence for every target concept and domain distribution. A precise boundary between rational and exponential convergence is drawn for simple concept chains. Here we show that somewhere dense chains always force rational convergence in the worst case, but exponential convergence can always be achieved for nowhere dense chains.
Abstract-found: 1
Intro-found: 1
Reference: [AFS92] <author> S. Amari, N. Fujita, and S. Shinomoto. </author> <title> Four types of learning curves. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 605-618, </pages> <year> 1992. </year>
Reference-contexts: Therefore, E P t err (L; c) N (1 p 0 ) t = e O (t) . fl Furthermore, it is impossible to achieve better than exponential worst case convergence for any non-trivial concept class. 5 Amari et al. <ref> [AFS92] </ref> also provide a general Bayesian analysis of "average case" learning curves, but demonstrate only rational forms. Proposition 4 (Universal LB).
Reference: [Ash72] <author> R. B. Ash. </author> <title> Real Analysis and Probability. </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1972. </year>
Reference-contexts: that any learner L must obtain E P t err (L; c) = 0 (1=t) for a non-trivial portion of the c 2 C (Lemma 16). fl 6 Such a measure can always be constructed by the same procedure used to construct the Lebesgue measure on [0; 1]; see e.g., <ref> [Ash72, Chapter 1] </ref>.
Reference: [BL91] <author> E. B. Baum and Y.-D. Lyuu. </author> <title> The transition to perfect generalization in perceptrons. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 386-401, </pages> <year> 1991. </year>
Reference-contexts: Given these stronger assumptions, many researchers have shown that both rational and exponential learning curves are possible. For example, exponential convergence has been demonstrated in many distribution specific analyses of particular con-cept spaces <ref> [GM93, BL91, PS90, SSSD90, SST91] </ref>, and rational convergence has been demonstrated for other spaces [OH91]. 5 This paper shows how, in a general way, this dichotomy can still be revealed under much weaker assumptions.
Reference: [Bru77] <author> R. A. Brualdi. </author> <title> Introductory Combinatorics. </title> <publisher> North-Holland, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: Thus we get E P t jC [c y x t ]j = i=0 i y i (1 y) ti fi fl t X t h i+1 + ti+1 : This can be simplified via the Binomial Theorem (see e.g., <ref> [Bru77, Chapter 4] </ref>) to obtain the stated result. fl Lemma 15. For the uniform prior P C on C, any learner L obtains E P C E P t err (L; c) 1 Proof. (Outline) Fix an arbitrary learner L.
Reference: [CT90] <author> D. Cohn and G. Tesauro. </author> <title> Can neural networks do better than the Vapnik-Chervonenkis bounds? In D. </title> <editor> Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 911-917. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: However, it turns out that one does not always observe rational learning curves in practice. This is clearly demonstrated in a recent study by Cohn and Tesauro who show that exponential learning curves can be obtained in many experimental settings <ref> [CT90, CT92] </ref>. Of course, these experimental results do not directly contradict the previous theory, since this exponential convergence behavior was only demonstrated for specific target concepts and domain distributions, and may not accurately reflect the worst case behavior. <p> In particular, the lower bound result (2) chooses a different domain distribution (and possibly a different target concept) for each training sample size t. This does not accurately reflect the situation encountered in practice where these are held fixed (for example, as experimentally investigated by <ref> [CT90, CT92] </ref>). This raises the obvious question of whether, in a model where the domain distribution and target concept are held fixed, there are situations where the best achievable worst case learning curve is exponential, and other situations where it is rational. <p> Therefore, although each individual curve is exponential, any universal upper bound over all curves is rational. Figure 1 illustrates this discrepancy between the worst case bounds of [HLW88] which consider a different P for each t, and the experimental results of <ref> [CT90] </ref> which consider a single P for all t. Eerr 10 20 30 40 50 60 0.01 0.03 0.05 training sample size ln Eerr 20 40 60 80 100 -3 -1 training sample size Fig. 1. Comparing uniform versus non-uniform bounds. <p> issues it addresses shed light on the fundamental nature of worst case learning curves. 2.5 Scaling effects Although the previous results draw a precise boundary between exponential and rational worst case learning curves (for concept chains), in a fundamental sense they miss the point demonstrated by the experimental results of <ref> [CT90, CT92] </ref>: since their experiments were computer simulations conducted with finite precision, all of the concept classes Cohn and Tesauro considered were fundamentally finite. The fact that they observe rational convergence in some cases seems to directly contradict the theoretical results presented here. <p> This explains the dichotomy obtained in <ref> [CT90] </ref>, as they tested the same training sample sizes for concept classes with vastly different inter-concept distances; observing exponential convergence when the gaps were large, and apparently rational convergence when the gaps were small. A Proofs of some lemmas Lemma 14.
Reference: [CT92] <author> D. Cohn and G. Tesauro. </author> <title> How tight are the Vapnik-Chervonenkis bounds? Neural Computation, </title> <booktitle> 4 </booktitle> <pages> 249-269, </pages> <year> 1992. </year>
Reference-contexts: However, it turns out that one does not always observe rational learning curves in practice. This is clearly demonstrated in a recent study by Cohn and Tesauro who show that exponential learning curves can be obtained in many experimental settings <ref> [CT90, CT92] </ref>. Of course, these experimental results do not directly contradict the previous theory, since this exponential convergence behavior was only demonstrated for specific target concepts and domain distributions, and may not accurately reflect the worst case behavior. <p> In particular, the lower bound result (2) chooses a different domain distribution (and possibly a different target concept) for each training sample size t. This does not accurately reflect the situation encountered in practice where these are held fixed (for example, as experimentally investigated by <ref> [CT90, CT92] </ref>). This raises the obvious question of whether, in a model where the domain distribution and target concept are held fixed, there are situations where the best achievable worst case learning curve is exponential, and other situations where it is rational. <p> issues it addresses shed light on the fundamental nature of worst case learning curves. 2.5 Scaling effects Although the previous results draw a precise boundary between exponential and rational worst case learning curves (for concept chains), in a fundamental sense they miss the point demonstrated by the experimental results of <ref> [CT90, CT92] </ref>: since their experiments were computer simulations conducted with finite precision, all of the concept classes Cohn and Tesauro considered were fundamentally finite. The fact that they observe rational convergence in some cases seems to directly contradict the theoretical results presented here.
Reference: [GM93] <author> M. Golea and M. Marchand. </author> <title> Average case analysis of the clipped Hebb rule for nonoverlapping Perceptron networks. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Computational Learning Theory (COLT-93), </booktitle> <pages> pages 151-157, </pages> <year> 1993. </year>
Reference-contexts: Given these stronger assumptions, many researchers have shown that both rational and exponential learning curves are possible. For example, exponential convergence has been demonstrated in many distribution specific analyses of particular con-cept spaces <ref> [GM93, BL91, PS90, SSSD90, SST91] </ref>, and rational convergence has been demonstrated for other spaces [OH91]. 5 This paper shows how, in a general way, this dichotomy can still be revealed under much weaker assumptions.
Reference: [HKST94] <author> D. Haussler, M. Kearns, H. S. Seung, and N. Tishby. </author> <title> Rigorous learning curve bounds from statistical mechanics. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Computational Learning Theory (COLT-94), </booktitle> <pages> pages 76-87, </pages> <year> 1994. </year>
Reference-contexts: It turns out the answer to these questions is yes. 3 This is also known as "power law" convergence <ref> [HKST94] </ref>. 1.3 Results By carrying out an analysis of worst case learning curves where the domain distribution and target concept are held fixed, this paper shows how the dichotomy between rational and exponential convergence can be recovered in the distribution free setting. <p> By pursuing a uniform analysis, we are able to distinguish the conditions under which rational and exponential worst case convergence take place, based solely on the structure of the concept class C and ignoring any special properties of the domain distribution P (contrary to common suggestion <ref> [SST91, HKST94] </ref>). Most theoretical studies of learning curve behavior adopt a distribution specific model that assumes the domain distribution P is known a priori. Given these stronger assumptions, many researchers have shown that both rational and exponential learning curves are possible. <p> As most distribution specific analyses address particular case studies, they do not provide a precise, general characterization of the concept space properties that permit or prohibit exponential convergence. (Some progress towards such a characterization has been obtained for finite concept spaces <ref> [HKST94] </ref>.) Interestingly, only two possible modes of worst case convergence appear possible in the distribution free setting: rational and exponential (proved for concept chains, but conjecture in general). This is quite unlike the distribution specific case where all intermediate forms of convergence are apparently possible [HKST94]. <p> obtained for finite concept spaces <ref> [HKST94] </ref>.) Interestingly, only two possible modes of worst case convergence appear possible in the distribution free setting: rational and exponential (proved for concept chains, but conjecture in general). This is quite unlike the distribution specific case where all intermediate forms of convergence are apparently possible [HKST94]. Clearly the distribution specific analyses give tighter characterizations of the actual learning curves one might observe in practice, but require more problem specific information [HKST94] | in fact more than is generally available in practice. <p> This is quite unlike the distribution specific case where all intermediate forms of convergence are apparently possible <ref> [HKST94] </ref>. Clearly the distribution specific analyses give tighter characterizations of the actual learning curves one might observe in practice, but require more problem specific information [HKST94] | in fact more than is generally available in practice. The benefits of the distribution free theory is its wider range of applicability in practical situations.
Reference: [HLW88] <author> D. Haussler, N. Littlestone, and M. K. Warmuth. </author> <title> Predicting f0,1g-functions on randomly drawn points. </title> <booktitle> In Proceedings of the First Workshop on Computational Learning Theory (COLT-88), </booktitle> <pages> pages 280-296, </pages> <year> 1988. </year>
Reference-contexts: Specifically, for a concept class C we are interested in determining the best learning curve that can be obtained in the worst case over all possible target concepts c 2 C and domain distributions P. An analysis of this form has been carried out by Haussler et al. <ref> [HLW88] </ref> who develop a special learning strategy 1IGPS (for "1-inclusion graph prediction strategy") which, given t training examples, always attains an expected error of at most E P t err (1IGPS; c) 2d (1) for any target concept c 2 C and any domain distribution P (where d is the "Vapnik-Chervonenkis <p> special learning strategy 1IGPS (for "1-inclusion graph prediction strategy") which, given t training examples, always attains an expected error of at most E P t err (1IGPS; c) 2d (1) for any target concept c 2 C and any domain distribution P (where d is the "Vapnik-Chervonenkis dimension" of C) <ref> [HLW88, Theorem 5.1] </ref>. 2 Moreover, they show that no learner can do significantly better than this in the worst case: given t &gt; d training examples, any learner L must obtain an expected error of at least E P t err (L; c) 2e (t + 1) 1 This is identical <p> &gt; d training examples, any learner L must obtain an expected error of at least E P t err (L; c) 2e (t + 1) 1 This is identical to the probability that L correctly classifies the t + 1st random example after training on the first t random examples <ref> [HLW88, Lemma 6.1] </ref>. 2 This result has been improved to d=(t + 1) for a slightly modified strategy [HLW90]. for some domain distribution P and target concept c 2 C [HLW88, Theorem 5.2]. <p> the probability that L correctly classifies the t + 1st random example after training on the first t random examples [HLW88, Lemma 6.1]. 2 This result has been improved to d=(t + 1) for a slightly modified strategy [HLW90]. for some domain distribution P and target concept c 2 C <ref> [HLW88, Theorem 5.2] </ref>. <p> An adequate explanation of empirical learning curve behavior must explain how the worst case results are typical in some cases, while not in others. It turns out that this discrepancy can be resolved via a simple observation about the previous theory. A close inspection of the results of <ref> [HLW88] </ref> reveals that the analysis is nonuniform in training sample size t. In particular, the lower bound result (2) chooses a different domain distribution (and possibly a different target concept) for each training sample size t. <p> Previous results on distribution free learning curves <ref> [HLW88] </ref> suggested that rational convergence was the only possible worst case form; however this is based on a nonuniform analysis. <p> average expected error of at least e (t) for every t t 0 implies that L must obtain at least this expected error on one of c 1 or c 2 for infinitely many t. fl It is interesting to see how these results compare to the non-uniform theory of <ref> [HLW88] </ref>. Although we obtain exponential learning curves for any fixed distribution, there is no single "worst case" domain distribution here. That is, we obtain a different worst case domain distribution for each training sample size t. <p> That is, we obtain a different worst case domain distribution for each training sample size t. Therefore, although each individual curve is exponential, any universal upper bound over all curves is rational. Figure 1 illustrates this discrepancy between the worst case bounds of <ref> [HLW88] </ref> which consider a different P for each t, and the experimental results of [CT90] which consider a single P for all t. Eerr 10 20 30 40 50 60 0.01 0.03 0.05 training sample size ln Eerr 20 40 60 80 100 -3 -1 training sample size Fig. 1. <p> Furthermore, any learner L that guesses consistent concepts from C obtains E P t err (L; c) = O (ln t=t) <ref> [HLW88] </ref>. Here, we strengthen these results slightly by showing that any consistent learner L actually obtains E P t err (L; c) = O (1=t). This is done by noticing that the worst case situation is represented by the uniform chain (proof omitted): Definition 6 (Uniform chain).
Reference: [HLW90] <author> D. Haussler, N. Littlestone, and M. K. Warmuth. </author> <title> Predicting f0,1g-functions on randomly drawn points. </title> <type> Technical Report UCSC-CRL-90-54, </type> <institution> Computer Research Laboratory, University of California at Santa Cruz, </institution> <year> 1990. </year>
Reference-contexts: err (L; c) 2e (t + 1) 1 This is identical to the probability that L correctly classifies the t + 1st random example after training on the first t random examples [HLW88, Lemma 6.1]. 2 This result has been improved to d=(t + 1) for a slightly modified strategy <ref> [HLW90] </ref>. for some domain distribution P and target concept c 2 C [HLW88, Theorem 5.2]. <p> Next, we define a natural "prior" on the 7 Haussler et al. prove a similar result to Lemmas 14 and 15 in their later technical report <ref> [HLW90] </ref>; however, they do not supply Lemma 16. <p> Now notice B ff T and therefore P C D ff T P C B ff T ff=(ff + 11) for all T , which gives the desired result. fl 12 Haussler et al. prove a similar result in their later technical report <ref> [HLW90] </ref>, but they use a different argument than the one presented here.
Reference: [LM81] <author> R. J. Larsen and M. L. Marx. </author> <title> An Introduction to Mathematical Statistics and its Applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: In general, we have ER t = r=(t + 1) for any random variable R ~ uniform (0; r) (see e.g., <ref> [LM81, pp.99] </ref>).
Reference: [OH91] <author> M. Opper and D. Haussler. </author> <title> Generalization performance of Bayes optimal classification algorithm for learning a Perceptron. </title> <journal> Physical Review Letters, </journal> <volume> 66(20) </volume> <pages> 2677-2680, </pages> <year> 1991. </year>
Reference-contexts: Given these stronger assumptions, many researchers have shown that both rational and exponential learning curves are possible. For example, exponential convergence has been demonstrated in many distribution specific analyses of particular con-cept spaces [GM93, BL91, PS90, SSSD90, SST91], and rational convergence has been demonstrated for other spaces <ref> [OH91] </ref>. 5 This paper shows how, in a general way, this dichotomy can still be revealed under much weaker assumptions. We also draw a clean boundary between these two modes of convergence in terms of a simple structural property of concept classes, namely the presence of a dense sub-chain.
Reference: [PS90] <author> M. J. Pazzani and W. Sarrett. </author> <title> Average case analysis of conjunctive learning algorithms. </title> <booktitle> In Proceedings of Seventh International Conference on Machine Learning (ML-90), </booktitle> <pages> pages 339-347, </pages> <year> 1990. </year>
Reference-contexts: Given these stronger assumptions, many researchers have shown that both rational and exponential learning curves are possible. For example, exponential convergence has been demonstrated in many distribution specific analyses of particular con-cept spaces <ref> [GM93, BL91, PS90, SSSD90, SST91] </ref>, and rational convergence has been demonstrated for other spaces [OH91]. 5 This paper shows how, in a general way, this dichotomy can still be revealed under much weaker assumptions.
Reference: [Ros82] <author> J. G. Rosenstein. </author> <title> Linear Orderings. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: ordinal order in a scattered chain, but regardless, all concepts of any particular order are isolated in the concepts of the same or higher order. (A proof of this fact follows as a corollary to Hausdorff's Theorem, which provides a suitably constructive characterization of the class of scattered linear orderings <ref> [Ros82, Chapter 5] </ref>.) Lemma 10. (Corollary to Hausdorff 's Theorem) For a scattered concept chain C, there is some least ordinal ff for which: (i) every limit concept in C has ordinal order fi &lt; ff, (ii) C has limit concepts of every ordinal order fi &lt; ff, (iii) all limit
Reference: [SSSD90] <author> D. B. Schwartz, V. K. Samalam, S. A. Solla, and J. S. Denker. </author> <title> Exhaustive learning. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 374-385, </pages> <year> 1990. </year>
Reference-contexts: Given these stronger assumptions, many researchers have shown that both rational and exponential learning curves are possible. For example, exponential convergence has been demonstrated in many distribution specific analyses of particular con-cept spaces <ref> [GM93, BL91, PS90, SSSD90, SST91] </ref>, and rational convergence has been demonstrated for other spaces [OH91]. 5 This paper shows how, in a general way, this dichotomy can still be revealed under much weaker assumptions.
Reference: [SST91] <author> H. S. Seung, H. Sompolinsky, and N. Tishby. </author> <title> Learning curves in large neural networks. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory (COLT-91), </booktitle> <pages> pages 112-127, </pages> <year> 1991. </year>
Reference-contexts: By pursuing a uniform analysis, we are able to distinguish the conditions under which rational and exponential worst case convergence take place, based solely on the structure of the concept class C and ignoring any special properties of the domain distribution P (contrary to common suggestion <ref> [SST91, HKST94] </ref>). Most theoretical studies of learning curve behavior adopt a distribution specific model that assumes the domain distribution P is known a priori. Given these stronger assumptions, many researchers have shown that both rational and exponential learning curves are possible. <p> Given these stronger assumptions, many researchers have shown that both rational and exponential learning curves are possible. For example, exponential convergence has been demonstrated in many distribution specific analyses of particular con-cept spaces <ref> [GM93, BL91, PS90, SSSD90, SST91] </ref>, and rational convergence has been demonstrated for other spaces [OH91]. 5 This paper shows how, in a general way, this dichotomy can still be revealed under much weaker assumptions.
Reference: [Val84] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: Obtaining rapid convergence to zero error is more interesting if we know less about the target concept and domain distribution beforehand. Here, we adopt the model of prior knowledge first introduced by Valiant <ref> [Val84] </ref>: we assume the target concept c is known to belong to some class C, but that nothing is known about the domain distribution P (which could be arbitrary). Given this model, we naturally consider what can be achieved in the "worst case, distribution free" sense.
References-found: 17

