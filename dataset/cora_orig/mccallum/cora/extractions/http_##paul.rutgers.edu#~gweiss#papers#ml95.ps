URL: http://paul.rutgers.edu/~gweiss/papers/ml95.ps
Refering-URL: http://paul.rutgers.edu/~gweiss/papers/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: gary.m.weiss@att.com  
Title: Learning with Rare Cases and Small Disjuncts  
Author: Gary M. Weiss 
Address: 200 Laurel Avenue Middletown, NJ 07748  
Affiliation: Rutgers University/AT&T Bell Labs  
Date: 1995, 558-565.  
Note: Appears in Proceedings of the 12 th International Conference on Machine Learning, Morgan Kaufmann,  
Abstract: Systems that learn from examples often create a disjunctive concept definition. Small disjuncts are those disjuncts which cover only a few training examples. The problem with small disjuncts is that they are more error prone than large disjuncts. This paper investigates the reasons why small disjuncts are more error prone than large disjuncts. It shows that when there are rare cases within a domain, then factors such as attribute noise, missing attributes, class noise and training set size can result in small disjuncts being more error prone than large disjuncts and in rare cases being more error prone than common cases. This paper also assesses the impact that these error prone small disjuncts and rare cases have on inductive learning (i.e., on error rate). One key conclusion is that when low levels of attribute noise are applied only to the training set (the ability to learn the correct concept is being evaluated), rare cases within a domain are primarily responsible for making learning difficult.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ali, K. M. & Pazzani, M. J. </author> <year> (1992). </year> <title> Reducing the small disjuncts problem by learning probabilistic concept descriptions. </title> <type> Technical Report 92-111, </type> <institution> Irvine, CA: University of California at Irvine, Department of Information and Computer Sciences. </institution> <note> To appear in T. </note>
Reference: <editor> Petsche (ed.), </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, Volume 3, </booktitle> <address> Cambridge, Massachusetts. </address> <publisher> MIT Press. </publisher>
Reference: <author> Danyluk, A. P. & Provost, F. J. </author> <year> (1993). </year> <title> Small disjuncts in action: learning to diagnose errors in the local loop of the telephone network. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <pages> 81-88, </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This type of distribution has been seen in existing domains, including the KPa7KR chess endgame domain (Holte, et al., 1989), the NYNEX MAX domain <ref> (Danyluk & Provost, 1993) </ref> and the Wisconsin breast cancer domain (Weiss, 1994). Table 1 shows how the skewed distribution is formed by dividing the 32 distinct cases unequally into five bands and then duplicating the cases in each band by the specified amount. There are a total of 96 cases.
Reference: <author> Holte, R. C., Acker, L. E. & Porter, B.W. </author> <year> (1989). </year> <title> Concept learning and the problem of small disjuncts. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> 813-818. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Small disjuncts are those disjuncts which cover only a few training cases. The problem with small disjuncts is that they often have a much higher error rate than large disjuncts and are therefore considered error prone <ref> (Holte, Acker & Porter, 1989) </ref>. Furthermore, although small disjuncts may individually cover only a few examples, collectively they can cover a significant percentage (e.g., 20%) of the total examples. Thus, they cannot be disregarded if a high level of predictive accuracy is to be achieved. <p> WHY ARE SMALL DISJUNCTS SO ERROR PRONE? Although it is well known that small disjuncts are more error prone than large disjuncts, the only current explanation for this behavior is the effect of bias <ref> (Holte, et al., 1989) </ref>. This section will explain how attribute noise, missing attributes, class noise and training set size can interact with rare cases within a domain to cause error prone small disjuncts and error prone rare cases. <p> This section will explain how attribute noise, missing attributes, class noise and training set size can interact with rare cases within a domain to cause error prone small disjuncts and error prone rare cases. Bias will not be discussed since it has already been studied in detail <ref> (Holte, et al., 1989) </ref>. However, it should be noted that all of the factors studied in this paper are associated with a domain, while bias is a property of a specific inductive learner. Attribute noise will be considered first. <p> This type of distribution has been seen in existing domains, including the KPa7KR chess endgame domain <ref> (Holte, et al., 1989) </ref>, the NYNEX MAX domain (Danyluk & Provost, 1993) and the Wisconsin breast cancer domain (Weiss, 1994).
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> The effect of noise on concept learning. </title> <editor> In R. S. Michalski, J. G. Carbonell & T. </editor> <publisher> M. </publisher>
Reference-contexts: Noise was applied to either the class label or to all of the attributes, in either a random or systematic fashion. N% random noise signifies that, with probability N/100, a value is randomly selected from the remaining alternatives <ref> (Quinlan, 1986) </ref>. A very simple model of systematic noise is used in which noise only corrupts values in one direction. <p> Frequently, however, when the effects of noise are studied, noise is applied to the training set but not to the test set <ref> (Quinlan, 1986) </ref>. What is being studied in this case is the ability to learn the correct concept when noise is present. Table 2 summarizes the experimental parameters.
Reference: <editor> Mitchell (eds.), </editor> <booktitle> Machine Learning, an Artificial Intelligence Approach, </booktitle> <volume> Volume II, </volume> <pages> 149-166, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1991). </year> <title> Technical note: improved estimates for the accuracy of small disjuncts. </title> <journal> Machine Learning, </journal> <volume> 6(1). </volume>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These cases were then divided into a training and a test set, which were then fed into C4.5, a program for inducing decision trees from a set of preclassified training examples <ref> (Quinlan, 1993) </ref>. C4.5 was modified by the author to to collect statistics relating to disjunct size and to disable the default pruning strategy, since pruning might obscure the small disjuncts in the underlying concept definition.
Reference: <author> Weiss, G. M. </author> <year> (1994). </year> <title> The problem with noise and small disjuncts. </title> <type> Technical Report ML-TR-38, </type> <institution> New Brunswick, NJ: Rutgers University, Department of Computer Science. </institution>
Reference-contexts: This type of distribution has been seen in existing domains, including the KPa7KR chess endgame domain (Holte, et al., 1989), the NYNEX MAX domain (Danyluk & Provost, 1993) and the Wisconsin breast cancer domain <ref> (Weiss, 1994) </ref>. Table 1 shows how the skewed distribution is formed by dividing the 32 distinct cases unequally into five bands and then duplicating the cases in each band by the specified amount. There are a total of 96 cases. <p> Error rate (%) Delta level parity/eqparity voting/eqvote_ _____________________________________________________ _ _____________________________________________________ 2% 5.13/4.72 8.3% 2.68/2.04 27.1% _ _____________________________________________________ 5% 12.33/11.38 8.0% 6.04/4.36 16.2% _ _____________________________________________________ 20% 28.33/35.78 -23.0% 15.32/19.83 -25.7% _ _____________________________________________________ 6.1.3 The Two Effects of Attribute Noise Noise can be thought of as having two distinct, albeit interacting, effects <ref> (Weiss, 1994) </ref>.
References-found: 9

