URL: http://www.cs.princeton.edu/~ristad/papers/dwohl94a.ps.gz
Refering-URL: http://www.cs.princeton.edu/~ristad/bibliography.html
Root-URL: http://www.cs.princeton.edu
Title: LANGUAGE ACQUISITION IN THE MDL FRAMEWORK  
Author: JORMA RISSANEN AND ERIC SVEN RISTAD 
Abstract: The Minimum Description Length (MDL) principle provides guidance to the fundamental question of determining what a given set of observed data tells us about the underlying data generating machinery. Hence, in the broadest sense the MDL principle relates to the central question of all science, although its most useful applications have been to the more practical problem of fitting statistical models to data. In this article, we review the MDL principle and demonstrate how it may be profitably applied to the logical problem of language acquisition. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Robert Berwick, </author> <title> The acquisition of syntactic knowledge, </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1985. </year>
Reference-contexts: Generative Framework. According to modern generative linguistics, language acquisition is the process of deterministically fixing the values of a finite set of parameters upon exposure to a finite set of linguistic evidence <ref> [3, 1, 8] </ref>.
Reference: 2. <author> Gregory Chaitin, </author> <title> On the length of programs for computing finite binary sequences, </title> <booktitle> JACM 13 (1966), </booktitle> <pages> 547-569. </pages>
Reference-contexts: The MDL principle has its roots both in Shannon's information theory [21] and in the theories of inductive inference [9, 22] and algorithmic complexity <ref> [2, 10] </ref>. As we shall see, the shortest code length, relative to a class of models, also called the stochastic complexity, extends Shannon's information in a natural manner. <p> The principle of searching for the model or model class that permits the shortest encoding of the data, together with the model and the class, is called the MDL (Minimum Description Length) principle, [14, 15]. Finally, although Shannon's theorem (together with the algorithmic notion of information <ref> [22, 10, 2] </ref>) did provide the initial inspiration for this principle it is crucial to compare the models by the code length for the actually observed data sequence rather than by the mean code length.
Reference: 3. <author> Noam Chomsky, </author> <title> Lectures on government and binding, </title> <publisher> Foris Publications, </publisher> <address> Dordrecht, </address> <year> 1981. </year>
Reference-contexts: Generative Framework. According to modern generative linguistics, language acquisition is the process of deterministically fixing the values of a finite set of parameters upon exposure to a finite set of linguistic evidence <ref> [3, 1, 8] </ref>.
Reference: 4. <author> A.P. Dawid, </author> <title> Present position and potential developments: Some personal views, statistical theory, the prequential approach, </title> <journal> J. Royal Stat. Soc. A 147 (1984), </journal> <volume> no. 2, </volume> <pages> 278-292. </pages> <month> 5. </month> , <title> Prequential analysis, stochastic complexity and bayesian inference, </title> <booktitle> Proc. of the Fourth Valencia International Meeting on Bayesian Statistics, </booktitle> <month> April 15-20 </month> <year> 1991. </year>
Reference-contexts: The model selection criterion (6) without the code length interpretation was discovered independently by Dawid, <ref> [4, 5] </ref>, who called it the `prequential' principle or a principle for sequential prediction. MDL PRINCIPLE 11 4.3. Stochastic Complexity. Despite their many virtues, neither the MDL nor the PMDL criterion allow us to directly compare entire model classes.
Reference: 6. <author> Elan B. Dresher and Jonathan D. Kaye, </author> <title> A computational learning model for metrical phonology, </title> <booktitle> Cognition 34 (1990), </booktitle> <pages> 137-195. </pages>
Reference-contexts: This variant of the traditional approach is exemplified by the work of Dresher (this volume). It has variously been called a "cues" or "triggers" framework <ref> [6, 11] </ref>. The correctness requirement for such framework is that each l i output either the correct setting of i (`0' or `1') or "don't know" (`?').
Reference: 7. <author> Peter Elias, </author> <title> Universal codeword sets and representations of the integers, </title> <journal> IEEE Trans. Information Theory IT-21 (1975), </journal> <volume> no. 2, </volume> <pages> 194-203. </pages>
Reference-contexts: Because the next symbol is a `0', this number is the the binary representation of the encoded number. Had the next symbol been a `1' instead, then we would have continued decoding the length information. This code is called the Elias code, after its inventor Peter Elias <ref> [7] </ref>. The Elias code is particularly efficient for large integers. Its code length function is L W (n) = log fl (n) + log c where c = 2:8650 : : : is a constant needed to satisfy the Kraft Inequality with equality.
Reference: 8. <author> E.M. Gold, </author> <title> Language identification in the limit, </title> <booktitle> Information and Control 10 (1967), </booktitle> <pages> 447-474. </pages>
Reference-contexts: Our principal purpose in doing so is to demonstrate the MDL PRINCIPLE 13 advantages of the minimum description length framework for modeling language acquisition problems. The first framework we consider is based on modern linguistic theory and on Gold's "identification in the limit" paradigm <ref> [8] </ref>. It is exemplified by the work of Halle and Idsardi (this volume), Dresher (this volume), and Burzio (this volume). Here, the simplicity criterion is used by the linguist to guide the search for an empirically correct theory of the language user. <p> Generative Framework. According to modern generative linguistics, language acquisition is the process of deterministically fixing the values of a finite set of parameters upon exposure to a finite set of linguistic evidence <ref> [3, 1, 8] </ref>.
Reference: 9. <author> Nelson Goodman, </author> <title> On the simplicity of ideas, </title> <journal> Journal of Symbolic Logic 8 (1943), </journal> <volume> no. 4, </volume> <pages> 107-121. </pages>
Reference-contexts: The MDL principle has its roots both in Shannon's information theory [21] and in the theories of inductive inference <ref> [9, 22] </ref> and algorithmic complexity [2, 10]. As we shall see, the shortest code length, relative to a class of models, also called the stochastic complexity, extends Shannon's information in a natural manner.
Reference: 10. <author> A. N. </author> <title> Kolmogorov, Three approaches for defining the concept of `information quantity', </title> <booktitle> Problems of Information Transmission 1 (1965), </booktitle> <pages> 4-7. </pages>
Reference-contexts: The MDL principle has its roots both in Shannon's information theory [21] and in the theories of inductive inference [9, 22] and algorithmic complexity <ref> [2, 10] </ref>. As we shall see, the shortest code length, relative to a class of models, also called the stochastic complexity, extends Shannon's information in a natural manner. <p> The principle of searching for the model or model class that permits the shortest encoding of the data, together with the model and the class, is called the MDL (Minimum Description Length) principle, [14, 15]. Finally, although Shannon's theorem (together with the algorithmic notion of information <ref> [22, 10, 2] </ref>) did provide the initial inspiration for this principle it is crucial to compare the models by the code length for the actually observed data sequence rather than by the mean code length. <p> Where do the models and their classes come from? By a theorem of Kolmogorov's in the algorithmic theory of complexity, the problem of finding a best model of data has no algorithmic solution as soon as the model class is large enough to include the computable functions and distributions <ref> [10] </ref>. Hence, we do not wish to consider just one huge class of models as in the algorithmic theory of information, but rather we select the classes of interest suitably small using all available prior knowledge collected by others, human ingenuity, and even guess work.
Reference: 11. <author> David Lightfoot, </author> <title> The child's trigger experience: Degree-0 learnability, </title> <booktitle> Behavioral and Brain Sciences 12 (1989), </booktitle> <pages> 321-375. </pages>
Reference-contexts: This variant of the traditional approach is exemplified by the work of Dresher (this volume). It has variously been called a "cues" or "triggers" framework <ref> [6, 11] </ref>. The correctness requirement for such framework is that each l i output either the correct setting of i (`0' or `1') or "don't know" (`?').
Reference: 12. <author> J. Rissanen, T. Speed, and B. Yu, </author> <title> Density estimation by stochastic complexity, </title> <journal> IEEE Trans. Inf. Theory IT-38 (1992), </journal> <volume> no. </volume> <pages> 2. </pages>
Reference-contexts: Consequently, although there is some justification for this definition for data generated by parametric distributions, the predictive code length is often the shortest achievable and asymptotically strictly shorter than the two-part code length for data generated by nonparametric distributions <ref> [12, 13] </ref>. Moreover, the mixture distributions are restricted to relatively few but still important cases where the required mixture distribution exists and can be evaluated. For this coding we need a distribution (), the traditional `prior'.
Reference: 13. <author> J. Rissanen and B. Yu, </author> <title> Mdl learning, </title> <booktitle> Progress in Automation and Information Systems (J. </booktitle> <editor> Baras, ed.), </editor> <publisher> Springer Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Consequently, although there is some justification for this definition for data generated by parametric distributions, the predictive code length is often the shortest achievable and asymptotically strictly shorter than the two-part code length for data generated by nonparametric distributions <ref> [12, 13] </ref>. Moreover, the mixture distributions are restricted to relatively few but still important cases where the required mixture distribution exists and can be evaluated. For this coding we need a distribution (), the traditional `prior'.
Reference: 14. <author> Jorma Rissanen, </author> <title> Modeling by shortest data description, </title> <booktitle> Automatica 14 (1978), </booktitle> <pages> 465-471. </pages> <month> 15. </month> , <title> A universal prior for integers and estimation by minimum description length, </title> <journal> Annals of Statistics 11 (1983), </journal> <volume> no. 2, </volume> <pages> 416-431. </pages> <month> 16. </month> , <title> Universal coding, information, prediction, and estimation, </title> <journal> IEEE Trans. Inf. Theory IT-30 (1984), </journal> <volume> no. 4, </volume> <pages> 629-636. </pages> <month> 17. </month> , <title> Stochastic complexity and modeling, </title> <journal> Annals of Statistics 14 (1986), </journal> <pages> 1080-1100. </pages> <month> 18. </month> , <title> Stochastic complexity in statistical inquiry, </title> <publisher> World Scientific, </publisher> <address> Teaneck, NJ, </address> <year> 1989. </year>
Reference-contexts: The principle of searching for the model or model class that permits the shortest encoding of the data, together with the model and the class, is called the MDL (Minimum Description Length) principle, <ref> [14, 15] </ref>. Finally, although Shannon's theorem (together with the algorithmic notion of information [22, 10, 2]) did provide the initial inspiration for this principle it is crucial to compare the models by the code length for the actually observed data sequence rather than by the mean code length. <p> asymptotically the optimal code length for each parameter as log ^ ffi = 1 2 log n; and we get the total code length as M DL (x n jk) = log P (x n j ^ ) + 2 Having been discovered first, this was called the MDL criterion <ref> [14, 15] </ref>. It may be minimized over the number of the parameters k to get the optimal model as well as its complexity. Formally the same criterion but with a quite different Bayesian interpretation and valid for the exponential family of distributions only was also found in [20]. 4.2.
Reference: 19. <author> Jorma Rissanen and Eric Sven Ristad, </author> <title> Unsupervised classification with stochastic complexity, </title> <booktitle> Proceedings of the First US/Japan Conference on the Frontiers of Statistical Modeling (Dordrecht) (Hamparsum Bozdogan, </booktitle> <publisher> ed.), Kluwer Academic, </publisher> <year> 1993. </year>
Reference-contexts: The greatest obstacle to actually calculating the SC is the mathematical intractability of integrating over the model class M, which has only proven feasible for parametric model classes such as the normal mixture <ref> [18, 19] </ref>. Consequently, although there is some justification for this definition for data generated by parametric distributions, the predictive code length is often the shortest achievable and asymptotically strictly shorter than the two-part code length for data generated by nonparametric distributions [12, 13].
Reference: 20. <author> G. Schwarz, </author> <title> Estimating the dimension of a model, </title> <journal> Annals of Statistics 6 (1978), </journal> <pages> 461-464. </pages>
Reference-contexts: It may be minimized over the number of the parameters k to get the optimal model as well as its complexity. Formally the same criterion but with a quite different Bayesian interpretation and valid for the exponential family of distributions only was also found in <ref> [20] </ref>. 4.2. Predictive Coding. Suppose we do the coding sequentially as follows.
Reference: 21. <author> Claude Shannon, </author> <title> A mathematical theory of communication, </title> <journal> Bell System Technical Journal 27 (1948), </journal> <pages> 379-423, 623-656. </pages> <note> 18 JORMA RISSANEN AND ERIC SVEN RISTAD </note>
Reference-contexts: When we have to stop, we know that we have exhausted all the means provided by the studied models, and no one can teach us more about the data without proposing new models. The MDL principle has its roots both in Shannon's information theory <ref> [21] </ref> and in the theories of inductive inference [9, 22] and algorithmic complexity [2, 10]. As we shall see, the shortest code length, relative to a class of models, also called the stochastic complexity, extends Shannon's information in a natural manner.
Reference: 22. <author> R.J. Solomonoff, </author> <title> A formal theory of inductive inference, parts I and II, </title> <booktitle> Information and Control 7 (1964), </booktitle> <pages> 1-22, 224-254. </pages> <institution> IBM Almaden Research Center, </institution> <address> San Jose, CA 95120-6099 E-mail address: </address> <institution> rissanen@almaden.ibm.com Department of Computer Science, Princeton University, Princeton, </institution> <address> NJ 08544 E-mail address: ristad@princeton.edu </address>
Reference-contexts: The MDL principle has its roots both in Shannon's information theory [21] and in the theories of inductive inference <ref> [9, 22] </ref> and algorithmic complexity [2, 10]. As we shall see, the shortest code length, relative to a class of models, also called the stochastic complexity, extends Shannon's information in a natural manner. <p> The principle of searching for the model or model class that permits the shortest encoding of the data, together with the model and the class, is called the MDL (Minimum Description Length) principle, [14, 15]. Finally, although Shannon's theorem (together with the algorithmic notion of information <ref> [22, 10, 2] </ref>) did provide the initial inspiration for this principle it is crucial to compare the models by the code length for the actually observed data sequence rather than by the mean code length.
References-found: 17

