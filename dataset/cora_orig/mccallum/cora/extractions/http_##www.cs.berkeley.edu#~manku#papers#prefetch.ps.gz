URL: http://www.cs.berkeley.edu/~manku/papers/prefetch.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~manku/papers2.html
Root-URL: http://www.cs.berkeley.edu
Title: A New Voting Based Hardware Data Prefetch Scheme  
Author: Gurmeet Singh Manku Mukul R Prasad David A Patterson 
Address: Berkeley, CA 94720  
Affiliation: EECS Department, University of California at  
Abstract: The dramatic increase in the processor-memory gap in recent years has led to the development of techniques like data prefetching that hide the latency of cache misses. Two such hardware techniques are the stream buffer and the stride predictor. They have dissimilar architectures, are effective for different kinds of memory access patterns and require different amounts of extra memory bandwidth. We compare the performance of these two techniques and propose a scheme that unifies them 1 . Simulation studies on six benchmark programs confirm that the combined scheme is more effective in reducing the average memory access time (AMAT) than either of the two individually. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.L. Baer and T.F. Chen. </author> <title> An Effective On-chip Preload-ing Scheme to Reduce Data Access Penalty. </title> <booktitle> In Proc. of Supercomputing '91, </booktitle> <pages> pages 176-186, </pages> <year> 1991. </year>
Reference-contexts: Thus the growth of the ratio of the processor to memory performance is exponential. Its impact can be alleviated by either restructuring the design of the computer systems themselves, by designing algorithms that are aware of the memory hierarchy [3], or perhaps more easily, by using techniques like prefetching <ref> [1, 4, 7, 8, 12, 13, 14, 15] </ref> to hide the memory latency. Ten years ago, floating-point operations were considered expensive, often costing 10 times as much as an uncached memory reference. <p> According to our knowledge Sun Microsystems Inc. has recently filed a patent on this idea. We designed and evaluated the specific scheme described in this paper independently. and prefetching. Among the hardware prefetching techniques proposed, stride predictors <ref> [1] </ref> and stream buffers [7] are popular. The two techniques have different architectural costs and extra memory bandwidth requirements and work well for different memory access patterns. In this paper, we analyze and compare their performance under a common architecture and simulation framework . <p> McKee and Wulf [10] have merged off-chip stream buffers and the memory controller to improve memory system performance by reordering memory accesses. A different prefetching scheme, proposed by Lee [8], is to decode ahead in the instruction stream. More recently, Baer and Chen <ref> [1] </ref> have come up with the idea of maintaining the past history of references in a table and a look-ahead program counter for prefetch-ing. <p> Mehrotra [11] has improved upon this idea by be ing able to predict some indirect reference patterns occurring in sparse arrays and symbolic computations. 3 Design of the Combined Prefetching Scheme The block diagram of a processor with stride prediction based data prefetching <ref> [1] </ref> is shown in Figure 1 (a). A Reference Prediction Table (RPT) is used to remember past values of load/store PCs along with the addresses of the last memory location they reference and the corresponding stride that these references exhibit. Whenever a load/store instruction is issued, the RPT gets updated. <p> However, not all prefetches are actually issued. A small finite state machine associated with each row of the RPT decides whether the prefetch is really issued or not <ref> [1] </ref>. The LAPC is updated even when the PC is stalled upon a cache miss. Cache misses allow the LAPC to get ahead and prefetch. However, a threshold is set for the maximum distance between PC and LAPC [1]. <p> of the RPT decides whether the prefetch is really issued or not <ref> [1] </ref>. The LAPC is updated even when the PC is stalled upon a cache miss. Cache misses allow the LAPC to get ahead and prefetch. However, a threshold is set for the maximum distance between PC and LAPC [1]. Also, whenever a branch instruction is issued, the BPT is checked (before updation) to find out if the branch was mispredicted by it. If so, LAPC is set equal to the next value of PC. The Stream Buffer prefetching hardware [7, 13] is shown in Figure 1. <p> However, a stream buffer is the obvious choice for a set of accesses which are not strided but have good spatial locality and block-wise contiguity. The stride predictor would fail to detect such cases; they would be categorized as irregular <ref> [1] </ref> and no prefetch issued. Secondly, the stream buffer triggers a prefetch on the first cache Processor cycle time 4 ns (250 MHz) Memory bus 64-bit wide. DRAM 45 cycle (180 ns) latency. 8 bytes per cycle transfer time. L1 data cache 16KB direct-mapped. Write-back with write-allocate. <p> has been shown that a 256 entry direct-mapped RPT for the stride predictor is sufficient to hold the entire working set of data reference instructions for typical programs and that a good value for the threshold of the maximum distance between PC and LAPC is twice the cache miss latency <ref> [1] </ref>. The size of a direct-mapped Branch Prediction Table is typically 256 entries. A study by [13] has shown that 8 to 20 streams with 2 cache block buffers per stream suffice for good performance on scientific programs. <p> Similar assumptions have been made in other works on hardware data prefetching <ref> [1, 13] </ref>. For simplicity, we assumed that the processor stalls only upon a data cache miss. There are no stalls for prefetching; a block is prefetched instantaneously. Thus, the LAPC gets ahead only when there is a data cache miss. <p> And its performance as measured by the AMAT is almost always better than the stride predictor and the stream buffer individually. In the next section, we explain our observations based on the data access patterns of these benchmarks as characterized in [13] and <ref> [1] </ref>. 6 Program Analysis As pointed out in section 3, the stride predictor and stream buffer perform well for different types of data access patterns. The relative performance of the various prefetching schemes on different benchmarks, is analyzed in the following . SPICE is a general purpose circuit simulation program.
Reference: [2] <author> T.F. Chen and J-L. Baer. </author> <title> A Performance Study of Software and Hardware Data Prefetching Schemes. </title> <booktitle> In Appl. in Parallel and Distrib. Comp., IFIP WG10.3 Caracas, Venezuela, </booktitle> <month> Apr </month> <year> 1994. </year>
Reference-contexts: Factors that determine the success of any prefetch scheme are the ability to predict future data references, the timing of prefetches and the amount of extra memory bandwidth required. If the prefetch is not issued well in advance, the processor stalls on a cache miss <ref> [2] </ref>. However, if issued very early, it might evict a useful block from the cache. Early studies in hardware prefetching focussed on simple one block lookahead schemes, i.e. upon referencing block i, block i + 1 is to be prefetched. Smith [15] studied variations of this scheme. <p> Compared with the stride predictor, the combined scheme has fewer prefetched blocks which are requested too late, for all benchmarks. This fraction is much smaller for IJPEG and TRFD. This represents the fraction of prefetches that would most likely contribute to hit-wait cycles <ref> [2] </ref> were a better timing model used. In Fig. 5, useless prefetches correspond to cache blocks that were never referenced by any instruction. The remaining are all useful.
Reference: [3] <author> D.C. Burger D, J.R. Goodman, and A. Kagi. </author> <title> The Declining Effectiveness of Dynamic Caching for General-purpose Microprocessors. </title> <type> Technical Report TR-1261, </type> <institution> Univ. of Wis-consin, Deptt. of Comp. </institution> <address> Sc., </address> <year> 1994. </year>
Reference-contexts: Thus the growth of the ratio of the processor to memory performance is exponential. Its impact can be alleviated by either restructuring the design of the computer systems themselves, by designing algorithms that are aware of the memory hierarchy <ref> [3] </ref>, or perhaps more easily, by using techniques like prefetching [1, 4, 7, 8, 12, 13, 14, 15] to hide the memory latency. Ten years ago, floating-point operations were considered expensive, often costing 10 times as much as an uncached memory reference.
Reference: [4] <author> E. Gornish, E. Granston, and A. Veidenbaum. </author> <title> Compiler-Directed Data Prefetching in Multiprocessors with Memory Hierarchies. </title> <booktitle> In ICS, </booktitle> <pages> pages 354-68, </pages> <year> 1990. </year>
Reference-contexts: Thus the growth of the ratio of the processor to memory performance is exponential. Its impact can be alleviated by either restructuring the design of the computer systems themselves, by designing algorithms that are aware of the memory hierarchy [3], or perhaps more easily, by using techniques like prefetching <ref> [1, 4, 7, 8, 12, 13, 14, 15] </ref> to hide the memory latency. Ten years ago, floating-point operations were considered expensive, often costing 10 times as much as an uncached memory reference.
Reference: [5] <author> J.L. Hennessey and D.A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach, Second Edition. </title> <publisher> Morgan Kaufmann publishers, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction It has been estimated that the performance of the fastest available microprocessors is increasing at approximately 55% per year, while the speed of memory systems has been growing at only about 7% per year <ref> [5] </ref>. Thus the growth of the ratio of the processor to memory performance is exponential. <p> To bridge the processor-memory gap, architects have started employing more aggressive techniques such as larger on chip and off chip caches, wider memory buses, cache bypassing <ref> [5] </ref>, non-blocking caches, out of order execution, pipelined memory systems 1 We thank Sharad Mehrotra of Sun Microsystems Inc. for suggesting the idea of combining the two schemes, for a research project. According to our knowledge Sun Microsystems Inc. has recently filed a patent on this idea. <p> Whenever a load/store instruction is issued, the RPT gets updated. Prefetching is implemented through a look-ahead program counter (LAPC) which runs a few instructions ahead of the regular program counter. It updates itself using a Branch Prediction Table (BPT) <ref> [5] </ref>. For each value that the LAPC takes, the RPT is checked. If an entry is found, a prefetch request is generated by adding the last memory reference with the stride recorded in the RPT. However, not all prefetches are actually issued. <p> Assuming perfect pipelining and no data cache stalls, the average memory access time (AMAT) <ref> [5] </ref> % Data references % Useful prefetches % Data references that are prefetched that are early prefetched early Benchmark Stride Stream Comb. Stride Stream Comb.
Reference: [6] <author> M.D. Hill and A.J. Smith. </author> <title> Evaluating Associativity in CPU Caches. </title> <journal> IEEE Tran. on Comp., </journal> <volume> 38(12) </volume> <pages> 1612-30, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: Section 4 presents our simulation methodology. Results are presented in section 5 followed by performance analysis and conclusions in sections 6 and 7. 2 Background and Previous Work What does prefetching buy us? In terms of the 3C model proposed by Hill <ref> [6] </ref>, prefetching reduces compulsory and capacity misses. However, it might increase conflict misses in case a prefetched block is placed directly into the cache much earlier than needed, thereby evicting some useful block.
Reference: [7] <author> N.P. Jouppi. </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proc. of the 17th Ann. Intl. Symp. on Comp. Arch., </booktitle> <pages> pages 364-73, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Thus the growth of the ratio of the processor to memory performance is exponential. Its impact can be alleviated by either restructuring the design of the computer systems themselves, by designing algorithms that are aware of the memory hierarchy [3], or perhaps more easily, by using techniques like prefetching <ref> [1, 4, 7, 8, 12, 13, 14, 15] </ref> to hide the memory latency. Ten years ago, floating-point operations were considered expensive, often costing 10 times as much as an uncached memory reference. <p> According to our knowledge Sun Microsystems Inc. has recently filed a patent on this idea. We designed and evaluated the specific scheme described in this paper independently. and prefetching. Among the hardware prefetching techniques proposed, stride predictors [1] and stream buffers <ref> [7] </ref> are popular. The two techniques have different architectural costs and extra memory bandwidth requirements and work well for different memory access patterns. In this paper, we analyze and compare their performance under a common architecture and simulation framework . <p> Early studies in hardware prefetching focussed on simple one block lookahead schemes, i.e. upon referencing block i, block i + 1 is to be prefetched. Smith [15] studied variations of this scheme. An extension to this idea, namely stream buffers, was proposed by Jouppi <ref> [7] </ref>. McKee and Wulf [10] have merged off-chip stream buffers and the memory controller to improve memory system performance by reordering memory accesses. A different prefetching scheme, proposed by Lee [8], is to decode ahead in the instruction stream. <p> Also, whenever a branch instruction is issued, the BPT is checked (before updation) to find out if the branch was mispredicted by it. If so, LAPC is set equal to the next value of PC. The Stream Buffer prefetching hardware <ref> [7, 13] </ref> is shown in Figure 1. It consists of a set of buffers, called stream buffers, each capable of holding a certain number of cache blocks equal to the depth of the stream buffer. <p> Firstly, in its simplest form <ref> [7] </ref>, the stream buffer cannot detect a stream with a stride that spans several blocks. On the other hand, the stride predictor can easily detect arbitrary sized strides.
Reference: [8] <author> R.L. Lee. </author> <title> The Effectiveness of Caches and Data Prefetch Buffers in Large-Scale Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Deptt. of Comp. Sc., Univ. of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: Thus the growth of the ratio of the processor to memory performance is exponential. Its impact can be alleviated by either restructuring the design of the computer systems themselves, by designing algorithms that are aware of the memory hierarchy [3], or perhaps more easily, by using techniques like prefetching <ref> [1, 4, 7, 8, 12, 13, 14, 15] </ref> to hide the memory latency. Ten years ago, floating-point operations were considered expensive, often costing 10 times as much as an uncached memory reference. <p> Smith [15] studied variations of this scheme. An extension to this idea, namely stream buffers, was proposed by Jouppi [7]. McKee and Wulf [10] have merged off-chip stream buffers and the memory controller to improve memory system performance by reordering memory accesses. A different prefetching scheme, proposed by Lee <ref> [8] </ref>, is to decode ahead in the instruction stream. More recently, Baer and Chen [1] have come up with the idea of maintaining the past history of references in a table and a look-ahead program counter for prefetch-ing.
Reference: [9] <author> J.D. McCalpin. </author> <title> Memory Bandwidth and Machine Balance in Current High Performance Computers. </title> <journal> IEEE Tech. Committee on Comp. Arch. Newsletter, </journal> <month> Dec </month> <year> 1995. </year>
Reference-contexts: Today the situation is dramatically reversed, with the fastest current processors able to perform 100 or more floating-point operations in the time required to service a cache miss. As an extreme example, McCalpin <ref> [9] </ref> has shown that several current high-end machines run simple arithmetic kernels for out-of-cache operands at 4-5% of their rated peak speeds | that means that 95-96% of their time is spent waiting for cache misses.
Reference: [10] <author> S.A. McKee et al. </author> <title> Design and Evaluation of Dynamic Access Ordering Hardware. </title> <type> ICS, </type> <month> May </month> <year> 1996. </year>
Reference-contexts: Early studies in hardware prefetching focussed on simple one block lookahead schemes, i.e. upon referencing block i, block i + 1 is to be prefetched. Smith [15] studied variations of this scheme. An extension to this idea, namely stream buffers, was proposed by Jouppi [7]. McKee and Wulf <ref> [10] </ref> have merged off-chip stream buffers and the memory controller to improve memory system performance by reordering memory accesses. A different prefetching scheme, proposed by Lee [8], is to decode ahead in the instruction stream.
Reference: [11] <author> S. Mehrotra. </author> <title> Data Prefetch Mechanisms for Accelerating Symbolic and Numeric Computations. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, </institution> <year> 1996. </year>
Reference-contexts: A different prefetching scheme, proposed by Lee [8], is to decode ahead in the instruction stream. More recently, Baer and Chen [1] have come up with the idea of maintaining the past history of references in a table and a look-ahead program counter for prefetch-ing. Mehrotra <ref> [11] </ref> has improved upon this idea by be ing able to predict some indirect reference patterns occurring in sparse arrays and symbolic computations. 3 Design of the Combined Prefetching Scheme The block diagram of a processor with stride prediction based data prefetching [1] is shown in Figure 1 (a).
Reference: [12] <author> C.M. Mowry. </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Thus the growth of the ratio of the processor to memory performance is exponential. Its impact can be alleviated by either restructuring the design of the computer systems themselves, by designing algorithms that are aware of the memory hierarchy [3], or perhaps more easily, by using techniques like prefetching <ref> [1, 4, 7, 8, 12, 13, 14, 15] </ref> to hide the memory latency. Ten years ago, floating-point operations were considered expensive, often costing 10 times as much as an uncached memory reference.
Reference: [13] <author> S. Palacharla and R.E. Kessler. </author> <title> Evaluating Stream Buffers as a Secondary Cache Replacement. </title> <booktitle> In ISCA, </booktitle> <year> 1992. </year>
Reference-contexts: Thus the growth of the ratio of the processor to memory performance is exponential. Its impact can be alleviated by either restructuring the design of the computer systems themselves, by designing algorithms that are aware of the memory hierarchy [3], or perhaps more easily, by using techniques like prefetching <ref> [1, 4, 7, 8, 12, 13, 14, 15] </ref> to hide the memory latency. Ten years ago, floating-point operations were considered expensive, often costing 10 times as much as an uncached memory reference. <p> Also, whenever a branch instruction is issued, the BPT is checked (before updation) to find out if the branch was mispredicted by it. If so, LAPC is set equal to the next value of PC. The Stream Buffer prefetching hardware <ref> [7, 13] </ref> is shown in Figure 1. It consists of a set of buffers, called stream buffers, each capable of holding a certain number of cache blocks equal to the depth of the stream buffer. <p> The size of a direct-mapped Branch Prediction Table is typically 256 entries. A study by <ref> [13] </ref> has shown that 8 to 20 streams with 2 cache block buffers per stream suffice for good performance on scientific programs. Based on a survey of commercial microprocessors in 1996, we used the hardware configuration described in Table 1. <p> Similar assumptions have been made in other works on hardware data prefetching <ref> [1, 13] </ref>. For simplicity, we assumed that the processor stalls only upon a data cache miss. There are no stalls for prefetching; a block is prefetched instantaneously. Thus, the LAPC gets ahead only when there is a data cache miss. <p> And its performance as measured by the AMAT is almost always better than the stride predictor and the stream buffer individually. In the next section, we explain our observations based on the data access patterns of these benchmarks as characterized in <ref> [13] </ref> and [1]. 6 Program Analysis As pointed out in section 3, the stride predictor and stream buffer perform well for different types of data access patterns. The relative performance of the various prefetching schemes on different benchmarks, is analyzed in the following .
Reference: [14] <author> A.K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Dept. of Comp. Sc., Rice Univ., </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Thus the growth of the ratio of the processor to memory performance is exponential. Its impact can be alleviated by either restructuring the design of the computer systems themselves, by designing algorithms that are aware of the memory hierarchy [3], or perhaps more easily, by using techniques like prefetching <ref> [1, 4, 7, 8, 12, 13, 14, 15] </ref> to hide the memory latency. Ten years ago, floating-point operations were considered expensive, often costing 10 times as much as an uncached memory reference.
Reference: [15] <author> A.J. Smith. </author> <title> Prefetching in Supercomputer Instruction Caches. </title> <booktitle> In Supercomputing 92, </booktitle> <pages> pages 588-597, </pages> <year> 1992. </year>
Reference-contexts: Thus the growth of the ratio of the processor to memory performance is exponential. Its impact can be alleviated by either restructuring the design of the computer systems themselves, by designing algorithms that are aware of the memory hierarchy [3], or perhaps more easily, by using techniques like prefetching <ref> [1, 4, 7, 8, 12, 13, 14, 15] </ref> to hide the memory latency. Ten years ago, floating-point operations were considered expensive, often costing 10 times as much as an uncached memory reference. <p> However, if issued very early, it might evict a useful block from the cache. Early studies in hardware prefetching focussed on simple one block lookahead schemes, i.e. upon referencing block i, block i + 1 is to be prefetched. Smith <ref> [15] </ref> studied variations of this scheme. An extension to this idea, namely stream buffers, was proposed by Jouppi [7]. McKee and Wulf [10] have merged off-chip stream buffers and the memory controller to improve memory system performance by reordering memory accesses.
Reference: [16] <institution> Sun Microsystems Laboratories Inc. </institution> <note> Introduction to Shade, </note> <month> April </month> <year> 1993. </year>
Reference-contexts: In the meantime, the stride predictor detects the stream and then takes over. The block diagram for the combined scheme is shown in Fig. 2. 4 Simulation Methodology We implemented our simulator in Shade <ref> [16] </ref>, an instruction level simulation environment for the SUN SPARC v8 instruction set, and carried out immediate simulation on six popular benchmarks. The choice of various parameters for the hardware configuration that we simulated was influenced by several known results.
Reference: [17] <author> S.J.E. Wilton and N. Jouppi. CACTI: </author> <title> An Enhanced Cache Access and Cycle Time Model. </title> <journal> IEEE J. of Solid-State Circuits, </journal> <volume> 31(5) </volume> <pages> 377-88, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: The choice of various parameters for the hardware configuration that we simulated was influenced by several known results. For example, although recent improvements in integrated-circuit densities allow for allocation of more die area for on-chip caches, the accompanying increase in cycle time limits their size and associativity <ref> [17] </ref>. The net effect is that primary caches have small size and low associativity, a typical configuration being 8K 2-way set associative with 32 byte line size.
Reference: [18] <author> W. Yamamoto. </author> <title> Cache and Memory System Design of the NS486SXF Integrated Processor. </title> <booktitle> In High-Performance System Design Conference. </booktitle> <institution> National Semiconductor Corporation, </institution> <year> 1996. </year>
Reference-contexts: The net effect is that primary caches have small size and low associativity, a typical configuration being 8K 2-way set associative with 32 byte line size. The NS486SXF Integrated Processor cache <ref> [18] </ref> is one example where a direct mapped cache was implemented for shorter cycle time even though a 2-way set associative cache would have yielded 13 to 30% less cache misses.
References-found: 18

