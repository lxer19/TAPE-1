URL: ftp://ftp.cs.ucla.edu/pub/ficus/ratner_thesis.ps.gz
Refering-URL: http://ficus-www.cs.ucla.edu/ficus-members/ratner/research.html
Root-URL: http://www.cs.ucla.edu
Title: Selective Replication: Fine-Grain Control of Replicated Files  
Author: David H. Ratner 
Degree: A thesis submitted in partial satisfaction of the requirements for the degree Master of Science in Computer Science by  
Date: 1995  
Affiliation: University of California Los Angeles  
Abstract-found: 0
Intro-found: 1
Reference: [FM82] <author> Michael J. Fischer and Alan Michael. </author> <title> "Sacrificing Serializability to Attain High Availability of Data in an Unreliable Network." </title> <booktitle> In Proceedings of the ACM Symposium on Principles of Database Systems, </booktitle> <month> March </month> <year> 1982. </year>
Reference-contexts: The first is the potential loss of updates. The second is the resolution of the create/delete ambiguity, first noted by Fischer and Michael in <ref> [FM82] </ref>.
Reference: [GHM + 90] <author> Richard G. Guy, John S. Heidemann, Wai Mak, Thomas W. Page, Jr., Gerald J. Popek, and Dieter Rothmeier. </author> <title> "Implementation of the Ficus Replicated File System." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 63-71. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: All these examples suggest that the system cannot restrict a given replica's possible communication partners. All replicas must be peers, to permit any-to-any communication topologies. Fine-grain replication control: Existing optimistic services, such as Fi-cus <ref> [GHM + 90] </ref> and Coda [SKK + 90] replicate on the volume granularity, where a volume is defined to be smaller than a disk partition but larger than a directory [SHN + 85]. For example, a user's home directory and all sub-directories might constitute a volume. <p> The potential for disconnected updates, or updates produced at isolated machines, implies that the lost-update notification scheme must be a two-way process, because when two partitioned machines resume communication, each could be knowledgeable of updates unknown by the others. Ficus <ref> [Guy91, GHM + 90] </ref> is one system that supports optimistic replication. Ficus provides a peer-to-peer, rather than client-server, model, meaning there is no master copy. Updates generated at any one replica are just as valid as those generated at any other. <p> of the two-level granularity structure, which will include: * New data structures necessary to implement the two-level hierarchy * System invariants enforced by the two-level hierarchy * Tools for controlling replication factors at both granularities * Replica selection 3.1 First-class replication Ficus already provides first-class replication at the volume granularity <ref> [GHM + 90] </ref>. We build on this base to provide first-class replication at the file granularity by forcing all participants to maintain a volume replica for each file under replication control. Storing the volume replica guarantees that all objects within will be first-class objects. <p> This is the task of replica selection. Note that replica selection as it is described here only operates within a specific volume. Access to remote volume replicas involves a different process, since the lists of volume replicas are maintained in separate data structures, and is described in <ref> [GHM + 90] </ref>. Replica selection assumes that in the usual case, the user (or replica selection client) desires access to a data-storing replica. Occasionally a specific replica might be requested for a specific purpose. In these cases, replica selection is bypassed, and the requested replica is accessed (if it exists). <p> These changes are resolved using a version-vector approach as described in Chapter 3, Section 3.2.1. Finally, reconciliation performs the "standard" reconciliation actions described in <ref> [GHM + 90] </ref>, such as conflict detection, update propagation, and create/delete ambiguity resolution. 45 4.3 Reconciliation optimizations We would like to consider what optimizations to the above algorithm can be made to reduce the total reconciliation time from initiation to completion. <p> The cp performance is barely acceptable at 35% overhead, but the find test measured a 153% overhead in elapsed time. These results are unfortunately due to the Ficus implementation used by selective replication, described in <ref> [GHM + 90] </ref>. The Ficus design and implementation accrues four I/Os beyond the normal Unix overhead on each file open in a non-recently opened directory. The extra I/Os are due to the way Ficus currently interacts with the UFS layer and maintains its replication information.
Reference: [GPP93] <author> Richard G. Guy, Gerald J. Popek, and Thomas W. Page, Jr. </author> <title> "Consistency Algorithms for Optimistic Replication." </title> <booktitle> In Proceedings of the First International Conference on Network Protocols. IEEE, </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: Ficus uses a fully-distributed two-phase algorithm to ensure that all replicas are knowledgeable of the garbage collection process and can eventually complete it, although all participants may never be present simultaneously <ref> [GPP93] </ref>. Ficus replicates at the volume granularity. Volumes can be created at arbitrary positions in the namespace using simple tools. In addition, volumes can be dynamically added, deleted, or moved at any time. <p> The local state of "no links" can be a transient case, as new names are created at remote replicas and learned later via reconciliation; the global state of "no links" is persistent. Guy developed algorithms in <ref> [GPP93] </ref> for garbage collection in a distributed environment which ensure that all participants terminate correctly and tolerate 16 long-term communication failures. <p> Maintaining it is unsatisfactory since a file replica deletion causes remnants to forever remain locally. A superior method is to adapt Guy's solution for resolving the create/delete 27 ambiguity in garbage collection of file system objects, discussed in <ref> [GPP93] </ref>. Guy keeps a record of the action temporarily and executes a distributed algorithm to guarantee both that all replicas learn of the action and that the record is eventually removed. <p> This chapter will discuss how garbage collection works in Ficus, and why its solution is not applicable to selective replication. It will then present a solution for selective replication, and discuss its performance and implementation, concluding with an informal proof of correctness. 5.1 Garbage collection in Ficus Guy, in <ref> [GPP93] </ref>, discusses how garbage collection of files operates in Ficus. However, his results assume full-replication: that is, Guy assumes that each file is replicated at each volume replica. For this reason, Guy's algorithm does not directly apply to selective replication. <p> Additionally, due to the analogy between file removals in selective replication and volume removals in Ficus, such a solution would be directly applicable to the Ficus problem as well. 5.2 Garbage collection for selective replication As previously mentioned, Ficus uses algorithms developed by Guy in <ref> [GPP93] </ref> to garbage collect files, but these algorithms assume the files are fully-replicated. We will expand the algorithms to remove the assumption. To briefly review Guy's algorithms, Guy developed a fully-distributed, two-phase algorithm which executes independently at all volumes replicas. <p> Guy's algorithm requires three phases for garbage collection to complete at all replicas (two phases for the algorithm itself and one phase for all replicas to learn of the completion) <ref> [GPP93] </ref>. Therefore, the total running time of garbage collection on our r-replica file is 3 * O (r) = O (r), which satisfies the design constraints of Chapter 2. Garbage collection now consists of two parts | participant informing and Guy's algorithm. <p> In this way we handle file replica additions or deletions in parallel, and the modified garbage collection algorithm has the same performance properties as Guy's original algorithm. 5.4 Garbage collection implementation and correctness The actual implementation of the new garbage collection algorithm is very much like Guy describes it in <ref> [GPP93] </ref>, with the modifications to identify and inform the garbage collection participants executing in parallel. Each replica initiates the algorithm when the number of local links for a file drops to zero. If a new link is discovered, garbage collection is terminated. <p> Each replica initiates the algorithm when the number of local links for a file drops to zero. If a new link is discovered, garbage collection is terminated. When all replicas complete Guy's two-phase algorithm, then each replica independently removes the file and de-allocates its resources. Guy showed in <ref> [GPP93] </ref> that his garbage collection algorithm operates correctly. In order to show that the new algorithm is also correct, it remains to prove that the informing-phase eventually terminates, and that all sites that are supposed to participate do in fact participate.
Reference: [Guy91] <author> Richard G. Guy. Ficus: </author> <title> A Very Large Scale Reliable Distributed File System. </title> <type> Ph.D. dissertation, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <month> June </month> <year> 1991. </year> <note> Also available as UCLA technical report CSD-910018. </note>
Reference-contexts: The potential for disconnected updates, or updates produced at isolated machines, implies that the lost-update notification scheme must be a two-way process, because when two partitioned machines resume communication, each could be knowledgeable of updates unknown by the others. Ficus <ref> [Guy91, GHM + 90] </ref> is one system that supports optimistic replication. Ficus provides a peer-to-peer, rather than client-server, model, meaning there is no master copy. Updates generated at any one replica are just as valid as those generated at any other. <p> The underlying reconciliation algorithms are discussed in <ref> [Guy91] </ref>. While these algorithms are topology independent, meaning that no specific reconciliation topology is required for correctness, different topologies yield different results in terms of performance and message complexity, or the total number of inter-replica messages exchanged globally.
Reference: [HKM + 88] <author> John Howard, Michael Kazar, Sherri Menees, David Nichols, Ma-hadev Satyanarayanan, Robert Sidebotham, and Michael West. </author> <title> "Scale and Performance in a Distributed File System." </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: We qualitatively analyzed both topologies to reach our conclusions. 6.2 The benchmarks Six benchmarks were used in this evaluation. The first test is the modified Andrew Benchmark (mab) <ref> [HKM + 88, Ous90] </ref>, which is intended to model a normal mix of filing operations and hence be representative of performance in actual use.
Reference: [HP94] <author> John S. Heidemann and Gerald J. Popek. </author> <title> "File-System Development with Stackable Layers." </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12(1) </volume> <pages> 58-89, </pages> <year> 1994. </year> <note> Preliminary version available as UCLA technical report CSD-930019. </note>
Reference-contexts: Thus, replica selection needs no special mechanism in this case. The transparency is due to the vnode interface [Kle86] and the Ficus layered architecture <ref> [HP94] </ref>. 3.4.2.4 Algorithm The following is the high level implementation of the replica selection algorithm. /* * Procedure ReplicaSelect: * Return a data-storing replica of the file F in directory D */ Procedure ReplicaSelect (directory object D, file component F) begin let R = the currently-accessing-replica of object D /* *
Reference: [Kle86] <author> S. R. Kleiman. "Vnodes: </author> <title> An Architecture for Multiple File System Types in Sun Unix." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 238-247. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1986. </year>
Reference-contexts: Thus, replica selection needs no special mechanism in this case. The transparency is due to the vnode interface <ref> [Kle86] </ref> and the Ficus layered architecture [HP94]. 3.4.2.4 Algorithm The following is the high level implementation of the replica selection algorithm. /* * Procedure ReplicaSelect: * Return a data-storing replica of the file F in directory D */ Procedure ReplicaSelect (directory object D, file component F) begin let R = the
Reference: [KS91] <author> James J. Kistler and Mahadev Satyanarayanan. </author> <title> "Disconnected Operation in the Coda File System." </title> <type> Technical Report CMU-CS-91-166, </type> <institution> Carnegie-Mellon University, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Between partitions, LOCUS allows concurrent updates, and provides a special mechanism to merge partitions together. The mechanism, however, is overly complex and does not scale, because it attempts to always maintain a correct picture of partition membership. 73 7.4 Coda The Coda file system <ref> [KS91, SKK + 90] </ref> uses two separate replication policies. First, Coda provides first-class replication at the volume granularity on a backbone of servers, aptly named server replication. The degree of replication and identity of the replication sites must be specified at volume creation time, unlike in Ficus.
Reference: [KS92] <author> James J. Kistler and Mahadev Satyanarayanan. </author> <title> "Disconnected Operation in the Coda File System." </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 3-25, </pages> <year> 1992. </year> <month> 81 </month>
Reference-contexts: Specifically, the conflict-resolution process suffers as a direct result of the second-class nature. Updates generated at second-class replicas which, if integrated, would create conflicts with the first-class replica are essentially aborted, and must be re-integrated by hand at a later time <ref> [KS92] </ref>. Such actions can be painful and time-consuming for the user, especially since first-class updates that create conflicts can often be automatically resolved [RHR + 94]. 2.2.2 First-class On the opposite side of the replication quality spectrum lies first-class replication.
Reference: [KS93] <author> Puneet Kumar and Mahadev Satyanarayanan. </author> <title> "Supporting Application-Specific Resolution in an Optimistically Replicated File System." </title> <booktitle> In Proceedings of the Fourth Workshop on Workstation Operating Systems, </booktitle> <pages> pp. 66-70, </pages> <address> Napa, California, </address> <month> October </month> <year> 1993. </year> <note> IEEE. </note>
Reference-contexts: Conservative replication strategies by definition cannot offer this ability uniformly. Instead, the above modes of use require optimistic replication, which allows independent updates to all replicas, detecting and resolving possible conflicts later <ref> [KS93, RHR + 94] </ref>. Support for peer replication: The models' communication patterns are not always known a priori. A given developer may communicate with a variety of other collaborators, depending on the specific data subset and the colleagues' geographic locations.
Reference: [Ous90] <author> John K. Ousterhout. </author> <title> "Why Aren't Operating Systems Getting Faster As Fast as Hardware?" In USENIX Conference Proceedings, </title> <journal> pp. </journal> <pages> 247-256. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: We qualitatively analyzed both topologies to reach our conclusions. 6.2 The benchmarks Six benchmarks were used in this evaluation. The first test is the modified Andrew Benchmark (mab) <ref> [HKM + 88, Ous90] </ref>, which is intended to model a normal mix of filing operations and hence be representative of performance in actual use.
Reference: [PGPH90] <author> Gerald J. Popek, Richard G. Guy, Thomas W. Page, Jr., and John S. Heidemann. </author> <title> "Replication in Ficus Distributed File Systems." </title> <booktitle> In Proceedings of the Workshop on Management of Replicated Data, </booktitle> <pages> pp. 20-25. </pages> <publisher> IEEE, </publisher> <month> November </month> <year> 1990. </year>
Reference-contexts: An interesting problem is to avoid the quadratic cost when suitable inter-site communication is available, but still gracefully handle degraded communication. The Ficus solution is an adaptive ring <ref> [PGPH90] </ref>, which is a ring capable of dynamic reconfiguration during network failures and adaptation to volume replica additions and deletions. However, the adaptive ring assumes that all replicas are capable of communicating the sum total of global knowledge, and selective replication breaks this assumption, as we will see.
Reference: [PPR + 83] <author> D. Stott Parker, Jr., Gerald Popek, Gerard Rudisin, Allen Stoughton, Bruce J. Walker, Evelyn Walton, Johanna M. Chow, David Edwards, Stephen Kiser, and Charles Kline. </author> <title> "Detection of Mutual Inconsistency in Distributed Systems." </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 9(3) </volume> <pages> 240-247, </pages> <month> May </month> <year> 1983. </year>
Reference-contexts: Updates are tracked using version vectors <ref> [PPR + 83] </ref>. A version vector is an array of monotonically-increasing counters, one counter per replica. Each counter i tracks the total number of known updates generated by replica i. <p> For example, we allow a site A to add file replicas at a site B, on B's behalf and possibly without even B's knowledge. The conflicts are dealt with by adapting the version vector <ref> [PPR + 83] </ref> technology to the status vector. Each status element is actually implemented as a monotonically-increasing counter, like the elements of the version vector. The status value is determined by applying a mapping function from the set of integers to the set of status values.
Reference: [PWC + 81] <author> Gerald Popek, Bruce Walker, Johanna Chow, David Edwards, Charles Kline, Gerald Rudisin, and Greg Thiel. </author> <title> "LOCUS: A Network Transparent, High Reliability Distributed System." </title> <booktitle> In Proceedings of the Eighth Symposium on Operating Systems Principles, </booktitle> <pages> pp. 169-177. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1981. </year>
Reference-contexts: Furthermore, the in-memory tree could potentially absorb much of main memory. 7.3 LOCUS The LOCUS operating system <ref> [WPE + 83, PWC + 81] </ref> essentially provides the volume notion and allows selective replication within the volume. In LOCUS, replication is provided at the volume granularity.
Reference: [RHR + 94] <author> Peter Reiher, John S. Heidemann, David Ratner, Gregory Skinner, and Gerald J. Popek. </author> <title> "Resolving File Conflicts in the Ficus File System." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 183-195. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: Conservative replication strategies by definition cannot offer this ability uniformly. Instead, the above modes of use require optimistic replication, which allows independent updates to all replicas, detecting and resolving possible conflicts later <ref> [KS93, RHR + 94] </ref>. Support for peer replication: The models' communication patterns are not always known a priori. A given developer may communicate with a variety of other collaborators, depending on the specific data subset and the colleagues' geographic locations. <p> This reflects the peer nature of replication in Ficus. When version-vectors are found to conflict, their respective file replicas are said to be "in conflict". Special mechanisms must be invoked to resolve the conflict <ref> [RHR + 94] </ref>. When one version-vector dominates the other, the corresponding replica's data is more recent. As replicas communicate and the pairwise version-vector comparison continues throughout the network, the most recent data propagates, and eventually all replicas converge to a common global state. <p> Such actions can be painful and time-consuming for the user, especially since first-class updates that create conflicts can often be automatically resolved <ref> [RHR + 94] </ref>. 2.2.2 First-class On the opposite side of the replication quality spectrum lies first-class replication. First-class replication, while suffering from additional complexity, provides increased functionality, most notably the ability for any-to-any communication.
Reference: [SBM90] <author> Alex Siegel, Kenneth Birman, and Keith Marzullo. "Deceit: </author> <title> A Flexible Distributed File System." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 51-61. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: However, NFS does not support replication of the data, only remote access to it, and operates strictly at the volume granularity. In addition, the volumes must be in well-known locations, and the system must be manually changed when the location changes. 7.2 Deceit The Deceit file system <ref> [SBM90, Sie92] </ref>, places all files into one "volume" and allows each individual file to be replicated independently with varying numbers of replicas. In this sense it provides selective replication. <p> However, Deceit employs a conservative approach to replication, namely a writer-token mechanism, and as such is unable to provide the high availability offered by optimistic mechanisms. In addition, Deceit cannot tolerate long-term network partitions; instead, only a related failure known as a virtual partition <ref> [SBM90] </ref>, which eventually corrects itself, can be tolerated. Furthermore, Deceit uses a vastly-simplified garbage collection algorithm, one not guaranteed to operate correctly in environments with common network partitions. These factors make the Deceit system unsuitable for the modes of use discussed in Chapter 1.
Reference: [SGK + 85] <author> Russel Sandberg, David Goldberg, Steve Kleiman, Dan Walsh, and Bob Lyon. </author> <title> "Design and Implementation of the Sun Network File System." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 119-130. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1985. </year>
Reference-contexts: Sun Microsystem's NFS is an important example of a service which provides access to non-local files. Deceit, LOCUS and Coda are all different examples of replication services which have made different design choices and assumptions about their target environments. 7.1 NFS SUN Microsystem's NFS <ref> [SGK + 85] </ref> allows remote access to non-local files using a special mount system call. The remote file system is "mounted" into a specific location in the local file system.
Reference: [SHN + 85] <author> Mahadev Satyanarayanan, John H. Howard, David A. Nichols, Rob-ert N. Sidebotham, Alfred Z. Spector, and Michael J. West. </author> <title> "The 82 ITC Distributed File System: </title> <booktitle> Principles and Design." In Proceed--ings of the Tenth Symposium on Operating Systems Principles, </booktitle> <pages> pp. 35-50. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1985. </year>
Reference-contexts: All replicas must be peers, to permit any-to-any communication topologies. Fine-grain replication control: Existing optimistic services, such as Fi-cus [GHM + 90] and Coda [SKK + 90] replicate on the volume granularity, where a volume is defined to be smaller than a disk partition but larger than a directory <ref> [SHN + 85] </ref>. For example, a user's home directory and all sub-directories might constitute a volume. The volume is used because it provides several key benefits, such as integrity, locality, and naming services. However, the volume is often too coarse a granularity for replication control.
Reference: [Sie92] <author> Alexander Siegel. </author> <title> Performance in Flexible Distributed File Systems. </title> <type> Ph.D. dissertation, </type> <institution> Cornell, </institution> <month> February </month> <year> 1992. </year> <note> Also available as Cornell technical report TR 92-1266. </note>
Reference-contexts: However, NFS does not support replication of the data, only remote access to it, and operates strictly at the volume granularity. In addition, the volumes must be in well-known locations, and the system must be manually changed when the location changes. 7.2 Deceit The Deceit file system <ref> [SBM90, Sie92] </ref>, places all files into one "volume" and allows each individual file to be replicated independently with varying numbers of replicas. In this sense it provides selective replication.
Reference: [SKK + 90] <author> Mahadev Satyanarayanan, James J. Kistler, Puneet Kumar, Maria E. Okasaki, Ellen H. Siegel, and David C. Steere. "Coda: </author> <title> A Highly Available File System for a Distributed Workstation Environment." </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4) </volume> <pages> 447-459, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: All these examples suggest that the system cannot restrict a given replica's possible communication partners. All replicas must be peers, to permit any-to-any communication topologies. Fine-grain replication control: Existing optimistic services, such as Fi-cus [GHM + 90] and Coda <ref> [SKK + 90] </ref> replicate on the volume granularity, where a volume is defined to be smaller than a disk partition but larger than a directory [SHN + 85]. For example, a user's home directory and all sub-directories might constitute a volume. <p> Between partitions, LOCUS allows concurrent updates, and provides a special mechanism to merge partitions together. The mechanism, however, is overly complex and does not scale, because it attempts to always maintain a correct picture of partition membership. 73 7.4 Coda The Coda file system <ref> [KS91, SKK + 90] </ref> uses two separate replication policies. First, Coda provides first-class replication at the volume granularity on a backbone of servers, aptly named server replication. The degree of replication and identity of the replication sites must be specified at volume creation time, unlike in Ficus.
Reference: [WPE + 83] <author> Bruce Walker, Gerald Popek, Robert English, Charles Kline, and Greg Thiel. </author> <title> "The LOCUS Distributed Operating System." </title> <booktitle> In Proceedings of the Ninth Symposium on Operating Systems Principles, </booktitle> <pages> pp. 49-70. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1983. </year> <month> 83 </month>
Reference-contexts: Furthermore, the in-memory tree could potentially absorb much of main memory. 7.3 LOCUS The LOCUS operating system <ref> [WPE + 83, PWC + 81] </ref> essentially provides the volume notion and allows selective replication within the volume. In LOCUS, replication is provided at the volume granularity.
References-found: 21

