URL: http://polaris.cs.uiuc.edu/reports/1214.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Multiple Omega Networks for Parallel Processing  
Author: Jose Eduardo Moreira 
Keyword: High Performance Processing  
Note: To be presented at the IV Brazilian Symposium on Computer Architecture  October 26-29, 1992, S~ao Paulo, SP, Brazil  
Address: 465 CSRL, 1308W Main St Urbana, IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Email: moreira@csrd.uiuc.edu  
Phone: phone: 217-244-0049 fax: 217-244-1351  
Abstract: In this paper we propose the use of multiple Omega networks as an interconnection system for shared memory multiprocessors. This allows us to achieve a much higher bandwidth of communication, accommodating the needs of current high-performance processors, including those with multiple memory ports. We also obtain a very scalable system, by defining a processor-switch-memory building block, that can be used in systems with processor count in the range of a few units to several thousands. The performance evaluation of multiple Omega networks is done through a simple analytical model that allows us to compare their performance to a that of a single network, and investigate alternatives for processors with multiple memory ports. The results show that the performance (in terms of bandwidth and latency of communication) of systems with multiple networks is more stable with respect to variations in systems parameters, such as number of processors and memory access rate, than that of systems with just a single network. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> John B. Andrews, Carl J. Beckmann, and David K. Poulsen. </author> <title> Notification and Multicast Networks for Synchronization and Coherence. </title> <journal> Journal of Parallel and Disrtributed Computing, </journal> <volume> (15), </volume> <year> 1992. </year>
Reference-contexts: The authors claim that such change allows a Cray Y-MP like system to scale up to 64 processors. Franklin and Dhar [2] present some considerations on physical constraints and modularity issues in the design of a large (2048 fi 2048) interconnection network. Andrews, Beckmann and Poulsen <ref> [1] </ref> have developed some networks that provide efficient cache coherence schemes for systems with hundreds and thousands of processors.
Reference: [2] <author> Mark A. Franklin and Sanjay Dhar. </author> <title> On Designing Inteconnection Networks for Multiprocessors. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 208-215, </pages> <address> St. Charles, Illinois, </address> <year> 1986. </year>
Reference-contexts: Each physical bank, in a system such as the Cray Y-MP, is replaced by a logical bank consisting of a number of physical banks. The authors claim that such change allows a Cray Y-MP like system to scale up to 64 processors. Franklin and Dhar <ref> [2] </ref> present some considerations on physical constraints and modularity issues in the design of a large (2048 fi 2048) interconnection network. Andrews, Beckmann and Poulsen [1] have developed some networks that provide efficient cache coherence schemes for systems with hundreds and thousands of processors.
Reference: [3] <author> William T. Hsu and Pen-Chung Yew. </author> <title> The Performance of Hierarchical Systems with Wiring Constraints. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, pages I:9-16, </booktitle> <address> St. Charles, Illinois, </address> <year> 1991. </year>
Reference-contexts: Since the use of caches reduces the memory bandwidth required by processors, this is another solution for the problem of providing enough memory bandwidth. 9 Hsu and Yew <ref> [3] </ref> propose the use of hierarchical (clus-tered) systems to reduce the bandwidth requirements of interconnection network, and show that this is particularly important in face of packaging constraints. 6 Conclusion We have shown how multiple Omega networks can be used to build a shared memory multiprocessor that has a higher bandwidth
Reference: [4] <author> Young Man Kim and Kyungsook Y. Lee. </author> <title> Performance Analysis of Buffered Banyan Network under Nonuniform Traffic. </title> <booktitle> In Proceedings of the 1989 International Conference on Supercomputing, </booktitle> <pages> pages 452-460, </pages> <address> Crete, Greece, </address> <month> June 5-9, </month> <year> 1989. </year>
Reference-contexts: The performance model used here for multistage interconnection networks is very similar to that of Patel [8]. More complete analytical performance models can be found in <ref> [4, 9] </ref>. A basic assumption for the derivation of our performance models is that the processors generate uniformly distributed random requests for memory modules. That means that in a request a processor requests a given memory module with probability 1=M .
Reference: [5] <author> J. Konicek et al. </author> <title> The Organization of the Cedar System. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, pages I:49-56, </booktitle> <address> St. Charles, Illinois, </address> <year> 1991. </year>
Reference-contexts: We also only consider systems utilizing complete networks: those that use all the inputs and outputs. Systems with a varied number of processors can be built by utilizing only a subset of a complete network (we call these incomplete networks). The Cedar multiprocessor <ref> [5] </ref>, for instance, utilizes a 2-stage Omega network of 8 fi 8 switches.
Reference: [6] <author> Duncan H. Lawrie. </author> <title> Access and Alignment of Data in an Array Processor. </title> <journal> IEEE Transactions 10 on Computers, </journal> <volume> C-24(12):1145-1155, </volume> <month> December </month> <year> 1975. </year>
Reference-contexts: We proceed with some performance evaluation of multiple Omega networks, showing their advantage over single networks, and conclude with some discussion of related work. 2 The Omega Network An Omega (or multistage shu*e-exchange) network <ref> [6, 17] </ref> can be described by a pair of integers = (N; K), where N is the number of input and output ports to the network, and K is the radix of the network. We only consider the simplest case, N = K n .
Reference: [7] <author> T. N. Mudge and B. A. Makrucki. </author> <title> Probabilistic Analysis of a Crossbar Switch. </title> <booktitle> In Proceedings of the 9th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 311-319, </pages> <address> Austin, Texas, </address> <month> April 26-29, </month> <year> 1982. </year>
Reference-contexts: If we assume uniformly distributed random selection of outputs by the inputs, and always one request per input every cycle, the throughput of a N fiN crossbar approaches the limit of 1e 1 = 0:632 as N approaches 1 (for a 16 fi 16 crossbar the value is 0:644) <ref> [7] </ref>. The Omega network cannot do better than this, and as the number of stages in the network increases, the throughput ratio between the Omega and the crossbar decreases. See Figure 8 for a plot of these throughputs (the formulas used to obtain these figures are derived in section 4). <p> The limiting throughput (as N ! 1) of m N fi N crossbars operating in parallel to connect N processors to mN memories is given by m (1 e 1=m ) <ref> [7] </ref>, notice that m N fi N parallel crossbars is essentially the same as one N fi mN crossbar. This is an upper limit for the throughput of multiple Omega networks in parallel, and Figure 9 is a plot of this limit for different values of m. <p> Figures 16 and 17 compare the performance of these two approaches, using expressions derived in the next section. 4 Performance Evaluation The basic expressions used in this paper for the performance of a crossbar switch can be found in <ref> [7] </ref>. The performance model used here for multistage interconnection networks is very similar to that of Patel [8]. More complete analytical performance models can be found in [4, 9].
Reference: [8] <author> Janak H. Patel. </author> <title> Performance of Processor Memory Interconnections for Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(10):771-780, </volume> <month> October </month> <year> 1981. </year>
Reference-contexts: The performance model used here for multistage interconnection networks is very similar to that of Patel <ref> [8] </ref>. More complete analytical performance models can be found in [4, 9]. A basic assumption for the derivation of our performance models is that the processors generate uniformly distributed random requests for memory modules.
Reference: [9] <author> Mahib Rahman and David G. Meyer. </author> <title> General Analytic Models for the Performance Analysis of Unique and Redundant Path Interconnection Networks. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, pages I:584-591, </booktitle> <address> St. Charles, Illinois, </address> <year> 1991. </year>
Reference-contexts: The performance model used here for multistage interconnection networks is very similar to that of Patel [8]. More complete analytical performance models can be found in <ref> [4, 9] </ref>. A basic assumption for the derivation of our performance models is that the processors generate uniformly distributed random requests for memory modules. That means that in a request a processor requests a given memory module with probability 1=M .
Reference: [10] <author> K. A. Robbins and S. Robbins. </author> <title> Bus Conflicts for Logical Memory Banks on a Cray Y-MP type Processor System. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, pages I:21-24, </booktitle> <address> St. Charles, Illinois, </address> <year> 1991. </year>
Reference-contexts: Shing and Ni [11] address the problem of memory and interconnection network contention by essentially time-multiplexing physical resources (network switches and memory modules). Each user of a physical resource has a designated time slot in which it can use the resource. Robbins and Robbins <ref> [10] </ref> solution to increase the efficiency of shared memory systems involves no change in the interconnection network, but only in the memory system. Each physical bank, in a system such as the Cray Y-MP, is replaced by a logical bank consisting of a number of physical banks.
Reference: [11] <author> Honda Shing and Lionel M. Ni. </author> <title> A Conflict-Free Memory Design for Multiprocessors. </title> <booktitle> In Proceedings of Supercomputing'91, </booktitle> <pages> pages 46-55, </pages> <address> Albu-querque, New Mexico, </address> <month> November 18-22, </month> <year> 1991. </year>
Reference-contexts: The authors' results show that wide fanout gives better performance than narrow fanout, and that an equal number of memories and (total) processors ports is of little value for supercomputer design. Shing and Ni <ref> [11] </ref> address the problem of memory and interconnection network contention by essentially time-multiplexing physical resources (network switches and memory modules). Each user of a physical resource has a designated time slot in which it can use the resource.
Reference: [12] <author> Howard Jay Siegel and William Tsun yuk Hsu. </author> <title> Computer Architecture Concepts and Systems, chapter Interconnection Networks. </title> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: In between these two extremes, there is a rich variety of alternatives, which have been the subject of extensive research. A very popular class of interconnection networks, which has received considerable attention from both the industry and academia is the multistage interconnection network (MIN) <ref> [12] </ref>. These networks are composed of multiple stages of crossbar switches, connecting processors and memory modules. We will assume here that the reader is somewhat familiar with the general structure of MINs.
Reference: [13] <author> J. E. Smith, W.-C. Hsu, and C. Hsiung. </author> <title> Future General Purpose Supercomputer Architectures. </title> <booktitle> In Proceedings of Supercomputing'90, </booktitle> <pages> pages 796-804, </pages> <address> New York, New York, </address> <month> November 12-16, </month> <year> 1990. </year>
Reference-contexts: Smith et al. <ref> [13] </ref> discuss the need for supercomputers to support scalability, and recognize that the dominant scalability problem is the support of shared memory for multiple processors and memory ports.
Reference: [14] <author> J. E. Smith and W. R. Taylor. </author> <title> Accurate Modeling of Interconnection Networks in Vector Supercomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Supercomputing, </booktitle> <pages> pages 264-273, </pages> <address> Cologne, Germany, </address> <month> June 17-21, </month> <year> 1991. </year>
Reference-contexts: One of the solutions they propose is the extension of multistage interconnection networks such as used in the Cray Y-MP, that in this particular case connects 8 processor to 256 memory banks. Smith and Tay-lor <ref> [14] </ref> do a performance analysis of such networks, which have a fanout, since they connect a number of processors to a larger number of memory modules.
Reference: [15] <author> Ted Szymanski and Chien Fang. </author> <title> Design and Analysis of Buffered Crossbars and Banyans with Cut-Through Switching. </title> <booktitle> In Proceedings of Supercomputing'90, </booktitle> <pages> pages 264-273, </pages> <address> New York, New York, </address> <month> November 12-16, </month> <year> 1990. </year>
Reference-contexts: Szymanski and Fang <ref> [15] </ref> have analyzed several configurations of switches and banyan networks, and compared the performance of a single network to that of multiple parallel networks (with equivalent total cost) and concluded that single networks perform better for small systems and light loads, while parallel networks are better for larger systems or heavier
Reference: [16] <author> Peixiong Tang and Raul H. Mendez. </author> <title> Memory Conflicts and Machine Performance. </title> <booktitle> In Proceedings of Supercomputing'89, </booktitle> <pages> pages 826-831, </pages> <address> Reno, Nevada, </address> <month> November 13-17, </month> <year> 1989. </year>
Reference-contexts: This more than compensates for the extra crossbar stage and the replication in the virtual processor approach. Similar considerations are also valid for the systems with 16 processors. 5 Related Work Tang and Mendez <ref> [16] </ref> have identified the efficiency of data transfer between processor and memory as a limiting factor in the performance of vector computers, and they concluded that the number of memory modules in a system should be proportional to the product of memory access ports and the memory cycle time (in units

References-found: 16

