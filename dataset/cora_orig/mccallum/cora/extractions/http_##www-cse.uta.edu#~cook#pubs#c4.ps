URL: http://www-cse.uta.edu/~cook/pubs/c4.ps
Refering-URL: http://cygnus.uta.edu/subdue/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: galalg@cse.uta.edu  
Title: Discovering Concepts in Structural Data  
Author: Diane J. Cook, Lawrence B. Holder, and Gehad Galal 
Address: fcook, holder,  
Affiliation: Department of Computer Science Engineering University of Texas at Arlington  
Abstract: The explosive growth of databases in scientific, industrial, and commercial fields has not been accompanied by a similar growth in our ability to analyze and digest this data. The increasing amount and complexity of data creates an urgent need for automatic database analysis tools. This trend is evident in molecular biology data which continues to grow in both size and complexity. This research outlines a general approach to automatically discover repetitive and functional concepts in large structural databases. The Subdue system discovers substructures that compress the database and represent structural concepts in the data. By replacing previously-discovered substructures in the data, multiple passes of Subdue produce a hierarchical description of the structural regularities in the data. To increase the flexibility of the system, we describe methods of incorporating domain-dependent information into the discovery process. Because discovery systems such as Subdue are very computationally expensive, we also explore ways of parallelizing the system to improve scalability.
Abstract-found: 1
Intro-found: 1
Reference: [ Bunke and Allermann, 1983 ] <author> H. Bunke and G. Allermann. </author> <title> Inexact graph matching for structural pattern recognition. </title> <journal> Pattern Recognition Letters, </journal> <volume> 1(4) </volume> <pages> 245-253, </pages> <year> 1983. </year>
Reference-contexts: Furthermore, we want to associate a distance measure between a pair of graphs consisting of a given substructure and a subgraph of the input graph. We adopt the approach to inexact graph match given by Bunke and Allermann <ref> [ Bunke and Allermann, 1983 ] </ref> . In this inexact match approach, each distortion of a graph is assigned a cost. A distortion is described in terms of basic transformations such as deletion, insertion, and substitution of vertices and edges.
Reference: [ Cheesman and Stutz, 1996 ] <author> P. Cheesman and J. Stutz. </author> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, chapter 6, </booktitle> <pages> pages 153-180. </pages> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: In response to this problem, a number of researchers have developed techniques for discovering concepts in databases. These techniques work well for data expressed in a non-structural, attribute-value representation, and address issues of data relevance, missing data, noise and uncertainty, and utilization of domain knowledge <ref> [ Cheesman and Stutz, 1996; Fisher, 1987 ] </ref> . However, recent data acquisition projects are collecting structural data describing the relationships among the data objects. Correspondingly, there exists a need for techniques to analyze and discover concepts in structural databases [ Fayyad et al., 1996 ] .
Reference: [ Derthick, 1991 ] <author> M. Derthick. </author> <title> A minimal encoding approach to feature discovery. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 565-571, </pages> <year> 1991. </year>
Reference-contexts: The MDL principle has been used for decision tree induction [ Quinlan and Rivest, 1989 ] , image processing [ Pednault, 1989; Pentland, 1989; Leclerc, 1989 ] , concept learning from relational data <ref> [ Derthick, 1991 ] </ref> , and learning models of non-homogeneous engineering domains [ Rao and Lu, 1992 ] . We demonstrate how the minimum description length principle can be used to discover substructures in complex data.
Reference: [ Djoko et al., 1996 ] <author> S. Djoko, D. J. Cook, and L. B. Holder. </author> <title> Discovering informative structural concepts using domain knowledge. </title> <journal> IEEE Expert, </journal> <volume> 10 </volume> <pages> 59-68, </pages> <year> 1996. </year>
Reference-contexts: A more detailed description of these experiments can be found in the literature <ref> [ Djoko et al., 1996 ] </ref> . Scalability A goal of knowledge discovery in database (KDD) systems is to discover knowledge in large databases that cannot be effectively processed by humans. For this reason KDD systems are required to handle very large databases.
Reference: [ Fayyad et al., 1996 ] <author> U. M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> From data mining to knowledge discovery: An overview. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, chapter 1, </booktitle> <pages> pages 1-34. </pages> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: However, recent data acquisition projects are collecting structural data describing the relationships among the data objects. Correspondingly, there exists a need for techniques to analyze and discover concepts in structural databases <ref> [ Fayyad et al., 1996 ] </ref> . One method for discovering knowledge in structural data is the identification of common substructures within the data.
Reference: [ Fisher, 1987 ] <author> Doug Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: In response to this problem, a number of researchers have developed techniques for discovering concepts in databases. These techniques work well for data expressed in a non-structural, attribute-value representation, and address issues of data relevance, missing data, noise and uncertainty, and utilization of domain knowledge <ref> [ Cheesman and Stutz, 1996; Fisher, 1987 ] </ref> . However, recent data acquisition projects are collecting structural data describing the relationships among the data objects. Correspondingly, there exists a need for techniques to analyze and discover concepts in structural databases [ Fayyad et al., 1996 ] .
Reference: [ Geist et al., 1994 ] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM: Parallel Virtual Machine, A User's guide and Tutorial for Networked Parallel Computing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: ART 2.57 6.09 48.45 162.38 32 ART 3.16 10.31 34.12 82.99 Table 4: ART databases distributed version speedups Because the data is partitioned among the processors, SP-Subdue can also utilize the increased memory resources of a network of workstations using communication software such as the Parallel Virtual Machine (PVM) system <ref> [ Geist et al., 1994 ] </ref> . We implemented SP-Subdue on a network of 14 Pentium PCs using PVM. The speedup results are given in Tables 3 and 4. By partitioning the database effectively, SP-Subdue proves to be a highly scalable system.
Reference: [ Karypis and Kumar, 1995 ] <author> G. Karypis and V. Kumar. </author> <title> Multilevel k-way partitioning scheme for irregular graphs. </title> <type> Technical Report TR-95-XXX, </type> <institution> Department of Computer Sience, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: In partitioning the graph we want to balance the work load equally between processors while retaining as much information as possible (edges along which the graph is partitioned may represent important information). SP-Subdue utilizes the Metis graph partitioning package <ref> [ Karypis and Kumar, 1995 ] </ref> . Metis accepts a graph with weights assigned to edges and vertices, and tries to partition the graph so that the sum of the weights of the cut edges is minimized and the sum of vertex weights in each partition is roughly equal.
Reference: [ Leclerc, 1989 ] <author> Y. G. Leclerc. </author> <title> Constructing simple stable descriptions for image partitioning. </title> <journal> International Journal of Computer Vision, </journal> <volume> 3(1) </volume> <pages> 73-102, </pages> <year> 1989. </year>
Reference-contexts: The MDL principle has been used for decision tree induction [ Quinlan and Rivest, 1989 ] , image processing <ref> [ Pednault, 1989; Pentland, 1989; Leclerc, 1989 ] </ref> , concept learning from relational data [ Derthick, 1991 ] , and learning models of non-homogeneous engineering domains [ Rao and Lu, 1992 ] . We demonstrate how the minimum description length principle can be used to discover substructures in complex data.
Reference: [ Pednault, 1989 ] <author> E. P. D. Pednault. </author> <title> Some experiments in applying inductive inference principles to surface reconstruction. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1603-1609, </pages> <year> 1989. </year>
Reference-contexts: The MDL principle has been used for decision tree induction [ Quinlan and Rivest, 1989 ] , image processing <ref> [ Pednault, 1989; Pentland, 1989; Leclerc, 1989 ] </ref> , concept learning from relational data [ Derthick, 1991 ] , and learning models of non-homogeneous engineering domains [ Rao and Lu, 1992 ] . We demonstrate how the minimum description length principle can be used to discover substructures in complex data.
Reference: [ Pentland, 1989 ] <author> A. Pentland. </author> <title> Part segmentation for object recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 82-91, </pages> <year> 1989. </year>
Reference-contexts: The MDL principle has been used for decision tree induction [ Quinlan and Rivest, 1989 ] , image processing <ref> [ Pednault, 1989; Pentland, 1989; Leclerc, 1989 ] </ref> , concept learning from relational data [ Derthick, 1991 ] , and learning models of non-homogeneous engineering domains [ Rao and Lu, 1992 ] . We demonstrate how the minimum description length principle can be used to discover substructures in complex data.
Reference: [ Quinlan and Rivest, 1989 ] <author> J. R. Quinlan and R. L. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: The MDL principle has been used for decision tree induction <ref> [ Quinlan and Rivest, 1989 ] </ref> , image processing [ Pednault, 1989; Pentland, 1989; Leclerc, 1989 ] , concept learning from relational data [ Derthick, 1991 ] , and learning models of non-homogeneous engineering domains [ Rao and Lu, 1992 ] .
Reference: [ Rao and Lu, 1992 ] <author> R. B. Rao and S. C. Lu. </author> <title> Learning engineering models with the minimum description length principle. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 717-722, </pages> <year> 1992. </year>
Reference-contexts: The MDL principle has been used for decision tree induction [ Quinlan and Rivest, 1989 ] , image processing [ Pednault, 1989; Pentland, 1989; Leclerc, 1989 ] , concept learning from relational data [ Derthick, 1991 ] , and learning models of non-homogeneous engineering domains <ref> [ Rao and Lu, 1992 ] </ref> . We demonstrate how the minimum description length principle can be used to discover substructures in complex data. In particular, a substructure is evaluated based on how well it can compress the entire data set.

References-found: 13

