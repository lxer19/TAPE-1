URL: http://www.eecs.umich.edu/PPP/icpads94.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Title: Grouping Array Layouts to Reduce Communication and Improve Locality of Parallel Programs  
Author: Tien-Pao Shih Edward S. Davidson 
Address: Ann Arbor, MI 48109-2122  
Affiliation: Department of Electrical Engineering and Computer Science University of Michigan  
Abstract: A data layout method, array grouping, is proposed to improve communication efficiency and cache utilization of parallel programs containing indirect array references or nonunit stride indexing. Conditions on where to apply this technique are specified in a series of theorems. The technique is then applied to a real finite element application. The experimental results show that communication is reduced by 15%, and data subcache misses by 40%, on 56 processors of the KSR1 parallel computer. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Codd, </author> <title> "Further normalization of the data base relational model," </title> <booktitle> in Courant Computer Science Symposium 6, Data Base Systems (R. </booktitle> <address> Rustin, </address> <publisher> ed.), </publisher> <pages> pp. 33-64, </pages> <publisher> Prentice-Hall, </publisher> <month> May </month> <year> 1972. </year>
Reference-contexts: Although the example contains only constant strides, our technique is also applicable to nonconsecutive references caused by irregular (computed) strides and indirect array references. A similar data layout concept involving the interleaving of elements (or groups of elements) from arrays has been used in relational data base applications <ref> [1, 2] </ref> where records that are typically accessed together are stored contiguously in memory. This idea was also suggested to reduce page faults in paged memory systems [3]. <p> This observation leads to Theorem 4.1. Theorem 4.1 For I = (i 1 ; i 2 ; ; i n ) and 8j 2 <ref> [1; r] </ref>, e j = g: (1) if 8k 2 [2; n], d k s=g, then T G (I) &lt; T U (I), (2) if 8k 2 [2; n], d k 2 [q; s=g), and c = 0, then T G (I) T U (I), and (3) if 8k 2 [2; <p> Theorem 4.2 generalizes the above theorem for arbitrary reference distances. Theorem 4.2 If 8j 2 <ref> [1; r] </ref> e j = g and eq = s (i.e., c = 0), then T G (I) T U (I). Proof: Since eq = s and rg = e, the number of element groups per subpage of each of the given arrays, s=g = rq, is also an integer. <p> Theorem 4.3 For I = (i 1 ; i 2 ; ; i n ), t = T U (I)=r, and 8j 2 <ref> [1; r] </ref>, e j = g: (2) if e ng=qt; T G (I) T U (I) Proof: Since t is the number of transactions for a given array, ng=t is the average number of useful data in a transaction before grouping.
Reference: [2] <author> J. Martin, </author> <title> Computer Data-Base Organization. </title> <address> Engle-wood Cliffs, New Jersey: </address> <publisher> Prentice-Hall, </publisher> <year> 1977. </year>
Reference-contexts: Although the example contains only constant strides, our technique is also applicable to nonconsecutive references caused by irregular (computed) strides and indirect array references. A similar data layout concept involving the interleaving of elements (or groups of elements) from arrays has been used in relational data base applications <ref> [1, 2] </ref> where records that are typically accessed together are stored contiguously in memory. This idea was also suggested to reduce page faults in paged memory systems [3]. <p> If 8k 2 <ref> [2; n] </ref>, d k f, each referenced element group is in a distinct subpage and jIj = n transactions are required. If 8k 2 [2; n], 1 d k &lt; f , then a set of (x + (s=f ) fi (i n i 1 + 1) + x 0 )=s <p> If 8k 2 <ref> [2; n] </ref>, d k f, each referenced element group is in a distinct subpage and jIj = n transactions are required. If 8k 2 [2; n], 1 d k &lt; f , then a set of (x + (s=f ) fi (i n i 1 + 1) + x 0 )=s consecutive subpages is referenced, where x is size of the initial unreferenced portion of the first subpage, and x 0 is the final unreferenced <p> This observation leads to Theorem 4.1. Theorem 4.1 For I = (i 1 ; i 2 ; ; i n ) and 8j 2 [1; r], e j = g: (1) if 8k 2 <ref> [2; n] </ref>, d k s=g, then T G (I) &lt; T U (I), (2) if 8k 2 [2; n], d k 2 [q; s=g), and c = 0, then T G (I) T U (I), and (3) if 8k 2 [2; n], d k 2 [1; q), and c = 0, <p> Theorem 4.1 For I = (i 1 ; i 2 ; ; i n ) and 8j 2 [1; r], e j = g: (1) if 8k 2 <ref> [2; n] </ref>, d k s=g, then T G (I) &lt; T U (I), (2) if 8k 2 [2; n], d k 2 [q; s=g), and c = 0, then T G (I) T U (I), and (3) if 8k 2 [2; n], d k 2 [1; q), and c = 0, then T G (I) T U (I). <p> [1; r], e j = g: (1) if 8k 2 <ref> [2; n] </ref>, d k s=g, then T G (I) &lt; T U (I), (2) if 8k 2 [2; n], d k 2 [q; s=g), and c = 0, then T G (I) T U (I), and (3) if 8k 2 [2; n], d k 2 [1; q), and c = 0, then T G (I) T U (I). Theorem 4.2 generalizes the above theorem for arbitrary reference distances.
Reference: [3] <author> L.-Y. Chang and H. G. Dietz, </author> <title> "Data layout optimization and code transformation for paged memory systems," </title> <type> Tech. Rep. </type> <institution> TR-EE 90-43, Purdue University, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: This idea was also suggested to reduce page faults in paged memory systems <ref> [3] </ref>. We use this technique to deal with a different class of problems involving communication efficiency and cache performance on cache-based shared-memory parallel systems, and present a systematic method of analysis for this class of problems. Section 2 describes the program, programming environment, and machine models that underlay the method.
Reference: [4] <author> H. Zima and B. Chapman, </author> <title> Supercompilers for Parallel and Vector Computers. </title> <address> New York: </address> <publisher> Addison-Wesley, </publisher> <address> 1991. </address> <publisher> ACM Press. </publisher>
Reference-contexts: The work of a serial section is executed by only one processor, that of a parallel section is partitioned and executed concurrently by multiple processors, and that of a replicate section is replicated and executed by every processor <ref> [4] </ref>. We focus initially on a simple, but common parallel program structure (Fig. 1). This structure consists of a series of parallel sections, each enclosed by a parallel construct that enforces barriers (replicate sections) at the beginning and end of the sec tion. <p> In a COMA system, data items do not have static home addresses, 1 A loop is normalized if its initial loop index and step size are one <ref> [4] </ref>. they are instead dynamically moved and replicated among the caches in various nodes in response to an application's reference patterns. The KSR1 is chosen as our experimental testbed because it is the only commercially available COMA system.
Reference: [5] <institution> Kendall Square Research Corporation, </institution> <address> 170 Tracer Lane, Waltham, MA 02154-1379, USA, </address> <note> KSR1 Principles of Operation, October 1992. Rev. 6.0. </note>
Reference-contexts: Similarly, the master processor cannot begin executing the following section until every team member has finished its parallel component. Although the concepts introduced in this paper are applicable to other cache-based shared-memory multiprocessor systems, we concentrate on Cache-Only Memory Architecture (COMA) systems, such as the KSR1 <ref> [5] </ref>, DDM [6], and COMA-FLAT [7]. <p> The KSR1 is chosen as our experimental testbed because it is the only commercially available COMA system. The KSR1 (Fig. 2) <ref> [5, 8, 9] </ref> is built as a group of ALLCACHE engines, connected by a fat tree hierarchy of unidirectional pipelined slotted rings. Each lowest level ring includes 32 processor cells and two ALL-CACHE directories.
Reference: [6] <author> E. Hagersten, A. Landin, and S. Haridi, </author> <title> "DDM - a cache-only memory architecture," </title> <journal> IEEE Computer, </journal> <volume> vol. 25, </volume> <pages> pp. 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Similarly, the master processor cannot begin executing the following section until every team member has finished its parallel component. Although the concepts introduced in this paper are applicable to other cache-based shared-memory multiprocessor systems, we concentrate on Cache-Only Memory Architecture (COMA) systems, such as the KSR1 [5], DDM <ref> [6] </ref>, and COMA-FLAT [7].
Reference: [7] <author> P. Stenstrom, T. Joe, and A. Gupta, </author> <title> "Comparative performance evaluation of cache-coherent NUMA and COMA architectures," </title> <booktitle> in Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pp. 80-91, </pages> <year> 1992. </year>
Reference-contexts: Although the concepts introduced in this paper are applicable to other cache-based shared-memory multiprocessor systems, we concentrate on Cache-Only Memory Architecture (COMA) systems, such as the KSR1 [5], DDM [6], and COMA-FLAT <ref> [7] </ref>. In a COMA system, data items do not have static home addresses, 1 A loop is normalized if its initial loop index and step size are one [4]. they are instead dynamically moved and replicated among the caches in various nodes in response to an application's reference patterns.
Reference: [8] <author> D. Windheiser, E. L. Boyd, E. Hao, S. G. Abraham, and E. S. Davidson, </author> <title> "KSR1 multiprocessor: Analysis of latency hiding techniques in a sparse solver," </title> <booktitle> in the 7th International Parallel Processing Symposium, </booktitle> <pages> pp. 454-461, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: The KSR1 is chosen as our experimental testbed because it is the only commercially available COMA system. The KSR1 (Fig. 2) <ref> [5, 8, 9] </ref> is built as a group of ALLCACHE engines, connected by a fat tree hierarchy of unidirectional pipelined slotted rings. Each lowest level ring includes 32 processor cells and two ALL-CACHE directories.
Reference: [9] <author> E. L. Boyd and E. S. Davidson, </author> <title> "Communication in the KSR1 MPP: Performance Evaluation Using Synthetic Workload Experiments," </title> <booktitle> in 8th ACM International Conference on Supercomputing, </booktitle> <pages> pp. 166-175, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The KSR1 is chosen as our experimental testbed because it is the only commercially available COMA system. The KSR1 (Fig. 2) <ref> [5, 8, 9] </ref> is built as a group of ALLCACHE engines, connected by a fat tree hierarchy of unidirectional pipelined slotted rings. Each lowest level ring includes 32 processor cells and two ALL-CACHE directories. <p> In the KSR1, communication latency varies between 135 and 600 cycles, while subcache misses served by that node's local cache take an average of 23.4 cycles (when the corresponding block is already allocated) or 49.2 cycles (otherwise) <ref> [9] </ref>. Although nonconsecutive references affect both communication and cache performance, only communication will be considered when deciding which arrays to reorganize. However, array reorganization should also improve sub-cache utilization.
Reference: [10] <author> R. A. Sugumar and S. G. Abraham, </author> <title> "Efficient simulation of caches under optimal replacement with applications to miss characterization," </title> <booktitle> in ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 24-35, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Therefore, communication is caused only by coherence misses, i.e., no communication is caused by any other types of cache misses, including capacity, matching, replacement, and compulsory misses <ref> [10] </ref>.
Reference: [11] <author> J. R. Allen and K. Kennedy, </author> <title> "Automatic loop interchange," </title> <journal> ACM SIGPLAN Notices, </journal> <volume> vol. 19, </volume> <pages> pp. 233-246, </pages> <month> June </month> <year> 1984. </year> <booktitle> Proceedings of the ACM SIGPLAN'84 Symposium on Compiler Construction. </booktitle>
Reference-contexts: Finally, wherever A or B appear in the the inner loop body, the leftmost two index expressions are interchanged. This grouped data structure layout should improve the efficiency of communication. Loop transformations such as loop interchange <ref> [11] </ref>, loop distribution [12], or loop fusion [13, 14] as well as additional array dimension reordering may improve the locality of reference, further improving overall performance.
Reference: [12] <author> D. J. Kuck, </author> <booktitle> The Structure of Computers and Computations, </booktitle> <volume> vol. </volume> <pages> 1. </pages> <address> New York: </address> <publisher> John Wiley & Sons, </publisher> <year> 1978. </year>
Reference-contexts: Finally, wherever A or B appear in the the inner loop body, the leftmost two index expressions are interchanged. This grouped data structure layout should improve the efficiency of communication. Loop transformations such as loop interchange [11], loop distribution <ref> [12] </ref>, or loop fusion [13, 14] as well as additional array dimension reordering may improve the locality of reference, further improving overall performance.
Reference: [13] <author> W. Abu-Sufah, D. Kuck, and D. Lawrie, </author> <title> "Automatic program transformations for virtual memory computers," </title> <booktitle> in Proceedings of the National Computer Conference, </booktitle> <volume> vol. 48, </volume> <pages> pp. 969-974, </pages> <publisher> AFIPS Press, </publisher> <year> 1979. </year>
Reference-contexts: Finally, wherever A or B appear in the the inner loop body, the leftmost two index expressions are interchanged. This grouped data structure layout should improve the efficiency of communication. Loop transformations such as loop interchange [11], loop distribution [12], or loop fusion <ref> [13, 14] </ref> as well as additional array dimension reordering may improve the locality of reference, further improving overall performance. Although finding the best transformations may take exponential time [15], the time may be reduced by prior array grouping since arrays with similar memory access patterns are gathered together.
Reference: [14] <author> M. Wolfe, </author> <title> Optimizing Supercompilers for Supercomputers. </title> <address> Cambridge, Massachusetts: </address> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Finally, wherever A or B appear in the the inner loop body, the leftmost two index expressions are interchanged. This grouped data structure layout should improve the efficiency of communication. Loop transformations such as loop interchange [11], loop distribution [12], or loop fusion <ref> [13, 14] </ref> as well as additional array dimension reordering may improve the locality of reference, further improving overall performance. Although finding the best transformations may take exponential time [15], the time may be reduced by prior array grouping since arrays with similar memory access patterns are gathered together.
Reference: [15] <author> Y.-J. Ju, </author> <title> Compiler Data Layout and Code Transformation for Reducing Cache Coherence Overhead. </title> <type> PhD thesis, </type> <institution> Purdue University, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: Loop transformations such as loop interchange [11], loop distribution [12], or loop fusion [13, 14] as well as additional array dimension reordering may improve the locality of reference, further improving overall performance. Although finding the best transformations may take exponential time <ref> [15] </ref>, the time may be reduced by prior array grouping since arrays with similar memory access patterns are gathered together. Read-only and consecutively-referenced arrays were not considered since grouping them does not reduce coherence communication.
References-found: 15

