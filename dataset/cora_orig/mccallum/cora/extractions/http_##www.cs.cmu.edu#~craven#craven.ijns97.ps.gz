URL: http://www.cs.cmu.edu/~craven/craven.ijns97.ps.gz
Refering-URL: http://www.cs.cmu.edu/~craven/rule-extraction.html
Root-URL: 
Email: mark.craven@cs.cmu.edu shavlik@cs.wisc.edu  
Title: on Noisy Time Series. Understanding Time-Series Networks: A Case Study in Rule Extraction  
Author: Mark W. Craven Jude W. Shavlik 
Address: 1210 West Dayton St. Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Note: To appear in the International Journal of Neural Systems special issue  
Abstract: A significant limitation of neural networks is that the representations they learn are usually incomprehensible to humans. We have developed an algorithm, called Trepan, for extracting comprehensible, symbolic representations from trained neural networks. Given a trained network, Trepan produces a decision tree that approximates the concept represented by the network. In this article, we discuss the application of Trepan to a neural network trained on a noisy time series task: predicting the Dollar-Mark exchange rate. We present experiments that show that Trepan is able to extract a decision tree from this network that equals the network in terms of predictive accuracy, yet provides a comprehensible concept representation. Moreover, our experiments indicate that decision trees induced directly from the training data using conventional algorithms do not match the accuracy nor the comprehensibility of the tree extracted by Trepan.
Abstract-found: 1
Intro-found: 1
Reference: <author> Alexander, J. A. & Mozer, M. C. </author> <year> (1995). </year> <title> Template-based algorithms for connectionist rule extraction. </title> <editor> In Tesauro, G., Touretzky, D., & Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 7). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. 15 Angluin, D. </address> <year> (1988). </year> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342. </pages>
Reference: <author> Breiman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth and Brooks, </publisher> <address> Monterey, CA. </address>
Reference-contexts: The advantage of learning with queries, as opposed to ordinary training examples, is that they can be used to garner information precisely where it is needed during the learning process. Our algorithm, as shown in Table 1, is similar to conventional decision-tree algorithms, such as CART <ref> (Breiman et al., 1984) </ref> and C4.5 (Quinlan, 1993), which learn directly from a training set. These algorithms build decision trees by recursively partitioning the input space.
Reference: <author> Craven, M. & Shavlik, J. </author> <year> (1993). </year> <title> Learning symbolic rules using artificial neural networks. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> (pp. 73-80), </pages> <address> Amherst, MA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Craven, M. W. </author> <year> (1996). </year> <title> Extracting Comprehensible Models from Trained Neural Networks. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, WI. </institution> <note> Available as CS Technical Report 1326. Available by WWW as ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/craven.thesis.ps.Z. </note>
Reference-contexts: Without this restriction, Trepan might have to solve difficult satisfiability problems in order create instances for nodes on such a path. More detail about the algorithm used by Trepan to ensure that m-of-n constraints are satisfied can be found elsewhere <ref> (Craven, 1996) </ref>. 7 Using a heuristic search to find m-of-n tests as we do, it can be easy to overfit a given set of instances when there are many possible operator applications. To avoid this pitfall, we place an additional restriction on operator applications.
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1996). </year> <title> Extracting tree-structured representations of trained networks. </title> <editor> In Touretzky, D., Mozer, M., & Hasselmo, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 8). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Without this restriction, Trepan might have to solve difficult satisfiability problems in order create instances for nodes on such a path. More detail about the algorithm used by Trepan to ensure that m-of-n constraints are satisfied can be found elsewhere <ref> (Craven, 1996) </ref>. 7 Using a heuristic search to find m-of-n tests as we do, it can be easy to overfit a given set of instances when there are many possible operator applications. To avoid this pitfall, we place an additional restriction on operator applications.
Reference: <author> Crites, R. H. & Barto, A. G. </author> <year> (1996). </year> <title> Improving elevator performance using reinforcement learning. </title> <editor> In Touretzky, D., Mozer, M., & Hasselmo, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 8). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Hayashi, Y. </author> <year> (1991). </year> <title> A neural expert system with automated extraction of fuzzy if-then rules. </title> <editor> In Lippmann, R., Moody, J., & Touretzky, D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 3). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Hogg, R. V. & Tanis, E. A. </author> <year> (1983). </year> <title> Probability and Statistical Inference. </title> <publisher> MacMillan Publishing, </publisher> <address> New York, NY. </address>
Reference-contexts: To make this decision, Trepan estimates the proportion of examples, prop c , that fall into the most common class at a given node, and then calculates a confidence interval around this estimated proportion <ref> (Hogg & Tanis, 1983) </ref>. Trepan makes the node a leaf if P rob (prop c &lt; 1 *) &lt; ff. Here, ff is the significance level for the test, and * specifies how tight the confidence interval around the estimated value of prop c must be.
Reference: <author> McMillan, C., Mozer, M. C., & Smolensky, P. </author> <year> (1992). </year> <title> Rule induction through integrated symbolic and sub-symbolic processing. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 4). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Murphy, P. M. & Pazzani, M. J. </author> <year> (1991). </year> <title> ID2-of-3: Constructive induction of M-of-N concepts for discriminators in decision trees. </title> <booktitle> In Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <pages> (pp. 183-187), </pages> <address> Evanston, IL. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For continuous features we consider binary tests on thresholds, as is done by C4.5. The selected binary test serves as a seed for the m-of-n search process. This search uses the information-gain measure as its heuristic evaluation function, and uses the following two operators <ref> (Murphy & Pazzani, 1991) </ref>: * m-of-n+1 : Add a new literal to the set, and hold the threshold constant. For example, 2-of-fx 1 ; x 2 g =) 2-of-fx 1 ; x 2 ; x 3 g. * m+1-of-n+1: Add a new literal to the set, and increment the threshold. <p> As a baseline for evaluating the accuracy and comprehensibility of the trees extracted by Trepan, we also run two conventional decision-tree algorithms: C4.5 (Quinlan, 1993) and our enhanced version of ID2-of-3 <ref> (Murphy & Pazzani, 1991) </ref>. C4.5 (the successor to ID3) is one of the most widely used inductive learning algorithms, and is perhaps the most popular inductive algorithm for learning "symbolic" hypotheses. ID2-of-3 is a similar algorithm that differs from C4.5 primarily in the nature of the splitting tests it uses.
Reference: <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Our algorithm, as shown in Table 1, is similar to conventional decision-tree algorithms, such as CART (Breiman et al., 1984) and C4.5 <ref> (Quinlan, 1993) </ref>, which learn directly from a training set. These algorithms build decision trees by recursively partitioning the input space. Each internal node in such a tree represents a splitting criterion that partitions some 4 part of the input space, and each leaf represents a predicted class. <p> Like the ID2-of-3 algorithm, Trepan uses a heuristic search process to construct its m-of-n splitting tests. In the experiments presented here, a beam search is used. The search process begins by first selecting the best binary test at the current node; Trepan uses the information gain criterion <ref> (Quinlan, 1993) </ref> to evaluate candidate splitting tests. For two-valued features, a binary test separates examples according to their values for the feature. For discrete features with more than two values, we consider binary tests based on each allowable value of the feature (e.g., color=red?, color=blue?, ...). <p> As a baseline for evaluating the accuracy and comprehensibility of the trees extracted by Trepan, we also run two conventional decision-tree algorithms: C4.5 <ref> (Quinlan, 1993) </ref> and our enhanced version of ID2-of-3 (Murphy & Pazzani, 1991). C4.5 (the successor to ID3) is one of the most widely used inductive learning algorithms, and is perhaps the most popular inductive algorithm for learning "symbolic" hypotheses.
Reference: <author> Rice, J. A. </author> <year> (1995). </year> <title> Mathematical Statistics and Data Analysis. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA. </address>
Reference-contexts: Trepan rejects the null hypothesis (that the distributions at the two nodes are the same) if the marginal distributions for any feature are significantly different. Since each feature presents an opportunity to spuriously reject the null hypothesis, Trepan uses the Bonferroni correction <ref> (Rice, 1995) </ref> to adjust the significance level of the overall test downward for the individual tests.
Reference: <author> Sachs, L. </author> <year> (1984). </year> <title> Applied Statistics: A Handbook of Techniques. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, 2nd edition. </address>
Reference-contexts: To decide if the distributions are significantly different, Trepan compares the marginal distributions for each unconstrained feature separately using a 2 test for discrete-valued features and the Kolmogorov-Smirnov test <ref> (Sachs, 1984) </ref> for real-valued features. An unconstrained feature is one whose value is not constrained by tests at internal nodes that are ancestors in the tree. <p> These results suggest that, by exploiting the concept representation learned by the neural network, Trepan is able to find a better decision tree than either C4.5 and ID2-of-3+. We test the statistical significance of these accuracy differences using the sign test known as the McNemar 2 test <ref> (Sachs, 1984) </ref>. Possibly because the test set for this problem is small, none of the differences are significant at p 0:05.
Reference: <author> Sethi, I. K., Yoo, J. H., & Brickman, C. M. </author> <year> (1993). </year> <title> Extraction of diagnostic rules using neural networks. </title> <booktitle> In Proceedings of the Sixth IEEE Symposium on Computer-Based Medical Systems, </booktitle> <pages> (pp. 217-222), </pages> <address> Ann Arbor, MI. </address> <publisher> IEEE Press. </publisher>
Reference: <author> Setiono, R. & Liu, H. </author> <year> (1995). </year> <title> Understanding neural networks via rule extraction. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 480-485), </pages> <address> Montreal, Quebec. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Silverman, B. W. </author> <year> (1986). </year> <title> Density Estimation for Statistics and Data Analysis. </title> <publisher> Chapman and Hall, </publisher> <address> New York, NY. </address>
Reference-contexts: Trepan uses empirical distributions to model discrete-valued features, and kernel density estimates <ref> (Silverman, 1986) </ref> to model continuous features. The empirical distribution of a feature is simply the distribution of values that occurs in a given sample of the feature.
Reference: <author> Tan, A.-H. </author> <year> (1994). </year> <title> Rule learning and extraction with self-organizing neural networks. </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> (pp. 192-199), </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Tchoumatchenko, I. & Ganascia, J.-G. </author> <year> (1994). </year> <title> A Bayesian framework to integrate symbolic and neural learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> (pp. 302-308), </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Towell, G. & Shavlik, J. </author> <year> (1993). </year> <title> Extracting refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 71-101. </pages>
Reference: <author> Weigend, A. S., Zimmermann, H. G., & Neuneier, R. </author> <year> (1996). </year> <editor> Clearning. In Refenes, P., Abu-Mostafa, Y., Moody, J., & Weigend, A. S., editors, </editor> <booktitle> Neural Networks in Financial Engineering. World Scientific, Singapore. </booktitle> <pages> 16 </pages>
Reference-contexts: Specifically, Trepan uses queries to the network to induce a decision tree that approximates the network's concept description. Recently we have used Trepan to extract descriptions of a network trained on the problem of predicting the Dollar-Mark exchange rate <ref> (Weigend et al., 1996) </ref>. In this article we review the Trepan algorithm and describe our experiments in applying it to this time-series network. The comprehensibility of learned solutions is an important consideration in many problem domains. <p> Moreover, the Trepan tree is more concise than those induced by these conventional methods. 2 The Exchange-Rate Prediction Domain The time-series domain that we consider in our experiments is that of predicting the daily foreign exchange rate between the U.S. Dollar and the German Mark <ref> (Weigend et al., 1996) </ref>. The data for this task consists of daily values from the period of January 15, 1985 through January 27, 1994. The last 216 days were set aside as a test set before the neural network was trained. <p> The third output unit predicts the return between today and the next turning point. The network was trained using the technique of clearning <ref> (Weigend et al., 1996) </ref>. The clearning method involves simultaneously cleaning the data and learning the underlying structure of the data.
References-found: 20

