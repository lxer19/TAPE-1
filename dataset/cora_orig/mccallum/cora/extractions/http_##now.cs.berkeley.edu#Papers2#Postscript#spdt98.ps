URL: http://now.cs.berkeley.edu/Papers2/Postscript/spdt98.ps
Refering-URL: http://http.cs.berkeley.edu/~remzi/papers.html
Root-URL: 
Email: fdusseau,remzi,culler,jmh,pattrsng@cs.berkeley.edu  
Title: Searching for the Sorting Record: Experiences in Tuning NOW-Sort  
Author: Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, David E. Culler, Joseph M. Hellerstein, and David A. Patterson 
Address: Berkeley  
Affiliation: Computer Science Division, University of California,  
Abstract: We present our experiences in developing and tuning the performance of NOW-Sort, a parallel, disk-to-disk sorting algorithm. NOW-Sort currently holds two world records in database-industry standard benchmarks. Critical to the tuning process was the setting of expectations, which tell the programmer both where to tune and when to stop. We found three categories of useful tools: tools that help set expectations and configure the application to different hardware parameters, visualization tools that animate performance counters, and search tools that track down performance anomalies. All such tools must interact well with all layers of the underlying software (e.g., the operating system), as well as with applications that leverage modern OS features, such as threads and memory-mapped I/O. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Acharya, M. Uysal, R. Bennett, A. Mendelson, M. Beynon, J. K. Hollingsworth, J. Saltz, and A. Sussman. </author> <title> Tuning the performance of I/O intensive parallel applications. </title> <booktitle> In Proceedings of the Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 15-27, </pages> <address> Philadelphia, May 1996. </address> <publisher> ACM Press. </publisher>
Reference: [2] <author> R. C. Agarwal. </author> <title> A Super Scalar Sort Algorithm for RISC Processors. </title> <booktitle> In 1996 ACM SIGMOD Conference, </booktitle> <pages> pages 240-246, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Therefore, not only does paging hurt memory performance, it also interferes with the sequential access patterns of the disk transfers. Our third expectation is motivated by previous work in disk-to-disk sorting. Previous researchers found clever ways to hide the cost of internal sorting by overlapping it with disk I/O <ref> [2, 25] </ref>. Therefore, our goal is do the same in our implementation. Finally, transfers to and from disk should proceed at the maximum rate. <p> The low CPU utilization that we observed during the read phase implied that we could overlap part of the internal sort with the read phase. As suggested in <ref> [2] </ref>, we performed a bucket sort while reading records from disk, followed by a partial-radix sort. With this optimization, we reduced the in-memory sort time for one-million records with four disks from 20% of the total execution time down to only 4%. <p> Tuning the in-memory behavior of the sort was easier than expected, largely due to the development of cache-sensitive algorithms by previous researchers <ref> [2, 25] </ref>. Early in our experience, we ran the Shade instruction set simulator to evaluate our implementation [9]; however, largely because we could not easily match the reported statistics back to specific lines of our user-level code, we were not able to use this information for tuning.
Reference: [3] <author> T. E. Anderson, D. E. Culler, D. A. Patterson, </author> <title> and The NOW Team. A Case for NOW (Networks of Workstations). </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1994. </year>
Reference-contexts: Our goal was to surpass these records with a cluster of 105 UltraSPARC I workstations connected with a high-speed network. Over one year later, on April 1, 1997 (April Fool's day), with the help of many people in the U.C. Berkeley NOW project <ref> [3] </ref>, we laid claim to both benchmark records. On 32 machines, we reduced the time to sort 1 million records to 2.41 seconds; more importantly, in 59.7 seconds we sorted over 8.4 GB on 95 machines. These NOW-Sort Datamation and MinuteSort records still stand today.
Reference: [4] <author> T. E. Anderson and E. Lazowska. Quartz: </author> <title> A Tool for Tuning Parallel Program Performance. </title> <booktitle> In Proceedings of the 1989 ACM SIGMETRICS and PERFORMANCE Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 115-125, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: In large-scale systems, system designers must have access to tools that are normally restricted to application-level programs; without such support, system-level performance tuning is an ad hoc, laborious process. Parallel profiling tools, such as Quartz <ref> [4] </ref>, modified for our cluster environment, would have been useful for the developers of GLUnix. A tool such as Paradyn [24] would also have been helpful for avoiding lengthy recompiles by allowing dynamic instrumentation of interesting pieces of code and providing immediate visual feedback.
Reference: [5] <author> A. C. Arpaci-Dusseau, R. H. Arpaci-Dusseau, D. E. Culler, J. M. Hellerstein, and D. A. Patterson. </author> <title> High-Performance Sorting on Networks of Workstations. </title> <booktitle> In SIGMOD '97, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: These NOW-Sort Datamation and MinuteSort records still stand today. We described the NOW-Sort algorithms and our performance measurements in a previous paper <ref> [5] </ref>; we now relate our difficulties in scaling this memory- and I/O-intensive application to 95 machines. Along the way, we learned much about scalable systems, disk performance, cache-sensitive algorithms, operating system interfaces, and memory management.
Reference: [6] <author> R. H. Arpaci-Dusseau, A. C. Arpaci-Dusseau, D. E. Culler, J. M. Hellerstein, and D. A. Patterson. </author> <title> The Architectural Costs of Streaming I/O: A Comparison of Workstations, Clusters, and SMPs. </title> <booktitle> In HPCA '98, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: Running the sorting code with counters produced a performance profile revealing that with more than two disks per workstation, both the CPU and I/O bus are saturated <ref> [6] </ref>. The CPU reaches its peak utilization due to the cost of initiating reads from disk while simultaneously sending records to remote nodes and copying keys into buckets.
Reference: [7] <author> N. Boden, D. Cohen, R. E. Felderman, A. Kulawik, and C. Seitz. Myrinet: </author> <title> A Gigabit-per-second Local Area Network. </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: In addition to the usual connection to the outside world via 10 Mb/s Ethernet, every workstation contains a single Myrinet network card, also attached to the S-Bus. Myrinet is a switch-based, high-speed, local-area network, with links capable of bi-directional transfer rates of 160 MB/s <ref> [7] </ref>. Each Myrinet switch has eight ports. <p> In general, problems in the communication layer were difficult to isolate because they were almost indistinguishable from performance problems within the application itself. The Myrinet local-area network was designed to provide an MPP-like backplane in a local area network setting <ref> [7] </ref>. In its goal to export a low-overhead, high-throughput communication layer, the design of Generic Active Messages (GAM) for Myrinet assumed complete hardware messaging reliability. On medium-sized clusters, this assumption was reasonable, and bit errors were rarely encountered.
Reference: [8] <author> J. Chapin, S. Herrod, M. Rosenblum, and A. Gupta. </author> <title> Memory System Performance of UNIX on CC-NUMA Multiprocessors. </title> <booktitle> In 1995 ACM SIGMETRICS/Performance Conference, </booktitle> <pages> pages 1-13, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Though most modern processors have a reasonable set of performance counters [12, 22] that have been shown to be useful for detailed performance profiling [27], other components of the machine are ignored. For example, researchers have shown that network packet counters can be extremely useful <ref> [8, 20] </ref>. However, just monitoring in-coming and out-going packets is not enough. Minimally, 32-bit counters should be available for every interconnection in the system, from the memory and I/O bus of each workstation, out into the switches of the network.
Reference: [9] <author> B. Cmelik and D. Keppel. Shade: </author> <title> A Fast Instruction-Set Simulator For Execution Profiling. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference, </booktitle> <pages> pages 128-37, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Tuning the in-memory behavior of the sort was easier than expected, largely due to the development of cache-sensitive algorithms by previous researchers [2, 25]. Early in our experience, we ran the Shade instruction set simulator to evaluate our implementation <ref> [9] </ref>; however, largely because we could not easily match the reported statistics back to specific lines of our user-level code, we were not able to use this information for tuning. Further, Shade does not trace kernel code, which has important interactions with the NOW-Sort algorithm.
Reference: [10] <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <year> 1993. </year>
Reference-contexts: Since all of our measurements took place in a dedicated environment, we primarily used GLUnix as a parallel program launcher. The parallel versions of NOW-Sort are written with the support of the Split-C library <ref> [10] </ref>. Split-C is a parallel extension to C that supports efficient access to a global address space on distributed memory machines. The Split-C library provides many useful group synchronization and communication primitives, including barriers and reductions.
Reference: [11] <author> D. E. Culler, L. T. Liu, R. P. Martin, and C. O. Yoshikawa. </author> <title> LogP Performance Assessment of Fast Network Interfaces. </title> <journal> IEEE Micro, </journal> <volume> 2/1996. </volume>
Reference-contexts: The version of Active Messages that we used, GAM, supports only a single communicating process per workstation at a time. GAM over the Myrinet has a round-trip latency of roughly 20 s and a uni-directional bandwidth (one node sending, another receiving) of 35 MB/s <ref> [11] </ref>. 3 Single-Node Performance When tuning an application, most programmers have a set of performance expectations they hope to achieve, whether a MFLOPS rating, an absolute execution time, or a certain level of speed-up. <p> That is, the scalability of NOW-Sort should be perfect as the problem size is increased linearly with the number of workstations. Microbenchmarks of Active Message communication performance indicate that 35 MB/s can be transferred between nodes <ref> [11] </ref>. The implication of this level of bandwidth is that sending a set of records to another node requires noticeably less time than reading those records from disk. Therefore, we add a parallel expectation to the four enumerated for the single-node sort: 5.
Reference: [12] <author> DEC. </author> <title> DECChip 21064 RISC Microprocessor Preliminary Data Sheet. </title> <type> Technical report, </type> <institution> Digital Equipment Corporation, </institution> <year> 1992. </year>
Reference-contexts: Such models could be easily constructed from a more comprehensive set of hardware counters at all levels of the machine. Though most modern processors have a reasonable set of performance counters <ref> [12, 22] </ref> that have been shown to be useful for detailed performance profiling [27], other components of the machine are ignored. For example, researchers have shown that network packet counters can be extremely useful [8, 20]. However, just monitoring in-coming and out-going packets is not enough.
Reference: [13] <author> A. et. al. </author> <title> A Measure of Transaction Processing Power. </title> <journal> Data-mation, </journal> <volume> 31(7) </volume> <pages> 112-118, </pages> <year> 1985. </year> <note> Also in Readings in Database Systems, </note> <editor> M.H. Stonebraker ed., </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1989. </year>
Reference-contexts: Third, well-understood sorting algorithms exist for both sequential and parallel environments. Finally, the presence of industry-standard benchmarks allows us to compare our performance to that of other large-scale systems. At that time, a 12-processor SGI Challenge held the world-records on the two existing benchmarks. For the Datamation benchmark <ref> [13] </ref>, the SGI had sorted 1 million 100-byte records with 10-byte keys from disk to disk in 3.52 seconds. For MinuteSort [25], 1.6 GB of these 100-byte records were sorted within one minute [32].
Reference: [14] <author> D. P. Ghormley, D. Petrou, S. H. Rodrigues, A. M. Vahdat, and T. E. Anderson. </author> <title> A Global Layer Unix for a Network of Workstations. </title> <note> To appear in Software Practice and Experience, </note> <year> 1998. </year>
Reference-contexts: However, 105 machines running Solaris does not a cluster make. To present the illusion of a single, large-scale system to the end user, our cluster employs GLUnix, the prototype distributed operating system for the Berkeley NOW <ref> [14] </ref>. GLUnix monitors nodes in the system for load-balancing, coschedules parallel programs, and provides job control and I/O redirection. Since all of our measurements took place in a dedicated environment, we primarily used GLUnix as a parallel program launcher. <p> This essential task within GLUnix <ref> [14] </ref> was a bot-tleneck on large cluster sizes.
Reference: [15] <author> J. K. Hollingsworth. </author> <title> Finding Bottlenecks in Large-scale Parallel Programs. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, </institution> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: In general, this was a hierarchical process, similar to the search process described in <ref> [15] </ref>. In the first step, finding the needle in a NOW-stack, the slow workstations in the cluster were identified.
Reference: [16] <author> J. K. Hollingsworth and E. L. Miller. </author> <title> Using Content-Derived Names for Configuration Management. </title> <booktitle> In 1997 ACM Symposium on Software Reusibility, </booktitle> <address> Boston, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: Tools that understand the complete dependence tree of an application and notify the user when changes occur would be helpful in developmental systems; perhaps configuration management tools are a good match to this problem <ref> [16] </ref>. One difficulty with this approach is following the chain of dependencies. For example, in our environment, applications link with the GLUnix library to access GLUnix. Though the GLUnix library may not have changed, the GLUnix user-level daemon, which is contacted through the library, may have.
Reference: [17] <author> S. Kleiman, J. Voll, J. Eykholt, A. Shivalingiah, D. Williams, M. Smith, S. Barton, and G. Skinner. </author> <title> Symmetric Multiprocessing in Solaris 2.0. </title> <booktitle> In Proceedings of COMPCON Spring '92, </booktitle> <year> 1992. </year>
Reference-contexts: Each machine in our cluster runs a copy of Solaris 2.5.1, a multi-threaded version of UNIX <ref> [17] </ref>. Some of the strengths of Solaris include its efficient support for kernel-level threads, a well-developed Unix file system, and sophisticated memory management. However, 105 machines running Solaris does not a cluster make.
Reference: [18] <author> A. R. Lebeck and D. A. Wood. </author> <title> Cache Profiling and the SPEC Benchmarks: A Case Study. </title> <booktitle> IEEE COMPUTER, </booktitle> <pages> pages 15-26, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Further, Shade does not trace kernel code, which has important interactions with the NOW-Sort algorithm. If our internal sort had consumed a larger fraction of the total execution time, then tools specifically aimed at finding memory bottlenecks, such as Cprof <ref> [18] </ref> and MemSpy [21], could have been useful for fine-tuning. However, for codes that interact with underlying software systems (especially the OS and communication layer), it is crucial that these tools give information on more than just user-level code.
Reference: [19] <author> A. M. Mainwaring. </author> <title> Active Message Application Programming Interface and Communication Subsystem Organization. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1995. </year>
Reference-contexts: The errors manifested themselves as CRC errors, which the communication layer recognized, but could not recover from, thus terminating the application. The current version of Active Messages performs reliable message transfer on top of this usually reliable medium <ref> [19] </ref>; however, the temporary solution in GAM was to reduce the number of outstanding messages allowed in the flow-control layer, limiting the load placed on the network.
Reference: [20] <author> M. Martonosi, D. W. Clark, and M. Mesarina. </author> <title> The SHRIMP Hardware Performance Monitor: </title> <booktitle> Design and Applications. In Proceedings of 1996 SIGMETRICS Symposium on Parallel and Distributed Tools (SPDT), </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Though most modern processors have a reasonable set of performance counters [12, 22] that have been shown to be useful for detailed performance profiling [27], other components of the machine are ignored. For example, researchers have shown that network packet counters can be extremely useful <ref> [8, 20] </ref>. However, just monitoring in-coming and out-going packets is not enough. Minimally, 32-bit counters should be available for every interconnection in the system, from the memory and I/O bus of each workstation, out into the switches of the network.
Reference: [21] <author> M. Martonosi, A. Gupta, and T. E. Anderson. MemSpy: </author> <title> Analyzing Memory System Bottlenecks in Programs. </title> <booktitle> In Proceedings of the 1992 ACM SIGMETRICS and PERFORMANCE Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Further, Shade does not trace kernel code, which has important interactions with the NOW-Sort algorithm. If our internal sort had consumed a larger fraction of the total execution time, then tools specifically aimed at finding memory bottlenecks, such as Cprof [18] and MemSpy <ref> [21] </ref>, could have been useful for fine-tuning. However, for codes that interact with underlying software systems (especially the OS and communication layer), it is crucial that these tools give information on more than just user-level code.
Reference: [22] <author> T. Mathisen. </author> <title> Pentium Secrets. </title> <journal> Byte, </journal> <pages> pages 191-192, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Such models could be easily constructed from a more comprehensive set of hardware counters at all levels of the machine. Though most modern processors have a reasonable set of performance counters <ref> [12, 22] </ref> that have been shown to be useful for detailed performance profiling [27], other components of the machine are ignored. For example, researchers have shown that network packet counters can be extremely useful [8, 20]. However, just monitoring in-coming and out-going packets is not enough.
Reference: [23] <author> R. V. Meter. </author> <title> Observing the Effects of Multi-Zone Disks. </title> <booktitle> In Proceedings of the 1997 USENIX Conference, </booktitle> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: Disk Layout: The performance of NOW-Sort is quite sensitive to the layout of files on each local disk. Because multi-zone disks give significantly higher bandwidth when data is allocated on outer tracks <ref> [23] </ref>, full disks lead to less than expected data rates. Our disk configuration tool verified that read performance varies significantly (30%), depending on disk layout. Several times, after tracking down a slow node, a quick inspection revealed a scratch disk full of another user's temporary files.
Reference: [24] <author> B. P. Miller, M. D. Callaghan, J. M. Cargille, J. K. Hollingsworth, R. B. Irvin, K. L. Karavanic, K. Kunchithapadam,and T. Newhall. </author> <title> The Paradyn Parallel Performance Measurement Tools. </title> <journal> IEEE Computer, </journal> <volume> 28(11), </volume> <year> 1995. </year>
Reference-contexts: Parallel profiling tools, such as Quartz [4], modified for our cluster environment, would have been useful for the developers of GLUnix. A tool such as Paradyn <ref> [24] </ref> would also have been helpful for avoiding lengthy recompiles by allowing dynamic instrumentation of interesting pieces of code and providing immediate visual feedback. Isolating performance problems is particularly frustrating in a dynamic environment, where libraries, daemons, and other system services change without altering the application binary. <p> Most of these limitations arise due to the advanced features of Solaris that we leverage: kernel threads and memory-mapped files. For example, Paradyn does not work with threaded applications, which eliminates all but our single-node, one-pass sort <ref> [24] </ref>. Our codes also make heavy use of mmap () to access files, whereas many tools only track the read () and write () system calls when reporting I/O behavior.
Reference: [25] <author> C. Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. AlphaSort: </author> <title> A RISC Machine Sort. </title> <booktitle> In 1994 ACM SIGMOD Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: At that time, a 12-processor SGI Challenge held the world-records on the two existing benchmarks. For the Datamation benchmark [13], the SGI had sorted 1 million 100-byte records with 10-byte keys from disk to disk in 3.52 seconds. For MinuteSort <ref> [25] </ref>, 1.6 GB of these 100-byte records were sorted within one minute [32]. Our goal was to surpass these records with a cluster of 105 UltraSPARC I workstations connected with a high-speed network. <p> Therefore, not only does paging hurt memory performance, it also interferes with the sequential access patterns of the disk transfers. Our third expectation is motivated by previous work in disk-to-disk sorting. Previous researchers found clever ways to hide the cost of internal sorting by overlapping it with disk I/O <ref> [2, 25] </ref>. Therefore, our goal is do the same in our implementation. Finally, transfers to and from disk should proceed at the maximum rate. <p> Before the quicksort, each 10-byte key is separated from the 100-byte record and a pointer is set up to the full record. This separation allows the sort to operate more efficiently, swapping keys and pointers instead of full records during the quicksort <ref> [25] </ref>. * Write: The list of sorted keys and pointers is traversed in order to gather the sorted records into an output buffer. When the output buffer fills, it is written to disk with the write () system call. <p> Tuning the in-memory behavior of the sort was easier than expected, largely due to the development of cache-sensitive algorithms by previous researchers <ref> [2, 25] </ref>. Early in our experience, we ran the Shade instruction set simulator to evaluate our implementation [9]; however, largely because we could not easily match the reported statistics back to specific lines of our user-level code, we were not able to use this information for tuning.
Reference: [26] <author> S. Perl and W. E. Weihl. </author> <title> Performance Assertion Checking. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 134-45, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: A more automatic approach is clearly desirable, especially for users not as familiar with all components of the system. For example, problematic workstations could be flagged with a more sophisticated tool, such as Performance Assertion Checking <ref> [26] </ref>. With such a tool, simple assertions about performance expectations are placed in the code, and flagged when they do not succeed. While standard parallel tools could have been useful in the search process, some limitations prevented our use of them.
Reference: [27] <author> S. E. Perl and R. L. </author> <title> Sites. Studies of Windows NT Performance Using Dynamic Execution Traces. </title> <booktitle> In OSDI 2, </booktitle> <pages> pages 169-184, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Such models could be easily constructed from a more comprehensive set of hardware counters at all levels of the machine. Though most modern processors have a reasonable set of performance counters [12, 22] that have been shown to be useful for detailed performance profiling <ref> [27] </ref>, other components of the machine are ignored. For example, researchers have shown that network packet counters can be extremely useful [8, 20]. However, just monitoring in-coming and out-going packets is not enough.
Reference: [28] <author> D. A. Reed, C. L. Elford, T. Madhyastha, W. H. Scullin, R. A. Aydt, and E. Smirni. </author> <title> I/O, Performance Analysis, and Performance Data Immersion. </title> <booktitle> In Proceedings of MASCOTS '96, </booktitle> <pages> pages 5-16, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: One such system, Pablo, tracks read and write calls, and uses them to give breakdowns of I/O requests, sizes, and times of access <ref> [28] </ref>. With mmap (), this is much more difficult, requiring the instrumentation of all loads and stores to mapped file regions. Identifying specific problems was simplified with a few visualization tools that we developed, as described in Section 3.
Reference: [29] <author> M. Rosenblum, E. Bugnion, S. Devine, and S. A. Herrod. </author> <title> Using the SimOS Machine Simulator to Study Complex Computer Systems. </title> <journal> ACM Transactions on Modelling and Computer Simulation (TOMACS), </journal> <month> January </month> <year> 1997. </year>
Reference-contexts: However, for codes that interact with underlying software systems (especially the OS and communication layer), it is crucial that these tools give information on more than just user-level code. Modifying tools such as these to work in tandem with a complete machine simulator (e.g., SimOS <ref> [29] </ref>) would thus be more useful. 4 Parallel Performance By focusing on single-node performance, we developed an excellent building block for a scalable sorting algorithm. Our next step towards large-scale parallelism was to define a set of expectations for parallel performance.
Reference: [30] <author> R. H. Saavedra-Barrera. </author> <title> CPU Performance Evaluation and Execution Time Prediction Using Narrow Spectrum Benchmarking. </title> <type> PhD thesis, </type> <institution> U.C. Berkeley, Computer Science Division, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Running NOW-Sort with a different cache architecture would require two changes. First, the size of the second-level cache must be determined, either via look-up in a hardware parameter table, or with a cache configuration tool, similar to the micro-benchmarks described in <ref> [30] </ref>. Second, the sort must be trivially modified to accept the cache size as a parameter instead of as a hard-coded constant. 3.3 Two-Pass Algorithm The two-pass version of NOW-sort leverages much of the code that was already optimized for the one-pass sort.
Reference: [31] <author> M. Stonebraker. </author> <title> Operating System Support for Database Management. </title> <journal> Communications of the ACM, </journal> <volume> 24(7) </volume> <pages> 412-418, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: As we increased the input size, the traffic steadily worsened, until the machine was thrashing throughout the sort and write phases. We readily diagnosed the cause of this classic problem <ref> [31] </ref>: when using the read () system call, the file system was unnecessarily buffering the input file.
Reference: [32] <author> A. Sweeney, D. Doucette, W. Hu, C. Anderson, M. Nishimoto, and G. Peck. </author> <title> Scalability in the XFS File System. </title> <booktitle> In Proceedings of the USENIX 1996 Annual Technical Conference, </booktitle> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: For the Datamation benchmark [13], the SGI had sorted 1 million 100-byte records with 10-byte keys from disk to disk in 3.52 seconds. For MinuteSort [25], 1.6 GB of these 100-byte records were sorted within one minute <ref> [32] </ref>. Our goal was to surpass these records with a cluster of 105 UltraSPARC I workstations connected with a high-speed network. Over one year later, on April 1, 1997 (April Fool's day), with the help of many people in the U.C.
Reference: [33] <author> M. Tremblay, D. Greenley, and K. Normoyle. </author> <booktitle> The Design of the Microarchitecture of UltraSPARC-I. Proceedings of the IEEE, </booktitle> <volume> 83(12) </volume> <pages> 1653-63, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: In Section 6, we relate the experience of isolating performance problems in large systems, what we call finding the needle in the NOW-stack. Finally, we conclude with some reflections on our experience. 2 The NOW Cluster 2.1 Hardware The Berkeley NOW cluster consists of 105 commodity Ultra1 workstations <ref> [33] </ref>. Each Ultra1 workstation contains a single UltraSPARC I processor, with 16 KB on-chip instruction and data caches, and a unified 512 KB second-level cache. At the base of the memory hierarchy is 128 MB of DRAM.
Reference: [34] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Split-C is a parallel extension to C that supports efficient access to a global address space on distributed memory machines. The Split-C library provides many useful group synchronization and communication primitives, including barriers and reductions. For communication, we utilized Active Messages <ref> [34] </ref>, a communication layer designed for the low latency and high bandwidth of switch-based networks. An Active Message is essentially a restricted, lightweight remote procedure call. When a process sends an Active Message, it specifies a handler to be executed on the remote node.
References-found: 34

