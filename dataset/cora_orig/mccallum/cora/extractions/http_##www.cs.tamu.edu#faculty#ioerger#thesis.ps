URL: http://www.cs.tamu.edu/faculty/ioerger/thesis.ps
Refering-URL: http://www.cs.tamu.edu/faculty/ioerger/education.html
Root-URL: http://www.cs.tamu.edu
Title: c  
Author: flCopyright by THOMAS RICHARD IOERGER 
Date: 1996  
Abstract-found: 0
Intro-found: 1
Reference: <author> Abu-Mostafa, Y. S. </author> <year> 1994. </year> <title> Learning from hints. </title> <journal> Journal of Complexity 10 </journal> <pages> 165-178. </pages>
Reference: <author> Aha, D.W.; Kibler, D.; and Albert, M.K. </author> <year> 1991. </year> <title> Instance-based learning algorithms. </title> <booktitle> Machine Learning 6 </booktitle> <pages> 37-66. </pages>
Reference-contexts: Our goal is to improve the predictive ability of homology modeling using our framework for change-of-representation. Homology modeling is equivalent to a nearest-neighbor learning algorithm <ref> (Aha et al., 1991) </ref>, with sequence alignment scores playing the role of a sophisticated similarity metric. Note that examples in this domain (the amino acid sequences) are initially represented as variable-length strings over a twenty-letter alphabet, which is quite different from the fixed-length feature-vector-based domains typically studied in machine learning. <p> Our notion of representation is distinct from the internal hypothesis language, referring only to the language in which examples are presented to the learner. In fact, some induction algorithms do not even form explicit internal hypotheses, such as neural networks (Rumelhart et al., 1986) and instance-based algorithms <ref> (Aha et al., 1991) </ref>. The role that representation plays in machine learning can be explained in terms of bias (Utgoff, 1986). According to the classification model of learning, the goal of the learner is to find a partition of the domain objects, consistent with the training examples. <p> Despite the differences among algorithms like ID3, GREEDY3, NT-Growth, and BackProp, they all implement the similarity-based learning (SBL) bias, tending to classify unseen instances like nearby instances that were seen during training (Matheus and Rendell, 1989). Nearest-neighbor learning algorithms implement the SBL bias directly <ref> (Aha et al., 1991) </ref>. This bias is very general and is pervasive among commonly known learning algorithms. For example, the SBL bias is implicit in the way a recursive partitioner splits the instance space into regions (Breiman et al., 1984). <p> Thus a good bias for one domain can actually interfere with learning in another domain. Each algorithm has certain criteria for applicability, such non-disjunctiveness for version spaces (Mitchell, 1982), or linear-separability for perceptrons (Minsky and Papert, 1969), or smoothness of class functions for instance-based learners <ref> (Aha et al., 1991) </ref>, or independence for Bayesian learners (Cheeseman et al., 1988). The appropriateness of the bias of a given algorithm for a particular domain depends on that domain; so the domain expert is ultimately responsible for making a reasonable choice. <p> Many algorithms have parameters that must be specified, such as the stopping criterion of ID3 (Quinlan, 1983), the learning rate of BackProp (Rumelhart et al., 1986), or the number of neighbors maintained for each cluster in an instance-based learner <ref> (Aha et al., 1991) </ref>. The accuracy of a learner can be sensitive to the choices for such parameters, so domain experts often use background knowledge to help make reasonable selections. <p> As shown in Table 6.1, homology modeling can be thought of as a nearest-neighbor 88 Table 6.1: Analogy between inductive learning and homology modeling. inductive learning homology modeling training examples protein database description language amino acid sequences classifications fold classes inductive bias nearest-neighbor similarity metric alignment score learning algorithm <ref> (Aha et al., 1991) </ref>. The proteins in the database are the examples, described initially as amino acid sequences. They are labeled with their known fold-classifications. Given a new sequence, it is compared to all the database entries by doing sequence alignments.
Reference: <author> Alberts, B.; Bray, D.; Lewis, J.; Raff, M.; Roberts, K.; and Watson, J.D. </author> <year> 1983. </year> <title> Molecular Biology of the Cell. </title> <publisher> Garland Publishing: </publisher> <address> New York. </address>
Reference-contexts: This folded structure is the culmination of a synthesis process of many steps in the cell <ref> (Alberts et al., 1983) </ref>. The instructions for the protein sequence originate as a gene in the DNA, located in the nucleus. DNA encodes information as a string over a 4-symbol alphabet (the nucleotides: adenine, guanine, cytosine, and thymine). <p> After translation, the protein folds into a three-dimensional shape, is occasionally modified, and is finally transported to its destination in the cellular environment. According to the central dogma of molecular biology, the synthesis of a protein is a deterministic process <ref> (Alberts et al., 1983) </ref>. During transcription, each nucleotide in the DNA sequence for a gene gets copied into a corresponding nucleotide in the mRNA transcript. During translation, the mRNA sequence is read in non-overlapping triplets (codons) to derive the sequence of amino acids for synthesizing the protein.
Reference: <author> Altschul, S.F.; Gish, W.; Miller, W.; Myers, </author> <title> E.W.; and Lipman, D.J. 1990. Basic local alignment search tool. </title> <journal> Journal of Molecular Biology 215 </journal> <pages> 403-410. </pages>
Reference-contexts: As substitutions accumulate over time, the structures slowly diverge (staying in the same fold class) in correlation with decreasing percent amino acid identity. Percent amino acid identity is the basis for programs like BLAST <ref> (Altschul et al., 1990) </ref>, which retrieve protein database entries that contain sub-sequences with high similarity to segments of the query. These programs require a method for testing the significance of similarity scores to determine when they signal a structural relationship.
Reference: <author> Amarel, S. </author> <year> 1970. </year> <title> On the representation of problems and goal-directed procedures for computers. </title> <editor> In Banerji, R.B. and Mesarovic, M., editors 1970, </editor> <title> Theoretical Approaches to Nonnumerical Problem Solving. </title> <publisher> Springer-Verlag: </publisher> <address> Heidelberg, Germany. </address>
Reference: <author> Anderberg, </author> <title> M.R. 1973. Cluster analysis for applications. </title> <publisher> Academic Press: </publisher> <address> New York. </address>
Reference-contexts: Similarly, Dietterich and Michalski (1986) describe a program that transforms sequences of moves in a card game by manipulating objects in the sequence to summarize key aspects of the surrounding moves. A common constructive induction technique is to invent new terms, such as by clustering <ref> (Anderberg, 1973) </ref>. Clustering can improve a representation by giving a unique label to separate peaks in instance space (Rendell, 1985). Examples of induction systems that exploit clustering are CLUSTER (Michalski and Stepp, 1983) and COBWEB (Fisher, 1987). Another method 26 for inventing new terms is inductive-logic programming (ILP).
Reference: <author> Baldwin, R.L. </author> <year> 1989. </year> <title> How does protein folding get started? Theoretical Issues in Biological Sciences 14 </title> <type> 291-294. </type>
Reference-contexts: The genetic code is a universal many-to-one mapping of the 64 possible nucleotide triplets into the 20 amino acids (Schulz and Schirmer, 1979). Once the amino acid sequence of a protein has been assembled, it usually takes between several milliseconds and several seconds to fold <ref> (Baldwin, 1989) </ref>. Although the pathways of folding are not well-understood, it is postulated that proteins fold into structures of globally minimum energy (McCammon and Harvey, 1987).
Reference: <author> Bax, A. </author> <year> 1989. </year> <title> Two-dimensional nmr and protein structure. </title> <booktitle> Annual Review of Biochemistry 58 </booktitle> <pages> 223-256. </pages>
Reference-contexts: All of these methods have serious limitations. In the next We will also introduce another computational method called homology modeling, whose accuracy we will attempt to improve using our framework. 5.4.1 Laboratory Methods One of the primary methods for physically determining the structure of a protein is X-ray crystallography <ref> (Bax, 1989) </ref>. For crystallographic analysis, a protein must first be expressed, purified, and crystallized. X-rays are directed into the crystal, and diffraction patterns are captured on the other side with photographic plates. Within the crystal, the individual protein molecules line up in a three-dimensional lattice with complex symmetries.
Reference: <author> Bergadano, F. and Giordana, A. </author> <year> 1990. </year> <title> Guiding induction with domain theories. </title> <editor> In Kodratoff, Y. and Michalski, R., editors 1990, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Volume III. </booktitle> <publisher> Morgan Kaufmann: </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Theory revision may also be looked at as a way of incorporating domain knowledge into learning (Pazzani, 1988). There are several perspectives on theory revision, such as the incremental refinement of a theory that fails to explain a few examples <ref> (Bergadano and Giordana, 1990) </ref>, or as an integration of EBL and SBL (Mooney and Ourston, 1994). However, given that the inputs to theory revision include both examples and knowledge, it can also be thought of as method for incorporating knowledge into inductive learning.
Reference: <author> Bernstein, F.; Koetzle, T.; Williams, G.; Meyer, E.; Brice, M.; Rodgers, J.; Kennard, O.; Shimanouchi, T.; and Tasumi, M. </author> <year> 1977. </year> <title> The protein data bank: A computer-based archival file for macromolecular structures. </title> <journal> Journal of Molecular Biology 112 </journal> <pages> 535-542. </pages>
Reference-contexts: Rather than trying to compute this mapping from first principles (electrostatics, Newtonian mechanics, etc.), another approach is to try to learn the relationship between sequence and structure from examples in protein databases. Protein databases, such as the Brookhaven PDB <ref> (Bernstein et al., 1977) </ref>, hold hundreds of entries for proteins whose structures have already been solved. <p> On a 256-processor partition, doing all 20,000 pairwise alignments necessary to evaluate one representation takes approximately 30 96 Table 6.3: The protein data used in the experiments in this thesis. Sequences are specified by their 4-character PDB name <ref> (Bernstein et al., 1977) </ref>, along with optional chain letters and sequence limits.
Reference: <author> Blundell, T.L.; Sibanda, B.L.; Sternberg, M.J.E.; and Thornton, J.M. </author> <year> 1987. </year> <title> Knowledge-based prediction of protein structures and the design of novel molecules. </title> <booktitle> Nature 326 </booktitle> <pages> 347-352. </pages>
Reference-contexts: The goal is to use these examples to make predictions about the structures of other proteins. 10 One widely used method for predicting the structures of new proteins using protein databases is called homology modeling <ref> (Blundell et al., 1987) </ref>. Homology modeling refers to the use of a sequence alignment algorithm (which inserts gaps to maximize local matches of amino acids) to detect similarities among proteins (Needleman and Wunsch, 1970). <p> Homology modeling refers to the process of aligning the sequence of a protein whose structure is unknown to the sequence of a protein whose structure is known. Provided their sequences are sufficiently similar, an approximate structure can be inferred by analogy <ref> (Blundell et al., 1987) </ref>. Homology modeling can be adapted as a general method for structure prediction by aligning a sequence with all the proteins in a protein database to identify the closest match. <p> Insertions and deletions are typically one to ten residues long (Doolittle, 1981), and are most often found in the loop regions at the surface of a protein <ref> (Blundell et al., 1987) </ref>. Insertions or deletions in the tightly packed core of a protein are more disruptive and can completely destabilize a structure, whereas the insertion or deletion of amino acids at the surface are more easily accommodated.
Reference: <author> Bohr, H.; Bohr, J.; Brunak, S.; Cotterill, R.M.; Lautrup, B.; and Petersen, S.B. </author> <year> 1990. </year> <title> A novel approach to prediction of the three-dimensional structures of protein backbones by neural networks. </title> <journal> FEBS Letters 261 </journal> <pages> 43-46. </pages>
Reference: <author> Bowie, J.U. and Eisenberg, D. </author> <year> 1993. </year> <title> Inverted protein structure prediction. </title> <booktitle> Current Opinions in Structural Biology 3 </booktitle> <pages> 437-444. </pages> <note> 150 Bowie, </note> <author> J.U.; Luthy, R.; and Eisenberg, D. </author> <year> 1991. </year> <title> A method to identify protein sequences that fold into a known three-dimensional structure. </title> <booktitle> Science </booktitle> 253:164-170. 
Reference-contexts: One approach is to fold the query sequence into the shape of the candidate structure, and then evaluate the "quality" of the fit. This approach is often used to solve the inverse folding problem, in which possible sequences are generated for a given structure <ref> (Bowie and Eisenberg, 1993) </ref>. 85 For fold-class prediction, the evaluation of fit provides an independent indexing mechanism for identifying structures in a database that a query sequence could potentially match.
Reference: <author> Breiman, L.; Friedman, J.H.; Olshen, R.A.; and Stone, C.J. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth: </publisher> <address> Belmont, CA. </address>
Reference-contexts: Nearest-neighbor learning algorithms implement the SBL bias directly (Aha et al., 1991). This bias is very general and is pervasive among commonly known learning algorithms. For example, the SBL bias is implicit in the way a recursive partitioner splits the instance space into regions <ref> (Breiman et al., 1984) </ref>. Algorithms that implement the SBL bias perform best when the instances of a concept are clustered together into a few large peaks (E-space regularity; Rendell, 1985). <p> Many algorithms, from CART <ref> (Breiman et al., 1984) </ref> to stepwise multiple regression (Draper and Smith, 1981), work by iteratively identifying the variable most correlated with a subset of the training sample. These local decisions are not guaranteed to be globally optimal; a more compact concept description that recognizes higher-level patterns might exist.
Reference: <author> Brown, M.; Hughey, R.; Krogh, A.; Mian, I.S.; Sjolander, K.; and Haussler, D. </author> <year> 1993. </year> <title> Using dirichlet mixture priors to derive hidden markov models for protein families. </title> <booktitle> In Proceedings of the First International Conference on Intelligent Systems for Molecular Biology. </booktitle> <pages> 47-55. </pages>
Reference: <author> Buchanan, B.; Mitchell, T.M.; Smith, R.G.; and Johnson, C.R. </author> <year> 1978. </year> <title> Models of learning system. </title> <editor> In Belzer, J., editor 1978, </editor> <booktitle> Encyclopedia of computer science and technology, </booktitle> <volume> volume 11. </volume> <publisher> Marcel Dekker: </publisher> <address> New York. </address>
Reference: <author> Cheeseman, P.; Kelly, J.; Self, M.; Stutz, J.; Taylor, W.; and Freeman, D. </author> <year> 1988. </year> <title> Autoclass: A baysian classification system. </title> <booktitle> In Proceedings of the Fifth International Workshop on Machine Learning. </booktitle> <pages> 54-64. </pages>
Reference-contexts: Each algorithm has certain criteria for applicability, such non-disjunctiveness for version spaces (Mitchell, 1982), or linear-separability for perceptrons (Minsky and Papert, 1969), or smoothness of class functions for instance-based learners (Aha et al., 1991), or independence for Bayesian learners <ref> (Cheeseman et al., 1988) </ref>. The appropriateness of the bias of a given algorithm for a particular domain depends on that domain; so the domain expert is ultimately responsible for making a reasonable choice. In a similar manner, domain knowledge is often used to select the parameters for the induction algorithm. <p> Some of the existing roles for knowledge are known to be sensitive to uncertainty. For example, wrong predictions may be made by a Bayesian method if it is given inaccurate estimates of prior probabilities <ref> (Cheeseman et al., 1988) </ref>. As another example, the original EBL method was criticized for being unable to learn with an incomplete domain theory (DeJong and Mooney, 1986).
Reference: <author> Chothia, C. and Lesk, A.M. </author> <year> 1986. </year> <title> The relation between divergence of sequence and structure in proteins. </title> <journal> EMBO Journal 5 </journal> <pages> 823-826. </pages>
Reference: <author> Chothia, C. </author> <year> 1988. </year> <title> The fourteenth barrel rolls out. </title> <booktitle> Nature 333 </booktitle> <pages> 598-599. </pages>
Reference-contexts: Occasionally, two protein will have similar structures but have apparently dissimilar sequences. For example, there are at least fourteen proteins that have the beta-barrel shape but are all pairwise distinct (nonhomologous) in sequence <ref> (Chothia, 1988) </ref>. <p> In fact, there are at least 14 mutually distinct beta-barrels known, including the prototypical triose-phosphate isomerase (TIM-barrel), as well as more diverse proteins such as taka alpha amylase and tryptophan synthase <ref> (Chothia, 1988) </ref>. The observation that proteins with similar structures do not necessarily have similar sequences is a major drawback for homology modeling. In particular, the hidden-homology problem causes false-negative predictions.
Reference: <author> Chothia, C. </author> <year> 1992. </year> <title> One thousand families for the molecular biologist. </title> <booktitle> Nature 357 </booktitle> <pages> 543-544. </pages>
Reference: <author> Chou, P.Y. and Fasman, G.D. </author> <year> 1974. </year> <title> Prediction of protein conformation. </title> <type> Biochemistry 13 </type> <pages> 222-244. </pages>
Reference-contexts: Many different algorithms have been invented for predicting secondary structure. One of the earliest approaches combined simple statistics for residue preferences with rules for nucleating, propagating, and terminating contiguous stretches of residues in a particular conformation <ref> (Chou and Fasman, 1974) </ref>. Since then, the statistics have been replaced by elaborate neural networks (Qian and Sejnowski, 1988), and the rules have grown into large knowledge bases of the roles specific amino acids can play in determining secondary structure (Lim, 1974).
Reference: <author> Chrisman, L. </author> <year> 1989. </year> <title> Evaluating bias during pac-learning. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning. </booktitle> <pages> 469-471. </pages>
Reference-contexts: NTC; Seshu et al., 1988) to version-space collapse (e.g. STABB; (Utgoff, 1986)) to variance minimization (e.g. MARS; Friedman, 1991) to PAC-criteria <ref> (Chrisman, 1989) </ref>. Finally, to the degree that domain knowledge is involved, it can be understood as helping to choose alternatives to explore.
Reference: <author> Cohen, W.W. </author> <year> 1990. </year> <title> An analysis of representation shift in concept learning. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning. </booktitle> <pages> 104-112. </pages>
Reference: <author> Cooper, A. </author> <year> 1976. </year> <title> Thermodynamic fluctutations in protein molecules. </title> <booktitle> Proceedings of the National Academy of Sciences, USA 73 </booktitle> <pages> 2740-2741. </pages>
Reference-contexts: White (1961) showed that, after denaturation, proteins can often spontaneously re-fold in the absence of all other cellular components, proving that the amino acid sequence alone determines the structure of a protein. While proteins naturally flex due to Brownian motion <ref> (Cooper, 1976) </ref>, and might undergo slight structural adjustments 71 full names, and 3-letter abbreviations. 72 during catalysis (Koshland, 1958), these variations are much insignificant compared to the differences in structure between different proteins (Sander and Schneider, 1991). 5.2 Levels of Structural Description Protein structures can be described at various levels of
Reference: <author> Cornette, J.L.; Cease, K.B.; Margalit, H.; Spouge, J.L.; Berzofsky, J.A.; and DeLisi, C. </author> <year> 1987. </year> <title> Hydrophobic scales and computational techniques for detecting amphipathic structures in proteins. </title> <journal> Journal of Molecular Biology 195 </journal> <pages> 659-685. </pages>
Reference-contexts: Some research has focused on independently ranking the 20 amino acids along various scales. For example, many different definitions for hydrophobicity have been proposed, based on criteria ranging from theoretical calculations of polarity to empirical measurements of partitioning between various solvents <ref> (Cornette et al., 1987) </ref>. Kidera et al. (1985) analyzed 188 amino acid property scales gathered from the literature and found that they clustered into nine general groups.
Reference: <author> Davies, T.R. and Russell, S.J. </author> <year> 1987. </year> <title> A logical approach to reasoning by analogy. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence. </booktitle> <pages> 264-270. </pages>
Reference-contexts: Abu-Mostafa (1994) presents a technique for using knowledge to generate artificial examples to illustrate some property of the target concept. Knowledge may also be incorporated into learning in the form of analogies, such as through determinations <ref> (Davies and Russell, 1987) </ref>, or by mapping the structure of one concept onto another when they are believed to be similar (Falkenhainer et al., 1989).
Reference: <author> Davis, R. </author> <year> 1982. </year> <title> Application of meta-level knowledge in the construction, maintenance and use of large knowledge bases. </title> <editor> In Davis, R. and Lenat, D.B., editors 1982, </editor> <booktitle> Knowledg-Based Systems in Artificial Intelligence. </booktitle> <publisher> McGraw-Hill: </publisher> <address> New York. </address> <pages> 229-490. </pages>
Reference-contexts: Instead, the feedback produced by our framework could also be given directly to the domain expert, who is in a better position to understand how to repair the theory <ref> (Davis, 1982) </ref>. A system based on our framework, could return critiques of individual rules in the theory, based on how useful they were during the search. It would then be the responsibility of the domain expert to decide how to update the theory.
Reference: <author> Davison, </author> <title> M.L. 1983. Multidimensional Scaling. </title> <publisher> Wiley: </publisher> <address> New York. </address>
Reference-contexts: Multidimensional scaling (MDS), which derives a new set of basis features from analysis of the differences among examples, can also be seen as a method of changing representations <ref> (Davison, 1983) </ref>. 2.3.5 The Complexity of Change-of-Representation A ubiquitous problem that all methods for change-of-representation must find a way to deal with is complexity. In general, there are usually an infinite number of ways to represent a given data set.
Reference: <author> Dayhoff, M.; Eck, R.; and Park, C. </author> <year> 1972. </year> <title> A model of evolutionary change in proteins. In Dayhoff, M., editor 1972, Atlas of Protein Sequence and Structure, </title> <booktitle> volume 5. National Biomedical Research Foundation: </booktitle> <address> Silver Springs, MD. </address> <note> 151 Dayhoff, </note> <author> M.; Schwartz, R.; and Orcutt, B. </author> <year> 1978. </year> <title> A model of evolutionary change in proteins. In Dayhoff, M., editor 1978, Atlas of Protein Sequence and Structure, </title> <booktitle> volume 5 (suppl. 3). National Biomedical Research Foundation: </booktitle> <address> Silver Springs, MD. </address> <pages> 345-358. </pages>
Reference-contexts: So it is 11 convenient that our framework allows us to improve homology modeling as a domain-specific method. To apply our framework, we need some domain knowledge, and for this purpose we can make use of biophysicists' knowledge of the chemical and physical properties of the amino acids <ref> (Dayhoff et al., 1972) </ref>. Based on the unique side-chain structure of each amino acid they can be described as large or small, flexible or rigid, polar or hydrophobic, aromatic, hydroxylated, charged, etc. (Taylor, 1986). <p> For example, analysis of our results suggest that the set of small amino acids should be reduced <ref> (Dayhoff et al., 1972) </ref>, histidine belongs in the positive group rather than the aromatic group (Taylor, 1986), and glutamine may play a unique role (Richardson and Richardson, 1989). * The relevant property at a site depends on its context, and this allows many more prop erties to be important besides hydrophobicity. <p> These differences do not significantly affect the qualitative nature of the results. 4 In this experiment, cysteine was initially grouped with the small residues, based on its occasional substitution with other members of this exchange group <ref> (Dayhoff et al., 1972) </ref>. 103 perturbing the Dayhoff partition. The Dayhoff partition (at iteration 0) is significantly improved during search, though the average accuracy never surpasses the baseline for identities (indicated by the horizontal line).
Reference: <author> DeJong, G.F. and Mooney, R.J. </author> <year> 1986. </year> <title> Explanation-based learning: An alternative view. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 145-176. </pages>
Reference-contexts: PROTOS uses knowledge to help decide the best way to weight attributes for indexing cases (Porter et al., 1990). Explanation-based learning uses knowledge to explain the classification of examples, thereby indicating which features are relevant to the classification and which may be dropped from the description to generalize it <ref> (DeJong and Mooney, 1986) </ref>. ILP uses knowledge to generate new terms for covering examples by the process of inverse resolution (Muggleton and Buntine, 1988). KBANN uses knowledge to define the topology of a neural network (Towell et al., 1990). <p> Some roles for knowledge consist of enhancements to standard induction algorithms, such as KBANN (Towell et al., 1990) or PROTOS (Porter et al., 1990). Others roles introduce novel learning algorithms to accommodate the knowledge, such as explanation-based learning <ref> (DeJong and Mooney, 1986) </ref> or analogy (Rajamoney, 1990). <p> Explanation (EBL) is another role that knowledge can play in learning <ref> (DeJong and Mooney, 1986) </ref>. This approach is useful when the domain knowledge can be used to explain why examples are classified the way they are. For example, if the domain theory is in the form of sentential logic (e.g. Horn clauses), then a proof-tree for each instance is constructed. <p> For example, wrong predictions may be made by a Bayesian method if it is given inaccurate estimates of prior probabilities (Cheeseman et al., 1988). As another example, the original EBL method was criticized for being unable to learn with an incomplete domain theory <ref> (DeJong and Mooney, 1986) </ref>.
Reference: <author> DeJong, </author> <title> G.F 1994. Learning to plan in continuous domains. </title> <booktitle> Artificial Intelligence 65 </booktitle> <pages> 71-141. </pages>
Reference-contexts: As another example, the original EBL method was criticized for being unable to learn with an incomplete domain theory (DeJong and Mooney, 1986). We want any new roles for knowledge to be more tolerant of uncertainty, such as approaches based on plausible inference <ref> (DeJong, 1994) </ref>. 35 Chapter 3 A Search-Based Framework for Change-of-Representation in Learning 3.1 Overview of the Framework In this chapter, we present a search-based framework for change-of-representation in machine learning.
Reference: <author> Devijver, P.A. and Kittler, J. </author> <year> 1982. </year> <title> Pattern Recognition: A Statistical Approach. </title> <publisher> Prentice Hall: </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: Several automated methods for changing representation in machine learning have been proposed (Schlimmer, 1987; Matheus, 1989; Pagallo and Haussler, 1990; Ragavan and Rendell, 1993). These methods have often been called constructive induction (Michalski, 1983), feature extraction <ref> (Devijver and Kittler, 1982) </ref>, or dynamic bias (Utgoff, 1986), though related methods have been proposed in other fields such as statistics and pattern recognition. However, a ubiquitous problem that all of these techniques must deal with in some way is the complexity of change-of-representation.
Reference: <author> Dietterich, T.G. and Michalski, R. </author> <year> 1986. </year> <title> Learning to predict sequences. </title> <editor> In Michalski, R.S.; Carbonell, J.G.; and Mitchell, T.M., editors 1986, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach II. </booktitle> <publisher> Morgan Kaufmann: </publisher> <address> Los Altos, CA. </address>
Reference-contexts: The task of finding higher-level representations seems easier, or at least more automatable, than inventing a novel induction algorithm with a radically different bias. Anecdotal evidence for the advantage gained by using change-of-representation in machine learning comes from studies in domains such as checkers <ref> (Flann and Dietterich, 1986) </ref> and recognition of promoters in DNA sequences (Hirsch and Japkowicz, 1994). Research on computational learning theory also lends theoretical support by linking the difficulty of learning a concept to its concept class, which can be manipulated by changing representations (Haussler, 1988). <p> MARS; Friedman, 1991) to PAC-criteria (Chrisman, 1989). Finally, to the degree that domain knowledge is involved, it can be understood as helping to choose alternatives to explore. Examples include the restriction of constructions in MIRO (Drastal et al., 1989), the choice of sequence re-representation operators in SPARC/E <ref> (Dietterich and Michalski, 1986) </ref>, the APC rules in Michalski's (1983) framework, and the syntactic class of features to extract from DNA (Hirsch and Japkowicz, 1994).
Reference: <author> Dietterich, T.G.; London, B.; Clarkson, K.; and Dromey, G. </author> <year> 1982. </year> <title> Learning and inductive inference. </title> <editor> In Cohen, P.R. and Feigenbaum, E.A., editors 1982, </editor> <booktitle> The Handbook of Artificial Intelligence III. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Given this ability to critique the theory as a whole, a more in-depth evaluation of knowledge would attempt to localize the parts of the domain theory that are incorrect, which is often called credit assignment <ref> (Dietterich et al., 1982) </ref>. While the abstract framework does not make any assumptions about the form of the domain theory, CORIL, our implementation in CLP (R), assumes that the domain knowledge is encoded in the form of a Horn-clause theory.
Reference: <author> Dietterich, T.G. </author> <year> 1986. </year> <title> Learning at the knowledge level. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 287-316. </pages>
Reference-contexts: The task of finding higher-level representations seems easier, or at least more automatable, than inventing a novel induction algorithm with a radically different bias. Anecdotal evidence for the advantage gained by using change-of-representation in machine learning comes from studies in domains such as checkers <ref> (Flann and Dietterich, 1986) </ref> and recognition of promoters in DNA sequences (Hirsch and Japkowicz, 1994). Research on computational learning theory also lends theoretical support by linking the difficulty of learning a concept to its concept class, which can be manipulated by changing representations (Haussler, 1988). <p> MARS; Friedman, 1991) to PAC-criteria (Chrisman, 1989). Finally, to the degree that domain knowledge is involved, it can be understood as helping to choose alternatives to explore. Examples include the restriction of constructions in MIRO (Drastal et al., 1989), the choice of sequence re-representation operators in SPARC/E <ref> (Dietterich and Michalski, 1986) </ref>, the APC rules in Michalski's (1983) framework, and the syntactic class of features to extract from DNA (Hirsch and Japkowicz, 1994).
Reference: <author> Dill, K. </author> <year> 1990. </year> <title> Dominant forces in protein folding. </title> <type> Biochemistry 29 </type> <pages> 7133-7155. </pages>
Reference-contexts: Examples of tertiary structures include helix-bundles (four parallel alpha-helices) and beta-barrels (a beta-sheet closed into a cylinder, with 8 alpha-helices packed around the outside; Chothia, 1988). 5.3 Factors Affecting Protein Structure A variety of factors affect the structure into which a protein will fold <ref> (Dill, 1990) </ref>. The forces involved can be broadly classified as `entropic' or `enthalpic.' Entropic forces are mainly concerned with the interaction between the protein molecule and the surrounding solvent molecules, such as water and salt ions. Water molecules tend to form a vast, loose network of hydrogen bonds.
Reference: <author> Donoho, S. and Rendell, L. </author> <year> 1996. </year> <title> Constructive induction using fragmentary knowledge. </title> <booktitle> In Proceedings of the International Conference on Machine Learning. </booktitle> <pages> accepted. </pages>
Reference-contexts: This approach is distinct from search through "feature space," which has been proposed to understand feature construction techniques <ref> (Donoho and Rendell, 1996) </ref>. Our framework treats representations as a whole, rather than searching for individual features that facilitate learning. This approach generalizes more easily to domains not based on fixed-length feature vectors, which will be important in our application to protein structure prediction. <p> Other studies have focused on categorizing ways of expressing knowledge so that it is relevant to representation <ref> (Donoho and Rendell, 1996) </ref>. The present research has focussed on mechanisms for using domain knowledge once it has been adapted in this way by the domain expert. <p> While we claim that there is often a fairly natural connection in many real-world domains, our framework does not automatically transform arbitrary domain knowledge into changes-of 140 representations, nor has this research focused on cataloging roles for knowledge to guide the domain expert <ref> (Donoho and Rendell, 1996) </ref>. The framework merely provides a general guideline for expressing knowledge (i.e. as a partial order among alternatives that are expected to improve learning), and it remains the expert's responsibility to find a way of adapting knowledge of the domain for this purpose.
Reference: <author> Doolittle, R.F. </author> <year> 1981. </year> <title> Similar amino acid sequences: Chance or common ancestry? Science 214 </title> <type> 149-159. </type>
Reference-contexts: For example, two randomly generated sequences of the 20 amino acids can be expected match about 5% of the time. Unrelated sequences in the protein databases actually have slightly higher similarities, partly due to non-uniform distributions of the amino acids <ref> (Doolittle, 1981) </ref>. Thus, a significance test is needed to determine whether a given score is high enough to be confident that they are evolutionarily divergent and hence structurally similar. <p> During the evolutionary divergence of two protein sequences, random segments of amino acids can be inserted or deleted at various points, accounting for the variation in length among proteins in the same fold class. Insertions and deletions are typically one to ten residues long <ref> (Doolittle, 1981) </ref>, and are most often found in the loop regions at the surface of a protein (Blundell et al., 1987). <p> The significance of an alignment score is much harder to determine theoretically than the significance of the percent amino acid identity between two protein sequences without gaps (Fitch, 1970; Steele, 1982). A more practical method for estimating the significance of an alignment score, called "shu*ing," is based on simulation <ref> (Doolittle, 1981) </ref>. Given the alignment score between two sequences, we want to know how likely it is that they could match that well without being evolutionarily related.
Reference: <author> Doolittle, R.F. </author> <year> 1986. </year> <title> Of Urfs and Orfs: A Primer on How to Analyze Devised Amino Acid Sequences. </title> <publisher> Oxford University Press: Oxford. </publisher>
Reference: <author> Draper, N.R. and Smith, H. </author> <year> 1981. </year> <title> Applied Regression Analysis. </title> <publisher> Wiley. </publisher>
Reference-contexts: Many algorithms, from CART (Breiman et al., 1984) to stepwise multiple regression <ref> (Draper and Smith, 1981) </ref>, work by iteratively identifying the variable most correlated with a subset of the training sample. These local decisions are not guaranteed to be globally optimal; a more compact concept description that recognizes higher-level patterns might exist. <p> Devijver and Kittler (1982) propose several heuristic techniques for feature selection, which reduces dimensionality by finding a subset of variables that separates classes well. This approach is similar to the use of stepwise multiple regression for selecting variables to include in a linear model <ref> (Draper and Smith, 1981) </ref>. Factor analysis can also be used to find a minimal set of variables that explains a maximal amount of variance in a dataset (Harman, 1976). <p> And stepwise multiple regression uses a greedy method for testing and selecting variables to add or drop one at a time, based on their effect on the variance of the model <ref> (Draper and Smith, 1981) </ref>. These techniques do not guarantee finding the best representation, and are unable to cover the whole space of possibilities, though they have been found to be effective heuristics in improving predictive accuracy.
Reference: <author> Drastal, G.; Czako, G.; and Raatz, S. </author> <year> 1989. </year> <title> Induction in an abstraction space. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence. </booktitle> <pages> 708-712. </pages>
Reference-contexts: ILP uses knowledge to generate new terms for covering examples by the process of inverse resolution (Muggleton and Buntine, 1988). KBANN uses knowledge to define the topology of a neural network (Towell et al., 1990). MIRO uses knowledge to filter the features generated by a feature construction algorithm <ref> (Drastal et al., 1989) </ref>. Abu-Mostafa (1994) presents a technique for using knowledge to generate artificial examples to illustrate some property of the target concept. <p> Another example of a method for using knowledge to improve a known learning technique is MIRO, which constrains the features generated by a feature-construction algorithm by filtering out all new features except those that can be deductively derived from the domain theory <ref> (Drastal et al., 1989) </ref>. Explanation (EBL) is another role that knowledge can play in learning (DeJong and Mooney, 1986). This approach is useful when the domain knowledge can be used to explain why examples are classified the way they are. <p> However, these approaches tend to de-emphasize the role of knowledge. Our framework is intentionally designed to replace complex techniques for selecting operators, operands, etc. with domain knowledge, making feature construction more knowledge-intensive. A system that illustrates how knowledge may be used in feature construction is MIRO <ref> (Drastal et al., 1989) </ref>, which restricts constructions to those that can be deduced from a background Horn-clause theory. <p> STABB; (Utgoff, 1986)) to variance minimization (e.g. MARS; Friedman, 1991) to PAC-criteria (Chrisman, 1989). Finally, to the degree that domain knowledge is involved, it can be understood as helping to choose alternatives to explore. Examples include the restriction of constructions in MIRO <ref> (Drastal et al., 1989) </ref>, the choice of sequence re-representation operators in SPARC/E (Dietterich and Michalski, 1986), the APC rules in Michalski's (1983) framework, and the syntactic class of features to extract from DNA (Hirsch and Japkowicz, 1994).
Reference: <author> Duda, R.O. and Hart, P.E. </author> <year> 1973. </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley. </publisher>
Reference-contexts: Feature extraction can transform the space in a fundamental way (more radical than simple projection, for example axis rotation) that might be necessary for detecting certain patterns <ref> (Duda and Hart, 1973) </ref>. New variables might be linear combinations of original features, as in principal component analysis. Or the examples might be completely re-described over a new set of basis features, such as in a Fourier transform (Mansour, 1994).
Reference: <author> Falkenhainer, B.; Forbus, K.D.; and Gentner, D. </author> <year> 1989. </year> <title> The structure-mapping engine: Algorithm and examples. </title> <booktitle> Artificial Intelligence 41 </booktitle> <pages> 1-63. </pages>
Reference-contexts: Knowledge may also be incorporated into learning in the form of analogies, such as through determinations (Davies and Russell, 1987), or by mapping the structure of one concept onto another when they are believed to be similar <ref> (Falkenhainer et al., 1989) </ref>. More generally, knowledge can be considered an aspect of bias, which is anything used to select hypotheses during learning, other than the data itself (Mitchell, 1980). Russell and Grosof (1987) show how inductive biases are equivalent to non-monotonic beliefs. <p> Several methods have been proposed for learning from analogies. One well-known approach is structure-mapping, which refers to the assumption that, aside from the specific names for terms in the new domain, the syntactic relationships among those terms is parallel to that in the old domain <ref> (Falkenhainer et al., 1989) </ref>. Thus detailed knowledge of how one domain works, plus a belief that is it similar to a new domain, plus an plausible mapping between terms in the two domains, can produce hypotheses in the new domain that follow the same pattern as in the old.
Reference: <author> Finkelstein, A.V. and Ptitsyn, O.B. </author> <year> 1987. </year> <title> Why do globular proteins fit the limited set of folding patterns? Progress in Biophysics and Molecular Biology 50 </title> <type> 171-190. </type>
Reference-contexts: This limit is also supported by a theoretical study of the number of ways in which secondary structures like alpha-helices and beta-sheets can possibly be combined in a typical-sized protein <ref> (Finkelstein and Ptitsyn, 1987) </ref>. Thus databases like the PDB may eventually grow from the approximately 100 distinct folds currently represented into a complete library for homology modeling. Homology modeling has been the most successful computational approach to protein tertiary structure prediction to date. However, even homology modeling has some limitations.
Reference: <author> Fisher, R. </author> <year> 1936. </year> <title> The use of multiple measurements in taxonomic problems. </title> <booktitle> Annual Eugenics 7 </booktitle> <pages> 179-188. </pages>
Reference-contexts: The efficacy of these algorithms has been demonstrated on a wide variety of benchmark domains (Weiss and Kapouleas, 1989). However, current machine learning techniques are inadequate for learning in more difficult real-world domains. Whereas benchmark domains have usually been studied over a extended period of time <ref> (e.g. the iris database, Fisher, 1936) </ref>, researchers often lack a good understanding of phenomena in a real-world domain, and machine learning itself may be being used as a tool to evolve such a theory.
Reference: <author> Fisher, F.H. </author> <year> 1987. </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <booktitle> Machine Learning 2 </booktitle> <pages> 139-172. </pages> <note> 152 Fitch, </note> <author> W.M. and Smith, T.F. </author> <year> 1983. </year> <title> Optimal sequence alignments. </title> <booktitle> Proceedings of the National Academy of Sciences, USA </booktitle> 80:1382-1386. 
Reference-contexts: A common constructive induction technique is to invent new terms, such as by clustering (Anderberg, 1973). Clustering can improve a representation by giving a unique label to separate peaks in instance space (Rendell, 1985). Examples of induction systems that exploit clustering are CLUSTER (Michalski and Stepp, 1983) and COBWEB <ref> (Fisher, 1987) </ref>. Another method 26 for inventing new terms is inductive-logic programming (ILP).
Reference: <author> Fitch, W.M. </author> <year> 1970. </year> <title> Further improvements in the method of testing for evolutionary homology among proteins. </title> <journal> Journal of Molecular Biology 49 </journal> <pages> 1-14. </pages>
Reference: <author> Flann, N.S. and Dietterich, T.G. </author> <year> 1986. </year> <title> Selecting appropriate representations for learning from examples. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence. </booktitle> <pages> 460-466. </pages>
Reference-contexts: The task of finding higher-level representations seems easier, or at least more automatable, than inventing a novel induction algorithm with a radically different bias. Anecdotal evidence for the advantage gained by using change-of-representation in machine learning comes from studies in domains such as checkers <ref> (Flann and Dietterich, 1986) </ref> and recognition of promoters in DNA sequences (Hirsch and Japkowicz, 1994). Research on computational learning theory also lends theoretical support by linking the difficulty of learning a concept to its concept class, which can be manipulated by changing representations (Haussler, 1988).
Reference: <author> Ford, K.M. and Hayes, P.J., </author> <title> editors 1991. Reasoning Agents in a Dynamic World: The Frame Problem. </title> <publisher> JAI Press: London. </publisher>
Reference: <author> Friedman, J. </author> <year> 1991. </year> <title> Multivariate adaptive regression splines. </title> <journal> Annals of Statistics 19 </journal> <pages> 1-141. </pages>
Reference-contexts: A domain expert suggested that the occurrence of a particular subsequence was probably relevant because of how it twisted the strand to interact with DNA-binding proteins. Hirsch and Japkowicz generalized this knowledge to try extracting other features with a similar syntactic structure. Even MARS <ref> (Friedman, 1991) </ref> can be considered to do change-of-representation in statistical context. MARS is based on a parametric method for fitting spline curves to multi-dimensional functions.
Reference: <author> Fu, L. and Buchanan, B.G. </author> <year> 1985. </year> <title> Learning intermediate concepts in constraining a hierarchical knowledge base. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence. </booktitle> <pages> 659-666. </pages>
Reference-contexts: One might know spatial relationships among the attributes measured, such as proximity of positions on a chess board (Ragavan and Rendell, 1991). One might have a plausible model for a causal mechanism that underlies the phenomenon (Rajamoney, 1990). Intermediate concepts or knowledge of related concepts might be available <ref> (Fu and Buchanan, 1985) </ref>. Knowledge about the data itself, such sample bias, precision of the instruments, or 30 critical cases might be known (Gaines, 1989). Another class of knowledge relates to properties of the target concept, such as degree of disjunction (Valiant, 1984). <p> The primitives in symbolic descriptions, along with their syntactic relationships, reflect semantic relationships among domain objects via the denotation function. Thus other concepts that cover different subsets of objects can become new syntactic elements, which is most clearly illustrated by encoding intermediate concepts as new attributes <ref> (Fu and Buchanan, 1985) </ref>. These might help learning if they share boundaries with the target concept, because they give the learner hints for where to extend generalizations (Utgoff, 1986).
Reference: <author> Fu, K.S. </author> <year> 1982. </year> <title> Syntactic Pattern Recognition and Applications. </title> <publisher> Prentice-Hall: </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Gaines, B. </author> <year> 1989. </year> <title> An ounce of knowledge is worth a ton of data. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning. </booktitle> <pages> 156-159. </pages>
Reference-contexts: One might have a plausible model for a causal mechanism that underlies the phenomenon (Rajamoney, 1990). Intermediate concepts or knowledge of related concepts might be available (Fu and Buchanan, 1985). Knowledge about the data itself, such sample bias, precision of the instruments, or 30 critical cases might be known <ref> (Gaines, 1989) </ref>. Another class of knowledge relates to properties of the target concept, such as degree of disjunction (Valiant, 1984). Most induction algorithms are not designed to make use of such domain knowledge. Standard learning algorithms often attempt to construct generalizations from a set of examples alone.
Reference: <author> Genesereth, M.R. and Nilsson, N.J. </author> <year> 1987. </year> <booktitle> Logical Foundations of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann: </publisher> <address> Palo Alto, CA. </address>
Reference-contexts: Our goal is to show how to encode the physician's background knowledge in our formalism such that it generates suggestions for alternative representations. We will encode it in first-order logic (FOL), since this has an simple and agreed-upon semantics <ref> (Genesereth and Nilsson, 1987) </ref>. Recall that knowledge in our framework is formally defined as a mapping from partial state function to a partial order among other representations based on their expected effects on accuracy. To simulate this, we will use some special `input' and `output' predicates. <p> The theory will have to be adapted to conclude order (r 1 ; r 2 ) for each pair of alternative representations such that acc (r 1 ) &gt; acc (r 2 ). The partial order, as a mathematical relation, will arise from the model-theoretic interpretation of `order' predicate <ref> (Genesereth and Nilsson, 1987) </ref>. We begin by formalizing the intuition that fevers are sign of infection, so discretizing temperature at a cutoff slightly above normal body temperature might help learning. First, infection is our target concept, and infections are usually associated with bacteria, which produce toxins in the blood.
Reference: <author> Ginsberg, A.; Weiss, S.M.; and Politakis, P. </author> <year> 1988. </year> <title> Automatic knowledge base refinement for classification systems. </title> <booktitle> Artificial Intelligence 35 </booktitle> <pages> 197-226. </pages>
Reference-contexts: This tree reveals the dependence of erroneous conclusions on individual parts of the theory. More specific localization of a fault could be achieved by accumulating statistics on the involvement of each rule over multiple successes and failures, as is done in SEEK2 <ref> (Ginsberg et al., 1988) </ref>. This method for evaluating knowledge could be used to support a semi-automated version of theory revision (Pazzani, 1988; Towell et al., 1990; Mooney and Ourston, 1994).
Reference: <author> Glasgow, J.I.; Fortier, S.; and Allen, F.H. </author> <year> 1993. </year> <title> Molecular scene analysis: Crystal structure determination through imagery. </title> <editor> In Hunter, L., editor 1993, </editor> <booktitle> Artificial Intelligence and Molecular Biology. </booktitle> <publisher> MIT Press: </publisher> <address> Cambridge, CA. </address>
Reference-contexts: Also, the interpretation of the diffraction data is under-constrained, which is a significant problem because there are a huge number of possible three-dimensional conformations and hence potential models to consider. Highly compute-intensive programs have been written to facilitate crystallographic data analysis <ref> (Glasgow et al., 1993) </ref>, but it still can take months before a structure is confidently identified. Nuclear magnetic resonance (NMR; Richards, 1992), another physical method for determining protein structures, also suffers from difficulties in obtaining material samples.
Reference: <author> Gordon, D. and DesJardins, M. </author> <year> 1995. </year> <title> Evaluation and selection of biases in machine learning. </title> <booktitle> Machine Learning 20 </booktitle> <pages> 5-22. </pages>
Reference: <author> Gotoh, O. </author> <year> 1982. </year> <title> An improved algorithm for matching biological sequences. </title> <journal> Journal of Molecular Biology 162 </journal> <pages> 705-708. </pages>
Reference: <author> Gribskov, M.; McLachlan, A.D.; and Eisenberg, D. </author> <year> 1987. </year> <title> Profile analysis: Detection of distantly related proteins. </title> <booktitle> Proceedings of the National Academy of Sciences, USA 84 </booktitle> <pages> 4355-4358. </pages>
Reference-contexts: Furthermore, our improvement was achieved without resorting to tertiary structure data as an extra source of information, as is done in Eisenberg's 3D-1D profiling method <ref> (Gribskov et al., 1987) </ref> and Lathrop's ARIADNE/ARIEL method (Lathrop et al., 1993).
Reference: <author> Gribskov, M.; Homyak, M.; Edenfield, J.; and Eisenberg, D. </author> <year> 1988. </year> <title> Profile scanning for three-dimensional structural patterns in protein sequences. </title> <booktitle> CABIOS 4 </booktitle> <pages> 61-66. </pages>
Reference-contexts: If the two proteins are indeed unrelated, then their alignment score should follow the scores from comparisons with other such proteins. Although this distribution is not necessarily normal, alignment scores more than three or so standard deviations above the mean are generally considered to be significant <ref> (Gribskov et al., 1988) </ref>. While this method for estimating the significance of an alignment score makes fewer assumptions about evolutionary processes, one drawback is that it is sensitive to the kinds of sequences represented 82 in the database (such as primarily alpha proteins versus primarily beta proteins).
Reference: <author> Harman, H.H. </author> <year> 1976. </year> <title> Modern Factor Analysis. </title> <publisher> University of Chicago Press: </publisher> <address> Chicago, IL. </address>
Reference-contexts: This approach is similar to the use of stepwise multiple regression for selecting variables to include in a linear model (Draper and Smith, 1981). Factor analysis can also be used to find a minimal set of variables that explains a maximal amount of variance in a dataset <ref> (Harman, 1976) </ref>. Devijver and Kittler (1982) also propose methods for feature extraction, in which multiple features are summarized; some of the information lost in deleted features is incorporated in new (computed) variables.
Reference: <author> Haussler, D. </author> <year> 1988. </year> <title> Quantifying inductive bias: Ai learning algorithms and valiant's learning framework. </title> <booktitle> Artificial Intelligence 36 </booktitle> <pages> 177-221. </pages> <note> 153 Hayes-Roth, </note> <author> B.; Buchanan, B.; Lichtarge, O.; Hewett, M.; Altman, R.; Brinkley, J.; Cor--nelius, C.; Duncan, B.; and Jardetsky, O. </author> <year> 1986. </year> <title> Protean: Deriving protein structure from constraints. </title> <booktitle> In Fifth National Conference on Artificial Intelligence. </booktitle> <pages> 904-909. </pages>
Reference-contexts: Research on computational learning theory also lends theoretical support by linking the difficulty of learning a concept to its concept class, which can be manipulated by changing representations <ref> (Haussler, 1988) </ref>. Several automated methods for changing representation in machine learning have been proposed (Schlimmer, 1987; Matheus, 1989; Pagallo and Haussler, 1990; Ragavan and Rendell, 1993). <p> Saxena (1991) has presented a theoretical explanation 19 of this interaction between representational and inductive biases based on data compression. Other theoretical work has looked at the effect of different representations on learning in terms of altering the concept-class complexity <ref> (Haussler, 1988) </ref>. 2.2 Low-Level Representations Because of interaction between the representation and the inductive algorithm, the utility of a representation for learning depends on the algorithm being used. So a given representation may produce adequate results with one algorithm but poor performance with another.
Reference: <author> Henikoff, S. and Henikoff, J.G. </author> <year> 1993. </year> <title> Performance evaluation of amino acid substitution matrices. </title> <booktitle> Proteins 17 </booktitle> <pages> 49-61. </pages>
Reference: <author> Hirsch, H. and Japkowicz, N. </author> <year> 1994. </year> <title> Bootstrapping training-data representations for inductive learning: A case study in molecular biology. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle> <pages> 639-644. </pages>
Reference-contexts: Anecdotal evidence for the advantage gained by using change-of-representation in machine learning comes from studies in domains such as checkers (Flann and Dietterich, 1986) and recognition of promoters in DNA sequences <ref> (Hirsch and Japkowicz, 1994) </ref>. Research on computational learning theory also lends theoretical support by linking the difficulty of learning a concept to its concept class, which can be manipulated by changing representations (Haussler, 1988). <p> Examples include the restriction of constructions in MIRO (Drastal et al., 1989), the choice of sequence re-representation operators in SPARC/E (Dietterich and Michalski, 1986), the APC rules in Michalski's (1983) framework, and the syntactic class of features to extract from DNA <ref> (Hirsch and Japkowicz, 1994) </ref>.
Reference: <author> Hirschberg, D.S. </author> <year> 1975. </year> <title> A linear space algorithm for computing maximal common subsequences. </title> <booktitle> Communication of the ACM 18 </booktitle> <pages> 341-343. </pages>
Reference: <author> Holland, J.H. </author> <year> 1975. </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press: </publisher> <address> Ann Arbor, MI. </address>
Reference-contexts: Another similar variation would be to carry out a parallel search, for example by using techniques such as genetic algorithms <ref> (Holland, 1975) </ref>. Partitions could be encoded in a special language as bit-vectors which allow cross-overs to be defined. Individual groups could then be optimized is separate partitions and recombined later. Finally, we could encode a more intelligent swapping operator that incorporates more specific domain knowledge.
Reference: <author> Holte, R.C. and Zimmer, R.M. </author> <year> 1989. </year> <title> A mathematical framework for studying representation. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning. </booktitle> <publisher> Morgan Kaufmann: </publisher> <address> San Mateo, CA. </address> <pages> 454-456. </pages>
Reference: <author> Holte, R.C. </author> <year> 1993. </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning 11 </booktitle> <pages> 63-91. </pages>
Reference-contexts: In contrast, the benchmark domains discussed above have already had high-level features extracted which have a strong relationship to the target concept and make patterns more obvious <ref> (Holte, 1993) </ref>. Low-level representations inhibit learning because they interfere with several biases that are pervasive among known induction algorithms. Low-level representations may cause phenomena such as concept dispersion or feature interaction, which interfere respectively with the similarity bias and the greedy bias (Rendell and Seshu, 1990).
Reference: <author> Hong, S.J. </author> <year> 1994. </year> <title> Use of contextual information for feature ranking and discretization. </title> <type> Technical Report Research report RC19664, </type> <institution> IBM Research Division; Yorktown Heights, NY. </institution>
Reference-contexts: For example, STAGGER evaluates the utility of features by computing a function of the number of positive and negative examples covered by each construction. Other approaches to feature evaluation include blurring (Rendell and Ragavan, 1993) and contextual merit <ref> (Hong, 1994) </ref>. In comparison to our framework, evaluating independent features may be more efficient, since bad ones can be ruled out once, rather than tested with many different representations.
Reference: <author> Ioerger, T.R.; Rendell, L.; and Subramaniam, S. </author> <year> 1995. </year> <title> Searching for representations to improve protein sequence fold-class prediction. </title> <booktitle> Machine Learning 21 </booktitle> <pages> 151-175. </pages>
Reference-contexts: Our framework has enabled us to achieve significant results in this difficult but important domain <ref> (Ioerger et al., 1995) </ref>. Proteins are complex molecules in cells that serve many diverse biological functions (Zubay, 1983). Proteins are composed of linear chains of smaller compounds called amino acids, of which there are twenty. The DNA in a cell encodes a unique sequence of amino acids for each protein. <p> The best eleven partitions found are listed in Table 6.4. The highest accuracy achieved by any partition when used to transform amino acid sequences for homology modeling was 81.0%, as measured over the entire dataset, which was over 3 This graph was reported in <ref> (Ioerger et al., 1995) </ref>. The data points were generated using a 179-protein subset of the dataset described in Table 6.3. Also, the gap weights used in the alignment algorithm were 3.0 for gap-open-penalty and 0.1 for gap-extend-penalty.
Reference: <author> Jaffar, Joxan and Lassez, </author> <title> Jean-Louis 1987. Constraint logic programming. </title> <booktitle> In Fourth ACM Symposium on Principles of Programming Languages. </booktitle> <pages> 111-119. </pages>
Reference-contexts: we give an extended example, focusing at first on how domain knowledge can be encoded in the formalism of the framework, and then showing how the example could be implemented in CORIL to actually solve the problem. 4.1 Implementation: CORIL CORIL is based on CLP (R), a constraint logic-programming language <ref> (Jaffar and Lassez, 1987) </ref>. Like PROLOG, CLP (R) can make inferences from Horn-clause theories using SLD resolution (Lloyd, 1988).
Reference: <author> Jones, D. and Thornton, J. </author> <year> 1993. </year> <title> Protein fold recognition. </title> <journal> Journal of Computer-Aided Molecular Design 7 </journal> <pages> 439-456. </pages>
Reference-contexts: So the goal of protein tertiary structure prediction can be reduced to simply identifying the fold class of a protein, which has many fewer degrees of freedom than a complete set of atomic coordinates <ref> (Jones and Thornton, 1993) </ref>. After the fold class of a protein has been identified, its sequence may be aligned to another member of the same class to derive an approximate structure, and the predicted atomic coordinates may subsequently be refined by molecular simulation (Nell et al., 1992). <p> The basic problem with homology modeling stems from the fact that, while proteins with similar sequences always have similar structures, the reverse does not always hold true. Recently it has been observed that some proteins with similar structures do not appear to have significant sequence similarity <ref> (Jones and Thornton, 1993) </ref>.
Reference: <author> Jones, R. </author> <year> 1992. </year> <title> Protein sequence and structure comparison on massively parallel computers. </title> <journal> International Journal of Supercomputer Applications 6 </journal> <pages> 138-146. </pages>
Reference-contexts: Another improvement in the sequence alignment algorithm was to reduce the memory requirements to linear space-complexity, which can be important when aligning long pairs of sequences (Hirschberg, 1975; Myers and Miller, 1988). Parallel implementations have also been studied, in order to take advantage of massively-parallel supercomputer architectures <ref> (Jones, 1992) </ref>. The significance of an alignment score is much harder to determine theoretically than the significance of the percent amino acid identity between two protein sequences without gaps (Fitch, 1970; Steele, 1982). <p> To accomplish this, we implemented a standard alignment algorithm on the CM5 massively-parallel supercomputer. The module was written in C using the CMMD message-passing library. Whole alignments were distributed as independent tasks to available processors, rather than parallelizing the alignment algorithm itself <ref> (Jones, 1992) </ref>. On a 256-processor partition, doing all 20,000 pairwise alignments necessary to evaluate one representation takes approximately 30 96 Table 6.3: The protein data used in the experiments in this thesis.
Reference: <author> Kabsch, W. and Sander, C. </author> <year> 1983. </year> <title> Dictionary of protein secondary structure: Pattern recognition of hydrogen-bonded and geometric features. </title> <type> Biopolymers 22 </type> <pages> 2577-2637. </pages>
Reference-contexts: Repetitions of (60; 60) form an alpha-helix, and repetitions of (90; +120) form a beta-strand. Hydrogen-bonding patterns between backbone atoms have also been used to define secondary structures at the local level, such 73 as between loops in an alpha-helix and between strands in a beta-sheet <ref> (Kabsch and Sander, 1983) </ref>. At the highest level of description is tertiary structure, which refers to the global arrangement of secondary structures, also called the fold of the protein (Richardson, 1981).
Reference: <author> Kadie, </author> <title> C.M. 1995. SEER: Maximum-likelihood regression curves for learning-speed curves. </title> <type> Ph.D. Dissertation, </type> <institution> University of Illinois, Department of Computer Science. </institution>
Reference-contexts: a conditional: A (S) = h ( jIj X 1 fP red (x)=Class (x)jSg (P (i))i IN : This approach does not capture the general performance of a learner, as is commonly done with learning curves that average expected accuracies over multiple samples as a function of training set size <ref> (Kadie, 1995) </ref>. Rather, our pragmatic focus is on assessing the ability of various learners to make the best generalizations from a given set of data. We cannot know the "true" error rate of a learner, since by assumption we do not have unlimited access to the distribution sampling test cases.
Reference: <author> Kidera, A.; Konishi, Y.; Oka, M.; Ooi, T.; and Scheraga, H.A. </author> <year> 1985. </year> <title> Statisitical analysis of the physical properties of the 20 naturally occuring amino acids. </title> <journal> Journal of Protein Chemistry 4 </journal> <pages> 23-54. </pages>
Reference: <author> King, R.D.; Hirst, J.D.; and Sternberg, M.J.E. </author> <year> 1995. </year> <title> A comparison of artificial intelligence methods for modelling pharmaceutical qsars. </title> <booktitle> Applied Artificial Intelligence 9 </booktitle> <pages> 213-234. </pages> <address> 154 Korf, R.E. </address> <year> 1980. </year> <title> Towards a model of representation changes. </title> <booktitle> Artificial Intelligence </booktitle> 14:41-78. 
Reference-contexts: Examples of such domains include weather prediction (Packard, 1989), financial risk analysis (Ragavan et al., 1993), and drug design <ref> (King et al., 1995) </ref>. In real-world domains, huge databases may be available, but simply applying one of the standard induction algorithms often produces only low predictive accuracy. Several potential causes of learning difficulty in real-world domains have been studied.
Reference: <author> Korf, R.E. </author> <year> 1982. </year> <title> A program that learns to solve rubik's cube. </title> <booktitle> In Proceedings of AAAI-82. </booktitle> <pages> 164-167. </pages>
Reference: <author> Koshland, D.E. </author> <year> 1958. </year> <title> Application of a theory of enzyme specificity to protein synthesis. </title> <booktitle> Proceedings of the National Academy of Sciences, USA 44 </booktitle> <pages> 98-104. </pages>
Reference-contexts: While proteins naturally flex due to Brownian motion (Cooper, 1976), and might undergo slight structural adjustments 71 full names, and 3-letter abbreviations. 72 during catalysis <ref> (Koshland, 1958) </ref>, these variations are much insignificant compared to the differences in structure between different proteins (Sander and Schneider, 1991). 5.2 Levels of Structural Description Protein structures can be described at various levels of detail.
Reference: <author> Langley, P.; Bradshaw, G.L.; and Simon, H.A. </author> <year> 1983. </year> <title> Redicovering chemistry with the bacon system. </title> <editor> In Michalski, R.S.; Carbonell, J.G.; and Mitchell, T.M., editors 1983, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This causes the instances to shift in the space so that there are fewer, larger peaks. entropy (Ragavan and Rendell, 1993). BACON decides which features to construct by looking for products and ratios of numeric variables that are nearly constant <ref> (Langley et al., 1983) </ref>. 2.3.3 Dynamic Bias Another way of looking at change-of-representation in machine learning is as a form of dynamic bias (Utgoff, 1986). <p> For example, FRINGE looks for patterns of variables that recur at the leaves of a decision tree (Pagallo and Haussler, 1990). BACON looks for mathematical combinations of numeric variables that are nearly constant <ref> (Langley et al., 1983) </ref>. LFC uses a geometric constraint to rule out combinations unlikely to decrease entropy, and then uses lookahead to dynamically explore a limited amount 135 of feature space to decide which features to construct (Ragavan and Rendell, 1993).
Reference: <author> Lasters, I. </author> <year> 1990. </year> <title> The design of idealized alpha-beta barrels: Analysis of beta-sheet closure requirements. </title> <booktitle> Proteins 7 </booktitle> <pages> 249-256. </pages>
Reference-contexts: Many rules governing the packing of secondary structures are known, such as the propensity for beta-sheets to lie in the core of a protein (Schulz and Schirmer, 1979), and the restriction of contacts between helices to specific angles <ref> (Lasters, 1990) </ref>. Examples of tertiary structures include helix-bundles (four parallel alpha-helices) and beta-barrels (a beta-sheet closed into a cylinder, with 8 alpha-helices packed around the outside; Chothia, 1988). 5.3 Factors Affecting Protein Structure A variety of factors affect the structure into which a protein will fold (Dill, 1990).
Reference: <author> Lathrop, R.H. and Smith, T.F. </author> <year> 1996. </year> <title> Global optimum protein threading with gapped alignment and empirical pair score functions. </title> <journal> Journal of Molecular Biology 255 </journal> <pages> 641-665. </pages>
Reference-contexts: One of the difficult aspects of evaluating fold potential is the initial step of folding the sequence into the candidate structure, also called threading <ref> (Lathrop and Smith, 1996) </ref>. This structural alignment suffers the same complications caused by insertions and deletions as in sequence alignment (Taylor and Orengo, 1989).
Reference: <author> Lathrop, R.H.; Webster, T.A.; and Smith, T.F. </author> <year> 1987. </year> <title> Ariadne: Pattern-directed and hierarchical abstraction in protein structure prediction. </title> <journal> Communications of the ACM 30 </journal> <pages> 909-921. </pages>
Reference-contexts: Thus the symbols act as discrete versions of the distributions in profiles, but are extra-restrictive to filter out less informative sites. ARIADNE is another template-based program for generalizing multiple examples of a fold <ref> (Lathrop et al., 1987) </ref>. ARIADNE uses a hierarchical pattern-language to represent folds at multiple levels of description.
Reference: <author> Lathrop, R.; Webster, T.A.; Smith, T.F.; and Winston, P.H. </author> <year> 1990. </year> <title> Ariel: A massively-parallel symbolic learning assistant for protein structure/function. </title> <editor> In Winston, P.H. and Shellard, S., editors 1990, </editor> <booktitle> Artificial Intelligence at MIT: Expanding Frontiers. </booktitle> <publisher> MIT Press: </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The actual generation of fold patterns from examples is carried out by ARIEL, a massively-parallel program that implements inductive heuristics to extract commonalities among hierarchical descriptions <ref> (Lathrop et al., 1990) </ref>. 87 Chapter 6 Experiments In this chapter, we will demonstrate the framework by applying it to improve homology modeling, particularly by addressing the hidden-homology problem.
Reference: <author> Lathrop, R.; Webster, T.; Smith, R.; Winston, P.; and Smith, T. </author> <year> 1993. </year> <title> Integrating ai with sequence analysis. </title> <editor> In Hunter, L., editor 1993, </editor> <booktitle> Artificial Intelligence and Molecular Biology. </booktitle> <publisher> AAAI Press: </publisher> <address> Menlo Park, CA. </address> <pages> 210-258. </pages>
Reference-contexts: Furthermore, our improvement was achieved without resorting to tertiary structure data as an extra source of information, as is done in Eisenberg's 3D-1D profiling method (Gribskov et al., 1987) and Lathrop's ARIADNE/ARIEL method <ref> (Lathrop et al., 1993) </ref>. Though no direct comparison was made to these systems, they are likely to improve the predictive ability of homology modeling beyond our approach, since knowledge of at least one of the structures of the two sequences being aligned can greatly inform the local matching process. <p> Substitution tables ignore this background knowledge altogether, instead resorting to statistical summaries of observed substitutions to define the similarities among the amino acids (Dayhoff et al., 1978). Approaches such as ARIADNE/ARIEL <ref> (Lathrop et al., 1993) </ref> and PIMA (Smith and Smith, 1990) can incorporate knowledge of amino acid properties into pattern-based representations of proteins. However, these systems require multiple alignments among a set of related sequences to determine the constraints on each site for generalization.
Reference: <author> Levitt, M. and Chothia, C. </author> <year> 1976. </year> <title> Structural patterns in globular proteins. </title> <booktitle> Science 261 </booktitle> <pages> 552-557. </pages>
Reference-contexts: Structural similarity has a technical definition, called RMSD, that is based on measuring the deviation of corresponding atoms in the protein backbone when they are optimally super-imposed <ref> (Levitt and Chothia, 1976) </ref>. However, Richardson (1981) observed that protein structures tend to fall into clusters of shapes, which she called folds, and a simpler expression of structural similarity is that two proteins belong to the same fold class. <p> The trace of backbone atoms contributed by each amino acid forms a super-structure around which the side-chains pack, filling up the volume and generally leaving no cavities (Ponder and Richards, 1987). Certain sub-structures within proteins have been observed as recurrent patterns in the meander of the backbone <ref> (Levitt and Chothia, 1976) </ref>. There are two main classes of substructures, also called secondary structures: alpha-helices and beta-sheets. An alpha-helix is a short stretch of about 20 amino acids wound in a tight coil with about 3.5 residues per turn (Srinivasan, 1976).
Reference: <author> Lim, V. </author> <year> 1974. </year> <title> Algorithms for proediction of alpha-helical and beta-structural regions in globular proteins. </title> <journal> Journal of Molecular Biology 88 </journal> <pages> 873-894. </pages>
Reference-contexts: Since then, the statistics have been replaced by elaborate neural networks (Qian and Sejnowski, 1988), and the rules have grown into large knowledge bases of the roles specific amino acids can play in determining secondary structure <ref> (Lim, 1974) </ref>. The accuracy of secondary structure prediction algorithms can be tested on individual sites in the amino acid sequences of proteins whose structures have been determined.
Reference: <author> Lipman, D.J. and Pearson, W.R. </author> <year> 1985. </year> <title> Rapid and sensitive protein similarity searches. </title> <booktitle> Science 227 </booktitle> <pages> 1435-1441. </pages>
Reference-contexts: A sampling of the similarity scores between shu*ed sequences forms a distribution that is approximately normal and can be used to compute a z-score for the original alignment, in the standard statistical sense <ref> (Lipman and Pearson, 1985) </ref>. One disadvantage of shu*ing is that it relies on various assumptions about the independence of mutations, which might not be an accurate model of molecular evolution.
Reference: <author> Lloyd, J. </author> <year> 1988. </year> <title> Foundations of Logic Programming. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Like PROLOG, CLP (R) can make inferences from Horn-clause theories using SLD resolution <ref> (Lloyd, 1988) </ref>. The requires the domain knowledge to be translated into Horn-clauses, though this format is fairly close in syntax and semantics to first-order logic, making it fairly easy to convert theories either automatically or with some minor manual assistance.
Reference: <author> Mansour, Y. </author> <year> 1994. </year> <title> Learning boolean functions via the fourier transform. </title> <editor> In Roychodhury, V.P.; Siu, K-Y.; and Orlitsky, A., editors 1994, </editor> <booktitle> Advances in Neural Computation. </booktitle> <publisher> Kluwer. </publisher>
Reference-contexts: New variables might be linear combinations of original features, as in principal component analysis. Or the examples might be completely re-described over a new set of basis features, such as in a Fourier transform <ref> (Mansour, 1994) </ref>.
Reference: <author> Matheus, C.J. and Rendell, L.A. </author> <year> 1989. </year> <title> Constructive induction on decision trees. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence. </booktitle> <pages> 645-650. </pages>
Reference-contexts: Despite the differences among algorithms like ID3, GREEDY3, NT-Growth, and BackProp, they all implement the similarity-based learning (SBL) bias, tending to classify unseen instances like nearby instances that were seen during training <ref> (Matheus and Rendell, 1989) </ref>. Nearest-neighbor learning algorithms implement the SBL bias directly (Aha et al., 1991). This bias is very general and is pervasive among commonly known learning algorithms. <p> ILP uses inverse resolution to augment a domain theory with a new predicate that covers some unexplained observations (Muggleton and Buntine, 1988). 2.3.2 Feature Construction In the context of feature-vector based domains, much of the research on constructive induction has focused on a specialized class of techniques called feature construction <ref> (Matheus and Rendell, 1989) </ref>. In feature construction, the initial representation of a domain is extended by computing a new feature as a function of the original feature values for each example, and then literally appending it as an extra component to each vector. <p> If newly constructed features are allowed to be candidates in subsequent constructions, then there can be an exponential number of extensions to an initial feature vector <ref> (Matheus, 1989) </ref>. Each of the automated methods for change-of-representation we discussed above has some specific way of dealing with this complexity. Typically, constraints on which changes-of 29 representation will be considered are implicitly built into the algorithm. <p> Domain knowledge can act in several ways to guide the search. First, knowledge can limit the space of alternatives by specifying a class of syntactic changes that are worth considering. This is most clearly illustrated in feature construction in the phase of operator selection 44 <ref> (Matheus, 1989) </ref>. By choosing a particular operator, such as PLUS or XOR, the space of possi-ble alternative representations is restricted to those which include only those new features that can be constructed from the original attributes using this operator. <p> Some change-of-representation techniques, such as FRINGE (Pagallo and Haussler, 1990), require the use of a specific kind of induction algorithm, which would probably result in an initial decrease in performance from the current established level. Furthermore, most known change-of-representation techniques are based on feature construction <ref> (Matheus, 1989) </ref>, which assumes that examples can be expressed as feature-vectors of uniform length; but proteins are fundamentally variable in length. <p> We discuss the generality and flexibility of this role for knowledge, and we conclude this section with a discussion of the limitations of our approach. 7.2.1 Comparison to Other Change-of-Representation Systems 7.2.1.1 Feature Construction The approach to change-of-representation in machine learning that is closest to our framework is feature construction <ref> (Matheus, 1989) </ref>. Feature construction refers to the generation of new variables by applying operators, often mathematical or logical, to attributes in the original feature vector. Values for constructed features are computed for each example and appended to its description. <p> Values for constructed features are computed for each example and appended to its description. Some example systems that construct features are FRINGE (Pagallo and Haussler, 1990), STAGGER (Schlimmer, 1987), CITRE <ref> (Matheus and Rendell, 1989) </ref>, and LFC (Ragavan and Rendell, 1993).
Reference: <author> Matheus, C.J. </author> <year> 1989. </year> <title> Feature Construction: An Analytic Framework and an Application to Decision Trees. </title> <type> Ph.D. Dissertation, </type> <institution> University of Illinois, Department of Computer Science. </institution>
Reference-contexts: Despite the differences among algorithms like ID3, GREEDY3, NT-Growth, and BackProp, they all implement the similarity-based learning (SBL) bias, tending to classify unseen instances like nearby instances that were seen during training <ref> (Matheus and Rendell, 1989) </ref>. Nearest-neighbor learning algorithms implement the SBL bias directly (Aha et al., 1991). This bias is very general and is pervasive among commonly known learning algorithms. <p> ILP uses inverse resolution to augment a domain theory with a new predicate that covers some unexplained observations (Muggleton and Buntine, 1988). 2.3.2 Feature Construction In the context of feature-vector based domains, much of the research on constructive induction has focused on a specialized class of techniques called feature construction <ref> (Matheus and Rendell, 1989) </ref>. In feature construction, the initial representation of a domain is extended by computing a new feature as a function of the original feature values for each example, and then literally appending it as an extra component to each vector. <p> If newly constructed features are allowed to be candidates in subsequent constructions, then there can be an exponential number of extensions to an initial feature vector <ref> (Matheus, 1989) </ref>. Each of the automated methods for change-of-representation we discussed above has some specific way of dealing with this complexity. Typically, constraints on which changes-of 29 representation will be considered are implicitly built into the algorithm. <p> Domain knowledge can act in several ways to guide the search. First, knowledge can limit the space of alternatives by specifying a class of syntactic changes that are worth considering. This is most clearly illustrated in feature construction in the phase of operator selection 44 <ref> (Matheus, 1989) </ref>. By choosing a particular operator, such as PLUS or XOR, the space of possi-ble alternative representations is restricted to those which include only those new features that can be constructed from the original attributes using this operator. <p> Some change-of-representation techniques, such as FRINGE (Pagallo and Haussler, 1990), require the use of a specific kind of induction algorithm, which would probably result in an initial decrease in performance from the current established level. Furthermore, most known change-of-representation techniques are based on feature construction <ref> (Matheus, 1989) </ref>, which assumes that examples can be expressed as feature-vectors of uniform length; but proteins are fundamentally variable in length. <p> We discuss the generality and flexibility of this role for knowledge, and we conclude this section with a discussion of the limitations of our approach. 7.2.1 Comparison to Other Change-of-Representation Systems 7.2.1.1 Feature Construction The approach to change-of-representation in machine learning that is closest to our framework is feature construction <ref> (Matheus, 1989) </ref>. Feature construction refers to the generation of new variables by applying operators, often mathematical or logical, to attributes in the original feature vector. Values for constructed features are computed for each example and appended to its description. <p> Values for constructed features are computed for each example and appended to its description. Some example systems that construct features are FRINGE (Pagallo and Haussler, 1990), STAGGER (Schlimmer, 1987), CITRE <ref> (Matheus and Rendell, 1989) </ref>, and LFC (Ragavan and Rendell, 1993).
Reference: <author> McCammon, J. and Harvey, S. </author> <year> 1987. </year> <title> Dynamics of Proteins and Nucleic Acids. </title> <publisher> Cambridge University Press: </publisher> <address> New York. 155 McLachlan, A.D. </address> <year> 1971. </year> <title> Tests for comparing related amino-acid sequences: </title> <journal> Cytochrome c and cytochrome c551. Journal of Molecular Biology 61 </journal> <pages> 409-424. </pages>
Reference-contexts: The mapping between sequence and structure turns out to be very difficult to compute. For example, one approach, simulation of folding by energy minimization, is known to be intractable for computing structures de novo (from a random initial configuration) <ref> (McCammon and Harvey, 1987) </ref>. Rather than trying to compute this mapping from first principles (electrostatics, Newtonian mechanics, etc.), another approach is to try to learn the relationship between sequence and structure from examples in protein databases. <p> Once the amino acid sequence of a protein has been assembled, it usually takes between several milliseconds and several seconds to fold (Baldwin, 1989). Although the pathways of folding are not well-understood, it is postulated that proteins fold into structures of globally minimum energy <ref> (McCammon and Harvey, 1987) </ref>. White (1961) showed that, after denaturation, proteins can often spontaneously re-fold in the absence of all other cellular components, proving that the amino acid sequence alone determines the structure of a protein. <p> As an alternative, computational methods attempt to predict the structure of a protein from its amino acid sequence, rather than measure it. The most common prediction method is called molecular simulation <ref> (McCammon and Har-vey, 1987) </ref>. Molecular simulation is carried out by assigning three-dimensional coordinates to each atom in a protein molecule based on a random initial configuration.
Reference: <author> McLachlan, A.D. </author> <year> 1972. </year> <title> Gene duplication in carp muscle calcium-binding protein. </title> <booktitle> Nature New Biology 240 </booktitle> <pages> 83-85. </pages>
Reference-contexts: It was recognized early on that proteins with a high percentage of amino acids in common appeared strikingly similar by visual inspection (Perutz et al., 1965). Structural similarity can be quantified in terms of RMSD, the root mean square distance between corresponding atoms when two protein structures are superimposed <ref> (McLachlan, 1972) </ref>.
Reference: <author> Michalski, R.S. and Stepp, R.E. </author> <year> 1983. </year> <title> Learning from observation: Conceptual clustering. </title> <editor> In Michalski, R.S.; Carbonell, J.G.; and Mitchell, T.M., editors 1983, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Several automated methods for changing representation in machine learning have been proposed (Schlimmer, 1987; Matheus, 1989; Pagallo and Haussler, 1990; Ragavan and Rendell, 1993). These methods have often been called constructive induction <ref> (Michalski, 1983) </ref>, feature extraction (Devijver and Kittler, 1982), or dynamic bias (Utgoff, 1986), though related methods have been proposed in other fields such as statistics and pattern recognition. However, a ubiquitous problem that all of these techniques must deal with in some way is the complexity of change-of-representation. <p> In this section, we discuss several learning systems that actively manipulate the representation of a domain to increase the accuracy of an induction algorithm (which is held constant). 2.3.1 Constructive Induction In machine learning, automated methods for changing representation have generally been called it constructive induction <ref> (Michalski, 1983) </ref>. Constructive induction typically refers to the extension of a representation by a new syntactic component that is a function of the original description. This new component is computed for all examples simultaneously, and used to augment their descriptions in some way. <p> This new component is computed for all examples simultaneously, and used to augment their descriptions in some way. For example, the INDUCE program uses special transformation operators in annotated predicate calculus (APC) to generate new terms in its search for hypotheses to model a concept <ref> (Michalski, 1983) </ref>. Similarly, Dietterich and Michalski (1986) describe a program that transforms sequences of moves in a card game by manipulating objects in the sequence to summarize key aspects of the surrounding moves. A common constructive induction technique is to invent new terms, such as by clustering (Anderberg, 1973). <p> A common constructive induction technique is to invent new terms, such as by clustering (Anderberg, 1973). Clustering can improve a representation by giving a unique label to separate peaks in instance space (Rendell, 1985). Examples of induction systems that exploit clustering are CLUSTER <ref> (Michalski and Stepp, 1983) </ref> and COBWEB (Fisher, 1987). Another method 26 for inventing new terms is inductive-logic programming (ILP). <p> Donoho and Rendell (1996) discuss other roles that knowledge can play in feature construction. 7.2.1.2 Constructive Induction and Dynamic Bias Another approach to change-of-representation, constructive induction <ref> (Michalski, 1983) </ref>, is more general than feature construction because it is based on a logical formulation of learning. Examples are described using predicates, and background knowledge is encoded as statements in annotated predicate calculus (APC). <p> The semantic interpretation of attributes as subsets of objects with a given value makes them equivalent to logical concepts, which are relations over the objects in model theory <ref> (Michalski, 1983) </ref>. The primitives in symbolic descriptions, along with their syntactic relationships, reflect semantic relationships among domain objects via the denotation function.
Reference: <author> Michalski, R. </author> <year> 1983. </year> <title> A theory and methodology of inductive learning. </title> <booktitle> Artificial Intelligence 20 </booktitle> <pages> 111-161. </pages>
Reference-contexts: Several automated methods for changing representation in machine learning have been proposed (Schlimmer, 1987; Matheus, 1989; Pagallo and Haussler, 1990; Ragavan and Rendell, 1993). These methods have often been called constructive induction <ref> (Michalski, 1983) </ref>, feature extraction (Devijver and Kittler, 1982), or dynamic bias (Utgoff, 1986), though related methods have been proposed in other fields such as statistics and pattern recognition. However, a ubiquitous problem that all of these techniques must deal with in some way is the complexity of change-of-representation. <p> In this section, we discuss several learning systems that actively manipulate the representation of a domain to increase the accuracy of an induction algorithm (which is held constant). 2.3.1 Constructive Induction In machine learning, automated methods for changing representation have generally been called it constructive induction <ref> (Michalski, 1983) </ref>. Constructive induction typically refers to the extension of a representation by a new syntactic component that is a function of the original description. This new component is computed for all examples simultaneously, and used to augment their descriptions in some way. <p> This new component is computed for all examples simultaneously, and used to augment their descriptions in some way. For example, the INDUCE program uses special transformation operators in annotated predicate calculus (APC) to generate new terms in its search for hypotheses to model a concept <ref> (Michalski, 1983) </ref>. Similarly, Dietterich and Michalski (1986) describe a program that transforms sequences of moves in a card game by manipulating objects in the sequence to summarize key aspects of the surrounding moves. A common constructive induction technique is to invent new terms, such as by clustering (Anderberg, 1973). <p> A common constructive induction technique is to invent new terms, such as by clustering (Anderberg, 1973). Clustering can improve a representation by giving a unique label to separate peaks in instance space (Rendell, 1985). Examples of induction systems that exploit clustering are CLUSTER <ref> (Michalski and Stepp, 1983) </ref> and COBWEB (Fisher, 1987). Another method 26 for inventing new terms is inductive-logic programming (ILP). <p> Donoho and Rendell (1996) discuss other roles that knowledge can play in feature construction. 7.2.1.2 Constructive Induction and Dynamic Bias Another approach to change-of-representation, constructive induction <ref> (Michalski, 1983) </ref>, is more general than feature construction because it is based on a logical formulation of learning. Examples are described using predicates, and background knowledge is encoded as statements in annotated predicate calculus (APC). <p> The semantic interpretation of attributes as subsets of objects with a given value makes them equivalent to logical concepts, which are relations over the objects in model theory <ref> (Michalski, 1983) </ref>. The primitives in symbolic descriptions, along with their syntactic relationships, reflect semantic relationships among domain objects via the denotation function.
Reference: <author> Miller, S.; Janin, J.; Lesk, A.M.; and Chothia, C. </author> <year> 1987. </year> <title> Interior and surface of monomeric proteins. </title> <journal> Journal of Molecular Biology 196 </journal> <pages> 641-656. </pages>
Reference-contexts: Another useful criterion is based on the solvation preferences of the residues, which should be reflected in the partitioning of residues between the surface and core of the protein <ref> (Miller et al., 1987) </ref>.
Reference: <author> Minsky, M.L. and Papert, S. </author> <year> 1969. </year> <title> Perceptrons: An Introduction to Computational Geometry. </title> <publisher> MIT Press: </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Similarly, computing the square or logarithm of a numeric variable can facilitate an algorithm like the perceptron by extending discrimination beyond linearly-separable partitions <ref> (Minsky and Papert, 1969) </ref>. Saxena (1991) has presented a theoretical explanation 19 of this interaction between representational and inductive biases based on data compression. <p> Thus a good bias for one domain can actually interfere with learning in another domain. Each algorithm has certain criteria for applicability, such non-disjunctiveness for version spaces (Mitchell, 1982), or linear-separability for perceptrons <ref> (Minsky and Papert, 1969) </ref>, or smoothness of class functions for instance-based learners (Aha et al., 1991), or independence for Bayesian learners (Cheeseman et al., 1988).
Reference: <author> Minsky, M. </author> <year> 1963. </year> <title> Steps toward artificial intelligence. </title> <editor> In Feigenbaum, E.A. and Feldman, J., editors 1963, </editor> <booktitle> Computers and Thought. </booktitle> <address> McGraw Hill: New York. </address>
Reference-contexts: Our framework allows us to generate explanations (in terms of domain knowledge) for the expected ordering of the two representations, and, with the help of a credit-assignment algorithm <ref> (Minsky, 1963) </ref>, this feedback can be propagated back through the explanation to critique individual pieces of knowledge (i.e. rules).
Reference: <author> Mitchell, T. </author> <year> 1980. </year> <title> The need for biases in learning generalizations. </title> <type> Technical Report CBM-TR-117, </type> <institution> Rutgers University, Department of Computer Science. </institution>
Reference-contexts: More generally, knowledge can be considered an aspect of bias, which is anything used to select hypotheses during learning, other than the data itself <ref> (Mitchell, 1980) </ref>. Russell and Grosof (1987) show how inductive biases are equivalent to non-monotonic beliefs. Despite this variety of methods for incorporating knowledge into learning, there are three potential problems. <p> Inductive-logic programming (ILP) also fits in this category, since new terms are invented to cover examples with the help of a domain theory by the process of inverse resolution (Muggleton and Buntine, 1988). More generally, the role knowledge plays in learning can be explained in terms of bias <ref> (Mitchell, 1980) </ref>. Bias refers to anything other than the data itself that is used to select hypotheses during generalization. Biases are often implicitly built into induction algorithms. <p> This view is based on treating the external representation of examples as an aspect of bias, which is anything that affects generalization besides the examples themselves <ref> (Mitchell, 1980) </ref>. For example, STABB shifts the symbolic language for describing mathematical functions to prevent version-space collapse in LEX, a system for learning how to do integration (Utgoff, 1986).
Reference: <author> Mitchell, T. </author> <year> 1982. </year> <title> Generalization as search. </title> <booktitle> Artificial Intelligence 21 </booktitle> <pages> 203-226. </pages>
Reference-contexts: Search 4 ing representation space is most directly suggested by various proposals for layered learning (Rendell, 1985; Gordon and DesJardins, 1995). At one level, induction itself is conceived of as searching through a space of hypotheses <ref> (Mitchell, 1982) </ref>. Then, at a higher level, a search is conducted through the space of representations, each of which generates its own hypothesis space, to find a representation which permits a good hypothesis to be found. <p> Inductive algorithms often use the terms in a representation to construct explicit internal hypotheses. For example, the candidate elimination algorithm forms intensional descriptions of classes (concepts) based on collections of terms (attribute-value pairs) that are common to all positive examples but do not cover any negative examples <ref> (Mitchell, 1982) </ref>. Similarly, ID3 builds decision trees by recording tests on the value of a term that provide the greatest information gain when used to separate examples at each node (Quinlan, 1983). <p> Thus a good bias for one domain can actually interfere with learning in another domain. Each algorithm has certain criteria for applicability, such non-disjunctiveness for version spaces <ref> (Mitchell, 1982) </ref>, or linear-separability for perceptrons (Minsky and Papert, 1969), or smoothness of class functions for instance-based learners (Aha et al., 1991), or independence for Bayesian learners (Cheeseman et al., 1988). <p> The model is based on multiple layers of search, each of which gives a space of ways for searching at the level below. The bottom layer is a hypothesis space, as is typical for models of learning <ref> (Mitchell, 1982) </ref>, and the top level must ground out in some absolute search methodology dependent on background knowledge.
Reference: <author> Mooney, R.J. and Ourston, D. </author> <year> 1994. </year> <title> A multistrategy approach to theory refinement. </title> <editor> In Michalski, R.S. and Tecuci, G., editors 1994, </editor> <booktitle> Machine Learning: A Multistrategy Approach, </booktitle> <volume> volume 4. </volume> <publisher> Morgan Kaufmann: </publisher> <address> San Fransisco. </address>
Reference-contexts: There are several perspectives on theory revision, such as the incremental refinement of a theory that fails to explain a few examples (Bergadano and Giordana, 1990), or as an integration of EBL and SBL <ref> (Mooney and Ourston, 1994) </ref>. However, given that the inputs to theory revision include both examples and knowledge, it can also be thought of as method for incorporating knowledge into inductive learning. In this view, the domain theory provides a very strong bias on the hypotheses that can be generated. <p> Any new roles for knowledge should be general, 34 in the sense that they can work with a wide variety of induction algorithms. Some of the known roles for knowledge only work with specific algorithms. For example, KBANN only works with neural nets <ref> (Mooney and Ourston, 1994) </ref>. However, we want to allow the domain expert to use whatever algorithm he or she believes has an appropriate bias for the domain. This could be especially important in real-world domains where a domain-specific method is already available that out-performs all other prediction schemes.
Reference: <author> Muggleton, S. and Buntine, W. </author> <year> 1988. </year> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning. </booktitle> <pages> 218-229. </pages>
Reference-contexts: ILP uses knowledge to generate new terms for covering examples by the process of inverse resolution <ref> (Muggleton and Buntine, 1988) </ref>. KBANN uses knowledge to define the topology of a neural network (Towell et al., 1990). MIRO uses knowledge to filter the features generated by a feature construction algorithm (Drastal et al., 1989). <p> Examples of induction systems that exploit clustering are CLUSTER (Michalski and Stepp, 1983) and COBWEB (Fisher, 1987). Another method 26 for inventing new terms is inductive-logic programming (ILP). ILP uses inverse resolution to augment a domain theory with a new predicate that covers some unexplained observations <ref> (Muggleton and Buntine, 1988) </ref>. 2.3.2 Feature Construction In the context of feature-vector based domains, much of the research on constructive induction has focused on a specialized class of techniques called feature construction (Matheus and Rendell, 1989). <p> This constraint allows nearly correct generalizations to be derived from a small number of examples. Inductive-logic programming (ILP) also fits in this category, since new terms are invented to cover examples with the help of a domain theory by the process of inverse resolution <ref> (Muggleton and Buntine, 1988) </ref>. More generally, the role knowledge plays in learning can be explained in terms of bias (Mitchell, 1980). Bias refers to anything other than the data itself that is used to select hypotheses during generalization. Biases are often implicitly built into induction algorithms.
Reference: <author> Myers, E.W. and Miller, W. </author> <year> 1988. </year> <title> Optimal alignments in linear space. </title> <booktitle> CABIOS 4 </booktitle> <pages> 11-17. </pages>
Reference: <author> Needleman, S. and Wunsch, C. </author> <year> 1970. </year> <title> A general method applicable to the search for similarities in the amino acid sequence of two proteins. </title> <journal> Journal of Molecular Biology 48 </journal> <pages> 443-453. </pages>
Reference-contexts: Homology modeling refers to the use of a sequence alignment algorithm (which inserts gaps to maximize local matches of amino acids) to detect similarities among proteins <ref> (Needleman and Wunsch, 1970) </ref>. Homology modeling is based on the observation that, whenever two proteins have similar sequences (e.g. &gt; 30% identity), they always have similar structures (Sander and Schneider, 1991). <p> However, because there was some uncertainty in this knowledge, it is possible that some variant partition 1 Most alignment algorithms require quadratic space (i.e. proportional to the product of the lengths of the sequences being aligned) to store the matrices used for dynamic programming <ref> (Needleman and Wunsch, 1970) </ref>. The Myers and Miller (1988) algorithm simulates the same computation with several parallel arrays. 2 A grid of parameters was set up for a reasonable range of values for each of the two gap-weight parameter.
Reference: <author> Nei, M. </author> <year> 1987. </year> <title> Molecular Evolutionary Genetics. </title> <publisher> Columbia University Press: </publisher> <address> New York. </address>
Reference-contexts: Averaged over the length of the two sequences, the percent amino acid identity is a simple but standard measure of sequence similarity. This measure is relevant to protein structure prediction because it reflects the processes of molecular evolution <ref> (Nei, 1987) </ref>. When a gene evolves along two separate paths (for example, in two different species diverging from a common ancestor), the amino acid sequences undergo mutations that differentiate them.
Reference: <author> Neidhart, D.J.; Kenyon, G.L.; Gerlt, J.A.; and Petsko, G.A. </author> <year> 1990. </year> <title> Mandelate racemase and muconate lactonizing enzyme are mechanistically distinct and structurally homologous. </title> <booktitle> Nature 347 </booktitle> <pages> 692-694. </pages>
Reference-contexts: Yet when their structures were solved, they were both found to be members of the beta-barrel fold class, and their structures differed by an RMSD of only 1.3 angstroms <ref> (Neidhart et al., 1990) </ref>. In fact, there are at least 14 mutually distinct beta-barrels known, including the prototypical triose-phosphate isomerase (TIM-barrel), as well as more diverse proteins such as taka alpha amylase and tryptophan synthase (Chothia, 1988). <p> The existence of proteins with similar structures but dissimilar amino acid sequences (e.g. mandelate racemase and muconate lactonizing enzyme <ref> (Neidhart et al., 1990) </ref>), along with the belief that the property sequences should be the same, leads to the inference that sequences of properties should be better for homology modeling than sequences of amino acid identities.
Reference: <author> Nell, L.J.; McCammon, J.A.; and Subramaniam, S. </author> <year> 1992. </year> <title> Anti-insulin antibody. structure and conformation i. molecular modeling and mechanics. </title> <type> Biopolymers 32 </type> <pages> 11-21. </pages>
Reference-contexts: After the fold class of a protein has been identified, its sequence may be aligned to another member of the same class to derive an approximate structure, and the predicted atomic coordinates may subsequently be refined by molecular simulation <ref> (Nell et al., 1992) </ref>. One potential criticism of the homology modeling approach to protein structure prediction is that it cannot be used to predict the structure of a truly novel protein.
Reference: <author> Newell, A. and Simon, H.A. </author> <year> 1972. </year> <title> Human Problem Solving. </title> <publisher> Prentice Hall: </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: This third component illustrates the significance of our search-based framework: it provides a systematic role for knowledge to play in the form of a search heuristic, which is a classical notion in AI <ref> (Newell and Simon, 1972) </ref>. Change-of-representation is difficult, and difficult problems are often described as having large search spaces. Knowledge can be used to help solve difficult problems by guiding the search toward nodes that meet the goal criterion more efficiently than random exploration (enumeration). <p> This reduces the complexity of change-of-representation by steering the search in a potentially huge space of alternatives toward regions that are more likely to contain improved representations. Thus knowledge is plays the classic role of a search heuristic <ref> (Newell and Simon, 1972) </ref>. We first give some examples of how knowledge can guide the search for alternative representations, and then we present a semantics that formalizes this role for knowledge. Domain knowledge can act in several ways to guide the search. <p> More generally, knowledge can be seen to act as a heuristic This role for knowledge takes advantage of a traditional concept in AI. Hard problems are often described in terms of large search spaces <ref> (Newell and Simon, 1972) </ref>. This applies especially to machine learning where the space of alternative representations is often huge. As in many other problems, knowledge can help by steering the search more efficiently toward solutions. <p> The search basis of our framework makes clear the tradeoff between the strength of the knowledge and the efficiency of change-of-representation <ref> (Newell and Simon, 1972) </ref>. 7.2.3 Limitations of the Approach Our framework cannot be used to improve induction in arbitrary domains. In this section we discuss some criteria, which, if not met in a given domain, could prevent successful application of the approach.
Reference: <author> Newell, A. </author> <year> 1969. </year> <title> Heuristic programming iii: Structured problems. </title> <booktitle> In Aronofsky, J.S., editor 1969, Progress in Operations Research, </booktitle> <volume> volume 3. </volume> <publisher> Wiley: </publisher> <address> New York. </address>
Reference: <author> Norton, S.W. </author> <year> 1989. </year> <title> Generating better decision trees. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence. </booktitle> <pages> 800-805. </pages>
Reference-contexts: Various attempts have been made to address the greediness of typical SBL algorithms, such as by using lookahead to search among more than one feature at a time for correlations with the target concept <ref> (Norton, 1989) </ref>. But ultimately, the number of combinations of features is exponential, so it is intractable to capture arbitrary feature interactions. LFC uses beam 25 search as a heuristic to explore this large space of potential features (Ragavan and Rendell, 1993).
Reference: <author> Overington, J.; Donnelly, D.; Johnson, J.S.; Sali, A.; and Blundell, T. </author> <year> 1992. </year> <title> Environment-specific amino acid substitution tables: Tertiary templates and prediction of protein folds. </title> <booktitle> Protein Science 1 </booktitle> <pages> 216-226. </pages>
Reference-contexts: Finally, if the expert suspects that the utility of some properties might be context-dependent, where a context is defined as a subset of sites within proteins, then search can be used to determine in which context a given property is most relevant <ref> (Overington et al., 1992) </ref>. <p> The similarity between any two amino acids at a given site depends on the structural role being played, and different physical and chemical properties are emphasized in different contexts <ref> (Overington et al., 1992) </ref>. Finally, the statistics in a substitution matrix reflect an overall bias toward evolutionarily-related proteins, since these are the ones that are typically used to construct multiple alignments. <p> The relevant property at a given site is probably context-dependent, since substitution patterns are known to be highly influenced by the surrounding tertiary environment <ref> (Overington et al., 1992) </ref>. <p> However, it is believed that the property that is most relevant at a given site depends on context, such as the tertiary environment <ref> (Overington et al., 1992) </ref>. This observation suggests that we could maintain different partitions for different contexts. In this experiment, we explore a sub-space of alternative representations in which amino acids are mapped to property names, but this is done differently in different contexts.
Reference: <institution> Packard, N.H. </institution> <year> 1989. </year> <title> Genetic learning algorithm for the analysis of complex data. </title> <type> Technical Report CCSR-89-10, </type> <institution> University of Illinois, Center of Complex Systems. </institution>
Reference-contexts: Examples of such domains include weather prediction <ref> (Packard, 1989) </ref>, financial risk analysis (Ragavan et al., 1993), and drug design (King et al., 1995). In real-world domains, huge databases may be available, but simply applying one of the standard induction algorithms often produces only low predictive accuracy.
Reference: <author> Pagallo, G. and Haussler, D. </author> <year> 1990. </year> <title> Feature discovery in empirical learning. </title> <booktitle> Machine Learning 5 </booktitle> <pages> 71-99. </pages>
Reference-contexts: Previous methods for automated change-of-representation all have some way of dealing with complexity, 6 but this is usually implicitly built into the algorithm. For example, FRINGE exploits the replication problem in decision trees to decide which features to combine <ref> (Pagallo and Haussler, 1990) </ref>. Our framework makes these decisions about change-of-representation more explicit and knowledge-intensive. This role for knowledge in learning is a fairly flexible. <p> Furthermore, if constructed features may be used in other constructions, then the number of possibilities is exponential in the number of original attributes. So the challenging aspect of feature construction is deciding which features to combine. FRINGE exploited the replication problem in decision trees to select features of construction <ref> (Pagallo and Haussler, 1990) </ref>. This involves looking for patterns of decisions that are frequently repeated at the leaves of a decision tree, which indicates that the composite variable might make a good choice higher up in the tree. <p> Typically, constraints on which changes-of 29 representation will be considered are implicitly built into the algorithm. For example, FRINGE uses the replication problem in decision trees to help decide which features should be combined <ref> (Pagallo and Haussler, 1990) </ref>. And stepwise multiple regression uses a greedy method for testing and selecting variables to add or drop one at a time, based on their effect on the variance of the model (Draper and Smith, 1981). <p> Some change-of-representation techniques, such as FRINGE <ref> (Pagallo and Haussler, 1990) </ref>, require the use of a specific kind of induction algorithm, which would probably result in an initial decrease in performance from the current established level. <p> Feature construction refers to the generation of new variables by applying operators, often mathematical or logical, to attributes in the original feature vector. Values for constructed features are computed for each example and appended to its description. Some example systems that construct features are FRINGE <ref> (Pagallo and Haussler, 1990) </ref>, STAGGER (Schlimmer, 1987), CITRE (Matheus and Rendell, 1989), and LFC (Ragavan and Rendell, 1993). <p> Feature construction systems typically use a specialized method to decide which features to construct, corresponding to the strategy component in our framework. For example, FRINGE looks for patterns of variables that recur at the leaves of a decision tree <ref> (Pagallo and Haussler, 1990) </ref>. BACON looks for mathematical combinations of numeric variables that are nearly constant (Langley et al., 1983).
Reference: <author> Pascarella, S. and Argos, P. </author> <year> 1992. </year> <title> A data bank merging related protein structures and sequences. </title> <booktitle> Protein Engineering 5 </booktitle> <pages> 121-137. </pages>
Reference-contexts: The sequences ranged in length from 39 (4gcr) to 478 (2taa). These proteins were selected for these experiments because they had been grouped into specific fold classes by visual inspection <ref> (Pascarella and Argos, 1992) </ref>. The 199 sequences fall into 37 different folds, with between 2 and 29 members per fold.
Reference: <author> Pazzani, M.J. </author> <year> 1988. </year> <title> Integrated learning with incorrect and incomplete theories. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning. </booktitle> <pages> 291-297. </pages>
Reference-contexts: The point of this approach is to simulate a teacher who designs a particular sequence of examples to lead the learner to realize some concept. Theory revision may also be looked at as a way of incorporating domain knowledge into learning <ref> (Pazzani, 1988) </ref>. There are several perspectives on theory revision, such as the incremental refinement of a theory that fails to explain a few examples (Bergadano and Giordana, 1990), or as an integration of EBL and SBL (Mooney and Ourston, 1994).
Reference: <author> Pearl, J. </author> <year> 1984. </year> <title> Heuristics: Intelligent Search Strategies for Computer Problem Solving. </title> <publisher> Addison-Wesley: </publisher> <address> Reading, MA. </address>
Reference-contexts: There are many general strategies for searching, but these are "weak methods" since they do not take advantage of any special properties of a given domain <ref> (Pearl, 1984) </ref>. We propose using domain knowledge explicitly to decide what node should be expanded next at each stage in the search. This reduces the complexity of change-of-representation by steering the search in a potentially huge space of alternatives toward regions that are more likely to contain improved representations.
Reference: <author> Perutz, M.F.; Kendrew, J.C.; and Watson, J.D. </author> <year> 1965. </year> <title> Structure and function of haemoglobin ii: Some relations between polypeptide congifuration and amino acid sequence. </title> <journal> Journal of Molecular Biology 13:669. </journal>
Reference-contexts: Sequence similarity is typically measured by the percent of amino acids 78 that are identical between two sequences. It was recognized early on that proteins with a high percentage of amino acids in common appeared strikingly similar by visual inspection <ref> (Perutz et al., 1965) </ref>. Structural similarity can be quantified in terms of RMSD, the root mean square distance between corresponding atoms when two protein structures are superimposed (McLachlan, 1972).
Reference: <author> Ponder, J.W. and Richards, F.M. </author> <year> 1987. </year> <title> Tertiary templates for proteins: Use of packing criteria in the enumeration of allowed sequences for different structural classes. </title> <journal> Journal of Molecular Biology 193 </journal> <pages> 775-791. </pages>
Reference-contexts: However, internally, each protein has a unique structure determined by the path of the backbone. The trace of backbone atoms contributed by each amino acid forms a super-structure around which the side-chains pack, filling up the volume and generally leaving no cavities <ref> (Ponder and Richards, 1987) </ref>. Certain sub-structures within proteins have been observed as recurrent patterns in the meander of the backbone (Levitt and Chothia, 1976). There are two main classes of substructures, also called secondary structures: alpha-helices and beta-sheets. <p> Once threading is completed, the fit of the sequence can be evaluated by a variety of methods that can take global properties into account. For example, Ponder and Richards <ref> (Ponder and Richards, 1987) </ref> used packing density as a criterion, since proteins pack uniformly very tightly, without any cavities (or steric overlaps).
Reference: <author> Porter, B.W.; Bareiss, R.; and Holte, R.C. </author> <year> 1990. </year> <title> Concept learning and heuristic classification in weak-theory domains. </title> <booktitle> Artificial Intelligence 45 </booktitle> <pages> 229-263. </pages>
Reference-contexts: Several methods are known for incorporating domain knowledge into learning. PROTOS uses knowledge to help decide the best way to weight attributes for indexing cases <ref> (Porter et al., 1990) </ref>. Explanation-based learning uses knowledge to explain the classification of examples, thereby indicating which features are relevant to the classification and which may be dropped from the description to generalize it (DeJong and Mooney, 1986). <p> Several explicit roles for incorporating knowledge more declaratively in learning are known. Some roles for knowledge consist of enhancements to standard induction algorithms, such as KBANN (Towell et al., 1990) or PROTOS <ref> (Porter et al., 1990) </ref>. Others roles introduce novel learning algorithms to accommodate the knowledge, such as explanation-based learning (DeJong and Mooney, 1986) or analogy (Rajamoney, 1990). <p> For example, PROTOS improves the ability of a case-based learner to index cases by letting the knowledge suggest attributes to 32 use as remindings for cross-linking the cases <ref> (Porter et al., 1990) </ref>. KBANN can improve the performance of training a neural network by pre-setting the weights on connections so that it has a topology that reflects the structure of the domain theory (Towell et al., 1990).
Reference: <author> Qian, N. and Sejnowski, T.J. </author> <year> 1988. </year> <title> Predicting the secondary structure of globular proteins using neural network models. </title> <journal> Journal of Molecular Biology 202 </journal> <pages> 865-884. </pages>
Reference-contexts: One of the earliest approaches combined simple statistics for residue preferences with rules for nucleating, propagating, and terminating contiguous stretches of residues in a particular conformation (Chou and Fasman, 1974). Since then, the statistics have been replaced by elaborate neural networks <ref> (Qian and Sejnowski, 1988) </ref>, and the rules have grown into large knowledge bases of the roles specific amino acids can play in determining secondary structure (Lim, 1974).
Reference: <author> Quinlan, J.R. </author> <year> 1983. </year> <title> Learning efficient classification procedure and their application to chess end-games. </title> <editor> In Michalski, R.S.; Carbonell, J.G.; and Mitchell, T.M., editors 1983, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Similarly, ID3 builds decision trees by recording tests on the value of a term that provide the greatest information gain when used to separate examples at each node <ref> (Quinlan, 1983) </ref>. Our notion of representation is distinct from the internal hypothesis language, referring only to the language in which examples are presented to the learner. <p> Such helpful constructions have been called "high-level features" because they are more directly related to the target concept, making patterns easier to detect. Another perspective is that constructed features can improve learning by acting as intermediate concepts, such as center control and knight fork in chess <ref> (Quinlan, 1983) </ref>. The type of feature construction operators available depends on the data types of the combined features. For Boolean features, logical operators like AND and OR can be used. For real-valued features, mathematical operators like MINUS and TIMES are commonly used. <p> In a similar manner, domain knowledge is often used to select the parameters for the induction algorithm. Many algorithms have parameters that must be specified, such as the stopping criterion of ID3 <ref> (Quinlan, 1983) </ref>, the learning rate of BackProp (Rumelhart et al., 1986), or the number of neighbors maintained for each cluster in an instance-based learner (Aha et al., 1991).
Reference: <author> Quinlan, J.R. </author> <year> 1986. </year> <title> The effect of noise on concept learning. </title> <editor> In Michalski, R.S.; Carbonell, J.G.; and Mitchell, T.M., editors 1986, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach II. </booktitle> <publisher> Morgan Kaufmann: </publisher> <address> Los Altos, CA. </address>
Reference-contexts: In real-world domains, huge databases may be available, but simply applying one of the standard induction algorithms often produces only low predictive accuracy. Several potential causes of learning difficulty in real-world domains have been studied. The attributes or class labels may be corrupted by noise <ref> (Quinlan, 1986) </ref>. Critical information may be not be encoded in the data (intrinsic inaccuracy; Rendell and Seshu, 1990). And lack of data may also inhibit learning, which is a side-effect of concept-class complexity (Valiant, 1984). <p> Many of these troublesome representations can be labeled as "low-level." There may be other causes of difficulty for learning in real-world domains, such as noise <ref> (Quinlan, 1986) </ref> or intrinsic inaccuracy (Rendell and Seshu, 1990), but we assume these are not a problem in the scope of this thesis. <p> One premise of the framework is that the domain is initially difficult for learning, so that current induction algorithms get only low accuracy. Furthermore, this difficulty should be due to a low-level representation, rather than other causes such as noise <ref> (Quinlan, 1986) </ref> or intrinsic inaccuracy (Rendell and Seshu, 1990). Another premise is that domain knowledge is available which can be used to help change representations. Our framework is semi-automated, requiring knowledge. It will not work in artificial domains lacking significant background knowledge.
Reference: <author> Quinlan, J.R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann: </publisher> <address> Palo Alto, CA. </address>
Reference: <author> Rabiner, </author> <title> L.R. 1989. A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE 77(2) </booktitle> <pages> 257-286. </pages> <note> 157 Ragavan, </note> <author> H. and Rendell, L. </author> <year> 1991. </year> <title> Relations, knowledge, and empirical learning. </title> <booktitle> In Eighth International Workshop on Machine Learning. </booktitle> <pages> 188-192. </pages>
Reference: <author> Ragavan, H. and Rendell, L. </author> <year> 1993. </year> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning. </booktitle> <pages> 252-259. </pages>
Reference-contexts: Examples of such domains include weather prediction (Packard, 1989), financial risk analysis <ref> (Ragavan et al., 1993) </ref>, and drug design (King et al., 1995). In real-world domains, huge databases may be available, but simply applying one of the standard induction algorithms often produces only low predictive accuracy. Several potential causes of learning difficulty in real-world domains have been studied. <p> Low-level representations may cause phenomena such as concept dispersion or feature interaction, which interfere respectively with the similarity bias and the greedy bias (Rendell and Seshu, 1990). Because most algorithms use these fundamental biases in some form, they may all perform uniformly poorly in a real-world domain <ref> (Ragavan et al., 1993) </ref>. Thus it may be necessary to change the representation itself to improve learning for such problems. The task of finding higher-level representations seems easier, or at least more automatable, than inventing a novel induction algorithm with a radically different bias. <p> But ultimately, the number of combinations of features is exponential, so it is intractable to capture arbitrary feature interactions. LFC uses beam 25 search as a heuristic to explore this large space of potential features <ref> (Ragavan and Rendell, 1993) </ref>. Still, feature interaction remains a hallmark of difficult, real-world domains for machine learning. 2.3 Change of Representation Although representation can be blamed for poor learning performance in some domains, representation can also be part of the solution to improving performance. <p> This causes the instances to shift in the space so that there are fewer, larger peaks. entropy <ref> (Ragavan and Rendell, 1993) </ref>. BACON decides which features to construct by looking for products and ratios of numeric variables that are nearly constant (Langley et al., 1983). 2.3.3 Dynamic Bias Another way of looking at change-of-representation in machine learning is as a form of dynamic bias (Utgoff, 1986). <p> Values for constructed features are computed for each example and appended to its description. Some example systems that construct features are FRINGE (Pagallo and Haussler, 1990), STAGGER (Schlimmer, 1987), CITRE (Matheus and Rendell, 1989), and LFC <ref> (Ragavan and Rendell, 1993) </ref>. <p> Many feature construction systems use a feature evaluation method to compare features discovered during search. For example, STAGGER evaluates the utility of features by computing a function of the number of positive and negative examples covered by each construction. Other approaches to feature evaluation include blurring <ref> (Rendell and Ragavan, 1993) </ref> and contextual merit (Hong, 1994). In comparison to our framework, evaluating independent features may be more efficient, since bad ones can be ruled out once, rather than tested with many different representations. <p> BACON looks for mathematical combinations of numeric variables that are nearly constant (Langley et al., 1983). LFC uses a geometric constraint to rule out combinations unlikely to decrease entropy, and then uses lookahead to dynamically explore a limited amount 135 of feature space to decide which features to construct <ref> (Ragavan and Rendell, 1993) </ref>. However, these approaches tend to de-emphasize the role of knowledge. Our framework is intentionally designed to replace complex techniques for selecting operators, operands, etc. with domain knowledge, making feature construction more knowledge-intensive.
Reference: <author> Ragavan, H.; Rendell, L.; Shaw, M.; and Tessmer, A. </author> <year> 1993. </year> <title> Complex concept acquisition through directed search and feature caching. </title> <booktitle> In Proceeding of the Thirteenth International Joint Conference on Artificial Intelligence. </booktitle> <pages> 946-951. </pages>
Reference-contexts: Examples of such domains include weather prediction (Packard, 1989), financial risk analysis <ref> (Ragavan et al., 1993) </ref>, and drug design (King et al., 1995). In real-world domains, huge databases may be available, but simply applying one of the standard induction algorithms often produces only low predictive accuracy. Several potential causes of learning difficulty in real-world domains have been studied. <p> Low-level representations may cause phenomena such as concept dispersion or feature interaction, which interfere respectively with the similarity bias and the greedy bias (Rendell and Seshu, 1990). Because most algorithms use these fundamental biases in some form, they may all perform uniformly poorly in a real-world domain <ref> (Ragavan et al., 1993) </ref>. Thus it may be necessary to change the representation itself to improve learning for such problems. The task of finding higher-level representations seems easier, or at least more automatable, than inventing a novel induction algorithm with a radically different bias. <p> But ultimately, the number of combinations of features is exponential, so it is intractable to capture arbitrary feature interactions. LFC uses beam 25 search as a heuristic to explore this large space of potential features <ref> (Ragavan and Rendell, 1993) </ref>. Still, feature interaction remains a hallmark of difficult, real-world domains for machine learning. 2.3 Change of Representation Although representation can be blamed for poor learning performance in some domains, representation can also be part of the solution to improving performance. <p> This causes the instances to shift in the space so that there are fewer, larger peaks. entropy <ref> (Ragavan and Rendell, 1993) </ref>. BACON decides which features to construct by looking for products and ratios of numeric variables that are nearly constant (Langley et al., 1983). 2.3.3 Dynamic Bias Another way of looking at change-of-representation in machine learning is as a form of dynamic bias (Utgoff, 1986). <p> Values for constructed features are computed for each example and appended to its description. Some example systems that construct features are FRINGE (Pagallo and Haussler, 1990), STAGGER (Schlimmer, 1987), CITRE (Matheus and Rendell, 1989), and LFC <ref> (Ragavan and Rendell, 1993) </ref>. <p> Many feature construction systems use a feature evaluation method to compare features discovered during search. For example, STAGGER evaluates the utility of features by computing a function of the number of positive and negative examples covered by each construction. Other approaches to feature evaluation include blurring <ref> (Rendell and Ragavan, 1993) </ref> and contextual merit (Hong, 1994). In comparison to our framework, evaluating independent features may be more efficient, since bad ones can be ruled out once, rather than tested with many different representations. <p> BACON looks for mathematical combinations of numeric variables that are nearly constant (Langley et al., 1983). LFC uses a geometric constraint to rule out combinations unlikely to decrease entropy, and then uses lookahead to dynamically explore a limited amount 135 of feature space to decide which features to construct <ref> (Ragavan and Rendell, 1993) </ref>. However, these approaches tend to de-emphasize the role of knowledge. Our framework is intentionally designed to replace complex techniques for selecting operators, operands, etc. with domain knowledge, making feature construction more knowledge-intensive.
Reference: <author> Rajamoney, S. </author> <year> 1990. </year> <title> A computational approach to theory revision. </title> <editor> In Shrager, J. and Lan-gley, P., editors 1990, </editor> <title> Computational Models of Scientific Discovery and Theory Formation. </title> <publisher> Morgan Kaufmann: </publisher> <address> San Mateo, CA. </address>
Reference-contexts: One might know spatial relationships among the attributes measured, such as proximity of positions on a chess board (Ragavan and Rendell, 1991). One might have a plausible model for a causal mechanism that underlies the phenomenon <ref> (Rajamoney, 1990) </ref>. Intermediate concepts or knowledge of related concepts might be available (Fu and Buchanan, 1985). Knowledge about the data itself, such sample bias, precision of the instruments, or 30 critical cases might be known (Gaines, 1989). <p> Some roles for knowledge consist of enhancements to standard induction algorithms, such as KBANN (Towell et al., 1990) or PROTOS (Porter et al., 1990). Others roles introduce novel learning algorithms to accommodate the knowledge, such as explanation-based learning (DeJong and Mooney, 1986) or analogy <ref> (Rajamoney, 1990) </ref>. Still others roles for knowledge are more general, and can be used with a wide range of algorithms, such as Abu-Mostafa's (1994) method of providing "hints." In the most general sense, knowledge plays a role as a bias, and we discuss some research on this perspective.
Reference: <author> Ramachandran, G.N. and Sasisekharan, V. </author> <year> 1968. </year> <title> Conformation of polypeptides and proteins. </title> <booktitle> Advances in Protein Chemistry 23 </booktitle> <pages> 283-437. </pages>
Reference-contexts: The peptide bond allows two degrees of rotational freedom, called the phi and psi angles. In a survey of thousands of amino acids in many different proteins, it has been observed that these backbone angles are generally found in two main configurations, primarily due to steric constraints <ref> (Ramachandran and Sasisekharan, 1968) </ref>. Repetitions of (60; 60) form an alpha-helix, and repetitions of (90; +120) form a beta-strand.
Reference: <author> Rendell, L.A. and Cho, H.H. </author> <year> 1990. </year> <title> Empirical concept learning as a function of concept character. </title> <booktitle> Maching Learning 5 </booktitle> <pages> 267-298. </pages>
Reference-contexts: Low-level representations inhibit learning because they interfere with several biases that are pervasive among known induction algorithms. Low-level representations may cause phenomena such as concept dispersion or feature interaction, which interfere respectively with the similarity bias and the greedy bias <ref> (Rendell and Seshu, 1990) </ref>. Because most algorithms use these fundamental biases in some form, they may all perform uniformly poorly in a real-world domain (Ragavan et al., 1993). Thus it may be necessary to change the representation itself to improve learning for such problems. <p> Many of these troublesome representations can be labeled as "low-level." There may be other causes of difficulty for learning in real-world domains, such as noise (Quinlan, 1986) or intrinsic inaccuracy <ref> (Rendell and Seshu, 1990) </ref>, but we assume these are not a problem in the scope of this thesis. <p> An analysis of dispersed concepts in feature-vector-based domains suggests that feature interaction is a primary cause of low-level representations in this common class of domains. 2.2.1 Concept Dispersion One indicator of low-level representations is concept dispersion <ref> (Rendell and Seshu, 1990) </ref>. Dispersion refers to the distribution of instances of a concept in the instance space. Although dispersion applies to concepts in any domain with a metric, one way of visualizing dispersion is to imagine the concept as a probability surface over a two-dimensional plane (see Figure 2.1). <p> In general, concepts that have many odd-shaped peaks scattered about in instance space are difficult to learn. The number of peaks, or peak multiplicity, is the primary factor in concept dispersion. A large number of peaks can make learning difficult for many algorithms <ref> (Rendell and Cho, 21 1990) </ref>. The reason a wide variety of learning algorithms are affected in a similar way is that the all share a common underlying bias. <p> Instead the concept must be learned as a collection of smaller peaks, again causing difficulty via peak multiplicity. 2.2.2 Feature Interaction In domains represented by feature-vectors, concept dispersion is often a symptom of an underlying phenomenon called feature interaction <ref> (Rendell and Seshu, 1990) </ref>. Feature interaction occurs when multiple parts of a representation must be considered together to observe the pattern in a concept. Specifically, the relationship between one feature and the target concept is affected by one or more other features. <p> New features can potentially improve learning performance in difficult domains by enhancing the metric in instance space. The new syntactic component might expose new similarities among the instances, effectively merging peaks (see Figure 2.5), and thus facilitating the broad class of SBL algorithms <ref> (Rendell and Seshu, 1990) </ref>. Such helpful constructions have been called "high-level features" because they are more directly related to the target concept, making patterns easier to detect. <p> One premise of the framework is that the domain is initially difficult for learning, so that current induction algorithms get only low accuracy. Furthermore, this difficulty should be due to a low-level representation, rather than other causes such as noise (Quinlan, 1986) or intrinsic inaccuracy <ref> (Rendell and Seshu, 1990) </ref>. Another premise is that domain knowledge is available which can be used to help change representations. Our framework is semi-automated, requiring knowledge. It will not work in artificial domains lacking significant background knowledge. However, low-level representations and background knowledge are common characteristics of real-world domains.
Reference: <author> Rendell, L. and Ragavan, H. </author> <year> 1993. </year> <title> Improving the design of induction methods by analyzing algorithm functionality and data-based complexity. </title> <booktitle> In Proceeding of the Thirteenth International Joint Conference on Artificial Intelligence. </booktitle> <pages> 952-958. </pages>
Reference-contexts: But ultimately, the number of combinations of features is exponential, so it is intractable to capture arbitrary feature interactions. LFC uses beam 25 search as a heuristic to explore this large space of potential features <ref> (Ragavan and Rendell, 1993) </ref>. Still, feature interaction remains a hallmark of difficult, real-world domains for machine learning. 2.3 Change of Representation Although representation can be blamed for poor learning performance in some domains, representation can also be part of the solution to improving performance. <p> This causes the instances to shift in the space so that there are fewer, larger peaks. entropy <ref> (Ragavan and Rendell, 1993) </ref>. BACON decides which features to construct by looking for products and ratios of numeric variables that are nearly constant (Langley et al., 1983). 2.3.3 Dynamic Bias Another way of looking at change-of-representation in machine learning is as a form of dynamic bias (Utgoff, 1986). <p> Values for constructed features are computed for each example and appended to its description. Some example systems that construct features are FRINGE (Pagallo and Haussler, 1990), STAGGER (Schlimmer, 1987), CITRE (Matheus and Rendell, 1989), and LFC <ref> (Ragavan and Rendell, 1993) </ref>. <p> Many feature construction systems use a feature evaluation method to compare features discovered during search. For example, STAGGER evaluates the utility of features by computing a function of the number of positive and negative examples covered by each construction. Other approaches to feature evaluation include blurring <ref> (Rendell and Ragavan, 1993) </ref> and contextual merit (Hong, 1994). In comparison to our framework, evaluating independent features may be more efficient, since bad ones can be ruled out once, rather than tested with many different representations. <p> BACON looks for mathematical combinations of numeric variables that are nearly constant (Langley et al., 1983). LFC uses a geometric constraint to rule out combinations unlikely to decrease entropy, and then uses lookahead to dynamically explore a limited amount 135 of feature space to decide which features to construct <ref> (Ragavan and Rendell, 1993) </ref>. However, these approaches tend to de-emphasize the role of knowledge. Our framework is intentionally designed to replace complex techniques for selecting operators, operands, etc. with domain knowledge, making feature construction more knowledge-intensive.
Reference: <author> Rendell, L. and Seshu, R. </author> <year> 1990. </year> <title> Learning hard concepts through constructive induction: Framework and rationale. </title> <booktitle> Computational Intelligence 6 </booktitle> <pages> 247-270. </pages>
Reference-contexts: Low-level representations inhibit learning because they interfere with several biases that are pervasive among known induction algorithms. Low-level representations may cause phenomena such as concept dispersion or feature interaction, which interfere respectively with the similarity bias and the greedy bias <ref> (Rendell and Seshu, 1990) </ref>. Because most algorithms use these fundamental biases in some form, they may all perform uniformly poorly in a real-world domain (Ragavan et al., 1993). Thus it may be necessary to change the representation itself to improve learning for such problems. <p> Many of these troublesome representations can be labeled as "low-level." There may be other causes of difficulty for learning in real-world domains, such as noise (Quinlan, 1986) or intrinsic inaccuracy <ref> (Rendell and Seshu, 1990) </ref>, but we assume these are not a problem in the scope of this thesis. <p> An analysis of dispersed concepts in feature-vector-based domains suggests that feature interaction is a primary cause of low-level representations in this common class of domains. 2.2.1 Concept Dispersion One indicator of low-level representations is concept dispersion <ref> (Rendell and Seshu, 1990) </ref>. Dispersion refers to the distribution of instances of a concept in the instance space. Although dispersion applies to concepts in any domain with a metric, one way of visualizing dispersion is to imagine the concept as a probability surface over a two-dimensional plane (see Figure 2.1). <p> In general, concepts that have many odd-shaped peaks scattered about in instance space are difficult to learn. The number of peaks, or peak multiplicity, is the primary factor in concept dispersion. A large number of peaks can make learning difficult for many algorithms <ref> (Rendell and Cho, 21 1990) </ref>. The reason a wide variety of learning algorithms are affected in a similar way is that the all share a common underlying bias. <p> Instead the concept must be learned as a collection of smaller peaks, again causing difficulty via peak multiplicity. 2.2.2 Feature Interaction In domains represented by feature-vectors, concept dispersion is often a symptom of an underlying phenomenon called feature interaction <ref> (Rendell and Seshu, 1990) </ref>. Feature interaction occurs when multiple parts of a representation must be considered together to observe the pattern in a concept. Specifically, the relationship between one feature and the target concept is affected by one or more other features. <p> New features can potentially improve learning performance in difficult domains by enhancing the metric in instance space. The new syntactic component might expose new similarities among the instances, effectively merging peaks (see Figure 2.5), and thus facilitating the broad class of SBL algorithms <ref> (Rendell and Seshu, 1990) </ref>. Such helpful constructions have been called "high-level features" because they are more directly related to the target concept, making patterns easier to detect. <p> One premise of the framework is that the domain is initially difficult for learning, so that current induction algorithms get only low accuracy. Furthermore, this difficulty should be due to a low-level representation, rather than other causes such as noise (Quinlan, 1986) or intrinsic inaccuracy <ref> (Rendell and Seshu, 1990) </ref>. Another premise is that domain knowledge is available which can be used to help change representations. Our framework is semi-automated, requiring knowledge. It will not work in artificial domains lacking significant background knowledge. However, low-level representations and background knowledge are common characteristics of real-world domains.
Reference: <author> Rendell, L.A. </author> <year> 1985. </year> <title> Substantial constructive induction using layered information compression: Tractable feature formation in search. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence. </booktitle> <pages> 650-658. </pages>
Reference-contexts: A common constructive induction technique is to invent new terms, such as by clustering (Anderberg, 1973). Clustering can improve a representation by giving a unique label to separate peaks in instance space <ref> (Rendell, 1985) </ref>. Examples of induction systems that exploit clustering are CLUSTER (Michalski and Stepp, 1983) and COBWEB (Fisher, 1987). Another method 26 for inventing new terms is inductive-logic programming (ILP).
Reference: <author> Rendell, L. </author> <year> 1987. </year> <title> Representation and models for concept learning. </title> <type> Technical Report UIUCDCS-R-87-1324, </type> <institution> Department of Computer Science, University of Illinios. </institution>
Reference: <author> Richards, F. </author> <year> 1992. </year> <title> Folded and unfolded proteins: An introduction. </title> <editor> In Creighton, T., editor 1992, </editor> <title> Protein Folding. </title> <publisher> Freeman: </publisher> <address> New York. </address> <pages> 1-58. </pages>
Reference-contexts: Since protein folding can take up to several seconds of real time, accurate determination of structures with globally minimum energy by molecular simulation de novo is intractable <ref> (Richards, 1992) </ref>. 5.4.3 Secondary Structure Prediction Another computational method for obtaining structural information about proteins from their amino acid sequences is secondary structure prediction. The goal of this approach is to predict the secondary structure-usually alpha-helix, beta-sheet, or random coil at each residue in a sequence.
Reference: <author> Richardson, J.S. and Richardson, </author> <title> D.C. 1989. Principles and patterns of protein conformation. In Fasman, G.D., editor 1989, Prediction of Protein Structure and the Principles of Protein Conformation. </title> <publisher> Plenum Press: </publisher> <address> New York. </address> <pages> 1-98. </pages>
Reference-contexts: For example, analysis of our results suggest that the set of small amino acids should be reduced (Dayhoff et al., 1972), histidine belongs in the positive group rather than the aromatic group (Taylor, 1986), and glutamine may play a unique role <ref> (Richardson and Richardson, 1989) </ref>. * The relevant property at a site depends on its context, and this allows many more prop erties to be important besides hydrophobicity. * The pattern of hydrophobicity among the three N-terminal and three C-terminal neighbors of each amino acid can be used an effective definition of <p> Also, glycine has no side-chain at all, but due to the exposure of the carbonyl in its backbone, it often acts hydrophilic <ref> (Richardson and Richardson, 1989) </ref>. 112 primaryEf f ectOnStructure (hydrophobicity) value (hydrophobicity; ala; hydrophobic) value (hydrophobicity; cys; hydrophilic) : : : partOf T ertEnv (X; N ) ^ primaryEf f ectOnStructure (H) ^ (1 &lt; N &lt; length (S)) ^ elem (seq (S; idents); N ) = A ^ (1 &lt; X <p> Other knowledge about properties that would help the search in this experiment could come from studies of amino acid substitution patterns in certain secondary structures. For example, it has been observed that beta-branched residues are preferred in beta-sheets, and are found less frequently than expected in alpha-helices <ref> (Richardson and Richardson, 1989) </ref>. Knowledge of the distributions of amino acids in these sub-structures could be used to estimate the contexts involved, establishing a direct relationship between context and property. 128 Chapter 7 Analysis and Evaluation There are several ways to evaluate our framework.
Reference: <author> Richardson, J.S. </author> <year> 1981. </year> <title> The anatomy and taxonomy of protein structure. </title> <booktitle> Advances in Protein Chemistry 34 </booktitle> <pages> 167-336. </pages>
Reference-contexts: At the highest level of description is tertiary structure, which refers to the global arrangement of secondary structures, also called the fold of the protein <ref> (Richardson, 1981) </ref>. While the structure of a protein can be precisely described using atomic coordinates, it is more convenient to specify the relative locations and orientations of the secondary-structure components. This approach abstracts away insignificant details about atom positions between proteins that are functionally equivalent.
Reference: <author> Rumelhart, D.E.; Hinton, G.E.; and Williams, R.J. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D.E. and McClelland, J.L., editors 1986, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press: </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Our notion of representation is distinct from the internal hypothesis language, referring only to the language in which examples are presented to the learner. In fact, some induction algorithms do not even form explicit internal hypotheses, such as neural networks <ref> (Rumelhart et al., 1986) </ref> and instance-based algorithms (Aha et al., 1991). The role that representation plays in machine learning can be explained in terms of bias (Utgoff, 1986). <p> In a similar manner, domain knowledge is often used to select the parameters for the induction algorithm. Many algorithms have parameters that must be specified, such as the stopping criterion of ID3 (Quinlan, 1983), the learning rate of BackProp <ref> (Rumelhart et al., 1986) </ref>, or the number of neighbors maintained for each cluster in an instance-based learner (Aha et al., 1991). The accuracy of a learner can be sensitive to the choices for such parameters, so domain experts often use background knowledge to help make reasonable selections.
Reference: <author> Russell, S.J. and Grosof, B.N. </author> <year> 1987. </year> <title> A declarative approach to bias in concept learning. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence. </booktitle> <pages> 505-510. </pages> <address> 158 Samuel, A.L. </address> <year> 1959. </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development 3 </journal> <pages> 210-229. </pages>
Reference-contexts: Abu-Mostafa (1994) presents a technique for using knowledge to generate artificial examples to illustrate some property of the target concept. Knowledge may also be incorporated into learning in the form of analogies, such as through determinations <ref> (Davies and Russell, 1987) </ref>, or by mapping the structure of one concept onto another when they are believed to be similar (Falkenhainer et al., 1989).
Reference: <author> Sander, C. and Schneider, R. </author> <year> 1991. </year> <title> Database of homology-derived protein structures and the structural meaning of sequence alignment. </title> <booktitle> Proteins 9 </booktitle> <pages> 56-68. </pages>
Reference-contexts: Homology modeling is based on the observation that, whenever two proteins have similar sequences (e.g. &gt; 30% identity), they always have similar structures <ref> (Sander and Schneider, 1991) </ref>. Structural similarity has a technical definition, called RMSD, that is based on measuring the deviation of corresponding atoms in the protein backbone when they are optimally super-imposed (Levitt and Chothia, 1976). <p> While proteins naturally flex due to Brownian motion (Cooper, 1976), and might undergo slight structural adjustments 71 full names, and 3-letter abbreviations. 72 during catalysis (Koshland, 1958), these variations are much insignificant compared to the differences in structure between different proteins <ref> (Sander and Schneider, 1991) </ref>. 5.2 Levels of Structural Description Protein structures can be described at various levels of detail. The lowest level of description is the primary structure, which consists of just the names of the amino acids in the sequence.
Reference: <author> Sankoff, D. and Kruskal, J. </author> <year> 1983. </year> <title> Time Warps, String Edits, and Macromolecules. </title> <address> Addison-Wessley: Reading, MA. </address>
Reference-contexts: The score between each pair of amino acids can be seen as an kind of "empirical" similarity measure. Substitution matrices can easily be incorporated into most sequence alignment algorithms, using the frequency of substitution between each pair of residues as a partial match score 84 <ref> (Sankoff and Kruskal, 1983) </ref>. Instead of penalizing all mismatches equally, this approach par-tially rewards mismatches between residues that are observed to exchange frequently. The use of such partial match scores should help distinguish distantly related proteins (with similar structures but low sequence similarity) from truly unrelated proteins. <p> Substitution tables contain numerical estimates for the likelihood of replacement between each pair of amino acids, based on observed counts of substitutions in multiple alignments of evolutionarily-related proteins. The pairwise similarity scores can be used directly in most alignment algorithms as partial match scores <ref> (Sankoff and Kruskal, 1983) </ref>. One of the most commonly used tables by molecular biologists is MDM78 (Dayhoff et al., 1978). We re-computed all pairwise alignments among the proteins in our dataset using 123 MDM78 for partial match scores.
Reference: <author> Saxena, S. </author> <year> 1991. </year> <title> On the effect of instance representation on generalization. </title> <booktitle> In Proceedings of the Eighth International Machine Learning Workshop. </booktitle> <pages> 198-202. </pages>
Reference: <author> Schaeffer, C. </author> <year> 1993. </year> <title> Selecting a classification method by cross-validation. </title> <booktitle> Machine Learning 13 </booktitle> <pages> 135-142. </pages>
Reference: <author> Schlimmer, J.C. </author> <year> 1987. </year> <title> Learning and representation change. </title> <booktitle> In Proceedings of the Sixth International Joint Conference on Artificial Intelligence. </booktitle> <pages> 511-515. </pages>
Reference-contexts: STAGGER estimates the utility of a potential construction based on the proportion of positive instances the new feature would cover versus the proportion of negative instances it would not cover <ref> (Schlimmer, 1987) </ref>. LFC uses a similar function of instances covered, based on a geometric argument of which new features could have the lowest 27 and thus improve the ability of an SBL algorithm to learn the concept accurately. <p> Values for constructed features are computed for each example and appended to its description. Some example systems that construct features are FRINGE (Pagallo and Haussler, 1990), STAGGER <ref> (Schlimmer, 1987) </ref>, CITRE (Matheus and Rendell, 1989), and LFC (Ragavan and Rendell, 1993).
Reference: <author> Schulz, G.E. and Schirmer, R.H. </author> <year> 1979. </year> <title> Principles of Protein Structure. </title> <publisher> Springer-Verlag: </publisher> <address> New York. </address>
Reference-contexts: During translation, the mRNA sequence is read in non-overlapping triplets (codons) to derive the sequence of amino acids for synthesizing the protein. The genetic code is a universal many-to-one mapping of the 64 possible nucleotide triplets into the 20 amino acids <ref> (Schulz and Schirmer, 1979) </ref>. Once the amino acid sequence of a protein has been assembled, it usually takes between several milliseconds and several seconds to fold (Baldwin, 1989). <p> This approach abstracts away insignificant details about atom positions between proteins that are functionally equivalent. Many rules governing the packing of secondary structures are known, such as the propensity for beta-sheets to lie in the core of a protein <ref> (Schulz and Schirmer, 1979) </ref>, and the restriction of contacts between helices to specific angles (Lasters, 1990). <p> The other major factors involved in determining the structure of a protein are enthalpic, relating to the free energy of the folded structure. Aside from the covalent bonds, there are many 74 non-covalent interactions which contribute to the stability of a protein <ref> (Schulz and Schirmer, 1979) </ref>. Salt bridges between ionic residues with opposite charges are rare but strong. Polar interactions are slightly more common, since many of the atoms in many of the amino acids carry partial charges. The most frequent polar interaction is hydrogen-bonding, which contributes highly to enthalpy. <p> Since hydrophobicity is known to be major influence on protein structure <ref> (Schulz and Schirmer, 1979) </ref>, we can use the pattern of hydrophobicity among these six neighbors to define a notion of context.
Reference: <author> Seshu, R.; Rendell, L.; and Tcheng, D. </author> <year> 1988. </year> <title> Managing constructive induction using subcomponent assessment and mulitple-objective optimization. </title> <booktitle> In Proceedings of the First International Workshop on Change of Representation and Inductive Bias. </booktitle> <pages> 293-305. </pages>
Reference-contexts: Some systems treat all external factors in a learner as parameters, and then attempt to optimize predictive accuracy over this space (e.g. AIMS; Tcheng et al., 1989). NTC shows how this can be applied to feature construction <ref> (Seshu et al., 1988) </ref>. Chrisman (1989) presents a criterion based on the PAC-learning model for deciding when to switch biases. 136 Dynamic bias methods such as these have tended to emphasize automation, and typically do not provide any roles for domain knowledge.
Reference: <author> Seshu, R. </author> <year> 1989. </year> <title> Solving the parity problem. </title> <booktitle> In Proceedings of the European Working Session on Learning. </booktitle> <pages> 263-271. </pages>
Reference-contexts: For example, the exclusive-or of two variables in a Boolean domain causes positive instances to be scattered diagonally throughout the space. Higher-order versions of this phenomenon, called parity, have been studied as a funda 22 algorithms. mental source of difficult Boolean concepts <ref> (Seshu, 1989) </ref>. In these situations, the instances of a concept may lie in a contiguous region, but the high-level pattern is hard for SBL algorithms to detect.
Reference: <author> Simon, H.A. and Lea, G. </author> <year> 1973. </year> <title> Problem solving and rule induction: A unified view. In Gregg, L.W., editor 1973, Knowledge and Cognition. </title> <publisher> Lawrence Erlbaum Associates: </publisher> <address> Potomac, MD. </address> <pages> 105-127. </pages>
Reference: <author> Sippl, M.J and Weitckus, S. </author> <year> 1992. </year> <title> Detection of native-like models for amino acid sequences of unknown three-dimensional structures in a database of known protein conformations. </title> <booktitle> Proteins 13 </booktitle> <pages> 258-271. </pages>
Reference-contexts: Yet another criterion is the propensity for each pair of residues to be near each other, which can be transformed into an energy function for evaluating the observed spatial distribution of residues <ref> (Sippl and Weitckus, 1992) </ref>. 5.8.3 Profiles and Templates Another approach to improving the recognition of fold class identity from an amino acid sequence is to incorporate information from multiple examples.
Reference: <author> Smith, R.F. and Smith, T.F. </author> <year> 1990. </year> <title> Automatic generation of primary sequence patterns from sets of related protein sequences. </title> <type> Biochemistry 87 </type> <pages> 118-122. </pages>
Reference-contexts: Substitution tables ignore this background knowledge altogether, instead resorting to statistical summaries of observed substitutions to define the similarities among the amino acids (Dayhoff et al., 1978). Approaches such as ARIADNE/ARIEL (Lathrop et al., 1993) and PIMA <ref> (Smith and Smith, 1990) </ref> can incorporate knowledge of amino acid properties into pattern-based representations of proteins. However, these systems require multiple alignments among a set of related sequences to determine the constraints on each site for generalization. <p> Previous methods in homology modeling that have used amino acid properties have usually ignored this uncertainty by picking a single, static set of property definitions that is thought to be a good approximation. For example, PIMA <ref> (Smith and Smith, 1990) </ref> uses a 4-level hierarchical classification of amino acids according to a few dominant properties.
Reference: <author> Smith, T.F. and Waterman, M.S. </author> <year> 1981. </year> <title> Identification of common molecular subsequences. </title> <journal> Journal of Molecular Biology 147 </journal> <pages> 195-197. </pages>
Reference-contexts: Many algorithms, from CART (Breiman et al., 1984) to stepwise multiple regression <ref> (Draper and Smith, 1981) </ref>, work by iteratively identifying the variable most correlated with a subset of the training sample. These local decisions are not guaranteed to be globally optimal; a more compact concept description that recognizes higher-level patterns might exist. <p> Devijver and Kittler (1982) propose several heuristic techniques for feature selection, which reduces dimensionality by finding a subset of variables that separates classes well. This approach is similar to the use of stepwise multiple regression for selecting variables to include in a linear model <ref> (Draper and Smith, 1981) </ref>. Factor analysis can also be used to find a minimal set of variables that explains a maximal amount of variance in a dataset (Harman, 1976). <p> And stepwise multiple regression uses a greedy method for testing and selecting variables to add or drop one at a time, based on their effect on the variance of the model <ref> (Draper and Smith, 1981) </ref>. These techniques do not guarantee finding the best representation, and are unable to cover the whole space of possibilities, though they have been found to be effective heuristics in improving predictive accuracy.
Reference: <author> Smith, T.F.; Waterman, M.S.; and Burks, C. </author> <year> 1985. </year> <title> The statistical distribution of nucleic acid similarities. </title> <journal> Nucleic Acids Research 13 </journal> <pages> 645-656. </pages>
Reference-contexts: Another approach would be to divide scores by the log of the product of the lengths, which was found to be appropriate for alignments of nucleotide sequences <ref> (Smith et al., 1985) </ref>. We used the following gap-weight parameters, relative to match value of 1.0: gap-open-penalty=1.5, gap-extension-penalty=0.2. These parameters were selected by a simple optimization procedure 2 and held constant throughout the experiments.
Reference: <author> Srinivasan, R. </author> <year> 1976. </year> <title> Helicial length distribution from protein crystallographic data. </title> <journal> Industrial Journal of Biochemistry and Biophysics 13 </journal> <pages> 192-193. </pages>
Reference-contexts: There are two main classes of substructures, also called secondary structures: alpha-helices and beta-sheets. An alpha-helix is a short stretch of about 20 amino acids wound in a tight coil with about 3.5 residues per turn <ref> (Srinivasan, 1976) </ref>. A beta-sheet is composed of two to eight extended stretches of amino acids (called beta-strands, about six residues long on average) aligned side-by-side (Sternberg and Thornton, 1977). In addition to alpha-helices and beta-sheets, certain kinds of turns (4-5 residues long) have been categorized (Thornton and Thornton, 1990).
Reference: <author> Steele, J.M. </author> <year> 1982. </year> <title> Long common subsequences and the proximity of two random strings. </title> <note> SIAM Journal of Applied Mathematics 42 731-737. 159 Sternberg, </note> <author> M.J.E. and Thornton, J. </author> <year> 1977. </year> <title> On the conformation of proteins: An analysis of beta-pleated sheets. </title> <journal> Journal of Molecular Biology </journal> 110:285-296. 
Reference: <author> Stolorz, P.; Lapedes, A.; and Xia, Y. </author> <year> 1992. </year> <title> Predicting protein secondary structure using neural net and statistical methods. </title> <journal> Journal of Molecular Biology 225 </journal> <pages> 363-377. </pages>
Reference-contexts: The accuracy of secondary structure prediction algorithms can be tested on individual sites in the amino acid sequences of proteins whose structures have been determined. One of the 77 highest accuracies achieved to date is 64.4% <ref> (Stolorz et al., 1992) </ref>, although improvements are being made by choosing different definitions of secondary structure classes (Zhang and Waltz, 1993).
Reference: <author> Stone, M. </author> <year> 1974. </year> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society 36 </journal> <pages> 111-147. </pages>
Reference-contexts: To solve these two problems, we could use cross-validation, which involves multiple iterations of this process of randomly dividing the data set and training and testing the learner <ref> (Stone, 1974) </ref>. The data is first partitioned into k balanced, disjoint subsets or "folds." 1 The learner is trained on (k 1)=k of the data, withholding one of the folds, and then tested on the 1=k unseen examples.
Reference: <author> Stryer, L. </author> <year> 1988. </year> <title> Biochemistry. </title> <publisher> Freeman: </publisher> <address> New York. </address>
Reference-contexts: To demonstrate our framework, we will apply it to a difficult, real-world domain: protein structure prediction, an important but very difficult problem in biology. Proteins are macromolecules produced by cells that serve a wide range of biological functions <ref> (Stryer, 1988) </ref>. Proteins are linear polymers of smaller compounds called amino acids which fold into complex three-dimensional structures soon after synthesis. Currently, only the amino acid sequences are known for many proteins (Watson, 1990).
Reference: <author> Tanford, C. </author> <year> 1973. </year> <title> The Hydrophobic Effect. </title> <publisher> Wiley: </publisher> <address> New York. </address>
Reference-contexts: However, this forced organization limits the mobility of the water molecules, decreasing entropy. So entropy favors the packing of hydrophobic residues in the core of a protein, with a roughly spherical surface (to minimize surface area) containing the more polar or ionic residues <ref> (Tanford, 1973) </ref>. This "hydrophobic effect" explains why beta-sheets, which are usually found in the core, generally contain hydrophobic residues, while alpha-helices, which often pack with one side covering the core and one side facing the solvent, are typically amphipathic (alternating hydrophobic and hydrophilic residues).
Reference: <editor> Taylor, W.R. and Orengo, </editor> <address> C.A. </address> <year> 1989. </year> <title> Protein structure alignment. </title> <journal> Journal of Molecular Biology 208 </journal> <pages> 1-22. </pages>
Reference-contexts: One of the difficult aspects of evaluating fold potential is the initial step of folding the sequence into the candidate structure, also called threading (Lathrop and Smith, 1996). This structural alignment suffers the same complications caused by insertions and deletions as in sequence alignment <ref> (Taylor and Orengo, 1989) </ref>. Bowie et al. (1991) have shown how structural information at each site can be encoded qualitatively in the sequence of a database protein by supplementing the amino acid identities with an indication of "environment class." This method allows standard alignment algorithms to accomplish threading.
Reference: <author> Taylor, W.R. </author> <year> 1986. </year> <title> Identification of protein sequence homology by consensus template alignment. </title> <journal> Journal of Molecular Biology 188 </journal> <pages> 233-258. </pages>
Reference-contexts: Based on the unique side-chain structure of each amino acid they can be described as large or small, flexible or rigid, polar or hydrophobic, aromatic, hydroxylated, charged, etc. <ref> (Taylor, 1986) </ref>. It is these properties that determine what role each amino acid plays in a protein, and hence what other amino acids with similar properties can be substituted for it and fulfill the same role. <p> For example, analysis of our results suggest that the set of small amino acids should be reduced (Dayhoff et al., 1972), histidine belongs in the positive group rather than the aromatic group <ref> (Taylor, 1986) </ref>, and glutamine may play a unique role (Richardson and Richardson, 1989). * The relevant property at a site depends on its context, and this allows many more prop erties to be important besides hydrophobicity. * The pattern of hydrophobicity among the three N-terminal and three C-terminal neighbors of each <p> Like profiles, templates are patterns generalized from multiple sequences in a fold class. However, template extraction methods attempt to focus pattern-recognition on only the most highly conserved regions of a protein, identifying diagnostic "fingerprints" <ref> (Taylor, 1986) </ref>. For example, the PIMA method of Smith and Smith (1990) uses a property-based hierarchy of amino acids to generalize the residues found at each site.
Reference: <author> Tcheng, D.K.; Lambert, B.L.; Lu, S.C.Y.; and Rendell, L.A. </author> <year> 1989. </year> <title> Building robust learning systems by combining induction and optimization. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence. </booktitle> <pages> 806-812. </pages>
Reference-contexts: This approach can also be thought of as a form of "dynamic bias," if the representation of examples is thought of as an aspect of the bias the learner exploits during generalization <ref> (Tcheng et al., 1989) </ref>. This approach is distinct from search through "feature space," which has been proposed to understand feature construction techniques (Donoho and Rendell, 1996). Our framework treats representations as a whole, rather than searching for individual features that facilitate learning. <p> Hence a common approach is to use an automated procedure to search the space of parameters of a learning algorithm for optimal settings, which neither requires a comprehension of how the algorithm works, nor uses any specific knowledge about the domain <ref> (Tcheng et al., 1989) </ref>. Finally, knowledge is naturally used to determine the initial representation of examples in a domain. A great deal of knowledge can be utilized in initially choosing how to describe examples.
Reference: <author> Thornton, S. and Thornton, W. </author> <year> 1990. </year> <title> Beta-turns and their distortions: A proposed new nomenclature. </title> <booktitle> Protein Engineering 3 </booktitle> <pages> 479-493. </pages>
Reference-contexts: A beta-sheet is composed of two to eight extended stretches of amino acids (called beta-strands, about six residues long on average) aligned side-by-side (Sternberg and Thornton, 1977). In addition to alpha-helices and beta-sheets, certain kinds of turns (4-5 residues long) have been categorized <ref> (Thornton and Thornton, 1990) </ref>. All other sections of backbone are typically labeled as "random coil." Secondary structure is a global property that emerges from local phenomena. Secondary structures arise from repeating patterns in the twist of the backbone at each amino acid.
Reference: <author> Towell, G.; Shavlik, J.; and Noordewier, M. </author> <year> 1990. </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence. </booktitle> <pages> 861-866. </pages>
Reference-contexts: ILP uses knowledge to generate new terms for covering examples by the process of inverse resolution (Muggleton and Buntine, 1988). KBANN uses knowledge to define the topology of a neural network <ref> (Towell et al., 1990) </ref>. MIRO uses knowledge to filter the features generated by a feature construction algorithm (Drastal et al., 1989). Abu-Mostafa (1994) presents a technique for using knowledge to generate artificial examples to illustrate some property of the target concept. <p> Several explicit roles for incorporating knowledge more declaratively in learning are known. Some roles for knowledge consist of enhancements to standard induction algorithms, such as KBANN <ref> (Towell et al., 1990) </ref> or PROTOS (Porter et al., 1990). Others roles introduce novel learning algorithms to accommodate the knowledge, such as explanation-based learning (DeJong and Mooney, 1986) or analogy (Rajamoney, 1990). <p> KBANN can improve the performance of training a neural network by pre-setting the weights on connections so that it has a topology that reflects the structure of the domain theory <ref> (Towell et al., 1990) </ref>. Another example of a method for using knowledge to improve a known learning technique is MIRO, which constrains the features generated by a feature-construction algorithm by filtering out all new features except those that can be deductively derived from the domain theory (Drastal et al., 1989).
Reference: <author> Utgoff, P. </author> <year> 1986. </year> <title> Shift of bias for inductive concept learning. </title> <editor> In Michalski, R.; Carbonell, J.; and Mitchell, T., editors 1986, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach II. </booktitle> <publisher> Morgan Kaufmann: </publisher> <address> Los Altos, CA. </address> <pages> 107-148. </pages>
Reference-contexts: Several automated methods for changing representation in machine learning have been proposed (Schlimmer, 1987; Matheus, 1989; Pagallo and Haussler, 1990; Ragavan and Rendell, 1993). These methods have often been called constructive induction (Michalski, 1983), feature extraction (Devijver and Kittler, 1982), or dynamic bias <ref> (Utgoff, 1986) </ref>, though related methods have been proposed in other fields such as statistics and pattern recognition. However, a ubiquitous problem that all of these techniques must deal with in some way is the complexity of change-of-representation. In general, there are many possible ways to change representations. <p> This approach generalizes more easily to domains not based on fixed-length feature vectors, which will be important in our application to protein structure prediction. This notion of change-of-representation is also distinct from methods for shifting the hypothesis language <ref> (Utgoff, 1986) </ref>. While the ideas are related, hypothesis language refers to the internal representation used by an induction algorithm, whereas our notion of representation refers explicitly to the external encoding of examples. <p> In fact, some induction algorithms do not even form explicit internal hypotheses, such as neural networks (Rumelhart et al., 1986) and instance-based algorithms (Aha et al., 1991). The role that representation plays in machine learning can be explained in terms of bias <ref> (Utgoff, 1986) </ref>. According to the classification model of learning, the goal of the learner is to find a partition of the domain objects, consistent with the training examples. <p> BACON decides which features to construct by looking for products and ratios of numeric variables that are nearly constant (Langley et al., 1983). 2.3.3 Dynamic Bias Another way of looking at change-of-representation in machine learning is as a form of dynamic bias <ref> (Utgoff, 1986) </ref>. In this view, representation is seen as an aspect of bias, in the sense that it maybe changed independent of the examples themselves to affect generalization. One of the earliest programs to use dynamic bias was STABB (Utgoff, 1986), which used constraint-propagation through problem-solving transcripts to suggest new attributes <p> change-of-representation in machine learning is as a form of dynamic bias <ref> (Utgoff, 1986) </ref>. In this view, representation is seen as an aspect of bias, in the sense that it maybe changed independent of the examples themselves to affect generalization. One of the earliest programs to use dynamic bias was STABB (Utgoff, 1986), which used constraint-propagation through problem-solving transcripts to suggest new attributes for describing problems in order to prevent a version-space from collapsing. <p> For example, STABB shifts the symbolic language for describing mathematical functions to prevent version-space collapse in LEX, a system for learning how to do integration <ref> (Utgoff, 1986) </ref>. Some systems treat all external factors in a learner as parameters, and then attempt to optimize predictive accuracy over this space (e.g. AIMS; Tcheng et al., 1989). NTC shows how this can be applied to feature construction (Seshu et al., 1988). <p> MARS; Friedman, 1991). Similarly, all these systems generally have the goal of improving the performance of the learner, though there are many variations on the particular evaluation mechanism from cross-validated accuracy (e.g. NTC; Seshu et al., 1988) to version-space collapse (e.g. STABB; <ref> (Utgoff, 1986) </ref>) to variance minimization (e.g. MARS; Friedman, 1991) to PAC-criteria (Chrisman, 1989). Finally, to the degree that domain knowledge is involved, it can be understood as helping to choose alternatives to explore. <p> These might help learning if they share boundaries with the target concept, because they give the learner hints for where to extend generalizations <ref> (Utgoff, 1986) </ref>. Other examples where knowledge can lead to new subsets of data that might be closer to the target concept include discretization of numeric variables and grouping of nominal variables.
Reference: <author> Valiant, L.G. </author> <year> 1984. </year> <title> A theory of the learnable. </title> <booktitle> Communcations of the ACM 27 </booktitle> <pages> 1134-1142. </pages>
Reference-contexts: The attributes or class labels may be corrupted by noise (Quinlan, 1986). Critical information may be not be encoded in the data (intrinsic inaccuracy; Rendell and Seshu, 1990). And lack of data may also inhibit learning, which is a side-effect of concept-class complexity <ref> (Valiant, 1984) </ref>. In this thesis, we focus on another common cause of learning difficulty in real-world domains: low-level representation. <p> Knowledge about the data itself, such sample bias, precision of the instruments, or 30 critical cases might be known (Gaines, 1989). Another class of knowledge relates to properties of the target concept, such as degree of disjunction <ref> (Valiant, 1984) </ref>. Most induction algorithms are not designed to make use of such domain knowledge. Standard learning algorithms often attempt to construct generalizations from a set of examples alone.
Reference: <author> Watson, J.D. </author> <year> 1990. </year> <title> The human genome project: Past, present, and future. </title> <booktitle> Science 248 </booktitle> <pages> 44-49. </pages>
Reference-contexts: Proteins are macromolecules produced by cells that serve a wide range of biological functions (Stryer, 1988). Proteins are linear polymers of smaller compounds called amino acids which fold into complex three-dimensional structures soon after synthesis. Currently, only the amino acid sequences are known for many proteins <ref> (Watson, 1990) </ref>. However, it is knowledge of the structure of a protein that reveals the deepest insights about its function.
Reference: <author> Weiss, S.M. and Kapouleas, I. </author> <year> 1989. </year> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann. </publisher> <pages> 781-787. </pages>
Reference-contexts: Introduction Many good induction algorithms have been developed (Michalski, 1983; Rumelhart et al., 1986; Cheeseman et al., 1988; Quinlan, 1993; Aha et al., 1991). The efficacy of these algorithms has been demonstrated on a wide variety of benchmark domains <ref> (Weiss and Kapouleas, 1989) </ref>. However, current machine learning techniques are inadequate for learning in more difficult real-world domains.
Reference: <author> Weiss, </author> <title> S.M. and Kulikowski, </title> <address> C.A. </address> <year> 1991. </year> <title> Computer Systems that Learn. </title> <publisher> Morgan Kaufmann: </publisher> <address> San Mateo, CA. </address>
Reference-contexts: But the general principle of training and testing the learner on different subsets of data still holds, regardless of how performance is measured. As with any learning algorithm, an important concern is whether the learner is over-fitting the data <ref> (Weiss and Kulikowski, 1991) </ref>. In our case, the search for improved representations might over-fit the data by tuning the representation to aspects of the particular examples in the dataset.
Reference: <author> White, F.H. </author> <year> 1961. </year> <title> Regneration of native secondary and tertiary structures by air oxidation of reduced ribonuclease. </title> <journal> Journal of Biological Chemistry 236 </journal> <pages> 1353-1360. </pages>
Reference-contexts: However, it is considerably easier to determine the amino acid sequence of a protein, such as through DNA sequencing, and it has been shown that the sequence alone is sufficient to determine the structure <ref> (White, 1961) </ref>. Thus one of the grand challenges of biology is to compute the structure of a protein from its amino acid sequence. The mapping between sequence and structure turns out to be very difficult to compute.
Reference: <author> Wilkins, </author> <title> D.C. 1990. Knowledge base refinement as improving an incomplete and incorrect domain theory. </title> <editor> In Kodratoff, Y. and Michalski, R.S., editors 1990, </editor> <booktitle> Machine Learning, an Artificial Intelligence Approach, Volume III. </booktitle> <publisher> Morgan Kaufmann: </publisher> <address> San Mateo, CA. 493-513. 160 Winston, P. </address> <year> 1975. </year> <title> Learning structural descriptions from examples. In Minsky, M.L., editor 1975, The Psychology of Computer Vision. </title> <publisher> McGraw Hill: NewYork. </publisher>
Reference-contexts: Finally, some of these roles for knowledge are sensitive to the correctness of the knowledge. For example, the original EBL method was criticized for being unable to learn (correctly) from an domain theory that is incomplete, incorrect, or inconsistent <ref> (Wilkins, 1990) </ref>. But knowledge in many real-world domains is uncertain, and we want any new roles for knowledge to be flexible enough to tolerate this uncertainty. So learning in real-world domains poses an additional set of challenges that need to be addressed more fully by machine learning research. <p> Another criterion that would be desirable for any new role for knowledge is that it is flexible, particularly in dealing with uncertainty. Knowledge, especially in real-world domains, can be uncertain, such as being incorrect, or incomplete, or inconsistent <ref> (Wilkins, 1990) </ref>. It is extremely difficult to state correct knowledge about the real-world, including all possible consequences, with qualifications and for every conceivable situation (i.e. the frame problem; Ford and Hayes, 1991). Some of the existing roles for knowledge are known to be sensitive to uncertainty. <p> The role for knowledge in our framework is also flexible in the sense that it can tolerate uncertainty. Knowledge in real-world domains can often be incomplete, incorrect, inconsistent, etc. <ref> (Wilkins, 1990) </ref>. Some roles for knowledge can be sensitive to this and will either fail to learn or learn incorrect generalizations in such situations (e.g. EBL; DeJong and Mooney, 1986). Because of the search basis of our framework, it is inherently tolerant of uncertainty. <p> However, a fully-automated implementation of theory revision would require a technique for generating repairs, which would require making assumptions about the class of errors in which the fault is expected to be <ref> (Wilkins, 1990) </ref>. Instead, the feedback produced by our framework could also be given directly to the domain expert, who is in a better position to understand how to repair the theory (Davis, 1982).
Reference: <author> Zhang, X. and Waltz, D. </author> <year> 1993. </year> <title> Developing hierarchical representations for protein structures: An incremental approach. </title> <editor> In Hunter, L., editor 1993, </editor> <booktitle> Artificial Intelligence and Molecular Biology. </booktitle> <publisher> MIT Press: </publisher> <address> Cambridge, CA. </address>
Reference-contexts: One of the 77 highest accuracies achieved to date is 64.4% (Stolorz et al., 1992), although improvements are being made by choosing different definitions of secondary structure classes <ref> (Zhang and Waltz, 1993) </ref>. Long range interactions may be causing intrinsic inaccuracy on this task, since these methods base their predictions only on sequentially adjacent residues, possibly missing the influence of residues that are globally adjacent in the 3D structure.
Reference: <author> Zubay, G. </author> <year> 1983. </year> <title> Biochemistry. </title> <publisher> Addison-Wesley: </publisher> <address> Reading, MA. </address> <month> 161 </month>
Reference-contexts: Our framework has enabled us to achieve significant results in this difficult but important domain (Ioerger et al., 1995). Proteins are complex molecules in cells that serve many diverse biological functions <ref> (Zubay, 1983) </ref>. Proteins are composed of linear chains of smaller compounds called amino acids, of which there are twenty. The DNA in a cell encodes a unique sequence of amino acids for each protein.
References-found: 172

