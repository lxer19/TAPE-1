URL: http://www.eecs.uic.edu/~sloan/circulate-colt94.ps
Refering-URL: http://www.eecs.uic.edu/~sloan/papers.html
Root-URL: 
Email: sloan@eecs.uic.edu  U11557@uicvm.uic.edu  
Title: Warning: missing six few referencesfixed in proceedings. Learning with Queries but Incomplete Information (Extended Abstract)  
Author: Robert H. Sloan Gyorgy Turan 
Address: Chicago, IL 60607  Szeged  
Affiliation: Dept. of Electrical Eng. and Computer Science University of Illinois at Chicago  Dept. of Math., Stat., and Comp. Sci. University of Illinois at Chicago, Automata Theory Research Group Hungarian Academy of Sciences,  
Note: In Proc. Seventh Annual ACM Conference on Computational Learning Theory, p. 237-245, 1994.  
Abstract: We investigate learning with membership and equivalence queries assuming that the information provided to the learner is incomplete. By incomplete we mean that some of the membership queries may be answered by I don't know. This model is a worst-case version of the incomplete membership query model of Angluin and Slonim. It attempts to model practical learning situations, including an experiment of Lang and Baum that we describe, where the teacher may be unable to answer reliably some queries that are critical for the learning algorithm. We present algorithms to learn monotone k-term DNF with membership queries only, and to learn monotone DNF with membership and equivalence queries. Compared to the complete information case, the query complexity increases by an additive term linear in the number of I don't know answers received. We also observe that the blowup in the number of queries can in general be exponential for both our new model and the incomplete membership model.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. R. Anderson. </author> <title> Cognitive Psychology and Its Implications. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1980. </year>
Reference-contexts: Presumably in applications the membership oracle is a human domain expert, and there are many cognitive psychology studies showing that people are typically very inconsistent in deciding where the precise boundary of a concept lies. (See, e.g. <ref> [1] </ref>.) Exactly this difficulty arose when two researchers tried to apply some computational learning theory algorithms involving membership queries to a real-world problem.
Reference: [2] <author> D. Angluin. </author> <title> Learning k-term DNF formulas using queries and counterexamples. </title> <type> Technical Report YALEU/DCS/RR-559, </type> <institution> Yale University Department of Computer Science, </institution> <month> Aug. </month> <year> 1987. </year>
Reference: [3] <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <month> Nov. </month> <year> 1987. </year>
Reference: [4] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <month> Apr. </month> <year> 1988. </year>
Reference-contexts: membership and equivalence query algorithm to use limited membership queries at the cost of an exponential blowup in the number of queries used, and show that at least for some classes such a blowup is required. 2 Definitions We follow the standard model of learning from membership and equivalence queries <ref> [4] </ref>. The goal of the learner is to infer an unknown target concept from some given concept class C over a given instance space or domain. We will view concepts interchangeably as subsets of the instance space and as 0-1 functions on the instance space. <p> Proof sketch: We modify Angluin's algorithm for learning monotone DNF from ordinary membership and equivalence queries <ref> [4] </ref>, by using Algorithm DELIMIT. Our algorithm is specified in Figure 5. Note that the same algorithm works for both the standard and strict models.
Reference: [5] <author> D. Angluin, M. Frazier, and L. Pitt. </author> <title> Learning conjunctions of Horn clauses. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 147-164, </pages> <year> 1992. </year>
Reference: [6] <author> D. Angluin, L. Hellerstein, and M. Karpinski. </author> <title> Learning read-once formulas with queries. </title> <journal> J. ACM, </journal> <volume> 40 </volume> <pages> 185-210, </pages> <year> 1993. </year>
Reference: [7] <author> P. Auer and P. M. </author> <title> Long. Simulating access to hidden information while learning. </title> <booktitle> In Proceedings of the Twenty-Sixth Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: If instead of equivalence queries one allows equivalence queries with any set, then such a blowup cannot occur. This follows from a result of Auer and Long <ref> [7] </ref> showing that in this model membership queries can speed up learning by only a constant factor. 5.2 The strict model The contents of this subsection are due to Dana Angluin and Martin s Krikis.
Reference: [8] <author> E. B. Baum. </author> <title> Neural net algorithms that learn in polynomial time from examples and queries. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(1) </volume> <pages> 5-19, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Baum developed an algorithm for learning neural nets from examples and membership queries <ref> [8] </ref>, and with Lang attempted to apply it to the problem of recognizing hand-written digits [12]. This experiment was a failure because the the membership questions the algorithm posed were too difficult for people to answer reliably. <p> It would be interesting to consider the complexity of the other problems as well. Also, in the model of PAC learning with membership queries, it would be interesting to see whether Baum's algorithm <ref> [8] </ref> can be modified to tolerate I don't know answers. Another open problem related to the results of this paper is to prove lower bounds for learning monotone DNF with both equivalence queries and LMQ's.
Reference: [9] <author> N. Bshouty, L. Hellerstein, and T. Hancock. </author> <title> Learning boolean read-once formulas over generalized bases. </title> <journal> J. ACM. </journal> <note> A preliminary version appeared in Proc. </note> <editor> 5th Annu. </editor> <title> Workshop on Comput. Learning Theory, under the title Learning Boolean read-once formulas with arbitrary symmetric and constant fan-in gates. </title>
Reference-contexts: 1 Introduction Recently, many researchers have investigated learning from membership and equivalence queries. Algorithms have been found for deterministic finite automata, monotone DNF formulas, k-term DNF formulas, various read-once formulas, Horn formulas, and Boolean decision trees, among other classes <ref> [2-5, ?-6, 9, ?] </ref>.
Reference: [10] <author> W. J. Bultman. </author> <title> Topics in the Theory of Machine Learning and Neural Computing. </title> <type> PhD thesis, </type> <institution> University of Illinois at Chicago Mathematics Department, </institution> <year> 1991. </year>
Reference-contexts: In the past several years, the study of the power of membership queries has been extended to cover the effects of noise and incomplete information on membership queries <ref> [?, 10, 11, 14] </ref>. These papers have assumed either that the noisy responses are randomly generated, or that if a single instance is repeatedly queried, then the correct classification will eventually be given with high probability. <p> In Sakaki-bara's model [14], each membership query is erroneously answered independently at random, and one can defeat the noise by querying points many times and taking a majority vote. This method also works for both of Bultman's models <ref> [10] </ref>. In the incomplete membership query model [?], all queries of the same instance give the same response, so repeated queries of an instance do not help. In this model, the response to a membership query is either correct or I don't know.
Reference: [11] <author> S. A. Goldman and H. D. Mathias. </author> <title> Learning k-term DNF formulas with an incomplete membership oracle. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 77-84. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: In the past several years, the study of the power of membership queries has been extended to cover the effects of noise and incomplete information on membership queries <ref> [?, 10, 11, 14] </ref>. These papers have assumed either that the noisy responses are randomly generated, or that if a single instance is repeatedly queried, then the correct classification will eventually be given with high probability. <p> In this model, the response to a membership query is either correct or I don't know. Angluin and Slonim introduced this model and obtained strong positive results in it; subsequently Goldman and Mathias obtained even stronger positive results <ref> [?, 11] </ref>. <p> Thus positive results in the malicious membership model are stronger, in that they imply positive results in our model. Since this paper will mostly be concerned with learning monotone DNF, let us introduce some terminology used in that learning problem. As do others (e.g. <ref> [?, 11] </ref>), we will view the instance space as a partially ordered set. The top element is the vector 1 n and the bottom element is 0 n .
Reference: [12] <author> K. J. Lang and E. B. Baum. </author> <title> Query learning can work poorly when a human oracle is used. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <address> Beijing, </address> <year> 1992. </year>
Reference-contexts: Baum developed an algorithm for learning neural nets from examples and membership queries [8], and with Lang attempted to apply it to the problem of recognizing hand-written digits <ref> [12] </ref>. This experiment was a failure because the the membership questions the algorithm posed were too difficult for people to answer reliably. The algorithm typically asked membership queries on, say, a random-looking blur midway between a 5 and a 7, and the humans being queried gave very inconsistent responses [12]. <p> digits <ref> [12] </ref>. This experiment was a failure because the the membership questions the algorithm posed were too difficult for people to answer reliably. The algorithm typically asked membership queries on, say, a random-looking blur midway between a 5 and a 7, and the humans being queried gave very inconsistent responses [12]. We believe there will often be a complicated dependence of the Don't knows on the target concept, and this is what we seek to model.
Reference: [13] <author> W. Maass and G. Turan. </author> <title> Lower bound methods and separation results for on-line learning models. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 107-145, </pages> <year> 1992. </year>
Reference-contexts: Notice that each copy of A (except one) may need to make one extra equivalence query at the end to determine whether it has the correct answer. b) We construct a concept class C that is a variant of AD DRESSING <ref> [13] </ref>. Let the instance space be X = S 2 ` where the X i 's are disjoint, X 0 = f1; : : : ; `g, and jX i j = m l + 1 for 1 i 2 ` .
Reference: [14] <author> Y. Sakakibara. </author> <title> On learning from queries and counterexamples in the presence of noise. </title> <journal> Information Processing Letters, </journal> <volume> 37(5) </volume> <pages> 279-284, </pages> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: In the past several years, the study of the power of membership queries has been extended to cover the effects of noise and incomplete information on membership queries <ref> [?, 10, 11, 14] </ref>. These papers have assumed either that the noisy responses are randomly generated, or that if a single instance is repeatedly queried, then the correct classification will eventually be given with high probability. <p> These papers have assumed either that the noisy responses are randomly generated, or that if a single instance is repeatedly queried, then the correct classification will eventually be given with high probability. In Sakaki-bara's model <ref> [14] </ref>, each membership query is erroneously answered independently at random, and one can defeat the noise by querying points many times and taking a majority vote. This method also works for both of Bultman's models [10].
Reference: [15] <author> R. Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> In Proc. 1st Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 91-96, </pages> <address> San Mateo, CA, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Answers to equivalence queries are assumed to be correct. Some comparisons between these two models are given below in Sections 2, 5, and 6. In addition, the malicious misclassification noise model for PAC learning <ref> [15] </ref> is also related to these models. In the next section we give some definitions; in Section 3, we give a key subroutine for finding one term of a monotone DNF starting from a given positive instance. Our main results are presented in Section 4.
References-found: 15

