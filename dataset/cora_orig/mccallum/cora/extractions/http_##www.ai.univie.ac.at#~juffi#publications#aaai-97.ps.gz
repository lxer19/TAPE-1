URL: http://www.ai.univie.ac.at/~juffi/publications/aaai-97.ps.gz
Refering-URL: http://www.ai.univie.ac.at/~juffi/publications/publications.html
Root-URL: 
Email: E-mail: juffi@ai.univie.ac.at  
Title: More Efficient Windowing  
Author: Johannes F urnkranz 
Address: Schottengasse 3, A-1010 Wien, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: Windowing has been proposed as a procedure for efficient memory use in the ID3 decision tree learning algorithm. However, previous work has shown that windowing may often lead to a decrease in performance. In this work, we try to argue that rule learning algorithms are more appropriate for windowing than decision tree algorithms, because the former typically learn and evaluate rules independently and are thus less susceptible to changes in class distributions. Most importantly, we present a new windowing algorithm that achieves additional gains in efficiency by saving promising rules and removing examples covered by these rules from the learning window. While the presented algorithm is only suitable for redundant, noise-free data sets, we will also briefly discuss the problem of noisy data for windowing algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L.; Friedman, J.; Olshen, R.; and Stone, C. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <address> Pacific Grove, CA: </address> <publisher> Wadsworth & Brooks. </publisher>
Reference-contexts: Adding uncovered positive examples to the current window will not alter the evaluation of rules 4 This equal-frequency subsampling method has first been described in <ref> (Breiman et al. 1984) </ref> for dynamically drawing a sub- sample at each node in a decision tree from which the best split for this node is determined. <p> Related Work There have been several approaches that use subsampling algorithms that differ from windowing. For decision tree algorithms it has been proposed to use dynamic subsam- pling at each node in order to determine the optimal test. This idea has been originally proposed, but not evaluated in <ref> (Breiman et al. 1984) </ref>. Catlett (1991b) has further explored this approach in his work on peepholing, which is a sophisticated procedure for using subsampling to eliminate unpromising attributes and thresholds from consideration. Most closely related to windowing is uncertainty sam pling (Lewis & Catlett 1994).
Reference: <author> Catlett, J. </author> <year> 1991a. </year> <title> Megainduction: A test flight. </title> <editor> In Birnbaum, L., and Collins, G., eds., </editor> <booktitle> Proceedings of the 8th International Workshop on Machine Learning (ML-91), 596599. </booktitle> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In noisy domains it can be considerably slower. There is some evidence that slight variations of the basic windowing procedure like the one employed in C4.5 (Quinlan 1993) can improve the performance of windowing, in particular on noise-free domains <ref> (Catlett 1991a) </ref>, but no further empirical study has been devoted to this subject. 1 Thus, one goal of this paper is to study the suitability of windowing for rule learning algorithms. <p> It can be assumed that many more examples have to be added to the window in order to recover the structure that is inherent in the data. This hypothesis is consistent with the results of (Wirth & Catlett 1988) and <ref> (Catlett 1991a) </ref>, where it was shown that windowing is highly sensitive to noise.
Reference: <author> Catlett, J. </author> <year> 1991b. </year> <title> Megainduction: Machine Learning on Very Large Databases. </title> <type> Ph.D. Dissertation, </type> <institution> Basser Department of Computer Science, University of Sydney. </institution>
Reference: <author> Domingos, P. </author> <year> 1996. </year> <title> Efficient specific-to-general rule in-duction. </title> <editor> In Simoudis, E., and Han, J., eds., </editor> <booktitle> Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining (KDD-96), </booktitle> <volume> 319322. </volume> <publisher> AAAI Press. </publisher>
Reference-contexts: Work on partitioning, i.e. splitting the example space into segments of equal size and combining the rules learned on each partition, has also produced promising results in noisy domains, but has substantially decreased learning accuracy in non-noisy domains <ref> (Domingos 1996) </ref>. Besides, the technique seems to be tailored to a specific learning algorithm and not generally applicable. Conclusion and Further Research We have presented a re-evaluation for windowing using separate-and-conquer rule learning algorithms, which shows that for this type of algorithm significant gains in efficiency are possible.
Reference: <author> Furnkranz, J. </author> <year> 1997a. </year> <title> Noise-tolerant windowing. </title> <type> Technical Report OEFAI-TR-97-07, </type> <institution> Austrian Research Institute for Artificial Intelligence. </institution> <note> Submitted to IJCAI-97. </note>
Reference-contexts: Instead of stopping to test a learned theory if enough exceptions have been found for the next window, the approach we take in <ref> (Furnkranz 1997a) </ref> tests each rule on the entire remaining data set. This is more costly, but on the other hand allows to immediately identify rules that are consistent with the data and to remove all examples they cover. <p> In particular, we have shown that separate-and-conquer algorithms allow a more flexible integration of windowing into the learning algorithm. However, the presented results are limited to redundant, noise-free domains. While we believe that the restriction to noise-free domains can be successfully tackled <ref> (Furnkranz 1997a) </ref>, it lies in the nature of windowing that it can only work successfully, if there is some redundancy in the domain. Meth- ods for characterizing redundant domains are thus a rewarding topic for further research. Our implementation currently is limited to 2-class problems and symbolic attributes.
Reference: <author> Furnkranz, J. </author> <year> 1997b. </year> <title> Separate-and-conquer rule learning. </title> <journal> Artificial Intelligence Review. </journal> <note> To appear. </note>
Reference-contexts: Finally, we will give some thought to the problem of noise for windowing algorithms. Separate-and-Conquer Rule Learning We have conducted our study in the framework of separate- and-conquer rule learning algorithms that has recently gained in popularity <ref> (Furnkranz 1997b) </ref>. Our basic learning algorithm, DOS, 2 is a simple propositional version of FOIL (Quinlan 1990). It employs a top-down hill-climbing search on the information gain heuristic.
Reference: <author> Kivinen, J., and Mannila, H. </author> <year> 1994. </year> <title> The power of sampling in knowledge discovery. </title> <booktitle> In Proceedings of the 13th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems (PODS-94), </booktitle> <pages> 7785. </pages>
Reference: <author> Lewis, D. D., and Catlett, J. </author> <year> 1994. </year> <title> Heterogeneous uncer-tainty sampling for supervised learning. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning (ML-94). </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Catlett (1991b) has further explored this approach in his work on peepholing, which is a sophisticated procedure for using subsampling to eliminate unpromising attributes and thresholds from consideration. Most closely related to windowing is uncertainty sam pling <ref> (Lewis & Catlett 1994) </ref>. Here the new window is not selected on the basis of misclassified examples, but on the basis of the learner's confidence in the learned theory. The examples that are classified with the least confidence will be added to the training set in the next iteration.
Reference: <author> Quinlan, J. R. </author> <year> 1983. </year> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In Michalski, R. S.; Carbonell, J. G.; and Mitchell, T. M., eds., </editor> <booktitle> Machine Learning. An Artificial Intelligence Approach. </booktitle> <address> Palo Alto, CA: </address> <publisher> Tioga. </publisher> <pages> 463482. </pages>
Reference-contexts: Such procedures are also known as subsampling. Window- ing has been proposed as a supplement to the inductive decision tree learner ID3 <ref> (Quinlan 1983) </ref> in order to allow it to tackle tasks which would otherwise have exceeded the memory capacity of the computers of those days. Despite first successful experiments in the KRKN domain (Quinlan 1983) windowing has not played a major role in machine learning research. <p> Window- ing has been proposed as a supplement to the inductive decision tree learner ID3 <ref> (Quinlan 1983) </ref> in order to allow it to tackle tasks which would otherwise have exceeded the memory capacity of the computers of those days. Despite first successful experiments in the KRKN domain (Quinlan 1983) windowing has not played a major role in machine learning research. One reason for this is certainly the rapid development of computer hardware, which made the motivation for windowing seem less compelling. <p> ; for Example 2 Test Test = Test n Example if CLASSIFY (Theory,Example) 6= CLASS (Example) NewTrain = NewTrain [ Example else OldTest = OldTest [ Example if jNewTrain j = MaxIncSize exit for Test = APPEND (Test,OldTest) Train = Train [ NewTrain until NewTrain = ; return (Theory) Windowing <ref> (Quinlan 1983) </ref>. The algorithm starts by picking a random sample of a user-settable size InitSize from the total set of Examples. It uses these examples for learning a theory with a given learning algorithm, in our case the DOS algorithm briefly described in the previous section.
Reference: <author> Quinlan, J. R. </author> <year> 1990. </year> <title> Learning logical definitions from relations. </title> <booktitle> Machine Learning 5:239266. </booktitle>
Reference-contexts: Separate-and-Conquer Rule Learning We have conducted our study in the framework of separate- and-conquer rule learning algorithms that has recently gained in popularity (Furnkranz 1997b). Our basic learning algorithm, DOS, 2 is a simple propositional version of FOIL <ref> (Quinlan 1990) </ref>. It employs a top-down hill-climbing search on the information gain heuristic. The only stopping criteria are completeness and consistency, i.e., rules are specialized until they do not cover any negative examples, and more rules are added to the theory until all positive examples are covered.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In noisy domains it can be considerably slower. There is some evidence that slight variations of the basic windowing procedure like the one employed in C4.5 <ref> (Quinlan 1993) </ref> can improve the performance of windowing, in particular on noise-free domains (Catlett 1991a), but no further empirical study has been devoted to this subject. 1 Thus, one goal of this paper is to study the suitability of windowing for rule learning algorithms. <p> We think that the reason for these different results is that divide-and-conquer learning as used in ID3 is more sensitive to changes in class distributions in the training examples. The sensitivity of ID3 to such changes is also confirmed by <ref> (Quinlan 1993) </ref> where it is reported that changing windowing in a way such that the class distribution in the initial window is as uniform as possible produces better results. 4 In our opinion this sensitivity is caused by the need of decision tree algorithms to optimize the class distribution in all <p> This finding contradicts the heuristic that is currently employed in C4.5, namely to add at least half of the total misclassified examples. However, this heuristic was formed in order to make windowing more effective in noisy domains <ref> (Quinlan 1993) </ref>, a goal that in our opinion cannot be achieved with merely using a noise-tolerant learner inside the windowing loop for reasons discussed in the next section. The Problem of Noise in Windowing An adaptation of the procedures discussed in this paper to noisy domains is a non-trivial endeavor.
Reference: <author> Toivonen, H. </author> <year> 1996. </year> <title> Sampling large databases for associa-tion rules. </title> <booktitle> In Proceedings of the 22nd Conference on Very Large Data Bases (VLDB-96), </booktitle> <pages> 134145. </pages>
Reference: <author> Wirth, J., and Catlett, J. </author> <year> 1988. </year> <title> Experiments on the costs and benefits of windowing in ID3. </title> <editor> In Laird, J., ed., </editor> <booktitle> Proceedings of the 5th International Conference on Machine Learning (ML-88), 8799. </booktitle> <address> Ann Arbor, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A good deal of the lack of interest in windowing can also be attributed to an empirical study <ref> (Wirth & Catlett 1988) </ref> which showed that windowing is unlikely to gain any efficiency. The authors studied windowing with ID3 in vari <p>- 0 Copyright c fl 1997, American Association for Artificial Intel- ligence (www.aaai.org). <p> This domain is particularly interesting, because window- ing with the decision tree learner ID3 could not achieve significant run-time gains over pure ID3 in a previous study (figure 2 of <ref> (Wirth & Catlett 1988) </ref>), while the slightly modified version of windowing used in C4.5 is able to achieve a run-time improvement of only about 15% (p. 59 of (Quin- lan 1993)). <p> of windowing, on the other hand, processes only a few more examples than DOS at lower sizes, but seems to be able to exploit some redundancies of the domain at 6 Note that this example set is only a subset of the Tic-Tac-Toe data set that has been studied in <ref> (Wirth & Catlett 1988) </ref>. We did not have access to the full data set. larger training set sizes. <p> WINDOS-95 exploits this fact by avoiding to re-learn rules that cover such parts of the example space. Parameter Settings All experiments reported above have been performed with InitSize = 100 and MaxIncSize = 50. Different variations of the InitSize parameter have been investigated in <ref> (Wirth & Catlett 1988) </ref>. Their results indicate that the algorithm is quite insensitive to this parameter, which we have empirically confirmed. <p> It can be assumed that many more examples have to be added to the window in order to recover the structure that is inherent in the data. This hypothesis is consistent with the results of <ref> (Wirth & Catlett 1988) </ref> and (Catlett 1991a), where it was shown that windowing is highly sensitive to noise.
Reference: <author> Wrobel, S. </author> <year> 1996. </year> <title> First order theory refinement. </title> <editor> In De Raedt, L., ed., </editor> <booktitle> Advances in Inductive Logic Programming. </booktitle> <address> Amsterdam, Netherlands: </address> <publisher> IOS Press. </publisher> <pages> 1433. </pages>
Reference-contexts: Otherwise, the ordering could maliciously effect the performance of the presented algorithms. Addi- tional gains in efficiency could also be achieved by trying to specialize over-general rules, instead of entirely removing them. For this purpose we plan to adapt ideas from research in theory revision <ref> (Wrobel 1996) </ref>. Acknowledgements This research is sponsored by the Austrian Fonds zur Forderung der Wissenschaftlichen Forschung (FWF) under grant number P10489-MAT. Financial support for the Austrian Research Institute for Artificial Intelligence is provided by the Austrian Federal Ministry of Science and Transport.
Reference: <author> Yang, Y. </author> <year> 1996. </year> <title> Sampling strategies and learning effi-ciency in text categorization. </title> <editor> In Hearst, M., and Hirsh, H., eds., </editor> <booktitle> Proceedings of the AAAI Spring Symposium on Machine Learning in Information Access, </booktitle> <pages> 8895. </pages> <note> AAAI Press. Technical Report SS-96-05. </note>
References-found: 15

