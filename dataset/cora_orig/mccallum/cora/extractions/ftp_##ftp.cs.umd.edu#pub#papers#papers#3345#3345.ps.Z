URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3345/3345.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Rounding Errors in Solving Block Hessenberg Systems  
Author: Urs von Matt and G. W. Stewart 
Keyword: Key words. Rounding error analysis, linear systems, block Hes senberg matrices, diagonally dominant matrices, M-matrices.  
Date: September, 1994  
Affiliation: University of Maryland College Park Institute for Advanced Computer Studies UMIACS-TR-94-105 Department of Computer Science  
Pubnum: CS-TR-3345  
Abstract: A rounding error analysis is presented for a divide-and-conquer algorithm to solve linear systems with block Hessenberg matrices. Conditions are derived under which the algorithm computes a backward stable solution. The algorithm is shown to be stable for diagonally dominant matrices and for M-matrices. fl Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742; e-mail: na.vonmatt@na-net.ornl.gov. y Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742; e-mail: stewart@cs.umd.edu. This work was supported in part by the National Science Foundation under grant CCR 9115568. This report is available by anonymous ftp from cs.umd.edu in the directory /pub/papers/TRs. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Berman and R. J. Plemmons, </author> <title> Nonnegative matrices in the mathematical sciences, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: M-Matrices A nonsingular n-by-n matrix A is called an M-matrix if a ij 0 for i 6= j and all the entries in A 1 are nonnegative. Many alternative characterizations of an M-matrix may be found in <ref> [1, Chapter 6] </ref> and [6, Section 6.4]. <p> Note that this definition is equivalent to the condition (M 35 ) in <ref> [1, p. 137] </ref>. In view of the preceding discussion in Section 7 the stability criterion (69) is satisfied for this particular choice of the matrix D. Therefore, Algorithm 1 will compute a stable solution for linear systems with M-matrices. 20 URS VON MATT AND G. W. STEWART 9.
Reference: 2. <author> T. F. Chan, </author> <title> Rank revealing QR factorizations, </title> <journal> Linear Algebra Appl. </journal> <volume> 88/89 (1987), </volume> <pages> 67-82. </pages>
Reference-contexts: The size of the error in computing an orthogonal URV-decomposition depends on the specifics of the decomposition. One may choose a QR-decomposition [4, Chapter 5], a rank-revealing decomposition <ref> [2, 8] </ref>, or the singular value decomposition [4, Section 8.3]. All of these factorizations have in common that they can be expressed as a sequence of orthogonal transformations applied from the left and the right to the initial matrix.
Reference: 3. <author> B. Char, K. Geddes, G. Gonnet, B. Leong, M. Monagan, and S. Watt, </author> <title> Maple V language reference manual, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: This condition is equivalent to the requirement that there exists a nonsingular block diagonal matrix D, partitioned commensurably with A, such that kD 1 ^ A 1 ADk 1 (69) 1 We used Maple <ref> [3] </ref> to derive this result. 18 URS VON MATT AND G. W. STEWART for all the matrices in the tear tree.
Reference: 4. <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix computations, second ed., </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: The well-known Sherman Morrison-Woodbury formula (cf. <ref> [4, p. 51] </ref>) would give us A 1 = ( ^ A + EA sw F T ) 1 = ^ A 1 ^ A 1 E (I + A sw F T ^ A 1 E) 1 A sw F T ^ A 1 4 URS VON MATT AND G. <p> The size of the error in computing an orthogonal URV-decomposition depends on the specifics of the decomposition. One may choose a QR-decomposition <ref> [4, Chapter 5] </ref>, a rank-revealing decomposition [2, 8], or the singular value decomposition [4, Section 8.3]. All of these factorizations have in common that they can be expressed as a sequence of orthogonal transformations applied from the left and the right to the initial matrix. <p> The size of the error in computing an orthogonal URV-decomposition depends on the specifics of the decomposition. One may choose a QR-decomposition [4, Chapter 5], a rank-revealing decomposition [2, 8], or the singular value decomposition <ref> [4, Section 8.3] </ref>. All of these factorizations have in common that they can be expressed as a sequence of orthogonal transformations applied from the left and the right to the initial matrix. The orthogonal transformations are also accumulated to give the matrices U and V .
Reference: 5. <author> N. J. Higham, </author> <title> How accurate is Gaussian elimination?, Numerical Analysis 1989, </title> <booktitle> Proceedings of the 13th Dundee Conference (D. </booktitle> <editor> F. Griffiths and G. A. Watson, eds.), </editor> <publisher> Longman Scientific and Technical, </publisher> <year> 1990, </year> <pages> pp. 137-154. </pages>
Reference-contexts: It also depends on the pivoting strategy used. See <ref> [5] </ref> for a more recent survey. Note that the bound (6) is only applicable if the right hand side of (5) is a vector.
Reference: 6. <author> H. </author> <title> Minc, Nonnegative matrices, </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: M-Matrices A nonsingular n-by-n matrix A is called an M-matrix if a ij 0 for i 6= j and all the entries in A 1 are nonnegative. Many alternative characterizations of an M-matrix may be found in [1, Chapter 6] and <ref> [6, Section 6.4] </ref>.
Reference: 7. <author> G. W. Stewart, </author> <title> On the solution of block Hessenberg systems, </title> <type> Tech. Report CS-TR-2973, </type> <institution> Department of Computer Science, University of Maryland, </institution> <month> October </month> <year> 1992, </year> <note> to appear in Numerical Linear Algebra and Applications. </note> <month> 8. </month> , <title> An updating algorithm for subspace tracking, </title> <journal> IEEE Trans. Signal Processing 40 (1992), </journal> <pages> 1535-1541. </pages> <month> 9. </month> , <title> Implementing an algorithm for solving block Hessenberg systems, </title> <type> Tech. Report CS-TR-3295, </type> <institution> Department of Computer Science, University of Maryland, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: 1. Introduction In <ref> [7] </ref> an algorithm was proposed for the solution of the linear system AX = B;(1) where A is a block Hessenberg matrix. Its development was motivated by the attempt to find the steady-state of certain Markov chains. <p> The overall procedure to solve the linear system (1) is also presented as Algorithm 1. In <ref> [7] </ref> this algorithm is refined further by introducing the auxiliary procedures "patchgen" and "topsolve".
Reference: 10. <author> The MathWorks Inc., </author> <title> MATLAB, high-performance numeric computation and visualization software, </title> <address> Natick, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: We can see from the values of Table 1 that all the subproblems in the tear tree are only moderately ill-conditioned. The entries of the right hand side b are given by b i = i. We computed our results on a DECstation 3100 using a MATLAB <ref> [10] </ref> implementation of Algorithm 1. The unit roundoff is given by " = 2 52 2:2204 10 16 . We used the singular value decomposition as our URV-decomposition. In Table 2 we present the absolute and relative residuals for increasing matrix sizes N.
Reference: 11. <author> J. H. Wilkinson, </author> <title> Error analysis of direct methods of matrix inversion, </title> <journal> J. Assoc. Comput. Mach. </journal> <volume> 8 (1961), </volume> <pages> 281-330. </pages> <month> 12. </month> , <title> Rounding errors in algebraic processes, </title> <publisher> Prentice-Hall, </publisher> <address> New Jersey, </address> <year> 1963. </year> <title> 13. , The algebraic eigenvalue problem, </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1965. </year> <title> 26 URS VON MATT AND G. </title> <type> W. </type> <institution> STEWART Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742 E-mail address: na.vonmatt@na-net.ornl.gov Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742 E-mail address: stewart@cs.umd.edu </institution>
Reference-contexts: We also assume that A 1 is nonsingular. Under these assumptions the inequality kA 1 applies. Proof. The proof is based on the observation that Gaussian elimination without pivoting can be applied to the matrix A, and that A remains diagonally dominant during this process (cf. <ref> [11, pp. 288-289] </ref>).
References-found: 9

