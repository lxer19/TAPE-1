URL: ftp://ftp.imag.fr/pub/DRAKKAR/duda/algebraic-video.ps.gz
Refering-URL: http://delos.imag.fr/publications.html
Root-URL: http://www.imag.fr
Title: Algebraic Video for Composition and Content-Based Access  
Author: Ron Weiss Andrzej Duda David K. Gifford 
Affiliation: Programming Systems Research Group MIT Laboratory for Computer Science  
Abstract: We introduce a new data model called algebraic video that provides operations for the composition, search, navigation and playback of digital video presentations. Video presentations are composed using a video algebra that consists of a set of basic operations on video segments to produce a desired video stream. The video algebra contains operations for temporally and spatially combining video segments as well as for attaching attributes to these segments. Algebraic video access methods also query and navigation operations. Query and navigation allow users to discover video presentations of interest by describing desired attributes and exploring a presentation's context. Unlike previous approaches, algebraic video permits video expressions to be nested in arbitrarily deep hierarchies. It also permits video segments to inherit attributes by context. Experience with a prototype algebraic video system suggests that algebraic video is an effective way to access and manage video. The prototype system is used to discover video segments of interest from existing collections and create new video presentations with algebraic combinations of these segments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Adobe Systems Incorporated, Mountain View, CA. </institution> <note> Adobe Premiere User Guide, first edition, </note> <year> 1991. </year>
Reference-contexts: Our data model provides a full hierarchical organization of video footage that permits flexible browsing. Unlike simple stratification, the algebraic video model preserves the nested relationships between strata and allows to explore the context in which a stratum appears. Commercially available tools such as Adobe Premiere <ref> [1] </ref>, DiVA VideoShop [7] and MacroMind Director [14] allow the user to create movies using audio and video tracks, and also enables the user to specify special effects during video segment transitions. These commercial systems are based on two distinct paradigms: timelines and scripts. <p> 1 ) - (x 2 ; y 2 ) priority specifies that E 1 will be displayed with priority in the window defined by (x 1 ; y 1 ) as the bottom-left corner, and (x 2 ; y 2 ) as the right-top corner such that x i 2 <ref> [0; 1] </ref> and y i 2 [0; 1] audio audio E 1 channel f orce priority specifies that the audio of E 1 will be output to channel with priority. If f orce is true, override audio specifications of the component expressions. <p> y 2 ) priority specifies that E 1 will be displayed with priority in the window defined by (x 1 ; y 1 ) as the bottom-left corner, and (x 2 ; y 2 ) as the right-top corner such that x i 2 <ref> [0; 1] </ref> and y i 2 [0; 1] audio audio E 1 channel f orce priority specifies that the audio of E 1 will be output to channel with priority. If f orce is true, override audio specifications of the component expressions. <p> The rectangular region is specified by two points in a relative coordinate system, the top-left (x 1 ; y 1 ) and bottom-right (x 2 ; y 2 ) corners, such that x i 2 <ref> [0; 1] </ref> and y i 2 [0; 1]. By default, a video expression is associated with a square that fits in the parent rectangle. Figures 8 and 9 give a simple example of an algebraic video node and illustrate the playback characteristics of this node using spatial and temporal coordinates. <p> The rectangular region is specified by two points in a relative coordinate system, the top-left (x 1 ; y 1 ) and bottom-right (x 2 ; y 2 ) corners, such that x i 2 <ref> [0; 1] </ref> and y i 2 [0; 1]. By default, a video expression is associated with a square that fits in the parent rectangle. Figures 8 and 9 give a simple example of an algebraic video node and illustrate the playback characteristics of this node using spatial and temporal coordinates.
Reference: [2] <author> T.G. Aguierre Smith and G. Davenport. </author> <title> The stratification system: A design environment for random access video. </title> <booktitle> In Proc. 3rd International Workshop on Network and Operating System Support for Digital Audio and Video., </booktitle> <address> La Jolla, CA, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Davenport et al. <ref> [3, 2] </ref> implemented a video annotation system. It uses the concept of stratification to assign descriptions to video footage, where each stratum refers to a sequence of video frames. The strata may overlap or totally encompass each other. be accessed using simple keyword search. <p> Davenport et al. <ref> [3, 2] </ref> defines a stratification mechanism, where textual descriptions called strata are associated with possibly overlapping portions of a linear video stream. In the algebraic video data model, linear strata are just algebraic video nodes.
Reference: [3] <author> T.G. Aguierre Smith and N.C. Pincever. </author> <title> Parsing movies in context. </title> <booktitle> In Proc Summer 1991 Usenix Conference., </booktitle> <pages> pages 157-168, </pages> <address> Nashville, Tennessee, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Davenport et al. <ref> [3, 2] </ref> implemented a video annotation system. It uses the concept of stratification to assign descriptions to video footage, where each stratum refers to a sequence of video frames. The strata may overlap or totally encompass each other. be accessed using simple keyword search. <p> Davenport et al. <ref> [3, 2] </ref> defines a stratification mechanism, where textual descriptions called strata are associated with possibly overlapping portions of a linear video stream. In the algebraic video data model, linear strata are just algebraic video nodes.
Reference: [4] <author> Tim Berners-Lee, Robert Cailliau, Jean-Francois Groff, and Bernd Pollermann. </author> <title> World-wide web: The information universe. </title> <journal> Electronic Networking, </journal> <volume> 2(1) </volume> <pages> 52-58, </pages> <year> 1992. </year>
Reference-contexts: The implementation is built on top of three existing subsystems: the VuSystem [22], the Semantic File System (SFS) [10], and the World-Wide-Web (WWW) <ref> [4] </ref>. The VuSystem provides an environment for recording, processing and playing video. A set of 11 C++ classes manage basic functions such as synchro-nizing video streams, displaying in a window, and processing video streams. TCL [16] scripts control C++ classes and offer a programmable user interface that can be customized.
Reference: [5] <author> A. S. Bruckman. </author> <title> Electronic scrapbook: Towards an intelligent home-video editing system. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Moreover, the system is focused on retrieving previously stored information and is not suitable for users that need to create, edit and annotate a personally customized view of the video footage. Electronic Scrapbook is a system for home-video video annotation and editing <ref> [5] </ref>, where the annotations can later be used for content-based access. The user can attach descriptions to video clips and use a modified form of case-based reasoning to edit and create personalized video stories.
Reference: [6] <author> M. Davis. </author> <title> Media Streams: An iconic visual language for video annotation. </title> <booktitle> In Proc. IEEE Symposium on Visual Languages, </booktitle> <pages> pages 196-202, </pages> <address> Bergen, Norway, </address> <year> 1993. </year>
Reference-contexts: However, it does not support a fully functional free form annotation mechanism that enables subsequent content-based access. Media Streams is an iconic visual language that enables users to create multi-layered, iconic annotations of video content <ref> [6] </ref>. Icons denoting objects and actions are organized into cascading hierarchies from levels of generality to increasing levels of specificity. Additionally, icons are organized across multiple axes of descriptions such as objects, characters, relative positions, time or transitions.
Reference: [7] <institution> DiVA Corporation, </institution> <address> Cambridge, MA. </address> <note> DiVA VideoShop User's Guide, </note> <year> 1991. </year>
Reference-contexts: Our data model provides a full hierarchical organization of video footage that permits flexible browsing. Unlike simple stratification, the algebraic video model preserves the nested relationships between strata and allows to explore the context in which a stratum appears. Commercially available tools such as Adobe Premiere [1], DiVA VideoShop <ref> [7] </ref> and MacroMind Director [14] allow the user to create movies using audio and video tracks, and also enables the user to specify special effects during video segment transitions. These commercial systems are based on two distinct paradigms: timelines and scripts.
Reference: [8] <author> E. Fiume, D. Tsichritzis, and L. Dami. </author> <title> A temporal scripting language for object-oriented animation. </title> <booktitle> In Proc. Eurographics 1987, </booktitle> <pages> pages 283-294, </pages> <address> Amsterdam, Netherlands, </address> <month> August </month> <year> 1987. </year>
Reference-contexts: We are investigating other algebraic video composition operations. These include operations that will achieve overlay of video streams, synchronization on events, a general synchronization operator (similar to operators defined by Fiume et al. <ref> [8] </ref>), and non-determinism. 3.1.2 Output Characteristics Because multiple video streams can be scheduled to play at any specific time within one video expression, the playback may require multiple screen displays and audio outputs.
Reference: [9] <author> S. Gibbs, C. Breiteneder, and D. Tsichritzis. </author> <title> Audio/Video databases: An object-oriented approach. </title> <booktitle> In Proc. 9th IEEE Int. Data Engineering Conference, </booktitle> <pages> pages 381-390, </pages> <year> 1993. </year> <month> 14 </month>
Reference-contexts: The system uses a small, special-purpose taxonomy that can be used in descriptions, but does not exploit the logical structure of video. For example, the user cannot describe hierarchical relationships where video segments are nested. Gibbs et al. <ref> [9] </ref> proposes an object-oriented approach to video databases. An audio/video database can be viewed as a collection of values (audio and video data) and activities (interconnectable components used to process values). Two abstraction mechanisms, temporal composition, and flow composition allow aggregation of values and activities.
Reference: [10] <author> D. K. Gifford, P. Jouvelot, M. A. Sheldon, and J. W. O'Toole. </author> <title> Semantic file systems. </title> <booktitle> In Thirteenth ACM Symposium on Operating Systems Principles. ACM, </booktitle> <month> October </month> <year> 1991. </year> <title> Available as Operating Systems Review Volume 25, Number 5. </title>
Reference-contexts: The implementation is built on top of three existing subsystems: the VuSystem [22], the Semantic File System (SFS) <ref> [10] </ref>, and the World-Wide-Web (WWW) [4]. The VuSystem provides an environment for recording, processing and playing video. A set of 11 C++ classes manage basic functions such as synchro-nizing video streams, displaying in a window, and processing video streams.
Reference: [11] <author> R. Hamakawa and J. Rekimoto. </author> <title> Object composition and playback models for handling multimedia data. </title> <booktitle> In Proc. First ACM International Conference on Multimedia., </booktitle> <pages> pages 273-281, </pages> <address> Ana-heim, CA, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Multimedia authoring systems such as CMIFed [24] have rich structuring primitives for multimedia documents, but fail to address the structure of the video data itself. The video is still treated as unstructured linear stream. Hamakawa and Rekimoto <ref> [11] </ref> propose a multimedia authoring system that supports editing and reuse of multimedia data. Their system is based on a hierarchical and compositional model of multimedia objects. It allows the user to mark objects with a title at a certain point in time.
Reference: [12] <author> T.D.C Little et al. </author> <title> A digital on-demand video service supporting content-based queries. </title> <booktitle> In Proc. First ACM International Conference on Multimedia., </booktitle> <pages> pages 427-436, </pages> <address> Anaheim, California, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: As stated in their description, both MHEG and HyTime are intended for final formatted documents, and lack mechanisms for content-based access, editing and annotation of the multimedia data. 2.2 Systems with Content-Based Access to Video Content-based access systems provide facilities to discover video segments of interest. Little et al. <ref> [12] </ref> implemented a system that supports content-based retrieval of video footage. They define a specific data schema composed of movie, scene and actor relations with a fixed set of attributes. The system requires manual feature extraction, and then fits these features into the data schema.
Reference: [13] <author> W. E. Mackay and G. Davenport. </author> <title> Virtual video editing in interactive multimedia applications. </title> <journal> Communications of the ACM, </journal> <volume> 32(7), </volume> <month> July </month> <year> 1989. </year>
Reference-contexts: Additionally, the system supports full-video searches for frames in which a specified object appears. Queries are accomplished using an image of the reference objects. 3 Algebraic Video Model In general, video is composed of different story units such as shots, scenes and sequences arranged according to some logical structure <ref> [13] </ref>. Frames recorded sequentially form a shot. One or several related shots are combined in a scene and a series of related scenes forms a sequence. The logical structure is defined by a screenplay that organizes story units and provides detailed descriptions of scenes and sequences.
Reference: [14] <author> MacroMind. </author> <note> Director Version 2.0, </note> <year> 1990. </year>
Reference-contexts: Unlike simple stratification, the algebraic video model preserves the nested relationships between strata and allows to explore the context in which a stratum appears. Commercially available tools such as Adobe Premiere [1], DiVA VideoShop [7] and MacroMind Director <ref> [14] </ref> allow the user to create movies using audio and video tracks, and also enables the user to specify special effects during video segment transitions. These commercial systems are based on two distinct paradigms: timelines and scripts.
Reference: [15] <author> Akio Nagasaka and Yuzuru Tanaka. </author> <title> Automatic video indexing and full-video search for object appearances. In Visual Database Systems, </title> <booktitle> II, </booktitle> <pages> pages 113-127. </pages> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference-contexts: The data schema used for this system is not 5 sufficiently flexible and therefore not suitable for free form modeling of the complex relations between video segments. Nagasaka <ref> [15] </ref> implemented a system that automatically indexes video by detecting cuts and associating a small icon of a representative frame with each subpart. The list of icons is used as an index of the video. Additionally, the system supports full-video searches for frames in which a specified object appears.
Reference: [16] <author> J.K. Ousterhout. </author> <title> An X11 toolkit based on the Tcl language. </title> <booktitle> In USENIX Association 1991 Winter Conference Proceedings, </booktitle> <pages> pages 105-115, </pages> <address> Dallas, TX, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: The VuSystem provides an environment for recording, processing and playing video. A set of 11 C++ classes manage basic functions such as synchro-nizing video streams, displaying in a window, and processing video streams. TCL <ref> [16] </ref> scripts control C++ classes and offer a programmable user interface that can be customized. The VuSystem is used for managing raw video data and for its support for TCL programming.
Reference: [17] <author> Roger Price. Mheg: </author> <title> An introduction to the future international standard for hypermedia object interchange. </title> <booktitle> In Proc. First ACM International Conference on Multimedia., </booktitle> <pages> pages 121-128, </pages> <address> Ana-heim, CA, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The icons are used to annotate video streams represented in a Media Time Line. Currently, around 2200 iconic primitives can be browsed. However, this user-friendly visual approach to annotation is limited by a fixed vocabulary. Also, it does not exploit textual data such as close-captioned text. The MHEG <ref> [17] </ref> standard is intended for "coded representation of final form multimedia and hyper-media objects that will be interchanged across service and applications". At the core of the standard are the MHEG objects (represented in Figure 5) that play a federated role between interacting applications.
Reference: [18] <author> R. Snodgrass. </author> <title> The temporal query language TQuel. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 12(2) </volume> <pages> 247-298, </pages> <month> June </month> <year> 1987. </year>
Reference: [19] <institution> International Standard. Information technology hypermedia/time-based structuring language (hytime). </institution> <address> ISO/IEC 10743, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: MHEG defines the formats used at the interchange point between applications that want to exchange multimedia data. The objects are synchronized and composed to form complex presentations using four mechanisms: script, conditional activation, spatio-temporal, and close system synchronization. The HyTime <ref> [19] </ref> hypermedia standard provides a mechanism to specify hyperlinks and schedule multi 4 MH-OBJECT DESCRIPTORNULL SCRIPT COMPONENT LINK MACRO CONTENT COMPOSITE INTERACTION VISUAL TEMPORAL AUDIBLE NUMERICAL TEXT GRAPHICS STILL VIDEO AUDIO AUDIOVISUAL media information in time and space.
Reference: [20] <author> D. Swanberg, C.F. Chu, and R. Jain. </author> <title> Architecture of a multimedia information system for content-based retrieval. </title> <booktitle> In Proc. 3rd International Workshop on Network and Operating System Support for Digital Audio and Video., </booktitle> <address> La Jolla, CA, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Several proposed systems extract information from these unstructured streams and then provide a data model that is used for content-based access. Swanberg et al. <ref> [20, 21] </ref> defines such an architecture for parsing data semantics from the video stream.
Reference: [21] <author> D. Swanberg, C.F. Chu, and R. Jain. </author> <title> Knowledge guided parsing in video datbases. </title> <booktitle> In IS&/SPIE's Symposium on Electronic Imaging: Science & Technology, </booktitle> <address> San Jose, CA, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Several proposed systems extract information from these unstructured streams and then provide a data model that is used for content-based access. Swanberg et al. <ref> [20, 21] </ref> defines such an architecture for parsing data semantics from the video stream.
Reference: [22] <author> D. K. Tennenhouse et al. </author> <title> A software-oriented approach to the design of media processing environments. </title> <booktitle> In Proc. IEEE International Conference on Multimedia Computing and Systems., </booktitle> <address> Boston, MA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: The implementation is built on top of three existing subsystems: the VuSystem <ref> [22] </ref>, the Semantic File System (SFS) [10], and the World-Wide-Web (WWW) [4]. The VuSystem provides an environment for recording, processing and playing video. A set of 11 C++ classes manage basic functions such as synchro-nizing video streams, displaying in a window, and processing video streams.
Reference: [23] <author> L. Teodosio and W. Bender. </author> <title> Salient video stills: Content and context preserved. </title> <booktitle> In Proc. First ACM International Conference on Multimedia., </booktitle> <pages> pages 39-46, </pages> <address> Anaheim, CA, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The priority parameter is defined in a manner analogous to the priority parameter of the window operation. 3.1.3 Descriptions The model permits the association of arbitrary descriptions with a given video algebra expression. It allows textual, as well as non-textual descriptions such as key frames, icons, salient stills <ref> [23] </ref>, and image features like color, texture, and shape. The description operation associates content information with a video expression.
Reference: [24] <author> G. van Rossum et al. CMIFed: </author> <title> A presentation environment for portable hypermedia documents. </title> <booktitle> In Proc. First ACM International Conference on Multimedia., </booktitle> <pages> pages 183-188, </pages> <address> Anaheim, CA, </address> <month> August </month> <year> 1993. </year> <month> 15 </month>
Reference-contexts: The toolkits do not take advantage of this distinctive feature. Moreover, the toolkits lack methods for specifying the elaborate logical structure of video data and do not address content-based access. Our approach allows structured, multi-stream composition using video algebra operations and content-based access. Multimedia authoring systems such as CMIFed <ref> [24] </ref> have rich structuring primitives for multimedia documents, but fail to address the structure of the video data itself. The video is still treated as unstructured linear stream. Hamakawa and Rekimoto [11] propose a multimedia authoring system that supports editing and reuse of multimedia data.
References-found: 24

