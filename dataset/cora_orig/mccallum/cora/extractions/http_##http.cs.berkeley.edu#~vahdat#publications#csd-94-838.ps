URL: http://http.cs.berkeley.edu/~vahdat/publications/csd-94-838.ps
Refering-URL: http://http.cs.berkeley.edu/~vahdat/publications/work.html
Root-URL: 
Title: Combining Parallel and Sequential Workloads on a Network of Workstations  
Author: Remzi Arpaci, Amin Vahdat, Thomas Anderson, and David Patterson 
Date: October 25, 1994  
Abstract: This paper examines the plausibility of using a network of workstations (NOW) for a mixture of parallel and sequential jobs. Through trace-driven simulation, our study identifies a number of results that should be of interest to NOW system designers. First, it is not sufficient to use workstation resources to provide a supercomputer only by night. Next, parallel programs can cause a significant number of lengthy delays to interactive users. Finally, simple scheduling techniques can identify available workstations and minimize user delays while providing parallel program performance comparable to a dedicated massively parallel processor. If these scheduling policies are employed, parallel programmers and interactive users can peacefully coexist on a NOW. 
Abstract-found: 1
Intro-found: 1
Reference: [Biagioni et al. 1993] <author> Biagioni, E., Cooper, E., and Sansom, R. </author> <title> Designing a practical ATM LAN. </title> <journal> IEEE Network, </journal> <volume> 7(2), </volume> <year> 1993. </year>
Reference-contexts: Because of the economies of scale, building parallel computers from mass-produced hardware and software components can be more cost effective than building the system from scratch. Though massively parallel processor (MPP) interconnects today have better performance than local area networks (LANs), emerging standards such as ATM <ref> [Biagioni et al. 1993] </ref> and the repackaging of MPP interconnects as LANs [Cohen et al. 1993] makes NOWs more attractive, even for communication intensive parallel programs. To date, NOWs have been used only for batch-style programming on a dedicated cluster of workstations, although idle machines represent a substantial untapped resource.
Reference: [Blumrich et al. 1994] <author> Blumrich, M., Li, K., Alpert, R., Dubnicki, C., Felton, E., and Sandberg, J. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference: [Carriero & Gelernter 1989] <author> Carriero, N. and Gelernter, D. </author> <title> Linda in Context. </title> <journal> Communications of the ACM, </journal> <month> April </month> <year> 1989. </year>
Reference: [Cohen et al. 1993] <author> Cohen, D., Finn, G. G., and Felderman, R. </author> <title> ATOMIC: A Very-High-Speed Local Communication Architecture. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Though massively parallel processor (MPP) interconnects today have better performance than local area networks (LANs), emerging standards such as ATM [Biagioni et al. 1993] and the repackaging of MPP interconnects as LANs <ref> [Cohen et al. 1993] </ref> makes NOWs more attractive, even for communication intensive parallel programs. To date, NOWs have been used only for batch-style programming on a dedicated cluster of workstations, although idle machines represent a substantial untapped resource.
Reference: [Douglis & Ousterhout 1991] <author> Douglis, F. and Ousterhout, J. </author> <title> Transparent Process Migration: Design Alternatives and the Sprite Implementation. </title> <journal> Software Practice and Experience, </journal> <volume> 21(8) </volume> <pages> 757-85, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The simulator also assumes that when one of the workstations used by a parallel job becomes unavailable (e.g. an interactive user resumes work), the parallel program is halted until another available workstation is found. The process state is then saved and migrated <ref> [Theimer et al. 1985, Douglis & Ousterhout 1991] </ref> to the new node. 3 Results In this section, we present our results on how resources should be scheduled on a NOW. <p> These costs may have adverse affects upon not only interactive users but to the parallel program performance as well. First, we examine the affects of migration. One hypothesis states that since migration is the rare case <ref> [Eager et al. 1986, Douglis & Ousterhout 1991] </ref>, its performance need not be optimized. To test this hypothesis, we varied the cost of process migration in our simulator. <p> On the right is the corresponding memory behavior. Experiments performed on a DECstation 5000/200 running Ultrix 4.2a. 9 parallel program throughput. Douglis and Ousterhout <ref> [Douglis & Ousterhout 1991] </ref> demonstrate that machines that have only recently gone idle should be avoided since they are most likely to once again become unavailable. Our simulations confirm this result; Figure 7 plots parallel program slowdown as a function of recruitment threshold. <p> Douglis and Ousterhout found that about two-thirds of machines were available on average <ref> [Douglis & Ousterhout 1991] </ref>, and Mutka and Livny [Mutka & Livny 1991] found similar results. Table 3 summarizes these results. We have identified a number of issues that have not been addressed by these previous efforts. First, the only workstation resource monitored is the CPU.
Reference: [Eager et al. 1986] <author> Eager, D. L., Lazowska, E. D., and Zahorjan, J. </author> <title> Adaptive Load Shating in Homogeneous Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 12(5) </volume> <pages> 662-675, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: These costs may have adverse affects upon not only interactive users but to the parallel program performance as well. First, we examine the affects of migration. One hypothesis states that since migration is the rare case <ref> [Eager et al. 1986, Douglis & Ousterhout 1991] </ref>, its performance need not be optimized. To test this hypothesis, we varied the cost of process migration in our simulator.
Reference: [Feitelson & Rudolph 1992] <author> Feitelson, D. G. and Rudolph, L. </author> <title> Gang Scheduling Performance Benefits for Fine-Grained Synchronization. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16(4) </volume> <pages> 306-18, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Two reasons account for this distinction in computing platforms. First, scheduling sequential and parallel jobs on a single platform is difficult because parallel programs must be co-scheduled <ref> [Ousterhout 1982, Gupta et al. 1991, Feitelson & Rudolph 1992] </ref> to achieve acceptable communication performance. Co-scheduling ensures that a parallel job simultaneously runs on each of its assigned processors, allowing the job's communication requests to be serviced without a context switch.
Reference: [Gelernter 1985] <author> Gelernter, D. </author> <title> Parallel Programming in Linda. </title> <booktitle> In Proceeding of the International Conference on Parallel Processing, </booktitle> <pages> pp. 255-263, </pages> <month> August </month> <year> 1985. </year>
Reference: [Gupta et al. 1991] <author> Gupta, A., Tucker, A., and Urushibara, S. </author> <title> The Impact of Operating System Scheduling Policies and Synchronization Methods on the Performance of Parallel Applications. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pp. 120-32, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Two reasons account for this distinction in computing platforms. First, scheduling sequential and parallel jobs on a single platform is difficult because parallel programs must be co-scheduled <ref> [Ousterhout 1982, Gupta et al. 1991, Feitelson & Rudolph 1992] </ref> to achieve acceptable communication performance. Co-scheduling ensures that a parallel job simultaneously runs on each of its assigned processors, allowing the job's communication requests to be serviced without a context switch.
Reference: [Kronenberg et al. 1986] <author> Kronenberg, N. P., Levy, H. M., and Strecker, W. D. VAXclusters: </author> <title> A Closely-Coupled Distributed System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(2), </volume> <year> 1986. </year>
Reference: [Leutenegger & Sun 1993] <author> Leutenegger, S. T. and Sun, X.-H. </author> <title> Distributed Computing Feasibility in a Non-Dedicated Homogenous Distributed System. </title> <booktitle> In Supercomputing 93, </booktitle> <year> 1993. </year>
Reference-contexts: The distribution of idle times|that afternoon idleness is more valuable than night-time idleness and that idle times come in short bursts making the cost of process migration important|is also relevant. In a related study, Leutenegger et al. <ref> [Leutenegger & Sun 1993] </ref> use simulation to study whether parallel programs can run in a non-dedicated environment (such as a NOW). While demonstrating that this may be possible without a significant impact on the parallel programs, their work does not discuss any potential impact upon interactive users.
Reference: [Litzkow & Solomon 1992] <author> Litzkow, M. and Solomon, M. </author> <title> Supporting Checkpointing and Process Migration Outside the UNIX Kernel. </title> <booktitle> In Winter 1992 USENIX Conference, </booktitle> <pages> pp. 283-290, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: For example, Litzkow and Solomon report that the Condor system requires two minutes for migration <ref> [Litzkow & Solomon 1992] </ref>, which corresponds to a 28% parallel program slowdown when compared to a system where process migration takes two seconds.
Reference: [Mutka & Livny 1991] <author> Mutka, M. M. and Livny, M. </author> <title> The Available Capacity of a Privately Owned Workstation Environment. Performance Evaluation, </title> <booktitle> 12(4) </booktitle> <pages> 269-84, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: With such a policy, one unlucky user would be delayed 80 times during the day on our measured workload. The second recruitment policy uses a method similar to that in Condor <ref> [Mutka & Livny 1991] </ref>: a machine is available if it's average CPU utilization is less than 20% and there is no keyboard activity, both for a minimum of 15 minutes. <p> Douglis and Ousterhout found that about two-thirds of machines were available on average [Douglis & Ousterhout 1991], and Mutka and Livny <ref> [Mutka & Livny 1991] </ref> found similar results. Table 3 summarizes these results. We have identified a number of issues that have not been addressed by these previous efforts. First, the only workstation resource monitored is the CPU.
Reference: [Nichols 1987] <author> Nichols, D. </author> <title> Using idle workstations in a shared computing environment. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 5-12, </pages> <month> November </month> <year> 1987. </year> <month> 15 </month>
Reference-contexts: On the right is a cumulative plot of the percentage of users which were delayed at least a give number of times during a 24 hour period. 11 Nichols measured 15 to 20% of workstations available during the day, which increased to roughly 30% at night <ref> [Nichols 1987] </ref>. Douglis and Ousterhout found that about two-thirds of machines were available on average [Douglis & Ousterhout 1991], and Mutka and Livny [Mutka & Livny 1991] found similar results. Table 3 summarizes these results.
Reference: [Ousterhout 1982] <author> Ousterhout, J. K. </author> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> In Third International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 22-30, </pages> <month> May </month> <year> 1982. </year>
Reference-contexts: Two reasons account for this distinction in computing platforms. First, scheduling sequential and parallel jobs on a single platform is difficult because parallel programs must be co-scheduled <ref> [Ousterhout 1982, Gupta et al. 1991, Feitelson & Rudolph 1992] </ref> to achieve acceptable communication performance. Co-scheduling ensures that a parallel job simultaneously runs on each of its assigned processors, allowing the job's communication requests to be serviced without a context switch.
Reference: [Sunderam 1990] <author> Sunderam, V. </author> <title> PVM: A Framework for Parallel Distributed Computing. </title> <journal> Con-currency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference: [Theimer & Lantz 1989] <author> Theimer, M. M. and Lantz, K. A. </author> <title> Finding Idle Machines in a Workstation-Based Distributed System. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(11) </volume> <pages> 1444-57, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Theimer and his colleagues estimated that roughly one-third of their 25 workstations were free, even at the busiest times of the day <ref> [Theimer & Lantz 1989] </ref>. 10 of the recruitment threshold. The minima is the point where a machine is likely to remain idle for a long period of time. upon the percentage of parallel jobs that complete in 24 hours.
Reference: [Theimer et al. 1985] <author> Theimer, M., Landtz, K., and Cheriton, D. </author> <title> Preemptable Remote Execution Facilities for the V System. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 2-12, </pages> <month> December </month> <year> 1985. </year> <month> 16 </month>
Reference-contexts: The simulator also assumes that when one of the workstations used by a parallel job becomes unavailable (e.g. an interactive user resumes work), the parallel program is halted until another available workstation is found. The process state is then saved and migrated <ref> [Theimer et al. 1985, Douglis & Ousterhout 1991] </ref> to the new node. 3 Results In this section, we present our results on how resources should be scheduled on a NOW.
References-found: 18

