URL: http://www.cs.utexas.edu/users/rdb/papers/STOC93.ps.gz
Refering-URL: http://www.cs.utexas.edu/users/cilk/papers.html
Root-URL: 
Title: Space-Efficient Scheduling of Multithreaded Computations (extended abstract)  
Author: Robert D. Blumofe Charles E. Leiserson 
Note: This research was supported in part by the Defense Advanced Research Projects Agency under Grant N00014-91-J-1698. Robert Blumofe was also supported in part by a National Science Foundation Graduate Fellowship.  
Address: 545 Technology Square Cambridge, Massachusetts 02139  
Affiliation: MIT Laboratory for Computer Science  
Abstract: This paper considers the problem of scheduling dynamic parallel computations to achieve linear speedup without using significantly more space per processor than that required for a single-processor execution. We develop a formal graph-theoretic model of multithreaded computation and give three important measures of time and space: T 1 is the time required for executing the computation on 1 processor, T 1 is the time required by an infinite number of processors, and S 1 is the space required to execute the computation on 1 processor. We consider a computation executed on P processors to be time-efficient if the time is O(T 1 =P + T 1 ), that is, it achieves linear speedup when P = O(T 1 =T 1 ). We consider a P -processor execution of the computation to be space-efficient if it uses O(S 1 P ) total space, that is, the space per processor is within a constant factor of the space required for a 1-processor execution. We show that there exist multithreaded computations such that no execution schedule can simultaneously achieve efficient time and efficient space. But by restricting ourselves to "strict" computations|those in which all arguments to a procedure must be available before the procedure can be invoked|we obtain much more positive results. Specifically, we show that for any strict multithreaded computation, there exists an execution schedule that achieves both efficient time and efficient space. We give a simple on-line algorithm to compute such schedules. Unfortunately, because the algorithm uses a global queue, the overhead of computing the schedule can be substantial. We show however, that this overhead problem can be overcome; we give a decentralized algorithm that can compute and execute a P -processor schedule on-line in expected time O(T 1 =P + T 1 lg P ) and worst-case space O(S 1 P lg P ), including overhead costs. We also indicate how some nonstrictness can be allowed in an otherwise strict computation in a way that may improve performance, but does not adversely affect the asymptotic time and space bounds. This paper appeared in the "Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing (STOC '93)," May 16-18, 1993, San Diego, California. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Beng-Hong Lim, David Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: At run time, a scheduler dynamically orders execution of the threads. Other systems employ schedulers that dynamically order threads based on the availability of data in shared-memory multiprocessors <ref> [1, 4] </ref> or message arrivals in message-passing multicomputers [9, 24]. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently active to keep the processors of the computer busy.
Reference: [2] <author> Sandeep Bhatt, David Greenberg, Tom Leighton, and Pangfeng Liu. </author> <title> Tight bounds for on-line tree embed-dings. </title> <booktitle> In Proceedings of the Second Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 344-350, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: Thus, the relevant issue in this work is how to balance the load across processors. Important work in this area includes a randomized work-stealing algorithm for load balancing [20]; dynamic tree-embedding algorithms <ref> [2, 17] </ref>; and algorithms for backtrack search [14, 19, 25], which can be viewed as a multithreaded computation with no data-dependency edges.
Reference: [3] <author> Robert D. Blumofe. </author> <title> Managing storage for multi-threaded computations. </title> <type> Master's thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massa-chusetts Institute of Technology, </institution> <month> September </month> <year> 1992. </year> <note> Also: </note> <institution> MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-552. </institution>
Reference-contexts: Section 6 presents a distributed scheduling algorithm for strict computations. Finally, in Section 7, we conclude with a discussion of related and future work. Where proofs are omitted or sketched, details can be found in <ref> [3] </ref>. 2 A model for multithreaded computation This section defines the model of multithreaded computation that we use in this paper. We also define what it means for a parallel computer to execute a multithreaded computation. <p> Alternatively, by "sequestering" the non-strictly spawned threads, the scheduler itself can budget the nonstrict spawns and achieve these same time and space bounds; details can be found in <ref> [3] </ref>. 6 Distributed scheduling algorithms In a distributed scheduling algorithm, each processor works depth-first out of its own local priority queue. Specifically, to get a thread to work on, a processor removes the deepest ready thread from its local queue. <p> In particular, threads must migrate occasionally and some degree of synchronization is needed to avoid the large deviations that result if this random process is run over a long period of time. Further discourse on these problems can be found in <ref> [3] </ref>. In order to achieve the desired result, we modify the Karp and Zhang technique by incorporating a new mechanism to enforce a modest degree of synchrony among the processors. <p> In particular, if we choose r = fi (lg P +lg S 1 ), then the total time spent rerandomizing is O (T 1 =P ) and the per-processor storage bound is O (S 1 (lg P + lg S 1 )). Details can be found in <ref> [3] </ref>. 7 Related and future work Although the work we have presented here provides some theoretical underpinnings for understanding the resource requirements of multithreaded computations, much remains to be done. In this section, we review some of the related work, both theoretical and empirical, on scheduling dynamic computations.
Reference: [4] <author> Bob Boothe and Abhiram Ranade. </author> <title> Improved multi-threading techniques for hiding communication latency in multiprocessors. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 214-223, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: At run time, a scheduler dynamically orders execution of the threads. Other systems employ schedulers that dynamically order threads based on the availability of data in shared-memory multiprocessors <ref> [1, 4] </ref> or message arrivals in message-passing multicomputers [9, 24]. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently active to keep the processors of the computer busy.
Reference: [5] <author> Richard P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 21(2) </volume> <pages> 201-206, </pages> <month> April </month> <year> 1974. </year>
Reference-contexts: A computer with P processors can execute at most P tasks per step, and since the computation has T 1 tasks, T P T 1 =P . And, of course, we also have T P T 1 . Brent's Theorem <ref> [5, Lemma 2] </ref> yields the bound T P T 1 =P +T 1 .
Reference: [6] <author> David E. Culler. </author> <title> Resource management for the tagged token dataflow architecture. </title> <type> Master's thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1980. </year>
Reference-contexts: The problem of storage management for multithreaded computations has been a growing concern among practitioners <ref> [6, 12] </ref>. To date, most existing techniques for controlling storage requirements have consisted of heuristics to either bound storage use by explicitly controlling storage as a resource or reduce storage use by modifying the scheduler's behavior.
Reference: [7] <author> David E. Culler and Arvind. </author> <title> Resource requirements of dataflow programs. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 141-150, </pages> <address> Honolulu, Hawaii, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: In attempting to expose parallelism, however, schedulers often end up exposing more parallelism than the computer can actually exploit, and since each active thread requires the use of a certain amount of memory, such schedulers can easily overrun the memory capacity of the machine <ref> [7, 12, 21] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [7, 12, 21]. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations. <p> the computer can actually exploit, and since each active thread requires the use of a certain amount of memory, such schedulers can easily overrun the memory capacity of the machine <ref> [7, 12, 21] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [7, 12, 21]. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations. Our goal is to develop scheduling algorithms that expose sufficient parallelism to obtain linear speedup, but without exposing so much parallelism that the space requirements become excessive. <p> A fair scheduler aggressively exposes parallelism, often resulting in excessive space requirements. In order to curb the excessive use of space exhibited by fair scheduling, researchers from the dataflow community have developed heuristics to explicitly manage storage <ref> [7, 21] </ref>. The effectiveness of these heuristics is documented with encouraging empirical evidence but no provable time bounds. In contrast with these heuristic techniques, we have chosen to develop an algorithmic foundation that manages stor age by allowing programmers to leverage their knowledge of storage requirements for serially executed programs.
Reference: [8] <author> David E. Culler, Anurag Sah, Klaus Erik Schauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <address> Santa Clara, Cal-ifornia, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: 1 Introduction In the course of investigating schemes for general-purpose MIMD-style parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [8, 11, 13, 22] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler dynamically orders execution of the threads.
Reference: [9] <author> William J. Dally, Linda Chao, Andrew Chien, Soha Hassoun, Waldemar Horwat, Jon Kaplan, Paul Song, Brian Totty, and Scott Wills. </author> <title> Architecture of a message-driven processor. </title> <booktitle> In Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 189-196, </pages> <address> Pittsburgh, Pennsylvania, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: At run time, a scheduler dynamically orders execution of the threads. Other systems employ schedulers that dynamically order threads based on the availability of data in shared-memory multiprocessors [1, 4] or message arrivals in message-passing multicomputers <ref> [9, 24] </ref>. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently active to keep the processors of the computer busy.
Reference: [10] <author> R. Feldmann, P. Mysliwietz, and B. Monien. </author> <title> A fully distributed chess program. </title> <type> Technical Report 79, </type> <institution> University of Paderborn, Germany, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: If the processor goes idle, however, linear speedup is not guaranteed. For Halstead's unfair scheduling policy, characterizing the performance analytically is difficult. Thread stealing has also been employed in two parallel chess-playing programs. Zugzwang <ref> [10] </ref> is a program in which processors steal subcomputations of a chess tree using a parallel alpha-beta search algorithm. StarTech [16] is another parallel program organized along similar lines, but with a parallel scout-search algorithm.
Reference: [11] <author> V. G. Grafe and J. E. Hoch. </author> <title> The Epsilon-2 hybrid dataflow architecture. </title> <booktitle> In COMPCON 90, </booktitle> <pages> pages 88-93, </pages> <address> San Francisco, California, </address> <month> February </month> <year> 1990. </year>
Reference-contexts: 1 Introduction In the course of investigating schemes for general-purpose MIMD-style parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [8, 11, 13, 22] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler dynamically orders execution of the threads.
Reference: [12] <author> Robert H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: In attempting to expose parallelism, however, schedulers often end up exposing more parallelism than the computer can actually exploit, and since each active thread requires the use of a certain amount of memory, such schedulers can easily overrun the memory capacity of the machine <ref> [7, 12, 21] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [7, 12, 21]. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations. <p> the computer can actually exploit, and since each active thread requires the use of a certain amount of memory, such schedulers can easily overrun the memory capacity of the machine <ref> [7, 12, 21] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [7, 12, 21]. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations. Our goal is to develop scheduling algorithms that expose sufficient parallelism to obtain linear speedup, but without exposing so much parallelism that the space requirements become excessive. <p> The problem of storage management for multithreaded computations has been a growing concern among practitioners <ref> [6, 12] </ref>. To date, most existing techniques for controlling storage requirements have consisted of heuristics to either bound storage use by explicitly controlling storage as a resource or reduce storage use by modifying the scheduler's behavior. <p> Other researchers have also addressed the storage issue by attempting to relate parallel storage requirements to serial storage requirements. Halstead <ref> [12] </ref>, for example, considered an unfair scheduling policy based on thread stealing. In Halstead's thread-stealing strategy, each processor works depth-first|just like a serial execution|but when a processor runs out of ready threads, it steals threads from other processors.
Reference: [13] <author> Robert A. </author> <title> Iannucci. Toward a dataflow / von Neu-mann hybrid architecture. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 131-140, </pages> <address> Honolulu, Hawaii, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: 1 Introduction In the course of investigating schemes for general-purpose MIMD-style parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [8, 11, 13, 22] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler dynamically orders execution of the threads.
Reference: [14] <author> Christos Kaklamanis and Giuseppe Persiano. </author> <title> Branch-and-bound and backtrack search on mesh-connected arrays of processors. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 118-126, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Thus, the relevant issue in this work is how to balance the load across processors. Important work in this area includes a randomized work-stealing algorithm for load balancing [20]; dynamic tree-embedding algorithms [2, 17]; and algorithms for backtrack search <ref> [14, 19, 25] </ref>, which can be viewed as a multithreaded computation with no data-dependency edges. Although this work ignores aggregate space requirements, it is interesting to note that Zhang's work-stealing algorithm for backtrack search [25] actually gives at most linear expansion of space, but he does not mention this fact.
Reference: [15] <author> Richard M. Karp and Yanjun Zhang. </author> <title> A randomized parallel branch-and-bound procedure. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 290-300, </pages> <address> Chicago, Illinois, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Of course, this approach could result in processors with empty queues sitting idle while other processors have large queues. Thus, we require each processor to have some access to nonlocal queues in order to facilitate some type of load balancing. The technique of Karp and Zhang <ref> [15] </ref> suggests a randomized algorithm in which threads are located in random queues in order to achieve some balance. We can show, however, that the naive adoption of this technique does not work.
Reference: [16] <author> Bradley Kuszmaul. </author> <title> Private communication, </title> <month> February </month> <year> 1993. </year>
Reference-contexts: For Halstead's unfair scheduling policy, characterizing the performance analytically is difficult. Thread stealing has also been employed in two parallel chess-playing programs. Zugzwang [10] is a program in which processors steal subcomputations of a chess tree using a parallel alpha-beta search algorithm. StarTech <ref> [16] </ref> is another parallel program organized along similar lines, but with a parallel scout-search algorithm. Although the authors make no guarantees of performance for their algorithms, the empirical results of these programs are good: both have won prizes in international chess competitions.
Reference: [17] <author> Tom Leighton, Mark Newman, Abhiram G. Ranade, and Eric Schwabe. </author> <title> Dynamic tree embeddings in butterflies and hypercubes. </title> <booktitle> In Proceedings of the 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 224-234, </pages> <address> Santa Fe, New Mexico, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Thus, the relevant issue in this work is how to balance the load across processors. Important work in this area includes a randomized work-stealing algorithm for load balancing [20]; dynamic tree-embedding algorithms <ref> [2, 17] </ref>; and algorithms for backtrack search [14, 19, 25], which can be viewed as a multithreaded computation with no data-dependency edges.
Reference: [18] <author> Prabhakar Raghavan. </author> <title> Probabilistic construction of deterministic algorithms: Approximating packing integer programs. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 37(2) </volume> <pages> 130-143, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: We can decompose W as a weighted sum of independent indicator random variables and show that E [W ] 2rS 1 . Then, using a theorem due to Raghavan <ref> [18, Theorem 1] </ref>, we can show that with probability at least 1 e 2r , we have W 2erS 1 .
Reference: [19] <author> Abhiram Ranade. </author> <title> Optimal speedup for backtrack search on a butterfly network. </title> <booktitle> In Proceedings of the Third Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 40-48, </pages> <address> Hilton Head, South Carolina, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Thus, the relevant issue in this work is how to balance the load across processors. Important work in this area includes a randomized work-stealing algorithm for load balancing [20]; dynamic tree-embedding algorithms [2, 17]; and algorithms for backtrack search <ref> [14, 19, 25] </ref>, which can be viewed as a multithreaded computation with no data-dependency edges. Although this work ignores aggregate space requirements, it is interesting to note that Zhang's work-stealing algorithm for backtrack search [25] actually gives at most linear expansion of space, but he does not mention this fact.
Reference: [20] <author> Larry Rudolph, Miriam Slivkin-Allalouf, and Eli Up-fal. </author> <title> A simple load balancing scheme for task allocation in parallel machines. </title> <booktitle> In Proceedings of the Third Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 237-245, </pages> <address> Hilton Head, South Car-olina, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Thus, the relevant issue in this work is how to balance the load across processors. Important work in this area includes a randomized work-stealing algorithm for load balancing <ref> [20] </ref>; dynamic tree-embedding algorithms [2, 17]; and algorithms for backtrack search [14, 19, 25], which can be viewed as a multithreaded computation with no data-dependency edges.
Reference: [21] <author> Carlos A. Ruggiero and John Sargeant. </author> <title> Control of parallelism in the Manchester dataflow machine. </title> <booktitle> In Functional Programming Languages and Computer Architecture, number 274 in Lecture Notes in Computer Science, </booktitle> <pages> pages 1-15. </pages> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: In attempting to expose parallelism, however, schedulers often end up exposing more parallelism than the computer can actually exploit, and since each active thread requires the use of a certain amount of memory, such schedulers can easily overrun the memory capacity of the machine <ref> [7, 12, 21] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [7, 12, 21]. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations. <p> the computer can actually exploit, and since each active thread requires the use of a certain amount of memory, such schedulers can easily overrun the memory capacity of the machine <ref> [7, 12, 21] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [7, 12, 21]. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations. Our goal is to develop scheduling algorithms that expose sufficient parallelism to obtain linear speedup, but without exposing so much parallelism that the space requirements become excessive. <p> A fair scheduler aggressively exposes parallelism, often resulting in excessive space requirements. In order to curb the excessive use of space exhibited by fair scheduling, researchers from the dataflow community have developed heuristics to explicitly manage storage <ref> [7, 21] </ref>. The effectiveness of these heuristics is documented with encouraging empirical evidence but no provable time bounds. In contrast with these heuristic techniques, we have chosen to develop an algorithmic foundation that manages stor age by allowing programmers to leverage their knowledge of storage requirements for serially executed programs.
Reference: [22] <author> Mitsuhisa Sato, Yuetsu Kodama, Shuichi Sakai, Yoshi-nori Yamaguchi, and Yasuhito Koumura. </author> <title> Thread-based programming for the EM-4 hybrid dataflow machine. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 146-155, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction In the course of investigating schemes for general-purpose MIMD-style parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [8, 11, 13, 22] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler dynamically orders execution of the threads.
Reference: [23] <author> Leslie G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: We can view the lg P factors in the space bound and the average available parallelism required to achieve linear speedup as the computational slack required by Valiant's bulk-synchronous model <ref> [23] </ref>. The space bound S P (X ) = O (S 1 P lg P ) indicates that Algorithm LDF (6 lg P ) requires memory to scale sufficiently to allow each physical processor enough space to simulate lg P virtual processors.
Reference: [24] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: At run time, a scheduler dynamically orders execution of the threads. Other systems employ schedulers that dynamically order threads based on the availability of data in shared-memory multiprocessors [1, 4] or message arrivals in message-passing multicomputers <ref> [9, 24] </ref>. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently active to keep the processors of the computer busy.
Reference: [25] <author> Yanjun Zhang. </author> <title> Parallel Algorithms for Combinatorial Search Problems. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, University of California at Berkeley, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: Thus, the relevant issue in this work is how to balance the load across processors. Important work in this area includes a randomized work-stealing algorithm for load balancing [20]; dynamic tree-embedding algorithms [2, 17]; and algorithms for backtrack search <ref> [14, 19, 25] </ref>, which can be viewed as a multithreaded computation with no data-dependency edges. Although this work ignores aggregate space requirements, it is interesting to note that Zhang's work-stealing algorithm for backtrack search [25] actually gives at most linear expansion of space, but he does not mention this fact. <p> Although this work ignores aggregate space requirements, it is interesting to note that Zhang's work-stealing algorithm for backtrack search <ref> [25] </ref> actually gives at most linear expansion of space, but he does not mention this fact. The problem of storage management for multithreaded computations has been a growing concern among practitioners [6, 12].
References-found: 25

