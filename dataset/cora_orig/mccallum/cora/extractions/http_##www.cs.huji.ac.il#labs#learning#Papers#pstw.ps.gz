URL: http://www.cs.huji.ac.il/labs/learning/Papers/pstw.ps.gz
Refering-URL: http://www.cs.huji.ac.il/labs/learning/Papers/NLP_list.html
Root-URL: http://www.cs.huji.ac.il
Email: pereira@research.att.com  singer@cs.huji.ac.il  tishby@cs.huji.ac.il  
Title: Beyond Word N -Grams  
Author: Fernando C. Pereira Yoram Singer Naftali Tishby 
Address: 600 Mountain Ave. Murray Hill, NJ 07974  Jerusalem 91904, Israel  Jerusalem 91904, Israel  
Affiliation: AT&T Bell Laboratories  Institute Computer Science Hebrew University  Institute of Computer Science Hebrew University  
Abstract: We describe, analyze, and experimentally evaluate a new probabilistic model for word-sequence prediction in natural languages, based on prediction suffix trees (PSTs). By using efficient data structures, we extend the notion of PST to unbounded vocabularies. We also show how to use a Bayesian approach based on recursive priors over all possible PSTs to efficiently maintain tree mixtures. These mixtures have provably and practically better performance than almost any single model. Finally, we evaluate the model on several corpora. The low perplexity achieved by relatively small PST mixture models suggests that they may be an advantageous alternative, both theoretically and practically, to the widely used n-gram models.
Abstract-found: 1
Intro-found: 1
Reference: <author> T.C. Bell, J.G. Cleary, I.H. Witten. </author> <year> 1990. </year> <title> Text Compression. </title> <publisher> Prentice Hall. </publisher>
Reference: <author> P.F. Brown, V.J. Della Pietra, P.V. deSouza, J.C. Lai, R.L. Mercer. </author> <year> 1990. </year> <title> Class-based n-gram models of natural language. </title> <booktitle> In Proceedings of the IBM Natural Language ITL, </booktitle> <pages> pages 283-298, </pages> <address> Paris, France, </address> <month> March. </month>
Reference: <author> N. Cesa-Bianchi, Y. Freund, D. Haussler, D.P. Helmbold, R.E. Schapire, M. K. Warmuth. </author> <year> 1993. </year> <title> How to use expert advice. </title> <booktitle> Proceedings of the 24th Annual ACM Symposium on Theory of Computing (STOC). </booktitle>
Reference: <author> K.W. Church and W.A. Gale. </author> <year> 1991. </year> <title> A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 19-54. </pages>
Reference: <author> A. DeSantis, G. Markowski, M.N. Wegman. </author> <year> 1988. </year> <title> Learning Probabilistic Prediction Functions. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pp. 312-328. </pages>
Reference: <author> R.A. Fisher, A.S. Corbet, C.B. Williams. </author> <year> 1943. </year> <title> The relation between the number of species and the number of individuals in a random sample of an animal population. </title> <journal> J. Animal Ecology, </journal> <volume> Vol. 12, </volume> <pages> pp. 42-58. </pages>
Reference-contexts: The GT method sets fl s (w 0 ) = t s n s , where t 1 is the total number of words that were observed only once in that context. This method has several justifications, such as a Poisson assumption on the appearance of new words <ref> (Fisher et al., 1943) </ref>. It is, however, difficult to analyze and requires keeping track of the rank of each word. Our learning scheme and data structures favor instead any method that is based only on word counts.
Reference: <author> G.I. Good. </author> <year> 1953. </year> <title> The population frequencies of species and the estimation of population parameters. </title> <journal> Biometrika, </journal> <volume> 40(3) </volume> <pages> 237-264. </pages>
Reference: <author> G.I. Good. </author> <year> 1969. </year> <title> Statistics of Language: Introduction. Encyclopedia of Linguistics, Information and Control. </title> <editor> A. R. Meetham and R. A. Hudson, editors. </editor> <address> pages 567-581. </address> <publisher> Pergamon Press, Oxford, </publisher> <address> England. </address>
Reference-contexts: 1 Introduction Finite-state methods for the statistical prediction of word sequences in natural language have had an important role in language processing research since Markov's and Shannon's pioneering investigations (C.E. Shannon, 1951). While it has always been clear that natural texts are not Markov processes of any finite order <ref> (Good, 1969) </ref>, because of very long range correlations between words in a text such as those arising from subject matter, low-order alphabetic n-gram models have been used very effectively for such tasks as statistical language identification and spelling correction, and low-order word n-gram models have been the tool of choice for
Reference: <author> D. Hindle. </author> <year> 1990. </year> <title> Noun classification from predicate-argument structures. </title> <booktitle> In 28th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 268-275, </pages> <institution> Pittsburgh, Pennsylvania. Association for Computational Linguistics, Morristown, </institution> <address> New Jersey. </address>
Reference: <author> D. Hindle. </author> <year> 1993. </year> <title> A parser for text corpora. </title> <editor> In B.T.S. Atkins and A. Zampoli, editors, </editor> <title> Computational Approaches to the Lexicon. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> England. </address> <note> To appear. </note>
Reference: <author> S.M. Katz. </author> <year> 1987. </year> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Trans. on ASSP 35(3) </journal> <pages> 400-401. </pages>
Reference-contexts: Based on this scheme several n-gram estimation methods, such as Katz's backoff scheme <ref> (Katz, 1987) </ref>, can be derived. Our learning algorithm has, however, the advantages of not being limited to a constant context length (by setting D to be arbitrarily large) and of being able to perform online adaptation.
Reference: <author> R.E. Krichevsky and V.K. Trofimov. </author> <year> 1981. </year> <title> The performance of universal encoding. </title> <journal> IEEE Trans. on Inform. Theory, </journal> <pages> pp. 199-207. </pages>
Reference: <author> P. Resnik. </author> <year> 1992. </year> <title> WordNet and distributional analysis: A class-based approach to lexical discovery. </title> <booktitle> In AAAI Workshop on Statistically-Based Natural-Language-Processing Techniques, </booktitle> <address> San Jose, California, </address> <month> July. </month>
Reference: <author> J. Rissanen. </author> <year> 1986. </year> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> The Annals of Statistics, </journal> <volume> 11(2) </volume> <pages> 416-431. </pages>
Reference: <author> D. Ron, Y. Singer, N. Tishby. </author> <year> 1994. </year> <title> The power of amnesia: learning probabilistic automata with variable memory length. </title> <note> Machine Learning (to appear). </note>
Reference-contexts: The key ingredient of the model construction is the prediction suffix tree (PST), whose nodes represent suffixes of past input and specify a predictive distribution over possible successors of the suffix. It was shown in <ref> (Ron et al., 1994) </ref> that under realistic conditions a PST is equivalent to a Markov 1 process of variable order and can be represented efficiently by a probabilistic finite-state automaton. For the purposes of this paper, however, we will use PSTs as our starting point.
Reference: <author> C.E. </author> <title> Shannon 1951. Prediction and Entropy of Printed English. </title> <journal> Bell Sys. Tech. J., </journal> <volume> Vol. 30, No. 1, </volume> <pages> pp. 50-64. </pages>
Reference-contexts: 1 Introduction Finite-state methods for the statistical prediction of word sequences in natural language have had an important role in language processing research since Markov's and Shannon's pioneering investigations <ref> (C.E. Shannon, 1951) </ref>.
Reference: <author> D.D. Sleator and R.E. Tarjan. </author> <year> 1985. </year> <title> Self-Adjusting Binary Search Trees. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 32, No. 3, </volume> <pages> pp. 653-686. </pages>
Reference-contexts: When we know in advance a (large) bound on vocabulary size, we represent the root node by arrays of word counts and possible sons subscripted by word indices. At other nodes, we used splay trees <ref> (Sleator and Tarjan, 1985) </ref> to store both the counts and the branches to longer contexts. Splay trees support search, insertion and deletion in amortized O (log (n)) time per operation.
Reference: <author> F.M.J. Willems, Y.M. Shtarkov, T.J. Tjalkens. </author> <year> 1994. </year> <title> The context tree weighting method: basic properties. </title> <note> Submitted to IEEE Trans. on Inform. Theory. </note>
Reference-contexts: Although there might be exponentially many different PSTs in the ensemble, it has been recently shown <ref> (Willems et al., 1994) </ref> that a mixture of PSTs can be efficiently computed for small alphabets. Here, we will use the Bayesian formalism to derive an online learning procedure for mixtures of PSTs of words.
Reference: <author> I.H. Witten and T.C. Bell. </author> <year> 1991. </year> <title> The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression. </title> <journal> IEEE Trans. on Inform. Theory, </journal> <volume> 37(4) </volume> <pages> 1085-1094. </pages>
Reference-contexts: In this case the probability fl s (w i ) of a word that has been observed n s i times is set to n s n s +r s . As reported in <ref> (Witten and Bell, 1991) </ref>, the performance of this method is similar to the GT estimation scheme, yet it is simpler since only the number of different words and their counts are kept. Finally, a careful analysis should be made when predicting novel events (new words).
References-found: 19

