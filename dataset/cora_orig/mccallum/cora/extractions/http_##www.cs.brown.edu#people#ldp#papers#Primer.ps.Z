URL: http://www.cs.brown.edu/people/ldp/papers/Primer.ps.Z
Refering-URL: http://www.cs.brown.edu/people/ldp/papers/paper.html
Root-URL: http://www.cs.brown.edu/
Title: Dimensionality reduction A Primer DRAFT VERSION  
Author: by Leonid Peshkin 
Note: Contents  
Date: November, 1995  
Address: Providence, RI 02912, USA  
Affiliation: Department of Computer Science Brown University  
Abstract-found: 0
Intro-found: 1
Reference: [Duda and Hart, 1986] <author> Duda, R. O. and Hart, P. E. </author> <year> (1986). </year> <title> Pattern Classification and Scene Analysis. </title> <note> A Wilet Interscience publication. </note>
Reference-contexts: Most fundamental works in this field are [Harman, 1976] and <ref> [Duda and Hart, 1986] </ref>. The principal component analysis was introduced in [Pearson, 1901] and later in [Hotelling, 1933]. The state of the art is well described in [Jolliffe, 1986].
Reference: [Friedman, 1987] <author> Friedman, J. </author> <year> (1987). </year> <title> Exploratory projection pursuit. </title> <journal> J. Am. Stat. Assoc., </journal> <volume> 82 </volume> <pages> 249-266. </pages>
Reference-contexts: An application of principal component analysis to the face recognition was analyzed in [Peshkin, 1994]. [Huber, 1985] is a comprehensive overview of projection pursuit approach. A basic paper introducing projection pursuit is [Friedman and Tukey, 1974]. More recent works in this area are <ref> [Friedman, 1987] </ref>, [Hall, 1989] and [Jones and Sibson, 1987]. 2 Principal Components Analysis 2.1 Method The objective of principal component analysis is to represent a variable in terms of several underlying factors. The simplest theoretical model for describing the variable in terms of several others is the linear one. <p> This expansion is then compared with a uniform density on the interval [1; 1] by an L 2 -distance. This is a good general index, fast to compute <ref> [Friedman, 1987] </ref>. The Hermite index is constructed using Hermite polynomials for the same expansion as underlines the Legendre index. This expansion is compared to a standard normal density using an L 2 -distance. This index works best when few terms are used in the expansion. <p> The coefficients are given by a j = 2 1 2j + 1 E R [P j (R)]: Finally, Legendre projection index is obtained by truncating the sum at order J : Q (Z) = j=1 R [P j (R)] : (6) <ref> [Friedman, 1987] </ref> notes that the results are insensitive to the value chosen for the order of polynomial expansion (J ) over a wide range (4 J 8). The computation increases linearly with J for 1D projection pursuit and quadratically for 2D. This projection index can be computed very rapidly. <p> The Legendre polynomials to order J are quickly obtained via the recursion relation 5. The Legendre projection index for a two-dimensional projection pursuit is developed in direct analogy with the one-dimensional index <ref> [Friedman, 1987] </ref>. An important detail of the algorithm is a technique used to optimize projection index function. The power of the method is reflected in its ability to find (for a given sample size and data dimension) substantial maxima of the projection index. Usually a hybrid optimization strategy is employed. <p> In this case the power of the projection pursuit procedure can be enhanced and computation reduced by restricting the pursuit search to the complement space. We have to restrict the projection pursuit search to the subspace spanned by few largest principal component axes. <ref> [Friedman, 1987] </ref> recommends that if a high-dimensional projection pursuit is unsuccessful in finding an interesting structure, one should restrict the search dimension and try again. Let us now consider an example corresponding to Figures 2.
Reference: [Friedman and Tukey, 1974] <author> Friedman, J. and Tukey, J. </author> <year> (1974). </year> <title> A projection pursuit algorithm for exploratory data analysis. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 23 </volume> <pages> 881-889. </pages>
Reference-contexts: The state of the art is well described in [Jolliffe, 1986]. An application of principal component analysis to the face recognition was analyzed in [Peshkin, 1994]. [Huber, 1985] is a comprehensive overview of projection pursuit approach. A basic paper introducing projection pursuit is <ref> [Friedman and Tukey, 1974] </ref>. More recent works in this area are [Friedman, 1987], [Hall, 1989] and [Jones and Sibson, 1987]. 2 Principal Components Analysis 2.1 Method The objective of principal component analysis is to represent a variable in terms of several underlying factors. <p> belongs to a general class of dimensionality reduction methods - projection pursuit. 3 Pro jection Pursuit 3.1 Method A general class of unsupervised dimensionality reduction methods, called exploratory projection pursuit, is based on seeking interesting projections of high dimensional data points 6 See [Harman, 1976]. 3 PROJECTION PURSUIT 6 cloud <ref> [Friedman and Tukey, 1974] </ref>. The notion of interesting projection is motivated by an observation made by Freedman that for most high-dimensional clouds, most low-dimensional projections are approximately normal. <p> Any structure seen in projection is a shadow of an actual structure in the full dimensionality. In this sense those projections that are the most revealing of the high-dimensional data distribution are those containing the sharpest structure. It is of interest then to pursue such projections. Friedman and Tukey <ref> [Friedman and Tukey, 1974] </ref> presented an algorithm for attempting this goal. The basic idea is to assign a numerical index Q (F A ) to every projection A. That index characterizes the amount of structure present (data density variation) in the projection. <p> This expansion is compared to a standard normal density using an L 2 -distance. This index works best when few terms are used in the expansion. The idea was proposed by [Hall, 1989]. The Friedman-Tukey index is based on an L 2 -norm of a local kernel density estimate <ref> [Friedman and Tukey, 1974] </ref>. The Entropy index is an extension of the Friedman-Tukey index constructed using the negative entropy of a kernel density estimate [Jones and Sibson, 1987]. Along this work we use the Legendre index.
Reference: [Gill et al., 1981] <author> Gill, P., Murray, W., and Wright, M. </author> <year> (1981). </year> <title> Practical Optimization. </title> <publisher> London: Academic Press. </publisher>
Reference-contexts: This estimate was given in 1985 taking into account limitations of computer capacity 9 . 7 For overview of the optimization methods like steepest descent, conjugate gradients and quasi-Newton which involve the use of first derivatives see <ref> [Gill et al., 1981] </ref>. 8 See [Peshkin, 1993] for the overview of clustering algorithms. 9 In the discussion of [Huber, 1985] P.
Reference: [Hall, 1989] <author> Hall, P. </author> <year> (1989). </year> <title> Polynomial projection pursuit. </title> <journal> Annals of Statistics, </journal> <volume> 17 </volume> <pages> 589-605. </pages>
Reference-contexts: An application of principal component analysis to the face recognition was analyzed in [Peshkin, 1994]. [Huber, 1985] is a comprehensive overview of projection pursuit approach. A basic paper introducing projection pursuit is [Friedman and Tukey, 1974]. More recent works in this area are [Friedman, 1987], <ref> [Hall, 1989] </ref> and [Jones and Sibson, 1987]. 2 Principal Components Analysis 2.1 Method The objective of principal component analysis is to represent a variable in terms of several underlying factors. The simplest theoretical model for describing the variable in terms of several others is the linear one. <p> The Hermite index is constructed using Hermite polynomials for the same expansion as underlines the Legendre index. This expansion is compared to a standard normal density using an L 2 -distance. This index works best when few terms are used in the expansion. The idea was proposed by <ref> [Hall, 1989] </ref>. The Friedman-Tukey index is based on an L 2 -norm of a local kernel density estimate [Friedman and Tukey, 1974]. The Entropy index is an extension of the Friedman-Tukey index constructed using the negative entropy of a kernel density estimate [Jones and Sibson, 1987].
Reference: [Harman, 1976] <author> Harman, H. H. </author> <year> (1976). </year> <title> Modern Factor Analysis. </title> <publisher> The University of Chicago Press, </publisher> <address> third edition. </address> <note> revised edition. </note>
Reference-contexts: Most fundamental works in this field are <ref> [Harman, 1976] </ref> and [Duda and Hart, 1986]. The principal component analysis was introduced in [Pearson, 1901] and later in [Hotelling, 1933]. The state of the art is well described in [Jolliffe, 1986]. <p> As it is described later on in Section 3, for multidimensional data, the majority of the projections are normal. As noted in <ref> [Harman, 1976] </ref>, when the point representation of a set of variables is used, the loci of uniform frequency density are essentially concentric, similar, and similarly situated ellipsoids, which means that the data point can be enclosed by equal-density contours, and these contours form (hyper)ellipses. <p> going to consider more advanced technique which belongs to a general class of dimensionality reduction methods - projection pursuit. 3 Pro jection Pursuit 3.1 Method A general class of unsupervised dimensionality reduction methods, called exploratory projection pursuit, is based on seeking interesting projections of high dimensional data points 6 See <ref> [Harman, 1976] </ref>. 3 PROJECTION PURSUIT 6 cloud [Friedman and Tukey, 1974]. The notion of interesting projection is motivated by an observation made by Freedman that for most high-dimensional clouds, most low-dimensional projections are approximately normal.
Reference: [Hotelling, 1933] <author> Hotelling, H. </author> <year> (1933). </year> <title> Analysis of a complex of statistical variables into principal components. </title> <journal> Journal of Educational Psychology, </journal> <volume> 24 </volume> <pages> 417-520. </pages>
Reference-contexts: Most fundamental works in this field are [Harman, 1976] and [Duda and Hart, 1986]. The principal component analysis was introduced in [Pearson, 1901] and later in <ref> [Hotelling, 1933] </ref>. The state of the art is well described in [Jolliffe, 1986]. An application of principal component analysis to the face recognition was analyzed in [Peshkin, 1994]. [Huber, 1985] is a comprehensive overview of projection pursuit approach. A basic paper introducing projection pursuit is [Friedman and Tukey, 1974]. <p> If the variables are uncorrelated, the ellipses will describe a sphere; if perfectly correlated, they will degenerate into a line. Pearson [Pearson, 1901] and, later, Hotelling <ref> [Hotelling, 1933] </ref> realized that the axes of the hyperellipsoid can be found from the eigenvectors of the correlation matrix 1 . The axes of these ellipsoids correspond to the principal components. <p> An empirical method for the reduction of a large body of data so that a maximum of variance is extracted was first proposed by Karl Pearson [Pearson, 1901] and fully developed as the method of principal components or component analysis, by Harold Hotelling <ref> [Hotelling, 1933] </ref>.
Reference: [Huber, 1985] <author> Huber, P. J. </author> <year> (1985). </year> <title> Projection pursuit. </title> <journal> Annals of Statistics, </journal> <volume> 13 </volume> <pages> 435-475. </pages>
Reference-contexts: The principal component analysis was introduced in [Pearson, 1901] and later in [Hotelling, 1933]. The state of the art is well described in [Jolliffe, 1986]. An application of principal component analysis to the face recognition was analyzed in [Peshkin, 1994]. <ref> [Huber, 1985] </ref> is a comprehensive overview of projection pursuit approach. A basic paper introducing projection pursuit is [Friedman and Tukey, 1974]. <p> This index is then maximized via numerical optimization with respect to the parameters defining the projections. We are interested not only in absolute, but also in local, extrema. The projection index can (and often should) be affine invariant <ref> [Huber, 1985] </ref>. <p> This function must vary continuously with the parameters defining the projection and have a large value when the (projected) distribution is defined to be "interesting" and small value otherwise. The notion of interesting obviously varies with the application <ref> [Huber, 1985] </ref>. We cannot expect universal agreement on what constitutes an interesting projection. A projection in which the data separates into distinct, meaningful clusters would certainly be interesting. <p> It would be appropriate to note that one should not be too optimistic about projection pursuit we cannot hope that this method will carry us more than two or three dimensions beyond the limits set by human endurance <ref> [Huber, 1985] </ref>. <p> taking into account limitations of computer capacity 9 . 7 For overview of the optimization methods like steepest descent, conjugate gradients and quasi-Newton which involve the use of first derivatives see [Gill et al., 1981]. 8 See [Peshkin, 1993] for the overview of clustering algorithms. 9 In the discussion of <ref> [Huber, 1985] </ref> P. Diaconis gives an estimation for lower and upper bounds of the 3 PROJECTION PURSUIT 10 3.2 Experiments The power of the projection pursuit algorithm to find important structure decreases with decreasing sample size and increasing dimension. <p> Having repeated this procedure, we end up with the largest projection index value, corresponding to the projection where one cloud is split into smaller ones. We have to mention that in the discussion to <ref> [Huber, 1985] </ref> the three referees, who tried to use exploratory projection pursuit in practice, were extremely pessimistic about its potential for finding clusters.
Reference: [Jolliffe, 1986] <author> Jolliffe, I. </author> <year> (1986). </year> <title> Principal Component Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: Most fundamental works in this field are [Harman, 1976] and [Duda and Hart, 1986]. The principal component analysis was introduced in [Pearson, 1901] and later in [Hotelling, 1933]. The state of the art is well described in <ref> [Jolliffe, 1986] </ref>. An application of principal component analysis to the face recognition was analyzed in [Peshkin, 1994]. [Huber, 1985] is a comprehensive overview of projection pursuit approach. A basic paper introducing projection pursuit is [Friedman and Tukey, 1974].
Reference: [Jones and Sibson, 1987] <author> Jones, M. and Sibson, R. </author> <year> (1987). </year> <journal> What is projection pursuit ? Journal of the Royal Statistical Society, </journal> <volume> 150 </volume> <pages> 1-36. </pages> <publisher> Series A. </publisher>
Reference-contexts: An application of principal component analysis to the face recognition was analyzed in [Peshkin, 1994]. [Huber, 1985] is a comprehensive overview of projection pursuit approach. A basic paper introducing projection pursuit is [Friedman and Tukey, 1974]. More recent works in this area are [Friedman, 1987], [Hall, 1989] and <ref> [Jones and Sibson, 1987] </ref>. 2 Principal Components Analysis 2.1 Method The objective of principal component analysis is to represent a variable in terms of several underlying factors. The simplest theoretical model for describing the variable in terms of several others is the linear one. <p> The idea was proposed by [Hall, 1989]. The Friedman-Tukey index is based on an L 2 -norm of a local kernel density estimate [Friedman and Tukey, 1974]. The Entropy index is an extension of the Friedman-Tukey index constructed using the negative entropy of a kernel density estimate <ref> [Jones and Sibson, 1987] </ref>. Along this work we use the Legendre index. In a one-dimensional exploratory projection pursuit we seek a linear combination Z = ff T X such that the probability density p ff (Z) is relatively highly structured.
Reference: [Pearson, 1901] <author> Pearson, K. </author> <year> (1901). </year> <title> On lines and planes of closest fit to systems of points in space. </title> <journal> Phil. Mag., </journal> <volume> 6 </volume> <pages> 559-572. ser. 2. </pages>
Reference-contexts: Most fundamental works in this field are [Harman, 1976] and [Duda and Hart, 1986]. The principal component analysis was introduced in <ref> [Pearson, 1901] </ref> and later in [Hotelling, 1933]. The state of the art is well described in [Jolliffe, 1986]. An application of principal component analysis to the face recognition was analyzed in [Peshkin, 1994]. [Huber, 1985] is a comprehensive overview of projection pursuit approach. <p> If the variables are uncorrelated, the ellipses will describe a sphere; if perfectly correlated, they will degenerate into a line. Pearson <ref> [Pearson, 1901] </ref> and, later, Hotelling [Hotelling, 1933] realized that the axes of the hyperellipsoid can be found from the eigenvectors of the correlation matrix 1 . The axes of these ellipsoids correspond to the principal components. <p> For such a matrix, all n principal components are real and positive. An empirical method for the reduction of a large body of data so that a maximum of variance is extracted was first proposed by Karl Pearson <ref> [Pearson, 1901] </ref> and fully developed as the method of principal components or component analysis, by Harold Hotelling [Hotelling, 1933].
Reference: [Peshkin, 1993] <author> Peshkin, L. </author> <year> (1993). </year> <title> Clustering algorithms. </title> <institution> Research Project. Weizmann Inst. of Science. </institution>
Reference-contexts: Compute the covariance matrix S; 2. Find the k largest eigenvalues V k and the corresponding eigenvectors U k of S; 4 Different approaches to classification or clustering were studied in <ref> [Peshkin, 1993] </ref>. 5 See Appendix A for the detailed description. 3 PROJECTION PURSUIT 5 3. Compute ^ F = Y U k ; An important issue is how many components one should extract to account for "satisfactory" amount of the total variation. <p> This estimate was given in 1985 taking into account limitations of computer capacity 9 . 7 For overview of the optimization methods like steepest descent, conjugate gradients and quasi-Newton which involve the use of first derivatives see [Gill et al., 1981]. 8 See <ref> [Peshkin, 1993] </ref> for the overview of clustering algorithms. 9 In the discussion of [Huber, 1985] P.
Reference: [Peshkin, 1994] <author> Peshkin, L. </author> <year> (1994). </year> <title> Eigenfaces for recognition. </title> <institution> Research Project. Weizmann Inst. of Science. </institution>
Reference-contexts: The principal component analysis was introduced in [Pearson, 1901] and later in [Hotelling, 1933]. The state of the art is well described in [Jolliffe, 1986]. An application of principal component analysis to the face recognition was analyzed in <ref> [Peshkin, 1994] </ref>. [Huber, 1985] is a comprehensive overview of projection pursuit approach. A basic paper introducing projection pursuit is [Friedman and Tukey, 1974]. <p> The dimensionality of the space is given by the number of nonzero eigenvalues. An example of eigen-analysis applied to the face recognition can be found in <ref> [Sirovich and Kirby, 1987, Peshkin, 1994] </ref>.
Reference: [Peshkin et al., 1994] <author> Peshkin, L., Kovaleno, M., and Ullman, S. </author> <year> (1994). </year> <title> Dominant orientation for computing the distance between face images. </title> <booktitle> In Proceedings of Japan-Israel Workshop on Computer Vision and Visual Communication, CVVC'94, </booktitle> <pages> pages 15-20, </pages> <address> Haifa, Israel. </address>
Reference: [Rissanen, 1978] <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference: [Rissanen, 1983] <author> Rissanen, J. </author> <year> (1983). </year> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Ann. Statist., </journal> <volume> 11 </volume> <pages> 416-431. </pages>
Reference: [Sirovich and Kirby, 1987] <author> Sirovich, L. and Kirby, M. </author> <year> (1987). </year> <title> A low dimensional procedure for the characterization of human faces. </title> <journal> J. Opt. Soc. Amer. A., </journal> <volume> 4(3) </volume> <pages> 519-524. </pages>
Reference-contexts: The dimensionality of the space is given by the number of nonzero eigenvalues. An example of eigen-analysis applied to the face recognition can be found in <ref> [Sirovich and Kirby, 1987, Peshkin, 1994] </ref>.
References-found: 17

