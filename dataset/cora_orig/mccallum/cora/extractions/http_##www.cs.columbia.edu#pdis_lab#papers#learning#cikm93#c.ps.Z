URL: http://www.cs.columbia.edu/pdis_lab/papers/learning/cikm93/c.ps.Z
Refering-URL: http://www.cs.columbia.edu/pdis_lab/
Root-URL: 
Email: pkc@cs.columbia.edu and sal@cs.columbia.edu  
Title: Experiments on Multistrategy Learning by Meta-Learning  
Author: Philip K. Chan and Salvatore J. Stolfo 
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Abstract: In this paper, we propose meta-learning as a general technique to combine the results of multiple learning algorithms, each applied to a set of training data. We detail several meta-learning strategies for combining independently learned classifiers, each computed by different algorithms, to improve overall prediction accuracy. The overall resulting classifier is composed of the classifiers generated by the different learning algorithms and a meta-classifier generated by a meta-learning strategy. The strategies described here are independent of the learning algorithms used. Preliminary experiments using different strategies and learning algorithms on two molecular biology sequence analysis data sets demonstrate encouraging results. Machine learning techniques are central to automated knowledge discovery systems and hence our approach can enhance the effectiveness of such systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: We applied the strategies to two real-world scientific tasks employing different combinations of several standard machine learning algorithms. 4 Experiments Four inductive learning algorithms were used in our experiments. We obtained ID3 [16] and CART <ref> [1] </ref> as part of the IND package [2] from NASA Ames Research Center; both algorithms compute decision trees. WPEBLS is the weighted version of PEBLS [7], which is a nearest-neighbor learning algorithm.
Reference: [2] <author> W. Buntine and R. Caruana. </author> <title> Introduction to IND and Recursive Partitioning. </title> <institution> NASA Ames Research Center, </institution> <year> 1991. </year>
Reference-contexts: We applied the strategies to two real-world scientific tasks employing different combinations of several standard machine learning algorithms. 4 Experiments Four inductive learning algorithms were used in our experiments. We obtained ID3 [16] and CART [1] as part of the IND package <ref> [2] </ref> from NASA Ames Research Center; both algorithms compute decision trees. WPEBLS is the weighted version of PEBLS [7], which is a nearest-neighbor learning algorithm. BAYES is a Bayesian classifier that is based on computing conditional probabilities (frequency distributions), which is described in [6].
Reference: [3] <author> P. Chan and S. Stolfo. </author> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Work. on Multistrategy Learning, </booktitle> <pages> pages 150-165, </pages> <year> 1993. </year>
Reference-contexts: The Machine Learning Toolbox project provides a set of learning algorithms as individual tools without much interaction among them. In this paper we present the concept of meta-learning, introduced in <ref> [3] </ref>, and its use in coalescing the results from multiple inductive learning systems to improve accuracy. Meta-learning can also be used to combine results from a set of parallel or distributed learning processes to improve learning speed.
Reference: [4] <author> P. Chan and S. Stolfo. </author> <title> Toward multistrategy parallel and distributed learning in sequence analysis. </title> <booktitle> In Proc. First Intl. Conf. Intel. Sys. Mol. Biol., </booktitle> <pages> pages 65-73, </pages> <year> 1993. </year>
Reference-contexts: Since the ultimate goal of this work is to improve both the accuracy and efficiency of machine learning, we have been working on combining ideas in multistrategy hypothesis boosting, described in this paper, with those in parallel learning. We call this approach multistrategy parallel learning. Preliminary results reported in <ref> [4] </ref> are encouraging.
Reference: [5] <author> P. Chan and S. Stolfo. </author> <title> Toward parallel and distributed learning by meta-learning. </title> <booktitle> In Working Notes AAAI Work. Know. Disc. Databases, </booktitle> <pages> pages 227-240, </pages> <year> 1993. </year>
Reference-contexts: Meta-learning can also be used to combine results from a set of parallel or distributed learning processes to improve learning speed. In this paper we focus primarily on boosting accuracy, leaving details of our preliminary experiments to improve speed to a companion paper <ref> [5] </ref>. The ultimate goal of this work is to improve both the accuracy and efficiency of machine learning by means of parallel processing of multiple learning systems applied to massive amounts of training data, which is further discussed in Section 6. <p> In classifying an instance, the base classifiers first generate their predictions. These predictions, together with the arbiter's prediction and a corresponding arbitration rule, generate the final prediction (see Figure 4). (This contrasts with the multi-level arbiter trees introduced in <ref> [5] </ref>.) In this strategy one learns to arbitrate among the potentially different predictions from the base classifiers, instead of learning to coalesce the predictions as in the combiner strategy. We first describe the schemes for the selection rule and then those for the arbitration rule. <p> Our approach is to apply learning processes to disjoint subsets of the original training set concurrently (a data reduction technique) and the results from the processes are then combined through meta-learning. Preliminary results reported in <ref> [5] </ref> are encouraging. Since the ultimate goal of this work is to improve both the accuracy and efficiency of machine learning, we have been working on combining ideas in multistrategy hypothesis boosting, described in this paper, with those in parallel learning. We call this approach multistrategy parallel learning.
Reference: [6] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-285, </pages> <year> 1987. </year>
Reference-contexts: WPEBLS is the weighted version of PEBLS [7], which is a nearest-neighbor learning algorithm. BAYES is a Bayesian classifier that is based on computing conditional probabilities (frequency distributions), which is described in <ref> [6] </ref>. The latter two algorithms were reimplemented in C. Two molecular biology sequence analysis data sets, obtained from the UCI Machine Learning Database, were used in our studies.
Reference: [7] <author> S. Cost and S. Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference-contexts: We obtained ID3 [16] and CART [1] as part of the IND package [2] from NASA Ames Research Center; both algorithms compute decision trees. WPEBLS is the weighted version of PEBLS <ref> [7] </ref>, which is a nearest-neighbor learning algorithm. BAYES is a Bayesian classifier that is based on computing conditional probabilities (frequency distributions), which is described in [6]. The latter two algorithms were reimplemented in C.
Reference: [8] <author> C. DeLisi. </author> <title> The human genome project. </title> <journal> American Scientist, </journal> <volume> 76 </volume> <pages> 488-493, </pages> <year> 1988. </year>
Reference-contexts: With the coming age of very large network computing, it is likely that orders of magnitude more data in databases will be available for various learning problems of real world importance. The Human Genome Project <ref> [8] </ref> and Grand Challenges of HPCC [22] are perhaps the best examples of large scale efforts demanding efficient knowledge discovery systems. Parallel and distributed processing would substantially increase the speed of processing data and hence the amount of data a knowledge discovery system can handle effectively.
Reference: [9] <author> H. Drucker, R. Schapire, and P. Simard. </author> <title> Boosting performance in neural networks. </title> <booktitle> Intl. J. Pat. Recog. Art. Intel., </booktitle> <year> 1993. </year> <note> To appear. </note>
Reference-contexts: In fact, he shows that arbitrarily high accuracy can be achieved by recursively applying the same procedure. Although his approach is limited to the PAC model of learning, some success was achieved in the domain of character recognition, using neural networks <ref> [9] </ref>. Freund [11] has a similar approach, but with potentially many more distributions.
Reference: [10] <author> N. Flann and T. Dietterich. </author> <title> A study of explanation-based mehtods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 187-266, </pages> <year> 1989. </year>
Reference-contexts: The basic notion behind this integration is to complement the different underlying learning strategies embodied by different learning algorithms by effectively reducing the space of incorrect classifications of a learned concept. There are many ways of integrating different learning algorithms. For example, work on integrating inductive and explanation-based learning <ref> [10, 20] </ref> requires a complicated new algorithm that implements both approaches to learning in a single system.
Reference: [11] <author> Y. Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> In Proc. 3rd Work. Comp. Learning Theory, </booktitle> <pages> pages 202-216, </pages> <year> 1990. </year>
Reference-contexts: In fact, he shows that arbitrarily high accuracy can be achieved by recursively applying the same procedure. Although his approach is limited to the PAC model of learning, some success was achieved in the domain of character recognition, using neural networks [9]. Freund <ref> [11] </ref> has a similar approach, but with potentially many more distributions. In addition to applying meta-learning to coalescing results from multiple different algorithms applied to the same set of data, meta-learning can also be used to combine results from a set of parallel or distributed learning processes to improve speed.
Reference: [12] <author> C. Matheus, P. Chan, and G. Piatesky-Shapiro. </author> <title> Systems for knowledge discovery in databases. </title> <journal> IEEE Trans. Know. Data. Eng., </journal> <note> 1993. To appear. </note>
Reference-contexts: Various machine learning algorithms have been proposed to learn descriptive relationships and uncover rules that classify and explain what the data mean. These techniques are central to automated knowledge dis covery systems <ref> [12] </ref>, which generally also contain statistical data analysis tools.
Reference: [13] <author> T. M. Mitchell. </author> <title> The need for biases in learning general-izaions. </title> <type> Technical Report CBM-TR-117, </type> <institution> Dept. Comp. Sci., Rutgers Univ., </institution> <year> 1980. </year>
Reference-contexts: Most research in inductive learning focuses on the conception and evaluation of distinct learning strategies embodied by an individual algorithm. Since different algorithms have different representations and search heuristics, different search spaces are being explored and hence potentially diverse results can be obtained from different algorithms. Mitchell <ref> [13] </ref> refers to this phenomenon as inductive bias. That is, the outcome of running an algorithm is biased in a certain direction. Furthermore, different data sets have different characteristics and the performance of different algorithms on these data sets might differ.
Reference: [14] <author> M. Noordewier, G. Towell, and J. Shavlik. </author> <title> Training knowledge-based neural networks to recognize genes in DNA sequences. </title> <booktitle> In Proc. NIPS-91, </booktitle> <pages> pages 530-536, </pages> <year> 1991. </year>
Reference-contexts: The sequences were then divided into a disjoint training and test set, according to the distribution described in [15]. The training set, E, for this task has 18105 instances and the test set has 3520. The DNA splice junction data set (SJ) <ref> [14] </ref>, courtesy of Noordewier, Towell, and Shavlik, contains sequences of nucleotides and the type of splice junction, if any, at the center of each sequence (three classes). Each sequence has 60 nucleotides with eight different values each (four base ones plus four combinations).
Reference: [15] <author> N. Qian and T. Sejnowski. </author> <title> Predicting the secondary structure of globular proteins using neural network models. </title> <journal> J. Mol. Biol., </journal> <volume> 202 </volume> <pages> 865-884, </pages> <year> 1988. </year>
Reference-contexts: The latter two algorithms were reimplemented in C. Two molecular biology sequence analysis data sets, obtained from the UCI Machine Learning Database, were used in our studies. The secondary protein structure data set (SS) <ref> [15] </ref>, courtesy of Qian and Sejnowski, contains sequences of amino acids and the secondary structures at the corresponding positions. There are three structures and 20 amino acids (21 attributes because of a spacer [15]) in the data. <p> The secondary protein structure data set (SS) <ref> [15] </ref>, courtesy of Qian and Sejnowski, contains sequences of amino acids and the secondary structures at the corresponding positions. There are three structures and 20 amino acids (21 attributes because of a spacer [15]) in the data. The amino acid sequences were split into shorter sequences of length 13 according to a windowing technique used in [15]. The sequences were then divided into a disjoint training and test set, according to the distribution described in [15]. <p> There are three structures and 20 amino acids (21 attributes because of a spacer <ref> [15] </ref>) in the data. The amino acid sequences were split into shorter sequences of length 13 according to a windowing technique used in [15]. The sequences were then divided into a disjoint training and test set, according to the distribution described in [15]. The training set, E, for this task has 18105 instances and the test set has 3520. <p> amino acids (21 attributes because of a spacer <ref> [15] </ref>) in the data. The amino acid sequences were split into shorter sequences of length 13 according to a windowing technique used in [15]. The sequences were then divided into a disjoint training and test set, according to the distribution described in [15]. The training set, E, for this task has 18105 instances and the test set has 3520.
Reference: [16] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: In the next section we describe our experimental results to explore the effectiveness of these strategies. We applied the strategies to two real-world scientific tasks employing different combinations of several standard machine learning algorithms. 4 Experiments Four inductive learning algorithms were used in our experiments. We obtained ID3 <ref> [16] </ref> and CART [1] as part of the IND package [2] from NASA Ames Research Center; both algorithms compute decision trees. WPEBLS is the weighted version of PEBLS [7], which is a nearest-neighbor learning algorithm.
Reference: [17] <author> R. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 197-226, </pages> <year> 1990. </year>
Reference-contexts: Meta-learning is a general technique to coalesce the results of multiple learners. In this paper we concentrate on using meta-learning to combine different learners to improve prediction accuracy. We call this approach multistrategy hypothesis boosting. (The term hypothesis boosting was introduced by Schapire <ref> [17] </ref>; his work on hypothesis boosting using only one strategy is discussed in Section 6.) It involves applying multiple algorithms on the same set of data and the results of the learned concepts are combined by meta-learning. <p> This and the aforementioned strategies and issues are the subject matter of ongoing experimentation. Hypothesis boosting using only a single strategy has recently attracted attention in the theoretical learning community. The pioneering work in this area is due to Schapire <ref> [17] </ref>. Based on an initial learned hypothesis for some concept derived from a random distribution of training data, Schapire's scheme iteratively generates two additional distributions of examples.
Reference: [18] <author> B. Silver, W. Frawley, G. Iba, J. Vittal, and K. Bradford. ILS: </author> <title> A framework for multi-paradigmatic learning. </title> <booktitle> In Proc. Seventh Intl. Conf. Machine Learning, </booktitle> <pages> pages 348-356, </pages> <year> 1990. </year>
Reference-contexts: Zhang et al.'s [24] work utilizes a similar approach to learn a combiner based on the predictions made by three different classifiers. The work from the two groups demonstrates that meta-learning can improve overall prediction accuracy. Silver et al.'s <ref> [18] </ref> work also employs multiple learners, but no learning is involved beyond those learners (i.e., at the meta level). The Machine Learning Toolbox project provides a set of learning algorithms as individual tools without much interaction among them.
Reference: [19] <author> S. Stolfo, Z. Galil, K. McKeown, and R. Mills. </author> <title> Speech recognition in parallel. </title> <booktitle> In Proc. Speech Nat. Lang. Work., </booktitle> <pages> pages 353-373. DARPA, </pages> <year> 1989. </year>
Reference-contexts: Recently, several researchers have proposed implementing learning systems by integrating in some fashion a number of different strategies and algorithms to boost overall accuracy <ref> [23, 24, 19] </ref>. The basic notion behind this integration is to complement the different underlying learning strategies embodied by different learning algorithms by effectively reducing the space of incorrect classifications of a learned concept. There are many ways of integrating different learning algorithms. <p> Our goal is to achieve an overall accuracy that is higher than the accuracy obtained by any of the individual learning algorithms. Similar ideas were first proposed in <ref> [19] </ref> in the domain of speech recognition. The aforementioned approaches used by Wolpert [23] and Zhang et al. [24] are examples of this approach.
Reference: [20] <author> G. Towell, J. Shavlik, and M. Noordewier. </author> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proc. AAAI-90, </booktitle> <pages> pages 861-866, </pages> <year> 1990. </year>
Reference-contexts: The basic notion behind this integration is to complement the different underlying learning strategies embodied by different learning algorithms by effectively reducing the space of incorrect classifications of a learned concept. There are many ways of integrating different learning algorithms. For example, work on integrating inductive and explanation-based learning <ref> [10, 20] </ref> requires a complicated new algorithm that implements both approaches to learning in a single system.
Reference: [21] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Comm. ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: Schapire rigorously proves that the overall accuracy is higher than the one achieved by simply applying the learning algorithm to the initial distribution under the PAC learning model <ref> [21] </ref>. In fact, he shows that arbitrarily high accuracy can be achieved by recursively applying the same procedure. Although his approach is limited to the PAC model of learning, some success was achieved in the domain of character recognition, using neural networks [9].
Reference: [22] <author> B. Wah et al. </author> <title> High performance computing and communications for grand challenge applications: Computer vision, speech and natural language processing, </title> <journal> and artificial intelligence. IEEE Trans. Know. Data. Eng., </journal> <volume> 5(1) </volume> <pages> 138-154, </pages> <year> 1993. </year>
Reference-contexts: With the coming age of very large network computing, it is likely that orders of magnitude more data in databases will be available for various learning problems of real world importance. The Human Genome Project [8] and Grand Challenges of HPCC <ref> [22] </ref> are perhaps the best examples of large scale efforts demanding efficient knowledge discovery systems. Parallel and distributed processing would substantially increase the speed of processing data and hence the amount of data a knowledge discovery system can handle effectively.
Reference: [23] <author> D. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259, </pages> <year> 1992. </year>
Reference-contexts: Recently, several researchers have proposed implementing learning systems by integrating in some fashion a number of different strategies and algorithms to boost overall accuracy <ref> [23, 24, 19] </ref>. The basic notion behind this integration is to complement the different underlying learning strategies embodied by different learning algorithms by effectively reducing the space of incorrect classifications of a learned concept. There are many ways of integrating different learning algorithms. <p> One advantage of this approach is its simplicity in treating the individual learning systems as black boxes with little or no modification required to achieve a final system. Therefore, individual systems can be added or replaced with relative ease. Wolpert <ref> [23] </ref> presents a theory of stacked generalization (meta-learning). Several (level 0) classifiers are first learned from the same training set. The predictions made by these classifiers on the training set and the correct classifications form the training set of the next level (level 1) classifier. <p> Our goal is to achieve an overall accuracy that is higher than the accuracy obtained by any of the individual learning algorithms. Similar ideas were first proposed in [19] in the domain of speech recognition. The aforementioned approaches used by Wolpert <ref> [23] </ref> and Zhang et al. [24] are examples of this approach. In the next section we will discuss our approach to using meta-learning for multistrategy hypothesis boosting. 3 Multistrategy Hypothesis Boosting The objective here is to improve prediction accuracy by exploring the diversity of multiple learning algorithms through meta-learning. <p> lamp attrvec 3 Training set for the meta-different-class-attribute hybrid scheme Class Attribute vector chair (table, chair, lamp, attrvec 2 ) lamp (lamp, chair, chair, attrvec 3 ) f (class (x); C 1 (x); C 2 (x); C 3 (x)) j x 2 Eg: This scheme was also used by Wolpert <ref> [23] </ref>. (For further reference, this scheme is denoted as meta-class.) A sample training set is displayed in Figure 3. 2. <p> One possible solution is to increase the number of base classifiers to lower the percentage of having one or none correct predictions. 6 Discussion Unlike Wolpert <ref> [23] </ref> and Zhang et al.'s [24] reports, we present results from all the combinations of presented strategies, base learners, and meta-learners.
Reference: [24] <author> X. Zhang, J. Mesirov, and D. Waltz. </author> <title> A hybrid system for protein secondary structure prediction. </title> <journal> J. Mol. Biol., </journal> <volume> 225 </volume> <pages> 1049-1063, </pages> <year> 1992. </year>
Reference-contexts: Recently, several researchers have proposed implementing learning systems by integrating in some fashion a number of different strategies and algorithms to boost overall accuracy <ref> [23, 24, 19] </ref>. The basic notion behind this integration is to complement the different underlying learning strategies embodied by different learning algorithms by effectively reducing the space of incorrect classifications of a learned concept. There are many ways of integrating different learning algorithms. <p> When an instance is being classified, the level 0 classifiers first make their predictions on the instance. The predictions are then presented to the level 1 classifier, which makes the final prediction. Zhang et al.'s <ref> [24] </ref> work utilizes a similar approach to learn a combiner based on the predictions made by three different classifiers. The work from the two groups demonstrates that meta-learning can improve overall prediction accuracy. <p> Our goal is to achieve an overall accuracy that is higher than the accuracy obtained by any of the individual learning algorithms. Similar ideas were first proposed in [19] in the domain of speech recognition. The aforementioned approaches used by Wolpert [23] and Zhang et al. <ref> [24] </ref> are examples of this approach. In the next section we will discuss our approach to using meta-learning for multistrategy hypothesis boosting. 3 Multistrategy Hypothesis Boosting The objective here is to improve prediction accuracy by exploring the diversity of multiple learning algorithms through meta-learning. <p> The names of meta-learning schemes are abbreviated in the tables: m-c represents meta-class, m-c-a represents meta-class-attribute, and so on. Results for the two data sets with single-strategy classifiers are displayed in Table 3. In addition, we experimented with a windowing scheme used in Zhang's work <ref> [24] </ref>, which is specific to the SS data. This scheme is similar to the meta-class scheme described above. <p> For the SS data, almost all the results did not outperform BAYES as a single-strategy learner. In general, the combiner strategies performed more effectively than the arbiter and hybrid strategies. To our surprise, the hybrid schemes did not improve the arbiter strategies. In addition, Zhang's <ref> [24] </ref> meta-class-window strategy for the SS data did not improve accuracy with the base and meta-learners used here. His study employed a neural net algorithm and different Bayesian and nearest-neighbor learners than those reported here. <p> One possible solution is to increase the number of base classifiers to lower the percentage of having one or none correct predictions. 6 Discussion Unlike Wolpert [23] and Zhang et al.'s <ref> [24] </ref> reports, we present results from all the combinations of presented strategies, base learners, and meta-learners.
References-found: 24

