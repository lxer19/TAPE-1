URL: ftp://ftp.cs.umass.edu/pub/lesser/sandholm-ijcai95ws-learning.ps
Refering-URL: http://dis.cs.umass.edu/lesserpubs.html
Root-URL: 
Email: fsandholm, critesg@cs.umass.edu  
Title: On Multiagent Q-Learning in a Semi-competitive Domain  longer exploration schedules fared best in the  
Author: Tuomas W. Sandholm and Robert H. Crites IPD games. 
Note: Supported by ARPA contract N00014-92-J-1698, Finnish Culture Foundation, Honkanen Foundation, and Ella and George Ehrnrooth Foundation. The content does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. An extended version will appear in (Sandholm and Crites, 95). Supported by Air Force Office of Scientific Research Grant F49620-93-1-0269.  
Address: Amherst, MA 01003  
Affiliation: University of Massachusetts at Amherst Computer Science Department  
Abstract: Q-learning is a recent reinforcement learning (RL) algorithm that does not need a model of its environment and can be used on-line. Therefore it is well-suited for use in repeated games against an unknown opponent. Most RL research has been confined to single agent settings or to multiagent settings where the agents have totally positively correlated payoffs (team problems) or totally negatively correlated payoffs (zero-sum games). This paper is an empirical study of reinforcement learning in the iterated prisoner's dilemma (IPD), where the agents' payoffs are neither totally positively nor totally negatively correlated. RL is considerably more difficult in such a domain. This paper investigates the ability of a variety of Q-learning agents to play the IPD game against an unknown opponent. In some experiments, the opponent is the fixed strategy Tit-for-Tat, while in others it is another Q-learner. All the Q-learners learned to play optimally against Tit-for-Tat. Playing against another learner was more difficult because the adaptation of the other learner creates a nonstationary environment in ways that are detailed in the paper. The learners that were studied varied along three dimensions: the length of history they received as context, the type of memory they employed (lookup tables based on restricted history windows or recurrent neural networks (RNNs) that can theoretically store features from arbitrarily deep in the past), and the exploration schedule they followed. Although all the learners faced difficulties when playing against other learners, agents with longer history windows, lookup table memories, and 
Abstract-found: 1
Intro-found: 1
Reference: [ 1 ] <author> Axelrod, R. </author> <year> 1984. </year> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, </publisher> <address> New York, NY. </address>
Reference: [ 2 ] <author> Barto, A. G., Sutton, R., and Anderson, C. W. </author> <year> 1983. </year> <title> Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems. </title> <journal> IEEE Trans. Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 834-846. </pages>
Reference: [ 3 ] <author> Crites, R. </author> <year> 1994. </year> <title> Multi-Agent Reinforcement Learning. </title> <type> PhD dissertation proposal. </type> <institution> Comp. Sci. Dept., Univ. of Mass. at Amherst. </institution>
Reference: [ 4 ] <author> Elman, J. </author> <year> 1990. </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211. </pages>
Reference: [ 5 ] <author> Haykin, S. </author> <year> 1994. </year> <title> Neural Networks: A Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York. </address>
Reference: [ 6 ] <author> Kinney, M. and Tsatsoulis, C. </author> <year> 1993. </year> <title> Learning Communication Strategies in Distributed Agent Environments. </title> <note> Working paper WP-93-4. </note> <institution> Intelligent Design Lab., Univ. of Kansas. </institution>
Reference: [ 7 ] <author> Kreps, D. </author> <year> 1990. </year> <title> A Course in Microeconomic Theory. </title> <publisher> Princeton Univ. Press, </publisher> <address> Princeton, NJ. </address>
Reference: [ 8 ] <author> Lin, L-J. </author> <year> 1993. </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> Ph.D. dissertation, </type> <institution> School of Comp. Sci., </institution> <address> CMU. </address>
Reference: [ 9 ] <author> Littman, M. </author> <year> 1993. </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> Machine Learning, Proc. 11th International Conf., </booktitle> <pages> pp. 157-163, </pages> <institution> Rutgers University, NJ. </institution>
Reference: [ 10 ] <author> Littman, M. and Boyan, J. </author> <year> 1993. </year> <title> A Distributed Reinforcement Learning Scheme for Network Routing. </title> <type> TR CS-93-165, CMU. </type>
Reference: [ 11 ] <author> Markey, K. L. </author> <year> 1993. </year> <title> Efficient Learning of Multiple Degree-of-Freedom Control Problems with Quasi-independent Q-agents. </title> <booktitle> Proc. Connectionist Models Summer School. </booktitle> <publisher> Erlbaum Associates, </publisher> <address> NJ. </address>
Reference: [ 12 ] <author> Samuel, A. L. </author> <year> 1963. </year> <title> Some studies in machine learning using the game of checkers. </title> <editor> In E. A. Feigenbaum and J. Feldman, eds., </editor> <booktitle> Computers and Thought, </booktitle> <publisher> McGraw-Hill, </publisher> <address> NY, </address> <year> 1963. </year>
Reference: [ 13 ] <author> Sandholm, T. W. </author> <year> 1993. </year> <title> An Implementation of the Contract Net Protocol Based on Marginal Cost Calculations. </title> <booktitle> AAAI-93, </booktitle> <pages> pp. 256-262, </pages> <address> Washington D.C. </address>
Reference: [ 14 ] <author> Sandholm, T. W. and Lesser, V. </author> <year> 1995. </year> <title> Issues in Automated Negotiation and Electronic Commerce: Extending the Contract Net Framework. </title> <booktitle> Proc. 1st Intl. Conf. on Multiagent Systems (ICMAS-95). </booktitle>
Reference: [ 15 ] <author> Sandholm, T. W. and Crites, R. H. </author> <year> 1995. </year> <title> Multia-gent Reinforcement Learning in the Iterated Prisoner's Dilemma. Biosystems, Special issues on the prisoner's dilemma. </title> <note> To appear. </note>
Reference: [ 16 ] <author> Sandholm, T. W. and Nagendraprasad, M. V. </author> <year> 1993. </year> <title> Learning Pursuit Strategies. Class project for Cmp-Sci 689 Machine Learning. </title> <institution> Comp. Sci. Dept., Univ. of Mass. at Amherst, </institution> <month> Spring </month> <year> 1993. </year>
Reference: [ 17 ] <author> Sen, S., Sekaran, M. and Hale, J. </author> <year> 1994. </year> <title> Learning to coordinate without sharing information. </title> <booktitle> AAAI-94, </booktitle> <pages> pp. 426-431, </pages> <address> Seattle, Washington. </address>
Reference: [ 18 ] <author> Shoham, Y. and Tennenholtz, M. </author> <year> 1993. </year> <title> Co-Learning and the Evolution of Coordinated Multi-Agent Activity. </title>
Reference: [ 19 ] <author> Sugawara, T. and Lesser, V. </author> <year> 1993. </year> <title> On-Line Learning of Coordination Plans. </title> <institution> Comp. Sci. TR-93-27, Univ. </institution> <address> of Mass., Amherst. </address>
Reference: [ 20 ] <author> Sutton, R. S. </author> <year> 1988. </year> <title> Learning to Predict by the Methods of Temporal Differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference: [ 21 ] <author> Tan, M. </author> <year> 1993. </year> <title> Multi-Agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents. Machine Learning, Proc. 10th International Conf., </booktitle> <pages> pp. 330-337, </pages> <address> Univ. of Mass., Amherst. </address>
Reference: [ 22 ] <author> Tesauro, G. J. </author> <year> 1992. </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 257-277. </pages>
Reference: [ 23 ] <author> Watkins, C. </author> <year> 1989. </year> <title> Learning from delayed rewards. </title> <type> PhD Thesis, </type> <institution> University of Cambridge, </institution> <address> England. </address>
Reference: [ 24 ] <author> Wei, G. </author> <year> 1993. </year> <title> Learning to Coordinate Actions in Multi-Agent Systems. </title> <booktitle> IJCAI-93, </booktitle> <pages> pp. 311-316, </pages> <address> Chambery, France. </address>
References-found: 24

