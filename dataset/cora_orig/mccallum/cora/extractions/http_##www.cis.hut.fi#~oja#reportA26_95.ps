URL: http://www.cis.hut.fi/~oja/reportA26_95.ps
Refering-URL: http://www.cis.hut.fi/~oja/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: erkki.oja@hut.fi  
Phone: Fax: +358-0-4513277  
Title: The Nonlinear PCA Learning Rule and Signal Separation Mathematical Analysis  
Author: Erkki Oja 
Note: ISBN 951-22-2706-1 ISSN 0783-7445 TKK OFFSET  
Date: (August 1995)  
Address: Rakentajanaukio 2 C, FIN-02150 Espoo, Finland  
Affiliation: Helsinki University of Technology Laboratory of Computer and Information Science  
Pubnum: Report A26  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P. Comon, </author> <title> "Independent component analysis anew concept?," </title> <booktitle> Signal Processing, </booktitle> <volume> 36, </volume> <month> 287-314 </month> <year> (1994). </year>
Reference-contexts: For review articles on these techniques, see [2], [4], <ref> [1] </ref>. Neural learning algorithms have been recently presented by a number of authors, see [3], [5], [10]. The full implications of the Nonlinear PCA learning algorithm, together with other related learning rules, in these practical signal processing problems will be shown elsewhere [18, 9]. We only give here an example. <p> The latter case is then more stable. The same effect was discussed in the example of Section 2. A simple example of a sub-Gaussian density is the uniform density on <ref> [1; 1] </ref>. Let us assume this for the elements of u to illustrate the Theorem 1. Condition 1 of Theorem 1 is then satisfied. It remains to check the stability conditions 4 and 5 of Theorem 1.
Reference: [2] <author> J. Friedman, </author> <title> "Exploratory projection pursuit," </title> <journal> J. Amer. Statistical Assoc., </journal> <volume> 82, no. 397, </volume> <month> 249-266 </month> <year> (1987). </year>
Reference-contexts: For review articles on these techniques, see <ref> [2] </ref>, [4], [1]. Neural learning algorithms have been recently presented by a number of authors, see [3], [5], [10]. The full implications of the Nonlinear PCA learning algorithm, together with other related learning rules, in these practical signal processing problems will be shown elsewhere [18, 9].
Reference: [3] <author> C. Fyfe, D.R. McGregor, and R. Baddeley, </author> <title> "Exploratory projection pursuit: an artificial neural network approach," </title> <institution> Department of Computer Science, University of Strathclyde (Glasgow, Scotland), </institution> <note> Research Report /94/160, </note> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: For review articles on these techniques, see [2], [4], [1]. Neural learning algorithms have been recently presented by a number of authors, see <ref> [3] </ref>, [5], [10]. The full implications of the Nonlinear PCA learning algorithm, together with other related learning rules, in these practical signal processing problems will be shown elsewhere [18, 9]. We only give here an example. Example 1 (from [18]).
Reference: [4] <author> P. Huber, </author> <title> "Projection pursuit," </title> <journal> The Annals of Statistics, </journal> <volume> 13, no. 2, </volume> <month> 435-475 </month> <year> (1985). </year>
Reference-contexts: For review articles on these techniques, see [2], <ref> [4] </ref>, [1]. Neural learning algorithms have been recently presented by a number of authors, see [3], [5], [10]. The full implications of the Nonlinear PCA learning algorithm, together with other related learning rules, in these practical signal processing problems will be shown elsewhere [18, 9]. <p> Thus, the orientation of w would be aligned with the non-Gaussian (horizontal) axis. This is the goal in Exploratory Projection Pursuit <ref> [4] </ref>: to find orientations in the input space that are as non-Gaussian as possible. For connections of nonlinear PCA and Projection Pursuit, see [18].
Reference: [5] <author> C. Jutten and J. Herault, </author> <title> "Blind separation of sources, part I: an adaptive algorithm based on neuromimetic architecture," </title> <booktitle> Signal Processing, </booktitle> <volume> 24, no. 1, </volume> <month> 1-10 </month> <year> (1991). </year>
Reference-contexts: For review articles on these techniques, see [2], [4], [1]. Neural learning algorithms have been recently presented by a number of authors, see [3], <ref> [5] </ref>, [10]. The full implications of the Nonlinear PCA learning algorithm, together with other related learning rules, in these practical signal processing problems will be shown elsewhere [18, 9]. We only give here an example. Example 1 (from [18]).
Reference: [6] <author> J. Karhunen and J. Joutsensalo, </author> <title> "Representation and separation of signals using nonlinear PCA type learning," </title> <booktitle> Neural Networks, </booktitle> <volume> 7, no. 1, </volume> <month> 113-127 </month> <year> (1994). </year>
Reference-contexts: This rule can be implemented in a one-layer Nonlinear PCA network shown in Fig. 1. In [16], these rules were not motivated from optimization criteria, nor were they applied to realistic signal processing problems. In two recent articles <ref> [6, 7] </ref>, it was shown by Karhunen and Joutsensalo that the rule (4) is an approximate stochastic gradient algorithm for minimizing the mean square representation error J (W) = Efkx Wg (W T x)k 2 g = Efkx m X g (w (j) T x)w (j)k 2 g: (7) The result <p> was shown by Karhunen and Joutsensalo that the rule (4) is an approximate stochastic gradient algorithm for minimizing the mean square representation error J (W) = Efkx Wg (W T x)k 2 g = Efkx m X g (w (j) T x)w (j)k 2 g: (7) The result given in <ref> [6] </ref> is that the stochastic gradient algorithm for the cost function (7) is k W k F (x T k W k )] (8) with e k = x k W k g (W T and k W k ) = diag [g 0 (x T k w k (1)); :::; <p> As motivated in more detail in <ref> [6] </ref>, writing the update rule (8) for an individual weight vector w k (i) shows that the first term in the brackets on the right hand side of algorithm (8) 3 affects the update of weight vectors much less than the second term, if the error e k is relatively small
Reference: [7] <author> J. Karhunen and J. Joutsensalo, </author> <title> "Generalizations of principal component analysis, optimization problems, and neural networks," </title> <booktitle> Neural Networks, </booktitle> <volume> 8, no. 4, </volume> <month> 549 - 562 </month> <year> (1995). </year>
Reference-contexts: This rule can be implemented in a one-layer Nonlinear PCA network shown in Fig. 1. In [16], these rules were not motivated from optimization criteria, nor were they applied to realistic signal processing problems. In two recent articles <ref> [6, 7] </ref>, it was shown by Karhunen and Joutsensalo that the rule (4) is an approximate stochastic gradient algorithm for minimizing the mean square representation error J (W) = Efkx Wg (W T x)k 2 g = Efkx m X g (w (j) T x)w (j)k 2 g: (7) The result
Reference: [8] <author> J. Karhunen, L. Wang, and J. Joutsensalo, </author> <title> "Neural estimation of basis vectors in Independent Component Analysis," </title> <booktitle> To appear in Proc. Int. Conf. on Artificial Neural Networks (Paris, </booktitle> <address> France, </address> <year> 1995). </year>
Reference-contexts: It depends then on the initial value where the weight vector will converge. 4 Conclusions The purpose here was to give rigorous mathematical results on the Nonlinear PCA learning rule, with little experimental validation outside artificial examples. More detailed experiments with real data will be reported elsewhere <ref> [8] </ref>, [9], [19]. These papers also review and introduce a number of other related neural network algorithms for computing the independent components, like the Bigradient learning rule that is minimizing or maximizing a cost function directly related to higher-order cumulants like the kurtosis.
Reference: [9] <author> J. Karhunen, L. Wang and R. Vigario, </author> <title> "Nonlinear PCA type approaches for source separation and independent component analysis," </title> <booktitle> To appear in Proc. Int. Conf. on Neural Networks (Perth, </booktitle> <address> Australia, </address> <year> 1995). </year>
Reference-contexts: Neural learning algorithms have been recently presented by a number of authors, see [3], [5], [10]. The full implications of the Nonlinear PCA learning algorithm, together with other related learning rules, in these practical signal processing problems will be shown elsewhere <ref> [18, 9] </ref>. We only give here an example. Example 1 (from [18]). We construct here the input vector x as a combination of three digital images, shown in the uppermost row in Fig. 2. <p> It depends then on the initial value where the weight vector will converge. 4 Conclusions The purpose here was to give rigorous mathematical results on the Nonlinear PCA learning rule, with little experimental validation outside artificial examples. More detailed experiments with real data will be reported elsewhere [8], <ref> [9] </ref>, [19]. These papers also review and introduce a number of other related neural network algorithms for computing the independent components, like the Bigradient learning rule that is minimizing or maximizing a cost function directly related to higher-order cumulants like the kurtosis.
Reference: [10] <author> B. Laheld and J.-F. Cardoso, </author> <title> "Adaptive source separation with uniform performance," </title> <booktitle> in Signal Processing VII: Theories and Applications (Proc. </booktitle> <editor> EUSIPCO-94), eds. M. Holt et al. (Lausanne: EURASIP, </editor> <booktitle> 1994, </booktitle> <volume> vol. 2, </volume> <pages> pp. 183-186). </pages>
Reference-contexts: For review articles on these techniques, see [2], [4], [1]. Neural learning algorithms have been recently presented by a number of authors, see [3], [5], <ref> [10] </ref>. The full implications of the Nonlinear PCA learning algorithm, together with other related learning rules, in these practical signal processing problems will be shown elsewhere [18, 9]. We only give here an example. Example 1 (from [18]).
Reference: [11] <author> C. Nikias and J. Mendel, </author> <title> "Signal processing with higher-order spectra," </title> <journal> IEEE Signal Proc. Mag., </journal> <month> July </month> <year> 1993, </year> <month> 10 - 37 </month> <year> (1993). </year>
Reference-contexts: For super-Gaussian densities that are "sharper" than the Gaussian, e.g. the Laplacian or two-sided exponential density, minimization of the criterion J (W) would result in the most dependent directions. Note that uniform density is sub-Gaussian or broader than the Gaussian; the relevant measure here is the kurtosis (see e.g. <ref> [11] </ref>), defined for a random variable y as kur (y) = Efy 4 g (Efy 2 g) 2 : (17) The kurtosis is negative for sub-Gaussians, zero for Gaussians, and positive for super-Gaussians. The example was also dependent on the nonlinearity g (y). <p> Consider next the case s = 3; g (u) = u 3 : (39) Now (37) gives Efu 4 g 3 (Efu 2 g) 2 &gt; 0: (40) This expression is exactly the kurtosis or the fourth order cumulant of u <ref> [11] </ref>. If and only if the density is positively kurtotic or super-Gaussian, this condition is satisfied and the cubic polynomial g (u) = u 3 gives asymptotic stability. Likewise, for s = 5 we get the condition Efu 6 g 5Efu 2 gEfu 4 g &gt; 0; (41) etc. <p> Likewise, for s = 5 we get the condition Efu 6 g 5Efu 2 gEfu 4 g &gt; 0; (41) etc. While this expression is not directly a cumulant, it is related to the second, fourth, and sixth order cumulants. The sixth order cumulant is <ref> [11] </ref> cum 6 (u) = Efu 6 g 15Efu 2 gEfu 4 g + 30 (Efu 2 g) 3 (42) = Efu 6 g 5Efu 2 gEfu 4 g 10Efu 2 gEfu 4 g + 30 (Efu 2 g) 3 (43) = Efu 6 g 5Efu 2 gEfu 4 g 10Efu
Reference: [12] <author> E. Oja, </author> <title> "A simplified neuron model as a principal component analyzer," </title> <journal> J. Math. Biol., </journal> <volume> 15, </volume> <month> 267-273 </month> <year> (1982). </year>
Reference-contexts: 1 Introduction The PCA neuron <ref> [12] </ref> and the PCA network [15, 17] were introduced by the author with the purpose of demonstrating how the well-known Principal Component Analysis expansion can be realized in a simple parallel layer of adaptive elements. These artificial neurons were linear because PCA is an inherently linear technique.
Reference: [13] <author> E. Oja, </author> <title> Subspace Methods of Pattern Recognition. </title> <address> Letchworth, England: </address> <publisher> Research Studies Press, </publisher> <address> and New York, NY: </address> <publisher> J. Wiley (1983). </publisher> <pages> 24 </pages>
Reference-contexts: In view of the original problem, in which the amplitudes of the signals s k (i) remain unknown, this is actually no restriction. To proceed, the difference equation (22) can be further analyzed by writing down the corresponding averaged differential equation; for a discussion of the technique, see e.g. <ref> [13] </ref>. The limit of convergence of the difference equation is among the asymptotically stable solutions of the averaged differential equation.
Reference: [14] <author> E. Oja and J. Karhunen, </author> " <title> On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix," </title> <journal> J. Math. Anal. Appl. </journal> <volume> 106, </volume> <month> 69-84 </month> <year> (1985). </year>
Reference-contexts: It can be shown that under conditions specified in <ref> [14] </ref>, w k tends to the dominant eigenvector of the input correlation matrix.
Reference: [15] <author> E. Oja, </author> <title> "Neural networks, principal components, and subspaces," </title> <journal> Int. J. Neural Systems, </journal> <volume> 1, </volume> <month> 61-68 </month> <year> (1989). </year>
Reference-contexts: 1 Introduction The PCA neuron [12] and the PCA network <ref> [15, 17] </ref> were introduced by the author with the purpose of demonstrating how the well-known Principal Component Analysis expansion can be realized in a simple parallel layer of adaptive elements. These artificial neurons were linear because PCA is an inherently linear technique. <p> It can be shown that under conditions specified in [14], w k tends to the dominant eigenvector of the input correlation matrix. The extension of (1) to a set of m parallel neural units (the Subspace rule, <ref> [15] </ref>) is: k (2) where now W k = [w k (1) w k (2) ::: w k (m)] is the weight matrix, whose columns are the individual neuron weight vectors w k (i), and y k = W T k x k is the output vector of m elements.
Reference: [16] <author> E. Oja, H. Ogawa, and J. Wangviwattana, </author> " <title> Learning in nonlinear constrained Hebbian networks," In Artificial Neural Networks, </title> <editor> eds. T. Kohonen, K. Mkisara, O. Simula, and J. Kangas, </editor> <publisher> (Amsterdam: Elsevier, </publisher> <month> 385 - 390 </month> <year> (1991)). </year>
Reference-contexts: It was obvious from the start that exactly the same learning rules and implementations could be extended in a straightforward way to nonlinear neurons; some such extensions were proposed in <ref> [16] </ref>. <p> This rule can be implemented in a one-layer Nonlinear PCA network shown in Fig. 1. In <ref> [16] </ref>, these rules were not motivated from optimization criteria, nor were they applied to realistic signal processing problems.
Reference: [17] <author> E. Oja, </author> <title> "Principal components, minor components, and linear neural networks," Neural Networks, </title> <type> 5, </type> <month> 927 - 935 </month> <year> (1992). </year>
Reference-contexts: 1 Introduction The PCA neuron [12] and the PCA network <ref> [15, 17] </ref> were introduced by the author with the purpose of demonstrating how the well-known Principal Component Analysis expansion can be realized in a simple parallel layer of adaptive elements. These artificial neurons were linear because PCA is an inherently linear technique. <p> This algorithm and some extensions were analyzed in <ref> [17] </ref>. Now the weight vectors w k (i) become orthonormal and tend to a basis of the m-dimensional dominant eigenvector subspace of the input correlation matrix; however, usually the individual weight vectors do not tend to the eigenvectors. <p> Inequality (37) is satisfied as an equality, however, which means that the solution in this case is stable but not asymptotically stable. In fact the stability of the linear function (when the nonlinear PCA learning rule becomes the Subspace rule) has been already established earlier, see e.g. <ref> [17] </ref>. Consider next the case s = 3; g (u) = u 3 : (39) Now (37) gives Efu 4 g 3 (Efu 2 g) 2 &gt; 0: (40) This expression is exactly the kurtosis or the fourth order cumulant of u [11]. <p> While the linear network finds the PCA subspace, and some versions of the network like the SGA (see e.g. <ref> [17] </ref>) also find the individual principal component directions, the nonlinear network is not as good in PCA.
Reference: [18] <author> E. Oja, J. Karhunen, L. Wang, and R. Vigario, </author> <title> "Principal and independent components in neural networks recent developments," </title> <booktitle> To appear in Proc. Italian Workshop on Neural Networks WIRN'95 (Vietri, </booktitle> <address> Italy, </address> <year> 1995). </year>
Reference-contexts: Neural learning algorithms have been recently presented by a number of authors, see [3], [5], [10]. The full implications of the Nonlinear PCA learning algorithm, together with other related learning rules, in these practical signal processing problems will be shown elsewhere <ref> [18, 9] </ref>. We only give here an example. Example 1 (from [18]). We construct here the input vector x as a combination of three digital images, shown in the uppermost row in Fig. 2. <p> The full implications of the Nonlinear PCA learning algorithm, together with other related learning rules, in these practical signal processing problems will be shown elsewhere [18, 9]. We only give here an example. Example 1 (from <ref> [18] </ref>). We construct here the input vector x as a combination of three digital images, shown in the uppermost row in Fig. 2. <p> The outputs are shown on the fourth row of Fig. 2. Full details of the implementation are given in <ref> [18] </ref>. <p> Thus, the orientation of w would be aligned with the non-Gaussian (horizontal) axis. This is the goal in Exploratory Projection Pursuit [4]: to find orientations in the input space that are as non-Gaussian as possible. For connections of nonlinear PCA and Projection Pursuit, see <ref> [18] </ref>. The purpose of this subsection is to give a similar result to Theorem 1 for the one-unit case, which could then be analyzed in a number of special cases.
Reference: [19] <author> L. Wang, J. Karhunen, E. Oja, and R. Vigario, </author> <title> "Blind separation of sources using nonlinear PCA type learning algorithms," </title> <booktitle> To appear in Proc. Int. Conf. on Neural Networks and Signal Processing (Nanjing, P.R. </booktitle> <address> China, </address> <year> 1995). </year> <month> 25 </month>
Reference-contexts: It depends then on the initial value where the weight vector will converge. 4 Conclusions The purpose here was to give rigorous mathematical results on the Nonlinear PCA learning rule, with little experimental validation outside artificial examples. More detailed experiments with real data will be reported elsewhere [8], [9], <ref> [19] </ref>. These papers also review and introduce a number of other related neural network algorithms for computing the independent components, like the Bigradient learning rule that is minimizing or maximizing a cost function directly related to higher-order cumulants like the kurtosis.
References-found: 19

