URL: http://www.cs.ucsd.edu/groups/csl/pubs/phd/ewa.thesis.ps
Refering-URL: http://www.cs.ucsd.edu/groups/csl/pubs/author.html
Root-URL: http://www.cs.ucsd.edu
Title: Container Shipping: A Uniform Interface for Fast, Efficient, High-Bandwidth I/O  
Author: Eric W. Anderson 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree Doctor of Philosophy in Computer Science by  Committee in charge: Professor Joseph C. Pasquale, Chair Professor Rene Cruz Professor Ramesh Jain Professor Keith Marzullo Professor F. Richard Moore Professor George Polyzos  
Date: 1995  
Affiliation: UNIVERSITY OF CALIFORNIA, SAN DIEGO  
Abstract-found: 0
Intro-found: 1
Reference: [AF88] <author> David P. Anderson, Domenico Ferrari, </author> <title> The DASH Project: An Overview, </title> <type> Technical Report CSD-88-405, </type> <institution> Computer Science Division, Department of Electrical Engineering and Computer Science, University of California, Berkeley, </institution> <month> February </month> <year> 1988. </year>
Reference-contexts: The basic technique of raising I/O performance by avoiding in-memory copies is well known. Copy elimination of some form or another is discussed or implemented in operating systems and system services such as Accent [RR81], Mach [FGB91], DASH <ref> [AF88] </ref>, fbufs [DP93], UCB mmap [LMKQ89], ACME [GA91], and the Alloc Stream Facility [KS94]. All of these approaches have limitations, some of which are more significant than others. Most support some, but not all, kinds of I/O. <p> A key feature of DASH was high-performance IPC, with fast local IPC via page remapping. Most of the DASH papers describe preliminary work <ref> [AF88] </ref> [ATG88]. However, DASH did include a data transfer mechanism for I/O that used virtual page remapping to avoid in-memory copies. The reported performance results primarily describe fast inter-process communication, rather than device I/O [TA91]. Local IPC performance was very high due to page remapping. DASH provided message-passing IPC.
Reference: [AT88] <author> David P. Anderson, Shin-Yuan Tzou, </author> <title> The DASH Local Kernel Structure, </title> <type> Technical Report CSD-88-463, </type> <institution> Computer Science Division, Department of Electrical Engineering and Computer Science, University of California, Berkeley, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: Device streaming: Data never enters main memory. I/O is con trolled indirectly by the kernel and the process. User Kernel Device Buffer Interface Peripheral Buffer Interface Peripheral Data Instructions: copy data Instructions: copy data 23 The DASH system incorporated a plan for general I/O through IPC <ref> [AT88] </ref>. A UOR (User Object Reference) provided a way to refer to paths of communication in a manner similar to that of a Unix file descriptor. These UORs could be used together with IPC page remapping to perform copy-free I/O.
Reference: [ATG88] <author> David P. Anderson, Shin-Yuan Tzou, G. Scott Graham, </author> <title> The DASH Virtual Memory System, </title> <type> Technical Report CSD-88-461, </type> <institution> Computer Science Division, Department of Electrical Engineering and Computer Science, University of California, Berkeley, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: Buffer User Kernel Device Buffer Interface Process gains memory containing data from a read operation: Process loses memory containing data for a write operation: 19 appear and vanish. Container Shipping, DASH <ref> [ATG88] </ref>, fbufs [DP93], and the mmap facility [LMKQ89] all employ some form of page transfer or sharing to eliminate copies. DASH and fbufs are discussed in detail in the next two sections. The mmap memory-mapped file facility is not a general-purpose I/O mechanism. <p> A key feature of DASH was high-performance IPC, with fast local IPC via page remapping. Most of the DASH papers describe preliminary work [AF88] <ref> [ATG88] </ref>. However, DASH did include a data transfer mechanism for I/O that used virtual page remapping to avoid in-memory copies. The reported performance results primarily describe fast inter-process communication, rather than device I/O [TA91]. Local IPC performance was very high due to page remapping. DASH provided message-passing IPC.
Reference: [BCD+95] <author> Carlo Basile, Alan P. Cavallerano, Michael S. Deiss, Robert Keeler, Jae S. Lim, Wayne C. Luplow, Woo H. Paik, Eric Petajan, Robert Rast, Glenn Reitmeier, Terrence R. Smith and Craig Todd, </author> <title> The U.S. HDTV Standard: The Grand Alliance, </title> <journal> IEEE Spectrum, </journal> <volume> Volume 32, Number 4, </volume> <pages> pp. 36-45, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Name Speed, megabits/s Type 1976 Ethernet 3, 10 CSMA/CD 1985 IBM Token Ring 4, 16 Token Ring 1992 FDDI 100 Token Ring 1994 Fast Ethernet 100 CSMA/CD 1994 ATM 155 Cell Switched 1995? ATM 622 Cell Switched 37 specifies 60 frames per second, each composed of 1920 by 1080 pixels <ref> [BCD+95] </ref>. Raw video at this rate, with four bytes of color information per pixel, consumes 474 megabytes per second of bandwidth. <p> The in-memory copying described in Chapter 2 is a serious problem now, and will become Table 4 Digital media bandwidth requirements [Fur94] <ref> [BCD+95] </ref>.
Reference: [BCE+94] <author> Brian N. Bershad, Craig Chambers, Susan Eggers, Chris Maeda, Dylan McNamee, Przemyslaw Pardyak, Stefan Savage, and Emin Gn Sirer, </author> <title> SPIN - An Extensible Microkernel for Application-specific Operating System Services, </title> <type> Technical Report 94-03-03, </type> <institution> University of Washington, </institution> <address> Seattle, </address> <month> February </month> <year> 1994. </year>
Reference-contexts: Unlike Mach, Container Shipping does not rely on page faults or COW, but does provide general-purpose copy-free I/O to application-level processes. 2.6 Others Two additional recent projects illustrate the points made in this chapter. SPIN is a project that allows user-written spindles to be inserted into the kernel dynamically <ref> [BCE+94] </ref>. SPIN should provide performance gains by moving code closer to the data it acts upon. The authors specifically claim that speedups result from providing direct access to kernel data structures, presumably including I/O data. SPIN also provides an in-kernel hardware abstraction between spindles and devices.
Reference: [BBMT72] <author> D. G. Bobrow, J. D. Burchel, D. L. Murphy, and R. S. Tomlinson, Tenex, </author> <title> A Paged Time Sharing System for the PDP-10, </title> <journal> Communications of the ACM, </journal> <volume> Volume 15, Number 3, </volume> <pages> pp. 135-143, </pages> <month> March </month> <year> 1972. </year> <month> 119 </month>
Reference: [CHKM88] <author> Luis-Felipe Cabrera, Edward Hunter, Michael J. Karels, and David A. </author> <title> Mosher, User-Process Communication Performance in Networks of Computers, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Volume 14, Number 1, </volume> <pages> pp. 38-53, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: After this diversion, the data copy function Copyin is bypassed, but the lowe-level functions proceed unchanged. Container Shipping takes effect above the IP protocol layer, which itself is above the device driver. Kernel function names in OSF/1 are very similar <ref> [CHKM88] </ref>. syscall sendto sendit sosend uiomove Copyin udp_usrreq udp__output udp_cksum ip_output enoutput if_wubaput enstart System-call Container Shipping detected In-memory data copy avoided IP network protocol Device driver 90 and IPC sockets do.
Reference: [CLBL92] <author> Jeff Chase, Hank Levy, Miche Baker-Harvey, Ed Lazowska, Opal: </author> <title> A Single Address Space System for 64-bit Architectures, </title> <booktitle> Proceedings of the Third Workshop on Workstation Operating Systems, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 80-85, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: The use of independent virtual address spaces for each process is possible, but not required. Opal is an operating system which provides a single virtual address space for all processes and the kernel, not only on a single system, but across a network of systems <ref> [CLBL92] </ref>. Within independent virtual address spaces, the range of addresses used for remapped pages may be common to all processes, as in DASH [TA91].
Reference: [CJRS89] <author> David Clark, Van Jacobson, John Romkey, and Howard Salwen, </author> <title> An Analysis of TCP Processing Overhead, </title> <journal> IEEE Communications, </journal> <volume> Volume 27, Number 6, </volume> <pages> pp. 23-29, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Second, the FDDI experiment employed much smaller transfers, which had much greater protocol costs than the IPC experiment. Much work has been done on analyzing and improving the performance of protocol processing within TCP/IP <ref> [CJRS89] </ref> [Ste94] [Kay95]. The application of such techniques while tuning Container Shipping should further improve the results.
Reference: [DA92] <author> Randall W. Dean, Francois Armand, </author> <title> Data Movement in Kernelized Systems, </title> <booktitle> Proceedings of the USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <address> Seattle, Washington, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: This copy requires a page fault, which is expensive. The Unix server atop Mach performs in-memory copying for I/O, just like any other Unix implementation <ref> [DA92] </ref>. Mach 3 includes support for user-level device drivers. The designers carefully point out that in this case, user-level means within a kernel layered atop Mach, such as Unix, rather than within application programs layered above such a kernel [FGB91].
Reference: [DEC92] <institution> Digital Equipment Corporation, </institution> <note> DECChip 21064-AA RISC Microprocessor Preliminary Data Sheet, </note> <month> April </month> <year> 1992. </year>
Reference-contexts: The data cache is invalidated by DMA activity to the area of main memory which it represents, if any. Both caches are allocated on read, and the data cache has write-through behavior. Even small block writes from the CPU are therefore limited to the L2 cache speed <ref> [DEC92] </ref> [DEK+92] [DWA+92]. The L2 cache is located in close proximity to the 21064 processor on the system motherboard. This cache is constructed of expensive high speed DRAM. The cache size ranges from 0.5 to 2.0 megabytes in systems marketed to date. <p> The TLB is typically brought up to date by emptying part or all of it, meaning that TLB misses may trigger software to update it, which can consume time [Sit92b] <ref> [DEC92] </ref>. 3.2.3 DMA in 21064 systems The nature of DMA in a system depends significantly on the memory structure that is used. In Alpha systems, devices perform DMA using only physical addresses.
Reference: [DEC94] <institution> Digital Equipment Corporation, </institution> <month> src/kernel/sys/param.h, </month> <title> OSF/1 v2.0 kernel source code, </title> <month> March </month> <year> 1994. </year>
Reference-contexts: BUGS Should more than -PIPE_BUF- bytes be necessary in any pipe among a loop of processes, deadlock will occur. The source code for DECs OSF/1 v2.0 Unix <ref> [DEC94] </ref> also contains a warn ing about the peril of changing pipe behavior: /* If PIPSIZ is set to &lt; 4096 experience shows that many applications * deadlock.
Reference: [DWA+92] <author> Daniel W. Dobberpuhl, Richard T. Witek, Randy Allmon, Robert Anglin, David Bertucci, Sharon Britton, Linda Chao, Robert A. Conrad, Daniel E. Dever, Bruce Gieseke, Soha M.N. Hassoun, Gregory W. Hoeppner, Kathryn Kuchler, Maureen Ladd, Burton M. Leary, Liam Madden, Edward J. McLellan, Derrick R. Meyer, James Montanaro, Donald A. Priore, Vidya Rajagopalan, Sridhar Samudrala, and Sribalan Santhanam, </author> <title> A 200-MHz 64-bit Dual-issue CMOS Microprocessor Digital Technical Journal, </title> <journal> Volume 4, </journal> <volume> Number 4, </volume> <year> 1992. </year>
Reference-contexts: The data cache is invalidated by DMA activity to the area of main memory which it represents, if any. Both caches are allocated on read, and the data cache has write-through behavior. Even small block writes from the CPU are therefore limited to the L2 cache speed [DEC92] [DEK+92] <ref> [DWA+92] </ref>. The L2 cache is located in close proximity to the 21064 processor on the system motherboard. This cache is constructed of expensive high speed DRAM. The cache size ranges from 0.5 to 2.0 megabytes in systems marketed to date. This cache is also direct-mapped, like the L1 cache.
Reference: [Dru94] <author> Peter Druschel, </author> <title> Operating System Support for High-speed Networking, </title> <type> Ph.D. Dissertation, </type> <institution> University of Arizona, </institution> <month> August </month> <year> 1994. </year> <month> 120 </month>
Reference-contexts: Finally, Container Shipping is kernel-independent, because the transfer mechanism is provided without dependencies on the overall design of the I/O system. 2.4 Fbufs The recent fbufs project at the University of Arizona <ref> [Dru94] </ref> has some similarity to Container Shipping. Virtual memory manipulations are used to move data between the kernel and a user-level process. Offset and length information are used to locate data within page-based buffers. The Fbufs mechanism requires that all data be read-only at every point after it enters memory. <p> Sustained copy rates measured for this dissertation include TLB misses and imperfect cache behavior, plus interrupt overhead from a running, though otherwise idle, Unix kernel. Burst transfer rates are higher [Dut93] <ref> [Dru94] </ref>. 43 Chapter 4 Container Shipping This chapter describes Container Shipping in detail. How to use Containers, the system calls, sample code, optimizations beyond copy and map elimination, and how to fit Container Shipping into an existing operating system are all described.
Reference: [DP93] <author> Peter Druschel and Larry Peterson, Fbufs: </author> <title> A High-Bandwidth Cross-Domain Transfer Facility, </title> <booktitle> Proceedings of the 14th Symposium on Operating System Principles, </booktitle> <pages> pp. 189-202, </pages> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: The basic technique of raising I/O performance by avoiding in-memory copies is well known. Copy elimination of some form or another is discussed or implemented in operating systems and system services such as Accent [RR81], Mach [FGB91], DASH [AF88], fbufs <ref> [DP93] </ref>, UCB mmap [LMKQ89], ACME [GA91], and the Alloc Stream Facility [KS94]. All of these approaches have limitations, some of which are more significant than others. Most support some, but not all, kinds of I/O. <p> Buffer User Kernel Device Buffer Interface Process gains memory containing data from a read operation: Process loses memory containing data for a write operation: 19 appear and vanish. Container Shipping, DASH [ATG88], fbufs <ref> [DP93] </ref>, and the mmap facility [LMKQ89] all employ some form of page transfer or sharing to eliminate copies. DASH and fbufs are discussed in detail in the next two sections. The mmap memory-mapped file facility is not a general-purpose I/O mechanism. <p> For I/O between a user-level process and the kernel, this may be true, but for I/O between several mutually suspicious user-level processes, such lax security is a serious problem. Substantial performance gains are reported for a particular device when tested with the fbufs mechanism <ref> [DP93] </ref>. This device is a highly 25 intelligent network adapter, which can be programmed to inspect arriving packets prior to their transfer into memory by DMA. The inspection makes it possible to arrange DMA according to the destination of the arriving packet.
Reference: [DAPP93] <author> Peter Druschel, Mark B. Abbott, Michael A. Pagels, and Larry L. Peterson, </author> <title> Network Subsystem Design, </title> <journal> IEEE Network, </journal> <pages> pp. 8-17, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Various forms of the Table 1 Sustained in-memory copy speeds for several workstation-class systems. Sustained speed indicates sequential copies to and from uncached locations, as commonly found in copy-based I/O <ref> [DAPP93] </ref>. <p> Both of these systems allow an abstract data type at the user level that represents data that spans several noncontiguous blocks of memory. DASH provides a large number of editing calls for manipulating these structures. Work from which the fbufs design evolved briey describes similar data-editing calls <ref> [DAPP93] </ref>. A procedural interface for data manipulation, coupled with a language-dependent data type at the kernel interface, is complicated and inconvenient. The use of a language-dependent interface makes porting to another language difficult.
Reference: [DEK+92] <author> Todd A. Dutton, Daniel Eiref, Hugh R. Kurth, James J. Reisert, and Robin L. </author> <title> Stewart The Design of the DEC 3000 AXP Systems, Two High-performance Workstations, </title> <journal> Digital Technical Journal, </journal> <volume> Volume 4, Number 4, </volume> <year> 1992. </year>
Reference-contexts: The data cache is invalidated by DMA activity to the area of main memory which it represents, if any. Both caches are allocated on read, and the data cache has write-through behavior. Even small block writes from the CPU are therefore limited to the L2 cache speed [DEC92] <ref> [DEK+92] </ref> [DWA+92]. The L2 cache is located in close proximity to the 21064 processor on the system motherboard. This cache is constructed of expensive high speed DRAM. The cache size ranges from 0.5 to 2.0 megabytes in systems marketed to date. This cache is also direct-mapped, like the L1 cache. <p> During DMA operations, the L2 cache is probed (for reads) or updated (for writes) if it matches the portion of main memory targeted by the DMA. The L2 cache is approximately five times slower than the L1 cache, depending on usage patterns <ref> [DEK+92] </ref>. The main memory system is arranged symmetrically around the CPU module, in a complex stack of daughterboards and SIMMs (Single Inline Memory Modules). All main memory can be directly accessed by the kernel without the TLB (Translation Lookaside Buffer), if physical memory addresses are used. <p> All main memory can be directly accessed by the kernel without the TLB (Translation Lookaside Buffer), if physical memory addresses are used. Main memory performance is roughly one third that of the L2 cache, again depending on usage patterns <ref> [DEK+92] </ref> [Sit92b]. 40 3.2.2 Translation Lookaside Buffer The virtual memory hardware and software translate virtual memory address references from the CPU into physical memory targets for the memory system to access. <p> Transfers from a device update main memory and any parts of the L2 cache which match that memory. Inbound transfers also invalidate any lines in the L1 cache that correspond to the transfer region [Sit92b] <ref> [DEK+92] </ref>. An interesting consequence of this DMA design is the potential to automatically load incoming data into the L2 cache without CPU intervention. If data can be used promptly after it arrives, before it is ushed from the L2 cache, the penalty of reading main memory can be avoided.
Reference: [Dut93] <author> Todd A. Dutton, </author> <title> The Design of the DEC 3000 Model 500 AXP Workstation, </title> <booktitle> Proceedings of the Annual IEEE Computer Society International Conference (Compcon), </booktitle> <pages> pp. 449-455, </pages> <year> 1993. </year>
Reference-contexts: Sustained copy rates measured for this dissertation include TLB misses and imperfect cache behavior, plus interrupt overhead from a running, though otherwise idle, Unix kernel. Burst transfer rates are higher <ref> [Dut93] </ref> [Dru94]. 43 Chapter 4 Container Shipping This chapter describes Container Shipping in detail. How to use Containers, the system calls, sample code, optimizations beyond copy and map elimination, and how to fit Container Shipping into an existing operating system are all described.
Reference: [Fall94] <author> Kevin R. </author> <month> Fall, </month> <title> A Peer-to-Peer I/O System in Support of I/O Intensive Workloads, </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science and Engineering, University of California, </institution> <address> San Diego, </address> <year> 1994. </year>
Reference-contexts: Furthermore, the most interesting applications of a high-speed system involve dynamic processing of data, such as video decompression, or silence detection. These functions are usually found only in user-level applications. One system which supports kernel streaming is Splice <ref> [Fall94] </ref>. A variant of kernel streaming allows user-level processes to insert code into the kernel. In this way, functionality lost when the user level was bypassed may be regained. Allowing the dynamic insertion of programs Kernel streaming: Data never enters user-process memory. I/O is controlled indirectly by the process.
Reference: [FO74] <author> R. Feiertag and E. Organick, </author> <title> The MULTICS Input/Output System, </title> <journal> Communications of the ACM, </journal> <volume> Volume 17, Number 7, </volume> <pages> pp. 35-41, </pages> <month> July </month> <year> 1974. </year>
Reference-contexts: A uniform I/O interface often allows code designed to access a particular device to be used with other kinds of devices. A single interface is also easier to understand and use than a collection of incompatible device-specific interfaces. Multics <ref> [FO74] </ref> was one of the first operating systems to support abstract, device-independent I/O. The simplest and most pervasive model for abstract I/O includes two system calls for data transfer.
Reference: [FGB91] <author> Alessandro Forin, David Golub, Brian Bershad, </author> <title> An I/O System for Mach 3.0, </title> <booktitle> Proceedings of the Second USENIX Mach Symposium, </booktitle> <pages> pp. 163-176, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: The basic technique of raising I/O performance by avoiding in-memory copies is well known. Copy elimination of some form or another is discussed or implemented in operating systems and system services such as Accent [RR81], Mach <ref> [FGB91] </ref>, DASH [AF88], fbufs [DP93], UCB mmap [LMKQ89], ACME [GA91], and the Alloc Stream Facility [KS94]. All of these approaches have limitations, some of which are more significant than others. Most support some, but not all, kinds of I/O. <p> Mach 3 includes support for user-level device drivers. The designers carefully point out that in this case, user-level means within a kernel layered atop Mach, such as Unix, rather than within application programs layered above such a kernel <ref> [FGB91] </ref>. Because a kernel running atop Mach may be trusted, these user-level device drivers may not pose a security threat. But because they are not available to actual application-level programs, they do not provide a general-purpose copy-free I/O mechanism. <p> Because this model is unconditionally copy-free, I/O transfer performance never suffers from in-memory copying costs. Performance is further boosted by the absence of expensive copy-on-write mechanisms and time-consuming page faults for lazy mapping, which slow down other approaches to copy-free I/O such as Mach <ref> [FGB91] </ref>. The gain/lose model does have some disadvantages. One issue is the requirement to copy data in memory if it will be written multiple times. Although the memory given away in a cs_write may be unchanged after the cs_write is completed, it cannot be reclaimed.
Reference: [Fur94] <author> Borko Furht, </author> <title> Multimedia Systems: An Overview, </title> <journal> IEEE Multimedia, </journal> <volume> Volume 1, Number 1, </volume> <pages> pp. 47-59, </pages> <month> Spring </month> <year> 1994. </year>
Reference-contexts: Present trends suggest that compression standards such as the various forms of MPEG (Motion Pictures Expert Group) will be nearly universal for video storage and transfer. Such compression reduces the bandwidth for a video signal to 0.2 to 2.5 megabytes per second, depending on the desired video quality <ref> [Fur94] </ref>. Compression reduces the storage requirements and transmission bandwidth needed for digital video. But, video must travel in uncompressed form when it is first recorded and each time it is played back. Video must also be uncompressed in order to apply arbitrary computational processing to the images. <p> Data rates for key audio and video media are listed in Table 4. Raw NTSC (National Television Systems Committee) video, of roughly the quality one would view in broadcast or cable television, requires 30 megabytes per second <ref> [Fur94] </ref>. This rate provides for images of 620 by 560 pixels, with 3 bytes for each pixel, presented 30 times per second. A recently proposed standard for HDTV (High Definition Television) Table 3 Recent performance of workstation-class network interfaces. CSMA/CD stands for Carrier Sense Multiple Access with Collision Detection [Tan88]. <p> The in-memory copying described in Chapter 2 is a serious problem now, and will become Table 4 Digital media bandwidth requirements <ref> [Fur94] </ref> [BCD+95].
Reference: [GA91] <author> Ramesh Govindan, David P. Anderson, </author> <title> Scheduling and IPC Mechanisms for Continuous Media, </title> <booktitle> in Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 68-80, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The basic technique of raising I/O performance by avoiding in-memory copies is well known. Copy elimination of some form or another is discussed or implemented in operating systems and system services such as Accent [RR81], Mach [FGB91], DASH [AF88], fbufs [DP93], UCB mmap [LMKQ89], ACME <ref> [GA91] </ref>, and the Alloc Stream Facility [KS94]. All of these approaches have limitations, some of which are more significant than others. Most support some, but not all, kinds of I/O. Some impose inconvenient usage constraints, require programming language support, or violate the protection usually provided by an operating system. <p> In addition to the shared buffer, each process has its own private memory that is not shared. These processes can communicate asynchronously through the shared memory without kernel or device participation. User Kernel Device 17 user-level process and the kernel can access the data without in-memory copies. ACME <ref> [GA91] </ref> includes a system for shared memory I/O. Although the concept of shared memory I/O is simple, the mechanism is complicated in practice. The user-level process and the kernel must coordinate their access to the data, because there is now only one copy in memory. <p> This system provides both a new system call interface, and a replacement I/O library that can be linked to existing programs. The replacement library provides performance gains in some cases, and the new interface provides greater gains. The basic mechanism is similar to ACME <ref> [GA91] </ref>, with a shared memory buffer, implemented using existing mmap functionality or similar services. For each file descriptor, a buffer is shared between a user-level process and the kernel, with all I/O data for that descriptor placed in the buffer.
Reference: [HP90] <author> John L. Hennessy and David A. Patterson, </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1990. </year> <month> 121 </month>
Reference-contexts: While CPU speed nearly doubles every year, nearly 10 years are required for DRAM speed to double. Figure 10 shows the rate of performance growth for CPU and DRAM components, adapted from that reported by Hennessy and Patterson <ref> [HP90] </ref>. The mismatch between CPU and DRAM speeds has become acute. In the system used for this dissertation, the DEC 3000 model 800, an uncached memory reference by the CPU requires up to 50 clock cycles. During this delay, the CPU must sit idle. <p> From 1980 to 1992, the best CPU speed has grown 280 times more than the best DRAM speed. A vertical log scale is required to see any DRAM speedup at all. The CPU trend shown by Hennessy and Patterson is continued by recent DEC Alpha multiple-issue processors <ref> [HP90] </ref> [UHMB94]. 10,000 1 Year Growth multiple 1,000 Best CPU 1983 1986 1989 1992 1995 DEC 21064 200 MHz x 2 Best DRAM 33 during this time, if there was no delay in accessing memory. To combat this problem, systems are designed with caches of expensive high-speed memory.
Reference: [KP93] <author> Jon Kay and Joseph Pasquale, </author> <title> The importance of non-data touching processing overheads in TCP/IP, </title> <booktitle> Proceedings of the ACM Communications Architectures and Protocols Conference (SIGCOMM), </booktitle> <pages> pp. 259-269, </pages> <address> San Francisco, California, </address> <month> September </month> <year> 1993. </year>
Reference: [Kay95] <author> Jonathan S. Kay, PathIDs: </author> <title> A Mechanism for Reducing Network Software Latency, </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science and Engineering, University of California, </institution> <address> San Diego, </address> <year> 1995. </year>
Reference-contexts: Recent work argues that the disabling of checksums for local area network traffic is safe and practical, and that the majority of all network traffic is local <ref> [Kay95] </ref>. The extent of time savings that results from selective map operations is more difficult to evaluate. Mapping, the establishment in hardware of a relationship between a virtual address and a physical page, is a bookkeeping operation which depends only loosely on the size of the object being mapped. <p> Second, the FDDI experiment employed much smaller transfers, which had much greater protocol costs than the IPC experiment. Much work has been done on analyzing and improving the performance of protocol processing within TCP/IP [CJRS89] [Ste94] <ref> [Kay95] </ref>. The application of such techniques while tuning Container Shipping should further improve the results.
Reference: [KS94] <author> Orran Krieger and Michael Stumm, </author> <title> The Alloc Stream Facility: A Redesign of Application-Level Stream I/O, </title> <journal> IEEE Computer, </journal> <volume> Volume 27, Number 3, </volume> <pages> pp. 75-82, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Copy elimination of some form or another is discussed or implemented in operating systems and system services such as Accent [RR81], Mach [FGB91], DASH [AF88], fbufs [DP93], UCB mmap [LMKQ89], ACME [GA91], and the Alloc Stream Facility <ref> [KS94] </ref>. All of these approaches have limitations, some of which are more significant than others. Most support some, but not all, kinds of I/O. Some impose inconvenient usage constraints, require programming language support, or violate the protection usually provided by an operating system. <p> Like any mechanism which allows the insertion of code into the kernel, SPIN faces a 27 formidable challenge in maintaining security. The authors mention a highly sophisticated verification layer that guarantees safety, but few details are provided. Another recent project is the Alloc Stream Facility <ref> [KS94] </ref>. This system provides both a new system call interface, and a replacement I/O library that can be linked to existing programs. The replacement library provides performance gains in some cases, and the new interface provides greater gains.
Reference: [Lam71] <author> Butler W. Lampson, </author> <title> Protection, </title> <booktitle> in the Proceedings of the Fifth Princeton Symposium on Information Sciences and Systems, </booktitle> <address> Princeton University, </address> <month> March </month> <year> 1971, </year> <pages> pp. 437-443, </pages> <booktitle> reprinted in Operating Systems Review, </booktitle> <volume> Volume 8, Number 1, </volume> <pages> pp. 18-24, </pages> <month> January </month> <year> 1974. </year>
Reference-contexts: The division of a system into user and kernel portions remains highly desirable, because of the labor efficiencies which are gained, and in most cases, because of the increased reliability gained from protection <ref> [Lam71] </ref>. However, user-level processes are separated from devices by an I/O system that generally depends on in-memory copying.
Reference: [LMKQ89] <author> Samuel J. Lefer, Marshall Kirk McKusick, Michael J. Karels, John S. Quarterman: </author> <title> The Design and Implementation of the 4.3 BSD UNIX Operating System, </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: The basic technique of raising I/O performance by avoiding in-memory copies is well known. Copy elimination of some form or another is discussed or implemented in operating systems and system services such as Accent [RR81], Mach [FGB91], DASH [AF88], fbufs [DP93], UCB mmap <ref> [LMKQ89] </ref>, ACME [GA91], and the Alloc Stream Facility [KS94]. All of these approaches have limitations, some of which are more significant than others. Most support some, but not all, kinds of I/O. <p> Buffer User Kernel Device Buffer Interface Process gains memory containing data from a read operation: Process loses memory containing data for a write operation: 19 appear and vanish. Container Shipping, DASH [ATG88], fbufs [DP93], and the mmap facility <ref> [LMKQ89] </ref> all employ some form of page transfer or sharing to eliminate copies. DASH and fbufs are discussed in detail in the next two sections. The mmap memory-mapped file facility is not a general-purpose I/O mechanism.
Reference: [MB76] <author> R. M. Metcalfe and D. R. Boggs, </author> <title> Ethernet: Distributed packet switching for local computer networks, </title> <journal> Communications of the ACM, </journal> <volume> Volume 19, Number 7, </volume> <pages> pp. 395-404, </pages> <month> July </month> <year> 1976. </year>
Reference: [PAM94] <author> Joseph Pasquale, Eric Anderson, P. Keith Muller, </author> <title> Container Shipping: Operating System Support for I/O-Intensive Applications, </title> <journal> IEEE Computer, </journal> <volume> Volume 27, Number 3, </volume> <pages> pp. 84-93, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: The cost of mapping in a tuned Container Shipping implementation should be substantially lower, but is still expected to be sufficient to warrant map avoidance. Previous work shows that map avoidance for large objects crossing many domains is particularly valuable <ref> [PAM94] </ref>. 112 6.3.3 New abilities gained by using Container Shipping The IPC test shows that Container Shipping allows IPC to be used for modularity, i.e. the breaking up of applications into processes, with far less concern about the communication cost.
Reference: [RR81] <author> R. Rashid and G. Robertson, </author> <title> Accent: A Communication-Oriented Network Operating System Kernel, </title> <booktitle> Proceedings of the 8th Symposium on Operating System Principles, </booktitle> <pages> pp. 64-85, </pages> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1981. </year> <month> 122 </month>
Reference-contexts: The basic technique of raising I/O performance by avoiding in-memory copies is well known. Copy elimination of some form or another is discussed or implemented in operating systems and system services such as Accent <ref> [RR81] </ref>, Mach [FGB91], DASH [AF88], fbufs [DP93], UCB mmap [LMKQ89], ACME [GA91], and the Alloc Stream Facility [KS94]. All of these approaches have limitations, some of which are more significant than others. Most support some, but not all, kinds of I/O.
Reference: [RDH+80] <author> David D. Redell, Yogen K. Dalal, Thomas R. Horsley, Hugh C. Lauer, William C. Lynch, Paul R. McJones, Hal G. Murray, and Stephen C. Purcell, </author> <title> Pilot: An operating system for a personal computer, </title> <journal> Communications of the ACM, </journal> <volume> Volume 23, Number 2, </volume> <pages> pp. 81-92, </pages> <month> February </month> <year> 1980. </year>
Reference-contexts: This restriction can be relaxed without changing the function of Container Shipping, though protection will be sacrificed. If Container Shipping is employed in a system which already lacks protection, such as Pilot, failure to enforce single ownership of Containers causes no additional loss of protection <ref> [RDH+80] </ref>. Container Shipping provides new memory to processes which perform cs_read and cs_alloc operations. To achieve this, a memory allocation mechanism is required. As described in Section 5.1.2, either wired or page-able memory may be used, but pageable memory is preferable, in a system which supports paging.
Reference: [Rit84] <author> Dennis M. Ritchie, </author> <title> A Stream Input-Output System, </title> <journal> AT&T Bell Laboratories Technical Journal, </journal> <volume> Volume 63, Number 8, </volume> <month> October </month> <year> 1984. </year>
Reference: [Sit92a] <author> Richard L. </author> <title> Sites, Alpha AXP Architecture, </title> <journal> Digital Technical Journal, </journal> <volume> Volume 4, Number 4, </volume> <year> 1992. </year>
Reference: [Sit92b] <author> Richard L. </author> <title> Sites, editor, Alpha Architecture Reference Manual, </title> <publisher> Digital Press, </publisher> <address> Burlington, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: All main memory can be directly accessed by the kernel without the TLB (Translation Lookaside Buffer), if physical memory addresses are used. Main memory performance is roughly one third that of the L2 cache, again depending on usage patterns [DEK+92] <ref> [Sit92b] </ref>. 40 3.2.2 Translation Lookaside Buffer The virtual memory hardware and software translate virtual memory address references from the CPU into physical memory targets for the memory system to access. <p> The TLB is typically brought up to date by emptying part or all of it, meaning that TLB misses may trigger software to update it, which can consume time <ref> [Sit92b] </ref> [DEC92]. 3.2.3 DMA in 21064 systems The nature of DMA in a system depends significantly on the memory structure that is used. In Alpha systems, devices perform DMA using only physical addresses. <p> Transfers from a device update main memory and any parts of the L2 cache which match that memory. Inbound transfers also invalidate any lines in the L1 cache that correspond to the transfer region <ref> [Sit92b] </ref> [DEK+92]. An interesting consequence of this DMA design is the potential to automatically load incoming data into the L2 cache without CPU intervention. If data can be used promptly after it arrives, before it is ushed from the L2 cache, the penalty of reading main memory can be avoided.
Reference: [Ste94] <author> Peter A. Steenkiste, </author> <title> A Systematic Approach to Host Interface Design for High-Speed Networks, </title> <journal> IEEE Computer, </journal> <volume> Volume 27, Number 3, </volume> <pages> pp. 47-57, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Second, the FDDI experiment employed much smaller transfers, which had much greater protocol costs than the IPC experiment. Much work has been done on analyzing and improving the performance of protocol processing within TCP/IP [CJRS89] <ref> [Ste94] </ref> [Kay95]. The application of such techniques while tuning Container Shipping should further improve the results.
Reference: [Sun90] <author> Sun Microsystems, Pipe(2V), </author> <title> Online documentation for SunOS 4.1, </title> <month> January </month> <year> 1990. </year>
Reference-contexts: Even within a generic, uniform I/O interface, different types of devices may have different perfor 51 mance characteristics. For example, reading on a Unix pipe will never produce more than one page of data, no matter how much data is waiting. The on-line documentation for SunOS 4.1.3 <ref> [Sun90] </ref> identifies this behavior as a bug: PIPE (2V) SYSTEM CALLS PIPE (2V) NAME pipe - create an interprocess communication channel ... BUGS Should more than -PIPE_BUF- bytes be necessary in any pipe among a loop of processes, deadlock will occur.
Reference: [Tan88] <author> Andrew S. Tannenbaum, </author> <title> Computer Networks, second edition, </title> <publisher> Prentice-Hall, </publisher> <address> New Jersey, </address> <year> 1988. </year>
Reference-contexts: This rate provides for images of 620 by 560 pixels, with 3 bytes for each pixel, presented 30 times per second. A recently proposed standard for HDTV (High Definition Television) Table 3 Recent performance of workstation-class network interfaces. CSMA/CD stands for Carrier Sense Multiple Access with Collision Detection <ref> [Tan88] </ref>. FDDI stands for Fiber Distributed Data Interface.
Reference: [Tan92] <author> Andrew S. Tannenbaum, </author> <title> Modern Operating Systems, </title> <publisher> Prentice-Hall, </publisher> <address> New Jersey, </address> <year> 1992. </year>
Reference: [Tho78] <author> Ken Thompson, </author> <title> UNIX Implementation, </title> <journal> The Bell System Technical Journal, </journal> <volume> Volume 57, Number 6, </volume> <pages> pp. 1931-1946, </pages> <month> July-August </month> <year> 1978. </year>
Reference-contexts: For example, the DEC (Digital Equipment Corporation) OSF/1 (Open Software Foundation) v2.0 Unix kernel used for this dissertation consists of roughly one million lines of source code spread among two thousand files, with no organized documentation. This size is roughly one hundred times larger than early Unix implementations <ref> [Tho78] </ref>. Elevating low-level services out of the kernel also leads to a loss of generality. For example, a device whose registers are mapped by one process cannot be simultaneously accessed in the same manner by any other process.
Reference: [TA91] <author> Shin-Yuan Tzou and David. P. Anderson, </author> <title> The Performance of Message-Passing Using Restricted Virtual Memory Remapping, </title> <journal> Software -- Practice and Experience, </journal> <volume> Volume 21, Number 3, </volume> <pages> pp. 251-267, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Most of the DASH papers describe preliminary work [AF88] [ATG88]. However, DASH did include a data transfer mechanism for I/O that used virtual page remapping to avoid in-memory copies. The reported performance results primarily describe fast inter-process communication, rather than device I/O <ref> [TA91] </ref>. Local IPC performance was very high due to page remapping. DASH provided message-passing IPC. Messages could include pages of data located in a special IPC region of virtual memory. These IPC pages were transferred by remapping to the address space of the recipient of a message. <p> For example, in a system where all pages eligible for remapping are given virtual addresses that are unique system-wide, such as was done in DASH <ref> [TA91] </ref>, these options may force the kernel to do extra work to change the addresses of the pages involved. The cs_alloc system call allocates a new Container for use by the calling process. This operation would be used by a process that generates new data to be written with cs_write. <p> In 64-bit systems, the available address space is generally 2 32 or more times larger than physical memory, so address space is readily available. This technique has already been proven viable with 32-bit systems <ref> [TA91] </ref>. Pages of Container memory that are freed to the kernel, either by a cs_free system call or after the completion of a cs_write system call, are likely to be in the cache, because their contents may have recently been used. <p> Within independent virtual address spaces, the range of addresses used for remapped pages may be common to all processes, as in DASH <ref> [TA91] </ref>. For performance reasons it is desirable that the virtual address space be capable of sparse mappings, so that a Container which is unmapped can leave a hole in the address space which is not mapped to 74 any memory.
Reference: [UNS+94] <author> Richard Uhlig, David Nagle, Tim Stanley, Trevor Mudge, Stuart Sechrest, and Richard Brown, </author> <title> Design Tradeoffs for Software-Managed TLBs, </title> <journal> ACM Transactions on Computer Systems, pp. </journal> <volume> 175-205, Volume 12, Number 3, </volume> <month> August </month> <year> 1994. </year> <month> 123 </month>
Reference: [UHMB94] <author> Michael Upton, Thomas Huff, Trevor Mudge and Richard Brown, </author> <title> Resource Allocation in a High Clock Rate Microprocessor, </title> <booktitle> Proceedings of ASPLOS-VI, </booktitle> <pages> pp 98-109, </pages> <address> San Jose, California, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: From 1980 to 1992, the best CPU speed has grown 280 times more than the best DRAM speed. A vertical log scale is required to see any DRAM speedup at all. The CPU trend shown by Hennessy and Patterson is continued by recent DEC Alpha multiple-issue processors [HP90] <ref> [UHMB94] </ref>. 10,000 1 Year Growth multiple 1,000 Best CPU 1983 1986 1989 1992 1995 DEC 21064 200 MHz x 2 Best DRAM 33 during this time, if there was no delay in accessing memory. To combat this problem, systems are designed with caches of expensive high-speed memory.
Reference: [Vet95] <author> Ronald J. Vetter, </author> <title> ATM Concepts, Architectures, and Protocols, </title> <journal> Communications of the ACM, </journal> <volume> Volume 38, Number 2, </volume> <pages> pp. 30-38, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Local area ATM (Asynchronous Transfer Mode) networks are becoming available at speeds of up to 155 megabits (19.5 megabytes) per second, with a 622 megabit (78 megabyte) per second version presently in the experimental stage of development <ref> [Vet95] </ref>. Obtaining a meaningful average of this rather uneven growth over the last two decades is difficult, but the rate is clearly closer to CPU speed growth than DRAM speed growth. The progression from 10-megabit Ethernet to 622-megabit ATM represents six doublings of speed in about two decades.
References-found: 45

