URL: http://ferdowsi.eecs.berkeley.edu/~nlachang/Research/imdsp96.ps.gz
Refering-URL: http://ferdowsi.eecs.berkeley.edu/papers.html
Root-URL: http://www.cs.berkeley.edu
Email: e-mail: nlachang@eecs.Berkeley.EDU, avz@eecs.Berkeley.EDU  
Title: IMDSP Workshop  VIEW GENERATION FOR 3-D SCENES FROM VIDEO SEQUENCES  
Author: Belize City, Belize Nelson L. Chang and Avideh Zakhor 
Address: Berkeley, CA 94720 USA  
Affiliation: Department of Electrical Engineering and Computer Sciences University of California,  
Date: 3-6 March 1996  
Abstract: This paper focuses on a representation for 3-D scenes consisting of dense depth maps at preselected viewpoints from video sequences derived from unknown but approximately horizontal motion. In contrast to existing methods that construct a full 3-D model or those that exploit geometric invariants, an intensity-depth representation is used to generate arbitrary views of the 3-D scene. Specifically, the depth maps are regarded as vertices of a deformable 2-D mesh which are transformed in 3-D, projected to 2-D, and rendered to generate the desired view. Experimental results are presented to verify our approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Higuchi, M. Hebert, and K. </author> <title> Ikeuchi, "Building 3-D models from unregistered range images," </title> <type> Tech. Rep. </type> <institution> CMU-CS-93-214, Carnegie Mellon University, </institution> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: We propose to represent the scene by dense depth maps and corresponding intensity information at selected viewpoints in the scene called reference frames. Previous approaches attempt to construct a full 3-D model (e.g. <ref> [1] </ref>) or reconstruct views directly without first estimating depth (e.g. [2]). More details are given in [3]. Our approach with dense depth maps is not as complex as 3-D modeling and is capable of handling occlusions better than the direct methods. 2.
Reference: [2] <author> S. Laveau and O. Faugeras, </author> <title> "3-D scene representation as a collection of images and fundamental matrices," </title> <type> Tech. Rep. 2205, </type> <institution> INRIA, </institution> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: We propose to represent the scene by dense depth maps and corresponding intensity information at selected viewpoints in the scene called reference frames. Previous approaches attempt to construct a full 3-D model (e.g. [1]) or reconstruct views directly without first estimating depth (e.g. <ref> [2] </ref>). More details are given in [3]. Our approach with dense depth maps is not as complex as 3-D modeling and is capable of handling occlusions better than the direct methods. 2.
Reference: [3] <author> N. L. Chang, </author> <title> "View reconstruction from uncalibrated cameras for three-dimensional scenes," </title> <type> Master's thesis, </type> <institution> UC Berkeley, </institution> <year> 1994. </year>
Reference-contexts: We propose to represent the scene by dense depth maps and corresponding intensity information at selected viewpoints in the scene called reference frames. Previous approaches attempt to construct a full 3-D model (e.g. [1]) or reconstruct views directly without first estimating depth (e.g. [2]). More details are given in <ref> [3] </ref>. Our approach with dense depth maps is not as complex as 3-D modeling and is capable of handling occlusions better than the direct methods. 2. <p> We employ an adaptive region matching scheme <ref> [3] </ref> which adapts the region size according to the local variance of the image. Moreover, the scheme identifies low confidence matches due to various artifacts discussed in [3]. <p> We employ an adaptive region matching scheme <ref> [3] </ref> which adapts the region size according to the local variance of the image. Moreover, the scheme identifies low confidence matches due to various artifacts discussed in [3]. Because these low confidence regions may differ in depth maps, we combine the information contained in multiple depth maps to obtain a single dense depth map for the given reference frames.
Reference: [4] <author> N. L. Chang and A. Zakhor, </author> <title> "View generation for three-dimensional scenes from video sequences," </title> <journal> IEEE Transactions on Image Processing, </journal> <month> Aug. </month> <year> 1995. </year> <note> In submission. </note>
Reference-contexts: Neighboring points in the reference frame arrays are viewed as connected to one another to form a 2-D mesh consisting of quadrilateral patches. These patches are projected into 3-D space, transformed, and then reprojected to the 2-D image plane, with only valid patches rendered <ref> [4] </ref>. Finally, the view estimates are combined into a single image, interpolating when necessary. In this way, points not visible from one view estimate may be filled in by other view estimates. 4.
References-found: 4

