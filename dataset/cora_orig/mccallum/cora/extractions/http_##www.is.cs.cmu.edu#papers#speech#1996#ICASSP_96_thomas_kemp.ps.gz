URL: http://www.is.cs.cmu.edu/papers/speech/1996/ICASSP_96_thomas_kemp.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Title: MODELLING UNKNOWN WORDS IN SPONTANEOUS SPEECH  
Author: T. Kemp A. Jusek 
Address: 76131 Karlsruhe, Germany Postfach 100131 33501 Bielefeld, Germany  
Affiliation: Interactive Systems Laboratories AG Angewandte Informatik Department of Computer Science Technische Fakultat University of Karlsruhe University of Bielefeld  
Abstract: In this paper we describe our experiments with different acoustic and language models for unknown words in spontaneous speech. We propose a syllable based approach for the acoustic modelling of new words. Several models of different degrees of complexity are evaluated against each other. We show that the modelling of new words can decrease the er ror rate in the recognition of spontaneous human-to-human speech. In addition, the new word models can be used as a measure of confidence capable of detecting errors in the recognition of spontaneous speech. Although the best performance is reached by applying phonetic a-priori knowledge in the design of the acoustic models, a pure data-driven approach is proposed which performs only slightly less efficiently. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. </author> <type> Suhm, </type> <institution> Erkennung und Transkription Neuer Worter in der Spracherkennung, Diplomarbeit, University of Karlsruhe, Karlsruhe, Germany, </institution> <note> April 1993 (in German) </note>
Reference-contexts: Each unknown word will inevitably lead to a recognition error. Therefore it would be desirable to have a detector for unknown words, so that appropriate action (e.g. a repair dialog ) can be initiated. Two different approaches to solve this problem have been proposed. In the first one <ref> [1] </ref>, a generic word model, usually consisting of phonemes with some constraints in their possible succession, is used. This word model competes in the decoder with the dictionary words.
Reference: [2] <author> L. Chase, R. Rosenfeld, A. Hauptmann, M. Ravis-hankar, E. Thayer, P. Placeway, R. Weide, C. Lu, </author> <title> Improvements in Language, Lexical, and Phonetic Modeling in Sphinx-II, </title> <booktitle> in Proc. ARPA Spoken Language Technology Workshop, </booktitle> <address> Austin, TX, </address> <month> January </month> <year> 1995 </year>
Reference-contexts: Its demands regarding memory and computational power are very high. For read speech and vocabularies exceeding approximately 1,000 words, the extension of the dictionary usually outperforms the modelling of new words with generic new word models <ref> [2] </ref>. However, for spontaneous speech the situation is somewhat different, as up to 50% of the new words in spon taneous data are word fragments and cannot even be covered by a dictionary of arbitrary size.
Reference: [3] <author> P. Fetter, F. Class, U. Haiber, A. Kaltenmeier, U. Kil-ian, P. Regel-Brietzmann, </author> <title> Detection of unknown words in spontaneous speech, </title> <booktitle> Proc. EUROSPEECH 1995 </booktitle>
Reference-contexts: However, for spontaneous speech the situation is somewhat different, as up to 50% of the new words in spon taneous data are word fragments and cannot even be covered by a dictionary of arbitrary size. Recent experimental results on spontaneous data <ref> [3] </ref> seem to indicate that these word fragments can be adequately modelled with a generic new word model. This is particularly interesting as there are no large corpora of spontaneous speech available, that can be used for language modelling. <p> The test set perplexity decreased by 3.5% with respect to the uniform distribution for the unknown words. 2.1. Optimizing the language model In <ref> [3] </ref> was observed that the entropy of the probability distribution into new words is lower than the entropy of the distribution from new words. <p> We expect this situation to change when larger corpora of spontaneous speech for more robust language modelling become available. This is in contrast to the results of <ref> [3] </ref>, where significant improvements could be achieved on word lattices. However, the June 1995 VERBMOBIL evaluation has shown that new word detection rates in word lattices may not be significant to results on first best hypotheses. 5.
Reference: [4] <author> A. Jusek, H. Rautenstrauch, G. A. Fink, F. </author> <title> Kummert, </title> <editor> G. Sagerer, J. Carson-Berndsen, and D. Gibbon, De-tektion unbekannter Worter mit Hilfe phonotaktischer Modelle, In W.G. Kropatsch and H. Bischof, editors, </editor> <booktitle> Mustererkennung 94, 16. DAGM-Symposium und 18. Workshop der OAGM Wien, </booktitle> <pages> pages 238-245. </pages> <note> 1994 (in German) </note>
Reference-contexts: We distinguish two types of syllables, reduced and non-reduced ones. The main difference is, that reduced syllables allow neutral vowels in the nucleus. According to the phonotactic rules of these two types of syllables we built a generic model for arbitrary German words based on the recognizers phonetic inventory <ref> [4] </ref>. This model was added to the recognizers dictionary. 1.2. The augmented phonotactic model The described phonotactic model implicitely assigns unity probabilities to all state transitions between its phoneme states.
Reference: [5] <author> M. Woszczyna, N. Coccaro, A. Eisele, A. Lavie, A. Mc-Nair, T. Polzin, I. Rogina, C.P. Rose, T. Sloboda, M. Tomita, J. Tsutsumi, N. Aoki-Waibel, A. Waibel, W. Ward, </author> <title> Recent Advances in Janus, a Speech-to-Speech Translation System, </title> <booktitle> Proc. EUROSPEECH 1993, </booktitle> <pages> pp. 1295-1298 </pages>
Reference-contexts: Trigrams and cross-word triphones can be em ployed, however, in the experiments described in this paper no cross-word models were used. After recognition, a maximum likelihood codebook adaptation using the recognition result and an additional recognition run are performed. For a more detailed description, refer to <ref> [5] </ref> [11] [12] [9]. The JANUS-2 decoder achieved a word error rate of 28.6% in the VERBMOBIL June 95 evaluation. This was the lowest error rate of the 5 participating institutions.
Reference: [6] <author> M. Woszczyna, N.Aoki-Waibel, F.D.Buo, N. Coccaro, K. Horigushi, T. Kemp, A. Lavie, A. McNair, T. Polzin, I. Rogina, C.P. Rose, T. Schultz, B. Suhm, M. Tomita, A. Waibel, </author> <title> Janus 93: Towards Spontaneous Speech Translation, </title> <booktitle> Proc. ICASSP-94, </booktitle> <pages> pp. 345-349, </pages> <month> April </month> <year> 1994 </year>
Reference-contexts: The database contains about 183,000 words of speech and has a bigram test set perplexity of around 95. 2.3. The JANUS-2 system The speech-to-speech translation system JANUS-2 <ref> [6] </ref> [7] is a joint effort of the Interactive Systems Labs at Carnegie Mellon University, Pittsburgh, and at the University of Karlsruhe, Germany. The baseline speech recognition component of JANUS-2 uses mixture-gaussian densities with a scalable amount of parameter tying.
Reference: [7] <author> P.Geutner, B.Suhm, F.D.Buo, T.Kemp, L.Mayfield, A.E.McNair, I.Rogina, T.Schultz, T.Sloboda, W.Ward, M.Woszczyna, A.Waibel. </author> <title> Integrating different learning approaches into a multilingual spoken language translation system, </title> <booktitle> in Proc. of the IJCAI workshop on New Approaches to Learning for Natural Language Processing, </booktitle> <pages> pp 33 ff, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 1995 </year>
Reference-contexts: The database contains about 183,000 words of speech and has a bigram test set perplexity of around 95. 2.3. The JANUS-2 system The speech-to-speech translation system JANUS-2 [6] <ref> [7] </ref> is a joint effort of the Interactive Systems Labs at Carnegie Mellon University, Pittsburgh, and at the University of Karlsruhe, Germany. The baseline speech recognition component of JANUS-2 uses mixture-gaussian densities with a scalable amount of parameter tying.
Reference: [8] <author> G. Yu, R.Schwartz: </author> <title> Discriminant analysis and supervised vector quantization for continuous speech recognition, </title> <booktitle> Proc. ICASSP 1990, </booktitle> <pages> pp. 685 ff. </pages>
Reference-contexts: In the preprocessing stage mel-scale spectra with a frame rate of 10 ms and their deltas, power, zerocrossing rate and peak-to-peak value were computed. The 37-dimensional input vector was transformed by linear discriminant analysis (LDA <ref> [8] </ref>) and split into two 16-component data streams. Training was done with Viterbi alignment. To capture some of the effects of spontaneous speech, specialized noise nodels were included [10]. The decoder computes word lattices with a multi-pass strategy.
Reference: [9] <author> I. Rogina and A. Waibel, </author> <title> Learning State-Dependent Stream Weights for Multi-Codebook HMM Speech Recognition Systems, </title> <booktitle> Proc. ICASSP 1994 </booktitle>
Reference-contexts: Trigrams and cross-word triphones can be em ployed, however, in the experiments described in this paper no cross-word models were used. After recognition, a maximum likelihood codebook adaptation using the recognition result and an additional recognition run are performed. For a more detailed description, refer to [5] [11] [12] <ref> [9] </ref>. The JANUS-2 decoder achieved a word error rate of 28.6% in the VERBMOBIL June 95 evaluation. This was the lowest error rate of the 5 participating institutions.
Reference: [10] <author> T. Schultz and I. Rogina, </author> <title> Acoustic and Language Modeling of Human and Nonhuman Noises for Human-to-Human Spontaneous Speech Recognition, </title> <booktitle> Proc. ICASSP 1995, </booktitle> <volume> vol 1, </volume> <pages> pp 293-296 </pages>
Reference-contexts: The 37-dimensional input vector was transformed by linear discriminant analysis (LDA [8]) and split into two 16-component data streams. Training was done with Viterbi alignment. To capture some of the effects of spontaneous speech, specialized noise nodels were included <ref> [10] </ref>. The decoder computes word lattices with a multi-pass strategy. Trigrams and cross-word triphones can be em ployed, however, in the experiments described in this paper no cross-word models were used. After recognition, a maximum likelihood codebook adaptation using the recognition result and an additional recognition run are performed.
Reference: [11] <author> T. Kemp. </author> <title> Data-driven codebook adaptation in phonetically tied SCHMMs, </title> <booktitle> in Proc. ICASSP-95, </booktitle> <volume> vol 1, </volume> <pages> pp 477ff, </pages> <address> Detroit, </address> <month> May </month> <year> 1995 </year>
Reference-contexts: Trigrams and cross-word triphones can be em ployed, however, in the experiments described in this paper no cross-word models were used. After recognition, a maximum likelihood codebook adaptation using the recognition result and an additional recognition run are performed. For a more detailed description, refer to [5] <ref> [11] </ref> [12] [9]. The JANUS-2 decoder achieved a word error rate of 28.6% in the VERBMOBIL June 95 evaluation. This was the lowest error rate of the 5 participating institutions.

References-found: 11

