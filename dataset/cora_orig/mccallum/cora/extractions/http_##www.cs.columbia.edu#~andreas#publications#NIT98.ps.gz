URL: http://www.cs.columbia.edu/~andreas/publications/NIT98.ps.gz
Refering-URL: http://www.cs.columbia.edu/~andreas/publications/publications.html
Root-URL: http://www.cs.columbia.edu
Email: fandreas,salg@cs.columbia.edu  
Title: Pruning Meta-Classifiers in a Distributed Data Mining System  
Author: Andreas L. Prodromidis and Salvatore J. Stolfo 
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Abstract: JAM is a powerful and portable agent-based distributed data mining system that employs meta-learning techniques to integrate a number of independent classifiers (models) derived in parallel from independent and (possibly) inherently distributed databases. Although meta-learning promotes scalability and accuracy in a simple and straightforward manner, brute force meta-learning techniques can result in large, redundant, inefficient and some times inaccurate meta-classifier hierarchies. In this paper we explore several methods for evaluating classifiers and composing meta-classifiers, we expose their limitations and we demonstrate that meta-learning combined with certain pruning methods has the potential to achieve similar or even better performance results in a much more cost effective manner. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Ali and M. Pazzani. </author> <title> Error reduction through learning multiple descriptions. </title> <booktitle> Machine Learning, </booktitle> <address> 24:173202, </address> <year> 1996. </year>
Reference-contexts: Next, we focus on the diversity and specialty metrics. Accuracy, correlation error and coverage are other metrics metrics explored in the literature. For example, Ali and Pazzani <ref> [1] </ref> define correlation error as the fraction of instances for which a pair of classifiers make the same incorrect predictions and Brodley and Lane [4] measure coverage by computing the fraction of instances for which at least one of the classifiers produces the correct prediction. 1 TP stands for True Positive, <p> of the base classifiers. (When the predictions of the classifiers are distributed evenly across the possible classes, the entropy is higher and the set of classifiers more diverse.) Kwok and Carter [13] correlate the error rates of a set of decision trees to their syntactical diversity, while Ali and Pazzani <ref> [1] </ref> studied the impact of the number of gain ties 2 on the accuracy of an ensemble of classifiers.
Reference: [2] <author> L. Breiman. </author> <title> Stacked regressions. </title> <booktitle> Machine Learning, </booktitle> <address> 24:4148, </address> <year> 1996. </year>
Reference-contexts: Sections 3 describes the pruning algorithms and Section 4 presents the experiments performed and the results collected. Finally, Section 5 concludes the paper. 2 Evaluating and Selecting Classifiers One can employ several different measures and methods to analyze, compare and combine ensembles of classifiers. Leo Breiman <ref> [2] </ref> and LeBlanc and Tibshirani [14] acknowledge the value of using multiple predictive models to increase accuracy, but they view the problem from a different perspective. They rely on cross-validation data and analytical methods, (e.g. least squares regression), to estimate the best linear combination of the available hypotheses (models).
Reference: [3] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: We obtained these classifiers by applying 5 learning algorithms (three decision-tree algorithms, ID3, its successor C4.5, and Cart <ref> [3] </ref>, a naive Bayesian algorithm [18], and the rule induction algorithm Ripper [9]) on each month of data, therefore creating 60 base classifiers (10 classifiers per data site).
Reference: [4] <author> C. Brodley and T. Lane. </author> <title> Creating and exploiting coverage and diversity. </title> <booktitle> AAAI-96 Workshop on Integrating Multiple Learned Models. </booktitle>
Reference-contexts: Accuracy, correlation error and coverage are other metrics metrics explored in the literature. For example, Ali and Pazzani [1] define correlation error as the fraction of instances for which a pair of classifiers make the same incorrect predictions and Brodley and Lane <ref> [4] </ref> measure coverage by computing the fraction of instances for which at least one of the classifiers produces the correct prediction. 1 TP stands for True Positive, i.e. percentage of actual fraud that is caught, FP stands for False Positive, i.e. percentage of false alarms 2.1 Diversity Brodley [5] defines diversity
Reference: [5] <author> C.Brodley. </author> <title> Addressing the selective superiority problem: Automatic algorithm/model class selection. </title> <booktitle> 10th Intl. Conf. Mach. Learning, </booktitle> <year> 1993. </year>
Reference-contexts: and Lane [4] measure coverage by computing the fraction of instances for which at least one of the classifiers produces the correct prediction. 1 TP stands for True Positive, i.e. percentage of actual fraud that is caught, FP stands for False Positive, i.e. percentage of false alarms 2.1 Diversity Brodley <ref> [5] </ref> defines diversity by measuring the classification overlap of a pair of classifiers, i.e. the percentage of the instances classified the same way by two classifiers, while Chan [6] associates it with the entropy in the predictions of the base classifiers. (When the predictions of the classifiers are distributed evenly across
Reference: [6] <author> P. Chan. </author> <title> An Extensible Meta-Learning Approach for Scalable and Accurate Inductive Learning. </title> <type> PhD thesis, </type> <institution> Columbia Univ., </institution> <year> 1996. </year>
Reference-contexts: Positive, i.e. percentage of actual fraud that is caught, FP stands for False Positive, i.e. percentage of false alarms 2.1 Diversity Brodley [5] defines diversity by measuring the classification overlap of a pair of classifiers, i.e. the percentage of the instances classified the same way by two classifiers, while Chan <ref> [6] </ref> associates it with the entropy in the predictions of the base classifiers. (When the predictions of the classifiers are distributed evenly across the possible classes, the entropy is higher and the set of classifiers more diverse.) Kwok and Carter [13] correlate the error rates of a set of decision trees
Reference: [7] <author> P. Chan and S. Stolfo. </author> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> 2nd Intl. Work. Multistrategy Learning, </booktitle> <pages> pages 150165, </pages> <year> 1993. </year>
Reference-contexts: We call the problem of learning useful new information from large and inherently distributed databases, the scaling problem for machine learning. Meta-learning <ref> [7] </ref>, a technique similar to stacking [26], was developed recently to deal with the scaling problem.
Reference: [8] <author> P. Chan and S. Stolfo. </author> <title> Sharing learned models among remote database partitions by local meta-learning. </title> <booktitle> In Proc. KDD'96, </booktitle> <pages> pages 27, </pages> <year> 1996. </year>
Reference-contexts: Retaining a large number of base classifiers and meta-classifiers may not be practical nor feasible. Meta classifiers are defined recursively as collections of classifiers structured in multi-level trees <ref> [8] </ref>, hence determining the optimal set of classifiers is a combinatorial problem. Pre-training pruning (as opposed to post-training pruning [20] which denotes the evaluation and revision/pruning of the meta-classifier after it is computed) refers to the filtering of the classifiers before they are used in the training of a meta-classifier.
Reference: [9] <author> W. Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> In Proc. 12th Intl. Conf. Machine Learning, </booktitle> <pages> pages 115123, </pages> <year> 1995. </year>
Reference-contexts: We obtained these classifiers by applying 5 learning algorithms (three decision-tree algorithms, ID3, its successor C4.5, and Cart [3], a naive Bayesian algorithm [18], and the rule induction algorithm Ripper <ref> [9] </ref>) on each month of data, therefore creating 60 base classifiers (10 classifiers per data site).
Reference: [10] <author> T.G. Dietterich. </author> <title> Machine learning research: Four current directions. </title> <journal> AI Magazine, </journal> <volume> 18(4):97136, </volume> <year> 1997. </year>
Reference-contexts: The intent of these algorithms is to discover and prune away the sub-optimal models, while in this work, we focus on evaluation metrics that provide information about the interdependencies among the (base) classifiers and their potential when forming ensembles of classifiers <ref> [10, 12] </ref>. In fact, the performance of sub-optimal yet diverse models can be substantially improved when combined together and even surpass that of the best single models.
Reference: [11] <author> Y. Freund and R. Schapire. </author> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Proc. Thirteenth Conf. Machine Learning, </booktitle> <pages> pages 148156, </pages> <year> 1996. </year>
Reference-contexts: In other related work, Margineantu and Dietterich [15] studied the problem of pruning the ensemble of classifiers (i.e. the set of hypothesis (classifiers)) obtained by the boosting algorithm ADABOOST <ref> [11] </ref>. According to their findings, by examining the diversity and accuracy of the available classifiers, it is possible for a subset of classifiers to achieve similar levels of performance as the entire set. <p> As we have already noted, the more diverse the set of base-classifiers is, the more room for improvement the meta-classifier has. For example, to obtain diverse classifiers from a single learning program Freund and Schapire <ref> [11] </ref> introduced the boosting algorithm for re-sampling the data set to artificially generate diverse training subsets. In our experiments, the diversity of the base classifiers is attributed, first, to the use of disparate learning algorithms, and second, to the degree the training sets are different.
Reference: [12] <author> L. Hansen and P. Salamon. </author> <title> Neural network ensembles. </title> <journal> IEEE Trans. Pattern Analysis and Mach. Itell., </journal> <volume> 12:9931001, </volume> <year> 1990. </year>
Reference-contexts: The intent of these algorithms is to discover and prune away the sub-optimal models, while in this work, we focus on evaluation metrics that provide information about the interdependencies among the (base) classifiers and their potential when forming ensembles of classifiers <ref> [10, 12] </ref>. In fact, the performance of sub-optimal yet diverse models can be substantially improved when combined together and even surpass that of the best single models.
Reference: [13] <author> S. Kwok and C. Carter. </author> <title> Multiple decision trees. </title> <booktitle> In Uncertainty in Aritificial Intelligence 4, </booktitle> <pages> pages 327335, </pages> <year> 1990. </year>
Reference-contexts: instances classified the same way by two classifiers, while Chan [6] associates it with the entropy in the predictions of the base classifiers. (When the predictions of the classifiers are distributed evenly across the possible classes, the entropy is higher and the set of classifiers more diverse.) Kwok and Carter <ref> [13] </ref> correlate the error rates of a set of decision trees to their syntactical diversity, while Ali and Pazzani [1] studied the impact of the number of gain ties 2 on the accuracy of an ensemble of classifiers.
Reference: [14] <author> M. LeBlanc and R. Tibshirani. </author> <title> Combining estimates in regression and classification. </title> <type> TR 9318, </type> <institution> Dept. of Statistics, Univ. of Toronto, </institution> <year> 1993. </year>
Reference-contexts: Finally, Section 5 concludes the paper. 2 Evaluating and Selecting Classifiers One can employ several different measures and methods to analyze, compare and combine ensembles of classifiers. Leo Breiman [2] and LeBlanc and Tibshirani <ref> [14] </ref> acknowledge the value of using multiple predictive models to increase accuracy, but they view the problem from a different perspective. They rely on cross-validation data and analytical methods, (e.g. least squares regression), to estimate the best linear combination of the available hypotheses (models).
Reference: [15] <author> D. Margineantu and T. Dietterich. </author> <title> Pruning adaptive boosting. </title> <booktitle> In Proc. Fourteenth Intl. Conf. Machine Learning, </booktitle> <pages> pages 211218, </pages> <year> 1997. </year>
Reference-contexts: In fact, the performance of sub-optimal yet diverse models can be substantially improved when combined together and even surpass that of the best single models. In other related work, Margineantu and Dietterich <ref> [15] </ref> studied the problem of pruning the ensemble of classifiers (i.e. the set of hypothesis (classifiers)) obtained by the boosting algorithm ADABOOST [11].
Reference: [16] <author> C. Merz. </author> <title> Using correspondence analysis to combine classifiers. </title> <booktitle> Machine Learning, </booktitle> <year> 1998. </year> <note> to appear. </note>
Reference-contexts: Meta-learning has the advantage of employing an arbitrary learning algorithm for computing non-linear relations among the classifiers (at the expense, perhaps, of generating less intuitive representations). In a related study, Merz's SCAN N algorithm <ref> [16] </ref> employs correspondence analysis (similar to Principal Component Analysis) to map the predictions of the available classifiers onto a new scaled space that clusters similar prediction behaviors and then uses the nearest neighbor algorithm to meta-learn the transformed predictions of the individual classifiers.
Reference: [17] <author> R. Michalski. </author> <title> A theory and methodology of inductive learning. </title> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> pages 83134, </pages> <year> 1983. </year>
Reference-contexts: Machine learning or Inductive learning (or learning from examples <ref> [17] </ref>) aims to identify regularities in a given set of training examples with little or no knowledge about the domain from which the examples are drawn.
Reference: [18] <author> M. Minksy and S. Papert. </author> <title> Perceptrons: An Introduction to Computation Geometry. </title> <publisher> MIT Press, </publisher> <address> (Expanded edition, </address> <year> 1988). </year>
Reference-contexts: We obtained these classifiers by applying 5 learning algorithms (three decision-tree algorithms, ID3, its successor C4.5, and Cart [3], a naive Bayesian algorithm <ref> [18] </ref>, and the rule induction algorithm Ripper [9]) on each month of data, therefore creating 60 base classifiers (10 classifiers per data site).
Reference: [19] <author> T. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18:203226, </volume> <year> 1982. </year>
Reference-contexts: Meta-learning, is scalable because meta-classifiers can be similarly combined into higher level meta-classifiers in a distributed fashion and it improves predictive performance by combining classifiers with different inductive bias <ref> [19] </ref>. The J AM system (Java Agents for Meta-learning) [25] is a distributed agent-based data mining system that implements meta-learning.
Reference: [20] <author> A. L. Prodromidis. </author> <title> On the management of distributed learning agents. </title> <type> CUCS-032-97 (PhD Thesis proposal), CS. </type> <institution> Dept. Columbia Univ.,1997. </institution>
Reference-contexts: Retaining a large number of base classifiers and meta-classifiers may not be practical nor feasible. Meta classifiers are defined recursively as collections of classifiers structured in multi-level trees [8], hence determining the optimal set of classifiers is a combinatorial problem. Pre-training pruning (as opposed to post-training pruning <ref> [20] </ref> which denotes the evaluation and revision/pruning of the meta-classifier after it is computed) refers to the filtering of the classifiers before they are used in the training of a meta-classifier.
Reference: [21] <author> F. Provost and T. Fawcett. </author> <title> Analysis and visualization of classifier performance: Comparison under imprecise class and cost distributions. </title> <booktitle> In Proc. Third Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 4348, </pages> <year> 1997. </year>
Reference-contexts: Furthermore, the pruning methods presented in this paper preceed the meta-learning phase and, as such, can be used in conjunction with SCAN N or any other algorithm if computational constraints pose no problem. Provost and Fawcett <ref> [21] </ref> introduced the ROC convex hull method for its intuitiveness and flexibility. The method evaluates models for binary classification problems, by mapping them onto a True Positive/False Positive plane and by allowing comparisons under different metrics (TP/FP rates, accuracy, cost, etc.). <p> In comparing the classifiers, one can replace the (T P F P ) spread, which defines a certain family of curves in the ROC plot, with a different metric or even with a complete analysis <ref> [21] </ref> in the ROC space. 6 Fraud detection is a binary classification problem, hence we need not examine specialty on both target classes (Section 3).
Reference: [22] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <address> 1:81106, </address> <year> 1986. </year>
Reference: [23] <author> J. R. Quinlan. C4.5: </author> <title> programs for machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference: [24] <author> S. Stolfo, W. Fan, W. Lee, A. Prodromidis, and P. Chan. </author> <title> Credit card fraud detection using meta-learning: Issues and initial results. </title> <booktitle> Working notes of AAAI Workshop on AI Approaches to Fraud Detection and Risk Management, </booktitle> <year> 1997. </year>
Reference-contexts: For example, one possible approach would be to combine the (base-) classifiers with high coverage and low correlation error. In another study <ref> [24] </ref> concerning credit card fraud detection we employed evaluation formulas for selecting classifiers that are based on diversity, coverage and correlated error and their combinations. 3 Pruning algorithms Pruning refers to the evaluation and selection of classifiers before they are used for the training of the meta-classifier.
Reference: [25] <author> S. Stolfo, A. Prodromidis, S. Tselepis, W. Lee, W. Fan, and P. Chan. </author> <title> JAM: Java agents for meta-learning over distributed databases. </title> <booktitle> In Proc. 3rd Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 7481, </pages> <year> 1997. </year>
Reference-contexts: Meta-learning, is scalable because meta-classifiers can be similarly combined into higher level meta-classifiers in a distributed fashion and it improves predictive performance by combining classifiers with different inductive bias [19]. The J AM system (Java Agents for Meta-learning) <ref> [25] </ref> is a distributed agent-based data mining system that implements meta-learning.
Reference: [26] <author> D. Wolpert. </author> <title> Stacked generalization. Neural Networks, </title> <address> 5:241259, </address> <year> 1992. </year>
Reference-contexts: We call the problem of learning useful new information from large and inherently distributed databases, the scaling problem for machine learning. Meta-learning [7], a technique similar to stacking <ref> [26] </ref>, was developed recently to deal with the scaling problem.
References-found: 26

