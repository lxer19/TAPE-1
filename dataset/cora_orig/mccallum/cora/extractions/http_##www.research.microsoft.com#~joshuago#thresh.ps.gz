URL: http://www.research.microsoft.com/~joshuago/thresh.ps.gz
Refering-URL: http://www.research.microsoft.com/~joshuago/
Root-URL: http://www.research.microsoft.com
Email: goodman@das.harvard.edu  
Title: Global Thresholding and Multiple-Pass Parsing  
Author: Joshua Goodman 
Address: 40 Oxford St. Cambridge, MA 02138  
Affiliation: Harvard University  
Abstract: We present a variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method, at the same performance level. We also present a new thresholding technique, global thresholding, which, combined with the new beam thresholding, gives an additional factor of two improvement, and a novel technique, multiple pass parsing, that can be combined with the others to yield yet another 50% improvement. We use a new search algorithm to simultaneously optimize the thresholding parameters of the various algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Caraballo, Sharon and Eugene Charniak. </author> <year> 1996. </year> <title> Figures of merit for best-first probabilistic chart parsing. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> pages 127-132, </pages> <address> Philadelphia, </address> <month> May. </month>
Reference: <author> Caraballo, Sharon and Eugene Charniak. </author> <year> 1997. </year> <title> New figures of merit for best first probabilistic chart parsing. </title> <note> In submission. Available from http://www.cs.brown.edu/people/sc/ NewFiguresofMerit.ps.Z </note> . 
Reference-contexts: The best comparison of these functions is due to Caraballo and Charniak (1996; 1997), who tried various prioritization methods. Several of their techniques are similar to our beam thresholding technique, and one of their techniques, not yet published <ref> (Caraballo and Charniak, 1997) </ref>, would probably work better. The only technique that Caraballo and Charniak (1996) give that took into account the scores of other nodes in the priority function, the "prefix model," required O (n 5 ) time to compute, compared to our O (n 3 ) system. <p> This could propagate through much of the chart. To remedy this, Caraballo et al. only propagated probabilities that caused a large enough change <ref> (Caraballo and Charniak, 1997) </ref>. Also, the question of when an agenda-based system should stop is a little discussed issue, and difficult since there is no obvious stopping criterion. Because of these issues, we chose not to implement an agenda-based system for comparison.
Reference: <author> Charniak, Eugene. </author> <year> 1996. </year> <title> Tree-bank grammars. </title> <type> Technical Report CS-96-02, </type> <institution> Department of Computer Science, Brown University. </institution> <note> Available from ftp://ftp.cs.brown.edu/pub/techreports/ 96/cs96-02.ps.Z </note> . 
Reference: <author> Collins, Michael. </author> <year> 1996. </year> <title> A new statistical parser based on bigram lexical dependencies. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the ACL, </booktitle> <pages> pages 184-191, </pages> <address> Santa Cruz, CA, </address> <month> June. </month>
Reference: <author> Lafferty, John, Daniel Sleator, and Davy Temper-ley. </author> <year> 1992. </year> <title> Grammatical trigrams: A probabilistic model of link grammar. </title> <booktitle> In Proceedings of the 1992 AAAI Fall Symposium on Probabilistic Approaches to Natural Language, </booktitle> <month> October. </month>
Reference-contexts: However, just about any probabilistic grammar formalism for which inside and outside probabilities can be computed can benefit from these techniques. For instance, Probabilistic Link Grammars <ref> (Lafferty, Sleator, and Temperley, 1992) </ref> could benefit from our algorithms. We have however had trouble using global thresholding with grammars that strongly violated the independence assumptions of global thresholding.
Reference: <author> Rayner, Manny and David Carter. </author> <year> 1996. </year> <title> Fast parsing using pruning and grammar specialization. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the ACL, </booktitle> <pages> pages 223-230, </pages> <address> Santa Cruz, CA, </address> <month> June. </month>
Reference: <author> Resnik, P. </author> <year> 1992. </year> <title> Probabilistic tree-adjoining grammar as a framework for statistical natural language processing. </title> <booktitle> In Proceedings of the 14th International Conference on Computational Linguistics, </booktitle> <address> Nantes, France, </address> <month> August. </month>
Reference: <author> Schabes, Y. and R. Waters. </author> <year> 1994. </year> <title> Tree insertion grammar: A cubic-time parsable formalism that lexicalizes context-free grammar without changing the tree produced. </title> <type> Technical Report TR-94-13, </type> <institution> Mitsubishi Electric Research Laboratories. </institution>
Reference-contexts: Most STAG productions in practical grammars are actually context-free. The traditional way to speed up STAG parsing is to use the context-free subset of an STAG to form a Stochastic Tree Insertion Grammar (STIG) <ref> (Schabes and Waters, 1994) </ref>, an O (n 3 ) formalism, but this method has problems, because the STIG undergenerates since it is missing some elementary trees. A different approach would be to use multiple-pass parsing.
Reference: <author> Schabes, Yves. </author> <year> 1992. </year> <title> Stochastic lexicalized tree-adjoining grammars. </title> <booktitle> In Proceedings of the 14th International Conference on Computational Linguistics, </booktitle> <pages> pages 426-432, </pages> <address> Nantes, France, </address> <month> August. </month>
Reference: <author> Schwartz, Richard, Steve Austin, Francis Kubala, John Makhoul, Long Nguyen, Paul Placeway, and George Zavaliagkos. </author> <year> 1992. </year> <title> New uses for the n-best sentence hypothesis within the byb-los speech recognition system. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume I, </volume> <pages> pages 1-4, </pages> <address> San Francisco, California. </address>
Reference-contexts: Hwa (personal communication) using a model similar to PCFGs, Stochastic Lexicalized Tree Insertion Grammars, also was not able to obtain a speedup using this technique. There is previous work in the speech recognition community on automatically optimizing some parameters <ref> (Schwartz et al., 1992) </ref>. However, this previous work differed significantly from ours both in the techniques used, and in the parameters optimized. In particular, previous work focused on optimizing weights for various components, such as the language model component. In contrast, we optimize thresholding parameters.
Reference: <author> Stolcke, Andreas. </author> <year> 1993. </year> <title> An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. </title> <type> Technical Report TR-93-065, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA. </address>
Reference-contexts: The probability of eventually reaching the final state from any state is always 1. Thus, the forward probability is all that is needed. The same is true in some top down probabilistic parsing algorithms, such as stochastic versions of Earley's algorithm <ref> (Stolcke, 1993) </ref>. However, in a bottom-up algorithm, we need the extra factor that indicates the probability of getting from the start symbol to the nonterminal in question, which we approximate by the prior probability. <p> Unfortunately, there is no way to do this efficiently as part of the intermedi ate computation of a bottom-up chart parser. 1 We will approximate P (L) as follows: P (L) = i Y P (L i ) 1 Some other parsing techniques, such as stochastic versions of Earley parsers <ref> (Stolcke, 1993) </ref>, efficiently compute related probabilities, but we won't explore these parsers here. We confess that our real interest is in more complicated grammars, such as those that use head words. Grammars such as these can best be parsed bottom up. <p> Therefore, we doubled the size of the grammar so that there would be both primed and non-primed 4 We have skipped over details regarding our handling of unary branching nodes. Unary branching nodes are in general difficult to deal with <ref> (Stolcke, 1993) </ref>. The actual grammars we used contained additional symbols in such a way that there could not be more than one unary branch in a row. This greatly simplified computations, especially of the inside and outside probabilities.
Reference: <author> Wu, Dekai. </author> <year> 1996. </year> <title> A polynomial-time algorithm for statistical machine translation. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the ACL, </booktitle> <pages> pages 152-158, </pages> <address> Santa Cruz, CA, </address> <month> June. </month>
Reference-contexts: We have however had trouble using global thresholding with grammars that strongly violated the independence assumptions of global thresholding. One especially interesting possibility is to apply multiple-pass techniques to formalisms that require O (n 3 ) parsing time, such as Stochastic Bracketing Transduction Grammar (SBTG) <ref> (Wu, 1996) </ref> and Stochastic Tree Adjoining Grammars (STAG) (Resnik, 1992; Schabes, 1992). SBTG is a context-free-like formalism designed for translation from one language to another; it uses a four dimensional chart to index spans in both the source and target language simultaneously.
Reference: <author> Zavaliagkos, G., T. Anastasakos, G. Chou, C. Lapre, F. Kubala, J. Makhoul, L. Nguyen, R. Schwartz, and Y. Zhao. </author> <year> 1994. </year> <title> Improved search, acoustic and language modeling in the BBN Byblos large vocabulary CSR system. </title> <booktitle> In Proceedings of the ARPA Workshop on Spoken Language Technology, </booktitle> <pages> pages 81-88, </pages> <address> Plainsboro, New Jersey. </address>
Reference-contexts: We show that multiple-pass parsing techniques can yield large speedups. Multiple-pass parsing is a variation on a new technique in speech recognition, multiple-pass speech recognition <ref> (Zavaliagkos et al., 1994) </ref>, which we introduce first. 4.1 Multiple-Pass Speech Recognition In an idealized multiple-pass speech recognizer, we first run a simple pass, computing the forward and backward probabilities. This first pass runs relatively quickly.
References-found: 13

