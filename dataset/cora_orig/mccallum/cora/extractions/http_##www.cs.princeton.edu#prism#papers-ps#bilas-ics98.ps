URL: http://www.cs.princeton.edu/prism/papers-ps/bilas-ics98.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Email: fbilas, jpsg@cs.princeton.edu, iftode@cs.rutgers.edu  
Phone: 2  
Title: Evaluation of Hardware Write Propagation Support for Next-Generation Shared Virtual Memory Clusters  
Author: Angelos Bilas Liviu Iftode and Jaswinder Pal Singh 
Address: Princeton, NJ 08544  Piscataway, NJ 08855  
Affiliation: 1 Department of Computer Science, Princeton University  Department of Computer Science, Rutgers University  
Abstract: Clusters of symmetric multiprocessors (SMPs), connected by commodity system-area networks (SANs) and interfaces are fast being adopted as platforms for parallel computing. Page-grained shared virtual memory (SVM) is a popular way to support a coherent shared address space programming model on these clusters. Previous research has identified several key bottlenecks in the communication, protocol and application layers of a software SVM system that are not so significant in more mainstream, hardware-coherent multiprocessors. A key question for the communication layer is how much and what kind of hardware support is particularly valuable in improving the performance of such systems. This paper examines a popular form of hardware support|namely, support for automatic, hardware propagation of writes to remote memories|discussing new design issues and evaluating performance in the context of emerging clusters. Since much of the performance difference is due to differences in contention effects in various parts of the system, performance is examined through very detailed simulation, utilizing the deep visibility into the simulated system to analyze the causes of observed effects. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Amza, A. L. Cox, S. Dwarkadas, and W. Zwaenepoel. </author> <title> Software DSM protocols that adapt between single writer and multiple writer. </title> <booktitle> In Proc. of the 3rd IEEE Symp. on High-Performance Computer Architecture (HPCA-3), </booktitle> <pages> pages 261-271, </pages> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: Obviously f N 2 <ref> [0; 1] </ref>. Protocol efficiency is the ratio of total compute plus local stall time for all processors to the total elapsed time for all processors. Note that the sequential execution is not involved in this metric. 4 Applications We use the SPLASH-2 [29] application suite. <p> Our analysis confirms several key outstanding problems for home-based SVM systems, most of which have been observed earlier (e.g. <ref> [15, 16, 3, 5, 22, 1] </ref>), and that are not alleviated fully by AU.
Reference: [2] <author> R. Bianchini, L. Kontothanassis, R. Pinto, M. D. Maria, M. Abud, and C. Amorim. </author> <title> Hiding communication latency and coherence overhead in software dsms. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Our results for AURC on uniprocessor systems are consistent with the results obtained in [15]. The problem of write through caches is touched upon in [22], in the context of different SVM protocols with uniprocessor nodes. Bianchini et al. in <ref> [2] </ref> propose diffing in hardware as another form of hardware support for SVM. They assume write through caches and uniprocessor nodes. A dedicated protocol processor performs diffs on-the-fly and o*oads overheads from the computation processor.
Reference: [3] <author> A. Bilas, L. Iftode, R. Samanta, and J. P. Singh. </author> <title> Supporting a coherent shared address space across SMP nodes: An application-driven investigation. </title> <booktitle> In IMA Workshop on Parallel Algorithms and Parallel Systems, </booktitle> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: How beneficial AU support will be with these NIs rather than the customized NIs in SHRIMP or Memory Channel [11] is unclear. Finally, an important trend is the use of commodity SMPs rather than uniprocessors as the building blocks for clusters, which can improve software shared memory performance <ref> [8, 17, 3, 24] </ref>. In the protocol layer, all-software versions of the home based protocols have also recently been developed [14, 30]. <p> We use simulation because appropriate real systems don't exist, and real systems don't allow detailed enough analysis without additional hardware/software instrumentation of NIs. Most of the key performance bottlenecks themselves have been diagnosed in detail in earlier research <ref> [15, 16, 5, 3, 23] </ref>; the focus here is on the impact of write propagation support. The major remaining bottlenecks, with or without AU, also guide us to where application and system design energy should be spent for SVM. <p> Other possibilities include having the memory controller perform diffs in hardware on cache lines as they are written back to local memory, to detect the modified words. Details about extending home-based protocols for systems with SMP nodes can be found in <ref> [3, 23] </ref>. <p> The performance improvement in the other applications, which do not suffer from very high AU traffic, is marginal. Customized NIs make AU-based SVM more attractive for some applications. 9 Related Work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [8, 17, 10, 24, 3, 23] </ref> for different protocols. We focus here on studies that propose or evaluate hardware support. Our results for AURC on uniprocessor systems are consistent with the results obtained in [15]. <p> Our analysis confirms several key outstanding problems for home-based SVM systems, most of which have been observed earlier (e.g. <ref> [15, 16, 3, 5, 22, 1] </ref>), and that are not alleviated fully by AU.
Reference: [4] <author> A. Bilas, C. Liao, and J. P. Singh. </author> <title> Network interface support for shared virtual memory on clusters. </title> <type> Technical Report TR-579-98, </type> <institution> Computer Science Department, Princeton University, Princeton, NJ-08544, </institution> <month> Mar. </month> <year> 1998. </year>
Reference-contexts: Inspired by this, smarter NIs (like Myrinet) are used to o*oad some key mechanisms for explicit data movement (remote read and remote put, as opposed to the implicit write propagation studied here) and synchronization from the main processor in <ref> [4] </ref>, and the interactions with the protocol layer are examined. Holt et al. present a metric similar to protocol efficiency [12] to characterize performance on a hardware cache coherent machine. 10 Discussion and Conclusions We have examined the effectiveness of automatic update support for shared virtual memory on modern systems. <p> interface to perform some of the key general-purpose data movement and synchronization functions instead of interrupting the processor, most of which functionality is supported in the Virtual Interface Architecture industry standard API, and which may be aided by altering the laziness and other properties of the protocol layer as well <ref> [4] </ref>. In the protocol layer itself, the homes of locks and pages are often not assigned well with respect to computation in irregular applications, which can exacerbate end-point contention (e.g. Volrend). Adaptive placement may be useful here.
Reference: [5] <author> A. Bilas and J. P. Singh. </author> <title> The effects of communication parameters on end performance of shared virtual memory clusters. </title> <booktitle> In In Proceedings of Supercomputing 97, </booktitle> <address> San Jose, CA, </address> <month> November </month> <year> 1997. </year>
Reference-contexts: We use simulation because appropriate real systems don't exist, and real systems don't allow detailed enough analysis without additional hardware/software instrumentation of NIs. Most of the key performance bottlenecks themselves have been diagnosed in detail in earlier research <ref> [15, 16, 5, 3, 23] </ref>; the focus here is on the impact of write propagation support. The major remaining bottlenecks, with or without AU, also guide us to where application and system design energy should be spent for SVM. <p> Issuing an interprocessor interrupt costs 500 processor cycles, and invoking the handler is another 500 cycles. This is aggressive compared to what current operating systems provide, but is implementable [28] and prevents interrupt cost from swamping out the effects of other system parameters <ref> [5] </ref>. The page size is 4 KBytes, and the cost to access the TLB from a handler running in the kernel is 50 processor cycles. Each protocol handler is charged a cost depending on the work it does. <p> Results for using a coprocessor for diff computation and application for HLRC on the Intel Paragon are less optimistic [30]. The preeminence of interrupt cost among communication layer parameters is established in <ref> [5] </ref>. Inspired by this, smarter NIs (like Myrinet) are used to o*oad some key mechanisms for explicit data movement (remote read and remote put, as opposed to the implicit write propagation studied here) and synchronization from the main processor in [4], and the interactions with the protocol layer are examined. <p> Our analysis confirms several key outstanding problems for home-based SVM systems, most of which have been observed earlier (e.g. <ref> [15, 16, 3, 5, 22, 1] </ref>), and that are not alleviated fully by AU.
Reference: [6] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A virtual memory mapped network interface for the shrimp multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: The most popular form of hardware support used so far is the propagation of fine-grained writes to remote memories <ref> [6, 18, 11] </ref>. <p> Data are kept consistent according to a page-based software consistency protocol such as lazy release consistency [20]. Thus, consistency in maintained at page granularity, while there is some hardware support for fine-grained communication. The SHRIMP system <ref> [6] </ref> provides both the write snooping hardware as well as a customized network interface (NI) to support automatic update, and the Automatic Update Release Consistency (AURC) protocol has been implemented on it. <p> WB-AURC performs best by a significant amount. It does not suffer from AURC's contention-induced load balancing problems. 8 Customized Network Interfaces We also examined the impact of customized network interfaces like the one in SHRIMP <ref> [6] </ref> on AU based protocols. A customized NI allows for much shorter packet preparation occupancies since it consists of a dedicated state machine instead of a more general purpose processor on commodity interfaces like Myrinet.
Reference: [7] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Ku-lawik, C. L. Seitz, J. N. Seizovic, and W.-K. Su. Myrinet: </author> <title> A gigabit-per-second local area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: To use AU in SVM protocols, we must design new mechanisms that work with write-back caches. Another important development is the appearance of efficient commodity NIs such as Myrinet <ref> [7] </ref> in the marketplace. How beneficial AU support will be with these NIs rather than the customized NIs in SHRIMP or Memory Channel [11] is unclear. <p> The simulated architecture (Figure 2) models a cluster of 4-processor SMPs connected with a commodity interconnect like Myrinet <ref> [7] </ref>, for a total of 16 processors. Contention is modeled in detail at all levels except in the network links. The processor is P6-like, but is assumed to be a 1 IPC processor.
Reference: [8] <author> A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software versus hardware shared-memory implementation: A case study. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: How beneficial AU support will be with these NIs rather than the customized NIs in SHRIMP or Memory Channel [11] is unclear. Finally, an important trend is the use of commodity SMPs rather than uniprocessors as the building blocks for clusters, which can improve software shared memory performance <ref> [8, 17, 3, 24] </ref>. In the protocol layer, all-software versions of the home based protocols have also recently been developed [14, 30]. <p> The performance improvement in the other applications, which do not suffer from very high AU traffic, is marginal. Customized NIs make AU-based SVM more attractive for some applications. 9 Related Work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [8, 17, 10, 24, 3, 23] </ref> for different protocols. We focus here on studies that propose or evaluate hardware support. Our results for AURC on uniprocessor systems are consistent with the results obtained in [15].
Reference: [9] <author> C. Dubnicki, A. Bilas, K. Li, and J. Philbin. </author> <title> Design and implementation of Virtual Memory-Mapped Communication on Myrinet. </title> <booktitle> In Proceedings of the 1997 International Parallel Processing Symposium, </booktitle> <pages> pages 388-396, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: Each NI has two 1 MByte memory queues for incoming and outgoing packets. Network links operate at processor speed and are 16 bits wide. We assume a fast messaging library <ref> [9] </ref> that supports explicit messages. Initiating a message takes on the order of tens of I/O bus cycles. If the NI queues fill, the NI interrupts the main processor and delays it to allow queues to drain. A snooping device on the memory bus forwards AU traffic to the NI.
Reference: [10] <author> A. Erlichson, N. Nuckolls, G. Chesson, and J. Hennessy. SoftFLASH: </author> <title> analyzing the performance of clustered distributed virtual shared memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 210-220, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: The performance improvement in the other applications, which do not suffer from very high AU traffic, is marginal. Customized NIs make AU-based SVM more attractive for some applications. 9 Related Work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [8, 17, 10, 24, 3, 23] </ref> for different protocols. We focus here on studies that propose or evaluate hardware support. Our results for AURC on uniprocessor systems are consistent with the results obtained in [15].
Reference: [11] <author> R. Gillett, M. Collins, and D. Pimm. </author> <title> Overview of network memory channel for PCI. </title> <booktitle> In Proceedings of the IEEE Spring COMPCON '96, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: The most popular form of hardware support used so far is the propagation of fine-grained writes to remote memories <ref> [6, 18, 11] </ref>. <p> Another important development is the appearance of efficient commodity NIs such as Myrinet [7] in the marketplace. How beneficial AU support will be with these NIs rather than the customized NIs in SHRIMP or Memory Channel <ref> [11] </ref> is unclear. Finally, an important trend is the use of commodity SMPs rather than uniprocessors as the building blocks for clusters, which can improve software shared memory performance [8, 17, 3, 24].
Reference: [12] <author> C. Holt, M. Heinrich, J. P. Singh, , and J. L. Hennessy. </author> <title> The effects of latency and occupancy on the performance of dsm multiprocessors. </title> <type> Technical Report CSL-TR-95-xxx, </type> <institution> Stanford University, </institution> <year> 1995. </year>
Reference-contexts: Holt et al. present a metric similar to protocol efficiency <ref> [12] </ref> to characterize performance on a hardware cache coherent machine. 10 Discussion and Conclusions We have examined the effectiveness of automatic update support for shared virtual memory on modern systems.
Reference: [13] <author> L. Iftode, C. Dubnicki, E. W. Felten, and K. Li. </author> <title> Improving release-consistent shared virtual memory using automatic update. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Essentially, every shared page has a home node, and writes observed to a page are propagated to the home at a fine granularity in hardware <ref> [13, 15, 21] </ref>, without interrupting the processor at the home. In the automatic write propagation (also called automatic update or AU) approach, shared pages are mapped write-through in the caches so that writes can be snooped off the memory bus. <p> When a node incurs a page fault, the page fault handler retrieves the page from the home where it is guaranteed to be up to date <ref> [13, 15] </ref>. Data are kept consistent according to a page-based software consistency protocol such as lazy release consistency [20]. Thus, consistency in maintained at page granularity, while there is some hardware support for fine-grained communication. <p> With proper data placement, they do not exercise the update propagation features that distinguish these protocols. There is essentially no update or write 1 Pages in the home nodes can either be mapped write-through, as assumed in <ref> [13] </ref>, or can be mapped write-back since coherent DMA will provide the latest data to an incoming page fetch request; we assume the latter here since it performs better especially with SMPs. 2 If the processor architecture does not allow flushes at user level, this may have to be done through
Reference: [14] <author> L. Iftode, J. Singh, and K. Li. </author> <title> Scope consistency: a bridge between release consistency and entry consistency. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: Finally, an important trend is the use of commodity SMPs rather than uniprocessors as the building blocks for clusters, which can improve software shared memory performance [8, 17, 3, 24]. In the protocol layer, all-software versions of the home based protocols have also recently been developed <ref> [14, 30] </ref>. The result is a protocol very much like the hardware-supported AURC, except that propagation of changes to the home is done either at release or acquire time, using software diffs, rather than using AU or other write propagation at the time of the writes themselves.
Reference: [15] <author> L. Iftode, J. P. Singh, and K. Li. </author> <title> Understanding application performance on shared virtual memory. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Essentially, every shared page has a home node, and writes observed to a page are propagated to the home at a fine granularity in hardware <ref> [13, 15, 21] </ref>, without interrupting the processor at the home. In the automatic write propagation (also called automatic update or AU) approach, shared pages are mapped write-through in the caches so that writes can be snooped off the memory bus. <p> When a node incurs a page fault, the page fault handler retrieves the page from the home where it is guaranteed to be up to date <ref> [13, 15] </ref>. Data are kept consistent according to a page-based software consistency protocol such as lazy release consistency [20]. Thus, consistency in maintained at page granularity, while there is some hardware support for fine-grained communication. <p> Previous studies have shown these hardware-supported, home-based protocols to outperform earlier, non home-based all-software protocols, and have discussed the key causes of performance bottlenecks in each case <ref> [15] </ref>. How ever, the systems compared differ in both the communica-tion and the protocol layers, so the value of hardware AU support is not isolated. <p> We use simulation because appropriate real systems don't exist, and real systems don't allow detailed enough analysis without additional hardware/software instrumentation of NIs. Most of the key performance bottlenecks themselves have been diagnosed in detail in earlier research <ref> [15, 16, 5, 3, 23] </ref>; the focus here is on the impact of write propagation support. The major remaining bottlenecks, with or without AU, also guide us to where application and system design energy should be spent for SVM. <p> Note that the sequential execution is not involved in this metric. 4 Applications We use the SPLASH-2 [29] application suite. A more detailed classification and description of the application behavior for SVM systems with uniprocessor nodes is provided in <ref> [15] </ref>. Here we describe only the characteristics of greatest relevance to this study. The applications can be divided in two groups, regular and irregular. The regular applications are FFT, LU and Ocean. <p> Thus, in AURC we do not need to use a write through cache policy, and in HLRC we do not need to compute diffs. Protocol action is required only to fetch pages. The applications have different inherent and induced communication patterns <ref> [29, 15] </ref>, which affect their overall performance, but we should expect the different protocols to perform very similarly on their best versions. The irregular applications in our suite are Barnes, Radix, Raytrace, Volrend and Water. 5 Presentation of Results The next few sections present our results. <p> The simulator lets us separate these out too, and we call the former wait time and the latter the protocol cost. Idle time for locks is often increased greatly in SVM systems due to protocol activity and page misses occurring frequently inside critical sections and the resulting serialization <ref> [15, 16] </ref>, as well as due to synchronization requests getting stuck behind other messages in queues. 6 Automatic update support with write-back caches This section presents a new protocol (WB-AURC) that provides AU support with write-back caches, and its differences from AURC. 1 The problem with write-back caches for AU support <p> Third, page faults that occur within critical sections often have to be satisfied remotely (especially when and WB-AURC. the locks and protected data are migratory), dilating the critical section <ref> [15, 16] </ref>. This becomes worse as page fetches themselves incur contention, exacerbated by lack of locality in the assignment of page homes. <p> We focus here on studies that propose or evaluate hardware support. Our results for AURC on uniprocessor systems are consistent with the results obtained in <ref> [15] </ref>. The problem of write through caches is touched upon in [22], in the context of different SVM protocols with uniprocessor nodes. Bianchini et al. in [2] propose diffing in hardware as another form of hardware support for SVM. They assume write through caches and uniprocessor nodes. <p> While there are some exceptions, most often the advantages of the AU-based approaches are not very large, at least with commodity network interfaces, confirming that much of the performance benefit of AURC over traditional LRC in <ref> [15] </ref> comes more from the home-based nature of the protocol layer rather than from the AU support [30]. <p> Our analysis confirms several key outstanding problems for home-based SVM systems, most of which have been observed earlier (e.g. <ref> [15, 16, 3, 5, 22, 1] </ref>), and that are not alleviated fully by AU.
Reference: [16] <author> D. Jiang, H. Shan, and J. P. Singh. </author> <title> Application restructuring and performance portability across shared virtual memory and hardware-coherent multiprocessors. </title> <booktitle> In Proceedings of the 6th ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: We use simulation because appropriate real systems don't exist, and real systems don't allow detailed enough analysis without additional hardware/software instrumentation of NIs. Most of the key performance bottlenecks themselves have been diagnosed in detail in earlier research <ref> [15, 16, 5, 3, 23] </ref>; the focus here is on the impact of write propagation support. The major remaining bottlenecks, with or without AU, also guide us to where application and system design energy should be spent for SVM. <p> The simulator lets us separate these out too, and we call the former wait time and the latter the protocol cost. Idle time for locks is often increased greatly in SVM systems due to protocol activity and page misses occurring frequently inside critical sections and the resulting serialization <ref> [15, 16] </ref>, as well as due to synchronization requests getting stuck behind other messages in queues. 6 Automatic update support with write-back caches This section presents a new protocol (WB-AURC) that provides AU support with write-back caches, and its differences from AURC. 1 The problem with write-back caches for AU support <p> Volrend (Figure 6): Data traffic and wait time are not very substantial in Volrend. AURC does very well. The major problem arises from imbalances in the compute time and high lock wait time due to the high overhead of task stealing <ref> [16] </ref>. Locks in Volrend protect task queue entries and hence migratory data. <p> Third, page faults that occur within critical sections often have to be satisfied remotely (especially when and WB-AURC. the locks and protected data are migratory), dilating the critical section <ref> [15, 16] </ref>. This becomes worse as page fetches themselves incur contention, exacerbated by lack of locality in the assignment of page homes. <p> Our analysis confirms several key outstanding problems for home-based SVM systems, most of which have been observed earlier (e.g. <ref> [15, 16, 3, 5, 22, 1] </ref>), and that are not alleviated fully by AU. <p> Protocol overheads and contention due to diffing and page invalidations at barriers also requires addressing. Finally, an interesting observation is that improvements to the application layer <ref> [16] </ref> increase performance much more than automatic update support (e.g. Barnes in this study), and these improvements tend to reduce the differences among protocols.
Reference: [17] <author> M. Karlsson and P. Stenstrom. </author> <title> Performance evaluation of cluster-based multiprocessor built from atm switches and bus-based multiprocessor servers. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: How beneficial AU support will be with these NIs rather than the customized NIs in SHRIMP or Memory Channel [11] is unclear. Finally, an important trend is the use of commodity SMPs rather than uniprocessors as the building blocks for clusters, which can improve software shared memory performance <ref> [8, 17, 3, 24] </ref>. In the protocol layer, all-software versions of the home based protocols have also recently been developed [14, 30]. <p> The performance improvement in the other applications, which do not suffer from very high AU traffic, is marginal. Customized NIs make AU-based SVM more attractive for some applications. 9 Related Work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [8, 17, 10, 24, 3, 23] </ref> for different protocols. We focus here on studies that propose or evaluate hardware support. Our results for AURC on uniprocessor systems are consistent with the results obtained in [15].
Reference: [18] <author> M. G. H. Katevenis, E. P. Markatos, G. Kalokerinos, and A. Dollas. Telegraphos: </author> <title> A substrate for high-performance computing on workstation clusters. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 43(2) </volume> <pages> 94-108, </pages> <month> 15 June </month> <year> 1997. </year>
Reference-contexts: The most popular form of hardware support used so far is the propagation of fine-grained writes to remote memories <ref> [6, 18, 11] </ref>.
Reference: [19] <author> P. Keleher, A. Cox, S. Dwarkadas, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: They assume write through caches and uniprocessor nodes. A dedicated protocol processor performs diffs on-the-fly and o*oads overheads from the computation processor. Their simulations show that using the coprocessor can double the performance of TreadMarks <ref> [19] </ref> on a 16-node configuration. Results for using a coprocessor for diff computation and application for HLRC on the Intel Paragon are less optimistic [30]. The preeminence of interrupt cost among communication layer parameters is established in [5].
Reference: [20] <author> P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Lazy consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: When a node incurs a page fault, the page fault handler retrieves the page from the home where it is guaranteed to be up to date [13, 15]. Data are kept consistent according to a page-based software consistency protocol such as lazy release consistency <ref> [20] </ref>. Thus, consistency in maintained at page granularity, while there is some hardware support for fine-grained communication.
Reference: [21] <author> L. I. Kontothanassis, G. Hunt, R. Stets, N. Hardavellas, M. Cierniak, S. Parthasarathy, W. Meira, Jr., S. Dwarkadas, and M. L. Scott. </author> <title> VM-based shared memory on low-latency, remote-memory-access networks. </title> <booktitle> In Proc. of the 24th Annual Int'l Symp. on Computer Architecture (ISCA'97), </booktitle> <pages> pages 157-169, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Essentially, every shared page has a home node, and writes observed to a page are propagated to the home at a fine granularity in hardware <ref> [13, 15, 21] </ref>, without interrupting the processor at the home. In the automatic write propagation (also called automatic update or AU) approach, shared pages are mapped write-through in the caches so that writes can be snooped off the memory bus.
Reference: [22] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> High performance software coherence for current and future architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: We focus here on studies that propose or evaluate hardware support. Our results for AURC on uniprocessor systems are consistent with the results obtained in [15]. The problem of write through caches is touched upon in <ref> [22] </ref>, in the context of different SVM protocols with uniprocessor nodes. Bianchini et al. in [2] propose diffing in hardware as another form of hardware support for SVM. They assume write through caches and uniprocessor nodes. A dedicated protocol processor performs diffs on-the-fly and o*oads overheads from the computation processor. <p> Our analysis confirms several key outstanding problems for home-based SVM systems, most of which have been observed earlier (e.g. <ref> [15, 16, 3, 5, 22, 1] </ref>), and that are not alleviated fully by AU.
Reference: [23] <author> R. Samanta, A. Bilas, L. Iftode, and J. P. Singh. </author> <title> Home-based svm protocols for smp clusters: Design, simulations, implementation and performance. </title> <booktitle> In Proceedings of the 4th International Symposium on High Performance Computer Architecture, </booktitle> <address> Las Vegas, </address> <month> February </month> <year> 1998. </year>
Reference-contexts: We use simulation because appropriate real systems don't exist, and real systems don't allow detailed enough analysis without additional hardware/software instrumentation of NIs. Most of the key performance bottlenecks themselves have been diagnosed in detail in earlier research <ref> [15, 16, 5, 3, 23] </ref>; the focus here is on the impact of write propagation support. The major remaining bottlenecks, with or without AU, also guide us to where application and system design energy should be spent for SVM. <p> Other possibilities include having the memory controller perform diffs in hardware on cache lines as they are written back to local memory, to detect the modified words. Details about extending home-based protocols for systems with SMP nodes can be found in <ref> [3, 23] </ref>. <p> The performance improvement in the other applications, which do not suffer from very high AU traffic, is marginal. Customized NIs make AU-based SVM more attractive for some applications. 9 Related Work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [8, 17, 10, 24, 3, 23] </ref> for different protocols. We focus here on studies that propose or evaluate hardware support. Our results for AURC on uniprocessor systems are consistent with the results obtained in [15].
Reference: [24] <author> D. Scales, K. Gharachorloo, and C. Thekkath. </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grain shared memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: How beneficial AU support will be with these NIs rather than the customized NIs in SHRIMP or Memory Channel [11] is unclear. Finally, an important trend is the use of commodity SMPs rather than uniprocessors as the building blocks for clusters, which can improve software shared memory performance <ref> [8, 17, 3, 24] </ref>. In the protocol layer, all-software versions of the home based protocols have also recently been developed [14, 30]. <p> The performance improvement in the other applications, which do not suffer from very high AU traffic, is marginal. Customized NIs make AU-based SVM more attractive for some applications. 9 Related Work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [8, 17, 10, 24, 3, 23] </ref> for different protocols. We focus here on studies that propose or evaluate hardware support. Our results for AURC on uniprocessor systems are consistent with the results obtained in [15].
Reference: [25] <author> A. Sharma, A. T. Nguyen, J. Torellas, M. Michael, and J. Carbajal. Augmint: </author> <title> a multiprocessor simulation environment for Intel x86 architectures. </title> <type> Technical report, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: is very intrusive into the underlying node; (iii) overall, the benefits do not appear to justify the design effort with commodity NIs, and NIs customized for AU packet generation may be important for using AU effectively. 2 Simulated Platforms The simulation environment we use is built on top of aug-mint <ref> [25] </ref>, an execution driven simulator using the x86 instruction set, and runs on x86 systems. The simulated architecture (Figure 2) models a cluster of 4-processor SMPs connected with a commodity interconnect like Myrinet [7], for a total of 16 processors.
Reference: [26] <author> J. P. Singh, A. Bilas, D. Jiang, and Y. Zhou. </author> <title> Limits to the performance of software shared memory: A layered approach. </title> <type> Technical Report TR-576-98, </type> <institution> Computer Science Department, Princeton University, Princeton, NJ-08544, </institution> <month> Nov. </month> <year> 1997. </year>
Reference-contexts: 1 Introduction The performance of an application running on a parallel system is affected by several layers of software and hardware. For page-grained shared virtual memory (SVM) on clusters, the layers are as shown in Figure 1 <ref> [26] </ref>. A key question for such systems is how much and what kind of limited hardware support is most effective in accelerating their performance, thus bringing it closer to that of hardware coherence.
Reference: [27] <author> K. Skadron and D. W. Clark. </author> <title> Design issues and tradeoffs for write buffers. </title> <booktitle> In The 3nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb </month> <year> 1997. </year>
Reference-contexts: The processor is P6-like, but is assumed to be a 1 IPC processor. The data cache hierarchy consists of a 8 KBytes first-level direct mapped write-through cache and a 512 KBytes second-level two-way set associative cache, each with a line size of 32 Bytes. The write buffer <ref> [27] </ref> has 26 entries, 1 cache line wide each, and a retire-at-4 policy. The read hit cost is one cycle in the write buffer and first level cache and 10 cycles in the second-level cache. The memory subsystem is fully pipelined.
Reference: [28] <author> D. Stodolsky, J. B. Chen, and B. Bershad. </author> <title> Fast interrupt priority management in operating system kernels. </title> <booktitle> In USENIX Association, editor, Proceedings of the USENIX Symposium on Microkernels and Other Kernel Architectures: </booktitle> <address> Septem-ber 20-21, 1993, San Diego, California, USA, </address> <pages> pages 105-110, </pages> <address> Berkeley, CA, USA, Sept. 1993. </address> <publisher> USENIX. </publisher>
Reference-contexts: AU writes within a cache line are combined in the NI to reduce the number of packets. Issuing an interprocessor interrupt costs 500 processor cycles, and invoking the handler is another 500 cycles. This is aggressive compared to what current operating systems provide, but is implementable <ref> [28] </ref> and prevents interrupt cost from swamping out the effects of other system parameters [5]. The page size is 4 KBytes, and the cost to access the TLB from a handler running in the kernel is 50 processor cycles.
Reference: [29] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta. </author> <title> Methodological considerations and characterization of the SPLASH-2 parallel application suite. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Obviously f N 2 [0; 1]. Protocol efficiency is the ratio of total compute plus local stall time for all processors to the total elapsed time for all processors. Note that the sequential execution is not involved in this metric. 4 Applications We use the SPLASH-2 <ref> [29] </ref> application suite. A more detailed classification and description of the application behavior for SVM systems with uniprocessor nodes is provided in [15]. Here we describe only the characteristics of greatest relevance to this study. The applications can be divided in two groups, regular and irregular. <p> Thus, in AURC we do not need to use a write through cache policy, and in HLRC we do not need to compute diffs. Protocol action is required only to fetch pages. The applications have different inherent and induced communication patterns <ref> [29, 15] </ref>, which affect their overall performance, but we should expect the different protocols to perform very similarly on their best versions. The irregular applications in our suite are Barnes, Radix, Raytrace, Volrend and Water. 5 Presentation of Results The next few sections present our results.
Reference: [30] <author> Y. Zhou, L. Iftode, and K. Li. </author> <title> Performance evaluation of two home-based lazy release consistency protocols for shared virtual memory systems. </title> <booktitle> In Proceedings of the Operating Systems Design and Implementation Symposium, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Finally, an important trend is the use of commodity SMPs rather than uniprocessors as the building blocks for clusters, which can improve software shared memory performance [8, 17, 3, 24]. In the protocol layer, all-software versions of the home based protocols have also recently been developed <ref> [14, 30] </ref>. The result is a protocol very much like the hardware-supported AURC, except that propagation of changes to the home is done either at release or acquire time, using software diffs, rather than using AU or other write propagation at the time of the writes themselves. <p> In an evaluation on the Intel Paragon multiprocessor <ref> [30] </ref>, this software home-based protocol (called HLRC) was found to outperform earlier distributed all-software protocols even without hardware support. <p> Their simulations show that using the coprocessor can double the performance of TreadMarks [19] on a 16-node configuration. Results for using a coprocessor for diff computation and application for HLRC on the Intel Paragon are less optimistic <ref> [30] </ref>. The preeminence of interrupt cost among communication layer parameters is established in [5]. <p> some exceptions, most often the advantages of the AU-based approaches are not very large, at least with commodity network interfaces, confirming that much of the performance benefit of AURC over traditional LRC in [15] comes more from the home-based nature of the protocol layer rather than from the AU support <ref> [30] </ref>. The use of lower-occupancy, customized rather than commodity NIs improves AURC and WB-AURC performance substantially for the applications in which outgoing AU traffic is a problem; namely, Radix and to some extent Water-spatial.
References-found: 30

