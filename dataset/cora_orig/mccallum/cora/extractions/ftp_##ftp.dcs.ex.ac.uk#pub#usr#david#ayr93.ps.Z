URL: ftp://ftp.dcs.ex.ac.uk/pub/usr/david/ayr93.ps.Z
Refering-URL: http://www.dcs.ex.ac.uk/~david/research/york.html
Root-URL: http://www.dcs.ex.ac.uk
Title: Profiling Parallel Functional Computations (Without Parallel Machines)  
Author: Colin Runciman and David Wakeling 
Affiliation: University of York  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> L. Augustsson and T. Johnsson. </author> <title> Parallel Graph Reduction with the h-,Gi-machine. </title> <booktitle> In Proceedings of the 1989 Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 202-213. </pages> <publisher> ACM Press, </publisher> <month> September </month> <year> 1989. </year>
Reference-contexts: When an overflow is about to occur a new stack frame with sufficient extra space is allocated in the heap and the contents of the old frame are copied to the new one. In <ref> [1] </ref>, Augustsson and Johnsson observed that about 70% of EVAL instructions immediately encounter a node in normal form. When this happens, the stack frame set aside for the EVAL is not used.
Reference: [2] <author> L. Augustsson and T. Johnsson. </author> <title> The Chalmers Lazy-ML Compiler. </title> <journal> Computer Journal, </journal> <volume> 32(2) </volume> <pages> 127-141, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Below, we shall assume some familiarity with both the Chalmers compiler and the basics of parallel graph reduction; for those without such familiarity, we strongly recommend Augustsson and Johnsson's overview paper <ref> [2] </ref> and Peyton Jones' textbook [9]. 4 The easiest way to make a quasi-parallel HBC compiler is to extend the standard G--machine to allow multiple threads of control, each represented by a different task. The processor switches between these tasks giving each a small "slice" of its time.
Reference: [3] <author> C. Clack and S. L. Peyton Jones. </author> <title> The Four-Stroke Reduction Engine. </title> <booktitle> In Proceedings of the 1986 ACM Symposium on LISP and Functional Programming, </booktitle> <pages> pages 220-232. </pages> <publisher> ACM Press, </publisher> <month> August </month> <year> 1986. </year>
Reference-contexts: Kelly's Caliban [8] requires the program to be transformed so that it corresponds to a process network; annotations are then added to identify the tasks which should be allocated to neighbouring processors. Variants of the spark model <ref> [3] </ref> proposed by Clack and Pey-ton Jones are widely used: when the program is run, annotated expressions are evaluated in parallel if there is a processor available.
Reference: [4] <author> D. L. Eager, J. Zahorjan, and E. D. Lazowska. </author> <title> Speedup Versus Efficiency in Parallel Systems. </title> <type> Technical Report 86-12, </type> <institution> Department of Computational Science, University of Saskatchewan, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: The parallelism figure is calculated as the area of the band representing running tasks divided by the time that the program takes to run. When the number of processors is unlimited, this is the average parallelism as defined by Eager <ref> [4] </ref>. 4 Implementation Our implementation centres on a version of the standard Chalmers HBC compiler modified to produce programs that run in quasi-parallel on an ordinary workstation.
Reference: [5] <author> A. Gibson and W. Rytter. </author> <title> Efficient Parallel algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: Also, task-blocking is something to be avoided so far as possible. As the size of input data rises, these properties should continue to hold. Just as we prefer sequential programs whose costs rise only linearly with data size, so-called "optimal parallel" algorithms have the property <ref> [5] </ref>: no. of processors fi computation time = O (n) We deliberately ignore (in this paper) the cost of communication between tasks, excepting only the cost of keeping a task that is blocked waiting for another to reduce something.
Reference: [6] <author> P. Hudak. </author> <title> Para-functional Programming in Haskell. </title> <editor> In B. K. Szymanski, editor, </editor> <booktitle> Parallel Functional Languages and Compilers, chapter 5, </booktitle> <pages> pages 159-196. </pages> <publisher> Addison-Wesley (Reading), </publisher> <year> 1991. </year>
Reference-contexts: Secondly, annotations mean that the compiler does not have to tackle the difficult problem of determining a suitable parallel evaluation strategy for the program. That remains the programmer's responsibility. Many authors have described annotation schemes for parallel functional programming. Hudak's Para-functional programming <ref> [6] </ref> allows the programmer to describe the mapping of parallel tasks onto the processor configuration of the machine at hand.
Reference: [7] <author> P. Hudak, S. L. Peyton Jones, Arvind, B. Boutel, J. Fairbairn, J. Fasel, M. Guzman, K. Hammond, J. Hughes, T. Johnsson, R. Kieburtz, R. S. Nikhil, W. Partain, and J. Peterson. </author> <title> Report on the Functional Programming Language Haskell, Version 1.2. </title> <journal> SIGPLAN Notices, </journal> <volume> 27(5), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: So, by using a quasi-parallel implementation and ignoring the extra constraints imposed by a parallel computer, we are by no means avoiding "all the hard problems". 2 Profiling Parallelism Our parallel functional programs are written in Haskell <ref> [7] </ref> using two extra combinators, seq and par.
Reference: [8] <author> P. H. J. Kelly. </author> <title> Functional Programming for Loosely-Coupled Multiprocessors. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: That remains the programmer's responsibility. Many authors have described annotation schemes for parallel functional programming. Hudak's Para-functional programming [6] allows the programmer to describe the mapping of parallel tasks onto the processor configuration of the machine at hand. Kelly's Caliban <ref> [8] </ref> requires the program to be transformed so that it corresponds to a process network; annotations are then added to identify the tasks which should be allocated to neighbouring processors.
Reference: [9] <editor> S. L. Peyton Jones. </editor> <booktitle> The Implementation of Functional Programming Languages. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1987. </year>
Reference-contexts: Below, we shall assume some familiarity with both the Chalmers compiler and the basics of parallel graph reduction; for those without such familiarity, we strongly recommend Augustsson and Johnsson's overview paper [2] and Peyton Jones' textbook <ref> [9] </ref>. 4 The easiest way to make a quasi-parallel HBC compiler is to extend the standard G--machine to allow multiple threads of control, each represented by a different task. The processor switches between these tasks giving each a small "slice" of its time.
Reference: [10] <author> R. Plasmeijer and M. van Eekelen. </author> <title> Functional Programming and Parallel Graph Rewriting. </title> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference-contexts: Nonetheless, it has already proved useful as a second system for users of the GRIP machine to compare the quasi-parallel and parallel behaviour of their programs. Others have investigated the behaviour of parallel functional programs with the aid of different quasi-parallel implementations <ref> [11, 10] </ref>. We are currently working on increasing the amount of information recorded in a log file, and on new ways of presenting this information.
Reference: [11] <author> P. Roe. </author> <title> Parallel Programming Using Functional Languages. </title> <type> PhD thesis, </type> <institution> University of Glasgow, </institution> <month> April </month> <year> 1991. </year> <month> 16 </month>
Reference-contexts: Nonetheless, it has already proved useful as a second system for users of the GRIP machine to compare the quasi-parallel and parallel behaviour of their programs. Others have investigated the behaviour of parallel functional programs with the aid of different quasi-parallel implementations <ref> [11, 10] </ref>. We are currently working on increasing the amount of information recorded in a log file, and on new ways of presenting this information.
References-found: 11

