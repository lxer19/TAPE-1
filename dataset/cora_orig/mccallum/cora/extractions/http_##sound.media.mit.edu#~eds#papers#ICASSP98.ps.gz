URL: http://sound.media.mit.edu/~eds/papers/ICASSP98.ps.gz
Refering-URL: http://sound.media.mit.edu/papers.html
Root-URL: http://www.media.mit.edu
Email: eds@media.mit.edu  
Title: THE MPEG-4 STRUCTURED AUDIO STANDARD  
Author: Eric D. Scheirer 
Address: Cambridge MA 02139  
Affiliation: Laboratory  
Note: To appear in Proc. IEEE ICASSP 1998. This article copyright (c) 1998 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for creating new works or to reuse any component of this work in other works must be obtained from the IEEE.  
Pubnum: E15-401D MIT Media  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Bristow-Johnson. </author> <title> Wavetable synthesis 101: a fundamental perspective, </title> <booktitle> Proc AES 101 st Reprint #4400). </booktitle>
Reference-contexts: Since most of the complexity in SAOL is encapsulated in these primitive functions, they may be carefully optimized for each particular implementation. The bitstream header may also contain audio samples (blocks of floating-point data). This data is typically used in wavetable synthesis <ref> [1] </ref>, but may be used for any other purpose in SAOL in addition or instead.
Reference: [2] <author> M. A. Casey and P. Smaragdis, Netsound. </author> <booktitle> Proc Int. Computer Music Conf., </booktitle> <address> Hong Kong, </address> <year> 1996. </year>
Reference-contexts: The technical basis of such a description is familiar to computer musicians, as it is similar to previous music languages such as Music V and Csound [9]. In fact, a previous (non-standardized) structured audio experiment called NetSound <ref> [2] </ref> made use of Csound to perform the synthesis. For MPEG-4 certain changes have been made to this framework.
Reference: [3] <author> A. Eleftheriadis, C. Herpel, G. Rajan, L. Ward (Eds). </author> <note> ISO 14496-1 (MPEG-4 Systems) Committee Draft. MPEG document N1901, Fribourg CH, </note> <year> 1997. </year>
Reference-contexts: The mixing process is specified using the MPEG-4 Binary Format for Scene Description, or BIFS. In general, BIFS is similar in concept to the virtual-reality description language VRML, but the audio component in particular has been expanded in functionality. BIFS is standardized as part of the MPEG-4 Systems <ref> [3] </ref> toolset; audio experts have been involved in developing the audio aspects of this tool.
Reference: [4] <author> B. Grill, B. Edler, I. Kaneko, Y. Lee, M. Nishiguchi, E. D. Scheirer, and M. Vnnen (Eds). </author> <title> ISO 14496-3 (MPEG-4 Audio) Committee Draft. MPEG document N1903, </title> <type> Fribourg CH, </type> <year> 1997. </year>
Reference-contexts: Among these new tools is an audio coder called Structured Audio (strictly, ISO 14496-3 subpart 5 <ref> [4] </ref>). Structured coding schemes have been described as data formats which make use of low-dimensional, semantic, and/or model-based representations of multimedia content [10]. The MPEG-4 Structured Audio standard has been created by MPEG to allow for standardized description of synthetic sounds, and guaranteed reproduction performance on all types of terminals. <p> There are five major elements to the Structured Audio toolset. Each of them will be described briefly (the brief descriptions are taken, in slightly modified form from <ref> [4] </ref>), and a discussion of the manner in which they are unified in the overall decoding framework follows. 2.1. Components The Structured Audio Orchestra Language, or SAOL, is the synthesis-description language at the heart of the standard.
Reference: [5] <author> MIDI Manufacturers Association. </author> <title> The Complete MIDI 1.0 Detailed Specification v. </title> <address> 96.2, </address> <year> 1996. </year>
Reference-contexts: Like a traditional synthesizer, the Structured Audio system can process commands in the MIDI protocol; they are internally converted into native events by the scheduler. The syntax of these commands is the same as that standardized in the relevant MIDI documentation <ref> [5] </ref>, but certain semantics are changed to enable tight normative control over the synthesis process. In addition, a second control language, SASL, is standardized within MPEG-4. SASL is more flexible than MIDI and allows more complex control functions and mappings to be specified than are possible with MIDI.
Reference: [6] <author> S. Quackenbush, </author> <title> Coding of natural audio in MPEG-4. </title> <booktitle> Proc IEEE ICASSP, </booktitle> <address> Seattle, </address> <note> 1998 (this volume). </note>
Reference-contexts: Interaction with Natural Audio Besides the Structured Audio toolset (and the other synthetic audio component, the Text-to-Speech interface), MPEG-4 standardizes state-of-the-art versions of several traditional audio formats, called natural content coders in the MPEG framework <ref> [6] </ref>. A great deal of the flexibility in MPEG-4 comes from the juxtaposition and interface of these coders with the synthetic coders; Synthetic/Natural Hybrid Coding, or SNHC, is an important concept in both the audio and visual toolsets of MPEG-4.
Reference: [7] <author> E. D. Scheirer, </author> <title> Structured audio and effects processing in the MPEG-4 multimedia standard, </title> <booktitle> ACM Multimedia Systems, </booktitle> <publisher> in press. </publisher>
Reference-contexts: Such a sound only requires as much bandwidth as is needed for the flat speech, plus a tiny overhead for the reverberator. These concepts, of juxtaposing real and synthetic sounds and mixing and combining them with synthetic post-production, are explored in more depth in <ref> [7] </ref>. 3. PROFILES AND APPLICATIONS As in previous MPEG standards, MPEG-4 describes the definition of several profiles which allow the implementation of terminals compliant to a subset of the full standard. There are three restricted profiles of the full MPEG-4 Structured Audio standard, each targeted at certain applications.
Reference: [8] <author> J. O. Smith III, </author> <title> Viewpoints on the history of digital synthesis, Keynote paper, </title> <booktitle> Proc Int. Computer Music Conf, </booktitle> <address> Montreal, </address> <year> 1991. </year>
Reference-contexts: O. Smith, <ref> [8] </ref>) Injecting advanced technology into the marketplace in the form of an open standard with sufficiently broad application may help to promote a continuing evolution of commercial musicsynthesis devices. It is important to realize that there is no inherent sound quality created with synthetic sound.
Reference: [9] <author> B. L. Vercoe. Csound: </author> <title> a manual for the audio-processing system. </title> <publisher> MIT Media Lab, </publisher> <year> 1995. </year>
Reference-contexts: THE STRUCTURED AUDIO TOOLSET The MPEG-4 Structured Audio toolset is based around a software synthesizer-description language. The technical basis of such a description is familiar to computer musicians, as it is similar to previous music languages such as Music V and Csound <ref> [9] </ref>. In fact, a previous (non-standardized) structured audio experiment called NetSound [2] made use of Csound to perform the synthesis. For MPEG-4 certain changes have been made to this framework.
Reference: [10] <author> B. L. Vercoe, W. G. Gardner and E. D. Scheirer. </author> <title> Structured Audio: Creation, transmission, and rendering of parametric sound representations, </title> <booktitle> Proc. </booktitle> <publisher> IEEE, in press. </publisher>
Reference-contexts: Among these new tools is an audio coder called Structured Audio (strictly, ISO 14496-3 subpart 5 [4]). Structured coding schemes have been described as data formats which make use of low-dimensional, semantic, and/or model-based representations of multimedia content <ref> [10] </ref>. The MPEG-4 Structured Audio standard has been created by MPEG to allow for standardized description of synthetic sounds, and guaranteed reproduction performance on all types of terminals.
References-found: 10

