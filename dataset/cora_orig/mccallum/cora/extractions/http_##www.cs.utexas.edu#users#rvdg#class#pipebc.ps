URL: http://www.cs.utexas.edu/users/rvdg/class/pipebc.ps
Refering-URL: http://www.cs.utexas.edu/users/rvdg/class/materials.html
Root-URL: 
Title: A PIPELINED BROADCAST FOR MULTIDIMENSIONAL MESHES  
Author: JERRELL WATTS and ROBERT VAN DE GEIJN 
Keyword: broadcast, pipelining, communication, mesh, torus  
Address: Pasadena, California 91125, USA  Austin, Texas 78712, USA  
Affiliation: Scalable Concurrent Programming Laboratory, California Institute of Technology,  Department of Computer Sciences, University of Texas at Austin,  
Note: Parallel Processing Letters c World Scientific Publishing Company  Received (received date) Revised (revised date) Communicated by (Name of Editor)  
Abstract: We address the problem of performing a pipelined broadcast on a mesh architecture. Meshes require a different approach than other topologies, and their very nature puts a tighter bound on the performance that one can hope to achieve. By using the appropriate techniques, however, one can obtain excellent performance for sufficiently long messages. The resulting algorithm will work on meshes of any dimension with any number of nodes. Our model assumes that the mesh is a torus and/or that it has bidirectional links and uses wormhole routing. Performance data from the Cray T3D are included. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. Barnett, D. Payne, R. van de Geijn and J. Watts, </author> <title> Broadcasting on meshes with worm-hole routing, </title> <type> Technical Report TR-93-24, </type> <institution> Department of Computer Sciences, the University of Texas at Austin, </institution> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Therefore, one must use a suite of algorithms (and their hybrids) to achieve piecewise optimality over a wide range of parameters [2, 3]. Previously, we presented the recursive-splitting (RS) and scatter-collect (SC) broadcasts, efficient algorithms for short and long messages, respectively <ref> [1] </ref>. These routines could then be hybridized to produce an algorithm that is efficient over a wide range of message lengths, 1 as we have shown in [2, 3]. <p> While the idea of a pipelined broadcast is certainly not a new one, methods for implementing them efficiently on a modern mesh interconnect topology have, to our knowledge, not yet been found. Attempts have been made in <ref> [1, 5, 12, 13] </ref>, but these techniques either rely on assumptions that are not applicable to today's mesh-based architectures, or they do not achieve the same performance possible with pipelined broadcasts on other topologies, such as hypercubes [7]. <p> Typically, various nodes receive multiple messages simultaneously, greatly increasing the running time of the broadcast and producing very erratic behavior. This phenomenon was illustrated by the EDF broadcast in <ref> [1] </ref>. Explicit synchronization can alleviate this problem but would greatly increase the running time of the algorithm. The best approach would then seem to be one that reduces pipeline depth and requires as little explicit synchronization as possible to function properly. <p> 3 holds, a broadcast will require (log 2 (p)) steps. (Given Assumption 3, the number of nodes to which a message has been sent at each stage in an algorithm can at most double.) Optimally, this sequence of message sends forms a conflict-free logical minimum spanning tree within the network <ref> [1] </ref>. On meshes and hypercubes, the depth of this spanning tree is (log 2 (p)). In a pipelined broadcast, one divides the message into many parts and overlaps the transmission stages. <p> the minimum pipeline depth of a d-dimensional mesh is approximately the sum of its dimensions: m 0 + m 1 + : : : + m d1 = (dp 1=d ). c In previous work, we demonstrated how to perform a pipelined broadcast on a r fi c two-dimensional mesh <ref> [1] </ref>. <p> One might pipeline the entire message along the first dimension then along the second dimension. In this case, the running time of the broadcast, T DOP L , would have a lower bound of roughly 2nfi, which is no better than the SC broadcast we presented previously in <ref> [1] </ref>. By cutting the message in half, one can improve on this bound considerably. To start, pipeline the first half along the root node's row. Then, pipeline the second half along the root node's column, and at the same time, pipeline the first half along the other columns. <p> Comparison to Other Algorithms In this section, the DOPL broadcast is compared with the SC and EDF broadcasts originally presented in <ref> [1] </ref>. The comparisons are made for running times for variable message length and fixed mesh size (i.e., raw performance) and for fixed message length and vari able mesh size (i.e., scalability). In terms of both predicted and empirical times, the DOPL 7 Fig. 5. <p> As a result, the value of fi must be adjusted to compensate. The predicted execution times are given by Eq. 3 and 6 as well as those in <ref> [1] </ref>. For the T3D, ff = 4:5sec and fi is varied according to the inverse of the bandwidths shown in Fig. 7. Fig. 8 and 9 show the predicted versus actual times for various message lengths and partition sizes, respectively.
Reference: 2. <author> M. Barnett, S. Gupta, D. Payne, L. Shuler, R. van de Geijn and J. Watts, </author> <title> Interpro-cessor Collective Communications Library (InterCom), </title> <booktitle> in Proc. Scalable High Perf. Comput. Conf., </booktitle> <publisher> (IEEE Computer Society Press, </publisher> <year> 1994) </year> <month> 357-364. </month>
Reference-contexts: For longer messages, data transmission time dominates, so the best algorithm minimizes data flow and contention, even if additional message operations are required to do so. Therefore, one must use a suite of algorithms (and their hybrids) to achieve piecewise optimality over a wide range of parameters <ref> [2, 3] </ref>. Previously, we presented the recursive-splitting (RS) and scatter-collect (SC) broadcasts, efficient algorithms for short and long messages, respectively [1]. These routines could then be hybridized to produce an algorithm that is efficient over a wide range of message lengths, 1 as we have shown in [2, 3]. <p> range of parameters <ref> [2, 3] </ref>. Previously, we presented the recursive-splitting (RS) and scatter-collect (SC) broadcasts, efficient algorithms for short and long messages, respectively [1]. These routines could then be hybridized to produce an algorithm that is efficient over a wide range of message lengths, 1 as we have shown in [2, 3]. Although the strategy presented in those papers is very effective, the pure SC broadcast is still a factor of two worse than the theoretical optimal for very long messages. <p> One way to improve performance in this range would be to switch from the SC broadcast to a pipelined broadcast when the message reaches a certain size. (This crossover point can be accurately determined by using the execution time formulae for the algorithms used.) This strategy was not adopted in <ref> [2, 3] </ref> because no good mesh-based pipelined algorithm was then available; theoretically superior pipelined algorithms were outperformed by the simpler SC broadcast. Also, in those papers, we investigated the implementation of an entire collective communication library, for which the SC broadcast was more convenient.
Reference: 3. <author> M. Barnett, S. Gupta, D. Payne, L. Shuler, R. van de Geijn and J. Watts, </author> <title> Building a Collective Communications Library, </title> <booktitle> to appear in Proc. Supercomputing `94, </booktitle> <address> Washington, DC, </address> <month> Nov. </month> <pages> 14-18, </pages> <year> 1994. </year>
Reference-contexts: For longer messages, data transmission time dominates, so the best algorithm minimizes data flow and contention, even if additional message operations are required to do so. Therefore, one must use a suite of algorithms (and their hybrids) to achieve piecewise optimality over a wide range of parameters <ref> [2, 3] </ref>. Previously, we presented the recursive-splitting (RS) and scatter-collect (SC) broadcasts, efficient algorithms for short and long messages, respectively [1]. These routines could then be hybridized to produce an algorithm that is efficient over a wide range of message lengths, 1 as we have shown in [2, 3]. <p> range of parameters <ref> [2, 3] </ref>. Previously, we presented the recursive-splitting (RS) and scatter-collect (SC) broadcasts, efficient algorithms for short and long messages, respectively [1]. These routines could then be hybridized to produce an algorithm that is efficient over a wide range of message lengths, 1 as we have shown in [2, 3]. Although the strategy presented in those papers is very effective, the pure SC broadcast is still a factor of two worse than the theoretical optimal for very long messages. <p> One way to improve performance in this range would be to switch from the SC broadcast to a pipelined broadcast when the message reaches a certain size. (This crossover point can be accurately determined by using the execution time formulae for the algorithms used.) This strategy was not adopted in <ref> [2, 3] </ref> because no good mesh-based pipelined algorithm was then available; theoretically superior pipelined algorithms were outperformed by the simpler SC broadcast. Also, in those papers, we investigated the implementation of an entire collective communication library, for which the SC broadcast was more convenient.
Reference: 4. <author> R. Barriuso and A. Knies, </author> <title> SHMEM user's guide for C, Cray Research, </title> <publisher> Inc., </publisher> <year> 1994. </year>
Reference-contexts: The empirical data that follows was taken from the Cray T3D [6]. The message-passing routines used in the implementations were layered directly on the explicit shared memory routines provided by Cray <ref> [4] </ref>. While the T3D comes very close to the ideal model presented in Section 2, it deviates in two significant ways: First, pairs of processors share a single network interface [6]. As a result, if both processors are sending and/or receiving simultaneously, they do so at half normal bandwidth.
Reference: 5. <author> J.-C. Bermond, P. Michallon and D. Trystram, </author> <title> Broadcasting in wraparound meshes with parallel monodirectional links, </title> <institution> Parallel Comput. </institution> <month> 18(6) </month> <year> (1992) </year> <month> 639-648. </month>
Reference-contexts: While the idea of a pipelined broadcast is certainly not a new one, methods for implementing them efficiently on a modern mesh interconnect topology have, to our knowledge, not yet been found. Attempts have been made in <ref> [1, 5, 12, 13] </ref>, but these techniques either rely on assumptions that are not applicable to today's mesh-based architectures, or they do not achieve the same performance possible with pipelined broadcasts on other topologies, such as hypercubes [7]. <p> By alternating less frequently, one is able to attain much of the theoretical performance of the EDF broadcast while avoiding the high costs that result from its practical difficulties. c The spanning trees presented in <ref> [5] </ref> are actually of depth p p for a square, two-dimensional p-processor mesh. However, the authors assume a true torus interconnect under the store-and-forward model. The tree will thus not work on a logical unidirectional torus simulated with wormhole routing on a non-torus mesh.
Reference: 6. <author> Cray Research, Inc., </author> <title> CRAY T3D System Architectural Overview, </title> <year> 1993. </year>
Reference-contexts: Target Architectures The assumptions listed in Section 2 are applicable to some of today's prevailing architectures, such as the Cray T3D, which has a three-dimensional torus interconnect <ref> [6] </ref>. They only partially apply to the Intel Touchstone Delta and Paragon systems, because the nodes cannot send and receive simultaneously at full bandwidth, contradicting Assumption 3 [9, 10]. a In the case of wormhole routing, this assumption is empirically valid for known architectures. <p> The empirical data that follows was taken from the Cray T3D <ref> [6] </ref>. The message-passing routines used in the implementations were layered directly on the explicit shared memory routines provided by Cray [4]. While the T3D comes very close to the ideal model presented in Section 2, it deviates in two significant ways: First, pairs of processors share a single network interface [6]. <p> <ref> [6] </ref>. The message-passing routines used in the implementations were layered directly on the explicit shared memory routines provided by Cray [4]. While the T3D comes very close to the ideal model presented in Section 2, it deviates in two significant ways: First, pairs of processors share a single network interface [6]. As a result, if both processors are sending and/or receiving simultaneously, they do so at half normal bandwidth. This particularly diminishes the performance of the DOPL broadcast, which depends on the ability of all nodes to send and receive simultaneously at full bandwidth.
Reference: 7. <author> C.-T. Ho and S. L. Johnsson, </author> <title> Distributed routing algorithms for broadcasting and personalized communication on hypercubes, </title> <booktitle> in Proc. 1986 Int. Conf. on Parallel Processing, </booktitle> <publisher> (IEEE Computer Society Press, </publisher> <year> 1986) </year> <month> 640-648. </month>
Reference-contexts: Attempts have been made in [1, 5, 12, 13], but these techniques either rely on assumptions that are not applicable to today's mesh-based architectures, or they do not achieve the same performance possible with pipelined broadcasts on other topologies, such as hypercubes <ref> [7] </ref>. Furthermore, these methods are designed for two-dimensional meshes and do not readily generalize to meshes of higher dimension. Meshes present two difficulties for pipelined broadcasts: First, the minimum pipeline depth is considerably greater than many other topologies. <p> Second, edge-disjoint spanning trees embedded in meshes often lead to synchronization difficulties which do not occur in hypercube-based algorithms such as Ho and Johnsson's edge-disjoint spanning tree (EDST) algorithm <ref> [7] </ref>. Typically, various nodes receive multiple messages simultaneously, greatly increasing the running time of the broadcast and producing very erratic behavior. This phenomenon was illustrated by the EDF broadcast in [1]. Explicit synchronization can alleviate this problem but would greatly increase the running time of the algorithm. <p> In non-array topologies, the depth of the pipeline can be greatly reduced. Ho and Johns-son's EDST algorithm accomplishes this by embedding log 2 (p) edge-disjoint spanning trees rooted at the nearest neighbors of the source node <ref> [7] </ref>. (See Fig. 2.) The source node alternates among the spanning trees, sending pieces of the message along each of them. This reduces the pipeline depth to log 2 (p) + 1. 4 Fig. 2.
Reference: 8. <author> Intel Corporation, </author> <title> Paragon OSF/1 User's Guide, </title> <year> 1993. </year>
Reference: 9. <author> S. L. Lillevik, </author> <title> The Touchstone 30 gigaflop Delta prototype, </title> <booktitle> in Sixth Distributed Memory Comput. Conf. Proc., </booktitle> <publisher> (IEEE Computer Society Press, </publisher> <year> 1991) </year> <month> 671-677. </month>
Reference-contexts: They only partially apply to the Intel Touchstone Delta and Paragon systems, because the nodes cannot send and receive simultaneously at full bandwidth, contradicting Assumption 3 <ref> [9, 10] </ref>. a In the case of wormhole routing, this assumption is empirically valid for known architectures. The variation in ff is typically greater than the time variation between sending to nearest and most distant nodes on a very large mesh.
Reference: 10. <author> R. Littlefield, </author> <title> Characterizing and tuning communications performance on the Touchstone Delta and iPSC/860, </title> <booktitle> in Proc. 1992 Intel User's Group Meeting, </booktitle> <address> Dallas, TX, </address> <month> Oct. </month> <pages> 4-7, </pages> <year> 1992. </year>
Reference-contexts: They only partially apply to the Intel Touchstone Delta and Paragon systems, because the nodes cannot send and receive simultaneously at full bandwidth, contradicting Assumption 3 <ref> [9, 10] </ref>. a In the case of wormhole routing, this assumption is empirically valid for known architectures. The variation in ff is typically greater than the time variation between sending to nearest and most distant nodes on a very large mesh.
Reference: 11. <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> May 5, </month> <year> 1994. </year>
Reference-contexts: Examples of such functions include MPI Ssend (), MPI Irecv () and MPI Wait () un der the MPI standard <ref> [11] </ref>. The following notational conventions are used in this paper: * c i indicates node c's position in the i th dimension. * x (i;j) indicates the i th portion of a vector of data, x, divided into j pieces of approx imately equal length. 3.
Reference: 12. <author> Y. Saad and M. H. Shultz, </author> <title> Data communication in parallel architectures, </title> <institution> Parallel Comput. </institution> <month> 11(2) </month> <year> 1989 </year> <month> 131-150. </month>
Reference-contexts: While the idea of a pipelined broadcast is certainly not a new one, methods for implementing them efficiently on a modern mesh interconnect topology have, to our knowledge, not yet been found. Attempts have been made in <ref> [1, 5, 12, 13] </ref>, but these techniques either rely on assumptions that are not applicable to today's mesh-based architectures, or they do not achieve the same performance possible with pipelined broadcasts on other topologies, such as hypercubes [7]. <p> For example, on a p-node linear array, one would send the first piece of the message to the next node in the array. Then, while the second node sends the first piece to the third node, one would send the second piece to the second node, and so forth <ref> [12] </ref>. (See Fig. 1.) In this case, if a length n message is divided into k parts, the time for completion would be: T 1D = (p 1)(ff + k n fi) (1) The first term is the time required for the first packet to reach the far end of the
Reference: 13. <author> M. Simmen, </author> <title> Comments on broadcast algorithms for two-dimensional grids, </title> <journal> Parallel Comput. </journal> <volume> 17(1) 1991 109-112. </volume> <pages> 12 </pages>
Reference-contexts: While the idea of a pipelined broadcast is certainly not a new one, methods for implementing them efficiently on a modern mesh interconnect topology have, to our knowledge, not yet been found. Attempts have been made in <ref> [1, 5, 12, 13] </ref>, but these techniques either rely on assumptions that are not applicable to today's mesh-based architectures, or they do not achieve the same performance possible with pipelined broadcasts on other topologies, such as hypercubes [7]. <p> The tree will thus not work on a logical unidirectional torus simulated with wormhole routing on a non-torus mesh. Furthermore, their trees are dependent on the assumption that a node can send/receive simultaneously in all four directions, contradicting Assumption 3 in Section 2. d The algorithm in <ref> [13] </ref> uses the same spanning trees as the EDF broadcast. However, as above, the author assumed that nodes could simultaneously send/receive in all four directions. Under such circumstances, there is no need to alternate between spanning trees.
References-found: 13

