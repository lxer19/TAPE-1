URL: ftp://ftp.pmg.lcs.mit.edu/pub/thor/lazycons.ps.gz
Refering-URL: http://www.pmg.lcs.mit.edu/papers/podc97/lc.html
Root-URL: 
Email: fadya,liskovg@lcs.mit.edu  
Title: Lazy Consistency Using Loosely Synchronized Clocks  
Author: Atul Adya Barbara Liskov 
Affiliation: Laboratory for Computer Science, Massachusetts Institute of Technology,  
Date: August 1997  
Address: Santa Barbara, CA,  545 Technology Square, Cambridge, MA 02139  
Note: Appears in the Proceedings of the ACM Symposium on Principles of Distributed Computing (PODC '97),  
Abstract: This paper describes a new scheme for guaranteeing that transactions in a client/server system observe consistent state while they are running. The scheme is presented in conjunction with an optimistic concurrency control algorithm, but could also be used to prevent read-only transactions from conflicting with read/write transactions in a multi-version system. The scheme is lazy about the consistency it provides for running transactions and also in the way it generates the consistency information. The paper presents results of simulation experiments showing that the cost of the scheme is negligible. The scheme uses multipart timestamps to inform nodes about information they need to know. Today the utility of such schemes is limited because timestamp size is proportional to system size and therefore the schemes don't scale to very large systems. We show how to solve this problem. Our multipart timestamps are based on real rather than logical clocks; we assume clocks in the system are loosely synchronized. Clocks allow us to keep multipart timestamps small with minimal impact on performance: we remove old information that is likely to be known while retaining recent information. Only performance and not correctness is affected if clocks get out of synch. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Adya, R. Gruber, B. Liskov, and U. Maheshwari. </author> <title> Efficient optimistic concurrency control using loosely synchronized clocks. </title> <booktitle> In SIGMOD, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: 1 Introduction This paper describes a new algorithm that insures transactions running at clients in a client/server system always view a consistent state as they run. The algorithm is useful in conjunction with optimistic concurrency control mechanisms (e.g., <ref> [1] </ref>), and also for ensuring that read-only transactions can commit without interfering with read/write transactions in a multi-version system (e.g., [2, 7, 30]). <p> This paper describes the new scheme in conjunction with an optimistic concurrency control algorithm, AOCC, which was developed for use in a distributed client/server environment. AOCC has been shown to outperform other concurrency control mechanisms for all common workloads and realistic system assumptions <ref> [1, 13] </ref>. In particular, AOCC outperforms the best locking approach, adaptive callback locking [6]. One reason for AOCC's good performance is that it allows transactions to use cached information without communication with servers; locking mechanisms require such communication at least when objects are modified. <p> Note that locking ensures global causality for active transactions. 4 Base Algorithm This section provides an overview of our system and AOCC. More information can be found in <ref> [1, 10, 13, 19] </ref>. Servers store the database objects in pages on disk; objects are typically smaller than pages. Clients maintain a page cache; they fetch missing pages from servers and use approximate LRU for cache management. Clients and servers rely on ordered communication, e.g., TCP. <p> Clients and servers rely on ordered communication, e.g., TCP. Transactions run entirely at clients; clients communicate with servers only when there is a miss in the cache, and to commit transactions. The system serializes transactions using AOCC <ref> [1, 13] </ref> and two-phase commit [11]. (Two-phase commit is avoided for transactions that use objects at only one server.) AOCC works as follows: While a transaction T is running, client C keeps track of the objects it used and objects it modified. <p> Validation uses a validation queue or VQ to track read/write sets of prepared and committed transactions; details can be found in <ref> [1] </ref>. Each participant sends its validation results to the coordinator who commits the transaction iff all participants voted positively. The coordinator sends its decision to client C and the participants. The delay observed by the client is due to phase 1 only; phase 2 happens in the background.
Reference: [2] <author> D. Agrawal, A. J. Bernstein, P. Gupta, and S. Sengupta. </author> <title> Distributed Multi-version Optimistic Concurrency Control with Reduced Rollback. </title> <journal> Distributed Computing, </journal> <volume> 2(1), </volume> <year> 1987. </year>
Reference-contexts: The algorithm is useful in conjunction with optimistic concurrency control mechanisms (e.g., [1]), and also for ensuring that read-only transactions can commit without interfering with read/write transactions in a multi-version system (e.g., <ref> [2, 7, 30] </ref>). The algorithm is intended for use in a distributed environment in which servers provide reliable persistent This research was supported in part by the Advanced Research Projects Agency of the Department of Defense, monitored by the Office of Naval Research under contract N00014-91-J-4136.
Reference: [3] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison Wesley, </publisher> <year> 1987. </year>
Reference-contexts: Earlier research has investigated ways of providing consistency for read-only transactions <ref> [3, 30] </ref> using serializabil-ity or weaker notions of consistency that are different from lazy consistency. For example, some schemes ensure that a read-only transaction is serializable with all read-write transactions but the whole system may not be serializable when all read-only transactions are taken into account. <p> In this case, T anti-depends on U. Figure 1 shows a cycle that is formed in the dependency graph <ref> [3] </ref> for this schedule due to anti-dependencies. consistent database state.
Reference: [4] <author> K. Birman, A. Schiper, and P. Stephenson. </author> <title> Lightweight Causal and Atomic Group Multicast. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(3), </volume> <month> August </month> <year> 1991. </year>
Reference-contexts: Today the utility of multistamps is limited because their size is proportional to system size and therefore they don't scale to very large systems. We show how to reduce the size of multi-stamps. Unlike other multistamp (or vector clock) schemes, e.g., <ref> [4, 17, 25, 8, 15] </ref>, our scheme is based on time rather than on logical clocks: each entry in a multistamp contains a timestamp representing the clock time at some server in the system. Using time rather than logical clocks allows us to keep multistamps small by removing old information.
Reference: [5] <author> M. </author> <title> Blaze. Caching in Large-Scale Distributed File Systems. </title> <type> Technical Report TR-397-92, </type> <institution> Princeton University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Disks are not modeled explicitly but we assume a server cache hit ratio of 50%. This value is much higher than hit ratios observed in real systems <ref> [5, 24] </ref>. A lower server cache hit ratio would reduce the stall rate since transactions would take longer to execute and thus dependencies would spread more slowly (we ran an experiment to confirm this). Furthermore, a lower hit rate would reduce the relative impact of stalls on overall execution time.
Reference: [6] <author> M. Carey, M. Franklin, and M. Zaharioudakis. </author> <title> Fine-Grained Sharing in a Page Server OODBMS. </title> <booktitle> In Proc. SIGMOD Int'l Conf. on Management of Data, </booktitle> <pages> pages 359-370. </pages> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: AOCC has been shown to outperform other concurrency control mechanisms for all common workloads and realistic system assumptions [1, 13]. In particular, AOCC outperforms the best locking approach, adaptive callback locking <ref> [6] </ref>. One reason for AOCC's good performance is that it allows transactions to use cached information without communication with servers; locking mechanisms require such communication at least when objects are modified. <p> The details of the algorithm are simulated precisely; we replaced client-server tuples in multistamps with server stamps when there were at least 10 entries for the same server. We constructed the simulator and workloads starting from earlier concurrency control studies <ref> [13, 6] </ref>. These studies were performed for a single-server, multi-client system; we extended them to a distributed database with multiple servers. We ran experiments with varying values of simulator parameters; we present results for a particular setup and mention results of other experiments along the way. <p> In a multi-server transaction, the number of accesses are equally divided among the servers. 20% of the accesses are writes. The client cache size is 875 pages; cache management is done using LRU. The client can potentially access 5000 pages (1250 from each server). In single-server studies <ref> [13, 6] </ref> the client cache was 1/4 of the in-use pages, so it might seem that our cache is too small. However, we observed that more than 85% of accesses are to preferred servers, i.e., to 2500 pages. <p> Thus, a client cache size between 625 and 1250 pages, with a bias towards 625, is in line with earlier work. The total in-use database in our system is more than twice the size used in <ref> [13, 6] </ref>. This means we have lower contention (less than half) than what was observed in those studies. We therefore also ran experiments with a smaller database (to make contention levels similar) and observed a stall rate increase of about 50-75%. <p> transactions and these transactions involve two servers; other researchers have also reported two-server transactions to be common for distributed transactions [26]. 6.1.5 Workloads We now discuss the different workloads used in our study; these workloads have been used in the past to compare the performance of various concurrency control mechanisms <ref> [13, 6] </ref>. LOWCON is intended to be a realistic low-contention workload; the others are intended to stress the system. LOWCON. This workload models a realistic system with low contention (like those observed in [16, 27]). Each client has a private region of 50 pages at each preferred server. <p> After it has received replies from these clients, it sends the commit decision to the participants. (The invalidation phase is similar to communication in other concurrency control schemes, e.g., callbacks in <ref> [6] </ref>, except that in those schemes, communication occurs in the foreground.) The eager scheme avoids fetch stalls entirely, but at the cost of extra messages and an extra commit phase.
Reference: [7] <author> A. Chan and R. Gray. </author> <title> Implementing Distributed Read-Only Transactions. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11(2), </volume> <year> 1985. </year>
Reference-contexts: The algorithm is useful in conjunction with optimistic concurrency control mechanisms (e.g., [1]), and also for ensuring that read-only transactions can commit without interfering with read/write transactions in a multi-version system (e.g., <ref> [2, 7, 30] </ref>). The algorithm is intended for use in a distributed environment in which servers provide reliable persistent This research was supported in part by the Advanced Research Projects Agency of the Department of Defense, monitored by the Office of Naval Research under contract N00014-91-J-4136. <p> For example, some schemes ensure that a read-only transaction is serializable with all read-write transactions but the whole system may not be serializable when all read-only transactions are taken into account. Chan and Gray <ref> [7] </ref> propose lazy consistency as a correctness criterion but their work is concerned with committing read-only transactions more cheaply as opposed to providing consistency for transactions that abort. <p> If Q reads x i and also reads y, it must read y j or a version of y that is later than y j . The same condition is used for providing consistent views for read-only transactions by Chan and Gray <ref> [7] </ref>; that paper proves that if the condition is satisfied, Q will observe a consistent snapshot of the database. Here we present a brief synopsis of the proof. <p> The scheme is presented in conjunction with an optimistic concurrency control algorithm, but could also be used to prevent read-only transactions from conflicting with read/write transactions in a multi-version system <ref> [7] </ref>. The scheme is lazy about providing consistency. It simply gathers information as transactions commit, and propagates information to clients in fetch replies. Clients use information only when necessary to ensure that the current transaction observes a consistent view. 11 The scheme is based on multipart timestamps.
Reference: [8] <author> A. Demers, K. Petersen, M. Spreitzer, D. Terry, M. Theimer, and B. Welch. </author> <title> Session Guarantees for Weakly Consistent Replicated Data. </title> <type> Technical Report CSL-94-9, </type> <institution> Xerox Parc, </institution> <address> Palo Alto, CA, </address> <year> 1994. </year>
Reference-contexts: Today the utility of multistamps is limited because their size is proportional to system size and therefore they don't scale to very large systems. We show how to reduce the size of multi-stamps. Unlike other multistamp (or vector clock) schemes, e.g., <ref> [4, 17, 25, 8, 15] </ref>, our scheme is based on time rather than on logical clocks: each entry in a multistamp contains a timestamp representing the clock time at some server in the system. Using time rather than logical clocks allows us to keep multistamps small by removing old information.
Reference: [9] <author> H. Garcia and G. Weiderhold. </author> <title> Read-Only Transactions in a Distributed Database. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(2), </volume> <month> June </month> <year> 1982. </year>
Reference-contexts: We therefore also ran experiments with a smaller database (to make contention levels similar) and observed a stall rate increase of about 50-75%. As before, our parameters are designed to stress the lazy scheme. Although real systems are dominated by read-only transactions <ref> [9, 26] </ref>, we don't have any since otherwise invalidations would be generated with very low frequency making stalls highly unlikely. Also, we have a relatively high percentage of multi-server transactions (20%); 11.5% of our transactions use two servers and 8.5% use more than two servers.
Reference: [10] <author> S. Ghemawat. </author> <title> The Modified Object Buffer: A Storage Management Technique for Object-Oriented Databases. </title> <type> Technical Report MIT/LCS/TR-666, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> Septem-ber </month> <year> 1995. </year>
Reference-contexts: Note that locking ensures global causality for active transactions. 4 Base Algorithm This section provides an overview of our system and AOCC. More information can be found in <ref> [1, 10, 13, 19] </ref>. Servers store the database objects in pages on disk; objects are typically smaller than pages. Clients maintain a page cache; they fetch missing pages from servers and use approximate LRU for cache management. Clients and servers rely on ordered communication, e.g., TCP.
Reference: [11] <author> J. N. Gray. </author> <title> Notes on Database Operating Systems. </title> <editor> In R. Bayer, R. Graham, and G. Seegmuller, editors, </editor> <booktitle> Operating Systems: An Advanced Course, number 60 in Lecture Notes in Computer Science, </booktitle> <pages> pages 393-481. </pages> <publisher> Springer-Verlag, </publisher> <year> 1978. </year>
Reference-contexts: Clients and servers rely on ordered communication, e.g., TCP. Transactions run entirely at clients; clients communicate with servers only when there is a miss in the cache, and to commit transactions. The system serializes transactions using AOCC [1, 13] and two-phase commit <ref> [11] </ref>. (Two-phase commit is avoided for transactions that use objects at only one server.) AOCC works as follows: While a transaction T is running, client C keeps track of the objects it used and objects it modified.
Reference: [12] <author> J. N. Gray and A. Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <year> 1993. </year>
Reference-contexts: Section 4 describes our system and how AOCC works; Section 5 describes our implementation of lazy consistency; and Section 6 presents our performance results. We conclude with a summary of our results. 2 Related Work Lazy consistency is similar to degree-2 isolation; using the terminology of Gray and Reuter <ref> [12] </ref>, our system provides degree-3 isolation for all committed transactions and degree-2 isolation for running transactions. Earlier research has investigated ways of providing consistency for read-only transactions [3, 30] using serializabil-ity or weaker notions of consistency that are different from lazy consistency.
Reference: [13] <author> R. Gruber. </author> <title> Optimism vs. Locking: A Study of Concurrency Control for Client-Server Object-Oriented Databases. </title> <type> PhD thesis, </type> <institution> M.I.T., </institution> <address> Cambridge, MA, </address> <year> 1997. </year>
Reference-contexts: This paper describes the new scheme in conjunction with an optimistic concurrency control algorithm, AOCC, which was developed for use in a distributed client/server environment. AOCC has been shown to outperform other concurrency control mechanisms for all common workloads and realistic system assumptions <ref> [1, 13] </ref>. In particular, AOCC outperforms the best locking approach, adaptive callback locking [6]. One reason for AOCC's good performance is that it allows transactions to use cached information without communication with servers; locking mechanisms require such communication at least when objects are modified. <p> Note that locking ensures global causality for active transactions. 4 Base Algorithm This section provides an overview of our system and AOCC. More information can be found in <ref> [1, 10, 13, 19] </ref>. Servers store the database objects in pages on disk; objects are typically smaller than pages. Clients maintain a page cache; they fetch missing pages from servers and use approximate LRU for cache management. Clients and servers rely on ordered communication, e.g., TCP. <p> Clients and servers rely on ordered communication, e.g., TCP. Transactions run entirely at clients; clients communicate with servers only when there is a miss in the cache, and to commit transactions. The system serializes transactions using AOCC <ref> [1, 13] </ref> and two-phase commit [11]. (Two-phase commit is avoided for transactions that use objects at only one server.) AOCC works as follows: While a transaction T is running, client C keeps track of the objects it used and objects it modified. <p> The details of the algorithm are simulated precisely; we replaced client-server tuples in multistamps with server stamps when there were at least 10 entries for the same server. We constructed the simulator and workloads starting from earlier concurrency control studies <ref> [13, 6] </ref>. These studies were performed for a single-server, multi-client system; we extended them to a distributed database with multiple servers. We ran experiments with varying values of simulator parameters; we present results for a particular setup and mention results of other experiments along the way. <p> We chose 10 clusters to model a system in which multistamps must be truncated; otherwise, they would contain 800 entries (since there are 800 client-server connections in the setup). 6.1.2 Network Model We used an abstract model of the network as in <ref> [13] </ref>. Apart from time spent on the wire, each message has a fixed and variable processor cost for sending/receiving the message. <p> In a multi-server transaction, the number of accesses are equally divided among the servers. 20% of the accesses are writes. The client cache size is 875 pages; cache management is done using LRU. The client can potentially access 5000 pages (1250 from each server). In single-server studies <ref> [13, 6] </ref> the client cache was 1/4 of the in-use pages, so it might seem that our cache is too small. However, we observed that more than 85% of accesses are to preferred servers, i.e., to 2500 pages. <p> Thus, a client cache size between 625 and 1250 pages, with a bias towards 625, is in line with earlier work. The total in-use database in our system is more than twice the size used in <ref> [13, 6] </ref>. This means we have lower contention (less than half) than what was observed in those studies. We therefore also ran experiments with a smaller database (to make contention levels similar) and observed a stall rate increase of about 50-75%. <p> transactions and these transactions involve two servers; other researchers have also reported two-server transactions to be common for distributed transactions [26]. 6.1.5 Workloads We now discuss the different workloads used in our study; these workloads have been used in the past to compare the performance of various concurrency control mechanisms <ref> [13, 6] </ref>. LOWCON is intended to be a realistic low-contention workload; the others are intended to stress the system. LOWCON. This workload models a realistic system with low contention (like those observed in [16, 27]). Each client has a private region of 50 pages at each preferred server.
Reference: [14] <author> T. Haerder. </author> <title> Observations on Optimistic Concurrency Control Schemes. </title> <journal> Information Systems, </journal> <volume> 9(2) </volume> <pages> 111-120, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: The participants validate T to ensure that it used only up-to-date versions of all objects and that it has not modified any objects used by other prepared or committed transactions, i.e., we use backward validation <ref> [14] </ref>. Validation uses a validation queue or VQ to track read/write sets of prepared and committed transactions; details can be found in [1]. Each participant sends its validation results to the coordinator who commits the transaction iff all participants voted positively.
Reference: [15] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proc. of 19th Int'l Symp. on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Today the utility of multistamps is limited because their size is proportional to system size and therefore they don't scale to very large systems. We show how to reduce the size of multi-stamps. Unlike other multistamp (or vector clock) schemes, e.g., <ref> [4, 17, 25, 8, 15] </ref>, our scheme is based on time rather than on logical clocks: each entry in a multistamp contains a timestamp representing the clock time at some server in the system. Using time rather than logical clocks allows us to keep multistamps small by removing old information.
Reference: [16] <author> J. J. Kistler and M. Satyanarayanan. </author> <title> Disconnected Operation in the Coda File System. </title> <booktitle> In SOSP, </booktitle> <year> 1991. </year>
Reference-contexts: LOWCON is intended to be a realistic low-contention workload; the others are intended to stress the system. LOWCON. This workload models a realistic system with low contention (like those observed in <ref> [16, 27] </ref>). Each client has a private region of 50 pages at each preferred server. Each server has a shared region of 1200 pages. 80% of a client's accesses go to its private region; the rest go to the shared regions at its connected servers.
Reference: [17] <author> R. Ladin, B. Liskov, L. Shrira, and S. Ghemawat. </author> <title> Providing High Availability Using Lazy Replication. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(4), </volume> <month> November </month> <year> 1992. </year>
Reference-contexts: Today the utility of multistamps is limited because their size is proportional to system size and therefore they don't scale to very large systems. We show how to reduce the size of multi-stamps. Unlike other multistamp (or vector clock) schemes, e.g., <ref> [4, 17, 25, 8, 15] </ref>, our scheme is based on time rather than on logical clocks: each entry in a multistamp contains a timestamp representing the clock time at some server in the system. Using time rather than logical clocks allows us to keep multistamps small by removing old information.
Reference: [18] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Comm. of the ACM, </journal> <volume> 21(7), </volume> <month> July </month> <year> 1978. </year>
Reference-contexts: In this case, T anti-depends on U. Figure 1 shows a cycle that is formed in the dependency graph [3] for this schedule due to anti-dependencies. consistent database state. Lazy consistency is independent of causality <ref> [18] </ref>: a transaction Q might observe the effects of T 2 while missing the effects of T 1 , where both T 1 and T 2 ran at the same client in order T 1 ; T 2 . <p> Both hot and cold regions are shared uniformly among clients. 6.2 Basic Experimental Results for multistamps increases from zero entries (just the threshold timestamp) to the size reached when just aging is used. (A system that uses just the threshold timestamp is using Lamport clocks <ref> [18] </ref>.) The X axis represents increasing multistamp size; the Y axis shows the stall rate. The number in parenthesis after the workload name gives the number of fetches per transaction. The results show that even for small multistamps, the percentage of stalls is low.
Reference: [19] <author> B. Liskov et al. </author> <title> Safe and Efficient Sharing of Persistent Objects in Thor. </title> <booktitle> In Proc. SIGMOD Int'l Conf. on Management of Data, </booktitle> <year> 1996. </year>
Reference-contexts: Note that locking ensures global causality for active transactions. 4 Base Algorithm This section provides an overview of our system and AOCC. More information can be found in <ref> [1, 10, 13, 19] </ref>. Servers store the database objects in pages on disk; objects are typically smaller than pages. Clients maintain a page cache; they fetch missing pages from servers and use approximate LRU for cache management. Clients and servers rely on ordered communication, e.g., TCP. <p> The costs for reading and writing an object are about 64 sec and 128 sec respectively. These times are based on the observation that a transaction operates on an object for some time when it accesses it, e.g., the time spent per object in the OO7 benchmark by Thor <ref> [19] </ref> is around 35 sec (and OO7 methods do not perform any significant amount of work). Other costs such as those for validation, cache lookup, etc., are not shown due to lack of space; they are negligible compared to the access and fetch costs.
Reference: [20] <author> B. Liskov, R. Scheifler, E. Walker, and W. Weihl. </author> <title> Orphan Detection. Programming Methodology Group Memo 53, </title> <institution> Laboratory for Computer Science, MIT, </institution> <month> February </month> <year> 1987. </year>
Reference-contexts: They also propose an implementation technique for a distributed client/server system using a kind of multistamp scheme but ignore all details that would make it a practical scheme. Our work is related to orphan detection schemes <ref> [20] </ref>. Such schemes abort transactions before they can observe an inconsistency; they guarantee a stronger property than lazy consistency since they detect anti-dependencies also (discussed in Section 3). 3 Lazy Consistency This section defines the kind of consistency our new mechanism provides.
Reference: [21] <author> Barbara Liskov. </author> <title> Practical Uses of Synchronized Clocks in Distributed Systems. </title> <journal> Distributed Computing, </journal> <volume> 6, </volume> <month> August </month> <year> 1993. </year>
Reference-contexts: Our scheme assumes that server clocks never run backwards, and advance rapidly enough that each transaction can be assigned a distinct timestamp; these assumptions are easy to guarantee (see for example, the discussion in <ref> [21] </ref>). The basis of the scheme is the invalidations generated when transactions commit. The fundamental idea is this: if a client running transaction U observes a modification made by transaction T, then it must already have received all the invalidations of T and any transactions T depended on.
Reference: [22] <author> D. L. Mills. </author> <title> Network Time Protocol (Version 3) Specification, Implementation and Analysis. Network Working Report RFC 1305, </title> <month> March </month> <year> 1992. </year>
Reference-contexts: The timeout period is chosen to be 0.5 second. Clock skews are not modeled since they are insignificant compared to the timeout period; the Network Time Protocol <ref> [22] </ref> maintains clock skews that are less than 10 milliseconds on LANs and WANs [23]. 6.1.4 Database and Transaction Parameters The database is divided into pages of equal size. Each simulator run accesses a small subset of the database; this set is called in-use pages of the run.
Reference: [23] <author> D. L. Mills. </author> <title> The Network Computer as Precision Timekeeper. In Proc. Precision Time and Time Interval (PTTI) Applications and Planning Meeting, </title> <month> December </month> <year> 1996. </year>
Reference-contexts: Furthermore, because we prune based on time, the discarded information is likely to already be known to interested parties; therefore discarding it has little impact on system performance. We assume that clocks are loosely synchronized; such an assumption is realistic in today's environment <ref> [23] </ref>. The correctness of the scheme is not affected if information of interest is pruned too early or clocks get out of synch (although performance may be). <p> The timeout period is chosen to be 0.5 second. Clock skews are not modeled since they are insignificant compared to the timeout period; the Network Time Protocol [22] maintains clock skews that are less than 10 milliseconds on LANs and WANs <ref> [23] </ref>. 6.1.4 Database and Transaction Parameters The database is divided into pages of equal size. Each simulator run accesses a small subset of the database; this set is called in-use pages of the run. As shown in Figure 2, there are 1250 database in-use pages per server.
Reference: [24] <author> D. Muntz and P. Honeyman. </author> <title> Multi-level caching in distributed file systems or- your cache ain't nuthin' but trash. </title> <booktitle> In USENIX Winter Conference, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: Disks are not modeled explicitly but we assume a server cache hit ratio of 50%. This value is much higher than hit ratios observed in real systems <ref> [5, 24] </ref>. A lower server cache hit ratio would reduce the stall rate since transactions would take longer to execute and thus dependencies would spread more slowly (we ran an experiment to confirm this). Furthermore, a lower hit rate would reduce the relative impact of stalls on overall execution time.
Reference: [25] <author> D. S. Parker et al. </author> <title> Detection of Mutual Inconsistency in Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-9(3), </volume> <month> May </month> <year> 1983. </year>
Reference-contexts: Today the utility of multistamps is limited because their size is proportional to system size and therefore they don't scale to very large systems. We show how to reduce the size of multi-stamps. Unlike other multistamp (or vector clock) schemes, e.g., <ref> [4, 17, 25, 8, 15] </ref>, our scheme is based on time rather than on logical clocks: each entry in a multistamp contains a timestamp representing the clock time at some server in the system. Using time rather than logical clocks allows us to keep multistamps small by removing old information.
Reference: [26] <author> G. Samaras, K. Britton, A. Citron, and C. Mohan. </author> <title> Two-Phase Commit Optimizations in a Commercial Distributed Environment. Distributed and Parallel Databases, </title> <type> 3, </type> <month> October </month> <year> 1995. </year>
Reference-contexts: We therefore also ran experiments with a smaller database (to make contention levels similar) and observed a stall rate increase of about 50-75%. As before, our parameters are designed to stress the lazy scheme. Although real systems are dominated by read-only transactions <ref> [9, 26] </ref>, we don't have any since otherwise invalidations would be generated with very low frequency making stalls highly unlikely. Also, we have a relatively high percentage of multi-server transactions (20%); 11.5% of our transactions use two servers and 8.5% use more than two servers. <p> Benchmarks such as TPC-A and TPC-C [28] have fewer than 10-15% multi-server transactions and these transactions involve two servers; other researchers have also reported two-server transactions to be common for distributed transactions <ref> [26] </ref>. 6.1.5 Workloads We now discuss the different workloads used in our study; these workloads have been used in the past to compare the performance of various concurrency control mechanisms [13, 6]. LOWCON is intended to be a realistic low-contention workload; the others are intended to stress the system. LOWCON.
Reference: [27] <author> M. Spasojevic and M. Satyanarayanan. </author> <title> An Empirical Study of a Wide-Area Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(2), </volume> <month> May </month> <year> 1996. </year>
Reference-contexts: LOWCON is intended to be a realistic low-contention workload; the others are intended to stress the system. LOWCON. This workload models a realistic system with low contention (like those observed in <ref> [16, 27] </ref>). Each client has a private region of 50 pages at each preferred server. Each server has a shared region of 1200 pages. 80% of a client's accesses go to its private region; the rest go to the shared regions at its connected servers.
Reference: [28] <author> Transaction Processing Performance Council (TPC). </author> <title> TPC-A Standard Specification, Revision 1.1, TPC-C Standard Specification, Revision 1.0. Shanley Public Relations, </title> <year> 1992. </year>
Reference-contexts: Also, we have a relatively high percentage of multi-server transactions (20%); 11.5% of our transactions use two servers and 8.5% use more than two servers. Benchmarks such as TPC-A and TPC-C <ref> [28] </ref> have fewer than 10-15% multi-server transactions and these transactions involve two servers; other researchers have also reported two-server transactions to be common for distributed transactions [26]. 6.1.5 Workloads We now discuss the different workloads used in our study; these workloads have been used in the past to compare the performance
Reference: [29] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In SOSP, </booktitle> <year> 1995. </year>
Reference-contexts: Apart from time spent on the wire, each message has a fixed and variable processor cost for sending/receiving the message. Our network parameters were obtained from the experimental results reported for U-Net <ref> [29] </ref> running over a 155 Mbps ATM. 6.1.3 Client/Server Parameters Client processors run at 200 MIPS and the server CPU at 300 MIPS; these values reflect typical processor speeds expected in the near future. Disks are not modeled explicitly but we assume a server cache hit ratio of 50%.
Reference: [30] <author> W. E. Weihl. </author> <title> Distributed Version Management for Read-only Actions. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1), </volume> <month> January </month> <year> 1987. </year> <month> 12 </month>
Reference-contexts: The algorithm is useful in conjunction with optimistic concurrency control mechanisms (e.g., [1]), and also for ensuring that read-only transactions can commit without interfering with read/write transactions in a multi-version system (e.g., <ref> [2, 7, 30] </ref>). The algorithm is intended for use in a distributed environment in which servers provide reliable persistent This research was supported in part by the Advanced Research Projects Agency of the Department of Defense, monitored by the Office of Naval Research under contract N00014-91-J-4136. <p> Earlier research has investigated ways of providing consistency for read-only transactions <ref> [3, 30] </ref> using serializabil-ity or weaker notions of consistency that are different from lazy consistency. For example, some schemes ensure that a read-only transaction is serializable with all read-write transactions but the whole system may not be serializable when all read-only transactions are taken into account.
References-found: 30

