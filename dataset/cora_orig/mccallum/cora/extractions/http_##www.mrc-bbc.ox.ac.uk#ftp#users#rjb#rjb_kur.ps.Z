URL: http://www.mrc-bbc.ox.ac.uk/ftp/users/rjb/rjb_kur.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00060.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Finding Compact and Sparse Distributed Representations of Visual Images  
Author: Colin Fyfe Roland Baddeley, 
Address: Oxford.  
Affiliation: Department of Computing and Information Systems, University of Paisley.  Department of Experimental Psychology, University of  
Abstract: Some recent work has investigated the dichotomy between compact coding using dimensionality reduction and sparse distributed coding in the context of understanding biological information processing. We introduce an artificial neural network which self organises on the basis of simple Hebbian learning and negative feedback of activation and show that it is capable of both forming compact codings of data distributions and also of identifying filters most sensitive to sparse distributed codes. The network is extremely simple and its biological relevance is investigated via its response to a set of images which are typical of everyday life. However, an analysis of the network's identification of the filter for sparse coding reveals that this coding may not be globally optimal and that there exists an innate limiting factor which cannot be transcended. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Barlow and D. Tolhurst. </author> <title> Why do you have edge detectors. </title> <booktitle> JOSA Meeting, </booktitle> <address> Albuquerque, </address> <year> 1992. </year>
Reference-contexts: Given this, it is therefore of interest to find representations that form optimal "sparse distributed" codes for natural images so that these can be compared to known physiology or psychophysics. Both Barlow <ref> [1] </ref> and Field [2] have proposed that one measure of the quality of a sparse distributed code is the kurtosis of the probability distribution of filter outputs.
Reference: [2] <author> David J. </author> <title> Field. What is the goal of sensory coding. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 559-601, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Recently, Field <ref> [2] </ref> has made an important distinction between a "compact code" and a "sparse distributed code". A compact code is defined to be one in which the redundancy contained in the input data is removed by representing the data in fewer dimensions. <p> Following Field, these two methods of coding are shown diagramatically in Figure 1. It has been argued that this form of redundancy reduction, making the code sparse rather than simply reducing the dimensionality, may be employed in the early visual system <ref> [2] </ref>. Given this, it is therefore of interest to find representations that form optimal "sparse distributed" codes for natural images so that these can be compared to known physiology or psychophysics. Both Barlow [1] and Field [2] have proposed that one measure of the quality of a sparse distributed code is <p> rather than simply reducing the dimensionality, may be employed in the early visual system <ref> [2] </ref>. Given this, it is therefore of interest to find representations that form optimal "sparse distributed" codes for natural images so that these can be compared to known physiology or psychophysics. Both Barlow [1] and Field [2] have proposed that one measure of the quality of a sparse distributed code is the kurtosis of the probability distribution of filter outputs. <p> Pre-processing We repeat the above experiment with the same network but using slightly different input data to the PCA network: firstly we consider the effect of preprocessing the data with a logarithmic function and secondly the effect of a Gaussian mask on the data. 5.1 The effect non-linear pre-processing Field <ref> [2] </ref> performed his experiments upon log preprocessed images. This can be justified for two reasons. Firstly, human physiology appears to be more linear in the logarithm of contrast as opposed to simple contrast. Secondly, log preprocessing may help alleviate problems of differing illumination.
Reference: [3] <author> C. Fyfe. </author> <title> A comparative study of two neural methods of exploratory projection pursuit. </title> <note> (submitted). </note>
Reference-contexts: Therefore we must ask if it is possible to dispense with the first stage of the network and seek kurtosis in the original data space. Unfortunately, it is our finding <ref> [3] </ref> that, with Hebbian type learning on unsphered data, the effect of the larger magnitude second order correlations interferes with convergence to kurtotic filters i.e. convergence is partly towards the filter with (in this case) largest kurtosis but also partly towards filters with the largest variance (the Principal Components filters).
Reference: [4] <author> C. Fyfe. </author> <title> Pca properties of interneurons. In From Neurobiology to Real World Computing, </title> <type> ICANN 93, </type> <year> 1993. </year>
Reference-contexts: We then discuss an innate limitation of the method. The filters found may be more sensitive to highly kurtotic directions but cannot be guaranteed to be globally optimal. 2 The Compact Encoder Recently, it has been shown <ref> [4] </ref> that a layer of neurons (Figure 2) whose weights are modified using only simple Hebbian learning can be used to calculate principal components.
Reference: [5] <author> C. Fyfe and R. Baddeley. </author> <title> Non-linear data structure extraction using simple hebbian networks. </title> <journal> Biological Cybernetics, </journal> <note> 1995. (In press). </note>
Reference-contexts: If we accept that kurtosis is a measure of how appropriate a given filter is for creating a code for a given domain, then we wish to find a filter that maximises (5). This can be done using a novel neural network technique <ref> [5] </ref> which was developed as an instance of the statistical method of Exploratory Projection Pursuit. <p> The neural network method is essentially a non-linear modification of Oja's subspace algorithm ( details can be found in the Appendix and in <ref> [5] </ref>). We use a two stage process.
Reference: [6] <author> Peter J Huber. </author> <title> Projection pursuit. </title> <journal> Annals of Statistics, </journal> <volume> 13 </volume> <pages> 435-475, </pages> <year> 1985. </year>
Reference-contexts: Exploratory Projection Pursuit is a recent method of data analysis which seeks to find interesting filters in high dimensional data by using the high order (i.e. greater than second order) statistics of the data. (See Huber <ref> [6] </ref> for a review). The neural network method is essentially a non-linear modification of Oja's subspace algorithm ( details can be found in the Appendix and in [5]). We use a two stage process. <p> The fact that the same network structure is capable of performing a PCA as well as EPP is unsurprising since Huber <ref> [6] </ref> has shown that PCA may be viewed as a particular case of Projection Pursuit.
Reference: [7] <author> Juha Karhunen and Jyrki Joutsensalo. </author> <title> Representation and separation of signals using nonlinear pca type learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(1) </volume> <pages> 113-127, </pages> <year> 1994. </year>
Reference-contexts: The basic finding is that we cannot transcend a limitation of the method. 8 Acknowledgement We would gratefully acknowledge the assistance of Horace Barlow who provided insightful comments on an earlier version of this paper. Appendix Derivation of the Algorithm Following <ref> [7] </ref>, we can derive the neural network algorithm as an approxi mation to the maximisation of a function, J, of the weights J (W) = P M g [x T w i ]jw i &gt;.
Reference: [8] <author> E. Oja. </author> <title> Neural networks, principal components and subspaces. </title> <journal> International Journal of Neural Systems, </journal> <year> 1989. </year> <month> 21 </month>
Reference-contexts: In fact, to remove one aspect at odds with biological plausibility, it was shown that we can use different weights in the forward and backward directions. The weights,W, are updated using simple Hebbian learning. The learning used in the network was shown to be equivalent to Oja's Subspace Algorithm <ref> [8] </ref>. In performing a Principal Component Analysis, the network is using the redundancy in the input data to find filters which are decorrelated and retain as much of the variance in the input data as possible.
References-found: 8

