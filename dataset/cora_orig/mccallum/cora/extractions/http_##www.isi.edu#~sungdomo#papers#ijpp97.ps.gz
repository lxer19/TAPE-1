URL: http://www.isi.edu/~sungdomo/papers/ijpp97.ps.gz
Refering-URL: http://www.isi.edu/~sungdomo/publications.html
Root-URL: http://www.isi.edu
Title: Adaptive Granularity: Transparent Integration of Fine- and Coarse-Grain Communication  
Author: Daeyeon Park Rafael H. Saavedra Sungdo Moon 
Keyword: Adaptive Granularity, Distributed Shared-Memory Multiprocessor, Bulk Data Transfer, Memory Repli cation.  
Address: Yongin, Kyoungkido, 449-791, Republic of Korea,  Los Angeles, California, 90089-0781, USA,  
Affiliation: Department of Control and Instrumentation Engineering, Hankuk University of Foreign Studies,  Department of Computer Science, SAL-300, University of Southern California,  
Note: Appeared in the International Journal of Parallel Programming, Volume 25, Number 5, pp.419--446,  This research was supported in part by NSF under grant CCR-9308981 by ARPA under Rome Laboratories Contract F30602-91-C-0146, and by the USC Zumberge Fund. Computing resources were provided in part by NSF infrastructure grant CDA-9216321.  
Email: E-mail: daeyeon@maincc.hufs.ac.kr.  E-mail: fsaavedra,sungdomog@cs.usc.edu.  
Date: 1997  
Abstract: The granularity of shared data is one of the key factors affecting the performance of distributed shared memory machines (DSM). Given that programs exhibit quite different sharing patterns, providing only one or two fixed granularities cannot result in an efficient use of resources. On the other hand, supporting arbitrarily granularity sizes significantly increases not only hardware complexity but software overhead as well. Furthermore, the efficient use of arbitrarily granularities put the burden on users to provide information about program behavior to compilers and/or runtime systems. These kind of requirements tend to restrict the programmability of the shared memory model. In this paper, we present a new communication scheme, called Adaptive Granularity (AG). Adaptive Granularity makes it possible to transparently integrate bulk transfer into the shared memory model by supporting variable-size granularity and memory replication. It consists of two protocols: one for small data and another for large data. For small size data, the standard hardware DSM protocol is used and the granularity is fixed to the size of a cache line. For large array data, the protocol for bulk data is used instead, and the granularity varies depending on the runtime sharing behavior of the applications. Simulation results show that AG improves performance up to 43% over the hardware implementation of DSM (e.g., DASH, Alewife). Compared with an equivalent architecture that supports fine-grain memory replication at the fixed granularity of a cache line (e.g., Typhoon), AG reduces execution time up to 35%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the 2nd ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <pages> pages 168176, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: The use of a fixed coarse granularity, however, tends to severely degrade performance in the presence of fine-grain sharing. To reduce this problem several mechanisms such as lazy release consistency [12] have been proposed. IVY [16] and Munin <ref> [1] </ref> fall into this category. Another proposed approach for reducing the mismatch of the page-based software DSM, has been to impose a new programming model 3 . Examples of these are object-based and region-based software DSMs, in which a variable grain size is used [2, 5, 19].
Reference: [2] <author> B. N. Bershad, M. J. Zekauskas, and W. A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In Proceedings of the 1993 IEEE CompCon Conference, </booktitle> <pages> pages 528537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: A hardware DSM (HW-DSM) refers to machines that implement the cache coherence protocol completely in hardware (e.g., Stanford DASH, MIT Alewife). A software DSM (SW-DSM) is a shared memory abstraction that is implemented on top of loosely coupled multicomputers based on a network of workstations (e.g., TreadMarks [11], Midway <ref> [2] </ref>). 2.1 Message Passing Machines and Hardware DSM Message passing and hardware DSM machines are two well-known representatives of parallel machines that use, respectively, send-receive and load-store instructions for communication. <p> IVY [16] and Munin [1] fall into this category. Another proposed approach for reducing the mismatch of the page-based software DSM, has been to impose a new programming model 3 . Examples of these are object-based and region-based software DSMs, in which a variable grain size is used <ref> [2, 5, 19] </ref>.
Reference: [3] <author> R. Chandra, K. Gharachorloo, V. Soundararajan, and A. Gupta. </author> <title> Performance evaluation of hybrid hardware and software distributed shared memory protocols. </title> <booktitle> In Proceedings of the 8th ACM International Conference on Supercomputing, </booktitle> <pages> pages 274288, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: In order to use the bulk transfer facility on these machines, several approaches have been proposed such as explicit messages [9, 22] and new programming models <ref> [3] </ref>. In explicit messages, message passing communication primitives such as send-receive or memory-copy are used selectively to communicate coarse-grain data, while load-store communication is used for fine-grain data [22]. In other words, two communication paradigms coexist in the program and it is the user's responsibility to select the appropriate model. <p> In other words, two communication paradigms coexist in the program and it is the user's responsibility to select the appropriate model. Another approach for exploiting the advantages of bulk transfer is to use a new programming model. One example is the Hybrid protocol <ref> [3] </ref>, in which programmer-supplied annotations are used to support a variable size sharing granularity. The Hybrid protocol consists of standard hardware and software (region-based) DSM protocols that are used for fine and coarse data, respectively. <p> The bulk transfer facility can also be used to implement new programming models that can help reduce the complexity of selecting the best communication mechanism to use on each data structure <ref> [3] </ref>. Here, programmers provide granularity information to compilers and/or runtime systems, which in turn have the responsibility of selecting the right communication model. One example is an hybrid protocol on top of a software DSM where programmer-supplied annotations identify shared data regions for which specific granularity sizes are set. <p> First, either programs have to be modified to effectively make use of the message passing substrate [22] or they have to be rewritten in a new programming model, one that captures the characteristics of the integrated machine <ref> [3] </ref>. Both alternative, however, impose an extra burden on programmers which diminishes the intended benefits of using shared-memory machines. Second, because bulk-transferred data can be cached, coherence has to be maintained amongst the processors. <p> Another approach that attempts to exploit the benefits of bulk transfer requires sacrificing in some measure the programmability offered by the shared memory model. Here, annotations or explicit messages are inserted in the code to communicate bulk data. The Hybrid protocol <ref> [3] </ref> and Shared Regions [19] both allow coherence to occur at any granularity with the use of annotations and identify regions in which a specific granularity is used.
Reference: [4] <author> S. Chandra, J. R. Larus, and A. Rogers. </author> <booktitle> Where is time spent in message-passing and shared-memory programs? In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 6173, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Supporting arbitrary data alignment adds to hardware and software costs [9]. Finally, recent studies indicate that bulk transfer may not actually help the performance of shared memory applications running on message passing machines <ref> [4, 22] </ref>. The main reason is due to the message passing overheads. Furthermore, this is also true even when the overheads are greatly reduced by allowing user-level access to the messaging facilities. 3 Adaptive Granularity In this section we describe Adaptive Granularity in detail.
Reference: [5] <author> J. S. Chase, F. G. Amador, E. D. Lazowska, H. M. Levy, and R. J. Littlefied. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Oerating Systems Principles, </booktitle> <pages> pages 147168, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: IVY [16] and Munin [1] fall into this category. Another proposed approach for reducing the mismatch of the page-based software DSM, has been to impose a new programming model 3 . Examples of these are object-based and region-based software DSMs, in which a variable grain size is used <ref> [2, 5, 19] </ref>. <p> Examples of these are object-based and region-based software DSMs, in which a variable grain size is used [2, 5, 19]. In an object-based system a particular size is associated to each data object <ref> [5, 10] </ref>, while in a region-based system the grains are defined by programmers and can be of arbitrary size. 2.3 Integrated DSM On integrated DSM machines (INT-DSM) that support a bulk transfer facility in a cache-coherent shared address space, a programmer is presented with three mechanisms for communicating data: (i) standard
Reference: [6] <author> C. Dubnicki and T. LeBlanc. </author> <title> Adjustable block size coherent caches. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 170180, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The main difference is that Typhoon uses tagged memory with a fixed size grain (cache line). AG, on the other hand, uses general memory and supports variable size granularity. Several other approaches have been proposed to support multi-granularities in the context of shared memory machines. Dubnicki and LeBlanc <ref> [6] </ref> describe a hardware cache coherent system that dynamically adjusts the cache block size based on the behavior of references. Cache blocks are split when false sharing occurs and merged back into a larger cache line when a single processor owns both of the sub-blocks.
Reference: [7] <author> M. Dubois, C. Scheurich, and F. A. Briggs. </author> <title> Synchronization, coherence, and event ordering in multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 21(2):921, </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: The node controller contains a programmable processor which uses handlers to process all messages from the processor and network. Memory pages are allocated to the memory modules in a round-robin way. The physical memory is equally distributed among the nodes and a sequential memory consistency model <ref> [7] </ref> is enforced. Cache coherence for shared memory is maintained using a distributed directory-based protocol similar to the Stanford FLASH and Wisconsin Typhoon protocols. The only difference is that on AG, the bulk transfer facility for the message passing is not needed.
Reference: [8] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> DDM Cache-only memory architecture. </title> <journal> IEEE Computer, </journal> <volume> 25(9):44 54, </volume> <month> September </month> <year> 1992. </year>
Reference-contexts: Memory replication has been implemented on other systems through different mechanisms. Cache-only memory architectures (COMA) <ref> [8] </ref> use local memory as a cache which allows complete replicating of all shared data. Software DSMs implement memory replication through the operating system at the granularity of a page frame or through the runtime systems at the granularity of user-defined objects or regions.
Reference: [9] <author> J. Heinlein, K. Gharachorloo, S. Dresser, and A. Gupta. </author> <title> Integration of message passing and shared memory in the Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 3850, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: In order to use the bulk transfer facility on these machines, several approaches have been proposed such as explicit messages <ref> [9, 22] </ref> and new programming models [3]. In explicit messages, message passing communication primitives such as send-receive or memory-copy are used selectively to communicate coarse-grain data, while load-store communication is used for fine-grain data [22]. <p> Second, because bulk-transferred data can be cached, coherence has to be maintained amongst the processors. Enforcing global coherency for arbitrarily sized bulk data using a standard load-store mechanism substantially increases the hardware complexity and/or software overhead <ref> [9] </ref>. Third, data alignment becomes an issue because bulk transfers might not start or end at cache line boundaries. Supporting arbitrary data alignment adds to hardware and software costs [9]. <p> Enforcing global coherency for arbitrarily sized bulk data using a standard load-store mechanism substantially increases the hardware complexity and/or software overhead <ref> [9] </ref>. Third, data alignment becomes an issue because bulk transfers might not start or end at cache line boundaries. Supporting arbitrary data alignment adds to hardware and software costs [9]. Finally, recent studies indicate that bulk transfer may not actually help the performance of shared memory applications running on message passing machines [4, 22]. The main reason is due to the message passing overheads.
Reference: [10] <author> E. Jul, H. M. Levy, N. Hutchinson, and A. Black. </author> <title> Fine-grained mobility in the Emerald system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1):109133, </volume> <month> February </month> <year> 1988. </year> <month> 15 </month>
Reference-contexts: Examples of these are object-based and region-based software DSMs, in which a variable grain size is used [2, 5, 19]. In an object-based system a particular size is associated to each data object <ref> [5, 10] </ref>, while in a region-based system the grains are defined by programmers and can be of arbitrary size. 2.3 Integrated DSM On integrated DSM machines (INT-DSM) that support a bulk transfer facility in a cache-coherent shared address space, a programmer is presented with three mechanisms for communicating data: (i) standard
Reference: [11] <author> P. Keleher, A. L. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: A hardware DSM (HW-DSM) refers to machines that implement the cache coherence protocol completely in hardware (e.g., Stanford DASH, MIT Alewife). A software DSM (SW-DSM) is a shared memory abstraction that is implemented on top of loosely coupled multicomputers based on a network of workstations (e.g., TreadMarks <ref> [11] </ref>, Midway [2]). 2.1 Message Passing Machines and Hardware DSM Message passing and hardware DSM machines are two well-known representatives of parallel machines that use, respectively, send-receive and load-store instructions for communication.
Reference: [12] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Laze relaese consistecny for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 1321, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The use of a fixed coarse granularity, however, tends to severely degrade performance in the presence of fine-grain sharing. To reduce this problem several mechanisms such as lazy release consistency <ref> [12] </ref> have been proposed. IVY [16] and Munin [1] fall into this category. Another proposed approach for reducing the mismatch of the page-based software DSM, has been to impose a new programming model 3 .
Reference: [13] <author> D. Kranz, K. L. Johnson, A. Agarwal, J. Kubiatowicz, and B.-H. Lim. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <booktitle> In Proceedings of the 3rd ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <pages> pages 5463, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction The shared memory programming model provides a simple communication abstraction that greatly simplifies parallel programming, particularly on problems that exhibit dynamic communication and fine-grain sharing patterns. Furthermore, most hardware distributed shared memory machines (DSM) <ref> [13, 15] </ref> achieve high performance by allowing the caching of shared writable data as well as read-only data. <p> Furthermore, most hardware distributed shared memory machines (DSM) [13, 15] achieve high performance by allowing the caching of shared writable data as well as read-only data. One disadvantage of this kind of cache-coherent machines (e.g., Stanford DASH [15] or MIT Alewife <ref> [13] </ref>) is that they use a fixed size block (i.e., a cache line for loads and stores) as a way of a communication. While this works well for fine-grain data, on some application, data bulk transfer can sometimes be more effective.
Reference: [14] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: For fine-grain sharing, it exploits the advantages of the standard load-store communication by using 1 A hardware-software DSM (HS-DSM) refers to a hardware DSM that implements in software the coherence protocol on the separate node controller (e.g., Stanford FLASH <ref> [14] </ref>, Wisconsin Typhoon [18]. 2 The type is initially saved on the node controller at the time the page mapping is made. 2 cache line transfers.
Reference: [15] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hannessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148159, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: 1 Introduction The shared memory programming model provides a simple communication abstraction that greatly simplifies parallel programming, particularly on problems that exhibit dynamic communication and fine-grain sharing patterns. Furthermore, most hardware distributed shared memory machines (DSM) <ref> [13, 15] </ref> achieve high performance by allowing the caching of shared writable data as well as read-only data. <p> Furthermore, most hardware distributed shared memory machines (DSM) [13, 15] achieve high performance by allowing the caching of shared writable data as well as read-only data. One disadvantage of this kind of cache-coherent machines (e.g., Stanford DASH <ref> [15] </ref> or MIT Alewife [13]) is that they use a fixed size block (i.e., a cache line for loads and stores) as a way of a communication. While this works well for fine-grain data, on some application, data bulk transfer can sometimes be more effective.
Reference: [16] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4):321359, </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: The use of a fixed coarse granularity, however, tends to severely degrade performance in the presence of fine-grain sharing. To reduce this problem several mechanisms such as lazy release consistency [12] have been proposed. IVY <ref> [16] </ref> and Munin [1] fall into this category. Another proposed approach for reducing the mismatch of the page-based software DSM, has been to impose a new programming model 3 . Examples of these are object-based and region-based software DSMs, in which a variable grain size is used [2, 5, 19].
Reference: [17] <author> D. Park and R. H. Saavedra. </author> <title> Trojan: High-performance simulator for parallel shared-memory architecture. </title> <booktitle> In Proceedings of the 29th Annual Simulation Symposium, </booktitle> <pages> pages 4453, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: The machine was simulated using an execution-driven simulator called Trojan <ref> [17] </ref>. It allows the simulation of process-based and thread-based applications by modeling in detail the various hardware components. Instruction references are assumed to take one cycle and virtual memory is enabled in all simulations.
Reference: [18] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Typhoon and Tempest: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: For fine-grain sharing, it exploits the advantages of the standard load-store communication by using 1 A hardware-software DSM (HS-DSM) refers to a hardware DSM that implements in software the coherence protocol on the separate node controller (e.g., Stanford FLASH [14], Wisconsin Typhoon <ref> [18] </ref>. 2 The type is initially saved on the node controller at the time the page mapping is made. 2 cache line transfers. <p> Page-level memory replication, however, is not effective for applications exhibiting fine-grain sharing, which tend to suffer from poor performance. To solve this problem, Typhoon allows memory replication at a finer granularity (i.e., cache line) through a mechanism called Stache <ref> [18] </ref>. Stache consists of user-level handlers that can be used to implement page-fault, message, and line-access-fault handlers. Our particular implementation of memory replication borrows many ideas from Typhoon. The main difference is that Typhoon uses tagged memory with a fixed size grain (cache line).
Reference: [19] <author> H. S. Sandhu, B. Gamsa, and S. Zhou. </author> <title> The shared regions approach to software cache coherence on multiprocessors. </title> <booktitle> In Proceedings of the 3rd ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <pages> pages 229238, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: IVY [16] and Munin [1] fall into this category. Another proposed approach for reducing the mismatch of the page-based software DSM, has been to impose a new programming model 3 . Examples of these are object-based and region-based software DSMs, in which a variable grain size is used <ref> [2, 5, 19] </ref>. <p> Another approach that attempts to exploit the benefits of bulk transfer requires sacrificing in some measure the programmability offered by the shared memory model. Here, annotations or explicit messages are inserted in the code to communicate bulk data. The Hybrid protocol [3] and Shared Regions <ref> [19] </ref> both allow coherence to occur at any granularity with the use of annotations and identify regions in which a specific granularity is used. Under explicit messages, a programmer is presented with two communication paradigms (load-store and message passing) and he/she selects the appropriate type of communication [22].
Reference: [20] <author> A. S. Tanenbaum. </author> <title> Modern Operating Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: The BDP and HDP protocols are quite similar in the number of states and state transitions, but in order to efficiently support variable granularity sizes, BDP implements a buddy system <ref> [20] </ref>. The buddy system was originally adopted in memory management algorithms in order to speed up the merging of adjacent holes when a memory block was returned. It exploits the fact that computers use binary numbers for addressing.
Reference: [21] <author> A. W. Wilson, Jr. and R. P. LaRowe, Jr. </author> <title> Hiding shared memory reference latency on the Galactica Net distributed shared memory architecture. </title> <booktitle> Jornal of Parallel and Distributed Computing, </booktitle> <address> 15(4):351367, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Compared to AG, however, their system is limited to a maximum grain size of one cache line, which derives little benefits from bulk transfer. Galactica Net <ref> [21] </ref> and MGS [23] support two fixed size granularities: cache line and page frame. Depending on the behavior of memory accesses, both schemes select the correct grain size to use.
Reference: [22] <author> S. C. Woo, J. P. Singh, and J. L. Hennessy. </author> <title> The performance advantages of intergrating block data transfer in cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 219229, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: While this works well for fine-grain data, on some application, data bulk transfer can sometimes be more effective. Bulk transfer has several advantages over fine-grain communication: 1) the pipelin-ing of data transfers, 2) the overlapping of communication with computation, and 3) the replication of data in local memory <ref> [22] </ref>. To exploit the advantages of fine-grain and coarse-grain communication, recent shared memory machines such as Stanford FLASH and Wisconsin Typhoon integrate both models within a single architecture and implement coherence protocols in software rather than in hardware. <p> In order to use the bulk transfer facility on these machines, several approaches have been proposed such as explicit messages <ref> [9, 22] </ref> and new programming models [3]. In explicit messages, message passing communication primitives such as send-receive or memory-copy are used selectively to communicate coarse-grain data, while load-store communication is used for fine-grain data [22]. <p> In explicit messages, message passing communication primitives such as send-receive or memory-copy are used selectively to communicate coarse-grain data, while load-store communication is used for fine-grain data <ref> [22] </ref>. In other words, two communication paradigms coexist in the program and it is the user's responsibility to select the appropriate model. Another approach for exploiting the advantages of bulk transfer is to use a new programming model. <p> Although these approaches achieve the goal of providing variable grain sizes to programs, they suffer from several problems. First, either programs have to be modified to effectively make use of the message passing substrate <ref> [22] </ref> or they have to be rewritten in a new programming model, one that captures the characteristics of the integrated machine [3]. Both alternative, however, impose an extra burden on programmers which diminishes the intended benefits of using shared-memory machines. <p> Supporting arbitrary data alignment adds to hardware and software costs [9]. Finally, recent studies indicate that bulk transfer may not actually help the performance of shared memory applications running on message passing machines <ref> [4, 22] </ref>. The main reason is due to the message passing overheads. Furthermore, this is also true even when the overheads are greatly reduced by allowing user-level access to the messaging facilities. 3 Adaptive Granularity In this section we describe Adaptive Granularity in detail. <p> Under explicit messages, a programmer is presented with two communication paradigms (load-store and message passing) and he/she selects the appropriate type of communication <ref> [22] </ref>.
Reference: [23] <author> D. Yeung, J. Kubiatowicz, and A. Agarwal. MGS: </author> <title> A multigrain shared memory system. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 4455, </pages> <month> May </month> <year> 1996. </year> <month> 16 </month>
Reference-contexts: Compared to AG, however, their system is limited to a maximum grain size of one cache line, which derives little benefits from bulk transfer. Galactica Net [21] and MGS <ref> [23] </ref> support two fixed size granularities: cache line and page frame. Depending on the behavior of memory accesses, both schemes select the correct grain size to use.
References-found: 23

