URL: http://www.cs.orst.edu/~chown/papers/ml96-new.ps
Refering-URL: http://www.cs.orst.edu/~chown/papers/conf.html
Root-URL: 
Title: Learning in the Presence of Prior Knowledge: A Case Study Using Model Calibration  
Abstract: Computational models of natural systems often contain free parameters that must be set to optimize the predictive accuracy of the models. This process| called calibration|can be viewed as a form of supervised learning in the presence of prior knowledge. In this view, the fixed aspects of the model constitute the prior knowledge, and the goal is to learn values for the free parameters. We report on a series of attempts to learn parameter values for a global vegetation model called MAPSS (Mapped Atmosphere-Plant-Soil System) developed by our collaborator, Ron Neilson. Standard machine learning methods do not work with MAPSS, because the constraints introduced by the structure of the model create a very difficult non-linear optimization problem. We developed a new divide-and-conquer approach in which subsets of the parameters are calibrated while others are held constant. This approach succeeds because it is possible to select training examples that exercise only portions of the model. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Brent, R. P. </author> <year> (1973). </year> <title> Algorithms for Minimization without Derivatives. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: However, with these numerically-computed gradients, it is possible to apply gradient descent algorithms to minimize J. The second approach we pursued was to apply Powell's method <ref> (Brent, 1973) </ref>. This is a search technique that can be applied when no gradient information is available. Like the conjugate-gradient algorithm, Powell's method is a direction set algorithm that tries to identify good directions along which to minimize J.
Reference: <author> Combettes, P. L. </author> <year> (1993). </year> <title> The foundations of set theoretic estimation. </title> <journal> Proc. IEEE, </journal> <volume> 81 (2), </volume> <pages> 182-208. </pages>
Reference-contexts: The sets from each data point are intersected to find a small set of possible modifications that are consistent with all of the training examples. Examples of this approach include the Incremental Version Space Merging (IVSM) method of Hirsh (1994) and similar methods from system identification <ref> (Combettes, 1993) </ref>. Klepper and Hendrix (1994) have applied this approach to calibrate ecosystem models. 1 As the size and complexity of prior knowledge increases, neither of these methods scales.
Reference: <author> Dempster, A. P., Laird, N. M., & Rubin, D. B. </author> <year> (1976). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> R. Stat. Soc., B, </journal> <volume> 39, </volume> <pages> 1-38. </pages>
Reference-contexts: An open problem for future research is to find a strategy that can handle a wide range of initial values for the transpiration parameters (e.g., by alternating between calibrating the soil parameters and the transpiration parameters in the style of the EM algorithm <ref> (Dempster, Laird, & Rubin, 1976) </ref>). We did attempt one global calibration of the 12 soil water parameters simultaneously using a simulated annealing search algorithm on Training Set 3.
Reference: <author> Hirsh, H. </author> <year> (1994). </year> <title> Generalizing version spaces. </title> <journal> Machine Learning, </journal> <volume> 17, </volume> <pages> 5-46. </pages>
Reference: <author> Ingber, L. </author> <year> (1995). </year> <title> Adaptive simulated annealing. </title> <type> Tech. rep., </type> <institution> Cal. Inst. Tech. </institution>
Reference: <author> Klepper, O., & Hendrix, E. M. T. </author> <year> (1994). </year> <title> A method for calibration of ecological models under different types of uncertainty. Eco. </title> <journal> Mod., </journal> <volume> 74, </volume> <pages> 161-182. </pages>
Reference: <author> Kuchler, A. W. </author> <year> (1964). </year> <title> The potential natural vegetation of the conterminous United States. </title> <type> Tech. rep. 36, </type> <institution> Am. Geo. Soc. </institution> <note> Special Pub. </note>
Reference: <author> Neilson, R. P. </author> <year> (1995). </year> <title> A model for predicting continental-scale vegetation distribution and water balance. </title> <journal> Ecological Applications, </journal> <volume> 5, </volume> <pages> 362-385. </pages>
Reference: <author> Pazzani, M., & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 57-94. </pages>
Reference-contexts: A global search then considers modifications or refinements of the prior knowledge that minimize this error. There are many examples of this approach, including KBANN (Shavlik & Towell, 1989), fitting Bayesian networks (Russell, Binder, Koller, & Kanazawa, 1995), and the FOCL system <ref> (Pazzani & Kibler, 1992) </ref>. Set-intersection methods take each training data point and compute the set of possible modifications consistent with the prior knowledge that can account for that data point.
Reference: <author> Russell, S., Binder, J., Koller, D., & Kanazawa, K. </author> <year> (1995). </year> <title> Local learning in probabilistic networks with hidden variables. </title> <booktitle> In IJCAI, </booktitle> <pages> pp. </pages> <address> 1146-1152 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Global optimization approaches define an error function that measures the misfit between the model and the data. A global search then considers modifications or refinements of the prior knowledge that minimize this error. There are many examples of this approach, including KBANN (Shavlik & Towell, 1989), fitting Bayesian networks <ref> (Russell, Binder, Koller, & Kanazawa, 1995) </ref>, and the FOCL system (Pazzani & Kibler, 1992). Set-intersection methods take each training data point and compute the set of possible modifications consistent with the prior knowledge that can account for that data point.
Reference: <author> Shavlik, J. W., & Towell, G. G. </author> <year> (1989). </year> <title> An approach to combining explanation-based and neural learning algorithms. </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> 233-255. </pages>
Reference-contexts: Global optimization approaches define an error function that measures the misfit between the model and the data. A global search then considers modifications or refinements of the prior knowledge that minimize this error. There are many examples of this approach, including KBANN <ref> (Shavlik & Towell, 1989) </ref>, fitting Bayesian networks (Russell, Binder, Koller, & Kanazawa, 1995), and the FOCL system (Pazzani & Kibler, 1992). Set-intersection methods take each training data point and compute the set of possible modifications consistent with the prior knowledge that can account for that data point.
Reference: <author> Simard, P., Le Cun, Y., & Denker, J. </author> <year> (1993). </year> <title> Efficient pattern recognition using a new transformation distance. </title> <booktitle> NIPS 5, </booktitle> <pages> pp. 50-58. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Williams, B. C., & Millar, B. </author> <year> (1996). </year> <title> Automated decomposition of model-based learning problems. In Qualitative Reasoning: </title> <booktitle> The Tenth International Workshop, </booktitle> <pages> pp. </pages> <address> 265-273 Cambridge, MA. </address> <publisher> AAAI Press/MIT Press. </publisher> <pages> 16 </pages>
References-found: 13

