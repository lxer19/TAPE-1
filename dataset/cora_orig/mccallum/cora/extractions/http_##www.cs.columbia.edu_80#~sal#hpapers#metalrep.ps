URL: http://www.cs.columbia.edu:80/~sal/hpapers/metalrep.ps
Refering-URL: http://www.cs.columbia.edu:80/~sal/recent-papers.html
Root-URL: 
Email: pkc@cs.fit.edu  sal@cs.columbia.edu  
Title: Scaling Learning by Meta-Learning over Disjoint and Partially Replicated Data  
Author: Philip K. Chan Salvatore J. Stolfo 
Address: Melbourne, FL 32901  New York, NY 10027  
Affiliation: Computer Science Florida Institute of Technology  Department of Computer Science Columbia University  
Abstract: Many existing learning algorithms assume that the entire data set fits into main memory, which is not feasible for massive amounts of inherently distributed data. One approach we explore to handling a large data set is to partition the data into disjoint subsets, run the learning algorithm on each of these subsets of data, and combine the results by some means. The results achieved to date show promising results, but in nearly all cases accuracy of the final combined classifier is not as great as a single classifier computed from the entire data set. In this paper we evaluate our approach, called meta-learning, to learning from partitioned data where we relax the restriction that each subset of training data is disjoint, i.e. some amount of replication of training data is allowed. We anticipated data replication could improve overall accuracy, however, our findings suggest the contrary. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: class-attribute-combiner.) The next section discusses our findings from experiments where a controlled amount of "arbitrary replication" is permitted in each subset of data used to train the base classifiers. 3 Learning Tasks and Experi mental Setup Two decision-tree inductive learning algorithms were used in our experiments: ID3 [8] and CART <ref> [1] </ref>, both obtained from NASA Ames Research Center in the IND package [2]. Two data sets were used in our studies. The DNA splice junction (SJ) data set 1 [9] contains 3,190 training instances of nucleotides and the type of splice junction, if any, at the center of each sequence.
Reference: [2] <author> W. Buntine and R. Caruana. </author> <title> Introduction to IND and Recursive Partitioning. </title> <institution> NASA Ames Research Center, </institution> <year> 1991. </year>
Reference-contexts: amount of "arbitrary replication" is permitted in each subset of data used to train the base classifiers. 3 Learning Tasks and Experi mental Setup Two decision-tree inductive learning algorithms were used in our experiments: ID3 [8] and CART [1], both obtained from NASA Ames Research Center in the IND package <ref> [2] </ref>. Two data sets were used in our studies. The DNA splice junction (SJ) data set 1 [9] contains 3,190 training instances of nucleotides and the type of splice junction, if any, at the center of each sequence.
Reference: [3] <author> P. Chan and S. Stolfo. </author> <title> Experiments on mul-tistrategy learning by meta-learning. </title> <booktitle> In Proc. Second Intl. Conf. Info. Know. Manag., </booktitle> <pages> pages 314-323, </pages> <year> 1993. </year>
Reference-contexts: Our techniques fall into two general categories: the arbiter and combiner strategies. Due to space limitations, only the combiner strategies are discussed here. Detailed descriptions of the arbiter strategies can be found in <ref> [3] </ref>. Combiner In the combiner [3] strategy, the predictions of the learned base classifiers on the training set form the basis of the meta-learner's training set. A composition rule, which varies in different schemes, determines the content of training examples for the meta-learner. <p> Our techniques fall into two general categories: the arbiter and combiner strategies. Due to space limitations, only the combiner strategies are discussed here. Detailed descriptions of the arbiter strategies can be found in <ref> [3] </ref>. Combiner In the combiner [3] strategy, the predictions of the learned base classifiers on the training set form the basis of the meta-learner's training set. A composition rule, which varies in different schemes, determines the content of training examples for the meta-learner.
Reference: [4] <author> P. Chan and S. Stolfo. </author> <title> Meta-learning for mul-tistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Work. on Multistrategy Learning, </booktitle> <pages> pages 150-165, </pages> <year> 1993. </year>
Reference-contexts: Some amount of important information may be lost when reducing the data. Our proposed meta-learning techniques, which were first presented in <ref> [4] </ref>, involves applying a learning algorithm to sets of predictions (treated as training data) made by a set of base classifiers in order to learn how to combine their collective predictions. Comparative results from applying meta-learning and other voting based strategies to learning from disjoint subsets were discussed in [5].
Reference: [5] <author> P. Chan and S. Stolfo. </author> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> In Proc. Twelfth Intl. Conf. Machine Learning, </booktitle> <pages> pages 90-98, </pages> <year> 1995. </year>
Reference-contexts: Others have studied approaches based upon direct parallelization of a learning algorithm run on a centralized multiprocessor with "scalable" main-memory resources [13]. A review of such approaches has appeared elsewhere <ref> [5] </ref>. <p> Comparative results from applying meta-learning and other voting based strategies to learning from disjoint subsets were discussed in <ref> [5] </ref>. In this paper we provide an evaluation of our techniques and another published method on combining classifiers learned from partitioned subsets where some amount of replication is allowed. We believe this situation might be common in certain real world contexts.
Reference: [6] <author> M. Craven and J. Shavlik. </author> <title> Learning to represent codons: A challenge problem for constructive induction. </title> <booktitle> In Proc. IJCAI-93, </booktitle> <pages> pages 1319-1324, </pages> <year> 1993. </year>
Reference-contexts: There are three possible junctions, and hence 3 categorical classes in this task. The protein coding region (PCR) data set 2 <ref> [6] </ref> contains DNA nucleotide sequences and their binary classifications (coding or non-coding). The PCR data set has 20,000 sequences.
Reference: [7] <author> J. R. Quinlan. </author> <title> Induction over large data bases. </title> <type> Technical Report STAN-CS-79-739, </type> <institution> Comp. Sci. Dept., Stanford Univ., </institution> <year> 1979. </year>
Reference-contexts: In such situations, it may not be possible, nor feasible, to inspect all of the data at one processing site to compute one primary "global" classifier. Popular incremental learning algorithms such as <ref> [7, 10] </ref>, aim to solve the scaling problem by piecemeal processing of a large data set that is consumed in a sequential fashion. Others have studied approaches based upon direct parallelization of a learning algorithm run on a centralized multiprocessor with "scalable" main-memory resources [13].
Reference: [8] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: is denoted as class-attribute-combiner.) The next section discusses our findings from experiments where a controlled amount of "arbitrary replication" is permitted in each subset of data used to train the base classifiers. 3 Learning Tasks and Experi mental Setup Two decision-tree inductive learning algorithms were used in our experiments: ID3 <ref> [8] </ref> and CART [1], both obtained from NASA Ames Research Center in the IND package [2]. Two data sets were used in our studies.
Reference: [9] <author> G. Towell, J. Shavlik, and M. Noordewier. </author> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proc. AAAI-90, </booktitle> <pages> pages 861-866, </pages> <year> 1990. </year>
Reference-contexts: Two data sets were used in our studies. The DNA splice junction (SJ) data set 1 <ref> [9] </ref> contains 3,190 training instances of nucleotides and the type of splice junction, if any, at the center of each sequence. There are three possible junctions, and hence 3 categorical classes in this task.
Reference: [10] <author> J. Wirth and J. Catlett. </author> <title> Experiments on the costs and benefits of windowing in ID3. </title> <booktitle> In Proc. Fifth Intl. Conf. Machine Learning, </booktitle> <pages> pages 87-99, </pages> <year> 1988. </year>
Reference-contexts: In such situations, it may not be possible, nor feasible, to inspect all of the data at one processing site to compute one primary "global" classifier. Popular incremental learning algorithms such as <ref> [7, 10] </ref>, aim to solve the scaling problem by piecemeal processing of a large data set that is consumed in a sequential fashion. Others have studied approaches based upon direct parallelization of a learning algorithm run on a centralized multiprocessor with "scalable" main-memory resources [13].
Reference: [11] <author> D. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259, </pages> <year> 1992. </year>
Reference-contexts: Return meta-level training instances with the correct classification and the predictions; i.e., T = f (class (x); C 1 (x); C 2 (x); :::C k (x)) j x 2 Eg: This scheme was also used by Wolpert <ref> [11] </ref>. (For further reference, this scheme is denoted as class combiner.) 2.
Reference: [12] <author> L. Xu, A. Krzyzak, and C. Suen. </author> <title> Methods of combining multiple classifires and their applications to handwriting recognition. </title> <journal> IEEE Trans. Sys. Man. Cyb., </journal> <volume> 22 </volume> <pages> 418-435, </pages> <year> 1992. </year>
Reference-contexts: We call the accuracy of this global classifier produced by ID3 and CART the "baseline case," which is plotted in our graphs as the "one subset" case on the X-axis. In addition to our meta-learning strategies, a Bayesian statistical approach (bayesian-belief) as presented in <ref> [12] </ref> was also used in our experiments to combine classifiers. 4 Experimental Results on Replicated Data In our experiments each partition of data allowed some amount of replication. We prepare each learning task by generating subsets of training data for the base classifiers according to the following generative scheme. 1.
Reference: [13] <author> X. Zhang, M. Mckenna, J. Mesirov, and D. Waltz. </author> <title> An efficient implementation of the backpropagation algorithm on the connection machine CM-2. </title> <type> Technical Report RL89-1, </type> <institution> Thinking Machines Corp., </institution> <year> 1989. </year> <note> from 0% to 30%. </note>
Reference-contexts: Others have studied approaches based upon direct parallelization of a learning algorithm run on a centralized multiprocessor with "scalable" main-memory resources <ref> [13] </ref>. A review of such approaches has appeared elsewhere [5].
References-found: 13

