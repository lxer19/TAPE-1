URL: http://www.pdos.lcs.mit.edu/~skgupta/papers/thesis.ps
Refering-URL: http://www.pdos.lcs.mit.edu/~skgupta/papers/index.html
Root-URL: 
Title: Protocol Optimizations for the CRL Distributed Shared Memory System  
Author: by Sandeep K. Gupta M. Frans Kaashoek 
Degree: B.S., Electrical and Computer Engineering (1994) B.A., Computer Science (1994)  Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Master of Science in Computer Science and Engineering at the  All rights reserved. Author  Certified by  Associate Professor of Computer Science and Engineering Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Department Committee on Graduate Theses  
Affiliation: Rice University  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1996.  Department of Electrical Engineering and Computer Science  
Date: (1994)  September 1996  August 28, 1996  
Note: B.A., Cognitive Sciences  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk L. Johnson, David Kranz, John Ku-biatowicz, Beng-Hong Lim, Ken Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: There are some cases when DSM performs better than message passing though. For example in Kevin Lew's master's thesis [27], for a particular problem size where there was little memory contention, DSM performed better than message passing on the Alewife machine <ref> [1] </ref>. In this case the better performance of the DSM application was attributed to the fact that the hardware platform had hardware support for DSM when only a few nodes shared the data. <p> When data needs to be transferred from one node to another, some sort of handshake protocol may be needed. On the MIT Alewife machine <ref> [1] </ref>, it is possible to write directly to an address on a remote node using Alewife's builtin data transfer functions. However on the SP/2, there is no such mechanism. The only way to transfer data is by using message passing.
Reference: [2] <author> Anant Agarwal, David Chaiken, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kuri-hara, Beng-Hong Lim, Gino Maa, and Dan Nussbaum. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <type> Technical Report MIT/LCS/TM-454, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: DSM implementations that are mostly software are implemented over some message passing mechanism [3, 21, 22, 33] while ones that are mostly hardware are usually implemented by using a fixed mapping from global address to node number, and then storing the information at that node <ref> [2, 25] </ref>. In DSM systems, applications are not required to keep track of which processor has a valid copy of the data, where the data is located locally in memory, and which processors are allowed to read from/write to the data.
Reference: [3] <author> John B. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: It is essentially an abstraction of a global shared memory. DSM implementations that are mostly software are implemented over some message passing mechanism <ref> [3, 21, 22, 33] </ref> while ones that are mostly hardware are usually implemented by using a fixed mapping from global address to node number, and then storing the information at that node [2, 25].
Reference: [4] <author> Satish Chandra, James R. Larus, and Anne Rogers. </author> <booktitle> Where is Time Spent in Message-Passing and Shared-Memory Programs? In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 61-73, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: In papers comparing the execution time of message passing programs to that of DSM programs, message passing programs usually perform better <ref> [30, 28, 4] </ref>.
Reference: [5] <author> Satish Chandra, Brad Richards, and James R. Larus. Teapot: </author> <title> Language Support for Writing Memory Coherence Protocols. </title> <booktitle> In Proceedings of Conference on Programming Language Design and Implementation (PLDI), </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Thus, software can also support heterogeneous environments much more easily. Most importantly, a software DSM system's protocols can be altered and optimized to the needs of a particular application running on top of it <ref> [21, 9, 13, 5] </ref> without adding much complexity to the DSM system. For example, it would be very difficult to add the Three-Message-Invalidation and Floating-Home-Node optimizations to an existing hardware DSM system.
Reference: [6] <author> Chi-Chao Chang, Grzegorz Czajkowski, Chris Hawblitzel, and Thorsten von Eicken. </author> <title> Low-Latency Communication on the IBM RISC System/6000 SP. </title> <booktitle> To appear in Proceedings of Supercomputing '96, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: This goal was the same reason why the Three-Message-Invalidation and Floating-Home-Node protocols were examined in this thesis. Beng-Hong Lim also ported CRL to the IBM SP/2. However, instead of using MPL for communication, he used an Active Message library written for the IBM SP/2 (SP-2 AM) <ref> [7, 6] </ref> which bypassed MPL and is layered directly on top of the SP/2's network adapter. His measurements, reported in a person communication, were the same or slightly better than the ones obtained in this thesis. There are two reasons why Lim's results were better.
Reference: [7] <author> Chi-Chao Chang, Grzegorz Czajkowski, and Thorsten von Eicken. </author> <title> Design and Performance of Active Messages on the IBM SP-2. </title> <type> Technical Report CS-TR-96-1572, </type> <institution> Cornell University, </institution> <year> 1996. </year>
Reference-contexts: This goal was the same reason why the Three-Message-Invalidation and Floating-Home-Node protocols were examined in this thesis. Beng-Hong Lim also ported CRL to the IBM SP/2. However, instead of using MPL for communication, he used an Active Message library written for the IBM SP/2 (SP-2 AM) <ref> [7, 6] </ref> which bypassed MPL and is layered directly on top of the SP/2's network adapter. His measurements, reported in a person communication, were the same or slightly better than the ones obtained in this thesis. There are two reasons why Lim's results were better.
Reference: [8] <institution> Comparison of MPL and MPI latency and bandwidth. </institution> <note> Available on the World Wide Web at URL http://www.rs6000.ibm.com/software/sp products/performance/switch.html. </note>
Reference-contexts: The SP/2 connects up to 512 RS/6000 processors using IBM's low-latency (39.2 microseconds on a 66MHz processor) and high-bandwidth (35.6 megabytes per second point-to-point on a 66MHz processor) SP Switch <ref> [8] </ref>. The SP/2 implementation of CRL (from now on called CRL-SP/2) uses IBM's Message Passing Library (MPL) [17], which is part of the IBM AIX Parallel Environment [15]. Another message passing library option available on the SP/2 is the Message Passing Interface (MPI) [31]. <p> Another message passing library option available on the SP/2 is the Message Passing Interface (MPI) [31]. However, the performance of MPL is superior to MPI <ref> [8] </ref> as can be seen in appendix Section A.1. Thus MPL is used in lieu of MPI. <p> The sections compare MPL and MPI on the IBM SP/2, display CRL operation latencies, and display CRL and MPL event counts for each application. A.1 IBM Message Passing Library (MPL) versus Message Passing Interface (MPI) This table was obtained from <ref> [8] </ref> and compares the performance of MPL and MPI. Notes: 1. Exchange bandwidth is defined as a simultaneous send and receive data exchange between nodes. 2. Megabytes are defined as 10**6 bytes.
Reference: [9] <author> Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, Ioannis Schoinas, Mark D. Hill, James R. Larus, Anne Rogers, and David A. Wood. </author> <title> Application-Specific Protocols for User-Level Shared Memory. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 380-389, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Thus, software can also support heterogeneous environments much more easily. Most importantly, a software DSM system's protocols can be altered and optimized to the needs of a particular application running on top of it <ref> [21, 9, 13, 5] </ref> without adding much complexity to the DSM system. For example, it would be very difficult to add the Three-Message-Invalidation and Floating-Home-Node optimizations to an existing hardware DSM system. <p> With respect to application specific optimizations, FLASH has a programmable protocol processor for each node in the system. Thus, the Three-Message-Invalidate and the Floating-Home-Node protocol should be relatively easily implementable on FLASH. Falsafi et al. <ref> [9] </ref> describes some application specific protocol optimizations that can be implemented on Blizzard to improve application performance. Barnes-Hut is the only application that is examined in both this thesis and [9]. None of the optimizations mentioned in their research are similar to the Three-Message-Invalidation or the Floating-Home-Node protocol. <p> Thus, the Three-Message-Invalidate and the Floating-Home-Node protocol should be relatively easily implementable on FLASH. Falsafi et al. <ref> [9] </ref> describes some application specific protocol optimizations that can be implemented on Blizzard to improve application performance. Barnes-Hut is the only application that is examined in both this thesis and [9]. None of the optimizations mentioned in their research are similar to the Three-Message-Invalidation or the Floating-Home-Node protocol. These two optimizations complement the work done by Falsafi et al. Much research has been done in Cache-Only Memory Architectures (COMA) [23, 12, 34, 35].
Reference: [10] <author> A. Geist, A. Beguelin, J. J. Dongarra, W. Jiang, R. Manchek, and V. S. Sunderam. </author> <title> PVM 3 User's Guide and Reference Manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: In spite of this fact, message passing environments such as PVM <ref> [36, 10, 32] </ref> and MPI [31] are often the de facto standards for programming multicomputers and networks of workstations.
Reference: [11] <author> Sandeep K. Gupta, Alejandro A. Schaffer, Alan L. Cox, Sandhya Dwarkadas, and Willy Zwae-nepoel. </author> <title> Integrating Parallelization Strategies for Linkage Analysis. </title> <journal> Computers and Biomedical Research, </journal> <volume> 28 </volume> <pages> 116-139, </pages> <year> 1995. </year>
Reference-contexts: The region can then be assigned to that node at program initialization. But there are also many applications for which better performance can be achieved. For instance, in parallel FASTLINK <ref> [11] </ref>, many iterations of a function evaluation are performed. During a single execution, sometimes all of the nodes are working toward computing an iteration, and other times the nodes are split into two groups that are each computing an iteration.
Reference: [12] <author> Erik Hagersten, Anders Landin, and Seif Haridi. </author> <title> DDM a Cache-Only Memory Architecture. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 44-54, </pages> <month> September </month> <year> 1992. </year> <month> 173 </month>
Reference-contexts: None of the optimizations mentioned in their research are similar to the Three-Message-Invalidation or the Floating-Home-Node protocol. These two optimizations complement the work done by Falsafi et al. Much research has been done in Cache-Only Memory Architectures (COMA) <ref> [23, 12, 34, 35] </ref>. In COMAs, each processor contains a portion of the address space, however, the partition is not fixed. Instead, the address space of a processor is like another level of cache.
Reference: [13] <author> Mark D. Hill, James R. Larus, and David A. Wood. </author> <title> Tempest: A Substrate for Portable Parallel Programs. </title> <booktitle> In Proceedings of COMPCON Spring 95, </booktitle> <pages> pages 327-332, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Thus, software can also support heterogeneous environments much more easily. Most importantly, a software DSM system's protocols can be altered and optimized to the needs of a particular application running on top of it <ref> [21, 9, 13, 5] </ref> without adding much complexity to the DSM system. For example, it would be very difficult to add the Three-Message-Invalidation and Floating-Home-Node optimizations to an existing hardware DSM system.
Reference: [14] <institution> IBM SP/2 Home Page. </institution> <note> Available on the World Wide Web at URL http://www.rs6000.ibm.com/. </note>
Reference-contexts: The overflowing cache results in much more communication caused by region flushes and start operation misses. 31 32 Chapter 4 An Implementation of CRL on the SP/2 The RS/6000 Scalable POWERparallel Systems 2 (SP/2) is IBM's general purpose scalable supercomputer <ref> [14] </ref>. The SP/2 connects up to 512 RS/6000 processors using IBM's low-latency (39.2 microseconds on a 66MHz processor) and high-bandwidth (35.6 megabytes per second point-to-point on a 66MHz processor) SP Switch [8].
Reference: [15] <author> International Business Machines. </author> <title> IBM AIX Parallel Environment, Operation and Use, Release 2.0, </title> <month> June </month> <year> 1994. </year>
Reference-contexts: The SP/2 implementation of CRL (from now on called CRL-SP/2) uses IBM's Message Passing Library (MPL) [17], which is part of the IBM AIX Parallel Environment <ref> [15] </ref>. Another message passing library option available on the SP/2 is the Message Passing Interface (MPI) [31]. However, the performance of MPL is superior to MPI [8] as can be seen in appendix Section A.1. Thus MPL is used in lieu of MPI.
Reference: [16] <author> International Business Machines. </author> <title> IBM AIX Parallel Environment, Parallel Programming Subroutine Reference, Release 2.0, </title> <month> June </month> <year> 1994. </year>
Reference-contexts: For a more in-depth description of MPL, consult the IBM AIX Parallel Environment Parallel Programming Subroutine Reference <ref> [16] </ref>.
Reference: [17] <institution> International Business Machines. IBM AIX Parallel Environment, </institution> <note> Programming Primer, Release 2.0, </note> <month> June </month> <year> 1994. </year>
Reference-contexts: The SP/2 connects up to 512 RS/6000 processors using IBM's low-latency (39.2 microseconds on a 66MHz processor) and high-bandwidth (35.6 megabytes per second point-to-point on a 66MHz processor) SP Switch [8]. The SP/2 implementation of CRL (from now on called CRL-SP/2) uses IBM's Message Passing Library (MPL) <ref> [17] </ref>, which is part of the IBM AIX Parallel Environment [15]. Another message passing library option available on the SP/2 is the Message Passing Interface (MPI) [31]. However, the performance of MPL is superior to MPI [8] as can be seen in appendix Section A.1.
Reference: [18] <author> Kirk L. Johnson. </author> <title> High-Performance All-Software Distributed Shared Memory. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Laboratory for Computer Science, </institution> <year> 1995. </year>
Reference-contexts: The Floating-Home-Node protocol provides the ability to change the node responsible for the coherence of a region's data. A similar type of protocol where data migrates is used in COMA machines [35]. The idea of examining such a protocol on CRL appeared in <ref> [18] </ref>. The results in this thesis show that the Floating-Home-Node protocol decreases the execution time of Barnes-Hut by over 50%. 1.2 Background of Distributed Shared Memory This section describes why DSM is interesting and why it is worth exploring the possibilities of improving it. <p> Most of the following text is taken directly from the CRL User Documentation [20]. For more information on CRL, consult <ref> [19, 21, 18] </ref>. 2.1 Description of CRL The C Region Library (CRL) is an all-software distributed shared memory (DSM) system intended for use on message-passing multicomputers and distributed systems. Parallel applications built on top of CRL share data through regions. Each region is an arbitrarily sized, contiguous area of memory. <p> LU, Water, and Barnes are taken from the SPLASH-2 parallel application suite [38] and their descriptions are taken directly from [38]. Some of the text regarding the communication and computation granularity of LU, Water, and Barnes was taken directly from <ref> [18] </ref>. 3.1 Traveling Salesman Problem The Traveling Salesman Problem solves the NP-complete problem of finding the shortest cyclic path going through every node of a graph. In this implementation, one master node creates a job queue of possible solutions, and the rest of the nodes become slaves. <p> This increment policy guarantees that each copy of the data received by a remote node has a unique region version number, and thus remote nodes can detect and appropriately handle out-of-order invalidate messages <ref> [18] </ref>. In CRL-SP/2, a region's version number is not incremented for Shared-Requests handled when other nodes are already sharing that particular region. It follows that if multiple nodes all have valid shared copies of a region, then all nodes have the same version number for the region. <p> Section 5.3.1 contains a discussion on how this change further assists in the implementation of the Three-Message-Invalidate protocol. One concern with this change is whether remote nodes are still able to appropriately handle out-of-order invalidate messages. For a discussion of out-of-order invalidate messages in the original CRL, consult <ref> [18] </ref>. It is important that each node identify which invalidates are old and can be ignored, which invalidates refer to the current version and must be processed, and which invalidates refer to a requested version that has not arrived yet and must be saved. <p> This case is handled without any extra coding though, since this case is identical to what would happen with out-of-order acknowledgment/invalidate messages in the original CRL protocol <ref> [18] </ref>. 5.3.3 Handling Flushes If a flush arrives at a home node during a Three-Message-Invalidation, it is possible that the remote node sent the flush before receiving the invalidate message. In this case, an invalidate acknowledgment must be sent to the requesting node on behalf of the flushing node. <p> Microseconds Table A.4: Preliminary switch performance using MPI (udp/IP, application space to application space) 74 A.2 CRL Operation Latencies Kirk Johnson wrote a simple microbenchmark to measure the cost of various CRL events. A description of the microbenchmark from <ref> [18] </ref> follows: 64 regions are allocated on a selected home node. <p> Table A.5 (taken directly from <ref> [18] </ref>) describes the 26 different types of events measured by the microbenchmark. This microbenchmark is executed twice, once with nonrequesting nodes polling for messages, and once with nonrequesting nodes not polling for messages. <p> The following, with parts taken from <ref> [18] </ref>, explains the contents of each table: The first section of the table indicates how many times rgn_map was called and, of those calls, how many (1) referenced remote regions and (2) were misses. <p> The first section describes the protocol states and events. The second and third sections describe the home- and remote-side protocol state machines, respectively. Since this protocol's base was CRL's original protocol, which was described in <ref> [18] </ref>, annotations and markings appear in the state diagrams and pseudocode to indicate what was changed from this base. Readers who are not interested in this level of detail may find that skimming this material (or skipping it entirely) is more useful than a careful, detailed reading.
Reference: [19] <author> Kirk L. Johnson, Joseph Adler, and Sandeep K. Gupta. </author> <title> CRL 1.0 Software Distribution, </title> <month> August </month> <year> 1995. </year> <note> Available on the World Wide Web at URL http://www.pdos.lcs.mit.edu/crl/. </note>
Reference-contexts: Most of the following text is taken directly from the CRL User Documentation [20]. For more information on CRL, consult <ref> [19, 21, 18] </ref>. 2.1 Description of CRL The C Region Library (CRL) is an all-software distributed shared memory (DSM) system intended for use on message-passing multicomputers and distributed systems. Parallel applications built on top of CRL share data through regions. Each region is an arbitrarily sized, contiguous area of memory.
Reference: [20] <author> Kirk L. Johnson, Joseph Adler, and Sandeep K. Gupta. </author> <title> CRL version 1.0 User Documentation, </title> <month> August </month> <year> 1995. </year> <note> Available on the World Wide Web at URL http://www.pdos.lcs.mit.edu/crl/. </note>
Reference-contexts: Appendix C describes the Floating-Home-Node protocol implementation in great detail, including state diagrams and pseudocode. 23 24 Chapter 2 CRL Overview This chapter provides a brief overview of CRL, the C Region Library. Most of the following text is taken directly from the CRL User Documentation <ref> [20] </ref>. For more information on CRL, consult [19, 21, 18]. 2.1 Description of CRL The C Region Library (CRL) is an all-software distributed shared memory (DSM) system intended for use on message-passing multicomputers and distributed systems. Parallel applications built on top of CRL share data through regions.
Reference: [21] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the Fifteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 213-228, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: In addition, it discusses the design, implementation, and evaluation of two optimizations to the original CRL protocol, a Three-Message-Invalidation protocol [26] and a Floating-Home-Node protocol [35]. 1.1 Contributions of This Thesis The C Region Library (CRL) <ref> [21] </ref> is an all-software region-based DSM system that provides applications with the ability to specialize the communication protocols used for synchronization, and transfer of shared regions. 19 This thesis has a number of contributions. The first is a design, implementation, and evaluation of CRL version 1.0 for the IBM SP/2. <p> It is essentially an abstraction of a global shared memory. DSM implementations that are mostly software are implemented over some message passing mechanism <ref> [3, 21, 22, 33] </ref> while ones that are mostly hardware are usually implemented by using a fixed mapping from global address to node number, and then storing the information at that node [2, 25]. <p> The foremost reason is that hardware is HARDware; it is not malleable to the specific needs of an application, and cannot be ported. One can port software DSMs to newer platforms quickly <ref> [21] </ref>. And once the initial development costs have been paid, the incremental costs for updates and porting of a software DSM system is minimal. <p> Thus, software can also support heterogeneous environments much more easily. Most importantly, a software DSM system's protocols can be altered and optimized to the needs of a particular application running on top of it <ref> [21, 9, 13, 5] </ref> without adding much complexity to the DSM system. For example, it would be very difficult to add the Three-Message-Invalidation and Floating-Home-Node optimizations to an existing hardware DSM system. <p> Most of the following text is taken directly from the CRL User Documentation [20]. For more information on CRL, consult <ref> [19, 21, 18] </ref>. 2.1 Description of CRL The C Region Library (CRL) is an all-software distributed shared memory (DSM) system intended for use on message-passing multicomputers and distributed systems. Parallel applications built on top of CRL share data through regions. Each region is an arbitrarily sized, contiguous area of memory.
Reference: [22] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: It is essentially an abstraction of a global shared memory. DSM implementations that are mostly software are implemented over some message passing mechanism <ref> [3, 21, 22, 33] </ref> while ones that are mostly hardware are usually implemented by using a fixed mapping from global address to node number, and then storing the information at that node [2, 25]. <p> The most difficult 21 part about the installation is actually finding source or object files for the DSM system. And then, after finding the source, it may have to be ported to the appropriate network, CPU hardware, and OS combination. TreadMarks <ref> [22] </ref> is one example of a DSM system that was written on standard UNIX systems, such as SunOS and Ultrix, at user-level. Because it is at user-level, TreadMarks does not require any modifications to the operating system. TreadMarks has a few differences though from CRL. <p> This section focuses on those research projects that have similarities with topics in this thesis. TreadMarks <ref> [22] </ref> is a mostly-software DSM system available on many standard UNIX operating systems including SunOS, Ultrix, and AIX.
Reference: [23] <institution> Kendall Square Research. </institution> <type> KSR-1 Technical Summary, </type> <year> 1992. </year>
Reference-contexts: None of the optimizations mentioned in their research are similar to the Three-Message-Invalidation or the Floating-Home-Node protocol. These two optimizations complement the work done by Falsafi et al. Much research has been done in Cache-Only Memory Architectures (COMA) <ref> [23, 12, 34, 35] </ref>. In COMAs, each processor contains a portion of the address space, however, the partition is not fixed. Instead, the address space of a processor is like another level of cache. <p> In reality, some COMA machines do have home nodes for data items, but these nodes are only responsible for keeping a valid copy of the data if no other node has a valid copy in its cache. The Kendall Square Research KSR1 system <ref> [23, 34] </ref> is an example of a COMA machine. The KSR1 processors are grouped into a hierarchy. 32 processing nodes are grouped together into a ring called Ring:0. 32 Ring:0s grouped together make a Ring:1, and 32 Ring:1s grouped together make a 68 Ring:2.
Reference: [24] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Ghar-achorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: It was determined that Request Forwarding could reduced the average cache miss latency by 12% for real applications on their hardware platform. This thesis showed that Three-Message-Invalidation could also significantly reduce cache miss latencies and overall application execution time in an all-software DSM system. The Stanford FLASH multiprocessor <ref> [24] </ref> was created after the Stanford DASH machine. [24] states that the main difference between DASH's and FLASH's protocols is that in DASH each cluster collects its own invalidation acknowledgments, whereas in FLASH invalidation acknowledgments are collected at the home node, that is, the node where the directory data is stored <p> This thesis showed that Three-Message-Invalidation could also significantly reduce cache miss latencies and overall application execution time in an all-software DSM system. The Stanford FLASH multiprocessor <ref> [24] </ref> was created after the Stanford DASH machine. [24] states that the main difference between DASH's and FLASH's protocols is that in DASH each cluster collects its own invalidation acknowledgments, whereas in FLASH invalidation acknowledgments are collected at the home node, that is, the node where the directory data is stored for that block.
Reference: [25] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: DSM implementations that are mostly software are implemented over some message passing mechanism [3, 21, 22, 33] while ones that are mostly hardware are usually implemented by using a fixed mapping from global address to node number, and then storing the information at that node <ref> [2, 25] </ref>. In DSM systems, applications are not required to keep track of which processor has a valid copy of the data, where the data is located locally in memory, and which processors are allowed to read from/write to the data. <p> The cost of handling an interrupt (130s) was much greater than the cost of an unsuccessful Active Message poll (1.3s). Second, SP-2 AM reduced the latency of small messages by 40%. The Stanford DASH machine <ref> [25, 26] </ref> is a shared-memory multiprocessor with hardware cache coherence. The coherence protocol has an optimization identical to the Three-Message-Invalidation called Request Forwarding. This optimization forwards remote requests from the home cluster to a third cluster that currently caches the requested memory block.
Reference: [26] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH Prototype: Logic Overhead and Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 41-61, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: This thesis discusses the design, implementation, and evaluation of a C Region Library port to the IBM SP/2. In addition, it discusses the design, implementation, and evaluation of two optimizations to the original CRL protocol, a Three-Message-Invalidation protocol <ref> [26] </ref> and a Floating-Home-Node protocol [35]. 1.1 Contributions of This Thesis The C Region Library (CRL) [21] is an all-software region-based DSM system that provides applications with the ability to specialize the communication protocols used for synchronization, and transfer of shared regions. 19 This thesis has a number of contributions. <p> Optimizations, such as the two that follow, can significantly improve application running times on the SP/2 implementation of CRL (CRL-SP/2). The second contribution is an improvement in the performance of CRL-SP/2 by designing and implementing a Three-Message-Invalidation protocol, which is a well-known optimization and enhancement <ref> [26] </ref>. Evaluation of the Three-Message-Invalidation protocol shows that it reduces the number of messages in an invalidation. This enhancement reduces the number of messages sent by up to 25% and thus improves application running times significantly on CRL-SP/2. <p> The cost of handling an interrupt (130s) was much greater than the cost of an unsuccessful Active Message poll (1.3s). Second, SP-2 AM reduced the latency of small messages by 40%. The Stanford DASH machine <ref> [25, 26] </ref> is a shared-memory multiprocessor with hardware cache coherence. The coherence protocol has an optimization identical to the Three-Message-Invalidation called Request Forwarding. This optimization forwards remote requests from the home cluster to a third cluster that currently caches the requested memory block.
Reference: [27] <author> Kevin Lew. </author> <title> A Case Study of Shared Memory and Message Passing: The Triangle Puzzle. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <year> 1995. </year>
Reference-contexts: There are some cases when DSM performs better than message passing though. For example in Kevin Lew's master's thesis <ref> [27] </ref>, for a particular problem size where there was little memory contention, DSM performed better than message passing on the Alewife machine [1].
Reference: [28] <author> Kevin Lew, Kirk Johnson, and Frans Kaashoek. </author> <title> A Case Study of Shared-Memory and Message-Passing Implementations of Parallel Breadth-First Search: The Triangle Problem. </title> <booktitle> In Proceedings of the Third DIMACS International Algorithm Implementation Challenge Workshop, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: In papers comparing the execution time of message passing programs to that of DSM programs, message passing programs usually perform better <ref> [30, 28, 4] </ref>.
Reference: [29] <author> Kai Li. IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing. </title> <booktitle> In Proceedings of the International Conference on Parallel Computing, </booktitle> <pages> pages 94-101, </pages> <year> 1988. </year>
Reference-contexts: During the past ten years, a lot of research has been performed to find ways to improve the speedup of parallel programs, and to improve their ease of use <ref> [29] </ref>. In particular, DSM systems and many of the tradeoffs involved in their implementations have been actively examined. This thesis discusses the design, implementation, and evaluation of a C Region Library port to the IBM SP/2.
Reference: [30] <author> Honghui Lu, Sandhya Dwarkadas, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Message Passing Versus Distributed Shared Memory on Networks of Workstations. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year> <month> 174 </month>
Reference-contexts: In papers comparing the execution time of message passing programs to that of DSM programs, message passing programs usually perform better <ref> [30, 28, 4] </ref>.
Reference: [31] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard. Interna--tional Journal of Supercomputer Applications and High Performance Computing, </title> <address> 8(3-4):169-416, </address> <month> fall/winter </month> <year> 1994. </year>
Reference-contexts: The availability of message passing libraries that are available free of charge for a wide range of operating systems and hardware platforms is much greater than the availability of such DSM libraries. For example, PVM [36] and MPI <ref> [31] </ref> are two message passing libraries that are in wide spread use on various operating systems and hardware platforms. PVM itself has thousands of users [32]. <p> In spite of this fact, message passing environments such as PVM [36, 10, 32] and MPI <ref> [31] </ref> are often the de facto standards for programming multicomputers and networks of workstations. We believe that this is primarily due to the fact that these systems require no special hardware, compiler, or operating system support, thus enabling them to run entirely at user level on unmodified, "stock" systems. <p> The SP/2 implementation of CRL (from now on called CRL-SP/2) uses IBM's Message Passing Library (MPL) [17], which is part of the IBM AIX Parallel Environment [15]. Another message passing library option available on the SP/2 is the Message Passing Interface (MPI) <ref> [31] </ref>. However, the performance of MPL is superior to MPI [8] as can be seen in appendix Section A.1. Thus MPL is used in lieu of MPI.
Reference: [32] <institution> PVM Home Page. </institution> <note> Available on the World Wide Web at URL http://www.epm.ornl.gov/pvm/. </note>
Reference-contexts: For example, PVM [36] and MPI [31] are two message passing libraries that are in wide spread use on various operating systems and hardware platforms. PVM itself has thousands of users <ref> [32] </ref>. Also, PVM applications executing on different platforms can transfer messages to each other, and any necessary data conversion involving byte order or data-type size is performed transparently to the PVM user. So PVM is extremely useful and easy to use in a heterogeneous environment. <p> In spite of this fact, message passing environments such as PVM <ref> [36, 10, 32] </ref> and MPI [31] are often the de facto standards for programming multicomputers and networks of workstations.
Reference: [33] <author> Steve K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: It is essentially an abstraction of a global shared memory. DSM implementations that are mostly software are implemented over some message passing mechanism <ref> [3, 21, 22, 33] </ref> while ones that are mostly hardware are usually implemented by using a fixed mapping from global address to node number, and then storing the information at that node [2, 25].
Reference: [34] <author> E. Rosti, E. Smirni, T.D. Wagner, A.W. Apon, and L.W. Dowdy. </author> <title> The KSR1: Experimentation and Modeling of Poststore. </title> <booktitle> In Proceedings of ACM SIGMETRICS '93, </booktitle> <pages> pages 74-85, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: None of the optimizations mentioned in their research are similar to the Three-Message-Invalidation or the Floating-Home-Node protocol. These two optimizations complement the work done by Falsafi et al. Much research has been done in Cache-Only Memory Architectures (COMA) <ref> [23, 12, 34, 35] </ref>. In COMAs, each processor contains a portion of the address space, however, the partition is not fixed. Instead, the address space of a processor is like another level of cache. <p> In reality, some COMA machines do have home nodes for data items, but these nodes are only responsible for keeping a valid copy of the data if no other node has a valid copy in its cache. The Kendall Square Research KSR1 system <ref> [23, 34] </ref> is an example of a COMA machine. The KSR1 processors are grouped into a hierarchy. 32 processing nodes are grouped together into a ring called Ring:0. 32 Ring:0s grouped together make a Ring:1, and 32 Ring:1s grouped together make a 68 Ring:2.
Reference: [35] <author> Ashley Saulsbury, Tim Wilkinson, John Carter, and Anders Landin. </author> <title> An Argument for Simple COMA. </title> <booktitle> In Proceedings of the First Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 276-285, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: This thesis discusses the design, implementation, and evaluation of a C Region Library port to the IBM SP/2. In addition, it discusses the design, implementation, and evaluation of two optimizations to the original CRL protocol, a Three-Message-Invalidation protocol [26] and a Floating-Home-Node protocol <ref> [35] </ref>. 1.1 Contributions of This Thesis The C Region Library (CRL) [21] is an all-software region-based DSM system that provides applications with the ability to specialize the communication protocols used for synchronization, and transfer of shared regions. 19 This thesis has a number of contributions. <p> The Floating-Home-Node protocol provides the ability to change the node responsible for the coherence of a region's data. A similar type of protocol where data migrates is used in COMA machines <ref> [35] </ref>. The idea of examining such a protocol on CRL appeared in [18]. <p> None of the optimizations mentioned in their research are similar to the Three-Message-Invalidation or the Floating-Home-Node protocol. These two optimizations complement the work done by Falsafi et al. Much research has been done in Cache-Only Memory Architectures (COMA) <ref> [23, 12, 34, 35] </ref>. In COMAs, each processor contains a portion of the address space, however, the partition is not fixed. Instead, the address space of a processor is like another level of cache.
Reference: [36] <author> Vaidy Sunderam, Al Geist, Jack Dongarra, and Robert Mancheck. </author> <title> The PVM Concurrent Computing System: Evolution, Experiences, and Trends. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 531-545. </pages>
Reference-contexts: The availability of message passing libraries that are available free of charge for a wide range of operating systems and hardware platforms is much greater than the availability of such DSM libraries. For example, PVM <ref> [36] </ref> and MPI [31] are two message passing libraries that are in wide spread use on various operating systems and hardware platforms. PVM itself has thousands of users [32]. <p> In spite of this fact, message passing environments such as PVM <ref> [36, 10, 32] </ref> and MPI [31] are often the de facto standards for programming multicomputers and networks of workstations.
Reference: [37] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Only having the message identifier makes the job of the message handler more difficult since the message handler must somehow determine where the message buffer is stored, based solely on the message identifier. Many message passing systems, including Berkeley's Active Message Library <ref> [37] </ref>, provide for a user-settable one word argument to the message handler. <p> Instead, it responds indirectly via other remote nodes, so the home node also sends region information (such as the version number) via this indirect path. For portability, especially with Active-Messages <ref> [37] </ref>, each protocol message is limited to a size of five words. Three words are needed to specify the remote message handler, the region, and the message type (such as Shared Request). The remaining two words are used as arguments for the message type's handler.
Reference: [38] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The applications are the Traveling Salesman Problem (TSP), Blocked LU (LU), Water-Nsquared (Water), and Barnes-Hut (Barnes). LU, Water, and Barnes are taken from the SPLASH-2 parallel application suite <ref> [38] </ref> and their descriptions are taken directly from [38]. <p> The applications are the Traveling Salesman Problem (TSP), Blocked LU (LU), Water-Nsquared (Water), and Barnes-Hut (Barnes). LU, Water, and Barnes are taken from the SPLASH-2 parallel application suite <ref> [38] </ref> and their descriptions are taken directly from [38]. Some of the text regarding the communication and computation granularity of LU, Water, and Barnes was taken directly from [18]. 3.1 Traveling Salesman Problem The Traveling Salesman Problem solves the NP-complete problem of finding the shortest cyclic path going through every node of a graph. <p> Instead, receive buffers for synchronization operations are setup with mpc recv to prevent an interrupt from occurring when synchronization messages arrive. This change results in significant decreases in execution time. In particular, the execution time of the water application <ref> [38] </ref> decreases by 15%. 40 4.3.2 Two Message Data Transfer Because of the SP/2's very expensive interrupt overhead (130 microseconds) and high message latency (55 microseconds), each message involved in data transfers dramatically increases the response time for the data request.
References-found: 38

