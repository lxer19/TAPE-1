URL: http://www.speech.sri.com/people/victor/papers/srs.92.cohen.ps
Refering-URL: http://www.speech.sri.com/people/victor/publications.html
Root-URL: 
Title: sMultiple-State Context-Dependent Phonetic Modeling with MLP average reductions in word error rates of 20% in
Author: Michael Cohen*, Horacio Franco*, Nelson Morgan**, David Rumelhart***, and Victor Abrash* 
Keyword: P  
Address: Menlo Park, CA 94025  1947 Center Street, Suite 600, Berkeley, CA 9470  Stanford, CA 94305  
Affiliation: Speech Research Program, SRI International,  Intl. Computer Science Inst.,  Stanford University, Dept. of Psychology,  E  
Note: Proceedings of Speech Research Symposium XII,  d  
Phone: 4**  
Date: June, 1992.  Abstract  
Abstract: arlier hybrid multilayer perceptron (MLP)/hidden Markov model (HMM) continuous speech recognition sys-r tems have not modeled context-dependent phonetic effects, sequences of distributions for phonetic models, o ender-based speech consistencies. In this paper we present a new MLP architecture and training procedure for t modeling context-dependent phonetic classes with a sequence of distributions. A new training procedure tha smooths" networks with different degrees of context-dependence is proposed in order to obtain a robust esti p mate of the context-dependent probabilities. We have used this new architecture to model generalized biphone honetic contexts. Tests with the speaker-independent DARPA Resource Management database have shown ith our earlier context-independent MLP/HMM hybrid. 
Abstract-found: 1
Intro-found: 1
Reference: <author> 1] H. Bourlard, and N. Morgan, </author> <title> ``Merging Multilayer Perceptrons and Hidden Markov Models: Some Experi , ments in Continuous Speech Recognition,'' </title> <editor> in E. Gelenbe, Ed., </editor> <booktitle> Neural Networks: Advances and Applications msterdam, </booktitle> <publisher> North Holland Press, </publisher> <year> 1990. </year>
Reference: [2] <author> N. Morgan and H. Bourlard, </author> <title> ``Continuous Speech Recognition Using Multilayer Perceptrons with Hidden [ Markov Models,'' </title> <booktitle> ICASSP 90, </booktitle> <pages> pp. 413-416, </pages> <address> Alburquerque, New Mexico, </address> <year> 1990. </year> <note> 3] H. </note> <author> Murveit, M. Cohen, P. Price, G. Baldwin, M. Weintraub, and J. Bernstein, </author> <title> ``SRI's DECIPHER System,'' </title> <booktitle> DARPA Speech and Natural Language Workshop, </booktitle> <month> February </month> <year> 1989. </year> - <title> 9 - S </title>
Reference-contexts: 1. Introduction revious work by Morgan, Bourlard, et al. <ref> [1, 2] </ref> has shown both theoretically and practically that multilayer s perceptrons (MLPs) can be successfully used in a hidden Markov model (HMM) continuous speech recognition ystem for estimation of the state dependent observation probabilities. <p> Many contexts will be poorly represented in training databases of realistic size, which can lead o models that lack robustness. <ref> [2] </ref> A straightforward extension of our original MLP to the modeling of multiple distributions for phonetic s classes leads to discriminative training between subphonetic classes (corresponding to HMM states).
Reference: [4] <author> S. Renals, N. Morgan, M. Cohen, H. Franco, </author> <title> ``Connectionist Probability Estimation in the DECIPHE peech Recognition System,'' </title> <booktitle> ICASSP 92, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 601-604, </pages> <address> San Francisco, 1992. d </address>
Reference-contexts: Recently, this approach was applied to a tate-of-the-art speech recognition system (the SRI-DECIPHER system [3]) in which an MLP provided estimates - of context-independent posterior probabilities of phone classes, which were then converted to HMM context ndependent state observation likelihoods using Bayes' rule <ref> [4] </ref>. The best recognition performance on the M DARPA Resource Management database with the DECIPHER system has been achieved using a hybrid LP/HMM that employs a weighted mixture of state observation likelihoods provided by the MLP and the e HMM. <p> Hybrid MLP/HMM The baseline MLP/HMM DECIPHER hybrid (described by Renals et al. <ref> [4] </ref>) substitutes (scaled) probability esti-t mates computed with MLPs for the tied mixture HMM state-dependent observation probability densities. The opology of the HMM system is kept unchanged. - 3 - The hybrid system is bootstrapped from the basic HMM DECIPHER system [3] already trained using the e forward-backward maximum-likelihood method. <p> Frame classification on an . independent cross-validation set is used to control the learning rate and to decide when to stop training as in <ref> [4] </ref> he initial learning rate is kept constant until cross-validation performance increases less than 0.5%, after which hhh until performance increases no further. 1 it is reduced as 2 n The network architecture consists of an input layer of 234 units, spanning 9 frames of cepstra, delta ceps - tra, energy, <p> Context-dependent training and smoothing :In order to achieve robust training of context-specific nets, we use the following method Initially, a context-independent MLP is trained as in <ref> [4] </ref> to estimate the context-independent posterior pro u babilities over the N phone classes. After the context-independent training converges, the resulting weights are sed to initialize the weights of the context-specific nets. <p> Otherwise, the training procedure is similar to that for the context-l independent net, using stochastic gradient descent and a relative entropy training criterion. The overal lassification performance evaluated on an independent cross-validation set is used to determine the learning rate , as in <ref> [4] </ref>. Training stops when overall cross-validation performance does not improve any further. In this phase e are actually training a set of M independent nets, each one trained on a nonoverlaping subset of the original training data.

Reference: [10] <author> H. Bourlard, N. Morgan, C. Wooters, S Renals, </author> <title> ``CDNN: A Context Dependent Neural Network for Con [ tinuous Speech Recognition,'' </title> <booktitle> ICASSP 92, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 349-352, </pages> <address> San Francisco, </address> <year> 1992. </year> <note> 11] S. </note> <author> E. Levinson, L. R. Rabiner, and M.M. Sondhi, </author> <title> ``An introduction to the application of the theory of pro - babilistic functions of a Markov process to automatic speech recognition,'' </title> <journal> Bell Syst. Tech. Journal 62, </journal> <volume> 1035 074, </volume> <year> 1983. </year>
Reference-contexts: An earlier approach to context-dependent phonetic modeling with MLPs was proposed by Bourlard et al. s <ref> [10] </ref>. It is based on factoring the context-dependent likelihood and uses a set of binary inputs to the network to pecify context classes. The number of parameters and the computational load using this approach are not much greater than those for the original context-independent net.
References-found: 4

