URL: http://wwwipd.ira.uka.de/~prechelt/Biblio/iconip96.ps.gz
Refering-URL: 
Root-URL: 
Email: (prechelt@ira.uka.de)  
Title: Comparing Adaptive and Non-Adaptive Connection Pruning With Pure Early Stopping  
Author: Lutz Prechelt 
Address: D-76128 Karlsruhe, Germany  
Affiliation: Fakultat fur Informatik Universitat Karlsruhe  
Abstract: Neural network pruning methods on the level of individual network parameters (e.g. connection weights) can improve generalization, as is shown in this empirical study. However, an open problem in the pruning methods known today (OBD, OBS, autoprune, epsiprune) is the selection of the number of parameters to be removed in each pruning step (pruning strength). This work presents a pruning method lprune that automatically adapts the pruning strength to the evolution of weights and loss of generalization during training. The method requires no algorithm parameter adjustment by the user. Results of statistical significance tests comparing autoprune, lprune, and static networks with early stopping are given, based on extensive experimentation with 14 different problems. The results indicate that training with pruning is often significantly better and rarely significantly worse than training with early stopping without pruning. Furthermore, lprune is often superior to autoprune (which is superior to OBD) on diagnosis tasks unless severe pruning early in the training process is required. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Yann Le Cun, John S. Denker, and Sara A. Solla. </author> <title> Optimal brain damage. </title> <booktitle> In [10], </booktitle> <pages> pages 598-605, </pages> <year> 1990. </year>
Reference-contexts: Several such methods have been suggested. The simplest one | with obvious flaws [3] | is to assume the importance to be proportional to the magnitude of a weight. More sophisticated approaches are the well-known optimal brain damage (OBD) and optimal brain surgeon (OBS) methods. OBD <ref> [1] </ref> uses an approximation to the second derivative of the error with respect to each weight to determine the saliency of the removal of that weight. Low saliency means low importance of a weight.
Reference: [2] <author> Scott E. Fahlman. </author> <title> An empirical study of learning speed in back-propagation networks. </title> <type> Technical Report CMU-CS-88-162, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: RPROP is a fast backpropagation variant that is about as fast as quickprop <ref> [2] </ref> but more robust in the choice of parameters. Note that RPROP requires a modification in the way the T (w i ) are computed, because the weight change is not proportional to @E=@w i .
Reference: [3] <author> William Finnoff, Ferdinand Hergert, and Hans Georg Zimmermann. </author> <title> Improving model selection by nonconvergent methods. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 771-783, </pages> <year> 1993. </year>
Reference-contexts: The present paper makes up for that. 1.1 Related Work: Some Known Pruning Methods The key to pruning is a method to calculate the approximate importance of each parameter. Several such methods have been suggested. The simplest one | with obvious flaws <ref> [3] </ref> | is to assume the importance to be proportional to the magnitude of a weight. More sophisticated approaches are the well-known optimal brain damage (OBD) and optimal brain surgeon (OBS) methods. <p> Both methods have the disadvantage of requiring training to the error minimum before pruning may occur. For many problems, this introduces massive overfitting which often cannot be repaired by subsequent pruning. The autoprune method <ref> [3] </ref> avoids this problem. <p> A large value of T indicates high importance of the connection with weight w i . Connections with small T can be pruned. <ref> [3] </ref> have convincingly shown autoprune to be superior to OBD. Note that many more pruning methods than discussed here have been proposed in the literature. <p> Significantly lower pruning strengths could avoid this, but would exhibit another problem: namely that overfitting cannot be reduced as fast as it builds up. Therefore, pruning with very small pruning strength and fixed schedule would probably be similar to OBD, which has been shown inferior to autoprune by <ref> [3] </ref>.
Reference: [4] <editor> Stephen J. Hanson, Jack D. Cowan, and C. Lee Giles, editors. </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: [5] <author> Babak Hassibi and David G. Stork. </author> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <booktitle> In [4], </booktitle> <pages> pages 164-171, </pages> <year> 1993. </year>
Reference-contexts: OBD [1] uses an approximation to the second derivative of the error with respect to each weight to determine the saliency of the removal of that weight. Low saliency means low importance of a weight. OBS <ref> [5] </ref> avoids the drawbacks of the approximation by computing the second derivatives (almost) exactly, but is computationally very expensive. Both methods have the disadvantage of requiring training to the error minimum before pruning may occur. For many problems, this introduces massive overfitting which often cannot be repaired by subsequent pruning.
Reference: [6] <editor> Richard P. Lippmann, John E. Moody, and David S. Touretzky, editors. </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: [7] <author> Lutz Prechelt. </author> <title> PROBEN1 | A set of benchmarks and benchmarking rules for neural network training algorithms. </title> <type> Technical Report 21/94, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, Germany, </institution> <month> September </month> <year> 1994. </year> <note> Anonymous FTP: /pub/papers/techreports/1994/1994-21.ps.gz on ftp.ira.uka.de. </note>
Reference-contexts: The result of the training is the network that exhibited the lowest validation error E opt . 3 Results And Discussion 3.1 Experiment Setup Extensive benchmark comparisons were made between autoprune, lprune, and static backpropagation with early stopping. 14 different problems were used, all from the Proben1 benchmark set <ref> [7] </ref>, a collection of diagnosis problems 1 . <p> Each of these datasets was trained with two different initial topologies. The first is the dataset's standard architecture network topology (see <ref> [7] </ref>; in that reference, the standard architectures are called pivot architectures.), which can be considered a "reasonable" topology for the dataset.
Reference: [8] <author> Lutz Prechelt. </author> <title> A study of experimental evaluations of neural network learning algorithms: Current research practice. </title> <type> Technical Report 19/94, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, Germany, </institution> <month> August </month> <year> 1994. </year> <note> Anonymous FTP: /pub/papers/techreports/1994/1994-19.ps.gz on ftp.ira.uka.de. </note>
Reference-contexts: As the very different results for the various problems and even for the dataset permutations show, benchmarking has to be extensive and careful in order to yield significant and correct results | this is in sharp contrast to the state of the practice as described in <ref> [8] </ref>.
Reference: [9] <author> Martin Riedmiller and Heinrich Braun. </author> <title> A direct adaptive method for faster backpropagation learning: The RPROP algorithm. </title> <booktitle> In Proc. of the IEEE Intl. Conf. on Neural Networks, </booktitle> <pages> pages 586-591, </pages> <address> San Francisco, CA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: All runs were done using the RPROP weight update rule <ref> [9] </ref>, squared error function, and the RPROP parameters + = 1:2, = 0:5, 0 2 [0:05 : : :0:2] randomly per weight, max = 50, min = 0, initial weights from [-0.1: : :0.1] randomly.
Reference: [10] <editor> David S. Touretzky, editor. </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: [11] <author> Peter M. Williams. </author> <title> Bayesian regularization and pruning using a Laplace prior. </title> <type> Technical Report CSRP-312, </type> <institution> School of Cognitive and Computing Sciences, University of Sussex, </institution> <address> Brighton, England, </address> <month> February </month> <year> 1994. </year> <month> ftp://ftp.cogs.susx.ac.uk/pub/reports/csrp/csrp312.ps.Z. </month>
Reference-contexts: Connections with small T can be pruned. [3] have convincingly shown autoprune to be superior to OBD. Note that many more pruning methods than discussed here have been proposed in the literature. In particular, Bayesian methods can unify the notions of regularization and pruning <ref> [11] </ref>. 1.2 An Open Problem: How Much To Prune? Given the importance T of each weight at any time during training, two questions remain to be answered: 1. When should we prune? 2.
References-found: 11

