URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1991/tr-91-032.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1991.html
Root-URL: http://www.icsi.berkeley.edu
Title: GAL: Networks that grow when they learn and shrink when they forget  
Author: Ethem Alpaydn 
Keyword: Incremental learning, supervised learning, classification, pruning, destructive methods, growth, constructive methods, nearest neighbor.  
Address: Street Suite 600, Berkeley CA 94704-1105 USA.  
Note: For contact, write to E. Alpaydn, ICSI, 1947  
Affiliation: International Computer Science Institute  Center  
Email: Email: ethem@icsi.berkeley.edu  
Date: May 1991  
Abstract: Learning when limited to modification of some parameters has a limited scope; the capability to modify the system structure is also needed to get a wider range of the learnable. In the case of artificial neural networks, learning by iterative adjustment of synaptic weights can only succeed if the network designer predefines an appropriate network structure, i.e., number of hidden layers, units, and the size and shape of their receptive and projective fields. This paper advocates the view that the network structure should not, as usually done, be determined by trial-and-error but should be computed by the learning algorithm. Incremental learning algorithms can modify the network structure by addition and/or removal of units and/or links. A survey of current connectionist literature is given on this line of thought. "Grow and Learn" (GAL) is a new algorithm that learns an association at one-shot due to being incremental and using a local representation. During the so-called "sleep" phase, units that were previously stored but which are no longer necessary due to recent modifications are removed to minimize network complexity. The incrementally constructed network can later be finetuned off-line to improve performance. Another method proposed that greatly increases recognition accuracy is to train a number of networks and vote over their responses. The algorithm and its variants are tested on recognition of handwritten numerals and seem promising especially in terms of learning speed. This makes the algorithm attractive for on-line learning tasks, e.g., in robotics. The biological plausibility of incremental learning is also discussed briefly. Earlier part of this work was realized at the Laboratoire de Microinformatique of Ecole Polytechnique Federale de Lausanne and was supported by the Fonds National Suisse de la Recherche Scientifique. Later part was realized at and supported by the International Computer Science Institute. A number of people helped by guiding, stimulating discussions or questions: Subutai Ahmad, Peter Clarke, Jerry Feldman, Christian Jutten, Pierre Marchal, Jean Daniel Nicoud, Steve Omohondro and Leon Personnaz. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alpaydn, E. </author> <title> (1988) "Grow and Learn" Internal Note, </title> <address> Lai-EPF Lausanne, Switzerland. </address>
Reference-contexts: If one starts with a large network and if the problem in fact requires a simpler network, one likes to have the weights of all unnecessary connections and the output of all unnecessary units equal to zero. There are two approaches in achieving this: <ref> [1] </ref> One may explicitly try to compute how important is the existence of a connection/unit in keeping the error low after the network has been trained and a number of the least important may then be deleted. The remaining network needs to continue to be trained. <p> One needs a certain mechanism whereby addition of a new unit improves success instead of corrupting the harmony as one would normally expect. There are two possibilities: <ref> [1] </ref> If one can make sure that when the new unit gets activated, none of the ancient units get activated, there will be no problem. The units should thus somehow be able to suppress other units when they get control. <p> In GAL the only way to modify the network is by adding a new exemplar and once an exemplar is added, the only way it is modified afterwards is by being removed during "sleep" if necessary. Other than those, the connection weights are not modified. The advantages are: <ref> [1] </ref> Learning is very fast as network modification is very simple. [2] The precision required to store the connection weight values need not be more precise than that is required to store the input pattern. 2 2 For example, when input vectors are binary, binary weights and Hamming distance measure may <p> network modification is very simple. [2] The precision required to store the connection weight values need not be more precise than that is required to store the input pattern. 2 2 For example, when input vectors are binary, binary weights and Hamming distance measure may be 18 The disadvantages are: <ref> [1] </ref> The method is not immune to noise. <p> Although all give 100% on the training set, they may lead to different success percentages with the test set. Instead of using one GAL network, one may use a number of these and decide based on their responses: <ref> [1] </ref> One possibility is to count how many of the networks give a certain class code as output and set response r equal to the class having the maximum count.
Reference: [2] <author> Alpaydn, E. </author> <title> (1990a) Neural models of incremental supervised and unsupervised learning, </title> <type> PhD dissertation, </type> <institution> Ecole Polytechnique Federale de Lausanne, Switzerland. </institution>
Reference-contexts: From "broad" networks with few layers and many units on each layer, after training, they trim as many units as possible and by adding extra layers, generate "long narrow" networks with many layers but few units on each layer; they discover however that networks of the latter type generalize poorly. <ref> [2] </ref> Instead of approximating how much the error will change if the unit/connection is eliminated, one may also modify the learning algorithm so that after training, the unnecessary connec tions/units will have zero weight/output. * One may build a tendency in the learning algorithm to have those weights that are not <p> It learns at one-shot but orthogonality of patterns is no longer required. <ref> [2] </ref> Another possibility is to divide the network into separately trained subnetworks where such subnetworks can be added in an incremental manner. <p> Other than those, the connection weights are not modified. The advantages are: [1] Learning is very fast as network modification is very simple. <ref> [2] </ref> The precision required to store the connection weight values need not be more precise than that is required to store the input pattern. 2 2 For example, when input vectors are binary, binary weights and Hamming distance measure may be 18 The disadvantages are: [1] The method is not immune <p> If there is considerable amount of noise in input patterns or if there is teacher noise, this causes a big degradation in the response of the network. <ref> [2] </ref> Even if there is no noise, the piecewise boundaries found by GAL may be rather different from boundaries in the Bayesian sense as densities are not taken into account. <p> N et pc = 1; if the p th net gives class c as output; 0; otherwise. 8c; R c = p R r = max (R c ) : (4) <ref> [2] </ref> One may use a more sophisticated voting scheme by weighting the response of each network by its "certainty" that its response is correct, computed in the same way as in the case of GAL with reject.
Reference: [3] <author> Alpaydn, E. </author> <title> (1990b) "Grow and Learn: An incremental method for category learning" Int. </title> <booktitle> Neural Network Conf., </booktitle> <address> Paris, France. </address>
Reference: [4] <author> Ash, T. </author> <title> (1989) "Dynamic node creation in backpropagation networks," </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> 365-375. </pages>
Reference: [5] <author> Carpenter, G.A., Grossberg, S. </author> <year> (1987) </year> <month> "ART2: </month> <title> Self-organization of stable category recognition codes for analog input patterns," </title> <journal> Applied Optics, </journal> <volume> 26, </volume> <pages> 4919-4930. </pages>
Reference: [6] <author> Chauvin, Y. </author> <title> (1989) "A back-propagation algorithm with optimal use of hidden units," </title> <booktitle> in Advances in neural information processing systems, </booktitle> <editor> D.S. Touretzky (ed.), </editor> <volume> 1, </volume> <pages> 519-526, </pages> <publisher> Morgan Kaufman. </publisher>
Reference: [7] <author> Clarke, P.G.H. </author> <title> (1985) "Neuronal death in the development of the vertebrate nervous system," </title> <journal> Trends in Neuroscience, </journal> <volume> 8, </volume> <pages> 345-349. </pages>
Reference: [8] <author> Cowan, W.M. </author> <title> (1979) "The development of the brain," </title> <journal> Scientific American, </journal> <volume> 241(3), </volume> <pages> 106-117. </pages>
Reference: [9] <author> Cowan, W.M., Fawcett, J.W., O'Leary, D.D.M., Stanfield, B.B. </author> <title> (1984) "Regressive events in neurogenesis," </title> <journal> Science, </journal> <volume> 225, </volume> <pages> 1258-1265. </pages>
Reference: [10] <author> Crick, F., Mitchison, G. </author> <title> (1983) "The function of dream sleep," </title> <journal> Nature, </journal> <volume> 304, </volume> <pages> 111-114. </pages>
Reference: [11] <author> Denker, J., Schwartz, D., Wittner, B., Solla, S., Howard, R., Jackel, L., Hopfield, J. </author> <title> (1987) "Large automatic learning, rule extraction, and generalization," </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 877-922. </pages>
Reference: [12] <author> Diederich, J. </author> <booktitle> (1988) "Connectionist recruitment learning" Proc. of the 8th European conf. on Artificial Intelligence, </booktitle> <address> London, UK. </address>
Reference: [13] <author> Duda, R.O., Hart, P.E. </author> <title> (1973) Pattern classification and scene analysis, </title> <publisher> John Wiley and sons. </publisher>
Reference: [14] <author> Fahlman, S.E., Lebiere, C. </author> <booktitle> (1990) "The cascade-correlation architecture," in Advances in neural information processing systems, </booktitle> <editor> D.S. Touretzky (ed.), </editor> <volume> 2, </volume> <pages> 524-532, </pages> <publisher> Morgan Kaufman. </publisher>
Reference: [15] <author> Feldman, J. </author> <title> (1982) "Dynamic connections in neural networks," </title> <journal> Biological Cybernetics, </journal> <volume> 46, </volume> <pages> 27-39. </pages>
Reference: [16] <author> Frean, M. </author> <title> (1990) "The upstart algorithm: A method for constructing and training feedforward neural networks," </title> <journal> Neural Computation, </journal> <volume> 2, </volume> <pages> 198-209. </pages>
Reference: [17] <author> Guyon, I., Poujoud, I., Personnaz, L., Dreyfus, G., Denker, J., le Cun, Y. </author> <title> (1989) "Comparing different neural network architectures for classifying handwritten digits" Int. </title> <booktitle> Joint Conf. on Neural Networks, </booktitle> <address> Washington, USA. </address>
Reference: [18] <author> Hanson, S.J., Pratt, L.Y. </author> <title> (1989) "Comparing biases for minimal network construction with backpropagation," </title> <booktitle> in Advances in neural information processing systems, </booktitle> <editor> D.S. Touretzky (ed.), </editor> <volume> 1, </volume> <pages> 177-185, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: [19] <author> Hanson, S.J., Burr, </author> <title> D.J. (1990) "What connectionist models learn: Learning and representation in connectionist networks," </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 13, </volume> <pages> 471-518. </pages>
Reference: [20] <author> Harp, S.A., Samad, T., Guha, A. </author> <title> (1990) "Designing application-specific neural networks using the genetic algorithm," </title> <booktitle> in Advances in neural information processing systems, </booktitle> <editor> D.S. Touretzky (ed.), </editor> <volume> 2, </volume> <publisher> Morgan Kaufmann, </publisher> <pages> 447-454. </pages>
Reference: [21] <author> Hertz, J., Krogh, A., Palmer, R.G. </author> <title> (1991) Introduction to the theory of neural computation, </title> <publisher> Addison Wesley. </publisher>
Reference: [22] <author> Hinton, G.E., Sejnowski, T.J. </author> <title> (1986) "Learning and relearning in Boltzmann machines," in Parallel distributed processing, </title> <editor> D.E. Rumelhart, J.L. McClelland (eds.), </editor> <volume> 1, </volume> <publisher> MIT Press, </publisher> <pages> 282-317. </pages>
Reference: [23] <author> Hirose, Y., Yamashita, K., Hijiya, S. </author> <title> (1991) "Back-propagation algorithm which varies the number of hidden units," </title> <booktitle> Neural Networks, </booktitle> <volume> 4, </volume> <pages> 61-66. </pages>
Reference: [24] <author> Honavar, V., Uhr, L. </author> <title> (1988) "A network of neuron-like units that learns to perceive by generation as well as reweighting of its links" Proc. </title> <booktitle> of the 1988 Connectionist Summer School, </booktitle> <editor> D. Touretzky, G. Hinton, T. Sejnowski (eds.), </editor> <publisher> Morgan Kaufman. </publisher>
Reference: [25] <author> Hopfield, J.J., Feinstein, D.I., Palmer, R.G. </author> <title> (1983) "`Unlearning' has a stabilizing effect in collective memories," </title> <journal> Nature, </journal> <volume> 304, </volume> <pages> 158-159. 24 </pages>
Reference: [26] <author> Jouvet, M. </author> <title> (1983) "Neurophysiology of dreaming," in Functions of the nervous system, </title> <editor> M. Monnier, M. Meulders (eds.), </editor> <volume> 4, </volume> <publisher> Psycho-Neurobiology, Elsevier, </publisher> <pages> 227-248. </pages>
Reference: [27] <author> Kandel, E.R., Schwartz, J.H. </author> <booktitle> (1985) Principles of neural science, 2nd edition, </booktitle> <publisher> Elsevier. </publisher>
Reference: [28] <author> Karnin, E.D. </author> <title> (1990) "A simple procedure for pruning back-propagation trained neural networks," </title> <journal> IEEE trans. on neural networks, </journal> <volume> 1, </volume> <pages> 239-242. </pages>
Reference: [29] <author> Knerr, S., Personnaz, L., Dreyfus, G. </author> <title> (1989) "Single layer learning revisited: A stepwise procedure for building and training a neural network," in Neurocomputing: Algorithms, architectures, and applications, </title> <editor> F. Fogelman-Soulie, J. Herault (eds.), </editor> <booktitle> NATO ASI Series, </booktitle> <publisher> Springer, in print. </publisher>
Reference: [30] <author> Kohonen, T. </author> <title> (1988) Self organization and associative memory, 2nd edition, </title> <publisher> Springer. </publisher>
Reference: [31] <author> Le Cun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jeckel, L.D. </author> <title> (1989) "Backpropagation applied to handwritten zip recognition," </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 541-551. </pages>
Reference: [32] <author> Le Cun, Y., Denker, J.S., Solla, S.A. </author> <title> (1990) "Optimal brain damage," </title> <booktitle> in Advances in neural information processing systems, </booktitle> <editor> D.S. Touretzky (ed.), </editor> <volume> 2, </volume> <publisher> Morgan Kaufman, </publisher> <pages> 598-605. </pages>
Reference: [33] <author> Leveit, W.J.M. </author> <title> (1990) "On learnability, empirical foundations, and naturalness," </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 13, </volume> <pages> 501. </pages>
Reference: [34] <author> Lippman, </author> <title> R.P. (1987) "An introduction to computing with neural nets," </title> <journal> IEEE ASSP magazine, </journal> <volume> 4, </volume> <pages> 4-22. </pages>
Reference: [35] <author> Mezard, M., Nadal, J.-P. </author> <title> (1989) "Learning in feedforward layered networks: The tiling algorithm," </title> <journal> Journal of Physics A, </journal> <volume> 22, </volume> <pages> 2191-2204. </pages>
Reference: [36] <author> McCarthy, J. </author> <title> (1990) "Interview: Approaches to artificial intelligence," </title> <journal> IEEE Expert, </journal> <volume> 5(3), </volume> <pages> 87-89. </pages>
Reference: [37] <author> Mozer, M.C., Smolensky, P. </author> <title> (1989) "Skeletonization: A technique for trimming the fat from a network via relevance assessment," </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> 3-26. </pages>
Reference: [38] <author> Muller, B., Reinhardt, J. </author> <title> (1990) Neural networks: An introduction, </title> <publisher> Springer Verlag. </publisher>
Reference: [39] <author> Omohondro, S. </author> <title> (1989) Geometric learning algorithms, </title> <type> ICSI Technical Report 89-041. </type>
Reference: [40] <author> Reilly, D.L., Cooper, L.N., Elbaum, C. </author> <title> (1982) "A neural model for category learning," </title> <journal> Biological Cybernetics, </journal> <volume> 45, </volume> <pages> 35-41. </pages>
Reference: [41] <author> Rissanen, J. </author> <title> (1987) "Stochastic complexity," </title> <journal> Journal of Royal Statistical Society B, </journal> <note> q, 49.223-239 and 252-265 </note>
Reference: [42] <author> Rumelhart, D.E., Hinton, G.E., Williams, </author> <title> R.J. (1986) "Learning internal representations by error propagation," in Parallel distributed processing, </title> <editor> D.E. Rumelhart, J.L. McClelland (eds.), </editor> <publisher> MIT Press, </publisher> <pages> 151-193. </pages>
Reference: [43] <author> Schmidt, J., Tieman, </author> <title> S.B. (1989) "Activity, growth cones and the selectivity of visual connections," Comments on Developmental Neurobiology, </title> <booktitle> 1, </booktitle> <pages> 11-28. </pages>
Reference: [44] <author> Siestma, J., Dow, R.J.F. </author> <title> (1991) "Creating artificial neural networks that generalize," </title> <booktitle> Neural Networks, </booktitle> <volume> 4, </volume> <pages> 67-79. </pages>
Reference: [45] <author> Weigend, A.S., Rumelhart, D.E., Huberman, </author> <title> B.A. (1991) "Generalization by weight-elimination with application to forecasting," </title> <booktitle> in Advances in neural information processing systems, </booktitle> <editor> R.P. Lippman, J. Moody, D.S. Touretzky (eds.), </editor> <publisher> Morgan Kaufmann, in print. </publisher>
References-found: 45

