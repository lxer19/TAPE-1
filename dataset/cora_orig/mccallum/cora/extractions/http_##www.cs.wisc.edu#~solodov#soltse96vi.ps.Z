URL: http://www.cs.wisc.edu/~solodov/soltse96vi.ps.Z
Refering-URL: http://www.cs.wisc.edu/~solodov/solodov.html
Root-URL: 
Title: MODIFIED PROJECTION-TYPE METHODS FOR MONOTONE VARIATIONAL INEQUALITIES  
Author: MICHAEL V. SOLODOV AND PAUL TSENG 
Keyword: Key words. Monotone variational inequalities, projection-type methods, error bound, linear convergence.  
Date: 1814-1830, September 1996 000  
Note: SIAM J. CONTROL AND OPTIMIZATION c 1996 Society for Industrial and Applied Mathematics Vol. 34, No. 5, pp.  AMS subject classifications. 49M45, 90C25, 90C33  
Abstract: We propose new methods for solving the variational inequality problem where the underlying function F is monotone. These methods may be viewed as projection-type methods in which the projection direction is modified by a strongly monotone mapping of the form I ffF or, if F is affine with underlying matrix M, of the form I + ffM T , with ff 2 (0; 1). We show that these methods are globally convergent and, if in addition a certain error bound based on the natural residual holds locally, the convergence is linear. Computational experience with the new methods is also reported. 1. Introduction. We consider the monotone variational inequality problem of 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. A. Auslender, </author> <title> Optimisation: </title> <publisher> Methodes Numeriques, Masson, </publisher> <address> Paris, </address> <year> 1976. </year>
Reference-contexts: This problem, which we abbreviate as VI (X; F ), is well known in optimization (see <ref> [1, 6, 15] </ref>) and, in the special case where F is affine and X is the nonnegative orthant, reduces to the classical monotone linear complementarity problem (see [7, 36]). Many methods have been proposed to solve VI (X; F ). <p> Many methods have been proposed to solve VI (X; F ). The simplest of these is the projection method [46] (also see <ref> [1, 2, 3, 8, 27] </ref>) which, starting with any x 2 &lt; n , iteratively updates x according to the formula x new := [x ffF (x)] + ; where [] + denotes the orthogonal projection map onto X and ff is a judiciously chosen positive stepsize. <p> The first six test problems were randomly generated, with the entries of c uniformly generated from <ref> [1; 100] </ref>, with the number of nonzeros per column of A fixed at 5% and the nonzeros uniformly generated from [5; 5], and with b = Ax, where x = (10=l; :::; 10=l). The seventh to ninth test problems were taken from the Netlib library (see [14]).
Reference: [2] <author> A. B. Bakusinskii and B. T. Polyak, </author> <title> On the solution of variational inequalities, </title> <journal> Soviet Math. Doklady, </journal> <volume> 15 (1974), </volume> <pages> pp. 1705-1710. </pages>
Reference-contexts: Many methods have been proposed to solve VI (X; F ). The simplest of these is the projection method [46] (also see <ref> [1, 2, 3, 8, 27] </ref>) which, starting with any x 2 &lt; n , iteratively updates x according to the formula x new := [x ffF (x)] + ; where [] + denotes the orthogonal projection map onto X and ff is a judiciously chosen positive stepsize.
Reference: [3] <author> D. P. Bertsekas and E. M. Gafni, </author> <title> Projection methods for variational inequalities with application to the traffic assignment problem, Math. Programming Study, </title> <booktitle> 17 (1982), </booktitle> <pages> pp. 139-159. </pages>
Reference-contexts: Many methods have been proposed to solve VI (X; F ). The simplest of these is the projection method [46] (also see <ref> [1, 2, 3, 8, 27] </ref>) which, starting with any x 2 &lt; n , iteratively updates x according to the formula x new := [x ffF (x)] + ; where [] + denotes the orthogonal projection map onto X and ff is a judiciously chosen positive stepsize. <p> [x ffF (x)] + fl + This method, by virtue of its using only function evaluations and projection onto X, is easy to implement, uses little storage, and can readily exploit any sparsity or separable structure in F or in X, such as those arising in the applications considered in <ref> [3, 9, 38, 45] </ref>. Moreover, its convergence requires only that a solution exists [20], fl Received by the editors xxxx, 1994; accepted for publication (in revised form) xxxx, 1995. The work of the first author was supported by the Air Force Office of Scientific Research, Grant No.
Reference: [4] <author> J. F. Bonnans, </author> <title> Local analysis of Newton-type methods for variational inequalities and nonlinear programming, </title> <journal> Appl. Math. Optim., </journal> <volume> 29 (1994), </volume> <pages> pp. 161-186. </pages>
Reference-contexts: SOLODOV AND PAUL TSENG while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 27, 32, 33, 34, 38, 40, 50, 54] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [5] <author> H.-G. Chen, </author> <title> Forward-backward splitting techniques: theory and applications, </title> <type> Ph.D. Thesis, </type> <institution> Department of Applied Mathematics, University of Washington, </institution> <address> Seattle, WA, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: And even on structured problems such as the discrete-time deterministic optimal control problem [45], the extragradient method may yet be practical since it is linearly convergent like the methods in <ref> [5, 10, 49, 55] </ref> while its iterations are simpler. <p> The first six test problems were randomly generated, with the entries of c uniformly generated from [1; 100], with the number of nonzeros per column of A fixed at 5% and the nonzeros uniformly generated from <ref> [5; 5] </ref>, and with b = Ax, where x = (10=l; :::; 10=l). The seventh to ninth test problems were taken from the Netlib library (see [14]). <p> The first three (respectively, fourth to sixth) test problems were randomly generated with M = !EE T + E E T ; where ! = 0 (respectively, ! = 1) and every entry of the n fi n matrix E was uniformly generated from <ref> [5; 5] </ref>, and with q = M x + y, where each entry of x has equal probability of being 0 or being uniformly generated from [5; 10] and each entry of y is 0 if the corresponding entry of x is 0 and otherwise has equal probability of being 0 <p> T ; where ! = 0 (respectively, ! = 1) and every entry of the n fi n matrix E was uniformly generated from [5; 5], and with q = M x + y, where each entry of x has equal probability of being 0 or being uniformly generated from <ref> [5; 10] </ref> and each entry of y is 0 if the corresponding entry of x is 0 and otherwise has equal probability of being 0 or being uniformly generated from [5; 10] (so x is a solution). <p> M x + y, where each entry of x has equal probability of being 0 or being uniformly generated from <ref> [5; 10] </ref> and each entry of y is 0 if the corresponding entry of x is 0 and otherwise has equal probability of being 0 or being uniformly generated from [5; 10] (so x is a solution).
Reference: [6] <author> R. W. Cottle, F. Giannessi, and J.-L. Lions, Eds., </author> <title> Variational Inequalities and Complementarity Problems: Theory and Applications, </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1980. </year>
Reference-contexts: This problem, which we abbreviate as VI (X; F ), is well known in optimization (see <ref> [1, 6, 15] </ref>) and, in the special case where F is affine and X is the nonnegative orthant, reduces to the classical monotone linear complementarity problem (see [7, 36]). Many methods have been proposed to solve VI (X; F ).
Reference: [7] <author> R. W. Cottle, J.-S. Pang, and R. E. Stone, </author> <title> The Linear Complementarity Problem, </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: This problem, which we abbreviate as VI (X; F ), is well known in optimization (see [1, 6, 15]) and, in the special case where F is affine and X is the nonnegative orthant, reduces to the classical monotone linear complementarity problem (see <ref> [7, 36] </ref>). Many methods have been proposed to solve VI (X; F ).
Reference: [8] <author> S. C. Dafermos, </author> <title> An iterative scheme for variational inequalities, </title> <journal> Math. Programming, </journal> <volume> 26 (1983), </volume> <pages> pp. 40-47. </pages>
Reference-contexts: Many methods have been proposed to solve VI (X; F ). The simplest of these is the projection method [46] (also see <ref> [1, 2, 3, 8, 27] </ref>) which, starting with any x 2 &lt; n , iteratively updates x according to the formula x new := [x ffF (x)] + ; where [] + denotes the orthogonal projection map onto X and ff is a judiciously chosen positive stepsize. <p> SOLODOV AND PAUL TSENG while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 27, 32, 33, 34, 38, 40, 50, 54] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [9] <author> S. C. Dafermos and S. C. McKelvey, </author> <title> Partitionable variational inequalities with applications to network and economic equilibria, </title> <journal> J. Optim. Theory Appl., </journal> <volume> 73 (1992), </volume> <pages> pp. 243-268. </pages>
Reference-contexts: [x ffF (x)] + fl + This method, by virtue of its using only function evaluations and projection onto X, is easy to implement, uses little storage, and can readily exploit any sparsity or separable structure in F or in X, such as those arising in the applications considered in <ref> [3, 9, 38, 45] </ref>. Moreover, its convergence requires only that a solution exists [20], fl Received by the editors xxxx, 1994; accepted for publication (in revised form) xxxx, 1995. The work of the first author was supported by the Air Force Office of Scientific Research, Grant No.
Reference: [10] <author> J. Eckstein and M. C. Ferris, </author> <title> Operator splitting methods for monotone affine variational inequalities, with a parallel application to optimal control, </title> <type> Technical Report, </type> <institution> Thinking Machines Corporation, Cambridge, MA, and Computer Sciences Department, University of Wisconsin, Madison, WI, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: 40, 50, 54] require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded and polyhedral), while the matrix-splitting methods in <ref> [10, 34, 49, 51] </ref> are applicable only when F is affine (and these methods also have, at best, linear convergence). And all these methods require more computation per iteration than the extragradient method. <p> In contrast, the matrix-splitting methods in <ref> [10, 34, 49, 51] </ref> require solving a nontrivial strongly monotone variational inequality problem over X at each iteration. <p> And even on structured problems such as the discrete-time deterministic optimal control problem [45], the extragradient method may yet be practical since it is linearly convergent like the methods in <ref> [5, 10, 49, 55] </ref> while its iterations are simpler. <p> T ; where ! = 0 (respectively, ! = 1) and every entry of the n fi n matrix E was uniformly generated from [5; 5], and with q = M x + y, where each entry of x has equal probability of being 0 or being uniformly generated from <ref> [5; 10] </ref> and each entry of y is 0 if the corresponding entry of x is 0 and otherwise has equal probability of being 0 or being uniformly generated from [5; 10] (so x is a solution). <p> M x + y, where each entry of x has equal probability of being 0 or being uniformly generated from <ref> [5; 10] </ref> and each entry of y is 0 if the corresponding entry of x is 0 and otherwise has equal probability of being 0 or being uniformly generated from [5; 10] (so x is a solution).
Reference: [11] <author> M. C. Ferris and O. L. Mangasarian, </author> <title> Error bounds and strong upper semicontinuity for monotone affine variational inequalities, </title> <journal> Ann. Oper. Res., </journal> <volume> 47 (1993), </volume> <pages> pp. 293-305. </pages>
Reference-contexts: Then any sequence fx i g generated by Algorithm 2.2 converges to an element of S and, if (3) holds for some and ffi, the convergence is R-linear. We note that Algorithm 2.2 is closely related to the following iterative method proposed in <ref> [11] </ref> x i+1 = arg min n - kx x i k 2 ;(14) where is a positive scalar. <p> We note that in <ref> [11] </ref> no convergence result is given for (14). Theorem 2.2 shows that if the step (11) is added, the resulting method (11)-(13) converges to a solution of VI (X; F ) and, if (3) holds (as in the case where X is also polyhedral), the convergence is R-linear.
Reference: [12] <author> M. Fukushima, </author> <title> Equivalent differentiable optimization problems and descent methods for asymmetric variational inequality problems, </title> <journal> Math. Programming, </journal> <volume> 53 (1992), </volume> <pages> pp. 99-110. </pages>
Reference-contexts: SOLODOV AND PAUL TSENG while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 27, 32, 33, 34, 38, 40, 50, 54] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [13] <author> E. M. Gafni and D. P. Bertsekas, </author> <title> Two-metric projection methods for constrained optimization, </title> <journal> SIAM J. Contr. Optim., </journal> <volume> 22 (1984), </volume> <pages> pp. 936-964. </pages>
Reference-contexts: i z i ffF (x i ) + ffF (z i ))k 1 kx i z i k 2 (1 ff)kP 1=2 k 1 (1 + ffL) 1 minf1; ffgkr (x i )k for all i, where L denotes the Lipschitz constant of F and the last inequality follows from <ref> [13, Lemma 1] </ref>. Thus, the rightmost term in (24) is bounded above by a positive constant times kr (x i )k 2 and, whenever this term converges R-linearly to zero as i ! 1, so does kx i+1 x i k 2 P ; hence fx i g converges R-linearly. <p> [x i ffF (x i )] + ) T (F (x i ) F (z i (ff))) ff 2 (1 )kr (x i )k 2 where the first inequality uses the Cauchy-Schwartz inequality and the nonexpansive property of [] + ; the last inequality uses ff 2 (0; 1] and <ref> [13, Lemma 1] </ref>. Thus, the claim holds. To show that fx i g converges to an element of S, let x fl be any element of S.
Reference: [14] <author> D. Gay, </author> <title> Electronic mail distribution of linear programming test problems, </title> <journal> Math. Programming Soc. COAL Newslett., </journal> <month> December, </month> <year> 1985. </year>
Reference-contexts: The seventh to ninth test problems were taken from the Netlib library (see <ref> [14] </ref>).
Reference: [15] <author> R. Glowinski, J.-L. Lions, and R. </author> <title> Tr emoli eres, Numerical Analysis of Variational Inequalities, </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1981. </year>
Reference-contexts: This problem, which we abbreviate as VI (X; F ), is well known in optimization (see <ref> [1, 6, 15] </ref>) and, in the special case where F is affine and X is the nonnegative orthant, reduces to the classical monotone linear complementarity problem (see [7, 36]). Many methods have been proposed to solve VI (X; F ).
Reference: [16] <author> P. Harker and J.-S. Pang, </author> <title> A damped-Newton method for the linear complementarity problem, in Computational Solution of Nonlinear Systems of Equations, Lectures in Applied Mathematics 26, </title> <editor> G. Allgower and K. Georg, eds., </editor> <publisher> American Mathematical Society, </publisher> <address> Providence, RI, </address> <year> 1990, </year> <pages> pp. 265-284. </pages> <note> MODIFIED PROJECTION-TYPE METHODS 17 </note>
Reference-contexts: The remaining test problems were borrowed from <ref> [16, x5] </ref>.
Reference: [17] <author> B. </author> <title> He, A projection and contraction method for a class of linear complementarity problems and its application in convex quadratic programming, </title> <journal> Appl. Math. Optim., </journal> <volume> 25 (1992), </volume> <pages> pp. </pages> <month> 247-262. </month> <title> [18] , A new method for a class of linear variational inequalities, </title> <journal> Math. Programming, </journal> <volume> 66 (1994), </volume> <pages> pp. </pages> <month> 137-144. </month> <title> [19] , Solving a class of linear projection equations, </title> <journal> Numer. Math., </journal> <volume> 68 (1994), </volume> <pages> pp. 71-80. </pages>
Reference-contexts: He's convergence and rate of convergence results for his methods are similar to ours for Algorithm 2.1 (Theorem 2.1), although He's rate of convergence results further require X to be an orthant. In an earlier work <ref> [17] </ref>, He proposed a related method which may be viewed as a version of Algorithm 2.3 in x2 with P = I and X an orthant, but using a different choice of the stepsize fl i . <p> Also, unlike 1 Subsequent to the writing of the original paper, we were informed by B. He that, for the method he proposed in <ref> [17] </ref> (which may be viewed as Algorithm 2.3 with P = I and X an orthant, but using a different choice of fl i ), he could show a linear convergence result analogous to that shown in [18] (see [19, Eq. (8)]).
Reference: [20] <author> A. N. Iusem, </author> <title> An iterative algorithm for the variational inequality problem, </title> <journal> Mat. Apl. Comput., </journal> <volume> 13 (1994), </volume> <pages> pp 103-114. </pages>
Reference-contexts: However, the projection method requires the restrictive assumption that F or F 1 be strongly monotone for convergence. The extragradient method [22] (also see <ref> [47, 20, 21, 31] </ref> for extensions) overcomes this difficulty by the ingenious technique of updating x according to the double projection formula: x new := x ffF [x ffF (x)] + fl + This method, by virtue of its using only function evaluations and projection onto X, is easy to implement, <p> Moreover, its convergence requires only that a solution exists <ref> [20] </ref>, fl Received by the editors xxxx, 1994; accepted for publication (in revised form) xxxx, 1995. The work of the first author was supported by the Air Force Office of Scientific Research, Grant No. F49620-94-1-0036, and by the National Science Foundation, Grant No. CCR-9101801.
Reference: [21] <author> E. N. Khobotov, </author> <title> A modification of the extragradient method for the solution of variational inequalities and some optimization problems, </title> <journal> Zh. Vychisl. Mat. i Mat. Fiz., </journal> <volume> 27 (1987), </volume> <pages> pp. 1462-1473. </pages>
Reference-contexts: However, the projection method requires the restrictive assumption that F or F 1 be strongly monotone for convergence. The extragradient method [22] (also see <ref> [47, 20, 21, 31] </ref> for extensions) overcomes this difficulty by the ingenious technique of updating x according to the double projection formula: x new := x ffF [x ffF (x)] + fl + This method, by virtue of its using only function evaluations and projection onto X, is easy to implement,
Reference: [22] <author> G. M. Korpelevich, </author> <title> The extragradient method for finding saddle points and other problems, </title> <journal> Matecon, </journal> <volume> 12 (1976), </volume> <pages> pp. 747-756. </pages>
Reference-contexts: However, the projection method requires the restrictive assumption that F or F 1 be strongly monotone for convergence. The extragradient method <ref> [22] </ref> (also see [47, 20, 21, 31] for extensions) overcomes this difficulty by the ingenious technique of updating x according to the double projection formula: x new := x ffF [x ffF (x)] + fl + This method, by virtue of its using only function evaluations and projection onto X, is
Reference: [23] <author> X.-D. Luo and P. Tseng, </author> <title> On global projection-type error bound for the linear complementarity problem, </title> <note> Linear Algebra Appl., to appear. </note>
Reference-contexts: Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see <ref> [23, 24, 29, 39] </ref>).
Reference: [24] <author> Z.-Q. Luo, O. L. Mangasarian, J. Ren, and M. V. Solodov, </author> <title> New error bounds for the linear complementarity problem, </title> <journal> Math. Oper. Res., </journal> <volume> 19 (1994), </volume> <pages> pp. 880-892. </pages>
Reference-contexts: Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see <ref> [23, 24, 29, 39] </ref>).
Reference: [25] <author> Z.-Q. Luo and P. Tseng, </author> <title> On the linear convergence of descent methods for convex essentially smooth minimization, </title> <journal> SIAM J. Control Optim., </journal> <volume> 30 (1992), </volume> <pages> pp. </pages> <month> 408-425. </month> <title> [26] , Error bound and convergence analysis of matrix splitting algorithms for the affine variational inequality problem, </title> <journal> SIAM J. Optim., </journal> <volume> 2 (1992), </volume> <pages> pp. 43-54. </pages>
Reference-contexts: to S. (It is well known that an x fl 2 &lt; n solves VI (X; F ) if and only if r (x fl ) = 0.) This growth condition on kr ()k (also called error bound) has been used in the rate of convergence analysis of various methods <ref> [25, 26, 51] </ref> and is known to hold whenever X is polyhedral and either F is affine (see [26, 43]) or F has certain strong monotonicity structure (see [51, Theorem 2]). Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see [23, 24, 29, 39]). <p> The analysis is also similar in spirit to those for feasible descent methods (see <ref> [25, 26, 28] </ref>) but uses d (; S) 2 , rather than the objective function, as the merit function. Our main results are as follows: In x2, we consider the special case of VI (X; F ) where F is affine.
Reference: [27] <author> T. L. Magnanti and G. Perakis, </author> <title> On the convergence of classical variational inequality algorithms, </title> <type> Working Paper, </type> <institution> Operations Research Center, Masachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Many methods have been proposed to solve VI (X; F ). The simplest of these is the projection method [46] (also see <ref> [1, 2, 3, 8, 27] </ref>) which, starting with any x 2 &lt; n , iteratively updates x according to the formula x new := [x ffF (x)] + ; where [] + denotes the orthogonal projection map onto X and ff is a judiciously chosen positive stepsize. <p> SOLODOV AND PAUL TSENG while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 27, 32, 33, 34, 38, 40, 50, 54] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [28] <author> O. L. Mangasarian, </author> <title> Convergence of iterates of an inexact matrix splitting algorithm for the symmetric monotone linear complementarity problem, </title> <journal> SIAM J. Optim., </journal> <volume> 1 (1991), </volume> <pages> pp. 114-122. </pages>
Reference-contexts: The analysis is also similar in spirit to those for feasible descent methods (see <ref> [25, 26, 28] </ref>) but uses d (; S) 2 , rather than the objective function, as the merit function. Our main results are as follows: In x2, we consider the special case of VI (X; F ) where F is affine.
Reference: [29] <author> O. L. Mangasarian and J. Ren, </author> <title> New improved error bounds for the linear complementarity problem, </title> <journal> Math. Programming, </journal> <volume> 66 (1994), </volume> <pages> pp. 241-255. </pages>
Reference-contexts: Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see <ref> [23, 24, 29, 39] </ref>).
Reference: [30] <author> O. L. Mangasarian and M. V. Solodov, </author> <title> Nonlinear complementarity as unconstrained and constrained minimization, </title> <journal> Math. Programming, </journal> <volume> 62 (1993), </volume> <pages> pp. 277-297. </pages>
Reference: [31] <author> P. Marcotte, </author> <title> Application of Khobotov's algorithm to variational inequalities and network equilibrium problems, </title> <journal> Inform. Systems Oper. Res., </journal> <volume> 29 (1991), </volume> <pages> pp. 258-270. </pages>
Reference-contexts: However, the projection method requires the restrictive assumption that F or F 1 be strongly monotone for convergence. The extragradient method [22] (also see <ref> [47, 20, 21, 31] </ref> for extensions) overcomes this difficulty by the ingenious technique of updating x according to the double projection formula: x new := x ffF [x ffF (x)] + fl + This method, by virtue of its using only function evaluations and projection onto X, is easy to implement, <p> see that this does not affect the convergence (and, in fact, accelerates convergence) of the methods, we use the following fact about nearest-point projection: k [y] + P ky x fl k 2 P k 2 for all y 2 &lt; n and all x fl 2 X (see, e.g., <ref> [31, Appendix] </ref>). <p> For benchmark, we compared the performance of these implementations with analogous implementations of the extragradient method as described in <ref> [31] </ref>. (We have included LPs and dense monotone LCPs in our tests not because they are problems for which the new methods are designed to solve, but because these problems are well known special cases of VI (X; F ) and tests on them give us a better overall understanding of <p> 3372 36.7 14277 159.3 Bandm 305 472 1202607 12837 - hline Table 1 Results for Algorithm 2.1 and extragradient method on LP. 1 Algorithm 2.1 with P being the diagonal part of (I + M T )(I + M ) and = :7. 2 The extragradient method as described in <ref> [31] </ref>, with fi = :7 and initial ff = 1. 3 For all methods, x 0 = 0 and the termination criterion is kr (x)k *. 4 Time (in seconds) obtained using the intrinsic function SECNDS and with the codes compiled by the DEC Fortran-77 compiler and ran on a Decstation <p> 3.8 855 16.2 1115 20.9 Lemke 100 1057 21.3 1107 22.0 1508 27.5 2261 44.3 Table 2 Results for Algorithm 2.1 and extragradient method on LCP. 1 Algorithm 2.1 with P = (I + M T )(I + M ) and = 1. 2 The extragradient method as described in <ref> [31] </ref>, with fi = :7 and initial ff = 1. 3 For all methods, x 0 = 0 and the termination criterion is kr (x)k *. 4 Time (in seconds) obtained using the intrinsic function SECNDS and with the codes compiled by the DEC Fortran-77 compiler and ran on a Decstation <p> method on linearly constrained variational inequality problems. 1 Algorithm 2.1 with P = (I + M T )(I + M ) and = 1:5. 2 Algorithm 3.2 with P = I, ff 1 = 1, = 1:5, = :1 and fi = :3. 3 The extragradient method as described in <ref> [31] </ref>, with fi = :7 and initial ff = 1. 4 For all methods, the termination criterion is kr (x)k 10 4 . (nf denotes the total number of times F is evaluated and np denotes the total number of times a projection onto X is performed.) On the Mathiesen problem,
Reference: [32] <author> P. Marcotte and J.-P. Dussault, </author> <title> A note on a globally convergent Newton method for solving monotone variational inequalities, </title> <journal> Oper. Res. Lett., </journal> <volume> 6 (1987), </volume> <pages> pp. </pages> <month> 35-42. </month> <title> [33] , A sequential linear programming algorithm for solving monotone variational inequalities, </title> <journal> SIAM J. Control Optim., </journal> <volume> 27 (1989), </volume> <pages> pp. 1260-1278. </pages>
Reference-contexts: SOLODOV AND PAUL TSENG while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 27, 32, 33, 34, 38, 40, 50, 54] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [34] <author> P. Marcotte and J.-H. Wu, </author> <title> On the convergence of projection methods: application to the decomposition of affine variational inequalities, </title> <journal> J. Optim. Theory Appl., </journal> <volume> 85 (1995), </volume> <pages> pp. 347-362. </pages>
Reference-contexts: SOLODOV AND PAUL TSENG while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 27, 32, 33, 34, 38, 40, 50, 54] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded <p> 40, 50, 54] require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded and polyhedral), while the matrix-splitting methods in <ref> [10, 34, 49, 51] </ref> are applicable only when F is affine (and these methods also have, at best, linear convergence). And all these methods require more computation per iteration than the extragradient method. <p> In contrast, the matrix-splitting methods in <ref> [10, 34, 49, 51] </ref> require solving a nontrivial strongly monotone variational inequality problem over X at each iteration.
Reference: [35] <author> L. Mathiesen, </author> <title> An algorithm based on a sequence of linear complementarity problems applied to a Walras' equilibrium model: An example, </title> <journal> Math. Programming, </journal> <volume> 37 (1987), </volume> <pages> pp. 1-18. </pages>
Reference-contexts: Our third set of tests was conducted on VI (X; F ) where X is not an orthant or a box. The first test problem, used first by Mathiesen <ref> [35] </ref>, and later in [41, 54], has F (x 1 ; x 2 ; x 3 ) = 4 :1 (5x 2 + 3x 3 )=x 2 5 3 X = (x 1 ; x 2 ; x 3 ) 2 &lt; 3 We had trouble finding more test problems from
Reference: [36] <author> K. G. Murty, </author> <title> Linear Complementarity, Linear and Nonlinear Programming, </title> <address> Helderman-Verlag, Berlin, </address> <year> 1988. </year>
Reference-contexts: This problem, which we abbreviate as VI (X; F ), is well known in optimization (see [1, 6, 15]) and, in the special case where F is affine and X is the nonnegative orthant, reduces to the classical monotone linear complementarity problem (see <ref> [7, 36] </ref>). Many methods have been proposed to solve VI (X; F ).
Reference: [37] <author> J. M. Ortega and W. C. Rheinboldt, </author> <title> Iterative Solution of Nonlinear Equations in Several Variables, </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1970. </year>
Reference-contexts: We denote by I either the identity matrix or the identity map and, by R-linear convergence and Q-linear convergence, we mean linear convergence in the root sense and in the quotient sense, respectively, as defined in <ref> [37] </ref>. 2. Algorithms for F Affine.
Reference: [38] <author> J.-S. Pang, </author> <title> Asymmetric variational inequality problems over product sets: applications and iterative methods, </title> <journal> Math. Programming, </journal> <volume> 31 (1985), </volume> <pages> pp. </pages> <month> 206-219. </month> <title> [39] , A posteriori error bounds for the linearly-constrained variational inequality problem, </title> <journal> Math. Oper. Res., </journal> <volume> 12 (1987), </volume> <pages> pp. 474-484. </pages>
Reference-contexts: [x ffF (x)] + fl + This method, by virtue of its using only function evaluations and projection onto X, is easy to implement, uses little storage, and can readily exploit any sparsity or separable structure in F or in X, such as those arising in the applications considered in <ref> [3, 9, 38, 45] </ref>. Moreover, its convergence requires only that a solution exists [20], fl Received by the editors xxxx, 1994; accepted for publication (in revised form) xxxx, 1995. The work of the first author was supported by the Air Force Office of Scientific Research, Grant No. <p> SOLODOV AND PAUL TSENG while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 27, 32, 33, 34, 38, 40, 50, 54] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [40] <author> J.-S. Pang and D. Chan, </author> <title> Iterative methods for variational and complementarity problems, </title> <journal> Math. Programming, </journal> <volume> 24 (1982), </volume> <pages> pp. 284-313. </pages>
Reference-contexts: SOLODOV AND PAUL TSENG while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 27, 32, 33, 34, 38, 40, 50, 54] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [41] <author> J.-S. Pang and S. A. Gabriel, NE/SQP: </author> <title> A robust algorithm for the nonlinear complementarity problem, </title> <journal> Math. Programming, </journal> <volume> 60 (1993), </volume> <pages> pp. 295-337. </pages>
Reference-contexts: Our third set of tests was conducted on VI (X; F ) where X is not an orthant or a box. The first test problem, used first by Mathiesen [35], and later in <ref> [41, 54] </ref>, has F (x 1 ; x 2 ; x 3 ) = 4 :1 (5x 2 + 3x 3 )=x 2 5 3 X = (x 1 ; x 2 ; x 3 ) 2 &lt; 3 We had trouble finding more test problems from the literature, so we <p> &lt; n + j x 1 + + x n = ng and F and n are specified as follows: For the first three problems, F is the function from, respectively, the Kojima-Shindo NCP (with n = 4) and the Nash-Cournot NCP (with n = 5 and n = 10) <ref> [41, pp. 321-322] </ref>; for the fourth problem, F is affine and is generated as in the problem HPHard of Table 2, but with n = 20; for the fifth problem, we took the F from the fourth problem and added to its ith component the linear/quadratic term maxf0; x i g
Reference: [42] <author> P. M. Pardalos and N. Kovoor, </author> <title> An algorithm for a singly constrained class of quadratic programs subject to upper and lower bounds, </title> <journal> Math. Programming, </journal> <volume> 46 (1986), </volume> <pages> pp. 235-238. </pages>
Reference-contexts: The extragradient method can be practically implemented to solve this special case of VI (X; F ) since it requires only projection onto the simplices and ellipsoids (for which many efficient methods exist <ref> [42, 53] </ref>) and multiplication of x by the sparse matrix M . In contrast, the matrix-splitting methods in [10, 34, 49, 51] require solving a nontrivial strongly monotone variational inequality problem over X at each iteration.
Reference: [43] <author> S. M. Robinson, </author> <title> Some continuity properties of polyhedral multifunctions, Math. Programming Study, </title> <booktitle> 14 (1981), </booktitle> <pages> pp. 206-214. </pages>
Reference-contexts: if and only if r (x fl ) = 0.) This growth condition on kr ()k (also called error bound) has been used in the rate of convergence analysis of various methods [25, 26, 51] and is known to hold whenever X is polyhedral and either F is affine (see <ref> [26, 43] </ref>) or F has certain strong monotonicity structure (see [51, Theorem 2]). Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see [23, 24, 29, 39]).
Reference: [44] <author> R. T. Rockafellar, </author> <title> Monotone operators and the proximal point algorithm, </title> <journal> SIAM J. Control Optim., </journal> <volume> 14 (1976), </volume> <pages> pp. 877-898. </pages>
Reference-contexts: The remaining argument is patterned after the proof of <ref> [44, Theorem 1] </ref> and of [51, Theorem 1]. Since (8) holds for all i, it follows that kx i x fl k P is nonincreasing with i and that kr (x i )k ! 0 as i ! 1.
Reference: [45] <author> R. T. Rockafellar and R. J.-B. Wets, </author> <title> Generalized linear-quadratic problems of deterministic and stochastic optimal control in discrete time, </title> <journal> SIAM J. Control Optim., </journal> <volume> 28 (1990), </volume> <pages> pp. </pages> <note> 810 18 MICHAEL V. SOLODOV AND PAUL TSENG 822. </note>
Reference-contexts: [x ffF (x)] + fl + This method, by virtue of its using only function evaluations and projection onto X, is easy to implement, uses little storage, and can readily exploit any sparsity or separable structure in F or in X, such as those arising in the applications considered in <ref> [3, 9, 38, 45] </ref>. Moreover, its convergence requires only that a solution exists [20], fl Received by the editors xxxx, 1994; accepted for publication (in revised form) xxxx, 1995. The work of the first author was supported by the Air Force Office of Scientific Research, Grant No. <p> In contrast, the matrix-splitting methods in [10, 34, 49, 51] require solving a nontrivial strongly monotone variational inequality problem over X at each iteration. And even on structured problems such as the discrete-time deterministic optimal control problem <ref> [45] </ref>, the extragradient method may yet be practical since it is linearly convergent like the methods in [5, 10, 49, 55] while its iterations are simpler.
Reference: [46] <author> M. Sibony, </author> <title> Methodes iteratives pour les equations et inequations aux derivees partielles non-lineares de type monotone, Calcolo, </title> <booktitle> 7 (1970), </booktitle> <pages> pp. 65-183. </pages>
Reference-contexts: Many methods have been proposed to solve VI (X; F ). The simplest of these is the projection method <ref> [46] </ref> (also see [1, 2, 3, 8, 27]) which, starting with any x 2 &lt; n , iteratively updates x according to the formula x new := [x ffF (x)] + ; where [] + denotes the orthogonal projection map onto X and ff is a judiciously chosen positive stepsize.
Reference: [47] <author> D. Sun, </author> <title> A new step-size skill for solving a class of nonlinear projection equations, </title> <journal> J. Comput. Math., </journal> <month> 13 </month> <year> (1995), </year> <title> to appear. [48] , A class of iterative methods for solving nonlinear projection equations, </title> <journal> J. Optim. Theory Appl., </journal> <note> 91 (1996), to appear. </note>
Reference-contexts: However, the projection method requires the restrictive assumption that F or F 1 be strongly monotone for convergence. The extragradient method [22] (also see <ref> [47, 20, 21, 31] </ref> for extensions) overcomes this difficulty by the ingenious technique of updating x according to the double projection formula: x new := x ffF [x ffF (x)] + fl + This method, by virtue of its using only function evaluations and projection onto X, is easy to implement,
Reference: [49] <author> P. Tseng, </author> <title> Further applications of a splitting algorithm to decomposition in variational inequalities and convex programming, </title> <journal> Math. Programming, </journal> <volume> 48 (1990), </volume> <pages> pp. </pages> <month> 249-263. </month> <title> [50] , Applications of a splitting algorithm to decomposition in convex programming and variational inequalities, </title> <journal> SIAM J. Control Optim., </journal> <volume> 29 (1991), </volume> <pages> pp. </pages> <month> 119-138. </month> <title> [51] , On linear convergence of iterative methods for the variational inequality problem, </title> <journal> J. Comput. Appl. Math., </journal> <volume> 60 (1995), </volume> <pages> pp. 237-252. </pages>
Reference-contexts: 40, 50, 54] require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded and polyhedral), while the matrix-splitting methods in <ref> [10, 34, 49, 51] </ref> are applicable only when F is affine (and these methods also have, at best, linear convergence). And all these methods require more computation per iteration than the extragradient method. <p> In contrast, the matrix-splitting methods in <ref> [10, 34, 49, 51] </ref> require solving a nontrivial strongly monotone variational inequality problem over X at each iteration. <p> And even on structured problems such as the discrete-time deterministic optimal control problem [45], the extragradient method may yet be practical since it is linearly convergent like the methods in <ref> [5, 10, 49, 55] </ref> while its iterations are simpler.
Reference: [52] <author> N. Yamashita and M. Fukushima, </author> <title> On dtationary points of the implicit Lagrangian for nonlinear complementarity problems, </title> <journal> J. Optim. Theory Appl., </journal> <volume> 84 (1995), </volume> <pages> pp. 653-663. </pages>
Reference: [53] <author> Y. Ye, </author> <title> A new complexity result on minimization of a quadratic function with a sphere constraint, in Recent Advances in Global Optimization, </title> <editor> C. Floudas and P. M. Pardalos, eds., </editor> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1992. </year>
Reference-contexts: The extragradient method can be practically implemented to solve this special case of VI (X; F ) since it requires only projection onto the simplices and ellipsoids (for which many efficient methods exist <ref> [42, 53] </ref>) and multiplication of x by the sparse matrix M . In contrast, the matrix-splitting methods in [10, 34, 49, 51] require solving a nontrivial strongly monotone variational inequality problem over X at each iteration.
Reference: [54] <author> L. Zhao and S. Dafermos, </author> <title> General economic equilibrium and variational inequalities, </title> <journal> Oper. Res. Lett., </journal> <volume> 10 (1991), </volume> <pages> pp. 369-376. </pages>
Reference-contexts: SOLODOV AND PAUL TSENG while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 27, 32, 33, 34, 38, 40, 50, 54] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded <p> Our third set of tests was conducted on VI (X; F ) where X is not an orthant or a box. The first test problem, used first by Mathiesen [35], and later in <ref> [41, 54] </ref>, has F (x 1 ; x 2 ; x 3 ) = 4 :1 (5x 2 + 3x 3 )=x 2 5 3 X = (x 1 ; x 2 ; x 3 ) 2 &lt; 3 We had trouble finding more test problems from the literature, so we <p> On the Mathiesen problem, we used the same x 0 as in <ref> [54] </ref>; on the other problems, we used x 0 = (1; :::; 1). (The F from the Mathiesen problem and from the Nash-Cournot NCP are defined on the positive orthant only.) The test results are summarized in Table 3.
Reference: [55] <author> C. Zhu and R. T. Rockafellar, </author> <title> Primal-dual projected gradient algorithms for extended linear-quadratic programming, </title> <journal> SIAM J. Optim., </journal> <volume> 3 (1993), </volume> <pages> pp. 751-783. </pages>
Reference-contexts: And even on structured problems such as the discrete-time deterministic optimal control problem [45], the extragradient method may yet be practical since it is linearly convergent like the methods in <ref> [5, 10, 49, 55] </ref> while its iterations are simpler.
References-found: 47

