URL: http://www.cs.rochester.edu/u/salgian/publ/ICCV98.ps.gz
Refering-URL: http://www.cs.rochester.edu/stats/oldmonths/1998.06/docs-bytes.html
Root-URL: 
Title: Visual Routines for Autonomous Driving  
Author: Garbis Salgian Dana H. Ballard 
Address: Rochester, NY 14627  
Affiliation: Computer Science Department University of Rochester  
Date: January 4-7 1998,  
Note: In Proceedings of the 6-th ICCV,  Bombay, India, pp 876-882  
Abstract: The paper describes visual routines based on models of color and shape, as well as crucial issues involving the scheduling of such routines. The visual routines are developed in a unique platform. The view from a car driving in a simulated world is fed into a Dat-acube pipeline video processor. The use of this simulation provides a flexible environment from which to set crucial image processing parameters of the individual routines. In addition to the simulations, the routines are also tested in similar images generated by driving in the real world, to assure the generalizability of the simulation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Reddy, </author> <title> "The challenge of artificial intelligence," </title> <journal> IEEE Computer, </journal> <volume> vol. 29, </volume> <pages> pp. 86-98, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: While road following has already been successfully demonstrated [8], [10], autonomous tactical level driving (where the agent has the ability to do traffic maneuvers) is still an open research problem [11]. Reddy <ref> [1] </ref> notes that the development of an autonomous vehicle capable of driving in traffic is one of the major challenges for Artificial Intelligence. As little as a decade ago, it was widely accepted that the visual world could be completely segmented into constituent parts prior to analysis.
Reference: [2] <author> J. K. Tsotsos, </author> <title> "The complexity of perceptual search tasks," </title> <booktitle> in IJCAI-89, </booktitle> <volume> vol. </volume> <pages> 2, </pages> <address> (Detroit, MI, USA), </address> <pages> pp. 1571-1577, </pages> <month> 20-25 August </month> <year> 1989. </year>
Reference-contexts: This view was supported in part by the belief that additional computing cycles would eventually be available to solve this problem. However the complexity of vision's initial segmentation can easily be unbounded for all practical purposes <ref> [2] </ref>, so that the goal of determining a complete segmentation of an individual scene in real time is impractical.
Reference: [3] <author> S. M. Kosslyn, </author> <title> "Scanning visual images: Some structural implications," </title> <journal> Perception and Psychophysics, </journal> <volume> vol. 14, no. 1, </volume> <pages> pp. 90-94, </pages> <year> 1973. </year>
Reference-contexts: A solution to this is to have a library of visual routines. The concept of visual routines was first developed by Kosslyn <ref> [3] </ref> but the rationale for their use was elaborated by Ullman [4]. The central idea is to have a collection of routines that represent different kinds of basic image processing sub-functions. These then can be composed to subserve more elaborate goal-directed programs.
Reference: [4] <author> S. Ullman, </author> <title> "Visual routines," </title> <journal> Cognition, </journal> <volume> no. 18, </volume> <pages> pp. 97-160, </pages> <year> 1984. </year>
Reference-contexts: A solution to this is to have a library of visual routines. The concept of visual routines was first developed by Kosslyn [3] but the rationale for their use was elaborated by Ullman <ref> [4] </ref>. The central idea is to have a collection of routines that represent different kinds of basic image processing sub-functions. These then can be composed to subserve more elaborate goal-directed programs. The crucial compositional capability allows the visual routines to span the huge space of different task goals. <p> The behaviors use visual routines to gather the information they need and act accordingly. Finally, the visual routines are composed from an alphabet of basic operations (similar to Ull-man's proposal <ref> [4] </ref>). To illustrate how modules on different levels in the hierarchy interact, consider the case when the sched-uler activates the stop sign behavior. In order to determine whether there is a stop sign within certain range, the stop sign detection visual routine is invoked.
Reference: [5] <author> D. Chapman, </author> <title> Vision, Instruction and Action. </title> <address> Cam-bridge, Massachusetts: </address> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: These then can be composed to subserve more elaborate goal-directed programs. The crucial compositional capability allows the visual routines to span the huge space of different task goals. Although the idea of visual routines is compelling, and they have been used in several simulations <ref> [5] </ref>, [6], so far they have been used in image analysis only in a few restricted circumstances [7]. Dick-manns [8] mentions among the advantages of his spatio-temporal approach "intelligent nonuniform image analysis . . . allowing to concentrate limited computer resources to areas of interest".
Reference: [6] <author> D. A. Reece, </author> <title> "Selective perception for robot driving," </title> <type> Tech. Rep. </type> <institution> CMU-CS-92-139, Carnegie Mellon University, </institution> <year> 1992. </year>
Reference-contexts: These then can be composed to subserve more elaborate goal-directed programs. The crucial compositional capability allows the visual routines to span the huge space of different task goals. Although the idea of visual routines is compelling, and they have been used in several simulations [5], <ref> [6] </ref>, so far they have been used in image analysis only in a few restricted circumstances [7]. Dick-manns [8] mentions among the advantages of his spatio-temporal approach "intelligent nonuniform image analysis . . . allowing to concentrate limited computer resources to areas of interest". <p> This issue has been addressed by other researchers and several solutions have been proposed: inference trees <ref> [6] </ref>, and more recently, distributed architectures with centralized arbitration [17], [18]. We have just started to experiment with different alternatives for the scheduler design. Currently our principal method is to alternate between the two existing behaviors (traffic light and stop sign), but there are important subsidiary considerations. <p> This allows us to explore the scheduling tradeoffs that humans use. This provides a benchmark for the automated driver and also is a source for ideas as to priorities that work. Future work will flesh out the design in Figure 2. The key point made by earlier work [8], <ref> [6] </ref> is that context in driving is required to reduce that amount of computation to practical levels. Adding additional routines will further exacerbate the demands on computation.
Reference: [7] <author> M. P. Johnson, P. Maes, and T. Darrell, </author> <title> "Evolving visual routines," </title> <booktitle> in Artificial Life IV Conference, </booktitle> <month> July 6-8 </month> <year> 1994. </year>
Reference-contexts: Although the idea of visual routines is compelling, and they have been used in several simulations [5], [6], so far they have been used in image analysis only in a few restricted circumstances <ref> [7] </ref>. Dick-manns [8] mentions among the advantages of his spatio-temporal approach "intelligent nonuniform image analysis . . . allowing to concentrate limited computer resources to areas of interest". Burt defined smart sensing as the "selective, task oriented, gathering of information from the visual world" [9].
Reference: [8] <author> E. D. Dickmanns, B. D. Mysliwetz, and T. Chris-tians, </author> <title> "An integrated spatio-temporal approach to automatic visual guidance of autonomous vehicles," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 20, </volume> <pages> pp. 1273-1284, </pages> <month> Nov/Dec </month> <year> 1990. </year>
Reference-contexts: Nowhere is this more apparent than in the application of automated driving. Steering corrections in very complex environments normally have to be made at a rate of at least one hertz, placing enormous demands on image analysis systems. While road following has already been successfully demonstrated <ref> [8] </ref>, [10], autonomous tactical level driving (where the agent has the ability to do traffic maneuvers) is still an open research problem [11]. Reddy [1] notes that the development of an autonomous vehicle capable of driving in traffic is one of the major challenges for Artificial Intelligence. <p> Although the idea of visual routines is compelling, and they have been used in several simulations [5], [6], so far they have been used in image analysis only in a few restricted circumstances [7]. Dick-manns <ref> [8] </ref> mentions among the advantages of his spatio-temporal approach "intelligent nonuniform image analysis . . . allowing to concentrate limited computer resources to areas of interest". Burt defined smart sensing as the "selective, task oriented, gathering of information from the visual world" [9]. <p> One may argue that we should have started with the road following behavior, since a real vehicle will need it to be able to move. Road following has been intensely studied for more than a decade <ref> [8] </ref>, [10] and it was successfully demonstrated at high speeds and over extended distances. Therefore we decided not to duplicate these efforts initially and instead to take advantage of the simulated environment. <p> This allows us to explore the scheduling tradeoffs that humans use. This provides a benchmark for the automated driver and also is a source for ideas as to priorities that work. Future work will flesh out the design in Figure 2. The key point made by earlier work <ref> [8] </ref>, [6] is that context in driving is required to reduce that amount of computation to practical levels. Adding additional routines will further exacerbate the demands on computation.
Reference: [9] <author> P. J. Burt, </author> <title> "Smart sensing in machine vision," in Machine Vision. Algorithms, Architectures, and Systems (H. </title> <publisher> Freeman, ed.), </publisher> <pages> pp. 1-30, </pages> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1988. </year>
Reference-contexts: Dick-manns [8] mentions among the advantages of his spatio-temporal approach "intelligent nonuniform image analysis . . . allowing to concentrate limited computer resources to areas of interest". Burt defined smart sensing as the "selective, task oriented, gathering of information from the visual world" <ref> [9] </ref>. The framework gives general guidelines for limiting visual searches, but it does not present specific methods for choosing the regions of interest for further analysis. This paper describes the use of visual routines for driving.
Reference: [10] <author> D. Pomerleau, "Ralph: </author> <title> rapidly adapting lateral position handler," </title> <booktitle> in Proceedings of the Intelligent Vehicles '95 Symposium, </booktitle> <address> (New York, NY, USA), </address> <pages> pp. 506-511, </pages> <publisher> IEEE, </publisher> <month> September </month> <year> 1995. </year>
Reference-contexts: Nowhere is this more apparent than in the application of automated driving. Steering corrections in very complex environments normally have to be made at a rate of at least one hertz, placing enormous demands on image analysis systems. While road following has already been successfully demonstrated [8], <ref> [10] </ref>, autonomous tactical level driving (where the agent has the ability to do traffic maneuvers) is still an open research problem [11]. Reddy [1] notes that the development of an autonomous vehicle capable of driving in traffic is one of the major challenges for Artificial Intelligence. <p> One may argue that we should have started with the road following behavior, since a real vehicle will need it to be able to move. Road following has been intensely studied for more than a decade [8], <ref> [10] </ref> and it was successfully demonstrated at high speeds and over extended distances. Therefore we decided not to duplicate these efforts initially and instead to take advantage of the simulated environment.
Reference: [11] <author> E. D. Dickmanns, </author> <title> "Performance improvements for autonomous road vehicles," </title> <booktitle> in Proceedings of the 4th International Conference on Intelligent Autonomous Systems, </booktitle> <address> (Karlsruhe, Germany), </address> <pages> pp. 2-14, </pages> <month> March 27-30 </month> <year> 1995. </year>
Reference-contexts: While road following has already been successfully demonstrated [8], [10], autonomous tactical level driving (where the agent has the ability to do traffic maneuvers) is still an open research problem <ref> [11] </ref>. Reddy [1] notes that the development of an autonomous vehicle capable of driving in traffic is one of the major challenges for Artificial Intelligence. As little as a decade ago, it was widely accepted that the visual world could be completely segmented into constituent parts prior to analysis.
Reference: [12] <author> D. Terzopoulos and T. F. Rabie, </author> <title> "Animat vision: </title> <booktitle> Active vision in artificial animals," in ICCV-95, </booktitle> <pages> pp. 801-808, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The visual routines presented here are a major component of the perception subsystem of an intelligent vehicle. They are developed in a unique platform. The view from a car driving in a simulated world is fed into a Datacube pipeline video processor. Terzopoulos <ref> [12] </ref> pioneered the use of simulated images in his animat vision architecture. However, the processing is carried out in software only, and it is not clear how the algorithms would transfer into the real world.
Reference: [13] <author> D. H. Ballard and C. M. Brown, </author> <title> Computer Vision. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference-contexts: This also helps eliminating noise, since at every pyramid level the image is smoothed before subsam-pling. After the reduced size mask is transferred into host memory, a blob labeling algorithm <ref> [13] </ref> is applied. The end result is a list of bounding rectangles for the blobs of the given color. Steerable filters. The role of the steerable filters primitive is to detect objects of a specific appearance.
Reference: [14] <author> W. T. Freeman and E. H. Adelson, </author> <title> "The design and use of steerable filters," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 13, </volume> <pages> pp. 891-906, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: The end result is a list of bounding rectangles for the blobs of the given color. Steerable filters. The role of the steerable filters primitive is to detect objects of a specific appearance. Steerable filters were first proposed by Freeman and Adelson <ref> [14] </ref>, who showed how a filter of arbitrary orientation and phase can be synthesized from a set of basis filters (oriented derivatives of a two-dimensional, circularly symmetric Gaussian function). Other researchers [15], [16] have used these filters for object identification.
Reference: [15] <author> D. H. Ballard and L. E. Wixson, </author> <title> "Object recognition using steerable filters at multiple scales," </title> <booktitle> in Proceedings of IEEE Workshop on Qualitative Vision, </booktitle> <pages> pp. 2-10, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Steerable filters were first proposed by Freeman and Adelson [14], who showed how a filter of arbitrary orientation and phase can be synthesized from a set of basis filters (oriented derivatives of a two-dimensional, circularly symmetric Gaussian function). Other researchers <ref> [15] </ref>, [16] have used these filters for object identification. The idea is to create a unique index for every image location by convolving the image at different spatial resolutions with filters from the basis set.
Reference: [16] <author> R. P. Rao and D. H. Ballard, </author> <title> "Object indexing using an iconic sparse distributed memory," </title> <booktitle> in ICCV-95, </booktitle> <pages> pp. 24-31, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Steerable filters were first proposed by Freeman and Adelson [14], who showed how a filter of arbitrary orientation and phase can be synthesized from a set of basis filters (oriented derivatives of a two-dimensional, circularly symmetric Gaussian function). Other researchers [15], <ref> [16] </ref> have used these filters for object identification. The idea is to create a unique index for every image location by convolving the image at different spatial resolutions with filters from the basis set.
Reference: [17] <author> J. K. Rosenblatt, DAMN: </author> <title> A Distributed Architecture for Mobile Navigation. </title> <type> PhD thesis, </type> <institution> The Robotics Institute, </institution> <address> CMU, </address> <month> January </month> <year> 1997. </year> <month> CMU-RI-TR-97-01. </month>
Reference-contexts: This issue has been addressed by other researchers and several solutions have been proposed: inference trees [6], and more recently, distributed architectures with centralized arbitration <ref> [17] </ref>, [18]. We have just started to experiment with different alternatives for the scheduler design. Currently our principal method is to alternate between the two existing behaviors (traffic light and stop sign), but there are important subsidiary considerations.

References-found: 17

