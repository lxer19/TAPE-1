URL: http://www.win.tue.nl/cs/pa/research/networks/csr95_01.ps
Refering-URL: http://www.win.tue.nl/cs/pa/johanl/
Root-URL: http://www.win.tue.nl
Email: Email: johanl@win.tue.nl  
Phone: Telephone: +31 40 475147 Telefax: +31 40 436685  
Title: The Construction of a small Communication Library  
Author: Johan J. Lukkien 
Date: November 30, 1994  
Address: P.O. Box 513 5600 MB Eindhoven, The Netherlands  
Affiliation: Eindhoven University of Technology Department of Mathematics and Computing Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Back, R.J.R., </author> <title> On the correctness of refinement steps in program development, </title> <type> report A-1978-4, </type> <institution> Abo Akademo, Department of computer science, Finland. </institution>
Reference-contexts: Implementing an action by simpler ones is called action refinement. Formal discussions on the subject can be found in <ref> [1, 16] </ref>. We illustrate this by an example of an implementation. If two processes communicating along a channel are mapped onto the same processor or onto processors with shared memory, we map it onto an edge of length 0 using shared memory. The implementation has to satisfy C0 through C2.
Reference: [2] <author> Dally, W.J., Seitz, C.L., </author> <title> Deadlock free message routing in multiprocessor interconnections networks, </title> <institution> Dept. Comp. Science, California Institute of Technology, </institution> <type> Tech. Rep. </type> <institution> 5206:TR:86, </institution> <year> 1986. </year>
Reference: [3] <author> Dijkstra, </author> <title> E.W., A discipline of programming, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1976. </year>
Reference: [4] <author> Hilbers, P.A.J., Lukkien, J.J.: </author> <title> Deadlock-free message routing in multicomputer networks. In: </title> <journal> Distributed Computing, </journal> <volume> Vol. 3, Nr. 4, </volume> <year> 1989. </year>
Reference-contexts: For our application we study different solutions. In the above algorithms we have used a type message consisting of a header and a contents. In process Router the entire contents is read before it is forwarded. This method is known as store-and-forward routing (see, for instance, <ref> [4, 9] </ref>). A much lower latency is obtained using cut-through or wormhole routing in which case a message is split up into packets of some fixed size. An algorithm for cut-through routing can be used instead of the above store-and-forward routing.
Reference: [5] <author> Hilbers, P.A.J., </author> <title> Processor networks and aspects of the mapping problem, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1991. </year>
Reference-contexts: (G I )) that satisfies s:m E (e) = m V (s:e) 2 Of course one could argue that through such a physical connection we can simulate a shared memory but this is one of the problems we are solving. 6 A discussion of this problem can be found in <ref> [5] </ref>. In this paper we assume that function m V is given. The problem that remains is the construction of m E and programs to implement it.
Reference: [6] <author> Hoare, </author> <title> C.A.R., Communicating Sequential Processes, </title> <journal> CACM, Vol.21(8), </journal> <volume> pp.666-677, </volume> <month> August </month> <year> 1978. </year>
Reference: [7] <author> Inmos ltd, </author> <title> Transputer Reference Manual, </title> <publisher> Prentice Hall, </publisher> <address> London, </address> <year> 1988. </year>
Reference-contexts: How this can be done can be found, for instance, in <ref> [7] </ref>. Notice that, if two processors communicate through shared memory only, we may use the implementation of the previous section to model a channel between the processors. We develop additional processes for routing messages from a source processor to a target processor.
Reference: [8] <author> JaJa, J., </author> <title> An introduction to parallel algorithms, </title> <publisher> Addison Wesley, </publisher> <address> Amsterdam. </address> <year> 1992. </year>
Reference: [9] <editor> Kumar, V., et al., </editor> <title> Introduction to parallel computing, </title> <publisher> Benjamin/Cummings publishing Company, </publisher> <address> Redwood City CA, </address> <year> 1994. </year>
Reference-contexts: For our application we study different solutions. In the above algorithms we have used a type message consisting of a header and a contents. In process Router the entire contents is read before it is forwarded. This method is known as store-and-forward routing (see, for instance, <ref> [4, 9] </ref>). A much lower latency is obtained using cut-through or wormhole routing in which case a message is split up into packets of some fixed size. An algorithm for cut-through routing can be used instead of the above store-and-forward routing.
Reference: [10] <author> Martin, A.J. </author> <title> An axiomatic definition of synchronization primitives, </title> <journal> Acta Informatica, </journal> <month> 16 </month> <year> (1981) </year> <month> 219-235. </month>
Reference-contexts: On the one hand, we regard it as a requirement for an implementation, i.e., an implementation satisfying this formalization is called a channel. On the other hand, given an implementation of communication actions we use the formalization to prove facts about programs. Our formalization is based on <ref> [10, 14] </ref>. From the introduction we obtain two requirements, viz. that input and output are synchronized and that together they implement a (distributed) assignment. These two can be regarded as safety requirements.
Reference: [11] <author> McColl, W.F., </author> <title> General purpose parallel computing, </title> <booktitle> Programming Research Group, </booktitle> <address> Oford University, </address> <month> april </month> <year> 1992. </year>
Reference-contexts: These developments have lead to the definition of models that are both general and admit an efficient implementation on a variety of architectures. One such a model is the Bulk Synchronous Computer (BSP, <ref> [11] </ref>). In this model, the parallel machine consists of a set of processors, each having a local memory. Each processor has access to all non-local memory in a uniform and efficient way. Each processor executes the same program, acting on local data.
Reference: [12] <author> MPI: </author> <title> a message passing interface standard, </title> <institution> University of Tennessee, Knoxville, </institution> <year> 1994. </year>
Reference-contexts: The importance of such a model is that it admits a reasonably simple performance analysis and that it can be implemented straightforwardly as we show in this paper. Other models are the ones provided by communication libraries like Parallel Virtual Machine (PVM, [13]) or Message Passing Interface (MPI, <ref> [12] </ref>). In these models a parallel program consists of a collection of communicating processes; the program is executed by a collection of processors. The library provides a variety of routines for process creation and for communication and synchronization between the processes.
Reference: [13] <author> Geist, A., et al., </author> <title> PVM 3 user's guide and reference manual, </title> <institution> Oak Ridge National Laboratory, </institution> <year> 1994. </year> <type> 16 [14] van de Snepscheut, </type> <institution> J.L.A., Martin, </institution> <month> A.J., </month> <title> Design of synchronization algorithms, </title> <editor> In: M. Broy (ed.), </editor> <booktitle> Constructive methods in computing science, NATO ASI series, </booktitle> <volume> Vol. </volume> <publisher> F55, Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year> <title> [15] van de Snepscheut, J.L.A., What computing is all about, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1993. </year> <title> [16] van de Snepscheut, </title> <editor> J.L.A (Editor), </editor> <booktitle> Mathematics of program construction, conference proceedings, LNCS 375, </booktitle> <address> Heidelberg, </address> <year> 1989. </year>
Reference-contexts: The importance of such a model is that it admits a reasonably simple performance analysis and that it can be implemented straightforwardly as we show in this paper. Other models are the ones provided by communication libraries like Parallel Virtual Machine (PVM, <ref> [13] </ref>) or Message Passing Interface (MPI, [12]). In these models a parallel program consists of a collection of communicating processes; the program is executed by a collection of processors. The library provides a variety of routines for process creation and for communication and synchronization between the processes.
References-found: 13

