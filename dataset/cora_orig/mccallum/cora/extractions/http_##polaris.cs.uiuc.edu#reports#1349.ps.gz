URL: http://polaris.cs.uiuc.edu/reports/1349.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Email: email: rwerger@csrd.uiuc.edu  
Phone: telephone: (217) 333-6578, fax: (217) 244-1351.  
Title: Parallelizing WHILE Loops for Multiprocessor Systems  
Author: Lawrence Rauchwerger and David Padua Lawrence Rauchwerger. 
Note: Corresponding Author:  Research supported in part by Army contract #DABT63-92-C-0033. This work is not necessarily representative of the positions or policies of the Army or the Government.  
Address: 1308 W. Main St., Urbana, IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: Current parallelizing compilers treat WHILE loops and DO loops with conditional exits as sequential constructs because their iteration space is unknown. Motivated by the fact that these types of loops arise frequently in practice, we have developed techniques that can be used to automatically transform them for parallel execution. We succeed in parallelizing loops involving linked lists traversals | something that has not been done before. This is an important problem since linked list traversals arise frequently in loops with irregular access patterns, such as sparse matrix computations. The methods can even be applied to loops whose data dependence relations cannot be analyzed at compile-time. We outline a cost/performance analysis that can be used to decide when the methods should be applied. Since, as we show, the expected speedups are significant, our conclusion is that they should almost always be applied | providing there is sufficient parallelism available in the original loop. We present experimental results on loops from the PERFECT Benchmarks and sparse matrix packages which substantiate our conclusion that these techniques can yield significant speedups. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Alliant Computer Systems Corporation, 42 Nagog Park, Acton, Massachusetts 01720. FX/Series Architecture Manual, </institution> <year> 1986. </year> <title> Part Number: </title> <publisher> 300-00001-B. </publisher>
Reference-contexts: This iteration must be found so that any iterations that need to be undone can be identified. On computers, such as the Alliant <ref> [1] </ref>, in which iterations are issued in order, the test L [vpn] &gt; i is unnecessary. In order to terminate the parallel loop cleanly before all iterations have been executed, a QUIT operation similar to the one on Alliant computers [1] could be used. <p> On computers, such as the Alliant <ref> [1] </ref>, in which iterations are issued in order, the test L [vpn] &gt; i is unnecessary. In order to terminate the parallel loop cleanly before all iterations have been executed, a QUIT operation similar to the one on Alliant computers [1] could be used. Once a QUIT command is issued by an iteration, all iterations with loop counters less than that of the issuing iteration will be initiated and completed, but no iterations with larger loop counters will be begun. <p> the cost of creating these copies is not too great, this technique should maximize the potential gains attainable from parallel execution, while, at the same time, minimizing the costs. 9 Experimental Results In this section we present experimental results obtained on a modestly parallel machine with 8 processors (Alliant FX/80 <ref> [1] </ref>) using a Fortran implementation of our methods. It should be pointed out that our results scale with the number of processors and the data size and that they should be extrapolated for MPPs, the actual target of our methods.
Reference: [2] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer. </publisher> <address> Boston, MA., </address> <year> 1988. </year> <month> 19 </month>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [19, 13, 2, 27, 30] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [3] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orzag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin. </author> <title> The PERFECT club benchmarks: Effective performance evaluation of supercomputers. </title> <type> Technical Report CSRD-827, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: We considered five WHILE loops that could not be parallelized by any compiler available to us; two loops are from the PERFECT Benchmarks <ref> [3] </ref>, two loops are from MA28, a sparse UN-symmetric linear solver [6], and one loop is extracted from MCSPARSE, a parallel version of a non-symmetric sparse linear systems solver [7, 8]. Our results are summarized in Table 2.
Reference: [4] <author> M. Burke, R. Cytron, J. Ferrante, and W. Hsieh. </author> <title> Automatic generation of nested, fork-join parallelism. </title> <journal> Journal of Supercomputing, </journal> <pages> pages 71-88, </pages> <year> 1989. </year>
Reference-contexts: In order to remove certain types of memory-related dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [4, 16, 18, 23, 24] </ref>). For example, the anti dependences between statement s6 in iteration i 1 and statement s4 in iteration i + 1 in the loop shown in Figure 5 (b) can be removed by privatizing the temporary variable tmp.
Reference: [5] <author> S. C. Chen, D. J. Kuck, and A. H. Sameh. </author> <title> Practical parallel band triangular solvers. </title> <journal> ACM Transaction on Mathematical Software, </journal> <volume> 4(1) </volume> <pages> 270-277, </pages> <month> September </month> <year> 1978. </year>
Reference-contexts: Although the concurrent evaluation of recurrences is in general not possible, some special cases lend themselves to either full or partial parallelization. There are parallel algorithms to solve simple inductions (the case of DO loops) [29] and associative recurrences <ref> [5, 14, 11, 12] </ref> but the evaluation of general recurrences has always been of a sequential nature.
Reference: [6] <author> I. S. Duff. </author> <title> Ma28- a set of fortran subroutines for sparse unsymmetric linear equations. </title> <type> Technical Report Report AERE R8730, </type> <address> HMSO, London, </address> <year> 1977. </year>
Reference-contexts: We considered five WHILE loops that could not be parallelized by any compiler available to us; two loops are from the PERFECT Benchmarks [3], two loops are from MA28, a sparse UN-symmetric linear solver <ref> [6] </ref>, and one loop is extracted from MCSPARSE, a parallel version of a non-symmetric sparse linear systems solver [7, 8]. Our results are summarized in Table 2. For each method applied to a loop, we give the speedup that was obtained, and, mention whether backups and time-stamping were necessary.
Reference: [7] <author> K. Gallivan, B. Marsolf, and H. Wijshoff. </author> <title> A large-grain parallel sparse system solver. </title> <booktitle> In Proc. Fourth SIAM Conf. on Parallel Proc. for Scient. Comp., </booktitle> <pages> pages 23-28, </pages> <address> Chicago, IL, </address> <year> 1989. </year>
Reference-contexts: five WHILE loops that could not be parallelized by any compiler available to us; two loops are from the PERFECT Benchmarks [3], two loops are from MA28, a sparse UN-symmetric linear solver [6], and one loop is extracted from MCSPARSE, a parallel version of a non-symmetric sparse linear systems solver <ref> [7, 8] </ref>. Our results are summarized in Table 2. For each method applied to a loop, we give the speedup that was obtained, and, mention whether backups and time-stamping were necessary. Whenever necessary, we performed a simple preventive backup of the variables potentially written in the loop.
Reference: [8] <author> K. A. Gallivan, B. A. Marsolf, and H. A. G. Wijshoff. </author> <title> MCSPARSE: A parallel sparse unsymmetric linear system solver. </title> <type> Technical Report CSRD Report No. 1142, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1991. </year>
Reference-contexts: five WHILE loops that could not be parallelized by any compiler available to us; two loops are from the PERFECT Benchmarks [3], two loops are from MA28, a sparse UN-symmetric linear solver [6], and one loop is extracted from MCSPARSE, a parallel version of a non-symmetric sparse linear systems solver <ref> [7, 8] </ref>. Our results are summarized in Table 2. For each method applied to a loop, we give the speedup that was obtained, and, mention whether backups and time-stamping were necessary. Whenever necessary, we performed a simple preventive backup of the variables potentially written in the loop.
Reference: [9] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantatative Approach. </title> <publisher> Morgan Kauffman, </publisher> <address> San Mateo,CA, </address> <year> 1990. </year>
Reference-contexts: One simple method, referred to as General-1, is to serialize the accesses to the next () operation. This technique is equivalent to hardware pipelinen-ing which has been well studied in the literature <ref> [9] </ref>. The cost of synchronization and the limited amount of parallelism make this scheme unattractive.
Reference: [10] <author> K. Kennedy and K. S. McKinley. </author> <title> Loop distribution with arbitrary control flow. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 407-416, </pages> <month> November </month> <year> 1990. </year>
Reference: [11] <author> C. Kruskal. </author> <title> Efficient parallel algorithms for graph problems. </title> <month> August </month> <year> 1985. </year>
Reference-contexts: Although the concurrent evaluation of recurrences is in general not possible, some special cases lend themselves to either full or partial parallelization. There are parallel algorithms to solve simple inductions (the case of DO loops) [29] and associative recurrences <ref> [5, 14, 11, 12] </ref> but the evaluation of general recurrences has always been of a sequential nature.
Reference: [12] <author> C. Kruskal. </author> <title> Efficient parallel algorithms for graph problems. </title> <address> pages 869-876, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Although the concurrent evaluation of recurrences is in general not possible, some special cases lend themselves to either full or partial parallelization. There are parallel algorithms to solve simple inductions (the case of DO loops) [29] and associative recurrences <ref> [5, 14, 11, 12] </ref> but the evaluation of general recurrences has always been of a sequential nature.
Reference: [13] <author> D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [19, 13, 2, 27, 30] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [14] <author> R. Ladner and M. Fisher. </author> <title> Parallel prefix computation. </title> <journal> J. ACM, </journal> <pages> pages 831-838, </pages> <year> 1980. </year>
Reference-contexts: Although the concurrent evaluation of recurrences is in general not possible, some special cases lend themselves to either full or partial parallelization. There are parallel algorithms to solve simple inductions (the case of DO loops) [29] and associative recurrences <ref> [5, 14, 11, 12] </ref> but the evaluation of general recurrences has always been of a sequential nature.
Reference: [15] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference: [16] <author> Zhiyuan Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 313-322, </pages> <year> 1992. </year>
Reference-contexts: In order to remove certain types of memory-related dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [4, 16, 18, 23, 24] </ref>). For example, the anti dependences between statement s6 in iteration i 1 and statement s4 in iteration i + 1 in the loop shown in Figure 5 (b) can be removed by privatizing the temporary variable tmp.
Reference: [17] <author> S. A. Mahlke, W. Y. Chen, W. W. Hwu, B. R. Rau, and M. S. Schl ansker. </author> <title> Sentinel scheduling for VLIW and superscalar processors. </title> <booktitle> In Proceedings of 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: However, in this case the compiler could predict the number of iterations using branch statistics, where the branch is on the termination condition of the WHILE loop. Although the application is different, this in not a new idea since branch speculation has been used effectively in superscalar compilers <ref> [17, 21, 22] </ref>. Since branch statistics have already been collected for many benchmarks, these collection mechanisms are available. 8 Strategies for Applying the Techniques In the previous section we discussed the speedups and potential slowdowns that can be expected when using our techniques for parallelizing WHILE loops.
Reference: [18] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In Proceedings 5th Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: In order to remove certain types of memory-related dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [4, 16, 18, 23, 24] </ref>). For example, the anti dependences between statement s6 in iteration i 1 and statement s4 in iteration i + 1 in the loop shown in Figure 5 (b) can be removed by privatizing the temporary variable tmp.
Reference: [19] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [19, 13, 2, 27, 30] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [20] <author> L. Rauchwerger and D. Padua. </author> <title> The privatizing doall test: A run-time technique for doall loop identification and array privatization. </title> <booktitle> In Proceedings of the 1994 International Conference on Supercomputing, </booktitle> <pages> pages 33-43, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Recently, we have proposed a run-time technique, called the PRIVATIZING DOALL test (PD test), for detecting the presence of cross-iteration dependences in a loop <ref> [20] </ref>. This test was originally developed to test at run-time whether a DO loop was fully parallel, i.e., whether it could be executed as a DOALL. <p> Before discussing how the PD test is used for WHILE loops, we first need to briefly describe the types of operations it performs, and the data structures it uses (see <ref> [20] </ref> for a complete description of the test). The PD test is applied to each shared variable referenced during the loop whose accesses cannot be analyzed at compile-time. For convenience, we discuss the test as applied to a shared array A.
Reference: [21] <author> M. D. Smith, M. S. Lam, and M. A. Horowitz. </author> <title> Boosting beyond static scheduling in a superscalar processor. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 344-354, </pages> <month> May </month> <year> 1990. </year> <month> 20 </month>
Reference-contexts: However, in this case the compiler could predict the number of iterations using branch statistics, where the branch is on the termination condition of the WHILE loop. Although the application is different, this in not a new idea since branch speculation has been used effectively in superscalar compilers <ref> [17, 21, 22] </ref>. Since branch statistics have already been collected for many benchmarks, these collection mechanisms are available. 8 Strategies for Applying the Techniques In the previous section we discussed the speedups and potential slowdowns that can be expected when using our techniques for parallelizing WHILE loops.
Reference: [22] <author> P. Tirumalai, M. Lee, and M. Schlansker. </author> <title> Parallelization of loops with exits on pipelined architectures. </title> <booktitle> In Supercomputing, </booktitle> <month> November </month> <year> 1990. </year>
Reference-contexts: However, in this case the compiler could predict the number of iterations using branch statistics, where the branch is on the termination condition of the WHILE loop. Although the application is different, this in not a new idea since branch speculation has been used effectively in superscalar compilers <ref> [17, 21, 22] </ref>. Since branch statistics have already been collected for many benchmarks, these collection mechanisms are available. 8 Strategies for Applying the Techniques In the previous section we discussed the speedups and potential slowdowns that can be expected when using our techniques for parallelizing WHILE loops. <p> Note that the available parallelism, and therefore our obtained speedup, is strongly dependent on the data input. 10 Related Work We can find in the literature several efforts in improving the performance of the WHILE loop execution. In <ref> [22] </ref> the authors have proposed some methods for achieving vector-like performance on multiple issue pipelined machines. They do not try to address the problem for large multiprocessors. Some techniques for solving certain types of recurrences in parallel were proposed by Harrison in [25] for Lisp-like languages.
Reference: [23] <author> P. Tu and D. Padua. </author> <title> Array privatization for shared and distributed memory machines. </title> <booktitle> In Proceedings 2nd Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Machines, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: In order to remove certain types of memory-related dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [4, 16, 18, 23, 24] </ref>). For example, the anti dependences between statement s6 in iteration i 1 and statement s4 in iteration i + 1 in the loop shown in Figure 5 (b) can be removed by privatizing the temporary variable tmp.
Reference: [24] <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proceedings 6th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: In order to remove certain types of memory-related dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [4, 16, 18, 23, 24] </ref>). For example, the anti dependences between statement s6 in iteration i 1 and statement s4 in iteration i + 1 in the loop shown in Figure 5 (b) can be removed by privatizing the temporary variable tmp.
Reference: [25] <author> III W. Ludwell Harrison. </author> <title> The Interprocedural Analysis and Automatic Parallelization of Scheme Programs. </title> <type> Technical Report 860, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Superco mputing Res. & Dev., </institution> <month> Feb., </month> <year> 1989. </year>
Reference-contexts: In [22] the authors have proposed some methods for achieving vector-like performance on multiple issue pipelined machines. They do not try to address the problem for large multiprocessors. Some techniques for solving certain types of recurrences in parallel were proposed by Harrison in <ref> [25] </ref> for Lisp-like languages. His main goal was to parallelize list operations (e.g., traversing a linked lists). Generally, his methods assume that the terminator is RI and it is known that there are no cross-iteration dependences in the loop.
Reference: [26] <author> III W. Ludwell Harrison. </author> <title> Compiling Lisp for Evaluation on a Tightly Coupled Multiproces sor. </title> <type> Technical Report 565, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Superco mputing Res. & Dev., </institution> <month> Mar. 20, </month> <year> 1986. </year>
Reference-contexts: These new parallel constructs could be called WHILE-DOALL, WHILE-DOACROSS, and WHILE-DOANY and could prove useful in the parallel programming (manual parallelization) of applications. 1 The methods described here extend previous works <ref> [26, 29] </ref> in that they: 1. can handle remainder variant termination conditions, 2. can test at run-time for cross-iteration data dependences in the remainder, 3. do not require work and storage for saving the values computed in the recurrence, 4. support both static and dynamic scheduling, and 5. present a comprehensive <p> We present some experimental results in Section 9. In Section 10 we discuss related work. 2 Transforming WHILE Loops for Parallel Execution WHILE loops have often been treated by parallelizing compilers as an intrinsically sequential constructs because their iteration space is unknown <ref> [26] </ref>. A related case which is generally also handled sequentially by compilers is the DO loop with a conditional exit. In this paper we propose techniques that can be used to execute such loops in parallel. <p> In fact, the author mentions that if the chunk sizes become too small, then the result might be an "inefficient restructured version of the loop that contains too little parallelism to recover the expense [invested]" <ref> [26] </ref>. We note that when the entire list resides in a single chunk (i.e., an array), then this method is equivalent to the method we describe in Section 3.2 for associative recurrences, i.e., loop distribution together with a parallel prefix computation to evaluate the dispatcher in parallel. <p> When the terminator is RI and it is known that there are no cross-iteration data dependences in the loop, they suggest using the naive form of loop distribution mentioned in Section 3.3 (also implicit in <ref> [26] </ref>), i.e., first a sequential WHILE loop evaluates the dispatcher and stores its values in an array, and then the loop iterations are performed in parallel using this array. 18 For the case of RV termination conditions no methods have been proposed in the past.
Reference: [27] <author> M. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Boston, MA, </address> <year> 1989. </year>
Reference-contexts: To aid our analysis of the dispatching recurrence, it is convenient to extract, at least conceptually, this recurrence from the original WHILE loop by distributing <ref> [27] </ref> the original loop into two DO loops with conditional exits: 1. A loop that evaluates the terms of the dispatcher (recurrence) and any termination condition that is strongly connected to the dispatcher. 2. <p> In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [19, 13, 2, 27, 30] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program. <p> The only previous work of which we are aware (except some early work by <ref> [27] </ref>) for parallelizing WHILE loops in languages such as FORTRAN for multiprocessors is due to Wu and Lewis [29]. One method they propose is to pipeline the loop by executing it in DOACROSS fashion, and to enforce any cross-iteration data dependences with explicit synchronization operations.
Reference: [28] <author> M. Wolfe. Doany: </author> <title> Not just another parallel loop. </title> <booktitle> In Proceedings 5th Annual Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <volume> volume 757. </volume> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: In other words the program is designed to be insensitive to the order in which the columns and rows of the matrix are searched for the pivot. Originally, only the row search was paralellized by applying a technique equivalent to a DOANY construct <ref> [28] </ref>, leaving the traversal of columns in a sequential WHILE loop. We fused the two loops, effectively implementing a new WHILE DOANY parallel construct. Through this technique we were able to parallelize the pivot search across the whole matrix.
Reference: [29] <author> Youfeng Wu and Ted G. Lewis. </author> <title> Parallelizing while loops. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, volume II, Software, </booktitle> <pages> pages 1-8, </pages> <year> 1990. </year>
Reference-contexts: Although the concurrent evaluation of recurrences is in general not possible, some special cases lend themselves to either full or partial parallelization. There are parallel algorithms to solve simple inductions (the case of DO loops) <ref> [29] </ref> and associative recurrences [5, 14, 11, 12] but the evaluation of general recurrences has always been of a sequential nature. <p> These new parallel constructs could be called WHILE-DOALL, WHILE-DOACROSS, and WHILE-DOANY and could prove useful in the parallel programming (manual parallelization) of applications. 1 The methods described here extend previous works <ref> [26, 29] </ref> in that they: 1. can handle remainder variant termination conditions, 2. can test at run-time for cross-iteration data dependences in the remainder, 3. do not require work and storage for saving the values computed in the recurrence, 4. support both static and dynamic scheduling, and 5. present a comprehensive <p> The only previous work of which we are aware (except some early work by [27]) for parallelizing WHILE loops in languages such as FORTRAN for multiprocessors is due to Wu and Lewis <ref> [29] </ref>. One method they propose is to pipeline the loop by executing it in DOACROSS fashion, and to enforce any cross-iteration data dependences with explicit synchronization operations.
Reference: [30] <author> H. Zima. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <year> 1991. </year> <pages> 21 22 23 </pages>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [19, 13, 2, 27, 30] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
References-found: 30

