URL: ftp://ftp.cs.columbia.edu/reports/reports-1995/cucs-001-95.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1995.html
Root-URL: http://www.cs.columbia.edu
Email: sayan@cs.columbia.edu nayar@cs.columbia.edu  
Phone: 2  
Title: Automatic Generation of RBF Networks  
Author: Shayan Mukherjee and Shree K. Nayar 
Web: 76-92-C-0007.  
Note: 1 This research was conducted at the Center for Research in  It was supported in part by the David and Lucile Packard Fellowship and in part by ARPA Contract No. DACA  
Date: November, 1994  
Address: New York, NY 10027 USA  New York, N.Y. 10027  
Affiliation: Department of Computer Science Columbia University  Intelligent Systems, Department of Computer Science, Columbia University.  Department of Applied Mathematics and Physics, Columbia University,  
Pubnum: CUCS-001-95  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P. Baldi, </author> <title> "Computing with Arrays of Bell-Shaped and Sigmoidal Functions," </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <editor> R.P. Lippman, J.E. Moody, and D.S. Touret-zky (ed.), </editor> <publisher> Morgan Kaufman, </publisher> <pages> pp. 735-742, </pages> <year> 1991. </year>
Reference-contexts: Therefore, the approximating function can easily be cast into a neural network called RBF networks [12]. These networks have been shown to be universal approximators, theoretically capable of approximating any function to a reasonable degree of precision <ref> [1] </ref> with only one layer of basis functions. Networks with sigmoidal basis functions are universal approximators as well, however, they require two layers of basis functions.
Reference: [2] <author> M. D. Buhmann and C. A. Micchelli, </author> <title> "Spline Prewavelets for Non-uniform Knots," </title> <journal> Numerische Mathematik, </journal> <volume> Vol. 61, </volume> <pages> pp. 455-474, </pages> <year> 1992. </year>
Reference: [3] <author> M. D. Buhmann and C. A. Micchelli, </author> <title> "Spline Prewavelets for Non-uniform Knots," </title> <journal> Numerische Mathematik, </journal> <volume> Vol. 61, </volume> <pages> pp. 455-474, </pages> <year> 1992. </year>
Reference-contexts: A discussion of what approximations are introduced due to this oversampling assumption is beyond the scope of this paper. A rigorous approach to performing wavelet transforms on non-uniform data has been developed by Buhmann and Micchelli <ref> [3] </ref> [4]. We have not used their approach for two reasons. First, extending their approach to multidimensional problems results in a tremendous number of computations. Secondly, they introduce an extra spline space of radial basis functions called prewavelets to perform the transform.
Reference: [4] <author> M. D. Buhmann, </author> <title> "Multiquadratic Prewavelets on Non-Equally Spaced Centres," </title> <type> Technical Report 94-06, </type> <institution> Seminar fur Angewandte Mathematik Eidgenossische Technische Hochshule, </institution> <month> July </month> <year> 1994. </year> <month> 24 </month>
Reference-contexts: A discussion of what approximations are introduced due to this oversampling assumption is beyond the scope of this paper. A rigorous approach to performing wavelet transforms on non-uniform data has been developed by Buhmann and Micchelli [3] <ref> [4] </ref>. We have not used their approach for two reasons. First, extending their approach to multidimensional problems results in a tremendous number of computations. Secondly, they introduce an extra spline space of radial basis functions called prewavelets to perform the transform.
Reference: [5] <author> C. K. Chui, </author> <title> An Introduction to Wavelets, </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1992. </year>
Reference-contexts: Parseval's theorem can then be used to calculate the number of basis functions required for high performance. A similar approach is employed here to set the parameters of the RBF network. Instead of the Fourier transform, the integral wavelet transform is used <ref> [5] </ref>. The basis functions for the integral wavelet transform are orthonormal and local. These local orthonormal bases can be 1 constructed from the set of radial basis functions. Once the basis function is chosen, the integral wavelet transform is applied to the training data. <p> Furthermore, the use of optimization methods makes this process cumbersome and time consuming. In the next section we introduce a technique for determining network parameters that avoids these shortcomings. 3 Integral Wavelet Transforms and RBF Networks In this section, the use of an integral wavelet transform <ref> [5] </ref> to set the parameters of a RBF network is developed. It shall be shown that by using the transform, the number of basis functions required for a given mapping and error bound can be determined analytically. <p> Using the IWT we can construct orthonormal basis functions that are localized in space and then decompose a function in terms of these bases. For the case of 1-dimensional functions, R 1 7! R 1 . The IWT has the basic form (see <ref> [5] </ref>) : (T f)(b; a) = 1 6 where b;a (x) = jaj 1=2 ( a are the bases called wavelets. <p> The result is a set of orthonormal bases : &lt; j;k ; l;m &gt;= ffi j;l ffi k;m : An approximation to a function f (x) exists at different resolution levels. At the resolution level, j, the approximating function has the form <ref> [5] </ref>: f j (x) = k X d j (k) (2 j x k): (14) From the above, it can be shown that the exact representation of the function f (x) has the form [5]: f (x) = j k where : d j (k) =&lt; f (k); (2 j x <p> At the resolution level, j, the approximating function has the form <ref> [5] </ref>: f j (x) = k X d j (k) (2 j x k): (14) From the above, it can be shown that the exact representation of the function f (x) has the form [5]: f (x) = j k where : d j (k) =&lt; f (k); (2 j x k) &gt; : (16) The decomposition in equation (16) is equivalent to a weighted sum of scaling functions at the finest resolution level: f (x) = k=1 The wavelet transform allows us to create
Reference: [6] <author> R. A. Devore, B. Jawerth, and B. J. Lucier, </author> <title> "Image Compression Through Wavelet Transform Coding," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 38, No. 2, </volume> <pages> pp. 719-746, </pages> <year> 1992. </year>
Reference-contexts: It has been shown that the ff class and Besov spaces are closely related <ref> [6] </ref> : 2 when the error function is L 2 . B ff 2 is a Besov space. In addition, the relation between Besov spaces and continuity conditions of the class of functions in the space have been explored in approximation theory.
Reference: [7] <author> R. A. Devore, B. Jawerth, and V. A. Popov, </author> <title> "Compression of Wavelet Decompositions" American Journal of Math, </title> <year> 1992. </year>
Reference-contexts: The question now arises, which representation of f (x) is better ? Approximation spaces such as the Besov space offer possibilities <ref> [7] </ref> to answering the above question. For the representation ~ f (x) an error function : ~a N ( ~ f ) = jj ~ f (x) ~ f 0 (x)jj 2 is defined where ~ f 0 (x) is a decomposition of ~ f (x) into N orthonormal basis.
Reference: [8] <author> N. R. Lomb, </author> <title> "Least-Squares Frequency Analysis of Unequally Spaced Data," </title> <booktitle> Astrophysics and Space Science, </booktitle> <volume> Vol. 39, </volume> <pages> pp. 447-462, </pages> <year> 1976. </year>
Reference-contexts: A solution to this problem requires an extension to Mallat's fast wavelet algorithm [9]. Spectral analysis of unevenly sampled data has been explored by astronomers. The most frequently used method is the Lomb periodogram <ref> [8] </ref>. We use it to help define the operation in equation (20). The periodogram performs a least-mean-square fit of the data to sines and cosines under the assumption that the error in the fit decreases with the addition of frequency terms.
Reference: [9] <author> S. G. Mallat, </author> <title> "A Theory for Multiresolution Signal Decomposition: The Wavelet Representation," </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 11, No. 7, </volume> <pages> pp. 674-693, </pages> <year> 1989. </year>
Reference-contexts: So a Discrete Integral Wavelet Transform <ref> [9] </ref> is implemented. The input space is discretized into 2 J bins, where J typically ranges from 9 to 11. The f (x i ) values are then placed into the appropriate bin by rounding off x i . <p> Since the bins are small, we assume that any error introduced from the rounding off is negligible. The next step involves calculating the transform coefficients, d j;k , for the wavelet bases, j;k . A variation of Mallat's fast wavelet algorithm <ref> [9] </ref> is used to calculate these coefficients. We extend Mallat's algorithm to the case of unevenly sampled discrete signals. This algorithm is pyramidal in nature [9] : c j1 (k) = [ v flc j ] #2 (k) (18) ffi # 2 denotes downsampling by two, keep every other term. <p> The next step involves calculating the transform coefficients, d j;k , for the wavelet bases, j;k . A variation of Mallat's fast wavelet algorithm <ref> [9] </ref> is used to calculate these coefficients. We extend Mallat's algorithm to the case of unevenly sampled discrete signals. This algorithm is pyramidal in nature [9] : c j1 (k) = [ v flc j ] #2 (k) (18) ffi # 2 denotes downsampling by two, keep every other term. The c 0 values correspond to the signal at the finest resolution level, the discrete function f (k). <p> The c 0 values correspond to the signal at the finest resolution level, the discrete function f (k). The formulas for ffi ffi for the Battle-Lamarie basis <ref> [9] </ref> constructed from cubic fi-splines are given in Appendix B. This algorithm is iterated for j = 0 to 1 J , where j = 0 is the finest resolution level and j = 1 J is the coarsest level. <p> This brings up the question of how to implement: a (x) = f (x) fl g (x) (20) when f (x) is an unevenly sampled discrete signal and g (x) is a continuous function. A solution to this problem requires an extension to Mallat's fast wavelet algorithm <ref> [9] </ref>. Spectral analysis of unevenly sampled data has been explored by astronomers. The most frequently used method is the Lomb periodogram [8]. We use it to help define the operation in equation (20). <p> In the fast wavelet algorithm <ref> [9] </ref>, the above technique can be used to perform the convolutions in equations (18) and (19). The unevenly sampled c j (k) is replaced by the continuous c 0 j (x) and C j (!) = C 0 j (!). <p> This is the rationale for using cubic fi-splines as a basis functions in our RBF networks. 23 9 Appendix B There exist several ways to construct wavelet bases using fi-splines as scaling functions. The wavelet basis used in our work was the Battle-Lamarie orthonormal basis <ref> [9] </ref>. The scaling functions are cubic fi-splines : fi 3 (x) = j=0 3! j ) (x + 2 j) 3 (x + 2 j) where is the unit step function.
Reference: [10] <author> C. A. Micchelli, </author> <title> "Interpolation of Scattered Data: Distance Matrices and Conditionally Positive Definite Functions," Constructive Approximation, </title> <journal> Vol. </journal> <volume> 2, </volume> <pages> pp. 11-22, </pages> <year> 1986. </year>
Reference-contexts: If G is a positive definite matrix then it is invertible. Two theorems by Micchelli <ref> [10] </ref> exploit this property to impose sufficient conditions for the basis functions. <p> A wavelet basis derived using using fi splines as scaling functions approach the Gabor function as n 7! 1 : fi n (x) ~ = 6 e 6x 2 This approximation of fi n (x) satisfies both of Micchelli's conditions <ref> [10] </ref>. The approximation is asymptotic and can be shown to be highly accurate even for as low an order as n = 3.
Reference: [11] <author> H. Murase and S. K. Nayar, </author> <title> "Learning and Recognition of 3D Objects from Appearance," </title> <booktitle> Proc. of IEEE Workshop on Qualitative Vision, </booktitle> <pages> pp. 39-50, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Next, the transform approach was applied to a high dimensional problem in computer vision, namely, recognizing an object and estimating its pose. The result is a network that maps the projection of an input image in an n-dimensional subspace, called the eigenspace <ref> [11] </ref>, to an object number and object pose. The effectiveness of our approach in terms of time to determine network parameters, time to perform the mapping, accuracy of the mapping, and memory required to store the network are examined. The network's performance is shown to be favorable. <p> The experiments involve a network that recognizes an object and estimates its pose in a scene. Murase and Nayar <ref> [11] </ref> developed a system that uses principal component analysis to a create a compact representation of object appearance parameterized by pose. Since our network is used to enhance the performance of the methodology proposed by Murase and Nayar, a brief overview of their work is in order. <p> This results in a single point in eigenspace. The projections of all images of an object results in a set of points (corresponding to the different discrete poses), that is referred to as a discrete manifold. In <ref> [11] </ref>, the discrete manifold is interpolated using biquadratic splines to obtain a continuous manifold that is parameterized by object pose. The manifold is densely resampled to get a large number of manifold points. This large point set represents that object's appearance model. <p> Given a novel object image, the object region is segmented, normalized in scale and projected to universal eigenspace. The closest manifold determines the identity of the object in the image and the exact position of the new projection on the manifold yields the pose of the object. In <ref> [11] </ref>, the closest manifold point is determined either by exhaustive search (which is inefficient in time and memory) or by binary search (which is inefficient in memory). <p> In our experiments, an optimal network was constructed for each of the 20 objects used by Nayar and Murase <ref> [11] </ref> (see Figure 6). The input space, a 15-dimensional eigenspace, was discretized into 1024 boxes in each of its dimensions. The networks ability to learn and generalize examples presented to it was tested using two data sets.
Reference: [12] <author> T. Poggio and F. Girosi, </author> <title> "Networks for Approximation and Learning," </title> <journal> Proc. IEEE, </journal> <volume> Vol. 78, </volume> <pages> pp. 1481-1497, </pages> <year> 1990. </year>
Reference-contexts: In this context, learning is equivalent to generating a continuous function that approximates the given data. A rigorous formulation of this approximating function results in a weighted sum of radial basis functions (RBF) <ref> [12] </ref>. Therefore, the approximating function can easily be cast into a neural network called RBF networks [12]. These networks have been shown to be universal approximators, theoretically capable of approximating any function to a reasonable degree of precision [1] with only one layer of basis functions. <p> In this context, learning is equivalent to generating a continuous function that approximates the given data. A rigorous formulation of this approximating function results in a weighted sum of radial basis functions (RBF) <ref> [12] </ref>. Therefore, the approximating function can easily be cast into a neural network called RBF networks [12]. These networks have been shown to be universal approximators, theoretically capable of approximating any function to a reasonable degree of precision [1] with only one layer of basis functions. Networks with sigmoidal basis functions are universal approximators as well, however, they require two layers of basis functions. <p> How RBF networks relate to the problem of input-output mapping, the formulation of a network, and short-comings in this formulation are described in this section. The first part of this section is a summary of Poggio and Girosi <ref> [12] </ref>. Learning a mapping between an input and output space is often viewed as determin-ing a function that performs the mapping. <p> For this reason, the approximation problem is ill-posed for discrete data, the data does not contain sufficient information for a unique mapping. The approximation problem is made well-posed by introducing apriori assumptions about the mapping. Normally the assumptions pertain to the smoothness of the mapping. Regularization techniques <ref> [12] </ref> are invoked to introduce smoothness constraints into the approximation problem. <p> If G is a positive definite matrix then it is invertible. Two theorems by Micchelli [10] exploit this property to impose sufficient conditions for the basis functions. The following are a few basis functions that satisfy Micchelli's conditions <ref> [12] </ref> : G (r) = e r 2 = 2 (c 2 +r 2 ) ff ff &gt; 0 G (r) = r 4 where, r = jjx x i jj. <p> Once the type of basis function has been selected, the approximating function : F (W; x) = i=1 is easily cast as a network. The RBF <ref> [12] </ref> network has three layers, each fully connected to the next layer (see Figure 1). The first layer consists of a single input unit, the vector x. The second layer is composed of the series of multidimensional radial basis functions G (x; x i ). <p> Due to the large number of data points in most practical applications, the speed of the mapping becomes a serious limitation. This problem can be avoided by using fewer basis functions than data points <ref> [12] </ref>. The approximating function, however, is no longer an exact representation of f (x) and the approximation gets worse as the number of basis functions is reduced. <p> In the above algorithm, the spans of the basis functions were equal and not allowed to vary as it took too much time to search the parameter spaces corresponding to both position and span. The above algorithm is commonly used to set RBF network parameters <ref> [12] </ref>. The accuracy of the optimal and conventional networks as a function of the number of bases, for both functions, is shown in Figure (3). For both functions, the error functional decays faster for the optimal network. In the case of the sine function the improvement is negligible.
Reference: [13] <author> M. Unser, A. Aldroubi, M. Eden, </author> <title> "On the Asymptotic Convergence of fi-spline Wavelets to Gabor Functions," </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> Vol. 38, </volume> <pages> pp. 864-872, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The assumptions made in using the periodogram and the errors introduced in going between continuous and discrete representations need to be explored. In future research, we hope to address the issues raised in these sections. 8 Appendix A TheoremA:1 (U nser; Aldroubi; Eden 1992 <ref> [13] </ref>) : Functions of the form fi n (x), fi-splines, tend to Gaussians as the order of the spline tends to infinity.
Reference: [14] <author> M. Unser, A. Aldroubi, and M. Eden, </author> <title> "A Family of Polynomial Spline Wavelet Transforms," </title> <booktitle> Signal Processing, </booktitle> <volume> Vol. 30, </volume> <pages> pp. 141-162, </pages> <year> 1993. </year>
Reference-contexts: The filter formulas, ffi ffi w (k), that were used in section 3.2 to calculate the wavelet coefficients, d j;k , are given in Tables 3 and 4. These formulas were calculated by Unser, Aldroubi, and Eden <ref> [14] </ref> [15]. filter frequency response ffi 2 U 3 r 1 (f) ffi 2 e 2f U 3 2 ) B 7 2 ) 1 (2f) Table 3: The filter formulas for the transfer functions ffi ffi function frequency response B 7 5040 (2416 + 1191 [e 2f + e 2f
Reference: [15] <author> M. Unser, A. Aldroubi, M. Eden, </author> <title> "The L 2 Polynomial Spline Pyramid," </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 15, No. 4, </volume> <pages> pp. 364-378, </pages> <month> April </month> <year> 1993. </year> <month> 25 </month>
Reference-contexts: The filter formulas, ffi ffi w (k), that were used in section 3.2 to calculate the wavelet coefficients, d j;k , are given in Tables 3 and 4. These formulas were calculated by Unser, Aldroubi, and Eden [14] <ref> [15] </ref>. filter frequency response ffi 2 U 3 r 1 (f) ffi 2 e 2f U 3 2 ) B 7 2 ) 1 (2f) Table 3: The filter formulas for the transfer functions ffi ffi function frequency response B 7 5040 (2416 + 1191 [e 2f + e 2f ]
References-found: 15

