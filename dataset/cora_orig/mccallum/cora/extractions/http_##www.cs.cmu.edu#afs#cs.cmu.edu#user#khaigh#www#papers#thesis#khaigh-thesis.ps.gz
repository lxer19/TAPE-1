URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/khaigh/www/papers/thesis/khaigh-thesis.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/khaigh/www/papers/thesis/thesis.abstract.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Situation-Dependent Learning for Interleaved Planning and Robot Execution  
Author: Karen Zita Haigh Manuela M. Veloso, Chair Tom Mitchell Reid Simmons R. James Firby (Neodesic Corporation) 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy. Thesis Committee:  
Note: Copyright c 1998 Karen Zita Haigh  
Address: Pittsburgh, PA 15213-3891  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: February 1998  
Pubnum: CMU-CS-98-108  
Abstract: This research is sponsored in part by (1) the National Science Foundation under Grant No. IRI-9502548, (2) by the Defense Advanced Research Projects Agency (DARPA), and Rome Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-95-1-0018, (3) the Natural Sciences and Engineering Council of Canada (NSERC), and (4) the Canadian Space Agency (CSA). The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the NSF, DARPA, Rome Laboratory, the U.S. Government, NSERC or the CSA. 
Abstract-found: 1
Intro-found: 1
Reference: [Ambros-Ingerson & Steel, 1988] <author> Jose A. </author> <title> Ambros-Ingerson and Sam Steel (1988). Integrating planning, execution and monitoring. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI-88), </booktitle> <pages> pages 83-88, </pages> <address> St. Paul, MN. (Menlo Park, CA: </address> <publisher> AAAI Press). </publisher>
Reference: [Atkins et al., 1996] <author> Ella M. Atkins, Edmund H. Durfee, and Kang G. </author> <title> Shin (1996). Detecting and reacting to unplanned-for world states. </title> <booktitle> In Papers from the 1996 AAAI Fall Symposium "Plan Execution: Problems and Issues", </booktitle> <pages> pages 1-7, </pages> <address> Boston, MA. (Menlo Park, CA: </address> <publisher> AAAI Press). </publisher>
Reference-contexts: Conditional planning is one approach that aims at considering in the domain model all the possible contingencies of the world and plan ahead for each individual one <ref> [Atkins et al., 1996; Mansell, 1993; Pryor, 1994; Schoppers, 1989] </ref>. In most complex environments, the large number of possible contingencies means that complete conditional planning becomes infeasible, but may nevertheless be appropriate in particularly dangerous domains.
Reference: [Baird, 1995] <author> Leemon C. </author> <title> Baird (1995). Residual algorithms: Reinforcement learning with function approximation. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference (ICML95), </booktitle> <pages> pages 30-37, </pages> <address> Tahoe City, CA. (San Mateo, CA: </address> <publisher> Morgan Kaufmann). </publisher>
Reference-contexts: This approach can be viewed as learning the integral of action costs. However, most Reinforcement Learning techniques are unable to generalize learned information, and as a result, they have only been used in small domains. Recently, several research have been exploring techniques for allowing generalization in Reinforcement Learning <ref> [Baird, 1995; Boyan & Moore, 1995; McCallum, 1995] </ref>. Essentially, these systems replace Reinforcement Learning's standard table-lookup mechanism with alternative function approximation techniques, such as decision trees or neural networks. Experimentally, these algorithms seem to produce reasonable policies.
Reference: [Baroglio et al., 1996] <author> C. Baroglio, A. Giordana, M. Kaiser, M. Nuttin, and R. </author> <month> Piola </month> <year> (1996). </year> <title> Learning controllers for industrial robots. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 221-249. </pages>
Reference-contexts: Common applications include map learning and localization (e.g. [Koenig & Simmons, 1996; Kortenkamp & Weymouth, 1994; Thrun, 1996]), or learning operational parameters for better actuator control (e.g. <ref> [Baroglio et al., 1996; Bennett & DeJong, 1996; Pomerleau, 1993] </ref>). Instead of improving low-level actuator control, our work focusses instead at the planning stages of the system. In this section, we describe some of the work related to our learning approach.
Reference: [Becker et al., 1988] <author> Richard A. Becker, John M. Chambers, and Allan R. </author> <title> Wilks (1988). The New S Language. </title> <address> (Pacific Grove, CA: </address> <note> Wadsworth & Brooks/Cole). Code available from http://www.mathsoft.com/splus/. </note>
Reference-contexts: Other learning mechanisms may be appropriate in different robot architectures with different data representations. We selected an off-the-shelf package, namely S-PLUS <ref> [Becker et al., 1988] </ref>, as the regression tree implementation. A regression tree is created for each event, in which features are splits and costs are learned values.
Reference: [Bennett & DeJong, 1996] <author> Scott W. Bennett and Gerald F. </author> <title> DeJong (1996). Real-world robotics: Learning to plan for robust execution. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 121-161. </pages>
Reference-contexts: Common applications include map learning and localization (e.g. [Koenig & Simmons, 1996; Kortenkamp & Weymouth, 1994; Thrun, 1996]), or learning operational parameters for better actuator control (e.g. <ref> [Baroglio et al., 1996; Bennett & DeJong, 1996; Pomerleau, 1993] </ref>). Instead of improving low-level actuator control, our work focusses instead at the planning stages of the system. In this section, we describe some of the work related to our learning approach.
Reference: [Blythe, 1994] <author> Jim Blythe (1994). </author> <title> Planning with external events. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 94-101, </pages> <address> Seattle, WA. (San Mateo, CA: </address> <publisher> Morgan Kaufmann). </publisher>
Reference-contexts: A network routing planner may select one route when congestion is high, and another otherwise. Learning operator probabilities for probabilistic or conditional planners, such as for Weaver <ref> [Blythe, 1994] </ref> or U-PLAN [Mansell, 1993], or Xavier's navigation module.
Reference: [Bonasso & Kortenkamp, 1996] <author> R. Peter Bonasso and David Kortenkamp (1996). </author> <title> Using a layered control architecture to alleviate planning with incomplete information. </title> <booktitle> In Proceedings of the AAAI Spring Symposium "Planning with Incomplete Information for Robot Problems", </booktitle> <pages> pages 1-4, </pages> <address> Stanford, CA. (Menlo Park, CA: </address> <publisher> AAAI Press). </publisher>
Reference-contexts: Once the important request has been fulfilled, Rogue reactivates the less important task (s). Some systems respond to asynchronous goals by restarting the planner, losing planning effort as well as placing high demands on sensing to determine the current status of the environment and interrupted tasks <ref> [Pell et al., 1997; Bonasso & Kortenkamp, 1996] </ref>. Rogue, however, suspends and reactivates tasks efficiently, without losing any of the prior planning information. It monitors the environment to identify unexpected changes, such as side-effects and exogenous events, that can affect the validity and applicability of plans. <p> how the prodigy4.0 planner and the TCA approach in Xavier are in fact robust architectures.) Strictly looking at Rogue only from the viewpoint of the integration of planning and execution, Rogue compares well with other integrated planning and execution systems such as NMRA [Pell et al., 1997] and 3 T <ref> [Bonasso & Kortenkamp, 1996] </ref> (Section 5.1 discusses this comparison in more detail). Given the general-purpose character of the prodigy4.0 planner, Rogue could easily be applied to other executing platforms and tasks by a flexible change of prodigy4.0's specification of the domain. <p> Rogue however abstracts much of the lower level details that PRS explicitly reasons about, meaning that Rogue can be seen as more reliable and efficient because system functionality is suitably partitioned [Pell et al., 1997; Simmons et al., 1997]. NMRA [Pell et al., 1997] and 3 T <ref> [Bonasso & Kortenkamp, 1996] </ref> both function in domains with many asynchronous goals, but both planners respond to new goals and serious action failures by abandoning existing planning and restarting the planner. <p> and reactions to be controlled by a deliberative system. raps and TCA have been used as the underlying control mechanism on a variety of robots, from indoor mobile robots [Gat, 1992; Simmons et al., 1997] to outdoor legged robots [Simmons, 1991] to planetary rovers [Krotkov et al., 1995] and spacecraft <ref> [Bonasso & Kortenkamp, 1996] </ref>. TCA provides facilities for scheduling and synchronizing tasks, resource allocation, environment monitoring and exception handling. raps allow you to specify the methods and context for actions, and can therefore be constructed to provide the same facilities as TCA.
Reference: [Borrajo & Veloso, 1994] <author> Daniel Borrajo and Manuela Veloso (1994). </author> <title> Incremental learning of control knowledge for improvement of planning efficiency and plan quality. </title> <booktitle> In Working 165 166 BIBLIOGRAPHY notes from the AAAI Fall Symposium "Planning and Learning: On to Real Applications", </booktitle> <pages> pages 5-9, </pages> <address> New Orleans, LA. </address>
Reference-contexts: It relies on a comparison of pairs of complex plans to learn control rules that bias the planner towards the higher quality plan. New learned knowledge overrides previous knowledge, but noise is not accounted for. hamlet <ref> [Borrajo & Veloso, 1994] </ref> learns control rules that improve planning efficiency and the quality of plans generated. It assumes that all operators have equivalent cost.
Reference: [Boyan & Moore, 1995] <author> Justin A. Boyan and Andrew W. </author> <title> Moore (1995). Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 369-76, </pages> <address> Cambridge, MA. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: This approach can be viewed as learning the integral of action costs. However, most Reinforcement Learning techniques are unable to generalize learned information, and as a result, they have only been used in small domains. Recently, several research have been exploring techniques for allowing generalization in Reinforcement Learning <ref> [Baird, 1995; Boyan & Moore, 1995; McCallum, 1995] </ref>. Essentially, these systems replace Reinforcement Learning's standard table-lookup mechanism with alternative function approximation techniques, such as decision trees or neural networks. Experimentally, these algorithms seem to produce reasonable policies.
Reference: [Breiman et al., 1984] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. </author> <title> Stone (1984). Classification and Regression Trees. </title> <address> (Pacific Grove, CA: </address> <publisher> Wadsworth & Brooks/Cole). </publisher>
Reference-contexts: The input to the algorithm is the events matrix described in Section 3.4. The desired output is situation-dependent knowledge in a form that can be used by the planner. We selected regression trees <ref> [Breiman et al., 1984] </ref> as our learning mechanism because * the data often contains disjunctive descriptions, * the data may contain irrelevant features, * the data might be sparse, especially for certain features, * the learned costs are continuous values.
Reference: [Carbonell et al., 1992] <author> Jaime G. Carbonell, </author> <title> and the prodigy Research Group (1992). prodigy4.0: The manual and tutorial. </title> <type> Technical Report CMU-CS-92-150, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: Control rules are if-then rules that indicate which choices should be made (or avoided) depending on the current state and other meta-level information. In particular, control rules can select, prefer or reject specific planning choices at every decision point <ref> [Carbonell et al., 1992] </ref>. Control rules can be used to focus planning on particular goals and towards desirable plans. Rogue primarily uses two types of control rules: those that control goal decisions, and those that control applicable operator decisions.
Reference: [Carbonell et al., 1990] <author> Jaime G. Carbonell, Craig A. Knoblock, and Steven Minton (1990). </author> <title> Prodigy: An integrated architecture for planning and learning. </title> <editor> In K. VanLehn, editor, </editor> <booktitle> Architectures for Intelligence. </booktitle> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ. </address> <note> Also available as Technical Report CMU-CS-89-189, </note> <institution> Computer Science Department, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: Deliberative 17 18 CHAPTER 2. THE TASK PLANNER behaviours include vision, occupancy grids and topological maps, and path planning and global navigation. prodigy is a domain-independent planner that serves as a testbed for machine learning research <ref> [Carbonell et al., 1990; Veloso et al., 1995] </ref>. The current implementation, prod-igy4.0, is a nonlinear planner that follows a state-space search guided by means-ends analysis and backward chaining. It reasons about multiple goals and multiple alternative operators to achieve the goals.
Reference: [Cassandra et al., 1994] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. </author> <title> Littman (1994). Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> pages 1023-1028, </pages> <address> Seattle, WA. (Menlo Park, CA: </address> <publisher> AAAI Press). </publisher>
Reference-contexts: very high, and so the path planner determines that the longer path might take less time on average. 1 It is infeasible to determine optimal POMDP solutions given our real-time constraints and the size of our state spaces (over 3000 states for the map shown in Figure 1.4, page 11) <ref> [Cassandra et al., 1994; Lovejoy, 1991] </ref>. Reasoning about blockage probabilities and recovery costs is also notably easier in the topological map. 54 CHAPTER 3. LEARNING FOR THE PATH PLANNER get trapped in the dead end. Reproduced from Simmons et al. [1997].
Reference: [Chambers & Hastie, 1992] <author> John M. Chambers and Trevor Hastie (1992). </author> <title> Statistical models in S. </title> <address> (Pacific Grove, CA: </address> <publisher> Wadsworth & Brooks/Cole). </publisher>
Reference: [Dean et al., 1990] <author> Thomas Dean, Kenneth Basye, Robert Chekaluk, Seungseok Hyun, Moi-ses Lejter, and Margaret Randazza (1990). </author> <title> Coping with uncertainty in a control system for navigation and exploration. </title> <booktitle> In Proceedings of the Eigth National Conference on Artificial Intelligence (AAAI-90), </booktitle> <pages> pages 1010-1015, </pages> <address> Boston, MA. (Cambridge, MA: </address> <publisher> MIT Press). </publisher>
Reference: [Dean & Boddy, 1988] <author> Thomas L. Dean and Mark Boddy (1988). </author> <title> An analysis of time-dependent planning. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI-88), </booktitle> <pages> pages 49-54, </pages> <address> St. Paul, MN. (Menlo Park, CA: </address> <publisher> AAAI Press). </publisher>
Reference: [DellaFera et al., 1988] <author> C. Anthony DellaFera, Mark W. Eichin, Robert S. French, David C. Jedlinsky, John T. Kohl, and William E. </author> <month> Sommerfeld </month> <year> (1988). </year> <title> The Zephyr notification service. </title> <booktitle> In Proceedings of the USENIX Winter Conference, </booktitle> <pages> pages 213-219, </pages> <address> Dallas, TX. (Berkeley, CA: </address> <publisher> USENIX Association). </publisher>
Reference-contexts: Users submit their task requests through one of three different interfaces: the World Wide Web [Simmons et al., 1997], Zephyr <ref> [DellaFera et al., 1988; Simmons et al., 1997] </ref>, or a specially designed graphical user interface (Figure 2.2) [Haigh & Veloso, 1996]. The slots in this last interface are automatically filled in with default information related to the task (e.g.
Reference: [Drummond et al., 1993] <author> Mark Drummond, Keith Swanson, John Bresina, and Richard Levinson (1993). </author> <title> Reaction-first search. </title> <booktitle> In Proceedings of the Thirteenth International Joint BIBLIOGRAPHY 167 Conference on Artificial Intelligence (IJCAI-93), </booktitle> <pages> pages 1408-1414, </pages> <address> Chambery, France. (San Mateo, CA: </address> <publisher> Morgan Kaufmann). </publisher>
Reference: [Fikes et al., 1972] <author> Richard E. Fikes, Peter E. Hart, and Nils J. </author> <title> Nilsson (1972). Learning and executing generalized robot plans. </title> <journal> Artificial Intelligence, </journal> <volume> 3(4) </volume> <pages> 231-249. </pages>
Reference-contexts: Shakey [Nilsson, 1984] was the first system to use a planning system on a robot. This project was based on a classical planner which ignored real world uncertainty <ref> [Fikes et al., 1972] </ref> and followed a deterministic model to generate a single executable plan. When execution failures occurred, replanning was invoked.
Reference: [Fink & Veloso, 1994] <author> Eugene Fink and Manuela Veloso (1994). </author> <title> prodigy planning algorithm. </title> <type> Technical Report CMU-CS-94-123, </type> <institution> Deparment of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: It continues adding operators to the incomplete plan until a solution to the problem is found. In Figure 2.3 we show a simple incomplete plan. An incomplete plan consists of two parts, the head-plan and the tail-plan <ref> [Fink & Veloso, 1994] </ref>. The tail-plan is built by a backward-chaining algorithm, which starts from the list of goals, G, and adds operators, one by one, to achieve its pending goals, i.e., to achieve preconditions of other operators that are not satisfied in the current state.
Reference: [Firby, 1989] <author> Robert James Firby (1989). </author> <title> Adaptive Execution in Complex Dynamic Worlds. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <address> New Haven, CT. </address>
Reference-contexts: These execution behaviours resemble schemas [Georgeff & Ingrand, 1989; Hormann et al., 1991] or RAPs <ref> [Firby, 1989; Gat, 1992; Pell et al., 1997] </ref>, in that they specify how to execute the action, what to monitor in the environment, and some internal recovery procedures. Rogue's execution behaviours, however, do not contain complex recovery or monitoring procedures, which we believe the planner should explicitly reason about. <p> the planner is "a costly activity, as it causes [the system] to interrupt the ongoing planned activities and lose important opportunities." Throwing out all existing planning and starting over not only delays execution and but also can place high demands on sensing to determine current status of partially-executed tasks. raps <ref> [Firby, 1994; Firby, 1989] </ref>, like TCA, is an architecture that enables a library of behaviours and reactions to be controlled by a deliberative system. raps and TCA have been used as the underlying control mechanism on a variety of robots, from indoor mobile robots [Gat, 1992; Simmons et al., 1997] to <p> was explicitly designed to interact with a planner: It defines a well-structured, flexible and extensible mechanism for describing modular behaviors that can be both executed and reasoned about... the ability to reason about them independently provides a hook for interfacing the system to more deliberative planning and problem solving processes. <ref> [Firby, 1989] </ref> To build a set of raps, the programmer must explicitly account for all goal interactions, pre-determine all preference rankings between actions, and eliminate all accidental paths to dangerous states.
Reference: [Firby, 1994] <author> R. James Firby (1994). </author> <title> Task networks for controlling continuous processes. </title> <editor> In K. Hammond, editor, </editor> <booktitle> Artificial Intelligence Planning Systems: Proceedings of the Second International Conference (AIPS-94), </booktitle> <pages> pages 49-54, </pages> <address> Chicago, IL. (Menlo Park, CA: </address> <publisher> AAAI Press). </publisher>
Reference-contexts: the planner is "a costly activity, as it causes [the system] to interrupt the ongoing planned activities and lose important opportunities." Throwing out all existing planning and starting over not only delays execution and but also can place high demands on sensing to determine current status of partially-executed tasks. raps <ref> [Firby, 1994; Firby, 1989] </ref>, like TCA, is an architecture that enables a library of behaviours and reactions to be controlled by a deliberative system. raps and TCA have been used as the underlying control mechanism on a variety of robots, from indoor mobile robots [Gat, 1992; Simmons et al., 1997] to
Reference: [Gat, 1992] <author> Erann Gat (1992). </author> <title> Integrating planning and reacting in a heterogeneous asynchronous architecture for controlling real-world mobile robots. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI-92), </booktitle> <pages> pages 809-815, </pages> <address> San Jose, CA. </address>
Reference-contexts: These execution behaviours resemble schemas [Georgeff & Ingrand, 1989; Hormann et al., 1991] or RAPs <ref> [Firby, 1989; Gat, 1992; Pell et al., 1997] </ref>, in that they specify how to execute the action, what to monitor in the environment, and some internal recovery procedures. Rogue's execution behaviours, however, do not contain complex recovery or monitoring procedures, which we believe the planner should explicitly reason about. <p> current status of partially-executed tasks. raps [Firby, 1994; Firby, 1989], like TCA, is an architecture that enables a library of behaviours and reactions to be controlled by a deliberative system. raps and TCA have been used as the underlying control mechanism on a variety of robots, from indoor mobile robots <ref> [Gat, 1992; Simmons et al., 1997] </ref> to outdoor legged robots [Simmons, 1991] to planetary rovers [Krotkov et al., 1995] and spacecraft [Bonasso & Kortenkamp, 1996].
Reference: [Georgeff & Ingrand, 1989] <author> Michael P. Georgeff and Fran~cois F. </author> <month> Ingrand </month> <year> (1989). </year> <title> Decision-making in an embedded reasoning system. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <pages> pages 972-978, </pages> <address> Detroit, MI. (San Mateo, CA: </address> <publisher> Morgan Kaufmann). </publisher>
Reference-contexts: In general, this behaviour is 1. to verify the preconditions of the operator, 2. to execute an associated command sequence, 3. to verify the postconditions of the operator, and 4. if necessary, to attempt to recover from simple failures. These execution behaviours resemble schemas <ref> [Georgeff & Ingrand, 1989; Hormann et al., 1991] </ref> or RAPs [Firby, 1989; Gat, 1992; Pell et al., 1997], in that they specify how to execute the action, what to monitor in the environment, and some internal recovery procedures. <p> Rogue easily incorporates asynchronous goals into its system without losing any context of existing tasks, allowing it to take advantage of opportunities as they arise. By intelligent combining of compatible tasks, Rogue can respond quickly and efficiently to user requests. Amongst the other interleaving planners, only PRS <ref> [Georgeff & Ingrand, 1989] </ref> handles multiple asynchronous goals. Rogue however abstracts much of the lower level details that PRS explicitly reasons about, meaning that Rogue can be seen as more reliable and efficient because system functionality is suitably partitioned [Pell et al., 1997; Simmons et al., 1997].
Reference: [Gervasio & DeJong, 1991] <author> Melinda T. Gervasio and Gerald F. </author> <title> DeJong (1991). Learning probably completable plans. </title> <type> Technical Report UIUCDCS-R-91-1686, </type> <institution> University of Illinois at Urbana-Champaign, IL, Urbana, IL. </institution>
Reference: [Gil, 1992] <author> Yolanda Gil (1992). </author> <title> Acquiring domain knowledge for planning by experimentation. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, PA. </institution> <note> Also available as Technical Report CMU-CS-92-175. </note>
Reference-contexts: Artificial intelligence researchers have explored this area extensively, but have generally limited their efforts to simulated worlds with no noise or exogenous events. AI research that most closely resembles ours has explored how to learn and correct action models (e.g. <ref> [Gil, 1992; Pearson, 1996; Wang, 1996] </ref>). These systems observe or experiment in the environment to correct action descriptions, which are then directly used for planning. <p> OBSERVER [Wang, 1996] and ARMS [Segre, 1991] learn action models by observing another agent's solution; they rely on complete observation of the environment and external agents or noise. Learning is assumed to be correct and irreversible. EXPO <ref> [Gil, 1992] </ref> learns operators by experimentation; it designs experiments, and explicitly monitors effects in environment. It also assumes complete and immediate sensing with no external events or noise.
Reference: [Goodwin, 1994] <author> Richard Goodwin (1994). </author> <title> Reasoning about when to start acting. </title> <editor> In K. Hammond, editor, </editor> <booktitle> Artificial Intelligence Planning Systems: Proceedings of the Second International Conference (AIPS-94), </booktitle> <pages> pages 86-91, </pages> <address> Chicago, IL. (Menlo Park, CA: </address> <publisher> AAAI Press). </publisher>
Reference: [Goodwin, 1996] <author> Richard Goodwin (1996). </author> <title> Meta-Level Control for Decision-Theoretic Planners. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, PA. </institution> <note> Available as Technical Report CMU-CS-96-186. 168 BIBLIOGRAPHY </note>
Reference-contexts: The path planner creates plans for moving from one location in the environment to another. The path planner uses a decision-theoretic A* algorithm on a topological map with metric information <ref> [Goodwin, 1996] </ref>. The planner creates a plan with the best expected travel time, taking into account distance, blockage probability, traversal weight, and recovery costs (for, say, missing difficult turns). Rogue depends most heavily on Xavier's reliable navigation module, which reaches its destination approximately 95% of the time. <p> Figure 3.1 shows how our algorithm fits into the framework of the Xavier architecture. The path planner uses a A* algorithm on a topological map that has additional metric information <ref> [Goodwin, 1996] </ref>. Knowledge in the path planner is represented as a topological map of the robot's navigation environment. The map is a graph with nodes and arcs representing office rooms, corridors, doors and lobbies. <p> The training data for this learning is provided by the navigation module. The path planner uses an A* algorithm with an arc/node representation <ref> [Goodwin, 1996] </ref>. Navigation, meanwhile, uses a Markov state representation inside a Partially Observable Markov Decision Process (POMDP) model [Simmons & Koenig, 1995; Koenig, 1997]. 3.1. <p> The environment is modelled as a topological map with nodes and arcs. Nodes represent junctions, such as those between corridors or at doors. Arcs represent connections between junctions. Topological arcs are augmented with length estimates. Plans are generated using a decision-theoretic A* search strategy <ref> [Goodwin, 1996] </ref>. The path planner operates on the augmented topological map rather than using the POMDP model directly. 1 The path planner creates a path with the best expected travel time.
Reference: [Goodwin & Simmons, 1992] <author> Richard Goodwin and Reid G. </author> <title> Simmons (1992). Rational handling of multiple goals for mobile robots. </title> <editor> In J. Hendler, editor, </editor> <booktitle> Artificial Intelligence Planning Systems: Proceedings of the First International Conference (AIPS-92), </booktitle> <pages> pages 86-91, </pages> <address> College Park, MD. (San Mateo, CA: </address> <publisher> Morgan Kaufmann). </publisher>
Reference-contexts: One common method for handling these multiple goal requests is simply to process them in a first-come-first-served manner; however, this method leads to inefficiencies and lost opportunities for combined execution of compatible tasks <ref> [Goodwin & Simmons, 1992] </ref>. Rogue is able to process incoming asynchronous goal requests, to prioritize them, and to suspend lower priority actions when necessary. It successfully interleaves compatible requests and creates efficient plans for completing all the tasks. 20 CHAPTER 2.
Reference: [Grant & Feng, 1989] <author> E. Grant and Cao Feng (1989). </author> <title> Experiments in robot learning. </title> <booktitle> In Proceedings of IEEE International Symposium on Intelligent Control 1989, </booktitle> <pages> pages 561-5, </pages> <address> Albany, NY. </address>
Reference: [Haigh et al., 1997a] <author> Karen Zita Haigh, Jonathan Richard Shewchuk, and Manuela M. </author> <title> Veloso (1997a). Exploiting domain geometry in analogical route planning. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 9 </volume> <pages> 509-541. </pages>
Reference: [Haigh et al., 1997b] <author> Karen Zita Haigh, Peter Stone, and Manuela M. </author> <title> Veloso (1997b). Execution in prodigy4.0: The user's manual. </title> <type> Technical Report CMU-CS-97-187, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: Rogue's task planner is based on prod-igy4.0 [Veloso et al., 1995], a domain-independent nonlinear state-space planner that uses means-ends analysis and backward chaining to reason about multiple goals and multiple alternative operators to achieve the goals. It has been extended to support real world execution of its symbolic actions <ref> [Haigh et al., 1997b; Stone & Veloso, 1996] </ref>. Rogue handles multiple asynchronous task requests and controls the real-world execution of Xavier to achieve tasks in this dynamic office delivery domain. <p> EXECUTION AND MONITORING 35 2.3.2.1 prodigy4.0's Mechanisms for Supporting Execution prodigy4.0 allows arbitrary procedural attachments that are called during the operator application phase of the planning cycle <ref> [Haigh et al., 1997b; Stone & Veloso, 1996] </ref>. Typically, we use these functions to give the planner additional information about the state of the world that might not be accurately predictable from the model of the environment. For example, this new information might show resource consumption or action outcomes. <p> In order to execute operators, several mechanisms were added to prodigy4.0 <ref> [Haigh et al., 1997b; Stone & Veloso, 1996] </ref>. The new prodigy4.0/execute algorithm is shown in Table 2.9. First, the designer needs to define the execution behaviour of the operator (Change-state-on-execute). Second, the designer needs to define when to execute the operator (Automatically-decideto-execute).
Reference: [Haigh & Veloso, 1996] <author> Karen Zita Haigh and Manuela Veloso (1996). </author> <title> Interleaving planning and robot execution for asynchronous user requests. </title> <booktitle> In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), </booktitle> <pages> pages 148-155, </pages> <address> Osaka, Japan. (New York, NY: </address> <publisher> IEEE Press). </publisher>
Reference-contexts: Users submit their task requests through one of three different interfaces: the World Wide Web [Simmons et al., 1997], Zephyr [DellaFera et al., 1988; Simmons et al., 1997], or a specially designed graphical user interface (Figure 2.2) <ref> [Haigh & Veloso, 1996] </ref>. The slots in this last interface are automatically filled in with default information related to the task (e.g. FedEx delivery location) as well as information extracted from the user's plan file through a simple template-matching mechanism. The deadline time defaults to one hour in the future.
Reference: [Haigh & Veloso, 1997] <author> Karen Zita Haigh and Manuela M. </author> <title> Veloso (1997). Interleaving planning and robot execution for asynchronous user requests. Autonomous Robots. </title> <publisher> In press. </publisher>
Reference: [Haigh & Veloso, 1998a] <author> Karen Zita Haigh and Manuela M. </author> <title> Veloso (1998a). Learning situation-dependent costs: Improving planning from probabilistic robot execution. </title> <editor> In Ka-tia P. Sycara, editor, </editor> <booktitle> Proceedings of the Second International Conference on Autonomous Agents, </booktitle> <address> Minneapolis, MN. (Menlo Park, CA: </address> <publisher> AAAI Press). In Press. </publisher>
Reference: [Haigh & Veloso, 1998b] <author> Karen Zita Haigh and Manuela M. </author> <title> Veloso (1998b). Planning, execution and learning in a robotic agent. </title> <editor> In R. Simmons, M. Veloso, and S. Smith, editors, </editor> <booktitle> Artificial Intelligence Planning Systems: Proceedings of the Fourth International Conference (AIPS-98), </booktitle> <address> Pittsburgh, PA. (Menlo Park, CA: </address> <publisher> AAAI Press). Submission. </publisher>
Reference: [Hammond, 1987] <author> Kristian J. </author> <title> Hammond (1987). Learning and reusing explanations. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pages 141-147, </pages> <address> Irvine, CA. </address>
Reference-contexts: Rules are incrementally refined and with more training examples will converge towards a possibly disjunctive set of correct rules. Noise is also not accounted for in this system. CHEF <ref> [Hammond, 1987] </ref>, prodigy/analogy [Veloso, 1994] and Haigh & Veloso [1997a] use analogical reasoning to create plans based on past successful experiences, where the belief is that past success might help lead to future success.
Reference: [Hormann et al., 1991] <author> Andreas Hormann, Wolfgang Meier, and Jan Schloen (1991). </author> <title> A control architecture for and advanced fault-tolerant robot system. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <address> 7(2-3):211-225. </address>
Reference-contexts: In general, this behaviour is 1. to verify the preconditions of the operator, 2. to execute an associated command sequence, 3. to verify the postconditions of the operator, and 4. if necessary, to attempt to recover from simple failures. These execution behaviours resemble schemas <ref> [Georgeff & Ingrand, 1989; Hormann et al., 1991] </ref> or RAPs [Firby, 1989; Gat, 1992; Pell et al., 1997], in that they specify how to execute the action, what to monitor in the environment, and some internal recovery procedures.
Reference: [Hughes & Ranganathan, 1994] <author> Ken Hughes and N. </author> <title> Ranganathan (1994). Modeling sensor confidence for sensor integration tasks. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 8(6) </volume> <pages> 1301-1318. BIBLIOGRAPHY 169 </pages>
Reference-contexts: We assume that verification step gives complete and correct information about the robot's actual location; other researchers are focussing on the open problem of sensor reliability <ref> [Hughes & Ranganathan, 1994; Thrun, 1996] </ref>. If Rogue detects that in fact the robot is not at the correct goal location, Rogue updates the navigation module with the new information and re-attempts to navigate to the desired location.
Reference: [Kaelbling et al., 1996] <author> Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. </author> <title> Moore (1996). Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285. </pages>
Reference: [Kibler, 1993] <author> Dennis Kibler (1993). </author> <title> Some real-world domains for learning problem solvers. </title> <booktitle> In Proceedings of KCSL93, 3rd International Workshop on Knowledge Compilation and Speedup Learning (in ICML93), </booktitle> <address> Amherst, MA. </address>
Reference: [Klingspor et al., 1996] <author> Volker Klingspor, Katharina J. Morik, and Anke D. </author> <title> Rieger (1996). Learning concepts from sensor data of a mobile robot. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 305-332. </pages>
Reference: [Koenig, 1997] <author> Sven Koenig (1997). </author> <title> Goal-Directed Acting with Incomplete Information. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, PA. </institution> <note> Available as Technical Report CMU-CS-97-199. </note>
Reference-contexts: The training data for this learning is provided by the navigation module. The path planner uses an A* algorithm with an arc/node representation [Goodwin, 1996]. Navigation, meanwhile, uses a Markov state representation inside a Partially Observable Markov Decision Process (POMDP) model <ref> [Simmons & Koenig, 1995; Koenig, 1997] </ref>. 3.1. ARCHITECTURE AND REPRESENTATION 53 There is, unfortunately, no clear correspondence between the representation used in the navigation module and the representation used in the path planning module.
Reference: [Koenig & Simmons, 1996] <author> Sven Koenig and Reid G. </author> <title> Simmons (1996). Passive distance learning for robot navigation. </title> <editor> In Lorenza Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference (ICML96), </booktitle> <pages> pages 266-274, </pages> <address> Bari, Italy. (San Mateo, CA: </address> <publisher> Morgan Kaufmann). </publisher>
Reference-contexts: INTRODUCTION Prior Learning Efforts for Robotics. Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. <ref> [Koenig & Simmons, 1996; Kortenkamp & Weymouth, 1994; Thrun, 1996] </ref>), or learning operational parameters for better actuator control (e.g. [Baroglio et al., 1996; Bennett & DeJong, 1996; Grant & Feng, 1989; Pomerleau, 1993]). Instead of improving low-level actuator control, our work focusses at the planning stages of the system. <p> The behaviours demonstrated by Rogue under TCA could be easily transferred to another robot control architecture. 5.2 Learning Although there is extensive machine learning research in the artificial intelligence community, very little of it has been applied to real-world domains. Common applications include map learning and localization (e.g. <ref> [Koenig & Simmons, 1996; Kortenkamp & Weymouth, 1994; Thrun, 1996] </ref>), or learning operational parameters for better actuator control (e.g. [Baroglio et al., 1996; Bennett & DeJong, 1996; Pomerleau, 1993]). Instead of improving low-level actuator control, our work focusses instead at the planning stages of the system.
Reference: [Kortenkamp & Weymouth, 1994] <author> David Kortenkamp and Terry Weymouth (1994). </author> <title> Topological mapping for mobile robots using a combination of sonar and vision sensing. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> pages 979-984, </pages> <address> Seattle, WA. (Menlo Park, CA: </address> <publisher> AAAI Press). </publisher>
Reference-contexts: INTRODUCTION Prior Learning Efforts for Robotics. Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. <ref> [Koenig & Simmons, 1996; Kortenkamp & Weymouth, 1994; Thrun, 1996] </ref>), or learning operational parameters for better actuator control (e.g. [Baroglio et al., 1996; Bennett & DeJong, 1996; Grant & Feng, 1989; Pomerleau, 1993]). Instead of improving low-level actuator control, our work focusses at the planning stages of the system. <p> The behaviours demonstrated by Rogue under TCA could be easily transferred to another robot control architecture. 5.2 Learning Although there is extensive machine learning research in the artificial intelligence community, very little of it has been applied to real-world domains. Common applications include map learning and localization (e.g. <ref> [Koenig & Simmons, 1996; Kortenkamp & Weymouth, 1994; Thrun, 1996] </ref>), or learning operational parameters for better actuator control (e.g. [Baroglio et al., 1996; Bennett & DeJong, 1996; Pomerleau, 1993]). Instead of improving low-level actuator control, our work focusses instead at the planning stages of the system.
Reference: [Krotkov et al., 1995] <author> Eric Krotkov, Martial Hebert, and Reid Simmons (1995). </author> <title> Stereo perception and dead reckoning for a prototype lunar rover. </title> <booktitle> Autonomous Robots, </booktitle> <volume> 2(4) </volume> <pages> 313-331. </pages>
Reference-contexts: that enables a library of behaviours and reactions to be controlled by a deliberative system. raps and TCA have been used as the underlying control mechanism on a variety of robots, from indoor mobile robots [Gat, 1992; Simmons et al., 1997] to outdoor legged robots [Simmons, 1991] to planetary rovers <ref> [Krotkov et al., 1995] </ref> and spacecraft [Bonasso & Kortenkamp, 1996]. TCA provides facilities for scheduling and synchronizing tasks, resource allocation, environment monitoring and exception handling. raps allow you to specify the methods and context for actions, and can therefore be constructed to provide the same facilities as TCA.
Reference: [Kushmerick et al., 1993] <author> Nick Kushmerick, Steve Hanks, and Dan Weld (1993). </author> <title> An algorithm for probabilistic planning. </title> <type> Technical Report 93-06-03, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, WA. </address>
Reference: [Lindner et al., 1994] <author> John Lindner, Robin R. Murphy, and Elizabeth Nitz (1994). </author> <title> Learning the expected utility of sensors and algorithms. </title> <booktitle> In IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, </booktitle> <pages> pages 583-590. </pages> <address> (New York, NY: </address> <publisher> IEEE Press). </publisher>
Reference-contexts: These systems observe or experiment in the environment to correct action descriptions, which are then directly used for planning. In the robotics community, closely related work comes from those who have explored learning costs and applicability of actions (e.g. <ref> [Lindner et al., 1994; Shen, 1994; Tan, 1991] </ref>). These systems learn improved domain models and this knowledge is then used by the system's planner, as costs or control knowledge, so that the planner can then select more appropriate actions. Situation-dependent Learning Approach. <p> Other researchers have also explored the area of learning action costs. CSL [Tan, 1991] and Clementine <ref> [Lindner et al., 1994] </ref> both learn sensor utilities, including which sensor to use for what information.
Reference: [Lovejoy, 1991] <author> W. </author> <month> Lovejoy </month> <year> (1991). </year> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28(1) </volume> <pages> 47-65. </pages>
Reference-contexts: very high, and so the path planner determines that the longer path might take less time on average. 1 It is infeasible to determine optimal POMDP solutions given our real-time constraints and the size of our state spaces (over 3000 states for the map shown in Figure 1.4, page 11) <ref> [Cassandra et al., 1994; Lovejoy, 1991] </ref>. Reasoning about blockage probabilities and recovery costs is also notably easier in the topological map. 54 CHAPTER 3. LEARNING FOR THE PATH PLANNER get trapped in the dead end. Reproduced from Simmons et al. [1997].
Reference: [Lyons & Hendriks, 1992] <author> D. M. Lyons and A. J. </author> <title> Hendriks (1992). A practical approach to integrating reaction and deliberation. </title> <editor> In J. Hendler, editor, </editor> <booktitle> Artificial Intelligence Planning Systems: Proceedings of the First International Conference (AIPS-92), </booktitle> <pages> pages 153-162. </pages> <address> (San Mateo, CA: </address> <publisher> Morgan Kaufmann). 170 BIBLIOGRAPHY </publisher>
Reference: [Mansell, 1993] <author> Todd Michael Mansell (1993). </author> <title> A method for planning given uncertain and incomplete information. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 250-358, </pages> <address> Washington, DC. (San Mateo, CA: </address> <publisher> Morgan Kaufmann). </publisher>
Reference-contexts: Conditional planning is one approach that aims at considering in the domain model all the possible contingencies of the world and plan ahead for each individual one <ref> [Atkins et al., 1996; Mansell, 1993; Pryor, 1994; Schoppers, 1989] </ref>. In most complex environments, the large number of possible contingencies means that complete conditional planning becomes infeasible, but may nevertheless be appropriate in particularly dangerous domains. <p> A network routing planner may select one route when congestion is high, and another otherwise. Learning operator probabilities for probabilistic or conditional planners, such as for Weaver [Blythe, 1994] or U-PLAN <ref> [Mansell, 1993] </ref>, or Xavier's navigation module.
Reference: [McCallum, 1995] <author> Andrew Kachites McCallum (1995). </author> <title> Reinforcement Learning with Selective Perception and Hidden State. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Rochester, Rochester, NY. </institution>
Reference-contexts: This approach can be viewed as learning the integral of action costs. However, most Reinforcement Learning techniques are unable to generalize learned information, and as a result, they have only been used in small domains. Recently, several research have been exploring techniques for allowing generalization in Reinforcement Learning <ref> [Baird, 1995; Boyan & Moore, 1995; McCallum, 1995] </ref>. Essentially, these systems replace Reinforcement Learning's standard table-lookup mechanism with alternative function approximation techniques, such as decision trees or neural networks. Experimentally, these algorithms seem to produce reasonable policies.
Reference: [McDermott, 1992] <author> Drew McDermott (1992). </author> <title> Transformational planning of reactive behavior. </title> <type> Technical Report YALE/CSD/RR#941, </type> <institution> Computer Science Department, Yale University, </institution> <address> New Haven, CT. </address>
Reference: [Mitchell, 1997] <author> Tom M. </author> <title> Mitchell (1997). </title> <booktitle> Machine Learning. </booktitle> <address> (New York, NY: </address> <publisher> McGraw Hill). </publisher>
Reference-contexts: Bayesian learning would not successfully handle disjunctive functions, k-Nearest Neighbour algorithms would not handle irrelevant features well, neural networks would not generalize well for sparse data, and standard decision trees do not handle continuous valued output particularly well <ref> [Mitchell, 1997; Quinlan, 1993] </ref>. Other learning mechanisms may be appropriate in different robot architectures with different data representations. We selected an off-the-shelf package, namely S-PLUS [Becker et al., 1988], as the regression tree implementation.
Reference: [Mitchell et al., 1994] <author> Tom M. Mitchell, Rich Caruana, Dayne Freitag, John P. McDermott, and David Zabowski (1994). </author> <title> Experience with a learning personal assistant. </title> <journal> CACM, </journal> <volume> 37(7) </volume> <pages> 80-91. </pages>
Reference: [Nilsson, 1984] <author> Nils J. </author> <title> Nilsson (1984). Shakey the robot. </title> <type> Technical Report 323, </type> <institution> AI Center, SRI International, </institution> <address> Menlo Park, CA. </address>
Reference-contexts: A learning robot will be more flexible and adaptive than a pre-programmed system. An office delivery robot, for example, would be able to move from working in a university classroom building to a hospital with few, if any, design changes. Since Shakey the robot <ref> [Nilsson, 1984] </ref>, researchers have been trying to build autonomous robots that are capable of planning and executing high-level tasks, as well as learning from the analysis of execution experience. <p> In Section 5.1, we present work related to the task planner. In Section 5.2 we present research related to our learning framework. 5.1 Task Planning There are a few approaches to creating plans for execution. Shakey <ref> [Nilsson, 1984] </ref> was the first system to use a planning system on a robot. This project was based on a classical planner which ignored real world uncertainty [Fikes et al., 1972] and followed a deterministic model to generate a single executable plan. When execution failures occurred, replanning was invoked.
Reference: [Nourbakhsh, 1997] <author> Illah Nourbakhsh (1997). </author> <title> Interleaving Planning and Execution for Autonomous Robots. (Dordrecht, Netherlands: </title> <type> Kluwer Academic). PhD thesis. </type> <note> Also available as technical report STAN-CS-TR-97-1593, </note> <institution> Department of Computer Science, Stanford University, Stanford, </institution> <address> CA. </address>
Reference: [O'Sullivan et al., 1997] <author> Joseph O'Sullivan, Karen Zita Haigh, and G. D. </author> <title> Armstrong (1997). </title> <type> Xavier. </type> <institution> Carnegie Mellon University, Pittsburgh, PA. </institution> <note> Manual, Version 0.3, unpublished internal report. Available via http://www.cs.cmu.edu/~Xavier/. </note>
Reference-contexts: Xavier is a mobile robot being developed at Carnegie Mellon University <ref> [O'Sullivan et al., 1997; Simmons et al., 1997] </ref> (see Figure 1.1). It is built on an RWI B24 base and includes bump sensors, a laser range finder, sonars, a color camera and a speech board. <p> Most of these "problems" model the actual behaviour of the robot, allowing code developed on the simulator to run successfully on the robot with no modification <ref> [O'Sullivan et al., 1997] </ref>. The simulator allows the tight control of experiments, to ensure that the learning algorithm is indeed learning appropriate situation-dependent costs. 3.7.1 Simulated World 1: Learning Patterns The first environment tests Rogue's ability to learn situation-dependent costs.
Reference: [Pearson, 1996] <author> Douglas John Pearson (1996). </author> <title> Learning Procedural Planning Knowledge in Complex Environments. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, University of Michigan, </institution> <address> Ann Arbor, MI. </address> <note> Available as Technical Report CSE-TR-309-96. </note>
Reference-contexts: Artificial intelligence researchers have explored this area extensively, but have generally limited their efforts to simulated worlds with no noise or exogenous events. AI research that most closely resembles ours has explored how to learn and correct action models (e.g. <ref> [Gil, 1992; Pearson, 1996; Wang, 1996] </ref>). These systems observe or experiment in the environment to correct action descriptions, which are then directly used for planning. <p> LIVE [Shen, 1994], like EXPO, also uses experimentation to learn a model of the environment. It extends EXPO's abilities by learning stochastic effects from incomplete sensing, but does not handle environments with noise or exogenous events. IMPROV <ref> [Pearson, 1996] </ref> is one system which relaxes the assumption about complete and correct sensing, but still manages to learn operator descriptions. The planner learns through experimentation, by trying alternative operators until it achieves a success.
Reference: [Pell et al., 1997] <author> Barney Pell, Douglas E. Bernard, Steve A. Chien, Erann Gat, Nicola Muscettola, P. Pandurang Nayak, Michael D. Wagner, and Brian C. </author> <title> Williams (1997). An autonomous spacecraft agent prototype. </title> <booktitle> In Proceedings of the First International Conference on Autonomous Agents, </booktitle> <pages> pages 253-261, </pages> <address> Marina del Rey, CA. (New York, NY: </address> <publisher> ACM Press). </publisher>
Reference-contexts: Once the important request has been fulfilled, Rogue reactivates the less important task (s). Some systems respond to asynchronous goals by restarting the planner, losing planning effort as well as placing high demands on sensing to determine the current status of the environment and interrupted tasks <ref> [Pell et al., 1997; Bonasso & Kortenkamp, 1996] </ref>. Rogue, however, suspends and reactivates tasks efficiently, without losing any of the prior planning information. It monitors the environment to identify unexpected changes, such as side-effects and exogenous events, that can affect the validity and applicability of plans. <p> These execution behaviours resemble schemas [Georgeff & Ingrand, 1989; Hormann et al., 1991] or RAPs <ref> [Firby, 1989; Gat, 1992; Pell et al., 1997] </ref>, in that they specify how to execute the action, what to monitor in the environment, and some internal recovery procedures. Rogue's execution behaviours, however, do not contain complex recovery or monitoring procedures, which we believe the planner should explicitly reason about. <p> asynchronous user requests. (Rogue therefore also shows how the prodigy4.0 planner and the TCA approach in Xavier are in fact robust architectures.) Strictly looking at Rogue only from the viewpoint of the integration of planning and execution, Rogue compares well with other integrated planning and execution systems such as NMRA <ref> [Pell et al., 1997] </ref> and 3 T [Bonasso & Kortenkamp, 1996] (Section 5.1 discusses this comparison in more detail). Given the general-purpose character of the prodigy4.0 planner, Rogue could easily be applied to other executing platforms and tasks by a flexible change of prodigy4.0's specification of the domain. <p> Amongst the other interleaving planners, only PRS [Georgeff & Ingrand, 1989] handles multiple asynchronous goals. Rogue however abstracts much of the lower level details that PRS explicitly reasons about, meaning that Rogue can be seen as more reliable and efficient because system functionality is suitably partitioned <ref> [Pell et al., 1997; Simmons et al., 1997] </ref>. NMRA [Pell et al., 1997] and 3 T [Bonasso & Kortenkamp, 1996] both function in domains with many asynchronous goals, but both planners respond to new goals and serious action failures by abandoning existing planning and restarting the planner. <p> Rogue however abstracts much of the lower level details that PRS explicitly reasons about, meaning that Rogue can be seen as more reliable and efficient because system functionality is suitably partitioned [Pell et al., 1997; Simmons et al., 1997]. NMRA <ref> [Pell et al., 1997] </ref> and 3 T [Bonasso & Kortenkamp, 1996] both function in domains with many asynchronous goals, but both planners respond to new goals and serious action failures by abandoning existing planning and restarting the planner.
Reference: [Perez, 1995] <author> M. </author> <title> Alicia Perez (1995). Learning Search Control Knowledge to Improve Plan Quality. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, PA. </institution> <note> Available as Technical Report CMU-CS-95-175. BIBLIOGRAPHY 171 </note>
Reference-contexts: Hand-built domain-dependent control rules use the 5.2. LEARNING 127 utility function to determine which plans to expand and which flaws to fix. The system learns neither the control rules nor the utility function. quality <ref> [Perez, 1995] </ref> learns control rules to generate high quality plans, where quality can be defined in terms of execution cost, reliability or user satisfaction, and operators may have different costs.
Reference: [Pomerleau, 1993] <author> Dean A. </author> <title> Pomerleau (1993). Neural network perception for mobile robot guidance. </title> <address> (Dordrecht, Netherlands: </address> <publisher> Kluwer Academic). </publisher>
Reference-contexts: Common applications include map learning and localization (e.g. [Koenig & Simmons, 1996; Kortenkamp & Weymouth, 1994; Thrun, 1996]), or learning operational parameters for better actuator control (e.g. <ref> [Baroglio et al., 1996; Bennett & DeJong, 1996; Pomerleau, 1993] </ref>). Instead of improving low-level actuator control, our work focusses instead at the planning stages of the system. In this section, we describe some of the work related to our learning approach.
Reference: [Pryor, 1994] <author> Louise Margaret Pryor (1994). </author> <title> Opportunities and Planning in an Unpredictable World. </title> <type> PhD thesis, </type> <institution> Northwestern University, Evanston, Illinois. </institution> <note> Available as Technical Report number 53. </note>
Reference-contexts: Conditional planning is one approach that aims at considering in the domain model all the possible contingencies of the world and plan ahead for each individual one <ref> [Atkins et al., 1996; Mansell, 1993; Pryor, 1994; Schoppers, 1989] </ref>. In most complex environments, the large number of possible contingencies means that complete conditional planning becomes infeasible, but may nevertheless be appropriate in particularly dangerous domains.
Reference: [Quinlan, 1993] <author> J. Ross Quinlan (1993). C4.5: </author> <title> Programs for Machine Learning. </title> <address> (San Mateo, CA: </address> <publisher> Morgan Kaufmann). </publisher>
Reference-contexts: Bayesian learning would not successfully handle disjunctive functions, k-Nearest Neighbour algorithms would not handle irrelevant features well, neural networks would not generalize well for sparse data, and standard decision trees do not handle continuous valued output particularly well <ref> [Mitchell, 1997; Quinlan, 1993] </ref>. Other learning mechanisms may be appropriate in different robot architectures with different data representations. We selected an off-the-shelf package, namely S-PLUS [Becker et al., 1988], as the regression tree implementation.
Reference: [Rabiner & Juang, 1986] <author> L. R. Rabiner and B. H. </author> <title> Juang (1986). An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 6(3) </volume> <pages> 4-16. </pages>
Reference-contexts: The POMDP navigation module keeps track of the most likely states but not the most likely sequence of states. Viterbi's algorithm is guaranteed to find the single best state sequence with the highest probability, given the actions, observations and initial state distribution <ref> [Rabiner & Juang, 1986] </ref>. However, Viterbi's algorithm was not designed for use in a 60 CHAPTER 3. LEARNING FOR THE PATH PLANNER Markov model that represents uncertain length information. We extend Viterbi's algorithm to compensate for this uncertainty, giving us a powerful way to identify likely paths through the environment. <p> In order to reconstruct the arc traversal sequence, we must first reconstruct the Markov state traversal sequence. The algorithm to calculate this sequence is known as Viterbi's algorithm <ref> [Rabiner & Juang, 1986] </ref>. The algorithm is reproduced in full in Table 3.3. In step 1, variables are initialized. In step 2, Viterbi's algorithm maintains an estimate of which state the robot was in at the previous time step, for each possible state. <p> LEARNING FOR THE PATH PLANNER 3.3.1.1 Problems with the Viterbi Sequence Viterbi's algorithm is guaranteed to find the most likely sequence of Markov states <ref> [Rabiner & Juang, 1986] </ref>. However, the Markov models we use for robot navigation differ from standard Markov models used by speech systems: we represent length uncertainty. <p> We present a comparison between Viterbi's algorithm, Multi/Markov Viterbi and Amal-gamViterbi in three different environments. E.1 Markov Models with High Branching Factors Viterbi's algorithm is guaranteed to find the most likely sequence of Markov states <ref> [Rabiner & Juang, 1986] </ref>. Unfortunately, Viterbi's algorithm was not designed for use in Markov models with additional uncertainty factors that change the structure of the model. In the context of navigation, standard Markov models represent only position uncertainty.
Reference: [Rizzo et al., 1997] <author> Paola Rizzo, Manuela Veloso, Maria Miceli, </author> <title> and Amedeo Cesta (1997). Personality-driven social behaviors in believable agents. </title> <booktitle> In AAAI Fall Symposium on Socially Intelligent Agents, </booktitle> <pages> pages 109-116, </pages> <address> Cambridge, MA. (Menlo Park, CA: </address> <publisher> AAAI Press). </publisher>
Reference: [Rizzo et al., 1998] <author> Paola Rizzo, Manuela M. Veloso, Maria Miceli, </author> <title> and Amedeo Cesta (1998). Goal-based personalities and social behaviors in believable agents. </title> <journal> Applied Artificial Intelligence. </journal> <month> Submitted Jan </month> <year> 1998. </year>
Reference: [Rosenkrantz et al., 1977] <author> Daniel J. Rosenkrantz, Richard E. Stearns, and Philip M. </author> <title> Lewis (1977). An analysis of several heuristics for the traveling salesman problem. </title> <journal> SIAM Journal of Computing, </journal> <volume> 6(3) </volume> <pages> 563-581. </pages>
Reference-contexts: The heuristic performs well in our environment. Table 2.7 shows one of the applied operator control rules. When all n locations are known before-hand, this heuristic has been shown to be within 1 2 dlg ne of optimal <ref> [Rosenkrantz et al., 1977] </ref>. However, the asynchronous requests in our environment mean that some locations are not known before-hand; therefore each of Rogue's decisions depend only on what it knows at that time. Plans are efficient with respect to the order that task requests arrive.
Reference: [Salganicoff & Ungar, 1995] <author> Marcos Salganicoff and Lyle H. </author> <title> Ungar (1995). Active exploration and learning in real-valued spaces using multi-armed bandit allocation indices. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference (ICML95), </booktitle> <pages> pages 480-487, </pages> <address> Tahoe City, CA. </address>
Reference: [Schoppers, 1989] <author> Marcel Joachim Schoppers (1989). </author> <title> Representation and Automatic Synthesis of Reaction Plans. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana-Champaign, IL. </institution> <note> Available as Technical Report UIUCDCS-R-89-1546. </note>
Reference-contexts: Conditional planning is one approach that aims at considering in the domain model all the possible contingencies of the world and plan ahead for each individual one <ref> [Atkins et al., 1996; Mansell, 1993; Pryor, 1994; Schoppers, 1989] </ref>. In most complex environments, the large number of possible contingencies means that complete conditional planning becomes infeasible, but may nevertheless be appropriate in particularly dangerous domains.
Reference: [Segre, 1991] <author> Alberto Segre (1991). </author> <title> Learning how to plan. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <address> 8(1-2):93-111. </address>
Reference-contexts: In the artificial intelligence community, several researchers have explored techniques for learning or changing action models. Most of these systems rely on complete and correct sensing, in simulated environments with no noise or exogenous events. OBSERVER [Wang, 1996] and ARMS <ref> [Segre, 1991] </ref> learn action models by observing another agent's solution; they rely on complete observation of the environment and external agents or noise. Learning is assumed to be correct and irreversible. EXPO [Gil, 1992] learns operators by experimentation; it designs experiments, and explicitly monitors effects in environment.
Reference: [Shen, 1994] <author> Wei-Min Shen (1994). </author> <title> Autonomous Learning from the Environment. </title> <address> (New York, NY: </address> <publisher> Computer Science Press). </publisher>
Reference-contexts: These systems observe or experiment in the environment to correct action descriptions, which are then directly used for planning. In the robotics community, closely related work comes from those who have explored learning costs and applicability of actions (e.g. <ref> [Lindner et al., 1994; Shen, 1994; Tan, 1991] </ref>). These systems learn improved domain models and this knowledge is then used by the system's planner, as costs or control knowledge, so that the planner can then select more appropriate actions. Situation-dependent Learning Approach. <p> Even though they explicitly state "the ultrasonic sensors were reliable for other settings, they are less desirable for sensing [glass]," they do not incorporate situation-dependent features in their utility estimates. LIVE <ref> [Shen, 1994] </ref> learns a model of the environment, as well as the costs of applying actions in that environment. For example, it can learn that a particular corridor has a higher-than-average cost. It does not, however, consider the possibility that costs may change according to a predictable pattern. <p> It also assumes complete and immediate sensing with no external events or noise. Learning in real world domains, however, cannot utilize techniques that rely on closed-world assumptions such as complete observation, single agents, or exogenous events. LIVE <ref> [Shen, 1994] </ref>, like EXPO, also uses experimentation to learn a model of the environment. It extends EXPO's abilities by learning stochastic effects from incomplete sensing, but does not handle environments with noise or exogenous events.
Reference: [Simmons, 1991] <author> Reid Simmons (1991). </author> <title> Concurrent planning and execution for a walking robot. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 300-305, </pages> <address> Sacramento, CA. (New York, NY: </address> <publisher> IEEE Press). 172 BIBLIOGRAPHY </publisher>
Reference-contexts: like TCA, is an architecture that enables a library of behaviours and reactions to be controlled by a deliberative system. raps and TCA have been used as the underlying control mechanism on a variety of robots, from indoor mobile robots [Gat, 1992; Simmons et al., 1997] to outdoor legged robots <ref> [Simmons, 1991] </ref> to planetary rovers [Krotkov et al., 1995] and spacecraft [Bonasso & Kortenkamp, 1996].
Reference: [Simmons, 1994] <author> Reid Simmons (1994). </author> <title> Structured control for autonomous robots. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10(1) </volume> <pages> 34-43. </pages>
Reference-contexts: It is built on an RWI B24 base and includes bump sensors, a laser range finder, sonars, a color camera and a speech board. The software controlling Xavier includes both reactive and deliberative behaviours, integrated using the Task Control Architecture (TCA) <ref> [Simmons, 1994] </ref>. Much of the software can be classified into five layers, shown in Figure 1.2: Obstacle Avoidance, Navigation, Path Planning, Task Planning (provided by Rogue), and the User Interface. <p> The various software modules communicate with each other through the Task Control Architecture (TCA) <ref> [Simmons, 1994; Simmons et al., 1990] </ref>. TCA provides facilities for scheduling and synchronizing tasks, resource allocation, environment monitoring and exception handling. The reactive behaviours enable the robot to handle real-time local navigation, obstacle avoidance, and emergency situations (such as detecting a bump). <p> Figure 2.1 shows the general architecture of the planning and execution part of Rogue's system. Rogue interfaces with Xavier through the Task Control Architecture (TCA) <ref> [Simmons, 1994] </ref>. TCA provides the communication network between each of the processes controlling the robot's behaviour, as well as facilities for scheduling and synchronizing tasks, resource allocation, environment monitoring and exception handling. These processes include both reactive behaviours and deliberative behaviours.
Reference: [Simmons et al., 1997] <author> Reid Simmons, Rich Goodwin, Karen Zita Haigh, Sven Koenig, and Joseph O'Sullivan (1997). </author> <title> A layered architecture for office delivery robots. </title> <editor> In W. Lewis Johnson, editor, </editor> <booktitle> Proceedings of the First International Conference on Autonomous Agents, </booktitle> <pages> pages 245-252, </pages> <address> Marina del Rey, CA. (New York, NY: </address> <publisher> ACM Press). </publisher>
Reference-contexts: Xavier is a mobile robot being developed at Carnegie Mellon University <ref> [O'Sullivan et al., 1997; Simmons et al., 1997] </ref> (see Figure 1.1). It is built on an RWI B24 base and includes bump sensors, a laser range finder, sonars, a color camera and a speech board. <p> Detailed navigation results are presented elsewhere <ref> [Simmons et al., 1997] </ref>. Xavier does not currently have the ability to manipulate objects itself. It therefore relies on humans in the environment to place or remove objects from its basket. Xavier's vision system is minimally used by researchers in the group. Current abilities 6 CHAPTER 1. <p> Users submit their task requests through one of three different interfaces: the World Wide Web <ref> [Simmons et al., 1997] </ref>, Zephyr [DellaFera et al., 1988; Simmons et al., 1997], or a specially designed graphical user interface (Figure 2.2) [Haigh & Veloso, 1996]. The slots in this last interface are automatically filled in with default information related to the task (e.g. <p> Users submit their task requests through one of three different interfaces: the World Wide Web [Simmons et al., 1997], Zephyr <ref> [DellaFera et al., 1988; Simmons et al., 1997] </ref>, or a specially designed graphical user interface (Figure 2.2) [Haigh & Veloso, 1996]. The slots in this last interface are automatically filled in with default information related to the task (e.g. <p> Amongst the other interleaving planners, only PRS [Georgeff & Ingrand, 1989] handles multiple asynchronous goals. Rogue however abstracts much of the lower level details that PRS explicitly reasons about, meaning that Rogue can be seen as more reliable and efficient because system functionality is suitably partitioned <ref> [Pell et al., 1997; Simmons et al., 1997] </ref>. NMRA [Pell et al., 1997] and 3 T [Bonasso & Kortenkamp, 1996] both function in domains with many asynchronous goals, but both planners respond to new goals and serious action failures by abandoning existing planning and restarting the planner. <p> current status of partially-executed tasks. raps [Firby, 1994; Firby, 1989], like TCA, is an architecture that enables a library of behaviours and reactions to be controlled by a deliberative system. raps and TCA have been used as the underlying control mechanism on a variety of robots, from indoor mobile robots <ref> [Gat, 1992; Simmons et al., 1997] </ref> to outdoor legged robots [Simmons, 1991] to planetary rovers [Krotkov et al., 1995] and spacecraft [Bonasso & Kortenkamp, 1996].
Reference: [Simmons & Koenig, 1995] <author> Reid Simmons and Sven Koenig (1995). </author> <title> Probabilistic robot navigation in partially observable environments. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pages 1080-1087, </pages> <address> Montreal, Quebec, Canada. (San Mateo, CA: </address> <publisher> Morgan Kaufmann). </publisher>
Reference-contexts: Rogue depends most heavily on Xavier's reliable navigation module, which reaches its destination approximately 95% of the time. Navigation is done using Partially Observable Markov Decision Process Models (POMDPs) <ref> [Simmons & Koenig, 1995] </ref>. In the period from December 1, 1995 to August 31, 1997 Xavier attempted 3245 navigation requests and reached its intended destination in 3060 cases, where on average each job required it to move 43 meters, for a total travel distance of over 125 kilometers. <p> The command performs well given incomplete or incorrect metric information about the environment and in the presence of noisy effectors and sensors, arriving at its destination approximately 95% of the time <ref> [Simmons & Koenig, 1995] </ref>. At the scene of a pickup or delivery, if Rogue times-out while waiting for a response to a query, Rogue will prompt for a user a second time before failing. <p> The module is able to autonomously compensate for certain problems, such as obstacles and missing landmarks. Navigation is done using Partially Observable Markov Decision Process models <ref> [Simmons & Koenig, 1995] </ref>, and the inherent uncertainty of this probabilistic model means that the module may occasionally report success even when it has not actually arrived at the desired goal location. When navigateToG reports a failure or a low-probability success, Rogue verifies the location. <p> LEARNING FOR THE PATH PLANNER and position confidence. Features, F , include both robot sensor data and high-level features such as date and goals. The execution traces from which learning can occur are provided by the navigation module. Navigation is done using Partially Observable Markov Decision Process Models (POMDPs) <ref> [Simmons & Koenig, 1995] </ref>. The execution trace includes observed features of the environment as well as the probability distribution over the Markov states at each time step. Identifying the path planner's events from this trace is challenging because the execution traces contain a massive, continual stream of probabilistic data. <p> The training data for this learning is provided by the navigation module. The path planner uses an A* algorithm with an arc/node representation [Goodwin, 1996]. Navigation, meanwhile, uses a Markov state representation inside a Partially Observable Markov Decision Process (POMDP) model <ref> [Simmons & Koenig, 1995; Koenig, 1997] </ref>. 3.1. ARCHITECTURE AND REPRESENTATION 53 There is, unfortunately, no clear correspondence between the representation used in the navigation module and the representation used in the path planning module. <p> These learned weights effectively modify the estimated traversal time to reflect experienced traversal time. Learning situation-dependent costs will allow the path planner to respond to patterns and changes in the environment. 3.1.2 Navigation Navigation on the robot is done using Partially Observable Markov Decision Process models (POMDPs) <ref> [Simmons & Koenig, 1995] </ref>. The navigation module estimates the robot's current location, determines the direction the robot should be heading at that location to follow the path, and then sets a directional heading. <p> Only forward transitions are marked. Reproduced from Simmons & Koenig [1995]. we use parallel Markov chains, where each corresponds to one of the possible lengths of the edge <ref> [Simmons & Koenig, 1995] </ref>. Figure 3.3 illustrates an example for a corridor that may be two, three or four metres long. This representation is an effective way to represent worlds in which lengths are not known with certainty. Table 3.1 shows the probability update calculation.
Reference: [Simmons et al., 1990] <author> Reid Simmons, Long-Ji Lin, and Chris Fedor (1990). </author> <title> Autonomous task control for mobile robots. </title> <booktitle> In Proceedings of the IEEE Symposium on Reactive Control, </booktitle> <pages> pages 663-668, </pages> <address> Philadelphia, PA. </address>
Reference-contexts: The various software modules communicate with each other through the Task Control Architecture (TCA) <ref> [Simmons, 1994; Simmons et al., 1990] </ref>. TCA provides facilities for scheduling and synchronizing tasks, resource allocation, environment monitoring and exception handling. The reactive behaviours enable the robot to handle real-time local navigation, obstacle avoidance, and emergency situations (such as detecting a bump).
Reference: [Simmons et al., 1996] <author> Reid Simmons, Sebastian Thrun, Greg Armstrong, Richard Good-win, Karen Haigh, Sven Koenig, Shyjan Mahamud, Daniel Nikovski, and Joseph O'Sullivan (1996). Amelia. </author> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> page 1358, </pages> <address> Portland, OR. (Menlo Park, CA: </address> <publisher> AAAI Press). Introduction for robot competition. </publisher>
Reference-contexts: The observation routine can vary depending on the kind of information needed. It can range from an actual interpretation of some of Xavier's sensors or its visual images, to specific input by a user. During the competition, we used motion detection and face detection routines <ref> [Simmons et al., 1996] </ref>. The command C say sends the string to the speech board. Once the navigate module has successfully completed, Rogue tells Xavier's vision module to observe the room.
Reference: [Stone & Veloso, 1996] <author> Peter Stone and Manuela M. </author> <title> Veloso (1996). User-guided interleaving of planning and execution. </title> <booktitle> In New Directions in AI Planning, </booktitle> <pages> pages 103-112. </pages> <address> (Amsterdam, Netherlands: </address> <publisher> IOS Press). </publisher>
Reference-contexts: Rogue's task planner is based on prod-igy4.0 [Veloso et al., 1995], a domain-independent nonlinear state-space planner that uses means-ends analysis and backward chaining to reason about multiple goals and multiple alternative operators to achieve the goals. It has been extended to support real world execution of its symbolic actions <ref> [Haigh et al., 1997b; Stone & Veloso, 1996] </ref>. Rogue handles multiple asynchronous task requests and controls the real-world execution of Xavier to achieve tasks in this dynamic office delivery domain. <p> EXECUTION AND MONITORING 35 2.3.2.1 prodigy4.0's Mechanisms for Supporting Execution prodigy4.0 allows arbitrary procedural attachments that are called during the operator application phase of the planning cycle <ref> [Haigh et al., 1997b; Stone & Veloso, 1996] </ref>. Typically, we use these functions to give the planner additional information about the state of the world that might not be accurately predictable from the model of the environment. For example, this new information might show resource consumption or action outcomes. <p> In order to execute operators, several mechanisms were added to prodigy4.0 <ref> [Haigh et al., 1997b; Stone & Veloso, 1996] </ref>. The new prodigy4.0/execute algorithm is shown in Table 2.9. First, the designer needs to define the execution behaviour of the operator (Change-state-on-execute). Second, the designer needs to define when to execute the operator (Automatically-decideto-execute). <p> The default behaviour of prodigy4.0/execute is to interactively exploit the user's intuition about the domain <ref> [Stone & Veloso, 1996] </ref>. Rogue, on the other hand, eagerly executes actions, primarily because we want the system to function autonomously.
Reference: [Stone et al., 1994] <author> Peter Stone, Manuela M. Veloso, and Jim Blythe (1994). </author> <title> The need for different domain-independent heuristics. </title> <editor> In K. Hammond, editor, </editor> <booktitle> Artificial Intelligence Planning Systems: Proceedings of the Second International Conference (AIPS-94), </booktitle> <pages> pages 164-169, </pages> <address> Chicago, IL. (Menlo Park, CA: </address> <publisher> AAAI Press). </publisher>
Reference-contexts: Table 2.5 shows the prodigy4.0 planning algorithm, with its main decision consisting of whether to subgoal or apply an operator. Back-Chainer shows the subgoaling decisions made while back-chaining on the plan, and Operator-Application shows how an operator is applied. Rogue runs under prodigy4.0's SABA mode (Subgoal Always Before Apply) <ref> [Stone et al., 1994] </ref>. SABA delays operator application until all subgoals have been expanded. Essentially, this behaviour is equivalent to planning as far in advance as possible, but note that the plan may not be complete, since parts of the plan may depend on having applied other operators.
Reference: [Tan, 1991] <author> Ming Tan (1991). </author> <title> Cost-sensitive robot learning. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, PA. </institution> <note> Available as Technical Report CMU-CS-91-134. </note>
Reference-contexts: These systems observe or experiment in the environment to correct action descriptions, which are then directly used for planning. In the robotics community, closely related work comes from those who have explored learning costs and applicability of actions (e.g. <ref> [Lindner et al., 1994; Shen, 1994; Tan, 1991] </ref>). These systems learn improved domain models and this knowledge is then used by the system's planner, as costs or control knowledge, so that the planner can then select more appropriate actions. Situation-dependent Learning Approach. <p> A good example is Ming Tan's technique of selecting the feature with the maximum marginal information gain: I 2 =C where I is the information gain and C is the cost of measuring the feature <ref> [Tan, 1991] </ref>. In Section 4.4 we describe our re-implementation of the regression tree analysis to cope with feature costs. Feature costs were not an issue for the path planner experiments because the selected features were all high level, with a constant cost to acquire their values. <p> Other researchers have also explored the area of learning action costs. CSL <ref> [Tan, 1991] </ref> and Clementine [Lindner et al., 1994] both learn sensor utilities, including which sensor to use for what information.
Reference: [Thrun, 1996] <author> Sebastian Thrun (1996). </author> <title> A Bayesian approach to landmark discovery in mobile robot navigation. </title> <type> Technical Report CMU-CS-96-122, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: INTRODUCTION Prior Learning Efforts for Robotics. Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. <ref> [Koenig & Simmons, 1996; Kortenkamp & Weymouth, 1994; Thrun, 1996] </ref>), or learning operational parameters for better actuator control (e.g. [Baroglio et al., 1996; Bennett & DeJong, 1996; Grant & Feng, 1989; Pomerleau, 1993]). Instead of improving low-level actuator control, our work focusses at the planning stages of the system. <p> We assume that verification step gives complete and correct information about the robot's actual location; other researchers are focussing on the open problem of sensor reliability <ref> [Hughes & Ranganathan, 1994; Thrun, 1996] </ref>. If Rogue detects that in fact the robot is not at the correct goal location, Rogue updates the navigation module with the new information and re-attempts to navigate to the desired location. <p> The behaviours demonstrated by Rogue under TCA could be easily transferred to another robot control architecture. 5.2 Learning Although there is extensive machine learning research in the artificial intelligence community, very little of it has been applied to real-world domains. Common applications include map learning and localization (e.g. <ref> [Koenig & Simmons, 1996; Kortenkamp & Weymouth, 1994; Thrun, 1996] </ref>), or learning operational parameters for better actuator control (e.g. [Baroglio et al., 1996; Bennett & DeJong, 1996; Pomerleau, 1993]). Instead of improving low-level actuator control, our work focusses instead at the planning stages of the system.
Reference: [Veloso, 1994] <author> Manuela M. </author> <title> Veloso (1994). Planning and Learning by Analogical Reasoning. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <institution> Germany. </institution> <type> PhD Thesis, </type> <note> also available as Technical Report CMU-CS-92-174, </note> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. BIBLIOGRAPHY 173 </address>
Reference-contexts: Rules are incrementally refined and with more training examples will converge towards a possibly disjunctive set of correct rules. Noise is also not accounted for in this system. CHEF [Hammond, 1987], prodigy/analogy <ref> [Veloso, 1994] </ref> and Haigh & Veloso [1997a] use analogical reasoning to create plans based on past successful experiences, where the belief is that past success might help lead to future success.
Reference: [Veloso et al., 1995] <author> Manuela M. Veloso, Jaime Carbonell, M. Alicia Perez, Daniel Borrajo, Eugene Fink, and Jim Blythe (1995). </author> <title> Integrating planning and learning: The prodigy architecture. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 7(1) </volume> <pages> 81-120. </pages>
Reference-contexts: The connotation of a wandering beggar or vagrant is also appropriate. 1.1. APPROACH 3 appropriate plans, delivers them to the robot, monitors their execution, and learns from evaluation of execution performance. Rogue's task planner is based on the prodigy4.0 planning and learning system <ref> [Veloso et al., 1995] </ref>. The challenges for a task planner in this domain are due to the asynchronous goals and the dynamics and uncertainty of the world. The task planner generates and executes plans for multiple interacting goals, which arrive asynchronously and whose structure is not known a priori. <p> Rogue's task planner is based on prod-igy4.0 <ref> [Veloso et al., 1995] </ref>, a domain-independent nonlinear state-space planner that uses means-ends analysis and backward chaining to reason about multiple goals and multiple alternative operators to achieve the goals. It has been extended to support real world execution of its symbolic actions [Haigh et al., 1997b; Stone & Veloso, 1996]. <p> Deliberative 17 18 CHAPTER 2. THE TASK PLANNER behaviours include vision, occupancy grids and topological maps, and path planning and global navigation. prodigy is a domain-independent planner that serves as a testbed for machine learning research <ref> [Carbonell et al., 1990; Veloso et al., 1995] </ref>. The current implementation, prod-igy4.0, is a nonlinear planner that follows a state-space search guided by means-ends analysis and backward chaining. It reasons about multiple goals and multiple alternative operators to achieve the goals.
Reference: [Wang, 1996] <author> Xuemei Wang (1996). </author> <title> Leaning Planning Operators by Observation and Practice. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, PA. </institution> <note> Available as Technical Report CMU-CS-96-154. </note>
Reference-contexts: Artificial intelligence researchers have explored this area extensively, but have generally limited their efforts to simulated worlds with no noise or exogenous events. AI research that most closely resembles ours has explored how to learn and correct action models (e.g. <ref> [Gil, 1992; Pearson, 1996; Wang, 1996] </ref>). These systems observe or experiment in the environment to correct action descriptions, which are then directly used for planning. <p> In the artificial intelligence community, several researchers have explored techniques for learning or changing action models. Most of these systems rely on complete and correct sensing, in simulated environments with no noise or exogenous events. OBSERVER <ref> [Wang, 1996] </ref> and ARMS [Segre, 1991] learn action models by observing another agent's solution; they rely on complete observation of the environment and external agents or noise. Learning is assumed to be correct and irreversible.
Reference: [Williamson & Hanks, 1994] <author> Mike Williamson and Steve Hanks (1994). </author> <title> Optimal planning with a goal-directed utility model. </title> <editor> In K. Hammond, editor, </editor> <booktitle> Artificial Intelligence Planning Systems: Proceedings of the Second International Conference (AIPS-94), </booktitle> <pages> pages 176-180, </pages> <address> Chicago, IL. (Menlo Park, CA: </address> <publisher> AAAI Press). </publisher>
Reference-contexts: This function could easily be replaced with alternatives (e.g. <ref> [Williamson & Hanks, 1994] </ref>). When the deadline is reached, the goal is removed from prodigy4.0's pending goals list; otherwise even an extremely low priority task would eventually be attempted after all other pending tasks have been completed. <p> In the robot control domain, execution efficiency is extremely important, while planning efficiency is much less so. As pointed out by Kibler [1993], the major concern for real-world problems is the quality of the solution and not the speed at which the solution is reached. Pyrrhus <ref> [Williamson & Hanks, 1994] </ref> supports the generation of high quality plans through the use of utility functions. Hand-built domain-dependent control rules use the 5.2. LEARNING 127 utility function to determine which plans to expand and which flaws to fix.
Reference: [Zhao et al., 1994] <author> Min Zhao, Nirwan Ansari, and Edwin S. H. </author> <title> Hou (1994). Mobile manipulator path planning by a genetic algorithm. </title> <journal> Journal of Robotic Systems, </journal> <volume> 11(3) </volume> <pages> 153-153. </pages>
References-found: 88

