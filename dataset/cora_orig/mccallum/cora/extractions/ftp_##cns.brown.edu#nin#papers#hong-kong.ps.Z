URL: ftp://cns.brown.edu/nin/papers/hong-kong.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00155.html
Root-URL: 
Email: shimsh@math.tau.ac.il  
Title: Classifying Seismic Signals by Integrating Ensembles of Neural Networks  
Author: Yair Shimshoni and Nathan Intrator 
Address: Tel Aviv 69978, Israel  
Affiliation: School of Mathematical Sciences, Tel-Aviv University  
Abstract: This paper proposes a classification scheme based on integration of multiple Ensembles of ANNs. It is demonstrated on a classification problem, in which seismic signals of Natural Earthquakes must be distinguished from seismic signals of Artificial Explosions. A Redundant Classification Environment consists of several Ensembles of Neural Networks is created and trained on Bootstrap Sample Sets, using various data representations and architectures. The ANNs within the Ensembles are aggregated (as in Bagging) while the Ensembles are integrated non-linearly, in a signal adaptive manner, using a posterior confidence measure based on the agreement (variance) within the Ensembles. The proposed Integrated Classification Machine achieved 92.1% correct classifications on the seismic test data. Cross Validation evaluations and comparisons indicate that such integration of a collection of ANN's Ensembles is a robust way for handling high dimensional problems with a complex non-stationary signal space as in the current Seismic Classification problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman. </author> <title> Stacked regression, 1992. </title> <type> Technical report TR-367, </type> <institution> Univ. of Cal, Berkely. </institution>
Reference-contexts: A more general framework for combining multiple estimators is "Stacked Generalization" <ref> [19, 1] </ref>, where each estimator is trained with different sub-set of the data and the optimal combination is estimated using cross validation methods. <p> As we use sigmoidal outputs, y 2 <ref> [1; 1] </ref>, the predicted class label is given by thresholding y at zero. Each ANN can be trained on T repeated trials, changing only the initial random weights.
Reference: [2] <author> L. Breiman. Bagging predictors, </author> <year> 1994. </year> <type> Technical report TR-421, </type> <institution> Univ. of Cal, Berkeley. </institution>
Reference-contexts: Combining multiple classifiers can eliminate the need to regularize over-fitted models as it reduces the variance of the combined estimator [12]. A new method which produces an aggregated estimator using Bootstrap replicas of the training data is known as Bagging <ref> [2] </ref>. It is reported to be useful whenever the estimator is un-stable i.e., when perturbing the training set can cause significant changes in the constructed classifier. A Redundant Classification Environment Since the search for an optimal classifier is strongly bound with the search for a suitable data representation. <p> All the Networks in an Ensemble share the same data representation and the same architecture. The Ensemble's prediction-value is defined as the average prediction-values over all the participating Networks, as in `Bagging' <ref> [2] </ref>: y ENS (xjD r ) = B b=1 b (xjD b A collection of Ensembles (K), which use different input representations and architectures, form the Integrated Classification Machine (ICM). <p> The performance of an Ensemble could have been evaluated by averaging the performance of it's 30 Networks and the final integrated classification could have been evaluated accordingly by averaging over the Ensembles. However, this simple bottom-up evaluation does not take into consideration the combined classification <ref> [2, 15] </ref>, thus yields a pessimistic performance estimate. In order to present a more realistic cross validated error estimation, we classify every signal x, by combining only a subset of the Ensemble, namely only those Networks that were trained on data sets which did not contain the signal x.
Reference: [3] <author> F. U. Dowla, S.R. Taylor, and R.W. Anderson. </author> <title> Seismic discrimination with artificial neural networks: Preliminary results with regional spectral data. </title> <journal> Bull. of the Seis. Soc. of Am., </journal> <volume> 80 </volume> <pages> 1346-1373, </pages> <year> 1990. </year>
Reference-contexts: Then will follow the implementation details, namely, the pre-process of the seismic signals, the training and evaluation scheme and the results. Seismic Signal Classification In the recent years researchers have addressed the problems of Seismic Classification using various disciplines other than classic seismologic methods including Artificial Neural Networks <ref> [3, 4] </ref>. The vast majority of the recorded seismicity in most countries is artificial (i.e., man-made events like quarry blasts, mine explosions, etc.).
Reference: [4] <author> P. S. Dysart and J.J. Pulli. </author> <title> Regional seismic event classification at the NORESS array: Seismological measurements and the use of trained neural networks. </title> <journal> Bull. of the Seis. Soc. of Am., </journal> <volume> 80 </volume> <pages> 1910-1933, </pages> <year> 1990. </year>
Reference-contexts: Then will follow the implementation details, namely, the pre-process of the seismic signals, the training and evaluation scheme and the results. Seismic Signal Classification In the recent years researchers have addressed the problems of Seismic Classification using various disciplines other than classic seismologic methods including Artificial Neural Networks <ref> [3, 4] </ref>. The vast majority of the recorded seismicity in most countries is artificial (i.e., man-made events like quarry blasts, mine explosions, etc.). <p> Most of the works done so far on this classification problem concern with regional events vs. nuclear tests, rather than local events vs. conventional explosions at near distances. Many of the methods appear in the literature are based on geophysical parametric models <ref> [4] </ref> which need explicit information to be extracted or estimated by the analyst, thus are very difficult to be fully automated.
Reference: [5] <author> B. Efron and R. Tibshirani. </author> <title> Cross-validation and the bootstrap: Estimating the error rate of a prediction rule. 1995. </title> <type> Technical Report (TR-477), </type> <institution> Dept. of Statistics, Stanford University. </institution>
Reference-contexts: Notice that in a later work the :632 was refined to enable more freedom in choosing the above weights as a function of the relative overfitting rate <ref> [5] </ref>.
Reference: [6] <author> B. Efron and R. J. Tibshirani. </author> <title> An Introduction to the Bootstrap. </title> <publisher> Chapman And Hall, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: Each Ensemble is a collection of B Networks, where every Network is trained on one of B Bootstrap Sample Sets <ref> [6] </ref>, re-sampled from the original data: D b r b = 1; : : : ; B (for some data representation r). All the Networks in an Ensemble share the same data representation and the same architecture. <p> The performance estimates we used for comparison and evaluation of the classifiers, were based on Cross-Validation techniques <ref> [6] </ref>. A single Network's performance (averaged over 5 trials) is estimated via the misclassification rate (MCR) over the test-set corresponds to the train-set on which it was trained. <p> Hence, for all signals, we have produced a combined classification which can be cross-validated, based on approximately 11 Networks in the corresponding subset CV (x) as follows: ^y ENS (x) = B ? b2CV (x) b (x) (5) This estimate is similar to the * 0 in <ref> [6] </ref>, where it is used in a weighted combination with the Apparent error (calculated over the original data set) to construct the :632 estimator: Err :632 = 0:632 * 0 + 0:368 Err app .
Reference: [7] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias-variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: For all three T-F resolutions, the larger models (higher input dimensionalities) yielded better results than models with more smoothened data. This have probably occurred because of the lower bias of the large models due to their greater capacity <ref> [7] </ref>. The exact Bias-Variance calculation which appear in the complete results, showed that the massive averaging decreases the variance of the classifiers, thus eliminating the need for regularization.
Reference: [8] <author> L. K. Hansen and P. Salamon. </author> <title> Neural networks ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 993-1001, </pages> <year> 1990. </year>
Reference-contexts: This posterior confidence is based on the variance of the Networks' prediction-value which represents the amount of `agreement' among all the participating Networks in the Ensemble <ref> [8, 12] </ref>. CON F ENS (x) = [VAR (y NET (x))] 1 (4) 3 Combining the Hierarchy of Classifiers The ICM shown in Figure 1, is a hierarchy of classifiers which are combined in two levels.
Reference: [9] <author> R. A. Jacobs. </author> <title> Methods for combining experts' probability assessments. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 867-888, </pages> <year> 1995. </year>
Reference-contexts: It was shown that in order for the combination of experts to be optimal, the experts should be made as independent as possible <ref> [12, 9, 14] </ref> and the optimal combination of the experts should be estimated in a robust way i.e., by averaging or cross validation techniques rather than by parametric estimation based on the same training data [13].
Reference: [10] <author> R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1) </volume> <pages> 79-87, </pages> <year> 1991. </year>
Reference-contexts: Combining Multiple Estimators In the statistical and machine learning literature there are several methods for combining estimators and the questions involved with this topic mainly what types of estimators to combine and how to combine them, are recently getting considerable attention. Mixture models like Adaptive Mixture of Experts <ref> [10] </ref> and Hierarchical Mixture of Experts [11], are based on the divide and conquer approach in which a mixture of experts compete to gain responsibility in modeling the output in a given input region.
Reference: [11] <author> M. I. Jordan and R. A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: Mixture models like Adaptive Mixture of Experts [10] and Hierarchical Mixture of Experts <ref> [11] </ref>, are based on the divide and conquer approach in which a mixture of experts compete to gain responsibility in modeling the output in a given input region.
Reference: [12] <author> A. Krogh and J. Vedelsby. </author> <title> Neural network ensembles, cross validation, and active learning. </title> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <year> 1995. </year>
Reference-contexts: It was shown that in order for the combination of experts to be optimal, the experts should be made as independent as possible <ref> [12, 9, 14] </ref> and the optimal combination of the experts should be estimated in a robust way i.e., by averaging or cross validation techniques rather than by parametric estimation based on the same training data [13]. <p> Combining multiple classifiers can eliminate the need to regularize over-fitted models as it reduces the variance of the combined estimator <ref> [12] </ref>. A new method which produces an aggregated estimator using Bootstrap replicas of the training data is known as Bagging [2]. It is reported to be useful whenever the estimator is un-stable i.e., when perturbing the training set can cause significant changes in the constructed classifier. <p> This posterior confidence is based on the variance of the Networks' prediction-value which represents the amount of `agreement' among all the participating Networks in the Ensemble <ref> [8, 12] </ref>. CON F ENS (x) = [VAR (y NET (x))] 1 (4) 3 Combining the Hierarchy of Classifiers The ICM shown in Figure 1, is a hierarchy of classifiers which are combined in two levels.
Reference: [13] <author> M. LeBlanc and R. Tibshirani. </author> <title> Combining estimates in regression and classification. </title> <address> NeuroProse, </address> <year> 1993. </year>
Reference-contexts: combination of experts to be optimal, the experts should be made as independent as possible [12, 9, 14] and the optimal combination of the experts should be estimated in a robust way i.e., by averaging or cross validation techniques rather than by parametric estimation based on the same training data <ref> [13] </ref>. Combining multiple classifiers can eliminate the need to regularize over-fitted models as it reduces the variance of the combined estimator [12]. A new method which produces an aggregated estimator using Bootstrap replicas of the training data is known as Bagging [2]. <p> For each one we have constructed a data set D r ; r = 1; : : : ; 8, containing 191 Earthquakes and 189 Explosions. We followed the "Bootstrap Pairs" technique <ref> [13] </ref>, by which every data set D r was pseudo replicated B = 30 times by sampling uniformly with repetitions from the two sets of signals respectively (to preserve the class probability in the resulting replicated samples).
Reference: [14] <author> R. Meir. </author> <title> Bias, variance and the combination of estimators: The case of linear least squares, 1994. </title> <type> Technical report TR-922, </type> <institution> Dept of Electrical Eng., Technion, Haifa, Israel. </institution>
Reference-contexts: It was shown that in order for the combination of experts to be optimal, the experts should be made as independent as possible <ref> [12, 9, 14] </ref> and the optimal combination of the experts should be estimated in a robust way i.e., by averaging or cross validation techniques rather than by parametric estimation based on the same training data [13].
Reference: [15] <author> M. P. Perrone. </author> <title> Improving Regression Estimation: Averaging Methods for Variance Reduction with Extensions to General Convex Measure Optimization. </title> <type> PhD thesis, </type> <institution> Brown University, </institution> <year> 1993. </year>
Reference-contexts: The performance of an Ensemble could have been evaluated by averaging the performance of it's 30 Networks and the final integrated classification could have been evaluated accordingly by averaging over the Ensembles. However, this simple bottom-up evaluation does not take into consideration the combined classification <ref> [2, 15] </ref>, thus yields a pessimistic performance estimate. In order to present a more realistic cross validated error estimation, we classify every signal x, by combining only a subset of the Ensemble, namely only those Networks that were trained on data sets which did not contain the signal x.
Reference: [16] <author> Y. Shimshoni. </author> <title> Classification of seismic signals using ensembles of neural networks results appendix, </title> <year> 1995. </year> <month> (ftp.math.tau.ac.il/pub/shimsh/results.ps.Z). </month>
Reference-contexts: Fifteen Ensembles of 30 Networks each, were trained 5 trials, on 8 different input representation with various numbers of hidden units (2250 ANNs in total). The data set along with the complete results are available by ftp <ref> [16] </ref>. Table 1 shows the performance of the ANN classifiers, along with two other classical classifiers for comparison, LDA and k-NN (applied on the same Bootstrap Sample Sets, under the exact scheme of aggregating multiple classifiers of the ICM).
Reference: [17] <author> Y. Shimshoni and N. Intrator. </author> <title> Automatic discrimination between local earthquakes and quarry blasts by integrating ensembles of neural networks. </title> <booktitle> 1995. Proc. of the 2'nd Workshop for AI in Seismology, </booktitle> <month> Lxembourg </month> <year> 1995. </year>
Reference-contexts: We have detected (automatically) the signals' on-set and extracted a fixed size window of ~ 2000 samples, starting at the on-set. This window is about 45 seconds of recording <ref> [17] </ref>. We have used three spectral decompositions of the waveforms as our basic input representations (denoted: W1,W3,W10). The first decomposition is made of one window of 2048 samples. The second is based on three windows of 1024 samples each, with 384 overlapping samples.
Reference: [18] <author> V. Tresp and M. Taniguchi. </author> <title> Combining estimators using non-constant weighting functions. </title> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <year> 1995. </year>
Reference-contexts: The algorithm that is presented in the next section, finds this optimal Ensemble for each signal x separately, using the classification confidence CON F ENS (x). This approach is different from the linear non-fixed weighting that was suggested by <ref> [18] </ref> for combining ANNs, where all modules participate in the Classification Committee, with weights inversely proportional to the variance (which is similar to the CON F values used here, but estimated in a different way as they use single ANN modules rather than Ensembles of ANNs).
Reference: [19] <author> D. H. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5(2) </volume> <pages> 241-259, </pages> <year> 1992. </year>
Reference-contexts: A more general framework for combining multiple estimators is "Stacked Generalization" <ref> [19, 1] </ref>, where each estimator is trained with different sub-set of the data and the optimal combination is estimated using cross validation methods.
References-found: 19

