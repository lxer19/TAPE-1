URL: ftp://theory.cs.uni-bonn.de/pub/reports/cs-reports/1994/85116-cs.ps.gz
Refering-URL: http://cs.uni-bonn.de/info5/publications/CS-1994-en.html
Root-URL: http://cs.uni-bonn.de
Email: Email: marek@cs.uni-bonn.de  Email: ajm@maths.ox.ac.uk  
Title: Polynomial Bounds for VC Dimension of Sigmoidal Neural Networks  
Author: Marek Karpinski Angus Macintyre 
Note: Research partially supported by the International Computer Science Institute, Berkeley, by the DFG Grant KA 673/4-1, and by the ESPRIT BR Grants 7097 and ECUS 030.  Research supported in part by a Senior Research Fellowship of the SERC.  
Address: 53117 Bonn  Oxford OX1 3LB  
Affiliation: Dept. of Computer Science University of Bonn  Mathematical Institute University of Oxford  
Abstract: We introduce a new method for proving explicit upper bounds on the VC Dimension of general functional basis networks, and prove as an application, for the first time, the VC Dimension of analog neural networks with the sigmoid activation function oe(y) = 1=1 + e y to be bounded by a quadratic polynomial in the number of programmable parameters. 
Abstract-found: 1
Intro-found: 1
Reference: [AB92] <author> M. Anthony, N. Biggs, </author> <title> Computational Learning Theory: An Introduction, </title> <publisher> Cam-bridge University Press, </publisher> <year> 1992. </year>
Reference: [AS93] <author> M. Anthony, J. Shawe-Taylor, </author> <title> A Result of Vapnik with Applications, </title> <journal> Discrete Applied Math. </journal> <volume> 47 (1993), </volume> <pages> pp. 207-217. </pages>
Reference: [BT90] <author> A. Borodin, P. Tiwari, </author> <title> On the Decidability of Sparse Univariate Polynomial Interpolation, </title> <booktitle> Proc. 22nd ACM STOC (1990), </booktitle> <pages> pp. 535-545. </pages>
Reference: [D92] <author> L. van den Dries, </author> <title> Tame Topology and 0-minimal Structures, </title> <type> preprint, </type> <institution> University of Illinois, Urbana, </institution> <note> 1992; to appear as a book. </note>
Reference-contexts: Theorem 2. If M is an o-minimal structure then for every (v; ~y), C has finite V C-dimension. For Theorem 1, see <ref> [D92] </ref>, and [KPS86] for Theorem 2, see [L92].
Reference: [DMM94] <author> L. van den Dries, A.Macintyre and D.Marker, </author> <title> The Elementary Theory of Restricted Analytic Fields with Exponentation, </title> <booktitle> Annuals of Mathematics 140 (1994), </booktitle> <pages> pp 183-205. </pages>
Reference-contexts: Theorem 7. VC-dim () [2 log B + (17 log s)`]. Proof. Done. 2 2.6. An example involving exponentiation. We work with +, -, ., 0,1, &lt;, e x , and appeal to Wilkie's work [W94], or <ref> [DMM94] </ref>, for a proof of o-minimality. Let us suppose about that its terms o i (v; ~y)(i s) are polynomials of degree d in v; ~y and no more than q subterms exp (g (v; ~y)), where g is linear.
Reference: [GJ93] <author> P.Goldberg and M.Jerrum, </author> <title> Bounding the Vapnik Chervonenkis Dimension of Con--cept Classes Parametrized by Real Numbers. </title> <journal> Machine Learning, </journal> <note> 1994 (to appear). A preliminary version appeared in Proc. 6th ACM Workshop on Computational Learning Theory, pp. 361-369, </note> <year> 1993. </year>
Reference-contexts: 0 Introduction The most commonly used activation function in various neural networks applications is the sigmoid oe (y) = 1=1 + e y (cf. [HKP91]). In Maass's 1993 lecture notes [M93], Open Problem 10 (see also <ref> [GJ93] </ref> and [MS93]) asks: Is the VC-dimension of analog neural nets with the sigmoid activation function oe (y) = 1=1 + e y bounded by a polynomial in the number of programmable parameters? (In [MS93] the finiteness of VC Dimension of sigmoidal neural networks has been established for the first time.) <p> The details are given in Sections 2-3. The result is a special case of a much more general result about bounds for VC dimension in o-minimal theories. The paper was inspired by the work of Goldberg and Jerrum <ref> [GJ93] </ref>, who could deal with polynomial activation functions. A reference in [GJ93], to Warren's paper [W68], was of particular importance. Some other applications of our method will appear in the full version of this paper. 1 Model-theoretic Preliminaries The principles behind this paper are of great generality. <p> The details are given in Sections 2-3. The result is a special case of a much more general result about bounds for VC dimension in o-minimal theories. The paper was inspired by the work of Goldberg and Jerrum <ref> [GJ93] </ref>, who could deal with polynomial activation functions. A reference in [GJ93], to Warren's paper [W68], was of particular importance. Some other applications of our method will appear in the full version of this paper. 1 Model-theoretic Preliminaries The principles behind this paper are of great generality. <p> We do not seek here maximum generality, but restrict ourselves to work over the field of real numbers. (The general reader is also referred to [MS93] and <ref> [GJ93] </ref> for the definitions concerning Vapnik-Chervonenkis (VC) dimension.) We work with structure M which are enrichments of the real field R by certain total C 1 (infinitely differentiable) functions. <p> To handle general oe ( ff; ~ fi) one uses a variational argument (Corollary 2.1 in <ref> [GJ93] </ref>) which is everywhere dense in what follows. <p> reference is that the number oe ( ff; ~ fi) is bounded by the number of connected components of R `+1 n ih [f (~y; ") : o i ( ff; ~y) = "g); and this has a bound independent of ff. 2.2 Now we run through the argument of <ref> [GJ93] </ref>. Let (v 1 ; ; v k ; y 1 ; ; y ` ) be quantifier-free with terms o i (v; ~y); i s. <p> Let f ff 1 ; ; ff v g be distinct elements of R k such that f ff 1 ; ff v g is shattered by C . Then exactly as in <ref> [GJ93] </ref> one sees: 2 v the number of sequences of signs f+, -, 0g obtainable from ho 1 ( ff 1 ; ~y); o 1 ( ff 2 ; ~y); ; o 1 ( ff v ; ~y); o 2 ( ff 1 ; ~y); Note that the latter sequence has
Reference: [H92] <author> D. Haussler, </author> <title> Decision Theoretic Generalizations of the PAC Model for Neural Net and other Learning Applications, Information an Computation 100, </title> <booktitle> (1992), </booktitle> <pages> pp. 78-150. </pages>
Reference: [HKP91] <author> J. Hertz, A. Krogh and R. G. Palmer, </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: 0 Introduction The most commonly used activation function in various neural networks applications is the sigmoid oe (y) = 1=1 + e y (cf. <ref> [HKP91] </ref>).
Reference: [H76] <author> M. W. Hirsch, </author> <title> Differential Topology, </title> <publisher> Springer-Verlag, </publisher> <year> 1976. </year>
Reference: [KW93] <author> M.Karpinski and T.Werther, </author> <title> VC Dimension and Uniform Learnability of Sparse Polynomials and Rational Functions, </title> <journal> SIAM J. Computing 22 (1993), </journal> <pages> pp 1276-1285. </pages>
Reference: [K91] <editor> A.G.Khovanski, Fewnomials, </editor> <publisher> American Mathematical Society, </publisher> <address> Providence, R.I., </address> <year> 1991. </year>
Reference-contexts: Let us suppose about that its terms o i (v; ~y)(i s) are polynomials of degree d in v; ~y and no more than q subterms exp (g (v; ~y)), where g is linear. Khovanski <ref> [K91] </ref> has proved a basic result relating to this situation, namely: Theorem 8. <p> So applying the above, in the notation of 2.5 Then log B q (q 1)=2 + ` log d So in this case V C-dimension of C q (q 1)=2 + q log (` + 1)(d + 1) 2.7 Application to sparse formulas. Since Khovanski's <ref> [K91] </ref> one has known how to use Finiteness Theorems about exponentiation to give uniform estimates in problems involving families of polynomials where there is an absolute bound to the number of nonzero coefficients occurring, but no bound on the degree of the polynomials. <p> Then for fixed ff, the number of connected components in R `+2m of ( ff; z; ~y) = 0 is, by <ref> [K91] </ref>, 2 m (m1)=2 (2d) `+2m and this clearly gives a bound for the number for o ( ff; ~y) = ": But we need to handle j; o ( ff i ; ~y) = " i together.
Reference: [KPS86] <author> J.Knight, A.Pillay and C.Steinhorn, </author> <title> Definable Sets and Ordered Structures II, </title> <journal> Trans. American Mathematical Society 295 (1986), pp.593-605. </journal>
Reference-contexts: Theorem 2. If M is an o-minimal structure then for every (v; ~y), C has finite V C-dimension. For Theorem 1, see [D92], and <ref> [KPS86] </ref> for Theorem 2, see [L92]. <p> That the intersection of more than ` of the M i is empty follows from regularity and a dimension count. The finiteness of components follows from o-minimality, since each original component is definable ([D92], <ref> [KPS86] </ref>). And now we come to the crunch, which reduces all calculations, via small perturbations, to ones covered by Theorem 4. This corresponds to Warren's Lemmas 2.2, 2.3 and 2.4. We have to change his argument for 2.2, which appeals to complex projective geometry. We use instead Sard's Theorem [M65].
Reference: [L92] <author> M.C.Laskowsky, </author> <title> Vapnik-Chervonenkis Classes od Definable Sets, </title> <journal> J.London Math. Society 45 (1992), </journal> <pages> pp 377-384. </pages>
Reference-contexts: In this paper we will give good bounds for the VC dimension of C , for many natural . The following notion has in the last decade become central in the model theory of analysis <ref> [L92] </ref>. Definition. M is o-minimal if for every formula (v 1 ; y 1 ; y ` ) and every ~ fi 2 M ` , ~ fi is a finite union of intervals with endpoints in M [ f1g. <p> Theorem 2. If M is an o-minimal structure then for every (v; ~y), C has finite V C-dimension. For Theorem 1, see [D92], and [KPS86] for Theorem 2, see <ref> [L92] </ref>. <p> Let (v; ~y) be o (v; ~y) &gt; 0: Then (by definition) the V C-dimension of A is the V C-dimension of C . By <ref> [L92] </ref> (which appeals to Wilkie's [W94]) this dimension is finite, since oe is definable in +; ; ; 0; 1; e x . Given a sigmoidal network architecture A, we now apply our results to get a very good estimate for V C dim (A).
Reference: [M93] <author> W.Maass, </author> <title> On the Complexity of Learning on Feedforward neural Nets, </title> <booktitle> in Proc. EATCS Advanced School on Computational Learning and Cryptography, Vietri sul Mare, </booktitle> <year> 1993. </year>
Reference-contexts: 0 Introduction The most commonly used activation function in various neural networks applications is the sigmoid oe (y) = 1=1 + e y (cf. [HKP91]). In Maass's 1993 lecture notes <ref> [M93] </ref>, Open Problem 10 (see also [GJ93] and [MS93]) asks: Is the VC-dimension of analog neural nets with the sigmoid activation function oe (y) = 1=1 + e y bounded by a polynomial in the number of programmable parameters? (In [MS93] the finiteness of VC Dimension of sigmoidal neural networks has <p> Since the pseudo-dimension of an architecture A is bounded by the VC-Dimension of a new architecture A 0 (see [MS93]) got directly from A, we get polynomial bounds for the pseudo-dimension. This answers affirmatively the second part of problem 10 in <ref> [M93] </ref>. 2 Acknowledgement: We thank Gregory Cherlin, Mark Jerrum and Eduardo Sontag for a number of stimulating remarks and discussions.
Reference: [MSS91] <author> W. Maass, G. Schnitger and E. D. Sontag, </author> <title> On the Computational Power of Sig-moidal versus Boolean Threshold Circuits, </title> <booktitle> Proc. 32nd IEEE FOCS (1991), </booktitle> <pages> pp. 767-776. </pages>
Reference: [MS93] <author> A.J.Macintyre and E.D.Sontag, </author> <title> Finiteness results for Sigmoidal Neural Networks, </title> <booktitle> Proc. 25th ACM STOC (1993), </booktitle> <address> pp.325-334. </address>
Reference-contexts: 0 Introduction The most commonly used activation function in various neural networks applications is the sigmoid oe (y) = 1=1 + e y (cf. [HKP91]). In Maass's 1993 lecture notes [M93], Open Problem 10 (see also [GJ93] and <ref> [MS93] </ref>) asks: Is the VC-dimension of analog neural nets with the sigmoid activation function oe (y) = 1=1 + e y bounded by a polynomial in the number of programmable parameters? (In [MS93] the finiteness of VC Dimension of sigmoidal neural networks has been established for the first time.) In this <p> In Maass's 1993 lecture notes [M93], Open Problem 10 (see also [GJ93] and <ref> [MS93] </ref>) asks: Is the VC-dimension of analog neural nets with the sigmoid activation function oe (y) = 1=1 + e y bounded by a polynomial in the number of programmable parameters? (In [MS93] the finiteness of VC Dimension of sigmoidal neural networks has been established for the first time.) In this paper we give an affirmative answer, with a quadratic polynomial in a number of programable parameters. <p> We do not seek here maximum generality, but restrict ourselves to work over the field of real numbers. (The general reader is also referred to <ref> [MS93] </ref> and [GJ93] for the definitions concerning Vapnik-Chervonenkis (VC) dimension.) We work with structure M which are enrichments of the real field R by certain total C 1 (infinitely differentiable) functions. <p> ` [2 q (q1)=2 [2 (` + 1)] `+q ; so in usual notation log B ` log 3 + q (q 1)=2 whence V C dim C + `(log 3 + log 2 (` + 1) + 17 log s): 3 Application to sigmoidal neural networks We define (cf. <ref> [MS93] </ref>) a sigmoidal network architecture A. <p> We refer to <ref> [MS93] </ref> for the definition of the pseudo-dimension of an architecture. Since the pseudo-dimension of an architecture A is bounded by the VC-Dimension of a new architecture A 0 (see [MS93]) got directly from A, we get polynomial bounds for the pseudo-dimension. <p> We refer to <ref> [MS93] </ref> for the definition of the pseudo-dimension of an architecture. Since the pseudo-dimension of an architecture A is bounded by the VC-Dimension of a new architecture A 0 (see [MS93]) got directly from A, we get polynomial bounds for the pseudo-dimension. This answers affirmatively the second part of problem 10 in [M93]. 2 Acknowledgement: We thank Gregory Cherlin, Mark Jerrum and Eduardo Sontag for a number of stimulating remarks and discussions.
Reference: [M64] <author> J.Milnor, </author> <title> On the Betti Numbers of Real Varieties, </title> <booktitle> Proc. of the American Mathematical Society 15 (1964), </booktitle> <pages> pp 275-280. </pages>
Reference-contexts: Classical example. a polynomial of ~y degree d. Then (; m) can be taken as 2:(2d) ` . This is due to Milnor <ref> [M64] </ref>. For m = 1 one has the bound 2:d ` , and the general case reduces to this by replacing by P We shall see later the exponential analogue. In an o-minimal theory, of course exists.
Reference: [M65] <author> J.Milnor, </author> <title> Topology from the Differentiable Viewpoint, </title> <address> Univ.Press, Virginia, </address> <year> 1965. </year>
Reference-contexts: And now we come to the crunch, which reduces all calculations, via small perturbations, to ones covered by Theorem 4. This corresponds to Warren's Lemmas 2.2, 2.3 and 2.4. We have to change his argument for 2.2, which appeals to complex projective geometry. We use instead Sard's Theorem <ref> [M65] </ref>. Lemma 5. Let 1 ( ff 1 ; ~y); ; m ( ff m ; ~y) be as usual, and o ( ff; ~y) arbitrary.
Reference: [TV94] <author> G. Turan and F. Vatan, </author> <title> On the Computation of Boolean Functions by Analog Circuits of Bounded Fan-in, </title> <booktitle> Proc. 35th IEEE FOCS (1994), </booktitle> <pages> pp. 553-564. </pages>
Reference: [W68] <author> H.E.Warren, </author> <title> Lower Bounds for Approximation by Non-linear Manifolds, Trans. </title> <booktitle> of the AMS 133 (1968), </booktitle> <pages> pp. 167-178. </pages>
Reference-contexts: The result is a special case of a much more general result about bounds for VC dimension in o-minimal theories. The paper was inspired by the work of Goldberg and Jerrum [GJ93], who could deal with polynomial activation functions. A reference in [GJ93], to Warren's paper <ref> [W68] </ref>, was of particular importance. Some other applications of our method will appear in the full version of this paper. 1 Model-theoretic Preliminaries The principles behind this paper are of great generality. <p> In an o-minimal theory, of course exists. We show in 2.4 how the right hand side of (*) may be estimated in terms of . The idea comes from Warren's 1968 paper <ref> [W68] </ref>, and we now use the C 1 property of M for the first time. 2.4 We assume given terms 1 (v; ~y); ; n (v; ~y) and ff 1 ; ; ff n 2 R k : We consider the definition fy : in and say it is nonsingular if
Reference: [W94] <author> A.J.Wilkie, </author> <title> Model Completeness Results of Restricted Pfaffian Functions and the Exponential Function, </title> <note> to appear in Journal of the AMS, 1994. 14 </note>
Reference-contexts: Theorem 7. VC-dim () [2 log B + (17 log s)`]. Proof. Done. 2 2.6. An example involving exponentiation. We work with +, -, ., 0,1, &lt;, e x , and appeal to Wilkie's work <ref> [W94] </ref>, or [DMM94], for a proof of o-minimality. Let us suppose about that its terms o i (v; ~y)(i s) are polynomials of degree d in v; ~y and no more than q subterms exp (g (v; ~y)), where g is linear. <p> Let (v; ~y) be o (v; ~y) &gt; 0: Then (by definition) the V C-dimension of A is the V C-dimension of C . By [L92] (which appeals to Wilkie's <ref> [W94] </ref>) this dimension is finite, since oe is definable in +; ; ; 0; 1; e x . Given a sigmoidal network architecture A, we now apply our results to get a very good estimate for V C dim (A).
References-found: 21

