URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/111.ps.gz
Refering-URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/
Root-URL: http://www.cs.wisc.edu
Title: Two Computer Systems Paradoxes: Serialize-to-Parallelize, and Queuing Concurrent-Writes (Extended Abstract) providing useful, easy-to-program programming paradigms
Author: Rimon Orni and Uzi Vishkin 
Note: Assuming that  computer systems is of interest, this work is a modest  This work may be the first to relate  
Date: October 19, 1995  
Abstract: We present and examine the following Serialize-to-Parallelize Paradox: suppose a programmer has a parallel algorithm in mind; the programmer must serialize the algorithm, and is actually trained to suppress its parallelism, while writing code; later, however, compilation and runtime techniques are used to reverse the results of this serialization effort and extract as much parallelism as possible. This work actually provides examples where parallel or parallel-style code enables extracting more parallelism than standard serial code. The "arbitrary concurrent-write" convention is useful in parallel algorithms and programs and appears to be not too difficult to implement in hardware for serial machines. Still, typically concurrent-writes to the same memory location in a program are implemented by queuing the write operations, thus requiring time linear in the number of writes. We call this the Queuing Concurrent-Writes Paradox. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ANSI. </author> <title> ANSI Fortran draft S8, </title> <type> version 111. </type>
Reference-contexts: We only state that we would like to see parallel programs which implement in code the PRAM programmer's model, much in the way of parallel programming languages such as Fortran90 <ref> [1] </ref>, HPF [11], and C* [21], or NESL for functional programming [3]. Programming should be relatively easy, as suggested by the wealth of literature on this general approach to parallel algorithms and parallel programming.
Reference: [2] <author> T. M. Austin and G. S. Sohi. </author> <title> Dynamic dependency analysis of ordinary programs. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: Much effort has been devoted to measuring the amount of parallelism which can be automatically extracted from programs by these techniques <ref> [17, 5, 19, 26, 2] </ref>. These studies suggest that the amount of parallelism which can be thus extracted is limited, even when we invest much hardware and software in the effort, and is likely to be insufficient as the reliance on ILP increases.
Reference: [3] <author> G. E. Blelloch, S. Chatterjee, J. C. Hardwick, J. Sipelstein, and M. Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <booktitle> In 4th Symposium on Principles and Practice of Parallel Programming. ACM SIGPLAN, </booktitle> <year> 1993. </year>
Reference-contexts: We only state that we would like to see parallel programs which implement in code the PRAM programmer's model, much in the way of parallel programming languages such as Fortran90 [1], HPF [11], and C* [21], or NESL for functional programming <ref> [3] </ref>. Programming should be relatively easy, as suggested by the wealth of literature on this general approach to parallel algorithms and parallel programming. We demonstrate the usefulness of our approach by measuring the added amount of parallelism which could be extracted by the system we envision.
Reference: [4] <author> G. E. Blelloch and G. W. Sabot. </author> <title> Compiling collection-oriented languages onto massively parallel computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 119-134, </pages> <year> 1990. </year>
Reference: [5] <author> D.-K. Chen, H.-M. Su, and P.-C. Yew. </author> <title> The impact of synchronization and granularity on parallel systems. </title> <booktitle> In 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 239-248, </pages> <year> 1990. </year>
Reference-contexts: Much effort has been devoted to measuring the amount of parallelism which can be automatically extracted from programs by these techniques <ref> [17, 5, 19, 26, 2] </ref>. These studies suggest that the amount of parallelism which can be thus extracted is limited, even when we invest much hardware and software in the effort, and is likely to be insufficient as the reliance on ILP increases.
Reference: [6] <author> D. M. Eckstein. </author> <title> Parallel Processing using Depth-First Search and Breadth-First Search. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of Iowa, </institution> <address> Iowa City, IA, </address> <year> 1977. </year>
Reference-contexts: The full paper gives the sequential algorithm (pseudo) code (SAC) of this program. As has been observed by Eckstein in <ref> [6] </ref>, all the nodes in the same layer could in fact be visited concurrently.
Reference: [7] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependence graph and its uses in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <year> 1987. </year>
Reference-contexts: We also enhanced the LW Simulator so that it could simulate the capability of the envisioned system in addition to the automatic extraction capabilities it originally assumes. The new simulator is henceforth referred to as the Enhanced simulator. Ferrante et al. <ref> [7] </ref> introduced the Program Dependence Graph (PDG) which is a Directed 5 Acyclic Graph (DAG) representation of the program, incorporating the true data and con-trol dependences. The nodes of the PDG represent the instructions and the edges represent dependences between the instructions.
Reference: [8] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> The QRQW PRAM: Accounting for contention in parallel algorithms. </title> <booktitle> In Proceedings of the Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 638-648, </pages> <year> 1994. </year>
Reference-contexts: This is an overkill: correctness is indeed preserved but parallelism is lost. We see no inherent difficulty in preserving both parallelism and correctness for a parallel program being executed on a uniprocessor. It is interesting to relate this last paradox to the work of Gibbons, Matias and Ra-machandrans in <ref> [8] </ref>. They contributed a modeling of the queuing of concurrent writes (QRQW PRAM model).
Reference: [9] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufman Publishers Inc., </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference: [10] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture A Quantitative Approach, page 346. </title> <publisher> Morgan Kaufman Publishers Inc., </publisher> <address> San Mateo, California, </address> <note> second edition, 1994. Uncorrected preliminary manuscript. </note>
Reference: [11] <author> High Performance Fortran Forum. </author> <title> High performance Fortran language specification, </title> <year> 1993. </year>
Reference-contexts: We only state that we would like to see parallel programs which implement in code the PRAM programmer's model, much in the way of parallel programming languages such as Fortran90 [1], HPF <ref> [11] </ref>, and C* [21], or NESL for functional programming [3]. Programming should be relatively easy, as suggested by the wealth of literature on this general approach to parallel algorithms and parallel programming.
Reference: [12] <author> W. W. Hwu, T. M. Conte, and P. P. Chang. </author> <title> Comparing software and hardware schemes for reducing the cost of branches. </title> <booktitle> In 13th International Symposium on Computer Architecture, </booktitle> <pages> pages 224-233, </pages> <year> 1989. </year>
Reference: [13] <author> J. JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison Wesley Publishing Company, </publisher> <year> 1992. </year>
Reference-contexts: It is used by the so-called "work-time methodology" for describing parallel algorithms. This methodology was first presented in [23] (relying on a 1974 theorem by Brent for an abstract model of parallel computation). Later the book <ref> [13] </ref> used the methodology as a framework for describing most of the parallel algorithms presented there. For brevity, we avoid elaborating on this issue here. 4 paper, the program with the smaller number of operations is shown to also have fewer parallel cycles, making a comparison easy.
Reference: [14] <author> R. B. Jones and V. H. Allan. </author> <title> Software pipelining: An evaluation of enhanced pipelining. </title> <booktitle> In Proceedings of the 24th International Symposium on Microarchitecture, </booktitle> <pages> pages 82-92, </pages> <year> 1991. </year>
Reference: [15] <author> D. E. Knuth. </author> <title> The Stanford GraphBase. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: These graphs were constructed using The Stanford GraphBase <ref> [15] </ref>. Results for all abstract machine models and additional input graphs appear in the full paper. For brevity, we have excluded from the current paper results for other families of graphs.
Reference: [16] <author> S. M. Krishnamurthy. Ultra-pipelining: </author> <title> An efficient software pipelining algorithm. </title> <booktitle> In COMPCON Spring '93, </booktitle> <pages> pages 586-591, </pages> <year> 1993. </year>
Reference: [17] <author> M. Kumar. </author> <title> Measuring parallelism in computation-intensive scientific/engineering applications. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(9) </volume> <pages> 1088-1098, </pages> <year> 1988. </year>
Reference-contexts: Much effort has been devoted to measuring the amount of parallelism which can be automatically extracted from programs by these techniques <ref> [17, 5, 19, 26, 2] </ref>. These studies suggest that the amount of parallelism which can be thus extracted is limited, even when we invest much hardware and software in the effort, and is likely to be insufficient as the reliance on ILP increases.
Reference: [18] <author> M. Lam. </author> <title> Software pipelining: An effective scheduling technique for vliw machines. </title> <booktitle> In SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328. </pages> <publisher> ACM, </publisher> <year> 1988. </year>
Reference: [19] <author> M. S. Lam and R. P. Wilson. </author> <title> Limits of control flow on parallelism. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 46-57, </pages> <year> 1992. </year>
Reference-contexts: Much effort has been devoted to measuring the amount of parallelism which can be automatically extracted from programs by these techniques <ref> [17, 5, 19, 26, 2] </ref>. These studies suggest that the amount of parallelism which can be thus extracted is limited, even when we invest much hardware and software in the effort, and is likely to be insufficient as the reliance on ILP increases. <p> The simulator we used to produce the instruction trace and to analyze it is the one used by Lam and Wilson in <ref> [19] </ref> for MIPS instruction code. We also used an enhanced form of the simulator which analyzes the instructions as if they were coming from parallel code (as explained in the full paper). <p> It also explains in further detail the simulators used to measure parallelism, and presents the program we chose as input for these measurements and presents the full results obtained in our tests. 2 Overview of Tests and Results In studies done by Lam and Wilson <ref> [19] </ref> and by Wall [26] the amount of parallelism in programs was measured by running the instruction trace through a simulator, as explained below. <p> The abovementioned studies measured in this manner the parallelism extracted from a chosen set of standard benchmarks. For the strongest and most unrealistic abstract machine models, a mean parallelism of 23.6 was found in [26] and of 158.26 in <ref> [19] </ref>. When some of the unrealistic capabilities were modified to more feasible ones, the mean parallelism for their respective benchmarks dropped to 11.9 in [26] and to 39.62 in [19]. <p> the strongest and most unrealistic abstract machine models, a mean parallelism of 23.6 was found in [26] and of 158.26 in <ref> [19] </ref>. When some of the unrealistic capabilities were modified to more feasible ones, the mean parallelism for their respective benchmarks dropped to 11.9 in [26] and to 39.62 in [19]. We propose an envisioned system in which the programmer can identify parts of the code as parallelizable, and the compiler and hardware have the capability to utilize this parallelism, as well as extract parallelism automatically. <p> We examined the extent to which the envisioned system could increase the amount of parallelism which can be extracted overall. For this, we used the simulator written by Robert P. Wilson for the study in <ref> [19] </ref> (henceforth the LW simulator). We also enhanced the LW Simulator so that it could simulate the capability of the envisioned system in addition to the automatic extraction capabilities it originally assumes. The new simulator is henceforth referred to as the Enhanced simulator. <p> For (1) and (2) we simply ran the Lam and Wilson simulator over the MIPS trace of the respective code. In each of the tests we obtained results for each of the seven abstract machine models defined by Lam and Wilson in <ref> [19] </ref>. Table 1 gives a representative sample of our results.
Reference: [20] <author> S. McFarling and J. Hennessy. </author> <title> Reducing the cost of branches. </title> <booktitle> In 13th Annual International Symposium on Computer Architecture, Tokyo, </booktitle> <pages> pages 396-403, </pages> <year> 1986. </year>
Reference: [21] <author> J. R. Rose and G. L. Steele, Jr. </author> <title> C*: An extended c language for data parallel programming. </title> <booktitle> In Proceedings Second International Conference on Supercomputing, </booktitle> <volume> volume 2, </volume> <pages> pages 2-16, </pages> <year> 1987. </year>
Reference-contexts: We only state that we would like to see parallel programs which implement in code the PRAM programmer's model, much in the way of parallel programming languages such as Fortran90 [1], HPF [11], and C* <ref> [21] </ref>, or NESL for functional programming [3]. Programming should be relatively easy, as suggested by the wealth of literature on this general approach to parallel algorithms and parallel programming.
Reference: [22] <author> Y. Shiloach and U. Vishkin. </author> <title> An O(logn) parallel connectivity algorithm. </title> <journal> Journal of Algorithms, </journal> <volume> 3(1) </volume> <pages> 57-56, </pages> <year> 1982. </year>
Reference-contexts: In a parallel program several instructions that can be executed in parallel may include an update of the same variable. Suppose the programming language allows the programmer to specify that only one of these updates, chosen arbitrarily, should occur (the so-called Arbitrary Concurrent-Write primitive in <ref> [22] </ref>). Most current parallel (and serial) systems implement concurrent writes by queuing all the updates and handling them in sequence, relying on run-time protocols resolving "Write After Write" data dependences. This is an overkill: correctness is indeed preserved but parallelism is lost.
Reference: [23] <author> Y. Shiloach and U. Vishkin. </author> <title> An O(n 2 logn) parallel max-flow algorithm. </title> <journal> Journal of Algorithms, </journal> <volume> 3(2) </volume> <pages> 128-146, </pages> <year> 1982. </year>
Reference-contexts: It is used by the so-called "work-time methodology" for describing parallel algorithms. This methodology was first presented in <ref> [23] </ref> (relying on a 1974 theorem by Brent for an abstract model of parallel computation). Later the book [13] used the methodology as a framework for describing most of the parallel algorithms presented there.
Reference: [24] <author> M. D. Smith. </author> <title> Tracing with pixie. </title> <type> Version 1.1, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference: [25] <author> U. Vishkin. </author> <title> Can parallel algorithms enhance serial implementation? (extended abstract). </title> <booktitle> In 8th International Parallel Processing Symposium. IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: We chose Breadth First Search as a test algorithm. We showed that even for existing systems, running strictly sequential code, we can significantly improve ILP by identifying parallelism in the algorithm and designing the code so that it reveals this parallelism. The current paper enforces the call in <ref> [25] </ref> to start immediately including the topic of parallel algorithms and parallel algorithmic thinking in the standard curriculum for computer science and engineering undergraduates, since this curriculum should expose them to principles which they need to command as they graduate as well as over a life time career.
Reference: [26] <author> D. W. Wall. </author> <title> Limits of instruction-level parallelism. </title> <type> RR 93/6, </type> <institution> Digital Western Research Laboratory, Palo Alto, California, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Much effort has been devoted to measuring the amount of parallelism which can be automatically extracted from programs by these techniques <ref> [17, 5, 19, 26, 2] </ref>. These studies suggest that the amount of parallelism which can be thus extracted is limited, even when we invest much hardware and software in the effort, and is likely to be insufficient as the reliance on ILP increases. <p> It also explains in further detail the simulators used to measure parallelism, and presents the program we chose as input for these measurements and presents the full results obtained in our tests. 2 Overview of Tests and Results In studies done by Lam and Wilson [19] and by Wall <ref> [26] </ref> the amount of parallelism in programs was measured by running the instruction trace through a simulator, as explained below. <p> The abovementioned studies measured in this manner the parallelism extracted from a chosen set of standard benchmarks. For the strongest and most unrealistic abstract machine models, a mean parallelism of 23.6 was found in <ref> [26] </ref> and of 158.26 in [19]. When some of the unrealistic capabilities were modified to more feasible ones, the mean parallelism for their respective benchmarks dropped to 11.9 in [26] and to 39.62 in [19]. <p> For the strongest and most unrealistic abstract machine models, a mean parallelism of 23.6 was found in <ref> [26] </ref> and of 158.26 in [19]. When some of the unrealistic capabilities were modified to more feasible ones, the mean parallelism for their respective benchmarks dropped to 11.9 in [26] and to 39.62 in [19]. We propose an envisioned system in which the programmer can identify parts of the code as parallelizable, and the compiler and hardware have the capability to utilize this parallelism, as well as extract parallelism automatically.
Reference: [27] <author> S. Weiss and J. E. Smith. </author> <title> Power and PowerPC. </title> <publisher> Morgan Kaufman Publishers Inc., </publisher> <address> San Francisco, CA, </address> <year> 1994. </year> <month> 12 </month>
References-found: 27

