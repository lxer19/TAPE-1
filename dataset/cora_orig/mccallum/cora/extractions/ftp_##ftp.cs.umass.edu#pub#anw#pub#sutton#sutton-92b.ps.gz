URL: ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/sutton-92b.ps.gz
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Email: sutton@gte.com  
Title: Gain Adaptation Beats Least Squares?  
Author: Richard S. Sutton 
Address: Waltham, MA 02254  
Affiliation: GTE Laboratories Incorporated  
Note: Appeared in Proceedings of the Seventh Yale Workshop on Adaptive and Learning Systems, pp. 161-166, 1992.  
Abstract: I present computational results suggesting that gain-adaptation algorithms based in part on connectionist learning methods may improve over least squares and other classical parameter-estimation methods for stochastic time-varying linear systems. The new algorithms are evaluated with respect to classical methods along three dimensions: asymptotic error, computational complexity, and required prior knowledge about the system. The new algorithms are all of the same order of complexity as LMS methods, O(n), where n is the dimensionality of the system, whereas least-squares methods and the Kalman filter are O(n 2 ). The new methods also improve over the Kalman filter in that they do not require a complete statistical model of how the system varies over time. In a simple computational experiment, the new methods are shown to produce asymptotic error levels near that of the optimal Kalman filter and significantly below those of least-squares and LMS methods. The new methods may perform better even than the Kalman filter if there is any error in the filter's model of how the system varies over time. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Gluck, M.A., Glauthier, P.T., & Sutton, </author> <title> R.S. (1992) Adaptation of cue-specific learning rates in adaptive networks: Computational and psychological perspectives. </title> <booktitle> Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society. </booktitle>
Reference: <author> Goodwin, </author> <title> G.C. & Sin, K.S. (1984) Adaptive Filtering Prediction and Control. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1984. </year>
Reference-contexts: These estimates are exactly correct for the system used in the computational experiment. The normalized LMS (NLMS) algorithm uses K (t) = ^ R + (t) T (t) where 2 &gt; &gt; 0. This algorithm is normally slightly more efficient than LMS <ref> (see Goodwin & Sin, 1984) </ref> We turn now to the least-squares and Kalman-filter algorithms. Given the form of the system time variation, (2), the appropriate least-squares method is least squares with covariance modification (e.g., see Goodwin & Sin, 1984). <p> This algorithm is normally slightly more efficient than LMS (see Goodwin & Sin, 1984) We turn now to the least-squares and Kalman-filter algorithms. Given the form of the system time variation, (2), the appropriate least-squares method is least squares with covariance modification <ref> (e.g., see Goodwin & Sin, 1984) </ref>. <p> This choice treats all input signals symmetrically, and reflects no prior knowledge of correlations among the drift variables. If ^ Q = Q and ^ R = R, then the algorithm given above is the Kalman filter <ref> (see, e.g., Goodwin & Sin, 1984) </ref>.
Reference: <author> Jacobs, R.A. </author> <title> (1988) Increased rates of convergence through learning rate adaptation. </title> <booktitle> Neural Networks 1, </booktitle> <pages> 295-307. </pages>
Reference: <author> Kesten, H. </author> <title> (1958) Accelerated stochastic approximation. </title> <journal> Annals of Mathematical Statistics 29, </journal> <pages> 41-59. </pages>
Reference: <author> Lee, Y. & Lippmann, </author> <title> R.P. (1990) Practical characteristics of neural network and conventional pattern classifiers on artificial and speech problems. </title> <booktitle> In Advances in Neural Information Processing Systems 2, </booktitle> <editor> D.S. Touretzky, Ed., </editor> <address> 168-177, </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sanger T.D. </author> <title> (1991) Basis-function trees as a generalization of local variable selection methods for function approximation. </title> <booktitle> In: Advances in Neural Information Processing Systems 3, </booktitle> <editor> Lippmann R.P., Moody J.E., Touretzky D.S., Eds., </editor> <address> 700-706, </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Sanger, </author> <title> T.D., Sutton R.S., Matheus C.J. (1992) Iterative construction of sparse polynomial approximations. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, </author> <title> R.S. (1992) Adapting bias by gradient descent: An incremental version of delta-bar-delta. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: The Algorithms This study considered seven algorithms: the Kalman filter, a least-squares method with covariance modification, the least-mean-square (LMS) algorithm, normalized LMS (NLMS), and three DLR algorithms called K1, K2, and IDBD. K1 and K2 are new to this paper whereas IDBD was introduced in <ref> (Sutton, 1992) </ref>. This section completely describes the algorithms. They are all of the form ^ (t + 1) = ^ (t) + K (t) y (t) ^ (t) T (t) (3) where ^ (0) = (0), and K (t) is a gain vector that differs from algorithm to algorithm. <p> In their method, however, the inputs identified as relevant in this way were not given higher learning rates, but rather were favored for being combined multiplicatively to pro duce higher order terms in a polynomial approximation of a nonlinear system. Finally, the IDBD algorithm <ref> (Sutton, 1992) </ref> is the im mediate ancester of the K1 algorithm. In short, K1 is to NLMS as IDBD is to LMS. <p> The IDBD algorithm is defined by fi i (t+1) with fi i (t) defined by (13), with fi i (0) = log n and h i + The derivation of (13) and (20) from the gradient descent equation (12) is given in <ref> (Sutton, 1992) </ref>. It is similar to that for K1 given in the appendix. The Computational Experiment A simple computational experiment was performed to assess the asymptotic performance of the seven algorithms. The dimension of the system was n = 20.
Reference: <author> Sutton R.S., Matheus C.J. </author> <title> (1991) Learning polynomial functions by feature construction. </title> <booktitle> Proc. Eighth Intl. Workshop on Machine Learning, </booktitle> <pages> 208-212, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Williams, R.J. & Zipser, D. </author> <title> (1989) Experimental analysis of the real-time recurrent learning algorithm. </title> <journal> Connection Science 1, </journal> <volume> 87 111. </volume>
References-found: 10

