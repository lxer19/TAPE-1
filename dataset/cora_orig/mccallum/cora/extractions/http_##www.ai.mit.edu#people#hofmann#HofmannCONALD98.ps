URL: http://www.ai.mit.edu/people/hofmann/HofmannCONALD98.ps
Refering-URL: http://www.ai.mit.edu/people/hofmann/
Root-URL: 
Email: hofmann@ai.mit.edu  
Title: Learning and Representing Topic A Hierarchical Mixture Model for Word Occurrences in Document Databases  
Author: Thomas Hofmann 
Address: Cambridge, MA 02139, USA,  
Affiliation: Center for Biological and Computational Learning, MIT  
Abstract: This paper presents a novel statistical mixture model for natural language learning in information retrieval. The described learning architecture is based on word occurrence statistics and extracts hierarchical relations between groups of documents as well as an abstractive organization of keywords. To train the model we derive a generalized, annealed version of the Expectation-Maximization (EM) algorithm for maximum likelihood estimation. The benefits of the model for interactive information retrieval and automated cluster summarization are experimentally investi gated.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W.B. Croft. </author> <title> Clustering large files of documents using the single-link method. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 28 </volume> <pages> 341-344, </pages> <year> 1977. </year>
Reference-contexts: The presented cluster-abstraction model (CAM) is a statistical mixture model [6, 5] which organizes groups of documents in a hierarchy. Compared to most state-of-the-art techniques based on agglomerative clustering (e.g., <ref> [4, 1, 9] </ref>) is has several advantages and additional features As a generative model the most important advantages are: (i) a sound foundation on the likelihood principle (likelihood as a global clustering criterion), (ii) the probabilistic inference mechanism, (iii) evaluation of generalization performance for model complexity control, (iv) efficient model fitting
Reference: [2] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: The factorial expression for the joint probability reflects conditional independence assumptions about word occurrences (bag-of-words model). Starting from (1) the standard EM approach <ref> [2] </ref> yields the following coupled re-estimation equations P (H iff = 1jp; ) = Q N j=1 (p jjff ) n ij P K Q N ; (2) ff = N i=1 jjff = P N P N : (3) These equations are very intuitive: The posteriors encode a probabilistic clustering
Reference: [3] <author> T. Hofmann and J. Puzicha. </author> <title> Statistical models for co-occurrence data. </title> <type> Technical Report AI-Memo, </type> <note> to appear, Center for Biological and Computational Learning, </note> <institution> Massachusetts Institute of Technology, </institution> <year> 1998. </year>
Reference-contexts: And third, one may also want to find ways to reduce the sensitivity of the EM procedure to local maxima. An answer to all three questions is provided by a generalization called annealed EM <ref> [3] </ref>. Annealed EM is closely related to a technique known as deterministic annealing that has been applied to many clustering problems (e.g. [8, 7]). Since a throughout discussion of annealed EM is beyond the scope of this paper, I will skip the theoretical background and focus on a procedural description. <p> This avoids overfitting and improves the average solution quality of EM procedures. Moreover, it also offers a way to generate tree topologies, since annealing leads through a sequence of so-called phase transitions. More details on this subject can be found in <ref> [3] </ref>. 4 Results All documents utilized in the experiments have been preprocessed by word suffix stripping with a word stemmer. A standard stop word list has been utilized to eliminate the most frequent words, in addition very rarely occuring words have also been eliminated.
Reference: [4] <author> N. Jardine and C.J. van Rijsbergen. </author> <title> The use of hierarchical clustering in information retrieval. </title> <booktitle> Information Storage and Retrieval, </booktitle> <volume> 7 </volume> <pages> 217-240, </pages> <year> 1971. </year>
Reference-contexts: The presented cluster-abstraction model (CAM) is a statistical mixture model [6, 5] which organizes groups of documents in a hierarchy. Compared to most state-of-the-art techniques based on agglomerative clustering (e.g., <ref> [4, 1, 9] </ref>) is has several advantages and additional features As a generative model the most important advantages are: (i) a sound foundation on the likelihood principle (likelihood as a global clustering criterion), (ii) the probabilistic inference mechanism, (iii) evaluation of generalization performance for model complexity control, (iv) efficient model fitting
Reference: [5] <author> M.I. Jordan and R.A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: The proposed technique is purely data-driven and does not make use of domain-dependent background information, nor does it rely on predefined document categories or a given list of topics. The presented cluster-abstraction model (CAM) is a statistical mixture model <ref> [6, 5] </ref> which organizes groups of documents in a hierarchy.
Reference: [6] <author> G.J. McLachlan and K. E. Basford. </author> <title> Mixture Models. </title> <publisher> Marcel Dekker, INC, </publisher> <address> New York Basel, </address> <year> 1988. </year>
Reference-contexts: The proposed technique is purely data-driven and does not make use of domain-dependent background information, nor does it rely on predefined document categories or a given list of topics. The presented cluster-abstraction model (CAM) is a statistical mixture model <ref> [6, 5] </ref> which organizes groups of documents in a hierarchy.
Reference: [7] <author> F.C.N. Pereira, N.Z. Tishby, and L. Lee. </author> <title> Distributional clustering of english words. </title> <booktitle> In Proceedings of the Association for Computational Linguistics, </booktitle> <pages> pages 183-190, </pages> <year> 1993. </year>
Reference-contexts: document clustering, (vii) discriminative topic descriptors for document groups, (viii) coarse-to-fine approach by deterministic annealing. 2 Probabilistic Clustering of Documents Let us emphasis the clustering aspect by first introducing a simplified, non-hierarchical version of the CAM which performs `flat' probabilistic clustering and is closely related to the model proposed in <ref> [7] </ref> for word clustering. Let the symbols d i (1 i N ) and w j (1 j M ) denote documents and words (word stems), respectively. <p> An answer to all three questions is provided by a generalization called annealed EM [3]. Annealed EM is closely related to a technique known as deterministic annealing that has been applied to many clustering problems (e.g. <ref> [8, 7] </ref>). Since a throughout discussion of annealed EM is beyond the scope of this paper, I will skip the theoretical background and focus on a procedural description. The key idea in deterministic annealing is the introduction of a temperature parameter T 2 IR + .
Reference: [8] <author> K. Rose, E. Gurewitz, and G. Fox. </author> <title> Statistical mechanics and phase transitions in clustering. </title> <journal> Physical Review Letters, </journal> <volume> 65(8) </volume> <pages> 945-948, </pages> <year> 1990. </year>
Reference-contexts: An answer to all three questions is provided by a generalization called annealed EM [3]. Annealed EM is closely related to a technique known as deterministic annealing that has been applied to many clustering problems (e.g. <ref> [8, 7] </ref>). Since a throughout discussion of annealed EM is beyond the scope of this paper, I will skip the theoretical background and focus on a procedural description. The key idea in deterministic annealing is the introduction of a temperature parameter T 2 IR + .
Reference: [9] <author> P. Willett. </author> <title> Recent trends in hierarchical document clustering: a critical review. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 24(5) </volume> <pages> 577-597, </pages> <year> 1988. </year> <month> 6 </month>
Reference-contexts: The presented cluster-abstraction model (CAM) is a statistical mixture model [6, 5] which organizes groups of documents in a hierarchy. Compared to most state-of-the-art techniques based on agglomerative clustering (e.g., <ref> [4, 1, 9] </ref>) is has several advantages and additional features As a generative model the most important advantages are: (i) a sound foundation on the likelihood principle (likelihood as a global clustering criterion), (ii) the probabilistic inference mechanism, (iii) evaluation of generalization performance for model complexity control, (iv) efficient model fitting
References-found: 9

