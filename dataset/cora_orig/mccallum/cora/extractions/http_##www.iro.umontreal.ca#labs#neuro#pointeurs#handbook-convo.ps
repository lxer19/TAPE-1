URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/handbook-convo.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/other.html
Root-URL: http://www.iro.umontreal.ca
Email: yann@research.att.com bengioy@iro.umontreal.ca  
Title: Convolutional Networks for Images, Speech, and Time-Series  
Author: Yann LeCun Yoshua Bengio 
Address: 101 Crawfords Corner Road Operationnelle, Universite de Montreal, Holmdel, NJ 07733 Montreal, Qc, Canada, H3C-3J7  
Affiliation: AT&T Bell Laboratories Dept. Informatique et Recherche  
Pubnum: Rm 4G332,  
Abstract-found: 0
Intro-found: 1
Reference: <author> Bengio, Y., LeCun, Y., and Henderson, D. </author> <year> (1994). </year> <title> Globally Trained Handwritten Word Rec-ognizer using Spatial Representation, Space Displacement Neural Networks and Hidden Markov Models. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6, </volume> <pages> pages 937-944. </pages>
Reference: <author> Boser, B., Sackinger, E., Bromley, J., LeCun, Y., and Jackel, L. </author> <year> (1991). </year> <title> An analog neural network processor with programmable topology. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 26(12) </volume> <pages> 2017-2025. </pages>
Reference-contexts: Another interesting application of SDNNs is object spotting (Wolf and Platt, 1994). An important advantage of convolutional neural networks is the ease with which they can be implemented in hardware. Specialized analog/digital chips have been designed and used in character recognition, and in image preprocessing applications <ref> (Boser et al., 1991) </ref>. Speeds of more than 1000 characters per second were obtained with a network with around 100,000 connections (shown in figure 1). The idea of subsampling can be turned around to construct networks similar to TDNNs, but that can generate sequences from labels.
Reference: <author> Bottou, L., Cortes, C., Denker, J., Drucker, H., Guyon, I., Jackel, L., LeCun, Y., Muller, U., Sackinger, E., Simard, P., and Vapnik, V. </author> <year> (1994). </year> <title> Comparison of classifier methods: a case study in handwritten digit recognition. </title> <booktitle> In International Conference on Pattern Recognition, </booktitle> <address> Jerusalem, Israel. </address>
Reference-contexts: The network in figure 1 contains about 100,000 connections, but only about 2,600 free parameters because of the weight sharing. Such networks compare favorably with other methods on handwritten character recognition tasks <ref> (Bottou et al., 1994) </ref> (see also HAND WRITTEN DIGIT RECOGNITION), and they have been deployed in commercial applications. Fixed-size convolutional networks that share weights along a single temporal dimension are known as Time-Delay Neural Networks (TDNNs). <p> LeCun & Bengio: Convolutional Networks for Images, Speech, and Time-Series 10 4 DISCUSSION Convolutional neural networks are a good example of an idea inspired by biology that resulted in competitive engineering solutions that compare favorably with other methods <ref> (Bottou et al., 1994) </ref>. While applying convolutional nets to image recognition removes the need for a separate hand-crafted feature extractor, normalizing the images for size and orientation (if only approximately) is still required.
Reference: <author> Bottou, L., Fogelman-Soulie, F., Blanchet, P., and Lienard, J. S. </author> <year> (1990). </year> <title> Speaker independent isolated digit recognition: multilayer perceptrons vs dynamic time warping. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 453-465. </pages>
Reference-contexts: Fixed-size convolutional networks that share weights along a single temporal dimension are known as Time-Delay Neural Networks (TDNNs). TDNNs have been used in phoneme recognition (without subsampling) (Lang and Hinton, 1988; Waibel et al., 1989), spoken word recognition (with subsampling) <ref> (Bottou et al., 1990) </ref>, and on-line handwriting recognition (Guyon et al., 1991).
Reference: <author> Guyon, I., Albrecht, P., Le Cun, Y., Denker, J. S., and ubbard W., H. </author> <year> (1991). </year> <title> design of a neural network character recognizer for a touch termin al. </title> <journal> Pattern Recognition, </journal> <volume> 24(2) </volume> <pages> 105-119. </pages> <editor> LeCun & Bengio: </editor> <title> Convolutional Networks for Images, Speech, and Time-Series 12 Keeler, </title> <editor> J. and Rumelhart, D.and Leow, W. </editor> <year> (1991). </year> <title> integrated segmentation and recognition of hand-printed numerals. </title> <editor> In Lippman, R. P., Moody, J. M., and Touretzky, D. S., editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 557-563. </pages> <publisher> Mor-gan Kaufmann Publishers, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Fixed-size convolutional networks that share weights along a single temporal dimension are known as Time-Delay Neural Networks (TDNNs). TDNNs have been used in phoneme recognition (without subsampling) (Lang and Hinton, 1988; Waibel et al., 1989), spoken word recognition (with subsampling) (Bottou et al., 1990), and on-line handwriting recognition <ref> (Guyon et al., 1991) </ref>. LeCun & Bengio: Convolutional Networks for Images, Speech, and Time-Series 8 3 VARIABLE-SIZE CONVOLUTIONAL NETWORKS, SDNN While characters or short spoken words can be size-normalized and fed to a fixed-size network, more complex objects such as written or spoken words and sentences have inherently variable size.
Reference: <author> Lang, K. and Hinton, G. </author> <year> (1988). </year> <title> The development of the Time-Delay Neural Network architecture for speech recognition. </title> <type> Technical Report CMU-CS-88-152, </type> <institution> Carnegie-Mellon University. </institution>
Reference: <author> Le Cun, Y. </author> <year> (1986). </year> <title> Learning Processes in an Asymmetric Threshold Network. </title> <editor> In Bienen-stock, E., Fogelman-Soulie, F., and Weisbuch, G., editors, </editor> <booktitle> Disordered systems and biological organization, </booktitle> <pages> pages 233-240, </pages> <address> Les Houches, France. </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> LeCun, Y. </author> <year> (1989). </year> <title> Generalization and Network Design Strategies. </title> <type> Technical Report CRG-TR-89-4, </type> <institution> Department of Computer Science, University of Toronto. </institution>
Reference-contexts: Since all the weights are learned with back-propagation, convolutional networks can be seen as synthesizing their own feature extractor. The weight sharing technique has the interesting side effect of reducing the number of free parameters, thereby reducing the "capacity" of the machine and improving its generalization ability (see <ref> (LeCun, 1989) </ref> on weight sharing, and LEARNING AND GENERALIZATION for an explanation of notions of capacity and generalization). The network in figure 1 contains about 100,000 connections, but only about 2,600 free parameters because of the weight sharing.
Reference: <author> LeCun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W., and Jackel, L. </author> <year> (1990). </year> <title> Handwritten Digit Recognition with a Back-Propagation Network. </title> <editor> In Touret-zky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 396-404, </pages> <address> Denver 1989. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: A typical convolutional network for recognizing characters is shown in figure 1 (from <ref> (LeCun et al., 1990) </ref>). The input plane receives images of characters that are approximately size-normalized and centered. Each unit of a layer receives inputs from a set of units located in a small neighborhood in the previous layer.
Reference: <author> Matan, O., Burges, C., LeCun, Y., and Denker, J. </author> <year> (1992). </year> <title> Multi-Digit Recognition Using a Space Displacement Neural Network. </title> <editor> In Moody, J., Hanson, S., and Lipmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 488-495, </pages> <address> San Mateo CA. </address> <publisher> Morgan Kaufmann. </publisher> <editor> LeCun & Bengio: </editor> <title> Convolutional Networks for Images, Speech, and Time-Series 13 Mozer, </title> <editor> M. </editor> <year> (1991). </year> <title> The Perception of Multiple Objects, A Connectionist Approach. </title> <publisher> MIT Press. </publisher>
Reference: <author> Rumelhart, D., Hinton, G., and Williams, R. </author> <year> (1986). </year> <title> Learning Representations by Back-Propagating Errors. </title> <journal> Nature, </journal> <volume> 323 </volume> <pages> 533-536. </pages>
Reference-contexts: This knowledge can be applied by forcing a set of units, whose receptive fields are located at different places on the image, to have identical weight vectors <ref> (Rumelhart, Hinton and Williams, 1986) </ref>. The outputs of such a set of neurons constitute a feature map. At each position, different types of units in different LeCun & Bengio: Convolutional Networks for Images, Speech, and Time-Series 6 feature maps compute different types of features.
Reference: <author> Schenkel, M., Weissman, H., Guyon, I., Nohl, C., and Henderson, D. </author> <year> (1993). </year> <title> Recognition-Based Segmentation of On-Line Hand-Printed Words. </title> <editor> In Hanson, C. and Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> pages 723-730, </pages> <address> Denver, </address> <publisher> CO. </publisher>
Reference-contexts: The replicated network and the HMM can be trained simultaneously by back-propagating gradients through the HMM. Globally trained, variable-size TDNN/HMM hybrids have been used for speech recognition (see PATTERN RECOGNITION AND NEURAL NETWORKS for a list of references) and on-line handwriting recognition <ref> (Schenkel et al., 1993) </ref>. Two-dimensional replicated convolutional networks, called "Space Displacement Neural Networks" (SDNN) have been used in combination with HMMs or other elastic matching methods for handwritten word recognition (Keeler and Rumelhart, 1991; Matan et al., 1992; Bengio, LeCun and Henderson, 1994).
Reference: <author> Simard, P. and LeCun, Y. </author> <year> (1992). </year> <title> Reverse TDNN: An Architecture for Trajectory Generation. </title> <editor> In Moody, J., Hanson, S., and Lipmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 579-588, </pages> <address> Denver 1991. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: The idea of subsampling can be turned around to construct networks similar to TDNNs, but that can generate sequences from labels. These networks are called reverse-TDNNs because they can be viewed as upside-down TDNNs: temporal resolution increases from the input to the output, through alternated oversampling and convolution layers <ref> (Simard and LeCun, 1992) </ref>. LeCun & Bengio: Convolutional Networks for Images, Speech, and Time-Series 10 4 DISCUSSION Convolutional neural networks are a good example of an idea inspired by biology that resulted in competitive engineering solutions that compare favorably with other methods (Bottou et al., 1994).
Reference: <author> Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., and Lang, K. </author> <year> (1989). </year> <title> Phoneme Recognition Using Time-Delay Neural Networks. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 37 </volume> <pages> 328-339. </pages>
Reference: <author> Wolf, R. and Platt, J. </author> <year> (1994). </year> <title> Postal address block location using a convolutional locator network. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 745-752. </pages> <editor> LeCun & Bengio: </editor> <title> Convolutional Networks for Images, Speech, and Time-Series 14 </title>
Reference-contexts: Two-dimensional replicated convolutional networks, called "Space Displacement Neural Networks" (SDNN) have been used in combination with HMMs or other elastic matching methods for handwritten word recognition (Keeler and Rumelhart, 1991; Matan et al., 1992; Bengio, LeCun and Henderson, 1994). Another interesting application of SDNNs is object spotting <ref> (Wolf and Platt, 1994) </ref>. An important advantage of convolutional neural networks is the ease with which they can be implemented in hardware. Specialized analog/digital chips have been designed and used in character recognition, and in image preprocessing applications (Boser et al., 1991).
References-found: 15

