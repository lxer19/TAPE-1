URL: http://www.cs.colorado.edu/~suvas/papers/CU-CS-779-95/techreport.ps
Refering-URL: http://www.cs.colorado.edu/~suvas/papers/CU-CS-779-95/paper.html
Root-URL: http://www.cs.colorado.edu
Email: (Email:fgrunwald,suvasg@cs.colorado.edu)  
Phone: 430,  
Title: The Dude Runtime System: An Object-Oriented Macro-Dataflow Approach To Integrated Task and Object Parallelism  
Author: Dirk Grunwald Suvas Vajracharya 
Date: August 1995  
Address: Campus Box  Boulder, CO 80309-0430  Boulder  
Affiliation: Department of Computer Science,  University of Colorado,  ffi University of Colorado at  
Pubnum: CU-CS-779-95  
Abstract: Technical Report CU-CS-779-95 Department of Computer Science Campus Box 430 University of Colorado Boulder, Colorado 80309 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adams L. M, Jordan H. F. </author> <title> Is SOR Color Blind? SIAM Sci. </title> <journal> Stat. Computation, </journal> <volume> 7(2) </volume> <pages> 490-506, </pages> <year> 1986. </year>
Reference-contexts: A common implementation of this technique uses the "five point stencil" to compute the (k+1)st iteration from the kth iteration by traversing the SOR iteration array in row major order. The Red-Black SOR algorithm <ref> [22, 23, 1] </ref> provides parallelism by dividing the mesh into "even" and "odd" meshes, like the red and black squares of a checkerboard. All even or odd elements can be processed concurrently. We use the Red-Black SOR algorithm for two reasons. First, Red-Black SOR does not suffer from load-imbalance.
Reference: [2] <author> T. Agerwala and Arvind. </author> <title> Data flow systems. </title> <journal> IEEE Computer, </journal> <volume> 15(2) </volume> <pages> 10-13, </pages> <month> Feb </month> <year> 1982. </year>
Reference-contexts: This distributes synchronization overhead and provides a very flexible scheduling construct. We call our runtime system the Definition-Use Description Environment, or Dude, and it is currently implemented as a layer on top of the existing Awesime threads library [15]. Normally, dataflow execution models have been associated with dataflow processors <ref> [2, 26, 26] </ref>, but the macro-dataflow model has been implemented in software as well [3, 29]. Often, as in the case of MENTAT, an entire language is designed around the macro-dataflow approach.
Reference: [3] <author> Robert Babb. </author> <title> Parallel processing with large-grain data flow techniques. </title> <journal> IEEE Computer, </journal> <volume> ??(??):55-61, </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: Normally, dataflow execution models have been associated with dataflow processors [2, 26, 26], but the macro-dataflow model has been implemented in software as well <ref> [3, 29] </ref>. Often, as in the case of MENTAT, an entire language is designed around the macro-dataflow approach. By comparison, we simply use the macro dataflow notions to provide a description of the dependence relations in a program.
Reference: [4] <author> Vasanth Balasundaram. </author> <title> A mechanism for keeping useful internal information in parallel programming tools: The data access descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 151-170, </pages> <year> 1990. </year>
Reference-contexts: First, fixed size chunks simplify maintaining the dependence information, and make that process more efficient. Allowing variable size chunks imply a less efficient data descriptor that take a range of values instead of indices. We initially implemented such a structure, similar to the Data Access Descriptor <ref> [4] </ref>, but found it was too slow in practice. Secondly, and more importantly, fixed size chunks allows the scheduler to establish an affinity between a chunk and the processor improving data locality. Each chunk has a preferred processor when it is rescheduled on the next iteration of the loop.
Reference: [5] <author> B.N. Bershad, E.D. Lazowska, and H.M. Levy. </author> <title> PRESTO: A System for Object-Oriented Parallel Programming. </title> <journal> Software Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: These virtual processors then perform loop-level scheduling for the work of individual parallel constructs, such as doall loops. Other runtime systems support a large number of virtual processors, or threads, and concentrate on efficiently scheduling those threads <ref> [5] </ref>. Both loop-level and thread-level scheduling decisions require information about the machine architecture, taking into account the number of processors, memory bandwidth and communication delays. 1 Furthermore, such scheduling mechanisms should be portable across a range of architectures to simplify the code that must be generated by a compiler. <p> Why should we build yet another runtime system? First, many existing runtime systems provide a limited set of abstractions 12 for concurrency. For example, the PTHREADS [25] and PRESTO <ref> [5] </ref> libraries only provide threads and traditional synchronization mechanisms such as barriers and locks. We feel these abstractions are too simple to allow a compiler to generate code that can efficiently manage the resources of a complex architecture, such as a distributed shared memory computer.
Reference: [6] <author> A. Brandt. </author> <title> Eliptic problem solvers. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1981. </year> <month> 17 </month>
Reference-contexts: Each task executes data-parallel operations, as shown earlier in Figure 1, with additional interaction between the two tasks. The combined data and task parallelism provides many opportunities for improving performance. The multigrid method has received much attention due to its fast convergence and the challenge of parallelizing the algorithm <ref> [6, 30, 8, 13] </ref>. The basic idea of the multigrid method is to obtain an initial approximation for a given grid by first solving on a coarser grid.
Reference: [7] <author> Jeffrey B. Weiss Clive F. Baillie, James C. McWilliams and Irad Yavneh. </author> <title> Parallel superconvergent multigrid. </title> <note> In submitted to Supercomputing '95., </note> <year> 1995. </year>
Reference-contexts: Planetary-scale fluid motions in the Earth's atmosphere are important to the the study of Earth's climate. A more complete description of the QGMG application is available <ref> [7, 32] </ref>. Here, we concentrate on only the computational aspect of the problem as it relates to Dude runtime system. <p> Planetary-scale fluid motions in the Earth's atmosphere are important to the the study of Earth's climate. A more complete description of the QGMG application is available [7, 32]. Here, we concentrate on only the computational aspect of the problem as it relates to Dude runtime system. As described <ref> [7] </ref>, 4 5 the QG equations of motion are @q + @x @y @ + fi @x where q = @x 2 + @y 2 + @z S (z) @z The best method for solving these equations is the multigrid solver.
Reference: [8] <author> D. Gannon and J. van Rosendale. . J. </author> <booktitle> Parallel Distributed Computing, </booktitle> <volume> 3 </volume> <pages> 106-135, </pages> <year> 1986. </year>
Reference-contexts: Each task executes data-parallel operations, as shown earlier in Figure 1, with additional interaction between the two tasks. The combined data and task parallelism provides many opportunities for improving performance. The multigrid method has received much attention due to its fast convergence and the challenge of parallelizing the algorithm <ref> [6, 30, 8, 13] </ref>. The basic idea of the multigrid method is to obtain an initial approximation for a given grid by first solving on a coarser grid.
Reference: [9] <author> Dirk Grunwald and Harini Srinivasan. </author> <title> Data Flow Equations for Explicitly Parallel Programs. </title> <booktitle> In Conf. Record ACM Symp. Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: We assume the compiler may be able to determine some of this dependence information - computing dependence information has been extensively studied, and there has been recent work on analyzing explicitly parallel programs <ref> [12, 9] </ref>. A conventional runtime system might implement this program by closely following the structure of the original program. This is illustrated in Figure 1 for two processors. Each doall construct is executed in its entirety, and the execution of doall blocks is separated by barrier synchronization.
Reference: [10] <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support fo shared memory parallel computing. </title> <journal> ACM. Trans on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Using a conventional runtime system, dependence constraints between iterations are enforced by event synchronization (post and wait) or by nesting sequential constructs within the outer parallel loop. Eager et al <ref> [10] </ref> proposed a scheduling paradigm, called chores, that is similar to loop-level scheduling of multi-dimensional iteration spaces. The Chore system directly represents multi-dimensional iteration spaces using run-time data structures. The iteration space is dynamically subdivided, essentially providing the same scheduling decisions as existing dynamic scheduling algorithms. <p> By providing the concept of dependence and use specification in the runtime system, we can also execute multiple parallel operations concurrently. The QGMG program must solve two multi-grid problems to advance a single time-step. A traditional runtime system, or even advanced systems such as the Chores model <ref> [10] </ref> must sequentially schedule the computation in each doall or loop nesting. By allowing all operations to be evaluated in parallel, we increase the scheduling opportunities, allowing the runtime system to select a better schedule. <p> The charm system does not implement `threads' in the conventional sense state can not be saved and resumed at a later time. Furthermore, the runtime semantics are targeted towards message passing environments, and does not provide explicit support for shared address environments. The Chores <ref> [10] </ref> and Filaments [11] systems are the most germane runtime systems we've encountered. Filaments [11] are extremely fine grain stateless threads consisting of a pointer to code and a list of arguments.
Reference: [11] <author> Dawson R. Engler, Gregory R. Andrews, and David K. Lowenthal. </author> <title> Efficient support for fine-grain parallelism. </title> <type> TR 93-13, </type> <institution> Univ. Arizona, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: The charm system does not implement `threads' in the conventional sense state can not be saved and resumed at a later time. Furthermore, the runtime semantics are targeted towards message passing environments, and does not provide explicit support for shared address environments. The Chores [10] and Filaments <ref> [11] </ref> systems are the most germane runtime systems we've encountered. Filaments [11] are extremely fine grain stateless threads consisting of a pointer to code and a list of arguments. <p> Furthermore, the runtime semantics are targeted towards message passing environments, and does not provide explicit support for shared address environments. The Chores [10] and Filaments <ref> [11] </ref> systems are the most germane runtime systems we've encountered. Filaments [11] are extremely fine grain stateless threads consisting of a pointer to code and a list of arguments. Engler et al showed that filaments, due to their low overhead ( 16 bytes each), were suitable for exploiting fine grain parallelism on a variety of programs.
Reference: [12] <author> Jeanne Ferrante, Dirk Grunwald, and Harini Srinivasan. </author> <title> Array Section Analysis for Control Parallel Programs. </title> <type> Technical Report CU-CS-684-93, </type> <institution> University of Colorado at Boulder., </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: We assume the compiler may be able to determine some of this dependence information - computing dependence information has been extensively studied, and there has been recent work on analyzing explicitly parallel programs <ref> [12, 9] </ref>. A conventional runtime system might implement this program by closely following the structure of the original program. This is illustrated in Figure 1 for two processors. Each doall construct is executed in its entirety, and the execution of doall blocks is separated by barrier synchronization.
Reference: [13] <author> P. Frederickson and O. McBryan. </author> <title> Parallel superconvergent multigrid. </title> <booktitle> In Proceedings of the Third Copper Muntain Conference on Multigird Methods. </booktitle> <publisher> Marcel Dekker, </publisher> <year> 1989. </year>
Reference-contexts: Each task executes data-parallel operations, as shown earlier in Figure 1, with additional interaction between the two tasks. The combined data and task parallelism provides many opportunities for improving performance. The multigrid method has received much attention due to its fast convergence and the challenge of parallelizing the algorithm <ref> [6, 30, 8, 13] </ref>. The basic idea of the multigrid method is to obtain an initial approximation for a given grid by first solving on a coarser grid.
Reference: [14] <author> Susan Graham, Steven Lucco, and Oliver Sharp. </author> <title> Orchestrating interactions among parallel computations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> April </month> <year> 1993. </year> <note> ACM, ACM. </note>
Reference-contexts: By comparison, we are providing a library for an object-parallel language, and assume the compiler can distinguish between blocking and non-blocking operations. This simplifies the runtime system design and makes it more portable. Graham et al <ref> [14] </ref> extended the design of an existing compiler to overlap the execution of different loop constructs. Thus, the doall operations in each thread may be combined, reducing the execution overhead and improving load balance.
Reference: [15] <author> Dirk Grunwald. </author> <title> A users guide to awesime: An object oriented parallel programming and simulation system. </title> <type> Technical Report CU-CS-552-91, </type> <institution> University of Colorado, Boulder, </institution> <year> 1991. </year>
Reference-contexts: This distributes synchronization overhead and provides a very flexible scheduling construct. We call our runtime system the Definition-Use Description Environment, or Dude, and it is currently implemented as a layer on top of the existing Awesime threads library <ref> [15] </ref>. Normally, dataflow execution models have been associated with dataflow processors [2, 26, 26], but the macro-dataflow model has been implemented in software as well [3, 29]. Often, as in the case of MENTAT, an entire language is designed around the macro-dataflow approach. <p> In effect, the Dude runtime system provides a concrete runtime implementation for the dependence information shown in Figure 4. 3 The Design of Dude The Dude runtime system is based on Awesime <ref> [15] </ref> (A Widely Extensible Simulation Environment), an existing object-oriented runtime system for shared-address parallel architectures. The Awesime library currently runs on workstations using Digital Alpha AXP, SPARC, Intel `x86', MIPS R3000 and Motorola 68K processors, as well as the KSR-1 massively parallel processor.
Reference: [16] <author> Dirk Grunwald and Suvas Vajracharya. </author> <title> Efficient barriers for distributed shared memory computers. </title> <booktitle> In Intl. Parallel Processing Symposium. IEEE, IEEE Computer Society, </booktitle> <month> April </month> <year> 1994. </year> <note> (to appear). </note>
Reference-contexts: The most common work-sharing mechanism uses a separate scheduler for each CpuMux, and CpuMux's "steal" from each other if they are idle. As another example of dynamic dispatch, users can select a barrier algorithm that is most appropriate to the architecture <ref> [16] </ref> or problem. The Dude runtime system uses the abstraction and inheritance constructs of C++ to keep the scheduling policy, the underlying hardware, the type of objects being scheduling, the type of synchronization and other aspects of the system mutually orthogonal. <p> The improved performance is largely due to a more efficient barrier <ref> [16] </ref> which takes the hierarchical interconnect topology of the KSR1 into account. With dynamically scheduled Dude threads, threads grab a block of the matrix from a global descriptor containing information on what work remains to be done.
Reference: [17] <author> S. F. Hummel, Edith Schonberg, and L. E. Flynn. </author> <title> Factoring, a method for scheduling parallel loops. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 90-101, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Each doall construct is executed in its entirety, and the execution of doall blocks is separated by barrier synchronization. There have been numerous methods proposed to schedule the individual iterations of the doall loops, such as guided self scheduling and factoring <ref> [28, 17] </ref>. Conventional runtime systems use static, dynamic or some variant of adaptive scheduling to assign iterations to specific processors. Typically, a single dimension of a multidimensional iteration space is scheduled, although some researchers have considered scheduling nested loops [31].
Reference: [18] <author> L. V. Kale and W. Shu. </author> <title> The Chare-Kernel language for parallel programming: A perspective. </title> <type> Technical Report UIUCDCS-R-89-1451, </type> <institution> Univ. of Illinois, Urbana-Champaign, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: More recently, there have been a number of programming languages and runtime libraries stressing a diversity of parallelism constructs. The Chare language <ref> [18] </ref>, and more recently the CHARM++ [19] system, use actor-like message semantics and a continuation computation model. In the Charm runtime system, used by the Chare language, tasks are spawned by sending initiation messages to processors. Tasks subdivide work by creating other tasks.
Reference: [19] <author> Laxmikant Kale and Sanjeev Krishnan. Charm++: </author> <title> A portable concurrent object-oriented system based on c++. </title> <type> Technical report, </type> <institution> Univ. of Illinois, Urbana-Champaign, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: More recently, there have been a number of programming languages and runtime libraries stressing a diversity of parallelism constructs. The Chare language [18], and more recently the CHARM++ <ref> [19] </ref> system, use actor-like message semantics and a continuation computation model. In the Charm runtime system, used by the Chare language, tasks are spawned by sending initiation messages to processors. Tasks subdivide work by creating other tasks.
Reference: [20] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Winter Usenix Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Our runtime system is designed for shared memory computers that may be further connected using a message passing interface. We assume the shared memory computers have a pronounced memory hierarchy; examples of such architectures are the KSR-1 [21] and distributed shared memory systems <ref> [20] </ref>. Compilers must target a specific machine model supported by the runtime system, and we feel the art of designing a runtime system is to provide an interface with the most generality that can be implemented efficiently across a number of systems.
Reference: [21] <institution> Kendall Square Research, </institution> <address> Boston, MA. </address> <booktitle> The KSR-1 System Architecture Manual, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: Our runtime system is designed for shared memory computers that may be further connected using a message passing interface. We assume the shared memory computers have a pronounced memory hierarchy; examples of such architectures are the KSR-1 <ref> [21] </ref> and distributed shared memory systems [20]. Compilers must target a specific machine model supported by the runtime system, and we feel the art of designing a runtime system is to provide an interface with the most generality that can be implemented efficiently across a number of systems. <p> prior systems by combining the flexible intra-operation scheduling of the Chores system while the reducing the constraints of inter-operation scheduling, such as barrier operations. 5 Performance Results In this section we describe the performance of the Red/Black SOR and the Multigrid solver on the KSR1, a parallel cache-only memory architecture <ref> [21] </ref>. 5.1 Red Black SOR Problem The successive over-relaxation (SOR) method is an iterative technique to solve a linear system of equations.
Reference: [22] <author> Adams L. M. </author> <title> Iterative Methods for Solving Partial Differential Equations of Elliptic Type. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <address> Cambridge, Mass., </address> <year> 1950. </year> <month> 18 </month>
Reference-contexts: A common implementation of this technique uses the "five point stencil" to compute the (k+1)st iteration from the kth iteration by traversing the SOR iteration array in row major order. The Red-Black SOR algorithm <ref> [22, 23, 1] </ref> provides parallelism by dividing the mesh into "even" and "odd" meshes, like the red and black squares of a checkerboard. All even or odd elements can be processed concurrently. We use the Red-Black SOR algorithm for two reasons. First, Red-Black SOR does not suffer from load-imbalance.
Reference: [23] <author> Adams L. M. </author> <title> Iterative Algorithms for Large Sparse Linear Systems on Parallel Computers. </title> <type> PhD thesis, </type> <institution> Univ. of Virginia, Charlottesville, </institution> <year> 1982. </year>
Reference-contexts: A common implementation of this technique uses the "five point stencil" to compute the (k+1)st iteration from the kth iteration by traversing the SOR iteration array in row major order. The Red-Black SOR algorithm <ref> [22, 23, 1] </ref> provides parallelism by dividing the mesh into "even" and "odd" meshes, like the red and black squares of a checkerboard. All even or odd elements can be processed concurrently. We use the Red-Black SOR algorithm for two reasons. First, Red-Black SOR does not suffer from load-imbalance.
Reference: [24] <author> Markatos, E. P and T. J. LeBlanc. </author> <title> Load Balancing vs Locality Management in Shared Memory Multiprocessors. </title> <booktitle> In Intl. Conference on Parallel Processing, </booktitle> <pages> pages 258-257, </pages> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: The trend in more recent multiprocessor, such as the KSR-1, has been to increase the ratio of processor speed to communication speed, making good locality as important as good load-balance <ref> [24] </ref>. The Dude runtime system allows an affinity between PObjects and processors. Finally, the Chores system was intended to be used also by application programmers to simplify programming, chores have the option of suspending execution, but the normal convention is that individual chores usually run to completion.
Reference: [25] <author> Frank Mueller. </author> <title> Pthreads Library Interface. </title> <institution> Florida State University, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Why should we build yet another runtime system? First, many existing runtime systems provide a limited set of abstractions 12 for concurrency. For example, the PTHREADS <ref> [25] </ref> and PRESTO [5] libraries only provide threads and traditional synchronization mechanisms such as barriers and locks. We feel these abstractions are too simple to allow a compiler to generate code that can efficiently manage the resources of a complex architecture, such as a distributed shared memory computer.
Reference: [26] <author> Gregory M. Papadopoulos. </author> <title> Implementation of a General-Purpose Dataflow Multiprocessor. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, 02141, </address> <year> 1991. </year> <type> (1988 MIT Ph.D. Thesis, </type> <note> also published as MIT LCS TR 432). </note>
Reference-contexts: This distributes synchronization overhead and provides a very flexible scheduling construct. We call our runtime system the Definition-Use Description Environment, or Dude, and it is currently implemented as a layer on top of the existing Awesime threads library [15]. Normally, dataflow execution models have been associated with dataflow processors <ref> [2, 26, 26] </ref>, but the macro-dataflow model has been implemented in software as well [3, 29]. Often, as in the case of MENTAT, an entire language is designed around the macro-dataflow approach.
Reference: [27] <author> G. Pfister and V. Norton. </author> <title> Hot spot contention and combining in multistage interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):943-948, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: Note that the dependence constraints also distribute the synchronization that occurs in the program. In distributed shared memory computers, such as the KSR-1, synchronization among a large number of processors causes particular cache lines to become hot-spots <ref> [27] </ref>. By distributing the activity over a number of synchronization variables, the hardware parallelism supported by the multiple communication levels in a system such as the KSR can be exploited.
Reference: [28] <author> C. D. Polochronopoulous and D. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36(12) </volume> <pages> 1425-1439, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: Each doall construct is executed in its entirety, and the execution of doall blocks is separated by barrier synchronization. There have been numerous methods proposed to schedule the individual iterations of the doall loops, such as guided self scheduling and factoring <ref> [28, 17] </ref>. Conventional runtime systems use static, dynamic or some variant of adaptive scheduling to assign iterations to specific processors. Typically, a single dimension of a multidimensional iteration space is scheduled, although some researchers have considered scheduling nested loops [31].
Reference: [29] <author> Shankar Ramaswamy and Prithviraj Banerjee. </author> <title> Processor allocation and scheduling of macro dataflow graphs on distributed memory multicomputers by the paradigm compiler. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, volume II-Software, pages II-134-II-138. </booktitle> <publisher> CRC Press, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: Normally, dataflow execution models have been associated with dataflow processors [2, 26, 26], but the macro-dataflow model has been implemented in software as well <ref> [3, 29] </ref>. Often, as in the case of MENTAT, an entire language is designed around the macro-dataflow approach. By comparison, we simply use the macro dataflow notions to provide a description of the dependence relations in a program.
Reference: [30] <author> S.N. Gupta, M. Zubair and C.E. </author> <title> Grosch. </title> . <journal> J. of Scientific Commput., </journal> <volume> 7 </volume> <pages> 263-279, </pages> <year> 1992. </year>
Reference-contexts: Each task executes data-parallel operations, as shown earlier in Figure 1, with additional interaction between the two tasks. The combined data and task parallelism provides many opportunities for improving performance. The multigrid method has received much attention due to its fast convergence and the challenge of parallelizing the algorithm <ref> [6, 30, 8, 13] </ref>. The basic idea of the multigrid method is to obtain an initial approximation for a given grid by first solving on a coarser grid.
Reference: [31] <author> P. Tang and P.C. Yew. </author> <title> Processor self-scheduling for multiple nested parallel loops. </title> <booktitle> In Proc. Int. Conf. on Parallel Processing, </booktitle> <pages> pages 528-535. </pages> <publisher> IEEE, </publisher> <month> August </month> <year> 1986. </year>
Reference-contexts: Conventional runtime systems use static, dynamic or some variant of adaptive scheduling to assign iterations to specific processors. Typically, a single dimension of a multidimensional iteration space is scheduled, although some researchers have considered scheduling nested loops <ref> [31] </ref>. Using a conventional runtime system, dependence constraints between iterations are enforced by event synchronization (post and wait) or by nesting sequential constructs within the outer parallel loop. Eager et al [10] proposed a scheduling paradigm, called chores, that is similar to loop-level scheduling of multi-dimensional iteration spaces.
Reference: [32] <author> Irad Yavneh and James C. McWilliams. </author> <title> Multigrid solution of stably stratified flows: the quasi-geostrophic equations. </title> <note> In submitted to J. Sci. Comp., 1995. 19 </note>
Reference-contexts: Planetary-scale fluid motions in the Earth's atmosphere are important to the the study of Earth's climate. A more complete description of the QGMG application is available <ref> [7, 32] </ref>. Here, we concentrate on only the computational aspect of the problem as it relates to Dude runtime system.
References-found: 32

