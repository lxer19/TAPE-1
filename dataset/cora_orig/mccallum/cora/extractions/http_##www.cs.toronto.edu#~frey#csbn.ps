URL: http://www.cs.toronto.edu/~frey/csbn.ps
Refering-URL: http://www.cs.toronto.edu/~frey/index.html
Root-URL: 
Title: Continuous sigmoidal belief networks trained using slice sampling  
Author: Brendan J. Frey 
Address: 6 King's College Road, Toronto, Canada M5S 1A4  
Affiliation: Department of Computer Science, University of Toronto  
Abstract: Real-valued random hidden variables can be useful for modelling latent structure that explains correlations among observed variables. I propose a simple unit that adds zero-mean Gaussian noise to its input before passing it through a sigmoidal squashing function. Such units can produce a variety of useful behaviors, ranging from deterministic to binary stochastic to continuous stochastic. I show how "slice sampling" (Neal 1996) can be used for inference and learning in top-down networks of these units and demonstrate learning on two simple problems.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bishop, C. M, Svensen, M., and Williams, C.K.I. </author> <year> 1996. </year> <title> EM optimization of latent-variable density models. </title> <editor> In D. Touretzky, M. Mozer, and M. Hasselmo (editors), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. </author> <year> 1995. </year> <title> The Helmholtz machine. </title> <booktitle> Neural Computation 7, </booktitle> <pages> 889-904. </pages>
Reference: <author> Heckerman, D., and Geiger, D. </author> <year> 1994. </year> <title> Learning Bayesian networks: a unification for discrete and Gaussian domains. </title> <editor> In P. Besnard and S. Hanks (editors), </editor> <booktitle> Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <pages> 274-284. </pages>
Reference: <author> Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. </author> <year> 1995. </year> <title> The wake-sleep algorithm for unsupervised neural networks. </title> <booktitle> Science 268, </booktitle> <pages> 1158-1161. </pages>
Reference: <author> Hinton, G. E., and Sejnowski, T. J. </author> <year> 1986. </year> <title> Learning and relearning in Boltzmann machines. </title> <editor> In D. E. Rumelhart and J. L. McClelland (editors), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: 1 Introduction A variety of unsupervised connectionist models containing discrete-valued hidden units have been developed. These include Boltzmann machines <ref> (Hinton and Se-jnowski 1986) </ref>, binary sigmoidal belief networks (Neal 1992) and Helmholtz machines (Hinton et al. 1995; Dayan et al. 1995). However, some hidden variables, such as translation or scaling in images of shapes, are best represented using continuous values.
Reference: <author> Hofmann, R., and Tresp, V. </author> <year> 1996. </year> <title> Discovering structure in continuous variables using Bayesian networks. </title> <editor> In D. Touretzky, M. Mozer, and M. Hasselmo (editors), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Jaakkola, T., Saul, L. K., and Jordan, M. I. </author> <year> 1996. </year> <title> Fast learning by bounding likelihoods in sigmoid type belief networks. </title> <editor> In D. Touretzky, M. Mozer and M. Hasselmo (editors), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: (ie., y i &gt; 0:5 or, equivalently, x i &gt; 0): p (i onj i ; 2 Z 1 exp [(x i ) 2 =2 2 p i Z i exp [x 2 =2 2 p i i This sort of stochastic activation is found in binary sigmoidal belief networks <ref> (Jaakkola et al. 1996) </ref> and in the decision-making components of mixture of ex pert models and hierarchical mixture of expert models. 3 Continuous sigmoidal belief networks If the mean of each unit depends on the activities of other units and there are feedback connections, it is difficult to relate the density
Reference: <author> Lauritzen, S. L., Dawid, A. P., Larsen, B. N., and Leimer, H. G. </author> <year> 1990. </year> <title> Independence properties of directed Markov Fields. </title> <booktitle> Networks 20, </booktitle> <pages> 491-505. </pages>
Reference: <author> Mackay, D. J. C. </author> <year> 1995. </year> <title> Bayesian neural networks and density networks. Nuclear Instruments and Methods in Physics Research, </title> <booktitle> A 354, </booktitle> <pages> 73-80. </pages>
Reference: <author> Movellan, J. R., and McClelland, J. L. </author> <year> 1992. </year> <title> Learning continuous probability distributions with symmetric diffusion networks. </title> <booktitle> Cognitive Science 17, </booktitle> <pages> 463-496. </pages>
Reference: <author> Neal, R. M. </author> <year> 1992. </year> <title> Connectionist learning of belief networks. </title> <booktitle> Artificial Intelligence 56, </booktitle> <pages> 71-113. </pages>
Reference-contexts: 1 Introduction A variety of unsupervised connectionist models containing discrete-valued hidden units have been developed. These include Boltzmann machines (Hinton and Se-jnowski 1986), binary sigmoidal belief networks <ref> (Neal 1992) </ref> and Helmholtz machines (Hinton et al. 1995; Dayan et al. 1995). However, some hidden variables, such as translation or scaling in images of shapes, are best represented using continuous values.
Reference: <author> Neal, R. M. </author> <year> 1996. </year> <title> Markov chain Monte Carlo methods based on "slicing" the density function. </title> <note> In preparation. </note>
Reference-contexts: However, it is difficult to sample from this distribution because it may have many peaks that range from broad to narrow. I use a new Markov chain Monte Carlo method called "slice sampling" <ref> (Neal 1996) </ref> to pick a new activity for each unit. Consider the problem of drawing a value y from a univariate distribution P (y) | in this application, P (y) is the conditional distri bution p (y i jfy j g j6=i ).
Reference: <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: The proposed top-down model can be viewed as a continuous-valued belief network, which can be simulated by performing a quick top-down pass <ref> (Pearl 1988) </ref>. <p> Lauritzen et al. (1990) have included discrete random variables within the linear Gaussian framework. These approaches infer the distribution over unobserved unit activities given observed ones by "probability propagation" <ref> (Pearl 1988) </ref>. However, this procedure is highly suboptimal for the richly connected networks that I am interested in. Also, these approaches tend to assume that all the conditional Gaussian distributions represented by the belief network can be easily derived using information elicited from experts. <p> This ordered arrangement is the foundation of belief networks <ref> (Pearl, 1988) </ref>. I let the mean of each unit be determined by a linear combination of the postsigmoid activities of preceding units: i = j&lt;i w ij y j ; (7) where y 0 1 is used to implement biases. The variance for each unit is independent of unit activities.
Reference: <author> Tibshirani, R. </author> <year> (1992). </year> <title> Principal curves revisited. </title> <journal> Statistics and Computing 2, </journal> <pages> 183-190. </pages>
References-found: 14

