URL: http://www.cs.brown.edu/people/tld/postscript/KirmanNicholsonLejterDeanandSantosEWSP-94.ps
Refering-URL: http://www.cs.brown.edu/people/tld/
Root-URL: 
Title: Using Goals to Find Plans with High Expected Utility  
Author: Jak Kirman, Ann Nicholson, Moises Lejter, Thomas Dean Eugene Santos Jr. 
Address: Box 1910, Providence, RI 02912  OH 45433-7765  
Affiliation: Dept. of Computer Science Brown University,  Department of Electrical and Computer Engineering Air Force Institute of Technology Wright-Patterson AFB,  
Date: Dec 9-11th, 1993  
Note: To appear in 2nd European Workshop on Planning,  
Abstract: We describe a method for planning to achieve goals under uncertainty that makes use of decision-theoretic methods to guide search. Given a probabilistic model of the world and a utility measure on world states, we wish to find plans (sequences of actions) with high expected utility. Finding a plan maximizing the expected utility is combinatorial in nature. In previous research, we coped with the combinatorics by making simplifying assumptions that sometimes led to a poor choice of plan. In this paper, we reduce the combinatorics by restricting attention to plans that are likely to achieve specific goals; we then use a successive approximation algorithm to select from these plans one with high utility. We obtain the restricted set of plans using a procedure that can, given a goal, generate candidate plans one at a time as needed; these plans are produced in decreasing order of probability of achieving their goal. This procedure is also used to obtain iteratively refined bounds on the expected utility of the candidate plans; these bounds are used in choosing a plan to be executed, hence our method produces better plans the more time it is given. We apply this method to a robotics problem addressed in our previous research and show that the goal-directed search produces better plans.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Kenneth Basye, Thomas Dean, Jak Kirman, and Moises Lejter. </author> <title> A decision-theoretic approach to planning, </title> <journal> perception, and control. IEEE Expert, </journal> <volume> 7(4) </volume> <pages> 58-65, </pages> <year> 1992. </year>
Reference-contexts: Finding the plan with the maximum expected utility is combinatorial in nature, since we must consider all possible plans; therefore an exact solution is impractical. In our previous research <ref> [13, 1] </ref>, the complexity problem was handled by making simplifying assumptions about the world; these assumptions made it feasible to consider all possible plans but sometimes led to a poor choice of plan.
Reference: [2] <author> John S. Breese and Kenneth W. Fertig. </author> <title> Decision making with interval influence diagrams. </title> <booktitle> In Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 122-129, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Our use of intervals to cope with time-criticality in uncertain reasoning is in keeping with the work of Horvitz et al. [10] on bounding probabilities in updating Bayesian networks and Breese and Fertig <ref> [2] </ref> on propagating intervals in evaluating influence diagrams. Our use of intervals to bound expected utility estimates is purely a concession to complexity. 6. Conclusion Decision-theoretical models provide a convenient and well-understood framework for modeling the world and representing the effects of actions over time.
Reference: [3] <author> Thomas Dean and Mark Boddy. </author> <title> An analysis of time-dependent planning. </title> <booktitle> In Proceedings AAAI-88, </booktitle> <pages> pages 49-54. </pages> <publisher> AAAI, </publisher> <year> 1988. </year>
Reference-contexts: This is in contrast with methods that compute a policy [18] or a highly conditional plan [11, 17] that need only be computed once and will serve for any situation in which the agent finds itself. We are primarily concerned with choosing a good plan under time pressure <ref> [3] </ref>. Our method is similar in its motivation to that of Drummond and Bressina [8] in that it addresses the issues of uncertainty and time-criticality in planning.
Reference: [4] <author> Thomas Dean and Keiji Kanazawa. </author> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: We use Bayesian decision theory [14] as a basis for planning under uncertainty. We model the world with temporal Bayesian networks, a compact and well-understood formalism which allows us to reason about the effects of actions over time <ref> [4, 12] </ref>. We describe a method for goal-directed search in planning within such a decision-theoretic framework. Given a probabilistic model of the world and a utility measure on world states, the aim is to choose a plan with high expected utility. <p> Evidence can be specified about the state of any node in the network; this evidence is propagated through the network affecting the overall joint distribution, and producing posterior probability distributions for each node. In this paper we use a specialization of Bayesian networks, a model called temporal belief networks <ref> [4] </ref>. We apply our method to the same domain used in our previous work, that of Mobile Target Localization (MTL), with a mobile robot navigating and tracking a moving target in a cluttered environment.
Reference: [5] <author> Thomas Dean and Michael Wellman. </author> <title> On the value of goals. </title> <editor> In Josh Tenenberg, Jay Weber, and James Allen, editors, </editor> <booktitle> Proceedings from the Rochester Planning Workshop: From Formal Systems to Practical Systems, </booktitle> <pages> pages 129-140, </pages> <year> 1989. </year>
Reference-contexts: However, actually defining what constitutes a goal or a a satisficing solution to a given planning problem requires taking into account an agent's preferences over outcomes [6]. Of late there have appeared a number of proposals for reconciling goal-directed and utility-directed decision making strategies <ref> [5, 9, 20] </ref>. This paper provides a particular method for using goals to direct search while at the same time using expected utility to select among plans that have been determined to achieve goals with some probability.
Reference: [6] <author> Thomas Dean and Michael Wellman. </author> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1991. </year>
Reference-contexts: Satisficing approaches [19] and goal-directed search methods provide alternatives to the notion of maximizing utility dominant in decision theory. However, actually defining what constitutes a goal or a a satisficing solution to a given planning problem requires taking into account an agent's preferences over outcomes <ref> [6] </ref>. Of late there have appeared a number of proposals for reconciling goal-directed and utility-directed decision making strategies [5, 9, 20].
Reference: [7] <author> Thomas L. Dean, R. James Firby, and David P. Miller. </author> <title> Hierarchical planning involving deadlines, travel time and resources. </title> <journal> Computational Intelligence, </journal> <volume> 4(4) </volume> <pages> 381-398, </pages> <year> 1988. </year>
Reference-contexts: It is different from that work in that it adopts a more precise model for prediction in the form of Markov and semi-Markov processes <ref> [7, 12] </ref> and in that it recomputes a new plan at each step.
Reference: [8] <author> Mark Drummond and John Bresina. </author> <title> Anytime synthetic projection: Maximizing the probability of goal satisfaction. </title> <booktitle> In Proceedings AAAI-90, </booktitle> <pages> pages 138-144. </pages> <publisher> AAAI, </publisher> <year> 1990. </year>
Reference-contexts: We are primarily concerned with choosing a good plan under time pressure [3]. Our method is similar in its motivation to that of Drummond and Bressina <ref> [8] </ref> in that it addresses the issues of uncertainty and time-criticality in planning. It is different from that work in that it adopts a more precise model for prediction in the form of Markov and semi-Markov processes [7, 12] and in that it recomputes a new plan at each step.
Reference: [9] <author> Peter Haddawy and Steve Hanks. </author> <title> Issues in decision-theoretic planning: Symbolic goals and numeric utilities. </title> <booktitle> In Proceedings of the DARPA Workshop on Innovative Approaches to Planning, Scheduling, and Control, </booktitle> <pages> pages 48-58. DARPA, </pages> <year> 1990. </year>
Reference-contexts: However, actually defining what constitutes a goal or a a satisficing solution to a given planning problem requires taking into account an agent's preferences over outcomes [6]. Of late there have appeared a number of proposals for reconciling goal-directed and utility-directed decision making strategies <ref> [5, 9, 20] </ref>. This paper provides a particular method for using goals to direct search while at the same time using expected utility to select among plans that have been determined to achieve goals with some probability.
Reference: [10] <author> Eric J. Horvitz, H. Jacques Suermondt, and Gregory F. Cooper. </author> <title> Bounded conditioning: Flexible inference for decisions under scarce resources. </title> <booktitle> In UW89, </booktitle> <pages> pages 182-193, </pages> <year> 1989. </year>
Reference-contexts: Our use of intervals to cope with time-criticality in uncertain reasoning is in keeping with the work of Horvitz et al. <ref> [10] </ref> on bounding probabilities in updating Bayesian networks and Breese and Fertig [2] on propagating intervals in evaluating influence diagrams. Our use of intervals to bound expected utility estimates is purely a concession to complexity. 6.
Reference: [11] <author> Leslie Pack Kaelbling. </author> <title> Goals as parallel program specifications. </title> <booktitle> In Proceedings AAAI-88, </booktitle> <pages> pages 60-65. </pages> <publisher> AAAI, </publisher> <year> 1988. </year>
Reference-contexts: At each point in execution, our method chooses a plan, executes the first step in that plan, throws away the plan, and then starts all over again. This is in contrast with methods that compute a policy [18] or a highly conditional plan <ref> [11, 17] </ref> that need only be computed once and will serve for any situation in which the agent finds itself. We are primarily concerned with choosing a good plan under time pressure [3].
Reference: [12] <author> Keiji Kanazawa. </author> <title> Reasoning about Time and Probability. </title> <type> PhD thesis, </type> <institution> Brown University, Providence, RI, </institution> <year> 1991. </year> <month> 12 </month>
Reference-contexts: We use Bayesian decision theory [14] as a basis for planning under uncertainty. We model the world with temporal Bayesian networks, a compact and well-understood formalism which allows us to reason about the effects of actions over time <ref> [4, 12] </ref>. We describe a method for goal-directed search in planning within such a decision-theoretic framework. Given a probabilistic model of the world and a utility measure on world states, the aim is to choose a plan with high expected utility. <p> It is different from that work in that it adopts a more precise model for prediction in the form of Markov and semi-Markov processes <ref> [7, 12] </ref> and in that it recomputes a new plan at each step.
Reference: [13] <author> Jak Kirman, Kenneth Basye, and Thomas Dean. </author> <title> Sensor abstractions for control of navigation. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 2812-2817, </pages> <year> 1991. </year>
Reference-contexts: Finding the plan with the maximum expected utility is combinatorial in nature, since we must consider all possible plans; therefore an exact solution is impractical. In our previous research <ref> [13, 1] </ref>, the complexity problem was handled by making simplifying assumptions about the world; these assumptions made it feasible to consider all possible plans but sometimes led to a poor choice of plan. <p> Our temporal network model uses a discrete time representation, with a chance node for each variable at each time point. Figure 2 shows the temporal belief network for the MTL model over 4 time slices. (See <ref> [13] </ref> for a more detailed description of the original MTL model.) We have modified the model slightly to prevent the robot from attempting actions that are physically impossible (such as moving forward when facing a wall); these constraints are represented by the arc from L R to A R . <p> Related Work In earlier work <ref> [13] </ref> we used essentially the same model. The decision process then was correct in the sense that we examined all possible plans, and for each plan, examined all possible outcomes of the plan.
Reference: [14] <author> Howard Raiffa and R. Schlaifer. </author> <title> Applied Statistical Decision Theory. </title> <publisher> Harvard University Press, </publisher> <year> 1961. </year>
Reference-contexts: We view planning in terms of enumerating a set of possible sequences of actions, or plans, evaluating the outcomes of those plans to determine their expected utilities, and selecting the plan with the highest expected utility. We use Bayesian decision theory <ref> [14] </ref> as a basis for planning under uncertainty. We model the world with temporal Bayesian networks, a compact and well-understood formalism which allows us to reason about the effects of actions over time [4, 12]. We describe a method for goal-directed search in planning within such a decision-theoretic framework.
Reference: [15] <author> Eugene Santos, Jr. </author> <title> On the generation of alternative explanations with implications for belief revision. </title> <booktitle> In Proceedings of the 7th Conference on Uncertainty in Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: The actual procedure we use is an application of an algorithm for Bayesian belief revision based on linear constraint satisfaction <ref> [15, 16] </ref>. This procedure can be used either to generate explanations, as above, or predictively, by taking a plan and producing the most likely scenario (including an outcome) and its probability. Again, when used predictively, the procedure generates a sequence of these scenario/probability pairs in order of decreasing probability. <p> The method presented in this paper for guiding the search for a plan with high expected utility relies on a procedure developed by Santos <ref> [15, 16] </ref> to perform belief revision in Bayesian networks, based on linear constraint satisfaction. A system of linear constraints is generated automatically to represent a given Bayesian network.
Reference: [16] <author> Eugene Santos, Jr. </author> <title> A Linear Constraint Satisfaction Approach for Abductive Reasoning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1992. </year>
Reference-contexts: The actual procedure we use is an application of an algorithm for Bayesian belief revision based on linear constraint satisfaction <ref> [15, 16] </ref>. This procedure can be used either to generate explanations, as above, or predictively, by taking a plan and producing the most likely scenario (including an outcome) and its probability. Again, when used predictively, the procedure generates a sequence of these scenario/probability pairs in order of decreasing probability. <p> The method presented in this paper for guiding the search for a plan with high expected utility relies on a procedure developed by Santos <ref> [15, 16] </ref> to perform belief revision in Bayesian networks, based on linear constraint satisfaction. A system of linear constraints is generated automatically to represent a given Bayesian network.
Reference: [17] <author> Marcel J. Schoppers. </author> <title> Universal plans for reactive robots in unpredictable environments. </title> <booktitle> In Proceedings IJCAI 10, </booktitle> <pages> pages 1039-1046. </pages> <address> IJCAII, </address> <year> 1987. </year>
Reference-contexts: At each point in execution, our method chooses a plan, executes the first step in that plan, throws away the plan, and then starts all over again. This is in contrast with methods that compute a policy [18] or a highly conditional plan <ref> [11, 17] </ref> that need only be computed once and will serve for any situation in which the agent finds itself. We are primarily concerned with choosing a good plan under time pressure [3].
Reference: [18] <author> Ross D. Shachter. </author> <title> Evaluating influence diagrams. </title> <journal> Operations Research, </journal> <volume> 34(6) </volume> <pages> 871-882, </pages> <year> 1986. </year>
Reference-contexts: At each point in execution, our method chooses a plan, executes the first step in that plan, throws away the plan, and then starts all over again. This is in contrast with methods that compute a policy <ref> [18] </ref> or a highly conditional plan [11, 17] that need only be computed once and will serve for any situation in which the agent finds itself. We are primarily concerned with choosing a good plan under time pressure [3].
Reference: [19] <author> Herbert A. Simon. </author> <title> A behavioral model of rational choice. </title> <journal> Quarterly Journal of Economics, </journal> <volume> 69 </volume> <pages> 99-118, </pages> <year> 1955. </year>
Reference-contexts: This is still not as powerful as a complete policy, which would allow us to condition actions on the predictions made by the decision model, but provides a substantial improvement over previous models. Satisficing approaches <ref> [19] </ref> and goal-directed search methods provide alternatives to the notion of maximizing utility dominant in decision theory. However, actually defining what constitutes a goal or a a satisficing solution to a given planning problem requires taking into account an agent's preferences over outcomes [6].
Reference: [20] <author> Michael P. Wellman and Jon Doyle. </author> <title> Preferential semantics for goals. </title> <booktitle> In Proceedings AAAI-91. AAAI, </booktitle> <year> 1991. </year> <month> 13 </month>
Reference-contexts: However, actually defining what constitutes a goal or a a satisficing solution to a given planning problem requires taking into account an agent's preferences over outcomes [6]. Of late there have appeared a number of proposals for reconciling goal-directed and utility-directed decision making strategies <ref> [5, 9, 20] </ref>. This paper provides a particular method for using goals to direct search while at the same time using expected utility to select among plans that have been determined to achieve goals with some probability.
References-found: 20

