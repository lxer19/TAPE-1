URL: http://www.cse.ucsc.edu/~randal/thesis.ps
Refering-URL: http://www.cse.ucsc.edu/~randal/pubs.html
Root-URL: http://www.cse.ucsc.edu
Title: DIFFERENTIAL COMPRESSION: A GENERALIZED SOLUTION FOR BINARY FILES  is approved:  
Author: Randal C. Burns Randal C. Burns Prof. Darrell D. E. Long, Chair Prof. Glen G. Langdon, Jr. Dr. Ronald Fagin Dean 
Degree: A thesis submitted in partial satisfaction of the requirements for the degree of MASTER OF SCIENCE in COMPUTER SCIENCE by  
Note: The thesis of  
Date: December 1996  
Affiliation: UNIVERSITY of CALIFORNIA SANTA CRUZ  of Graduate Studies  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> AHO, A. V., SETHI, R., AND ULLMAN, J. D. </author> <booktitle> Compilers, principles, techniques, and tools. </booktitle> <publisher> Addison-Wesley Publishing Co., </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: A non-uniform distribution of footprints results in differing strings hashing to the same footprint with higher probability. Many hashing functions meet the requirement for a uniform distribution of keys <ref> [11, 1] </ref>. Differencing algorithms often need to calculate footprints at successive symbol offsets over a large portion of a file (Figure 2.1). This additional requirement makes KarpRabin hashing functions [8] more efficient than other methods. KarpRabin techniques permit the incremental calculation of footprints.
Reference: [2] <author> ALDERSON, A. </author> <title> A space-efficient technique for recording versions of data. </title> <journal> Software Engineering Journal 3, </journal> <month> 6 (June </month> <year> 1988), </year> <month> 240246. </month>
Reference-contexts: There are other representations including those that represent delta files as linked data structures such as B/B+ trees or lists <ref> [16, 2] </ref>, and one based upon matrix algebra [5]. In any representation scheme, a differencing algorithm must have found the copies and adds to be encoded.
Reference: [3] <author> APOSTOLICO, A., BROWNE, S., AND GUERRA, C. </author> <title> Fast linear-space computations of longest common subsequences. </title> <booktitle> Theoretical Computer Science 92, 1 (1992), </booktitle> <pages> 317. </pages>
Reference-contexts: This technology can be used to provide backup and restore services on a subscription basis over any network including the Internet. 4 1.3 Previous Work Differencing has it origins in both longest common subsequence (LCS) algorithms <ref> [3, 12] </ref> and the string-to-string correction problem [17]. Some of the first applications of differencing updated the screens of slow terminals by sending a set of edits to be applied locally rather than retransmitting a screen full of data.
Reference: [4] <author> BAKER, M. G., HARTMAN, J. H., KUPFER, M. D., SHIRRIFF, K. W., AND OUSTERHOUT, J. K. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of the 13th Annual Symposium on Operating Systems (Oct. </booktitle> <year> 1991). </year>
Reference-contexts: As we are interested in applications that operate on all data in a network file system, quadratic execution time renders differencing prohibitively expensive. While it is a well known result that the majority of the files are small, less than 1 kilobyte <ref> [4] </ref>, a file system has a minority of files that may be large, ten to hundreds of megabytes. In order to address the differential compression of large files, we devise a differencing algorithm that runs in both linear time, O (M + N ), and constant space, O (1). <p> Consequently, the algorithm uses memory proportional to the size of the input, O (N ), for a size N file. 2.3 A Simple Linear Differencing Algorithm Having motivated the need to difference all files in a file system and understanding that not all file are small <ref> [4] </ref>, we improve upon both the run-time performance bound and run-time memory utilization of the greedy algorithm. <p> These distributions included the gnu tools distribution and Linux kernels among other things. These files have many of the desirable properties for our experiments, including having many small files and few large files as we would expect in a file system <ref> [4] </ref>. It also contains a wide variety of files including images, text, source, libraries and binary files.
Reference: [5] <author> BLACK, A. P., AND CHARLES H. BURRIS, JR. </author> <title> A compact representation for file versions: A preliminary report. </title> <booktitle> In Proceedings of the 5th International Conference on Data Engineering (1989), IEEE, </booktitle> <pages> pp. 321329. </pages>
Reference-contexts: There are other representations including those that represent delta files as linked data structures such as B/B+ trees or lists [16, 2], and one based upon matrix algebra <ref> [5] </ref>. In any representation scheme, a differencing algorithm must have found the copies and adds to be encoded.
Reference: [6] <author> EHRENFEUCHT, A., AND HAUSSLER, D. </author> <title> A new distance metric on strings computable in linear time. </title> <note> Discrete Applied Mathematics 20 (1988), 191203. </note>
Reference-contexts: More generally, it has been asserted that string metrics that examine symbols sequentially fail to emphasize the global similarity of two strings <ref> [6] </ref>. Miller and Myers [9] established the limitations of LCS when they produced a new file compare program that executes at four times the speed of the diff program while producing significantly smaller deltas.
Reference: [7] <author> FRASER, C. W., AND MYERS, E. W. </author> <title> An editor for revision control. </title> <journal> ACM Transactions on Programming Languages and Systems 9, </journal> <month> 2 (Apr. </month> <year> 1987), </year> <month> 277295. </month>
Reference-contexts: Source code control has been the major application for differencing. These packages allow authors to store and recall file versions. Software releases may be restored exactly and changes are recoverable. Version control has also been integrated into a line editor <ref> [7] </ref> so that on every change a minimal delta is retained. This allows for an unlimited undo facility without excessive storage. While line granularity may seem appropriate for source code, the concept of revision control needs to be generalized to include binary files.
Reference: [8] <author> KARP, R. M., AND RABIN, M. O. </author> <title> Efficient randomized pattern-matching algorithms. </title> <journal> IBM Journal of Research and Development 31, </journal> <volume> 2 (1987), </volume> <pages> 249260. </pages>
Reference-contexts: Many hashing functions meet the requirement for a uniform distribution of keys [11, 1]. Differencing algorithms often need to calculate footprints at successive symbol offsets over a large portion of a file (Figure 2.1). This additional requirement makes KarpRabin hashing functions <ref> [8] </ref> more efficient than other methods. KarpRabin techniques permit the incremental calculation of footprints. As successive string prefixes differ by only a single symbol, one implementation has the relation given by table 2.2. <p> Therefore, identity mode proceeds through the input at as much as twice the rate of hashing mode. Furthermore, the byte identity function is far easier to compute than the KarpRabin <ref> [8] </ref> hashing function. On highly correlated files, the algorithm spends more time in identity mode than it would on less correlated versions.
Reference: [9] <author> MILLER, W., AND MYERS, E. W. </author> <title> A file comparison program. </title> <journal> Software Practice and Experience 15, </journal> <volume> 11 (Nov. </volume> <year> 1985), </year> <month> 10251040. </month>
Reference-contexts: More generally, it has been asserted that string metrics that examine symbols sequentially fail to emphasize the global similarity of two strings [6]. Miller and Myers <ref> [9] </ref> established the limitations of LCS when they produced a new file compare program that executes at four times the speed of the diff program while producing significantly smaller deltas.
Reference: [10] <author> MORRIS, R. </author> <title> Conversations regarding differential compression for file system backup and restore, </title> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: In distributed file system backup and restore, differential compression would reduce the time to perform file system backup, decrease network traffic during backup and restore, and lessen the storage to maintain a backup image <ref> [10] </ref>. Backup and restore can be limited by both bandwidth on the network, often 10 MB/s, and poor throughput to secondary and tertiary storage devices, often 500 KB/s to tape storage.
Reference: [11] <author> REICHENBERGER, C. </author> <title> Delta storage for arbitrary non-text files. </title> <booktitle> In Proceedings of the 3rd International Workshop on Software Configuration Management, </booktitle> <address> Trondheim, Norway, </address> <month> 12-14 June </month> <year> 1991 </year> <month> (June </month> <year> 1991), </year> <booktitle> ACM, </booktitle> <pages> pp. 144152. </pages>
Reference-contexts: This results in delta files that achieve little or no compression as compared to storing the versions uncompressed. Recently, an algorithm appeared that addresses differential compression of arbitrary byte streams <ref> [11] </ref>. The algorithm modifies the work of Tichy [14] to work on byte-wise data streams 6 rather than line oriented data. This algorithm adequately manages binary sources and is an effective developer's tool for source code control. <p> In order to better compare different techniques, all of the algorithms we develop use the same file encoding <ref> [11] </ref>. This encoding consists of three types of codewords. <p> A non-uniform distribution of footprints results in differing strings hashing to the same footprint with higher probability. Many hashing functions meet the requirement for a uniform distribution of keys <ref> [11, 1] </ref>. Differencing algorithms often need to calculate footprints at successive symbol offsets over a large portion of a file (Figure 2.1). This additional requirement makes KarpRabin hashing functions [8] more efficient than other methods. KarpRabin techniques permit the incremental calculation of footprints. <p> This requires it to maintain a canonical listing of all footprints in one file, generally kept by computing and storing a footprint at all string prefix offsets <ref> [11] </ref>.
Reference: [12] <author> RICK, C. </author> <title> A new flexible algorithm for the longest common subsequence problem. </title> <booktitle> In Proceedings of the 6th Annual Symposium on Combinatorial Patterm Matching Espoo, </booktitle> <address> Finland, </address> <month> 5-7 July </month> <year> 1995 (1995). </year> <month> 61 </month>
Reference-contexts: This technology can be used to provide backup and restore services on a subscription basis over any network including the Internet. 4 1.3 Previous Work Differencing has it origins in both longest common subsequence (LCS) algorithms <ref> [3, 12] </ref> and the string-to-string correction problem [17]. Some of the first applications of differencing updated the screens of slow terminals by sending a set of edits to be applied locally rather than retransmitting a screen full of data.
Reference: [13] <author> ROCHKIND, M. J. </author> <title> The source code control system. </title> <journal> IEEE Transactions on Software Engineering SE-1, </journal> <note> 4 (Dec. 1975), 364370. </note>
Reference-contexts: We term this differencing at line granularity. The delta file is a line by line edit script applied to a base file to convert it to the new version. Although the SCCS version control system <ref> [13] </ref> precedes RCS, RCS generates minimal line granularity delta files and is the definitive previous work in version control. Source code control has been the major application for differencing. These packages allow authors to store and recall file versions. Software releases may be restored exactly and changes are recoverable.
Reference: [14] <author> TICHY, W. F. </author> <title> The string-to-string correction problem with block move. </title> <journal> ACM Transactions on Computer Systems 2, </journal> <volume> 4 (Nov. </volume> <year> 1984). </year>
Reference-contexts: Miller and Myers [9] established the limitations of LCS when they produced a new file compare program that executes at four times the speed of the diff program while producing significantly smaller deltas. The edit distance <ref> [14] </ref> proved to be a better metric for the difference of files and techniques based on this method enhanced the utility and speed of file differencing. The edit distance assigns a cost to edit operations such as delete a symbol, insert a symbol, and copy a symbol. <p> In the string-to-string correction problem [17], an algorithm minimizes the edit distance to minimize the cost of a given string transformation. 1 A string/substring contains all consecutive symbols between and including its first and last symbol whereas a sequence/subsequence may omit symbols with respect to the corresponding string. 5 Tichy <ref> [14] </ref> adapted the string-to-string correction problem to file differencing using the concept of block move. Block move allows an algorithm to copy a string of symbols rather than an individual symbol. He then applied the algorithm to source code revision control package and created RCS [15]. <p> This results in delta files that achieve little or no compression as compared to storing the versions uncompressed. Recently, an algorithm appeared that addresses differential compression of arbitrary byte streams [11]. The algorithm modifies the work of Tichy <ref> [14] </ref> to work on byte-wise data streams 6 rather than line oriented data. This algorithm adequately manages binary sources and is an effective developer's tool for source code control.
Reference: [15] <author> TICHY, W. F. </author> <title> RCS A system for version control. </title> <journal> Software Practice and Experience 15, </journal> <month> 7 (July </month> <year> 1985), </year> <month> 637654. </month>
Reference-contexts: Block move allows an algorithm to copy a string of symbols rather than an individual symbol. He then applied the algorithm to source code revision control package and created RCS <ref> [15] </ref>. RCS detects the modified lines in a file and encodes a delta file by adding these lines and indicating lines to be copied from the base version. We term this differencing at line granularity.
Reference: [16] <author> TSOTRAS, V., AND GOPINATH, B. </author> <title> Optimal versioning of objects. </title> <booktitle> In Proceeedings of the Eight International Conference on Data Engineering, </booktitle> <address> Tempe, AZ, USA, </address> <month> 2-3 Feb. </month> <year> 1992 </year> <month> (Feb. </month> <year> 1992), </year> <journal> IEEE, </journal> <pages> pp. 358365. </pages>
Reference-contexts: There are other representations including those that represent delta files as linked data structures such as B/B+ trees or lists <ref> [16, 2] </ref>, and one based upon matrix algebra [5]. In any representation scheme, a differencing algorithm must have found the copies and adds to be encoded.
Reference: [17] <author> WAGNER, R., AND FISCHER, M. </author> <title> The string-to-string correction problem. </title> <journal> Journal of the ACM 21, </journal> <month> 1 (Jan. </month> <year> 1973), </year> <month> 168173. </month>
Reference-contexts: This technology can be used to provide backup and restore services on a subscription basis over any network including the Internet. 4 1.3 Previous Work Differencing has it origins in both longest common subsequence (LCS) algorithms [3, 12] and the string-to-string correction problem <ref> [17] </ref>. Some of the first applications of differencing updated the screens of slow terminals by sending a set of edits to be applied locally rather than retransmitting a screen full of data. <p> For example, one longest common subsequence between strings xyz and xzy is xy, which neglects the common symbol z. Using the edit distance metric, z may be copied between the two strings producing a smaller change cost than LCS. In the string-to-string correction problem <ref> [17] </ref>, an algorithm minimizes the edit distance to minimize the cost of a given string transformation. 1 A string/substring contains all consecutive symbols between and including its first and last symbol whereas a sequence/subsequence may omit symbols with respect to the corresponding string. 5 Tichy [14] adapted the string-to-string correction problem
References-found: 17

