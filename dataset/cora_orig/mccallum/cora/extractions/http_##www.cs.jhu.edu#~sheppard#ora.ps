URL: http://www.cs.jhu.edu/~sheppard/ora.ps
Refering-URL: http://www.cs.jhu.edu/~sheppard/pubs.html
Root-URL: 
Title: A SYSTEMS APPROACH TO SPECIFYING BUILT-IN TESTS  
Author: John W. Sheppard and William R. Simpson 
Address: 2551 Riva Road Annapolis, MD 21401  
Affiliation: ARINC Research Corporation  
Abstract: The need for specifying robust built-in test for systems is growing as systems become more complex. Further, detection-only BITthe predominant form of BITis insufficient to meet the needs of system test on current and future systems; localization and even isolation is becoming essential. In response to this need, several computer-based analysis tools have become available that provide the ability to assess system testability and BIT effectiveness. Yet few if any formal methods exist for providing optimal BIT specifications. In this paper, we explore two BIT figures of merittest point utilization and optimized resolution analysis. We will describe evaluation measures which we then apply to BIT specified using these two approaches and explain the resulting differences. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Coppola, Anthony, </author> <title> A Design Guide for Built-in Test (BIT), </title> <institution> RADC-TR-78-224, Rome Air Development Center, </institution> <month> April </month> <year> 1979. </year>
Reference-contexts: Fourth, after each test was eliminated, we computed three testability statistics to determine the impact of ordering BIT tests based on these criteria. The three testability measures used to evaluate TPU and ORA were non-detection percent (ND), operational isolation (OI <ref> [1] </ref>), and isolation level (IL). Non-detection percent is the ratio of undetectable faults to the total number of faults and is computed as where and SF i is the fault signature of fault i. The first operational isolation measure (OI [1]) is the percentage of the time one is able to <p> TPU and ORA were non-detection percent (ND), operational isolation (OI <ref> [1] </ref>), and isolation level (IL). Non-detection percent is the ratio of undetectable faults to the total number of faults and is computed as where and SF i is the fault signature of fault i. The first operational isolation measure (OI [1]) is the percentage of the time one is able to fault isolate to exactly one fault conclusion. This measure is computed as where RU = the set of replaceable units (for our analysis, the set of individual conclusions). <p> Consequently, the differences between ORA and TPU are not as pronounced. Operational isolation tends to track fairly closely between ORA and TPU for the two systems, but a couple of interesting observations can be made. First, we should expect OI <ref> [1] </ref> to be identical for the two BIT measures when we have all of the tests available and when we have a minimum set of tests available. This is because the isolation potential is identical before deleting any tests and because OI [1] converges to 0.0. <p> First, we should expect OI <ref> [1] </ref> to be identical for the two BIT measures when we have all of the tests available and when we have a minimum set of tests available. This is because the isolation potential is identical before deleting any tests and because OI [1] converges to 0.0. In both of the systems examined for this paper, OI [1] indeed converged to 0.0. Another interesting difference, however, becomes evident when examining the hypothetical model (Figure 3). This model includes failure rate information (the SQS-53 did not), and OI [1] includes weighting (e.g., failure probability) in <p> This is because the isolation potential is identical before deleting any tests and because OI <ref> [1] </ref> converges to 0.0. In both of the systems examined for this paper, OI [1] indeed converged to 0.0. Another interesting difference, however, becomes evident when examining the hypothetical model (Figure 3). This model includes failure rate information (the SQS-53 did not), and OI [1] includes weighting (e.g., failure probability) in its calculation. <p> deleting any tests and because OI <ref> [1] </ref> converges to 0.0. In both of the systems examined for this paper, OI [1] indeed converged to 0.0. Another interesting difference, however, becomes evident when examining the hypothetical model (Figure 3). This model includes failure rate information (the SQS-53 did not), and OI [1] includes weighting (e.g., failure probability) in its calculation. When the weights are not uniform, we would expect the ORA to yield better results, and this is demonstrated in the hypothetical system. <p> Finally, the isolation level measure shows almost no difference between the ORA and the TPU, but the difference is significant. (Note that IL has the same convergence properties as OI <ref> [1] </ref>.) Both the hypothetical system (Figure 5) and the SQS-53 (Figure 6) show ORA with a slight advantage, but (with two exceptions on the hypothetical system), ORA is consistently equal to or better than TPU.
Reference: 2. <author> Simpson, W. R., and J. W. Sheppard, </author> <title> ``Analysis of False Alarms During System Design,'' </title> <booktitle> Proceedings of the National Aerospace Electronics Conference, </booktitle> <address> Dayton, Ohio, </address> <month> May </month> <year> 1992. </year>
Reference: 3. <author> Hughes Aircraft Company, </author> <title> Analysis of Built-in Test (BIT) False Alarm Conditions, </title> <institution> RADC-TR-81-220, Rome Air Development Center, </institution> <month> August </month> <year> 1981. </year>
Reference: 4. <author> Malcom, J. G., </author> <title> ``BIT False Alarms: An Important Factor in Operational Readiness,'' </title> <booktitle> Proceedings of the Reliability and Maintainability Symposium, </booktitle> <year> 1982, </year> <note> p. 206. </note>
Reference: 5. <author> Franco, J. R., </author> <title> ``Experiences Gained Using the Navy's IDSS Weapon System Testability Analyzer,'' </title> <booktitle> Proceedings of AUTOTESTCON '88, </booktitle> <address> Minneapolis, Minnesota, </address> <month> September </month> <year> 1988. </year>
Reference: 6. <author> Johnson, F., and C. R. Unkle, </author> <title> ``The System Testability and Maintenance Program (STAMP): A Testability Assessment Tool for Aerospace Systems,'' </title> <booktitle> Proceedings of the AIAA/NASA Symposium on the Maintainability of Aerospace Systems, </booktitle> <address> Anaheim, California, </address> <month> July </month> <year> 1989. </year>
Reference: 7. <author> Quinlan, J. R., </author> <title> ``Simplifying Decision Trees,'' </title> <journal> Journal of Man-Machine Studies, </journal> <volume> Vol. 27, </volume> <year> 1987, </year> <pages> pp. 221-234. </pages>
Reference: 8. <author> Mingers, J., </author> <title> ``An Empirical Comparison of Pruning Methods for Decision Tree Induction,'' </title> <journal> Machine Learning, </journal> <volume> Vol. 3, </volume> <year> 1989, </year> <pages> pp. 227-243. </pages>
Reference: 9. <author> Simpson, W. R., and J. W. Sheppard, </author> <title> ``System Complexity and Integrated Diagnostics,'' </title> <journal> IEEE Design and Test of Computers, </journal> <volume> Vol. 8, No. 3, </volume> <month> September </month> <year> 1991, </year> <pages> pp. 16-30. </pages>
Reference: 10. <author> Sheppard, J. W., and W. R. Simpson, </author> <title> ``A Mathematical Model for Integrated Diagnostics,'' </title> <journal> IEEE Design and Test of Computers, </journal> <volume> Vol. 8, No. 4, </volume> <month> December </month> <year> 1991, </year> <pages> pp. 25-38. </pages>
Reference: 11. <author> Simpson, W. R., and J. W., Sheppard, </author> <title> ``System Testability Assessment for Integrated Diagnostics,'' </title> <journal> IEEE Design and Test of Computers, </journal> <volume> Vol. 9, No. 1, </volume> <month> March </month> <year> 1992, </year> <pages> pp. 40-54. </pages>
Reference: 12. <author> Sheppard, J. W., and W. R. Simpson, </author> <title> ``Applying Testability Analysis for Integrated Diagnostics,'' </title> <journal> IEEE Design and Test of Computers, </journal> <volume> Vol. 9, No. 3, </volume> <month> September </month> <year> 1992, </year> <pages> pp. 65-78. </pages>
Reference: 13. <author> Simpson, W. R., and J. W. Sheppard, </author> <title> ``Fault Isolation in an Integrated Diagnostics Environment, </title> <journal> IEEE Design and Test of Computers, </journal> <volume> Vol. 10, No. 1, </volume> <month> March </month> <year> 1993, </year> <pages> pp. 52-66. </pages>
Reference: 14. <author> Sheppard, J. W., and W. R. Simpson, </author> <title> ``Performing Effective Fault Isolation in Integrated Diagnostics,'' </title> <journal> to appear IEEE Design and Test of Computers, </journal> <volume> Vol. 10, No. 2, </volume> <month> June </month> <year> 1993. </year> <title> Hypothetical System. Hypothetical System. System. Beamformer. Beamformer. </title> <type> Beamformer. </type>
References-found: 14

