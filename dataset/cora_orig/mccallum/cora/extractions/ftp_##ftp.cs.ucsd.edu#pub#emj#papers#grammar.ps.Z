URL: ftp://ftp.cs.ucsd.edu/pub/emj/papers/grammar.ps.Z
Refering-URL: http://www.cs.ucsd.edu/users/emj/
Root-URL: http://www.cs.ucsd.edu
Title: Bayesian Inference on Visual Grammars by Neural Nets that Optimize  
Author: Eric Mjolsness 
Date: May 1, 1990  
Address: New Haven, CT 06520-2158  
Affiliation: Department of Computer Science Yale University  
Pubnum: YALEU-DCS-TR-854  
Abstract: We exhibit a systematic way to derive neural nets for vision problems. It involves formulating a vision problem as Bayesian inference or decision on a comprehensive model of the visual domain given by a probabilistic grammar. A key feature of this grammar is the way in which it eliminates model information, such as object labels, as it produces an image; correspondance problems and other noise removal tasks result. The neural nets that arise most directly are generalized assignment networks. Also there are transformations which naturally yield improved algorithms such as correlation matching in scale space and the Frameville neural nets for high-level vision. Networks derived this way generally have objective functions with spurious local minima; such minima may commonly be avoided by dynamics that include deterministic annealing, for example recent improvements to Mean Field Theory dynamics. The grammatical method of neural net design allows domain knowledge to enter from all levels of the grammar, including "abstract" levels remote from the final image data, and may permit new kinds of learning as well. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anandan, P., Letovsky, S., and Mjolsness, E. </author> <year> (1989). </year> <title> Connectionist variable-binding by optimization. </title> <booktitle> In 11th Annual Conference of the Cognitive Science Society. </booktitle> <publisher> Lawrence Earlbaum Associates. University of Michigan. </publisher>
Reference: <author> Ballard, D. H. and Brown, C. M. </author> <year> (1982). </year> <title> Computer Vision, </title> <booktitle> chapter 11, </booktitle> <pages> pages 360-362. </pages> <publisher> Prentice Hall. Equation 11.3, Missing Cost. </publisher>
Reference-contexts: It is also a standard cost metric for missing model links and extra data links in pure graph matching for computer vision <ref> (Ballard and Brown, 1982) </ref>, restricted to the case of permutation correspondances: E (P ) = missing model graph links + extra data graph links = fffi max 0; G fffi P P fffi max P ij g ij P ffi P fij G fffi P fffi G fffi P 2 Bayesian
Reference: <author> Bertsekas, D. P. and Tsitsiklis, J. N. </author> <year> (1989). </year> <title> Parallel and Distributed Computation, chapter 5, page 333. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: Equation (11) is representative of most of the grammatical probability distributions we will derive in one important way: it is a Boltzmann distribution whose objective function is a generalized "assignment" objective function. The "assignment problem" <ref> (Bertsekas and Tsitsiklis, 1989) </ref> is to minimize E = P ffa P ffa W ffa over permutations P , for constant weights W 0. A neural net approach to this problem is analysed in (Kosowsky and Yuille, 1991).
Reference: <author> Bienenstock, E. and Doursat, R. </author> <year> (1991). </year> <title> Issues of representation in neural networks. </title> <editor> In Gorea, A., editor, </editor> <title> Representations of Vision: Trends and Tacit Assumptions in Vision Research. </title> <publisher> Cambridge University Press. </publisher>
Reference: <author> Burns, J. B. </author> <year> (1986). </year> <title> Extracting straight lines. </title> <journal> IEEE Trans. PAMI, </journal> <volume> 8(4) </volume> <pages> 425-455. </pages>
Reference-contexts: The jitter is replaced by whatever positional inaccuracies come from an actual camera producing an 128 fi 128 image (Williams and Hanson, 1988) which is then processed by a high quality line-segment finding algorithm <ref> (Burns, 1986) </ref>. Better results would be expected of objective functions derived from grammars which explicitly model more of these noise processes, such as the grammars studied in Section 4.
Reference: <author> Cooper, P. R. </author> <year> (1989). </year> <title> Parallel Object Recognition from Structure (The Tinkertoy Project). </title> <type> PhD thesis, </type> <institution> University of Rochester Department of Computer Science. </institution> <type> Technical Report 301. </type>
Reference: <author> Derthick, M. </author> <year> (1988). </year> <title> Mundane Reasoning by Parallel Constraint Satisfaction. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University. </institution> <note> Available as CMU-CS-88-182 from the Computer Science Department. </note>
Reference: <author> Durbin, R. and Willshaw, D. </author> <year> (1987). </year> <title> An analog approach to the travelling salesman problem using an elastic net method. </title> <journal> Nature, </journal> <volume> 326 </volume> <month> 689-691. </month> <title> Bayesian Inference on Grammars by Neural Nets 54 Ehrig, </title> <editor> H., Nagl, M., Rosenfeld, A., and Rozenberg, G., editors (1983). </editor> <booktitle> Graph Grammars and their Application to Computer Science; Third International Workshop, volume 291 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The approximation (22) has the effect of eliminating the discrete P mi variables, rather than replacing them with continuous versions V mi . The same can be said for the "elastic net" method <ref> (Durbin and Willshaw, 1987) </ref>, which is a less aggressive and probably more accurate approximation in which the sum over all permutation matrices is extended to a sum over all 0/1 matrices with exactly one nonzero element in each row but any number of nonzero entries in each column (Simic, 1990b; Yuille,
Reference: <author> Fahlman, S. E. </author> <year> (1979). </year> <title> NETL: A System for Representing and Using Real-World Knowledge. </title> <publisher> MIT Press. </publisher>
Reference-contexts: The goal of the "Frameville" type of neural network architecture (Mjolsness et al., 1989; Anandan et al., 1989) is to satisfy such constraints in much the way they can be satisfied within a frame system as used in Artificial Intelligence programming <ref> (Fahlman, 1979) </ref>, while exhibiting a neural substrate Bayesian Inference on Grammars by Neural Nets 29 or implementation which provides the kind of inexact matching abilities that objective-function based neural nets are capable of.
Reference: <author> Feldman, J. A., Fanty, M. A., and Goddard, N. H. </author> <year> (1988). </year> <title> Computing with structured neural networks. </title> <booktitle> IEEE Computer, </booktitle> <pages> page 91. </pages>
Reference: <author> Garrett, C. </author> <year> (1990). </year> <title> Private communication. </title>
Reference-contexts: The procedure becomes more elaborate and malleable by using valid transformations (e.g. of probability distributions or objective functions) at each step to reduce network cost, improve network performance or achieve network implementability in some technology. A small catalog of valid objective function transformations for neural nets is presented in <ref> (Mjolsness and Garrett, 1990) </ref>, and the present paper illustrates several transformations of probability distributions. The entire method is sketched in This paper is organized as follows. <p> Then impose the P P constraints exactly, as above, or else impose both P a constraints exactly. Either scheme preserves i-m symmetry. To finally reduce the products of continuous versions of the discrete R and S variables to linear form, one may use the objective function transformations of <ref> (Mjolsness and Garrett, 1990) </ref>, e.g.: X ^ R ma ^ S ai x i u m ! ma X ^ S ai x i ( a ! a ) + symmetric potential terms: (21) The result is a symmetric neural net architecture for the same problem, posessing the same type of <p> A neural net for performing the maximization of (23) with respect to x has been reported in <ref> (Mjol-sness and Garrett, 1990) </ref>. (As mentioned in Section 2.4, the maximization with respect to ff can be handled by making one copy of this neural net for each model and adding a winner-take-all circuit.) r Bayesian Inference on Grammars by Neural Nets 16 was infinite. <p> j q i )(w 2 + s 2 2 i 2 exp 2 ij w 2 + (v ij fi r) 2 2 j i )(w 2 + s 2 2 i 2 (30) added to the objective for each pair of line segments, as was calculated by Charles Garrett <ref> (Garrett, 1990) </ref>. We experimented with minimizing this objective function with respect to unknown global translations and (sometimes) rotations, using the continuation method and sets of line segments derived from real images. <p> Such specialization of instance function could probably be removed at the cost of further entropy terms. The entropy terms are new, and easily implementable with analog neural networks by Stirling's approximation and algebraic transformations of the resulting X log X forms <ref> (Mjolsness and Garrett, 1990) </ref>. Thus we have translated the probability distribution of the Frameville grammar, specified by the objective and the constraints, into the standard Frameville variables, recovering the standard objective function terms and constraints along with a few new ones. <p> By using the Mean Field Theory approximation, a Bayesian inference problem on this distribution is transformed into an optimization problem with an algebraic objective function. This function can be further transformed, for example using the techniques of <ref> (Mjolsness and Garrett, 1990) </ref>, to reduce its cost or increase its circuit-level implementability; then a neural network follows from descent dynamics. We studied grammars that model visual phenomena such as missing and extra data, group invari-ances, hierarchical objects, multiple instances of an object in a scene, and flexible spline-like objects.
Reference: <author> Hinton, G. E. and Sejnowski, T. J. </author> <year> (1986). </year> <title> Learning and relearning in Boltzmann machines. </title> <editor> In Rumel-hart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, chapter 7. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Second, one could minimize the Kullback information between the probability distributions of an unknown grammar, images from which the perceiver sees, and a parameterized grammar. This algorithm would be similar to the "Boltzmann machine" for neural network learning <ref> (Hinton and Sejnowski, 1986) </ref>. Finally one could look for clusters in model space by defining a distance "metric" D between images and mathematically projecting it back though the grammar.
Reference: <author> Hopfield, J. J. </author> <year> (1984). </year> <title> Neurons with graded response have collective computational properties like those of two-state neurons. </title> <booktitle> Proceedings of the National Academy of Sciences USA, </booktitle> <volume> vol. 81 </volume> <pages> 3088-3092. </pages>
Reference: <author> Hopfield, J. J. and Tank, D. W. </author> <year> (1985). </year> <title> `Neural' computation of decisions in optimization problems. </title> <journal> Biological Cybernetics, </journal> <volume> vol. 52 </volume> <pages> 141-152. </pages>
Reference-contexts: 0 N X next ji 1 N X N X next ij E 2 (x j x i ; i ; j i ): (44) This objective function may be transformed to a neural network as in Section 2.4, resulting in a network analogous the the Traveling Salesman network of <ref> (Hopfield and Tank, 1985) </ref> which, because of the change of variables, has the advantage that a curve can change "phase" (s-numbering as specified by fmbrg) gradually and locally as the network runs, without changing the curve's connectedness (as specified by fnextg). 4.4 Frameville from a Grammar Most neural net architectures appear
Reference: <author> Hopfield, J. J. and Tank, D. W. </author> <year> (1986). </year> <title> Collective computation with continuous variables. </title> <booktitle> In Disordered Systems and Biological Organization, </booktitle> <pages> pages 155-170. </pages> <publisher> Springer-Verlag. </publisher>
Reference: <author> Kosowsky, J. J. and Yuille, A. L. </author> <year> (1991). </year> <title> The invisible hand algorithm: Solving the assignment problem with statistical physics. </title> <type> Technical Report 91-1, </type> <institution> Harvard Robotics Laboratory. </institution>
Reference-contexts: The "assignment problem" (Bertsekas and Tsitsiklis, 1989) is to minimize E = P ffa P ffa W ffa over permutations P , for constant weights W 0. A neural net approach to this problem is analysed in <ref> (Kosowsky and Yuille, 1991) </ref>.
Reference: <author> Lamdan, Y., Schwartz, J. T., and Wolfson, H. J. </author> <year> (1988). </year> <title> Object recognition by affine invariant matching. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 335-344. </pages>
Reference-contexts: Another efficient indexing scheme, not used in the Frameville networks, would be geometric hashing <ref> (Lamdan et al., 1988) </ref>. Either form of indexing could possibly be added as a learned computational shortcut. Learning is briefly discussed in the next section. Bayesian Inference on Grammars by Neural Nets 49 5 LEARNING Three possible methods for learning a grammar are suggested here.
Reference: <author> Milios, E. E. </author> <year> (1989). </year> <title> Shape matching using curvature processes. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 47 </volume> <pages> 203-226. </pages>
Reference-contexts: : fdot (c; s; x cs )g ! fimagedot (x i = P E 3 (fx i g) = log n P and cs P cs;i = 1 Pr (P ) i X P cs;i x cs ) This grammar may be compared to the somewhat different curve grammars of <ref> (Milios, 1989) </ref>.
Reference: <author> Miller, M. I., Roysam, B., Smith, K. R., and O'Sullivan, J. A. </author> <year> (1991). </year> <title> Representing and computing regular languages on massively parallel networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(1). </volume>
Reference-contexts: A different way to translate a class of grammars into neural network objective functions is presented in <ref> (Miller et al., 1991) </ref>; it currently applies to "regular" languages and has been demonstrated in a low-level vision problem. Other directions for generalization of the parallel grammar occur in the extensive literature on L-systems (Rozenberg and Salomaa, 1980) and graph grammars (Ehrig et al., 1983).
Reference: <author> Mjolsness, E. and Garrett, C. </author> <year> (1990). </year> <title> Algebraic transformations of objective functions. Neural Networks, 3 651-669. Bayesian Inference on Grammars by Neural Nets 55 Mjolsness, </title> <editor> E., Gindi, G., and Anandan, P. </editor> <year> (1989). </year> <title> Optimization in model matching and perceptual organization. </title> <booktitle> Neural Computation, </booktitle> <pages> 1. </pages>
Reference-contexts: The procedure becomes more elaborate and malleable by using valid transformations (e.g. of probability distributions or objective functions) at each step to reduce network cost, improve network performance or achieve network implementability in some technology. A small catalog of valid objective function transformations for neural nets is presented in <ref> (Mjolsness and Garrett, 1990) </ref>, and the present paper illustrates several transformations of probability distributions. The entire method is sketched in This paper is organized as follows. <p> Then impose the P P constraints exactly, as above, or else impose both P a constraints exactly. Either scheme preserves i-m symmetry. To finally reduce the products of continuous versions of the discrete R and S variables to linear form, one may use the objective function transformations of <ref> (Mjolsness and Garrett, 1990) </ref>, e.g.: X ^ R ma ^ S ai x i u m ! ma X ^ S ai x i ( a ! a ) + symmetric potential terms: (21) The result is a symmetric neural net architecture for the same problem, posessing the same type of <p> Such specialization of instance function could probably be removed at the cost of further entropy terms. The entropy terms are new, and easily implementable with analog neural networks by Stirling's approximation and algebraic transformations of the resulting X log X forms <ref> (Mjolsness and Garrett, 1990) </ref>. Thus we have translated the probability distribution of the Frameville grammar, specified by the objective and the constraints, into the standard Frameville variables, recovering the standard objective function terms and constraints along with a few new ones. <p> By using the Mean Field Theory approximation, a Bayesian inference problem on this distribution is transformed into an optimization problem with an algebraic objective function. This function can be further transformed, for example using the techniques of <ref> (Mjolsness and Garrett, 1990) </ref>, to reduce its cost or increase its circuit-level implementability; then a neural network follows from descent dynamics. We studied grammars that model visual phenomena such as missing and extra data, group invari-ances, hierarchical objects, multiple instances of an object in a scene, and flexible spline-like objects.
Reference: <author> Mjolsness, E., Rangarajan, A., and Garrett, C. </author> <year> (1991a). </year> <title> A neural net for reconstruction of multiple curves with a visual grammar. Manuscript in preparation. </title> <booktitle> Summary in International Joint Conference on Neural Networks Seattle, </booktitle> <month> July </month> <year> 1991. </year>
Reference-contexts: In <ref> (Mjolsness et al., 1991a) </ref> we show that this grammar has the joint probability distribution Pr (x i ; i ; P cs;i ; C; (S c ; c = 1; ::; C)jN) = (1 q 1 ) q 1 (1 q 2 ) C Z 2 exp (fiE (fP g; fx
Reference: <author> Mjolsness, E., Sharp, D. H., and Reinitz, J. </author> <year> (1991b). </year> <title> A connectionist model of development. </title> <journal> Journal of Theoretical Biology. </journal> <note> In press. Also available as Yale Computer Science technical report YALEU/DCS/796, </note> <month> June </month> <year> 1990. </year>
Reference-contexts: A somewhat more general view of grammars whose rules posess connectionist models (similar to the objective functions attached to rules in this paper) is presented in <ref> (Mjolsness et al., 1991b) </ref>, where such grammars are proposed for modelling the development of biological organisms.
Reference: <author> Peterson, C. and Soderberg, B. </author> <year> (1989). </year> <title> A new method for mapping optimization problems onto neural networks. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(3). </volume>
Reference: <author> Prusinkiewicz, P. </author> <year> (1990). </year> <title> The Algorithmic Beauty of Plants. </title> <publisher> Springer-Verlag New York. </publisher>
Reference: <author> Rozenberg, G. and Salomaa, A. </author> <year> (1980). </year> <title> The Mathematical Theory of L-systems. </title> <publisher> Academic Press. </publisher>
Reference-contexts: Other directions for generalization of the parallel grammar occur in the extensive literature on L-systems <ref> (Rozenberg and Salomaa, 1980) </ref> and graph grammars (Ehrig et al., 1983). The grammars examined in this paper do not yet produced realistic images, and one could consider adding new rules to move from the "pictures" we studied to gray-level images.
Reference: <author> Simic, P. D. </author> <year> (1990a). </year> <title> Constrainted nets for graph matching and other quadrateic assignment problems. </title> <type> Technical Report CALT-68-1672, </type> <institution> Caltech Physics Department. </institution>
Reference: <author> Simic, P. D. </author> <year> (1990b). </year> <title> Statistical mechanics as the underlying theory of `elastic' and `neural' optimization. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <volume> 1(1) </volume> <pages> 89-103. </pages>
Reference: <author> Smith, A. R. </author> <year> (1984). </year> <title> Plants, fractals and formal languages. </title> <journal> Computer Graphics, </journal> <volume> 18(3) </volume> <pages> 1-10. </pages> <note> Proceedings of SIGGRAPH '84. </note>
Reference: <author> Stolcke, A. </author> <year> (1989). </year> <title> Unification as constraint satisfaction in structured connectionist networks. </title> <journal> Neural Computation, </journal> <volume> 1(4) </volume> <pages> 559-567. </pages>
Reference: <author> Tresp, V. </author> <year> (1991). </year> <title> A neural network approach for three-dimensional object recognition. </title> <booktitle> In Neural Information Processing Systems 3. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Utans, J., Gindi, G., Mjolsness, E., and Anandan, P. </author> <year> (1989). </year> <title> Neural networks for object recognition within compositional hierarchies: Initial experiments. </title> <type> Technical Report Report No. 8903, </type> <institution> Center for Systems Science, Yale University Department of Electrical Engineering. </institution>
Reference-contexts: Those con straints are represented at three levels of hierarchical organization by E (c; d). In addition, constraints E (a; b) are conventional, appearing for example as penalty terms in "Rule 1, 2, and 3" of <ref> (Utans et al., 1989) </ref>. The static constraints of E (f ) are usually taken to be obvious: only models present in the model base may have match variables. This leaves E (e) as the only constraint not clearly accounted for in previous Frameville research. <p> This leaves E (e) as the only constraint not clearly accounted for in previous Frameville research. Conversely previous work has relied mostly on the constraints found here, including E (c; d); for example <ref> (Utans et al., 1989) </ref> includes E (c; d) as "Rules 5 and 6", leaving only the constraint of "Rule 4" to differ from E (e). (Rules 7-9 of that source just encoded the graph-matching objective function.) Thus, the constraints of Theorem 1 agree with those of Frameville, with a few minor
Reference: <author> Van den Bout, D. E. and Miller, III, T. K. </author> <year> (1990). </year> <title> Graph partitioning using annealed networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2) </volume> <month> 192-203. </month> <title> Bayesian Inference on Grammars by Neural Nets 56 von der Malsburg, </title> <address> C. </address> <year> (1988). </year> <title> Pattern recognition by labeled graph matching. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 141-148. </pages> <editor> von der Malsburg, C. and Bienenstock, E. </editor> <year> (1986). </year> <title> Statistical coding and short-term synaptic plasticity: A scheme for knowledge representation in the brain. </title> <booktitle> In Disordered Systems and Biological Organization, </booktitle> <pages> pages 247-252. </pages> <publisher> Springer-Verlag. </publisher>
Reference: <author> Williams, L. R. and Hanson, A. R. </author> <year> (1988). </year> <title> Translating optical flow into token matches and depth from looming. </title> <booktitle> In Second International Conference on Computer Vision, </booktitle> <pages> pages 441-448. </pages> <booktitle> Staircase test image sequence. </booktitle>
Reference-contexts: In addition, there are strong boundary effects due to parts of the scene being translated outside the camera's field of view. The jitter is replaced by whatever positional inaccuracies come from an actual camera producing an 128 fi 128 image <ref> (Williams and Hanson, 1988) </ref> which is then processed by a high quality line-segment finding algorithm (Burns, 1986). Better results would be expected of objective functions derived from grammars which explicitly model more of these noise processes, such as the grammars studied in Section 4.
Reference: <author> Witkin, A., Terzopoulos, D., and Kass, M. </author> <year> (1987). </year> <title> Signal matching through scale space. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1 </volume> <pages> 133-144. </pages>
Reference-contexts: The output of a coarse-scale optimization is taken as the input to the next finer-scale optimization, as in deterministic annealing and other continuation methods. The resulting coarse-to-fine correlation matching algorithm is similar to the scale-space matching procedure of <ref> (Witkin et al., 1987) </ref>. The approximation (22) has the effect of eliminating the discrete P mi variables, rather than replacing them with continuous versions V mi .
Reference: <author> Yuille, A. L. </author> <year> (1990). </year> <title> Generalized deformable models, statistical physics, and matching problems. </title> <journal> Neural Computation, </journal> <volume> 2(1) </volume> <pages> 1-24. </pages>
Reference-contexts: fx i g) = 1 2 r 2 jt n P and m P m;i = 1 e P 2N 2 r 2 2 jx i xR ()u ff m j 2 + m : This is closely related to the objective function recommended for rigid body feature matching in <ref> (Yuille, 1990) </ref>.
Reference: <author> Zemel, R. S. </author> <year> (1989). </year> <title> Traffic: A connectionist model of object recognition. </title> <type> Technical Report CRG-TR-89-2, </type> <institution> University of Toronto Connectionist Research Group. </institution>
Reference-contexts: In this and a number of other important respects, the Frameville networks resemble the TRAFFIC system of <ref> (Zemel, 1989) </ref>.
References-found: 36

