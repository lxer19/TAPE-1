URL: http://www.eecs.umich.edu/~pmchen/amdahl.ps
Refering-URL: http://www.eecs.umich.edu/~pmchen/
Root-URL: http://www.cs.umich.edu
Title: An Evaluation of Redundant Arrays of Disks using an Amdahl 5890  
Author: Peter M. Chen, Garth A. Gibson, Randy H. Katz, David A. Patterson 
Address: Berkeley  
Affiliation: Computer Science Division, University of California,  
Abstract: Recently we presented several disk array architectures designed to increase the data rate and I/O rate of supercomputing applications, transaction processing, and file systems [Patterson 88]. In this paper we present a hardware performance measurement of two of these architectures, mirroring and rotated parity. We see how throughput for these two architectures is affected by response time requirements, request sizes, and read to write ratios. We find that for applications with large accesses, such as many supercomputing applications, a rotated parity disk array far outperforms traditional mirroring architecture. For applications dominated by small accesses, such as transaction processing, mirroring architectures have higher performance per disk than rotated parity architectures. 
Abstract-found: 1
Intro-found: 1
Reference: [Amdahl 5890] <institution> ``5890 Processors'', Amdahl marketing publications MM001388, </institution> <year> 1988. </year> <title> [Amdahl 6380] ``6380E Direct Access Storage Device'', </title> <publisher> Amdahl marketing publications MM001352, </publisher> <year> 1987. </year>
Reference: [Amdahl 67] <author> G. </author> <title> Amdahl, ``Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities,'' </title> <booktitle> Proceedings AFIPS 1967 Spring Joint Computer Conference, Vol. </booktitle> <address> 30 (Atlantic City, NJ, </address> <month> April </month> <year> 1967), </year> <pages> pp. 483-485. </pages>
Reference-contexts: This imbalanced system growth has already led to many I/O bound supercomputer applications [Kim 87]. If the imbalance is not remedied, Amdahl's Law tells us that much of the astounding processor speedup and memory growth will be wasted <ref> [Amdahl 67] </ref>. Continued improvement in system performance depends in a large part on I/O systems with higher data and I/O rates. One way to increase I/O performance is by using an array of many disks [Kim 86, Salem 86].
Reference: [Anon 85] <editor> Anon Et Al, </editor> <title> ``A Measure of Transaction Processing Power,'' </title> <type> Tandem Technical Report TR85.2, </type> <year> 1985. </year>
Reference-contexts: Although throughput scales with the number of disks used, response time does not. We define the 90th percentile response time to be the time in which 90% of the requests in the run were serviced, similar to <ref> [Anon 85] </ref>. For example, a 90th percentile response time of 1 second would mean that 90% of all requests in that run returned to the user within 1 second. <p> Thus, with 10 disks and track-level striping, large requests have been exactly 400 KB and individual requests have been exactly 40 KB. However, in real-world systems, large requests are often much larger than 400 KB [Bucher 80] and small requests are often much smaller than 40 KB <ref> [Ousterhout 85, Anon 85] </ref>. Also, applications rarely issue requests that are all the same size. We therefore make two changes to the request size distribution: (1) We no longer restrict the workloads to one particular size. Rather, we use a distribution of request sizes.
Reference: [Bell 84] <author> C.G. Bell, </author> <title> ``The Mini and Micro Industries,'' </title> <journal> IEEE Computer, </journal> <volume> Vol. 17, No. </volume> <month> 10 (October </month> <year> 1984), </year> <pages> pp. 14-30. </pages>
Reference-contexts: 1. The I/O Crisis Over the past decade, processing speed, memory speed, memory capacity, and disk capacity have all grown tremendously: g Single chip processors have increased in speed at the rate of 40%-100% per year <ref> [Bell 84, Joy 85] </ref>. g Caches have increased in speed 40% to 100% per year. g Main memory has quadrupled in capacity every two or three years [Moore 75, Myers 86]. In contrast, disk access times have undergone only modest performance improvements.
Reference: [Bitton 88] <author> D. Bitton and J. Gray, </author> <title> ``Disk Shadowing,'' </title> <booktitle> Very Large Database Conference, </booktitle> <year> 1988. </year>
Reference-contexts: For RAID Level 0, the effective data capacity equals the total disk capacity, so the storage efficiency is 100%. RAID Level 1, mirrored disks, is a traditional way to incorporate redundancy in an array of disks <ref> [Bitton 88] </ref>. In RAID Level 1, each datum is kept on two distinct disks: a data disk and a shadow disk. Thus, for RAID Level 1, the effective storage capacity is half the total disk capacity and the storage efficiency is 50%. <p> The disconnect time of RAID Level 1 is shorter because RAID Level 1 requests can choose between two disks which have the same data. By choosing the copy of the data which results in the shorter seek time, the average disconnect time drops 3-4 ms from RAID Level 0 <ref> [Bitton 88] </ref>. 7.2. Analyzing a Saturated System As discussed in Section 6, we control system load by controlling the number of outstanding logical requests in the system. Increasing the target 90th percentile response time allows the system to have more outstanding logical requests.
Reference: [Bouknight 72] <author> W. Bouknight, S. Denenberg, D. McIntyre, J. Randall, A. Sameh, D. Slotnick, </author> <title> ``The Illiac IV System,'' </title> <booktitle> Proc. IEEE, </booktitle> <month> April </month> <year> 1972, </year> <pages> pp. 369-388. </pages>
Reference: [Bucher 80] <author> I. Bucher, A. Hayes, </author> <title> ``I/O Performance Measurement on CRAY-1 and CDC 7600 Computers,'' </title> <booktitle> 16th Meeting of Computer Performance Evaluation Users Group, </booktitle> <month> Oct. </month> <year> 1980. </year>
Reference-contexts: Thus, with 10 disks and track-level striping, large requests have been exactly 400 KB and individual requests have been exactly 40 KB. However, in real-world systems, large requests are often much larger than 400 KB <ref> [Bucher 80] </ref> and small requests are often much smaller than 40 KB [Ousterhout 85, Anon 85]. Also, applications rarely issue requests that are all the same size. We therefore make two changes to the request size distribution: (1) We no longer restrict the workloads to one particular size.
Reference: [Buzen 86] <author> J. Buzen, A. Shum, </author> <title> ``I/O Architecture in MVS/370 and MVS/XA'', </title> <journal> CMG Transactions, </journal> <volume> vol 54, </volume> <month> Fall </month> <year> 1986, </year> <pages> pp. 19-26. </pages>
Reference: [Chen 89] <author> P. Chen, </author> <title> ``An Evaluation of Redundant Arrays of Disks Using an Amdahl 5890 (full version),'' </title> <type> Master's Thesis, </type> <institution> University of Cal-ifornia at Berkeley Report No. UCB/Computer Science Division 89/506, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: From 0% reads to almost 95% reads, RAID Level 5 yields higher throughput per disk than RAID Level 1. We conclude that for applications with predominantly large requests, RAID Level 5, rotated parity, clearly outperforms RAID Level 1, mirroring. The full version of this paper appears in <ref> [Chen 89] </ref> and explores additional issues, such as further varying the request size distribution, varying the unit of interleaving, scaling the number of disks, and connecting more than one disk per string. We are continuing to analyze the performance of disk arrays.
Reference: [Harker 81] <author> J. Harker et al., </author> <title> ``A Quarter Century of Disk File Innovation,'' </title> <journal> IBM Journal of Research and Development, </journal> <volume> Vol 25, No. 5, </volume> <month> Sept. </month> <year> 1981, </year> <pages> pp. 677-689. </pages>
Reference-contexts: In contrast, disk access times have undergone only modest performance improvements. For example, seek time has improved only about 7% per year <ref> [Harker 81] </ref>. This imbalanced system growth has already led to many I/O bound supercomputer applications [Kim 87]. If the imbalance is not remedied, Amdahl's Law tells us that much of the astounding processor speedup and memory growth will be wasted [Amdahl 67].
Reference: [IBM 87] <institution> IBM System/370XA Principle of Operations, IBM publication SA22-7085-01, 2nd ed. </institution> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: This is not measured, but rather calculated based on statistics given in Section 6. Also see Section 7.1.1. g connect time: measured time spent in transferring data <ref> [IBM 87] </ref>. For full track data transfers, connect time is 16.2 ms. Note that request overhead is measured per logical request. IO CPU time, disconnect time, and connect time are all measured per disk access. 7.1.1.
Reference: [Joy 85] <author> B. Joy, </author> <note> presentation at ISSCC 1985 panel session, </note> <month> Feb. </month> <year> 1985. </year>
Reference-contexts: 1. The I/O Crisis Over the past decade, processing speed, memory speed, memory capacity, and disk capacity have all grown tremendously: g Single chip processors have increased in speed at the rate of 40%-100% per year <ref> [Bell 84, Joy 85] </ref>. g Caches have increased in speed 40% to 100% per year. g Main memory has quadrupled in capacity every two or three years [Moore 75, Myers 86]. In contrast, disk access times have undergone only modest performance improvements.
Reference: [Kim 86] <author> M. Kim, </author> <title> ``Synchronized Disk Interleaving,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-35, no. 11, </volume> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: Continued improvement in system performance depends in a large part on I/O systems with higher data and I/O rates. One way to increase I/O performance is by using an array of many disks <ref> [Kim 86, Salem 86] </ref>. By using many disks, both throughput (MB per second) and I/O rate (I/O's per second) can be increased. <p> Figure 1 shows the data layout in the three redundancy schemes. The rest of this section summarizes the RAID Levels--see [Patterson 88] for more details. In all organizations, data are interleaved across all disks <ref> [Kim 86, Salem 86] </ref>. We define a stripe of data to be one unit of interleaving from each disk. For example, the first stripe of data in Figure 1 consists of logical blocks 0, 1, 2, and 3.
Reference: [Kim 87] <author> M. Kim, A. Nigam, G. Paul, </author> <title> ``Disk Interleaving and Very Large Fast Fourier Transforms,'' </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> Vol 1, No. 3, </volume> <month> Fall </month> <year> 1987, </year> <pages> pp. 75-96. </pages>
Reference-contexts: In contrast, disk access times have undergone only modest performance improvements. For example, seek time has improved only about 7% per year [Harker 81]. This imbalanced system growth has already led to many I/O bound supercomputer applications <ref> [Kim 87] </ref>. If the imbalance is not remedied, Amdahl's Law tells us that much of the astounding processor speedup and memory growth will be wasted [Amdahl 67]. Continued improvement in system performance depends in a large part on I/O systems with higher data and I/O rates.
Reference: [Moore 75] <author> G. Moore, </author> <title> ``Progress in Digital Integrated Electronics,'' </title> <booktitle> Proc. IEEE Digital Integrated Electronic Device Meeting, </booktitle> <year> 1975, </year> <note> p. 11. </note>
Reference-contexts: capacity, and disk capacity have all grown tremendously: g Single chip processors have increased in speed at the rate of 40%-100% per year [Bell 84, Joy 85]. g Caches have increased in speed 40% to 100% per year. g Main memory has quadrupled in capacity every two or three years <ref> [Moore 75, Myers 86] </ref>. In contrast, disk access times have undergone only modest performance improvements. For example, seek time has improved only about 7% per year [Harker 81]. This imbalanced system growth has already led to many I/O bound supercomputer applications [Kim 87].
Reference: [Myers 86] <author> W. Myers, </author> <title> "The Competitiveness of the United States Disk Industry, </title> <journal> IEEE Computer, </journal> <volume> Vol. 19, No. 11, </volume> <month> January </month> <year> 1986, </year> <pages> pp. 85-90. </pages>
Reference-contexts: capacity, and disk capacity have all grown tremendously: g Single chip processors have increased in speed at the rate of 40%-100% per year [Bell 84, Joy 85]. g Caches have increased in speed 40% to 100% per year. g Main memory has quadrupled in capacity every two or three years <ref> [Moore 75, Myers 86] </ref>. In contrast, disk access times have undergone only modest performance improvements. For example, seek time has improved only about 7% per year [Harker 81]. This imbalanced system growth has already led to many I/O bound supercomputer applications [Kim 87].
Reference: [Ng 88] <author> S. Ng, D. Lang, R. Selinger, </author> <title> "Trade-offs Between Devices and Paths in Achieving Disk Interleaving", </title> <booktitle> The 15th Annual International Symposium on Computer Architecture (SIGARCH 88). </booktitle>
Reference: [Ousterhout 85] <author> J. Ousterhout, H. Da Costa, D. Harrison, J. Kunze, M. Kupfer, J. Thompson, </author> <title> ``A Trace-Driven Analysis of the UNIX 4.2 BSD File System,'' </title> <journal> ACM Operating Systems Review, </journal> <volume> Vol. 19, No. 5, </volume> <booktitle> Proceedings of the 10th ACM Symposium on Operating System Principles, </booktitle> <month> Dec. </month> <pages> 1-4, </pages> <year> 1985. </year>
Reference-contexts: Thus, with 10 disks and track-level striping, large requests have been exactly 400 KB and individual requests have been exactly 40 KB. However, in real-world systems, large requests are often much larger than 400 KB [Bucher 80] and small requests are often much smaller than 40 KB <ref> [Ousterhout 85, Anon 85] </ref>. Also, applications rarely issue requests that are all the same size. We therefore make two changes to the request size distribution: (1) We no longer restrict the workloads to one particular size. Rather, we use a distribution of request sizes.
Reference: [Patterson 88] <author> D. Patterson, G. Gibson, and R. Katz, </author> <title> ``A Case for Redundant Arrays of Inexpensive Disks (RAID),'' </title> <booktitle> ACM SIGMOD conference proceedings, </booktitle> <address> Chicago, IL., </address> <month> June 1-3, </month> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: We then present the hardware environment, the experiment workload design, the metrics for our results, and the performance results. We start by running the same workload as the RAID paper <ref> [Patterson 88] </ref>: request sizes are full stripe or individual blocks; requests are all reads or all writes. In Section 7.1, we analyze an idle system to break down the response time for a basic I/O. <p> As our last step in making the experiment more realistic, we allow workloads with both reads and writes. 2. Introduction to Redundant Arrays of Disks In "A Case for Redundant Arrays of Inexpensive Disks (RAID)", henceforth referred to as "The RAID paper" <ref> [Patterson 88] </ref>, Patterson, Gibson, and Katz present five ways to introduce redundancy into an array of disks: RAID Level 1 through RAID Level 5. <p> This paper focuses on these two RAID Levels, plus the basic non-redundant RAID Level 0, added to provide a basis of comparison between RAID Levels 1 and 5. Figure 1 shows the data layout in the three redundancy schemes. The rest of this section summarizes the RAID Levels--see <ref> [Patterson 88] </ref> for more details. In all organizations, data are interleaved across all disks [Kim 86, Salem 86]. We define a stripe of data to be one unit of interleaving from each disk. <p> Sha dow disks are shaded. (c) RAID Level 5--rotated parity. The shaded areas are parity. for different stripes to reduce hot spots for single block writes (see <ref> [Patterson 88] </ref>). hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh parity information onto disks. As shown in Figure 1c, parity for stripe 0 is kept on disk 0; parity for stripe 1 is kept on disk 1, and so on. <p> R,r: reads W,w: writes (N-1)/N R,W: large (full stripe) r,w: small (individual) Throughput Relative to RAID Level 0 (non-redundant array) is the estimated throughput of RAID Level 1 (mirrored RAID) and RAID Level 5 (rotated parity RAID) as a percentage of the estimated throughput of RAID Level 0 (non-redundant RAID) <ref> [Patterson 88] </ref>. RAID Level 5 large write performance is calculated assuming 11 total disks (N=11). 100% individual requests. These experiments deal with a distribution of request sizes, including partial stripe accesses and accesses larger than a full stripe. (3) The RAID paper workloads were either 100% reads or 100% writes. <p> 49% 91% 50% 100% 100% RAID Level 1 RAID Level 5 actual r,w: request size = individual = 40 KB R,W: request size = full stripe = 400 KB The measured maximum throughput per disk relative to RAID Level 0 is compared against the estimates made in the RAID paper <ref> [Patterson 88] </ref>. The only significant differences occur for RAID Level 1 reads, due to seek optimization, and RAID Level 5 individual writes, due to saving a seek. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Because of seek optimization, this assumption is no longer valid.
Reference: [Salem 86] <author> K. Salem, H. Garcia-Molina, </author> <title> ``Disk Striping,'' </title> <booktitle> IEEE 1986 Int. Conf on Data Engineering, </booktitle> <year> 1986. </year>
Reference-contexts: Continued improvement in system performance depends in a large part on I/O systems with higher data and I/O rates. One way to increase I/O performance is by using an array of many disks <ref> [Kim 86, Salem 86] </ref>. By using many disks, both throughput (MB per second) and I/O rate (I/O's per second) can be increased. <p> Figure 1 shows the data layout in the three redundancy schemes. The rest of this section summarizes the RAID Levels--see [Patterson 88] for more details. In all organizations, data are interleaved across all disks <ref> [Kim 86, Salem 86] </ref>. We define a stripe of data to be one unit of interleaving from each disk. For example, the first stripe of data in Figure 1 consists of logical blocks 0, 1, 2, and 3.
Reference: [Schulze 88] <author> M. Schulze, G. Gibson, R. Katz, and D. Patterson, </author> <title> ``How Reliable is a RAID?,'' </title> <booktitle> Spring COMPCON 89, </booktitle> <address> March 1, 1989, San Francisco, CA. </address>
Reference-contexts: Throughput can be increased by having many disks cooperate in transferring one block of information; the I/O rate can be increased by having multiple independent disks service multiple independent requests. With multiple disks, however, comes lower reliability. According to the commonly used exponential model for disk failures <ref> [Schulze 88] </ref>, 100 disks have a combined failure rate of 100 times the failure rate of a single disk. If every disk failure caused data loss, a 100 disk array would lost data every few hundred hours. This is intolerable for a supposedly stable storage system.
Reference: [Thisquen 88] <author> J. Thisquen, </author> <title> ``Seek Time Measurements'', </title> <type> Amdahl Peripheral Products Division Technical Report, </type> <month> May 9, </month> <year> 1988. </year>
Reference: [UTS 88] <institution> UTS580 User Reference Manual, Amdahl publication ML-142292, </institution> <year> 1988. </year>
Reference-contexts: Instead, we simply read and write bytes on the disks. This enables us to simulate a large range of I/O access patterns without dealing with the logistics of many benchmarks. In our experiment, the reads and writes are done by user processes accessing raw devices <ref> [UTS 88] </ref>. To achieve a certain target 90th percentile response time, we control the number of outstanding logical requests in the system (queue depth).
References-found: 23

