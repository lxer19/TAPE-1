URL: http://www.research.microsoft.com/~dbwilson/ja/cover.ps.gz
Refering-URL: http://www.research.microsoft.com/~dbwilson/ja/
Root-URL: http://www.research.microsoft.com
Title: How to Get an Exact Sample From a Generic Markov Chain and Sample a Random
Author: David Bruce Wilson James Gary Propp 
Abstract: This paper shows how to obtain unbiased samples from an unknown Markov chain by observing it for O(T c ) steps, where T c is the cover time. This algorithm improves on several previous algorithms, and there is a matching lower bound. Using the techniques from the sampling algorithm, we also show how to sample random directed spanning trees from a weighted directed graph, with arcs directed to a root, and probability proportional to the product of the edge weights. This tree sampling algorithm runs within 18 cover times of the associated random walk, and is more generally applicable than the algorithm of Broder and Aldous. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David Aldous. </author> <title> On simulating a Markov chain stationary distribution when transition probabilities are unknown, 1994. </title> <type> Preprint. </type>
Reference-contexts: However, their algorithm is complicated, it takes a long time, and no bounds on its runtime are given. Aldous <ref> [1] </ref> described an efficient procedure for sampling from an unknown Markov chain that can be simulated, but with some bias " in the samples. Lovasz and Winkler [28] found the first provably polynomial time procedure for obtaining unbiased samples by observing an unknown Markov chain. <p> If T c denotes the cover time of the random walk, i.e. the expected time it takes to visit all the states 1 2 David B. Wilson and James G. Propp Algorithm Expected run time Asmussen, Glynn, Thorisson [5] finite time Aldous <ref> [1] </ref> (" bias in sample) O (t =" 2 ) Lovasz, Winkler [28] O (h 2 n log n) or O (hT mix n log n) This paper O (T c ) O (h log n) or O (T mix n log n) n = number of states = stationary probability <p> * When we next visit some state i for the first time, commit to setting f (i) to be the state when the alarm clock next goes off. (See Figure 3.) clock random number between 1 and 4C for i 1 to n status [i] UNSEEN num assigned 0 status <ref> [1] </ref> SEEN stack [0] 1 ptr 1 while num assigned &lt; n while clock &lt; 4C clock clock + 1 s NextState () if status [s] = UNSEEN then status [s] SEEN stack [ptr] s ptr ptr + 1 clock 0 num assigned num assigned + ptr while ptr &gt; 0
Reference: [2] <author> David J. Aldous. </author> <title> A random walk construction of uniform spanning trees and uniform labelled trees. </title> <journal> SIAM Journal of Discrete Mathematics, </journal> <volume> 3(4) </volume> <pages> 450-465, </pages> <year> 1990. </year>
Reference-contexts: Colbourn, Myrvold, and Neufeld [13] simplified this algorithm, and showed how to sample random ar-borescences in the time required to multiply n fi n matrices, currently O (n 2:376 ) [14]. Several other algorithms are based on a Markov chain for arborescences in a graph. Broder [9] and Aldous <ref> [2] </ref> independently found an algorithm based on Markov chains for randomly generating spanning trees. They used a Markov chain for random trees which updated a tree by moving the root according to a random walk on the underlying graph, and making suitable updates to the edge set of the tree.
Reference: [3] <author> David J. Aldous and James A. Fill. </author> <title> Reversible Markov Chains and Random Walks on Graphs. </title> <note> Book in preparation, </note> <year> 1995. </year>
Reference-contexts: They also show how get unbiased samples in time O (hT mix n log n), where T mix is the mixing time threshold, a parameter which measures how long the Markov chain takes to randomize. (See [16] or <ref> [3] </ref> for background on the mixing time.) In x2 and x3 we describe an exact sampling algorithm that is simpler and much more efficient. If T c denotes the cover time of the random walk, i.e. the expected time it takes to visit all the states 1 2 David B. <p> maximum hitting time = max i;j E i T j E i C = expected time to visit all states starting from i T c = cover time = max i E i C T mix = mixing time threshold; time for Markov chain to "get within 1=e of random" <ref> [3] </ref> starting from the worst possible state, then our algorithm runs in O (T c ) time. (It is easy to show that T c O (h log n) [3].) It has been noted that any exact sampling algorithm must visit all the states, so if we may only observe the <p> max i E i C T mix = mixing time threshold; time for Markov chain to "get within 1=e of random" <ref> [3] </ref> starting from the worst possible state, then our algorithm runs in O (T c ) time. (It is easy to show that T c O (h log n) [3].) It has been noted that any exact sampling algorithm must visit all the states, so if we may only observe the Markov chain, our algorithm is best possible (up to a constant factor). <p> One simple variation runs in time O (T mix n log n), though for most chains the cover time bound is better. The table below summarizes the performance of the new and previous algorithms. See <ref> [3] </ref> for additional background on the relevant Markov chain parameters.
Reference: [4] <author> Laurent Alonso and Rene Schott. </author> <title> Random Generation of Trees: Random Generators in Computer Science. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995. </year>
Reference-contexts: Aside from much previous work at generating random arborescences, this application is interesting because it demonstrates that the technique of coupling-from-the-past may be effectively applied to non-monotone Markov chains with huge state spaces. The generation of random trees has been studied extensively (see <ref> [4] </ref>), and specifically the generation of random spanning trees (and arborescences) of an (undirected or directed) graph has been a useful subroutine in several algorithms.
Reference: [5] <author> Stren Asmussen, Peter W. Glynn, and Hermann Thorisson. </author> <title> Stationary detection in the initial transient problem. </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> 2(2) </volume> <pages> 130-157, </pages> <year> 1992. </year>
Reference-contexts: Then besides eliminating the initialization bias from our samples, we could get good samples in finite time without first having to analyze the convergence properties of the Markov chain. Asmussen, Glynn, and Thorisson <ref> [5] </ref> showed that it is possible to make such a procedure by a randomized stopping rule provided that the number of states n is known and finite. However, their algorithm is complicated, it takes a long time, and no bounds on its runtime are given. <p> If T c denotes the cover time of the random walk, i.e. the expected time it takes to visit all the states 1 2 David B. Wilson and James G. Propp Algorithm Expected run time Asmussen, Glynn, Thorisson <ref> [5] </ref> finite time Aldous [1] (" bias in sample) O (t =" 2 ) Lovasz, Winkler [28] O (h 2 n log n) or O (hT mix n log n) This paper O (T c ) O (h log n) or O (T mix n log n) n = number of
Reference: [6] <author> Rodney J. Baxter. </author> <title> Exactly Solved Models in Statistical Mechanics. </title> <publisher> Academic Press, </publisher> <year> 1982. </year>
Reference-contexts: One example of interest to physicists is a Markov chain for sampling from the Ising model. The Ising model has been found to accurately model the phase transition phenomena of real-world substances <ref> [6] </ref> [7].
Reference: [7] <author> J.J. Binney, N.J. Dowrick, A.J. Fisher, and M.E.J. Newman. </author> <title> The Theory of Critical Phenomena: An Introduction to the Renormalization Group. </title> <publisher> Oxford University Press, </publisher> <year> 1992. </year>
Reference-contexts: One example of interest to physicists is a Markov chain for sampling from the Ising model. The Ising model has been found to accurately model the phase transition phenomena of real-world substances [6] <ref> [7] </ref>. In [31] we show how to efficiently obtain unbiased samples from the Ising model at any temperature, and we have generated perfectly random Ising configurations from a sample space of more than 2 4;000;000 states (see Figure 1) even though no rigorous bounds on the mixing time are known.
Reference: [8] <author> Bela Bollobas. </author> <title> Graph Theory: An Introductory Course. </title> <publisher> Springer-Verlag, </publisher> <year> 1979. </year> <note> Graduate texts in mathematics, #63. </note>
Reference-contexts: Several algorithms for random spanning tree generation rely on the close relation between enumeration and random generation, and are based on the Matrix Tree Theorem which allows one to compute the number of spanning trees by evaluating a determinant (see <ref> [8, ch. 2, thm. 8] </ref>). Guenoche [20] and Kulkarni [25] gave one such algorithm that runs in O (n 3 m) time 1 , where n is the number of vertices and m is the number of edges.
Reference: [9] <author> Andrei Broder. </author> <title> Generating random spanning trees. </title> <booktitle> In Foundations of Computer Science, </booktitle> <pages> pages 442-447, </pages> <year> 1989. </year>
Reference-contexts: Colbourn, Myrvold, and Neufeld [13] simplified this algorithm, and showed how to sample random ar-borescences in the time required to multiply n fi n matrices, currently O (n 2:376 ) [14]. Several other algorithms are based on a Markov chain for arborescences in a graph. Broder <ref> [9] </ref> and Aldous [2] independently found an algorithm based on Markov chains for randomly generating spanning trees. <p> on an undi-rected graph is reversible, they were able to give unbiased samples in finite time, specifically within the cover time of the graph. (The cover time of a simple undirected graph is bounded by O (n 3 ), and is often as small as O (n log n); see <ref> [9] </ref> and references contained therein.) Broder also described an algorithm for the approximate random generation of arborescences from a directed graph with regular out-degree, which runs within a constant multiple of the cover time. Self-loops may be added to make the out-degree regular; using this trick, Broder [9] was able to <p> log n); see <ref> [9] </ref> and references contained therein.) Broder also described an algorithm for the approximate random generation of arborescences from a directed graph with regular out-degree, which runs within a constant multiple of the cover time. Self-loops may be added to make the out-degree regular; using this trick, Broder [9] was able to give a run time of O (nT c ) for simple directed graphs. Recently Kandel, Ma-tias, Unger, and Winkler [22] showed how to sample arborescences of a directed Eulerian graph (i.e., one in which in-degree equals out-degree at each node).
Reference: [10] <author> Charles J. Colbourn. </author> <title> The Combinatorics of Network Reliability. </title> <publisher> Oxford University Press, </publisher> <year> 1987. </year>
Reference-contexts: Details are in [31]. [12] [30]. In the network reliability application, random spanning trees are relevant to all-terminal reliability, while random arborescences with prescribed root are relevant to reachability <ref> [10] </ref>. Random arborescences are used to generate random Eulerian tours. Random Eulerian tours can be used to make "random genomes" for testing statistical significance of empirical evidence in computational biology [22], and to make random de Bruijn sequences, which in turn have a number of applications [15].
Reference: [11] <author> Charles J. Colbourn, Robert P.J. Day, and Louis D. Nel. </author> <title> Unranking and ranking spanning trees of a graph. </title> <journal> Journal of Algorithms, </journal> <volume> 10 </volume> <pages> 271-286, </pages> <year> 1989. </year>
Reference-contexts: This algorithm was optimized for the generation of many random spanning trees to make it more suitable for Monte Carlo studies [12]. Colbourn, Day, and Nel <ref> [11] </ref> showed how to reuse previous calculations and reduce the time spent computing determinants to get an O (n 3 ) algorithm for random spanning trees.
Reference: [12] <author> Charles J. Colbourn, Bradley M. Debroni, and Wendy J. Myrvold. </author> <title> Estimating the coefficients of the reliability polynomial. </title> <journal> Congressus Numeran-tium, </journal> <volume> 62 </volume> <pages> 217-223, </pages> <year> 1988. </year>
Reference-contexts: This sample was generated using the method of coupling-from-the-past. No good rigorous bounds on the mixing time are known, yet this sample was obtained in a reasonable amount of time. Details are in [31]. <ref> [12] </ref> [30]. In the network reliability application, random spanning trees are relevant to all-terminal reliability, while random arborescences with prescribed root are relevant to reachability [10]. Random arborescences are used to generate random Eulerian tours. <p> This algorithm was optimized for the generation of many random spanning trees to make it more suitable for Monte Carlo studies <ref> [12] </ref>. Colbourn, Day, and Nel [11] showed how to reuse previous calculations and reduce the time spent computing determinants to get an O (n 3 ) algorithm for random spanning trees.
Reference: [13] <author> Charles J. Colbourn, Wendy J. Myrvold, and Eugene Neufeld. </author> <title> Two algorithms for unranking arborescences. </title> <journal> Journal of Algorithms, </journal> <note> 1995. To appear. </note>
Reference-contexts: Colbourn, Day, and Nel [11] showed how to reuse previous calculations and reduce the time spent computing determinants to get an O (n 3 ) algorithm for random spanning trees. Colbourn, Myrvold, and Neufeld <ref> [13] </ref> simplified this algorithm, and showed how to sample random ar-borescences in the time required to multiply n fi n matrices, currently O (n 2:376 ) [14]. Several other algorithms are based on a Markov chain for arborescences in a graph.
Reference: [14] <author> Don Coppersmith and Shmuel Winograd. </author> <title> Matrix multiplication via arithmetic progressions. </title> <booktitle> In Symposium on the Theory of Computing, </booktitle> <pages> pages 1-6, </pages> <year> 1987. </year>
Reference-contexts: Colbourn, Myrvold, and Neufeld [13] simplified this algorithm, and showed how to sample random ar-borescences in the time required to multiply n fi n matrices, currently O (n 2:376 ) <ref> [14] </ref>. Several other algorithms are based on a Markov chain for arborescences in a graph. Broder [9] and Aldous [2] independently found an algorithm based on Markov chains for randomly generating spanning trees.
Reference: [15] <author> W. Fernandez de la Vega and A. Guenoche. </author> <title> Construction de mots circulaires aleatoires uni-formement distribues. </title> <journal> Mathematiques et Sciences Humaines, </journal> <volume> 58 </volume> <pages> 25-29, </pages> <year> 1977. </year>
Reference-contexts: Random arborescences are used to generate random Eulerian tours. Random Eulerian tours can be used to make "random genomes" for testing statistical significance of empirical evidence in computational biology [22], and to make random de Bruijn sequences, which in turn have a number of applications <ref> [15] </ref>. Not surprisingly there is a long history of algorithms for generating random spanning trees and arborescences.
Reference: [16] <author> Persi Diaconis. </author> <title> Group Representations in Probability and Statistics. </title> <institution> Institute of Mathematical Statistics, </institution> <year> 1988. </year>
Reference-contexts: The research that led to this article was done in part while the first author was visiting Sandia National Laboratories. y MIT Math Department, dbwilson@mit.edu z MIT Math Department, propp@math.mit.edu work at determining how long is "long enough" <ref> [16] </ref> [21] [18] [27] [32] [17], this remains an arduous task. <p> They also show how get unbiased samples in time O (hT mix n log n), where T mix is the mixing time threshold, a parameter which measures how long the Markov chain takes to randomize. (See <ref> [16] </ref> or [3] for background on the mixing time.) In x2 and x3 we describe an exact sampling algorithm that is simpler and much more efficient.
Reference: [17] <author> Persi Diaconis and Laurent Saloff-Coste. </author> <title> What do we know about the Metropolis algorithm? In Symposium on the Theory of Computing, </title> <address> pages 112-129, </address> <year> 1995. </year>
Reference-contexts: The research that led to this article was done in part while the first author was visiting Sandia National Laboratories. y MIT Math Department, dbwilson@mit.edu z MIT Math Department, propp@math.mit.edu work at determining how long is "long enough" [16] [21] [18] [27] [32] <ref> [17] </ref>, this remains an arduous task. Suppose that we have a general procedure that, when given a Markov chain that can be observed or simulated, determines on its own how long to run it, and then outputs a sample which is distributed exactly according to the stationary distribution .
Reference: [18] <author> Persi Diaconis and Daniel Stroock. </author> <title> Geometric bounds for eigenvalues of Markov chains. </title> <journal> The Annals of Applied Probability, </journal> <volume> 1(1) </volume> <pages> 36-61, </pages> <year> 1991. </year>
Reference-contexts: The research that led to this article was done in part while the first author was visiting Sandia National Laboratories. y MIT Math Department, dbwilson@mit.edu z MIT Math Department, propp@math.mit.edu work at determining how long is "long enough" [16] [21] <ref> [18] </ref> [27] [32] [17], this remains an arduous task.
Reference: [19] <author> C. M. Fortuin and P. W. Kasteleyn. </author> <title> On the random cluster model. I. Introduction and relation to other models. </title> <journal> Physica, </journal> <volume> 57(4) </volume> <pages> 536-564, </pages> <year> 1972. </year>
Reference-contexts: If T mix is the mixing time of the Markov chain, and l is the length of the longest chain in the partial order, then the runtime of monotone coupling-from-the-past is O (T mix log l). Other sample spaces which have monotone Markov chains include the random cluster model <ref> [19] </ref> (which was used to generate Figure 1 using the relation between the random cluster and Ising models), the set of antichains of a partially ordered set with uniform distribution, the set of independent sets of a bipartite graph with uniform distribution [24], the square-ice model [26], and the dimer model
Reference: [20] <author> A. Guenoche. </author> <title> Random spanning tree. </title> <journal> Journal of Algorithms, </journal> <volume> 4 </volume> <pages> 214-220, </pages> <year> 1983. </year> <note> In French. </note>
Reference-contexts: Several algorithms for random spanning tree generation rely on the close relation between enumeration and random generation, and are based on the Matrix Tree Theorem which allows one to compute the number of spanning trees by evaluating a determinant (see [8, ch. 2, thm. 8]). Guenoche <ref> [20] </ref> and Kulkarni [25] gave one such algorithm that runs in O (n 3 m) time 1 , where n is the number of vertices and m is the number of edges.
Reference: [21] <author> Mark Jerrum and Alistair Sinclair. </author> <title> Approximating the permanent. </title> <journal> SIAM Journal on Computing, </journal> <volume> 18(6) </volume> <pages> 1149-1178, </pages> <year> 1989. </year>
Reference-contexts: The research that led to this article was done in part while the first author was visiting Sandia National Laboratories. y MIT Math Department, dbwilson@mit.edu z MIT Math Department, propp@math.mit.edu work at determining how long is "long enough" [16] <ref> [21] </ref> [18] [27] [32] [17], this remains an arduous task.
Reference: [22] <author> D. Kandel, Y. Matias, R. Unger, and P. Winkler. </author> <title> Shu*ing biological sequences, 1995. </title> <type> Preprint. </type>
Reference-contexts: Random arborescences are used to generate random Eulerian tours. Random Eulerian tours can be used to make "random genomes" for testing statistical significance of empirical evidence in computational biology <ref> [22] </ref>, and to make random de Bruijn sequences, which in turn have a number of applications [15]. Not surprisingly there is a long history of algorithms for generating random spanning trees and arborescences. <p> Self-loops may be added to make the out-degree regular; using this trick, Broder [9] was able to give a run time of O (nT c ) for simple directed graphs. Recently Kandel, Ma-tias, Unger, and Winkler <ref> [22] </ref> showed how to sample arborescences of a directed Eulerian graph (i.e., one in which in-degree equals out-degree at each node). They used the same spanning tree Markov chain, but instead of relying on reversibility they used the Eulerian property to get an unbiased sample within the cover time.
Reference: [23] <author> P.W. Kasteleyn. </author> <title> The statistics of dimers on a lattice. I. The number of dimer arrangements on a quadratic lattice. </title> <journal> Physica, </journal> <volume> 27 </volume> <pages> 1209-1225, </pages> <year> 1961. </year>
Reference-contexts: generate Figure 1 using the relation between the random cluster and Ising models), the set of antichains of a partially ordered set with uniform distribution, the set of independent sets of a bipartite graph with uniform distribution [24], the square-ice model [26], and the dimer model on a square grid <ref> [23] </ref>. It is noteworthy that each of the three Markov chains that Luby, Ran-dall, and Sinclair proposed at the last FOCS [29] is monotone. 5 Random Trees Now we show how to obtain random spanning trees (arborescences) from a directed graph within O (T c ) time.
Reference: [24] <author> Jeong Han Kim, Peter Shor, and Peter Winkler. </author> <title> Random independent sets. </title> <note> Article in preparation. </note>
Reference-contexts: monotone Markov chains include the random cluster model [19] (which was used to generate Figure 1 using the relation between the random cluster and Ising models), the set of antichains of a partially ordered set with uniform distribution, the set of independent sets of a bipartite graph with uniform distribution <ref> [24] </ref>, the square-ice model [26], and the dimer model on a square grid [23].
Reference: [25] <author> V.G. Kulkarni. </author> <title> Generating random combinatorial objects. </title> <journal> Journal of Algorithms, </journal> <volume> 11(2) </volume> <pages> 185-207, </pages> <year> 1990. </year>
Reference-contexts: Several algorithms for random spanning tree generation rely on the close relation between enumeration and random generation, and are based on the Matrix Tree Theorem which allows one to compute the number of spanning trees by evaluating a determinant (see [8, ch. 2, thm. 8]). Guenoche [20] and Kulkarni <ref> [25] </ref> gave one such algorithm that runs in O (n 3 m) time 1 , where n is the number of vertices and m is the number of edges.
Reference: [26] <author> Elliott Lieb. </author> <title> Residual entropy of square ice. </title> <journal> Physical Review, </journal> <volume> 162 </volume> <pages> 162-172, </pages> <year> 1967. </year>
Reference-contexts: the random cluster model [19] (which was used to generate Figure 1 using the relation between the random cluster and Ising models), the set of antichains of a partially ordered set with uniform distribution, the set of independent sets of a bipartite graph with uniform distribution [24], the square-ice model <ref> [26] </ref>, and the dimer model on a square grid [23].
Reference: [27] <author> Laszlo Lovasz and Miklos Simonovits. </author> <title> On the randomized complexity of volume and diameter. </title> <booktitle> In Foundations of Computer Science, </booktitle> <pages> pages 482-491, </pages> <note> 1992. </note> <author> 10 David B. Wilson and James G. </author> <month> Propp </month>
Reference-contexts: The research that led to this article was done in part while the first author was visiting Sandia National Laboratories. y MIT Math Department, dbwilson@mit.edu z MIT Math Department, propp@math.mit.edu work at determining how long is "long enough" [16] [21] [18] <ref> [27] </ref> [32] [17], this remains an arduous task.
Reference: [28] <author> Laszlo Lovasz and Peter Winkler. </author> <title> Exact mixing in an unknown Markov chain. </title> <journal> Electronic Journal of Combinatorics, </journal> <volume> 2, </volume> <year> 1995. </year> <note> Paper #R15. </note>
Reference-contexts: However, their algorithm is complicated, it takes a long time, and no bounds on its runtime are given. Aldous [1] described an efficient procedure for sampling from an unknown Markov chain that can be simulated, but with some bias " in the samples. Lovasz and Winkler <ref> [28] </ref> found the first provably polynomial time procedure for obtaining unbiased samples by observing an unknown Markov chain. <p> Wilson and James G. Propp Algorithm Expected run time Asmussen, Glynn, Thorisson [5] finite time Aldous [1] (" bias in sample) O (t =" 2 ) Lovasz, Winkler <ref> [28] </ref> O (h 2 n log n) or O (hT mix n log n) This paper O (T c ) O (h log n) or O (T mix n log n) n = number of states = stationary probability distribution E i T j = expected time to reach j starting <p> Also note that in general some external random bits will be required, as the Markov chain might be deterministic or almost deterministic. When the Markov chain is not deterministic, Lovasz and Winkler <ref> [28] </ref> discuss how to make do without external random bits by observing the random transitions. 4 Monotone Random Maps When a Markov chain has special structure, sometimes there are more efficient ways to construct random maps than the general procedure given in x3.
Reference: [29] <author> Michael Luby, Dana Randall, and Alistair Sin-clair. </author> <title> Markov chain algorithms for planar lattice structures (extended abstract). </title> <booktitle> In Foundations of Computer Science, </booktitle> <pages> pages 150-159, </pages> <year> 1995. </year>
Reference-contexts: It is noteworthy that each of the three Markov chains that Luby, Ran-dall, and Sinclair proposed at the last FOCS <ref> [29] </ref> is monotone. 5 Random Trees Now we show how to obtain random spanning trees (arborescences) from a directed graph within O (T c ) time. Most graphs have exponentially many ar-borescences; the complete graph has n n1 of them.
Reference: [30] <author> Louis D. Nel and Charles J. Colbourn. </author> <title> Combining Monte Carlo estimates and bounds for network reliability. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 277-298, </pages> <year> 1990. </year>
Reference-contexts: This sample was generated using the method of coupling-from-the-past. No good rigorous bounds on the mixing time are known, yet this sample was obtained in a reasonable amount of time. Details are in [31]. [12] <ref> [30] </ref>. In the network reliability application, random spanning trees are relevant to all-terminal reliability, while random arborescences with prescribed root are relevant to reachability [10]. Random arborescences are used to generate random Eulerian tours.
Reference: [31] <author> James G. Propp and David B. Wilson. </author> <title> Exact sampling with coupled Markov chains and applications to statistical mechanics. Random Structures and Algorithms, </title> <note> 1995. To appear. </note>
Reference-contexts: In x4 we give a preview of our companion paper <ref> [31] </ref>, in which we investigate "monotone Markov chains", chains whose state spaces have a partial order that is preserved by the moves of the Markov chain. One example of interest to physicists is a Markov chain for sampling from the Ising model. <p> One example of interest to physicists is a Markov chain for sampling from the Ising model. The Ising model has been found to accurately model the phase transition phenomena of real-world substances [6] [7]. In <ref> [31] </ref> we show how to efficiently obtain unbiased samples from the Ising model at any temperature, and we have generated perfectly random Ising configurations from a sample space of more than 2 4;000;000 states (see Figure 1) even though no rigorous bounds on the mixing time are known. <p> This sample was generated using the method of coupling-from-the-past. No good rigorous bounds on the mixing time are known, yet this sample was obtained in a reasonable amount of time. Details are in <ref> [31] </ref>. [12] [30]. In the network reliability application, random spanning trees are relevant to all-terminal reliability, while random arborescences with prescribed root are relevant to reachability [10]. Random arborescences are used to generate random Eulerian tours. <p> Wilson and James G. Propp 2 Coupling From the Past The main idea used in both the algorithms in this paper is the "coupling-from-the-past" protocol, first introduced by Propp and Wilson <ref> [31] </ref>. The idea behind coupling-from-the-past is simple: Suppose that a Markov chain has been running for all time. Then the state at time 0 is distributed exactly according to the stationary distribution of the Markov chain, assuming the chain is ergodic. <p> Thus the composition of n independent outputs of RandomMap () has a positive chance of being a constant map, and condition 2 above is also satisfied. We quote from <ref> [31] </ref> the pseudocode for coupling-from-the-past (Figure 2) and the theorem proving that it works. x3 will describe a better implementation of the RandomMap () procedure, which in conjunction with the procedure below will yield unbiased samples in time proportional to the cover time. t 0 t the identity map while F <p> While most Markov chains do not have a suitable partial order, a number of practically important ones do. In this section we outline the main ideas behind monotone coupling-from-the-past, which we investigate more thoroughly in <ref> [31] </ref>. For expository purposes it is convenient to consider a specific Markov chain. One significant example is the Ising model, which was introduced to model ferromagnetic substances. The Ising model is a spin system on a grid: each grid point can be spin-up or spin-down. <p> Thus we may implement coupling-from-the-past by simulating the chain starting from just two states. For efficiency, it is better to evaluate F 0 t ( ^ 0) and t ( ^ 1) only when t is a power of two. In <ref> [31] </ref> we show that if a monotone Markov chain is "rapidly mixing", then this process couples rapidly: Theorem 4.1.
Reference: [32] <author> Alistair Sinclair. </author> <title> Algorithms for Random Generation and Counting: A Markov Chain Approach. </title> <publisher> Birkhauser, </publisher> <year> 1993. </year>
Reference-contexts: The research that led to this article was done in part while the first author was visiting Sandia National Laboratories. y MIT Math Department, dbwilson@mit.edu z MIT Math Department, propp@math.mit.edu work at determining how long is "long enough" [16] [21] [18] [27] <ref> [32] </ref> [17], this remains an arduous task. Suppose that we have a general procedure that, when given a Markov chain that can be observed or simulated, determines on its own how long to run it, and then outputs a sample which is distributed exactly according to the stationary distribution .
References-found: 32

