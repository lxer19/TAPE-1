URL: http://www.cs.columbia.edu/~andreas/publications/KDD98.ps.gz
Refering-URL: http://www.cs.columbia.edu/~andreas/publications/publications.html
Root-URL: http://www.cs.columbia.edu
Email: fandreas,salg@cs.columbia.edu  
Title: Mining databases with different schemas: Integrating incompatible classifers  
Author: Andreas L. Prodromidis and Salvatore Stolfo 
Address: 1214 Amsterdam Ave. Mail Code 0401 New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Abstract: Distributed data mining systems aim to discover (and combine) usefull information that is distributed across multiple databases. The JAM system, for example, applies machine learning algorithms to compute models over distributed data sets and employs meta-learning techniques to combine the multiple models. Occasionally, however, these models (or classifiers) are induced from databases that have (moderately) different schemas and hence are incompatible. In this paper, we investigate the problem of combining multiple models computed over distributed data sets with different schemas. Through experiments performed on actual credit card data provided by two different financial institutions, we evaluate the effectiveness of the proposed approaches and demonstrate their potential utility. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: To be more specific, by deploying regression methods (e.g. Cart <ref> (Breiman et al. 1984) </ref>, locally weighted regression (C. G. <p> Poisson). Experiments and Evaluation To evaluate these techniques, we used 5 inductive learning algorithms (ID3, C4.5, Cart <ref> (Breiman et al. 1984) </ref>, Naive Bayes (Minksy & Papert 1969) and Ripper (Co-hen 1995)) and 2 data sets of real credit card transactions provided by the Chase and First Union Banks. Each bank supplied .5 million records spanning one year.
Reference: <author> C. G. Atkeson, S. A. Schaal, A. W. M. </author> <year> 1997. </year> <title> Locally weighted learning. </title> <journal> AI Review, </journal> <note> To Appear. </note>
Reference: <author> Chan, P., and Stolfo, S. </author> <year> 1993. </year> <title> Meta-learning for mul and of Chase base-classifier/bridging classifier pairs (black bars) on First Union data. Center: Total accuracy (left), T P F P spread (middle) and total savings (right) of First Union meta-classifiers composed by Chase base-classifier/bridging-classifier pairs. Bottom: Total accuracy (left), T P F P spread (middle) and total savings (right) of First Union meta-classifiers combining both Chase and First Union base classifiers. </title> <booktitle> tistrategy and parallel learning. In Proc. Second Intl. Work. Multistrategy Learning, </booktitle> <pages> 150-165. </pages>
Reference-contexts: In the last part of the paper, we describe our experiments on credit card data obtained by two independent institutions and we establish that combining models from different sources can substantially improve performance. Database Compatibility Meta-learning <ref> (Chan & Stolfo 1993) </ref> is a technique that addresses the scaling problem of machine learning, i.e. the problem of learning useful information from large and inherently distributed databases.
Reference: <author> Cohen, W. </author> <year> 1995. </year> <title> Fast effective rule induction. </title> <booktitle> In Proc. 12th Intl. Conf. Machine Learning, </booktitle> <pages> 115-123. </pages>
Reference: <author> Dietterich, T. </author> <year> 1997. </year> <title> Machine learning research: Four current directions. </title> <journal> AI Magazine 18(4) </journal> <pages> 97-136. </pages>
Reference-contexts: One approach of mining and combining such information is to apply various machine learning programs to discover patterns exhibited in the data and then combine the computed descriptive representations. Combining multiple classification models has been receiving increased attention <ref> (Dietterich 1997) </ref>.
Reference: <author> J.H.Friedman. </author> <year> 1991. </year> <title> Multivariate adaptive regression splines. </title> <journal> The Annals of Statistics 19(1) </journal> <pages> 1-141. </pages>
Reference-contexts: To be more specific, by deploying regression methods (e.g. Cart (Breiman et al. 1984), locally weighted regression (C. G. Atkeson 1997), linear regression fit (Myers 1986), MARS <ref> (J.H.Friedman 1991) </ref>) for continuous attributes and machine learning algorithms for categorical attributes, data site A can compute one or more auxiliary classifier agents C Aj 0 that predict the value of attribute A n+1 based on the common attributes A 1 ; :::; An.
Reference: <author> Minksy, M., and Papert, S. </author> <year> 1969. </year> <title> Perceptrons: An Introduction to Computation Geometry. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher> <address> (Expanded edition, </address> <year> 1988). </year>
Reference-contexts: Poisson). Experiments and Evaluation To evaluate these techniques, we used 5 inductive learning algorithms (ID3, C4.5, Cart (Breiman et al. 1984), Naive Bayes <ref> (Minksy & Papert 1969) </ref> and Ripper (Co-hen 1995)) and 2 data sets of real credit card transactions provided by the Chase and First Union Banks. Each bank supplied .5 million records spanning one year.
Reference: <author> Myers, R. </author> <year> 1986. </year> <title> Classical and Modern Regression with Applications. </title> <address> Boston, MA: </address> <publisher> Duxbury. </publisher>
Reference-contexts: To be more specific, by deploying regression methods (e.g. Cart (Breiman et al. 1984), locally weighted regression (C. G. Atkeson 1997), linear regression fit <ref> (Myers 1986) </ref>, MARS (J.H.Friedman 1991)) for continuous attributes and machine learning algorithms for categorical attributes, data site A can compute one or more auxiliary classifier agents C Aj 0 that predict the value of attribute A n+1 based on the common attributes A 1 ; :::; An.
Reference: <author> Prodromidis, A.; Stolfo, S.; and Chan, P. </author> <year> 1998. </year> <title> Pruning classifiers in a distributed meta-learning system. </title> <type> Technical report, </type> <institution> Columbia Univ. CUCS-011-98. StatSci Division, MathSoft. </institution> <year> 1996. </year> <note> Splus, Version 3.4. </note>
Reference-contexts: The order of the base-classifiers selected is determined by a special pruning algorithm that aims to improve the (T P F P ) rate while maximizing coverage (i.e. the percentage of transactions classified correctly by at least one base-classifier) <ref> (Prodromidis, Stolfo, & Chan 1998) </ref>. <p> The new First Union meta-classifier is superior even to the best "pure" First Union meta-classifiers (i.e. the meta-classifier composed by local base-classifiers alone) as reported in <ref> (Prodromidis, Stolfo, & Chan 1998) </ref> improving total accuracy by 1.5%, T P F P spread by 5.3% and savings by $20K.
Reference: <author> Stolfo, S.; Prodromidis, A.; Tselepis, S.; Lee, W.; Fan, W.; and Chan, P. </author> <year> 1997. </year> <title> JAM: Java agents for meta-learning over distributed databases. </title> <booktitle> In Proc. 3rd Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> 74-81. </pages>
Reference-contexts: Although the J AM system <ref> (Stolfo et al. 1997) </ref> addresses the later by employing meta-learning techniques, integrating classification fl Supported in part by an IBM fellowship This research is supported by the Intrusion Detection Program (BAA9603) from DARPA (F30602-96-1-0311), NSF (IRI-96-32225, CDA-96-25374) and NYSSTF (423115-445). Copyright c fl1998, American Association for Artificial Intelligence (www.aaai.org). <p> From their predictions, the meta-learner will detect the properties, the behavior and performance of the base-classifiers and compute a meta-classifier that represents a model of the "global" data set. The J AM system <ref> (Stolfo et al. 1997) </ref> is designed to implement meta-learning.
References-found: 10

