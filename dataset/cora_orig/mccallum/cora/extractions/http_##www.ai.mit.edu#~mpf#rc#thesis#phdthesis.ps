URL: http://www.ai.mit.edu/~mpf/rc/thesis/phdthesis.ps
Refering-URL: 
Root-URL: 
Title: Reversibility for Efficient Computing  Revision: 2.4  
Author: Michael P. Frank 
Date: Started January 5, 1998.  February 12, 1998, 2:54 pm. Formatted April 25, 1998.  
Web: http://www.ai.mit.edu/~mpf  http://www.ai.mit.edu/~mpf/thesis/phdthesis.html  
Address: Rm. 747 545 Technology Sq. Cambridge, MA 02139  
Affiliation: MIT AI Lab,  
Note: Ph.D. Dissertation UNFINISHED WORKING DRAFT MANUSCRIPT  Revision Date:  A current version is available online at  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> William C. Athas, Lars "J." Svensson, Jeffrey G. Koller, Nestoras Tzartza-nis, and Eric Ying-Chin Chou. </author> <title> Low-power digital systems based on adiabatic-switching principles. </title> <journal> IEEE Transactions on Very Large Scale Integration (VLSI) Systems, </journal> <volume> 2(4) </volume> <pages> 398-407, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Moreover, in the April 25, 1998 **DRAFT** 24 CHAPTER 1. INTRODUCTION & BACKGROUND years since Fredkin & Toffoli's 1978 proposal [48] it has become quite feasiable and economical to build reversible devices using conventional VLSI electronic fabrication techniques (cf. Athas et al. 1994 <ref> [1] </ref>); we will review those developments in more detail in chapter 8. 1.3.4 Reversible computing theory Regardless of the devices used, there is also the need to study the algorithmic issues involved in performing large computations using logically reversible primitives. <p> A similar problem of following a chain of nodes April 25, 1998 **DRAFT** 82 CHAPTER 3. REVERSIBLE COMPUTING THEORY Given description d as described in the text, Let q <ref> [1] </ref> : : : q [t] be a table of node values, initially all NULL. Initialize all q [j]'s not pebbled in direction D, as specified by description d.
Reference: [2] <author> Henry G. Baker. </author> <title> Lively linear lisp|`Look ma, no garbage!'. </title> <journal> ACM SigPlan Notices, </journal> <volume> 27(8) </volume> <pages> 89-98, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: +=, and if all local variables are asserted (see assert (3)) to be restored to zero before function return, and if other assert ()s are used to inform the compiler of loop entry conditions, and if one avoids frequent use of dynamic memory allocation (because garbage collection is irreversible; see <ref> [3, 2] </ref>), and one's programs are written essentially in a style that looks basically like assembly language augmented with named variables, nested expressions, and April 25, 1998 **DRAFT** 184 CHAPTER 7. <p> However, Janus ran in SIMULA on a DECSYSTEM-20, and it may no longer exist anywhere in readble form. Also, Henry Baker described a reversible Lisp-like language called "-lisp" in a 1992 paper [3]. -lisp was based on so-called "linear" functional languages, cf. <ref> [2] </ref>. 9.3.3 The R compiler In addition to specifying the R language, We also wrote a simple compiler for translating R programs into assembly code executable on a certain version of the Pendulum architecture.
Reference: [3] <author> Henry G. Baker. </author> <title> NREVERSAL of fortune | the thermodynamics of garbage collection. </title> <editor> In Y. Bekkers, editor, </editor> <booktitle> International Workshop on Memory Management, </booktitle> <pages> pages 507-524. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: +=, and if all local variables are asserted (see assert (3)) to be restored to zero before function return, and if other assert ()s are used to inform the compiler of loop entry conditions, and if one avoids frequent use of dynamic memory allocation (because garbage collection is irreversible; see <ref> [3, 2] </ref>), and one's programs are written essentially in a style that looks basically like assembly language augmented with named variables, nested expressions, and April 25, 1998 **DRAFT** 184 CHAPTER 7. <p> However, Janus ran in SIMULA on a DECSYSTEM-20, and it may no longer exist anywhere in readble form. Also, Henry Baker described a reversible Lisp-like language called "-lisp" in a 1992 paper <ref> [3] </ref>. -lisp was based on so-called "linear" functional languages, cf. [2]. 9.3.3 The R compiler In addition to specifying the R language, We also wrote a simple compiler for translating R programs into assembly code executable on a certain version of the Pendulum architecture.
Reference: [4] <author> Adriano Barenco, David Deutsch, Artur K. Ekert, and Richard Jozsa. </author> <title> Conditional quantum dynamics and logic gates. </title> <journal> Physical Review Letters, </journal> <volume> 74(20) </volume> <pages> 4083-4086, </pages> <month> 15 May </month> <year> 1995. </year> <note> Preprint at Los Alamos Physics Preprint Archive, http:/ /xxx.lanl.gov/abs/quant-ph/9503017. </note>
Reference-contexts: Other proposals for implementation technologies for quantum computing, from various communities, include Teich et al. '88 [99], Lloyd '93 & '94 [72, 73], DiVincenzo April 25, 1998 **DRAFT** 98 CHAPTER 4. QUANTUM COMPUTATION '95 a [32], Sleator & Weinfurter '95 [95], Barenco et al. '95 b <ref> [4] </ref>, and Chuang & Yamamoto '95 [23]. The main lesson to be learned from this long list of proposals is that the details of the physical implementation of quantum computers are "just" an engineering concern, rather than a theoretical issue of fundamental importance.
Reference: [5] <author> Edward Barton. </author> <title> A reversible computer using conservative logic. Term paper for 6.895 at MIT, </title> <year> 1978. </year>
Reference-contexts: April 25, 1998 **DRAFT** 178 CHAPTER 7. DESIGN & PROGRAMMING 9.1.1 Previous reversible architectures The first reversible computer architectures that we know of were designed by Barton (1978, <ref> [5] </ref>) and Ressler (1981, [88]) as thesis projects. These designs were based on the conservative (reversible and 1-conserving) logic model developed by Fredkin and Toffoli (cf. [49]).
Reference: [6] <author> Jacob D. Bekenstein. </author> <title> Universal upper bound on entropy-to-energy ratio for bounded systems. </title> <journal> Phys. Rev. D, </journal> <volume> 23(2) </volume> <pages> 287-298, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: Some particular upper bounds on information content as a function of system size and energy are given by Bekenstein (1984, [7]) and by Joos and Qadir (1992, [57]). Bekenstein's bounds, which originally came out of his studies of the entropy of black holes (e.g., <ref> [6] </ref>), are fairly loose, in the sense that his bounds are much higher than the maximum information content seems to be for systems other than black holes.
Reference: [7] <author> Jacob D. Bekenstein. </author> <title> Entropy content and information flow in systems with limited energy. </title> <journal> Phyical Review D, </journal> <volume> 30(8) </volume> <pages> 1669-1679, </pages> <month> 15 October </month> <year> 1984. </year>
Reference-contexts: Let us look at some particular information density bounds in more detail. 2.2.1 Entropy bounds from black hole physics. Some particular upper bounds on information content as a function of system size and energy are given by Bekenstein (1984, <ref> [7] </ref>) and by Joos and Qadir (1992, [57]). <p> Much tighter bounds can be given for the entropy of normal (non black-hole) systems, given additional assumptions about their composition. This is done in Bekenstein's paper <ref> [7] </ref>, as well as in papers by Joos and Qadir [57] and Smith (1995 [97]) and the related literature.
Reference: [8] <author> C. H. Bennett. </author> <title> Logical reversibility of computation. </title> <journal> IBM J. Research and Development, </journal> <volume> 17(6) </volume> <pages> 525-532, </pages> <year> 1973. </year>
Reference-contexts: To his credit, Landauer realized that this argument was not rigorous, and did not present it as such. 1.3.2 Development of reversible models of computation Landauer's error was not caught until Bennett (1973, <ref> [8] </ref>) discovered that the recorded history could be cleared in a logically reversible way, while leaving only the input and the desired computational output in memory. This refuted Landauer's argument that each useful computational step must incur, in the long run, at least about k B T energy dissipation. <p> The other part requires showing that physically reversible primitive logic devices can actually be built. Bennett's 1973 paper <ref> [8] </ref> suggested the possibility of an enzymatic reversible computer using biomolecules, and in later papers such as (1982, [9]) he described a clockwork mechanical Turing machine powered by Brownian motion. <p> THEORY Notation Model name Example references PRF Primitive recursive functions Rogers 1987 [89], x1.2, pp. 5-9 RF Recursive functions [89], ch. 1 FA Finite automata ch. 2 of Hopcroft & Ullman 1979 [55] RFA Reversible finite automata Pin 1987 [84] TM Turing machines [55], ch. 7 TM Reversible Turing machines <ref> [64, 8, 63] </ref> NTM Nondeterministic Turing machines [55], x7.5 CA Cellular automata von Neumann 1966 [116], Toffoli & Margolus 1991 [104] RAM Random access machines PRAM Parallel random access machines BLC Boolean logic circuit 3dM 3-d mesh Leighton 1992 [66], ch. 1 Table 3.1: Some existing models of computation. model, since <p> Landauer's suggestion (cf. x3 of [62]) of embedding each irreversible operation into a reversible one makes it clear that the number of such operations in a reversible machine need not be larger than the number for an irreversible machine, as was demonstrated by many more explicit later embeddings e.g., <ref> [64, 8] </ref>. So under the time complexity measure by itself, reversibility does not hurt. Can a reversible machine perform a task in fewer operations than an irreversible one? Obviously not, if we take reversible operations to just be a special case of irreversible operations. <p> In Lecerf's embedding, by the time the reversible machine finishes its simulation of the irreversible machine, any outputs from the computation have been uncomputed, just like the garbage. This is not very useful! 3.3.4.2 The Bennett trick Fortunately, in 1973, Charles Bennett <ref> [8] </ref>, who was unaware of Lecerf's work but knew of Landauer's, independently rediscovered Lecerf reversal, and moreover added the ability to retain a useful output. <p> So part (a) reduces to proving RST (S; T) O = ST (S; T) O only for the case where S ~ T or T ~ 2 S . From here, the result follows due to the existing relativizable simulations. When S ~ T, Bennett's simple reversible simulation technique <ref> [8] </ref> can be applied because it takes time O (T) and space O (T). Similarly, when T ~ 2 S the simulation of Lange et al. [63] can be used because it takes time O (2 S ) and space O (S).
Reference: [9] <author> C. H. Bennett. </author> <title> The thermodynamics of computation, a review. </title> <journal> Int'l J. Theoretical Physics, </journal> <volume> 21(12) </volume> <pages> 905-940, </pages> <year> 1982. </year>
Reference-contexts: The other part requires showing that physically reversible primitive logic devices can actually be built. Bennett's 1973 paper [8] suggested the possibility of an enzymatic reversible computer using biomolecules, and in later papers such as (1982, <ref> [9] </ref>) he described a clockwork mechanical Turing machine powered by Brownian motion. Meanwhile, Fredkin and Toffoli had proposed an electronic implementation (1978, [48]), and an idealized model based on the ballistic motion of rigid spheres (1982, [49]).
Reference: [10] <author> C. H. Bennett. </author> <title> Notes on the history of reversible computation. </title> <journal> IBM J. Research and Development, </journal> <volume> 32(1) </volume> <pages> 16-23, </pages> <month> January </month> <year> 1988. </year> <note> Reprinted in [65], </note> <editor> ch. </editor> <volume> 4, </volume> <pages> pp. 281-288. 218 BIBLIOGRAPHY </pages>
Reference-contexts: This is not intended to be a complete account. Some additional historical information will be provided in later chapters. A more comprehensive review of the April 25, 1998 **DRAFT** 22 CHAPTER 1. INTRODUCTION & BACKGROUND early history of part of the field is provided in Bennett 1988 <ref> [10] </ref>. 1.3.1 Early thermodynamics of computation. The study of thermodynamically and logically reversible computational processes has historically been motivated by concerns in fundamental physics.
Reference: [11] <author> C. H. Bennett. </author> <title> Time/space trade-offs for reversible computation. </title> <journal> SIAM J. Computing, </journal> <volume> 18(4) </volume> <pages> 766-776, </pages> <year> 1989. </year>
Reference-contexts: In contexts where digital storage is expensive and energy is cheap, one might do better by just throwing away the bits instead. So in 1989 Bennett developed a more space-efficient version of his algorithm <ref> [11] </ref>. Unfortunately, it incurs an asymptotic slowdown factor that cannot be made arbitrarily small without making the space usage exponentially large (Levine and Sherman 1990 [68]). <p> This means a large asymptotic increase in memory usage for many problems; up to exponentially large. 3.3.5.2 Bennett's pebbling algorithm In 1989, Bennett <ref> [11] </ref> introduced a new, more space-efficient reversible simulation for Turing machines. Description of pebbling algorithm. Analysis of space-time complexity. Optimality of the algorithm. Li and Vitanyi '96 [69] analyzed Bennett's techniques and characterized the benefits of partial reversibility. <p> To our knowledge, no one has yet directly addressed the question of whether a single reversible simulation can run in linear time like Bennett's 1973 technique and in linear space like the new Lange et al. technique. Li and Vitanyi's analysis [69] of Bennett's 1989 algorithm <ref> [11] </ref> leads to our proof that any such simulation cannot relativize to oracles, or work in cases where the space bound is much less than the input length. <p> To do this, we show how M i can be interpreted as following the rules of Bennett's reversible "pebble game," introduced in <ref> [11] </ref> and analyzed by Li and Vitanyi in [69]. Pebble game rules. The game is played on a linear list of nodes, which we will identify with query strings q 1 ; : : : ; q t . <p> Given all these assumptions, just how general are the reversible scaling advantages? Do they cover very many practical applications in large problem classes, other than just physical simulations? The complete answer to this question is uncertain, but one observation is that Bennett's 1989 algorithm <ref> [11, 68] </ref> can be utilized to remove the requirement for the reversibility of the underlying task, while still permitting almost the same polynomial speedups and cost-efficiency benefits. (However, the assumptions regarding paralleliz-ability and communication requirements remain.) Explanation follows. 6.3.1 Speedups for irreversible computations on reversible machines Bennett's technique [11] allows one <p> 1989 algorithm [11, 68] can be utilized to remove the requirement for the reversibility of the underlying task, while still permitting almost the same polynomial speedups and cost-efficiency benefits. (However, the assumptions regarding paralleliz-ability and communication requirements remain.) Explanation follows. 6.3.1 Speedups for irreversible computations on reversible machines Bennett's technique <ref> [11] </ref> allows one to transform a logically irreversible algorithm that requires S memory cells ("space") and T primitive operations ("time") into a reversible algorithm that leaves behind no garbage information (other than input and output) and takes T 0 ~ T (T=S) " operations, and S 0 ~ S log (T=S)
Reference: [12] <author> Charles H. Bennett, Ethan Bernstein, Gilles Brassard, and Umesh V. Vazirani. </author> <title> Strengths and weaknesses of quantum computing. </title> <note> Preprint at http://vesta.- physics.ucla.edu/cgi-bin/uncompress ps cgi?bbbv94h.ps, </note> <year> 1994. </year>
Reference-contexts: Hogg [54] has investigated quantum algorithms that enhance the probability density found along solution paths in NP search problems, but not enough to allow measurements of the machine state for such problems to yield solutions in expected polynomial time. On the other side of the question, Bennett et al. <ref> [12] </ref> have provided suggestive evidence against the NP BQP conjecture, by showing that when R is a random oracle, NP R BQP R with probability 1. However, it is worth noting that since April 25, 1998 **DRAFT** 96 CHAPTER 4.
Reference: [13] <author> Ethan Bernstein and Umesh V. Vazirani. </author> <title> Quantum complexity theory. </title> <booktitle> In 25th Association for Computing Machinery Symposium on the Theory of Computing, </booktitle> <pages> pages 11-20, </pages> <year> 1993. </year>
Reference-contexts: Such a development could lead to eventual practical applications, if and when such quantum computers become buildable. Shor's Factoring Algorithm. However, this idea remained pure speculation until the last several years, when a series of papers on the power of quantum computers <ref> [31, 15, 14, 13, 94] </ref> culminated in Peter Shor's 1994 proof [92, 93] that a (somewhat idealized) quantum computer could factor large integers in polynomial time. This was an astounding discovery, since mathematicians throughout history have searched for April 25, 1998 **DRAFT** 88 CHAPTER 4. <p> This seems like an unnatural problem. But in any case, following the Deutsch-Jozsa paper, analysis of the power of quantum computers developed rapidly with papers <ref> [15, 14, 13] </ref> that defined various quantum complexity classes and compared them with various classical complexity classes in relativized oracle settings similar to Deutsch and Jozsa's. Quantum operations were also found to have uses in implementing various cryptographic operations; see the end of [15] for a summary. <p> Classical algorithms require exponentially many tries to achieve a reasonable probability of success. The extraordinary thing about Simon's construction was its use of a particular unitary transformation equivalent to a special-case of the discrete Fourier transformation that had been introduced earlier by Bernstein and Vazirani <ref> [13] </ref>. Originally this Fourier transform was used to solve a certain simple oracle problem using O (1) queries on a quantum computer as opposed to the fi (n) queries that were classically required. <p> Currently, it seems unlikely that quantum computers could solve NP-complete problems, due to the fact that the only known quantum algorithms that dominate classical algorithms either involve unrealistic oracle-dependent promise problems <ref> [13, 94] </ref>, or introduce only polynomial speedups [50, 51], or only simulate quantum mechanics [17], or depend on the ability to reduce the problem to one involving periodicities for which the quantum Fourier transform is useful [93, 18]; one would not expect such periodicities a priori to be characteristic of all
Reference: [14] <editor> A. Berthiaume and Gilles Brassard. </editor> <booktitle> Oracle quantum computing. In Proceedings of the Workshop on Physics of Computation: </booktitle> <volume> PhysComp '92, </volume> <pages> pages 195-199, </pages> <address> Los Alamitos, CA, </address> <year> 1992. </year> <institution> Institute of Electrical and Electronic Engineers Computer Society Press. </institution> <note> Also to appear in Journal of Modern Optics. </note>
Reference-contexts: Such a development could lead to eventual practical applications, if and when such quantum computers become buildable. Shor's Factoring Algorithm. However, this idea remained pure speculation until the last several years, when a series of papers on the power of quantum computers <ref> [31, 15, 14, 13, 94] </ref> culminated in Peter Shor's 1994 proof [92, 93] that a (somewhat idealized) quantum computer could factor large integers in polynomial time. This was an astounding discovery, since mathematicians throughout history have searched for April 25, 1998 **DRAFT** 88 CHAPTER 4. <p> This seems like an unnatural problem. But in any case, following the Deutsch-Jozsa paper, analysis of the power of quantum computers developed rapidly with papers <ref> [15, 14, 13] </ref> that defined various quantum complexity classes and compared them with various classical complexity classes in relativized oracle settings similar to Deutsch and Jozsa's. Quantum operations were also found to have uses in implementing various cryptographic operations; see the end of [15] for a summary.
Reference: [15] <author> A. Berthiaume and Gilles Brassard. </author> <title> The quantum challenge to structural complexity theory. </title> <booktitle> In Proceedings of the Seventh Annual Structure in Complexity Theory Conference, </booktitle> <pages> pages 132-137, </pages> <address> Los Alamitos, CA, 1992. </address> <publisher> Institute of Electrical and Electronic Engineers Computer Society Press. </publisher>
Reference-contexts: Such a development could lead to eventual practical applications, if and when such quantum computers become buildable. Shor's Factoring Algorithm. However, this idea remained pure speculation until the last several years, when a series of papers on the power of quantum computers <ref> [31, 15, 14, 13, 94] </ref> culminated in Peter Shor's 1994 proof [92, 93] that a (somewhat idealized) quantum computer could factor large integers in polynomial time. This was an astounding discovery, since mathematicians throughout history have searched for April 25, 1998 **DRAFT** 88 CHAPTER 4. <p> This seems like an unnatural problem. But in any case, following the Deutsch-Jozsa paper, analysis of the power of quantum computers developed rapidly with papers <ref> [15, 14, 13] </ref> that defined various quantum complexity classes and compared them with various classical complexity classes in relativized oracle settings similar to Deutsch and Jozsa's. Quantum operations were also found to have uses in implementing various cryptographic operations; see the end of [15] for a summary. <p> Quantum operations were also found to have uses in implementing various cryptographic operations; see the end of <ref> [15] </ref> for a summary. Quantum analogues to the popular classical complexity classes such as BPP (bounded-error probabilistic polynomial-time) and ZPP (zero-error probabilistic polynomial-time) were defined, and various of the quantum classes were shown to be larger than the various classical classes|but only in relativized oracle settings.
Reference: [16] <author> Gianfranco Bilardi and Franco Preparata. </author> <title> Horizons of parallel computation. </title> <type> Technical Report CS-93-20, </type> <institution> Brown University, </institution> <month> May </month> <year> 1993. </year> <note> Also available on the web at http://www.cs.brown.edu/publications/techreports/ reports/CS-93-20.html. </note>
Reference-contexts: The optimal scaling of computation within physically realistic constraints is an issue that has been studied previously (cf. Vitanyi 1988 [114], Bilardi & Preparata 1993 <ref> [16] </ref>, Smith 1995 [97]), but never before with particular attention to how the reversibility of physics allows reversible computation to improve physical scaling behavior.
Reference: [17] <author> Bruce M. Boghosian and Washington Taylor IV. </author> <title> Simulating quantum mechanics on a quantum computer. </title> <editor> In Toffoli et al. </editor> <volume> [103], </volume> <pages> pages 32-35. </pages> <address> http://xxx.lanl.- gov/abs/quant-ph/9701019. </address>
Reference-contexts: Currently, it seems unlikely that quantum computers could solve NP-complete problems, due to the fact that the only known quantum algorithms that dominate classical algorithms either involve unrealistic oracle-dependent promise problems [13, 94], or introduce only polynomial speedups [50, 51], or only simulate quantum mechanics <ref> [17] </ref>, or depend on the ability to reduce the problem to one involving periodicities for which the quantum Fourier transform is useful [93, 18]; one would not expect such periodicities a priori to be characteristic of all problems in NP. <p> However, there is hope that applications such as the use of quantum computers to efficiently simulate models of real quantum physical systems <ref> [17] </ref> might revolutionize physics as we know it. In chapter 2 and in this chapter, we have seen how some basic physical principles affect the limits of what is possible with computers, sometimes in surprising ways.
Reference: [18] <author> Dan Boneh and Richard Lipton. </author> <title> Quantum cryptoanalysis of hidden linear functions. </title> <booktitle> In Advances in Cryptology | CRYPTO '95. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <booktitle> Proceedings of the Crypto '95 Conference, </booktitle> <address> Santa Barbara, California, </address> <month> Aug. </month> <pages> 27-31. </pages> <note> To appear. http://www.cs.princeton.edu/~dabo/papers/quantum.ps. </note>
Reference-contexts: only known quantum algorithms that dominate classical algorithms either involve unrealistic oracle-dependent promise problems [13, 94], or introduce only polynomial speedups [50, 51], or only simulate quantum mechanics [17], or depend on the ability to reduce the problem to one involving periodicities for which the quantum Fourier transform is useful <ref> [93, 18] </ref>; one would not expect such periodicities a priori to be characteristic of all problems in NP. However, it is conceivable someone may yet discover a clever quantum algorithm for general simulation of nondeterministic Turing machines. <p> Another possible show-stopper is that factoring and a handful of other problems relating to cryptography <ref> [18] </ref> may turn out be the only real-world problems amenable to fast quantum solutions, which may not provide enough motivation to support the development of quantum computers.
Reference: [19] <author> Bob Boothe. </author> <title> Algorithms for bidirectional debugging. </title> <type> Technical Report USM/CS-98-2-23, </type> <institution> Computer Science Department, University of Southern Maine, </institution> <address> 96 Falmouth St, Portland ME 04104-9300, </address> <month> February </month> <year> 1998. </year> <title> Author's email: </title> <publisher> boothe@cs.usm.maine.edu. </publisher>
Reference-contexts: So we have seen that a reversible computing capability can ease debugging. However, reversible computing is not strictly necessary for implementing a bi-directional debugger. For example, Boothe (1998, <ref> [19] </ref>) describes algorithms that can be used to implement bi-directional debugging environments for normal (irreversible) programming languages. There are many other bi-directional debuggers as well; see for example [107] and the references in [19]. <p> However, reversible computing is not strictly necessary for implementing a bi-directional debugger. For example, Boothe (1998, <ref> [19] </ref>) describes algorithms that can be used to implement bi-directional debugging environments for normal (irreversible) programming languages. There are many other bi-directional debuggers as well; see for example [107] and the references in [19]. One simple technique that is sometimes used is to save periodic checkpoints of program state, and when stepping backwards, just re-compute forwards from the previous saved checkpoint to reach the state of the program at a desired time-point.
Reference: [20] <author> A. R. Calderbank and Peter W. Shor. </author> <title> Good quantum error-correcting codes exist. </title> <journal> Los Alamos Physics Preprint Archive, </journal> <note> http://xxx.lanl.gov/abs/quant-ph/9512032, December 1995. **DRAFT** April 25, 1998 BIBLIOGRAPHY 219 </note>
Reference-contexts: However, Coppersmith [25] has shown that simple imprecision does not cripple the quantum factoring algorithm, and several more recent papers <ref> [20, 21, 98] </ref> have addressed the more difficult issue of correcting errors due to decoherence of the quantum states.
Reference: [21] <author> Isaac L. Chuang and Raymond Laflamme. </author> <title> Quantum error correction by coding. </title> <journal> Los Alamos Physics Preprint Archive, </journal> <note> http://xxx.lanl.gov/abs/quant-ph/ 9511003, </note> <month> November </month> <year> 1995. </year>
Reference-contexts: However, Coppersmith [25] has shown that simple imprecision does not cripple the quantum factoring algorithm, and several more recent papers <ref> [20, 21, 98] </ref> have addressed the more difficult issue of correcting errors due to decoherence of the quantum states.
Reference: [22] <author> Isaac L. Chuang, Raymond Laflamme, Peter W. Shor, and W. H. Zurek. </author> <title> Quantum computers, factoring and decoherence. </title> <journal> Science, </journal> <volume> 270 </volume> <pages> 1635-1637, </pages> <year> 1995. </year> <note> Preprint at Los Alamos Physics Preprint Archive, http://xxx.lanl.gov/abs/ quant-ph/9503007. </note>
Reference-contexts: strictly more powerful than deterministic ones. 4.4.3 Can errors caused by imprecision and decoherence be controlled sufficiently to allow arbitrarily complex quantum computations to take place with an arbitrarily small probability of failure? A number of papers have expressed pessimism regarding the question of error accumulation in quantum computers, e.g., <ref> [61, 60, 108, 109, 22, 83] </ref>. These papers show that in the absence of error correction, the probability of error increases exponentially with both the time and space complexity of the computation, and the expected error **DRAFT** April 25, 1998 4.4.
Reference: [23] <author> Isaac L. Chuang and Yoshihisa Yamamoto. </author> <title> A simple quantum computer. </title> <journal> Physical Review A, </journal> <volume> 52 </volume> <pages> 3489-3496, </pages> <year> 1995. </year> <note> Preprint at Los Alamos Physics Preprint Archive, http://xxx.lanl.gov/abs/quant-ph/9505011. </note>
Reference-contexts: QUANTUM COMPUTATION '95 a [32], Sleator & Weinfurter '95 [95], Barenco et al. '95 b [4], and Chuang & Yamamoto '95 <ref> [23] </ref>. The main lesson to be learned from this long list of proposals is that the details of the physical implementation of quantum computers are "just" an engineering concern, rather than a theoretical issue of fundamental importance.
Reference: [24] <author> J. I. Cirac and P. Zoller. </author> <title> Quantum computations with cold trapped ions. </title> <journal> Physical Review Letters, </journal> <volume> 74 </volume> <pages> 4091-4094, </pages> <year> 1995. </year>
Reference-contexts: For example, researchers in quantum optics study how to manipulate information encoded in the polarization staes of photons; "cavity QED" workers study the interactions between photons and electron spins on individual atoms [38, 106]; and other experimentalists work with vibrational states in assemblages of interacting atoms <ref> [24] </ref>. An intriguing recent development in implementation techniques has been the investigation of NMR "ensemble quantum computing," in which the nuclear spins of atoms in molecules in solution are made to interact using nuclear magnetic resonance techniques [citations].
Reference: [25] <author> D. Coppersmith. </author> <title> An approximate Fourier transform useful in quantum factoring. </title> <institution> Research Report RC 19642, IBM, </institution> <year> 1994. </year>
Reference-contexts: IMPORTANT OPEN PROBLEMS 97 free running time for various experimental setups has been estimated to be roughly on the order of the time to perform a single computational step, seemingly ruling out the possibility of doing interesting quantum computations. However, Coppersmith <ref> [25] </ref> has shown that simple imprecision does not cripple the quantum factoring algorithm, and several more recent papers [20, 21, 98] have addressed the more difficult issue of correcting errors due to decoherence of the quantum states.
Reference: [26] <author> Don Coppersmith and Edna Grossman. </author> <title> Generators for certain alternating groups with applications to cryptography. </title> <journal> SIAM J. Appl. Math., </journal> <volume> 29(4) </volume> <pages> 624-627, </pages> <month> December </month> <year> 1975. </year>
Reference-contexts: This will be the subject of x3.4. 3.3.6 Miscellaneous developments Coppersmith and Grossman (1975, <ref> [26] </ref>) proved a result in group theory which implies that reversible boolean circuits only 1 bit wider than their fixed-length input can compute arbitrary boolean functions of that input. (Thanks to Alain Tapp for bringing this paper to our attention.) Toffoli (1977, [100]) showed that reversible cellular automata can simulate irreversible
Reference: [27] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <institution> The MIT Electrical Engineering and Computer Science Series. MIT Press/McGraw Hill, </institution> <year> 1990. </year>
Reference-contexts: complexity theory, and extend them to capture some new, more general measures of computational complexity and cost-efficiency. 3.2.1 Asymptotic order-of-growth notation A very powerful tool for characterizing scaling relations between quantities is the asymptotic order-of-growth notation commonly used in computer science (cf. x2.1 of the Cormen et al. 1990 text <ref> [27] </ref> for an in-depth discussion). This notation allows succinctly expressing the proportionality relations between two functions in the asymptotic limit when values on which those functions depend become arbitrarily large.
Reference: [28] <author> Pierluigi Crescenzi and Christos H. Papadimitriou. </author> <title> Reversible simulation of space-bounded computation. </title> <journal> Theoretical Computer Science, </journal> <volume> 143 </volume> <pages> 159-165, </pages> <year> 1995. </year>
Reference-contexts: Description of pebbling algorithm. Analysis of space-time complexity. Optimality of the algorithm. Li and Vitanyi '96 [69] analyzed Bennett's techniques and characterized the benefits of partial reversibility. Crescenzi and Papadimitriou '95 <ref> [28] </ref> extended Bennett's space-efficient reversible technique to the simulation of nondeterministic Turing machines. **DRAFT** April 25, 1998 3.4. REVERSIBLE VS.
Reference: [29] <author> DARPA. </author> <title> Scalable computing systems. </title> <address> http://www.darpa.mil/ito/ research/scalable. </address>
Reference: [30] <author> David Deutsch. </author> <title> Quantum theory, the Church-Turing principle and the universal quantum computer. </title> <journal> Proceedings of the Royal Society of London Ser. A, </journal> <volume> A400:97-117, </volume> <year> 1985. </year>
Reference-contexts: a microscopic level; that is, the quantum state (amplitude vector) of an isolated system at any time determines the quantum state of the system at all past and future times, as we mentioned in x2.5. 4.2 Quantum complexity theory Ever since Deutsch introduced his quantum generalization of the Turing machine <ref> [30] </ref>, researchers have wondered whether this computational model has computational capabilities greater than those of classical Turing machines. In his original paper, Deutsch showed that quantum computers could exploit "quantum parallelism" to simultaneously compute function values for N inputs using only one mechanism.
Reference: [31] <author> David Deutsch and Richard Jozsa. </author> <title> Rapid solution of problems by quantum computation. </title> <journal> Proceedings of the Royal Society of London Ser. A, </journal> <volume> A439:553-558, </volume> <year> 1992. </year>
Reference-contexts: Such a development could lead to eventual practical applications, if and when such quantum computers become buildable. Shor's Factoring Algorithm. However, this idea remained pure speculation until the last several years, when a series of papers on the power of quantum computers <ref> [31, 15, 14, 13, 94] </ref> culminated in Peter Shor's 1994 proof [92, 93] that a (somewhat idealized) quantum computer could factor large integers in polynomial time. This was an astounding discovery, since mathematicians throughout history have searched for April 25, 1998 **DRAFT** 88 CHAPTER 4. <p> QUANTUM COMPUTATION and XORed them. In a later paper <ref> [31] </ref>, Deutsch did much better|together with Jozsa he showed that a certain property of functions could be determined with certainty exponentially faster by quantum programs than by classical ones, if the function is given as a black box as input to the program.
Reference: [32] <author> David P. DiVincenzo. </author> <title> Two-bit gates are universal for quantum computation. </title> <journal> Physical Review A, </journal> <volume> 51(2) </volume> <pages> 1015-1022, </pages> <month> February </month> <year> 1995. </year> <note> Also at Los Alamos Physics Preprint Archive, http://xxx.lanl.gov/abs/cond-mat/9407022. April 25, 1998 **DRAFT** 220 BIBLIOGRAPHY </note>
Reference-contexts: Other proposals for implementation technologies for quantum computing, from various communities, include Teich et al. '88 [99], Lloyd '93 & '94 [72, 73], DiVincenzo April 25, 1998 **DRAFT** 98 CHAPTER 4. QUANTUM COMPUTATION '95 a <ref> [32] </ref>, Sleator & Weinfurter '95 [95], Barenco et al. '95 b [4], and Chuang & Yamamoto '95 [23].
Reference: [33] <author> K. Eric Drexler. Nanosystems: </author> <title> Molecular Machinery, Manufacturing, and Computation. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1992. </year> <note> http://nano.xerox.com/nanotech/ nanosystems.html. </note>
Reference-contexts: Likharev showed in 1982 [70] that superconducting Josephson junction circuits could be used reversibly. Later reversible device proposals include various mechanical and electronic proposals by the molecular nanotechnologists Drexler and Merkle (Drexler 1992 <ref> [33] </ref>, ch. 12; Merkle 1993 [78, 79]; Merkle & Drexler 1996 [80]), and a single-electron system analyzed by Likharev and Korotkov (1996, [71]). So at present there is no shortage of reversible device ideas. Moreover, in the April 25, 1998 **DRAFT** 24 CHAPTER 1.
Reference: [34] <author> Albert Einstein. </author> <title> Relativity. </title> <publisher> Crown Publishers, Inc., </publisher> <address> New York, </address> <year> 1961. </year>
Reference-contexts: One caveat is that in normal materials travelling at near the speed of light, the relativistic length contraction of the material should increase its effective entropy density, according to 0 where fl = p (2.13) is the normal relativistic correction factor (cf. <ref> [34] </ref>). We imagine that a given chunk of material maintains the same entropy at high speed, but is compacted into a shorter space. This would seem to allow arbitrarily high fluxes to be attained.
Reference: [35] <author> Artur K. Ekert and Richard Jozsa. </author> <title> Shor's quantum algorithm for factorising numbers. </title> <note> To appear in Rev. Mod. Phys. Preprint at ftp://eve.physics.ox.- ac.uk/Archive/Numbered/EJ95/EJ95.ps. </note>
Reference-contexts: The details of exactly how the quantum Fourier transform works are beyond the scope of this short survey. For more detailed expositions of Shor's algorithm, see Ekert and Jozsa's description in <ref> [36, 35] </ref>, and Shor's original papers [92, 93]. April 25, 1998 **DRAFT** 94 CHAPTER 4. QUANTUM COMPUTATION a in Shor's algorithm, when factoring N = 33 with the choice of x = 5. The value of a is now peaked at 10 points spaced 25.6 units apart.
Reference: [36] <author> Artur K. Ekert and Richard Jozsa. </author> <title> Notes on Shor's efficient algorithm for factoring on a quantum computer. </title> <note> Preprint at http://feynman.stanford.- edu/qcomp/ekert-jozsa/index.html, </note> <year> 1994. </year>
Reference-contexts: The details of exactly how the quantum Fourier transform works are beyond the scope of this short survey. For more detailed expositions of Shor's algorithm, see Ekert and Jozsa's description in <ref> [36, 35] </ref>, and Shor's original papers [92, 93]. April 25, 1998 **DRAFT** 94 CHAPTER 4. QUANTUM COMPUTATION a in Shor's algorithm, when factoring N = 33 with the choice of x = 5. The value of a is now peaked at 10 points spaced 25.6 units apart.
Reference: [37] <author> Fran~cois Englert. </author> <title> On the black hole unitarity issue. </title> <address> Los Alamos e-print http: //xxx.lanl.gov/abs/hep-th/9705115, </address> <month> 15 May </month> <year> 1997. </year>
Reference-contexts: However, there is currently no accepted, complete theory of black hole physics from which we could draw unarguable theoretical conclusions, and there is no experimental evidence that supports information loss. The truth of the issue is still being actively debated <ref> [37, 77] </ref>. Moreover, it appears that some recent developments in string theory would allow reversibility to be maintained, if the theory is correct (Myers 1997, [82]).
Reference: [38] <author> T. Pellizzari et al. Decoherence, </author> <title> continuous observation and quantum computing: a cavity QED model. </title> <type> Preprint, </type> <month> June </month> <year> 1995. </year> <institution> University of Innsbruck. </institution>
Reference-contexts: For example, researchers in quantum optics study how to manipulate information encoded in the polarization staes of photons; "cavity QED" workers study the interactions between photons and electron spins on individual atoms <ref> [38, 106] </ref>; and other experimentalists work with vibrational states in assemblages of interacting atoms [24].
Reference: [39] <author> William Feller. </author> <title> An Introduction to Probability Theory and Its Applications. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1950. </year>
Reference-contexts: Feller 1950 <ref> [39] </ref>, ch.
Reference: [40] <author> Richard Feynman. </author> <title> Simulating physics with computers. </title> <journal> International Journal of Theoretical Physics, </journal> 21(6&7):467-488, 1982. 
Reference-contexts: Quantitative Church's Thesis. The "Quantitative Church's Thesis" [111, 110] claims that Turing machines are as efficient as any realistic computer, within a polynomial factor. However, Feynman <ref> [40] </ref> has pointed out that Turing machines seem to be unable to efficiently simulate quantum physics; that is, they seem to require an exponential slowdown to simulate it (although this has not been proven). <p> Secondly, if the answer to the question is negative, i.e. if BQP BPP, then this has important implications for physics, because it might mean that existing classical computers could therefore simulate arbitrary quantum systems with only a polynomial slowdown, which is not currently known to be possible <ref> [40] </ref>; current classical simulations of quantum systems all suffer from an exponential slowdown.
Reference: [41] <author> Richard Feynman. </author> <title> Quantum mechanical computers. </title> <journal> Optics News, </journal> <volume> 11, </volume> <booktitle> 1985. Also in Foundations of Physics, </booktitle> <volume> 16(6) </volume> <pages> 507-531, </pages> <year> 1986. </year>
Reference-contexts: The main lesson to be learned from this long list of proposals is that the details of the physical implementation of quantum computers are "just" an engineering concern, rather than a theoretical issue of fundamental importance. Researchers since Feynman <ref> [41] </ref> have noted that there seems to be nothing fundamental in quantum physics that precludes using it for computation, and indeed, the multiplicity of ideas listed above seems to bear that out.
Reference: [42] <author> Michael Frank. </author> <title> Time-symmetric control-flow instructions for less garbage in reversible programs. </title> <note> Online draft memo, February 1996. http://www.ai.- mit.edu/~mpf/rc/memos/M01_symmarch.html. </note>
Reference: [43] <author> Michael Frank and Scott Rixner. Tick: </author> <title> A simple reversible processor (6.371 project report). Online term paper, </title> <month> may </month> <year> 1996. </year> <note> http://www.ai.mit.edu/~mpf/ rc/tick-report.ps. </note>
Reference-contexts: Also the instruction set did not guarantee full reversibility independently of program correctness, which precluded some of the possible applications for reversibility, such as bi-directional debugging. In our work we therefore studied several improved variations of PISA (cf.[42, 44]), which were used in our prototype reversible processor <ref> [43] </ref> and in the compiler design effort. Yet another improved and simplified version of PISA is being implemented now by Vieri for his dissertation research. Since reversible architectures still only exist for academic research purposes, there has not yet been much need to standardize on a particular version.
Reference: [44] <author> Michael P. Frank. </author> <title> Modifications to PISA architecture to support guaranteed reveribility and other features. </title> <note> Online draft memo, </note> <month> feb </month> <year> 1997. </year> <note> http://www.ai.- mit.edu/~mpf/rc/memos/M07/M07_revarch.html. </note>
Reference: [45] <author> Michael P. Frank and M. Josephine Ammer. </author> <title> Separations of reversible and irreversible space-time complexity classes. </title> <note> Extended abstract, to be revised for submission to CCC-98. http://www.ai.mit.edu/~mpf/rc/memos/M06_oracle.- html, 1997. **DRAFT** April 25, 1998 BIBLIOGRAPHY 221 </note>
Reference-contexts: IRREVERSIBLE SPACE-TIME COMPLEXITY 67 Acknowledgement. Some of the ideas in the proof below originated with M. Josephine Ammer, who was an undergraduate research assistant in our group at the time this work was done. Ms. Ammer also co-authored the manuscript <ref> [45] </ref> from which this section is derived, which has not yet been published by itself, but which may be in the future. 3.4.1 General definitions Space-time complexity classes.
Reference: [46] <author> Michael P. Frank and Thomas F. Knight, Jr. </author> <title> Ultimate theoretical models of nanocomputers. </title> <booktitle> Presented at the Fifth Foresight Conference on Molecular Nan-otechnology, </booktitle> <address> Palo Alto, CA, </address> <month> November </month> <year> 1997. </year> <note> http://www.ai.mit.edu/~mpf/ Nano97/paper.html. Submitted to Nanotechnology. </note>
Reference-contexts: Some of the analyses and results in this section were first reported in our earlier publications <ref> [46, 47] </ref>. 6.2.1 Entropy cost Perhaps the simplest physical measure of cost, which also gets us away from the bias towards the abstract time and space cost-measures featured in traditional complexity theory, is the idea of the cost of a computation being proportional to just the amount of new entropy that
Reference: [47] <author> Michael P. Frank, Thomas F. Knight, Jr., and Norman H. Margolus. </author> <title> Reversibil-ity in optimally scalable computer architectures. </title> <editor> In C. S. Calude, J. Casti, and M. J. Dinneen, editors, </editor> <booktitle> Unconventional Models of Computation, </booktitle> <pages> pages 165-182. </pages> <publisher> Springer, </publisher> <year> 1998. </year> <note> http://www.ai.mit.edu/~mpf/rc/scaling_paper/ scaling.html. </note>
Reference-contexts: Some of the analyses and results in this section were first reported in our earlier publications <ref> [46, 47] </ref>. 6.2.1 Entropy cost Perhaps the simplest physical measure of cost, which also gets us away from the bias towards the abstract time and space cost-measures featured in traditional complexity theory, is the idea of the cost of a computation being proportional to just the amount of new entropy that
Reference: [48] <author> E. F. Fredkin and T. Toffoli. </author> <title> Design principles for achieving high-performance submicron digital technologies. DARPA Proposal, </title> <month> November </month> <year> 1978. </year>
Reference-contexts: Bennett's 1973 paper [8] suggested the possibility of an enzymatic reversible computer using biomolecules, and in later papers such as (1982, [9]) he described a clockwork mechanical Turing machine powered by Brownian motion. Meanwhile, Fredkin and Toffoli had proposed an electronic implementation (1978, <ref> [48] </ref>), and an idealized model based on the ballistic motion of rigid spheres (1982, [49]). Likharev showed in 1982 [70] that superconducting Josephson junction circuits could be used reversibly. <p> So at present there is no shortage of reversible device ideas. Moreover, in the April 25, 1998 **DRAFT** 24 CHAPTER 1. INTRODUCTION & BACKGROUND years since Fredkin & Toffoli's 1978 proposal <ref> [48] </ref> it has become quite feasiable and economical to build reversible devices using conventional VLSI electronic fabrication techniques (cf.
Reference: [49] <author> E. F. Fredkin and T. Toffoli. </author> <title> Conservative logic. </title> <journal> Int'l J. Theoretical Physics, </journal> 21(3/4):219-253, 1982. 
Reference-contexts: Bennett described his technique using a formal Turing machine model, but later other researchers showed that the same trick of recording a history could also be applied to permit other models such as cellular automata (Toffoli 1977 [100]) and logic circuits (Toffoli 1980 [101], Fredkin & Toffoli 1982 <ref> [49] </ref>) to operate reversibly as well. Indeed, Bennett's technique seems to apply generally to "reversiblize" any model of computation. 1.3.3 Physically reversible logic devices However, showing that logically irreversible operations can be avoided in useful computations is only part of the problem of demonstrating that reversible computing can save energy. <p> Meanwhile, Fredkin and Toffoli had proposed an electronic implementation (1978, [48]), and an idealized model based on the ballistic motion of rigid spheres (1982, <ref> [49] </ref>). Likharev showed in 1982 [70] that superconducting Josephson junction circuits could be used reversibly. <p> Toffoli and Fredkin also developed much reversible circuit theory (1980-1982, <ref> [101, 102, 49] </ref>). <p> DESIGN & PROGRAMMING 9.1.1 Previous reversible architectures The first reversible computer architectures that we know of were designed by Barton (1978, [5]) and Ressler (1981, [88]) as thesis projects. These designs were based on the conservative (reversible and 1-conserving) logic model developed by Fredkin and Toffoli (cf. <ref> [49] </ref>). Much later (1994), Hall [52] described a reversible instruction set architecture based on his "retractile cascade" reversible circuit style and the PDP-10 instruction set. 9.1.2 Pendulum architecture In 1995, Vieri [113] developed the first version of the Pendulum architecture.
Reference: [50] <author> Lov. K. Grover. </author> <title> A fast quantum mechanical algorithm for database search. </title> <booktitle> In Proc. , 28th Annual ACM Symposium on the Theory of Computing (STOC), </booktitle> <pages> pages 212-219, </pages> <month> May </month> <year> 1996. </year> <note> http://xxx.lanl.gov/abs/quant-ph/9605043. </note>
Reference-contexts: Currently, it seems unlikely that quantum computers could solve NP-complete problems, due to the fact that the only known quantum algorithms that dominate classical algorithms either involve unrealistic oracle-dependent promise problems [13, 94], or introduce only polynomial speedups <ref> [50, 51] </ref>, or only simulate quantum mechanics [17], or depend on the ability to reduce the problem to one involving periodicities for which the quantum Fourier transform is useful [93, 18]; one would not expect such periodicities a priori to be characteristic of all problems in NP.
Reference: [51] <author> Lov K. Grover. </author> <title> Quantum computers can search rapidly by using almost any transformation. </title> <address> Los Alamos e-Print Archive, </address> <month> December </month> <year> 1997. </year> <note> http://xxx.- lanl.gov/abs/quant-ph/9712011. </note>
Reference-contexts: Currently, it seems unlikely that quantum computers could solve NP-complete problems, due to the fact that the only known quantum algorithms that dominate classical algorithms either involve unrealistic oracle-dependent promise problems [13, 94], or introduce only polynomial speedups <ref> [50, 51] </ref>, or only simulate quantum mechanics [17], or depend on the ability to reduce the problem to one involving periodicities for which the quantum Fourier transform is useful [93, 18]; one would not expect such periodicities a priori to be characteristic of all problems in NP.
Reference: [52] <author> J. Storrs Hall. </author> <title> A reversible instruction set architecture and algorithms. </title> <booktitle> In Physics and Computation, </booktitle> <pages> pages 128-134, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: These designs were based on the conservative (reversible and 1-conserving) logic model developed by Fredkin and Toffoli (cf. [49]). Much later (1994), Hall <ref> [52] </ref> described a reversible instruction set architecture based on his "retractile cascade" reversible circuit style and the PDP-10 instruction set. 9.1.2 Pendulum architecture In 1995, Vieri [113] developed the first version of the Pendulum architecture. <p> For arbitrary lists the worst-case garbage is (log n!) since that is the number of bits required to specify the original permutation of the elements. Simple insertion sort performs fi (n 2 ) comparisons in the worst case. Hall <ref> [52] </ref> observed however that only fi (n log n) garbage bits need be generated to run this algorithm reversibly: n words each fi (log n) bits long telling how far each element was moved down the list before being inserted in the proper place.
Reference: [53] <author> David A. Hodges and Horace G. Jackson. </author> <title> Analysis and Design of Digital Integrated Circuits. </title> <publisher> McGraw-Hill, Inc., </publisher> <address> second edition, </address> <year> 1988. </year>
Reference-contexts: Taken from the frontispieces of <ref> [53, 87] </ref>. width which is assumed to be equal to the minimum length; we find values ranging from 312 aF in 1997 to only 43 aF in 2012. Estimated load capacitance.
Reference: [54] <author> Tad Hogg. </author> <title> Quantum computing and phase transitions in combinatorial search. </title> <journal> Los Alamos Physics Preprint Archive, </journal> <note> http://xxx.lanl.gov/abs/quant-ph/ 9508012, </note> <month> August </month> <year> 1995. </year>
Reference-contexts: Hogg <ref> [54] </ref> has investigated quantum algorithms that enhance the probability density found along solution paths in NP search problems, but not enough to allow measurements of the machine state for such problems to yield solutions in expected polynomial time.
Reference: [55] <author> John E. Hopcroft and Jeffrey D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley Series in Computer Science. Addison-Wesley, </publisher> <year> 1979. </year>
Reference-contexts: REVERSIBLE COMPUTING THEORY Notation Model name Example references PRF Primitive recursive functions Rogers 1987 [89], x1.2, pp. 5-9 RF Recursive functions [89], ch. 1 FA Finite automata ch. 2 of Hopcroft & Ullman 1979 <ref> [55] </ref> RFA Reversible finite automata Pin 1987 [84] TM Turing machines [55], ch. 7 TM Reversible Turing machines [64, 8, 63] NTM Nondeterministic Turing machines [55], x7.5 CA Cellular automata von Neumann 1966 [116], Toffoli & Margolus 1991 [104] RAM Random access machines PRAM Parallel random access machines BLC Boolean logic <p> REVERSIBLE COMPUTING THEORY Notation Model name Example references PRF Primitive recursive functions Rogers 1987 [89], x1.2, pp. 5-9 RF Recursive functions [89], ch. 1 FA Finite automata ch. 2 of Hopcroft & Ullman 1979 <ref> [55] </ref> RFA Reversible finite automata Pin 1987 [84] TM Turing machines [55], ch. 7 TM Reversible Turing machines [64, 8, 63] NTM Nondeterministic Turing machines [55], x7.5 CA Cellular automata von Neumann 1966 [116], Toffoli & Margolus 1991 [104] RAM Random access machines PRAM Parallel random access machines BLC Boolean logic circuit 3dM 3-d mesh Leighton 1992 [66], ch. 1 Table 3.1: <p> Primitive recursive functions Rogers 1987 [89], x1.2, pp. 5-9 RF Recursive functions [89], ch. 1 FA Finite automata ch. 2 of Hopcroft & Ullman 1979 <ref> [55] </ref> RFA Reversible finite automata Pin 1987 [84] TM Turing machines [55], ch. 7 TM Reversible Turing machines [64, 8, 63] NTM Nondeterministic Turing machines [55], x7.5 CA Cellular automata von Neumann 1966 [116], Toffoli & Margolus 1991 [104] RAM Random access machines PRAM Parallel random access machines BLC Boolean logic circuit 3dM 3-d mesh Leighton 1992 [66], ch. 1 Table 3.1: Some existing models of computation. model, since our interest is in comparing the relative
Reference: [56] <author> K. Huang. </author> <title> Statistical Mechanics. </title> <publisher> Wiley, </publisher> <year> 1963. </year>
Reference-contexts: INFORMATION DENSITY LIMITS 33 statistical mechanics. The question can be answered by a detailed quantum mechanical counting of distinct (mutually orthogonal) states. It can also be well approximated in the macroscopic limit <ref> [56, 118] </ref> by simply calculating the volume of phase space accessible to the system, in units where Planck's constant is 1. Let us look at some particular information density bounds in more detail. 2.2.1 Entropy bounds from black hole physics.
Reference: [57] <author> E. Joos and A. Qadir. </author> <title> A quantum statistical upper bound on entropy. </title> <address> Il Nuovo Cimento, 107B(5):563-572, </address> <year> 1992. </year> <month> April 25, </month> <note> 1998 **DRAFT** 222 BIBLIOGRAPHY </note>
Reference-contexts: Let us look at some particular information density bounds in more detail. 2.2.1 Entropy bounds from black hole physics. Some particular upper bounds on information content as a function of system size and energy are given by Bekenstein (1984, [7]) and by Joos and Qadir (1992, <ref> [57] </ref>). Bekenstein's bounds, which originally came out of his studies of the entropy of black holes (e.g., [6]), are fairly loose, in the sense that his bounds are much higher than the maximum information content seems to be for systems other than black holes. <p> Much tighter bounds can be given for the entropy of normal (non black-hole) systems, given additional assumptions about their composition. This is done in Bekenstein's paper [7], as well as in papers by Joos and Qadir <ref> [57] </ref> and Smith (1995 [97]) and the related literature.
Reference: [58] <author> Tom Knight. </author> <title> An architecture for mostly functional languages. </title> <editor> In Patrick Henry Winston and Sarah Alexandra Shellard, editors, </editor> <booktitle> Artificial Intelligence at MIT: Expanding Frontiers, </booktitle> <volume> volume 1, chapter 19, </volume> <pages> pages 500-519. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: ALTERNATIVE APPLICATIONS FOR REVERSIBILITY multiprocessing system. The individual CPUs might optimistically perform computations on data under the assumption that the data is valid (as in Knight's paper <ref> [58] </ref>), but when an inconsistency is detected, rather than restarting the processor's computation entirely, the processor might be reversibly rolled back to the point at which it read the bad data, and then proceed from there using the new, correct data. 10.5 Simulating Physics Apart from the performance benefits discussed in
Reference: [59] <author> Donald E. Knuth. </author> <booktitle> The Art of Computer Programming, volume 2: Seminumer-ical Algorithms. </booktitle> <publisher> Addison-Wesley, </publisher> <address> second edition, </address> <year> 1981. </year>
Reference-contexts: The Fourier transform is linear and invertible; it turns out that it is unitary as well, and a discrete Fourier transform on functions of n-bit inputs can be performed on a quantum in time polynomial in n using a recursive procedure related to the classical fast-Fourier-transform (FFT) algorithm <ref> [59] </ref>. Simon's ingenious use of the quantum Fourier-transform algorithm to reduce an exponentially-hard problem to polynomial time was the original inspiration for Shor's application of a more general version of the transform to a difficult and practical problem: factoring large integers.
Reference: [60] <author> R. Landauer. </author> <title> Is quantum mechanically coherent computation useful? In D. </title> <editor> H. Feng and B-L. Hu, editors, </editor> <booktitle> Proceedings of the Drexel-4 Symposium on Quantum Nonintegrability|Quantum Classical Correspondence. </booktitle> <publisher> International Press, </publisher> <year> 1995. </year>
Reference-contexts: strictly more powerful than deterministic ones. 4.4.3 Can errors caused by imprecision and decoherence be controlled sufficiently to allow arbitrarily complex quantum computations to take place with an arbitrarily small probability of failure? A number of papers have expressed pessimism regarding the question of error accumulation in quantum computers, e.g., <ref> [61, 60, 108, 109, 22, 83] </ref>. These papers show that in the absence of error correction, the probability of error increases exponentially with both the time and space complexity of the computation, and the expected error **DRAFT** April 25, 1998 4.4.
Reference: [61] <author> R. </author> <title> Landauer. </title> <journal> Is quantum mechanics useful? Philosophical Transactions of the Royal Society of London Ser. </journal> <note> A, 1995. In press. </note>
Reference-contexts: strictly more powerful than deterministic ones. 4.4.3 Can errors caused by imprecision and decoherence be controlled sufficiently to allow arbitrarily complex quantum computations to take place with an arbitrarily small probability of failure? A number of papers have expressed pessimism regarding the question of error accumulation in quantum computers, e.g., <ref> [61, 60, 108, 109, 22, 83] </ref>. These papers show that in the absence of error correction, the probability of error increases exponentially with both the time and space complexity of the computation, and the expected error **DRAFT** April 25, 1998 4.4.
Reference: [62] <author> Rolf Landauer. </author> <title> Irreversibility and heat generation in the computing process. </title> <journal> IBM J. Research and Development, </journal> <volume> 5 </volume> <pages> 183-191, </pages> <year> 1961. </year> <note> Reprinted in [65], </note> <editor> ch. </editor> <volume> 4, </volume> <pages> pp. 188-196. </pages>
Reference-contexts: Wigington discusses dissipationless logic circuits, but does not appear to explicitly address the issue of their reversibility. Wigington also cites similar work that apparently occurred around the same time in Japan. Rolf Landauer (1961, <ref> [62] </ref>, x4) was apparently the first person to explicitly state the argument that the irreversible erasure of a bit of computational information inevitably requires the generation of a corresponding amount of physical entropy (namely 1 bit = ln 2 "nats" = k B ln 2 9:57 fi 10 24 J=K). <p> Thus the number of possible states of a system is irreducible over time. A popular way of expressing this is with the slogan, "state space is incompressible." The incompressibility of state space has an important consequence for information erasure within a computer, first described explicitly by Landauer <ref> [62] </ref>. Whenever we attempt to irreversibly erase a piece of information from a computer, that information is not truly destroyed, but instead is simply transferred to another part of the system, typically to the uncontrolled thermal state of the computer and its environment. <p> Thus the free energy is decreased by k B T ln 2. [Improve this explanation with a discussion of multiple heat baths and the lowest available temperature.] This statement was first explicitly conjectured by Landauer <ref> [62] </ref>. <p> In both reversible and irreversible models we may, if we wish, permit configurations having 0 predecessors (initial states) and/or 0 successors (final states). obviously a machine's efficiency at a task is meaningless if the machine cannot even perform the task. 3.3.2.1 Unbounded-space reversible machines are Turing-universal In his 1961 paper <ref> [62] </ref>, Landauer had already pointed out that arbitrary irreversible computations could be embedded into reversible ones by simply saving a record of all the information that would otherwise be thrown away (cf. x3 of [62]). <p> machine cannot even perform the task. 3.3.2.1 Unbounded-space reversible machines are Turing-universal In his 1961 paper <ref> [62] </ref>, Landauer had already pointed out that arbitrary irreversible computations could be embedded into reversible ones by simply saving a record of all the information that would otherwise be thrown away (cf. x3 of [62]). This observation makes it obvious that reversible machines with unbounded memory can certainly compute all the Turing-computable functions. <p> Landauer's suggestion (cf. x3 of <ref> [62] </ref>) of embedding each irreversible operation into a reversible one makes it clear that the number of such operations in a reversible machine need not be larger than the number for an irreversible machine, as was demonstrated by many more explicit later embeddings e.g., [64, 8]. <p> REVIEW OF EXISTING REVERSIBLE COMPUTING THEORY 63 3.3.4 Entropic complexity The original point of reversibility was not to reduce time but to reduce energy dissipation, or in other words entropy production. Can this be done by reversible machines? In 1961 Landauer <ref> [62] </ref> argued that it could not, since if we cannot get rid of the "garbage" bits that are accumulated in memory, they just constitute another form on entropy, no better asymptotically than the kind produced if we just irreversibly dissipated those bits into physical entropy right away. 3.3.4.1 Lecerf reversal However, <p> As Landauer pointed out <ref> [62] </ref>, the simple strategy of saving all the garbage information appears to suffer from the drawback that the amount of garbage that must be kept around in digital form is as large as the amount of entropy that would otherwise have been generated.
Reference: [63] <author> Klaus-Jorn Lange, Pierre McKenzie, and Alain Tapp. </author> <title> Reversible space equals deterministic space. </title> <booktitle> In Proc. 12th Annual IEEE Conf. on Computational Complexity (CCC '97), </booktitle> <pages> pages 45-50, </pages> <month> June </month> <year> 1997. </year> <note> http://www.iro.umontreal.ca/ ~tappa/Publications/LMT'97_abstract.html. </note>
Reference-contexts: So in 1989 Bennett developed a more space-efficient version of his algorithm [11]. Unfortunately, it incurs an asymptotic slowdown factor that cannot be made arbitrarily small without making the space usage exponentially large (Levine and Sherman 1990 [68]). Similarly, in 1997, Lange, McKenzie, and Tapp <ref> [63] </ref> gave a general algorithm for reversible simulation of irreversible computations using no extra space, but with exponentially large run-times. <p> THEORY Notation Model name Example references PRF Primitive recursive functions Rogers 1987 [89], x1.2, pp. 5-9 RF Recursive functions [89], ch. 1 FA Finite automata ch. 2 of Hopcroft & Ullman 1979 [55] RFA Reversible finite automata Pin 1987 [84] TM Turing machines [55], ch. 7 TM Reversible Turing machines <ref> [64, 8, 63] </ref> NTM Nondeterministic Turing machines [55], x7.5 CA Cellular automata von Neumann 1966 [116], Toffoli & Margolus 1991 [104] RAM Random access machines PRAM Parallel random access machines BLC Boolean logic circuit 3dM 3-d mesh Leighton 1992 [66], ch. 1 Table 3.1: Some existing models of computation. model, since <p> Crescenzi and Papadimitriou '95 [28] extended Bennett's space-efficient reversible technique to the simulation of nondeterministic Turing machines. **DRAFT** April 25, 1998 3.4. REVERSIBLE VS. IRREVERSIBLE SPACE-TIME COMPLEXITY 65 3.3.5.3 Achieving linear space complexity Most recently, Lange, McKenzie, and Tapp <ref> [63] </ref> showed how to simulate Turing machines reversibly in linear space, but using worst-case exponential time. [Illustrate Euler tour on the computation tree.] The Lange-McKenzie-Tapp technique was defined explicitly only for Turing machines, but it is easily generalized to any model of computation. <p> From here, the result follows due to the existing relativizable simulations. When S ~ T, Bennett's simple reversible simulation technique [8] can be applied because it takes time O (T) and space O (T). Similarly, when T ~ 2 S the simulation of Lange et al. <ref> [63] </ref> can be used because it takes time O (2 S ) and space O (S). Both techniques can be easily seen to relativize to any self-reversible oracle O.
Reference: [64] <author> Y. Lecerf. </author> <title> Machines de Turing reversibles. Insolubilite recursive en n 2 N de l'equation u = n , ou est un t isomorphisme de codes [Reversible Turing machines. Recursive insolubility in n 2 N of the equation u = n , where is an "isomorphism of codes"]. </title> <journal> Comptes Rendus Hebdomadaires des Seances de L'academie des Sciences [Weekly Proceedings of the Academy of Science], </journal> <volume> 257 </volume> <pages> 2597-2600, </pages> <month> October 28, </month> <year> 1963. </year> <note> Unauthorized English translation at http:/ /www.ai.mit.edu/~mpf/rc/Lecerf/lecerf.html. </note>
Reference-contexts: THEORY Notation Model name Example references PRF Primitive recursive functions Rogers 1987 [89], x1.2, pp. 5-9 RF Recursive functions [89], ch. 1 FA Finite automata ch. 2 of Hopcroft & Ullman 1979 [55] RFA Reversible finite automata Pin 1987 [84] TM Turing machines [55], ch. 7 TM Reversible Turing machines <ref> [64, 8, 63] </ref> NTM Nondeterministic Turing machines [55], x7.5 CA Cellular automata von Neumann 1966 [116], Toffoli & Margolus 1991 [104] RAM Random access machines PRAM Parallel random access machines BLC Boolean logic circuit 3dM 3-d mesh Leighton 1992 [66], ch. 1 Table 3.1: Some existing models of computation. model, since <p> Landauer's suggestion (cf. x3 of [62]) of embedding each irreversible operation into a reversible one makes it clear that the number of such operations in a reversible machine need not be larger than the number for an irreversible machine, as was demonstrated by many more explicit later embeddings e.g., <ref> [64, 8] </ref>. So under the time complexity measure by itself, reversibility does not hurt. Can a reversible machine perform a task in fewer operations than an irreversible one? Obviously not, if we take reversible operations to just be a special case of irreversible operations. <p> could not, since if we cannot get rid of the "garbage" bits that are accumulated in memory, they just constitute another form on entropy, no better asymptotically than the kind produced if we just irreversibly dissipated those bits into physical entropy right away. 3.3.4.1 Lecerf reversal However, in 1963, Lecerf <ref> [64] </ref> formally described a construction in which an irreversible machine was embedded into a reversible one which first simulated the irreversible machine running forwards, then turned around and simulated the irreversible machine in reverse, uncomputing all of the history information and returning to a state similar to the starting state.
Reference: [65] <author> Harvey S. Leff and Andrew F. Rex, editors. Maxwell's demon: </author> <title> entropy, </title> <booktitle> information, computing. Princeton series in physics. </booktitle> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1990. </year> <note> May be ordered through http://www.ioppublishing.- com/Books/Catalogue/020/__26/0750300566. </note>
Reference-contexts: The study of thermodynamically and logically reversible computational processes has historically been motivated by concerns in fundamental physics. For example, the proper resolution of the famous "Maxwell's Demon" paradox of thermodynamics (see the papers in <ref> [65] </ref>) required understanding that the means of disposal of unwanted information can be important when considering the thermodynamics of a system. The first connection between computation and fundamental thermodynamics was apparently made by John von Neumann ([116], p. 66).
Reference: [66] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays Trees Hypercubes. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, Cali-fornia, </address> <year> 1992. </year> <note> **DRAFT** April 25, 1998 BIBLIOGRAPHY 223 </note>
Reference-contexts: [84] TM Turing machines [55], ch. 7 TM Reversible Turing machines [64, 8, 63] NTM Nondeterministic Turing machines [55], x7.5 CA Cellular automata von Neumann 1966 [116], Toffoli & Margolus 1991 [104] RAM Random access machines PRAM Parallel random access machines BLC Boolean logic circuit 3dM 3-d mesh Leighton 1992 <ref> [66] </ref>, ch. 1 Table 3.1: Some existing models of computation. model, since our interest is in comparing the relative efficiency of different models.
Reference: [67] <author> A. K. Lenstra and H. W. Lenstra, </author> <title> editors. The Development of the Number Field Sieve, </title> <booktitle> volume 1554 of Lecture Notes in Mathematics. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year> . 
Reference-contexts: This was an astounding discovery, since mathematicians throughout history have searched for April 25, 1998 **DRAFT** 88 CHAPTER 4. QUANTUM COMPUTATION an efficient way to factor numbers without success, since at least the time of Euclid. The best known classical algorithm <ref> [67] </ref> takes exponential time. 1 4.1 Some fundamental quantum concepts Hilbert spaces.
Reference: [68] <author> Robert Y. Levine and Alan T. Sherman. </author> <title> A note on Bennett's time-space tradeoff for reversible computation. </title> <journal> SIAM J. Computing, </journal> <volume> 19(4) </volume> <pages> 673-677, </pages> <year> 1990. </year>
Reference-contexts: So in 1989 Bennett developed a more space-efficient version of his algorithm [11]. Unfortunately, it incurs an asymptotic slowdown factor that cannot be made arbitrarily small without making the space usage exponentially large (Levine and Sherman 1990 <ref> [68] </ref>). Similarly, in 1997, Lange, McKenzie, and Tapp [63] gave a general algorithm for reversible simulation of irreversible computations using no extra space, but with exponentially large run-times. <p> The most space-efficient linear-time reversible simulation technique currently known was provided by Bennett ([11], p. 770), and analyzed by Levine and Sherman <ref> [68] </ref> to take space O (S (T =S) 1=(0:58 lg (T=S)) ). Bennett's simulation can be easily seen to work with all self-reversible oracles, so it gives a relativizable upper bound on space. <p> Given all these assumptions, just how general are the reversible scaling advantages? Do they cover very many practical applications in large problem classes, other than just physical simulations? The complete answer to this question is uncertain, but one observation is that Bennett's 1989 algorithm <ref> [11, 68] </ref> can be utilized to remove the requirement for the reversibility of the underlying task, while still permitting almost the same polynomial speedups and cost-efficiency benefits. (However, the assumptions regarding paralleliz-ability and communication requirements remain.) Explanation follows. 6.3.1 Speedups for irreversible computations on reversible machines Bennett's technique [11] allows one <p> memory cells ("space") and T primitive operations ("time") into a reversible algorithm that leaves behind no garbage information (other than input and output) and takes T 0 ~ T (T=S) " operations, and S 0 ~ S log (T=S) memory, for any " &gt; 0. (See Levine & Sherman 1990 <ref> [68] </ref> for the derivation.) A finite irreversible processing element running for N steps steps performs T ~ N steps operations, and S ~ 1 space. <p> Moreover, the reversible advantages can become arbitrarily asymptotically close to those we calculated in the previous section for the case of simulating 3-D reversible systems. However, as pointed out by Levine and Sherman <ref> [68] </ref>, one must be careful when using Bennett's algorithm not to take " too close to zero, because the constant factor in the memory requirement increases exponentially in 1=", specifically as "2 1=" .
Reference: [69] <author> Ming Li and Paul M. B. Vitanyi. </author> <title> Reversibility and adiabatic computation: trading time and space for energy. </title> <journal> Proc. Royal Society of London, Ser. A, </journal> <volume> 452 </volume> <pages> 1-21, </pages> <year> 1996. </year>
Reference-contexts: Description of pebbling algorithm. Analysis of space-time complexity. Optimality of the algorithm. Li and Vitanyi '96 <ref> [69] </ref> analyzed Bennett's techniques and characterized the benefits of partial reversibility. Crescenzi and Papadimitriou '95 [28] extended Bennett's space-efficient reversible technique to the simulation of nondeterministic Turing machines. **DRAFT** April 25, 1998 3.4. REVERSIBLE VS. <p> To our knowledge, no one has yet directly addressed the question of whether a single reversible simulation can run in linear time like Bennett's 1973 technique and in linear space like the new Lange et al. technique. Li and Vitanyi's analysis <ref> [69] </ref> of Bennett's 1989 algorithm [11] leads to our proof that any such simulation cannot relativize to oracles, or work in cases where the space bound is much less than the input length. <p> In the cases where the reversible machine takes too much space, we will prove this by equating the machine's operation with the "pebble game" for which Li and Vitanyi <ref> [69] </ref> have already proven lower bounds, and by showing that if the machine does not take too much space, then we can build a shorter description of the chain of nodes using the machine's small intermediate configurations, thus contradicting our choice of an incompressible chain. <p> To do this, we show how M i can be interpreted as following the rules of Bennett's reversible "pebble game," introduced in [11] and analyzed by Li and Vitanyi in <ref> [69] </ref>. Pebble game rules. The game is played on a linear list of nodes, which we will identify with query strings q 1 ; : : : ; q t . At any time during the game some set of nodes is pebbled. Initially, no nodes are pebbled. <p> This corresponds to the fact already established that M i must at some point ask query q t1 or the oracle can be constructed to foil it trivially. Li and Vitanyi's analysis of the pebble game <ref> [69] </ref> showed that no strategy can win the game for 2 k nodes or more without at some time having more than k nodes pebbled at once. <p> Then the incompressibility of x in s shows that the assumption that q j is initially pebbled is inconsistent. Thus M i exactly obeys all the rules of the Bennett pebble game. Now, Li and Vitanyi have shown <ref> [69] </ref> that any strategy for the pebble game that eventually pebbles a node at or beyond node 2 k must at some time have at least k + 1 nodes pebbled at once.
Reference: [70] <author> K. K. Likharev. </author> <title> Classical and quantum limitations on energy consumption in computation. </title> <journal> Int'l J. Theoretical Physics, </journal> 21(3/4):311-326, 1982. 
Reference-contexts: Meanwhile, Fredkin and Toffoli had proposed an electronic implementation (1978, [48]), and an idealized model based on the ballistic motion of rigid spheres (1982, [49]). Likharev showed in 1982 <ref> [70] </ref> that superconducting Josephson junction circuits could be used reversibly.
Reference: [71] <author> Konstantin K. Likharev and Alexander N. Korotkov. "Single-electron parame-tron": </author> <title> Reversible computation in a discrete-state system. </title> <journal> Science, </journal> <volume> 273 </volume> <pages> 763-765, </pages> <month> 9 August </month> <year> 1996. </year>
Reference-contexts: Later reversible device proposals include various mechanical and electronic proposals by the molecular nanotechnologists Drexler and Merkle (Drexler 1992 [33], ch. 12; Merkle 1993 [78, 79]; Merkle & Drexler 1996 [80]), and a single-electron system analyzed by Likharev and Korotkov (1996, <ref> [71] </ref>). So at present there is no shortage of reversible device ideas. Moreover, in the April 25, 1998 **DRAFT** 24 CHAPTER 1.
Reference: [72] <author> Seth Lloyd. </author> <title> A potentially realizable quantum computer. </title> <journal> Science, </journal> <volume> 261 </volume> <pages> 1569-1571, </pages> <month> 17 September </month> <year> 1993. </year> <note> Preprint at http://www-im.lcs.mit.edu:80/poc/ lloyd/lloyd.ps.Z. </note>
Reference-contexts: The NMR experiments have had the most success of any techniques to date; quantum logic operations involving 2 and 3 bits have been successfully demonstrated. Other proposals for implementation technologies for quantum computing, from various communities, include Teich et al. '88 [99], Lloyd '93 & '94 <ref> [72, 73] </ref>, DiVincenzo April 25, 1998 **DRAFT** 98 CHAPTER 4. QUANTUM COMPUTATION '95 a [32], Sleator & Weinfurter '95 [95], Barenco et al. '95 b [4], and Chuang & Yamamoto '95 [23].
Reference: [73] <author> Seth Lloyd. </author> <title> Envisioning a quantum supercomputer. </title> <note> Science, 263:695, 4 Febru-ary 1994. Technical comment on [72]. </note>
Reference-contexts: The NMR experiments have had the most success of any techniques to date; quantum logic operations involving 2 and 3 bits have been successfully demonstrated. Other proposals for implementation technologies for quantum computing, from various communities, include Teich et al. '88 [99], Lloyd '93 & '94 <ref> [72, 73] </ref>, DiVincenzo April 25, 1998 **DRAFT** 98 CHAPTER 4. QUANTUM COMPUTATION '95 a [32], Sleator & Weinfurter '95 [95], Barenco et al. '95 b [4], and Chuang & Yamamoto '95 [23].
Reference: [74] <author> N. H. Margolus. </author> <title> Physics and Computation. </title> <type> PhD thesis, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <year> 1988. </year>
Reference-contexts: But such asymmetry was impossible since the reversible algorithm was completely time-symmetric. So any artifacts that appeared could not grow unboundedly; they remained small relative to the desired wave data. The advantages of reversibility in physical simulations are discussed further by Margolus <ref> [74] </ref>. Note however that these advantages can be gained as long as the simulation is simply reversible at the relatively high level of its state-update rule.
Reference: [75] <author> Norman Margolus and Lev B. Levitin. </author> <title> The maximum speed of dynamical evolution. </title> <editor> In Toffoli et al. </editor> <volume> [103], </volume> <pages> pages 208-211. </pages> <note> Available through http://www.- interjournal.org. Revised version available at ftp://im.lcs.mit.edu/poc/ margolus/speed.of.dynamics.ps.Z. </note>
Reference-contexts: We describe limits on the speed at which information can travel, the density at which it can be stored, and the rate at which it can cross a boundary. We also describe fundamental limits from Margolus and Levitin (1996 <ref> [75] </ref>) on the rate at which a computer can change state. We discuss the meaning and the computational implications of physical reversibility and the second law of thermodynamics. Chapter 4 reviews the possibility of computation using large coherent superposi-tions of states (quantum computation). Quantum computers are inherently reversible. <p> By the amount of information in a system, we mean simply the logarithm of the number of states that the system could occupy, given some definition of what constitutes "the system." (See x2.5.2.) According to Margolus (1996, <ref> [75] </ref>), [The question of the number of states] is really a very old question: the correct counting of physical states is the problem that led to the introduction of Planck's constant into physics [85], and is the basis of all of quantum **DRAFT** April 25, 1998 2.2. <p> However, there are other limits on processing rates that apply even to perfectly reversible computers. In particular, there is the result of Margolus and Levitin (1996, <ref> [75] </ref>) that the fundamental laws of quantum mechanics imply that the maximum rate -? at which a system at an average energy E (above some minimum energy E 0 ) can transition between distinguishable (i.e., orthogonal) states is -? 4 (E E 0 )=h: (2.17) This bound is derived in a
Reference: [76] <author> Norman Margolus, Tommaso Toffoli, and Gerard Vichniac. </author> <title> Cellular-automata supercomputers for fluid dynamics modeling. </title> <journal> Physical Review Letters, </journal> <volume> 56(16) </volume> <pages> 1694-1696, </pages> <month> 21 April </month> <year> 1986. </year>
Reference-contexts: Fortunately, many real computations of interest are indeed of the sort that requires communication. Our canonical example will be the simulation of physical systems; in particular, reversible 3-dimensional lattice simulations (cf. <ref> [76, 104] </ref>). In such computations, each update of a computational cell depends on the results of the updates of its immediate neighbors from the previous step. 6.2.3.1 Time for 3-D local array simulations Irreversible time.
Reference: [77] <author> Vladimir S. Mashkevich. </author> <title> Conservative model of black hole (sic) and lifting of the information loss paradox. </title> <address> Los Alamos e-print http://xxx.lanl.gov/abs/ gr-qc/9707055, </address> <month> 27 July </month> <year> 1997. </year>
Reference-contexts: However, there is currently no accepted, complete theory of black hole physics from which we could draw unarguable theoretical conclusions, and there is no experimental evidence that supports information loss. The truth of the issue is still being actively debated <ref> [37, 77] </ref>. Moreover, it appears that some recent developments in string theory would allow reversibility to be maintained, if the theory is correct (Myers 1997, [82]).
Reference: [78] <author> Ralph C. Merkle. </author> <title> Reversible electronic logic using switches. </title> <journal> Nanotechnology, </journal> <volume> 4 </volume> <pages> 21-40, </pages> <year> 1993. </year> <month> April 25, </month> <note> 1998 **DRAFT** 224 BIBLIOGRAPHY </note>
Reference-contexts: Likharev showed in 1982 [70] that superconducting Josephson junction circuits could be used reversibly. Later reversible device proposals include various mechanical and electronic proposals by the molecular nanotechnologists Drexler and Merkle (Drexler 1992 [33], ch. 12; Merkle 1993 <ref> [78, 79] </ref>; Merkle & Drexler 1996 [80]), and a single-electron system analyzed by Likharev and Korotkov (1996, [71]). So at present there is no shortage of reversible device ideas. Moreover, in the April 25, 1998 **DRAFT** 24 CHAPTER 1.
Reference: [79] <author> Ralph C. Merkle. </author> <title> Two types of mechanical reversible logic. </title> <journal> Nanotechnology, </journal> <volume> 4 </volume> <pages> 114-131, </pages> <year> 1993. </year>
Reference-contexts: Likharev showed in 1982 [70] that superconducting Josephson junction circuits could be used reversibly. Later reversible device proposals include various mechanical and electronic proposals by the molecular nanotechnologists Drexler and Merkle (Drexler 1992 [33], ch. 12; Merkle 1993 <ref> [78, 79] </ref>; Merkle & Drexler 1996 [80]), and a single-electron system analyzed by Likharev and Korotkov (1996, [71]). So at present there is no shortage of reversible device ideas. Moreover, in the April 25, 1998 **DRAFT** 24 CHAPTER 1.
Reference: [80] <author> Ralph C. Merkle and K. Eric Drexler. </author> <title> Helical logic. </title> <journal> Nanotechnology, </journal> <volume> 7(4) </volume> <pages> 325-339, </pages> <year> 1996. </year> <note> Available through http://www.ioppublishing.com. </note>
Reference-contexts: Likharev showed in 1982 [70] that superconducting Josephson junction circuits could be used reversibly. Later reversible device proposals include various mechanical and electronic proposals by the molecular nanotechnologists Drexler and Merkle (Drexler 1992 [33], ch. 12; Merkle 1993 [78, 79]; Merkle & Drexler 1996 <ref> [80] </ref>), and a single-electron system analyzed by Likharev and Korotkov (1996, [71]). So at present there is no shortage of reversible device ideas. Moreover, in the April 25, 1998 **DRAFT** 24 CHAPTER 1.
Reference: [81] <author> Cesar Miquel, Juan Pablo Paz, and Roberto Perazzo. </author> <title> Factoring in a dissipative quantum computer. </title> <type> Los Alamos Physics Preprint Archive, </type> <address> http://xxx.lanl.- gov/abs/quant-ph/9601021, </address> <month> January </month> <year> 1996. </year> <title> Discusses a simulation of Shor's algorithm with error correction. </title>
Reference-contexts: More sophisticated techniques might take that into account. In summary, although these papers appear to be on the right track to a solution, a more complete theory of quantum error correction is still needed, and remains to be worked out. The preprint <ref> [81] </ref> reports results of some simulation experiments on error-correcting versions of Shor's algorithm in the presence of errors. 4.4.4 How do we build it, physically? Although the question of how to implement quantum computations physically is of course a question of utmost importance for the future of the field, to a
Reference: [82] <author> Robert C. Myers. </author> <title> Pure states don't wear black. </title> <address> Los Alamos e-print http:// xxx.lanl.gov/abs/gr-qc/9705065, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: The truth of the issue is still being actively debated [37, 77]. Moreover, it appears that some recent developments in string theory would allow reversibility to be maintained, if the theory is correct (Myers 1997, <ref> [82] </ref>). In any case, it seems to be the general consensus among physicists that reversibility is certainly maintained in at least all areas of mechanics that do not involve extreme situations such as black holes.
Reference: [83] <author> G. Massimo Palma, Kalle-Antti Suominen, and Artur K. Ekert. </author> <title> Quantum computers and dissipation. </title> <journal> Proceedings of the Royal Society of London Ser. </journal> <note> A, 1995. Submitted. </note>
Reference-contexts: strictly more powerful than deterministic ones. 4.4.3 Can errors caused by imprecision and decoherence be controlled sufficiently to allow arbitrarily complex quantum computations to take place with an arbitrarily small probability of failure? A number of papers have expressed pessimism regarding the question of error accumulation in quantum computers, e.g., <ref> [61, 60, 108, 109, 22, 83] </ref>. These papers show that in the absence of error correction, the probability of error increases exponentially with both the time and space complexity of the computation, and the expected error **DRAFT** April 25, 1998 4.4.
Reference: [84] <author> J. E. Pin. </author> <title> On the languages accepted by finite reversible automata. </title> <editor> In Thomas Ottman, editor, </editor> <booktitle> Automata, Languages and Programming, Proc. 14th Int'l Col-loq. (ICALP), volume 267 of Lecture Notes in Computer Science, </booktitle> <pages> pages 237-249. </pages> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: So our universe is not even as powerful as the reversible finite automata models studied by Pin <ref> [84] </ref>, which are in turn less powerful than normal irreversible finite automata, which are less powerful than Turing machines. So our universe would be quite weak, in formal computability terms. However, the constant factors would still be extremely large. <p> REVERSIBLE COMPUTING THEORY Notation Model name Example references PRF Primitive recursive functions Rogers 1987 [89], x1.2, pp. 5-9 RF Recursive functions [89], ch. 1 FA Finite automata ch. 2 of Hopcroft & Ullman 1979 [55] RFA Reversible finite automata Pin 1987 <ref> [84] </ref> TM Turing machines [55], ch. 7 TM Reversible Turing machines [64, 8, 63] NTM Nondeterministic Turing machines [55], x7.5 CA Cellular automata von Neumann 1966 [116], Toffoli & Margolus 1991 [104] RAM Random access machines PRAM Parallel random access machines BLC Boolean logic circuit 3dM 3-d mesh Leighton 1992 [66], <p> We should call this idea, of embedding an irreversible computation into a reversible one by saving a history of garbage, a "Landauer embedding," since Landauer seems to have been the first to suggest it. 3.3.2.2 Reversible finite automata are especially weak In contrast, in 1987 Pin <ref> [84] </ref> investigated reversible finite automata (machines with fixed memory reading an unbounded-length one-way stream of data), and found that they cannot even decide all the regular languages, which means that technically they are strictly less powerful than normal irreversible finite automata, which are in turn April 25, 1998 **DRAFT** 62 CHAPTER <p> Toffoli and Fredkin also developed much reversible circuit theory (1980-1982, [101, 102, 49]). As we already mentioned, Pin (1987 <ref> [84] </ref>) showed that reversible finite automata cannot decide all regular languages. 3.3.7 Reducing space complexity 3.4 Reversible vs. irreversible space-time complexity In this section we prove that reversible machine models require higher complexity on some problems than corresponding irreversible models, when a certain new reversible primitive operation is made available to <p> We write S ` (R (M) , M). This follows from the Lange-McKenzie-Tapp technique we discussed in x3.3.5.3, p. 65. However, if the model includes an input stream that can only flow one way and whose length is not included in S, then Pin's proof <ref> [84] </ref> applies, and then S ` (R (M) C M) because there are regular languages that cannot be recognized by constant-space reversible machines. Memory and number of ops.
Reference: [85] <author> M. </author> <title> Planck. The Theory of Heat Radiation. </title> <publisher> Dover, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: system could occupy, given some definition of what constitutes "the system." (See x2.5.2.) According to Margolus (1996, [75]), [The question of the number of states] is really a very old question: the correct counting of physical states is the problem that led to the introduction of Planck's constant into physics <ref> [85] </ref>, and is the basis of all of quantum **DRAFT** April 25, 1998 2.2. INFORMATION DENSITY LIMITS 33 statistical mechanics. The question can be answered by a detailed quantum mechanical counting of distinct (mutually orthogonal) states.
Reference: [86] <author> John Preskill. </author> <title> Do black holes destroy information? Los Alamos e-print http: //xxx.lanl.gov/abs/hep-th/9209058, </title> <month> September </month> <year> 1992. </year>
Reference-contexts: The apparent nondeterminism of quantum events can be interpreted as merely a subjective, emergent phenomenon that is predicted by an underlying deterministic theory. One possible exception to reversibility may be black holes, which, in some theoretical arguments, are found to destroy information (see Preskill 1992 <ref> [86] </ref> for a review of the situation). However, there is currently no accepted, complete theory of black hole physics from which we could draw unarguable theoretical conclusions, and there is no experimental evidence that supports information loss. The truth of the issue is still being actively debated [37, 77].
Reference: [87] <author> J.M. Rabaey. </author> <title> Digital Integrated Circuits: A Design Perspective. </title> <publisher> Prentice-Hall, </publisher> <year> 1996. </year> <note> http://infopad.EECS.Berkeley.EDU/~icdesign. </note>
Reference-contexts: Taken from the frontispieces of <ref> [53, 87] </ref>. width which is assumed to be equal to the minimum length; we find values ranging from 312 aF in 1997 to only 43 aF in 2012. Estimated load capacitance. <p> For our purposes, we will start with the model described in (Rabaey 1996, <ref> [87] </ref>, x2.3, p. 54, eqs. (2.57)-(2.59)), and originally proposed by Toh et al. (1988, [105]), which incorporates threshold voltage, mobility degradation, and velocity saturation effects. [...derive my version of the model... use it to determine device speed for my voltage choices...then go on to calculate thickness...] [everything below here at least
Reference: [88] <author> A. L. Ressler. </author> <title> The design of a conservative logic computer and a graphical editor simulator. </title> <type> Master's thesis, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <year> 1981. </year>
Reference-contexts: April 25, 1998 **DRAFT** 178 CHAPTER 7. DESIGN & PROGRAMMING 9.1.1 Previous reversible architectures The first reversible computer architectures that we know of were designed by Barton (1978, [5]) and Ressler (1981, <ref> [88] </ref>) as thesis projects. These designs were based on the conservative (reversible and 1-conserving) logic model developed by Fredkin and Toffoli (cf. [49]).
Reference: [89] <author> Hartley Rogers, Jr. </author> <title> Theory of Recursive Functions and Effective Computability. </title> <publisher> The MIT Press, first mit press paperback edition edition, </publisher> <year> 1987. </year> <note> Original edition published by McGraw-Hill Book Company, </note> <year> 1967. </year>
Reference-contexts: In this thesis we do not wish to pick a particular April 25, 1998 **DRAFT** 52 CHAPTER 3. REVERSIBLE COMPUTING THEORY Notation Model name Example references PRF Primitive recursive functions Rogers 1987 <ref> [89] </ref>, x1.2, pp. 5-9 RF Recursive functions [89], ch. 1 FA Finite automata ch. 2 of Hopcroft & Ullman 1979 [55] RFA Reversible finite automata Pin 1987 [84] TM Turing machines [55], ch. 7 TM Reversible Turing machines [64, 8, 63] NTM Nondeterministic Turing machines [55], x7.5 CA Cellular automata von <p> In this thesis we do not wish to pick a particular April 25, 1998 **DRAFT** 52 CHAPTER 3. REVERSIBLE COMPUTING THEORY Notation Model name Example references PRF Primitive recursive functions Rogers 1987 <ref> [89] </ref>, x1.2, pp. 5-9 RF Recursive functions [89], ch. 1 FA Finite automata ch. 2 of Hopcroft & Ullman 1979 [55] RFA Reversible finite automata Pin 1987 [84] TM Turing machines [55], ch. 7 TM Reversible Turing machines [64, 8, 63] NTM Nondeterministic Turing machines [55], x7.5 CA Cellular automata von Neumann 1966 [116], Toffoli & Margolus 1991 <p> This question was the subject of much early research on computation, but eventually it was realized that a large variety of physically reasonable models of computation can all compute exactly the same set of functions, the recursive or computable functions (cf. <ref> [89] </ref>), and so the issue became less interesting. <p> Now we survey more closely the range of existing models. 5.2 Existing models of computation Computer science has historically used many different abstract models of what constitutes a computer. Probably the earliest computing model explored by matematicians was the concept of a recursive function (cf. the text <ref> [89] </ref>). Around the same time, Turing proposed his tape machine model. Shortly thereafter, von Neumann explored the power of cellular automata (cf. [116]).
Reference: [90] <institution> Semiconductor Industry Association. The National Technology Roadmap for Semiconductors: Technology Needs. SEMATECH, Inc., </institution> <note> 1997 edition, 1997. http://notes.sematech.org/1997pub.htm. </note>
Reference-contexts: ps 16-17 12-13 10-12 9-10 7 4-5 3-4 V T 3 varia. g , mV 60 50 45 40 40 40 40 Src./drn. junction 50-100 36-72 30-60 26-52 20-40 15-30 10-20 depth, g nm Table 7.1: Selected numbers from the 1997 edition of the Semiconductor Industry Association's national semiconductor roadmap <ref> [90] </ref>. These numbers are used for the calculations in tables 7.6 and : : : . a Logic transistor density in a packed, high-volume, cost-performance microprocessor, including on-chip SRAM. From [90], p. 14. b Size for a processor, year 1, before subsequent shrinks; [90] p. 15. c On-chip local clock frequency <p> 10-20 depth, g nm Table 7.1: Selected numbers from the 1997 edition of the Semiconductor Industry Association's national semiconductor roadmap <ref> [90] </ref>. These numbers are used for the calculations in tables 7.6 and : : : . a Logic transistor density in a packed, high-volume, cost-performance microprocessor, including on-chip SRAM. From [90], p. 14. b Size for a processor, year 1, before subsequent shrinks; [90] p. 15. c On-chip local clock frequency for high-performance chips, [90], p. 16. d Minimum logic power supply voltage V dd , [90], p. 17. e Maximum power consumption for a high-performance processor with heat sink, [90] <p> the Semiconductor Industry Association's national semiconductor roadmap <ref> [90] </ref>. These numbers are used for the calculations in tables 7.6 and : : : . a Logic transistor density in a packed, high-volume, cost-performance microprocessor, including on-chip SRAM. From [90], p. 14. b Size for a processor, year 1, before subsequent shrinks; [90] p. 15. c On-chip local clock frequency for high-performance chips, [90], p. 16. d Minimum logic power supply voltage V dd , [90], p. 17. e Maximum power consumption for a high-performance processor with heat sink, [90] p. 17. f Minimum feature sizes, [90], pp. 14, 85. g The last <p> From <ref> [90] </ref>, p. 14. b Size for a processor, year 1, before subsequent shrinks; [90] p. 15. c On-chip local clock frequency for high-performance chips, [90], p. 16. d Minimum logic power supply voltage V dd , [90], p. 17. e Maximum power consumption for a high-performance processor with heat sink, [90] p. 17. f Minimum feature sizes, [90], pp. 14, 85. g The last four lines in the table are all from [90], p. 46. <p> From <ref> [90] </ref>, p. 14. b Size for a processor, year 1, before subsequent shrinks; [90] p. 15. c On-chip local clock frequency for high-performance chips, [90], p. 16. d Minimum logic power supply voltage V dd , [90], p. 17. e Maximum power consumption for a high-performance processor with heat sink, [90] p. 17. f Minimum feature sizes, [90], pp. 14, 85. g The last four lines in the table are all from [90], p. 46. **DRAFT** April 25, 1998 7.2.1.2. <p> <ref> [90] </ref>, p. 14. b Size for a processor, year 1, before subsequent shrinks; [90] p. 15. c On-chip local clock frequency for high-performance chips, [90], p. 16. d Minimum logic power supply voltage V dd , [90], p. 17. e Maximum power consumption for a high-performance processor with heat sink, [90] p. 17. f Minimum feature sizes, [90], pp. 14, 85. g The last four lines in the table are all from [90], p. 46. **DRAFT** April 25, 1998 7.2.1.2. <p> processor, year 1, before subsequent shrinks; <ref> [90] </ref> p. 15. c On-chip local clock frequency for high-performance chips, [90], p. 16. d Minimum logic power supply voltage V dd , [90], p. 17. e Maximum power consumption for a high-performance processor with heat sink, [90] p. 17. f Minimum feature sizes, [90], pp. 14, 85. g The last four lines in the table are all from [90], p. 46. **DRAFT** April 25, 1998 7.2.1.2. <p> high-performance chips, <ref> [90] </ref>, p. 16. d Minimum logic power supply voltage V dd , [90], p. 17. e Maximum power consumption for a high-performance processor with heat sink, [90] p. 17. f Minimum feature sizes, [90], pp. 14, 85. g The last four lines in the table are all from [90], p. 46. **DRAFT** April 25, 1998 7.2.1.2. THE SIA SEMICONDUCTOR ROADMAP 151 Year of first product shipment 1997 1999 2001 2003 2006 2009 2012 Capacitance calculations: Gate oxide thickness ~ T ox , nm 4.5 3.5 2.5 2.5 1.75 1.5 1 Gate areal capac.
Reference: [91] <author> Peter W. Shor. </author> <title> Algorithms for quantum computation: Discrete log and factoring. </title> <booktitle> In Foundations of Computer Science, Proc. 35th Ann. Symp., </booktitle> <pages> pages 124-134. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year> <note> **DRAFT** April 25, 1998 BIBLIOGRAPHY 225 </note>
Reference-contexts: Of course, ultimately we would like to also know whether quantum models of computation are really physically implementable as well, and whether they are truly more powerful than any physically-implementable classical model as results such as Shor's <ref> [91] </ref> seem to indicate. But we are not specialists in quantum systems, and so for now we leave those deep and important questions to others. 3.5 Summary of reversible complexity results for traditional models Let us summarize the above results on complexity in reversible machines.
Reference: [92] <author> Peter W. Shor. </author> <title> Algorithms for quantum computation: Discrete log and factoring. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 124-134. </pages> <publisher> Institute of Electrical and Electronic Engineers Computer Society Press, </publisher> <month> November </month> <year> 1994. </year> <note> ftp://netlib.att.com/netlib/att/ math/shor/quantum.algorithms.ps.Z. </note>
Reference-contexts: Shor's Factoring Algorithm. However, this idea remained pure speculation until the last several years, when a series of papers on the power of quantum computers [31, 15, 14, 13, 94] culminated in Peter Shor's 1994 proof <ref> [92, 93] </ref> that a (somewhat idealized) quantum computer could factor large integers in polynomial time. This was an astounding discovery, since mathematicians throughout history have searched for April 25, 1998 **DRAFT** 88 CHAPTER 4. <p> The details of exactly how the quantum Fourier transform works are beyond the scope of this short survey. For more detailed expositions of Shor's algorithm, see Ekert and Jozsa's description in [36, 35], and Shor's original papers <ref> [92, 93] </ref>. April 25, 1998 **DRAFT** 94 CHAPTER 4. QUANTUM COMPUTATION a in Shor's algorithm, when factoring N = 33 with the choice of x = 5. The value of a is now peaked at 10 points spaced 25.6 units apart.
Reference: [93] <author> Peter W. Shor. </author> <title> Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer. </title> <journal> Los Alamos Physics Preprint Archive, </journal> <note> http://xxx.lanl.gov/abs/quant-ph/9508027, August 1995. Expanded version of [92]. </note>
Reference-contexts: Shor's Factoring Algorithm. However, this idea remained pure speculation until the last several years, when a series of papers on the power of quantum computers [31, 15, 14, 13, 94] culminated in Peter Shor's 1994 proof <ref> [92, 93] </ref> that a (somewhat idealized) quantum computer could factor large integers in polynomial time. This was an astounding discovery, since mathematicians throughout history have searched for April 25, 1998 **DRAFT** 88 CHAPTER 4. <p> The details of exactly how the quantum Fourier transform works are beyond the scope of this short survey. For more detailed expositions of Shor's algorithm, see Ekert and Jozsa's description in [36, 35], and Shor's original papers <ref> [92, 93] </ref>. April 25, 1998 **DRAFT** 94 CHAPTER 4. QUANTUM COMPUTATION a in Shor's algorithm, when factoring N = 33 with the choice of x = 5. The value of a is now peaked at 10 points spaced 25.6 units apart. <p> only known quantum algorithms that dominate classical algorithms either involve unrealistic oracle-dependent promise problems [13, 94], or introduce only polynomial speedups [50, 51], or only simulate quantum mechanics [17], or depend on the ability to reduce the problem to one involving periodicities for which the quantum Fourier transform is useful <ref> [93, 18] </ref>; one would not expect such periodicities a priori to be characteristic of all problems in NP. However, it is conceivable someone may yet discover a clever quantum algorithm for general simulation of nondeterministic Turing machines.
Reference: [94] <editor> David R. Simon. </editor> <booktitle> On the power of quantum computation. In Proceedings of the 35th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 116-123, </pages> <address> Los Alamitos, CA, </address> <year> 1994. </year> <institution> Institute of Electrical and Electronic Engineers Computer Society Press. </institution> <note> Preprint at http://vesta.physics.ucla.edu/cgi-bin/uncompress ps cgi?simon94.ps. Improves on Bernstein & Vazirani '93 [13]. Shor '94 [92] was inspired by this. </note>
Reference-contexts: Such a development could lead to eventual practical applications, if and when such quantum computers become buildable. Shor's Factoring Algorithm. However, this idea remained pure speculation until the last several years, when a series of papers on the power of quantum computers <ref> [31, 15, 14, 13, 94] </ref> culminated in Peter Shor's 1994 proof [92, 93] that a (somewhat idealized) quantum computer could factor large integers in polynomial time. This was an astounding discovery, since mathematicians throughout history have searched for April 25, 1998 **DRAFT** 88 CHAPTER 4. <p> However, none of the oracle problems addressed seemed particularly evocative of real problems until Simon's <ref> [94] </ref> paper, which showed that the following problem was in ZQP (zero-error quantum polynomial-time): We are given a function f , and told that either f is 1-to-1, or else it is 1-to-2 and there is some bit-mask s such that f (x) = f (s x) (where is bitwise exclusive-or) <p> Currently, it seems unlikely that quantum computers could solve NP-complete problems, due to the fact that the only known quantum algorithms that dominate classical algorithms either involve unrealistic oracle-dependent promise problems <ref> [13, 94] </ref>, or introduce only polynomial speedups [50, 51], or only simulate quantum mechanics [17], or depend on the ability to reduce the problem to one involving periodicities for which the quantum Fourier transform is useful [93, 18]; one would not expect such periodicities a priori to be characteristic of all
Reference: [95] <author> Tycho Sleator and Harald Weinfurter. </author> <title> Realizable universal quantum logic gates. </title> <journal> Physical Review Letters, </journal> <volume> 74 </volume> <pages> 4087-4090, </pages> <year> 1995. </year> <title> Proposes a universal gate and a Cavity QED implementation. </title>
Reference-contexts: Other proposals for implementation technologies for quantum computing, from various communities, include Teich et al. '88 [99], Lloyd '93 & '94 [72, 73], DiVincenzo April 25, 1998 **DRAFT** 98 CHAPTER 4. QUANTUM COMPUTATION '95 a [32], Sleator & Weinfurter '95 <ref> [95] </ref>, Barenco et al. '95 b [4], and Chuang & Yamamoto '95 [23]. The main lesson to be learned from this long list of proposals is that the details of the physical implementation of quantum computers are "just" an engineering concern, rather than a theoretical issue of fundamental importance.
Reference: [96] <author> Warren D. Smith. </author> <title> Church's thesis meets the N-body problem. </title> <type> Technical Report TM 93-105-3-0058-6, </type> <institution> NECI, </institution> <month> September </month> <year> 1993. </year> <note> http://www.neci.nj.nec.com/ homepages/wds/church.ps. </note>
Reference-contexts: However, our universe is not Newtonian, and anyway infinite precision in initial configurations is not a reasonable assumption. For some discussions of the power of analog computation models, see <ref> [112, 96] </ref>. Unlimited computer size. Finally, some parallel computing models presume that a parallel machine may have an arbitrarily large number of processing elements.
Reference: [97] <author> Warren D. Smith. </author> <title> Fundamental physical limits on computation. </title> <type> Technical report, </type> <institution> NECI, </institution> <month> May </month> <year> 1995. </year> <note> http://www.neci.nj.nec.com/homepages/wds/ fundphys.ps. </note>
Reference-contexts: The optimal scaling of computation within physically realistic constraints is an issue that has been studied previously (cf. Vitanyi 1988 [114], Bilardi & Preparata 1993 [16], Smith 1995 <ref> [97] </ref>), but never before with particular attention to how the reversibility of physics allows reversible computation to improve physical scaling behavior. <p> Macroscopic black holes have intrinsic temperatures near absolute zero, and in contrast to most systems, they get cooler as you dump more energy and entropy into them! (Cf. eq. 26 in Smith's paper <ref> [97] </ref>, and his references to Hawking, his source.) So a black hole would be a sort of natural heat sink, cooler even than the cosmic microwave background which is at 3 K. <p> Much tighter bounds can be given for the entropy of normal (non black-hole) systems, given additional assumptions about their composition. This is done in Bekenstein's paper [7], as well as in papers by Joos and Qadir [57] and Smith (1995 <ref> [97] </ref>) and the related literature. <p> At a mass density of 10 g=cm 3 (about that of lead), eq. 2.4 gives only 36 kilobytes per cubic Angstrom. However, achieving even this much more reasonable entropy density using photons requires extremely high temperatures. The temperature of a photon gas of energy density E is (solving Smith's <ref> [97] </ref> eq. 13 for T ): T = 4 E c (2.10) where SB is the Stefan-Boltzmann constant, SB = 60 B Achieving a mass density of 10g=cm 3 (that's one heavy field of light!) thus requires a temperature of roughly 10 9 Kelvins. <p> So the maximum entropy flux we derive from our entropy density bounds is not exceeded in materials travelling at relativistic speeds, if the energy invested in accelerating the material to that speed is taken into account. Smith 1995 <ref> [97] </ref>, p. 6, eq. 7 gives an explicit formula for the maximum entropy flux F S using light, given an energy flux F E : F S 3 1=4 3=4 This is the formula for the entropy flux emitted by a blackbody that is at the appropriate temperature to emit energy
Reference: [98] <author> Andrew Steane. </author> <title> Multiple particle interference and quantum error correction. </title> <journal> Proceedings of the Royal Society of London Ser. </journal> <note> A, 1996. (Submitted.) Preprint available at Los Alamos Physics Preprint Archive, </note> <editor> http://xxx.lanl.gov/abs/ quant-ph/9601029. </editor> <title> Introduces a parity encoding for quantum error correction. </title>
Reference-contexts: However, Coppersmith [25] has shown that simple imprecision does not cripple the quantum factoring algorithm, and several more recent papers <ref> [20, 21, 98] </ref> have addressed the more difficult issue of correcting errors due to decoherence of the quantum states.
Reference: [99] <author> W. G. Teich, K. Obermayer, and G. Mahler. </author> <title> Structural basis of multistation-ary quantum systems ii: Effective few-particle dynamics. </title> <journal> Physical Review B, </journal> <volume> 37 </volume> <pages> 8111-8121, </pages> <year> 1988. </year>
Reference-contexts: The NMR experiments have had the most success of any techniques to date; quantum logic operations involving 2 and 3 bits have been successfully demonstrated. Other proposals for implementation technologies for quantum computing, from various communities, include Teich et al. '88 <ref> [99] </ref>, Lloyd '93 & '94 [72, 73], DiVincenzo April 25, 1998 **DRAFT** 98 CHAPTER 4. QUANTUM COMPUTATION '95 a [32], Sleator & Weinfurter '95 [95], Barenco et al. '95 b [4], and Chuang & Yamamoto '95 [23].
Reference: [100] <author> Tommaso Toffoli. </author> <title> Computation and construction universality of reversible cellular automata. </title> <journal> J. Computer and System Sciences, </journal> <volume> 15 </volume> <pages> 213-231, </pages> <year> 1977. </year> <month> April 25, </month> <note> 1998 **DRAFT** 226 BIBLIOGRAPHY </note>
Reference-contexts: Bennett described his technique using a formal Turing machine model, but later other researchers showed that the same trick of recording a history could also be applied to permit other models such as cellular automata (Toffoli 1977 <ref> [100] </ref>) and logic circuits (Toffoli 1980 [101], Fredkin & Toffoli 1982 [49]) to operate reversibly as well. <p> For example, <ref> [100] </ref> uses essentially the same trick to create a time-efficient simulation of irreversible cellular automata on reversible ones, by using an extra dimension in the cell array to serve as a garbage stack for each cell in the original machine. **DRAFT** April 25, 1998 3.3. <p> x3.4. 3.3.6 Miscellaneous developments Coppersmith and Grossman (1975, [26]) proved a result in group theory which implies that reversible boolean circuits only 1 bit wider than their fixed-length input can compute arbitrary boolean functions of that input. (Thanks to Alain Tapp for bringing this paper to our attention.) Toffoli (1977, <ref> [100] </ref>) showed that reversible cellular automata can simulate irreversible ones in linear time using an extra spatial dimension. Toffoli and Fredkin also developed much reversible circuit theory (1980-1982, [101, 102, 49]).
Reference: [101] <author> Tommaso Toffoli. </author> <title> Reversible computing. </title> <type> Technical memo MIT/LCS/TM-151, </type> <institution> MIT Lab for Computer Science, </institution> <month> February </month> <year> 1980. </year> <note> Out of print; available from NTIS. Abridged version available as [102]. </note>
Reference-contexts: The idea of dissipationless computing using reversibility may also have originated with von Neumann. Toffoli (1980, <ref> [101] </ref>) reports that The idea that universal computing capabilities could be obtained from reversible, dissipationless (and, of course, nonlinear) physical circuits apparently first occurred to von Neumann, as reported in a posthumous paper (Wigington 1961 [119]). <p> Bennett described his technique using a formal Turing machine model, but later other researchers showed that the same trick of recording a history could also be applied to permit other models such as cellular automata (Toffoli 1977 [100]) and logic circuits (Toffoli 1980 <ref> [101] </ref>, Fredkin & Toffoli 1982 [49]) to operate reversibly as well. <p> Toffoli and Fredkin also developed much reversible circuit theory (1980-1982, <ref> [101, 102, 49] </ref>).
Reference: [102] <author> Tommaso Toffoli. </author> <title> Reversible computing. </title> <editor> In J. W. de Bakker and J. van Leeuwen, editors, </editor> <booktitle> Automata, Languages and Programming (Seventh Colloquium, </booktitle> <address> Noordwijkerhout, the Netherlands, </address> <month> July 14-18, </month> <year> 1980), </year> <booktitle> volume 85 of Lecture Notes in Computer Science, </booktitle> <pages> pages 632-644. </pages> <publisher> Springer-Verlag, </publisher> <year> 1980. </year> <note> Abridged version of [101]. </note>
Reference-contexts: Toffoli and Fredkin also developed much reversible circuit theory (1980-1982, <ref> [101, 102, 49] </ref>).
Reference: [103] <editor> Tommaso Toffoli, Michael Biafore, and Jo~ao Le~ao, editors. </editor> <booktitle> PhysComp96 (Proceedings of the Fourth Workshop of Physics and Computation, </booktitle> <address> Boston University, </address> <month> 22-24 November </month> <year> 1996). </year> <institution> New England Complex Systems Institute, </institution> <year> 1996. </year> <title> Copies may be ordered from PhysComp96, </title> <address> 44 Cummington St., Boston MA 02215 (PhysComp96@pm.bu.edu). </address> <note> Individual papers are available through http: //www.interjournal.org. </note>
Reference: [104] <author> Tommaso Toffoli and Norman Margolus. </author> <title> Cellular Automata Machines: A New Environment for Modeling. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: ch. 1 FA Finite automata ch. 2 of Hopcroft & Ullman 1979 [55] RFA Reversible finite automata Pin 1987 [84] TM Turing machines [55], ch. 7 TM Reversible Turing machines [64, 8, 63] NTM Nondeterministic Turing machines [55], x7.5 CA Cellular automata von Neumann 1966 [116], Toffoli & Margolus 1991 <ref> [104] </ref> RAM Random access machines PRAM Parallel random access machines BLC Boolean logic circuit 3dM 3-d mesh Leighton 1992 [66], ch. 1 Table 3.1: Some existing models of computation. model, since our interest is in comparing the relative efficiency of different models. <p> Fortunately, many real computations of interest are indeed of the sort that requires communication. Our canonical example will be the simulation of physical systems; in particular, reversible 3-dimensional lattice simulations (cf. <ref> [76, 104] </ref>). In such computations, each update of a computational cell depends on the results of the updates of its immediate neighbors from the previous step. 6.2.3.1 Time for 3-D local array simulations Irreversible time.
Reference: [105] <author> Kai-Yap Toh, Ping-Keung Ko, and Robert G. Meyer. </author> <title> An engineering model for short-channel mos devices. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 23(4) </volume> <pages> 950-958, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: For our purposes, we will start with the model described in (Rabaey 1996, [87], x2.3, p. 54, eqs. (2.57)-(2.59)), and originally proposed by Toh et al. (1988, <ref> [105] </ref>), which incorporates threshold voltage, mobility degradation, and velocity saturation effects. [...derive my version of the model... use it to determine device speed for my voltage choices...then go on to calculate thickness...] [everything below here at least needs work] 7.2.1.5 Wafer surface area (mass) times time [This section needs revamping given
Reference: [106] <author> Q. A. Turchette, C. J. Hood, W. Lange, H. Mabuchi, and H. Jeffrey Kim-ble. </author> <title> Measurement of conditional phase shifts for quantum logic. </title> <note> Submitted to Physical Review Letters. Abstract at http://www.cco.caltech.edu/~hood/ QO/Abstracts/Turc95b.html. </note>
Reference-contexts: For example, researchers in quantum optics study how to manipulate information encoded in the polarization staes of photons; "cavity QED" workers study the interactions between photons and electron spins on individual atoms <ref> [38, 106] </ref>; and other experimentalists work with vibrational states in assemblages of interacting atoms [24].
Reference: [107] <author> David Ungar, Henry Lieberman, and Christopher Fry. </author> <title> Debugging and the experience of immediacy. </title> <journal> Communications of the ACM, </journal> <volume> 40(4) </volume> <pages> 38-43, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: However, reversible computing is not strictly necessary for implementing a bi-directional debugger. For example, Boothe (1998, [19]) describes algorithms that can be used to implement bi-directional debugging environments for normal (irreversible) programming languages. There are many other bi-directional debuggers as well; see for example <ref> [107] </ref> and the references in [19]. One simple technique that is sometimes used is to save periodic checkpoints of program state, and when stepping backwards, just re-compute forwards from the previous saved checkpoint to reach the state of the program at a desired time-point.
Reference: [108] <author> W. G. Unruh. </author> <title> Decoherence and quantum computers: a problem. </title> <booktitle> In Proceedings of the Workshop on Physics of Computation: </booktitle> <address> PhysComp '94, Los Alamitos, CA, 1994. </address> <publisher> Institute of Electrical and Electronic Engineers Computer Society Press. </publisher>
Reference-contexts: strictly more powerful than deterministic ones. 4.4.3 Can errors caused by imprecision and decoherence be controlled sufficiently to allow arbitrarily complex quantum computations to take place with an arbitrarily small probability of failure? A number of papers have expressed pessimism regarding the question of error accumulation in quantum computers, e.g., <ref> [61, 60, 108, 109, 22, 83] </ref>. These papers show that in the absence of error correction, the probability of error increases exponentially with both the time and space complexity of the computation, and the expected error **DRAFT** April 25, 1998 4.4.
Reference: [109] <author> W. G. Unruh. </author> <title> Mainraining coherence in quantum computers. </title> <journal> Physical Review A, </journal> <volume> 51 </volume> <pages> 992-997, </pages> <year> 1995. </year> <note> Preprint at http://feynman.stanford.edu/qcomp/ unruh/index.html. </note>
Reference-contexts: strictly more powerful than deterministic ones. 4.4.3 Can errors caused by imprecision and decoherence be controlled sufficiently to allow arbitrarily complex quantum computations to take place with an arbitrarily small probability of failure? A number of papers have expressed pessimism regarding the question of error accumulation in quantum computers, e.g., <ref> [61, 60, 108, 109, 22, 83] </ref>. These papers show that in the absence of error correction, the probability of error increases exponentially with both the time and space complexity of the computation, and the expected error **DRAFT** April 25, 1998 4.4.
Reference: [110] <author> P. van Emde Boas. </author> <title> Machine models and simulations. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume A, </booktitle> <pages> pages 1-66. </pages> <publisher> Elsevier, </publisher> <address> Amsterdam, </address> <year> 1990. </year> <note> **DRAFT** April 25, 1998 BIBLIOGRAPHY 227 </note>
Reference-contexts: Therefore it is important for us, when considering the ultimate limits of computing, to at least be aware of the ways in which quantum computing may potentially belie the conventional wisdom about what is possible. Quantitative Church's Thesis. The "Quantitative Church's Thesis" <ref> [111, 110] </ref> claims that Turing machines are as efficient as any realistic computer, within a polynomial factor.
Reference: [111] <author> A. Vergis, K. Steiglitz, and B. Dickinson. </author> <title> The complexity of analog computation. </title> <journal> Math. and Computers in Simulation, </journal> <volume> 28 </volume> <pages> 91-113, </pages> <year> 1986. </year> . 
Reference-contexts: Therefore it is important for us, when considering the ultimate limits of computing, to at least be aware of the ways in which quantum computing may potentially belie the conventional wisdom about what is possible. Quantitative Church's Thesis. The "Quantitative Church's Thesis" <ref> [111, 110] </ref> claims that Turing machines are as efficient as any realistic computer, within a polynomial factor.
Reference: [112] <author> Anastasios Vergis, Kenneth Steiglitz, and Bradley Dickinson. </author> <title> The complexity of analog computation. </title> <booktitle> Mathematics and Computers in Simulation, </booktitle> <volume> 28 </volume> <pages> 91-113, </pages> <year> 1986. </year>
Reference-contexts: However, our universe is not Newtonian, and anyway infinite precision in initial configurations is not a reasonable assumption. For some discussions of the power of analog computation models, see <ref> [112, 96] </ref>. Unlimited computer size. Finally, some parallel computing models presume that a parallel machine may have an arbitrarily large number of processing elements.
Reference: [113] <author> Carlin J. Vieri. Pendulum: </author> <title> A reversible computer architecture. </title> <type> Master's thesis, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <year> 1995. </year>
Reference-contexts: These designs were based on the conservative (reversible and 1-conserving) logic model developed by Fredkin and Toffoli (cf. [49]). Much later (1994), Hall [52] described a reversible instruction set architecture based on his "retractile cascade" reversible circuit style and the PDP-10 instruction set. 9.1.2 Pendulum architecture In 1995, Vieri <ref> [113] </ref> developed the first version of the Pendulum architecture. Pendulum was unique in that it was the first reversible computing architecture designed for implementation in a real, reversible silicon technology (SCRL).
Reference: [114] <author> Paul M. B. Vitanyi. </author> <title> Locality, communication and interconnect length in mul-ticomputers. </title> <journal> SIAM J. Computing, </journal> <volume> 17 </volume> <pages> 659-672, </pages> <year> 1988. </year>
Reference-contexts: The optimal scaling of computation within physically realistic constraints is an issue that has been studied previously (cf. Vitanyi 1988 <ref> [114] </ref>, Bilardi & Preparata 1993 [16], Smith 1995 [97]), but never before with particular attention to how the reversibility of physics allows reversible computation to improve physical scaling behavior.
Reference: [115] <author> J. von Neumann. </author> <title> Non-linear capacitance or inductance switching, amplifying, and memory organs. </title> <type> U.S. Patent No. </type> <institution> 2,815,488, </institution> <month> December 3, </month> <year> 1957. </year> <title> Assigned to the IBM Corporation. </title>
Reference-contexts: Toffoli (1980, [101]) reports that The idea that universal computing capabilities could be obtained from reversible, dissipationless (and, of course, nonlinear) physical circuits apparently first occurred to von Neumann, as reported in a posthumous paper (Wigington 1961 [119]). Wigington's paper [119] is an explanation of a patent <ref> [115] </ref> submitted by von Neumann in 1954 and posthumously granted to him in 1957. Wigington discusses dissipationless logic circuits, but does not appear to explicitly address the issue of their reversibility. Wigington also cites similar work that apparently occurred around the same time in Japan.
Reference: [116] <author> John von Neumann. </author> <title> Theory of Self-Reproducing Automata. </title> <publisher> University of Illinois Press, </publisher> <year> 1966. </year>
Reference-contexts: 5-9 RF Recursive functions [89], ch. 1 FA Finite automata ch. 2 of Hopcroft & Ullman 1979 [55] RFA Reversible finite automata Pin 1987 [84] TM Turing machines [55], ch. 7 TM Reversible Turing machines [64, 8, 63] NTM Nondeterministic Turing machines [55], x7.5 CA Cellular automata von Neumann 1966 <ref> [116] </ref>, Toffoli & Margolus 1991 [104] RAM Random access machines PRAM Parallel random access machines BLC Boolean logic circuit 3dM 3-d mesh Leighton 1992 [66], ch. 1 Table 3.1: Some existing models of computation. model, since our interest is in comparing the relative efficiency of different models. <p> Probably the earliest computing model explored by matematicians was the concept of a recursive function (cf. the text [89]). Around the same time, Turing proposed his tape machine model. Shortly thereafter, von Neumann explored the power of cellular automata (cf. <ref> [116] </ref>). Over the decades, many other models of computation have popped up, from register machines to pointer machines and RAM machines to parallel PRAMs, hypercubes, butterfly networks, meshes, etc. The models mentioned so far seem at first glance to be resonable models of real computers.
Reference: [117] <author> Robert M. Wald. </author> <title> General Relativity. </title> <publisher> University of Chicago Press, </publisher> <year> 1984. </year>
Reference-contexts: The radius of a black hole is proportional to its mass M according to R = 2GM=c 2 (Wald 1984 <ref> [117] </ref>, p. 124, eq. 6.1.45), and so the energy of any system of that radius is less than the black hole rest energy, E &lt; 2G where G is Newton's gravitational constant, G = 6:67259 fi 10 11 N m 2 =kg 2 .
Reference: [118] <author> L. Wang. </author> <title> On the classical limit of phase-space formulation of quantum mechanics: entropy. </title> <journal> J. Math. Phys., </journal> <volume> 27 </volume> <pages> 483-487, </pages> <year> 1986. </year>
Reference-contexts: INFORMATION DENSITY LIMITS 33 statistical mechanics. The question can be answered by a detailed quantum mechanical counting of distinct (mutually orthogonal) states. It can also be well approximated in the macroscopic limit <ref> [56, 118] </ref> by simply calculating the volume of phase space accessible to the system, in units where Planck's constant is 1. Let us look at some particular information density bounds in more detail. 2.2.1 Entropy bounds from black hole physics.
Reference: [119] <author> R. L. Wigington. </author> <title> A new concept in computing. </title> <booktitle> Proceedings of the IRE, </booktitle> <volume> 47 </volume> <pages> 516-523, </pages> <month> April </month> <year> 1961. </year>
Reference-contexts: Toffoli (1980, [101]) reports that The idea that universal computing capabilities could be obtained from reversible, dissipationless (and, of course, nonlinear) physical circuits apparently first occurred to von Neumann, as reported in a posthumous paper (Wigington 1961 <ref> [119] </ref>). Wigington's paper [119] is an explanation of a patent [115] submitted by von Neumann in 1954 and posthumously granted to him in 1957. Wigington discusses dissipationless logic circuits, but does not appear to explicitly address the issue of their reversibility. <p> Toffoli (1980, [101]) reports that The idea that universal computing capabilities could be obtained from reversible, dissipationless (and, of course, nonlinear) physical circuits apparently first occurred to von Neumann, as reported in a posthumous paper (Wigington 1961 <ref> [119] </ref>). Wigington's paper [119] is an explanation of a patent [115] submitted by von Neumann in 1954 and posthumously granted to him in 1957. Wigington discusses dissipationless logic circuits, but does not appear to explicitly address the issue of their reversibility.
Reference: [120] <author> S. G. Younis. </author> <title> Asymptotically Zero Energy Computing Using Split-Level Charge Recovery Logic. </title> <type> PhD thesis, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <year> 1994. </year>
Reference-contexts: Namely, the technology of so-called "adiabatic" circuits. (We explain why this name is somewhat misleading.) We describe in detail SCRL, the particular variant of adiabatic circuit technology that was developed by our group <ref> [123, 121, 120, 122] </ref>. SCRL has desirable properties that make it suitable for achieving the asymptotic efficiency benefits discussed in ch. 6 and ch. 7. Finally, we describe our design using SCRL of a simple adiabatic circuit that we recently fabricated.
Reference: [121] <author> S. G. Younis and T. F. Knight, Jr. </author> <title> Asymptotically zero energy split-level charge recovery logic. </title> <booktitle> In International Workshop on Low Power Design, </booktitle> <pages> pages 177-182, </pages> <year> 1994. </year>
Reference-contexts: Namely, the technology of so-called "adiabatic" circuits. (We explain why this name is somewhat misleading.) We describe in detail SCRL, the particular variant of adiabatic circuit technology that was developed by our group <ref> [123, 121, 120, 122] </ref>. SCRL has desirable properties that make it suitable for achieving the asymptotic efficiency benefits discussed in ch. 6 and ch. 7. Finally, we describe our design using SCRL of a simple adiabatic circuit that we recently fabricated.
Reference: [122] <author> S. G. Younis and T. F. Knight, Jr. </author> <title> Harmonic resonant rail drivers for adiabatic logic. </title> <booktitle> In Proceedings of the 1995 Symposium on Advanced Research in VLSI. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Namely, the technology of so-called "adiabatic" circuits. (We explain why this name is somewhat misleading.) We describe in detail SCRL, the particular variant of adiabatic circuit technology that was developed by our group <ref> [123, 121, 120, 122] </ref>. SCRL has desirable properties that make it suitable for achieving the asymptotic efficiency benefits discussed in ch. 6 and ch. 7. Finally, we describe our design using SCRL of a simple adiabatic circuit that we recently fabricated.
Reference: [123] <author> S. G. Younis and Jr. T. F. Knight. </author> <title> Practical implementation of charge recovering asymptotically zero power CMOS. </title> <booktitle> In Proc. 1993 Symp. on Integrated Systems, </booktitle> <pages> pages 234-250. </pages> <publisher> MIT Press, </publisher> <year> 1993. </year> <month> April 25, </month> <title> 1998 **DRAFT** Index adiabatic circuits, see circuits, adiabat. BBMCA rules for updating, </title> <booktitle> 136 Cadence, 137 circuits adiabatic, </booktitle> <address> 135-137 HP14, </address> <month> 137 </month>
Reference-contexts: Namely, the technology of so-called "adiabatic" circuits. (We explain why this name is somewhat misleading.) We describe in detail SCRL, the particular variant of adiabatic circuit technology that was developed by our group <ref> [123, 121, 120, 122] </ref>. SCRL has desirable properties that make it suitable for achieving the asymptotic efficiency benefits discussed in ch. 6 and ch. 7. Finally, we describe our design using SCRL of a simple adiabatic circuit that we recently fabricated.
References-found: 123

