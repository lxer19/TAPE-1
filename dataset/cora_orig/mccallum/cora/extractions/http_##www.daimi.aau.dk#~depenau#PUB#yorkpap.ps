URL: http://www.daimi.aau.dk/~depenau/PUB/yorkpap.ps
Refering-URL: http://www.daimi.aau.dk/~depenau/PUB/pub.html
Root-URL: http://www.daimi.aau.dk
Email: Email: depenau@daimi.aau.dk  Email: depenau@terma.dk  
Phone: Phone: +45 89423371,  Phone: +45 86222000,  
Title: Neural Networks for Classification of Ice Type Concentration from ERS-1 SAR Images Classical Methods versus
Author: Jan Depenau 
Date: 14 July 1996  
Web: DK-8520 Lystrup  
Note: and TERMA Elektronik AS, Hovmarken 4,  
Address: Ny Munkegade, Bldg. 540, DK-8000 Aarhus C  
Affiliation: DAIMI, Computer Science Department, Aarhus University,  
Abstract: This paper describes a minor part of the work done in connection with a preliminary investigation of a neural network's capability to classify ice types. It includes a short review of earlier used techniques, implementation of different neural networks and results from various experiments with these networks. The estimation of ice type concentrations from Synthetic Aperture Radar (SAR) images has been investigated for several years, see e.g. [9]. The classification estimation has been performed by training a Bayesian Maximum Likelihood Classifier (BMLC) [8] with a classification rate about 80%. The neural networks considered are all of the feed-forward type. For training, different learning algorithms and error functions are used. Both pruning and construction algorithms are used to get an optimal architecture. Experiments showed that almost any kind of neural network, using a Standard Back-Propagation (Std BP ) learning algorithm for minimising the Mean Square Error (MSE), is able to perform better than the BMLC. The reason is that the neural network is able to use a larger training set than the BM LC. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Depenau, J. </author> <year> (1995), </year> <title> A Global-Local Learning Algorithm, </title> <booktitle> Proceedings from the World Congress on Neural Networks, </booktitle> <volume> Vol I, </volume> <pages> pp. 587-590, </pages> <address> Washington 1995. </address>
Reference-contexts: Both construction and pruning algorithms were used to get an optimal architecture. Four types of feed-forward network were considered, a simple perceptron, Multi-layer perceptron, Cascade-Correlation Architecture [2] (CCA) and the Global-Local Architecture <ref> [1] </ref> (GLOCAL), see figure 1. a V 11 V 12 V 13 V 1n V L1 V Lm c V 11 V 12 V 13 V 1n V L1 V Lm perceptron, c) CCA, and d) GLOCAL Three different pruning algorithms were applied, the Magnitude pruning algorithm [6], Optimal Brain Damages
Reference: [2] <author> Fahlman, S.E. and C. </author> <booktitle> Lebiere (1990), The Cascade-Correlation Learning Architecture. In Advances in Neural Information Processing Systems II (Denver 1989), </booktitle> <editor> ed. D.S. Touretzky, </editor> <address> 524-532. San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Both construction and pruning algorithms were used to get an optimal architecture. Four types of feed-forward network were considered, a simple perceptron, Multi-layer perceptron, Cascade-Correlation Architecture <ref> [2] </ref> (CCA) and the Global-Local Architecture [1] (GLOCAL), see figure 1. a V 11 V 12 V 13 V 1n V L1 V Lm c V 11 V 12 V 13 V 1n V L1 V Lm perceptron, c) CCA, and d) GLOCAL Three different pruning algorithms were applied, the Magnitude
Reference: [3] <author> Geman, S., Bienenstock, E. and R. </author> <month> Doursat </month> <year> (1992). </year> <title> Neural Networks and the Bias/Variance Dilemma. </title> <booktitle> Neural Computation 4 pp. </booktitle> <pages> 1-58. </pages>
Reference-contexts: In all experiments shu*e was used on the training set and it had a vital influence on 1 Some would argue that this is not the case, see for example <ref> [3] </ref> 5 the network's performance and result. A representative number of the networks designed and implemented is shown in table 4.
Reference: [4] <author> Hassibi, B., and Stork, D. </author> <year> (1993), </year> <title> Second order derivatives for network pruning: Optimal Brain Surgeon. </title> <booktitle> Advances in Neural Information Processing Systems V (Denver 1993). </booktitle> <editor> ed. S.J. Hanson et al., </editor> <address> 164-171. San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: V 11 V 12 V 13 V 1n V L1 V Lm c V 11 V 12 V 13 V 1n V L1 V Lm perceptron, c) CCA, and d) GLOCAL Three different pruning algorithms were applied, the Magnitude pruning algorithm [6], Optimal Brain Damages [7] and Optimal Brain Surgeon <ref> [4] </ref>.
Reference: [5] <author> Haralick R.M., </author> <year> (1979). </year> <title> Statistical and Structural Approaches to Texture, </title> <booktitle> Proc. IEEE 67, </booktitle> <pages> pp. 786-804. </pages>
Reference-contexts: The segmentation algorithm can be briefly described as: 1) Edge detection, 2) Centre point determination, 3) Segment border determination, and 4) Segment merging. For each segment (area) 16 features are calculated. The definition and further explanation of these features can be found both in <ref> [5] </ref> and [9]. The 16 features are used to classify the ice into one of 6 ice type classes: Multi-year ice (MY), New ice type a (NIa), New ice type b (NIb), New ice type c (NIc), Open water (WA) and Ice mixture (MIX).
Reference: [6] <author> Hertz, J., Krogh, A. and Palmer, R. </author> <year> (1991), </year> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison Wesley: </publisher> <pages> 115 - 162. </pages>
Reference-contexts: the Global-Local Architecture [1] (GLOCAL), see figure 1. a V 11 V 12 V 13 V 1n V L1 V Lm c V 11 V 12 V 13 V 1n V L1 V Lm perceptron, c) CCA, and d) GLOCAL Three different pruning algorithms were applied, the Magnitude pruning algorithm <ref> [6] </ref>, Optimal Brain Damages [7] and Optimal Brain Surgeon [4].
Reference: [7] <author> Le Cun, Y., Denker, J.S. and Solla, S.A. </author> <year> (1990), </year> <title> Optimal Brain Damage. </title> <booktitle> In Advances in Neural Information Processing Systems II (Denver 1989). </booktitle> <editor> ed. D.S. Touretzky, </editor> <address> 598-605. San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: (GLOCAL), see figure 1. a V 11 V 12 V 13 V 1n V L1 V Lm c V 11 V 12 V 13 V 1n V L1 V Lm perceptron, c) CCA, and d) GLOCAL Three different pruning algorithms were applied, the Magnitude pruning algorithm [6], Optimal Brain Damages <ref> [7] </ref> and Optimal Brain Surgeon [4].
Reference: [8] <author> Niblack, W. </author> <year> (1985), </year> <title> An Introduction to Digital Image Processing, </title> <publisher> Strandsberg Publishing Company, </publisher> <address> Birkerod, Denmark 1985. </address>
Reference-contexts: Each ice type is an indication of how old the ice is. The classification approach that has been used at EMI is the classical Bayesian Maximum Likelihood Classification (BMLC) as defined in <ref> [8] </ref>. Using the segmentation algorithm on a image from the European satellite ERS 1 on a location near the coast of Greenland, the result is an image that is split into more than 20,000 segments.
Reference: [9] <author> Skriver, H. </author> <title> (1994) On the accuracy of estimation of ice type concentration from ERS-1 SAR images from EARSEL 14th Symposium, </title> <address> Gothenburg, </address> <year> 1994). </year>
Reference-contexts: The segmentation algorithm can be briefly described as: 1) Edge detection, 2) Centre point determination, 3) Segment border determination, and 4) Segment merging. For each segment (area) 16 features are calculated. The definition and further explanation of these features can be found both in [5] and <ref> [9] </ref>. The 16 features are used to classify the ice into one of 6 ice type classes: Multi-year ice (MY), New ice type a (NIa), New ice type b (NIb), New ice type c (NIc), Open water (WA) and Ice mixture (MIX). <p> This is the same as reducing the influence of speckle in the image, which is normally done by simply using as large areas in the image as possible. In <ref> [9] </ref> only segments with more than 1000 pixels were used in the classification. <p> Combined with the fact that a large number of manually classified segments is in practical applications unrealistic, because it is a very time-consuming and difficult task, only 30 of the largest segments were used for the training set in <ref> [9] </ref>. This is of course unfortunate because the calculation of the model's parameters only is based on larger segments, but it is to be used for all segments.
Reference: [10] <author> H. Skriver, </author> <title> Extraction of Sea Ice Parameters from Synthetic Aperture Radar Images Ph.D. </title> <type> Thesis from Electromagnetic Institute, </type> <institution> Technical University of Denmark, Lyngby, </institution> <year> 1989) </year> <month> 8 </month>
Reference-contexts: 1 Introduction A SAR image has fine spatial resolution containing about 8,000 x 8,000 pixels. Before any analysis is carried out the image is segmented. At Electromagnetics 1 Institute (EMI), Technical University of Denmark a program which is able to segment the images has been developed <ref> [10] </ref>. The segmentation algorithm can be briefly described as: 1) Edge detection, 2) Centre point determination, 3) Segment border determination, and 4) Segment merging. For each segment (area) 16 features are calculated. The definition and further explanation of these features can be found both in [5] and [9]. <p> The segment's feature is strongly disturbed by noise because the backscatter (BS) coefficient inherits the so-called speckle noise in the SAR image. A statistical analysis of the features made by Skriver in <ref> [10] </ref> showed among other things that there was a strong correlation among the features of the classes when small areas were considered. Since the BMLC only works properly with uncorrelated parameters, it is necessary to reduce the variance of the estimated features and the correlations between the features.
References-found: 10

