URL: http://www.cs.umn.edu/Users/dept/users/shekhar/paralleNnet.TPDS94.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/shekhar/
Root-URL: http://www.cs.umn.edu
Email: kumar@cs.umn.edu shekhar@cs.umn.edu amin@cs.umn.edu  
Title: A Scalable Parallel Formulation of the Backpropagation Algorithm for Hypercubes and Related Architectures  
Author: Vipin Kumar, Shashi Shekhar and Minesh B. Amin 
Note: nCUBE2 is a trademark of the Ncube corporation CM5 is a trademark of the  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science, University of Minnesota,  Thinking Machines corporation  
Abstract: In this paper, we present a new technique for mapping the backpropagation algorithm on hypercubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. Checkerboarding allows us to replace the all-to-all broadcast operation performed by the commonly used vertical network partitioning scheme, with operations that are much faster on the hypercubes and related architectures. Checker-boarding can be combined with the pattern partitioning technique to form a hybrid scheme which performs better than either one of these schemes. Theoretical analysis and experimental results on nCUBE2 TM y and CM5 TM z show that our scheme performs better than the other schemes, both for uniform and non-uniform networks. fl This work was supported by Army Research Office grant # 28408-MA-SDI to the University of Minnesota, by a grant from the Graduate School of University of Minnesota, the Army High Performance Computing Research Center and Minnesota Supercomputer Institute at the University of Minnesota. The authors are listed in no particular order. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation, chapter 8. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference: [2] <author> T. J. Sejnowski and C. R. Rosenburg. Nettalk: </author> <title> a parallel network that learns to read aloud. </title> <type> Technical Report JHU/EECS-86/01, </type> <institution> Johns Hopkins University - EE/Csci, </institution> <year> 1986. </year>
Reference-contexts: 1 Introduction The Backpropagation algorithm (BP)[1] is one of the most popular neural network learning algorithms. It has been used in a large number of applications <ref> [2, 3, 4, 5] </ref>. This algorithm is computation intensive and as a result there has been a great interest in developing parallel formulations of this algorithm for a variety of parallel computers. BP can be parallelized either by network partitioning or by pattern partitioning. <p> Non-uniform networks are interesting, since these are used in many applications <ref> [2, 28] </ref>. Let I 0 ; I 1 ; : : :I J be the number of nodes in the different layers of the network. The network is uniform iff I 0 = I 1 = = I J ; otherwise, the network is called a non-uniform network.
Reference: [3] <author> S. Shekhar and M. B. Amin. </author> <title> Genralization performance of feed-forward neural networks. </title> <journal> IEEE Trans. On Knowledge and Data Engineering, </journal> <month> April </month> <year> 1992. </year>
Reference-contexts: 1 Introduction The Backpropagation algorithm (BP)[1] is one of the most popular neural network learning algorithms. It has been used in a large number of applications <ref> [2, 3, 4, 5] </ref>. This algorithm is computation intensive and as a result there has been a great interest in developing parallel formulations of this algorithm for a variety of parallel computers. BP can be parallelized either by network partitioning or by pattern partitioning. <p> We will focus on the set-training regime in this paper. 2.2 Application Domains of Backpropagation The applications of neural network learning algorithms can broadly be divided into two groups: recognition and generalization 3 <ref> [3] </ref>. In recognition problems, a set of learning patterns are used to train the network. The trained network is expected to recall the output part of a previously seen learning pattern, when the input portion of the same learning pattern is presented. <p> The number of learning samples should be an order of magnitude larger than the number of weights to be learned. This is required for reasonable confidence in the learned weights for generalization over unseen testing patterns. These problems can be characterized by L &gt; 10I 2 J <ref> [3, 4] </ref>. 2.3 Parallel Formulations of Backpropagation Here we survey existing schemes to parallelize BP for the fully connected multi-layer networks. In these networks, the adjacent layers are completely connected.
Reference: [4] <author> S. Dutta and S. Shekhar. Bond-rating: </author> <title> A non-conservative application of neural networks. </title> <booktitle> In IEEE Intl. Conf. on Neural Networks, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The Backpropagation algorithm (BP)[1] is one of the most popular neural network learning algorithms. It has been used in a large number of applications <ref> [2, 3, 4, 5] </ref>. This algorithm is computation intensive and as a result there has been a great interest in developing parallel formulations of this algorithm for a variety of parallel computers. BP can be parallelized either by network partitioning or by pattern partitioning. <p> On the other hand, in generalization problems neural networks are tested with input portions of new testing patterns. The network is expected to predict the output portions of the testing pattern, which have not been shown to the network during the learning phase. Generalization problems include assignment of bond-rating <ref> [4] </ref> and economic prediction [5]. Usually a large neural network is used in recognition problems. Furthermore, recognition problems have a small number of learning samples. For example, in the protein-folding problem, L = 14 and I = 1000 [28]. <p> The number of learning samples should be an order of magnitude larger than the number of weights to be learned. This is required for reasonable confidence in the learned weights for generalization over unseen testing patterns. These problems can be characterized by L &gt; 10I 2 J <ref> [3, 4] </ref>. 2.3 Parallel Formulations of Backpropagation Here we survey existing schemes to parallelize BP for the fully connected multi-layer networks. In these networks, the adjacent layers are completely connected.
Reference: [5] <author> H. White. </author> <title> Economic prediction using neural networks: The case of ibm daily stock returns. </title> <booktitle> In IEEE Intl. Conf. on Neural Networks, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The Backpropagation algorithm (BP)[1] is one of the most popular neural network learning algorithms. It has been used in a large number of applications <ref> [2, 3, 4, 5] </ref>. This algorithm is computation intensive and as a result there has been a great interest in developing parallel formulations of this algorithm for a variety of parallel computers. BP can be parallelized either by network partitioning or by pattern partitioning. <p> The network is expected to predict the output portions of the testing pattern, which have not been shown to the network during the learning phase. Generalization problems include assignment of bond-rating [4] and economic prediction <ref> [5] </ref>. Usually a large neural network is used in recognition problems. Furthermore, recognition problems have a small number of learning samples. For example, in the protein-folding problem, L = 14 and I = 1000 [28]. The recognition problems can be characterized by L o I 2 .
Reference: [6] <author> W. Allen and A. Saha. </author> <title> Parallel neural network simulation using backpropagation for the es-kit environment. </title> <booktitle> In Proceedings of the 1989 Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <pages> pages 1097-1102, </pages> <year> 1989. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently [17, 18, 19, 20]. Pattern partitioning and network partitioning can also be combined <ref> [6, 11, 12, 14, 20, 21] </ref> to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. <p> In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [22, 6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> The network partitioning schemes take advantage of the parallelism in the computation of node activations and node errors by distributing both the nodes and the weights of the network among different processors. Nodes can be partitioned in different ways. Complete partitioning assigns one node per processor <ref> [6, 8] </ref>. Vertical sectioning divides the nodes in a layer among the processors, and each processor gets some nodes from each layer [6, 9, 10, 11, 12, 13, 14, 15, 17, 23]. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. <p> Nodes can be partitioned in different ways. Complete partitioning assigns one node per processor [6, 8]. Vertical sectioning divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> The inset of a node is allocated to the processor possessing the node. Inset grouping reduces the need for communication in computing node activations during the forward propagation. It has been used in <ref> [6, 11, 12, 14] </ref>. Outset grouping schemes form sets, which represent the collection of weights on the outgoing edges from a specific node. The outset of a node is allocated to a processor possessing the node. <p> This scheme is preferred for problems with a large set of learning patterns on machines supporting efficient broadcast operation. This scheme has been used in [18, 19, 17, 20]. The computation in different layers of BP can be pipelined <ref> [6, 14] </ref>; i.e., while one pattern is being processed for some layer, a different pattern can be processed for a preceding layer. Even though pipelining partitions the network horizontally, it is more appropriate to view it as pattern partitioning. Hybrid schemes combine pattern partitioning with network partitioning. <p> Even though pipelining partitions the network horizontally, it is more appropriate to view it as pattern partitioning. Hybrid schemes combine pattern partitioning with network partitioning. For example, pipelin-ing can be combined with vertical sectioning <ref> [6, 14] </ref>. Other examples include the combination 7 of vertical sectioning with simple pattern partitioning [12]. We can view the forward and back-ward phase computations for a pattern as a sequence of weight-matrix to activation/error-vector products. <p> The vertical-sectioning of nodes with duplicate inset/outset weight partitioning is used for mesh-connected transputers in [9, 10]. Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in <ref> [13, 6, 16] </ref>. Techniques for hypercube and related architectures have been explored in [8, 11, 12, 15, 18, 19]. 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit.
Reference: [7] <author> W. M. Lin, V. K. Prasanna, and K. W. Przytula. </author> <title> Algorithmic mapping of neural network models onto parallel simd machines. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40(12) </volume> <pages> 1390-1401, </pages> <month> December </month> <year> 1991. </year> <month> 23 </month>
Reference-contexts: In these networks, the adjacent layers are completely connected. For schemes that primarily deal with randomly sparse networks, see <ref> [7, 29, 30, 31, 32] </ref>. 3 Many researchers use alternative categories, namely classification and regression. <p> We would also like to generalize our schemes by utilizing different lengths and breadths of processor grids for different layers of the network. It would also be interesting to compare the parallelization techniques for randomly sparse networks <ref> [7, 29, 30, 31, 32] </ref>. with our scheme for non-uniform networks. 9 Acknowledgement We would like to thank Prof. Joydeep Ghosh (University of Texas at Austin), George Karypis and Ananth Grama for their useful comments during the drafting of this paper, and Dr.
Reference: [8] <author> G. Blelloch and C. R. Rosenberg. </author> <title> Network learning on the connection machine. </title> <type> Technical report, </type> <institution> MIT, </institution> <month> November </month> <year> 1986. </year>
Reference-contexts: Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning [15], pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning [11, 12]. For hypercubes, the authors are aware of only one exception <ref> [8] </ref> in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . As discussed in [11, 12], this method incurs too much communication overhead. The paper is organized as follows. <p> The network partitioning schemes take advantage of the parallelism in the computation of node activations and node errors by distributing both the nodes and the weights of the network among different processors. Nodes can be partitioned in different ways. Complete partitioning assigns one node per processor <ref> [6, 8] </ref>. Vertical sectioning divides the nodes in a layer among the processors, and each processor gets some nodes from each layer [6, 9, 10, 11, 12, 13, 14, 15, 17, 23]. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. <p> Vertical sectioning divides the nodes in a layer among the processors, and each processor gets some nodes from each layer [6, 9, 10, 11, 12, 13, 14, 15, 17, 23]. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning <ref> [8, 16] </ref> allocates one processor per weight in the network. It allows maximum concurrency in the computation of the various terms of the node activation and node error. However, it requires communication to accumulate the terms. Inset and outset grouping schemes are often used in conjunction with the vertical-sectioning scheme. <p> Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. Techniques for hypercube and related architectures have been explored in <ref> [8, 11, 12, 15, 18, 19] </ref>. 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit.
Reference: [9] <author> H. Yoon and J. H. Nang. </author> <title> Multilayer neural networks on distributed-memory multiprocessors. </title> <booktitle> In Proc. Intl. Jt. Conf. on Neural Networks (IEEE/EEC), </booktitle> <pages> pages 669-672, </pages> <year> 1991. </year>
Reference-contexts: In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [22, 6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Nodes can be partitioned in different ways. Complete partitioning assigns one node per processor [6, 8]. Vertical sectioning divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> Outset grouping eliminates the need for communication to compute error vectors during the backward phase. It has been used in [15]. Both inset and outset grouping can be used together to improve the efficiency of both forward and backward phases <ref> [9, 10, 13] </ref>. However, this scheme duplicates each weight on two processors, increasing the work during weight updates. It can also be shown that the communication cost for inset grouping is the same as the communication cost incurred by the replication scheme, through the use of multiply-accumulate-rotate operation [11]. <p> Complete node partitioning and inset weight partitioning with pipelin-ing have been explored for linear arrays in [14]. The vertical-sectioning of nodes with duplicate inset/outset weight partitioning is used for mesh-connected transputers in <ref> [9, 10] </ref>. Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. <p> The processors also contain the incoming weights associated with the nodes that reside in them. Each processor computes node activations, node errors, and weight changes for one pattern at a time. As an example, consider the scheme given in [11]. A similar scheme is given in <ref> [9, 10] </ref>. In this scheme, I nodes of each layer are partitioned among P processors. Thus, each processor computes activations for I P nodes in each layer for the forward phase.
Reference: [10] <author> H. Yoon and J. H. Nang. </author> <title> A distributed backpropagation algorithm of neural networks on distributed-memory multiprocessors. </title> <booktitle> In Proc. Intl. Conf. on Parallel Processing, </booktitle> <pages> pages 358-363, </pages> <year> 1991. </year>
Reference-contexts: In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [22, 6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Nodes can be partitioned in different ways. Complete partitioning assigns one node per processor [6, 8]. Vertical sectioning divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> Outset grouping eliminates the need for communication to compute error vectors during the backward phase. It has been used in [15]. Both inset and outset grouping can be used together to improve the efficiency of both forward and backward phases <ref> [9, 10, 13] </ref>. However, this scheme duplicates each weight on two processors, increasing the work during weight updates. It can also be shown that the communication cost for inset grouping is the same as the communication cost incurred by the replication scheme, through the use of multiply-accumulate-rotate operation [11]. <p> Complete node partitioning and inset weight partitioning with pipelin-ing have been explored for linear arrays in [14]. The vertical-sectioning of nodes with duplicate inset/outset weight partitioning is used for mesh-connected transputers in <ref> [9, 10] </ref>. Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. <p> The processors also contain the incoming weights associated with the nodes that reside in them. Each processor computes node activations, node errors, and weight changes for one pattern at a time. As an example, consider the scheme given in [11]. A similar scheme is given in <ref> [9, 10] </ref>. In this scheme, I nodes of each layer are partitioned among P processors. Thus, each processor computes activations for I P nodes in each layer for the forward phase.
Reference: [11] <author> Xiru Zhang. </author> <title> An efficient implementation of the backpropagation algorithm on the connection machine cm-2. </title> <type> Technical Report RL89-1, </type> <institution> Thinking Machines Corporation, </institution> <month> August 29 </month> <year> 1989. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently [17, 18, 19, 20]. Pattern partitioning and network partitioning can also be combined <ref> [6, 11, 12, 14, 20, 21] </ref> to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. <p> In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [22, 6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning [15], pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning <ref> [11, 12] </ref>. For hypercubes, the authors are aware of only one exception [8] in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . As discussed in [11, 12], this method incurs too much communication overhead. <p> pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning <ref> [11, 12] </ref>. For hypercubes, the authors are aware of only one exception [8] in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . As discussed in [11, 12], this method incurs too much communication overhead. The paper is organized as follows. Section 2 overviews the serial BP algorithm and its existing parallel formulations. Section 3 discusses existing parallel formulations of BP for hypercube and presents our new parallel formulations. <p> Nodes can be partitioned in different ways. Complete partitioning assigns one node per processor [6, 8]. Vertical sectioning divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> The inset of a node is allocated to the processor possessing the node. Inset grouping reduces the need for communication in computing node activations during the forward propagation. It has been used in <ref> [6, 11, 12, 14] </ref>. Outset grouping schemes form sets, which represent the collection of weights on the outgoing edges from a specific node. The outset of a node is allocated to a processor possessing the node. <p> However, this scheme duplicates each weight on two processors, increasing the work during weight updates. It can also be shown that the communication cost for inset grouping is the same as the communication cost incurred by the replication scheme, through the use of multiply-accumulate-rotate operation <ref> [11] </ref>. Checkerboard partitions the weights by grouping the rows and columns of the weight matrix. It has been used for systolic arrays [16], and for transputers [21] connected in a mesh configuration. Given enough processors, checkerboarding reduces to a complete partitioning of weights. <p> Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. Techniques for hypercube and related architectures have been explored in <ref> [8, 11, 12, 15, 18, 19] </ref>. 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit. <p> The processors also contain the incoming weights associated with the nodes that reside in them. Each processor computes node activations, node errors, and weight changes for one pattern at a time. As an example, consider the scheme given in <ref> [11] </ref>. A similar scheme is given in [9, 10]. In this scheme, I nodes of each layer are partitioned among P processors. Thus, each processor computes activations for I P nodes in each layer for the forward phase.
Reference: [12] <author> Xiru Zhang and Michael Mckenna. </author> <title> The backpropagation algorithm on grid and hypercube architectures. </title> <type> Technical Report RL90-9, </type> <institution> Thinking Machines Corporation, </institution> <year> 1990. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently [17, 18, 19, 20]. Pattern partitioning and network partitioning can also be combined <ref> [6, 11, 12, 14, 20, 21] </ref> to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. <p> In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [22, 6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning [15], pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning <ref> [11, 12] </ref>. For hypercubes, the authors are aware of only one exception [8] in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . As discussed in [11, 12], this method incurs too much communication overhead. <p> pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning <ref> [11, 12] </ref>. For hypercubes, the authors are aware of only one exception [8] in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . As discussed in [11, 12], this method incurs too much communication overhead. The paper is organized as follows. Section 2 overviews the serial BP algorithm and its existing parallel formulations. Section 3 discusses existing parallel formulations of BP for hypercube and presents our new parallel formulations. <p> Nodes can be partitioned in different ways. Complete partitioning assigns one node per processor [6, 8]. Vertical sectioning divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> The inset of a node is allocated to the processor possessing the node. Inset grouping reduces the need for communication in computing node activations during the forward propagation. It has been used in <ref> [6, 11, 12, 14] </ref>. Outset grouping schemes form sets, which represent the collection of weights on the outgoing edges from a specific node. The outset of a node is allocated to a processor possessing the node. <p> Even though pipelining partitions the network horizontally, it is more appropriate to view it as pattern partitioning. Hybrid schemes combine pattern partitioning with network partitioning. For example, pipelin-ing can be combined with vertical sectioning [6, 14]. Other examples include the combination 7 of vertical sectioning with simple pattern partitioning <ref> [12] </ref>. We can view the forward and back-ward phase computations for a pattern as a sequence of weight-matrix to activation/error-vector products. The hybrid schemes that process multiple patterns simultaneously can be viewed as a sequence of matrix-matrix products. <p> Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. Techniques for hypercube and related architectures have been explored in <ref> [8, 11, 12, 15, 18, 19] </ref>. 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit. <p> Hence, in this paper set-training iteration has been used as the benchmark to compare pattern partitioning and network partitioning of BP as is done by <ref> [12, 21] </ref> among other papers. One full-iteration of the set-training regime is used as the benchmark task to compare the alternative parallel formulations. <p> The serial time refers to the execution time of the benchmark task on a single processor. We provide a translation of the speedup measurements into weight updates per second and connections per second to be able to compare our results to related results <ref> [12, 23] </ref> in the literature. Parallel formulations speed up the execution of BP by accelerating the execution of each iteration.
Reference: [13] <author> F. Baiardi, R. Mussard, R. Serr, and G. Valastro. </author> <title> Feedforward neural networks on message passing parallel computers. </title> <booktitle> In Proc. 2nd Italian Workshop on Parallel Architectures and Neural Networks. </booktitle> <editor> (Ed. E. R. </editor> <publisher> Caianielo) World Scientific, </publisher> <address> Singapore, </address> <year> 1990. </year>
Reference-contexts: In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [22, 6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Nodes can be partitioned in different ways. Complete partitioning assigns one node per processor [6, 8]. Vertical sectioning divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> Outset grouping eliminates the need for communication to compute error vectors during the backward phase. It has been used in [15]. Both inset and outset grouping can be used together to improve the efficiency of both forward and backward phases <ref> [9, 10, 13] </ref>. However, this scheme duplicates each weight on two processors, increasing the work during weight updates. It can also be shown that the communication cost for inset grouping is the same as the communication cost incurred by the replication scheme, through the use of multiply-accumulate-rotate operation [11]. <p> The vertical-sectioning of nodes with duplicate inset/outset weight partitioning is used for mesh-connected transputers in [9, 10]. Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in <ref> [13, 6, 16] </ref>. Techniques for hypercube and related architectures have been explored in [8, 11, 12, 15, 18, 19]. 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit.
Reference: [14] <author> M. Marchesi, G. Orlandi, F. Piazza, and A. Uncini. </author> <title> Linear array architecture implementing the backpropagation neural network. </title> <booktitle> In Proc. 2nd Italian Workshop on Parallel Architectures and Neural Networks. </booktitle> <editor> (Ed. E. R. </editor> <publisher> Caianielo) World Scientific, </publisher> <address> Singapore, </address> <year> 1990. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently [17, 18, 19, 20]. Pattern partitioning and network partitioning can also be combined <ref> [6, 11, 12, 14, 20, 21] </ref> to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. <p> In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [22, 6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Nodes can be partitioned in different ways. Complete partitioning assigns one node per processor [6, 8]. Vertical sectioning divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> The inset of a node is allocated to the processor possessing the node. Inset grouping reduces the need for communication in computing node activations during the forward propagation. It has been used in <ref> [6, 11, 12, 14] </ref>. Outset grouping schemes form sets, which represent the collection of weights on the outgoing edges from a specific node. The outset of a node is allocated to a processor possessing the node. <p> This scheme is preferred for problems with a large set of learning patterns on machines supporting efficient broadcast operation. This scheme has been used in [18, 19, 17, 20]. The computation in different layers of BP can be pipelined <ref> [6, 14] </ref>; i.e., while one pattern is being processed for some layer, a different pattern can be processed for a preceding layer. Even though pipelining partitions the network horizontally, it is more appropriate to view it as pattern partitioning. Hybrid schemes combine pattern partitioning with network partitioning. <p> Even though pipelining partitions the network horizontally, it is more appropriate to view it as pattern partitioning. Hybrid schemes combine pattern partitioning with network partitioning. For example, pipelin-ing can be combined with vertical sectioning <ref> [6, 14] </ref>. Other examples include the combination 7 of vertical sectioning with simple pattern partitioning [12]. We can view the forward and back-ward phase computations for a pattern as a sequence of weight-matrix to activation/error-vector products. <p> An orthogonal way of classifying the approaches to parallel BP is based on the architectures on which they are implemented. Complete node partitioning and inset weight partitioning with pipelin-ing have been explored for linear arrays in <ref> [14] </ref>. The vertical-sectioning of nodes with duplicate inset/outset weight partitioning is used for mesh-connected transputers in [9, 10]. Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16].
Reference: [15] <author> D. S. Newhall and J. C. Horvath. </author> <title> Analysis of text using a neural network: A hypercube implementation. </title> <booktitle> In Proceedings of the 1989 Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <pages> pages 1119-1122, </pages> <year> 1989. </year>
Reference-contexts: In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [22, 6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> The communication pattern used in our scheme is significantly different from those of other checkerboarding schemes [16, 21], leading to improved performance on hypercubes. Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning <ref> [15] </ref>, pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning [11, 12]. For hypercubes, the authors are aware of only one exception [8] in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . <p> Nodes can be partitioned in different ways. Complete partitioning assigns one node per processor [6, 8]. Vertical sectioning divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> The outset of a node is allocated to a processor possessing the node. Outset grouping eliminates the need for communication to compute error vectors during the backward phase. It has been used in <ref> [15] </ref>. Both inset and outset grouping can be used together to improve the efficiency of both forward and backward phases [9, 10, 13]. However, this scheme duplicates each weight on two processors, increasing the work during weight updates. <p> Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. Techniques for hypercube and related architectures have been explored in <ref> [8, 11, 12, 15, 18, 19] </ref>. 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit.
Reference: [16] <author> S. Y. Kung and J.N. Hwang. </author> <title> A unified systolic architecture for artificial neural networks. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 6 </volume> <pages> 358-387, </pages> <year> 1989. </year>
Reference-contexts: On a 256-node CM5 (with four vector boards per processor), our formulation performs over 500 Million weight updates per second [26]. The checkerboarding scheme has been used in other parallel formulations of BP for the mesh architecture [21] and systolic arrays <ref> [16] </ref>. But if these formulations are mapped onto hypercube, then their performance approaches that of vertical sectioning (in the case of [16]) or the hybrid of vertical sectioning and pattern partitioning (in the case of [21]). <p> The checkerboarding scheme has been used in other parallel formulations of BP for the mesh architecture [21] and systolic arrays <ref> [16] </ref>. But if these formulations are mapped onto hypercube, then their performance approaches that of vertical sectioning (in the case of [16]) or the hybrid of vertical sectioning and pattern partitioning (in the case of [21]). The communication pattern used in our scheme is significantly different from those of other checkerboarding schemes [16, 21], leading to improved performance on hypercubes. <p> The communication pattern used in our scheme is significantly different from those of other checkerboarding schemes <ref> [16, 21] </ref>, leading to improved performance on hypercubes. Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning [15], pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning [11, 12]. <p> Vertical sectioning divides the nodes in a layer among the processors, and each processor gets some nodes from each layer [6, 9, 10, 11, 12, 13, 14, 15, 17, 23]. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning <ref> [8, 16] </ref> allocates one processor per weight in the network. It allows maximum concurrency in the computation of the various terms of the node activation and node error. However, it requires communication to accumulate the terms. Inset and outset grouping schemes are often used in conjunction with the vertical-sectioning scheme. <p> Checkerboard partitions the weights by grouping the rows and columns of the weight matrix. It has been used for systolic arrays <ref> [16] </ref>, and for transputers [21] connected in a mesh configuration. Given enough processors, checkerboarding reduces to a complete partitioning of weights. Pattern partitioning replicates the network nodes and weights at each processor. It divides the pattern set equally among all processors. <p> The vertical-sectioning of nodes with duplicate inset/outset weight partitioning is used for mesh-connected transputers in [9, 10]. Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in <ref> [13, 6, 16] </ref>. Techniques for hypercube and related architectures have been explored in [8, 11, 12, 15, 18, 19]. 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit.
Reference: [17] <author> K. Joe, Y. Mori, and S. Miyake. </author> <title> Simulation of a large-scale neural network on a parallel computer. </title> <booktitle> In Proceedings of the 1989 Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <pages> pages 1111-1118, </pages> <year> 1989. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently <ref> [17, 18, 19, 20] </ref>. Pattern partitioning and network partitioning can also be combined [6, 11, 12, 14, 20, 21] to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. <p> In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [22, 6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Nodes can be partitioned in different ways. Complete partitioning assigns one node per processor [6, 8]. Vertical sectioning divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> Each processor also accumulates the weight changes according to the local patterns. Then, the processors communicate to accumulate the weight changes for updating weights. This scheme is preferred for problems with a large set of learning patterns on machines supporting efficient broadcast operation. This scheme has been used in <ref> [18, 19, 17, 20] </ref>. The computation in different layers of BP can be pipelined [6, 14]; i.e., while one pattern is being processed for some layer, a different pattern can be processed for a preceding layer.
Reference: [18] <author> M. Witbrock and M. Zagha. </author> <title> An implementation of back-propagation learning on gf11, a large simd parallel computer. </title> <type> Technical Report CMU-CS-89-208, CMU, </type> <month> December </month> <year> 1989. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently <ref> [17, 18, 19, 20] </ref>. Pattern partitioning and network partitioning can also be combined [6, 11, 12, 14, 20, 21] to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. <p> The communication pattern used in our scheme is significantly different from those of other checkerboarding schemes [16, 21], leading to improved performance on hypercubes. Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning [15], pattern partitioning <ref> [18, 19] </ref>, or a hybrid of vertical sectioning and pattern partitioning [11, 12]. For hypercubes, the authors are aware of only one exception [8] in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . <p> Each processor also accumulates the weight changes according to the local patterns. Then, the processors communicate to accumulate the weight changes for updating weights. This scheme is preferred for problems with a large set of learning patterns on machines supporting efficient broadcast operation. This scheme has been used in <ref> [18, 19, 17, 20] </ref>. The computation in different layers of BP can be pipelined [6, 14]; i.e., while one pattern is being processed for some layer, a different pattern can be processed for a preceding layer. <p> Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. Techniques for hypercube and related architectures have been explored in <ref> [8, 11, 12, 15, 18, 19] </ref>. 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit.
Reference: [19] <author> J. Bourrley. </author> <title> Parallelization of a neural learning algorithm. </title> <booktitle> In Proc. 1st Euro. Workshop on Hypercube and Distributed Computers. </booktitle> <editor> (Ed. F. Andre) North-Holland, </editor> <year> 1989. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently <ref> [17, 18, 19, 20] </ref>. Pattern partitioning and network partitioning can also be combined [6, 11, 12, 14, 20, 21] to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. <p> The communication pattern used in our scheme is significantly different from those of other checkerboarding schemes [16, 21], leading to improved performance on hypercubes. Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning [15], pattern partitioning <ref> [18, 19] </ref>, or a hybrid of vertical sectioning and pattern partitioning [11, 12]. For hypercubes, the authors are aware of only one exception [8] in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . <p> Each processor also accumulates the weight changes according to the local patterns. Then, the processors communicate to accumulate the weight changes for updating weights. This scheme is preferred for problems with a large set of learning patterns on machines supporting efficient broadcast operation. This scheme has been used in <ref> [18, 19, 17, 20] </ref>. The computation in different layers of BP can be pipelined [6, 14]; i.e., while one pattern is being processed for some layer, a different pattern can be processed for a preceding layer. <p> Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. Techniques for hypercube and related architectures have been explored in <ref> [8, 11, 12, 15, 18, 19] </ref>. 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit.
Reference: [20] <author> B. K. Mak and O. Egecioglu. </author> <title> Communication parameter tests and parallel backpropagation algorithms on ipsc/2 hypercube multiprocessor. </title> <booktitle> In IEEE Frontier, </booktitle> <pages> pages 1353-1364, </pages> <year> 1990. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently <ref> [17, 18, 19, 20] </ref>. Pattern partitioning and network partitioning can also be combined [6, 11, 12, 14, 20, 21] to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. <p> In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently [17, 18, 19, 20]. Pattern partitioning and network partitioning can also be combined <ref> [6, 11, 12, 14, 20, 21] </ref> to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. <p> Each processor also accumulates the weight changes according to the local patterns. Then, the processors communicate to accumulate the weight changes for updating weights. This scheme is preferred for problems with a large set of learning patterns on machines supporting efficient broadcast operation. This scheme has been used in <ref> [18, 19, 17, 20] </ref>. The computation in different layers of BP can be pipelined [6, 14]; i.e., while one pattern is being processed for some layer, a different pattern can be processed for a preceding layer.
Reference: [21] <author> A. Petrowski, L. Personnaz, G. Dreyfus, and C. Girault. </author> <title> Parallel implementations of neural network simulations. </title> <booktitle> In First European Workshop on hypercube and distributed computers, </booktitle> <pages> pages 205-218. </pages> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <year> 1989. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently [17, 18, 19, 20]. Pattern partitioning and network partitioning can also be combined <ref> [6, 11, 12, 14, 20, 21] </ref> to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. <p> A summary of results of this paper appear in [25]. On a 256-node CM5 (with four vector boards per processor), our formulation performs over 500 Million weight updates per second [26]. The checkerboarding scheme has been used in other parallel formulations of BP for the mesh architecture <ref> [21] </ref> and systolic arrays [16]. But if these formulations are mapped onto hypercube, then their performance approaches that of vertical sectioning (in the case of [16]) or the hybrid of vertical sectioning and pattern partitioning (in the case of [21]). <p> used in other parallel formulations of BP for the mesh architecture <ref> [21] </ref> and systolic arrays [16]. But if these formulations are mapped onto hypercube, then their performance approaches that of vertical sectioning (in the case of [16]) or the hybrid of vertical sectioning and pattern partitioning (in the case of [21]). The communication pattern used in our scheme is significantly different from those of other checkerboarding schemes [16, 21], leading to improved performance on hypercubes. <p> The communication pattern used in our scheme is significantly different from those of other checkerboarding schemes <ref> [16, 21] </ref>, leading to improved performance on hypercubes. Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning [15], pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning [11, 12]. <p> Checkerboard partitions the weights by grouping the rows and columns of the weight matrix. It has been used for systolic arrays [16], and for transputers <ref> [21] </ref> connected in a mesh configuration. Given enough processors, checkerboarding reduces to a complete partitioning of weights. Pattern partitioning replicates the network nodes and weights at each processor. It divides the pattern set equally among all processors. <p> The hybrid schemes that process multiple patterns simultaneously can be viewed as a sequence of matrix-matrix products. This allows the possibility of using parallel algorithms for computing a sequence of matrix operations <ref> [33, 21] </ref>. An orthogonal way of classifying the approaches to parallel BP is based on the architectures on which they are implemented. Complete node partitioning and inset weight partitioning with pipelin-ing have been explored for linear arrays in [14]. <p> Complete node partitioning and inset weight partitioning with pipelin-ing have been explored for linear arrays in [14]. The vertical-sectioning of nodes with duplicate inset/outset weight partitioning is used for mesh-connected transputers in [9, 10]. Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in <ref> [21] </ref> for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. <p> Hence, in this paper set-training iteration has been used as the benchmark to compare pattern partitioning and network partitioning of BP as is done by <ref> [12, 21] </ref> among other papers. One full-iteration of the set-training regime is used as the benchmark task to compare the alternative parallel formulations.
Reference: [22] <author> Nelson Morgan, James Beck, Phil Kohn, and Jeff Bilmes. </author> <title> Neurocomputing on the RAP. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1993. </year> <note> This is chapter 7 in a book by K. </note> <author> W. Przytula and V. K. </author> <title> Prasanna entitled Parallel Digital Implementations of Neural Networks. </title>
Reference-contexts: In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [22, 6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors.
Reference: [23] <author> D.A.Pomerleau G.L.Gusciora, H.T.Kung and D.S.Touretzky. </author> <title> Neural network simulation at warp speed: How we got 17 million connections per second. </title> <booktitle> In IEEE 2nd Intl. Conf. on Neural Networks, </booktitle> <pages> pages 143-50, </pages> <year> 1988. </year>
Reference-contexts: In this paper, we present a new technique for mapping the backpropagation algorithm on hyper-cubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [22, 6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Nodes can be partitioned in different ways. Complete partitioning assigns one node per processor [6, 8]. Vertical sectioning divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 23] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> The serial time refers to the execution time of the benchmark task on a single processor. We provide a translation of the speedup measurements into weight updates per second and connections per second to be able to compare our results to related results <ref> [12, 23] </ref> in the literature. Parallel formulations speed up the execution of BP by accelerating the execution of each iteration.
Reference: [24] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing. </title> <publisher> Benjamin Cummings, </publisher> <address> Redwood City, CA, </address> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Although the scheme is natural for the hypercube architecture (e.g., nCUBE, Intel iPSC T M x ), it is equally suitable for Fat-tree based architectures such as CM5 as well as mesh based architectures with worm-hole routing <ref> [24] </ref>. susch as Intel Paragon T M-. Our scheme is applicable only to fully connected networks; i.e., each node in a layer (except the output layer) is connected to all nodes in the next layer. <p> This increases the degree of concurrency of our scheme over the vertical-sectioning scheme. Our formulation is essentially an adaptation of the matrix vector multiplication scheme for hypercube presented in <ref> [34, 24] </ref>. Our scheme uses a well known embedding [37, 24] of a p p P grid of processor on a P (= 2 2k ) processor hypercube. The processors in each row and column of the grid form a 2 k = p processor hypercube. <p> This increases the degree of concurrency of our scheme over the vertical-sectioning scheme. Our formulation is essentially an adaptation of the matrix vector multiplication scheme for hypercube presented in [34, 24]. Our scheme uses a well known embedding <ref> [37, 24] </ref> of a p p P grid of processor on a P (= 2 2k ) processor hypercube. The processors in each row and column of the grid form a 2 k = p processor hypercube. <p> A number of metrics have been developed for characterizing the scalability of different parallel algorithms and architectures [38]. Isoefficiency is one such metric, which is defined as the rate of change of problem size as a function of number of processors to maintain fixed processor utilization <ref> [39, 24] </ref>. An algorithm that requires a smaller change in problem size to obtain fixed efficiency is considered more scalable. Isoefficiency function has been used to characterize scalability of a variety of parallel algorithms [40, 41, 42, 43].
Reference: [25] <author> V. Kumar, S. Shekhar, and M. B. Amin. </author> <title> A highly parallel formulation of backpropagation on hyper-cubes: A summary of results. </title> <booktitle> In Proc. Intl. Conf. on Neural Networks (IJCNN), </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: A summary of results of this paper appear in <ref> [25] </ref>. On a 256-node CM5 (with four vector boards per processor), our formulation performs over 500 Million weight updates per second [26]. The checkerboarding scheme has been used in other parallel formulations of BP for the mesh architecture [21] and systolic arrays [16].
Reference: [26] <author> William J. Leinberger. </author> <title> Vectorized checkerboarding formulation of the backpropagation algorithm for cm5. </title> <type> Technical Report Master's Thesis, </type> <institution> Computer Science Department, University of Minnesota, </institution> <year> 1993. </year>
Reference-contexts: A summary of results of this paper appear in [25]. On a 256-node CM5 (with four vector boards per processor), our formulation performs over 500 Million weight updates per second <ref> [26] </ref>. The checkerboarding scheme has been used in other parallel formulations of BP for the mesh architecture [21] and systolic arrays [16]. <p> With the upgrade time of processors to vector units, t c as well as the overall performance may improve by one or two orders of magnitude <ref> [26] </ref>. The proposed schemes scale well with problem size and achieve asymptotically optimal parallel execution time for backpropagation algorithm on uniform networks. The parallelization schemes for non-uniform networks need further exploration. We plan to explore the trade-off between processor utilization and speed-up in our future work.
Reference: [27] <author> D. E. Rumelhart J. L. McClelland. </author> <title> Explorations in Parallel Distributed Processing. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: Many neural network software (e.g. <ref> [27] </ref>) support an alternative training regime, called per-pattern training. In this training regime, the set of patterns are permuted to create an ordering among the learning patterns. The patterns are then examined in sequence. <p> Anonymous referees provided valuable comments to improve the presentation. William Leinberger vectorized the code on CM5 as his master's project. Ms. Christine McCarthy, Dept. of Composition, University of Minnesota provided editorial help to improve grammatical and stylistic aspects. We also thank <ref> [27] </ref>, which was an excellent source on BP.
Reference: [28] <author> G. L. Wilcox, M. Poliac, and M. Liebman. </author> <title> Protein tertiary structure prediction using a large backpropagation network. </title> <booktitle> In IJCNN (IEEE/EEC), </booktitle> <pages> pages 365-369, </pages> <year> 1990. </year>
Reference-contexts: In recognition problems, a set of learning patterns are used to train the network. The trained network is expected to recall the output part of a previously seen learning pattern, when the input portion of the same learning pattern is presented. Pattern recognition applications <ref> [28] </ref> are examples of recognition problems. On the other hand, in generalization problems neural networks are tested with input portions of new testing patterns. The network is expected to predict the output portions of the testing pattern, which have not been shown to the network during the learning phase. <p> Generalization problems include assignment of bond-rating [4] and economic prediction [5]. Usually a large neural network is used in recognition problems. Furthermore, recognition problems have a small number of learning samples. For example, in the protein-folding problem, L = 14 and I = 1000 <ref> [28] </ref>. The recognition problems can be characterized by L o I 2 . In contrast, generalization problems are characterized by a large number of learning samples. The number of learning samples should be an order of magnitude larger than the number of weights to be learned. <p> Non-uniform networks are interesting, since these are used in many applications <ref> [2, 28] </ref>. Let I 0 ; I 1 ; : : :I J be the number of nodes in the different layers of the network. The network is uniform iff I 0 = I 1 = = I J ; otherwise, the network is called a non-uniform network.
Reference: [29] <author> B. W. Wah and L. Chu. </author> <title> Efficient mapping of neural networks on multicomputers. </title> <booktitle> IEEE International Conference on Parallel Processing, </booktitle> <pages> pages 234-241, </pages> <year> 1990. </year>
Reference-contexts: In these networks, the adjacent layers are completely connected. For schemes that primarily deal with randomly sparse networks, see <ref> [7, 29, 30, 31, 32] </ref>. 3 Many researchers use alternative categories, namely classification and regression. <p> We would also like to generalize our schemes by utilizing different lengths and breadths of processor grids for different layers of the network. It would also be interesting to compare the parallelization techniques for randomly sparse networks <ref> [7, 29, 30, 31, 32] </ref>. with our scheme for non-uniform networks. 9 Acknowledgement We would like to thank Prof. Joydeep Ghosh (University of Texas at Austin), George Karypis and Ananth Grama for their useful comments during the drafting of this paper, and Dr.
Reference: [30] <author> M. Misra and V. K. Prasanna Kumar. </author> <title> Neural network simulation on a reduced mesh of tree organization. </title> <booktitle> In SPIE/SPSE Symposium on Electronic Images, </booktitle> <year> 1990. </year>
Reference-contexts: In these networks, the adjacent layers are completely connected. For schemes that primarily deal with randomly sparse networks, see <ref> [7, 29, 30, 31, 32] </ref>. 3 Many researchers use alternative categories, namely classification and regression. <p> We would also like to generalize our schemes by utilizing different lengths and breadths of processor grids for different layers of the network. It would also be interesting to compare the parallelization techniques for randomly sparse networks <ref> [7, 29, 30, 31, 32] </ref>. with our scheme for non-uniform networks. 9 Acknowledgement We would like to thank Prof. Joydeep Ghosh (University of Texas at Austin), George Karypis and Ananth Grama for their useful comments during the drafting of this paper, and Dr.
Reference: [31] <author> M. Misra. </author> <title> Implementation of neural networks on parallel architectures. </title> <type> In Tech. Report 295. </type> <institution> Dept. of EE Systems hi, Univ. of Soutern California, </institution> <year> 1992. </year>
Reference-contexts: In these networks, the adjacent layers are completely connected. For schemes that primarily deal with randomly sparse networks, see <ref> [7, 29, 30, 31, 32] </ref>. 3 Many researchers use alternative categories, namely classification and regression. <p> We would also like to generalize our schemes by utilizing different lengths and breadths of processor grids for different layers of the network. It would also be interesting to compare the parallelization techniques for randomly sparse networks <ref> [7, 29, 30, 31, 32] </ref>. with our scheme for non-uniform networks. 9 Acknowledgement We would like to thank Prof. Joydeep Ghosh (University of Texas at Austin), George Karypis and Ananth Grama for their useful comments during the drafting of this paper, and Dr.
Reference: [32] <author> J. Ghosh and K. Hwang. </author> <title> Mapping neural networks onto message passing multicomputers. </title> <editor> Jr. </editor> <booktitle> Parallel and Distributed Computing, </booktitle> <month> April </month> <year> 1989. </year>
Reference-contexts: In these networks, the adjacent layers are completely connected. For schemes that primarily deal with randomly sparse networks, see <ref> [7, 29, 30, 31, 32] </ref>. 3 Many researchers use alternative categories, namely classification and regression. <p> We would also like to generalize our schemes by utilizing different lengths and breadths of processor grids for different layers of the network. It would also be interesting to compare the parallelization techniques for randomly sparse networks <ref> [7, 29, 30, 31, 32] </ref>. with our scheme for non-uniform networks. 9 Acknowledgement We would like to thank Prof. Joydeep Ghosh (University of Texas at Austin), George Karypis and Ananth Grama for their useful comments during the drafting of this paper, and Dr.
Reference: [33] <author> Anshul Gupta and Vipin Kumar. </author> <title> On the scalability of Matrix Multiplication Algorithms on parallel computers. </title> <type> Technical Report TR 91-54, </type> <institution> Computer Science Department, University of Minnesota, </institution> <address> Minneapolis, MN 55455, </address> <year> 1991. </year> <note> Revised, </note> <month> Sep. </month> <year> 1992. </year>
Reference-contexts: The hybrid schemes that process multiple patterns simultaneously can be viewed as a sequence of matrix-matrix products. This allows the possibility of using parallel algorithms for computing a sequence of matrix operations <ref> [33, 21] </ref>. An orthogonal way of classifying the approaches to parallel BP is based on the architectures on which they are implemented. Complete node partitioning and inset weight partitioning with pipelin-ing have been explored for linear arrays in [14].
Reference: [34] <author> D. P. Bertsekas and J. N. </author> <title> Tsitsiklis. </title> <booktitle> Parallel and Distributed Computation, </booktitle> <pages> pages 50-55. </pages> <publisher> Prentice-Hall Inc, </publisher> <year> 1989. </year>
Reference-contexts: A message containing m words can be broadcast from a processor to all other processors in time (t s + t w m) fl log (P ), as a binary tree of depth log (P ) can be mapped on to a hypercube containing P processors <ref> [34] </ref> 4 . <p> These algorithms can be used to improve the asymptotic performance of our new scheme and pattern partitioning. For simplicity, we only consider the standard broadcasting algorithm from <ref> [34] </ref>. The relative performance of different schemes will be similar even with Johnson and Ho's broadcast algorithms. 8 In this scheme, the whole network is replicated on each of the P processors. Each processor then performs learning using L P patterns. <p> This increases the degree of concurrency of our scheme over the vertical-sectioning scheme. Our formulation is essentially an adaptation of the matrix vector multiplication scheme for hypercube presented in <ref> [34, 24] </ref>. Our scheme uses a well known embedding [37, 24] of a p p P grid of processor on a P (= 2 2k ) processor hypercube. The processors in each row and column of the grid form a 2 k = p processor hypercube. <p> A sample mapping of a 4*4 processor grid onto 6 All-to-all broadcast can be carried out in fl ( P log (P) [t s + I P t w ]) on a hypercube if simultaneous communication along all channels of hypercube is allowed <ref> [34] </ref>. 10 ? * ? ffi - 14151312 23 10 a hypercube with 16 processors is shown in Figure 3. The processor grid is shown as a matrix with 4 rows and 4 columns. The number inside each grid cell represents the address of the processor in the hypercube.
Reference: [35] <author> S. Lennart Johnsson and Ching-Tien Ho. </author> <title> Optimum broadcasting and personalized communication in hypercubes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(9):1249 - 1268, </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: But, the step of accumulating the weight changes requires communication among the processors. 4 Johnson and Ho present asymptotically faster broadcast algorithms for hypercubes in <ref> [35] </ref>. These algorithms can be used to improve the asymptotic performance of our new scheme and pattern partitioning. For simplicity, we only consider the standard broadcasting algorithm from [34].
Reference: [36] <author> Y. Saad and M. H. Schultz. </author> <title> Topological properties of hypercubes. </title> <journal> IEEE Trans. on Computers, </journal> <pages> pages 867-872, </pages> <year> 1988. </year>
Reference-contexts: In the forward phase, to compute activation values for any node i in layer j, it is necessary to have the activation values of all nodes from layer (j 1). Thus, the all-to-all broadcast operation <ref> [36] </ref>, in which each processor needs to send its I P activations to all other P processors, needs to be performed. This operation takes P [t s + I P t w ] for each of the J layers.
Reference: [37] <author> J. Jenq and S. Sahni. </author> <title> All pairs shortest paths on a hypercube multiprocessor. </title> <booktitle> In Int. Conf. on Parallel Processing, </booktitle> <pages> pages 713-716, </pages> <year> 1987. </year>
Reference-contexts: This increases the degree of concurrency of our scheme over the vertical-sectioning scheme. Our formulation is essentially an adaptation of the matrix vector multiplication scheme for hypercube presented in [34, 24]. Our scheme uses a well known embedding <ref> [37, 24] </ref> of a p p P grid of processor on a P (= 2 2k ) processor hypercube. The processors in each row and column of the grid form a 2 k = p processor hypercube.
Reference: [38] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing scalability of parallel algorithms and architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1993 (to appear). A short version of the paper appears in the Proceedings of the 1991 International Conference on Supercomputing, Germany, and as an invited paper in the Proceedings of 29th Annual Allerton Conference on Communication, Control and Computing, Urbana,IL, </note> <month> October </month> <year> 1991. </year>
Reference-contexts: In that sense, all the schemes are scalable. But the degree of scalability of these schemes is actually quite different from each other. A number of metrics have been developed for characterizing the scalability of different parallel algorithms and architectures <ref> [38] </ref>. Isoefficiency is one such metric, which is defined as the rate of change of problem size as a function of number of processors to maintain fixed processor utilization [39, 24]. An algorithm that requires a smaller change in problem size to obtain fixed efficiency is considered more scalable.
Reference: [39] <author> Ananth Grama, Anshul Gupta, and Vipin Kumar. </author> <title> Isoefficiency function: A scalability metric for parallel algorithms and architectures. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <note> 1994 (To be Published). Also available as Technical Report TR-93-24, </note> <institution> Department of Computer Science, </institution> <note> University of Minnesota and from anonymous ftp site ftp.cs.umn.edu, file users/kumar/isoeff-tutorial.ps. 25 </note>
Reference-contexts: A number of metrics have been developed for characterizing the scalability of different parallel algorithms and architectures [38]. Isoefficiency is one such metric, which is defined as the rate of change of problem size as a function of number of processors to maintain fixed processor utilization <ref> [39, 24] </ref>. An algorithm that requires a smaller change in problem size to obtain fixed efficiency is considered more scalable. Isoefficiency function has been used to characterize scalability of a variety of parallel algorithms [40, 41, 42, 43].
Reference: [40] <author> Anshul Gupta and Vipin Kumar. </author> <title> The scalability of FFT on parallel computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4 No. 8 </volume> <pages> 922-932, </pages> <month> August </month> <year> 1993. </year> <note> Available as a technical report TR 90-53, </note> <institution> Computer Science Department, University of Minnesota. </institution>
Reference-contexts: An algorithm that requires a smaller change in problem size to obtain fixed efficiency is considered more scalable. Isoefficiency function has been used to characterize scalability of a variety of parallel algorithms <ref> [40, 41, 42, 43] </ref>. The problem size (i.e., serial run time) for each iteration of BP is LJI 2 t c .
Reference: [41] <author> Vipin Kumar and Vineet Singh. </author> <title> Scalability of Parallel Algorithms for the All-Pairs Shortest Path Problem. </title> <journal> Journal of Parallel and Distributed Processing (special issue on massively parallel computation), </journal> <volume> 13(2) </volume> <pages> 124-138, </pages> <month> October </month> <year> 1991. </year> <note> A short version appears in the Proceedings of the International Conference on Parallel Processing, </note> <year> 1990. </year>
Reference-contexts: An algorithm that requires a smaller change in problem size to obtain fixed efficiency is considered more scalable. Isoefficiency function has been used to characterize scalability of a variety of parallel algorithms <ref> [40, 41, 42, 43] </ref>. The problem size (i.e., serial run time) for each iteration of BP is LJI 2 t c .
Reference: [42] <author> S. Ranka and S. Sahni. </author> <title> Hypercube Algorithms for Image Processing and Pattern Recognition. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: An algorithm that requires a smaller change in problem size to obtain fixed efficiency is considered more scalable. Isoefficiency function has been used to characterize scalability of a variety of parallel algorithms <ref> [40, 41, 42, 43] </ref>. The problem size (i.e., serial run time) for each iteration of BP is LJI 2 t c .
Reference: [43] <author> Vineet Singh, Vipin Kumar, Gul Agha, and Chris Tomlinson. </author> <title> Scalability of parallel sorting on mesh multicomputers. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20 (2), </volume> <year> 1991. </year> <note> A short version of this paper appears in the Proceedings of the Fifth International Parallel Processing Symposium, </note> <year> 1991. </year>
Reference-contexts: An algorithm that requires a smaller change in problem size to obtain fixed efficiency is considered more scalable. Isoefficiency function has been used to characterize scalability of a variety of parallel algorithms <ref> [40, 41, 42, 43] </ref>. The problem size (i.e., serial run time) for each iteration of BP is LJI 2 t c .
Reference: [44] <author> David J Kuck. </author> <title> A survey of parallel machine organization and programming. </title> <journal> ACM Computing Survey, </journal> <volume> 9(1), </volume> <month> March </month> <year> 1977. </year> <month> 26 </month>
Reference-contexts: In such an environment, the lower bound on the parallel computation time for evaluating an arithmetic expression with N atomic operations is dlog 2 (N )e for any number of processors <ref> [44] </ref>. The number of arithmetic operations in the expression computed at the output nodes of a feed-forward neural network at the end of forward propagation is fl (LI 2 ), assuming J to be a constant.
References-found: 44

