URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-284.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Title: Correlation and Interpolation Networks for Real-time Expression Analysis/Synthesis.  
Author: Trevor Darrell, Irfan Essa, Alex Pentland 
Affiliation: Perceptual Computing Group MIT Media Lab  
Abstract: We describe a framework for real-time tracking of facial expressions that uses neurally-inspired correlation and interpolation methods. A distributed view-based representation is used to characterize facial state, and is computed using a replicated correlation network. The ensemble response of the set of view correlation scores is input to a network based interpolation method, which maps perceptual state to motor control states for a simulated 3-D face model. Activation levels of the motor state correspond to muscle activations in an anatomically derived model. By integrating fast and robust 2-D processing with 3-D models, we obtain a system that is able to quickly track and interpret complex facial motions in real-time.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Beymer, A. Shashua, and T. Poggio, </author> <title> Example Based Image Analysis and Synthesis, </title> <institution> MIT AI Lab TR-1431, </institution> <year> 1993. </year>
Reference-contexts: In this model a small set of canonical expressions is defined, and intermediate expressions constructed via the interpolation technique. The representation used is a generic feature vector, which in the case of cartoon faces consists of the contour endpoints. Recently, Beymer et al. <ref> [1] </ref> extended this approach to use real images, relying on optical flow and image warping techniques to solve the correspondence and prediction problems, respectively. RBF-based techniques have the advantage of allowing for the efficient and fast computation of intermediate states in a representation.
Reference: [2] <author> T. Darrell and A. Pentland. </author> <title> Classification of Hand Gestures using a View-Based Distributed Representation In NIPS-6, </title> <year> 1993. </year>
Reference-contexts: Previously, we demonstrated the use of this type of representation for the tracking and recognition of hand gestures <ref> [2] </ref>. Like faces, hands are complex objects with both non-rigid and rigid dynamics. Direct use of a 3-D model for recognition has proved difficult for such objects, so we developed a view-based method for representation. <p> However, in practice good 3-D models that are useful for describing image intensity values are rare 2 , so we look to data-driven methods of acquiring object views. As described in <ref> [2] </ref> a simple clustering algorithm can find a set of views that span a training sequence of images, in the sense that for each image in the sequence at least one view is within some threshold similarity to that image. The algorithm is as follows.
Reference: [3] <author> I. Essa and A. Pentland. </author> <title> A vision system for observing and extracting facial action parameters. </title> <booktitle> In Proc. IEEE Conf. Computer Vision and Pattern Recognition, </booktitle> <year> 1994. </year>
Reference-contexts: Terzopoulos and Waters [10] developed a similar method to track linear facial features, estimate corresponding parameters of a three dimensional wireframe face model, and reproduce facial expression. A significant limitation of these systems is that successful tracking requires facial markings. Essa and Pentland <ref> [3] </ref> applied optical flow methods (see also Mase [5]) for the passive tracking of facial motion, and integrated the flow measurement method into a dynamic system model. Their method allowed for completely passive estimation of facial expressions, using all the constraints provided by a full 3-D model of facial expression.
Reference: [4] <author> S. Librande. </author> <title> Example-based Character Drawing. </title> <type> S.M. Thesis, </type> <institution> Media Arts and Science/Media Lab, MIT. </institution> <year> 1992 </year>
Reference-contexts: In the neural network field, several successful models of character expression modeling have been developed by Poggio and colleagues. These models apply multidimensional interpolation techniques, using the radial basis function method, to the task of interpolating 2D images of different facial expression. Librande <ref> [4] </ref> and Poggio and Brunelli [9] applied the Radial Basis Function (RBF) method to facial expression modeling, using a line drawing representation of cartoon faces. In this model a small set of canonical expressions is defined, and intermediate expressions constructed via the interpolation technique.
Reference: [5] <author> K. Mase. </author> <title> Recognition of facial expressions for optical flow. </title> <journal> IEICE Transactions, Special Issue on Computer Vision and its Applications, </journal> <volume> E 74(10), </volume> <year> 1991. </year>
Reference-contexts: A significant limitation of these systems is that successful tracking requires facial markings. Essa and Pentland [3] applied optical flow methods (see also Mase <ref> [5] </ref>) for the passive tracking of facial motion, and integrated the flow measurement method into a dynamic system model. Their method allowed for completely passive estimation of facial expressions, using all the constraints provided by a full 3-D model of facial expression.
Reference: [6] <author> S. Pieper, J. Rosen, and D. Zeltzer. </author> <title> Interactive graphics for plastic surgery: A task level analysis and implementation. </title> <booktitle> Proc. Siggraph-92, </booktitle> <pages> pages 127-134, </pages> <year> 1992. </year>
Reference-contexts: In the field of computer graphics, much work has been done on on the 3-D modeling of faces and facial expression. These models focus on the geometric and physical qualities of facial structure. Platt and Badler [7], Pieper <ref> [6] </ref>, Waters [11] and others have developed models of facial structure, skin dynamics, and muscle connections, respectively, based on available anatomical data. These models provide strong constraints for the tracking of feature locations on a face. <p> This model is based on the mesh developed by Platt and Badler [7], extended into a topologically invariant physics-based model through the addition of a dynamic skin and muscle model <ref> [6, 11] </ref>. These methods give the facial model an anatomically-based facial structure by modeling facial tissue/skin, and muscle actuators, with a geometric model to describe force-based deformations and control parameters.
Reference: [7] <author> S. M. Platt and N. I. Badler. </author> <title> Animating facial expression. </title> <booktitle> ACM SIGGRAPH Conference Proceedings, </booktitle> <volume> 15(3) </volume> <pages> 245-252, </pages> <year> 1981. </year>
Reference-contexts: In the field of computer graphics, much work has been done on on the 3-D modeling of faces and facial expression. These models focus on the geometric and physical qualities of facial structure. Platt and Badler <ref> [7] </ref>, Pieper [6], Waters [11] and others have developed models of facial structure, skin dynamics, and muscle connections, respectively, based on available anatomical data. These models provide strong constraints for the tracking of feature locations on a face. <p> This model is based on the mesh developed by Platt and Badler <ref> [7] </ref>, extended into a topologically invariant physics-based model through the addition of a dynamic skin and muscle model [6, 11]. These methods give the facial model an anatomically-based facial structure by modeling facial tissue/skin, and muscle actuators, with a geometric model to describe force-based deformations and control parameters.
Reference: [8] <author> T. Poggio and F. Girosi. </author> <title> A theory of networks for approximation and learning. </title> <institution> MIT AI Lab TR-1140, </institution> <year> 1989. </year>
Reference-contexts: The model-driven paradigm is simpler and faster, but the user-driven paradigm yields more detailed and authentic facial expressions. We use the Radial Basis Function (RBF) method presented in <ref> [8] </ref>, and define the interpolated motor controls to be a weighted sum of radial functions centered at each example: Y = i=1 where Y are the muscle states, X are the observed view-model scores, X i are the example scores, G is an RBF (and in our case was simply a <p> the muscle states, X are the observed view-model scores, X i are the example scores, G is an RBF (and in our case was simply a linear ramp G (x) = jjxjj), and the weights c i are computed from the example motor values Y i using the pseudo-inverse method <ref> [8] </ref>. 6 INTERACTIVE ANIMATION SYSTEM The correlation network, RBF interpolator, and facial model described above have been combined into a single system for interactive animation.
Reference: [9] <author> T. Poggio and R. Brunelli, </author> <title> A Novel Approach to Graphics, </title> <institution> MIT AI Lab TR- 1354. </institution> <year> 1992. </year>
Reference-contexts: In the neural network field, several successful models of character expression modeling have been developed by Poggio and colleagues. These models apply multidimensional interpolation techniques, using the radial basis function method, to the task of interpolating 2D images of different facial expression. Librande [4] and Poggio and Brunelli <ref> [9] </ref> applied the Radial Basis Function (RBF) method to facial expression modeling, using a line drawing representation of cartoon faces. In this model a small set of canonical expressions is defined, and intermediate expressions constructed via the interpolation technique.
Reference: [10] <author> D. Terzopoulus and K. Waters. </author> <title> Analysis and synthesis of facial image sequences using physical and anatomical models. </title> <journal> IEEE Trans. PAMI, </journal> <volume> 15(6) </volume> <pages> 569-579, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: These models provide strong constraints for the tracking of feature locations on a face. Williams et. al. [12] developed a method in which explicit feature marks are tracked on a 3-D face by use of two cameras. Terzopoulos and Waters <ref> [10] </ref> developed a similar method to track linear facial features, estimate corresponding parameters of a three dimensional wireframe face model, and reproduce facial expression. A significant limitation of these systems is that successful tracking requires facial markings.
Reference: [11] <author> K. Waters and D. Terzopoulos. </author> <title> Modeling and animating faces using scanned data. </title> <journal> The Journal of Visualization and Computer Animation, </journal> <volume> 2 </volume> <pages> 123-128, </pages> <year> 1991. </year>
Reference-contexts: In the field of computer graphics, much work has been done on on the 3-D modeling of faces and facial expression. These models focus on the geometric and physical qualities of facial structure. Platt and Badler [7], Pieper [6], Waters <ref> [11] </ref> and others have developed models of facial structure, skin dynamics, and muscle connections, respectively, based on available anatomical data. These models provide strong constraints for the tracking of feature locations on a face. <p> This model is based on the mesh developed by Platt and Badler [7], extended into a topologically invariant physics-based model through the addition of a dynamic skin and muscle model <ref> [6, 11] </ref>. These methods give the facial model an anatomically-based facial structure by modeling facial tissue/skin, and muscle actuators, with a geometric model to describe force-based deformations and control parameters.
Reference: [12] <author> L. Williams. </author> <title> Performance-driven facial animation. </title> <booktitle> ACM SIGGRAPH Conference Proceedings, </booktitle> <volume> 24(4) </volume> <pages> 235-242, </pages> <year> 1990. </year>
Reference-contexts: Platt and Badler [7], Pieper [6], Waters [11] and others have developed models of facial structure, skin dynamics, and muscle connections, respectively, based on available anatomical data. These models provide strong constraints for the tracking of feature locations on a face. Williams et. al. <ref> [12] </ref> developed a method in which explicit feature marks are tracked on a 3-D face by use of two cameras. Terzopoulos and Waters [10] developed a similar method to track linear facial features, estimate corresponding parameters of a three dimensional wireframe face model, and reproduce facial expression.
References-found: 12

