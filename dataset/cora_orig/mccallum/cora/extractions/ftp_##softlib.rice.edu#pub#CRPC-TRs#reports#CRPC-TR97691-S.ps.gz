URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR97691-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: abate@ticam.utexas.edu  fbischof,rohg@mcs.anl.gov  carle@cs.rice.edu  
Title: Algorithms and Design for a Second-Order Automatic Differentiation Module (to be presented at ISSAC 97)  
Author: Jason Abate Christian Bischof and Lucas Roh Alan Carle 
Date: May 1997  
Address: Austin  
Affiliation: Texas Institute for Computational Mathematics University of Texas at  Mathematics and Computer Science Division Argonne National Laboratory  Center for Research on Parallel Computation Rice University  
Pubnum: CRPC-TR97691-S  
Abstract: This paper describes approaches to computing second-order derivatives with automatic differentiation (AD) based on the forward mode and the propagation of univariate Taylor series. Performance results are given which show the speedup possible with these techniques. We also describe a new source transformation AD module for computing second-order derivatives of C and Fortran codes and the underlying infrastructure used to create a language-independent translation tool. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anderson, E., Bai, Z., Bischof, C., Demmel, J., Dongarra, J., DuCroz, J., Greenbaum, A., Ham-marling, S., McKenney, A., Ostrouchov, S., and Sorensen, D. </author> <title> LAPACK User's Guide Release 2.0. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1994. </year>
Reference-contexts: For n independent variables, gradients are stored in arrays of length n and Hessians, due to their symmetric nature, are stored using the LAPACK <ref> [1] </ref> packed symmetric scheme, which reduces the storage requirements from n 2 to 1 2 n (n + 1). The cost of computing a full Hessian using the forward mode is O (n 2 ).
Reference: [2] <author> Averick, B., Mor e, J., Bischof, C., Carle, A., and Griewank, A. </author> <title> Computing large sparse Jacobian matrices using automatic differentiation. </title> <journal> SIAM Journal on Scientific Computing 15, </journal> <volume> 2 (1994), </volume> <pages> 285-294. </pages>
Reference-contexts: These costs are summarized in Table 1. In the case of large Hessians and relatively small values of n v or n w , the saving can be significant. Additionally, the coloring techniques which have been applied to structured Jacobians <ref> [2] </ref> can be applied to Hessians for a significant savings.
Reference: [3] <author> Bischof, C., Carle, A., Khademi, P., and Mauer, A. ADIFOR 2.0: </author> <title> Automatic differentiation of Fortran 77 programs. </title> <booktitle> IEEE Computational Science & Engineering 3, 3 (1996), </booktitle> <pages> 18-32. </pages>
Reference-contexts: For details on the canonicalization and analysis phase in ADIC and ADIFOR, see <ref> [6, 3] </ref>. The front-end then collects code fragments, which may range in size from single assignment statements to entire subroutines, and passes them to the augmentation module. Figure 4 shows the AIF tree corresponding to the sample Fortran statement above. The first line in each node is the node type.
Reference: [4] <author> Bischof, C., Corliss, G., and Griewank, A. </author> <title> Structured second- and higher-order derivatives through uni-variate Taylor series. </title> <booktitle> Optimization Methods and Software 2 (1993), </booktitle> <pages> 211-232. </pages>
Reference-contexts: There is no "best" approach to computing Hessians the most efficient approach to computing second-order derivatives depends on the specifics of the code and, to a lesser extent, the target platform on which the code will be run <ref> [4, 8] </ref>. In all cases, however, derivative values are computed to machine precision, without the roundoff errors inherent in divided difference techniques. AD via source transformation provides great flexibility in implementing sophisticated algorithms which exploit the associativity of the chain rule of calculus (see [6] for a discussion). <p> variables, n v and n w are the number of columns of v and w, respectively. 2.2 Taylor Series Strategies As an alternative to the forward mode propagation of gra dients and Hessians, we can propagate two-term univariate Taylor series expansions about each of the non-zero directions in the Hessian <ref> [4] </ref>. To compute derivatives at a point x o in the direction u, we consider f as a scalar function f (x o + tu) of t.
Reference: [5] <author> Bischof, C., Green, L., Haigler, K., and Knauff, T. </author> <title> Parallel calculation of sensitivity derivatives for aircraft design using automatic differentiation. </title> <booktitle> In Proceedings of the 5th AIAA/NASA/USAF/ISSMO Symposium on Multidisciplinary Analysis and Optimization, AIAA 94-4261 (1994), American Institute of Aeronautics and Astronautics, </booktitle> <pages> pp. 73-84. </pages>
Reference-contexts: This allows very large Hessians, which can easily overwhelm the available memory, to be computed in a stripmined fashion by by partitioning the expansion directions and computing them independently with multiple sweeps through the code in a fashion that is similar to the stripmining technique de scribed in <ref> [5] </ref>. 2 2.3 Preaccumulation The associativity of the chain rule allows derivative propagation to be performed at arbitrary levels of abstraction. At the simplest, the forward mode works at the scope of a single binary operation.
Reference: [6] <author> Bischof, C., Roh, L., and Mauer, A. </author> <title> ADIC | An extensible automatic differentiation tool for ANSI-C. </title> <type> Preprint ANL/MCS-P626-1196, </type> <year> 1996. </year>
Reference-contexts: In all cases, however, derivative values are computed to machine precision, without the roundoff errors inherent in divided difference techniques. AD via source transformation provides great flexibility in implementing sophisticated algorithms which exploit the associativity of the chain rule of calculus (see <ref> [6] </ref> for a discussion). Unfortunately, the development of robust source transformation tools is a substantial effort. ADIFOR and ADIC, source transformation tools for Fortran and C, both implement relatively simple algorithms for propagating derivatives. <p> For details on the canonicalization and analysis phase in ADIC and ADIFOR, see <ref> [6, 3] </ref>. The front-end then collects code fragments, which may range in size from single assignment statements to entire subroutines, and passes them to the augmentation module. Figure 4 shows the AIF tree corresponding to the sample Fortran statement above. The first line in each node is the node type.
Reference: [7] <author> Bischof, C. H., and Haghighat, M. R. </author> <title> On hierarchical differentiation. </title> <type> Preprint ANL/MCS-P571-0396, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1996. </year>
Reference-contexts: By expanding the scope to a higher level, such as an assignment statement, a loop body or a subroutine, it is possible to decrease the amount of work necessary to propagate derivatives, as shown in <ref> [7, 9] </ref>. The preaccumulation technique computes the gradient and Hessian of the variable on the left side of the assignment statement in two steps.
Reference: [8] <author> Griewank, A. </author> <title> Some bounds on the complexity of gradients, Jacobians, and Hessians. </title> <type> Preprint MCS-P355-0393, </type> <institution> Mathematics and Computer Science Division, Ar-gonne National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: There is no "best" approach to computing Hessians the most efficient approach to computing second-order derivatives depends on the specifics of the code and, to a lesser extent, the target platform on which the code will be run <ref> [4, 8] </ref>. In all cases, however, derivative values are computed to machine precision, without the roundoff errors inherent in divided difference techniques. AD via source transformation provides great flexibility in implementing sophisticated algorithms which exploit the associativity of the chain rule of calculus (see [6] for a discussion).
Reference: [9] <author> Hovland, P., Bischof, C., Spiegelman, D., and Casella, M. </author> <title> Efficient derivative codes through automatic differentiation and interface contraction: An application in biostatistics. </title> <type> Preprint MCS-P491-0195, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1995. </year> <note> To appear in SIAM J. Scientific Computing 18-4 (July 97). </note>
Reference-contexts: By expanding the scope to a higher level, such as an assignment statement, a loop body or a subroutine, it is possible to decrease the amount of work necessary to propagate derivatives, as shown in <ref> [7, 9] </ref>. The preaccumulation technique computes the gradient and Hessian of the variable on the left side of the assignment statement in two steps.
Reference: [10] <author> Moore, R. E. </author> <title> Interval Analysis. </title> <publisher> Prentice-Hall, </publisher> <year> 1966. </year>
Reference-contexts: As with the forward mode, simple rules specify the propagation of the expansions for all arithmetic and intrinsic operators <ref> [10, 13] </ref>. The Taylor series approach can compute any set of Hessian entries without computing the entire Hessian. This technique is ideal for sparse Hessians when the sparsity pattern is known in advance and for situations where only certain elements (such as the diagonal entries) are desired.
Reference: [11] <author> Parr, T. </author> <title> SORCERER | a source-to-source translator generator. </title> <type> Preprint AHPCRC 93-094, </type> <institution> Army High Performance Computing Research Center, University of Minnesota, </institution> <year> 1993. </year>
Reference-contexts: In addition to the AIF trees, the front-end also passes a set of bindings to specify global information. This includes information about the desired augmentation strategy and the maximum number of independent variables. The augmentation module then modifies the tree to propagate derivative values. The SORCERER tree parser generator <ref> [11, 12] </ref> is used to analyze and modify these trees, along with a set of utility routines provided in the AIF library which assist the augmentation process.
Reference: [12] <author> Parr, T. J. </author> <title> Language Translation using PCCTS and C++: A Reference Guide. </title> <publisher> Automata Publishing Co., </publisher> <year> 1997. </year>
Reference-contexts: In addition to the AIF trees, the front-end also passes a set of bindings to specify global information. This includes information about the desired augmentation strategy and the maximum number of independent variables. The augmentation module then modifies the tree to propagate derivative values. The SORCERER tree parser generator <ref> [11, 12] </ref> is used to analyze and modify these trees, along with a set of utility routines provided in the AIF library which assist the augmentation process.
Reference: [13] <author> Rall, L. B. </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> vol. 120 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: As with the forward mode, simple rules specify the propagation of the expansions for all arithmetic and intrinsic operators <ref> [10, 13] </ref>. The Taylor series approach can compute any set of Hessian entries without computing the entire Hessian. This technique is ideal for sparse Hessians when the sparsity pattern is known in advance and for situations where only certain elements (such as the diagonal entries) are desired.
Reference: [14] <author> Shubin, G. R., Stephens, A. B., Glaz, H. M., Wardlaw, A. B., and Hackerman, L. B. </author> <title> Steady shock tracking, Newton's method, and the supersonic blunt body problem. </title> <journal> SIAM J. on Sci. and Stat. Computing 3, </journal> <month> 2 (June </month> <year> 1982), </year> <pages> 127-144. 7 </pages>
Reference-contexts: We believe that such context-sensitive strategies to be crucial for future improvement of AD tools. 3 Hessian Performance on a CFD Code Hessian code was generated for a steady shock tracking code provided by Greg Shubin, Boeing Computer Services, Seat tle, Washington <ref> [14] </ref>. Due to memory constraints, a 20 fi 20 section of the full, 190 fi 190 Hessian was computed for each code with 20 independent variables. of the 190 dependent variables. The section of the Hessian being studied exhibits some sparsity, with 72 nonzero entries on or above the diagonal.
References-found: 14

