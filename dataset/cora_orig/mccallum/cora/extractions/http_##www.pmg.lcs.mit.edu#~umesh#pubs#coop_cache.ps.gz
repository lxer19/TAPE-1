URL: http://www.pmg.lcs.mit.edu/~umesh/pubs/coop_cache.ps.gz
Refering-URL: http://www.pmg.lcs.mit.edu/~umesh/pubs/
Root-URL: 
Title: Fragment Reconstruction: Providing Global Cache Coherence in a Transactional Storage System  
Author: Atul Adya Miguel Castro Barbara Liskov Umesh Maheshwari Liuba Shrira 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: Cooperative caching is a promising technique to avoid the increasingly formidable disk bottleneck problem in distributed storage systems; it reduces the number of disk accesses by servicing client cache misses from the caches of other clients. However, existing cooperative caching techniques do not provide adequate support for fine-grained sharing. In this paper, we describe a new storage system architecture, split caching, and a new cache coherence protocol, fragment reconstruction, that combine cooperative caching with efficient support for fine-grained sharing and transactions. We also present the results of performance studies that show that our scheme introduces little overhead over the basic cooperative caching mechanism and provides better performance when there is fine-grained sharing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Adya, R. Gruber, B. Liskov, and U. Maheshwari. </author> <title> Efficient Optimistic Concurrency Control Using Loosely Synchronized Clocks. </title> <booktitle> In SIGMOD, </booktitle> <year> 1995. </year>
Reference-contexts: 1 Introduction This paper describes how to integrate two techniques that have been shown to improve performance in distributed client/server storage systems: cooperative caching [8, 11, 12, 20] and efficient support for fine-grained sharing <ref> [1, 4, 6, 9, 10, 15] </ref>. Cooperative caching combines client caches so that misses at one client can be serviced from caches of other clients. <p> Split caching fetches the containing pages from the cooperative cache thus providing the benefits of the m-cache without the cost of the installation reads. Fragment reconstruction ensures client cache consistency. It supports fine-grained concurrency control techniques such as adaptive call-back locking [4] and optimistic control <ref> [1] </ref>; such techniques improve performance because they avoid conflicts due to false sharing. When a transaction commits, copies of its modified objects in other client caches become obsolete, causing containing pages to become fragments with holes where the obsolete objects are. <p> At this point, the server adds an entry to its directory, indicating that client C is now caching p; the directory keeps track of which pages are cached by each client. 3.2 Transactions and Cache Coherence Transactions are serialized using optimistic concurrency control <ref> [1, 14] </ref>. The client keeps track of objects that are read and modified by its transaction; it sends this information, along with new copies of modified objects, to the servers when it tries to commit the transaction.
Reference: [2] <author> M. Carey et al. </author> <title> A Status Report on the OO7 OODBMS Benchmarking Effort. </title> <booktitle> In OOPSLA Proceedings, </booktitle> <year> 1994. </year>
Reference-contexts: The average fetch latency is determined not only by the latency of each type of fetch but also by the number of fetches of each type. Therefore, we ran the multi-user OO7 benchmark <ref> [2] </ref> in a version of our prototype instrumented to count the number of fetches of each type. The counts obtained were fed to the analytic model and used to predict average fetch latency. These results are presented in Section 5.3. <p> The predicted installation fetch latencies in can significantly speed up installations. 5.3 OO7 Benchmark This section describes experiments that ran the multiuser OO7 benchmark <ref> [2, 5] </ref> in our prototype to determine the distribution of fetches by type. We chose the multiuser OO7 benchmark to generate the workloads, because it is a standard benchmark and it allows us to control the percentages of shared and write accesses.
Reference: [3] <author> M. Carey, M. Franklin, M. Livny, and E. Shekita. </author> <title> Data caching tradeoffs in client-server DBMS architectures. </title> <booktitle> In SIGMOD, </booktitle> <year> 1991. </year>
Reference-contexts: Sending modifications to the client as part of fetching the page from its cache is similar to update propagation <ref> [3] </ref> but is not exactly the same. The server sends the m-cache updates relatively rarely, e.g., the updates may be sent after many updates to the same page have occurred.
Reference: [4] <author> M. Carey, M. Franklin, and M. Zaharioudakis. </author> <title> Fine-Grained Sharing in a Page Server OODBMS. </title> <booktitle> In SIGMOD, </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction This paper describes how to integrate two techniques that have been shown to improve performance in distributed client/server storage systems: cooperative caching [8, 11, 12, 20] and efficient support for fine-grained sharing <ref> [1, 4, 6, 9, 10, 15] </ref>. Cooperative caching combines client caches so that misses at one client can be serviced from caches of other clients. <p> However, existing cooperative caching techniques use pages as the unit of consistency and do not provide adequate support for fine-grained sharing, although studies in databases <ref> [4] </ref> and DSM systems [6, 15, 16] have shown that such support is important to avoid performance problems caused by false sharing. <p> Split caching fetches the containing pages from the cooperative cache thus providing the benefits of the m-cache without the cost of the installation reads. Fragment reconstruction ensures client cache consistency. It supports fine-grained concurrency control techniques such as adaptive call-back locking <ref> [4] </ref> and optimistic control [1]; such techniques improve performance because they avoid conflicts due to false sharing. When a transaction commits, copies of its modified objects in other client caches become obsolete, causing containing pages to become fragments with holes where the obsolete objects are. <p> These studies are complementary to ours; we explain how to extend the benefits of cooperative caching to workloads with fine-grained sharing. We can use the techniques developed in [8, 11, 20] to perform page replacement in the cooperative cache. Cache coherence work in client/server databases <ref> [4, 9] </ref> has addressed the performance problems caused by false sharing. The study by Carey et al. [4] describes a con-currency control and coherence protocol that supports fine-grained sharing efficiently. A coherence protocol using a cache of recent updates similar to the m-cache is studied in [9]. <p> We can use the techniques developed in [8, 11, 20] to perform page replacement in the cooperative cache. Cache coherence work in client/server databases [4, 9] has addressed the performance problems caused by false sharing. The study by Carey et al. <ref> [4] </ref> describes a con-currency control and coherence protocol that supports fine-grained sharing efficiently. A coherence protocol using a cache of recent updates similar to the m-cache is studied in [9]. However, neither of these studies integrates fine-grained sharing support with cooperative caching. <p> Therefore, individual clients still benefit from fine-grained sharing support because they need not evict a page just because some object in that page was modified by another client. We do not evaluate this benefit because that has been done by others <ref> [4] </ref>. Instead, we evaluate the benefit of allowing clients to fetch pages with holes from other client caches. We ran our experiments for a system with 8 clients, 4 each on 2 machines, and a single server on another machine. Each client manages its cache greedily, without global cache coordination.
Reference: [5] <author> Michael J. Carey, David J. DeWitt, and Jeffrey F. Naughton. </author> <title> The OO7 Benchmark. </title> <booktitle> In SIGMOD, </booktitle> <year> 1993. </year>
Reference-contexts: To evaluate the effectiveness of our techniques, we implemented them in Thor [17] and ran a set of experiments using the multi-user OO7 benchmark <ref> [5] </ref>. Our results show that our approach preserves the benefits of both cooperative caching and support for fine-grained sharing: it adds almost no overhead to the basic cooperative caching mechanism, and it substantially improves performance when fine-grained sharing affects the pages that clients fetch from the cooperative cache. <p> The predicted installation fetch latencies in can significantly speed up installations. 5.3 OO7 Benchmark This section describes experiments that ran the multiuser OO7 benchmark <ref> [2, 5] </ref> in our prototype to determine the distribution of fetches by type. We chose the multiuser OO7 benchmark to generate the workloads, because it is a standard benchmark and it allows us to control the percentages of shared and write accesses.
Reference: [6] <author> J. Carter, J. Bennett, and W. Zwaenepoel. </author> <title> Techniques for Reducing Consistency-Related Communication in Distributed Shared Memory Systems. </title> <journal> In ACM Transactions on Computer Systems, </journal> <month> August </month> <year> 1994. </year>
Reference-contexts: 1 Introduction This paper describes how to integrate two techniques that have been shown to improve performance in distributed client/server storage systems: cooperative caching [8, 11, 12, 20] and efficient support for fine-grained sharing <ref> [1, 4, 6, 9, 10, 15] </ref>. Cooperative caching combines client caches so that misses at one client can be serviced from caches of other clients. <p> However, existing cooperative caching techniques use pages as the unit of consistency and do not provide adequate support for fine-grained sharing, although studies in databases [4] and DSM systems <ref> [6, 15, 16] </ref> have shown that such support is important to avoid performance problems caused by false sharing. <p> However, neither of these studies integrates fine-grained sharing support with cooperative caching. DSM is similar to cooperative caching; it allows clients to fetch data from the memory of other clients. Some DSM systems <ref> [6, 15] </ref> provide efficient support for fine-grained sharing. However, these systems do not deal with accesses to large on-disk databases; they assume infinite client caches and they do not address the problems of efficiently updating on-disk data and reducing the latency of capacity misses. Furthermore, they do not support transactions.
Reference: [7] <author> M. Dahlin, C. Mather, R. Wang, T. Anderson, and D. Patterson. </author> <title> A Quantitative Analysis of Cache Policies for Scalable Network File Systems. </title> <booktitle> In SIGMETRICS, </booktitle> <year> 1994. </year>
Reference-contexts: We close with a discussion of what we have accomplished. 2 Related Work Our proposal builds on previous work on cooperative caching and support for fine-grained sharing. Cooperative caching has been studied in several contexts: distributed virtual memory [11], file systems <ref> [8, 7, 20] </ref>, and a transactional database [12]. All studies show that cooperative caching can reduce access latency and improve system scalability in workloads with coarse-grained sharing. These studies are complementary to ours; we explain how to extend the benefits of cooperative caching to workloads with fine-grained sharing.
Reference: [8] <author> M. Dahlin, R. Wang, T. Anderson, and D. Patterson. </author> <title> Cooperative Caching: Using Remote Client Memory to Improve File System Performance. </title> <booktitle> In OSDI, </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction This paper describes how to integrate two techniques that have been shown to improve performance in distributed client/server storage systems: cooperative caching <ref> [8, 11, 12, 20] </ref> and efficient support for fine-grained sharing [1, 4, 6, 9, 10, 15]. Cooperative caching combines client caches so that misses at one client can be serviced from caches of other clients. <p> We close with a discussion of what we have accomplished. 2 Related Work Our proposal builds on previous work on cooperative caching and support for fine-grained sharing. Cooperative caching has been studied in several contexts: distributed virtual memory [11], file systems <ref> [8, 7, 20] </ref>, and a transactional database [12]. All studies show that cooperative caching can reduce access latency and improve system scalability in workloads with coarse-grained sharing. These studies are complementary to ours; we explain how to extend the benefits of cooperative caching to workloads with fine-grained sharing. <p> All studies show that cooperative caching can reduce access latency and improve system scalability in workloads with coarse-grained sharing. These studies are complementary to ours; we explain how to extend the benefits of cooperative caching to workloads with fine-grained sharing. We can use the techniques developed in <ref> [8, 11, 20] </ref> to perform page replacement in the cooperative cache. Cache coherence work in client/server databases [4, 9] has addressed the performance problems caused by false sharing. The study by Carey et al. [4] describes a con-currency control and coherence protocol that supports fine-grained sharing efficiently. <p> We do not attempt to show the benefits of cooperative caching, since that has been shown by others <ref> [8, 11, 12, 20] </ref>. The key performance goals are reductions in client cache miss latency and in the latency to install modified objects. Therefore, we start (Section 5.1) by presenting a simple analytic model of the latency of different types of client and installation fetches.
Reference: [9] <author> A. Delis and N. Roussopoulos. </author> <title> Performance and Scalability of Client-Server Database Architecture. </title> <booktitle> In VLDB, </booktitle> <year> 1992. </year>
Reference-contexts: 1 Introduction This paper describes how to integrate two techniques that have been shown to improve performance in distributed client/server storage systems: cooperative caching [8, 11, 12, 20] and efficient support for fine-grained sharing <ref> [1, 4, 6, 9, 10, 15] </ref>. Cooperative caching combines client caches so that misses at one client can be serviced from caches of other clients. <p> These studies are complementary to ours; we explain how to extend the benefits of cooperative caching to workloads with fine-grained sharing. We can use the techniques developed in [8, 11, 20] to perform page replacement in the cooperative cache. Cache coherence work in client/server databases <ref> [4, 9] </ref> has addressed the performance problems caused by false sharing. The study by Carey et al. [4] describes a con-currency control and coherence protocol that supports fine-grained sharing efficiently. A coherence protocol using a cache of recent updates similar to the m-cache is studied in [9]. <p> The study by Carey et al. [4] describes a con-currency control and coherence protocol that supports fine-grained sharing efficiently. A coherence protocol using a cache of recent updates similar to the m-cache is studied in <ref> [9] </ref>. However, neither of these studies integrates fine-grained sharing support with cooperative caching. DSM is similar to cooperative caching; it allows clients to fetch data from the memory of other clients. Some DSM systems [6, 15] provide efficient support for fine-grained sharing.
Reference: [10] <author> M. Feeley, J. Chase, V. Narasayya, and H. Levy. </author> <title> Integrating Coherency and Recoverability in Distributed Systems. </title> <booktitle> In OSDI, </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction This paper describes how to integrate two techniques that have been shown to improve performance in distributed client/server storage systems: cooperative caching [8, 11, 12, 20] and efficient support for fine-grained sharing <ref> [1, 4, 6, 9, 10, 15] </ref>. Cooperative caching combines client caches so that misses at one client can be serviced from caches of other clients. <p> Furthermore, they do not support transactions. The work closest to ours is the log-based coherence study by Feeley et al. <ref> [10] </ref>, which extends DSM to support transactional updates to a persistent store. One key difference is that they associate mutexes with segments and require segments to be large to reduce the time overhead of acquiring mutexes. <p> This coarse-grained concurrency control can cause severe performance degradation due to lock conflicts in workloads with fine grained sharing. Our fine-grained optimistic concurrency control algorithm avoids these problems. The log-based coherence protocol in <ref> [10] </ref> ensures cache consistency by propagating fine-grained updates to all cached copies of a segment eagerly when a transaction commits, but the authors acknowledge that eager propagation does not scale to a large number of clients. In contrast, our coherence protocol uses a scalable lazy invalidation policy.
Reference: [11] <author> M. Feeley, W. Morgan, F. Pighin, A. Karlin, H. Levy, and C. Thekkath. </author> <title> Implementing Global Memory Management in a Workstation Cluster. </title> <booktitle> In SOSP, </booktitle> <year> 1995. </year>
Reference-contexts: 1 Introduction This paper describes how to integrate two techniques that have been shown to improve performance in distributed client/server storage systems: cooperative caching <ref> [8, 11, 12, 20] </ref> and efficient support for fine-grained sharing [1, 4, 6, 9, 10, 15]. Cooperative caching combines client caches so that misses at one client can be serviced from caches of other clients. <p> Section 5 evaluates the effectiveness of the new approach. We close with a discussion of what we have accomplished. 2 Related Work Our proposal builds on previous work on cooperative caching and support for fine-grained sharing. Cooperative caching has been studied in several contexts: distributed virtual memory <ref> [11] </ref>, file systems [8, 7, 20], and a transactional database [12]. All studies show that cooperative caching can reduce access latency and improve system scalability in workloads with coarse-grained sharing. <p> All studies show that cooperative caching can reduce access latency and improve system scalability in workloads with coarse-grained sharing. These studies are complementary to ours; we explain how to extend the benefits of cooperative caching to workloads with fine-grained sharing. We can use the techniques developed in <ref> [8, 11, 20] </ref> to perform page replacement in the cooperative cache. Cache coherence work in client/server databases [4, 9] has addressed the performance problems caused by false sharing. The study by Carey et al. [4] describes a con-currency control and coherence protocol that supports fine-grained sharing efficiently. <p> We do not attempt to show the benefits of cooperative caching, since that has been shown by others <ref> [8, 11, 12, 20] </ref>. The key performance goals are reductions in client cache miss latency and in the latency to install modified objects. Therefore, we start (Section 5.1) by presenting a simple analytic model of the latency of different types of client and installation fetches.
Reference: [12] <author> M. Franklin, M. Carey, and M. Livny. </author> <title> Global Memory Management in Client-Server DBMS Architectures. </title> <booktitle> In VLDB, </booktitle> <year> 1992. </year>
Reference-contexts: 1 Introduction This paper describes how to integrate two techniques that have been shown to improve performance in distributed client/server storage systems: cooperative caching <ref> [8, 11, 12, 20] </ref> and efficient support for fine-grained sharing [1, 4, 6, 9, 10, 15]. Cooperative caching combines client caches so that misses at one client can be serviced from caches of other clients. <p> We close with a discussion of what we have accomplished. 2 Related Work Our proposal builds on previous work on cooperative caching and support for fine-grained sharing. Cooperative caching has been studied in several contexts: distributed virtual memory [11], file systems [8, 7, 20], and a transactional database <ref> [12] </ref>. All studies show that cooperative caching can reduce access latency and improve system scalability in workloads with coarse-grained sharing. These studies are complementary to ours; we explain how to extend the benefits of cooperative caching to workloads with fine-grained sharing. <p> We do not attempt to show the benefits of cooperative caching, since that has been shown by others <ref> [8, 11, 12, 20] </ref>. The key performance goals are reductions in client cache miss latency and in the latency to install modified objects. Therefore, we start (Section 5.1) by presenting a simple analytic model of the latency of different types of client and installation fetches.
Reference: [13] <author> S. Ghemawat. </author> <title> The Modified Object Buffer: A Storage Management Technique for Object-Oriented Databases. </title> <type> PhD thesis, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <year> 1995. </year>
Reference-contexts: Disk reads are avoided by using the combined memory of the clients as a cooperative cache. Disk writes are avoided by using the entire server main memory as an object cache, called the m-cache <ref> [13, 19] </ref>, in which the server stores new versions of recently modified objects. When a transaction commits, a client sends its modified objects to the server, which stores them in the m-cache replacing any previously cached versions of those objects. <p> Therefore before sending a page to a client in response to a fetch request, the system retrieves new versions of the page's objects from the m-cache and installs them in the page. The m-cache architecture improves the efficiency of disk writes for fine-grained updates <ref> [13, 19] </ref>. It avoids installation reads at commit time and stores modifications in a compact form, since only the modified objects are stored. Nevertheless, installation reads can consume a significant portion of the available disk bandwidth [13, 19]. 4 The Split Caching Architecture The split caching architecture differs from what was <p> The m-cache architecture improves the efficiency of disk writes for fine-grained updates <ref> [13, 19] </ref>. It avoids installation reads at commit time and stores modifications in a compact form, since only the modified objects are stored. Nevertheless, installation reads can consume a significant portion of the available disk bandwidth [13, 19]. 4 The Split Caching Architecture The split caching architecture differs from what was described above in two important ways. First, it uses server memory only for the m-cache; there is no server page cache. Second, it implements cooperative caching.
Reference: [14] <author> R. Gruber. </author> <title> Optimism vs. Locking: A Study of Concurrency Control for Client-Server Object-Oriented Databases. </title> <type> PhD thesis, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <year> 1997. </year>
Reference-contexts: At this point, the server adds an entry to its directory, indicating that client C is now caching p; the directory keeps track of which pages are cached by each client. 3.2 Transactions and Cache Coherence Transactions are serialized using optimistic concurrency control <ref> [1, 14] </ref>. The client keeps track of objects that are read and modified by its transaction; it sends this information, along with new copies of modified objects, to the servers when it tries to commit the transaction.
Reference: [15] <author> P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In ISCA, </booktitle> <year> 1992. </year>
Reference-contexts: 1 Introduction This paper describes how to integrate two techniques that have been shown to improve performance in distributed client/server storage systems: cooperative caching [8, 11, 12, 20] and efficient support for fine-grained sharing <ref> [1, 4, 6, 9, 10, 15] </ref>. Cooperative caching combines client caches so that misses at one client can be serviced from caches of other clients. <p> However, existing cooperative caching techniques use pages as the unit of consistency and do not provide adequate support for fine-grained sharing, although studies in databases [4] and DSM systems <ref> [6, 15, 16] </ref> have shown that such support is important to avoid performance problems caused by false sharing. <p> However, neither of these studies integrates fine-grained sharing support with cooperative caching. DSM is similar to cooperative caching; it allows clients to fetch data from the memory of other clients. Some DSM systems <ref> [6, 15] </ref> provide efficient support for fine-grained sharing. However, these systems do not deal with accesses to large on-disk databases; they assume infinite client caches and they do not address the problems of efficiently updating on-disk data and reducing the latency of capacity misses. Furthermore, they do not support transactions.
Reference: [16] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Henessy. </author> <title> The Directory Based Cache Coherence Protocol for the DASH multiprocessor. </title> <booktitle> In ISCA, </booktitle> <year> 1990. </year>
Reference-contexts: However, existing cooperative caching techniques use pages as the unit of consistency and do not provide adequate support for fine-grained sharing, although studies in databases [4] and DSM systems <ref> [6, 15, 16] </ref> have shown that such support is important to avoid performance problems caused by false sharing.
Reference: [17] <author> B. Liskov, A. Adya, M. Castro, M. Day, S. Ghemawat, R. Gruber, U. Maheshwari, A. Myers, and L. Shrira. </author> <title> Safe and Efficient Sharing of Persistent Objects in Thor. </title> <booktitle> In SIGMOD, </booktitle> <year> 1996. </year>
Reference-contexts: Laziness reduces communication cost when pages are modified repeatedly and it is safe (i.e., causes no loss in reliability) because the m-cache is recoverable from the transaction log. To evaluate the effectiveness of our techniques, we implemented them in Thor <ref> [17] </ref> and ran a set of experiments using the multi-user OO7 benchmark [5]. <p> Split caching allows the server to delay installations for a longer time, thus reducing their number while still avoiding most installation reads by fetching pages from the cooperative cache. 3 Base System Architecture Our work is done in the context of Thor, a client/server object-oriented database system <ref> [17] </ref>. This section describes the system architecture before we extended it to support split caching and fragment reconstruction. Servers provide persistent storage for objects and clients cache copies of these objects. Applications run at the clients and interact with the system by making calls on methods of cached objects. <p> P (unswiz) is the cost of unswizzling. Like many other object-oriented databases, Thor uses pointer swizzling <ref> [17] </ref> to make code at clients run faster: persistent references stored in cached copies of objects are replaced by virtual memory pointers before they are used. Swiz-zled references in a page must be unswizzled into persistent references before the page can be shipped to another client.
Reference: [18] <author> J. O'Toole and L. Shrira. </author> <title> Shared data management needs adaptive methods. </title> <booktitle> In Proceedings of IEEE Workshop on Hot Topics in Operating Systems, </booktitle> <year> 1995. </year>
Reference-contexts: In contrast, our coherence protocol uses a scalable lazy invalidation policy. Furthermore, like other DSM systems, their system assumes infinite client caches. The study presented in <ref> [18] </ref> proposes sending pages from clients to the server at commit time to avoid installation reads.
Reference: [19] <author> James O'Toole and Liuba Shrira. </author> <title> Opportunistic Log: Efficient Installation Reads in a Reliable Object Server. </title> <booktitle> In OSDI, </booktitle> <year> 1994. </year>
Reference-contexts: Disk reads are avoided by using the combined memory of the clients as a cooperative cache. Disk writes are avoided by using the entire server main memory as an object cache, called the m-cache <ref> [13, 19] </ref>, in which the server stores new versions of recently modified objects. When a transaction commits, a client sends its modified objects to the server, which stores them in the m-cache replacing any previously cached versions of those objects. <p> Therefore before sending a page to a client in response to a fetch request, the system retrieves new versions of the page's objects from the m-cache and installs them in the page. The m-cache architecture improves the efficiency of disk writes for fine-grained updates <ref> [13, 19] </ref>. It avoids installation reads at commit time and stores modifications in a compact form, since only the modified objects are stored. Nevertheless, installation reads can consume a significant portion of the available disk bandwidth [13, 19]. 4 The Split Caching Architecture The split caching architecture differs from what was <p> The m-cache architecture improves the efficiency of disk writes for fine-grained updates <ref> [13, 19] </ref>. It avoids installation reads at commit time and stores modifications in a compact form, since only the modified objects are stored. Nevertheless, installation reads can consume a significant portion of the available disk bandwidth [13, 19]. 4 The Split Caching Architecture The split caching architecture differs from what was described above in two important ways. First, it uses server memory only for the m-cache; there is no server page cache. Second, it implements cooperative caching.
Reference: [20] <author> P. Sarkar and J. Hartman. </author> <title> Efficient Cooperative Caching Using Hints. </title> <booktitle> In OSDI, </booktitle> <year> 1996. </year>
Reference-contexts: 1 Introduction This paper describes how to integrate two techniques that have been shown to improve performance in distributed client/server storage systems: cooperative caching <ref> [8, 11, 12, 20] </ref> and efficient support for fine-grained sharing [1, 4, 6, 9, 10, 15]. Cooperative caching combines client caches so that misses at one client can be serviced from caches of other clients. <p> We close with a discussion of what we have accomplished. 2 Related Work Our proposal builds on previous work on cooperative caching and support for fine-grained sharing. Cooperative caching has been studied in several contexts: distributed virtual memory [11], file systems <ref> [8, 7, 20] </ref>, and a transactional database [12]. All studies show that cooperative caching can reduce access latency and improve system scalability in workloads with coarse-grained sharing. These studies are complementary to ours; we explain how to extend the benefits of cooperative caching to workloads with fine-grained sharing. <p> All studies show that cooperative caching can reduce access latency and improve system scalability in workloads with coarse-grained sharing. These studies are complementary to ours; we explain how to extend the benefits of cooperative caching to workloads with fine-grained sharing. We can use the techniques developed in <ref> [8, 11, 20] </ref> to perform page replacement in the cooperative cache. Cache coherence work in client/server databases [4, 9] has addressed the performance problems caused by false sharing. The study by Carey et al. [4] describes a con-currency control and coherence protocol that supports fine-grained sharing efficiently. <p> We do not attempt to show the benefits of cooperative caching, since that has been shown by others <ref> [8, 11, 12, 20] </ref>. The key performance goals are reductions in client cache miss latency and in the latency to install modified objects. Therefore, we start (Section 5.1) by presenting a simple analytic model of the latency of different types of client and installation fetches.
Reference: [21] <institution> Seagate Technology. </institution> <address> http://www.seagate.com, March 1997. </address>
Reference-contexts: The network times are for a fast implementation of TCP over ATM, U-Net [22], and the disk times are for a modern disk, a Barracuda 4 <ref> [21] </ref>. Note that the y-axis is broken to represent the true disk times. Fetches from disk have the same latency with or without our techniques, as do fetches of complete pages from other client caches.
Reference: [22] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In SOSP, </booktitle> <year> 1996. </year>
Reference-contexts: The network times are for a fast implementation of TCP over ATM, U-Net <ref> [22] </ref>, and the disk times are for a modern disk, a Barracuda 4 [21]. Note that the y-axis is broken to represent the true disk times. Fetches from disk have the same latency with or without our techniques, as do fetches of complete pages from other client caches.
References-found: 22

