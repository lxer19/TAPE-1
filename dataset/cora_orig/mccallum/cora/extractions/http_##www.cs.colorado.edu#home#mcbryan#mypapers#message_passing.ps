URL: http://www.cs.colorado.edu/home/mcbryan/mypapers/message_passing.ps
Refering-URL: http://www.cs.colorado.edu/home/mcbryan/mypapers.html
Root-URL: http://www.cs.colorado.edu
Email: Email: mcbryan@cs.colorado.edu  
Title: An Overview of Message Passing Environments  
Author: Oliver A. McBryan 
Note: Research supported in part by NSF Grand Challenges Applications Group grant ASC-9217394 and by NASA HPCC Group Grant NAG5-2218. To appear in Parallel Computing, April 1994.  
Address: Boulder, CO 80309.  
Affiliation: Dept. of Computer Science University of Colorado  
Abstract: In this paper we provide an introduction to MPP systems in general. We then introduce current MPP message passing interfaces, by tracing their historical development over the last 10 years. In addition to their use within a single MPP architecture, we discuss the use of message passing systems to interconnect more loosely coupled processors in heterogeneous environments. Finally we review the development of "portability platforms" - message passing systems that have been devised solely to allow portability of message passing programs between different systems. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. Flynn, </author> <title> "Some Computer Organizations and Their Effectiveness", </title> <journal> IEEE Transaction on Computer C-21 pp 948-60. </journal>
Reference: 2. <author> A. Kolawa, B. Zimmerman, </author> <title> "CrOS III Manual", </title> <address> Caltech C3P-253, </address> <year> 1986. </year>
Reference-contexts: All user communication with the hypercube occurred via the IH. The software environment of the system consisted of standard - 14 - Fortran 77 and C compilers, plus a library of communication calls, known as the Crystalline Operating System or CROS <ref> [2] </ref>. 3.2.1. Caltech Hypercube Programming: CROS Assignment of a numbering scheme for nodes of an MPP is a necessary prelude to any communication operations. The topology of an architecture usually suggests natural numbering schemes. For example, in a two-dimensional grid architecture, coordinate pairs provide a natural numbering system for nodes.
Reference: 3. <author> J. Seizovic, </author> <title> "The Reactive Kernel", </title> <address> Caltech CS-TR-88-10, </address> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: Intel iPSC1 Programming: NX1 While the underlying hardware of the Intel iPSC1 was fundamentally similar to the Caltech Hypercube, the software environment developed - known as the NX1 operating system - was fundamentally different. The system was based on the Reactive Kernel <ref> [3] </ref>, also developed at Caltech. We highlight key developments from the Reactive Kernel paradigm as these have all become important aspects of modern message passing systems. All of the key communication routines are shown in Table 4, and their purpose is described here.
Reference: 4. <author> P. Pierce, </author> <title> "The NX/2 Operating System", </title> <booktitle> Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <month> January </month> <year> 1988. </year>
Reference: 5. <author> P. Pierce, </author> <title> "The NX Message Passing Interface", </title> <note> later in this volume. </note>
Reference: 6. <author> M. Schmidt-Voigt, </author> <title> "Efficient Parallel Communication with the nCUBE 2S Processor", </title> <note> later in this volume. </note>
Reference-contexts: In the nCUBE2, messages travel at about 2.75MB/sec and message startup time is about 150ms (there is an extra 2ms overhead for each intermediate node traversed, up to a maximum therefore of 26ms) 3.5.1. nCUBE PSE The nCUBE Parallel Software Environment (PSE) provides a set of communication primitives <ref> [6] </ref> similar to those of Intel NX. Communication routines use a process id that combines both the processor location and the process number on that processor into a 32-bit integer. Messages carry an integer type, allowed in the range [0,32,767]. The basic communication calls are nwrite and nread.
Reference: 7. <author> V. Bala, J. Bruck, R. Bryant, R. Cypher, P. de Jong, P. Elustondo, D. Frye, A. Ho, C.- T. Ho, G. Irwin, S. Kipnis, R. Lawrence, M. Snir, </author> <title> "The IBM External User Interface for Scalable Parallel Systems", </title> <note> later in this volume. </note>
Reference-contexts: However we will use this opportunity to note here several of these because of particularly interesting features. 4.1. IBM EUI The IBM External User Interface (EUI) is the message passing system for the IBM SP MPP computer series <ref> [7] </ref>. EUI is also designed to run on a loosely coupled set of workstations such as the RS/6000. EUI supports both blocking and non-blocking I/O, and the usual forms of collective communication. The number of tasks in a job is fixed.
Reference: 8. <author> E. Barton, J. Cownie, M. McLaren, </author> <title> "Message Passing on the Meiko CS-2", </title> <note> later in this proceedings. </note>
Reference-contexts: These systems are typical message systems, and the most recent product, the CS-2, is among the most powerful MPP systems developed todate. - 23 - CS communication, outlined in Table 7, differs dramatically from other systems in that it is based on a communications name space <ref> [8] </ref>. Processes create virtual communication objects called transports. Transports become useful only when registered with a global name server - implemented by the function csnregname.
Reference: 9. <author> K. Solchenbach, U. Trottenberg, </author> <title> "SUPRENUM - System Essentials and Grid Applications", </title> <journal> Parallel Computing, </journal> <volume> 7, </volume> <publisher> North Holland, </publisher> <year> 1988. </year>
Reference-contexts: The disadvantage is that SUPRENUM message passing programs are then not completely portable. SUPRENUM is unique in providing a sophisticated high-level library interface to the communication system <ref> [9] </ref>. The library supports a range of 2D and 3D grid-oriented operations that largely shield a numerical user from dealing with the communication system directly. In addition to providing powerful programming tools, such systems deliver the possibility of substantial program portability across architectures that support the common set of primitives.
Reference: 10. <institution> Thinking Machines Corporation, </institution> <type> Connection Machine CM-5 Technical Summary. </type> <month> Nov </month> <year> 1993. </year>
Reference-contexts: Thinking Machines CMMD The Thinking Machines Corporation (TMC) Connection Machine CM-5 supports two distinct programming models. The simplest and most elegant is CMF - Connection Machine Fortran - which provides a Fortran 90 style SIMD programming system <ref> [10] </ref>. This is clearly the method of choice for those applications that are representable as SIMD processes in an efficient way. For truly MIMD applications it is necessary to write message passing programs, which are similar in style to Intel NX programs.
Reference: 11. <institution> Thinking Machines Corporation, </institution> <note> CMMD Reference Manual V 3.0, </note> <month> May </month> <year> 1993. </year>
Reference: 12. <author> L. Tucker and A. Mainwaring, </author> <title> "CMMD: Active Messages on the CM-5", </title> <note> later in this volume. </note>
Reference: 13. <author> T. Eicken, D. Culler, S. Goldstein and K. </author> <title> Schauser "Active messages; A mechanism for Integrated Communication and Computation" In Proceedings of the Nineteenth International Symposium on Computer Architecture. </title> <publisher> ACM Press, </publisher> <month> May </month> <year> 1992. </year>
Reference: 14. <author> O. McBryan, </author> <title> "Software Issues at the User Interface," </title> <booktitle> in Frontiers of Supercomputing II: </booktitle> <institution> A National Reassessment, ed. W.L. Thompson, University of Colorado CS Dept. Tech Report CU-CS-527-91 and MIT Press, </institution> <year> 1994, </year> <note> to appear. </note>
Reference-contexts: Injection of an Active Message into the network requires only about 1ms. 4.5. Virtual Shared Memory Several MPP systems such as the Myrias SPS-2, the Evans and Sutherland ES-1, and the Kendall Square KSR1 and KSR2, implement virtual shared memory (VSM) on a distributed memory system <ref> [14] </ref>. In these systems the user no longer needs to employ a message passing library for communication. Instead, all communication is handled by direct virtual memory loads and stores.
Reference: 15. <author> O. </author> <type> McBryan, </type> <institution> Los Alamos National Laboratory Annual Report, </institution> <year> 1983. </year>
Reference: 16. <author> O. McBryan, </author> <title> "Using Supercomputers as Attached Processors", in New Computing Environments: Microcomputers in Large-Scale Scientific Computing, </title> <editor> ed. A. Wouk, </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1987. </year> <month> - 33 </month> - 
Reference-contexts: For example, when communicating from a SUN or VAX to a CRAY, the conversions were always performed on the CRAY, using highly efficient vectorized code, see <ref> [16] </ref> for examples. As a result data conversion was a negligible part of communication cost to CRAY or other fast machines. For debugging purposes, an ascii format (i.e. formatted) data transfer was available in addition to the binary transfer mode. RPROC messages were actually active messages.
Reference: 17. <author> R. Butler and E. Lusk, </author> <title> "User's Guide to the P4 Programming System", </title> <type> Technical Report TM-ANL-92/17, </type> <institution> Argonne National Laboratory, </institution> <year> 1992. </year>
Reference: 18. <author> R.M. Butler, E.L. Lusk, </author> <title> "Monitors, Messages, and Clusters: the p4 Parallel Programming System", </title> <note> later in this volume. </note>
Reference: 19. <author> R. Calkin, R. Hempel, H.-C. Hoppe, P. Wypior, </author> <title> "Portable Programming with the PARMACS Message-Passing Library", </title> <note> later in this volume. </note>
Reference-contexts: P4 provides built-in facilities for process generation using a "procgroup" file. One notable restriction is that P4 message passing is entirely blocking. - 28 - 5.3. PARMACS PARMACS is a macro based message passing system developed initially from the P4 macros <ref> [19] </ref>. Execution starts with a single host process which can spawn node processes using the remote_create () macro which reads a machine dependent input file that specifies the programs to run on nodes, and the pid to assign to each. PARMACS supports both synchronous and asynchronous send routines.
Reference: 20. <author> J. Flower, A. Kolawa, </author> <title> "Express is not just a message passing system", </title> <note> later in this volume. </note>
Reference-contexts: This led to the development of mapping and communication libraries for rings, grids, tori and so on, each optimized to a specific hardware platform. EXPRESS has also begun to tackle the problem of performing parallel I/O as well as dynamic load balancing <ref> [20] </ref>. These latter two are issues that are almost universally ignored by message passing systems, including MPI. - 29 - PVM - Parallel Virtual Machine - represents an extremely successful example of a message passing environment for heterogeneous computing.
Reference: 21. <author> V.S. Sunderam, G.A. Geist, J. Dongarra, R. Manchek, </author> <title> "The PVM Concurrent Computing System: Evolution, Experiences and Trends", </title> <note> later in this volume. </note>
Reference-contexts: PVM, developed at Oak Ridge National Laboratory, uses a simple send/receive library to control the interaction of an arbitrary number of possibly remote computers <ref> [21] </ref>. Early versions of PVM used TCP/IP sockets to implement all communication, but recent versions also provide more efficient implementations for use within an MPP. Conseqwuently PVM has evolved to become a portability platform. <p> Because the PVM library routines are so similar to others we have seen above, we refer here to the detailed paper describing the system <ref> [21] </ref>. PVM differs from most message passing systems by supporting dynamic creation of processes. Because of the intended heterogeneity, PVM uses strongly typed constructs for buffering. The system is remarkably small, ignoring features such as collective communication or process topologies found in other systems.
Reference: 22. <author> A. Skjellum, S.G. Smith, N.E. Doss, A.P. Leung, M. Morari, </author> <title> "The Design and Evolution of Zipcode", </title> <note> later in this volume. </note>
Reference-contexts: PVM latency is typically of order milliseconds, but with the new native messaging implementations, substantial improvement can be expected. 5.6. Zipcode Zipcode is another portability platform that has been used effectively as an experimental laboratory for message passing <ref> [22] </ref>. During its development many new concepts have been tested, and the result has been a considerable influence on MPI design. Zipcode made a special effort to tackle the problem of providing a development environment for parallel libraries.
Reference: 23. <author> N. Carriero, D. Gelertner, T. Mattson, A. Sherman, </author> <title> "The Linda alternative to message-passing systems", </title> <note> later in this volume. </note>
Reference-contexts: Zipcode influence on the design of MPI is particularly apparent in the area of process groups and contexts. 5.7. LINDA Linda <ref> [23] </ref> is a communication system that is in a different category from the other message passing systems we have discussed above. Linda is an associative, virtual shared memory system. The associative memory is called Tuple Space.
Reference: 24. <author> C. Douglas, T. Mattson, M. Schultz, </author> <title> "Parallel Programming Systems for Workstation Clusters", </title> <institution> Yale CS Dept Research Report YALEU/DCS/TR-975, </institution> <month> Aug </month> <year> 1993. </year>
Reference-contexts: All of the required message passing is then generated automatically by the Linda translator. Linda is best suited to those problems that involve many processes per node, ill-defined communication, extensive asynchrony and global communication. Surprisingly, recent work at Yale <ref> [24] </ref> shows that Linda can be fairly competitive with standard message passing systems even for typical numerical applications. 5.8. MPI MPI is a new message passing standard that has evolved from a series of meetings held between November 1992 and January 1994 by the MPI Committee [25].
Reference: 25. <author> D. Walker, </author> <title> "The design of a standard message passing interface for distributed memory concurrent computers", </title> <note> later in this volume. </note>
Reference-contexts: MPI MPI is a new message passing standard that has evolved from a series of meetings held between November 1992 and January 1994 by the MPI Committee <ref> [25] </ref>. The committee consists of members from about 40 institutions and includes almost - 31 - all of the MPP vendors, as well as universities and government laboratories worldwide that are involved in parallel computing.
References-found: 25

