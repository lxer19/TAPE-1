URL: http://www.cs.utexas.edu/users/skumar/reports/vision-report.ps
Refering-URL: http://www.cs.utexas.edu/users/skumar/reports.html
Root-URL: 
Title: Feature Selection and Non-linear Feature Extraction  
Author: Shailesh Kumar 
Address: Austin.  
Affiliation: Department of Electrical and Computer Engineering The University of Texas at  
Abstract: Feature extraction and feature selection are two important tasks in pattern recognition. Classification algorithms like k-nearest neighbors, which are based on the assumption that patterns in the same class are close to each other and those in different classes are far apart (locality property), rely heavily on the quality of the features extracted from the input data. In this work, an objective function, which translates the locality property into a linear, Fisher like criteria, based on within and between class variance of the training data is proposed. This criteria is used for the two tasks of feature selection and feature extraction. Feature selection is done by introducing relevance measures for each input dimension. Feature extraction is defined as a linear combination of a set of non-linear functions. Closed form solutions for both, the relevance measures in feature selection and for the weights of linear combinations for feature extraction task are derived using the Fisher like criteria. Experiments over synthetic and real datasets have been used to highlight the strengths and weaknesses of these methods. Feature selection improves the performance of k-nearest neighbor classifier significantly for both synthetic and real data sets, while the feature extractor is found to be able to extract features from input spaces which are not suitable for simple classifiers like linear discriminant and k-nearest neighbors.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bishop, C. M. </author> <year> (1995). </year> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford: Clarendon Press. </publisher>
Reference-contexts: The set of parameters fl fl for which E (X; fl) is minimum captures the distribution of the patterns in the pattern space among different classes. Mixture of Gaussians, Neural networks, Radial Basis functions, Mixture of Experts are some of the examples of parametric supervised learning methods <ref> (Bishop 1995) </ref>. <p> No direct methods of judging quality of a feature for the task of classification at hand are available. A subset of features is evaluated based on how well the set does on the task <ref> (Bishop 1995) </ref> Although domain knowledge about the pattern/feature space can be used for feature selection and feature extraction, the knowledge may not be accurate or complete (Bishop 1995). <p> A subset of features is evaluated based on how well the set does on the task <ref> (Bishop 1995) </ref> Although domain knowledge about the pattern/feature space can be used for feature selection and feature extraction, the knowledge may not be accurate or complete (Bishop 1995). Thus the challange in pattern classification is that, given the data, first, either using feature extraction i.e. transforming the input patterns into useful features, or using feature selection i.e. picking the best of the features, simplify the problem and then learn a classifier.
Reference: <author> Devijver, P. A., and Schnabel, R. B. </author> <year> (1982). </year> <title> PAttern Recognition: A statistical Approach. </title> <address> Englewoods Cliffs, NJ: </address> <publisher> Printice Hall. </publisher>
Reference: <author> Duda, R. O., and Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: The generic form of a non-parmetric pattern classifier is (x; X). Nearest neighbour classifiers, Parzen windows <ref> (Duda and Hart 1973) </ref> etc fall into this category of pattern classifiers. Essentially a pattern classifier tries to learn the distribution of the patterns in the pattern space among different classes, or in other words, it tries to learn how the pattern space is shared by different classes. <p> In this work we will use Eucledian distance given by: ffi (x; x 0 ) = u t i=1 i ) 2 (1) Note that the distance metric assumes that all the dimensions are equally important. Nearest neighbor classifiers use the above distance metric <ref> (Duda and Hart 1973) </ref>.
Reference: <author> Fukunaga, K. </author> <year> (1990). </year> <title> Introduction to Statistical Pattern Recognition. </title> <address> New York: </address> <publisher> Academic Press Inc. </publisher>
Reference-contexts: More often than not, however, the distribution of patterns in the patten space is not trivial. Non-parametric methods like nearest neighbour, parzen windows and paramteric methods like radial basis functions and mixture of experts rely heavily on the locality property <ref> (Fukunaga 1990) </ref> according to which: 1 Patterns that belong to the same class are close to each other and those in differnt classes are relatively farther away, according to some distance metric. Patterns in the real world do not necessarily have the locality property.
Reference: <author> Hand, D. J. </author> <year> (1981). </year> <title> Discrimination and Classification. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference: <author> Haykin, S. </author> <year> (1994). </year> <title> Neural Networks: A Comprehensive Foundation. </title> <address> New York: </address> <publisher> Macmillan. </publisher>
Reference-contexts: Each one of these methods have their own set of parametres to be learned, for example in mixture of gaussians, the means, variances and weights of all the gaussian kernels are the parameters while in a neural network the weights of the links and the biases are the parameters <ref> (Haykin 1994) </ref>. Another class of supervised learning paradigm is non-parametric approaches where no parametric form of the calssifier is assumed and there are no parameters to train as such but, instead the data set X of labelled examples is used directly to make classification decisions for novel points.
Reference: <author> Siedlecki, W., and Sklansky, J. </author> <year> (1988). </year> <title> On automatic feature selection. </title>
Reference: <author> Zwillinger, D. </author> <year> (1996). </year> <title> Standard Mathematical Tables and Formulae. </title> <address> New York: </address> <publisher> CRC. </publisher>
Reference-contexts: to the between class variance and 2 is a Lagrangian multiplier for imposing the constraint given in equation (32). 3.2 The Optimal Solution Minimization of J (A) is done by setting @J @A =0. @A @B (A) ~ @A m (33) (using identity: @x T P @x = P ) <ref> (Zwillinger 1996) </ref> @(A T I fl @A m (34) @A X n ! (A T ~ ! ) ~ ! 2n (A T ~ ) ~ (35) @W (A) = 2 x2X X n ! (A T ~ ! ) ~ ! (36) Substituting (35) and (36) in (33), we get: <p> = 2 x2X X n ! (A T ~ ! ) ~ ! (36) Substituting (35) and (36) in (33), we get: 1 @J = (1 + ~) !2 X (A T (x))(x) I fl Equating (37) to 0 and using the identity (A T B)B = (B T B)A <ref> (Zwillinger 1996) </ref> (1 + ~) !2 X ((x) T (x))A = I fl or m (39) where fi is an m fi m matrix: fi = (1 + ~) !2 X ((x) T (x)) (40) If fi is non-singular, the optimal solution for A is given by: A = fi 1
References-found: 8

