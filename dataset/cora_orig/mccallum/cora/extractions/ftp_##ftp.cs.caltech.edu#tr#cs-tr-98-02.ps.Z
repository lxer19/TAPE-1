URL: ftp://ftp.cs.caltech.edu/tr/cs-tr-98-02.ps.Z
Refering-URL: ftp://ftp.cs.caltech.edu/tr/INDEX.html
Root-URL: http://www.cs.caltech.edu
Email: fzehra, yaser, magdong@cs.caltech.edu  
Title: No Free Lunch for Early Stopping  
Author: Zehra Cataltepe, Yaser S. Abu-Mostafa, Malik Magdon-Ismail 
Date: January 19, 1998  
Address: Pasadena, CA 91125  
Affiliation: Learning Systems Group California Institute of Technology  Caltech  
Pubnum: CS-TR-98-02  
Abstract: We show that, with a uniform prior on hypothesis functions having the same training error, early stopping at some fixed training error above the training error minimum results in an increase in the expected generalization error. We also show that regularization methods are equivalent to early stopping with certain non-uniform prior on the early stopping solutions.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. S. Abu-Mostafa and X. </author> <title> Song (1996), </title> <booktitle> "Bin Model for Neural Networks" in Proceedings of the International Conference on Neural Information Processing, </booktitle> <address> Hong Kong, </address> <year> 1996, </year> <pages> pp. 169-173. </pages>
Reference-contexts: For general hypotheses, the same result holds, but only within a small enough neighborhood of the training error minimum. For classification problems and the bin model <ref> [1] </ref>, the expected generalization error increases regardless of the probability of selection of hypotheses. <p> In other words, if there are infinitely many examples and no other information, the best strategy is to descend on the training error till the minimum. 4 Classification Problems and the Bin Model For classification problems, bin model <ref> [1] </ref> can be utilized to prove that mean generalization error increases as the training error increases. Since the proof does not have any assumptions about the probability distribution on the hypotheses with the same training error, it is worth mentioning here.
Reference: [2] <author> S. Amari, N. Murata, K. Muller, M. Finke, H. H. </author> <title> Yang (1997), "Asymptotic Statistical Theory of Overtraining and Cross-Validation", </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 8, no. 5, </volume> <pages> pp. 985-996. </pages>
Reference-contexts: Sjoberg and Ljung [8] linked early stopping using a validation set to regularization, and showed that emphasizing the validation set too much may result in an unregularized solution. Amari et. al. <ref> [2] </ref> determined the best validation set size in the asymptotic limit and showed that early stopping helps little in this limit even when the best stopping point fl Copyright@1998 Zehra Cataltepe, Available at ftp://ftp.cs.caltech.edu/tr/cs-tr-98-02 1 is known. <p> Early stopping using a validation set has two components that affect its performance: * Validation set size: If the validation set is too small or large, early stopping may result in worse performance than minimizing the training error on all data. Although <ref> [2] </ref> suggests a validation set size, it is valid for very large training sets. A possible remedy for the validation set size problem is using all data for training, and early stopping not based on a validation set, but some other criterion. <p> Another criteria is the level of training error, which we have examined in this paper. * Optimization algorithm: Which hypotheses are visited by the optimization algorithm [8], or the initial hypothesis at which training starts <ref> [2] </ref> highly affect the performance of early stopping. Instead of the optimization algorithm determining which solution should be chosen for early stopping, any prior information about the data should determine how the optimization algorithm should behave.
Reference: [3] <author> P. Baldi and Y. </author> <month> Chauvin </month> <year> (1991), </year> <title> "Temporal Evolution of Generalization during Learning in Linear Networks", </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 589-603. </pages>
Reference-contexts: Amari et. al. [2] determined the best validation set size in the asymptotic limit and showed that early stopping helps little in this limit even when the best stopping point fl Copyright@1998 Zehra Cataltepe, Available at ftp://ftp.cs.caltech.edu/tr/cs-tr-98-02 1 is known. Dodier [6] and Baldi and Chauvin <ref> [3] </ref> investigated the behav-ior of validation curves for linear problems, and the linear auto-association problem respectively. In this paper, we study early stopping at a predetermined training error level.
Reference: [4] <author> C. </author> <title> Bishop (1995), Neural Networks for Pattern Recognition, </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1995. </year>
Reference-contexts: For general hypotheses, the same result holds, but only within a small enough neighborhood of the training error minimum. For classification problems and the bin model [1], the expected generalization error increases regardless of the probability of selection of hypotheses. Regularization methods such as weight decay <ref> [4] </ref> and early stopping using a validation set are equivalent to early stopping at a fixed training error level with a non-uniform probability of selection over hypotheses with the same training error. We will use the following notation and definitions in this paper. <p> For generalized-linear hypotheses functions, when training starts from small weights and a small decent rate is used, the hypotheses visited during the descent usually lies close to weight decay solutions <ref> [4] </ref>. Provided that the validation set is not too large, early stopping using a validation set stops at a hypothesis close to a weight decay solution with a certain weight decay parameter.
Reference: [5] <author> Z. Cataltepe and Y. S. </author> <title> Abu-Mostafa (1994), "Estimating Learning Performance Using Hints", </title> <booktitle> Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <editor> M. Mozer et. al. (Eds.), </editor> <publisher> Lawrence Erlbaum Associates, Publishers, </publisher> <address> Hillsdale, NJ. pp.380-386. </address>
Reference-contexts: A possible remedy for the validation set size problem is using all data for training, and early stopping not based on a validation set, but some other criterion. For example, if prior information or hints about the target function, such as, invariances <ref> [5] </ref>, monotonicity [7] etc. are known, the hint error can be monitored while descending on the training error and training can be stopped according to the hint error.
Reference: [6] <author> R. </author> <month> Dodier </month> <year> (1996), </year> <title> "Geometry of Early Stopping in Linear Networks" In G. </title> <editor> Tesauro, D. S. Touretzky and T.K. Leen (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Amari et. al. [2] determined the best validation set size in the asymptotic limit and showed that early stopping helps little in this limit even when the best stopping point fl Copyright@1998 Zehra Cataltepe, Available at ftp://ftp.cs.caltech.edu/tr/cs-tr-98-02 1 is known. Dodier <ref> [6] </ref> and Baldi and Chauvin [3] investigated the behav-ior of validation curves for linear problems, and the linear auto-association problem respectively. In this paper, we study early stopping at a predetermined training error level.
Reference: [7] <author> J. Sill and Y. S. </author> <title> Abu-Mostafa (1997), "Monotonicity Hints", </title> <editor> in M. Mozer, M. Jordan and T. Petsche (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press, pp.634-640. </publisher>
Reference-contexts: A possible remedy for the validation set size problem is using all data for training, and early stopping not based on a validation set, but some other criterion. For example, if prior information or hints about the target function, such as, invariances [5], monotonicity <ref> [7] </ref> etc. are known, the hint error can be monitored while descending on the training error and training can be stopped according to the hint error.
Reference: [8] <author> J. Sjoberg and L. </author> <title> Ljung (1995), "Overtraining, regularization, and searching for a minimum, with application to neural networks", </title> <journal> Int. J. Control, </journal> <volume> vol. 62, no. 6, </volume> <pages> pp. 1391-1407. </pages>
Reference-contexts: Early stopping has been studied by Wang et. al. [9] who analyzed the average optimal stopping time for generalized-linear hypotheses and introduced and examined the effective size of the learning machine as training proceeds. Sjoberg and Ljung <ref> [8] </ref> linked early stopping using a validation set to regularization, and showed that emphasizing the validation set too much may result in an unregularized solution. <p> Another criteria is the level of training error, which we have examined in this paper. * Optimization algorithm: Which hypotheses are visited by the optimization algorithm <ref> [8] </ref>, or the initial hypothesis at which training starts [2] highly affect the performance of early stopping. Instead of the optimization algorithm determining which solution should be chosen for early stopping, any prior information about the data should determine how the optimization algorithm should behave.
Reference: [9] <author> C. Wang, S. S. Venkatesh, J. S. </author> <title> Judd (1994), "Optimal Stopping and Effective Machine Complexity in Learning", </title> <editor> In G. Tesauro, D. S. Touret-zky and T.K. Leen (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher> <pages> 10 </pages>
Reference-contexts: 1 Introduction Early stopping of training is one of the methods that aims to prevent over-training due to powerful hypothesis class, noisy training examples or small training set. Early stopping has been studied by Wang et. al. <ref> [9] </ref> who analyzed the average optimal stopping time for generalized-linear hypotheses and introduced and examined the effective size of the learning machine as training proceeds.
References-found: 9

