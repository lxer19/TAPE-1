URL: http://www.cs.berkeley.edu/~xiaoye/csd-96-919.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~xiaoye/
Root-URL: http://www.cs.berkeley.edu
Title: Sparse Gaussian Elimination on High Performance Computers  
Author: by Xiaoye S. Li Katherine A. Yelick John R. Gilbert 
Degree: 1990 A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA at BERKELEY Committee in charge: James W. Demmel, Chair  Phillip Colella  
Date: 1996  
Address: 1986 M.S., M.A.  
Affiliation: B.S. (Tsinghua University)  (Penn State University)  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R.C. Agarwal, F.G. Gustavson, P. Palkar, and M. Zubair. </author> <title> A performance analysis of the subroutines in the ESSL/LAPACK call conversion interface (CCI). </title> <institution> IBM T.J. Watson Research Center, Yorktown Heights, </institution> <year> 1994. </year>
Reference-contexts: In the inner loops of our sparse code, we call the two dense BLAS-2 routines DTRSV (triangular solve) and DGEMV (matrix-vector multiply) provided in the IBM ESSL library [77], whose BLAS-3 matrix-matrix multiply routine (DGEMM) achieves about 250 Mflops when the dimension of the matrix is larger than 60 <ref> [1] </ref>. In our sparse algorithm, we find that DGEMV typically accounts for more than 80% of the floating-point operations, as depicted in Figure 4.8. This percentage is higher than 95% for many matrices.
Reference: [2] <author> A. V. Aho, M. R. Garey, and J. D. Ullman. </author> <title> The transitive reduction of a directed graph. </title> <journal> SIAM J. Computing, </journal> <volume> 1 </volume> <pages> 131-137, </pages> <year> 1972. </year>
Reference-contexts: The edges in the tree can be succinctly represented by the following parent [fl] vector: parent [j] = min f i &gt; j j l ij 6= 0 g : In graph-theoretic terms, the elimination tree is simply the transitive reduction <ref> [2] </ref> of the directed graph G (L T ), 1 see Liu [83]. It is the minimal subgraph of G that preserves paths and provides the smallest possible description of column dependencies in the Cholesky factor.
Reference: [3] <author> P. R. Amestoy, T. A. Davis, and Iain S. Duff. </author> <title> An approximate minimum degree ordering algorithm. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <note> 1996. To appear. (Also University of Florida TR-94-039). </note>
Reference-contexts: Solution of Ly = b and L T x = y. In the first phase, although it is computationally expensive (NP-hard) to find an optimal P in terms of minimizing fills, many heuristics have been used successfully in practice, such as variants of minimum degree orderings <ref> [3, 10, 38, 52] </ref> and various dissection orderings based on graph partitioning [15, 58, 92] or hybrid approaches [13, 18, 76]. Two important data structures have been introduced in efficient implementations of the Cholesky factorization. One is the elimination tree and another is the supernode.
Reference: [4] <author> P. R. Amestoy and I. S. Duff. </author> <title> Vectorization of a multiprocessor multifrontal code. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 3 </volume> <pages> 41-59, </pages> <year> 1989. </year>
Reference: [5] <author> P. R. Amestoy and I.S. Duff. MUPS: </author> <title> a parallel package for solving sparse unsymmetric sets of linear equations. </title> <type> Technical report, </type> <institution> CERFACS, Toulouse, France, </institution> <year> 1994. </year>
Reference-contexts: More sophisticated parallel schemes were used by Amestoy and Duff [6, 35], which came with nontrivial runtime overhead. Recent submatrix codes include MA48 [33], Amestoy and Duff's symmetric pattern multifrontal code MUPS <ref> [5] </ref>, and Davis and Duff's unsymmetric multifrontal code UMFPACK [21, 23]. Column methods, by contrast, take j as the outer loop for Equation (2.1) and typically use classical partial pivoting. <p> Previous results showed much lower factorization rate because the machines used were relatively slow and the computational kernel in the earlier parallel algorithms was based on Level 1 BLAS. The closest work is the parallel symmetric pattern multifrontal factorization 113 by Amestoy and Duff <ref> [5] </ref>, also on shared memory machines. However, that approach may result in too many nonzeros and so is inefficient for unsymmetric pattern sparse matrices. Another contribution is providing detailed performance analysis and modeling for the underlying algorithm.
Reference: [6] <author> Patrick R. Amestoy. </author> <title> Factorization of large unsymmetric sparse matrices based on a multifrontal approach in a multiprocessor environment. </title> <type> Technical Report TH/PA/91/2, </type> <institution> CERFACS, Toulouse, France, </institution> <month> February </month> <year> 1991. </year> <type> Ph.D thesis. </type>
Reference-contexts: Based on this idea, some variations on the criteria for selecting pivots have been proposed to balance numerical stability and preservation of sparsity. The reader may consult Chapter 7 of Duff et al. [36] for a thorough treatment of this subject. Multifrontal approaches <ref> [6, 21, 32] </ref> are essentially variations of the submatrix methods. At each stage of the elimination, the update operations for the Schur complement are not applied directly to the target columns of the trailing submatrix. <p> Furthermore, working storage management is particularly hard in a parallel formulation, because the stack-based organization used for efficiency in the sequential algorithm severely limits the degree of parallelism. More sophisticated parallel schemes were used by Amestoy and Duff <ref> [6, 35] </ref>, which came with nontrivial runtime overhead. Recent submatrix codes include MA48 [33], Amestoy and Duff's symmetric pattern multifrontal code MUPS [5], and Davis and Duff's unsymmetric multifrontal code UMFPACK [21, 23]. <p> In addition to demonstrating the efficiency of our parallel algorithm on these machines, we also study the (theoretical) upper bound on performance of this algorithm. Several methods have been proposed to perform sparse Cholesky factorization [49, 73, 90] and sparse LU factorization <ref> [6, 57, 65] </ref> on shared memory machines. A common practice is to organize the program as a self-scheduling loop, interacting with a global pool of tasks that are ready to be executed. <p> MA27 [40] MF, LDL T BLAS-1 BLAS-3 Com/HSL s.p.d. Ng & Peyton [89] LL BLAS-3 Pub/Author Shared Memory Algorithms unsym. SuperLU LL, partial BLAS-2.5 Pub/UCB unsym. PARASPAR [112, 113] RL, Markowitz BLAS-1, SD Res/Author sym- MUPS <ref> [6] </ref> MF, threshold BLAS-3 Res/Author pattern unsym. George & Ng [57] RL, partial BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL BLAS-3 Com/SGI Ng & Peyton [73] Pub/Author s.p.d.
Reference: [7] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK Users' Guide, Release 2.0. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1995. </year> <pages> 324 pages. </pages>
Reference-contexts: These workstations provide a cost-effective way to achieve high performance and are more widely available than traditional supercomputers. The emergence of these novel architectures has motivated the redesign of linear algebra software for dense matrices, such as the well-known LAPACK library <ref> [7] </ref>. The earlier algorithms used in LINPACK and EIS-PACK are inefficient because they often spend more time moving data than doing useful floating-point operations. <p> One of the chief improvements of many new algorithms in LA-PACK is to use block matrix operations, whose improved data locality permits exploitation of cache and multiple functional units. Significant performance gains have been observed over the unblocked algorithms <ref> [7] </ref>. The analogous algorithmic improvements are much harder for sparse matrix algorithms, because of their irregular data structures and memory access patterns. This thesis will address this issue by studying one class of such algorithms, sparse LU factorization. <p> Care must be taken to strike a good balance between sustained amount of concurrency and per-processor performance. Before describing the detailed algorithm, we first address the above two issues in the context of our supernode-panel algorithm. 5.2.1 Parallelism In dense linear algebra software, such as LAPACK <ref> [7] </ref>, parallelism can simply rely on the parallel BLAS routines. So the sequential and shared memory parallel code 73 are identical, except that the BLAS implementations differ. However, for sparse matrix factorizations, parallelism in the dense matrix kernels is quite limited, because the dense submatrices are typically small.
Reference: [8] <author> E. Anderson et al. </author> <note> LAPACK User's Guide, Second Edition. SIAM, Philadelphia, </note> <year> 1995. </year>
Reference-contexts: Our techniques are successful in reducing the solution times for this type of problem. For a dense 1000 fi 1000 matrix, our code achieves 117 Mflops. This compares with 168 Mflops reported in the LAPACK manual <ref> [8] </ref> on a matrix of this size. <p> The complete SuperLU package includes condition number estimation, iterative refinement of solutions, and componentwise error bounds for the refined solutions [9]. These are all based on the dense matrix routines in LAPACK <ref> [8] </ref>. In addition, SuperLU includes a Matlab mex-file interface, so that our factor and solve routines can be called as alternatives to those built into Matlab.
Reference: [9] <author> M. Arioli, J. W. Demmel, and I. S. Duff. </author> <title> Solving sparse linear systems with sparse backward error. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 10(2) </volume> <pages> 165-190, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: In addition to the LU factorization algorithm described in this chapter, we have developed a suite of supporting routines to solve general sparse linear systems. The complete SuperLU package includes condition number estimation, iterative refinement of solutions, and componentwise error bounds for the refined solutions <ref> [9] </ref>. These are all based on the dense matrix routines in LAPACK [8]. In addition, SuperLU includes a Matlab mex-file interface, so that our factor and solve routines can be called as alternatives to those built into Matlab.
Reference: [10] <author> C. Ashcraft. </author> <title> Compressed graphs and the minimum degree algorithm. </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 16 </volume> <pages> 1404-1411, </pages> <year> 1995. </year>
Reference-contexts: Solution of Ly = b and L T x = y. In the first phase, although it is computationally expensive (NP-hard) to find an optimal P in terms of minimizing fills, many heuristics have been used successfully in practice, such as variants of minimum degree orderings <ref> [3, 10, 38, 52] </ref> and various dissection orderings based on graph partitioning [15, 58, 92] or hybrid approaches [13, 18, 76]. Two important data structures have been introduced in efficient implementations of the Cholesky factorization. One is the elimination tree and another is the supernode.
Reference: [11] <author> C. Ashcraft and R. Grimes. </author> <title> The influence of relaxed supernode partitions on the multifrontal method. </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 15 </volume> <pages> 291-309, </pages> <year> 1989. </year> <month> 124 </month>
Reference-contexts: But the column ordering is fixed, for sparsity, before numeric factorization; what can we do? In Cholesky factorization, the so-called fundamental supernodes can be made contiguous by permuting the matrix (symmetrically) according to a postorder on its elimination tree <ref> [11] </ref>. <p> For such values of r, the extra storage needed to store the logical zeros is very small for all our test matrices. Artificial supernodes are a special case of relaxed supernodes, which were used in the context of multifrontal methods for symmetric systems <ref> [11, 40] </ref>. Ashcraft and Grimes 21 allow a small number of zeros in the structure of any supernode, thus relaxing the condition that the columns must have strictly nested structures. <p> In what follows, we describe how these upper bounds can facilitate our storage management for the L supernodes. First, we need a notion of fundamental supernode, which was introduced by Ashcraft and Grimes <ref> [11] </ref> for symmetric matrices. In a fundamental supernode, every column except the last is an only child in the elimination tree.
Reference: [12] <author> C. Ashcraft, R. Grimes, J. Lewis, B. Peyton, and H. Simon. </author> <title> Progress in sparse matrix methods for large sparse linear systems on vector supercomputers. </title> <journal> Intern. J. of Supercomputer Applications, </journal> <volume> 1 </volume> <pages> 10-30, </pages> <year> 1987. </year>
Reference-contexts: A supernode is a set of contiguous columns in the Cholesky factor L that share essentially the same sparsity structure. More recently, supernodes have also been introduced in numeric factorization and triangular solution, in order to make better use of vector registers or cache memory. Indeed, supernodal <ref> [12] </ref> and multifrontal [41] elimination allow the use of dense vector operations for nearly all of the floating-point computation, thus reducing the symbolic overhead in numeric factorization to a smaller fraction. Overall, the Megaflop 1 The directed graph of a square matrix has n vertices corresponding to n rows/columns. <p> Supernodes were originally used for sparse Cholesky factorization; the first published results are by Ashcraft, Grimes, Lewis, Peyton, and Simon <ref> [12] </ref>. <p> Ng and Peyton reported that a sparse Cholesky algorithm based on sup-sup updates typically runs 2.5 to 4.5 times as fast as a col-col algorithm. Indeed, supernodes have become a standard tool in sparse Cholesky factorization <ref> [12, 87, 95, 105] </ref>. To sum up, supernodes as the source of updates (line 4) help because: 1. The inner loop (line 6) over rows i has no indirect addressing. (Sparse BLAS-1 is replaced by dense BLAS-1.) 2.
Reference: [13] <author> C. Ashcraft and J. Liu. </author> <title> Robust ordering of sparse matrices using multisection. </title> <type> Technical Report ISSTECH-96-002, </type> <institution> Boeing information and support services, </institution> <year> 1996. </year>
Reference-contexts: phase, although it is computationally expensive (NP-hard) to find an optimal P in terms of minimizing fills, many heuristics have been used successfully in practice, such as variants of minimum degree orderings [3, 10, 38, 52] and various dissection orderings based on graph partitioning [15, 58, 92] or hybrid approaches <ref> [13, 18, 76] </ref>. Two important data structures have been introduced in efficient implementations of the Cholesky factorization. One is the elimination tree and another is the supernode. The elimination tree [100] is defined for the Cholesky factor L. Each node in the tree corresponds to one row/column of the matrix.
Reference: [14] <author> Cleve Ashcraft, S.C. Eisenstat, Joseph Liu, and A.H. Sherman. </author> <title> A comparison of three column-based distributed sparse factorization schemes. </title> <type> Technical Report Research Report YALEU/DCS/RR-810, </type> <institution> Yale University, Department of Computer Science, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: Another is to use a more sophisticated dynamic scheduling algorithm that steals cycles from the idle processors to do useful work. 5.7.1 Independent domains The concept of domains has been widely used in sparse Cholesky factorizations, especially on distributed memory machines <ref> [14, 72, 93, 97] </ref>. A domain refers to a rooted subtree of the elimination tree such that all nodes in this subtree are mapped onto the same processor to factorize. In sparse LU factorization, we may define domains similarly, but we use the column etree.
Reference: [15] <author> S. T. Barnard and H. Simon. </author> <title> A fast multilevel implementation of recursive spectral bisection for partitioning unstructured problems. </title> <editor> In R. F. Sincovec, D. Keyes, M. Leuze, L. Petzold, and D. Reed, editors, </editor> <booktitle> Sixth SIAM conference on parallel processing for scientific computing, </booktitle> <pages> pages 711-718, </pages> <year> 1993. </year>
Reference-contexts: In the first phase, although it is computationally expensive (NP-hard) to find an optimal P in terms of minimizing fills, many heuristics have been used successfully in practice, such as variants of minimum degree orderings [3, 10, 38, 52] and various dissection orderings based on graph partitioning <ref> [15, 58, 92] </ref> or hybrid approaches [13, 18, 76]. Two important data structures have been introduced in efficient implementations of the Cholesky factorization. One is the elimination tree and another is the supernode. The elimination tree [100] is defined for the Cholesky factor L.
Reference: [16] <author> J. Bilmes, K. Asanovic, J. Demmel, D. Lam, and C.-W. Chin. </author> <title> Optimizing matrix multiply using PHiPAC: a portable, high-performance, ansi c coding methodology. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-96-326, </type> <institution> University of Tennessee, Knoxville, </institution> <month> May </month> <year> 1996. </year> <note> (LAPACK Working Note #111). </note>
Reference-contexts: When the vendors do not supply a BLAS library, we report the results from PHiPAC <ref> [16] </ref>, with an asterisk ( fl ) beside such a number. For some machines, PHiPAC is often faster than the vendor-supplied DGEMM. Because of physical memory limits on the Alpha 21064, the Sparc 20 and the UltraSparc-I, some large problems could not be tested.
Reference: [17] <author> A. Bjorck. </author> <title> Numerical Methods for Least Squares Problems. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: For more details and performance issues, we refer readers to the original references and a recent survey by Duff [34]. Besides sparse LU factorizations, Duff also summarized many other advances in sparse numerical linear algebra, including ordering, linear least-squares, and preconditioning. The new book by Bjorck <ref> [17] </ref> contains a complete list of algorithms and software for sparse least-squares problems. The last colum in the table shows the availability of each code. All the serial codes are publically available, and are portable to a majority of uniprocessor platforms. Shared memory codes have achieved reasonable success in portability.
Reference: [18] <author> T. Chan, J. Gilbert, and S.-H. Teng. </author> <title> Geometric spectral partitioning. </title> <type> Technical Report CSL-94-15, </type> <institution> Palo Alto Research Center, Xerox Corporation, California, </institution> <year> 1994. </year>
Reference-contexts: phase, although it is computationally expensive (NP-hard) to find an optimal P in terms of minimizing fills, many heuristics have been used successfully in practice, such as variants of minimum degree orderings [3, 10, 38, 52] and various dissection orderings based on graph partitioning [15, 58, 92] or hybrid approaches <ref> [13, 18, 76] </ref>. Two important data structures have been introduced in efficient implementations of the Cholesky factorization. One is the elimination tree and another is the supernode. The elimination tree [100] is defined for the Cholesky factor L. Each node in the tree corresponds to one row/column of the matrix.
Reference: [19] <author> J. Choi, J. Demmel, I. Dhillon, J. Dongarra, S. Ostrouchov, A. Petitet, K. Stanley, D. Walker, and R. C. Whaley. </author> <title> ScaLAPACK: A portable linear algebra library for distributed memory computers Design issues and performance. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-95-283, </type> <institution> University of Tennessee, Knoxville, </institution> <month> March </month> <year> 1995. </year> <note> (LAPACK Working Note #95). </note>
Reference-contexts: If a block is too large, we further divide it into smaller blocks. Then, we can apply the same block partitioning to the rows of matrix A. * block mapping. The global 2-D block cyclic mapping successfully used in dense algorithms <ref> [19] </ref> may cause serious load imbalance. Instead, we propose a two-phase mapping method as follows. First, we will use the column etree and arithmetic estimate based on the Householder matrix H to find independent domains and assign them to individual processors. We discussed this method in Section 5.7.1. <p> Each code usually works only on one parallel machine. So for distributed memory machines, much work remains to develop reliable, portable, and high performance sparse direct solvers. (This is in contrast to dense matrix problems, for which the ScaLAPACK library is available <ref> [19] </ref>.) In the future, it will be worthwhile to conduct direct comparisons and evaluations of some of these codes on the same machines and for the same input matrices. 122 Matrix Numerical Status Type Name Algorithm Kernel /Source Serial Algorithms unsym. SuperLU LL, partial BLAS-2.5 Pub/UCB unsym.
Reference: [20] <author> Thomas F. Coleman, Anders Edenbrandt, and John R. Gilbert. </author> <title> Predicting fill for sparse orthogonal factorization. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 33 </volume> <pages> 517-532, </pages> <year> 1986. </year>
Reference-contexts: If A is a nonsingular matrix with nonzero diagonal, and L and U are the triangular factors of A represented as above, then Struct (L) Struct (H), and Struct (U ) Struct (R). Theorem 7 <ref> [20] </ref> Suppose A has full (column) rank. If L c is the symbolic factor described above, then Struct (R T ) Struct (L c ).
Reference: [21] <author> T. A. Davis and I. S. Duff. </author> <title> An unsymmetric-pattern multifrontal method for sparse LU factorization. </title> <type> Technical Report RAL 93-036, </type> <institution> Rutherford Appleton Laboratory, Chilton, Didcot, Oxfordshire, </institution> <year> 1994. </year>
Reference-contexts: Based on this idea, some variations on the criteria for selecting pivots have been proposed to balance numerical stability and preservation of sparsity. The reader may consult Chapter 7 of Duff et al. [36] for a thorough treatment of this subject. Multifrontal approaches <ref> [6, 21, 32] </ref> are essentially variations of the submatrix methods. At each stage of the elimination, the update operations for the Schur complement are not applied directly to the target columns of the trailing submatrix. <p> More sophisticated parallel schemes were used by Amestoy and Duff [6, 35], which came with nontrivial runtime overhead. Recent submatrix codes include MA48 [33], Amestoy and Duff's symmetric pattern multifrontal code MUPS [5], and Davis and Duff's unsymmetric multifrontal code UMFPACK <ref> [21, 23] </ref>. Column methods, by contrast, take j as the outer loop for Equation (2.1) and typically use classical partial pivoting. The pivot is chosen from the current column according to numerical considerations alone; the columns may be preordered before factorization to preserve sparsity. <p> SuperLU LL, partial BLAS-2.5 Pub/UCB unsym. UMFPACK <ref> [21, 22] </ref> MF, Markowitz BLAS-3 Pub/netlib MA38 (same as UMFPACK) Com/HSL unsym. MA48 [39] Anal: RL, Markowitz Com/HSL Fact: LL, partial BLAS-1, SD unsym. SPARSE [79] RL, Markowitz Scalar Pub/netlib sym pattern ) ( MA42 [42] MF, threshold Frontal (eqn+element) BLAS-3 BLAS-3 Com/HSL sym.
Reference: [22] <author> T. A. Davis and I. S. Duff. </author> <title> A combined unifrontal/multifrontal method for unsymmet-ric sparse matrices. </title> <type> Technical Report TR-95-020, </type> <institution> Computer and Information Sciences Department, University of Florida, </institution> <year> 1995. </year>
Reference-contexts: SuperLU LL, partial BLAS-2.5 Pub/UCB unsym. UMFPACK <ref> [21, 22] </ref> MF, Markowitz BLAS-3 Pub/netlib MA38 (same as UMFPACK) Com/HSL unsym. MA48 [39] Anal: RL, Markowitz Com/HSL Fact: LL, partial BLAS-1, SD unsym. SPARSE [79] RL, Markowitz Scalar Pub/netlib sym pattern ) ( MA42 [42] MF, threshold Frontal (eqn+element) BLAS-3 BLAS-3 Com/HSL sym.
Reference: [23] <author> Timothy A. Davis. </author> <title> User's guide for the unsymmetric-pattern multifrontal package (UMFPACK). </title> <type> Technical Report TR-93-020, </type> <institution> Computer and Information Sciences Department, University of Florida, </institution> <month> June </month> <year> 1993. </year> <month> 125 </month>
Reference-contexts: More sophisticated parallel schemes were used by Amestoy and Duff [6, 35], which came with nontrivial runtime overhead. Recent submatrix codes include MA48 [33], Amestoy and Duff's symmetric pattern multifrontal code MUPS [5], and Davis and Duff's unsymmetric multifrontal code UMFPACK <ref> [21, 23] </ref>. Column methods, by contrast, take j as the outer loop for Equation (2.1) and typically use classical partial pivoting. The pivot is chosen from the current column according to numerical considerations alone; the columns may be preordered before factorization to preserve sparsity.
Reference: [24] <author> Timothy A. Davis, John R. Gilbert, Esmond Ng, and Barry Peyton. </author> <title> Approximate minimum degree ordering for unsymmetric matrices. </title> <note> Talk presented at XIII Householder Symposium on Numerical Algebra, June 1996. Journal version in preparation. </note>
Reference-contexts: The column minimum degree algorithm used in Matlab [62] is the first efficient implementation of the minimum degree algorithm on A T A without explicitly forming the nonzero structure of A T A. In recent work of Davis et al. <ref> [24] </ref>, better minimum degree algorithms for A T A are under investigation that will improve both fill and runtime. To summarize, in our column factorization methods, the row permutation P is used to maintain numerical stability and is obtained in the course of elimination.
Reference: [25] <author> James W. Demmel, Stanley C. Eisenstat, John R. Gilbert, Xiaoye S. Li, and Joseph W.H. Liu. </author> <title> A supernodal approach to sparse partial pivoting. </title> <type> Technical Report UCB//CSD-95-883, </type> <institution> Computer Science Division, U.C. Berkeley, </institution> <month> July </month> <year> 1995. </year> <note> (LAPACK Working Note #103). </note>
Reference-contexts: These are all based on the dense matrix routines in LAPACK [8]. In addition, SuperLU includes a Matlab mex-file interface, so that our factor and solve routines can be called as alternatives to those built into Matlab. We reported an earlier version of these results in a technical report <ref> [25] </ref>. 67 RS/6000-590 MIPS R8000 Alpha 21164 Matrix Seconds Mflops Seconds Mflops Seconds Mflops 1 Memplus 0.57 3.08 0.71 2.47 0.38 4.58 2 Gemat11 0.27 5.64 0.26 5.87 0.15 10.17 3 Rdist1 0.96 13.47 0.98 13.17 0.55 23.47 4 Orani678 1.11 13.48 1.15 13.01 0.63 23.63 5 Mcfe 0.24 17.42 0.23
Reference: [26] <institution> Guide to DECthreads. Digital Equipment Corporation, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: The system bus operates at 75 MHz which when applied to the 256-bit data path, produces a peak bandwidth of 2.4 GBytes/sec. However, a sustainable bandwidth is 1.6 GBytes/sec. In the parallel program development, we use the pthread interface provided by DECthreads, Digital's multithreading run-time library <ref> [26] </ref>. The pthread interface implements a version of the POSIX 1003.1c API draft standard for multithreaded programming [91]; thus, the code will be easily portable to future systems. Similar to the Solaris threads model, multiple threads execute concurrently within (and share) a single address space.
Reference: [27] <author> J. Dongarra, J. Du Croz, S. Hammarling, and Richard J. Hanson. </author> <title> An Extended Set of FORTRAN Basic Linear Algebra Subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: We hereby confine ourselves only to the most relevant concepts. In Section 3.1, we elaborate on the roles of row interchanges (partial pivoting) and column interchanges to maintain numerical stability and to preserve sparsity. Section 3.2 introduces unsymmetric supernodes, which are essential in order to use higher level BLAS <ref> [27, 28] </ref>. Section 3.3 gives the definition and properties of the column elimination tree, which is an important tool to assist in the sparse LU factorization, particularly in a parallel setting. <p> This reduces indirect addressing, and allows the inner loops to be unrolled. In effect, a sequence of column-column updates is replaced by a supernode-column update (loops 5-9). This so-called "sup-col update" can be implemented using a call to a standard dense BLAS-2 matrix-vector multiplication kernel <ref> [27] </ref>. This idea can be further extended to supernode-supernode updates ("sup-sup update", loops 2-12), which can be implemented using a BLAS-3 dense matrix-matrix kernel [28]. Sup-sup update can reduce memory traffic by an order of magnitude, because a supernode in the cache can participate in multiple column updates.
Reference: [28] <author> J. Dongarra, J. Du Croz, Duff I., and S. Hammarling. </author> <title> A Set of Level 3 Basic Linear Algebra Subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16 </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: We hereby confine ourselves only to the most relevant concepts. In Section 3.1, we elaborate on the roles of row interchanges (partial pivoting) and column interchanges to maintain numerical stability and to preserve sparsity. Section 3.2 introduces unsymmetric supernodes, which are essential in order to use higher level BLAS <ref> [27, 28] </ref>. Section 3.3 gives the definition and properties of the column elimination tree, which is an important tool to assist in the sparse LU factorization, particularly in a parallel setting. <p> This so-called "sup-col update" can be implemented using a call to a standard dense BLAS-2 matrix-vector multiplication kernel [27]. This idea can be further extended to supernode-supernode updates ("sup-sup update", loops 2-12), which can be implemented using a BLAS-3 dense matrix-matrix kernel <ref> [28] </ref>. Sup-sup update can reduce memory traffic by an order of magnitude, because a supernode in the cache can participate in multiple column updates. Ng and Peyton reported that a sparse Cholesky algorithm based on sup-sup updates typically runs 2.5 to 4.5 times as fast as a col-col algorithm.
Reference: [29] <author> J. Dongarra, F. Gustavson, and A. Karp. </author> <title> Implementing linear algebra algorithms for dense matrices on a vector pipeline machine. </title> <journal> SIAM Review, </journal> <volume> 26(1) </volume> <pages> 91-112, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: Six possible permutations of i, j and k are possible in the three nested loops. Dongarra et al. <ref> [29] </ref> studied the performance impact of each permutation for dense LU factorization algorithms on vector pipeline machines. Although the generic algorithm is very simple, significant complications in its actual implementation arise from sparsity, the need for numerical pivoting and diverse computer architectures.
Reference: [30] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and R. J. Hanson. </author> <title> An extended set of basic linear algebra subroutines. </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 14 </volume> <pages> 1-17, 18-32, </pages> <year> 1988. </year>
Reference-contexts: Figure 2.1 sketches a generic left-looking column LU factorization. Notice that the bulk of the numeric computation occurs in column-column updates ("col-col update" on line 5), or, to use BLAS terminology <ref> [30] </ref>, in sparse AXPYs. Column methods have the advantage that preordering the columns for sparsity is completely separate from the factorization, just as in the symmetric positive definite case.
Reference: [31] <author> I. S. Duff, R. Grimes, and J. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 15 </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference-contexts: Unsymmetric supernodes seem harder to characterize, but they also are related to dense submatrices arising from fill. Eisen-stat et al. [45] measured the supernodes according to each definition for 126 unsymmetric matrices from the Harwell-Boeing sparse matrix test collection <ref> [31] </ref> under various column orderings. Table 3.1 tabulates the results from their measurements. It shows, for each definition, the fraction of nonzeros of L that are not in the first column of a supernode; this measures how much row index storage is saved by using supernodes. <p> Some of the matrices are from the Harwell-Boeing collection <ref> [31] </ref>. Many of the larger matrices are from the ftp site maintained by Tim Davis of the University of Florida. Those matrices are as follows. Memplus is a circuit simulation matrix from Steve Hamm of Motorola.
Reference: [32] <author> I. S. Duff and J. K. Reid. </author> <title> The multifrontal solution of unsymmetric sets of linear equations. </title> <journal> SIAM J. Scientific and Statistical Computing, </journal> <volume> 5 </volume> <pages> 633-641, </pages> <year> 1984. </year>
Reference-contexts: Based on this idea, some variations on the criteria for selecting pivots have been proposed to balance numerical stability and preservation of sparsity. The reader may consult Chapter 7 of Duff et al. [36] for a thorough treatment of this subject. Multifrontal approaches <ref> [6, 21, 32] </ref> are essentially variations of the submatrix methods. At each stage of the elimination, the update operations for the Schur complement are not applied directly to the target columns of the trailing submatrix.
Reference: [33] <author> I. S. Duff and J. K. Reid. MA48, </author> <title> a Fortran code for direct solution of sparse un-symmetric linear systems of equations. </title> <type> Technical Report RAL-93-072, </type> <institution> Rutherford Appleton Laboratory, Oxon, UK, </institution> <year> 1993. </year>
Reference-contexts: More sophisticated parallel schemes were used by Amestoy and Duff [6, 35], which came with nontrivial runtime overhead. Recent submatrix codes include MA48 <ref> [33] </ref>, Amestoy and Duff's symmetric pattern multifrontal code MUPS [5], and Davis and Duff's unsymmetric multifrontal code UMFPACK [21, 23]. Column methods, by contrast, take j as the outer loop for Equation (2.1) and typically use classical partial pivoting. <p> The Harwell code MA48 <ref> [33, 39] </ref> employs such a switch to dense code, which has a significant beneficial effect on performance. Where is a good point to switch to dense LU ? Since our algorithm is left-looking, we do not know exactly when the trailing submatrix becomes dense or nearly so.
Reference: [34] <author> Iain S. Duff. </author> <title> Sparse numerical linear algebra: direct methods and preconditioning. </title> <type> Technical Report RAL-TR-96-047, </type> <institution> Rutherford Appleton Laboratory, </institution> <year> 1996. </year>
Reference-contexts: Table 6.1 tabulates these codes. Here we simply highlight the key algorithmic features of each code. For more details and performance issues, we refer readers to the original references and a recent survey by Duff <ref> [34] </ref>. Besides sparse LU factorizations, Duff also summarized many other advances in sparse numerical linear algebra, including ordering, linear least-squares, and preconditioning. The new book by Bjorck [17] contains a complete list of algorithms and software for sparse least-squares problems.
Reference: [35] <author> I.S Duff. </author> <title> Multiprocessing a sparse matrix code on the Alliant FX/8. </title> <journal> Journal of computational and applied mathematics, </journal> <volume> 27 </volume> <pages> 229-239, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Furthermore, working storage management is particularly hard in a parallel formulation, because the stack-based organization used for efficiency in the sequential algorithm severely limits the degree of parallelism. More sophisticated parallel schemes were used by Amestoy and Duff <ref> [6, 35] </ref>, which came with nontrivial runtime overhead. Recent submatrix codes include MA48 [33], Amestoy and Duff's symmetric pattern multifrontal code MUPS [5], and Davis and Duff's unsymmetric multifrontal code UMFPACK [21, 23].
Reference: [36] <author> I.S. Duff, I.M. Erisman, and J.K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Oxford University Press, </publisher> <address> London, </address> <year> 1986. </year>
Reference-contexts: Our second motivation is to exploit parallelism in order to solve the ever larger linear systems arising in practice. Twenty years ago, the day-to-day linear systems people wanted to solve usually had only tens or hundreds of unknowns (see Table 1.6.1 in Duff et al. <ref> [36] </ref>). In a more recent and widely used Harwell-Boeing collection of sparse matrices [37], the largest nonsymmetric system has about 5000 unknowns. Today, it is not uncommon to encounter systems involving 50,000 unknowns, for example, from three-dimensional simulations. <p> They typically use a combination of some form of Markowitz ordering [86] and numerical threshold pivoting <ref> [36] </ref> to choose the pivot element from the uneliminated submatrix. To illustrate this, let us assume that the first k 1 stages of Gaussian elimination have been completed. <p> Based on this idea, some variations on the criteria for selecting pivots have been proposed to balance numerical stability and preservation of sparsity. The reader may consult Chapter 7 of Duff et al. <ref> [36] </ref> for a thorough treatment of this subject. Multifrontal approaches [6, 21, 32] are essentially variations of the submatrix methods. At each stage of the elimination, the update operations for the Schur complement are not applied directly to the target columns of the trailing submatrix. <p> We do not intend to review all the basics of sparse matrix computations, such as matrix representation, nonzero manipulation, and graph-theoretic terminology. For that purpose, George and Liu [51] and Duff et al. <ref> [36] </ref> serve as excellent sources. We hereby confine ourselves only to the most relevant concepts. In Section 3.1, we elaborate on the roles of row interchanges (partial pivoting) and column interchanges to maintain numerical stability and to preserve sparsity.
Reference: [37] <author> I.S Duff, R.G Grimes, and J.G Lewis. </author> <title> Users' guide for the harwell-boeing sparse matrix collection (release 1). </title> <type> Technical Report RAL-92-086, </type> <institution> Rutherford Appleton Laboratory, </institution> <month> December </month> <year> 1992. </year> <month> 126 </month>
Reference-contexts: Twenty years ago, the day-to-day linear systems people wanted to solve usually had only tens or hundreds of unknowns (see Table 1.6.1 in Duff et al. [36]). In a more recent and widely used Harwell-Boeing collection of sparse matrices <ref> [37] </ref>, the largest nonsymmetric system has about 5000 unknowns. Today, it is not uncommon to encounter systems involving 50,000 unknowns, for example, from three-dimensional simulations. Examples of several large matrices will be used in our study (see Table 4.1).
Reference: [38] <author> I.S Duff and J. K. Reid. MA47, </author> <title> a Fortran code for direct solution of indefinite sparse symmetric linear systems. </title> <type> Technical Report RAL-95-001, </type> <institution> Rutherford Appleton Laboratory, </institution> <year> 1995. </year>
Reference-contexts: Solution of Ly = b and L T x = y. In the first phase, although it is computationally expensive (NP-hard) to find an optimal P in terms of minimizing fills, many heuristics have been used successfully in practice, such as variants of minimum degree orderings <ref> [3, 10, 38, 52] </ref> and various dissection orderings based on graph partitioning [15, 58, 92] or hybrid approaches [13, 18, 76]. Two important data structures have been introduced in efficient implementations of the Cholesky factorization. One is the elimination tree and another is the supernode.
Reference: [39] <author> I.S Duff and J. K. Reid. </author> <title> The design of MA48, a code for the direct solution of sparse unsymmetric linear systems of equations. </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 22 </volume> <pages> 187-226, </pages> <year> 1996. </year>
Reference-contexts: The Harwell code MA48 <ref> [33, 39] </ref> employs such a switch to dense code, which has a significant beneficial effect on performance. Where is a good point to switch to dense LU ? Since our algorithm is left-looking, we do not know exactly when the trailing submatrix becomes dense or nearly so. <p> SuperLU LL, partial BLAS-2.5 Pub/UCB unsym. UMFPACK [21, 22] MF, Markowitz BLAS-3 Pub/netlib MA38 (same as UMFPACK) Com/HSL unsym. MA48 <ref> [39] </ref> Anal: RL, Markowitz Com/HSL Fact: LL, partial BLAS-1, SD unsym. SPARSE [79] RL, Markowitz Scalar Pub/netlib sym pattern ) ( MA42 [42] MF, threshold Frontal (eqn+element) BLAS-3 BLAS-3 Com/HSL sym. MA27 [40] MF, LDL T BLAS-1 BLAS-3 Com/HSL s.p.d.
Reference: [40] <author> I.S Duff and J.K Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 9(3) </volume> <pages> 302-325, </pages> <month> September </month> <year> 1983. </year>
Reference-contexts: Liu [83] discusses the use of elimination trees in various aspects of sparse algorithms, including reordering, symbolic and numeric factorizations, and parallel elimination. The supernode structure has long been recognized and employed in enhancing the efficiency of both the minimum degree ordering <ref> [40, 48] </ref> and the symbolic factorization [102]. A supernode is a set of contiguous columns in the Cholesky factor L that share essentially the same sparsity structure. <p> For such values of r, the extra storage needed to store the logical zeros is very small for all our test matrices. Artificial supernodes are a special case of relaxed supernodes, which were used in the context of multifrontal methods for symmetric systems <ref> [11, 40] </ref>. Ashcraft and Grimes 21 allow a small number of zeros in the structure of any supernode, thus relaxing the condition that the columns must have strictly nested structures. <p> UMFPACK [21, 22] MF, Markowitz BLAS-3 Pub/netlib MA38 (same as UMFPACK) Com/HSL unsym. MA48 [39] Anal: RL, Markowitz Com/HSL Fact: LL, partial BLAS-1, SD unsym. SPARSE [79] RL, Markowitz Scalar Pub/netlib sym pattern ) ( MA42 [42] MF, threshold Frontal (eqn+element) BLAS-3 BLAS-3 Com/HSL sym. MA27 <ref> [40] </ref> MF, LDL T BLAS-1 BLAS-3 Com/HSL s.p.d. Ng & Peyton [89] LL BLAS-3 Pub/Author Shared Memory Algorithms unsym. SuperLU LL, partial BLAS-2.5 Pub/UCB unsym. PARASPAR [112, 113] RL, Markowitz BLAS-1, SD Res/Author sym- MUPS [6] MF, threshold BLAS-3 Res/Author pattern unsym.
Reference: [41] <author> I.S. Duff and J.K. Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations. </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 9 </volume> <pages> 302-325, </pages> <year> 1983. </year>
Reference-contexts: More recently, supernodes have also been introduced in numeric factorization and triangular solution, in order to make better use of vector registers or cache memory. Indeed, supernodal [12] and multifrontal <ref> [41] </ref> elimination allow the use of dense vector operations for nearly all of the floating-point computation, thus reducing the symbolic overhead in numeric factorization to a smaller fraction. Overall, the Megaflop 1 The directed graph of a square matrix has n vertices corresponding to n rows/columns.
Reference: [42] <author> I.S Duff and J. A. Scott. </author> <title> The design of a new frontal code for solving sparse unsym-metric systems. </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 22(1) </volume> <pages> 30-45, </pages> <year> 1996. </year>
Reference-contexts: SuperLU LL, partial BLAS-2.5 Pub/UCB unsym. UMFPACK [21, 22] MF, Markowitz BLAS-3 Pub/netlib MA38 (same as UMFPACK) Com/HSL unsym. MA48 [39] Anal: RL, Markowitz Com/HSL Fact: LL, partial BLAS-1, SD unsym. SPARSE [79] RL, Markowitz Scalar Pub/netlib sym pattern ) ( MA42 <ref> [42] </ref> MF, threshold Frontal (eqn+element) BLAS-3 BLAS-3 Com/HSL sym. MA27 [40] MF, LDL T BLAS-1 BLAS-3 Com/HSL s.p.d. Ng & Peyton [89] LL BLAS-3 Pub/Author Shared Memory Algorithms unsym. SuperLU LL, partial BLAS-2.5 Pub/UCB unsym.
Reference: [43] <author> S. C. Eisenstat and J. W. H. Liu. </author> <title> Exploiting structural symmetry in sparse unsym-metric symbolic factorization. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 13 </volume> <pages> 202-211, </pages> <year> 1992. </year>
Reference-contexts: Now l ik is in the diagonal block of the supernode, and hence is nonzero. Thus i L (:;J) L (:;J) A ! j, whence u ij is nonzero by Theorem 3. 2 4.3.2 Pruning the symbolic structure To speed up the depth-first search traversals, Eisenstat and Liu <ref> [43, 44] </ref> and Gilbert and Liu [61] have explored the idea of using a reduced graph in place of G = G (L (: ; J) T ). <p> An extreme choice of H is the elimination dag [61], which is the transitive reduction of G, or the minimal subgraph of G that preserves paths. However, the elimination dag is expensive to compute. The symmetric reduction <ref> [43] </ref> is a subgraph that has (in general) fewer edges than G but more edges than the elimination dag, and that is much less expensive than the latter to compute. <p> The only bookkeeping required by each processor is to record d, the most distant busy panel in this linear chain. 5.4.2 Symmetric pruning Symmetric pruning <ref> [43, 44] </ref> was discussed in Section 4.3.2 for the sequential algorithm. The idea is to use a graph H with fewer edges than G (L T ) to represent the structure of L. Traversing H gives the same reachable set as does traversing G, but is less expensive.
Reference: [44] <author> S. C. Eisenstat and J. W. H. Liu. </author> <title> Exploiting structural symmetry in a sparse partial pivoting code. </title> <journal> SIAM J. Scientific and Statistical Computing, </journal> <volume> 14 </volume> <pages> 253-257, </pages> <year> 1993. </year>
Reference-contexts: This gives a column code that runs in total time proportional to the number of floating-point operations, unlike earlier partial pivoting codes. We shall refer to their code as GP in our performance study in Chapter 4. Eisenstat and Liu <ref> [44] </ref> designed a pruning technique to reduce the amount of structural information required for the symbolic factorization, which we will describe further in Section 4.3. The result was that the time and space for symbolic factorization were typically reduced to a small fraction of the entire factorization. <p> Now l ik is in the diagonal block of the supernode, and hence is nonzero. Thus i L (:;J) L (:;J) A ! j, whence u ij is nonzero by Theorem 3. 2 4.3.2 Pruning the symbolic structure To speed up the depth-first search traversals, Eisenstat and Liu <ref> [43, 44] </ref> and Gilbert and Liu [61] have explored the idea of using a reduced graph in place of G = G (L (: ; J) T ). <p> The symmetric reduction of G (L (: ; J) T ) is obtained by removing all nonzero l rs for which l ts u st 6= 0 for some t &lt; min (r; j). Eisenstat and Liu <ref> [44] </ref> give an efficient method to compute the symmetric reduction during symbolic factorization, and demonstrate experimentally that it significantly reduces the total factorization time when used in an algorithm that does column-column updates. Our supernodal code uses symmetric reduction to speed up its symbolic factorization. <p> For most smaller matrices the speedups are below 10%. 4.8 Comparison with previous column LU factorization al gorithms In this section, we compare the performance of SuperLU with three of its predecessors, including GP by Gilbert and Peierls [64] (Figure 2.1), GP-Mod by Eisenstat and Liu <ref> [44] </ref> (Chapter 2, and Section 4.3.2). and SupCol by Eisenstat, Gilbert and Liu [45] (Figure 4.1). <p> The only bookkeeping required by each processor is to record d, the most distant busy panel in this linear chain. 5.4.2 Symmetric pruning Symmetric pruning <ref> [43, 44] </ref> was discussed in Section 4.3.2 for the sequential algorithm. The idea is to use a graph H with fewer edges than G (L T ) to represent the structure of L. Traversing H gives the same reachable set as does traversing G, but is less expensive.
Reference: [45] <author> Stanley C. Eisenstat, John R. Gilbert, and Joseph W.H. Liu. </author> <title> A supernodal approach to a sparse partial pivoting code. </title> <booktitle> In Householder Symposium XII, </booktitle> <year> 1993. </year>
Reference-contexts: One difficulty is that, unlike the symmetric case, supernodal structure cannot be determined in advance but rather emerges depending on pivoting choices during the factorization. Eisen-stat, Gilbert and Liu <ref> [45] </ref> discussed how to detect supernodes dynamically. In Chapter 4 we will review their approach and quantify the performance gains. There have been debates about whether submatrix methods are preferable to column methods, or vice versa. Their memory reference patterns are markedly different. <p> L (j : n; j) = f ; 13. Inner factorization for L (r 2 : n; r 2 : s 2 ); 14. end for; 14 3.2.1 Definition of a supernode Eisenstat, Gilbert and Liu <ref> [45] </ref> considered several possible ways to generalize the symmetric definition of supernodes to unsymmetric factorization. Here, we present all their characterizations and choose the most appropriate one to use. <p> The occurrence of symmetric supernodes is related to the clique structure of the chordal graph of the Cholesky factor, which arises because of fill during the factorization. Unsymmetric supernodes seem harder to characterize, but they also are related to dense submatrices arising from fill. Eisen-stat et al. <ref> [45] </ref> measured the supernodes according to each definition for 126 unsymmetric matrices from the Harwell-Boeing sparse matrix test collection [31] under various column orderings. Table 3.1 tabulates the results from their measurements. <p> Our test matrices were collected from diverse application fields with varied characteristics. We also analyze at great length the performance of the new algorithm. 4.1 Supernode-column updates Eisenstat et al. <ref> [45] </ref> first introduced the supernode-column algorithm, as formulated in Figure 4.1. We refer to this code as SupCol. The only difference from the column-column algorithm (Figure 2.1) is that all the updates to a column from a single supernode are done together. <p> 4.8 Comparison with previous column LU factorization al gorithms In this section, we compare the performance of SuperLU with three of its predecessors, including GP by Gilbert and Peierls [64] (Figure 2.1), GP-Mod by Eisenstat and Liu [44] (Chapter 2, and Section 4.3.2). and SupCol by Eisenstat, Gilbert and Liu <ref> [45] </ref> (Figure 4.1). <p> This is because each floating-point operation in the solve phase takes longer time than in the factorization. 4.11 Conclusions Our starting point in this chapter was the supernode-column LU factorization algorithm developed by Eisenstat et al. <ref> [45] </ref>. Based on this, we designed both symbolic and numeric algorithms to perform the supernode-panel updates, in order to achieve better 64 data reuse. For the new code, SuperLU, we have conducted careful performance studies on several high performance machines. We studied both runtime and working storage efficiency.
Reference: [46] <author> David M. Fenwick, Denis J. Foley, William B. Gist, Stephen R. VanDoren, and Daniel Wissel. </author> <title> The AlphaServer 8000 series: High-end server platform development. </title> <journal> Digital Technical Journal, </journal> <volume> 7(1) </volume> <pages> 43-65, </pages> <year> 1995. </year>
Reference-contexts: The primary objective of this chapter is to achieve good efficiency on shared memory systems with a modest number of processors. Examples of such commercially popular machines include Sun SPARCcenter 2000 [107], SGI Power Challenge [104], DEC AlphaServer 8400 <ref> [46] </ref>, and Cray C90/J90 [110, 111]. In addition to demonstrating the efficiency of our parallel algorithm on these machines, we also study the (theoretical) upper bound on performance of this algorithm. <p> The system provides hardware support for fast synchronization operations, such as fork and join, semaphores and locks. These allow for efficient fine-grain parallel processing. 71 5.1.3 The DEC AlphaServer 8400 The DEC AlphaServer 8400 is based on the 64-bit Alpha 21164 microprocessor and the AlphaServer 8000-series platform architecture <ref> [46] </ref>. The clock frequency of the processor is 300 MHz with peak floating-point rate 600 Mflops. Each microprocessor has its own independent caches, including an 8 KB instruction cache, an 8 KB data cache, a 96 KB write-back second-level cache, and a 4 MB tertiary cache.
Reference: [47] <author> G.A. Geist and E. Ng. </author> <title> Task scheduling for parallel sparse Cholesky factorization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18(4) </volume> <pages> 291-314, </pages> <year> 1989. </year>
Reference-contexts: It is necessary to round it to an integer. This rounding may cause serious load imbalance. Geist and Ng <ref> [47] </ref> proposed a tasking scheduling method that can alleviate this problem. They relaxed the condition of finding exactly P subtrees. Instead, their algorithm may find more than P subtrees, so there will be more flexibility to assign them to the P processors with reasonable load balance.
Reference: [48] <author> A. George and D. McIntyre. </author> <title> On the application of the minimum degree algorithm to finite element systems. </title> <journal> SIAM J. Numerical Analysis, </journal> <volume> 15 </volume> <pages> 90-111, </pages> <year> 1978. </year>
Reference-contexts: Liu [83] discusses the use of elimination trees in various aspects of sparse algorithms, including reordering, symbolic and numeric factorizations, and parallel elimination. The supernode structure has long been recognized and employed in enhancing the efficiency of both the minimum degree ordering <ref> [40, 48] </ref> and the symbolic factorization [102]. A supernode is a set of contiguous columns in the Cholesky factor L that share essentially the same sparsity structure.
Reference: [49] <author> Alan George, Michael T. Heath, Joseph Liu, and Esmond Ng. </author> <title> Solution of sparse positive definitive systems on a shared-memory multiprocessor. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 15(4) </volume> <pages> 309-325, </pages> <year> 1986. </year>
Reference-contexts: In addition to demonstrating the efficiency of our parallel algorithm on these machines, we also study the (theoretical) upper bound on performance of this algorithm. Several methods have been proposed to perform sparse Cholesky factorization <ref> [49, 73, 90] </ref> and sparse LU factorization [6, 57, 65] on shared memory machines. A common practice is to organize the program as a self-scheduling loop, interacting with a global pool of tasks that are ready to be executed.
Reference: [50] <author> Alan George, Joseph Liu, and Esmond Ng. </author> <title> A data structure for sparse QR and LU factorizations. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9 </volume> <pages> 100-121, </pages> <year> 1988. </year>
Reference-contexts: Almost all parallel sparse Cholesky factorization algorithms take advantage of this type of parallelism, referred to as tree or task parallelism. In unsymmetric LU factorization with partial pivoting, we also wish to determine column dependencies prior to the factorization. It has been shown in a series of studies <ref> [50, 54, 63, 65] </ref> that the column elimination tree gives the information about all potential dependencies. We herein simply state the most relevant results. The interested reader can consult Gilbert and Ng [63] for a complete and rigorous treatment of this topic. <p> If L c is the symbolic Cholesky factor described above, and L and U are the triangular factors of A represented as above, then Struct (L + U ) Struct (L c + L T c ). Theorem 6 <ref> [50, 55] </ref> Consider the QR factorization A = QR using Householder transfor mations. Let H be the symbolic Householder matrix consisting of the sequence of House 1 This L is different from the ^ L in P A = ^ LU.
Reference: [51] <author> Alan George and Joseph W. H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: We do not intend to review all the basics of sparse matrix computations, such as matrix representation, nonzero manipulation, and graph-theoretic terminology. For that purpose, George and Liu <ref> [51] </ref> and Duff et al. [36] serve as excellent sources. We hereby confine ourselves only to the most relevant concepts. In Section 3.1, we elaborate on the roles of row interchanges (partial pivoting) and column interchanges to maintain numerical stability and to preserve sparsity. <p> In the symmetric positive definite case, an ordering algorithm works only on the graph of A and a sequence of elimination graphs <ref> [51] </ref> thereafter. It does not need to know the numerical values of A. In the unsymmetric case, however, the elimination graph at each step changes with the numerical pivot selection, as we saw in the previous section.
Reference: [52] <author> Alan George and Joseph W. H. Liu. </author> <title> The evolution of the minimum degree ordering algorithms. </title> <journal> SIAM Review, </journal> <volume> 31(1) </volume> <pages> 1-19, </pages> <month> March </month> <year> 1989. </year> <month> 127 </month>
Reference-contexts: Solution of Ly = b and L T x = y. In the first phase, although it is computationally expensive (NP-hard) to find an optimal P in terms of minimizing fills, many heuristics have been used successfully in practice, such as variants of minimum degree orderings <ref> [3, 10, 38, 52] </ref> and various dissection orderings based on graph partitioning [15, 58, 92] or hybrid approaches [13, 18, 76]. Two important data structures have been introduced in efficient implementations of the Cholesky factorization. One is the elimination tree and another is the supernode.
Reference: [53] <author> Alan George, Joseph W. H. Liu, and Esmond Ng. </author> <title> Communication results for parallel sparse cholesky factorization on a hypercube. </title> <booktitle> Parallel Computing, </booktitle> <pages> pages 287-298, </pages> <year> 1989. </year>
Reference-contexts: The next question is how we shall find the domains. We first examine what people have done in sparse Cholesky factorizations. For a well balanced etree, often coming from a nested dissection ordering, the subtree-to-subcube mapping <ref> [53] </ref> is quite effective. In this method, the processors are recursively divided into two groups at each branching node of the tree, until the log P level is reached. At this level there are exactly P disjoint subtrees, or domains, each being assigned to one processor.
Reference: [54] <author> Alan George and Esmond Ng. </author> <title> An implementation of Gaussian elimination with partial pivoting for sparse systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 6(2) </volume> <pages> 390-409, </pages> <year> 1985. </year>
Reference-contexts: Almost all parallel sparse Cholesky factorization algorithms take advantage of this type of parallelism, referred to as tree or task parallelism. In unsymmetric LU factorization with partial pivoting, we also wish to determine column dependencies prior to the factorization. It has been shown in a series of studies <ref> [50, 54, 63, 65] </ref> that the column elimination tree gives the information about all potential dependencies. We herein simply state the most relevant results. The interested reader can consult Gilbert and Ng [63] for a complete and rigorous treatment of this topic. <p> Here we only quote the results without proof. Theorem 5 <ref> [54] </ref> Let A be a nonsingular matrix with nonzero diagonal. If L c is the symbolic Cholesky factor described above, and L and U are the triangular factors of A represented as above, then Struct (L + U ) Struct (L c + L T c ).
Reference: [55] <author> Alan George and Esmond Ng. </author> <title> Symbolic factorization for sparse Gaussian elimination with partial pivoting. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8(6) </volume> <pages> 877-898, </pages> <year> 1987. </year>
Reference-contexts: If L c is the symbolic Cholesky factor described above, and L and U are the triangular factors of A represented as above, then Struct (L + U ) Struct (L c + L T c ). Theorem 6 <ref> [50, 55] </ref> Consider the QR factorization A = QR using Householder transfor mations. Let H be the symbolic Householder matrix consisting of the sequence of House 1 This L is different from the ^ L in P A = ^ LU. <p> In both static and dynamic schemes, the bounds using H are tighter than those using L c . The difference is especially large in the static schemes. Note that this observation is consistent with what George and Ng observed for a set of smaller test problems in <ref> [55] </ref>. For most matrices, the storage utilizations using the static bound by H are quite high; they 88 are often greater than 70% and are over 85% for 14 out of the 21 problems.
Reference: [56] <author> Alan George and Esmond Ng. </author> <title> On the complexity of sparse QR and LU factorization on finite-element matrices. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9(5) </volume> <pages> 849-861, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: The question arises whether it is still possible to choose a fill-reducing ordering before the factorization begins. The answer is partially positive. The essence of our approach is based on a result proved by George and Ng <ref> [56] </ref>.
Reference: [57] <author> Alan George and Esmond Ng. </author> <title> Parallel sparse Gaussian elimination with partial pivoting. </title> <journal> Annals of Operation Research, </journal> <volume> 22 </volume> <pages> 219-240, </pages> <year> 1990. </year>
Reference-contexts: In addition to demonstrating the efficiency of our parallel algorithm on these machines, we also study the (theoretical) upper bound on performance of this algorithm. Several methods have been proposed to perform sparse Cholesky factorization [49, 73, 90] and sparse LU factorization <ref> [6, 57, 65] </ref> on shared memory machines. A common practice is to organize the program as a self-scheduling loop, interacting with a global pool of tasks that are ready to be executed. <p> This only says that the upper bounds of the fills and arithmetic on L and U are the same (Theorem 5 in Section 5.4.3), with no guarantees for L and U themselves. George and Ng <ref> [57] </ref> employed this technique in their parallel sparse Gaussian elimination algorithm. <p> This is in fact one of our motivations for building the theoretical model in the first place. We may also use the etree defined by U <ref> [57] </ref> instead of the column etree, which would present more concurrency than does the column etree. <p> MA27 [40] MF, LDL T BLAS-1 BLAS-3 Com/HSL s.p.d. Ng & Peyton [89] LL BLAS-3 Pub/Author Shared Memory Algorithms unsym. SuperLU LL, partial BLAS-2.5 Pub/UCB unsym. PARASPAR [112, 113] RL, Markowitz BLAS-1, SD Res/Author sym- MUPS [6] MF, threshold BLAS-3 Res/Author pattern unsym. George & Ng <ref> [57] </ref> RL, partial BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL BLAS-3 Com/SGI Ng & Peyton [73] Pub/Author s.p.d. SPLASH [78] RL, 2-D block BLAS-3 Pub/Stanford Distributed Memory Algorithms unsym. van der Stappen [108] RL, Markowitz Scalar Res/Author sym- Lucas et al. [85] MF, no pivoting BLAS-1 Res/Author pattern s.p.d.
Reference: [58] <author> J. George. </author> <title> Nested dissection of a regular finite element mesh. </title> <journal> SIAM J. Numerical Analysis, </journal> <volume> 10 </volume> <pages> 345-363, </pages> <year> 1973. </year>
Reference-contexts: In the first phase, although it is computationally expensive (NP-hard) to find an optimal P in terms of minimizing fills, many heuristics have been used successfully in practice, such as variants of minimum degree orderings [3, 10, 38, 52] and various dissection orderings based on graph partitioning <ref> [15, 58, 92] </ref> or hybrid approaches [13, 18, 76]. Two important data structures have been introduced in efficient implementations of the Cholesky factorization. One is the elimination tree and another is the supernode. The elimination tree [100] is defined for the Cholesky factor L.
Reference: [59] <author> J. A. George and E. Ng. </author> <title> An implementation of Gaussian elimination with partial pivoting for sparse systems. </title> <journal> SIAM J. Scientific and Statistical Computing, </journal> <volume> 6 </volume> <pages> 390-409, </pages> <year> 1985. </year>
Reference-contexts: T5 supernodes are motivated by 15 median 0.236 0.345 0.326 0.006 mean 0.284 0.365 0.342 0.052 Table 3.1: Fraction of nonzeros not in first column of supernode. George and Ng's observation <ref> [59] </ref> that (with suitable representations) the structures of L and U in the unsymmetric factorization P A = LU are contained in the structure of the Cholesky factor of A T A.
Reference: [60] <author> J. R. Gilbert. </author> <title> Predicting structure in sparse matrix computations. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 15 </volume> <pages> 62-79, </pages> <year> 1994. </year>
Reference-contexts: The notation i M =) j means that there is a directed path from i to j in the directed graph of M . Such a path may have length zero; that is, i M =) i holds if m ii 6= 0. Theorem 3 <ref> [60] </ref> f ij is nonzero (equivalently, i F ! j) if and only if i L (:;J) A ! j for some k i. This result implies that the symbolic factorization of column j can be obtained as follows.
Reference: [61] <author> J. R. Gilbert and J. W. H. Liu. </author> <title> Elimination structures for unsymmetric sparse LU factors. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 14 </volume> <pages> 334-352, </pages> <year> 1993. </year>
Reference-contexts: Thus i L (:;J) L (:;J) A ! j, whence u ij is nonzero by Theorem 3. 2 4.3.2 Pruning the symbolic structure To speed up the depth-first search traversals, Eisenstat and Liu [43, 44] and Gilbert and Liu <ref> [61] </ref> have explored the idea of using a reduced graph in place of G = G (L (: ; J) T ). <p> An extreme choice of H is the elimination dag <ref> [61] </ref>, which is the transitive reduction of G, or the minimal subgraph of G that preserves paths. However, the elimination dag is expensive to compute.
Reference: [62] <author> J. R. Gilbert, C. Moler, and R. Schreiber. </author> <title> Sparse matrices in Matlab: Design and implementation. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 13 </volume> <pages> 333-356, </pages> <year> 1992. </year>
Reference-contexts: In principle, any ordering heuristic used in the symmetric case can be applied to A T A to arrive at Q. The column minimum degree algorithm used in Matlab <ref> [62] </ref> is the first efficient implementation of the minimum degree algorithm on A T A without explicitly forming the nonzero structure of A T A. <p> Efficient BLAS-2 matrix-vector kernels can be used for the triangular solve and matrix-vector multiply. Furthermore, all the updates from the supernodal columns can be collected in a temporary packed vector before doing a single scatter into a full-length working array of size n, called SPA (for sparse accumulator <ref> [62] </ref>). This reduces the amount of indirect addressing. The use of the SPA allows random access to the entries of the active column. <p> The reason for this order will be described in more detail in section 4.6. This thesis does not address the performance of column preordering for sparsity. We simply use the existing ordering algorithms provided by Matlab <ref> [62] </ref>. For all matrices, except 1, 14 and 21, the columns were permuted by Matlab's minimum degree ordering of A T A, also known as "column minimum degree" ordering. <p> GP and GP-Mod are written in Fortran; SupCol and SuperLU are written in C. (Matlab contains C implementations of GP and GP-Mod <ref> [62] </ref>, which we did not test here.) We benchmarked the above four codes on six high-end workstations from four vendors, whose characteristics are tabulated in Table 4.5. The instruction caches, if separate from the data cache, are not listed in the table.
Reference: [63] <author> J. R. Gilbert and E. Ng. </author> <title> Predicting structure in nonsymmetric sparse matrix factorizations. </title> <editor> In Alan George, John R. Gilbert, and Joseph W. H. Liu, editors, </editor> <title> Graph Theory and Sparse Matrix Computation. </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Note that column i updates column j in LU factorization if and only if u ij 6= 0. Theorem 1 (Column Elimination Tree) <ref> [63] </ref> Let A be a square, nonsingular, possibly unsymmetric matrix, and let P A = LU be any factorization of A with pivoting by row interchanges. Let T be the column elimination tree of A. 1. <p> Almost all parallel sparse Cholesky factorization algorithms take advantage of this type of parallelism, referred to as tree or task parallelism. In unsymmetric LU factorization with partial pivoting, we also wish to determine column dependencies prior to the factorization. It has been shown in a series of studies <ref> [50, 54, 63, 65] </ref> that the column elimination tree gives the information about all potential dependencies. We herein simply state the most relevant results. The interested reader can consult Gilbert and Ng [63] for a complete and rigorous treatment of this topic. <p> It has been shown in a series of studies [50, 54, 63, 65] that the column elimination tree gives the information about all potential dependencies. We herein simply state the most relevant results. The interested reader can consult Gilbert and Ng <ref> [63] </ref> for a complete and rigorous treatment of this topic. Recall that column i of L and/or U modifies column j if and only if u ij 6= 0. Part 3 of Theorem 1 implies that the columns in different subtrees do not update one another.
Reference: [64] <author> J. R. Gilbert and T. Peierls. </author> <title> Sparse partial pivoting in time proportional to arithmetic operations. </title> <journal> SIAM J. Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 862-874, </pages> <year> 1988. </year>
Reference-contexts: One disadvantage of the column methods is that, unlike Markowitz ordering, they do not reorder the columns dynamically, so the fills may be greater. An early example of such a code is Sherman's NSPIV [103] (which is actually a row code). Gilbert and Peierls <ref> [64] </ref> showed how to use depth-first search and topological ordering to obtain the structure of each factor column. This gives a column code that runs in total time proportional to the number of floating-point operations, unlike earlier partial pivoting codes. <p> Indeed, F (: ; j) has the same structure as the solution vector for the following triangular system <ref> [64] </ref>: @ @ @ @ @ L (: ; J) I A straightforward way to compute the structure of F (: ; j) from the structures of L (: ; J) and A (: ; j) is to simulate the numerical computation. <p> These updates must be applied in an order consistent with a topological ordering of G. We use depth-first search to perform the traversal, which makes it possible to generate a topological order (specifically, reverse postorder) on the nonzeros of U (: ; j) as they are located <ref> [64] </ref>. Another consequence of the path theorem is the following corollary. It says that if we divide each column of U into segments, one per supernode, then within each segment the column of U just consists of a consecutive sequence of nonzeros. <p> For large matrices, the speedups are between 10% and 15%. For most smaller matrices the speedups are below 10%. 4.8 Comparison with previous column LU factorization al gorithms In this section, we compare the performance of SuperLU with three of its predecessors, including GP by Gilbert and Peierls <ref> [64] </ref> (Figure 2.1), GP-Mod by Eisenstat and Liu [44] (Chapter 2, and Section 4.3.2). and SupCol by Eisenstat, Gilbert and Liu [45] (Figure 4.1).
Reference: [65] <author> John R. Gilbert. </author> <title> An efficient parallel sparse partial pivoting algorithm. </title> <type> Technical Report CMI No. 88/45052-1, </type> <institution> Computer Science Department, University of Bergen, Norway, </institution> <month> 8 </month> <year> 1988. </year>
Reference-contexts: In addition to demonstrating the efficiency of our parallel algorithm on these machines, we also study the (theoretical) upper bound on performance of this algorithm. Several methods have been proposed to perform sparse Cholesky factorization [49, 73, 90] and sparse LU factorization <ref> [6, 57, 65] </ref> on shared memory machines. A common practice is to organize the program as a self-scheduling loop, interacting with a global pool of tasks that are ready to be executed. <p> Our parallel algorithm resembles the supernode-panel sparse Cholesky factorization studied in [73] in the way we define the basic tasks and the computational primitives. The way we handle the coordination of the dependent tasks is reminiscent of the approach used by Gilbert <ref> [65] </ref> in his column-wise sparse LU factorization. However, our algorithm represents a non-trivial extension to the earlier work in that we have incorporated several new mechanisms, such as unsymmetric supernodes and symmetric structure reduction. <p> Almost all parallel sparse Cholesky factorization algorithms take advantage of this type of parallelism, referred to as tree or task parallelism. In unsymmetric LU factorization with partial pivoting, we also wish to determine column dependencies prior to the factorization. It has been shown in a series of studies <ref> [50, 54, 63, 65] </ref> that the column elimination tree gives the information about all potential dependencies. We herein simply state the most relevant results. The interested reader can consult Gilbert and Ng [63] for a complete and rigorous treatment of this topic. <p> Part 3 of Theorem 1 implies that the columns in different subtrees do not update one another. Furthermore, the columns in independent subtrees can be computed without referring to any common memory, because the columns they depend on have completely disjoint row indices (Theorem 3.2 in <ref> [65] </ref>). In general we cannot predict the nonzero structure of U precisely before the factorization, because the pivoting choice and hence the exact nonzero structure depend on numerical values. The column elimination tree can overestimate the true column dependencies. <p> This section presents the organization of the scheduling algorithm, with more implementation details to appear in Section 5.4. Our scheduling approach used some techniques from the parallel algorithm developed by Gilbert <ref> [65] </ref>, which was based on the sequential GP algorithm. Figure 5.2 sketches the top level scheduling loop. Each processor executes this loop until its termination criterion is met, that is, all panels have been factorized.
Reference: [66] <author> John R. Gilbert. </author> <title> Predicting structures in sparse matrix computations. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 15(1) </volume> <pages> 62-79, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: The nonzero patterns in L and U depend on the row interchanges and cannot be predetermined precisely based solely on the structure of A. This can be best illustrated by the following example given by Gilbert <ref> [66] </ref>.
Reference: [67] <author> John R. Gilbert. </author> <type> Personal communication, </type> <year> 1995. </year> <month> 128 </month>
Reference-contexts: Thus, before we factor the matrix, we compute its column etree and permute the matrix columns according to a postorder on the tree. The following theorem, due to Gilbert <ref> [67] </ref>, shows that this does not change the factorization in any essential way. Theorem 2 Let A be a matrix with column etree T . Let be a permutation such that whenever (i) is an ancestor of (j) in T , we have i j.
Reference: [68] <author> John R. Gilbert and Joseph W.H. Liu. </author> <title> Elimination structures for unsymmetric sparse lu factors. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 14(2) </volume> <pages> 334-352, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: So both symbolic and numeric factorizations must interleave. In addition, some algorithms include a column pivoting strategy to preserve sparsity during the elimination process, mixing the ordering, symbolic and numeric phases altogether. Gilbert and Liu <ref> [68] </ref> introduced elimination dags (directed acyclic graphs), or edags for short, to study the structure changes during unsymmetric LU factorization. The edags are transitive reductions of the graphs G (L T ) and G (U ).
Reference: [69] <author> John R. Gilbert, Esmond G. Ng, and Barry W. Peyton. </author> <title> Computing row and column counts for sparse QR factorization. </title> <note> Talk presented at SIAM Symposium on Applied Linear Algebra, June 1994. Journal version in preparation. </note>
Reference-contexts: In fact, if we use fundamental L supernodes and ignore numerical cancellation (which we must do anyway for symmetric pruning), we can show that an L supernode is always contained in an L c (or H) supernode <ref> [69] </ref>. Our objective is to allocate storage based on number of nonzeros in either S L c or S H , so that this storage is sufficiently large to hold S L . Figure 5.8 illustrates the idea of using S L c as a bound. <p> However, forming the structure of A T A may be expensive and A T A may be much denser than A. To achieve the needed level of efficiency, Gilbert, Ng and Peyton <ref> [69] </ref> suggested ways to modify their Cholesky-column-count algorithm [70] to work with the structure of A without explicitly forming A T A.
Reference: [70] <author> John R. Gilbert, Esmond G. Ng, and Barry W. Peyton. </author> <title> An efficient algorithm to compute row and column counts for sparse Cholesky factorization. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 15 </volume> <pages> 1075-1091, </pages> <year> 1994. </year>
Reference-contexts: To compute nnz (L c flj ) and S L c , we can apply the supernodal count algorithm for sparse Cholesky factor <ref> [70] </ref> to A T A. However, forming the structure of A T A may be expensive and A T A may be much denser than A. To achieve the needed level of efficiency, Gilbert, Ng and Peyton [69] suggested ways to modify their Cholesky-column-count algorithm [70] to work with the structure <p> algorithm for sparse Cholesky factor <ref> [70] </ref> to A T A. However, forming the structure of A T A may be expensive and A T A may be much denser than A. To achieve the needed level of efficiency, Gilbert, Ng and Peyton [69] suggested ways to modify their Cholesky-column-count algorithm [70] to work with the structure of A without explicitly forming A T A. The running time of this algorithm is O (m ff (m; n)), where m = nnz (A) and ff (m; n) is the slowly-growing inverse of Ackermann's function coming from disjoint set union operations. <p> Table 5.2 reports the respective runtimes of the etree algorithm and the QR-column-count algorithm. In both the etree and QR-column-count algorithms, the disjoint set union operations are implemented using path halving and no union by rank. (see Gilbert et al. <ref> [70] </ref> for details.) One remaining issue yet to be addressed is what we should do if the static storage given by an upper bound structure is far too generous than actually needed. We developed a dynamic approach to better capture the structural changes of the LU factors during Gaussian elimination.
Reference: [71] <author> G. Golub and C. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, MD, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: abbreviates A (1 : n; J ). nnz (A) denotes the number of nonzeros in A. 6 The reduced submatrix R ~ K ~ K = A ~ K ~ K A ~ KK A 1 KK A K ~ K is known as the Schur complement of A KK <ref> [71] </ref>. For R ~ K ~ K , let r i denote the number of entries in row i, and let c j denote the number of entries in column j. The Markowitz count associated with entry (i; j) is defined as (r i 1)(c j 1). <p> In the next two subsections, we will study the purpose of applying P and Q. 3.1.1 Partial pivoting It is well known that for the special class of problems where A is symmetric and positive definite, pivots can be chosen down the diagonal in order <ref> [71, Chapter 5] </ref>. The factorization thus obtained can be written as A = LL T , which is known as Cholesky decomposition. For general unsymmetric A, however, it is possible to encounter arbitrarily small pivots on the diagonal. <p> This process is called partial pivoting. The efficacy of partial pivoting (as opposed to the more costly complete pivoting) to maintain numerical stability is well studied in a large body of literature; for example, see <ref> [71, Chapter 4] </ref>.
Reference: [72] <author> A. Gupta and V. Kumar. </author> <title> Optimally scalable parallel sparse Cholesky factorization. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Proceesing for Scientific Computing, </booktitle> <pages> pages 442-447. </pages> <publisher> SIAM, </publisher> <year> 1995. </year>
Reference-contexts: On uniprocessor machines for in-memory problems, Ng and Peyton [89] and Rothberg [99] conducted extensive experiments with sparse Cholesky factorization, and concluded that the supernodal left-looking algorithm is somewhat better than the multifrontal approach both in runtime and working storage requirement. Gupta and Kumar <ref> [72] </ref> developed a two-dimensional multifrontal Cholesky algorithm on 1024 nodes of the Cray T3D, and achieved up to 20 Gflops factor 8 ization rate for one problem. <p> Another is to use a more sophisticated dynamic scheduling algorithm that steals cycles from the idle processors to do useful work. 5.7.1 Independent domains The concept of domains has been widely used in sparse Cholesky factorizations, especially on distributed memory machines <ref> [14, 72, 93, 97] </ref>. A domain refers to a rooted subtree of the elimination tree such that all nodes in this subtree are mapped onto the same processor to factorize. In sparse LU factorization, we may define domains similarly, but we use the column etree. <p> Schreiber [101] modeled the lower bounds on parallel completion time of a left-looking column-oriented sparse Cholesky factorization, and concluded that a two-dimensional mapping is needed to achieve better scalability. Since then, several researchers <ref> [72, 97] </ref> have developed and demonstrated efficient and scalable 2-D distributed algorithms for sparse Cholesky. <p> SPLASH [78] RL, 2-D block BLAS-3 Pub/Stanford Distributed Memory Algorithms unsym. van der Stappen [108] RL, Markowitz Scalar Res/Author sym- Lucas et al. [85] MF, no pivoting BLAS-1 Res/Author pattern s.p.d. Rothberg et al. [98] RL, 2-D block BLAS-3 Res/Author s.p.d. Gupta <ref> [72] </ref> MF, 2-D block BLAS-3 Res/Author s.p.d. CAPSS [74] MF, full parallel BLAS-1 Pub/netlib (require coordinates) Table 6.1: Software to solve sparse linear systems using direct methods.
Reference: [73] <author> A. Gupta, E. Rothberg, E. Ng, and B. W. Peyton. </author> <title> Parallel sparse Cholesky factorization algorithms for shared-memory multiprocessor systems. </title> <editor> In R. Vichnevetsky, D. Knight, and G. Richter, editors, </editor> <booktitle> Advances in Computer Methods for Partial Differential Equations-VII. IMACS, </booktitle> <year> 1992. </year>
Reference-contexts: In addition to demonstrating the efficiency of our parallel algorithm on these machines, we also study the (theoretical) upper bound on performance of this algorithm. Several methods have been proposed to perform sparse Cholesky factorization <ref> [49, 73, 90] </ref> and sparse LU factorization [6, 57, 65] on shared memory machines. A common practice is to organize the program as a self-scheduling loop, interacting with a global pool of tasks that are ready to be executed. <p> There is no notion of ownership of tasks or submatrices by processors the assignment of tasks to processors is completely dynamic, depending on the execution speed of the individual processors. Our scheduling algorithm employs this model as well. Our parallel algorithm resembles the supernode-panel sparse Cholesky factorization studied in <ref> [73] </ref> in the way we define the basic tasks and the computational primitives. The way we handle the coordination of the dependent tasks is reminiscent of the approach used by Gilbert [65] in his column-wise sparse LU factorization. <p> SuperLU LL, partial BLAS-2.5 Pub/UCB unsym. PARASPAR [112, 113] RL, Markowitz BLAS-1, SD Res/Author sym- MUPS [6] MF, threshold BLAS-3 Res/Author pattern unsym. George & Ng [57] RL, partial BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL BLAS-3 Com/SGI Ng & Peyton <ref> [73] </ref> Pub/Author s.p.d. SPLASH [78] RL, 2-D block BLAS-3 Pub/Stanford Distributed Memory Algorithms unsym. van der Stappen [108] RL, Markowitz Scalar Res/Author sym- Lucas et al. [85] MF, no pivoting BLAS-1 Res/Author pattern s.p.d. Rothberg et al. [98] RL, 2-D block BLAS-3 Res/Author s.p.d.
Reference: [74] <author> M. T. Heath and P. Raghavan. </author> <title> Performance of a fully parallel sparse solver. </title> <booktitle> In Proc. Scalable High-Performance Computing Conf., </booktitle> <pages> pages 334-341, </pages> <address> Los Alamitos,CA, 1994. </address> <publisher> IEEE. </publisher>
Reference-contexts: Rothberg et al. [98] RL, 2-D block BLAS-3 Res/Author s.p.d. Gupta [72] MF, 2-D block BLAS-3 Res/Author s.p.d. CAPSS <ref> [74] </ref> MF, full parallel BLAS-1 Pub/netlib (require coordinates) Table 6.1: Software to solve sparse linear systems using direct methods.
Reference: [75] <author> M.T. Heath, E. Ng., and B.W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33(3) </volume> <pages> 420-460, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: In Chapter 4 we will review their approach and quantify the performance gains. There have been debates about whether submatrix methods are preferable to column methods, or vice versa. Their memory reference patterns are markedly different. Heath, Ng and Peyton <ref> [75] </ref> gave a thorough survey of many distinctions between left-looking and right-looking sparse Cholesky factorization algorithms.
Reference: [76] <author> B. Hendrickson and E. Rothberg. </author> <title> Improving the runtime and quality of nested dissection ordering. </title> <type> Technical Report SAND96-0868J, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, </address> <year> 1996. </year>
Reference-contexts: phase, although it is computationally expensive (NP-hard) to find an optimal P in terms of minimizing fills, many heuristics have been used successfully in practice, such as variants of minimum degree orderings [3, 10, 38, 52] and various dissection orderings based on graph partitioning [15, 58, 92] or hybrid approaches <ref> [13, 18, 76] </ref>. Two important data structures have been introduced in efficient implementations of the Cholesky factorization. One is the elimination tree and another is the supernode. The elimination tree [100] is defined for the Cholesky factor L. Each node in the tree corresponds to one row/column of the matrix.
Reference: [77] <institution> International Business Machines Corporation Engineering and Scientific Subroutine Library, </institution> <note> Guide and Reference. Version 2 Release 2, Order No. SC23-0526-01, </note> <year> 1994. </year>
Reference-contexts: The SuperLU algorithm is implemented in C, using double precision arithmetic; we use the AIX xlc compiler with -O3 optimization. In the inner loops of our sparse code, we call the two dense BLAS-2 routines DTRSV (triangular solve) and DGEMV (matrix-vector multiply) provided in the IBM ESSL library <ref> [77] </ref>, whose BLAS-3 matrix-matrix multiply routine (DGEMM) achieves about 250 Mflops when the dimension of the matrix is larger than 60 [1]. In our sparse algorithm, we find that DGEMV typically accounts for more than 80% of the floating-point operations, as depicted in Figure 4.8.
Reference: [78] <author> W-D. Webber J.P. Singh and A. Gupta. </author> <title> Splash: Stanford parallel applications for shared-memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <year> 1992. </year>
Reference-contexts: SuperLU LL, partial BLAS-2.5 Pub/UCB unsym. PARASPAR [112, 113] RL, Markowitz BLAS-1, SD Res/Author sym- MUPS [6] MF, threshold BLAS-3 Res/Author pattern unsym. George & Ng [57] RL, partial BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL BLAS-3 Com/SGI Ng & Peyton [73] Pub/Author s.p.d. SPLASH <ref> [78] </ref> RL, 2-D block BLAS-3 Pub/Stanford Distributed Memory Algorithms unsym. van der Stappen [108] RL, Markowitz Scalar Res/Author sym- Lucas et al. [85] MF, no pivoting BLAS-1 Res/Author pattern s.p.d. Rothberg et al. [98] RL, 2-D block BLAS-3 Res/Author s.p.d. Gupta [72] MF, 2-D block BLAS-3 Res/Author s.p.d.
Reference: [79] <author> Kenneth Kundert. </author> <title> Sparse matrix techniques. In Albert Ruehli, editor, Circuit Analysis, Simulation and Design. </title> <publisher> North-Holland, </publisher> <year> 1986. </year>
Reference-contexts: SuperLU LL, partial BLAS-2.5 Pub/UCB unsym. UMFPACK [21, 22] MF, Markowitz BLAS-3 Pub/netlib MA38 (same as UMFPACK) Com/HSL unsym. MA48 [39] Anal: RL, Markowitz Com/HSL Fact: LL, partial BLAS-1, SD unsym. SPARSE <ref> [79] </ref> RL, Markowitz Scalar Pub/netlib sym pattern ) ( MA42 [42] MF, threshold Frontal (eqn+element) BLAS-3 BLAS-3 Com/HSL sym. MA27 [40] MF, LDL T BLAS-1 BLAS-3 Com/HSL s.p.d. Ng & Peyton [89] LL BLAS-3 Pub/Author Shared Memory Algorithms unsym. SuperLU LL, partial BLAS-2.5 Pub/UCB unsym.
Reference: [80] <author> J. W. H. Liu. </author> <title> The role of elimination trees in sparse factorization. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 11 </volume> <pages> 134-172, </pages> <year> 1990. </year>
Reference-contexts: This postorder is an example of what Liu calls an equivalent reordering <ref> [80] </ref>, which does not change the sparsity of the Cholesky factor L, nor the amount of arithmetic to compute L. (Liu proved that any topological ordering, which numbers the children nodes before their parent node, is an equivalent reordering of the given matrix.) The postordered elimination tree can also be used <p> The column etree can be computed from A in time almost linear in the number of nonzeros of A by a variation of an algorithm of Liu <ref> [80] </ref>. The following theorem says that the column etree represents potential dependencies among columns in LU factorization, and that for strong Hall matrices (that is, they cannot be permuted to nontrivial block triangular forms), no stronger information is obtainable from the nonzero structure of A. <p> Then P T LP and P T U P are respectively unit lower triangular and upper triangular, so A = (P T LP )(P T U P ) is also an LU factorization. Remark: Liu <ref> [80] </ref> attributes to F. Peters a result similar to part (3) for the symmetric positive definite case, concerning the Cholesky factor and the (usual, symmetric) elimination tree. For completeness, we give the proof by Gilbert as follows. Proof: Part (1) is immediate from the definition of P . <p> For completeness, we give the proof by Gilbert as follows. Proof: Part (1) is immediate from the definition of P . Part (2) follows from Corollary 6.2 in Liu <ref> [80] </ref>, with the symmetric structure of the column intersection graph of our matrix A taking the place of Liu's symmetric matrix A. (Liu exhibits the isomorphism explicitly in the proof of his Theorem 6.1.) Now we prove part (3).
Reference: [81] <author> Joseph W.H. Liu. </author> <title> Equivalent sparse matrix reordering by elimination tree rotations. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 9(3) </volume> <pages> 424-444, </pages> <month> May </month> <year> 1988. </year> <month> 129 </month>
Reference-contexts: He used a two-step modular approach to ordering a matrix. He first applied a fill-reducing heuristic (such as minimum degree ordering) to the original matrix A, resulting in a permuted matrix A. In the second step, he applied a sequence of his etree rotation operations <ref> [81] </ref> to restructure the etree so that its height is reduced to nearly the minimum.
Reference: [82] <author> Joseph W.H. Liu. </author> <title> Reordering sparse matrices for parallel elimination. </title> <journal> Parallel Computing, </journal> <volume> 11 </volume> <pages> 73-91, </pages> <year> 1989. </year>
Reference-contexts: It is commonly accepted that minimum degree ordering, either on A T A or A + A T , tends to produce tall and narrow etrees. In the context of sparse Cholesky factorization, Liu <ref> [82] </ref> developed a reordering scheme to reduce the height of the etree (relabeling nodes in G (A) as well). He used a two-step modular approach to ordering a matrix.
Reference: [83] <author> Joseph W.H. Liu. </author> <title> The role of elimination trees in sparse factorization. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11(1) </volume> <pages> 134-172, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: the tree can be succinctly represented by the following parent [fl] vector: parent [j] = min f i &gt; j j l ij 6= 0 g : In graph-theoretic terms, the elimination tree is simply the transitive reduction [2] of the directed graph G (L T ), 1 see Liu <ref> [83] </ref>. It is the minimal subgraph of G that preserves paths and provides the smallest possible description of column dependencies in the Cholesky factor. Liu [83] discusses the use of elimination trees in various aspects of sparse algorithms, including reordering, symbolic and numeric factorizations, and parallel elimination. <p> g : In graph-theoretic terms, the elimination tree is simply the transitive reduction [2] of the directed graph G (L T ), 1 see Liu <ref> [83] </ref>. It is the minimal subgraph of G that preserves paths and provides the smallest possible description of column dependencies in the Cholesky factor. Liu [83] discusses the use of elimination trees in various aspects of sparse algorithms, including reordering, symbolic and numeric factorizations, and parallel elimination. The supernode structure has long been recognized and employed in enhancing the efficiency of both the minimum degree ordering [40, 48] and the symbolic factorization [102].
Reference: [84] <author> Joseph W.H. Liu, Esmond G. Ng, and Barry W. Peyton. </author> <title> On finding supernodes for sparse matrix computations. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 14(1) </volume> <pages> 242-252, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: First, we need a notion of fundamental supernode, which was introduced by Ashcraft and Grimes [11] for symmetric matrices. In a fundamental supernode, every column except the last is an only child in the elimination tree. Liu et al. <ref> [84] </ref> gave several reasons why fundamental supernodes are appropriate, one of which is that the set of fundamental supernodes are the same regardless of the particular postordering. For consistency, we now also impose this restriction on the supernodes in L, 2 L c and H, respectively.
Reference: [85] <author> Robert F. Lucas, Tom Blank, and Jerome J. Tiemann. </author> <title> A parallel solution method for large sparse systems of equations. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <volume> CAD-6(6):981-991, </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: George & Ng [57] RL, partial BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL BLAS-3 Com/SGI Ng & Peyton [73] Pub/Author s.p.d. SPLASH [78] RL, 2-D block BLAS-3 Pub/Stanford Distributed Memory Algorithms unsym. van der Stappen [108] RL, Markowitz Scalar Res/Author sym- Lucas et al. <ref> [85] </ref> MF, no pivoting BLAS-1 Res/Author pattern s.p.d. Rothberg et al. [98] RL, 2-D block BLAS-3 Res/Author s.p.d. Gupta [72] MF, 2-D block BLAS-3 Res/Author s.p.d. CAPSS [74] MF, full parallel BLAS-1 Pub/netlib (require coordinates) Table 6.1: Software to solve sparse linear systems using direct methods.
Reference: [86] <author> H. M. Markowitz. </author> <title> The elimination form of the inverse and its application to linear programming. </title> <journal> Management Sci., </journal> <volume> 3 </volume> <pages> 255-269, </pages> <year> 1957. </year>
Reference-contexts: Recent research on unsymmetric systems has concentrated on two basic approaches: submatrix-based (also called right-looking) methods and column-based (also called left-looking) methods. 2 Submatrix methods use k in the outer loop for Equation (2.1). They typically use a combination of some form of Markowitz ordering <ref> [86] </ref> and numerical threshold pivoting [36] to choose the pivot element from the uneliminated submatrix. To illustrate this, let us assume that the first k 1 stages of Gaussian elimination have been completed.
Reference: [87] <author> E. G. Ng and B. W. Peyton. </author> <title> Block sparse Cholesky algorithms on advanced uniprocessor computers. </title> <journal> SIAM J. Scientific and Statistical Computing, </journal> <volume> 14 </volume> <pages> 1034-1056, </pages> <year> 1993. </year>
Reference-contexts: An edge from i to j indicates a nonzero in row i and column j of the matrix. 5 rates of modern sparse Cholesky codes are comparable to those of dense solvers <ref> [87, 95] </ref> for some classes of problems, such as those with a great deal of fill. <p> structure below the diagonal; that is, L (r: s; r: s) is full lower triangular and every row of L (s + 1: n; r: s) is either full or zero. (In Cholesky, supernodes need not consist of contiguous columns, but we will consider only contiguous supernodes.) Ng and Peyton <ref> [87] </ref> analyzed the effect of supernodes in Cholesky factorization on modern uniprocessor machines with memory hierarchies and vector or superscalar hardware. We use Figure 3.1 to illustrate all the benefits from supernodes. <p> Ng and Peyton reported that a sparse Cholesky algorithm based on sup-sup updates typically runs 2.5 to 4.5 times as fast as a col-col algorithm. Indeed, supernodes have become a standard tool in sparse Cholesky factorization <ref> [12, 87, 95, 105] </ref>. To sum up, supernodes as the source of updates (line 4) help because: 1. The inner loop (line 6) over rows i has no indirect addressing. (Sparse BLAS-1 is replaced by dense BLAS-1.) 2. <p> Separate L and U : U (1: j; j) = f (1: j); L (j: n; j) = f (j: n); 12. Scale: L (j: n; j) = L (j: n; j)=L (j; j); 13. Prune symbolic structure based on column j; 14. end for; 24 symmetric case <ref> [87] </ref>. Efficient BLAS-2 matrix-vector kernels can be used for the triangular solve and matrix-vector multiply. Furthermore, all the updates from the supernodal columns can be collected in a temporary packed vector before doing a single scatter into a full-length working array of size n, called SPA (for sparse accumulator [62]). <p> This is analogous to loop tiling techniques used in optimizing compilers to improve cache behavior for two-dimensional arrays with regular stride. It is also somewhat analogous to the supernode-supernode updates that Ng and Peyton <ref> [87] </ref>, and Rothberg and Gupta [95] have used in symmetric Cholesky factorization. The structure of each supernode-column update is the same as in the supernode-column algorithm.
Reference: [88] <author> Esmond G. Ng. </author> <type> Personal communication, </type> <year> 1996. </year>
Reference-contexts: It can be easily incorporated into the column count algorithm for L c . Furthermore, the first vertices of the fundamental supernodes in H are characterized 85 by the following theorem, which we established through discussions with Ng <ref> [88] </ref>. Theorem 8 Vertex j is the first vertex in a fundamental supernode of H if and only if vertex j has two or more children in the column elimination tree T , or j is the column subscript of the first nonzero in some row of A.
Reference: [89] <author> Esmond G. Ng and Barry W. Peyton. </author> <title> Block sparse Cholesky algorithms on advanced uniprocessor computers. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14(5) </volume> <pages> 1034-1056, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Their memory reference patterns are markedly different. Heath, Ng and Peyton [75] gave a thorough survey of many distinctions between left-looking and right-looking sparse Cholesky factorization algorithms. On uniprocessor machines for in-memory problems, Ng and Peyton <ref> [89] </ref> and Rothberg [99] conducted extensive experiments with sparse Cholesky factorization, and concluded that the supernodal left-looking algorithm is somewhat better than the multifrontal approach both in runtime and working storage requirement. <p> MA48 [39] Anal: RL, Markowitz Com/HSL Fact: LL, partial BLAS-1, SD unsym. SPARSE [79] RL, Markowitz Scalar Pub/netlib sym pattern ) ( MA42 [42] MF, threshold Frontal (eqn+element) BLAS-3 BLAS-3 Com/HSL sym. MA27 [40] MF, LDL T BLAS-1 BLAS-3 Com/HSL s.p.d. Ng & Peyton <ref> [89] </ref> LL BLAS-3 Pub/Author Shared Memory Algorithms unsym. SuperLU LL, partial BLAS-2.5 Pub/UCB unsym. PARASPAR [112, 113] RL, Markowitz BLAS-1, SD Res/Author sym- MUPS [6] MF, threshold BLAS-3 Res/Author pattern unsym. George & Ng [57] RL, partial BLAS-1 Res/Author s.p.d.
Reference: [90] <author> Esmond G. Ng and Barry W. Peyton. </author> <title> A supernodal Cholesky factorization algorithm for shared-memory multiprocessors. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14(4) </volume> <pages> 761-769, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: In addition to demonstrating the efficiency of our parallel algorithm on these machines, we also study the (theoretical) upper bound on performance of this algorithm. Several methods have been proposed to perform sparse Cholesky factorization <ref> [49, 73, 90] </ref> and sparse LU factorization [6, 57, 65] on shared memory machines. A common practice is to organize the program as a self-scheduling loop, interacting with a global pool of tasks that are ready to be executed.
Reference: [91] <author> POSIX System Application Arogram Interface: </author> <title> Threads extension [C Language], </title> <type> POSIX 1003.1c draft 4. </type> <institution> IEEE Standards Department. </institution>
Reference-contexts: However, a sustainable bandwidth is 1.6 GBytes/sec. In the parallel program development, we use the pthread interface provided by DECthreads, Digital's multithreading run-time library [26]. The pthread interface implements a version of the POSIX 1003.1c API draft standard for multithreaded programming <ref> [91] </ref>; thus, the code will be easily portable to future systems. Similar to the Solaris threads model, multiple threads execute concurrently within (and share) a single address space.
Reference: [92] <author> A. Pothen, H. D. Simon, L. Wang, and S. Barnard. </author> <title> Towards a fast implementation of spectral nested dissection. </title> <booktitle> In Supercomputing, </booktitle> <publisher> ACM Press, </publisher> <pages> pages 42-51, </pages> <year> 1992. </year>
Reference-contexts: In the first phase, although it is computationally expensive (NP-hard) to find an optimal P in terms of minimizing fills, many heuristics have been used successfully in practice, such as variants of minimum degree orderings [3, 10, 38, 52] and various dissection orderings based on graph partitioning <ref> [15, 58, 92] </ref> or hybrid approaches [13, 18, 76]. Two important data structures have been introduced in efficient implementations of the Cholesky factorization. One is the elimination tree and another is the supernode. The elimination tree [100] is defined for the Cholesky factor L.
Reference: [93] <author> A. Pothen and C. Sun. </author> <title> A distributed multifrontal algorithm using clique trees. </title> <type> Tech report CTC91TR72, </type> <institution> Cornell Theory Center, Cornell University, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: Another is to use a more sophisticated dynamic scheduling algorithm that steals cycles from the idle processors to do useful work. 5.7.1 Independent domains The concept of domains has been widely used in sparse Cholesky factorizations, especially on distributed memory machines <ref> [14, 72, 93, 97] </ref>. A domain refers to a rooted subtree of the elimination tree such that all nodes in this subtree are mapped onto the same processor to factorize. In sparse LU factorization, we may define domains similarly, but we use the column etree. <p> At this level there are exactly P disjoint subtrees, or domains, each being assigned to one processor. A generalization of this method, called proportional mapping, was proposed by Pothen and Sun <ref> [93] </ref>, which is intended to work for any unbalanced tree. First, we compute the amount of arithmetic associated with each node when factorizing the corresponding column, and the amount of arithmetic associated with each subtree. Secondly, we traverse etree in a top-down fashion, starting with P processors at the root.
Reference: [94] <author> Alex Pothen and Chin-Ju Fan. </author> <title> Computing the block triangular form of a sparse matrix. </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> Vol. 16 </volume> <pages> 303-324, </pages> <year> 1990. </year>
Reference-contexts: with some pessimism in scheduling independent tasks. (In Section 5.2.2 we will be concrete about task definition.) For a matrix that is not strong Hall, we might be able to improve the quality of the estimate by permuting the matrix to a block upper triangular form (called the Dulmage-Mendelsohn decomposition) <ref> [94] </ref>, in which each diagonal block is strong Hall. Then, we only need to factorize the diagonal blocks. 74 Having studied the parallelism arising from different subtrees, we now turn our attention to the dependent columns, that is, the columns having ancestor-descendant relations.
Reference: [95] <author> E. Rothberg and A. Gupta. </author> <title> Efficient sparse matrix factorization on high-performance workstations exploiting the memory hierarchy. </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 17 </volume> <pages> 313-334, </pages> <year> 1991. </year>
Reference-contexts: An edge from i to j indicates a nonzero in row i and column j of the matrix. 5 rates of modern sparse Cholesky codes are comparable to those of dense solvers <ref> [87, 95] </ref> for some classes of problems, such as those with a great deal of fill. <p> Ng and Peyton reported that a sparse Cholesky algorithm based on sup-sup updates typically runs 2.5 to 4.5 times as fast as a col-col algorithm. Indeed, supernodes have become a standard tool in sparse Cholesky factorization <ref> [12, 87, 95, 105] </ref>. To sum up, supernodes as the source of updates (line 4) help because: 1. The inner loop (line 6) over rows i has no indirect addressing. (Sparse BLAS-1 is replaced by dense BLAS-1.) 2. <p> This is analogous to loop tiling techniques used in optimizing compilers to improve cache behavior for two-dimensional arrays with regular stride. It is also somewhat analogous to the supernode-supernode updates that Ng and Peyton [87], and Rothberg and Gupta <ref> [95] </ref> have used in symmetric Cholesky factorization. The structure of each supernode-column update is the same as in the supernode-column algorithm.
Reference: [96] <author> E. Rothberg and A. Gupta. </author> <title> An evaluation of left-looking, right-looking and multi-frontal approaches to sparse Cholesky factorization on hierarchical-memory machines. </title> <journal> Int. J. High Speed Computing, </journal> <volume> 5 </volume> <pages> 537-593, </pages> <year> 1993. </year>
Reference-contexts: By instrumenting the code, we found that the working sets of large matrices are much larger than the cache size. Hence, cache thrashing limits performance. We experimented with a scheme suggested by Rothberg <ref> [96] </ref>, in which the SPA has only as many rows as the number of nonzero rows in the panel (as predicted by symbolic 27 1. for j = 1 to n step w do 2. 3. for each updating supernode (r: s) &lt; j in topological order do 4.
Reference: [97] <author> Edward Rothberg. </author> <title> Performance of panel and block approaches to sparse Cholesky factorization on the iPSC/860 and Paragon multicomputers. </title> <journal> SIAM J. Scientific Computing, </journal> <volume> 17(3) </volume> <pages> 699-713, </pages> <month> May </month> <year> 1996. </year> <month> 130 </month>
Reference-contexts: Gupta and Kumar [72] developed a two-dimensional multifrontal Cholesky algorithm on 1024 nodes of the Cray T3D, and achieved up to 20 Gflops factor 8 ization rate for one problem. Rothberg <ref> [97] </ref> developed a two-dimensional block oriented right-looking Cholesky algorithm, and achieved up to 1.7 Gflops factorization rate on 128 nodes of the Intel Paragon. <p> Another is to use a more sophisticated dynamic scheduling algorithm that steals cycles from the idle processors to do useful work. 5.7.1 Independent domains The concept of domains has been widely used in sparse Cholesky factorizations, especially on distributed memory machines <ref> [14, 72, 93, 97] </ref>. A domain refers to a rooted subtree of the elimination tree such that all nodes in this subtree are mapped onto the same processor to factorize. In sparse LU factorization, we may define domains similarly, but we use the column etree. <p> Schreiber [101] modeled the lower bounds on parallel completion time of a left-looking column-oriented sparse Cholesky factorization, and concluded that a two-dimensional mapping is needed to achieve better scalability. Since then, several researchers <ref> [72, 97] </ref> have developed and demonstrated efficient and scalable 2-D distributed algorithms for sparse Cholesky. <p> Compared with the 1-D mapping, asymptotically, both the length of the critical path and the interprocessor communication 120 volume are reduced for a grid model problem <ref> [97] </ref>.
Reference: [98] <author> Edward Rothberg and Robert Schreiber. </author> <title> Improved load distribution in parallel sparse cholesky factorization. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 783-792, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Rothberg [97] developed a two-dimensional block oriented right-looking Cholesky algorithm, and achieved up to 1.7 Gflops factorization rate on 128 nodes of the Intel Paragon. Rothberg and Schreiber <ref> [98] </ref> further improved its performance by better block mapping, and achieved up to 3.2 Gflops factorization rate on 196 nodes of the Intel Paragon. No comprehensive comparisons have yet been made for the unsymmetric LU factorization algorithms. <p> We discussed this method in Section 5.7.1. Secondly, at the higher level of the etree outside domains, we will use a Cartesian product mapping heuristic proposed by Rothberg and Schreiber <ref> [98] </ref>. Here, we can estimate the work associated with each block using the two upper bound matrices H and L c (Section 5.4.3). One important observation is that the mapping functions for rows and columns can be defined independently. <p> Gupta, Rothberg, LL BLAS-3 Com/SGI Ng & Peyton [73] Pub/Author s.p.d. SPLASH [78] RL, 2-D block BLAS-3 Pub/Stanford Distributed Memory Algorithms unsym. van der Stappen [108] RL, Markowitz Scalar Res/Author sym- Lucas et al. [85] MF, no pivoting BLAS-1 Res/Author pattern s.p.d. Rothberg et al. <ref> [98] </ref> RL, 2-D block BLAS-3 Res/Author s.p.d. Gupta [72] MF, 2-D block BLAS-3 Res/Author s.p.d. CAPSS [74] MF, full parallel BLAS-1 Pub/netlib (require coordinates) Table 6.1: Software to solve sparse linear systems using direct methods.
Reference: [99] <author> Edward E. Rothberg. </author> <title> Exploiting the memory hierarchy in sequential and parallel sparse Cholesky factorization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Stanford University, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Their memory reference patterns are markedly different. Heath, Ng and Peyton [75] gave a thorough survey of many distinctions between left-looking and right-looking sparse Cholesky factorization algorithms. On uniprocessor machines for in-memory problems, Ng and Peyton [89] and Rothberg <ref> [99] </ref> conducted extensive experiments with sparse Cholesky factorization, and concluded that the supernodal left-looking algorithm is somewhat better than the multifrontal approach both in runtime and working storage requirement.
Reference: [100] <author> R. Schreiber. </author> <title> A new implementation of sparse Gaussian elimination. </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 8 </volume> <pages> 256-276, </pages> <year> 1982. </year>
Reference-contexts: Two important data structures have been introduced in efficient implementations of the Cholesky factorization. One is the elimination tree and another is the supernode. The elimination tree <ref> [100] </ref> is defined for the Cholesky factor L. Each node in the tree corresponds to one row/column of the matrix.
Reference: [101] <author> R. Schreiber. </author> <title> Scalability of sparse direct solvers. </title> <editor> In Alan George, John R. Gilbert, and Joseph W.H. Liu, editors, </editor> <booktitle> Graph theory and sparse matrix computation, </booktitle> <pages> pages 191-209. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: Using 2-D decomposition instead of 1-D decomposition Another remedy, which we believe will be more effective than simply restructuring the etree, is to parallelize the computation along both row and column dimensions of the matrix. Schreiber <ref> [101] </ref> modeled the lower bounds on parallel completion time of a left-looking column-oriented sparse Cholesky factorization, and concluded that a two-dimensional mapping is needed to achieve better scalability. Since then, several researchers [72, 97] have developed and demonstrated efficient and scalable 2-D distributed algorithms for sparse Cholesky.
Reference: [102] <author> A. H. Sherman. </author> <title> On the efficient solution of sparse systems of linear and nonlinear equations. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1975. </year>
Reference-contexts: Liu [83] discusses the use of elimination trees in various aspects of sparse algorithms, including reordering, symbolic and numeric factorizations, and parallel elimination. The supernode structure has long been recognized and employed in enhancing the efficiency of both the minimum degree ordering [40, 48] and the symbolic factorization <ref> [102] </ref>. A supernode is a set of contiguous columns in the Cholesky factor L that share essentially the same sparsity structure. More recently, supernodes have also been introduced in numeric factorization and triangular solution, in order to make better use of vector registers or cache memory. <p> This is similar to the effect of compressed subscripts in the symmetric case <ref> [102] </ref>.
Reference: [103] <author> A. H. Sherman. </author> <title> Algorithm 533: NSPIV, a FORTRAN subroutine for sparse Gaussian elimination with partial pivoting. </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 4 </volume> <pages> 391-398, </pages> <year> 1978. </year>
Reference-contexts: Pivot search is confined within one column, which can be done inexpensively. One disadvantage of the column methods is that, unlike Markowitz ordering, they do not reorder the columns dynamically, so the fills may be greater. An early example of such a code is Sherman's NSPIV <ref> [103] </ref> (which is actually a row code). Gilbert and Peierls [64] showed how to use depth-first search and topological ordering to obtain the structure of each factor column. This gives a column code that runs in total time proportional to the number of floating-point operations, unlike earlier partial pivoting codes.
Reference: [104] <editor> SGI Power Challenge. </editor> <title> Silicon Graphics, 1995. </title> <type> Technical Report. </type>
Reference-contexts: The primary objective of this chapter is to achieve good efficiency on shared memory systems with a modest number of processors. Examples of such commercially popular machines include Sun SPARCcenter 2000 [107], SGI Power Challenge <ref> [104] </ref>, DEC AlphaServer 8400 [46], and Cray C90/J90 [110, 111]. In addition to demonstrating the efficiency of our parallel algorithm on these machines, we also study the (theoretical) upper bound on performance of this algorithm. <p> Synchronization and context switching of the user-level threads are accomplished rapidly, without entering the OS kernel. 5.1.2 The SGI Power Challenge A 64-bit MIPS R8000 microprocessor and MIPS R8010 floating-point chip are used for each processor of the Power Challenge <ref> [104] </ref>. The chip set delivers peak performance of 360 MIPS and 360 double-precision Mflops with a clock frequency of 90 MHz. This processor was used in Chapter 4, see Table 4.5. Each processor contains a 16 KB direct-mapped level-one data cache in the integer unit (IU).
Reference: [105] <author> H. Simon, P. Vu, and C. Yang. </author> <title> Performance of a supernodal general sparse solver on the CRAY Y-MP: 1.68 GFLOPS with autotasking. </title> <type> Technical Report TR SCA-TR-117, </type> <institution> Boeing Computer Services, </institution> <year> 1989. </year>
Reference-contexts: Ng and Peyton reported that a sparse Cholesky algorithm based on sup-sup updates typically runs 2.5 to 4.5 times as fast as a col-col algorithm. Indeed, supernodes have become a standard tool in sparse Cholesky factorization <ref> [12, 87, 95, 105] </ref>. To sum up, supernodes as the source of updates (line 4) help because: 1. The inner loop (line 6) over rows i has no indirect addressing. (Sparse BLAS-1 is replaced by dense BLAS-1.) 2.
Reference: [106] <institution> Solaris SunOS 5.0 multithread architecture. Sun Microsystems, Inc., </institution> <month> November </month> <year> 1991. </year> <type> Technical White Paper. </type>
Reference-contexts: The packet-switched design permits split phase transactions of bus requests and their corresponding replies, and so enjoys higher bus utilization than circuit-switching. The dual buses provide 500 MB/sec effective data transfer bandwidth. To access multiple processors, we use a user-level multithread library implemented in the Solaris 2.x operating system <ref> [106] </ref>. In this model, the lightweight user-level threads within a single UNIX process are multiplexed on top of kernel-supported threads.
Reference: [107] <institution> SPARCcenter 2000 architecture and implementation. Sun Microsystems, Inc., </institution> <note> Novem-ber 1993. Technical White Paper. </note>
Reference-contexts: The primary objective of this chapter is to achieve good efficiency on shared memory systems with a modest number of processors. Examples of such commercially popular machines include Sun SPARCcenter 2000 <ref> [107] </ref>, SGI Power Challenge [104], DEC AlphaServer 8400 [46], and Cray C90/J90 [110, 111]. In addition to demonstrating the efficiency of our parallel algorithm on these machines, we also study the (theoretical) upper bound on performance of this algorithm. <p> It is important to minimize the use of critical sections to obtain the best performance. 5.1.1 The Sun SPARCcenter 2000 Each processor in the Sun SPARCcenter 2000 <ref> [107] </ref> is a SuperSPARC microprocessor rated at 50 MHz. The processor is capable of executing up to three independent instructions per clock cycle.
Reference: [108] <author> A. Frank van der Stappen, Rob H. Bisseling, and Johannes G. G. van der Vorst. </author> <title> Parallel sparse LU decomposition on a mesh network of transputers. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 14(3) </volume> <pages> 853-879, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: George & Ng [57] RL, partial BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL BLAS-3 Com/SGI Ng & Peyton [73] Pub/Author s.p.d. SPLASH [78] RL, 2-D block BLAS-3 Pub/Stanford Distributed Memory Algorithms unsym. van der Stappen <ref> [108] </ref> RL, Markowitz Scalar Res/Author sym- Lucas et al. [85] MF, no pivoting BLAS-1 Res/Author pattern s.p.d. Rothberg et al. [98] RL, 2-D block BLAS-3 Res/Author s.p.d. Gupta [72] MF, 2-D block BLAS-3 Res/Author s.p.d.
Reference: [109] <author> S. A. Vavasis. </author> <title> Stable finite elements for problems with wild coefficients. </title> <type> Technical Report 93-1364, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <year> 1993. </year> <note> To appear in SIAM J. Numerical Analysis. </note>
Reference-contexts: Wang3 is from solving a coupled nonlinear PDE system in a 3-D (30 fi 30 fi 30 uniform mesh) semiconductor device simulation, as provided by Song Wang of the University of New South Wales, Sydney. Vavasis3 is an unsymmetric augmented matrix for a 2-D PDE with highly varying coefficients <ref> [109] </ref>. Dense1000 is a dense 1000 fi 1000 random matrix. The matrices are sorted in increasing order of f lops=nnz (F ), the ratio of the number of floating-point operations to the number of nonzeros nnz (F ) in the factored matrix F = U + L I.
Reference: [110] <institution> The Cray C90 series. http://www.cray.com/PUBLIC/product-info/C90/. Cray Research, Inc. </institution>
Reference-contexts: The primary objective of this chapter is to achieve good efficiency on shared memory systems with a modest number of processors. Examples of such commercially popular machines include Sun SPARCcenter 2000 [107], SGI Power Challenge [104], DEC AlphaServer 8400 [46], and Cray C90/J90 <ref> [110, 111] </ref>. In addition to demonstrating the efficiency of our parallel algorithm on these machines, we also study the (theoretical) upper bound on performance of this algorithm. <p> Similar to the Solaris threads model, multiple threads execute concurrently within (and share) a single address space. On the DEC, the multithreaded program is capable of utilizing multiple processors if the operating system supports kernel threads. 5.1.4 The Cray C90/J90 The Cray C90 <ref> [110] </ref> and J90 [111] are Cray Research's two series of vector supercomputers. The J90 series is the latest entry-level supercomputing system that is designed to address low price and high performance. Both systems have multiple processors, in which each processor is a vector machine.
Reference: [111] <institution> The Cray J90 series. http://www.cray.com/PUBLIC/product-info/J90/. Cray Research, Inc. </institution>
Reference-contexts: The primary objective of this chapter is to achieve good efficiency on shared memory systems with a modest number of processors. Examples of such commercially popular machines include Sun SPARCcenter 2000 [107], SGI Power Challenge [104], DEC AlphaServer 8400 [46], and Cray C90/J90 <ref> [110, 111] </ref>. In addition to demonstrating the efficiency of our parallel algorithm on these machines, we also study the (theoretical) upper bound on performance of this algorithm. <p> Similar to the Solaris threads model, multiple threads execute concurrently within (and share) a single address space. On the DEC, the multithreaded program is capable of utilizing multiple processors if the operating system supports kernel threads. 5.1.4 The Cray C90/J90 The Cray C90 [110] and J90 <ref> [111] </ref> are Cray Research's two series of vector supercomputers. The J90 series is the latest entry-level supercomputing system that is designed to address low price and high performance. Both systems have multiple processors, in which each processor is a vector machine.
Reference: [112] <author> Z. Zlatev, J. Wasniewski, P. C. Hansen, and Tz. Ostromsky. PARASPAR: </author> <title> a package for the solution of large linear algebraic equations on parallel computers with shared memory. </title> <type> Technical Report 95-10, </type> <institution> Technical University of Denmark, </institution> <month> September </month> <year> 1995. </year> <month> 131 </month>
Reference-contexts: SPARSE [79] RL, Markowitz Scalar Pub/netlib sym pattern ) ( MA42 [42] MF, threshold Frontal (eqn+element) BLAS-3 BLAS-3 Com/HSL sym. MA27 [40] MF, LDL T BLAS-1 BLAS-3 Com/HSL s.p.d. Ng & Peyton [89] LL BLAS-3 Pub/Author Shared Memory Algorithms unsym. SuperLU LL, partial BLAS-2.5 Pub/UCB unsym. PARASPAR <ref> [112, 113] </ref> RL, Markowitz BLAS-1, SD Res/Author sym- MUPS [6] MF, threshold BLAS-3 Res/Author pattern unsym. George & Ng [57] RL, partial BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL BLAS-3 Com/SGI Ng & Peyton [73] Pub/Author s.p.d.
Reference: [113] <author> Zahari Zlatev. </author> <title> Computational methods for general sparse matrices. </title> <publisher> Kluwer Academic, </publisher> <address> Dordrecht; Boston, </address> <year> 1991. </year>
Reference-contexts: SPARSE [79] RL, Markowitz Scalar Pub/netlib sym pattern ) ( MA42 [42] MF, threshold Frontal (eqn+element) BLAS-3 BLAS-3 Com/HSL sym. MA27 [40] MF, LDL T BLAS-1 BLAS-3 Com/HSL s.p.d. Ng & Peyton [89] LL BLAS-3 Pub/Author Shared Memory Algorithms unsym. SuperLU LL, partial BLAS-2.5 Pub/UCB unsym. PARASPAR <ref> [112, 113] </ref> RL, Markowitz BLAS-1, SD Res/Author sym- MUPS [6] MF, threshold BLAS-3 Res/Author pattern unsym. George & Ng [57] RL, partial BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL BLAS-3 Com/SGI Ng & Peyton [73] Pub/Author s.p.d.
References-found: 113

