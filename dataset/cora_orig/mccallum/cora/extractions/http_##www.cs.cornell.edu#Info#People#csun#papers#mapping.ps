URL: http://www.cs.cornell.edu/Info/People/csun/papers/mapping.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/csun/sun.html
Root-URL: 
Title: Mapping Algorithm for Parallel Sparse Cholesky Factorization 1  
Author: Alex Pothen and Chunguang Sun 
Keyword: sparse Cholesky factorization, clique tree, distributed-memory multiprocessor, mapping algorithms, multifrontal method, parallel computing.  
Web: 65F50, 65F05.  
Note: A  AMS(MOS) subject classifications: primary  
Abstract: This paper appears in SIAM Journal on Scientific Computing, Vol 14, No. 5, pp. 1253-1257, Sep 1993. Abstract We describe a task-to-processor mapping algorithm for computing the parallel multi-frontal Cholesky factorization of irregular sparse problems on distributed-memory multiprocessors. The performance of the mapping algorithm is compared with the only general mapping algorithm previously reported. Using this mapping, our distributed multifrontal algorithm is nearly as efficient on a collection of problems with irregular sparsity structure as it is for the regular grid problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Ashcraft, </author> <title> The aggregate/element model for distributed column-based Cholesky factorization, </title> <type> Tech. Report ECA-TR-160, </type> <institution> Boeing Computer Services, </institution> <address> Seattle, WA, </address> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: 1. Introduction. Several implementations of parallel algorithms for sparse Cholesky factorization on distributed-memory multiprocessors have been described in the literature <ref> [1, 2, 3, 6, 8, 12] </ref>. Extensive discussions and bibliographies of previous work may be found in Liu's recent survey [11] of the multifrontal method, and in Heath, Ng, and Peyton's survey [9] of parallel sparse matrix factorization algorithms.
Reference: [2] <author> C. Ashcraft, S. C. Eisenstat, and J. W. H. Liu, </author> <title> A fan-in algorithm for distributed sparse numerical factorization, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11 (1990), </volume> <pages> pp. 593-599. </pages>
Reference-contexts: 1. Introduction. Several implementations of parallel algorithms for sparse Cholesky factorization on distributed-memory multiprocessors have been described in the literature <ref> [1, 2, 3, 6, 8, 12] </ref>. Extensive discussions and bibliographies of previous work may be found in Liu's recent survey [11] of the multifrontal method, and in Heath, Ng, and Peyton's survey [9] of parallel sparse matrix factorization algorithms.
Reference: [3] <author> C. Ashcraft, S. C. Eisenstat, J. W. H. Liu, and A. H. Sherman, </author> <title> A comparison of three column-based distributed sparse factorization schemes, </title> <type> Tech. Report CS-90-09, </type> <institution> Computer Science, York University, </institution> <address> North York, Ontario, Canada, </address> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: 1. Introduction. Several implementations of parallel algorithms for sparse Cholesky factorization on distributed-memory multiprocessors have been described in the literature <ref> [1, 2, 3, 6, 8, 12] </ref>. Extensive discussions and bibliographies of previous work may be found in Liu's recent survey [11] of the multifrontal method, and in Heath, Ng, and Peyton's survey [9] of parallel sparse matrix factorization algorithms.
Reference: [4] <author> G. A. Geist and E. G. Y. Ng, </author> <title> Task scheduling for parallel sparse Cholesky factorization, </title> <journal> Internat. J. Par. Progr., </journal> <volume> 18 (1989), </volume> <pages> pp. 291-314. </pages>
Reference-contexts: Extensive discussions and bibliographies of previous work may be found in Liu's recent survey [11] of the multifrontal method, and in Heath, Ng, and Peyton's survey [9] of parallel sparse matrix factorization algorithms. Geist and Ng <ref> [4] </ref> have discussed a bin-pack mapping strategy for the fan-out algorithm suitable for mapping irregular problems. <p> In this paper we describe a new task-to-processor mapping algorithm suitable for irregular sparse problems and apply it to the computation of a distributed multifrontal factorization. We compare the performance of the mapping algorithm with an adaptation of the sole previously reported general mapping algorithm|the bin-pack mapping algorithm <ref> [4] </ref>. We have experimented with a collection of problems with irregular sparsity structure and have obtained efficiencies comparable to those for the regular grid problems. A multifrontal factorization is computed as a sequence of partial factorizations of a set of dense submatrices. <p> We have adapted the bin-pack mapping scheme employed by Geist and Ng <ref> [4] </ref> originally to map elimination trees within the fan-out algorithm to map the clique tree for the multifrontal factorization instead. In this scheme, the clique tree is explored beginning at the root until at least vertex-disjoint subtrees are obtained.
Reference: [5] <author> J. A. George, </author> <title> Nested dissection of a regular finite element mesh, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 10 (1973), </volume> <pages> pp. 345-363. </pages>
Reference-contexts: Geist and Ng [4] have discussed a bin-pack mapping strategy for the fan-out algorithm suitable for mapping irregular problems. However, performance results for distributed sparse Cholesky factorization algorithms have been limited to regular model problems|e.g., the n fi n nine-point grid ordered by optimal nested dissection <ref> [5] </ref> and a slightly less regular L-shaped problem. Most practical problems are far more irregular in structure than these model problems, and several algorithmic issues need to be addressed to obtain good performance on such problems.
Reference: [6] <author> J. A. George, M. T. Heath, J. W. H. Liu, and E. G. Y. Ng, </author> <title> Solution of sparse positive definite systems on a hypercube, </title> <journal> J. Comp. Appl. Math, </journal> <volume> 27 (1989), </volume> <pages> pp. 129-156. </pages>
Reference-contexts: 1. Introduction. Several implementations of parallel algorithms for sparse Cholesky factorization on distributed-memory multiprocessors have been described in the literature <ref> [1, 2, 3, 6, 8, 12] </ref>. Extensive discussions and bibliographies of previous work may be found in Liu's recent survey [11] of the multifrontal method, and in Heath, Ng, and Peyton's survey [9] of parallel sparse matrix factorization algorithms.
Reference: [7] <author> J. A. George, J. W. H. Liu, and E. G. Y. Ng, </author> <title> Communication results for parallel sparse Cholesky factorization on a hypercube, Par. </title> <journal> Comput., </journal> <volume> 10 (1989), </volume> <pages> pp. 287-298. </pages>
Reference-contexts: Mapping schemes. The subtree-to-subcube mapping scheme was proposed by George, Liu, and Ng <ref> [7] </ref> for the model grid problem. This scheme is only effective for problems with balanced tree structure and almost balanced workload distribution such as the model problem ordered by optimal nested dissection.
Reference: [8] <author> J. R. Gilbert and R. S. Schreiber, </author> <title> Highly parallel sparse Cholesky factorization, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 13 (1992), </volume> <pages> pp. 1151-1172. </pages>
Reference-contexts: 1. Introduction. Several implementations of parallel algorithms for sparse Cholesky factorization on distributed-memory multiprocessors have been described in the literature <ref> [1, 2, 3, 6, 8, 12] </ref>. Extensive discussions and bibliographies of previous work may be found in Liu's recent survey [11] of the multifrontal method, and in Heath, Ng, and Peyton's survey [9] of parallel sparse matrix factorization algorithms. <p> Mapping schemes. The subtree-to-subcube mapping scheme was proposed by George, Liu, and Ng [7] for the model grid problem. This scheme is only effective for problems with balanced tree structure and almost balanced workload distribution such as the model problem ordered by optimal nested dissection. Gilbert and Schreiber <ref> [8] </ref> use a 2-dimensional bin-pack mapping to allocate processors for factoring the frontal matrices at a stage in their CM-2 multifrontal factorization, and Ashcraft (personal communication) has considered a generalization of the subtree-to-subcube mapping within his domain-separator model of Cholesky factorization.
Reference: [9] <author> M. T. Heath, E. G. Y. Ng, and B. W. Peyton, </author> <title> Parallel algorithms for sparse linear systems, </title> <journal> SIAM Review, </journal> <volume> 33 (1991), </volume> <pages> pp. 420-460. </pages>
Reference-contexts: Extensive discussions and bibliographies of previous work may be found in Liu's recent survey [11] of the multifrontal method, and in Heath, Ng, and Peyton's survey <ref> [9] </ref> of parallel sparse matrix factorization algorithms. Geist and Ng [4] have discussed a bin-pack mapping strategy for the fan-out algorithm suitable for mapping irregular problems.
Reference: [10] <author> J. G. Lewis, B. W. Peyton, and A. Pothen, </author> <title> A fast algorithm for reordering sparse matrices for parallel factorization, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10 (1989), </volume> <pages> pp. 1146-1173. </pages>
Reference-contexts: We have chosen these dense submatrices to correspond to the maximal cliques from a clique tree representation of the Cholesky factor. Discussions of clique trees in the context of sparse matrix algorithms may be found in <ref> [10] </ref> and elsewhere. We have found the clique tree to be a convenient data structure for organizing the distributed y This research was supported by NSF grant CCR-9024954 and by U. S.
Reference: [11] <author> J. W. H. Liu, </author> <title> The multifrontal method for sparse matrix solution: </title> <journal> theory and practice, SIAM Review, </journal> <volume> 34 (1992), </volume> <pages> pp. 82-109. </pages>
Reference-contexts: 1. Introduction. Several implementations of parallel algorithms for sparse Cholesky factorization on distributed-memory multiprocessors have been described in the literature [1, 2, 3, 6, 8, 12]. Extensive discussions and bibliographies of previous work may be found in Liu's recent survey <ref> [11] </ref> of the multifrontal method, and in Heath, Ng, and Peyton's survey [9] of parallel sparse matrix factorization algorithms. Geist and Ng [4] have discussed a bin-pack mapping strategy for the fan-out algorithm suitable for mapping irregular problems.
Reference: [12] <author> R. Lucas, T. Blank, and J. Tiemann, </author> <title> A parallel solution method for large sparse systems of equations, </title> <journal> IEEE Trans. Computer-Aided Design, </journal> <month> CAD-6 </month> <year> (1987), </year> <pages> pp. 981-991. </pages>
Reference-contexts: 1. Introduction. Several implementations of parallel algorithms for sparse Cholesky factorization on distributed-memory multiprocessors have been described in the literature <ref> [1, 2, 3, 6, 8, 12] </ref>. Extensive discussions and bibliographies of previous work may be found in Liu's recent survey [11] of the multifrontal method, and in Heath, Ng, and Peyton's survey [9] of parallel sparse matrix factorization algorithms.
Reference: [13] <author> A. Pothen, H. D. Simon, and L. Wang, </author> <title> Spectral nested dissection, </title> <type> Tech. Report 92-01, </type> <institution> Computer Science, Pennsylvania State University, University Park, </institution> <address> PA, </address> <year> 1992. </year> <note> Submitted. </note>
Reference-contexts: Optimal nested dissection of the model problem creates well-balanced trees, while the MMD ordering of the less regular problems leads to less balanced trees. The effect of recent orderings with low fill and good balance such as spectral nested dissection <ref> [13] </ref> remains to be evaluated.
Reference: [14] <author> A. Pothen and C. Sun, </author> <title> A distributed multifrontal algorithm using clique trees, </title> <type> Tech. Report 91-24, </type> <institution> Computer Science, Pennsylvania State University, University Park, </institution> <address> PA, </address> <month> Sep. </month> <year> 1991. </year>
Reference-contexts: It also affords clear and concise descriptions and implementations of the distributed assembly and factorization algorithms. A detailed description of these algorithms is provided in <ref> [14, 16] </ref>. We denote the number of maximal cliques in the clique tree by m, the size of the clique tree (the sum of the number of vertices in the maximal cliques) by q, and the number of processors by . 2. Mapping schemes.
Reference: [15] <author> P. Raghavan, </author> <title> Distributed Sparse Matrix Factorization: QR and Cholesky Decompositions, </title> <type> PhD thesis, </type> <institution> The Pennsylvania State University, University Park, </institution> <address> PA, </address> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: A variant of this mapping has been considered in the context of distributed sparse orthogonal factorization and the fan-in and fan-out algorithms <ref> [15] </ref>. The algorithm is described in Fig. 1.

References-found: 15

