URL: ftp://ftp.cs.brown.edu/pub/techreports/94/cs94-14.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-94-14.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> K. J. Astrom. </author> <title> Optimal control of markov decision processes with incomplete state estimation. </title> <journal> J. Math. Anal. Appl., </journal> <volume> 10 </volume> <pages> 174-205, </pages> <year> 1965. </year>
Reference-contexts: Among the earliest work dealing with partial observability are [5] and <ref> [1] </ref>. Although none of these presents algorithmic solutions to the general pomdp model, each provided some of the groundwork for treating the general problem. In this sub-section we will give just an overview of some of the pomdp researchers and their work. <p> In Cheng's thesis he refers to these vectors as the linear supports of the value 75 function. The algorithm starts by initializing a search list with the extreme points on the belief simplex (e.g., <ref> [1, 0, 0, : : : ] </ref>, [0, 1, 0, 0, : : : ], [0, 0, 1, 0, : : : ], etc.)and an empty set of vectors, ^ V that at any point in time form the current approximation. <p> In Cheng's thesis he refers to these vectors as the linear supports of the value 75 function. The algorithm starts by initializing a search list with the extreme points on the belief simplex (e.g., [1, 0, 0, : : : ], <ref> [0, 1, 0, 0, : : : ] </ref>, [0, 0, 1, 0, : : : ], etc.)and an empty set of vectors, ^ V that at any point in time form the current approximation. <p> In Cheng's thesis he refers to these vectors as the linear supports of the value 75 function. The algorithm starts by initializing a search list with the extreme points on the belief simplex (e.g., [1, 0, 0, : : : ], [0, 1, 0, 0, : : : ], <ref> [0, 0, 1, 0, : : : ] </ref>, etc.)and an empty set of vectors, ^ V that at any point in time form the current approximation.
Reference: [2] <author> L. E. Baum. </author> <title> An inequality and associated maximization technique in statistical estimation for probabilistic functions of a markov process. </title> <booktitle> Inequalities 3, </booktitle> <pages> pages 1-8, </pages> <year> 1972. </year>
Reference: [3] <author> Richard Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1957. </year>
Reference-contexts: For non-finitely transient policies we will not be able to construct policy graphs that are exactly optimal. 4 Finite Horizon Algorithms 4.1 POMDP History pomdp research grew directly out of the mdp research, both of which began to flourish in the 1960's. <ref> [3] </ref> and [7] provided much of the basic framework and solution procedures for mdp models. Among the earliest work dealing with partial observability are [5] and [1]. Although none of these presents algorithmic solutions to the general pomdp model, each provided some of the groundwork for treating the general problem.
Reference: [4] <author> Hsien-Te Cheng. </author> <title> Algorithms for Partially Observable Markov Decision Processes. </title> <type> PhD thesis, </type> <institution> University of British Columbia, British Columbia, Canada, </institution> <year> 1988. </year>
Reference-contexts: However, even with this optimization optimal policies to small problems can still require a substantial amount of computational resources. This technique still requires enumerating all the possible vectors, of which there are usually a large number. Cheng introduced two algorithms <ref> [4] </ref> the first of which (Relaxed Region) is very similar to Sondik's except that he defines regions that are typically larger than Sondik's and thus, the algorithm tends to be more efficient. <p> Some lp packages will implicitly perform this check for you as part of its optimization scheme. 4.7 Cheng's Algorithms In <ref> [4] </ref> two new algorithms are presented. They are both heavily influence by Sondik's One Pass algorithm, but typically require less computation time. One major deviation present in Cheng's algorithms is the elimination of the use of linear programming. <p> Notice that the optimal value function must always lie equal to or above the approximation. Since both the true function and the approximation are piecewise linear and convex the largest difference must occur at a corner point. This is proved in <ref> [4] </ref>. With this handy piece of information, Cheng then finds all the corner points of the regions in the partition induced by the approximation. Here again, Cheng utilizes an interior point algorithm instead of linear programming, since he must ensure all corner points are generated. <p> The most depressing news of solving the pomdp model is that it is impossible to compute the optimal policy for anything but small problems. 5.3 Contributions The existing literature on pomdps suffers from a few problems which this work has addressed. The papers that present specific algorithms <ref> [17, 16, 6, 4] </ref> are difficult to follow unless the reader already has an intimate knowledge of the area . The survey articles [12, 9] are at too high level to give much insight into the details of pomdps and the solution procedures. <p> Though at an extremely crude level, this analysis identifies the limiting elements of the running times of each and allows comparison between them. This rough analysis coincides with the existing empirical performance results, based upon our results and those of <ref> [4] </ref>.
Reference: [5] <author> A. W. Drake. </author> <title> Observation of a Markov Process Through a Noisy Channel. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts, </institution> <year> 1962. </year>
Reference-contexts: Among the earliest work dealing with partial observability are <ref> [5] </ref> and [1]. Although none of these presents algorithmic solutions to the general pomdp model, each provided some of the groundwork for treating the general problem. In this sub-section we will give just an overview of some of the pomdp researchers and their work.
Reference: [6] <author> James N. Eagle. </author> <title> The optimal search for a moving target when the search path is constrained. </title> <journal> Operations Research, </journal> <volume> 32(5) </volume> <pages> 1107-1115, </pages> <year> 1984. </year>
Reference-contexts: Thus, Monahan decides to enumerate all possible ff (t) vectors and check each for validity afterwards. Eagle <ref> [6] </ref> presented the details of an optimization of this method, first suggested by Monahan, that reduces the work needed to solve the problem. <p> In <ref> [6] </ref> this optimization is made more explicit. The optimization occurs in the phase where we need to enumerate all of the possible new ff (t) vectors. <p> The most depressing news of solving the pomdp model is that it is impossible to compute the optimal policy for anything but small problems. 5.3 Contributions The existing literature on pomdps suffers from a few problems which this work has addressed. The papers that present specific algorithms <ref> [17, 16, 6, 4] </ref> are difficult to follow unless the reader already has an intimate knowledge of the area . The survey articles [12, 9] are at too high level to give much insight into the details of pomdps and the solution procedures.
Reference: [7] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1960. </year>
Reference-contexts: For non-finitely transient policies we will not be able to construct policy graphs that are exactly optimal. 4 Finite Horizon Algorithms 4.1 POMDP History pomdp research grew directly out of the mdp research, both of which began to flourish in the 1960's. [3] and <ref> [7] </ref> provided much of the basic framework and solution procedures for mdp models. Among the earliest work dealing with partial observability are [5] and [1]. Although none of these presents algorithmic solutions to the general pomdp model, each provided some of the groundwork for treating the general problem.
Reference: [8] <author> Michael L. Littman. </author> <title> The witness algorithm for solving partially observable markov decision processes. </title> <type> Technical report, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1994. </year>
Reference-contexts: The actual method that needs to be used to break ties among vectors is conceptually straightforward. Unfortunately, the proof that this procedure is correct is quite tedious. Here we will only describe the procedure, though <ref> [8] </ref> gives the formal proof in its entirety. As mentioned before, there are two places where a maximization process can result in ties. The simplest to address is when ties result from formula 7. <p> We believe our approach is helpful for understanding pomdps and we have not seen it done this way in any other work to date. This paper has also attempted to sort out and clarify many of the existing bugs in the literature. There is a related document, <ref> [8] </ref>, that presents the Witness algorithm in a more rigorous and formal setting. It provides many of the derivations and proofs that are required to show that the algorithm is correct. In addition, it also delves into the details about the approximation bounds when exact 97 solutions cannot be found.
Reference: [9] <author> William S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observed markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28(1) </volume> <pages> 47-65, </pages> <year> 1991. </year>
Reference-contexts: Use of a linear programming formulation has resulted in empirically better running times. At this time it appears that seeking exact solutions to anything but small pomdps is not practical. There are many techniques for determining approximate methods and many of these are discussed in <ref> [9] </ref>. 4.2 Overview of Algorithms In the following sections we will give detailed descriptions of some of the better known algorithms for solving finite horizon pomdps. We have ordered the algorithms in a slightly out-of-chronological manner. <p> The formulas and notation in this article are much easier to follow than that of his thesis, and his article is devoid of the many theorems and proofs that serve to complicate the issues. However, a few typographical errors, a couple of vague sentences and one serious oversight <ref> [13, 9] </ref> make this exactly the wrong primary source to use for an attempted implementation. 4.6.2 The One-Pass Algorithm This algorithm begins where all the other ones begin; with the recursive value function: V fl a i i + fl j; ij r a (;a;) Here, unlike the exhaustive enumeration of <p> The papers that present specific algorithms [17, 16, 6, 4] are difficult to follow unless the reader already has an intimate knowledge of the area . The survey articles <ref> [12, 9] </ref> are at too high level to give much insight into the details of pomdps and the solution procedures. To compound this problem, each author uses different notation and terminology than the others.
Reference: [10] <author> T. H. Mattheis. </author> <title> An algorithm for determining irrelevant constraints and all verticies in systems of linear inequalities. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 247-260, </pages> <year> 1973. </year>
Reference-contexts: They are both heavily influence by Sondik's One Pass algorithm, but typically require less computation time. One major deviation present in Cheng's algorithms is the elimination of the use of linear programming. Instead, Cheng opts for the interior point method for convex polytopes (see <ref> [10] </ref> and [11]) to find the corner points of the regions define on the belief space. This interior point method is similiar to linear programming, but can require significantly more time.
Reference: [11] <author> T. H. Mattheis and David S. Rubin. </author> <title> A survey and comparison of methods for finding all vertices of convex polyhedral sets. </title> <journal> Mathematics of Operations Research, </journal> <volume> 5(2) </volume> <pages> 167-185, </pages> <year> 1980. </year> <month> 99 </month>
Reference-contexts: They are both heavily influence by Sondik's One Pass algorithm, but typically require less computation time. One major deviation present in Cheng's algorithms is the elimination of the use of linear programming. Instead, Cheng opts for the interior point method for convex polytopes (see [10] and <ref> [11] </ref>) to find the corner points of the regions define on the belief space. This interior point method is similiar to linear programming, but can require significantly more time.
Reference: [12] <author> George E. Monahan. </author> <title> A survey of partially observable markov deci-sion processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28(1) </volume> <pages> 1-16, </pages> <year> 1982. </year>
Reference-contexts: Once this region was known you would know exactly where points in neighboring regions would lie, namely, on the borders of the current region. The algorithm was guaranteed to terminate since there are only a finite number of these regions in the finite horizon problem. Monahan <ref> [12] </ref> presented a much simpler algorithm for computing optimal policies for the finite horizon. Although it is simpler to understand, at first it appears more inefficient than Sondik's method, because it is an exhaustive algorithm. However, for most problems it turns out to actually be more efficient than Sondik's method. <p> To summarize the addition of the ffi variable, the new Monahan lp formulation for a vector, ff j (t), is: max : ffi i i (t) ff i (t)) + ffi 0; 8k: i 54 4.5 Eagle's Variant of Monahan's Algorithm Monahan <ref> [12] </ref> mentions that dominated vectors could be removed to help reduce the number of linear programs that need to be solved. In [6] this optimization is made more explicit. The optimization occurs in the phase where we need to enumerate all of the possible new ff (t) vectors. <p> The papers that present specific algorithms [17, 16, 6, 4] are difficult to follow unless the reader already has an intimate knowledge of the area . The survey articles <ref> [12, 9] </ref> are at too high level to give much insight into the details of pomdps and the solution procedures. To compound this problem, each author uses different notation and terminology than the others.
Reference: [13] <author> Sraban Mukherjee and Kiran Seth. </author> <title> A corrected and improved computational scheme for partially observable markov processes. </title> <journal> INFOR, </journal> <volume> 29(3) </volume> <pages> 206-212, </pages> <year> 1991. </year>
Reference-contexts: The formulas and notation in this article are much easier to follow than that of his thesis, and his article is devoid of the many theorems and proofs that serve to complicate the issues. However, a few typographical errors, a couple of vague sentences and one serious oversight <ref> [13, 9] </ref> make this exactly the wrong primary source to use for an attempted implementation. 4.6.2 The One-Pass Algorithm This algorithm begins where all the other ones begin; with the recursive value function: V fl a i i + fl j; ij r a (;a;) Here, unlike the exhaustive enumeration of
Reference: [14] <author> Anton Schwartz. </author> <title> A reinforcement learning method for maximizing undis-counted rewards. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, Massachusetts, 1993. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: [15] <author> Satinder Pal Singh, Tommi Jaakkola, and Michael I. Jordan. </author> <title> Model-free reinforcement learning for non-markovian decision problems. </title> <booktitle> In Proceedings of the Machine Learning Conference, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference: [16] <author> Richard D. Smallwood and Edward J. Sondik. </author> <title> The optimal control of partially observable markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 1071-1088, </pages> <year> 1973. </year>
Reference-contexts: An attempt is made in a later section to present the details of Sondik's algorithm in a language that is more comprehensible than his thesis or subsequently published article <ref> [16] </ref>. Sondik's main contribution was to observe that, since there were a finite number of linear segments (regions over the belief simplex) in the value function, we could determine all of these segments by iteratively finding a particular segment and the belief space region for which it was optimal. <p> The exception, Mon-ahan's algorithm, is conceptually much simpler because it does not seek to find this set of points. 4.4 Monahan's Algorithm We start with the easiest algorithm of the bunch, though it was presented after the landmark work of Sondik ([17] <ref> [16] </ref>). Monahan actually credits this algorithm to Sondik and, though there are similarities, Monahan's description of Sondik's algorithm has enough differences to warrant a separate treatment. Monahan's algorithm is both easy to understand and easy to implement. <p> Sondik [17] presented the first solution techniques for finding optimal policies for general pomdp problems. This was the seminal work from which all the other algorithms described in this paper were derived. His algorithm for the finite horizon case is also described in <ref> [16] </ref>. This latter reference can be both a help and a hindrance. The formulas and notation in this article are much easier to follow than that of his thesis, and his article is devoid of the many theorems and proofs that serve to complicate the issues. <p> The following insight is directly from <ref> [16] </ref>: The procedure can be made more efficient if, for each iteration of the linear programming problem with the k th inequality as the objective function, all other constraints are tested as objective functions to see if they are optimized at the current feasible solution. <p> The most depressing news of solving the pomdp model is that it is impossible to compute the optimal policy for anything but small problems. 5.3 Contributions The existing literature on pomdps suffers from a few problems which this work has addressed. The papers that present specific algorithms <ref> [17, 16, 6, 4] </ref> are difficult to follow unless the reader already has an intimate knowledge of the area . The survey articles [12, 9] are at too high level to give much insight into the details of pomdps and the solution procedures.
Reference: [17] <author> Edward J. Sondik. </author> <title> The Optimal Control of Partially Observable Markov Processes. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, California, </institution> <year> 1971. </year>
Reference-contexts: To complete the proof, we use the base case referred to earlier. The terminating rewards we defined in formula 4 show that the value function for t = 0 is also piecewise linear and convex. This proof was first shown in <ref> [17] </ref> as was the observation that the value function could be represented by a finite set of vectors. <p> This can become time consuming. Fortunately there is a way to encode the policy in a graph such that no explicit belief states need to be maintained and no dot products performed. We refer to such graph as policy graphs and they appear first in <ref> [17] </ref>. The policy graphs we discuss apply to the infinite horizon solutions. Recall Figure 20 where we have converged upon an infinite horizon policy. If we continued to run the algorithm longer and longer, we would continue to get the exact same structure. <p> However, this will only happen for a certain class of problems. Only when it does converge can we can construct the policy graph. Finite Transience is formally defined in <ref> [17] </ref> and is the property of a policy not of the pomdp problem. Here, we opt for a more intuitive understanding. Recall that a particular policy will define a partition of the belief space and that each node of the policy graph represents one of these regions. <p> Their algorithms are treated in more detail in the subsequent sub-sections. The first researcher to give a detailed algorithm for finding optimal policies for the general pomdp model is E. J. Sondik <ref> [17] </ref>. <p> It is unlikely that the any lp package would be slow enough to prefer eliminating this step. 55 4.6 Sondik's One-Pass Algorithm 4.6.1 Background Edward J. Sondik <ref> [17] </ref> presented the first solution techniques for finding optimal policies for general pomdp problems. This was the seminal work from which all the other algorithms described in this paper were derived. His algorithm for the finite horizon case is also described in [16]. <p> The most depressing news of solving the pomdp model is that it is impossible to compute the optimal policy for anything but small problems. 5.3 Contributions The existing literature on pomdps suffers from a few problems which this work has addressed. The papers that present specific algorithms <ref> [17, 16, 6, 4] </ref> are difficult to follow unless the reader already has an intimate knowledge of the area . The survey articles [12, 9] are at too high level to give much insight into the details of pomdps and the solution procedures.
Reference: [18] <author> C. J. C. H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference: [19] <author> Wayne L. Winston. </author> <title> Introduction to Mathematical Programming: Applications and Algorithms. </title> <address> PWS-KENT, Boston, Massachusetts, </address> <year> 1991. </year> <month> 100 </month>
Reference-contexts: Linear programming is also used in most of the other algorithms discussed. What linear programming does is more important than how it does it, but for the reader interested in how lps work see <ref> [19] </ref>. There are many different ways to view what is happening in a linear program. We opt for the geometric view, which is more natural given that we have been trying to present the belief space and value functions geometrically. Geometrically, a linear program defines a region in some space.
References-found: 19

