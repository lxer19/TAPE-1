URL: http://www.llnl.gov/CASC/people/musick/papers/kdd.ps
Refering-URL: http://www.llnl.gov/CASC/people/musick/papers/kdd.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: rmusick@llnl.gov  
Title: Rethinking the Learning of Belief Network Probabilities  
Author: Ron Musick 
Address: P.O. Box 808, L-419, Livermore, CA 94551  
Affiliation: Advanced Information Technology Program Lawrence Livermore National Laboratory  
Abstract: Belief networks are a powerful tool for knowledge discovery that provide concise, understandable probabilistic models of data. There are methods grounded in probability theory to incrementally update the relationships described by the belief network when new information is seen, to perform complex inferences over any set of variables in the data, to incorporate domain expertise and prior knowledge into the model, and to automatically learn the model from data. This paper concentrates on part of the belief network induction problem, that of learning the quantitative structure (the conditional probabilities), given the qualitative structure. In particular, the current practice of rote learning the probabilities in belief networks can be significantly improved upon. We advance the idea of applying any learning algorithm to the task of conditional probability learning in belief networks, discuss potential benefits, and show results of applying neural networks and other algorithms to a medium sized car insurance belief network. The results demonstrate from 10 to 100% improvements in model error rates over the current approaches. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Charniak, E. </author> <year> 1991. </year> <title> Bayesian networks without tears. </title> <journal> AI Magazine 12(4) </journal> <pages> 50-63. </pages>
Reference-contexts: Figure 2 is a graphical example of applying neural networks to learn three of the tables in the Dog Out problem <ref> (Charniak 1991) </ref>. Let T i be the CPT for node X i , and ij be a unique parent instantiation for the node. The general process for learning a CPT with NN is for each T i : 1.
Reference: <author> D'Ambrosio, B. </author> <year> 1994. </year> <title> Symbolic probabilistic inference in large bn2o networks. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 128-135. </pages>
Reference: <author> Ezawa, K., and Norton, S. </author> <year> 1995. </year> <title> Knowledge discovery in telecommunication services data using bayesian networks. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 100-105. </pages>
Reference: <author> Musick, R. </author> <year> 1993. </year> <title> Maintaining inference distributions in belief nets. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence. </booktitle>
Reference-contexts: The bookkeeping approach is seductive because it is the easiest method to implement, is very understandable, and leads to Dirichlet distributions. Dirichlets have nice theoretical properties that can lead to effective measurements of accuracy during inference <ref> (Musick 1993) </ref>. However, the power and flexibility of being able to apply any machine learning technique to CPT learning has advantages that can not be ignored. The following is a brief argument for incorporating machine learning techniques into the statistically oriented techniques that are currently in force.
Reference: <author> Musick, R. </author> <year> 1994. </year> <title> Belief Network Induction. </title> <type> Ph.D. Dissertation, UCB Tech Report CSD-95-863. </type> <institution> University of California, Berkeley, Berkeley, </institution> <address> CA. </address>
Reference-contexts: We assume that the structure of the belief network is given. Formal details are kept to a minimum in this section; the interested reader can find in-depth descriptions of how these methods and others can be constructed and applied to CPT learning in <ref> (Musick 1994) </ref>. Bookkeeping Bookkeeping is a simple matter of counting the training samples that are relevant to each cell in the CPT. <p> A network can be built for any combination of input/output variables, including nodes with binary, discrete, continuous, and nominal values. Details of the construction and mapping into CPT learning can be found in <ref> (Musick 1994) </ref>. Note that while bookkeeping provides a distribution for each cell in the CPT, neural networks in general will only provide a point probability.
Reference: <author> Park, Y.; Han, Y.; and Choi, K. </author> <year> 1995. </year> <title> Automatic thesaurus construction using bayesian networks. </title> <booktitle> In Proceedings of the Fourth International Conference on Information and Knowledge Management, </booktitle> <pages> 212-217. </pages> <publisher> ACM Press. </publisher>
Reference: <author> Russell, S. J.; Binder, J.; Koller, D.; and Kanazawa, K. </author> <year> 1995. </year> <title> Local learning in probabilistic networks with hidden variables. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence. </booktitle> <address> Montreal, Canada: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The improvement in the overall model error rate with COMB is impressive. Results and Comparisons This section describes the results of an implementation of the above ideas. Experimental Methodology The belief network used in this paper is a realistic, moderately sized car insurance network <ref> (Russell et al. 1995) </ref> with 27 binary, discrete, nominal, and continuous nodes, 52 arcs and over 1400 conditional probabilities. The goal is to help predict how much financial risk is incurred from various policy holders given data about age, socio-economic class, and so on.
Reference: <author> Samuel, A. </author> <year> 1963. </year> <title> Some studies in machine learning using the game of checkers. </title> <editor> In Feigenbaum, E. A., and Feldman, J., eds., </editor> <booktitle> Computers and Thought. </booktitle> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: The current approach in most all cases is to learn the conditional probability tables with a simple statistical counting method ("bookkeeping") that can be likened to the rote learning done by chess and checkers programs back in the 60's <ref> (Samuel 1963) </ref>. The bookkeeping approach is seductive because it is the easiest method to implement, is very understandable, and leads to Dirichlet distributions. Dirichlets have nice theoretical properties that can lead to effective measurements of accuracy during inference (Musick 1993).
References-found: 8

