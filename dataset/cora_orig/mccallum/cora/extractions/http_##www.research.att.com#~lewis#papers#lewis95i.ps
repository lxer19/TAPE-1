URL: http://www.research.att.com/~lewis/papers/lewis95i.ps
Refering-URL: http://www.research.att.com/~lewis/chronobib.html
Root-URL: 
Email: lewis@research.att.com  
Title: Active by Accident: Relevance Feedback in Information Retrieval The goal of information retrieval (IR) techniques
Author: David D. Lewis Harper 
Keyword: Relevance Feedback  
Date: 1979; Salton Buckley 1988; Lewis 1991;  
Note: From the unpublished working notes of the 1995 AAAI Fall Symposium on Active Learning. Information Retrieval  also been applied  Relevance  This active character is critical to the success of rel  
Address: 600 Mountain Ave., 2C-408 Murray Hill, NJ 07974  
Affiliation: AT&T Bell Laboratories  
Abstract: Relevance feedback is a supervised learning method used to improve the effectiveness of information retrieval systems. It is an active learning technique, and may be the most widespread application of active learning to date. We briefly discuss information retrieval and the role of relevance feedback in it, and point out some areas where active learning could make further contributions. Conventional approaches to databases (e.g. relational databases) work well when the semantic of objects are well-understood and data processing can be reduced to working out the logical consequences of specified restrictions on small numbers of attributes. IR, on the other hand, works with objects with much murkier semantics. Formal inference is less applicable, and processing is based on classification or similarity measurement using large numbers of attributes and with some degree of uncertainty. Not surprisingly, machine learning and statistical techniques play a large role (Lewis 1991). Harman 1992a). (How a statistical classifier with tens of thousands of attributes is produced using essentially no training data is an interesting story itself, but will not be discussed here.) The classifier is used to rank documents by how likely they are to be class members (that is, relevant to the user) and display the top ranked documents to the user. In relevance feedback, the user has the option of labeling a few of the top documents in this ranking (perhaps 10 or so) according to whether they are relevant or not. The labeled documents are then given to a supervised learning procedure, which uses them, along with the original request, to produce a new classifier. The new classifier is used to produce a new ranking, which typically puts more relevant documents at higher ranks than the original did. A few iterations of this process are typically used. Relevance feedback is not as widely available as ranked retrieval, and some IR products advertised as having relevance feedback actually only support the use of single documents as queries. However, relevance feedback has been extensively used by IR researchers, and a crude version is available in the WAIS system. Relevance feedback has been used to train a variety of classifiers, including naive Bayes, prototype, inference net, decision rule, fuzzy, and neural classifiers. Classifiers produced using relevance feedback are typically found to be 20% to 200% more effective than classifiers produced using just the user's textual request (Salton & Buckley 1990; Harman 1992b). Relevance feedback can sometimes produce classifiers which are more effective than those built by expert human searchers (Koenemann et al. 1995). As a user interface strategy, ranked retrieval systems show the user those documents which the current classifier identifies as most likely to be class members. An important side effect of this (but one that has gotten scant attention in the relevance feedback literature) is that users performing relevance feedback are not labeling a random sample of the document collection. Therefore relevance feedback is an active rather than passive form of machine learning. 
Abstract-found: 1
Intro-found: 0
Reference: <author> Belkin, N. J., and Croft, W. B. </author> <year> 1992. </year> <title> Information filtering and information retrieval: </title> <journal> Two sides of the same coin? Communications of the ACM 35(12) </journal> <pages> 29-38. </pages>
Reference: <author> Bookstein, A. </author> <year> 1983. </year> <title> Information retrieval: A sequential learning process. </title> <journal> Journal of the American Society for Information Science 34 </journal> <pages> 331-342. </pages>
Reference: <author> Croft, W. B., and Harper, D. J. </author> <year> 1979. </year> <title> Using probabilistic models of document retrieval without relevance feedback. </title> <journal> Journal of Documentation 35(4) </journal> <pages> 285-295. </pages>
Reference: <author> Harman, D. </author> <year> 1992a. </year> <title> Ranking algorithms. </title> <editor> In Frakes, W. B., and Baeza-Yates, R., eds., </editor> <booktitle> Information Retrieval: Data Structures and Algorithms. </booktitle> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall. </publisher> <pages> 363-392. </pages>
Reference: <author> Harman, D. </author> <year> 1992b. </year> <title> Relevance feedback and other query modification techniques. </title> <editor> In Frakes, W. B., and Baeza-Yates, R., eds., </editor> <booktitle> Information Retrieval: Data Structures and Algorithms. </booktitle> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall. </publisher> <pages> 241-263. </pages>
Reference: <editor> Harman, D. K., ed. </editor> <booktitle> 1994. The Second Text REtrieval Conference (TREC-2). </booktitle> <address> Gaithersburg, </address> <institution> MD 20899: National Institute of Standards and Technology. </institution> <note> Special Publication 500-215. </note>
Reference: <author> Koenemann, J.; Quatrain, R.; Cool, C.; and Belkin, N. J. </author> <year> 1995. </year> <title> New tools and old habits: The interactive searching behavior of expert online searchers using INQUERY. </title> <editor> In Harman, D. K., ed., </editor> <booktitle> The Third Text Retrieval Conference (TREC-3). </booktitle> <address> Gaithersburg, MD: U. </address> <institution> S. Dept. of Commerce. </institution> <note> To appear. </note>
Reference: <author> Lewis, D. D., and Catlett, J. </author> <year> 1994. </year> <title> Heterogeneous uncertainty sampling for supervised learning. </title> <editor> In Co-hen, W. W., and Hirsh, H., eds., </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> 148-156. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lewis, D. D., and Gale, W. A. </author> <year> 1994. </year> <title> A sequential algorithm for training text classifiers. </title> <editor> In Croft, W. B., and van Rijsbergen, C. J., eds., </editor> <booktitle> SIGIR 94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> 3-12. </pages> <address> London: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Lewis, D. D., and Hayes, P. J. </author> <year> 1994. </year> <title> Guest editorial. </title> <journal> ACM Transactions on Information Systems 12(3):231. </journal>
Reference: <author> Lewis, D. D. </author> <year> 1991. </year> <booktitle> Learning in intelligent information retrieval. In Eighth International Workshop on Machine Learning, </booktitle> <pages> 235-239. </pages>
Reference: <author> Salton, G., and Buckley, C. </author> <year> 1988. </year> <title> Term-weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management 24(5) </booktitle> <pages> 513-523. </pages>
Reference: <author> Salton, G., and Buckley, C. </author> <year> 1990. </year> <title> Improving retrieval performance by relevance feedback. </title> <journal> Journal of the American Society for Information Science 41(4) </journal> <pages> 288-297. </pages>
References-found: 13

