URL: http://www.eecis.udel.edu/~agrawal/papers/shpcc94.ps
Refering-URL: http://www.eecis.udel.edu/~agrawal/pub.html
Root-URL: http://www.cis.udel.edu
Email: fgagan, als, saltzg@cs.umd.edu  
Title: Efficient Runtime Support for Parallelizing Block Structured Applications  
Author: Gagan Agrawal Alan Sussman Joel Saltz 
Address: College Park, MD 20742  
Affiliation: Dept. of Computer Science University of Maryland  
Abstract: Scientific and engineering applications often involve structured meshes. These meshes may be nested (for multigrid codes) and/or irregularly coupled (called multiblock or irregularly coupled regular mesh problems). In this paper, we describe a runtime library for parallelizing these applications on distributed memory parallel machines in an efficient and machine-independent fashion. This runtime library is currently implemented on several different systems. This library can be used by application programmers to port applications by hand and can also be used by a compiler to handle communication for these applications. Our experimental results show that our primitives have low runtime communication overheads. We have used this library to port a multiblock template and a multigrid code. Effort is also underway to port a complete multiblock computational fluid dynamics code using our library. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Compiler and runtime support for structured and block structured applications. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 578-587. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year> <note> An extended version available as University of Maryland Technical Report CS-TR-3052 and UMIACS-TR-93-29. </note>
Reference-contexts: We have experimented with one multiblock template [18] and one multi-grid code [15]. Our experimental results show that the primitives have low runtime communication overheads. In separate work, we have also developed methods for integrating this runtime support with compilers for HPF style parallel programming languages <ref> [1] </ref>. Several other researchers have also developed runtime libraries or programming environments for multiblock applications. Baden [11] has developed a Lattice Programming Model (LPAR). This system, however, achieves only coarse grained parallelism since a single block can only be assigned to one processor.
Reference: [2] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> On efficient runtime support for multiblock and multigrid applications: Regular section analysis. </title> <institution> Technical Report CS-TR-3140 and UMIACS-TR-93-92, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: Now, we present the details of these steps. In this paper, we just discuss the details of the analysis when the data is block distributed in both source and destination arrays. Details of the analysis when the data-distribution is cyclic are available in <ref> [2] </ref>. We consider a particular processor p which owns part of the source array s, of which the regular section S is a part.
Reference: [3] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> Fortran 90D/HPF compiler for distributed memory MIMD computers: Design, implementation and performance results. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 351-360. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: Since, in typical multiblock and multigrid applications, the number of blocks and their respective sizes is not known until runtime, the distribution of blocks onto processors is done at runtime. The distributed array descriptors (DAD) <ref> [3] </ref> for the arrays representing these blocks are, therefore, generated at runtime. Distributed array descriptors contain information about the portions of the arrays residing on each processor, and are used at runtime for performing communication and distributing loops iterations.
Reference: [4] <author> Kalpana Chawla and William R. Van Dalsem. </author> <title> Numerical simulation of a powered-lift landing. In Proceedings of the 72nd Fluid Dynamics Panel Meeting and Symposium on Computational and Experimental Assessment of Jets in Cross Flow, </title> <address> Winchester, UK. AGARD, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: SC 292-1-22913. The authors assume all responsibility for the contents of the paper. eled using a single regular mesh. Multiblock applications are used in important grand-challenge applications like air quality modeling [13], computational fluid dynamics [18], structure and galaxy formation [16], simulation of high performance aircrafts <ref> [4] </ref>, large scale climate modeling [6], reservoir modeling for porous media [6], simulation of propulsion systems [6], computational combustion dynamics [6] and land cover dynamics [10]. In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid.
Reference: [5] <author> D. Loveman (Ed.). </author> <title> Draft High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Even if the loop bounds are known at compile time, run-time resolution may be required if complete information about distribution of arrays A, B and C is not available to the compiler. This can happen primarily because of two reasons, use of dynamic data distributions <ref> [5] </ref> or the compiler not having sufficient support for inter-procedural information propagation [8].
Reference: [6] <institution> Survey of principal investigators of grand challenge applications: Workshop on grand challenge applications and software technology, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The authors assume all responsibility for the contents of the paper. eled using a single regular mesh. Multiblock applications are used in important grand-challenge applications like air quality modeling [13], computational fluid dynamics [18], structure and galaxy formation [16], simulation of high performance aircrafts [4], large scale climate modeling <ref> [6] </ref>, reservoir modeling for porous media [6], simulation of propulsion systems [6], computational combustion dynamics [6] and land cover dynamics [10]. In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid. <p> Multiblock applications are used in important grand-challenge applications like air quality modeling [13], computational fluid dynamics [18], structure and galaxy formation [16], simulation of high performance aircrafts [4], large scale climate modeling <ref> [6] </ref>, reservoir modeling for porous media [6], simulation of propulsion systems [6], computational combustion dynamics [6] and land cover dynamics [10]. In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid. <p> Multiblock applications are used in important grand-challenge applications like air quality modeling [13], computational fluid dynamics [18], structure and galaxy formation [16], simulation of high performance aircrafts [4], large scale climate modeling <ref> [6] </ref>, reservoir modeling for porous media [6], simulation of propulsion systems [6], computational combustion dynamics [6] and land cover dynamics [10]. In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid. <p> Multiblock applications are used in important grand-challenge applications like air quality modeling [13], computational fluid dynamics [18], structure and galaxy formation [16], simulation of high performance aircrafts [4], large scale climate modeling <ref> [6] </ref>, reservoir modeling for porous media [6], simulation of propulsion systems [6], computational combustion dynamics [6] and land cover dynamics [10]. In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid.
Reference: [7] <author> Michael Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The interpolation required during the prolongation step in multigrid codes also involves interaction among the neighboring array elements. Such communication is handled by allocation of extra space at the beginning and end of each array dimension on each processor. These extra elements are called overlap, or ghost, cells <ref> [7] </ref>. In our runtime system, communication is performed in two phases.
Reference: [8] <author> M.W. Hall, S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings Supercomputing '92, </booktitle> <pages> pages 522-534. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1992. </year>
Reference-contexts: This can happen primarily because of two reasons, use of dynamic data distributions [5] or the compiler not having sufficient support for inter-procedural information propagation <ref> [8] </ref>.
Reference: [9] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Current prototype Fortran D/HPF compilers compile under the assumptions that loop bounds, subscript functions and data distributions are always known at compile time <ref> [9] </ref>. However, this assumption often does not hold while compiling many real applications. Runtime resolution is required for determining exact communication and loop partitioning in such cases. The compiler can handle such cases easily by inserting calls to our primitives.
Reference: [10] <author> J.R.G.Townshend, C.O.Justice, W. Li, C.Gurney, and J.McManus. </author> <title> Global land cover classification by remote sensing:present capabilities and future possibilities. </title> <booktitle> Remote Sensing of Environment, </booktitle> <volume> 35 </volume> <pages> 243-256, </pages> <year> 1991. </year>
Reference-contexts: are used in important grand-challenge applications like air quality modeling [13], computational fluid dynamics [18], structure and galaxy formation [16], simulation of high performance aircrafts [4], large scale climate modeling [6], reservoir modeling for porous media [6], simulation of propulsion systems [6], computational combustion dynamics [6] and land cover dynamics <ref> [10] </ref>. In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid. In this paper, we present runtime support that we have designed and implemented for parallelizing these applications on distributed memory machines in an efficient, convenient and machine independent manner.
Reference: [11] <author> Scott R. Kohn and Scott B. Baden. </author> <title> An implementation of the LPAR parallel programming model for scientific computations. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 759-766. </pages> <publisher> SIAM, </publisher> <month> March </month> <year> 1993. </year>
Reference-contexts: Our experimental results show that the primitives have low runtime communication overheads. In separate work, we have also developed methods for integrating this runtime support with compilers for HPF style parallel programming languages [1]. Several other researchers have also developed runtime libraries or programming environments for multiblock applications. Baden <ref> [11] </ref> has developed a Lattice Programming Model (LPAR). This system, however, achieves only coarse grained parallelism since a single block can only be assigned to one processor. Quinlan [12] has developed P++, a set of C++ libraries for grid applications.
Reference: [12] <author> Max Lemke and Daniel Quinlan. </author> <title> P++, a C++ virtual shared grids based programming environment for architecture-independent development of structured grid applications. </title> <type> Technical Report 611, </type> <institution> GMD, </institution> <month> Febru-ary </month> <year> 1992. </year>
Reference-contexts: Several other researchers have also developed runtime libraries or programming environments for multiblock applications. Baden [11] has developed a Lattice Programming Model (LPAR). This system, however, achieves only coarse grained parallelism since a single block can only be assigned to one processor. Quinlan <ref> [12] </ref> has developed P++, a set of C++ libraries for grid applications. While this library provides a convenient interface, the libraries do not optimize communication overheads. Our library, in contrast, reduces communication costs by using message aggregation. The rest of this paper is organized as follows.
Reference: [13] <editor> Rohit Mathur, Leonard K. Peters, and Rick D. Saylor. </editor> <title> Sub-grid representation of emission source clusters in regional air quality monitoring. Atmospheric Environment, </title> <address> 26A(17):3219-3237, </address> <year> 1992. </year>
Reference-contexts: NAG-1-1485, by NSF under grant No. ASC 9213821 and by ONR under contract No. SC 292-1-22913. The authors assume all responsibility for the contents of the paper. eled using a single regular mesh. Multiblock applications are used in important grand-challenge applications like air quality modeling <ref> [13] </ref>, computational fluid dynamics [18], structure and galaxy formation [16], simulation of high performance aircrafts [4], large scale climate modeling [6], reservoir modeling for porous media [6], simulation of propulsion systems [6], computational combustion dynamics [6] and land cover dynamics [10].
Reference: [14] <author> S. McCormick. </author> <title> Multilevel Projection Methods for Partial Differential Equations. </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: One such class of scientific and engineering applications involves structured meshes. These meshes may be nested (as in multigrid codes) or may be irregularly coupled (called Multiblock or Irregularly Coupled Regular Mesh Problems). Multigrid is a common technique for accelerating the solution of partial-differential equations <ref> [14] </ref>. Multigrid codes employ a number of meshes at different levels of resolution. The restriction and prolongation operations for shifting between different multigrid levels require moving regular array sections with non-unit strides. In multiblock problems, the data is divided into several interacting regions (called blocks or subdomains).
Reference: [15] <author> Andrea Overman and John Van Rosendale. </author> <title> Mapping robust parallel multigrid algorithms to scalable memory architectures. </title> <booktitle> To appear in Proceedings of 1993 Copper Mountain Conference on Multigrid Methods, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: In this paper, we present runtime support that we have designed and implemented for parallelizing these applications on distributed memory machines in an efficient, convenient and machine independent manner. We have experimented with one multiblock template [18] and one multi-grid code <ref> [15] </ref>. Our experimental results show that the primitives have low runtime communication overheads. In separate work, we have also developed methods for integrating this runtime support with compilers for HPF style parallel programming languages [1]. Several other researchers have also developed runtime libraries or programming environments for multiblock applications.
Reference: [16] <author> J.M. Stone and M.L. Norman. Zeus-2d: </author> <title> A radiation magnetohydrodynamics code for astrophysical flows in two space dimensions: I. the hydrodynamic algorithms and tests. </title> <journal> The Astrophysical Journal Supplements, </journal> <volume> 80(753), </volume> <year> 1992. </year>
Reference-contexts: ASC 9213821 and by ONR under contract No. SC 292-1-22913. The authors assume all responsibility for the contents of the paper. eled using a single regular mesh. Multiblock applications are used in important grand-challenge applications like air quality modeling [13], computational fluid dynamics [18], structure and galaxy formation <ref> [16] </ref>, simulation of high performance aircrafts [4], large scale climate modeling [6], reservoir modeling for porous media [6], simulation of propulsion systems [6], computational combustion dynamics [6] and land cover dynamics [10].
Reference: [17] <author> Alan Sussman, Gagan Agrawal, and Joel Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives, revision 4.1. </title> <institution> Technical Report CS-TR-3070.1 and UMIACS-TR-93-36.1, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> Decem-ber </month> <year> 1993. </year>
Reference-contexts: We also discuss how this library can be used by application programmers and compilers. The set of runtime routines that we have developed is called the Multiblock Parti library <ref> [17] </ref>. In summary, these primitives allow an application programmer or a compiler to * Lay out distributed data in a flexible way, to enable good load balancing and minimize interprocessor com munication, * Give high level specifications for performing data movement, and * Distribute the computation across the processors. <p> Programmers can port their Fortran or C programs on distributed memory machines by manually inserting calls to the library routines. The resulting program has Single Program Multiple Data (SPMD) model of parallelism. 3.1 Multiblock Parti Library We now discuss the details of our runtime library primitives <ref> [17] </ref>. Since, in typical multiblock and multigrid applications, the number of blocks and their respective sizes is not known until runtime, the distribution of blocks onto processors is done at runtime. The distributed array descriptors (DAD) [3] for the arrays representing these blocks are, therefore, generated at runtime. <p> Distributed array descriptors contain information about the portions of the arrays residing on each processor, and are used at runtime for performing communication and distributing loops iterations. We will not discuss the details of the primitives which allow the user to specify data distribution. For more details, see <ref> [17] </ref>. Two types of communication are required in both multi-block and multigrid applications. We need intra-block communication because a single block may be partitioned across the processors of the distributed memory parallel machine.
Reference: [18] <author> V.N. Vatsa, M.D. Sanetrik, and E.B. Parlette. </author> <title> Development of a flexible and efficient multigrid-based multiblock flow solver; AIAA-93-0677. </title> <booktitle> In Proceedings of the 31st Aerospace Sciences Meeting and Exhibit, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: NAG-1-1485, by NSF under grant No. ASC 9213821 and by ONR under contract No. SC 292-1-22913. The authors assume all responsibility for the contents of the paper. eled using a single regular mesh. Multiblock applications are used in important grand-challenge applications like air quality modeling [13], computational fluid dynamics <ref> [18] </ref>, structure and galaxy formation [16], simulation of high performance aircrafts [4], large scale climate modeling [6], reservoir modeling for porous media [6], simulation of propulsion systems [6], computational combustion dynamics [6] and land cover dynamics [10]. <p> In this paper, we present runtime support that we have designed and implemented for parallelizing these applications on distributed memory machines in an efficient, convenient and machine independent manner. We have experimented with one multiblock template <ref> [18] </ref> and one multi-grid code [15]. Our experimental results show that the primitives have low runtime communication overheads. In separate work, we have also developed methods for integrating this runtime support with compilers for HPF style parallel programming languages [1]. <p> In the current definition of HPF (and hence in HPF compilers), this is not possible. In block structured codes, this feature allows us to keep the communication overheads low while maintaining the load balance. To study the benefit of this feature, we experimented with the multigrid template <ref> [18] </ref> for a two block input case. We ran the parallelized code, once distributing both the blocks over the entire processor space and then distributing each block over disjoint processor spaces.
References-found: 18

