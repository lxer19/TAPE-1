URL: http://www.robotics.stanford.edu/~koller/papers/bghk.ps
Refering-URL: http://www.robotics.stanford.edu/~koller/papers/bghk.html
Root-URL: http://www.robotics.stanford.edu
Email: fbacchus@logos.uwaterloo.ca  grove@research.nj.nec.com  halpern@almaden.ibm.com  daphne@cs.berkeley.edu  
Title: From Statistical Knowledge Bases to Degrees of Belief  
Author: Fahiem Bacchus Adam J. Grove Joseph Y. Halpern Daphne Koller 
Address: Waterloo, Ontario Canada, N2L 3G1  4 Independence Way Princeton, NJ 08540  650 Harry Road San Jose, CA 95120-6099  Berkeley, CA 94720  
Affiliation: Computer Science Dept. University of Waterloo  NEC Research Inst.  IBM Almaden Research Center  Computer Science Division University of California  
Abstract: An intelligent agent will often be uncertain about various properties of its environment, and when acting in that environment it will frequently need to quantify its uncertainty. For example, if the agent wishes to employ the expected-utility paradigm of decision theory to guide its actions, she will need to assign degrees of belief (subjective probabilities) to various assertions. Of course, these degrees of belief should not be arbitrary, but rather should be based on the information available to the agent. This paper describes one approach for inducing degrees of belief from very rich knowledge bases which might include information about particular individuals, statistical correlations, physical laws, and default rules. We call our approach the random-worlds method. This method is based on a principle of indifference: it treats all of the worlds the agent considers possible as being equally likely. It is able to integrate qualitative default reasoning with quantitative probabilistic reasoning by providing a language in which both types of information can be easily expressed. Our results show that a number of desiderata that arise in direct inference (reasoning from statistical information to conclusions about individuals) and default reasoning follow directly from the semantics of random worlds. For example, random worlds captures important patterns of reasoning such as specificity, inheritance, indifference to irrelevant information, and default assumptions of independence. Furthermore, the expressive power of the language used and the intuitive semantics of random worlds allow the method to deal well with problems that are beyond the scope of many other non-deductive reasoning systems. fl A preliminary version of this paper appeared in the International Joint Conference on Artificial Intelligence, 1993 [BGHK93]. Some of this research was performed while Adam Grove and Daphne Koller were at Stanford University and at the IBM Almaden Research Center. This research has been supported in part by the Canadian Government through their NSERC and IRIS programs, by the Air Force Office of Scientific Research (AFSC) under Contract F49620-91-C-0080, by an IBM Graduate Fellowship, and by a University of California President's Postdoctoral Fellowship. The United States Government is authorized to reproduce and distribute reprints for governmental purposes. 
Abstract-found: 1
Intro-found: 1
Reference: [Ada75] <author> E. Adams. </author> <title> The Logic of Conditionals. </title> <address> D. </address> <publisher> Reidel, </publisher> <address> Dordrecht, Netherlands, </address> <year> 1975. </year>
Reference-contexts: Using a higher fixed threshold in a straightforward way does not help. More successfully, Adams <ref> [Ada75] </ref>, and later Geffner and Pearl [GP90], suggested an interpretation of defaults based on "almost all". In their framework, this is done using extreme probabilities |conditional probabilities that are arbitrarily close to 1: i.e., within 1 * for some *, and considering the limit as * ! 0. <p> As shown in [GP90], *-entailment possesses a number of reasonable properties typically associated with default reasoning, including a preference for more specific information. However, *-entailment is very weak. In particular, as shown by Adams <ref> [Ada75] </ref>, the consequence relation defined by *-entailment satisfies only the five basic properties of default inference given in Section 3.2. Hence, among other limitations, it has no ability to ignore irrelevant information, so it cannot perform any inheritance reasoning.
Reference: [Ash93] <author> N. Asher. </author> <title> Extensions for commonsense entailment. </title> <booktitle> In Proceedings of the IJCAI Workshop on Conditionals in Knowledge Representation, </booktitle> <pages> pages 26-41, </pages> <year> 1993. </year>
Reference-contexts: This problem has been called the drowning problem <ref> [Ash93, BCD + 93] </ref>. Theories of default reasoning have had considerable difficulty in capturing an ability to inherit from superclasses that can deal properly with all of these different cases. In particular, the problem of inheritance to exceptional subclasses has been the most difficult.
Reference: [Bac90] <author> F. Bacchus. </author> <title> Representing and Reasoning with Probabilistic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: We now provide a brief overview of the random-worlds approach. We assume that the information in the knowledge base is expressed in a variant of the language introduced by Bac-chus <ref> [Bac90] </ref>. Bacchus's language augments first-order logic by allowing statements of the form kHep (x)jJaun (x)k x = 0:8, which says that 80% of patients with jaundice have hepatitis. <p> We therefore define a statistical language L , which is a variant of a language designed by Bacchus <ref> [Bac90] </ref>. For the remainder of the paper, let be a finite first-order vocabulary, consisting of predicate, function, and constant symbols, and let X be a set of variables. Our statistical language augments standard first-order logic with a form of statistical quantifier. <p> Finally, any rational number is also considered to be a proportion expression, and the set of proportion expressions is closed under addition and multiplication. One important difference between our syntax and that of <ref> [Bac90] </ref> is the use of approximate equality to compare proportion expressions. As we argued in the introduction, exact comparisons are sometimes inappropriate. Consider a statement such as "80% of patients with jaundice have hepatitis". <p> Notice that this definition allows arbitrary nesting of quantifiers and proportion expressions. In Section 4.3 we demonstrate the expressive power of the language. As observed in <ref> [Bac90] </ref>, the appearance of a variable x in the subscript of a proportion expression binds the variable x in the expression; indeed, we can view jjjj X as a new type of quantification. We now need to define the semantics of the logic.
Reference: [BCD + 93] <author> S. Benferhat, C. Cayrol, D. Dubois, J. Lang, and H. Prade. </author> <title> Inconsistency management and prioritized syntax-based entailment. </title> <booktitle> In Proc. Thirteenth International Joint Conference on Artificial Intelligence (IJCAI '93), </booktitle> <pages> pages 640-645, </pages> <year> 1993. </year>
Reference-contexts: This problem has been called the drowning problem <ref> [Ash93, BCD + 93] </ref>. Theories of default reasoning have had considerable difficulty in capturing an ability to inherit from superclasses that can deal properly with all of these different cases. In particular, the problem of inheritance to exceptional subclasses has been the most difficult.
Reference: [BGHK92] <author> F. Bacchus, A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> From statistics to belief. </title> <booktitle> In Proc. National Conference on Artificial Intelligence (AAAI '92), </booktitle> <pages> pages 602-608, </pages> <year> 1992. </year> <title> 19 Note that we are careful to mention the vocabulary in the superscript here, rather than suppressing it as we have up to now. This is because the vocabulary used plays a significant role in this proof. </title> <type> 61 </type>
Reference-contexts: is a predicate symbol in [ f=g of arity r and t 1 ; : : : ; t r are terms, (b) contains proportion formulas of the form i 0 and i 0 , where and 0 are proportion expressions and i is a natural number, and 5 In <ref> [BGHK92] </ref> the use of approximate equality was suppressed in order to highlight other issues. 20 (c) is closed under conjunction, negation, and first-order quantification. Notice that this definition allows arbitrary nesting of quantifiers and proportion expressions. In Section 4.3 we demonstrate the expressive power of the language. <p> A number of different approaches to doing this are discussed in [BGHK94b], and shown to be essentially equivalent. Yet another approach for dealing with the learning problem is to use a variant of random worlds presented in <ref> [BGHK92] </ref> called the random-propensities approach. Random worlds has a strong bias towards believing that exactly half the domain has any given property, and this is not always reasonable. <p> Initially, all propensities are equally likely. Observing a flying bird gives us information about the propensity of birds to fly, and hence about the flying ability of other birds. As shown in <ref> [BGHK92] </ref>, the random propensities method does, indeed, learn from samples. Unfortunately, random propensities has its own problems. In particular, it learns "too often", i.e., even from arbitrary subsets that are not representative samples. Given the assertion "All giraffes are tall", random propensities would conclude that almost everything is tall.
Reference: [BGHK93] <author> F. Bacchus, A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> Statistical foundations for default reasoning. </title> <booktitle> In Proc. Thirteenth International Joint Conference on Artificial Intelligence (IJCAI '93), </booktitle> <year> 1993. </year>
Reference: [BGHK94a] <author> F. Bacchus, A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> Forming beliefs about a changing world. </title> <booktitle> To appear in Proc. 12th National Conference on Artificial Intelligence (AAAI ' 94), </booktitle> <year> 1994. </year>
Reference-contexts: On the other hand, Hunter [Hun89] has shown that maximum entropy methods can deal with causal information, provided it is represented appropriately. We have recently shown that by using an appropriate representation (related to Hunter's but quite different), the random-worlds method can also deal well with causal information <ref> [BGHK94a] </ref>. Indeed, our representation allows us to (a) deal with prediction and explanation problems, (b) represent causal information of the type implicit in Bayesian causal networks [Pea88], and (c) provide a clean and concise solution to the frame problem in the situation calculus [MH69].
Reference: [BGHK94b] <author> F. Bacchus, A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> Generating new beliefs from old. </title> <booktitle> To appear in Proc. 10th Conference on Uncertainty in Artifical Intelligence (UAI '94), </booktitle> <year> 1994. </year>
Reference-contexts: Thus, in order to handle such a statement appropriately, we would need to ensure that our probability distribution over the possible worlds satisfies the associated constraint. A number of different approaches to doing this are discussed in <ref> [BGHK94b] </ref>, and shown to be essentially equivalent. Yet another approach for dealing with the learning problem is to use a variant of random worlds presented in [BGHK92] called the random-propensities approach.
Reference: [BGHK94c] <author> F. Bacchus, A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> A response to: "Believing on the basis of evidence". </title> <journal> Computational Intelligence, </journal> <volume> 10(1), </volume> <year> 1994. </year>
Reference-contexts: Most of the earlier work is based on the idea of finding a suitable reference class. In this section, we review some of this work and show why we believe that this approach, while it has some intuitively reasonable properties, is inadequate as a general methodology. (See also <ref> [BGHK94c] </ref> for further discussion of this issue.) We go into some detail here, since the issues that arise provide some motivation for the results that we prove later regarding our approach. 2.1 The basic approach The earliest sophisticated attempt at clarifying the connection between objective statistical knowledge and degrees of belief,
Reference: [Bou91] <author> C. Boutilier. </author> <title> Conditional Logics for Default Reasoning and Belief Revision. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1991. </year>
Reference-contexts: Such an approach enables us, among other things, to verify the consistency of a collection of defaults and to see whether a default follows logically from a collection of defaults. Of other existing theories, those based on conditional or modal logic come closest to achieving this (see <ref> [Bou91] </ref> for further discussion of this point). 3.2 Properties of default inference As we said, default reasoning systems have typically been measured by testing them on a number of important examples. Recently, a few tools have been developed that improve upon this approach. <p> For instance, *-semantics and variants [GP90, GMP90], modal approaches such as autoepistemic logic [Moo85], and conditional logics <ref> [Bou91] </ref>, are usually considered in a propositional framework. Others, such as Reiter's default logic and Delgrande's conditional logic [Del88], use a first-order language, but with a syntax that tends to decouple the issues of first-order reasoning and default reasoning; we discuss this below. <p> Default logic, for example, cannot even express such defaults. Many theories of conditional logics (for example, those of <ref> [Del88, Bou91] </ref>) can express this example, but do not handle it correctly.
Reference: [Car50] <author> R. Carnap. </author> <title> Logical Foundations of Probability. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1950. </year>
Reference-contexts: The result is our random-worlds method. The key ideas in the approach are not new. Many of them can be found in the work of Johnson [Joh32] and Carnap <ref> [Car50, Car52] </ref>, although these authors focus on knowledge bases that contain only first-order information, and for the most part restrict their attention to unary predicates.
Reference: [Car52] <author> R. Carnap. </author> <title> The Continuum of Inductive Methods. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1952. </year>
Reference-contexts: The result is our random-worlds method. The key ideas in the approach are not new. Many of them can be found in the work of Johnson [Joh32] and Carnap <ref> [Car50, Car52] </ref>, although these authors focus on knowledge bases that contain only first-order information, and for the most part restrict their attention to unary predicates. <p> One idea is to add statements about 16 A related observation, that random worlds cannot do learning (although in a somewhat different sense), was made by Carnap <ref> [Car52] </ref>, who apparently lost his enthusiasm for (his version of) random worlds for precisely this reason. 51 degrees of belief to the knowledge base.
Reference: [Che83] <author> P. C. Cheeseman. </author> <title> A method of computing generalized Bayesian probability values for expert systems. </title> <booktitle> In Proc. Eighth International Joint Conference on Artificial Intelligence (IJCAI '83), </booktitle> <pages> pages 198-202, </pages> <year> 1983. </year>
Reference: [Chu91] <author> R. Chuaqui. </author> <title> Truth, possibility, and probability: new logical foundations of probability and statistical inference. </title> <publisher> North-Holland, </publisher> <year> 1991. </year>
Reference-contexts: Related approaches have been used in the more recent works of Shastri [Sha89] and of Paris and Vencovska [PV89], in the context of a unary statistical language. Chuaqui's recent work <ref> [Chu91] </ref> is also relevant. His work, although technically quite different from ours, shares the idea of basing a theory of probabilistic reasoning upon notions of indifference and symmetry. The works of Chuaqui and Carnap investigate very different issues from those we examine in this paper.
Reference: [DD85] <author> K. G. Denbigh and J. S. Denbigh. </author> <title> Entropy in Relation to Incomplete Knowledge. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1985. </year>
Reference-contexts: As we have already claimed, this class of problems is an important one. In general, many properties of interest can be expressed using unary predicates, since they express properties of individuals. For example, in physics applications we are interested in such predicates as quantum state (see <ref> [DD85] </ref>). Similarly, AI applications and expert systems typically use only unary predicates ([Che83]) such as symptoms and diseases.
Reference: [Del88] <author> J. P. Delgrande. </author> <title> An approach to default reasoning based on a first-order conditional logic: Revised report. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 63-90, </pages> <year> 1988. </year>
Reference-contexts: Theories based on first-order conditional logic <ref> [Del88] </ref> often do use the syntax A (x) ! B (x). As we said in the introduction, in the random worlds framework this default is captured using the statistical assertion kB (x)jA (x)k x 1. <p> For instance, *-semantics and variants [GP90, GMP90], modal approaches such as autoepistemic logic [Moo85], and conditional logics [Bou91], are usually considered in a propositional framework. Others, such as Reiter's default logic and Delgrande's conditional logic <ref> [Del88] </ref>, use a first-order language, but with a syntax that tends to decouple the issues of first-order reasoning and default reasoning; we discuss this below. Of the better-known systems, circumscription seems to have the ability, at least in principle, of making the richest use of first-order logic. <p> Default logic, for example, cannot even express such defaults. Many theories of conditional logics (for example, those of <ref> [Del88, Bou91] </ref>) can express this example, but do not handle it correctly.
Reference: [EKP91] <author> D. W. Etherington, S. Kraus, and D. Perlis. </author> <title> Nonmonotonicity and the scope of reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 221-261, </pages> <year> 1991. </year>
Reference-contexts: Without logical closure, there is a danger of being too dependent on merely syntactic features of a problem. Another solution is to prevent a theory from reasoning about all N individuals at once <ref> [EKP91] </ref>. Finally, one can simply deny that :Winner (c) follows by default. Circumscription, for instance, does this: The standard representation of the problem would result in multiple extensions, such that for each individual c, there is one extension where c is the winner.
Reference: [Gab84] <author> D. Gabbay. </author> <title> Theoretical foundations for nonmonotonic reasoning in expert systems. </title> <editor> In K. R. Apt, editor, </editor> <booktitle> Proceedings of the NATO Advanced Study Institute on logics and models of concurrent systems. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Recently, a few tools have been developed that improve upon this approach. Gabbay <ref> [Gab84] </ref> (and later Makinson [Mak89] and Kraus, Lehmann, and Magidor [KLM90]) introduced the idea of investigating the input/output relation of a default reasoning system, with respect to certain general properties that such an inference relation might possess. Makinson [Mak94] gives a detailed survey of this work. The idea is simple.
Reference: [Gef92] <author> H. Geffner. </author> <title> Default reasoning: causal and conditional theories. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: In particular, the problem of inheritance to exceptional subclasses has been the most difficult. While some recent propositional theories have been more successful at dealing with exceptional subclass inheritance <ref> [GMP90, Gef92, GP92] </ref>, they encounter other difficulties, which we discuss in the next section. 3.4 Expressivity In the effort to discover basic techniques and principles for default reasoning, people have often looked at weak languages based on propositional logic.
Reference: [GHK92] <author> A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> Random worlds and maximum entropy. </title> <booktitle> In Proc. 7th IEEE Symp. on Logic in Computer Science, </booktitle> <pages> pages 22-33, </pages> <year> 1992. </year>
Reference-contexts: In Section 5, we state and prove a number of general theorems about the properties of the approach, and show how various desiderata follow from these theorems. In Section 6 we discuss the problem of calculating degrees of belief. Using results from <ref> [GHK92] </ref>, we demonstrate a close connection between random worlds and maximum entropy in the case of unary knowledge bases. Based on this connection, we show that in many cases of interest a maximum-entropy computation can be used to calculate an agent's degree of belief. <p> It is clear that we do not mean that exactly 80% of all patients with jaundice have hepatitis. Among other things, this would imply that the number of jaundiced patients is a multiple of five, which is surely not an intended implication. We therefore use the approach described in <ref> [GHK92, KH92] </ref>, and compare proportion expressions using (instead of = and ) one of an infinite family of connectives i and i , for i = 1; 2; 3 : : : ("i-approximately equal" or "i-approximately less than or equal"). 5 For example, we can express the statement "80% of jaundiced <p> This approach agrees with the standard interpretation of conditionals if jjjj X 6= 0. If jjjj X = 0, it enforces the convention that formulas such as k jk X = ff or k jk X ff are true for any ff. We used the same approach in <ref> [GHK92] </ref>, where we allowed approximate equality. Unfortunately, as the following example shows, this interpretation of conditional proportions can interact in an undesirable way with the semantics of approximate comparisons. In particular, this approach does not preserve the standard semantics of conditional equality if jjjj X is approximately 0. <p> However, as some of our examples show, in many situations the nonexistence of a degree of belief can be understood intuitively, and is sometimes related to the existence of multiple extensions of a default theory. (See Sections 4.3 and 5.3 and <ref> [GHK92] </ref>.) We remark that Shastri [Sha89] used a somewhat similar approach to defining degrees of belief. His language does not allow the direct expression of statistical information, but does allow us to talk about the number of domain individuals that satisfy a given predicate. <p> Shastri's result is based on maximum entropy. Maximum entropy is in fact a very general tool for computing degrees of belief, provided we restrict to knowledge bases that involve only unary predicates and are well-behaved in a sense made precise in <ref> [GHK92] </ref>. (See the discussion in Section 6.) 41 5.4 Independence As we have seen so far, random worlds captures a large number of the natural reasoning heuristics that have been proposed in the literature. Another heuristic is a default assumption that all properties are probabilistically independent unless we know otherwise. <p> In random worlds, there is a strong automatic bias towards unique names. If c 1 and c 2 are not mentioned anywhere in KB , then Pr 1 (c 1 = c 2 jKB) = 0 (See <ref> [GHK92, Lemma D.1] </ref> for a formal proof of this fact.) Of course, when we know something about c 1 and c 2 it is possible to find examples for which this result fails; for instance Pr 1 (c 1 = c 2 j (c 1 = c 2 ) _ (c <p> For unary languages (only) it can be shown that every formula can be rewritten in a canonical form from which constraints on the possible proportions of atoms can be simply derived. Details of this and all other specific results can be found in <ref> [GHK92] </ref>, although the general phenomenon we are about to discuss is addressed in many places, such as [PV89, Sha89] and in statistical physics (e.g., [Lan80]). The set of constraints generated by KB defines a subset of [0; 1] 2 k , which we call S (KB). <p> But now, we can use direct inference to conclude that Pr 1 (P 2 (c)jKB) 2 [0:3 *; 0:3 + *]. Since this holds for all sufficiently small *, we conclude that Pr 1 (P 2 (c)jKB) = 0:3, as desired. In <ref> [GHK92] </ref> we formalize this argument and generalize it to more complex examples. These techniques allow us to use a maximum entropy computation as a basis for computing degrees of belief. The resulting procedure applies to many cases not covered by our results in Section 5. <p> Not only does the random-worlds method provide motivation for maximum entropy, it can be viewed as a generalization of it. As discussed above, there is a strong connection between the random-worlds approach and maximum entropy in the unary case (see also <ref> [GHK92] </ref>). In fact, restricted versions of some of our results from Section 5 can be proved using maximum entropy (see [Sha89]). But our combinatorial proof techniques are far more general (and, in fact, simpler) than the ones based directly on entropy. <p> But our combinatorial proof techniques are far more general (and, in fact, simpler) than the ones based directly on entropy. The limitations of maximum entropy are perhaps inescapable, because (as we discuss in detail in <ref> [GHK92] </ref>) it is reasonable to conjecture that maximum entropy is inherently inapplicable once we move beyond unary predicates. <p> Note also that propositional variables become unary predicates. Hence, default rules become statistical assertions about classes of individuals. Under this translation, we obtain the following theorem (which is proved, and discussed in more detail, in <ref> [GHK92] </ref>). Theorem 6.1: Let c be a constant symbol. <p> But even in terms of computation, the situation with random worlds is not as bleak as it might seem. We have presented one class of much more tractable knowledge bases: those using only unary predicates and constants. We showed in <ref> [GHK92] </ref> and in Section 6 that, in this case, we can often use maximum entropy as a computational tool in deriving degrees of belief. While computing maximum entropy is also hard in general, there are many heuristic techniques that work efficiently in practical cases.
Reference: [GHK93a] <author> A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> Asymptotic conditional probabilities: Part I: the non-unary case. </title> <type> Research report, </type> <institution> IBM, </institution> <year> 1993. </year> <month> 62 </month>
Reference-contexts: Until such issues are understood, it is perhaps reasonable to ignore or downplay concerns about computation. If an ideal normative theory turns out to be impractical for computational reasons, we can still use it as guidance in a search for approximations and heuristics. As we show in <ref> [GHK93a] </ref>, computing degrees of belief according to random worlds is, indeed, intractable in general. This is not surprising: our language extends first-order logic, for which validity is undecidable. 17 Although unfortunate, we do not view this as an insurmountable problem. <p> Thus, results concerning computing degrees of belief for unary knowledge bases are quite significant in practice. Even for non-unary knowledge bases, there is hope. The intractability proofs given in <ref> [GHK93a] </ref> use knowledge bases that force the possible worlds to mimic a Turing machine computation. Typical knowledge bases do not usually encode Turing machines! There may therefore be many cases in which computation is practical. In particular, specific domains typically impose additional structure, which may simplify computation. <p> As observed in <ref> [GHK93a] </ref>, for all formulas ' and KB , if 0 , then Pr 1 ('jKB ) = Pr 0 (Intuitively, this is because the effect of changing the vocabulary cancels out in the numerator and denominator.) We thus get Pr 1 (' 1 ^ ' 2 jKB 1 ^ KB 2
Reference: [GHK93b] <author> A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> Asymptotic conditional probabilities: the unary case. </title> <type> Research report, </type> <institution> IBM, </institution> <year> 1993. </year>
Reference-contexts: Indeed, we do so in the one place where this dependence matters (Theorem 5.27). Fortunately, when computing degrees of belief, this dependence "cancels out": as shown in <ref> [GHK93b] </ref>, the degree of belief Pr ~t N ('jKB ) is independent of our choice of vocabulary, as long as it contains ' and KB . 23 Typically, we know neither N nor ~t exactly. All we know is that N is "large" and that ~t is "small". <p> Another way of seeing that 11 This actually relies on the fact that, with high probability, the proportion (as the domain size grows) of jaundiced patients with hepatitis is nonzero. We do not prove this fact here; see <ref> [PV89, GHK93b] </ref>. 32 the class (x) does not affect the random-worlds computation is to observe that its statistics are not informative, i.e., these statistics are true in almost all worlds. Hence (x)'s statistics places no constraints on the sets of worlds that determine the degree of belief.
Reference: [GMP90] <author> M. Goldszmidt, P. Morris, and J. Pearl. </author> <title> A maximum entropy approach to non-monotonic reasoning. </title> <booktitle> In Proc. National Conference on Artificial Intelligence (AAAI '90), </booktitle> <pages> pages 646-652, </pages> <year> 1990. </year>
Reference-contexts: Based on this connection, we show that in many cases of interest a maximum-entropy computation can be used to calculate an agent's degree of belief. Furthermore, we show that the maximum-entropy approach to default reasoning considered in <ref> [GMP90] </ref> can be embedded in our framework. <p> In particular, the problem of inheritance to exceptional subclasses has been the most difficult. While some recent propositional theories have been more successful at dealing with exceptional subclass inheritance <ref> [GMP90, Gef92, GP92] </ref>, they encounter other difficulties, which we discuss in the next section. 3.4 Expressivity In the effort to discover basic techniques and principles for default reasoning, people have often looked at weak languages based on propositional logic. <p> For instance, *-semantics and variants <ref> [GP90, GMP90] </ref>, modal approaches such as autoepistemic logic [Moo85], and conditional logics [Bou91], are usually considered in a propositional framework. <p> The connection between our work and *-semantics extends beyond the issue of representation: there is a deeper sense in which we can view our approach as the generalization of one of the extensions of *-semantics, namely the maximum-entropy approach of Goldszmidt, Morris, and Pearl <ref> [GMP90] </ref>, to the first-order setting. This issue is discussed in more detail in Section 6, where it is shown that this maximum-entropy approach can be embedded in our framework. <p> However, as we discuss below, the connection to random-worlds is interesting for other reasons as well. For instance, we use the connection to show that the maximum-entropy approach to default reasoning, considered in <ref> [GMP90] </ref>, can be embedded in our framework. <p> Maximum entropy has been a popular technique for probabilistic reasoning in AI and elsewhere. Two highly relevant works are the application to inheritance hierarchies by Shastri [Sha89] and to default reasoning by Goldszmidt, Morris, and Pearl <ref> [GMP90] </ref>. It is desirable that such a popular technique be well-understood and motivated, rather than be seen as an ad hoc heuristic. Random worlds, resting on the basic principle of indifference, provides motivation which some may find more convincing than the usual information-theoretic justifications. <p> Hence, among other limitations, it has no ability to ignore irrelevant information, so it cannot perform any inheritance reasoning. In order to obtain additional desirable properties, it is necessary to restrict the class of admissible PPD's. Goldszmidt, Morris, and Pearl <ref> [GMP90] </ref> focus attention on a single PPD: the maximum entropy PPD f fl *;R g *&gt;0 (See [GMP90] for precise definitions and technical details.) A rule B ! C is defined to be an ME-plausible consequence of R if lim *!0 fl *;R (CjB) = 1. <p> In order to obtain additional desirable properties, it is necessary to restrict the class of admissible PPD's. Goldszmidt, Morris, and Pearl <ref> [GMP90] </ref> focus attention on a single PPD: the maximum entropy PPD f fl *;R g *&gt;0 (See [GMP90] for precise definitions and technical details.) A rule B ! C is defined to be an ME-plausible consequence of R if lim *!0 fl *;R (CjB) = 1. The notion of ME-plausible consequence is analyzed in detail in [GMP90], where it is shown to inherit all the nice properties of <p> PPD: the maximum entropy PPD f fl *;R g *&gt;0 (See <ref> [GMP90] </ref> for precise definitions and technical details.) A rule B ! C is defined to be an ME-plausible consequence of R if lim *!0 fl *;R (CjB) = 1. The notion of ME-plausible consequence is analyzed in detail in [GMP90], where it is shown to inherit all the nice properties of *-entailment while successfully ignoring irrelevant information. Equally importantly, algorithms are provided for computing the ME-plausible consequences of a set of rules in certain cases (see also [GMP93]). <p> Equally importantly, algorithms are provided for computing the ME-plausible consequences of a set of rules in certain cases (see also [GMP93]). Our results relating random worlds to maximum entropy can be used to show that the approach of <ref> [GMP90] </ref> can be embedded in our framework in a straightforward manner. <p> Note that the formulas that arise under this conversion all use the same approximate equality relation 1 , since the approach of <ref> [GMP90] </ref> uses the same * for all default rules. Note also that propositional variables become unary predicates. Hence, default rules become statistical assertions about classes of individuals. Under this translation, we obtain the following theorem (which is proved, and discussed in more detail, in [GHK92]). <p> Theorem 6.1: Let c be a constant symbol. Using the translation described above, for any set R of defeasible rules, B ! C is an ME-plausible consequence of R iff Pr 1 ( C (c)j r2R Hence, all the computational techniques and results described in <ref> [GMP90] </ref> carry over to this special case of our approach. Furthermore, unary versions of all of our theorems carry over to the ME-plausible consequence relation. Examples demonstrating inheritance were given in [GMP90], but we can now use Theorem 5.16 to provide a formal characterization of some of the inheritance properties of <p> consequence of R iff Pr 1 ( C (c)j r2R Hence, all the computational techniques and results described in <ref> [GMP90] </ref> carry over to this special case of our approach. Furthermore, unary versions of all of our theorems carry over to the ME-plausible consequence relation. Examples demonstrating inheritance were given in [GMP90], but we can now use Theorem 5.16 to provide a formal characterization of some of the inheritance properties of this consequence relation. <p> We stress that the assumption that we use the same approximate equality relation is crucial in Theorem 6.1. Geffner [GP92] gives an example of an anomalous conclusion obtained in the system of <ref> [GMP90] </ref>. Suppose the rule set R consists of the two rules P ^ S ! Q and R ! :Q. In this case, the rule P ^ S ^ R ! Q is not an ME-plausible consequence of R.
Reference: [GMP93] <author> M. Goldszmidt, P. Morris, and J. Pearl. </author> <title> A maximum entropy approach to non-monotonic reasoning. </title> <journal> IEEE Transactions of Pattern Analysis and Machine Intelligence, </journal> <volume> 15(3) </volume> <pages> 220-232, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: The notion of ME-plausible consequence is analyzed in detail in [GMP90], where it is shown to inherit all the nice properties of *-entailment while successfully ignoring irrelevant information. Equally importantly, algorithms are provided for computing the ME-plausible consequences of a set of rules in certain cases (see also <ref> [GMP93] </ref>). Our results relating random worlds to maximum entropy can be used to show that the approach of [GMP90] can be embedded in our framework in a straightforward manner.
Reference: [Gol87] <author> S. A. Goldman. </author> <title> Efficient methods for calculating maximum entropy distributions. </title> <type> Master's thesis, </type> <institution> MIT EECS Department, </institution> <year> 1987. </year>
Reference-contexts: These techniques allow us to use a maximum entropy computation as a basis for computing degrees of belief. The resulting procedure applies to many cases not covered by our results in Section 5. Furthermore, since we can take advantage of existing algorithms for computing maximum entropy (see <ref> [Gol87] </ref> and the references therein), we obtain a technique of potential practical significance. The connection to maximum entropy is important for many reasons, aside from its computational implications. Maximum entropy has been a popular technique for probabilistic reasoning in AI and elsewhere.
Reference: [Goo92] <author> S. D. Goodwin. </author> <title> Second order direct inference: A reference class selection policy. </title> <journal> International Journal of Expert Systems: Research and Applications, </journal> <volume> 5(3) </volume> <pages> 185-210, </pages> <year> 1992. </year>
Reference-contexts: Example 5.25: We illustrate the problem with a example based on one of Goodwin's <ref> [Goo92] </ref>.
Reference: [GP90] <author> H. Geffner and J. Pearl. </author> <title> A framework for reasoning with defaults. </title> <editor> In H. E. Kyburg, Jr., R. Loui, and G. Carlson, editors, </editor> <booktitle> Knowledge Representation and Defeasible Reasoning, </booktitle> <pages> pages 245-26. </pages> <publisher> Kluwer Academic Press, </publisher> <address> Dordrecht, Netherlands, </address> <year> 1990. </year>
Reference-contexts: For instance, *-semantics and variants <ref> [GP90, GMP90] </ref>, modal approaches such as autoepistemic logic [Moo85], and conditional logics [Bou91], are usually considered in a propositional framework. <p> Using a higher fixed threshold in a straightforward way does not help. More successfully, Adams [Ada75], and later Geffner and Pearl <ref> [GP90] </ref>, suggested an interpretation of defaults based on "almost all". In their framework, this is done using extreme probabilities |conditional probabilities that are arbitrarily close to 1: i.e., within 1 * for some *, and considering the limit as * ! 0. <p> Finally, our results connecting random worlds to maximum entropy can also be put to use to help clarify the connection between random worlds and previous approaches to applying probabilistic semantics to default reasoning. The mainstay of most of this previous work has been the formalism of *-semantics <ref> [GP90] </ref>. We briefly review *-semantics here. <p> A set R of default rules *-entails B ! C if for every PPD that *-satisfies R, lim *!0 * (CjB) = 1. As shown in <ref> [GP90] </ref>, *-entailment possesses a number of reasonable properties typically associated with default reasoning, including a preference for more specific information. However, *-entailment is very weak.
Reference: [GP92] <author> H. Geffner and J. Pearl. </author> <title> Conditional entailment: bridging two approaches to default reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 53(2-3):209-244, </volume> <year> 1992. </year>
Reference-contexts: In particular, the problem of inheritance to exceptional subclasses has been the most difficult. While some recent propositional theories have been more successful at dealing with exceptional subclass inheritance <ref> [GMP90, Gef92, GP92] </ref>, they encounter other difficulties, which we discuss in the next section. 3.4 Expressivity In the effort to discover basic techniques and principles for default reasoning, people have often looked at weak languages based on propositional logic. <p> Let us examine how some existing systems do this. In propositional approaches, the usual strategy is to claim that there are different types of knowledge (see, for example, <ref> [GP92] </ref> and the references therein). General defaults, such as Bird ! Fly, are in one class. When we reason about an individual, such as Tweety, its properties are described by knowledge in a different class, the context . For Tweety, the context might be Bird ^ Yellow . <p> This is in keeping with the intuitive interpretations of rules and context used by propositional default systems (see Section 3.4). We stress that the assumption that we use the same approximate equality relation is crucial in Theorem 6.1. Geffner <ref> [GP92] </ref> gives an example of an anomalous conclusion obtained in the system of [GMP90]. Suppose the rule set R consists of the two rules P ^ S ! Q and R ! :Q.
Reference: [Hac75] <author> I. Hacking. </author> <title> The Emergence of Probability. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1975. </year>
Reference-contexts: Interestingly, the principle of indifference (sometimes also called the principle of insufficient reason) was originally promoted as part of the very definition of probability when the field was originally formalized by Jacob Bernoulli and others; the principle was later popularized further and applied with considerable success by Laplace. (See <ref> [Hac75] </ref> for a historical discussion.) It later fell into disrepute as a general definition of probability, largely because of the existence of paradoxes that arise when the principle is applied to infinite or continuous probability spaces.
Reference: [Hal90] <author> J. Y. Halpern. </author> <title> An analysis of first-order logics of probability. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 311-350, </pages> <year> 1990. </year>
Reference-contexts: So how do we choose the probability space? One general strategy, discussed by Halpern <ref> [Hal90] </ref>, is to give semantics to degrees of belief in terms of a probability distribution over a set of possible worlds, or first-order models. This semantics clarifies the distinction between statistical assertions and degrees of belief. <p> That is, we need to define semantics for k jk X even when there are no assignments to the variables in X that would satisfy . When standard equality is used rather than approximate equality, this problem is easily overcome. Following <ref> [Hal90] </ref>, we can eliminate conditional proportion expressions altogether by viewing a statement such as k jk X = ff as an abbreviation for jj ^ jj X = ffjjjj X . This approach agrees with the standard interpretation of conditionals if jjjj X 6= 0. <p> We give semantics to the language L by providing a translation from formulas in L to formulas in a language L = whose semantics is more easily described. The language L = is essentially the language of <ref> [Hal90] </ref>, that uses true equality rather than approximate equality. <p> We view an expression in L = that uses conditional proportion expressions as an abbreviation for the expression obtained by multiplying out. The semantics for L = is quite straightforward, and follows the lines of <ref> [Hal90] </ref>. Recall that we give semantics to L = in terms of worlds , or finite first-order models. For any natural number N , let W N () consist of all worlds with domain D = f1; : : : ; N g over the vocabulary .
Reference: [HM87] <author> S. Hanks and S. McDermott. </author> <title> Nonmonotonic logic and temporal projection. </title> <journal> Artificial Intelligence, </journal> <volume> 33(3) </volume> <pages> 379-412, </pages> <year> 1987. </year>
Reference-contexts: In particular, our proposal deals well with some of the standard problems in the area, for example the Yale Shooting Problem <ref> [HM87] </ref>. The details of the proposal are beyond the scope of this paper. However, the fact we want to emphasize here is that there may be more than one reasonable way to represent our knowledge of a given domain.
Reference: [Hun89] <author> D. Hunter. </author> <title> Causality and maximum entropy updating. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 3(1) </volume> <pages> 379-406, </pages> <year> 1989. </year>
Reference-contexts: As we have observed, the random-worlds method is closely related to maximum entropy (in the context of a unary knowledge base). One significant criticism of maximum entropy techniques has been that they seem to have difficulty dealing with causal information <ref> [Hun89, Pea88] </ref>. Hence, it is not surprising that the random-worlds method also gives peculiar answers if we represent causal and temporal information naively. On the other hand, Hunter [Hun89] has shown that maximum entropy methods can deal with causal information, provided it is represented appropriately. <p> One significant criticism of maximum entropy techniques has been that they seem to have difficulty dealing with causal information [Hun89, Pea88]. Hence, it is not surprising that the random-worlds method also gives peculiar answers if we represent causal and temporal information naively. On the other hand, Hunter <ref> [Hun89] </ref> has shown that maximum entropy methods can deal with causal information, provided it is represented appropriately. We have recently shown that by using an appropriate representation (related to Hunter's but quite different), the random-worlds method can also deal well with causal information [BGHK94a].
Reference: [Jay78] <author> E. T. Jaynes. </author> <title> Where do we stand on maximum entropy? In R. </title> <editor> D. Levine and M. Tribus, editors, </editor> <booktitle> The Maximum Entropy Formalism, </booktitle> <pages> pages 15-118. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1978. </year>
Reference-contexts: It has been argued [SW49] that the entropy measures the amount of "information" in a probability distribution, in the sense of information theory; note that the uniform distribution has the maximum possible entropy. The principle of maximum entropy <ref> [Jay78] </ref> addresses situations in which we have some constraints on a probability distribution, which may have many solutions, but where we must decide on one particular consistent distribution.
Reference: [Jef68] <author> R. C. Jeffrey. </author> <title> Probable knowledge. </title> <editor> In I. Lakatos, editor, </editor> <booktitle> International Colloquium in the Philosophy of Science: The Problem of Inductive Logic, </booktitle> <pages> pages 157-185. </pages> <publisher> North Holland Publishing Co., </publisher> <year> 1968. </year>
Reference-contexts: Nevertheless, if our confidence in White (b) exceeds some threshold, we might accept it (and so include it in KB). The problem of acceptance in such examples, concerned with what we learn directly from the senses, is well-known in philosophy <ref> [Jef68] </ref>. But the problem of acceptance we face is even more difficult than usual, because of our statistical language.
Reference: [Joh32] <author> W. E. Johnson. </author> <title> Probability: The deductive and inductive problems. </title> <journal> Mind, </journal> <volume> 41(164) </volume> <pages> 409-423, </pages> <year> 1932. </year>
Reference-contexts: The result is our random-worlds method. The key ideas in the approach are not new. Many of them can be found in the work of Johnson <ref> [Joh32] </ref> and Carnap [Car50, Car52], although these authors focus on knowledge bases that contain only first-order information, and for the most part restrict their attention to unary predicates.
Reference: [KH92] <author> D. Koller and J. Y. Halpern. </author> <title> A logic for approximate reasoning. </title> <editor> In B. Nebel, C. Rich, and W. Swartout, editors, </editor> <booktitle> Proc. Third International Conference on Principles of Knowledge Representation and Reasoning (KR '92), </booktitle> <pages> pages 153-164. </pages> <publisher> Mor-gan Kaufmann, </publisher> <address> San Francisco, </address> <year> 1992. </year>
Reference-contexts: It is clear that we do not mean that exactly 80% of all patients with jaundice have hepatitis. Among other things, this would imply that the number of jaundiced patients is a multiple of five, which is surely not an intended implication. We therefore use the approach described in <ref> [GHK92, KH92] </ref>, and compare proportion expressions using (instead of = and ) one of an infinite family of connectives i and i , for i = 1; 2; 3 : : : ("i-approximately equal" or "i-approximately less than or equal"). 5 For example, we can express the statement "80% of jaundiced <p> An alternative solution would be to use the approach presented in <ref> [KH92] </ref>. Roughly speaking, this approach interprets "almost all" as "arbitrarily close to 1" whenever such an interpretation is consistent (and thus allows us to get the benefits associated with this interpretation).
Reference: [KLM90] <author> S. Kraus, D. Lehmann, and M. Magidor. </author> <title> Nonmonotonic reasoning, preferential models and cumulative logics. </title> <journal> Artificial Intelligence, </journal> <volume> 44 </volume> <pages> 167-207, </pages> <year> 1990. </year>
Reference-contexts: Recently, a few tools have been developed that improve upon this approach. Gabbay [Gab84] (and later Makinson [Mak89] and Kraus, Lehmann, and Magidor <ref> [KLM90] </ref>) introduced the idea of investigating the input/output relation of a default reasoning system, with respect to certain general properties that such an inference relation might possess. Makinson [Mak94] gives a detailed survey of this work. The idea is simple. <p> There are five properties of j~ that have been viewed as being particularly desirable <ref> [KLM90] </ref>: * Right Weakening. If ' ) is logically valid and KB j~ ', then KB j~ . * Reflexivity. KB j~ KB . 11 * Left Logical Equivalence. <p> If KB j~ and KB ^ j~ ' then KB j~ '. * Cautious Monotonicity. If KB j~ and KB j~ ' then KB ^ j~ '. While it is beyond the scope of this paper to defend these criteria (see <ref> [KLM90] </ref>), we do want to stress Cut and Cautious Monotonicity, since they will be useful in our later results. <p> As shown in <ref> [KLM90] </ref>, numerous other conditions can be derived from these properties. For example, we can prove: * And. If KB j~ ' and KB j~ then KB j~ ' ^ . Other plausible properties do not follow from these basic five. <p> For example, the following property captures reasoning by cases: * Or. If KB j~ ' and KB 0 j~ ', then KB _ KB 0 j~ '. Perhaps the most interesting property that does not follow from the basic five properties is what has been called Rational Monotonicity <ref> [KLM90] </ref>. Note that the property of (full) monotonicity, which we do not want, says that KB j~ ' implies KB ^ j~ ', no matter what is.
Reference: [Kyb61] <author> H. E Kyburg, Jr. </author> <title> Probability and the Logic of Rational Belief. </title> <publisher> Wesleyan University Press, </publisher> <address> Middletown, Connecticut, </address> <year> 1961. </year>
Reference: [Kyb74] <author> H. E. Kyburg, Jr. </author> <title> The Logical Foundations of Statistical Inference. </title> <publisher> Reidel, </publisher> <address> Dor-drecht, Netherlands, </address> <year> 1974. </year>
Reference-contexts: For another, it is clear that the principle of preferring more specific information rarely suffices to deal with the cases that arise with a rich knowledge base. Nevertheless, much of the work on connecting statistical information and degrees of belief, including that of Kyburg <ref> [Kyb83, Kyb74] </ref> and of Pollock [Pol90], has built on Reichenbach's ideas of reference classes by elaborating the manner in which choices are made between reference classes. <p> Kyburg and Pollock deal with this difficulty by placing restrictions on the set of allowable reference classes that, although different, have the effect of disallowing disjunctive reference classes, including the problematic class described above. This approach suffers from two deficiencies. First, as Kyburg himself has observed <ref> [Kyb74] </ref>, these restrictions do not eliminate the problem completely. Furthermore, restricting the set of allowable reference classes may prevent us from making full use of the information we have.
Reference: [Kyb83] <author> H. E. Kyburg, Jr. </author> <title> The reference class. </title> <journal> Philosophy of Science, </journal> <volume> 50(3) </volume> <pages> 374-397, </pages> <year> 1983. </year>
Reference-contexts: For another, it is clear that the principle of preferring more specific information rarely suffices to deal with the cases that arise with a rich knowledge base. Nevertheless, much of the work on connecting statistical information and degrees of belief, including that of Kyburg <ref> [Kyb83, Kyb74] </ref> and of Pollock [Pol90], has built on Reichenbach's ideas of reference classes by elaborating the manner in which choices are made between reference classes. <p> Then we would want Pr ('(c)) = Pr ((c)). But this implies that all of the reference classes for (c) are relevant as well, because anything we can infer about Pr ((c)) tells us something about Pr ('(c)). Both Pollock [Pol90] and Kyburg <ref> [Kyb83] </ref> deal with this by considering all of the reference classes for any formula such that (c) , '(c) is known.
Reference: [Kyb88] <author> H. E. Kyburg, Jr. </author> <title> Full beliefs. </title> <journal> Theory and Decision, </journal> <volume> 25 </volume> <pages> 137-162, </pages> <year> 1988. </year>
Reference-contexts: That is, when do we accept information as knowledge? We do not have a good answer to this question. This is unfortunate, since it seems plausible that the processes of gaining knowledge and computing degrees of belief should be interrelated. In particular, Kyburg <ref> [Kyb88] </ref> has argued that perhaps we might accept assertions that are believed sufficiently strongly. For example, suppose we observe a block b that appears to be white. It could be that we are is not entirely sure that the block is indeed white; it might be some other light color.
Reference: [Lan80] <author> L. D. </author> <title> Landau. </title> <journal> Statistical Physics, </journal> <volume> volume 1. </volume> <publisher> Pergamon Press, </publisher> <year> 1980. </year>
Reference-contexts: Details of this and all other specific results can be found in [GHK92], although the general phenomenon we are about to discuss is addressed in many places, such as [PV89, Sha89] and in statistical physics (e.g., <ref> [Lan80] </ref>). The set of constraints generated by KB defines a subset of [0; 1] 2 k , which we call S (KB).
Reference: [Lif89] <author> V. Lifschitz. </author> <title> Benchmark problems for formal non-monotonic reasoning, version 2.00. </title> <editor> In M. Reinfrank, J. de Kleer, M. Ginsberg, and E. Sandewall, editors, </editor> <booktitle> Non-Monotonic Reasoning: 2nd International Workshop (Lecture Notes in Artificial Intelligence 346), </booktitle> <pages> pages 202-219. </pages> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: Second, a closely related issue is raised by Lifschitz's list of benchmark problems <ref> [Lif89] </ref>. Suppose we have a default, for instance Ticket (x) ! :Winner (x), and no other knowledge. <p> Unfortunately, to the best of our knowledge, there is (as yet) no general framework for evaluating default reasoning systems. In particular, evaluation still tends to be on the level of "Does this theory solve these particular examples correctly?" (see, for example, the list of benchmark problems in <ref> [Lif89] </ref>). While such examples are often important in identifying interesting aspects of the problem and defining our intuitions in these cases, they are clearly not a substitute for a comprehensive framework. <p> It is hard to give a general theorem saying precisely when the bias towards unique names overrides other considerations. However, we note that both of the "benchmark" examples that Lifschitz has given concerning unique names <ref> [Lif89] </ref> are correctly handled by random-worlds. For instance, Lifschitz's problem C1 is: 1. Different names normally denote different people. 2. The names "Ray" and "Reiter" denote the same person. 3. The names "Drew" and "McDermott" denote the same person.
Reference: [LM81] <author> R. J. Larsen and M. L. Mark. </author> <title> An introduction to mathematical statistics and its applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: '(x) holds) so that fl 0 n 0 elements come from A 0 ? We estimate this using the observation that the distribution of fl 0 n 0 is derived from a process of sampling without replacement. 18 Hence, it behaves according to the well-known hypergeometric distribution (see, for example, <ref> [LM81] </ref>). We can thus conclude that fl 0 is distributed with mean fl and variance fl (1 fl)(n n 0 ) fl (1 fl) 1 Since KB j= :(jj 0 (x)jj x 1 0), we know that n 0 = jA 0 j t 1 N .
Reference: [LM90] <author> D. Lehmann and M. Magidor. </author> <title> Preferential logics: the predicate calculus case. </title> <editor> In R. Parikh, editor, </editor> <booktitle> Theoretical Aspects of Reasoning about Knowledge: Proc. Third Conference, </booktitle> <pages> pages 57-72, </pages> <address> San Francisco, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Both destroy the modularity of the knowledge base, i.e., the form of a default becomes dependent on what other defaults are in the knowledge base. Hence, these techniques are highly impractical for large knowledge bases. The zookeeper example is similar to an example given by Lehmann and Magidor <ref> [LM90] </ref>. However, the solution they suggest to this problem does not provide an explicit interpretation for open defaults. Rather, the "meaning" of an open default is implicitly determined by a set of rules provided for manipulating such defaults.
Reference: [LM92] <author> D. Lehmann and M. Magidor. </author> <title> What does a conditional knowledge base entail? Artificial Intelligence, </title> <booktitle> 55(1) </booktitle> <pages> 1-60, </pages> <year> 1992. </year>
Reference-contexts: The following property asserts that monotonicity holds when adding such a formula to our knowledge base: * Rational Monotonicity. If KB j~ ' and it is not the case that KB j~ :, then KB ^ j~ '. Several people, notably Lehmann and Magidor <ref> [LM92] </ref>, have argued strongly for the desirability of this principle. One advantage of Rational Monotonicity is that it covers some fairly noncontroversial patterns of reasoning involving property inheritance. We explore this further in the next section. <p> The inheritance property, i.e., the ability to inherit defaults from superclasses, is a second criterion for successful default reasoning, and is not provided by Direct Inference for Defaults. In some sense, we can view Rational Monotonicity as providing a partial solution to this problem <ref> [LM92] </ref>. If a nonmonotonic reasoning system satisfies Rational Monotonicity in addition to Direct Inference for Defaults then it does achieve inheritance in a large number of examples. For instance, we have already observed that Direct Inference for Defaults gives Penguin (Tweety) j~ fly :Fly (Tweety), given KB fly . <p> Indeed, there are well-known systems that satisfy Rational Monotonicity but cannot conclude 4 In any system that treats universals reasonably, this is clearly equivalent to the assertion we are really interested in: Penguin (Tweety) j~ fly Warm-blooded (Tweety). 14 that Tweety, the yellow penguin, is easy to see <ref> [LM92, Pea90] </ref>. This problem has been called the drowning problem [Ash93, BCD + 93]. Theories of default reasoning have had considerable difficulty in capturing an ability to inherit from superclasses that can deal properly with all of these different cases.
Reference: [LR57] <author> R. D. Luce and H. Raiffa. </author> <title> Games and Decisions. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1957. </year>
Reference-contexts: Since the efficacy of a treatment will almost certainly depend on the disease, it is important for the doctor to be able to quantify the relative likelihood of various possibilities. More generally, to apply standard tools for decision making such as decision theory (see, e.g., <ref> [LR57, Sav54] </ref>), an agent must assign probabilities, or degrees of belief, to various events. For example, the doctor may wish to assign a degree of belief to an event such as "Eric has hepatitis".
Reference: [Mak89] <author> D. Makinson. </author> <title> General theory of cumulative inference. </title> <editor> In M. Reinfrank, J. de Kleer, M. Ginsberg, and E. Sandewall, editors, </editor> <booktitle> Non-Monotonic Reasoning: 2nd International Workshop (Lecture Notes in Artificial Intelligence 346), </booktitle> <pages> pages 1-18. </pages> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: Recently, a few tools have been developed that improve upon this approach. Gabbay [Gab84] (and later Makinson <ref> [Mak89] </ref> and Kraus, Lehmann, and Magidor [KLM90]) introduced the idea of investigating the input/output relation of a default reasoning system, with respect to certain general properties that such an inference relation might possess. Makinson [Mak94] gives a detailed survey of this work. The idea is simple.
Reference: [Mak94] <author> D. Makinson. </author> <title> General patterns in nonmonotonic reasoning. </title> <editor> In D. Gabbay, C. Hog-ger, and J. Robinson, editors, </editor> <booktitle> Handbook of Logic in Artificial Intelligence and Logic Programming, </booktitle> <volume> volume 3, </volume> <pages> pages 35-110. </pages> <publisher> Oxford University Press, </publisher> <year> 1994. </year>
Reference-contexts: Gabbay [Gab84] (and later Makinson [Mak89] and Kraus, Lehmann, and Magidor [KLM90]) introduced the idea of investigating the input/output relation of a default reasoning system, with respect to certain general properties that such an inference relation might possess. Makinson <ref> [Mak94] </ref> gives a detailed survey of this work. The idea is simple. Fix a theory of default reasoning and let KB be some knowledge base appropriate to this theory. Suppose ' is a default conclusion reached from KB according to the particular default approach being considered. <p> The set of properties we have discussed provides a simple, but useful, system for classifying default theories. There are certainly applications in which some of the properties are inappropriate; Reiter's default logic is still popular even though it does not satisfy Cautious Monotonicity, Or, or Rational Monotonicity <ref> [Mak94] </ref>. (We briefly discuss one of the consequent disadvantages of default logic in the next section.) Nevertheless, many people would argue that these properties constitute a reasonable, if incomplete, set of desiderata for mainstream default theories. 12 3.3 Specificity and inheritance As we have pointed out, systems of default reasoning have
Reference: [McC80] <author> J. McCarthy. </author> <title> Circumscription|a form of non-monotonic reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 13:1,2, </volume> <year> 1980. </year>
Reference-contexts: For instance, in Reiter's default logic [Rei80] we would write A (x) : B (x) while in a circumscriptive framework <ref> [McC80] </ref>, we might use 8x (A (x) ^ :Ab (x) ) B (x)) 3 We use ! for a default implication, reserving ) for standard material implication. 10 while circumscribing Ab (x). Theories based on first-order conditional logic [Del88] often do use the syntax A (x) ! B (x).
Reference: [McC86] <author> J. McCarthy. </author> <title> Applications of circumscription to formalizing common-sense knowledge. </title> <journal> Artificial Intelligence, </journal> <volume> 28 </volume> <pages> 86-116, </pages> <year> 1986. </year> <month> 64 </month>
Reference-contexts: Evaluating such a claim is hard because there are many, often rather vague, criteria for success that one can consider. In particular, not all criteria are appropriate for all default reasoning systems: Different applications (such as some of the ones outlined in <ref> [McC86] </ref>) require different interpretations for a default rule, and therefore need to satisfy different desiderata. Nevertheless, there are certain desiderata that have gained acceptance as measures for the success of a new nonmonotonic reasoning system. Some are general properties of nonmonotonic inference (see Section 3.2).
Reference: [MH69] <author> J. M. McCarthy and P. J. Hayes. </author> <title> Some philosophical problems from the standpoint of artificial intelligence. </title> <editor> In D. Michie, editor, </editor> <booktitle> Machine Intelligence 4, </booktitle> <pages> pages 463-502. </pages> <publisher> Edinburgh University Press, Edinburgh, </publisher> <address> UK, </address> <year> 1969. </year>
Reference-contexts: In Bayesian reasoning, relatively little is said about how this should be done. Indeed, the usual philosophy is that these decisions are subjective. The difficulty of making these decisions seems to have been an important reason for the historic unpopularity of the Bayesian approach in symbolic AI <ref> [MH69] </ref>. Our approach is different. We assume that the KB contains all the knowledge the agent has, and we allow a very expressive language so as to make this assumption reasonable. <p> Indeed, our representation allows us to (a) deal with prediction and explanation problems, (b) represent causal information of the type implicit in Bayesian causal networks [Pea88], and (c) provide a clean and concise solution to the frame problem in the situation calculus <ref> [MH69] </ref>. In particular, our proposal deals well with some of the standard problems in the area, for example the Yale Shooting Problem [HM87]. The details of the proposal are beyond the scope of this paper.
Reference: [Moo85] <author> R. C. Moore. </author> <title> Semantical considerations on nonmonotonic logic. </title> <journal> Artificial Intelligence, </journal> <volume> 25 </volume> <pages> 75-94, </pages> <year> 1985. </year>
Reference-contexts: For instance, *-semantics and variants [GP90, GMP90], modal approaches such as autoepistemic logic <ref> [Moo85] </ref>, and conditional logics [Bou91], are usually considered in a propositional framework. Others, such as Reiter's default logic and Delgrande's conditional logic [Del88], use a first-order language, but with a syntax that tends to decouple the issues of first-order reasoning and default reasoning; we discuss this below.
Reference: [Mor93] <author> M. </author> <title> Morreau. The conditional logic of generalizations. </title> <booktitle> In Proceedings of the IJCAI Workshop on Conditionals in Knowledge Representation, </booktitle> <pages> pages 108-118, </pages> <year> 1993. </year>
Reference-contexts: This is surely undesirable. Thus, it seems to be very hard to interpret generic (open) defaults properly. This is perhaps the best-known issue regarding the expressive power of various approaches to default logic. There are, of course, others; we close by mentioning one. Morreau <ref> [Mor93] </ref> has discussed the usefulness of being able to refer to "the class of individuals satisfying a certain default". For example, the assertion: Typically, people who normally go to bed late normally rise late. refers to "the class of people who normally go to bed late". <p> This default can easily be expressed in our language: kTall (x)j9y (Child (x; y) ^ Tall (y))k x i 1: We can even define defaults over classes themselves defined using default rules (as discussed by Morreau <ref> [Mor93] </ref>).
Reference: [Pea88] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: As we have observed, the random-worlds method is closely related to maximum entropy (in the context of a unary knowledge base). One significant criticism of maximum entropy techniques has been that they seem to have difficulty dealing with causal information <ref> [Hun89, Pea88] </ref>. Hence, it is not surprising that the random-worlds method also gives peculiar answers if we represent causal and temporal information naively. On the other hand, Hunter [Hun89] has shown that maximum entropy methods can deal with causal information, provided it is represented appropriately. <p> Indeed, our representation allows us to (a) deal with prediction and explanation problems, (b) represent causal information of the type implicit in Bayesian causal networks <ref> [Pea88] </ref>, and (c) provide a clean and concise solution to the frame problem in the situation calculus [MH69]. In particular, our proposal deals well with some of the standard problems in the area, for example the Yale Shooting Problem [HM87].
Reference: [Pea89] <author> J. Pearl. </author> <title> Probabilistic semantics for nonmonotonic reasoning: A survey. </title> <editor> In R. J. Brachman, H. J. Levesque, and R. Reiter, editors, </editor> <booktitle> Proc. First International Conference on Principles of Knowledge Representation and Reasoning (KR '89), </booktitle> <pages> pages 505-516, </pages> <year> 1989. </year> <note> Reprinted in Readings in Uncertain Reasoning, </note> <editor> G. Shafer and J. Pearl (eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990, </year> <pages> pp. 699-710. </pages>
Reference-contexts: We interpret a statement such as "Birds typically fly" as expressing the statistical assertion that "Almost 2 all birds fly". Using approximate equality, we can represent this as kFly (x)jBird (x)k x 1. This interpretation is closely related to various approaches applying probabilistic semantics to nonmonotonic logic; see Pearl <ref> [Pea89] </ref> for an overview of these approaches, and Section 6 for further discussion. Having described the language in which our knowledge base is expressed, we now need to decide how to assign degrees of belief given a knowledge base. <p> The basic system derived from this idea is called *-semantics. Later, stronger systems (that are able to make more inferences) based on the same probabilistic idea were introduced (see Pearl <ref> [Pea89] </ref> for a survey). The intuition behind *-semantics and its extensions is statistical. However, since the language used in these approaches is propositional, this intuition cannot be expressed directly.
Reference: [Pea90] <author> J. Pearl. </author> <title> System Z: A natural ordering of defaults with tractable applications to nonmonotonic reasoning. </title> <editor> In M. Vardi, editor, </editor> <booktitle> Theoretical Aspects of Reasoning about Knowledge: Proc. Third Conference, </booktitle> <pages> pages 121-135. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <year> 1990. </year>
Reference-contexts: Indeed, there are well-known systems that satisfy Rational Monotonicity but cannot conclude 4 In any system that treats universals reasonably, this is clearly equivalent to the assertion we are really interested in: Penguin (Tweety) j~ fly Warm-blooded (Tweety). 14 that Tweety, the yellow penguin, is easy to see <ref> [LM92, Pea90] </ref>. This problem has been called the drowning problem [Ash93, BCD + 93]. Theories of default reasoning have had considerable difficulty in capturing an ability to inherit from superclasses that can deal properly with all of these different cases.
Reference: [Pol90] <author> J. L. Pollock. </author> <title> Nomic Probabilities and the Foundations of Induction. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> U.K., </address> <year> 1990. </year>
Reference-contexts: For another, it is clear that the principle of preferring more specific information rarely suffices to deal with the cases that arise with a rich knowledge base. Nevertheless, much of the work on connecting statistical information and degrees of belief, including that of Kyburg [Kyb83, Kyb74] and of Pollock <ref> [Pol90] </ref>, has built on Reichenbach's ideas of reference classes by elaborating the manner in which choices are made between reference classes. <p> Then we would want Pr ('(c)) = Pr ((c)). But this implies that all of the reference classes for (c) are relevant as well, because anything we can infer about Pr ((c)) tells us something about Pr ('(c)). Both Pollock <ref> [Pol90] </ref> and Kyburg [Kyb83] deal with this by considering all of the reference classes for any formula such that (c) , '(c) is known.
Reference: [Poo89] <author> D. Poole. </author> <title> What the lottery paradox tells us about default reasoning. </title> <editor> In R. J. Brachman, H. J. Levesque, and R. Reiter, editors, </editor> <booktitle> Proc. First International Conference on Principles of Knowledge Representation and Reasoning (KR '89), </booktitle> <pages> pages 333-340. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <year> 1989. </year>
Reference-contexts: We then appeal to Proposition 5.2 to justify using the new knowledge base instead of the old one. The other rules are also useful, as shown in the following analysis of Poole's "broken-arm" example <ref> [Poo89] </ref>. Example 5.4: Suppose we have predicates LeftUsable, LeftBroken, RightUsable, RightBroken, indicating, respectively, that the left arm is usable, the left arm is broken, the right arm is usable, and the right arm is broken.
Reference: [Poo91] <author> D. Poole. </author> <title> The effect of knowledge on belief: conditioning, specificity and the lottery paradox in default reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 49(1-3):282-307, </volume> <year> 1991. </year>
Reference-contexts: If (A (x) ! B (x)) 2 KB def and KB def contains no asser tions mentioning c, then A (c) j~ def B (c). This requirement has been previously discussed by Poole <ref> [Poo91] </ref>, who called it the property of Conditioning. We have chosen a different name that relates the property more directly to earlier notions arising in work on direct inference. <p> As we shall see, the random-worlds approach obtains this conclusion as well. Finally, Poole <ref> [Poo91] </ref> has considered a variant of the lottery paradox that avoids entirely the issue of named individuals.
Reference: [PV89] <author> J. B. Paris and A. Vencovska. </author> <title> On the applicability of maximum entropy to inexact reasoning. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 3 </volume> <pages> 1-34, </pages> <year> 1989. </year>
Reference-contexts: Related approaches have been used in the more recent works of Shastri [Sha89] and of Paris and Vencovska <ref> [PV89] </ref>, in the context of a unary statistical language. Chuaqui's recent work [Chu91] is also relevant. His work, although technically quite different from ours, shares the idea of basing a theory of probabilistic reasoning upon notions of indifference and symmetry. <p> Another way of seeing that 11 This actually relies on the fact that, with high probability, the proportion (as the domain size grows) of jaundiced patients with hepatitis is nonzero. We do not prove this fact here; see <ref> [PV89, GHK93b] </ref>. 32 the class (x) does not affect the random-worlds computation is to observe that its statistics are not informative, i.e., these statistics are true in almost all worlds. Hence (x)'s statistics places no constraints on the sets of worlds that determine the degree of belief. <p> Details of this and all other specific results can be found in [GHK92], although the general phenomenon we are about to discuss is addressed in many places, such as <ref> [PV89, Sha89] </ref> and in statistical physics (e.g., [Lan80]). The set of constraints generated by KB defines a subset of [0; 1] 2 k , which we call S (KB).
Reference: [RC81] <author> R. Reiter and G. Criscuolo. </author> <title> On interacting defaults. </title> <booktitle> In Proc. Seventh International Joint Conference on Artificial Intelligence (IJCAI '81), </booktitle> <pages> pages 270-276, </pages> <year> 1981. </year>
Reference-contexts: However, default logic and circumscription are certainly powerful enough for us to be able to arrange specificity if we wish. For example, in default logic, this can be done by means of non-normal defaults 13 <ref> [RC81] </ref>. There is a cost to doing this, however: adding a default rule can require that all older default rules be reexamined, and possibly changed, to enforce the desired precedences. <p> We illustrate this theorem on what is, perhaps, the most famous example of conflicting information|the Nixon Diamond <ref> [RC81] </ref>. Suppose we are interested in assigning a degree of belief to the assertion "Nixon is a pacifist". Assume that we know that Nixon is both a Quaker and a Republican, and we have statistical information for the proportion of pacifists within both classes.
Reference: [Rei49] <author> H. Reichenbach. </author> <title> Theory of Probability. </title> <institution> University of California Press, Berkeley, </institution> <year> 1949. </year>
Reference-contexts: There has been a great deal of work addressing aspects of this general problem. Two large bodies of work that are particularly relevant are the work on direct inference, going back to Reichenbach <ref> [Rei49] </ref>, and the various approaches to nonmonotonic reasoning. Direct inference deals with the problem of deriving degrees of belief from statistical information, typically by attempting to find a suitable reference class whose statistics can be used to determine the degree of belief. <p> here, since the issues that arise provide some motivation for the results that we prove later regarding our approach. 2.1 The basic approach The earliest sophisticated attempt at clarifying the connection between objective statistical knowledge and degrees of belief, and the basis for most subsequent proposals, is due to Re-ichenbach <ref> [Rei49] </ref>. Reichenbach describes the idea as follows: "If we are asked to find the probability holding for an individual future event, we must first incorporate the case in a suitable reference class. An individual thing 5 or event may be incorporated in many reference classes: : : .
Reference: [Rei80] <author> R. Reiter. </author> <title> A logic for default reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 13 </volume> <pages> 81-132, </pages> <year> 1980. </year>
Reference-contexts: For instance, in Reiter's default logic <ref> [Rei80] </ref> we would write A (x) : B (x) while in a circumscriptive framework [McC80], we might use 8x (A (x) ^ :Ab (x) ) B (x)) 3 We use ! for a default implication, reserving ) for standard material implication. 10 while circumscribing Ab (x). <p> The fact that we obtain different limiting degrees of belief depending on how ~t goes to 0 is closely related to the existence of multiple extensions in many other theories of default reasoning (for instance, in default logic <ref> [Rei80] </ref>). Both non-robustness and the existence of more than one extension suggest a certain incompleteness of our knowledge. It is well-known that, in the presence of conflicting defaults, we often need more information about the strength of the different defaults in order to resolve the conflict.
Reference: [Sav54] <author> L. J. Savage. </author> <title> Foundations of Statistics. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1954. </year>
Reference-contexts: Since the efficacy of a treatment will almost certainly depend on the disease, it is important for the doctor to be able to quantify the relative likelihood of various possibilities. More generally, to apply standard tools for decision making such as decision theory (see, e.g., <ref> [LR57, Sav54] </ref>), an agent must assign probabilities, or degrees of belief, to various events. For example, the doctor may wish to assign a degree of belief to an event such as "Eric has hepatitis".
Reference: [Sha] <author> G. Shafer. </author> <type> Personal communication, </type> <year> 1993. </year> <month> 65 </month>
Reference-contexts: In fact, a good case can be made that statisticians tend to reformulate all problems in terms of unary predicates, since an event in a sample space can be identified with a unary predicate <ref> [Sha] </ref>. Indeed, most cases where statistics are used, we have a basic unit in mind (an individual, a family, a household, etc.), and the properties (predicates) we consider are typically relative to a single unit (i.e., unary predicates).
Reference: [Sha76] <author> G. Shafer. </author> <title> A Mathematical Theory of Evidence. </title> <publisher> Princeton University Press, </publisher> <address> Prince--ton, NJ, </address> <year> 1976. </year>
Reference-contexts: For simplicity, we omit the details of this extension here. It turns out that, under this assumption, random worlds provides an independent derivation of a well-known technique for combining evidence: Dempster's rule of combination <ref> [Sha76] </ref>. 13 Strictly speaking, a direct application of Theorem 5.23 would require that KB chirps contains :(jjMagpie (x)jj x i 0). But the maximum-entropy techniques of Section 6 can be used to show that this actually follows by default.
Reference: [Sha89] <author> L. Shastri. </author> <title> Default reasoning in semantic networks: a formalization of recognition and inheritance. </title> <journal> Artificial Intelligence, </journal> <volume> 39(3) </volume> <pages> 285-355, </pages> <year> 1989. </year>
Reference-contexts: Related approaches have been used in the more recent works of Shastri <ref> [Sha89] </ref> and of Paris and Vencovska [PV89], in the context of a unary statistical language. Chuaqui's recent work [Chu91] is also relevant. His work, although technically quite different from ours, shares the idea of basing a theory of probabilistic reasoning upon notions of indifference and symmetry. <p> However, as some of our examples show, in many situations the nonexistence of a degree of belief can be understood intuitively, and is sometimes related to the existence of multiple extensions of a default theory. (See Sections 4.3 and 5.3 and [GHK92].) We remark that Shastri <ref> [Sha89] </ref> used a somewhat similar approach to defining degrees of belief. His language does not allow the direct expression of statistical information, but does allow us to talk about the number of domain individuals that satisfy a given predicate. <p> Because our theorems are derived properties rather than postulates, consistency is assured and there are no ad hoc syntactic restrictions on the choice of possible reference classes. We remark that Shastri <ref> [Sha89] </ref> has also observed that irrelevance properties hold in his framework. Our first, and simpler, result is basic direct inference, where we have a single reference class that is precisely the "right one". <p> As we mentioned, Theorem 5.26 tells us only how to combine statistics from competing reference classes in the very special case where the intersection of the different reference classes is small. Shastri <ref> [Sha89, pp. 331-332] </ref> describes a result in the same spirit, but for a different special case: he assumes that, in addition to the statistics for P within each reference class, the statistics for P in the general population are also known. Shastri's result is based on maximum entropy. <p> Details of this and all other specific results can be found in [GHK92], although the general phenomenon we are about to discuss is addressed in many places, such as <ref> [PV89, Sha89] </ref> and in statistical physics (e.g., [Lan80]). The set of constraints generated by KB defines a subset of [0; 1] 2 k , which we call S (KB). <p> The connection to maximum entropy is important for many reasons, aside from its computational implications. Maximum entropy has been a popular technique for probabilistic reasoning in AI and elsewhere. Two highly relevant works are the application to inheritance hierarchies by Shastri <ref> [Sha89] </ref> and to default reasoning by Goldszmidt, Morris, and Pearl [GMP90]. It is desirable that such a popular technique be well-understood and motivated, rather than be seen as an ad hoc heuristic. <p> As discussed above, there is a strong connection between the random-worlds approach and maximum entropy in the unary case (see also [GHK92]). In fact, restricted versions of some of our results from Section 5 can be proved using maximum entropy (see <ref> [Sha89] </ref>). But our combinatorial proof techniques are far more general (and, in fact, simpler) than the ones based directly on entropy.
Reference: [SW49] <author> C. Shannon and W. Weaver. </author> <title> The Mathematical Theory of Communication. </title> <publisher> University of Illinois Press, </publisher> <year> 1949. </year>
Reference-contexts: Briefly, the entropy of a probability distribution over a finite space is defined as H () = !2 (!) ln ((!)). It has been argued <ref> [SW49] </ref> that the entropy measures the amount of "information" in a probability distribution, in the sense of information theory; note that the uniform distribution has the maximum possible entropy.
Reference: [THT87] <author> D. S. Touretzky, J. F. Horty, and R. H. Thomason. </author> <title> A clash of intuitions: the current state of nonmonotonic multiple inheritance systems. </title> <booktitle> In Proc. Tenth International Joint Conference on Artificial Intelligence (IJCAI '87), </booktitle> <pages> pages 476-482, </pages> <year> 1987. </year>
Reference-contexts: While these theorems show that random worlds does support many noncontroversial instances of such reasoning, proving a more general theorem asserting this claim is surprisingly subtle (partly because of the existence of numerous divergent semantics and intuitions for inheritance reasoning <ref> [THT87] </ref>). We are currently working towards stating and proving such a general claim, for the case in which we have an inheritance hierarchy of defaults and universal implications.
References-found: 70

