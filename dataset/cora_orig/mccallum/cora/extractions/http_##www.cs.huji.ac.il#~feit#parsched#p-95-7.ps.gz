URL: http://www.cs.huji.ac.il/~feit/parsched/p-95-7.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~feit/parsched/parsched95.html
Root-URL: http://www.cs.huji.ac.il
Email: feparsons,kcsg@cs.toronto.edu  
Title: Multiprocessor Scheduling for High-Variability Service Time Distributions  
Author: Eric W. Parsons and Kenneth C. Sevcik 
Address: Toronto  
Affiliation: Computer Systems Research Institute University of  
Abstract: Many disciplines have been proposed for scheduling and processor allocation in multiprogrammed multiprocessors for parallel processing. These have been, for the most part, designed and evaluated for workloads having relatively low variability in service demand. But with reports that variability in service demands at high performance computing centers can actually be quite high, these disciplines must be reevaluated. In this paper, we examine the performance of two well-known static scheduling disciplines, and propose preemptive versions of these that offer much better mean response times when the variability in service demand is high. We argue that, in systems in which dynamic repartitioning in applications is expensive or impossible, these preemptive disciplines are well suited for handling high variability in service demand.
Abstract-found: 1
Intro-found: 1
Reference: [BHMW94] <author> Douglas C. Burger, Rahmat S. Hyder, Barton P. Miller, and David A. Wood. </author> <title> Paging tradeoffs in distributed-shared-memory multiprocessors. </title> <booktitle> In Proceedings Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: One issue that the study does not consider is the resource requirements of jobs. In particular, none of the disciplines examined, particularly IEQ, will perform well if memory is overcommitted, as paging overhead can significantly impact the progress of a computation <ref> [BHMW94] </ref>. Clearly, future scheduling disciplines for multiprogrammed multiprocessor systems must take into account all requirements of jobs, including processor, memory, and I/O.
Reference: [CMV94] <author> Su-Hui Chiang, Rajesh K. Mansharamani, and Mary K. Vernon. </author> <title> Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modelling of Computer Systems, </booktitle> <pages> pages 33-44, </pages> <year> 1994. </year>
Reference-contexts: In another study, Chiang, Mansharamani, and Vernon report that the coefficient of variation observed on a weekly basis on the CM-5 at the University of Wisconsin ranges from 2.5 to 6, with 40% of them being above 4 <ref> [CMV94] </ref>. They also report that some measurements from Cray YMP sites range from 30 to 70 [CMV94, Ver94]. <p> They also report that some measurements from Cray YMP sites range from 30 to 70 <ref> [CMV94, Ver94] </ref>. In this paper, we consider two well-known static scheduling disciplines, showing how they behave under high variability in service demand, and propose ways in which they can be adapted to better handle this condition. <p> Subsequent simulation studies provide further evidence of the benefits of a variant of ASP where the maximum allocation to a job is bounded <ref> [CMV94] </ref>. Although most of the results relate to C d = 5, one experiment demonstrates that bounded ASP also performs well when C d = 30. In this paper, we only use ASP as originally defined [ST93], with no bound on the maximum allocation to a job. <p> Equipartition has been shown to be effective over a wide range of workloads and a wide range of distributions in service demand <ref> [LV90, CMV94] </ref>. When system loads increase, allocations to jobs decreases allowing them to operate at a more favourable point on their efficiency curve. Also, because equipartition is effectively the analog of RR in uniprocessing, it is relatively insensitive to variability in service requirement.
Reference: [EZL89] <author> Derek L. Eager, John Zahorjan, and Edward D. Lazowska. </author> <title> Speedup versus efficiency in parallel systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(3) </volume> <pages> 408-423, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: showed that a good number of processors to allocate to a job is the value that leads to the smallest ratio of execution time to efficiency (i.e., the value at the knee of the execution time-efficiency profile), as this is a point that maximizes the ratio of benefit to cost <ref> [EZL89] </ref>. If the number of processors allocated at the knee of the curve is not known, allocating a number of processors equal to the average parallelism has been shown to be a good alternative. <p> The average parallelism (A j ) of a job j is the average degree of parallelism exhibited by the job over its lifetime. If there are no overheads due to parallelism, A j is equivalent to the speedup on an unlimited number of processors <ref> [EZL89] </ref>. The overall workload volume, when known, should also be taken into account in the scheduling decision [Sev89]. By reducing the number of processors allocated to jobs as the system load increases, the mean response time can be greatly improved.
Reference: [FN95] <author> Dror G. Feitelson and Bill Nitzberg. </author> <title> Job characteristics of a production parallel scientific workload on the NASA Ames iPSC/860. In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <booktitle> Lecture Notes in Computer Science Vol. </booktitle> <volume> 949. </volume> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: This high degree of variability is further supported by a recent workload characterization study of parallel applications at the same NAS facility <ref> [FN95] </ref>. In another study, Chiang, Mansharamani, and Vernon report that the coefficient of variation observed on a weekly basis on the CM-5 at the University of Wisconsin ranges from 2.5 to 6, with 40% of them being above 4 [CMV94].
Reference: [FR90] <author> Dror G. Feitelson and Larry Rudolph. </author> <title> Distributed hierarchical control for parallel processing. </title> <journal> Computer, </journal> <volume> 23(5) </volume> <pages> 65-77, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In this category, the only discipline that has been previously studied is gang scheduling, in which sets of jobs are actively timesliced [Ous82, LV90, MVZ93]. Research into this topic has been primarily focussed on the mechanism, such as how it should be implemented <ref> [FR90] </ref> and how important it is for fine-grained applications [FR92].
Reference: [FR92] <author> D. G. Feitelson and L. Rudolph. </author> <title> Gang scheduling performance benefits for fine-grain synchronization. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 306-318, </pages> <year> 1992. </year>
Reference-contexts: Research into this topic has been primarily focussed on the mechanism, such as how it should be implemented [FR90] and how important it is for fine-grained applications <ref> [FR92] </ref>.
Reference: [GST91] <author> Dipak Ghosal, Guiseppe Serazzi, and Satish K. Tripathi. </author> <title> The processor working set and its use in scheduling multiprocessor systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(5) </volume> <pages> 443-453, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: An interesting generalization of efficiency is to consider the ratio of a job's speedup to an arbitrary cost function instead of to the number of processors. This ratio, called the efficacy of a job, is used to determine a job's processor working set (pws) <ref> [GST91] </ref> (described in more detail below). With the specific cost function chosen for use, the pws is equivalent to the number of processors at the knee of the execution time-efficiency profile. A variety of disciplines using knowledge of a job's pws have been examined. <p> A variety of disciplines using knowledge of a job's pws have been examined. It has been concluded that an important consideration is to not leave any processors idle while work is available. The best of these disciplines, referred to as FF+FIFO by Ghosal et al. <ref> [GST91] </ref>, but more commonly referred to as PWS, is investigated in this paper. A policy that does not make use of any job characteristics other than the maximum parallelism is Adaptive Static Partitioning (ASP) [ST93]. <p> This cost function expresses the notion that the cost of a processor depends on how efficiently it is being utilized. Ghosal et al. explore several different discipline that make use of the pws, and conclude that the following discipline (called FF+FIFO by them) performs best <ref> [GST91] </ref>: PWS When a job arrives in the system, and there are free processors, it is allocated the lesser of the number of free processors and its pws.
Reference: [GTS91] <author> Anoop Gupta, Andrew Tucker, and Luis Stevens. </author> <title> Making effective use of shared-memory multiprocessors: The process control approach. </title> <type> Technical Report CSL-TR-91-475A, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Dynamic Scheduling Most research in dynamic scheduling has been directed towards reducing or eliminating the cost of processor reallocations. In the process control approach, jobs and the operating system cooperate in processor reallocations, allowing a job to dynamically match its degree of parallelism to the current allocation <ref> [TG89, GTS91] </ref>. In this approach, the goal is to avoid altogether the problems of losing cache context and the blocking of critical threads. Two important disciplines that have been studied in the context of process control are equipartition and demand-driven scheduling (called dynamic scheduling originally).
Reference: [GTU91] <author> Anoop Gupta, Andrew Tucker, and Shigeru Urushibara. </author> <title> The impact of operating system scheduling policies and synchronization methods on the performance of parallel applications. </title> <booktitle> In Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 120-132, </pages> <year> 1991. </year>
Reference-contexts: The use of a combination of these techniques and others can be very effective in reducing the penalty associated with having fewer processors than threads in centralized shared-memory systems <ref> [GTU91] </ref>. Static Quantum-Based In this paper, we propose coordinated scheduling disciplines that are static quantum-based. In this category, the only discipline that has been previously studied is gang scheduling, in which sets of jobs are actively timesliced [Ous82, LV90, MVZ93]. <p> Gupta et al. studied the effect of combined thread scheduling features, including those just described, and showed that for a set of four applications, the processor utilization dropped by just under 9% over batch scheduling (see Fig. 6 in Gupta et al. <ref> [GTU91] </ref>). Adopting this result, we assume that a job that is running with fewer processors than its static configuration progresses 9% slower than q=p times its full execution rate. Experimentation indicates, however, that these two disciplines, particularly FB-ASP, tolerate higher slowdown values reasonably well.
Reference: [LV90] <author> Scott T. Leutenegger and Mary K. Vernon. </author> <title> The performance of multiprogrammed multiprocessor scheduling policies. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS Conference on Measurement and Modelling of Computer Systems, </booktitle> <pages> pages 226-236, </pages> <year> 1990. </year>
Reference-contexts: But because of the multiprocessor aspect, some anomalies can occur. In particular, RR will tend to give proportionately more processing time to jobs having a larger number of threads. RRJob avoids this by timeslicing equally among jobs as well as among the threads of a job <ref> [LV90] </ref>. Both RR and RRJob have been shown to perform well for values of C d ranging from 3 to 5 [LV90]. <p> RRJob avoids this by timeslicing equally among jobs as well as among the threads of a job <ref> [LV90] </ref>. Both RR and RRJob have been shown to perform well for values of C d ranging from 3 to 5 [LV90]. <p> Equipartition has been shown to be effective over a wide range of workloads and a wide range of distributions in service demand <ref> [LV90, CMV94] </ref>. When system loads increase, allocations to jobs decreases allowing them to operate at a more favourable point on their efficiency curve. Also, because equipartition is effectively the analog of RR in uniprocessing, it is relatively insensitive to variability in service requirement. <p> Static Quantum-Based In this paper, we propose coordinated scheduling disciplines that are static quantum-based. In this category, the only discipline that has been previously studied is gang scheduling, in which sets of jobs are actively timesliced <ref> [Ous82, LV90, MVZ93] </ref>. Research into this topic has been primarily focussed on the mechanism, such as how it should be implemented [FR90] and how important it is for fine-grained applications [FR92]. <p> These two disciplines differ primarily in the amount of information given to the scheduler; in PWS, characteristics of the speedup curve is known while in ASP only the maximum parallelism is known. We also experimented with other disciplines (in particular AVG <ref> [LV90] </ref> and AP [RSD + 94]), but as these did not offer much in terms of additional insight, we omit the results. 3.1 Standard Disciplines The pws of a job is the minimum number of processors that maximizes the ratio of its speedup S j (n) to a cost function C <p> In particular, if there are more jobs than processors, all jobs receive some fraction of the system's processing capacity. Once again, a job is never allocated more processors than its maximum parallelism. As in previous studies <ref> [LV90] </ref>, we assume that a job adapts instantaneously to the num-ber of processors allocated to it after each reallocation. 3.2 Multiprocessor Feedback Disciplines The feedback scheduling disciplines derived from the static disciplines all follow a similar pattern.
Reference: [MEB88] <author> S. Majumdar, D. L. Eager, and R. B. Bunt. </author> <title> Scheduling in multiprogrammed parallel systems. </title> <booktitle> In Proceedings of the 1988 ACM SIGMETRICS Conference on Measurement and Modelling of Computer Systems, </booktitle> <pages> pages 104-113, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Independent Scheduling The first independent multiprocessor scheduling strategies that were proposed were simple extensions of the uniprocessor FCFS, RR, SPT, and SRPT disciplines <ref> [MEB88] </ref>. In FCFS and RR, threads are ordered according to the time at which they were placed on the queue, while in SPT and SRPT, threads are ordered according to increasing cumulative service demand remaining for the job.
Reference: [MVZ93] <author> Cathy McCann, Raj Vaswani, and John Zahorjan. </author> <title> A dynamic processor allocation policy for multiprogrammed shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 146-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In order to handle jobs with varying degrees of parallelism, demand-driven scheduling uses process control both to allow the system to take into account changes in a job's parallelism and to allow the job to adapt to changes in processor allocation <ref> [ZM90, MVZ93] </ref>. Equipartition has been shown to be effective over a wide range of workloads and a wide range of distributions in service demand [LV90, CMV94]. When system loads increase, allocations to jobs decreases allowing them to operate at a more favourable point on their efficiency curve. <p> Static Quantum-Based In this paper, we propose coordinated scheduling disciplines that are static quantum-based. In this category, the only discipline that has been previously studied is gang scheduling, in which sets of jobs are actively timesliced <ref> [Ous82, LV90, MVZ93] </ref>. Research into this topic has been primarily focussed on the mechanism, such as how it should be implemented [FR90] and how important it is for fine-grained applications [FR92].
Reference: [NAS80] <institution> Numerical aerodynamic simulator processing system. </institution> <type> Technical Report PC320-02, </type> <institution> NASA Ames Research Center, </institution> <month> September </month> <year> 1980. </year>
Reference-contexts: However, high performance computing centers have reported that variability in service demands can in fact be quite high. In a detailed study of the anticipated workload of its Numerical Aerodynamic Simulation (NAS) facility <ref> [NAS80] </ref>, NASA specified a workload consisting of eight types of computational tasks with expected mean service requirements differing by as much as a factor 3500. <p> We study a third specific workload, which is chosen to represent the NASA workload described earlier. The service time distribution is an 8-stage hyperexponential distribution with one stage corresponding to each distinct workload component <ref> [NAS80] </ref>.
Reference: [Ous82] <author> John K. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Proceedings of the 3rd International Conference on Distributed Computing (ICDCS), </booktitle> <pages> pages 22-30, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: Static Quantum-Based In this paper, we propose coordinated scheduling disciplines that are static quantum-based. In this category, the only discipline that has been previously studied is gang scheduling, in which sets of jobs are actively timesliced <ref> [Ous82, LV90, MVZ93] </ref>. Research into this topic has been primarily focussed on the mechanism, such as how it should be implemented [FR90] and how important it is for fine-grained applications [FR92].
Reference: [RSD + 94] <author> E. Rosti, E. Smirni, L. W. Dowdy, G. Serazzi, and B. M. Carlson. </author> <title> Robust partitioning policies of multiprocessor systems. Performance Evaluation, </title> <booktitle> 19 </booktitle> <pages> 141-165, </pages> <year> 1994. </year>
Reference-contexts: These two disciplines differ primarily in the amount of information given to the scheduler; in PWS, characteristics of the speedup curve is known while in ASP only the maximum parallelism is known. We also experimented with other disciplines (in particular AVG [LV90] and AP <ref> [RSD + 94] </ref>), but as these did not offer much in terms of additional insight, we omit the results. 3.1 Standard Disciplines The pws of a job is the minimum number of processors that maximizes the ratio of its speedup S j (n) to a cost function C j (n) =
Reference: [Sch70] <author> Linus E. </author> <title> Schrage. Optimal scheduling rules for information systems. </title> <journal> Operations Research, </journal> <volume> 26, </volume> <month> August </month> <year> 1970. </year>
Reference-contexts: It has been shown that for some distributions (e.g., a 3-point distribution with high C d ), the performance of FB can be quite poor <ref> [Sch70] </ref>. For the hyperexponential class of distributions, as are used in this study, FB is markedly better than RR. Fig. 1. Mean response time as a function of coefficient of variation for uniprocessor scheduling disciplines at a loading factor of 0.75.
Reference: [Sev89] <author> Kenneth C. Sevcik. </author> <title> Characterizations of parallelism in applications and their use in scheduling. </title> <booktitle> In Proceedings of the 1988 ACM SIGMETRICS International Conference on Measurement and Modelling of Computer Systems, </booktitle> <pages> pages 171-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: If there are no overheads due to parallelism, A j is equivalent to the speedup on an unlimited number of processors [EZL89]. The overall workload volume, when known, should also be taken into account in the scheduling decision <ref> [Sev89] </ref>. By reducing the number of processors allocated to jobs as the system load increases, the mean response time can be greatly improved.
Reference: [Sev94] <author> K. C. Sevcik. </author> <title> Application scheduling and processor allocation in multiprogrammed parallel processing systems. Performance Evaluation, </title> <booktitle> 19 </booktitle> <pages> 107-140, </pages> <year> 1994. </year>
Reference-contexts: In this study, we used a speedup characterization that explicitly accounts for contention and overhead as a function of the number of processors. Thus, jobs which are allocated too many processors can experience a reduction in overall performance <ref> [Sev94] </ref>. Such a characterization is more realistic than those that simply consider the parallel and sequential components of a job. The structure of the paper is as follows. In the next section, we survey scheduling disciplines that have previously been proposed and evaluated. <p> Jobs are assumed to arrive according to a Poisson process, and have a service time distribution that is Erlang, exponential, or hyperexponential, depending on the specified coefficient of variation. 4.2 Workload Model Our choice of job characterization explicitly allows for overheads in a parallel computation <ref> [Sev94] </ref>.
Reference: [ST93] <author> Sanjeev Setia and Satish Tripathi. </author> <title> A comparative analysis of static processor partitioning policies for parallel computers. </title> <booktitle> In Proceedings of the International Workshop on Modeling and Simulation of Computer and Telecommunication Systems (MASCOTS), </booktitle> <pages> pages 283-286, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The best of these disciplines, referred to as FF+FIFO by Ghosal et al. [GST91], but more commonly referred to as PWS, is investigated in this paper. A policy that does not make use of any job characteristics other than the maximum parallelism is Adaptive Static Partitioning (ASP) <ref> [ST93] </ref>. Through analytic models, it has been shown that ASP is in general superior to PWS, but the results are only known for a 2-point service time distribution where C d = 0:31. <p> Although most of the results relate to C d = 5, one experiment demonstrates that bounded ASP also performs well when C d = 30. In this paper, we only use ASP as originally defined <ref> [ST93] </ref>, with no bound on the maximum allocation to a job. Dynamic Scheduling Most research in dynamic scheduling has been directed towards reducing or eliminating the cost of processor reallocations.
Reference: [TG89] <author> Andrew Tucker and Anoop Gupta. </author> <title> Process control and scheduling issues for multi-programmed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <year> 1989. </year>
Reference-contexts: Dynamic Scheduling Most research in dynamic scheduling has been directed towards reducing or eliminating the cost of processor reallocations. In the process control approach, jobs and the operating system cooperate in processor reallocations, allowing a job to dynamically match its degree of parallelism to the current allocation <ref> [TG89, GTS91] </ref>. In this approach, the goal is to avoid altogether the problems of losing cache context and the blocking of critical threads. Two important disciplines that have been studied in the context of process control are equipartition and demand-driven scheduling (called dynamic scheduling originally).
Reference: [Ver94] <author> Mary K. Vernon. </author> <title> Private communication, </title> <month> September </month> <year> 1994. </year>
Reference-contexts: They also report that some measurements from Cray YMP sites range from 30 to 70 <ref> [CMV94, Ver94] </ref>. In this paper, we consider two well-known static scheduling disciplines, showing how they behave under high variability in service demand, and propose ways in which they can be adapted to better handle this condition.
Reference: [Wu93] <author> Chee-Shong Wu. </author> <title> Processor scheduling in multiprogrammed shared memory NUMA multiprocessors. </title> <type> Master's thesis, </type> <institution> University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: Finally, fi represents the communication and congestion delays that increase with the number of processors. It has been shown by Wu <ref> [Wu93] </ref> that actual measured speedup functions can be represented with this functional form if , ff, and fi are chosen to yield the best fit (with respect to least-square error). Our choice of values for , ff, and fi are based on this work.
Reference: [ZM90] <author> John Zahorjan and Cathy McCann. </author> <title> Processor scheduling in shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS Conference on Measurement and Modelling of Computer Systems, </booktitle> <pages> pages 214-225, </pages> <year> 1990. </year>
Reference-contexts: For comparison purposes, we also consider the performance of an ideal form of equipartitioning (IEQ) in which each job receives an equal share of the processors <ref> [ZM90] </ref>. This discipline is very different from the static disciplines in that a job's processor allocation changes at each job arrival or completion. <p> Two important disciplines that have been studied in the context of process control are equipartition and demand-driven scheduling (called dynamic scheduling originally). In equipartition, each job is allocated an equal fraction of the processors, up to their maximum parallelism <ref> [ZM90] </ref>. This assumes that all jobs have a degree of parallelism that is constant throughout their lifetime. <p> In order to handle jobs with varying degrees of parallelism, demand-driven scheduling uses process control both to allow the system to take into account changes in a job's parallelism and to allow the job to adapt to changes in processor allocation <ref> [ZM90, MVZ93] </ref>. Equipartition has been shown to be effective over a wide range of workloads and a wide range of distributions in service demand [LV90, CMV94]. When system loads increase, allocations to jobs decreases allowing them to operate at a more favourable point on their efficiency curve.
References-found: 23

