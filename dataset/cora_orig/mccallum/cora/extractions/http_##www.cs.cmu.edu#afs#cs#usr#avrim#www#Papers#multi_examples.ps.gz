URL: http://www.cs.cmu.edu/afs/cs/usr/avrim/www/Papers/multi_examples.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs/usr/avrim/www/Papers/pubs.html
Root-URL: 
Email: avrim+@cs.cmu.edu  akalai+@cs.cmu.edu  
Title: A Note on Learning from Multiple-Instance Examples  
Author: AVRIM BLUM ADAM KALAI 
Keyword: Multiple-instance examples, classification noise, statistical queries  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science, Carnegie Mellon University,  
Note: 1-8 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: We describe a simple reduction from the problem of PAC-learning from multiple-instance examples to that of PAC-learning with one-sided random classification noise. Thus, all concept classes learnable with one-sided noise, which includes all concepts learnable in the usual 2-sided random noise model plus others such as the parity function, are learnable from multiple-instance examples. We also describe a more efficient (and somewhat technically more involved) reduction to the Statistical-Query model that results in a polynomial-time algorithm for learning axis-parallel rectangles with sample complexity ~ O(d 2 r=* 2 ), saving roughly a factor of r over the results of Auer et al. (1997). 
Abstract-found: 1
Intro-found: 1
Reference: <author> Auer, P. </author> <year> (1997). </year> <title> On learning from multi-instance examples: Empirical evaluation of a theoretical approach. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning. </booktitle>
Reference-contexts: We also describe a more efficient reduction to the Statistical-Query model (Kearns, 1993). For the case of axis-parallel rectangles, this results in an algorithm with sample complexity ~ O (d 2 r=* 2 ), saving roughly a factor of r over the results in <ref> (Auer et al., 1997) </ref>. 2. <p> BLUM AND A. KALAI and T depend on *=r). Then C is learnable from multiple-instance examples with probability at least 1 ffi, using O ( ln (n=ffi) r*t 2 ) r-instances, and in time O ( ln (n=ffi) * + The following theorem (given in <ref> (Auer et al., 1997) </ref> for the specific case of axis-parallel rectangles) gives a somewhat better bound on the error we need on single instance examples. <p> The basic approach to learning axis-parallel rectangles with statistical queries is outlined in (Kearns, 1993) and is similar to <ref> (Auer et al., 1997) </ref>. Suppose we have some target rectangle defined by two points, (a 1 ; : : : ; a d ) and (b 1 ; : : : ; b d ), with a i &lt; b i . <p> the rectangle, which by Theorem 2, is: O dm log m + t 2 q neg ln ((d log m)=ffi 0 ) * 2 log dr log ffi log d log * = ~ O d 3 r 2 This is almost exactly the same time bound as given in <ref> (Auer et al., 1997) </ref> except that they have an log ( d ffi ) instead of log ( d ffi ln ( r * )) for the last term.
Reference: <author> Auer, P., Long, P., and Srinivasan, A. </author> <year> (1997). </year> <title> Approximating hyper-rectangles: Learning and pseudo-random sets. </title> <booktitle> In Proceedings of the 29th Annual ACM Symposium on Theory of Computing. To appear. </booktitle>
Reference-contexts: We also describe a more efficient reduction to the Statistical-Query model (Kearns, 1993). For the case of axis-parallel rectangles, this results in an algorithm with sample complexity ~ O (d 2 r=* 2 ), saving roughly a factor of r over the results in <ref> (Auer et al., 1997) </ref>. 2. <p> BLUM AND A. KALAI and T depend on *=r). Then C is learnable from multiple-instance examples with probability at least 1 ffi, using O ( ln (n=ffi) r*t 2 ) r-instances, and in time O ( ln (n=ffi) * + The following theorem (given in <ref> (Auer et al., 1997) </ref> for the specific case of axis-parallel rectangles) gives a somewhat better bound on the error we need on single instance examples. <p> The basic approach to learning axis-parallel rectangles with statistical queries is outlined in (Kearns, 1993) and is similar to <ref> (Auer et al., 1997) </ref>. Suppose we have some target rectangle defined by two points, (a 1 ; : : : ; a d ) and (b 1 ; : : : ; b d ), with a i &lt; b i . <p> the rectangle, which by Theorem 2, is: O dm log m + t 2 q neg ln ((d log m)=ffi 0 ) * 2 log dr log ffi log d log * = ~ O d 3 r 2 This is almost exactly the same time bound as given in <ref> (Auer et al., 1997) </ref> except that they have an log ( d ffi ) instead of log ( d ffi ln ( r * )) for the last term.
Reference: <author> Dietterich, T. G., Lanthrop, R. H., and Lozano-Perez, T. </author> <year> (1997). </year> <title> Solving the multiple-instance problem with axis-parallel rectangles. </title> <booktitle> Artifical Intelligence, </booktitle> <address> 89(1-2):31-71. </address>
Reference-contexts: The goal of the algorithm is to approximate the target concept with respect to this distribution. In the multiple-instance example setting, introduced in <ref> (Dietterich et al., 1997) </ref>, the learning algorithm is given only the following weaker access to the target concept: instead of seeing individually labeled points from the instance space, each "example" is an r-tuple of points together with a single label that is positive if at least one of the points in
Reference: <author> Kearns, M. </author> <year> (1993). </year> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 392-401. </pages>
Reference-contexts: Thus, all concept classes learnable from one-sided noise are PAC-learnable from multiple-instance examples. This includes all classes learnable in the usual 2-sided random noise model, such as axis-parallel rectangles, plus others such as parity functions. We also describe a more efficient reduction to the Statistical-Query model <ref> (Kearns, 1993) </ref>. For the case of axis-parallel rectangles, this results in an algorithm with sample complexity ~ O (d 2 r=* 2 ), saving roughly a factor of r over the results in (Auer et al., 1997). 2. <p> A NOTE ON LEARNING FROM MULTIPLE-INSTANCE EXAMPLES 5 Armed with S += , S , and ^p neg , we are ready to handle a query. Our method will be similar in style to the usual simulation of Statistical Queries in the 2-sided noise model <ref> (Kearns, 1993) </ref>, but different in the details because we have 1-sided noise (and, in fact, simpler because we have an estimate ^p neg of the noise rate). Observe that, for an arbitrary subset S X, we can directly estimate P r x2D [x 2 S] from S += . <p> The basic approach to learning axis-parallel rectangles with statistical queries is outlined in <ref> (Kearns, 1993) </ref> and is similar to (Auer et al., 1997). Suppose we have some target rectangle defined by two points, (a 1 ; : : : ; a d ) and (b 1 ; : : : ; b d ), with a i &lt; b i .
Reference: <author> Long, P. and Tan, L. </author> <year> (1996). </year> <title> PAC learning axis-aligned rectangles with respect to product distributions from multiple-instance examples. </title> <booktitle> In Proceedings of the 9th Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 228-234. </pages>
References-found: 5

