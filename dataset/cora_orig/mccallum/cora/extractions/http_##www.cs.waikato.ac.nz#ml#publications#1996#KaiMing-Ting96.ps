URL: http://www.cs.waikato.ac.nz/ml/publications/1996/KaiMing-Ting96.ps
Refering-URL: http://www.cs.waikato.ac.nz/cs/Pub/Staff/kaiming.html
Root-URL: 
Email: kaiming@cs.waikato.ac.nz  
Title: The Characterisation of Predictive Accuracy and Decision Combination  
Author: Kai Ming Ting 
Address: Private Bag 3105, Hamilton, New Zealand.  
Affiliation: Department of Computer Science, The University of Waikato,  
Abstract: In this paper, we first explore an intrinsic problem that exists in the theories induced by learning algorithms. Regardless of the selected algorithm, search methodology and hypothesis representation by which the theory is induced, one would expect the theory to make better predictions in some regions of the description space than others. We term the fact that an induced theory will have some regions of relatively poor performance the problem of locally low predictive accuracy. Having characterised the problem of locally low predictive accuracy in Instance-Based and Naive Bayesian classifiers, we propose to counter this problem using a composite learner that incorporates both classifiers. The strategy is to select an estimated better performing classifier to do the final prediction during classification. Empirical results show that the strategy is capable of partially overcoming the problem and at the same time improving the overall performance of its constituent classifiers. We provide explanations of why the proposed composite learner performs better than the cross-validation method and the better of its constituent classifiers. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D.W. </author> <year> (1992), </year> <title> Generalizing from case studies: A Case Study, </title> <booktitle> in Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. 1-10. </pages> <publisher> Morgan-Kaufmann. </publisher>
Reference: <author> Aha, D.W., D. </author> <title> Kibler & M.K. Albert (1991), Instance-Based Learning Algorithms, </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> pp. 37-66. </pages>
Reference-contexts: They observe that rules that cover a small number of instances often entail high error rates; thus they name the phenomenon the problem of small disjuncts. We would anticipate the existence of a similar problem in Instance-Based Learning (IBL) algorithms <ref> (Aha, Kibler & Albert, 1991) </ref> and Naive Bayesian classifiers or Naive Bayes (Cestnik, 1990). Because both of these algorithms employ different representations from decision trees and rules, measures other than disjuncts are needed to characterise this intrinsic problem in these representations. <p> The two algorithms IB1-MVDM* and NB* (Ting, 1994c; 1996) are used to test the above characterisa-tions. IB1-MVDM* is a variant of IB1 <ref> (Aha, Kibler & Albert, 1991) </ref> that incorporates the modified value-difference metric (Cost & Salzberg, 1993) and NB* is an implementation of the Naive Bayes (Cestnik, 1990) algorithm. Both algorithms include a method (Fayyad & Irani, 1993) for discretising continuous-valued attributes in the preprocessing (indicated by "*").
Reference: <author> Breiman, L., J.H. Friedman, R.A. Olshen & C.J. </author> <title> Stone (1984), Classification And Regression Trees, </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference: <author> Brodley, C.E. </author> <year> (1993), </year> <title> Addressing the Selective Superiority Problem: Automatic Algorithm/Model Class Selection, </title> <booktitle> in Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. 17-24. </pages>
Reference-contexts: The accuracies for each induced theory are computed from the corresponding rows (by averaging) in the second matrix. The induced theory having the highest accu-racy is selected for final prediction. MCS <ref> (Brodley, 1993) </ref> uses a hand-crafted rule to guide the construction of three different models at the nodes of a decision tree, and each model is trained on part of the training set.
Reference: <author> Buntine, W. </author> <year> (1991), </year> <title> Classifiers: A Theoretical and Empirical Study, </title> <booktitle> in Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 638-644, </pages> <publisher> Morgan-Kaufmann. </publisher>
Reference: <author> Cestnik, B. </author> <year> (1990), </year> <title> Estimating Probabilities: A Crucial Task in Machine Learning, </title> <booktitle> in Proceedings of the European Conference on Artificial Intelligence, </booktitle> <pages> pp. 147-149. </pages>
Reference-contexts: We would anticipate the existence of a similar problem in Instance-Based Learning (IBL) algorithms (Aha, Kibler & Albert, 1991) and Naive Bayesian classifiers or Naive Bayes <ref> (Cestnik, 1990) </ref>. Because both of these algorithms employ different representations from decision trees and rules, measures other than disjuncts are needed to characterise this intrinsic problem in these representations. This consideration has prompted us to rename the problem so that it can be brought to a more general perspective. <p> The two algorithms IB1-MVDM* and NB* (Ting, 1994c; 1996) are used to test the above characterisa-tions. IB1-MVDM* is a variant of IB1 (Aha, Kibler & Albert, 1991) that incorporates the modified value-difference metric (Cost & Salzberg, 1993) and NB* is an implementation of the Naive Bayes <ref> (Cestnik, 1990) </ref> algorithm. Both algorithms include a method (Fayyad & Irani, 1993) for discretising continuous-valued attributes in the preprocessing (indicated by "*"). This preprocessing improved the performance of the two algorithms in most of the continuous-valued attribute domains studied by Ting (1994c).
Reference: <author> Chan, P.K. & S.J. </author> <title> Stolfo (1995), A Comparative Evaluation of Voting and Meta-learning on Partitioned Data, </title> <booktitle> in Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. 90-98, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cost, S & S. </author> <title> Salzberg (1993), A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features, </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> pp. 57-78. </pages>
Reference-contexts: The two algorithms IB1-MVDM* and NB* (Ting, 1994c; 1996) are used to test the above characterisa-tions. IB1-MVDM* is a variant of IB1 (Aha, Kibler & Albert, 1991) that incorporates the modified value-difference metric <ref> (Cost & Salzberg, 1993) </ref> and NB* is an implementation of the Naive Bayes (Cestnik, 1990) algorithm. Both algorithms include a method (Fayyad & Irani, 1993) for discretising continuous-valued attributes in the preprocessing (indicated by "*").
Reference: <author> Dawid, A.P. </author> <year> (1982), </year> <title> The Well-Calibrated Bayesian, </title> <journal> Journal of the American Statistical Association, </journal> <volume> Vol. 77, No. 379, </volume> <pages> pp. 605-610. </pages>
Reference: <author> Drucker, H., R. Schapire & P. </author> <month> Simad </month> <year> (1993), </year> <title> Improving the performance in neural networks using a boosting algorithm, </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 5, </volume> <pages> pp. 42-49. </pages>
Reference-contexts: Some other methods split the data into (i) mutually exclusive subsets (Tcheng et al, 1989; Utgoff, 1989; Chan & Stolfo, 1995) or (ii) resampling (with replacement) subsets <ref> (Drucker, Schapire & Simad, 1993) </ref>. Different classifiers are trained using these subsets. Several methods of re-ordering ranks when combining multiple models have been proposed in the literature. Buntine (1991) introduced an algorithm design strategy based on the approximating Bayesian decision theory of learning class probability decision trees.
Reference: <author> Dunn, G. </author> <year> (1989), </year> <title> Design and Analysis of Reliability Studies, </title> <publisher> Oxford University Press. </publisher>
Reference: <author> Efron, B. & R.J. </author> <month> Tibshirani </month> <year> (1993), </year> <title> Chapter 18 in An Introduction to the Bootstrap, </title> <publisher> Chapman & Hall. </publisher>
Reference: <author> Fayyad, </author> <title> U.M. & Irani, K.B. (1993), Multi-Interval Dis-cretization of Continuous-Valued Attributes for Classification Learning, </title> <booktitle> in Proceedings of 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1022-1027. </pages>
Reference-contexts: IB1-MVDM* is a variant of IB1 (Aha, Kibler & Albert, 1991) that incorporates the modified value-difference metric (Cost & Salzberg, 1993) and NB* is an implementation of the Naive Bayes (Cestnik, 1990) algorithm. Both algorithms include a method <ref> (Fayyad & Irani, 1993) </ref> for discretising continuous-valued attributes in the preprocessing (indicated by "*"). This preprocessing improved the performance of the two algorithms in most of the continuous-valued attribute domains studied by Ting (1994c).
Reference: <author> Ho, </author> <title> T.K., J.J. Hull & S.N. Srihari (1994), Decision Combination in Multiple Classifier Systems, </title> <journal> in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.16, </journal> <volume> no.1, </volume> <pages> pp. 66-75. </pages>
Reference: <author> Holte, R.C., L.E. Acker & B.W. </author> <title> Porter (1989), Concept Learning and the Problem of Small Disjuncts, </title> <booktitle> in Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 813-818. </pages>
Reference: <institution> Howell, </institution> <address> D.C. </address> <year> (1982), </year> <title> Statistical Methods for Psychology. </title> <publisher> PWS Publishers. </publisher>
Reference: <author> Kononenko, I. & M. </author> <title> Kovacic (1992), Learning as Optimization: Stochastic Generation of Multiple Knowledge, </title> <booktitle> in Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. 257-262, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Merz, C.J. </author> <year> (1995), </year> <title> Dynamic Learning Bias Selection, </title> <booktitle> in Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL: </address> <publisher> Unpublished, </publisher> <pages> pp. 386-395. </pages>
Reference: <author> Murphy, </author> <note> P.M. & D.W. Aha (1994), UCI Repository of machine learning databases [http://www.ics.uci.edu/ mlearn/MLRepository.html]. </note> <institution> Irvine, CA: University of California, Department of Information and Computer Science. </institution>
Reference-contexts: This is to ensure all graphs produced have the same bin size of fifteen at the end points. Note that we use the binned graphs for classification rather than function approximation problems. We conduct the experiments in eleven well-known domains obtained from the UCI repository of machine learning databases <ref> (Murphy & Aha, 1994) </ref>. They are the breast cancer Wisconsin (bcw), pima diabetes (diab), waveform-40 (wave), Cleveland heart disease (heart), hypothyroid (hypo), hepatitis (hepa), echocardiogram (echo), horse colic, soybean (soyb), glass and automobile (the last two domains have six classes and the soybean domain has nineteen classes).
Reference: <author> Perrone, </author> <title> M.P. & L.N. Cooper (1993), When Networks Disagree: Ensemble Methods for Hybrid Neural Networks, in Artificial Neural Networks for Speech and Vision, R.J. Mammone (editor), </title> <publisher> Chapman-Hall. </publisher>
Reference: <author> Rosch, E. & C.B. </author> <month> Mervis </month> <year> (1975), </year> <title> Family Resemblances: Studies in the Internal Structures of Categories, </title> <journal> Cognitive Psychology, </journal> <volume> vol. 7, </volume> <pages> pp. 573-605, </pages> <publisher> Academic Press. </publisher>
Reference-contexts: This characterisation agrees with the Bayesian approach in decision making. We explore an intuitive method for characterising the accuracy of each prediction in a IBL algorithm, namely typicality. This characterisation has its root in cognitive psychology <ref> (Rosch & Mervis, 1975) </ref>.
Reference: <author> Salzberg, S. </author> <year> (1991), </year> <title> A Nearest Hyperrectangle Learning Method, </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> pp. 251-276. </pages>
Reference-contexts: In domains where this assumption does not hold, the performance of this composite learner will be worse than Instance-Based methods. KBNGE (Wettschereck, 1994) in many respects resemble Ting's work without using an explicit char-acterisation of the problem of small disjuncts. It uses BNGE (i.e., a nested generalised exemplars algorithm <ref> (Salzberg, 1991) </ref> in regions that clearly belong to one class and a k-nearest neighbour algorithm otherwise. In an independent work, Merz (1995) describes a method of stacked generalisation where selection of algorithms can be done during classification.
Reference: <author> Schaffer, C. </author> <year> (1993), </year> <title> Selecting a Classification Method by Cross-validation. </title> <booktitle> Preliminary Papers of the Fourth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pp. 15-25. </pages>
Reference: <author> Silverman, B.W. </author> <year> (1986), </year> <title> Density Estimation for Statistics and Data Analysis, </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: For Instance-Based classifiers, there are other choices with regard to the method of characterisation. For example, one may intend to use some method of kernel density estimation <ref> (Silverman, 1986) </ref>, which may solve the problem faced by using typicality mentioned above. We have attempted another method of characterisa-tion that uses the nearest neighbour distance. However, this method does not perform well. <p> First, the methods for characterising predictive accuracy can be improved. Some of the problems with typicality have been discussed in Section 2.2. Use of an exponential distance function rather than Euclidean distance function in Equation (1), or even a kernel density estimation <ref> (Silverman, 1986) </ref> may improve the characterisation. Second, the predictive accuracy estimation method of producing binned graphs can be improved. Current weaknesses can be mitigated by employing cross-validation to select the best algorithm among the composite learner and its constituent algorithms.
Reference: <author> Smyth, P., R.M. Goodman & C. </author> <title> Higgins (1990), A Hybrid Rule-based/ Bayesian Classifier, </title> <booktitle> in Proceedings of the Ninth European Conference on Artificial Intelligence, </booktitle> <pages> pp. 610-615. </pages>
Reference: <author> Tcheng, D., B. Lambert, C-Y. Lu & L. </author> <title> Rendell (1989), Building Robust Learning Systems by Combining Induction and Optimization, </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 806-812, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ting, K.M. </author> <year> (1994a), </year> <title> The Problem of Small Disjuncts: its remedy in Decision Trees, </title> <booktitle> in Proceedings of the Tenth Canadian Conference on Artificial Intelligence, </booktitle> <pages> pp. 91-97. </pages>
Reference: <author> Ting, K.M. </author> <year> (1994b), </year> <title> The Problem of Atypicality in Instance-Based Learning, </title> <booktitle> in Proceedings of the Third Pacific Rim International Conference on Artificial Intelligence, </booktitle> <pages> pp. 360-366, </pages> <publisher> International Academic. </publisher>
Reference: <author> Ting, K.M. </author> <year> (1994c), </year> <title> Discretization of Continuous-Valued Attributes and Instance-Based Learning, </title> <type> Technical Report No.491, </type> <institution> Basser Department of Computer Science, University of Sydney. </institution>
Reference: <author> Ting, K.M. </author> <year> (1995), </year> <title> Common Issues in Instance-Based and Naive Bayesian Classifiers, </title> <type> PhD Thesis, </type> <institution> Basser Dept of Computer Science, University of Sydney. </institution>
Reference: <author> Ting, K.M. </author> <year> (1996), </year> <note> Discretisation in Lazy Learning Algorithms to appear in the special issue of Lazy Learning in Journal of Artificial Intelligence Review. </note>
Reference: <author> Utgoff, P.E. </author> <year> (1989), </year> <title> Perceptron Trees: A case study in hybrid concept representations, </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> pp. 337-391. </pages>
Reference: <author> Watson, D.F. </author> <year> (1981), </year> <title> Computing the n-dimensional Delaunay Tessellation with application to Voronoi Polytopes, </title> <journal> The Computer Journal, vol.24, no.2, </journal> <pages> pp. 167-172, </pages> <publisher> Heyden & Son Ltd. </publisher>
Reference-contexts: The decision boundaries of IBL and Naive Bayes are intrinsically implicit in their representations. One does not know the exact decision boundaries unless one takes extra effort to draw them out (e.g., into a Voronoi diagram <ref> (Watson, 1981) </ref> for IBL). While the methods of characterisation and estimation are not perfect for one reason or another, it suffices for our purpose here to show that the methods can be used to characterise and estimate predictive accuracy in most of the real-world domains studied.
Reference: <author> Wettschereck, D. </author> <year> (1994), </year> <title> A Hybrid Nearest-Neighbor and Nearest-Hyperrectangle Algorithm, </title> <booktitle> in Proceedings of the Seventh European Conference on Machine Learning, LNAI-784, </booktitle> <pages> pp. 323-335, </pages> <publisher> Springer Verlag. </publisher>
Reference-contexts: This arrangement is based on the assumption that decision trees perform better than Instance-Based methods in the regions of large disjuncts. In domains where this assumption does not hold, the performance of this composite learner will be worse than Instance-Based methods. KBNGE <ref> (Wettschereck, 1994) </ref> in many respects resemble Ting's work without using an explicit char-acterisation of the problem of small disjuncts. It uses BNGE (i.e., a nested generalised exemplars algorithm (Salzberg, 1991) in regions that clearly belong to one class and a k-nearest neighbour algorithm otherwise.
Reference: <author> Wolpert, D.H. </author> <year> (1992), </year> <title> Stacked Generalization, Neural Networks, </title> <booktitle> vol.5, </booktitle> <pages> pp. 241-259, </pages> <publisher> Pergamon Press. </publisher>
Reference-contexts: For example, the probabilities of a neural network's outputs could be used as the character-isation. In such cases, a scheme that combines the weights/evidences of the predictions might need to be incorporated into the system. A separate empirical evaluation shows that CL outperforms a scheme of stacked generalisation <ref> (Wolpert, 1992) </ref>, using either of the algorithms described as the high level generaliser, in most of the domains studied here.
Reference: <author> Zhang, J. </author> <year> (1992), </year> <title> Selecting Typical Instances in Instance-Based Learning, </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. 470-479, </pages> <publisher> Morgan Kaufmann. </publisher> .
References-found: 36

