URL: http://www.research.att.com/~mkearns/papers/pop.ps.Z
Refering-URL: http://www.research.att.com/~mkearns/
Root-URL: 
Email: mkearns@research.att.com, seung@physics.att.com  Editor:  
Title: Learning From a Population of Hypotheses  
Author: MICHAEL KEARNS AND H. SEBASTIAN SEUNG 
Address: 600 Mountain Avenue, Murray Hill, New Jersey 07974  
Affiliation: AT&T Bell Laboratories,  
Abstract: We introduce a new formal model in which a learning algorithm must combine a collection of potentially poor but statistically independent hypothesis functions in order to approximate an unknown target function arbitrarily well. Our motivation includes the question of how to make optimal use of multiple independent runs of a mediocre learning algorithm, as well as settings in which the many hypotheses are obtained by a distributed population of identical learning agents. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> S. Amari, N. Fujita, and S. Shinomoto. </author> <title> Four types of learning curves. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 605-618, </pages> <year> 1992. </year>
Reference-contexts: Finally, the population learning model could be viewed as an instance of what statisticians call meta-analysis, in which multiple sources of perhaps secondary data are combined to give a unified hypothesis. 6 3. An Illustrative Example: The High-Low Game In this example, the domain X is the real interval <ref> [0; 1] </ref>, and F is the class of all initial intervals. Thus, each target function is a real number f 2 [0; 1], and the positive examples are the subinterval [0; f], with the interval (f; 1] being the negative examples. Let D be the uniform distribution on [0; 1]. <p> An Illustrative Example: The High-Low Game In this example, the domain X is the real interval <ref> [0; 1] </ref>, and F is the class of all initial intervals. Thus, each target function is a real number f 2 [0; 1], and the positive examples are the subinterval [0; f], with the interval (f; 1] being the negative examples. Let D be the uniform distribution on [0; 1]. <p> real interval <ref> [0; 1] </ref>, and F is the class of all initial intervals. Thus, each target function is a real number f 2 [0; 1], and the positive examples are the subinterval [0; f], with the interval (f; 1] being the negative examples. Let D be the uniform distribution on [0; 1]. These settings are also known as the "high-low game", since each example x of f simply indicates whether x is smaller or larger than f. Let us examine the population learning problem (F ; D; m = 1; A = Gibbs ). <p> Let us examine the population learning problem (F ; D; m = 1; A = Gibbs ). In this problem, for target f the oracle POP (f ) behaves as follows: a single x 2 <ref> [0; 1] </ref> is chosen uniformly at random. If x f (positive example), then a random h 2 [x; 1] is chosen uniformly and returned. If x &gt; f (negative example), then a random h 2 [0; x) is chosen uniformly and returned. <p> In this problem, for target f the oracle POP (f ) behaves as follows: a single x 2 [0; 1] is chosen uniformly at random. If x f (positive example), then a random h 2 <ref> [x; 1] </ref> is chosen uniformly and returned. If x &gt; f (negative example), then a random h 2 [0; x) is chosen uniformly and returned. <p> For the specific case of the high-low game, it turns out to be sufficient for the analysis to compute E h2q f [h] = E [h], which is the expected value of the hypotheses h 2 <ref> [0; 1] </ref> generated by the distribution q f . (Throughout the paper, we use the subscript h 2 q f on an expectation or probability to denote that h is chosen randomly according to q f , and h 2 S to denote that h is chosen uniformly from the set <p> [h] = 0 = 0 Z 1 E h2 [0;x] [h]dx Z f 1 x Z 1 x dx f + 4 Here we have broken the expectation into two easily analyzed parts: the first where the single example x is positive (in which case h is drawn randomly from <ref> [x; 1] </ref> and thus has expected value x +(1 x)=2), and the second where x is negative (in which case h has expected value x=2). <p> Theorem 1 Let F be the class of initial intervals over <ref> [0; 1] </ref>, and D the uniform distribution on [0; 1]. Then for the population learning problem (F ; D; m = 1; A = Gibbs), `(*; ffi) = O (1=* 2 log 1=ffi). <p> Theorem 1 Let F be the class of initial intervals over <ref> [0; 1] </ref>, and D the uniform distribution on [0; 1]. Then for the population learning problem (F ; D; m = 1; A = Gibbs), `(*; ffi) = O (1=* 2 log 1=ffi). <p> Then it easy to see that as fl approaches 0, q f approaches the uniform distribution on <ref> [0; 1] </ref> independent of f . <p> As will be illustrated by specific examples in Section 5, F (and hence Q) often admits no continuous parametrization. Furthermore, even in the case of a continuous parametrization, the likelihood can be nondifferentiable in its parameters, as noted by Amari <ref> [1] </ref>. Hence classical statistics is not typically applicable to the learning problems of interest here. Instead we proceed by invoking uniform convergence theorems [7], [10], [3] to bound fluctuations in empirical log-loss. <p> In the following theorem, we show that with m = 2, we can obtain such a uniform bound. We include a proof sketch that is illustrative of the type of reasoning used to prove such bounds. 15 Theorem 5 Let F be the class of initial intervals over <ref> [0; 1] </ref>, and D the uniform distribution on [0; 1]. Then for any consistent agent algorithm A, the population learning problem (F ; D; m = 2; A) satisfies `(*; ffi) = O (1=* 8 log 1=* + log 1=ffi). <p> We include a proof sketch that is illustrative of the type of reasoning used to prove such bounds. 15 Theorem 5 Let F be the class of initial intervals over <ref> [0; 1] </ref>, and D the uniform distribution on [0; 1]. Then for any consistent agent algorithm A, the population learning problem (F ; D; m = 2; A) satisfies `(*; ffi) = O (1=* 8 log 1=* + log 1=ffi). <p> Proof: We demonstrate that the separation function for the variation distance obeys V (*; 2) = (* 2 ); the stated upper bound on `(*; ffi) can then be obtained as outlined in Section 4.3 and Theorem 3. Let f 2 <ref> [0; 1] </ref> be a potential target function. Recall that in the population learning problem (F; D; m = 2; A), POP (f ) draws two points uniformly from [0; 1], labels them according to f, and applies the consistent agent algorithm A to the resulting sample to obtain the returned hypothesis <p> Let f 2 <ref> [0; 1] </ref> be a potential target function. Recall that in the population learning problem (F; D; m = 2; A), POP (f ) draws two points uniformly from [0; 1], labels them according to f, and applies the consistent agent algorithm A to the resulting sample to obtain the returned hypothesis h 2 [0; 1]. <p> Recall that in the population learning problem (F; D; m = 2; A), POP (f ) draws two points uniformly from <ref> [0; 1] </ref>, labels them according to f, and applies the consistent agent algorithm A to the resulting sample to obtain the returned hypothesis h 2 [0; 1]. Without loss of generality, we will use x L to denote the smaller of the two chosen sample points, and x R to denote the larger. <p> To prove that V (*; 2) = (* 2 ) it suffices to show that for any * and any target functions f 1 ; f 2 2 <ref> [0; 1] </ref> such that D [f 1 f 2 ] *, V (q f 1 ; q f 2 ) = (* 2 ). For S; S L ; S R [0; 1], let us use q f [Sjx L 2 S L ; x R 2 S R ] to <p> ) it suffices to show that for any * and any target functions f 1 ; f 2 2 <ref> [0; 1] </ref> such that D [f 1 f 2 ] *, V (q f 1 ; q f 2 ) = (* 2 ). For S; S L ; S R [0; 1], let us use q f [Sjx L 2 S L ; x R 2 S R ] to denote the probability that q f generates a hypothesis h falling in S given that in the two-point sample, x L fell in S L and x R fell in S <p> For f 1 ; f 2 satisfying D [f 1 f 2 ] = * (let us assume without loss of generality that f 1 f 2 = f 1 + *), we first have that for any S <ref> [0; 1] </ref>, q f 1 [Sjx L ; x R 62 f 1 f 2 ] = q f 2 [Sjx L ; x R 62 f 1 f 2 ]: This is because the behavior of POP (f ) depends only on the labeled sample, and not directly on the <p> It is easy to see that q f 1 [[0; z]jx L 2 [f 1 ; z]; x R 2 [z; f 2 ]] = 1 q f 2 <ref> [[z; 1] </ref>jx L 2 [f 1 ; z]; x R 2 [z; f 2 ]] = 1: Furthermore, the probability that x L 2 [f 1 ; z] and x R 2 [z; f 2 ] is * 2 =4. <p> q f 2 differ by fi (* 2 ): that is, q f 1 is * 2 =4 more likely than q f 2 to generate a hypothesis in [0; z] and q f 2 is * 2 =4 more likely than q f 1 to generate a hypothesis in <ref> [z; 1] </ref>. It is fairly straightforward to show that the remaining cases of x L and x R do not alter this difference, thus giving q f 1 [[0; z]] = q f 2 [[0; z]] + * 2 =4 and q f 2 [[z; 1]] = q f 1 [[z; <p> It is fairly straightforward to show that the remaining cases of x L and x R do not alter this difference, thus giving q f 1 [[0; z]] = q f 2 [[0; z]] + * 2 =4 and q f 2 <ref> [[z; 1] </ref>] = q f 1 [[z; 1]] + * 2 =4. Either of these suffice to show V (q f 1 ; q f 2 ) * 2 =4, as desired. Better upper bounds for this problem may be possible by direct analysis of the Kullback-Leibler separation function. <p> It is fairly straightforward to show that the remaining cases of x L and x R do not alter this difference, thus giving q f 1 [[0; z]] = q f 2 [[0; z]] + * 2 =4 and q f 2 <ref> [[z; 1] </ref>] = q f 1 [[z; 1]] + * 2 =4. Either of these suffice to show V (q f 1 ; q f 2 ) * 2 =4, as desired. Better upper bounds for this problem may be possible by direct analysis of the Kullback-Leibler separation function.
Reference: 2. <author> Nicolo Cesa-Bianchi, Yoav Freund, David P. Helmbold, David Haussler, Robert E. Schapire, and Manfred K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 382-391, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: There are two lines of prior research in computational learning theory and related fields that immediately come to mind in our setting. The first is the recent work on combining "expert" opinions in an optimal on-line fashion (see Cesa-Bianchi et al. <ref> [2] </ref> for recent results and an extensive bibliography). Briefly, in the research on experts, we assume that have access to the predictions of a panel of experts, and our goal is to make predictions with a mistake rate approaching that of the best expert. <p> regarding the sequence being predicted or the experts (for instance, the sequence may be arbitrarily time-dependent, so an expert's performance on any part of the sequence may be a poor predictor of its future performance), approaching the best expert's mistake rate is the most that can expected in such models <ref> [2] </ref>. 2 In contrast, in this paper we make assumptions about both the desired predictions and the "experts" (which we do not regard as being especially expert).
Reference: 3. <author> R. M. Dudley. </author> <title> Central limit theorems for empirical measures. </title> <journal> The Annals of Probability, </journal> <volume> 6(6) </volume> <pages> 899-929, </pages> <year> 1978. </year>
Reference-contexts: Furthermore, even in the case of a continuous parametrization, the likelihood can be nondifferentiable in its parameters, as noted by Amari [1]. Hence classical statistics is not typically applicable to the learning problems of interest here. Instead we proceed by invoking uniform convergence theorems [7], [10], <ref> [3] </ref> to bound fluctuations in empirical log-loss. These theorems are relevant because maximizing the likelihood is equivalent to minimizing the empirical log-loss, which is 1=` i=1 log q f 0 [h i ]. Hence maximum likelihood is but a specific case of the general class of empirical loss minimization algorithms.
Reference: 4. <author> A. Erdelyi. </author> <title> Asymptotic expansions. </title> <publisher> Dover, </publisher> <year> 1956. </year>
Reference: 5. <author> Yoav Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 202-216, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: By making these assumptions, we allow the possibility of somehow combining the independent hypotheses in a way that considerably outperforms any single hypothesis. The second loosely related line of research is the work on boosting weak learning algorithms [11], <ref> [5] </ref>, [6], in which the goal is to combine a collection of hypotheses from a mediocre learning algorithm in order to obtain an arbitrarily accurate hypothesis. <p> Here we assume no such mechanism, and each hypothesis is trained on the same fixed distribution. Indeed, it is interesting to note that natural schemes for combining hypotheses that are successful in the boosting setting, such as majority vote <ref> [5] </ref>, often fail in our setting. 1.1. Overview of Results We now give a summary of the paper. In Section 2, we introduce and motivate our model, which we call population learning .
Reference: 6. <author> Yoav Freund. </author> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 391-398, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: By making these assumptions, we allow the possibility of somehow combining the independent hypotheses in a way that considerably outperforms any single hypothesis. The second loosely related line of research is the work on boosting weak learning algorithms [11], [5], <ref> [6] </ref>, in which the goal is to combine a collection of hypotheses from a mediocre learning algorithm in order to obtain an arbitrarily accurate hypothesis.
Reference: 7. <author> David Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100(1) </volume> <pages> 78-150, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: to lower bound KL (q f 1 jjq f 2 ) let us choose F 0 to be the *=2-ball around f 2 in F with respect to D, that is F 0 = ff 2 F : D [f 2 f ] *=2g: Now using uniform convergence methods [13], <ref> [7] </ref> one can show q f 2 [F 0 ] 1 c d! and (2m) d e ff* 2 m for constants c; ff &gt; 0, where d is the Vapnik-Chervonenkis dimension of F . <p> Furthermore, even in the case of a continuous parametrization, the likelihood can be nondifferentiable in its parameters, as noted by Amari [1]. Hence classical statistics is not typically applicable to the learning problems of interest here. Instead we proceed by invoking uniform convergence theorems <ref> [7] </ref>, [10], [3] to bound fluctuations in empirical log-loss. These theorems are relevant because maximizing the likelihood is equivalent to minimizing the empirical log-loss, which is 1=` i=1 log q f 0 [h i ]. <p> Combined with Lemma 1, which relates log-loss in Q to loss in the parameter space F , the uniform convergence bounds lead to the following upper bound on population size, whose proof is omitted due to space considerations, but is a fairly straightforward application of the main theorem of Haussler <ref> [7] </ref>. Theorem 3 Let (F ; D; m; A) be any population learning problem. Then `(*; ffi) = O dim (Q) M M + log ffi and ( V (*; m)) 4 log V (*; m) 1 Here dim (Q) is the combinatorial dimension [7] of the distribution class Q, and <p> of the main theorem of Haussler <ref> [7] </ref>. Theorem 3 Let (F ; D; m; A) be any population learning problem. Then `(*; ffi) = O dim (Q) M M + log ffi and ( V (*; m)) 4 log V (*; m) 1 Here dim (Q) is the combinatorial dimension [7] of the distribution class Q, and M is a bound on the empirical log-loss of any distribution in Q. Let us take a moment to absorb this result. <p> In the finite F case, dim (Q) log jF j. We refer the interested reader to Haussler's paper <ref> [7] </ref> for details. <p> The general approach is to lower bound a separation function and then apply Theorem 3. It should be noted that since Theorem 3 is obtained by Haussler <ref> [7] </ref> in an extremely general setting, we suspect the existence of considerably better upper bounds than those we provide here; for now, however, we restrict our efforts towards proving polynomial bounds, leaving improvement of the polynomial degree for future research. 5.1.
Reference: 8. <author> David Haussler, Michael Kearns, and Robert E. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 83-113, </pages> <year> 1994. </year>
Reference-contexts: A special case of interest occurs when the agent algorithm A is the well-studied Gibbs algorithm, which is known to be a near-optimal learning algorithm in terms of its expected error as a function of the number of examples m <ref> [8] </ref>. This algorithm simply chooses h uniformly at random from the version space VS (S f ).
Reference: 9. <author> S. Kullback. </author> <title> A lower bound for discrimination information in terms of variation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13 </volume> <pages> 126-127, </pages> <year> 1967. </year>
Reference-contexts: We will use the following theorem due to Kullback <ref> [9] </ref>: Theorem 2 For any distributions q f 1 ; q f 2 KL (q f 1 jjq f 2 ) V 2 (q f 1 ; q f 2 ): 4.1. <p> For any numbers 0 r; s 1 let us define KL (rjjs) = r log (r=s) + (1 r) log ((1 r)=(1 s)); it is easy to show that this is lower bounded by maxfr log 1=s 1; (1 r) log 1=(1 s) 1g. Now it is also true <ref> [9] </ref> that for any F 0 F , KL (q f 1 jjq f 2 ) KL (q f 1 [F 0 ]jjq f 2 [F 0 ]): (1) 10 Thus to lower bound KL (q f 1 jjq f 2 ) let us choose F 0 to be the *=2-ball
Reference: 10. <author> David Pollard. </author> <title> Convergence of Stochastic Processes. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Furthermore, even in the case of a continuous parametrization, the likelihood can be nondifferentiable in its parameters, as noted by Amari [1]. Hence classical statistics is not typically applicable to the learning problems of interest here. Instead we proceed by invoking uniform convergence theorems [7], <ref> [10] </ref>, [3] to bound fluctuations in empirical log-loss. These theorems are relevant because maximizing the likelihood is equivalent to minimizing the empirical log-loss, which is 1=` i=1 log q f 0 [h i ].
Reference: 11. <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: By making these assumptions, we allow the possibility of somehow combining the independent hypotheses in a way that considerably outperforms any single hypothesis. The second loosely related line of research is the work on boosting weak learning algorithms <ref> [11] </ref>, [5], [6], in which the goal is to combine a collection of hypotheses from a mediocre learning algorithm in order to obtain an arbitrarily accurate hypothesis.
Reference: 12. <author> H.S. Seung, H. Sompolinsky, and N. Tishby. </author> <title> Statistical mechanics of learning from examples. </title> <journal> Physical Review A, </journal> <volume> 45(8) </volume> <pages> 6056-6091, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: This technical difficulty is related to the difficulty of performing the quenched average in statistical mechanical analyses of learning <ref> [12] </ref>. In the absence of general bounds, we must settle for calculation of the separation functions for some specific learning problems, to be done in Section 5.
Reference: 13. <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: Thus to lower bound KL (q f 1 jjq f 2 ) let us choose F 0 to be the *=2-ball around f 2 in F with respect to D, that is F 0 = ff 2 F : D [f 2 f ] *=2g: Now using uniform convergence methods <ref> [13] </ref>, [7] one can show q f 2 [F 0 ] 1 c d! and (2m) d e ff* 2 m for constants c; ff &gt; 0, where d is the Vapnik-Chervonenkis dimension of F .

References-found: 13

