URL: http://www.cs.umass.edu/~papka/tr98-21.ps.gz
Refering-URL: http://www.cs.umass.edu/~papka/
Root-URL: 
Email: fpapka,allang@cs.umass.edu  
Title: On-Line New Event Detection using Single Pass Clustering  
Author: Ron Papka and James Allan 
Address: Amherst, MA 01003  
Affiliation: Center for Intelligent Information Retrieval Department of Computer Science University of Massachusetts  
Abstract: This paper discusses the implementation and evaluation of a new-event detection system. We focus on a strict on-line setting, in that the system must indicate whether the current document contains or does not contain discussion of a new event before looking at the next document. Our approach to the problem uses a single pass clustering algorithm and a novel thresholding model that incorporates the properties of events as a major component. A corpus containing newswire and transcribed broadcast news was analyzed using our system, and our results compared favorably to those of other systems. We develop an evaluation methodology based on a combination of techniques that allows us to infer the expected performance of our approach in the field, and to suggest avenues for future research that may lead to better performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Allan, L. Ballesteros, J. Callan, W.B. Croft, and Z. Lu, </author> <title> "Recent Experiments with Inquery," </title> <booktitle> Proceedings of TREC-4, </booktitle> <pages> 49-63, </pages> <year> 1995. </year>
Reference-contexts: triggers an existing query, the document is assumed to discuss the event represented in the query, otherwise it contains a new event. 4 Experiments 4.1 Implementation The new-event detection algorithm was implemented by combining the ranked-retrieval mechanisms of In-query [4], a feature extraction and selection process based on relevance feedback <ref> [1] </ref>, and the routing architecture of InRoute [5].
Reference: [2] <author> D. Beeferman, A. Berger, and J. Lafferty, </author> <title> "Text Segmentation Using Exponential Models," Proceedings for Empirical Methods in NLP, </title> <year> 1997. </year>
Reference-contexts: Broadcast news from various sources is monitored by the system that operates on text documents. When the source is radio or television, a speech transcription and segmentation process first converts the data to text documents, each containing a complete news story <ref> [17, 2] </ref>. If a document discusses a new event, it is fed to a tracking process that classifies the document stream based on existing news stories in an on-line unsupervised setting.
Reference: [3] <author> C. Buckley and G. Salton, </author> <title> "Optimization of Relevance Feedback Weights," </title> <booktitle> Proceedings of SIGIR, </booktitle> <pages> 351-357, </pages> <year> 1995. </year>
Reference-contexts: At a certain level of abstraction, the task of new-event detection is a classification task where documents are either members of the set of documents containing a new event or not. Previous work related to on-line document classification has often focused on supervised-training algorithms that use labeled documents <ref> [18, 3, 13, 16] </ref>, a resource not available during new-event detection. Most experimental Information Retrieval (IR) literature discusses evaluation using topics and not events. In [10], for example, a knowledge-based approach to text pattern-matching was used to categorize news stories into general topics.
Reference: [4] <author> J. P. Callan, W.B. Croft, </author> <title> and S.M. Harding, "The INQUERY Retrieval System," Database and Expert Systems Applications: </title> <booktitle> Proceedings of the International Conference in Valencia Spain. A.M. </booktitle> <editor> Tjoa and I. Ramos eds., </editor> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: If a new document triggers an existing query, the document is assumed to discuss the event represented in the query, otherwise it contains a new event. 4 Experiments 4.1 Implementation The new-event detection algorithm was implemented by combining the ranked-retrieval mechanisms of In-query <ref> [4] </ref>, a feature extraction and selection process based on relevance feedback [1], and the routing architecture of InRoute [5].
Reference: [5] <author> J.P. Callan, </author> <title> "Document Filtering With Inference Networks," </title> <booktitle> Proceedings of SIGIR, </booktitle> <pages> 262-269, </pages> <year> 1996. </year>
Reference-contexts: assumed to discuss the event represented in the query, otherwise it contains a new event. 4 Experiments 4.1 Implementation The new-event detection algorithm was implemented by combining the ranked-retrieval mechanisms of In-query [4], a feature extraction and selection process based on relevance feedback [1], and the routing architecture of InRoute <ref> [5] </ref>.
Reference: [6] <author> C. Carrick, and C. Watters, </author> <title> "Automatic Association of News Items," </title> <booktitle> Information Processing & Management, </booktitle> <volume> 33(5) </volume> <pages> 615-632, </pages> <year> 1997. </year>
Reference-contexts: It chose the correct script with a classification accuracy of 38% for the documents for which it had a sketchy script. Recent work by Carrick and Watters discussed an application that matches news stories to photo captions using a frame-based approach modelling some of the properties of events <ref> [6] </ref>. They concluded that using the extracted lexical features in a word-cooccurrence 2 retrieval model was nearly as effective as using the same features in their frame-based approach.
Reference: [7] <author> P.R. Cohen, </author> <title> Empirical Methods for Artificial Intelligence, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1995. </year>
Reference-contexts: The parameters that give rise to the smallest overall performance error are used. Because the TDT data contains only 25 instances, we chose k = 25, effectively performing leave-one-out cross-validation. Once the threshold parameters are obtained, we infer their expected performance using a simple bootstrap process <ref> [7] </ref>. The process randomly selects 25 instances (with replacement) from the data to form a bootstrap sample. Performance is calculated on the sample.
Reference: [8] <author> G. DeJong, </author> <title> "Prediction and Substantiation: A New Approach to Natural Language Processing," </title> <journal> Cognitive Science, </journal> <volume> 3: </volume> <pages> 251-273, </pages> <year> 1979. </year>
Reference-contexts: Some event-related work has been reported prior to the TDT workshop, but new-event detection has not been a focus. A frame-based system that attempted to detect events on a UPI newswire was constructed by DeJong in the late 1970s. He used pre-specified software objects called sketchy scripts <ref> [8] </ref>. Frames associated with 50 general events were constructed by hand. The goal of his system was to predict which frame needed to be populated, and then to produce a short summary of the event.
Reference: [9] <author> D.K. </author> <title> Harman, </title> <booktitle> Proceedings of Text REtrieval Conferences (TREC), NIST Special Publication, </booktitle> <pages> 1993-7. </pages>
Reference-contexts: Most experimental Information Retrieval (IR) literature discusses evaluation using topics and not events. In [10], for example, a knowledge-based approach to text pattern-matching was used to categorize news stories into general topics. In addition, almost all TREC <ref> [9] </ref> information requests for the routing and filtering experiments are about topics even though the testing and training corpora are mostly from newsprint sources. Some event-related work has been reported prior to the TDT workshop, but new-event detection has not been a focus. <p> Therefore, one could estimate idf using retrospective statistics from the current stream or from an auxiliary corpus with a similar domain. The idf component incorporated here utilizes several volumes from the TREC collection as an auxiliary corpus <ref> [9] </ref>. A feature selection process was used to build and rebuild queries. In the experiments that follow, a query was built using the n most frequent single word features found in the document (s).
Reference: [10] <author> P.J. Hayes, L.E. Knecht, and M.J. Cellio, </author> <title> "A News Story Categorization System," </title> <booktitle> Proceedings of the 2nd Conference on Applied Natural Language Processing (1988), reprinted in Readings in Information Retrieval, </booktitle> <editor> K. Sparck Jones and P. Willet editors, </editor> <publisher> Morgan Kaufmann Publishing, </publisher> <address> San Francisco, CA, 518-526, </address> <year> 1997. </year>
Reference-contexts: Previous work related to on-line document classification has often focused on supervised-training algorithms that use labeled documents [18, 3, 13, 16], a resource not available during new-event detection. Most experimental Information Retrieval (IR) literature discusses evaluation using topics and not events. In <ref> [10] </ref>, for example, a knowledge-based approach to text pattern-matching was used to categorize news stories into general topics. In addition, almost all TREC [9] information requests for the routing and filtering experiments are about topics even though the testing and training corpora are mostly from newsprint sources.
Reference: [11] <author> R. Kohavi, </author> <title> "A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection," </title> <booktitle> Proceedings of International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: Separate training and testing phases were not performed due to the unavailability of more judged events. In order to avoid overfitting our threshold parameters p and tp, we selected parameters based on k-fold cross-validation <ref> [11] </ref>. The general algorithm is to randomly partition the data into k sets, and to leave one set out while finding parameters that best fit the remaining k 1 sets. The process repeats for k iterations. The parameters that give rise to the smallest overall performance error are used.
Reference: [12] <author> D.D. Lewis, </author> <title> and W.A. Gale, "A Sequential Algorithm for Training Text Classifiers," </title> <booktitle> Proceedings of SIGIR, </booktitle> <pages> 3-13, </pages> <year> 1994. </year>
Reference-contexts: In addition to these performance metrics we calculated the traditional recall and precision met-rics, and F1-Measure <ref> [12] </ref>.
Reference: [13] <author> D. Lewis, R. Schapire, J. Callan, and R. Papka, </author> <title> "Training Algorithms for Linear Text Classifiers," </title> <booktitle> Proceeding of SIGIR, </booktitle> <pages> 298-306, </pages> <year> 1996. </year>
Reference-contexts: At a certain level of abstraction, the task of new-event detection is a classification task where documents are either members of the set of documents containing a new event or not. Previous work related to on-line document classification has often focused on supervised-training algorithms that use labeled documents <ref> [18, 3, 13, 16] </ref>, a resource not available during new-event detection. Most experimental Information Retrieval (IR) literature discusses evaluation using topics and not events. In [10], for example, a knowledge-based approach to text pattern-matching was used to categorize news stories into general topics.
Reference: [14] <author> A. Martin, G. Doddington, T. Kamm, M. Or-dowski, and M. Przybocki, </author> <title> "The DET Curve in Assessment of Detection Task Performance," </title> <booktitle> in Proceedings of EuroSpeech'97, </booktitle> <volume> 4 </volume> <pages> 1895-1898, </pages> <year> 1997. </year>
Reference-contexts: The UMASS and CMU systems are using representations of dimensionality 100, and the Dragon system is using full dimensionality. The figure is a Detection Error Tradeoff (DET) graph that illustrates the estimated perfor mance error tradeoff between miss rate and false alarm rate <ref> [14] </ref>. The graph is scaled based on a normal distribution of the performance metrics. Each curve can be viewed as an analogue to a recall-precision curve which is used to depict the retrieval performance tradeoff between recall and precision.
Reference: [15] <author> P. E. Mayeux, </author> <title> Broadcast News: Writing & Reporting, </title> <publisher> 2ed, Brown & Benchmark Publishers, </publisher> <address> Guilford CT, </address> <year> 1996, </year> <note> p. 79. </note>
Reference-contexts: From a journalist's perspective, a news story about an event will typically specify 1) when the event occurred; 2) who was involved; 3) where it took place; 4) how it happened; and 5) the impact, significance, or consequence of the event on the intended audience <ref> [15] </ref>. However, as an event evolves, many of these properties are either not initially known, or are assumed to be known by the audience and are therefore not referenced within the text of documents relating to the same event.
Reference: [16] <author> R. Papka, J.P. Callan, and A.G. Barto, </author> <title> "Text-Based Information Retrieval Using Exponentiated Gradient Descent," </title> <booktitle> Proceedings of the 10th Annual Conference of Advances in Neural Information Processing Systems, </booktitle> <pages> 3-9, </pages> <year> 1996. </year>
Reference-contexts: At a certain level of abstraction, the task of new-event detection is a classification task where documents are either members of the set of documents containing a new event or not. Previous work related to on-line document classification has often focused on supervised-training algorithms that use labeled documents <ref> [18, 3, 13, 16] </ref>, a resource not available during new-event detection. Most experimental Information Retrieval (IR) literature discusses evaluation using topics and not events. In [10], for example, a knowledge-based approach to text pattern-matching was used to categorize news stories into general topics.
Reference: [17] <author> J.M. Ponte and W. B. Croft. </author> <title> "Text Segmentation by Topic," </title> <booktitle> Proceedings of the First Eu-ropean Conference on Research and Advanced Technology for Digital Libraries, </booktitle> <pages> 113-125, </pages> <year> 1997. </year>
Reference-contexts: Broadcast news from various sources is monitored by the system that operates on text documents. When the source is radio or television, a speech transcription and segmentation process first converts the data to text documents, each containing a complete news story <ref> [17, 2] </ref>. If a document discusses a new event, it is fed to a tracking process that classifies the document stream based on existing news stories in an on-line unsupervised setting.
Reference: [18] <author> J.J. Rocchio, </author> <title> "Relevance Feedback in Information Retrieval," in The Smart System - Experiments in Automatic Document Processing, </title> <address> 313-323, Englewood Cliffs, NJ: </address> <publisher> Prentice Hall Inc. </publisher> <year> 1971. </year>
Reference-contexts: At a certain level of abstraction, the task of new-event detection is a classification task where documents are either members of the set of documents containing a new event or not. Previous work related to on-line document classification has often focused on supervised-training algorithms that use labeled documents <ref> [18, 3, 13, 16] </ref>, a resource not available during new-event detection. Most experimental Information Retrieval (IR) literature discusses evaluation using topics and not events. In [10], for example, a knowledge-based approach to text pattern-matching was used to categorize news stories into general topics.
Reference: [19] <author> G. Salton, </author> <title> Automatic Text Processing, </title> <publisher> Addison-Wesley Publishing Co, </publisher> <address> Massachusetts, </address> <year> 1989. </year>
Reference-contexts: a better approach to new-event detection is to use general word-cooccurrence retrieval in a process that specifically models event-level features in addition to topic-level features. 3 New-Event Detection Algorithm If new events were to be sought from a time-ordered static collection, one solution would be to use document clustering techniques <ref> [22, 19] </ref> to cluster the collection, and then to return the document from each cluster containing the earliest timestamp. However, we are interested in the strict on-line case, which has real-time constraints and imposes a single pass restriction over the incoming stream of documents. <p> The consolidation threshold was used to build lists of documents that were assumed to be related to each query. We tested various methods of combining documents that exceeded this threshold into one query. One of the methods for agglomerating queries used average link clustering <ref> [22, 19] </ref>. We found that agglomerating using low values for p had worse performance than agglomerating at higher values, but in general, agglomeration with good parameters had no effect on detection performance.
Reference: [20] <institution> Proceedings of the TDT Workshop, University of Maryland, College Park, MD, </institution> <month> October </month> <year> 1997. </year> <note> (Unpublished) </note>
Reference-contexts: This project (joint with DARPA, CMU, and Dragon Systems) set out to explore issues related to detecting and tracking events in broadcast news. The results of the first year's efforts were reported at a workshop in October, 1997 <ref> [20] </ref>. One of the issues explored in the pilot study was the meaning of event, which was defined as some unique thing that happens at some point in time. The property of time is what distinguishes an event from the more general topic. For example, "The Eruption of Mt.
Reference: [21] <author> C.J. van Rijsbergen, </author> <title> Information Retrieval, </title> <publisher> 2ed., Butterworths, </publisher> <address> Massachusetts, </address> <year> 1979. </year>
Reference-contexts: However, we are interested in the strict on-line case, which has real-time constraints and imposes a single pass restriction over the incoming stream of documents. We have developed a solution to new-event detection using a modification of the single pass clustering algorithm described in <ref> [21] </ref>. Our algorithm processes each new document on the stream sequentially, as follows: 1. Use feature extraction and selection techniques to build a query representation for the document's content. 2. Determine the query's initial threshold by evalu ating the new document with the query. 3.
Reference: [22] <author> P. Willett, </author> <title> "Recent Trends in Hierarchic Document Clustering: A Critical Review," </title> <booktitle> Information Processing & Management, </booktitle> <volume> 24(5): </volume> <pages> 577-597, </pages> <year> 1988. </year>
Reference-contexts: a better approach to new-event detection is to use general word-cooccurrence retrieval in a process that specifically models event-level features in addition to topic-level features. 3 New-Event Detection Algorithm If new events were to be sought from a time-ordered static collection, one solution would be to use document clustering techniques <ref> [22, 19] </ref> to cluster the collection, and then to return the document from each cluster containing the earliest timestamp. However, we are interested in the strict on-line case, which has real-time constraints and imposes a single pass restriction over the incoming stream of documents. <p> The consolidation threshold was used to build lists of documents that were assumed to be related to each query. We tested various methods of combining documents that exceeded this threshold into one query. One of the methods for agglomerating queries used average link clustering <ref> [22, 19] </ref>. We found that agglomerating using low values for p had worse performance than agglomerating at higher values, but in general, agglomeration with good parameters had no effect on detection performance.
References-found: 22

