URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/thrun.london96.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/thrun.london96.html
Root-URL: http://www.cs.cmu.edu
Title: LEARNING MORE FROM LESS DATA: EXPERIMENTS WITH LIFELONG ROBOT LEARNING  
Author: Sebastian Thrun and Joseph O'Sullivan 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> C. A. Atkeson. </author> <title> Using locally weighted regression for robot learning. </title> <booktitle> In Proceedings of the 1991 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 958-962, </pages> <address> Sacra-mento, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: distance between training examples with opposite class labels: E (d) = x;y ffi xy dist d (x ; y) where ffi xy = 1 if class (x) = class (y) 1 if class (x) 6= class (y) Notice that the idea of tuning a distance metric is well-established (see e.g. <ref> [1, 4, 2, 3] </ref>). TC applies these ideas to the transfer of knowledge (concerning the importance of input features) across multiple learning tasks. 3. Estimating the task transfer matrix.
Reference: [2] <author> J. H. Friedman. </author> <title> Flexible metric nearest neighbor classification. </title> <month> November </month> <year> 1994. </year>
Reference-contexts: distance between training examples with opposite class labels: E (d) = x;y ffi xy dist d (x ; y) where ffi xy = 1 if class (x) = class (y) 1 if class (x) 6= class (y) Notice that the idea of tuning a distance metric is well-established (see e.g. <ref> [1, 4, 2, 3] </ref>). TC applies these ideas to the transfer of knowledge (concerning the importance of input features) across multiple learning tasks. 3. Estimating the task transfer matrix.
Reference: [3] <author> T. Hastie and R. Tibshirani. </author> <title> Discriminant adaptive nearest neighbor classification. </title> <note> Submitted for publication, </note> <month> December </month> <year> 1994. </year>
Reference-contexts: distance between training examples with opposite class labels: E (d) = x;y ffi xy dist d (x ; y) where ffi xy = 1 if class (x) = class (y) 1 if class (x) 6= class (y) Notice that the idea of tuning a distance metric is well-established (see e.g. <ref> [1, 4, 2, 3] </ref>). TC applies these ideas to the transfer of knowledge (concerning the importance of input features) across multiple learning tasks. 3. Estimating the task transfer matrix.
Reference: [4] <author> A. W. Moore, D. J. Hill, and M. P. Johnson. </author> <title> An Empirical Investigation of Brute Force to choose Features, Smoothers and Function Approximators. </title> <editor> In S. Hanson, S. Judd, and T. Petsche, editors, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Volume 3. </volume> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: distance between training examples with opposite class labels: E (d) = x;y ffi xy dist d (x ; y) where ffi xy = 1 if class (x) = class (y) 1 if class (x) 6= class (y) Notice that the idea of tuning a distance metric is well-established (see e.g. <ref> [1, 4, 2, 3] </ref>). TC applies these ideas to the transfer of knowledge (concerning the importance of input features) across multiple learning tasks. 3. Estimating the task transfer matrix.
Reference: [5] <author> C. Stanfill and D. Waltz. </author> <title> Towards memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Nearest neighbor generalization. At the underlying function approximation level, the TC algorithm uses nearest neighbor for generalization (see e.g. <ref> [5] </ref>). 2. Adjusting the distance metric. TC uses a globally weighted Euclidean distance metric: dist d (x ; y) = s i 2 Here d denotes an adjustable vector of weighting factors. TC transfers knowledge across tasks by adjusting d for some tasks, then re-using it in others.
Reference: [6] <author> S. Thrun. </author> <title> Explanation-Based Neural Network Learning: A Lifelong Learning Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1996. </year> <note> to appear. </note>
Reference-contexts: a learner re-use knowledge gathered in its previous n 1 learning tasks to reduce the sample complexity in its n-th? In other words: How can the learner transfer knowledge across multiple tasks? Recent research has produced a variety of methods that transfer knowledge across learning tasks (see literature review in <ref> [6] </ref>). Such methods learn domain-specific knowledge, and use it to guide generalization in subsequent learning tasks. However, these methods weigh previous learning tasks equally stronglythus, they may fail when only a small subset of learning tasks is related appropriately. <p> These results are well in tune with other results obtained in robot perception, robot control and game playing domains <ref> [6] </ref>, which illustrate that a lifelong learner can generalize more accurately from less data if it transfers knowledge acquired in previous learning tasks.
Reference: [7] <author> S. Thrun and J. O'Sullivan. </author> <title> Clustering learning tasks and the selective cross-task transfer of knowledge. </title> <type> Technical Report CMU-CS-95-209, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA 15213, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: In this paper we describe a selective approach to lifelong learning, the TC algorithm (short for task clustering), which we just have begun to explore in our lab <ref> [7] </ref>. TC transfers knowledge across multiple tasks by adjusting the distance metric in nearest neighbor generalization. To increase robustness to unrelated tasks, TC arranges all learning tasks hierarchically. <p> Four of those tasks use the same encoding as the door status recognition tasks (hence are well-related), whereas the other eight tasks use different encodings (they are unrelated; in fact, transfer from those hurts the performance. See <ref> [7] </ref> for details). The task transfer matrix is shown in Figure 2. Each entry in this matrix indicates the effect of transferring knowledge from a task n to another task m. The first line corresponds to the new task.
References-found: 7

