URL: http://www.cs.uchicago.edu/~swain/pubs/CVPR97sub.ps
Refering-URL: http://infolab.cs.uchicago.edu/webseer/
Root-URL: 
Email: f swain, frankel, vassilisg@cs.uchicago.edu  
Title: WebSeer: An Image Search Engine for the World Wide Web  
Author: Michael J. Swain, Charles Frankel, and Vassilis Athitsos 
Address: Chicago, Illinois 60637  
Affiliation: Department of Computer Science The University of Chicago  
Abstract: Because of the size of the World Wide Web and its inherent lack of structure, finding what one is looking for can be a challenge. In fact, some of the most highly visited Web sites are search engines. However, while Web pages typically contain both text and images, most currently available search engines only index text. This paper describes WebSeer, a system for locating images on the Web. Web-Seer uses image content in addition to associated text to index images; the image analysis is designed to complement the information obtained from the text. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Amit, D. Geman, and K. Wilder. </author> <title> Recognizing shapes from simple queries about geometry. </title> <year> 1995. </year>
Reference-contexts: In photographs, the ratio tends to be be tween 0.5 and 2, whereas in drawings it is often less than 0.5. 5.1.2 Decision making Yali Amit <ref> [1] </ref> describes how to use multiple decision trees to classify objects into certain categories. We use multiple decision trees to classify an image.
Reference: [2] <author> M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom, M. Gorkani, J. Hafner, D. Lee, D. Petkovic, D. Steele, and P. Yonker. </author> <title> Query by image and video content: </title> <booktitle> The qbic system. Computer, </booktitle> <address> 28:2332, </address> <year> 1995. </year>
Reference-contexts: The first system to attract considerable attention was the Query By Image Content (QBIC) system from IBM Almaden <ref> [2] </ref>. This system, which has continued to evolve, allowed the user to find images similar to a given example image using low-level cues such as color and texture similarity.
Reference: [3] <author> V. Ogle and M. Stonebraker. Chabot: </author> <title> Retrieval from a relational database of images. </title> <journal> IEEE Computer, </journal> <volume> 28(9):4048, </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: One approach that is more promising in a multi-media database such as the World Wide Web is to supplement the image content with other types of information associated with the image. Ogle and Stonebraker's Cypress system <ref> [3] </ref> uses information contained in hand-keyed database fields to supplement image content information. Srihari's Piction system [9] uses the captions of newspaper photographs containing human faces to help locate the faces. As in Srihari's work, we intend to use text associated with the image to guide interpretation of the image.
Reference: [4] <author> R. W. Picard and T. P. Minka. </author> <title> Vision texture for annotation. </title> <journal> Journal of Multimedia Systems, </journal> <volume> 3:314, </volume> <year> 1995. </year>
Reference-contexts: Other researchers who have made contributions in the area of image similarity techniques for content-based indexing into image databases include Carlo Tomassi [6] and Rosalind Picard <ref> [4] </ref>. As a paradigm for an image search engine for the World Wide Web, image similarity has its difficulties. <p> We are working on identifying a taxonomy that fits users' needs and is constructed of image classes that can be reliably identified. Some of these categories may include advertisements, geographic maps, cartoons, graphs, landscapes, city/country scenes <ref> [4] </ref>, night scenes, sunsets, scenes with foliage, and so on. We believe that we need a close interaction between the image understanding algorithms and the associated text indexing algorithms in order to successfully categorize images.
Reference: [5] <author> H. A. Rowley, S. Baluja, and T. Kanade. </author> <title> Neural network-based face detection. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 203208, </pages> <year> 1996. </year>
Reference-contexts: So, the overall error rate for GIF images is 0:042 and the overall error rate for JPEGs is 0:053. 5.2 Locating Faces The current version of WebSeer uses a face-finder developed by Rowley, et. al. <ref> [5] </ref>, which searches for upright faces oriented towards the camera. The efficiency of the face finder is improved by searching for faces only in images determined to be photographs, and by detecting possible face locations using color cues [11] prior to pattern analysis using multiple neural networks.
Reference: [6] <author> Y. Rubner and C. Tomasi. </author> <title> Coalescing texture descriptors. </title> <booktitle> In Proceedings of the ARPA Image Understanding Workshop, </booktitle> <pages> pages 927936, </pages> <year> 1996. </year>
Reference-contexts: Santini and Jain have predicted: The basic operation in query-by-content will be ranking portions of the database with respect to similarity with the query. Other researchers who have made contributions in the area of image similarity techniques for content-based indexing into image databases include Carlo Tomassi <ref> [6] </ref> and Rosalind Picard [4]. As a paradigm for an image search engine for the World Wide Web, image similarity has its difficulties.
Reference: [7] <author> J. R. Smith and S. Chang. </author> <title> Searching for images and videos on the world-wide web. </title> <type> Technical report, </type> <institution> Center for Image Technology for New Media, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: They can therefore can be incomplete, in that on their own, they would not provide enough information to give a comprehensive description of the contents of the image. Independently of WebSeer, a project at Columbia University called WebSeek has recently come on-line <ref> [7] </ref>. WebSeek is another project aimed at studying content-based indexing for the World Wide Web, and it provides a search tool for the Web. WebSeek performs a semi-automated classification of the images on the Web into a hierarchy of categories, using associated text and filename cues.
Reference: [8] <author> R. Srihari. </author> <title> Linguistic context in vision. </title> <booktitle> In Proceed--ings of the Workshop on Context-Based Vision, </booktitle> <year> 1995. </year>
Reference-contexts: We believe that we need a close interaction between the image understanding algorithms and the associated text indexing algorithms in order to successfully categorize images. Srihari's theory of visual semantics <ref> [8] </ref> provides useful insight into some of the challenges of integrating text indexing with image understanding algorithms. A closer interaction between text and image processing will also provide significant improvements in indexing speed.
Reference: [9] <author> R. K. Srihari. </author> <title> Automatic indexing and content-based retrieval of captioned images. </title> <journal> IEEE Computer, </journal> <volume> 28(9):4956, </volume> <year> 1995. </year>
Reference-contexts: Ogle and Stonebraker's Cypress system [3] uses information contained in hand-keyed database fields to supplement image content information. Srihari's Piction system <ref> [9] </ref> uses the captions of newspaper photographs containing human faces to help locate the faces. As in Srihari's work, we intend to use text associated with the image to guide interpretation of the image.
Reference: [10] <author> D. A. White and R. Jain. </author> <title> Algorithms and strategies for similarity retrieval. </title> <type> Technical Report VCL-96-101, </type> <institution> Visual Computing Laboratory, University of California, </institution> <address> San Diego, </address> <year> 1996. </year>
Reference-contexts: For existing methods of similarity retrieval, search time increases exponentially with the dimensionality of the feature space 1 and logarithmically with the number of images in the database <ref> [10] </ref>. One approach that is more promising in a multi-media database such as the World Wide Web is to supplement the image content with other types of information associated with the image. Ogle and Stonebraker's Cypress system [3] uses information contained in hand-keyed database fields to supplement image content information.
Reference: [11] <author> J. Yang and A. Waibel. </author> <title> Tracking human faces in real-time. </title> <type> Technical Report CMU-CS-95-210, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: The efficiency of the face finder is improved by searching for faces only in images determined to be photographs, and by detecting possible face locations using color cues <ref> [11] </ref> prior to pattern analysis using multiple neural networks.
References-found: 11

