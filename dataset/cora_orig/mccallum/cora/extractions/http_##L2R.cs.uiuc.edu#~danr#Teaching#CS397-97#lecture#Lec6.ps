URL: http://L2R.cs.uiuc.edu/~danr/Teaching/CS397-97/lecture/Lec6.ps
Refering-URL: http://L2R.cs.uiuc.edu/~danr/Teaching/CS397-97/lectures.html
Root-URL: http://www.cs.uiuc.edu
Title: CS397: Computational Theories of Learning and Reasoning Fall 1997 a training set of examples and
Author: VC-Dimension 
Note: Theorem 1 Given  In particular, it is assumed in  For example, it is hard to factor. Another problem that is considered hard is the discrete  
Date: September 18, 1997 Lecture 6  
Abstract: In this Lecture we first make a few comments on the unlearnability results that we discussed previously and then move to consider the difficulties that arise when learning infinite classes. We proved that some particular concept classes are hard to PAC learn if we place certain restrictions on the hypothesis space. There are many other results of this kind in particular in the context of neural networks it can be shown: But, there are also results that show hardness of learning independently of the representation. From what we have seen so far there can be two reasons for unlearnability: Are there cases in which we cannot learn c 2 C using any polynomially evaluatable hypothesis h ? In this case C is called inherently unpredictable. In these case one can actually show that we cannot weakly-learn C, that is, learn C with * that is only slightly greater than 1=2. These proofs usually resort to cryptographic assumptions. It is important to notice that there is a deep relation between cryptography and learning when we assume that some function is hard to invert we actually mean that we know the value of the function, but it is hard to find the arguments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Devroye, J. </author> <title> Multivariate Analysis 12, </title> <type> 1 </type> <month> 72-79 </month> <year> (1982), </year> . 
Reference-contexts: Notice that this means that if we find even one set S of size d, s.t. all possible labeling are possible, then V C (C) d. 5 Examples: Finding V C DIM 4 CS397: VC-Dimension 1. Left Bounded Intervals: Consider the concept class of intervals <ref> [a; 1] </ref> on the real number line, 0 &lt; a &lt; 1. Clearly samples of size 1 can be shattered by this class, since if we pick some point x as our sample, the point a can be placed to the left or the right, thus excluding or including x.
Reference: [2] <author> Vapnik, V.N. and Chervonenkis, </author> <title> A.Y. On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <month> 264-280 </month> <year> (1971). </year>
Reference: [3] <author> M. Kearns and U. Vazirani. </author> <title> An Introduction to Computational Learning Theory. </title> <type> 57-62. </type>
Reference: [4] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4): </volume> <pages> 929-965, </pages> <year> 1989. </year>
References-found: 4

