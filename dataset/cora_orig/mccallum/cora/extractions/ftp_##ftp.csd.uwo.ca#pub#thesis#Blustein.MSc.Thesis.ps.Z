URL: ftp://ftp.csd.uwo.ca/pub/thesis/Blustein.MSc.Thesis.ps.Z
Refering-URL: http://union.ncsa.uiuc.edu/HyperNews/get/www/html/converters.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: An Evaluation of Tools for Converting Text to Hypertext  
Author: by William James Blustein 
Degree: Submitted in partial fulfillment of the requirements for the degree of Master of Science Faculty of Graduate Studies  c William James Blustein 1994  
Date: December 1993  
Address: Ontario London, Ontario  
Affiliation: Department of Computer Science  The University of Western  
Abstract-found: 0
Intro-found: 1
Reference: [ACG91] <author> Maristella Agosti, Roberto Colotti, and Girolamo Gradenigo. </author> <title> A Two-Level Hypertext Retrieval Model for Legal Data. </title> <booktitle> In SIGIR '91 , pages 316 - 325, </booktitle> <year> 1991. </year>
Reference-contexts: Part of the MAESTRO Project [Fah88, FB90] is to make links between texts marked-up in SGML [Gol90]. Legal documents with numbered sections, paragraphs, and clauses are a prime example of highly structured text <ref> [ACG91] </ref>. Some attention has been given to turning text marked-up in *roff format into hypertext [TW86, FPS89] with keywords identified by text highlighting, e.g., italicized words.
Reference: [ACM85] <editor> ACM-SIGIR. </editor> <booktitle> Research and Development in Information Retrieval Eighth Annual International ACM SIGIR Conference. ACM, </booktitle> <month> 5 - 7 June </month> <year> 1985. </year>
Reference: [ACM87] <editor> ACM. </editor> <booktitle> Hypertext '87 Papers, </booktitle> <institution> The University of North Carolina, Chapel Hill, North Carolina, </institution> <month> 13 - 15 November </month> <year> 1987. </year> <institution> Association for Computing Machinery. </institution>
Reference: [ACM88] <editor> ACM. </editor> <booktitle> Proceedings of the ACM Conference on Office Information Systems, </booktitle> <address> Palo Alto, CA, </address> <month> 23 - 25 March </month> <year> 1988. </year> <note> SIGOIS Bulletin v.9 #2 - 3 (April - July 1988). </note>
Reference: [ACM89] <editor> ACM. </editor> <booktitle> Hypertext 89 Proceedings, </booktitle> <address> Pittsburgh, Pensylvania, </address> <month> 5 - 8 November </month> <year> 1989. </year> <institution> Association for Computing Machinery. </institution>
Reference: [ACM91] <editor> ACM. </editor> <booktitle> Hypertext '91 Third ACM Conference on Hypertext Proceedings. Association for Computing Machinery, </booktitle> <month> 15 - 18 December </month> <year> 1991. </year>
Reference: [AL92] <author> Philippe Aigrain and Veronique Longueville. </author> <title> Evaluation of navigational links between images. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 28(4):517 - 528, </volume> <month> July August </month> <year> 1992. </year>
Reference-contexts: Full details are presented in Chapter 4. Methods for Evaluating Hypertext Although there are many papers about evaluating the user-interfaces of hypertext systems, I found only a couple of papers about evaluating the indexing of hypertext systems. Philippe Aigrain and Veronique Longueville <ref> [AL92] </ref> are developing methods to evaluate the quality of links in a hypermedia database of images. They compute the link distance as a function dp using a probabilistic model of user behaviour.
Reference: [Bak62] <author> Frank B. Baker. </author> <title> Information retrieval based upon latent class analysis. </title> <journal> Association for Computing Machinery Journal, </journal> <volume> 9:512 - 521, </volume> <year> 1962. </year> <pages> 53 54 </pages>
Reference-contexts: This is accomplished by comparing documents to the pseudo-document that is a perfect representative of a class. This is similar to the vector space concept of a centroid, a single document that represents the average values of all the documents in the collection. Frank B. Baker <ref> [Bak62] </ref> proposed using a probabilistic approach for this. He was not able however to perform rigorous testing of the idea. G. Salton and Chris Buckley [SB90] propose using clustering as part of a linking strategy to improve relevance. 2.2.3 IR Techniques Choosing Terms G. Salton, C. S. Yang, and C.
Reference: [Bal93] <author> V. Balasubramanian. </author> <title> List of Open Issues in Hypertext/Outline of Review. File //eies2/eies2/www/WWW/bala/issues in the World Wide Web hypertext database, </title> <month> October </month> <year> 1993. </year> <institution> Telnet address 128.235.1.43, </institution> <note> login id www. </note>
Reference-contexts: Although there has been great interest in the automatic conversion of general texts into hypertext documents, little work has been done to date [Rai87, CRM89, Ber90]. Chapter 2 Background 2.1 Previous Work Automatic conversion of text into a hypertext document is an interesting open problem <ref> [Bal93] </ref>.
Reference: [Bas91] <author> Reva Basch. </author> <title> Books Online: Visions, Plans, and Perspectives for Electronic Text. Online, </title> <month> July </month> <year> 1991. </year>
Reference-contexts: IR techniques are also applied to full text databases in which queries are evaluated with respect to entire documents. This process is illustrated in Figure 2.1 1 . There is a good deal of overlap between hypertext and IR, but the research areas are quite different <ref> [Hal88, CNBKO90, You90, Bas91] </ref>. Information Problem Documents Text Representation Query Retrieved Documents Representation Documents Indexed Figure adapted from: Figure 1 of `Hypertext and Information Retrieval: Croft, panel proposer. (Hypertext: Concepts, Systems and Applications, 1990). What are the Fundamental Concepts?' [A panel discussion] W.
Reference: [BCSR91] <editor> A. Bookstei, Y. Chiaramella, G. Salton, and V.V. Raghava, editors. </editor> <booktitle> SIGIR '91 Proceedings of the 14th Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <address> Chicago, Illinois USA, </address> <month> 13 - 16 October </month> <year> 1991. </year> <booktitle> ACM SIGIR, The Association for Computing Machinery. </booktitle>
Reference: [BD91] <author> Emily Berk and Joseph Devlin, </author> <title> editors. </title> <booktitle> Hypertext/Hypermedia Handbook. Software Engineering. </booktitle> <publisher> Intertext Publications, McGraw-Hill Publishing, </publisher> <year> 1991. </year>
Reference-contexts: The number of links from a document could be limited by the number of terms in the document or I could allow an arbitrary number of links. Permitting an unlimited number of links would create documents with an unwieldy number of links <ref> [BD91] </ref>. An analysis by link distance would be meaningless, since every document would be one link away from every other document. Such a strategy would be equivalent to allowing calls to a search engine from within a hypertext browser.
Reference: [Ben93] <author> Alfred Benn. </author> <type> Personal communication, </type> <month> 17 December </month> <year> 1993. </year>
Reference-contexts: However, the greater the similarity between two documents the shorter the path should be between them, so it is expected that correlations between these tables should be less than zero. Generally, a correlation with magnitude greater than or equal to 0:20 is significant <ref> [Ben93] </ref>.
Reference: [Ber90] <author> Mark Bernstein. </author> <title> An Apprentice That Discovers Hypertext Links. In Hypertext: </title> <booktitle> Concepts, Systems and Applications , pages 212 - 223, </booktitle> <year> 1990. </year>
Reference-contexts: In such cases, the links between words and entries have already been made by humans, based on an understanding of the text. Although there has been great interest in the automatic conversion of general texts into hypertext documents, little work has been done to date <ref> [Rai87, CRM89, Ber90] </ref>. Chapter 2 Background 2.1 Previous Work Automatic conversion of text into a hypertext document is an interesting open problem [Bal93]. <p> The style of these articles varies widely as well. Thus, my initial text is more closely related to the kinds of texts that information retrieval (IR) researchers process, rather than those studied by AI researchers. One example of the IR approach is the work of Mark Bernstein <ref> [Ber90] </ref>. His system makes links between similar passages in monographs. His similarity measure is based on a Bloom filter hashing [Blo70] of the individual words and word stems. <p> His similarity measure is based on a Bloom filter hashing [Blo70] of the individual words and word stems. This approach works with `compact, independent hypertext documents which may be considered to address a single subject, and which are intended to be read rather than queried.' <ref> [Ber90, p. 213] </ref> He specifically excludes electronic mail and Usenet messages from the program's domain [Ber90, p. 213]. Bloom filters use multiple hash functions to generate a probabilistic set membership test. <p> This approach works with `compact, independent hypertext documents which may be considered to address a single subject, and which are intended to be read rather than queried.' <ref> [Ber90, p. 213] </ref> He specifically excludes electronic mail and Usenet messages from the program's domain [Ber90, p. 213]. Bloom filters use multiple hash functions to generate a probabilistic set membership test. They can never falsely report a member as a non-member but will sometimes declare a non-member to be a member. <p> Conclusion In practice anything that normalizes for document length, e.g., the cosine measure is acceptable [Ber93]. Susan Dumais [Dum93] has found empirically that the cosine measure works best. I use the cosine measure for my experiments. 2.3.2 Other Methods Bloom Filter Apprentice Bernstein <ref> [Ber90, MB93] </ref> developed a novel method for using Bloom filters [Blo70] to compute the similarity of passages in monographs (see also Section 2.1.3). His `link apprentice' is part of an interactive program for reading texts, e.g., high school level history textbooks. <p> The semantic links in my implementation connect articles that contain the same technical terms. Another possible method would be to connect articles with similar content | as Bernstein <ref> [Ber90] </ref> 17 18 Relay-Version: version B 2.10 5/3/83; site utzoo.UUCP Posting-Version: version B 2.10.2 (Tek) 9/28/84 based on 9/17/84; site orca.UUC P Path: utzoo!watmath!clyde![: : :]!orca!warner From: warner@orca.UUCP ( Ken Warner ) Newsgroups: net.graphics Subject: Re: Graphics Standards Information! Message-ID: &lt; 1414@orca.UUCP &gt; Date: Thu, 21-Mar-85 12:56:10 EST Article-I.D.: orca.1414 Posted:
Reference: [Ber93] <author> Mark Bernstein. </author> <type> Personal communication. e-mail, </type> <month> 8 June </month> <year> 1993. </year>
Reference-contexts: Bernstein's method is to compare the dot product of vectors thus generated from two passages. The higher the magnitude of the product the greater the similarity. He also normalizes the results with respect to passage length <ref> [Ber93] </ref>. 2.2 Information Retrieval Most information retrieval work is done with bibliographic databases in mind [Mea92]. IR is usually thought of as a process of query refinement, in which a user queries a computerized library catalogue to find records of interest. <p> Further, terms which are not part of both documents will increase the angle of separation between the vectors thus decreasing the cosine measure. Conclusion In practice anything that normalizes for document length, e.g., the cosine measure is acceptable <ref> [Ber93] </ref>. Susan Dumais [Dum93] has found empirically that the cosine measure works best.
Reference: [BL89] <author> Ken Bice and Clayton Lewis, </author> <title> editors. CHI '89 `Wings For The Mind', </title> <address> Austin, Texas, 30 April - 4 May 1989. </address> <publisher> ACM SIGCHI, Addison-Wesley. </publisher>
Reference: [BLC] <author> Tim Berners-Lee and Daniel Connolly. </author> <title> Hypertext Markup Language A Representation of Textual Information and Meta Information for Retrieval and Interchange. World Wide Web, 15 March 1993 version edition. This hypertext document has WWW address http:info.cern.ch/hypertext/WWW/MarkUp/MarkUp.html. It can be read by connecting to info.cern.ch (numeric address 128.141.201.74) with the telnet protocol. </title>
Reference-contexts: WWW is a distributed hypermedia system. The Web's collection of hypermedia documents are accessible through client software, called browsers, which connect to WWW servers over the Internet. Hypermedia documents in the WWW are marked-up in HTML <ref> [BLC] </ref>, which is similar to SGML [Gol90]. I chose to implement my linking strategies by converting text documents into HTML form. Almost all previous research on automatically converting text into hypertext documents assumes that the text is highly structured and can thus be parsed by a computer. <p> Chapter 3 Approach 3.1 Experimental Design I wrote prototype programs to convert text files containing 1609 Usenet news postings (described in Appendix II) containing 2411 terms into a hypertext document. I converted the text into an equivalent marked-up form using HTML, The Hypertext Markup Language <ref> [BLC] </ref>. Marked-up documents have all the same content as the original document as well as special symbols representing the logical structure of the document. Additional symbols are used to indicate the presence of hypertext links.
Reference: [Blo70] <author> Burton H. Bloom. </author> <title> Space/Time Trade-offs in Hashing Coding with Allowable Errors. </title> <journal> Communications of the ACM, </journal> <volume> 13(7):422 - 426, </volume> <month> July </month> <year> 1970. </year>
Reference-contexts: One example of the IR approach is the work of Mark Bernstein [Ber90]. His system makes links between similar passages in monographs. His similarity measure is based on a Bloom filter hashing <ref> [Blo70] </ref> of the individual words and word stems. <p> Susan Dumais [Dum93] has found empirically that the cosine measure works best. I use the cosine measure for my experiments. 2.3.2 Other Methods Bloom Filter Apprentice Bernstein [Ber90, MB93] developed a novel method for using Bloom filters <ref> [Blo70] </ref> to compute the similarity of passages in monographs (see also Section 2.1.3). His `link apprentice' is part of an interactive program for reading texts, e.g., high school level history textbooks.
Reference: [BP88] <author> Christine L. Borgman and Edward Y.H. Pai, </author> <title> editors. </title> <booktitle> Information & Technology Planning for the Second 50 Years Proceedings of the 50th Annual Meeting 55 of the American Society for Information Science, volume 25, </booktitle> <address> Atlanta, Georgia, </address> <month> 23 - 27 October </month> <year> 1988. </year> <title> Learned Information, </title> <publisher> Inc. </publisher>
Reference: [BS93] <author> Richard Brandwein and Mike Sendall. </author> <note> HTML converters. WWW address http://info.cern.ch/hypertext/WWW/Tools/Filters.html, 1993. </note>
Reference-contexts: Some attention has been given to turning text marked-up in *roff format into hypertext [TW86, FPS89] with keywords identified by text highlighting, e.g., italicized words. Programs to convert from a variety of structured formats to a marked-up form are available <ref> [BS93] </ref>. 2.1.2 The AI Approach The papers illustrating the AI approach [CRM89, Rai87, HR88] deal with text from highly restricted domains. They employ semantic parsers to find relationships in the texts. Among other problems, such systems require grammatically correct texts.
Reference: [Bus45] <author> Vannevar Bush. </author> <title> As we may think. </title> <booktitle> The Atlantic, </booktitle> <year> 1945. </year> <note> As reprinted by Nelson [Nel87]. </note>
Reference-contexts: Introduction Hypertext documents are a way of presenting textual information [Con87, Nel87, Eas90]. Unlike traditional documents, such as books, hypertext documents are interactive and record explicitly the relationships between different parts of their text [DeR89]. Hypertext was first described as part of a machine called memex <ref> [Bus45] </ref> in 1945. The term hypertext was coined by Theodore Nelson [Nel87] in the 1960's. 1.1 Advantages of Hypertext It is much easier to search for text in a hypertext document than in a book [ELK + 91].
Reference: [CLR91] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> McGraw-Hill, </publisher> <year> 1991. </year>
Reference-contexts: Figure III.1: Modified Floyd-Warshall Shortest Path Algorithm <ref> [CLR91, pp. 558 - 563] </ref> G is an n d fi n d adjacency matrix for a directed weighted graph. The initial edge weights are one of: zero, one or infinity. All elements g ii have initial weight 0.
Reference: [CNBKO90] <author> W. Bruce Croft, Marie-France Bruandet Nicholas Belkin, Rainer Kuhlen, and Tim Oren. </author> <title> Hypertext and Information Retrieval: What are the Fundamental Concepts? In Hypertext: </title> <booktitle> Concepts, Systems and Applications , pages 362 - 366, </booktitle> <year> 1990. </year>
Reference-contexts: IR techniques are also applied to full text databases in which queries are evaluated with respect to entire documents. This process is illustrated in Figure 2.1 1 . There is a good deal of overlap between hypertext and IR, but the research areas are quite different <ref> [Hal88, CNBKO90, You90, Bas91] </ref>. Information Problem Documents Text Representation Query Retrieved Documents Representation Documents Indexed Figure adapted from: Figure 1 of `Hypertext and Information Retrieval: Croft, panel proposer. (Hypertext: Concepts, Systems and Applications, 1990). What are the Fundamental Concepts?' [A panel discussion] W. <p> Precision is the number of found records relevant to the query divided by the number of records found. The relevance of a record to a query is a subjective measure. A document may be 1 The figure is adapted from a similar figure accompanying a panel discussion <ref> [CNBKO90] </ref>. 7 Number of Collection Terms Documents TIME [Dum91] 10 337 425 MED [DDL + 88] 5831 1033 Cranfield [Dum91] 4486 1400 CISI [DDL + 88] 1460 5143 ADI [Dum91] 374 82 comp.graphics 2478 1608 Table 2.1: Sizes of Some Typical Datasets considered relevant to a query even if they have
Reference: [Con87] <author> Jeff Conklin. </author> <title> Hypertext: An Introduction and Survey. </title> <journal> IEEE Computer, </journal> <volume> 20:17 - 41, </volume> <year> 1987. </year>
Reference-contexts: Introduction Hypertext documents are a way of presenting textual information <ref> [Con87, Nel87, Eas90] </ref>. Unlike traditional documents, such as books, hypertext documents are interactive and record explicitly the relationships between different parts of their text [DeR89]. Hypertext was first described as part of a machine called memex [Bus45] in 1945. <p> A successor method to LSA should not have this property. Appendix I An example hypertext system Hypertext systems are typically implemented in a graphical user interface with multiple windows each of which contain text <ref> [Con87] </ref>. New windows are brought up as the user selects links to follow. Information can be found either by having the system search for a string or by following links. Some hypertext systems provide visual browsers to show the user a graphical representation of the database they are querying.
Reference: [CRM89] <author> Peter Clitherow, Doug Riecken, and Michael Muller. VISAR: </author> <title> A System for Inference and Navigation in Hypertext. </title> <booktitle> In Hypertext '89 Proceedings , pages 293 - 304, </booktitle> <year> 1989. </year>
Reference-contexts: In such cases, the links between words and entries have already been made by humans, based on an understanding of the text. Although there has been great interest in the automatic conversion of general texts into hypertext documents, little work has been done to date <ref> [Rai87, CRM89, Ber90] </ref>. Chapter 2 Background 2.1 Previous Work Automatic conversion of text into a hypertext document is an interesting open problem [Bal93]. <p> Programs to convert from a variety of structured formats to a marked-up form are available [BS93]. 2.1.2 The AI Approach The papers illustrating the AI approach <ref> [CRM89, Rai87, HR88] </ref> deal with text from highly restricted domains. They employ semantic parsers to find relationships in the texts. Among other problems, such systems require grammatically correct texts. These systems use knowledge bases to aid in the parsing of such texts. <p> They employ semantic parsers to find relationships in the texts. Among other problems, such systems require grammatically correct texts. These systems use knowledge bases to aid in the parsing of such texts. For each new subject, a new knowledge base must be built. VISAR <ref> [CRM89] </ref> is a hypertext system for maintaining a database of journal citations. It is knowledge intensive and requires the user to explicitly customize the knowledge base.
Reference: [DB91] <author> Nicholas Duncan and David T. Barnard. </author> <title> The document-to-document correction problem. </title> <type> Technical Report 91-315, </type> <institution> Department of Computing and Information Science, Queen's University at Kingston, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Lev Goldfarb [Gol86] has proposed measuring the similarity of documents by the minimal number of steps required to convert the tree describing one document to a tree describing the other. Some related work is being carried out at Queen's University <ref> [DB91] </ref>, where a program has been written to build hypertext links between documents marked-up with an explicit tree structure [Fah88, FB90]. 9 Clustering There are ways of extracting essential predictors about documents and using them for comparisons that do not rely on an explicit vector space model.
Reference: [DDB] <author> Scott Deerwester, Susan Dumais, and Michael Berry. </author> <title> LSI package. </title> <institution> copyright c fl1990 Bell Communications Research, Inc. Provided by Susan Dumais hdumais@bellcore.comi Bell Communications Research, </institution> <address> 445 South St., Morristown, NJ 07960, USA. </address>
Reference-contexts: The terms were weighted with a combination of local log and global entropy weighting (see Section 2.2.3) as suggested by Dumais [Dum91]. The list of 2478 terms is effectively represented by a reduced model of 100 factors by the LSA package <ref> [DDB] </ref>. The number of factors chosen was suggested by Dumais [Dum91]. 3.1.3 Evaluating Semantic Links LSA seems to have the best performance (in terms of recall and precision) of any IR method for general text [Dum91].
Reference: [DDF + 90] <author> Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. </author> <title> Indexing by Latent Semantic Analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41:391 - 407, </volume> <month> September </month> <year> 1990. </year> <month> 56 </month>
Reference-contexts: Large systems are not always tested using measures of recall and precision. Scott Deerwester et al. use the terms polysemy and synonymy to describe the difficulties that make up the vocabulary problem <ref> [DDF + 90] </ref>. They use polysemy to refer to the problem that many words have more than one meaning | one needs context to determine the meaning. This can pose a big problem for mechanical indexing methods. <p> It can also be used to measure the amount of overlap between single documents and collections. LSA can be used to compare the similarity of terms, `lighting' and `shading' for example. It can also be used to measure the relatedness of a term to a document <ref> [DDF + 90] </ref>. The method performs at least as well as standard methods for standard test datasets such as SMART [DDF + 90, p. 400 - 404]. <p> It can also be used to measure the relatedness of a term to a document [DDF + 90]. The method performs at least as well as standard methods for standard test datasets such as SMART <ref> [DDF + 90, p. 400 - 404] </ref>. Dumais reported that a 40% improvement over such standard methods has been achieved by applying LSA to matrices where the entries are a combination of local log weights (Equation 2.2-7, page 11) and global entropy (Equation 2.2-4) weighting. <p> The elements of the singular vectors represent the factors extracted by SVD. The essential factors are represented by the matrix ^ X (Equation 2.3-13) of rank k in a reduced model. The reduced matrix ^ X is closest `in the least squares sense' <ref> [DDF + 90, p. 398] </ref> to X. The new diagonal matrix S is obtained by removing rows and columns of zeros from S 0 . The new singular vectors are obtained by deleting the corresponding rows and columns from D 0 and T 0 . <p> The new singular vectors are obtained by deleting the corresponding rows and columns from D 0 and T 0 . X ^ X = T SD T (2.3-13) The matrix ^ X is presumed to represent the important and reliable patterns underlying the data in X <ref> [DDF + 90, p. 398] </ref>. Similarity measures are simply computed from the model. The similarity of two terms, i and j is (T S 2 T T ) ij = ( ^ X ^ X T ) ij . <p> If too few factors are chosen, then it is likely that some essential factors will not be included [Dum91]. There is no a priori method for choosing the best value of k although research is being conducted to determine some useful heuristics <ref> [DDF + 90, Dum91] </ref>. Chapter 3 Approach 3.1 Experimental Design I wrote prototype programs to convert text files containing 1609 Usenet news postings (described in Appendix II) containing 2411 terms into a hypertext document. I converted the text into an equivalent marked-up form using HTML, The Hypertext Markup Language [BLC]. <p> A method for rolling in new terms and documents into the model without recomputing the SVD would be useful. A method whereby the essential terms are extracted from new docs might help here. Deerwester et al. <ref> [DDF + 90, p. 405] </ref> note that the LSA method does not deal well with words with multiple meanings, e.g., `bank'. Such words will be represented by the weighted average of their different meanings.
Reference: [DDL + 88] <author> Scott Deerwester, Susan Dumais, Thomas Landauer, George Furna, and Laura Beck. </author> <title> Improving information retrieval with latent semantic indexing. </title> <booktitle> In ASIS '88 Proceedings of the 51st Annual Meeting , pages 36 - 40, </booktitle> <year> 1988. </year>
Reference-contexts: The relevance of a record to a query is a subjective measure. A document may be 1 The figure is adapted from a similar figure accompanying a panel discussion [CNBKO90]. 7 Number of Collection Terms Documents TIME [Dum91] 10 337 425 MED <ref> [DDL + 88] </ref> 5831 1033 Cranfield [Dum91] 4486 1400 CISI [DDL + 88] 1460 5143 ADI [Dum91] 374 82 comp.graphics 2478 1608 Table 2.1: Sizes of Some Typical Datasets considered relevant to a query even if they have no vocabulary in common. <p> A document may be 1 The figure is adapted from a similar figure accompanying a panel discussion [CNBKO90]. 7 Number of Collection Terms Documents TIME [Dum91] 10 337 425 MED <ref> [DDL + 88] </ref> 5831 1033 Cranfield [Dum91] 4486 1400 CISI [DDL + 88] 1460 5143 ADI [Dum91] 374 82 comp.graphics 2478 1608 Table 2.1: Sizes of Some Typical Datasets considered relevant to a query even if they have no vocabulary in common.
Reference: [DeR89] <author> Steven J. DeRose. </author> <title> Expanding the Notion of Links. </title> <booktitle> In Hypertext '89 Proceedings , pages 249 - 255, </booktitle> <year> 1989. </year>
Reference-contexts: Introduction Hypertext documents are a way of presenting textual information [Con87, Nel87, Eas90]. Unlike traditional documents, such as books, hypertext documents are interactive and record explicitly the relationships between different parts of their text <ref> [DeR89] </ref>. Hypertext was first described as part of a machine called memex [Bus45] in 1945. The term hypertext was coined by Theodore Nelson [Nel87] in the 1960's. 1.1 Advantages of Hypertext It is much easier to search for text in a hypertext document than in a book [ELK + 91].
Reference: [Dum91] <author> Susan T. Dumais. </author> <title> Improving the retrieval of information from external sources. Behavior Research Methods, Instruments, </title> & <journal> Computers, </journal> <volume> 23(2):229 - 236, </volume> <year> 1991. </year>
Reference-contexts: The relevance of a record to a query is a subjective measure. A document may be 1 The figure is adapted from a similar figure accompanying a panel discussion [CNBKO90]. 7 Number of Collection Terms Documents TIME <ref> [Dum91] </ref> 10 337 425 MED [DDL + 88] 5831 1033 Cranfield [Dum91] 4486 1400 CISI [DDL + 88] 1460 5143 ADI [Dum91] 374 82 comp.graphics 2478 1608 Table 2.1: Sizes of Some Typical Datasets considered relevant to a query even if they have no vocabulary in common. <p> The relevance of a record to a query is a subjective measure. A document may be 1 The figure is adapted from a similar figure accompanying a panel discussion [CNBKO90]. 7 Number of Collection Terms Documents TIME <ref> [Dum91] </ref> 10 337 425 MED [DDL + 88] 5831 1033 Cranfield [Dum91] 4486 1400 CISI [DDL + 88] 1460 5143 ADI [Dum91] 374 82 comp.graphics 2478 1608 Table 2.1: Sizes of Some Typical Datasets considered relevant to a query even if they have no vocabulary in common. <p> A document may be 1 The figure is adapted from a similar figure accompanying a panel discussion [CNBKO90]. 7 Number of Collection Terms Documents TIME <ref> [Dum91] </ref> 10 337 425 MED [DDL + 88] 5831 1033 Cranfield [Dum91] 4486 1400 CISI [DDL + 88] 1460 5143 ADI [Dum91] 374 82 comp.graphics 2478 1608 Table 2.1: Sizes of Some Typical Datasets considered relevant to a query even if they have no vocabulary in common. <p> Systems are often evaluated by the recall and precision values [Sal92, TS92] for standard queries on standard document collections such as Cranfield [Har92] or MEDLARS <ref> [Dum91] </ref>. Table 2.1 summarizes characteristics of databases composed of articles from: Time magazine, abstracts in the U.S. National Library of Medicine (MED), abstracts about aeronautics from the Cranfield collection, abstracts about library science and related fields (CISI and ADI), and the collection I use for my tests. <p> Although the vector model does not have a strong theoretical basis [RW86, WZW85], it has been used extensively [Sal91]. The model is often used with the implicit assumption that terms are independent <ref> [Dum91, WZW85] </ref>, so jT j terms can be represented by jT j orthogonal dimensions. Tree Models Documents may be described by trees, i.e., directed acyclic graphs. Tree structures are a natural way to represent the hierarchical nature of many documents. <p> weighting when she as signed each term in a collection D of documents the weight of dlog 2 jDje dlog 2 f t D e + 1 (2.2-1) where 2 f t D is the frequency of term t in the collection of jDj documents. * Inverse Document Frequency (Idf) <ref> [Dum91, Sal89] </ref> Frequency-based indexing models are based on the premise that the best indexing terms `are those that occur frequently in individual documents but rarely in the remainder of the collection' [Sal89, p. 280]. <p> This is the inspiration for the inverse document frequency weighting: 2 A summary of notation used throughout this document begins on page xi. 11 jD t j + 1 (2.2-2) * GfIdf <ref> [Dum91] </ref> f t jD t j * entropy [Dum91] The entropy weighting scheme was developed from information theory [Sal89]. 1 d d D d D log (jDj) * term-discrimination value (TDV) [Sal89, pp.281 - 4] The term-discrimination value is a sophisticated measurement of the distinguishing power of each term. <p> This is the inspiration for the inverse document frequency weighting: 2 A summary of notation used throughout this document begins on page xi. 11 jD t j + 1 (2.2-2) * GfIdf <ref> [Dum91] </ref> f t jD t j * entropy [Dum91] The entropy weighting scheme was developed from information theory [Sal89]. 1 d d D d D log (jDj) * term-discrimination value (TDV) [Sal89, pp.281 - 4] The term-discrimination value is a sophisticated measurement of the distinguishing power of each term. <p> Local Weightings Some local weighting schemes are: * raw term frequency <ref> [Dum91, Har86, Sal89] </ref>: f t * Binary or Boolean [Dum91] 8 &gt; &gt; &gt; &lt; 1 if f t 0 otherwise (2.2-6) log (f t 12 Salton and Buckley [SB91, p. 24] have produced weighting formulae for use with an electronic copy of an encyclopaedia. <p> Local Weightings Some local weighting schemes are: * raw term frequency [Dum91, Har86, Sal89]: f t * Binary or Boolean <ref> [Dum91] </ref> 8 &gt; &gt; &gt; &lt; 1 if f t 0 otherwise (2.2-6) log (f t 12 Salton and Buckley [SB91, p. 24] have produced weighting formulae for use with an electronic copy of an encyclopaedia. Their formulae are neither local nor global but a combination of the two. <p> The parameters of the method are the term-document weight matrix and the number of factors to use in the reduced model. If too many factors are chosen then performance suffers. If too few factors are chosen, then it is likely that some essential factors will not be included <ref> [Dum91] </ref>. There is no a priori method for choosing the best value of k although research is being conducted to determine some useful heuristics [DDF + 90, Dum91]. <p> If too few factors are chosen, then it is likely that some essential factors will not be included [Dum91]. There is no a priori method for choosing the best value of k although research is being conducted to determine some useful heuristics <ref> [DDF + 90, Dum91] </ref>. Chapter 3 Approach 3.1 Experimental Design I wrote prototype programs to convert text files containing 1609 Usenet news postings (described in Appendix II) containing 2411 terms into a hypertext document. I converted the text into an equivalent marked-up form using HTML, The Hypertext Markup Language [BLC]. <p> Donna Harman [Har87] found that stemming makes little difference in the performance of IR systems. I do not stem any words for my experiments. The terms were weighted with a combination of local log and global entropy weighting (see Section 2.2.3) as suggested by Dumais <ref> [Dum91] </ref>. The list of 2478 terms is effectively represented by a reduced model of 100 factors by the LSA package [DDB]. The number of factors chosen was suggested by Dumais [Dum91]. 3.1.3 Evaluating Semantic Links LSA seems to have the best performance (in terms of recall and precision) of any IR <p> terms were weighted with a combination of local log and global entropy weighting (see Section 2.2.3) as suggested by Dumais <ref> [Dum91] </ref>. The list of 2478 terms is effectively represented by a reduced model of 100 factors by the LSA package [DDB]. The number of factors chosen was suggested by Dumais [Dum91]. 3.1.3 Evaluating Semantic Links LSA seems to have the best performance (in terms of recall and precision) of any IR method for general text [Dum91]. I chose LSA because it performed well for general text and avoids the synonymy problem so common with document collections from many authors. <p> The number of factors chosen was suggested by Dumais <ref> [Dum91] </ref>. 3.1.3 Evaluating Semantic Links LSA seems to have the best performance (in terms of recall and precision) of any IR method for general text [Dum91]. I chose LSA because it performed well for general text and avoids the synonymy problem so common with document collections from many authors. I used the document-to-document similarity measure available with latent semantic analysis (see Section 2.3.2) as an approximation of semantic similarity.
Reference: [Dum93] <author> Susan T. </author> <title> Dumais. </title> <type> Personal communication. e-mail, </type> <month> 5 May </month> <year> 1993. </year> <title> Dumais is the corresponding author on many papers about latent semantic analysis (LSA) and a co-author of others [FDD + 88, </title> <address> DDL + 88, DDF + 90, Dum91]. </address>
Reference-contexts: Further, terms which are not part of both documents will increase the angle of separation between the vectors thus decreasing the cosine measure. Conclusion In practice anything that normalizes for document length, e.g., the cosine measure is acceptable [Ber93]. Susan Dumais <ref> [Dum93] </ref> has found empirically that the cosine measure works best. I use the cosine measure for my experiments. 2.3.2 Other Methods Bloom Filter Apprentice Bernstein [Ber90, MB93] developed a novel method for using Bloom filters [Blo70] to compute the similarity of passages in monographs (see also Section 2.1.3).
Reference: [Eas90] <author> Steve M. Easterbrook. </author> <title> What is Hypertext? In Text Retrieval The State of the Art , pages 119 - 137, </title> <year> 1990. </year>
Reference-contexts: Introduction Hypertext documents are a way of presenting textual information <ref> [Con87, Nel87, Eas90] </ref>. Unlike traditional documents, such as books, hypertext documents are interactive and record explicitly the relationships between different parts of their text [DeR89]. Hypertext was first described as part of a machine called memex [Bus45] in 1945.
Reference: [ELK + 91] <author> Dennis E. Egan, Michael E. Lesk, R. Daniel Ketchum, Carol C. Lochbaum, Joel R. Remde, Michael Littman, and Thomas K. Landauer. </author> <title> Hypertrext for the electronic library? CORE sample results. </title> <booktitle> In Hypertext '91 Proceedings , pages 299 - 312, </booktitle> <year> 1991. </year>
Reference-contexts: Hypertext was first described as part of a machine called memex [Bus45] in 1945. The term hypertext was coined by Theodore Nelson [Nel87] in the 1960's. 1.1 Advantages of Hypertext It is much easier to search for text in a hypertext document than in a book <ref> [ELK + 91] </ref>. To find details in a traditional book, one reads all of it or searches through the index. Book indexes are not as useful as the distributed indexes (called links) available in hypertext documents.
Reference: [Fah88] <author> Eanass Fahmy. </author> <title> Programmatically generating connections in document forests. </title> <type> Master's thesis, </type> <institution> Queen's University, Kingston, </institution> <address> Ontario, Canada, </address> <year> 1988. </year> <institution> Department of Computing and Information Science. </institution>
Reference-contexts: Other researchers are working with electronic versions of text that have already been structured for use with computers. Part of the MAESTRO Project <ref> [Fah88, FB90] </ref> is to make links between texts marked-up in SGML [Gol90]. Legal documents with numbered sections, paragraphs, and clauses are a prime example of highly structured text [ACG91]. <p> Some related work is being carried out at Queen's University [DB91], where a program has been written to build hypertext links between documents marked-up with an explicit tree structure <ref> [Fah88, FB90] </ref>. 9 Clustering There are ways of extracting essential predictors about documents and using them for comparisons that do not rely on an explicit vector space model. Clustering is a method by which documents are grouped into classes by topic [Sal91].
Reference: [FB90] <author> Eanass Fahmy and David T. Barnard. </author> <title> Adding hypertext links to an archive of documents. </title> <journal> The Canadian Journal of Information Science, </journal> <volume> 15(3):25 - 41, </volume> <month> September </month> <year> 1990. </year>
Reference-contexts: Other researchers are working with electronic versions of text that have already been structured for use with computers. Part of the MAESTRO Project <ref> [Fah88, FB90] </ref> is to make links between texts marked-up in SGML [Gol90]. Legal documents with numbered sections, paragraphs, and clauses are a prime example of highly structured text [ACG91]. <p> Some related work is being carried out at Queen's University [DB91], where a program has been written to build hypertext links between documents marked-up with an explicit tree structure <ref> [Fah88, FB90] </ref>. 9 Clustering There are ways of extracting essential predictors about documents and using them for comparisons that do not rely on an explicit vector space model. Clustering is a method by which documents are grouped into classes by topic [Sal91].
Reference: [FDD + 88] <author> George W. Furnas, Scott Deerwester, Susan T. Dumais, Thomas K. Landauer, Richard A. Harshman, Lynn A. Streeter, and Laren E. Lochbaum. </author> <title> Information retrieval using a singular value decomposition model of latent semantic structure. </title> <booktitle> In SIGIR '88, </booktitle> <address> Grenoble, France, </address> <year> 1988. </year>
Reference-contexts: In the model, a collection of documents is represented as a matrix of vectors of terms weights within documents. They suggest that a true matrix 15 model would allow arithmetic manipulation of the vectors to form new matrices. In 1988 Furnas et al. <ref> [FDD + 88] </ref> presented latent semantic analysis (LSA). LSA is a method based on the singular value decomposition (SVD) [FMM77] of a matrix representing a document collection. SVD is used to represent the matrix as a combination of linearly independent factors.
Reference: [Fic87] <author> David K. Fickes. </author> <title> Usenet in Hypertext form. </title> <note> Usenet Message-ID h8711130839.AA02525@bucsb.bu.edui, posted to comp.society.futures, </note> <month> 57 November </month> <year> 1987. </year> <note> Obtained using anonymous ftp from site nl.cs.cmu.edu (numeric address 128.2.222.56) in directory /usr/toad/hypertext. File name hyperusenet.proposal.1. </note>
Reference-contexts: A hypertext system could provide that and more. Current technology forces users to view messages in one of a few rigidly defined ways. However there would be great benefits to viewing messages in other orders. The idea of a dynamic version of Usenet was suggested as early as 1987 <ref> [Fic87] </ref>. It is convenient to use articles collected from Usenet as raw data for my thesis, but the results of my thesis project should apply equally well to a wide variety of unstructured texts.
Reference: [FLGD87] <author> G.W. Furnas, T.K. Landauer, L.M. Gomez, and S.T. Dumais. </author> <title> The vocabulary problem in human-system communication. </title> <journal> Communications of the ACM, </journal> <volume> 30(11):964 - 971, </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: The vocabulary problem in IR is that different people will often choose different words with which to express the same concept <ref> [FLGD87, KN90] </ref> or to index the same document [Jon90, KN90]. Systems are often evaluated by the recall and precision values [Sal92, TS92] for standard queries on standard document collections such as Cranfield [Har92] or MEDLARS [Dum91].
Reference: [FMM77] <author> George E. Forsythe, Michael A. Malcolm, and Cleve B. Moler. </author> <title> Computer Methods for Mathematical Computations, chapter 9. Least Squares and the Singular Value Decomposition, </title> <booktitle> pages 192 - 239. Prentice-Hall, first edition, </booktitle> <year> 1977. </year>
Reference-contexts: They suggest that a true matrix 15 model would allow arithmetic manipulation of the vectors to form new matrices. In 1988 Furnas et al. [FDD + 88] presented latent semantic analysis (LSA). LSA is a method based on the singular value decomposition (SVD) <ref> [FMM77] </ref> of a matrix representing a document collection. SVD is used to represent the matrix as a combination of linearly independent factors. The matrix is then approximated by using only the most significant k factors. LSA can be used to compare documents for similarity with each other.
Reference: [FPS89] <author> Richard Furuta, Catherine Plaisant, and Ben Shneiderman. </author> <title> A Spectrum of Automatic Hypertext Constructions. </title> <journal> Hypermedia, </journal> <volume> 1(2):179 - 195, </volume> <year> 1989. </year>
Reference-contexts: Part of the MAESTRO Project [Fah88, FB90] is to make links between texts marked-up in SGML [Gol90]. Legal documents with numbered sections, paragraphs, and clauses are a prime example of highly structured text [ACG91]. Some attention has been given to turning text marked-up in *roff format into hypertext <ref> [TW86, FPS89] </ref> with keywords identified by text highlighting, e.g., italicized words. Programs to convert from a variety of structured formats to a marked-up form are available [BS93]. 2.1.2 The AI Approach The papers illustrating the AI approach [CRM89, Rai87, HR88] deal with text from highly restricted domains.
Reference: [Gil90] <editor> Peter Gillman, editor. </editor> <booktitle> Text Retrieval The State of the Art. Institute of Information Scientists, Taylor Graham, 1990. Proceedings of `The User's Perspective' (1988) and `Text Management' (1989). </booktitle>
Reference: [Glu89] <author> Robert J. Glushko. </author> <title> Transforming Text Into Hypertext for a Compact Disc Encyclopedia. </title> <booktitle> In CHI '89 `Wings for the Mind' , pages 293 - 298, </booktitle> <year> 1989. </year>
Reference-contexts: with only highly structured text; 2) an AI artificial intelligence approach employing semantic parsers and knowledge bases; and 3) an information retrieval (IR) approach involving statistics derived from the text. 2.1.1 Highly Structured Documents The Oxford English Dictionary (OED) Project [RT87, RT88] and the conversion of The Engineering Data Compendium <ref> [Glu89] </ref> are examples of the creation of hypertext links in existing highly structured traditional documents. In the case of the OED Project, typefaces and abbreviations made some of the connections, that would be represented as links, explicit in the original text.
Reference: [Gol86] <author> Lev Goldfarb. </author> <title> Metric data models and associated search strategies. </title> <journal> SIGIR Forum, </journal> <volume> 20(1 - 4):7 - 11, </volume> <month> Spring Summer </month> <year> 1986. </year>
Reference-contexts: A particular subsection is part of exactly one section. There may be many trees describing one document since the division into parts is often a matter of interpretation. Lev Goldfarb <ref> [Gol86] </ref> has proposed measuring the similarity of documents by the minimal number of steps required to convert the tree describing one document to a tree describing the other.
Reference: [Gol90] <author> Charles F. Goldfarb. </author> <title> The SGML Handbook. </title> <publisher> Clarendon Press, </publisher> <year> 1990. </year>
Reference-contexts: WWW is a distributed hypermedia system. The Web's collection of hypermedia documents are accessible through client software, called browsers, which connect to WWW servers over the Internet. Hypermedia documents in the WWW are marked-up in HTML [BLC], which is similar to SGML <ref> [Gol90] </ref>. I chose to implement my linking strategies by converting text documents into HTML form. Almost all previous research on automatically converting text into hypertext documents assumes that the text is highly structured and can thus be parsed by a computer. <p> Other researchers are working with electronic versions of text that have already been structured for use with computers. Part of the MAESTRO Project [Fah88, FB90] is to make links between texts marked-up in SGML <ref> [Gol90] </ref>. Legal documents with numbered sections, paragraphs, and clauses are a prime example of highly structured text [ACG91]. Some attention has been given to turning text marked-up in *roff format into hypertext [TW86, FPS89] with keywords identified by text highlighting, e.g., italicized words.
Reference: [HA87] <author> M. Horton and R. Adams. </author> <title> Standard for interchange of USENET messages. Request for Comments 1036, </title> <institution> Internet Engineering Task Force, </institution> <month> December </month> <year> 1987. </year> <note> Obsoletes RFC 850 [Hor83]. </note>
Reference-contexts: I have represented the news articles as two logical units: 1) the text body 2) a header containing information about the message and the delivery system used for it. In the case of my data, the `References' field in a header makes an explicit connection between articles <ref> [Hor83, HA87] </ref>. Figure 3.1 shows a typical message header with its structural links highlighted. Semantic links make connections between documents based on the similarity of their information content that was implicit in the original data collection. The semantic links in my implementation connect articles that contain the same technical terms. <p> II-1 What format does the raw data have? News postings (like those found in the following figures) consist of two parts: 1) a message consisting of text and 2) a header containing information about the message and the delivery system used for it <ref> [Hor83, HA87] </ref>. The header often contains lines tagged `Subject', `Summary', and `Keywords' to aid in indexing. However, in practice these lines are seldom useful because they are maintained haphazardly.
Reference: [Hal88] <author> Frank G. </author> <title> Halasz. Reflections on Notecards: Seven Issue for the Next Generation of Hypermedia Systems. </title> <journal> Communications of the ACM, </journal> <volume> 31(7):836 - 852, </volume> <month> July </month> <year> 1988. </year>
Reference-contexts: IR techniques are also applied to full text databases in which queries are evaluated with respect to entire documents. This process is illustrated in Figure 2.1 1 . There is a good deal of overlap between hypertext and IR, but the research areas are quite different <ref> [Hal88, CNBKO90, You90, Bas91] </ref>. Information Problem Documents Text Representation Query Retrieved Documents Representation Documents Indexed Figure adapted from: Figure 1 of `Hypertext and Information Retrieval: Croft, panel proposer. (Hypertext: Concepts, Systems and Applications, 1990). What are the Fundamental Concepts?' [A panel discussion] W.
Reference: [Har86] <author> Donna Harman. </author> <title> An experimental study of factors important in document ranking. </title> <booktitle> In SIGIR '86 , 1986. </booktitle> <pages> 58 </pages>
Reference-contexts: The use of real valued term weights provides a better way to distinguish between documents. The weights provide a representation of documents in space. Similarity measures are used to determine the distance between vectors in that space. 10 Weighting Terms Assigning weights to terms improves performance considerably <ref> [Spa72, Har86] </ref>. Karen Sparck Jones [Spa72] was the first to assign weights to terms by their frequency in the document collection. She found substantial improvement over boolean weighting when she assigned each term in a collection D of documents the weight defined in Equation 2.2-1. S. K. M. <p> Local Weightings Some local weighting schemes are: * raw term frequency <ref> [Dum91, Har86, Sal89] </ref>: f t * Binary or Boolean [Dum91] 8 &gt; &gt; &gt; &lt; 1 if f t 0 otherwise (2.2-6) log (f t 12 Salton and Buckley [SB91, p. 24] have produced weighting formulae for use with an electronic copy of an encyclopaedia.
Reference: [Har87] <author> Donna Harman. </author> <title> A failure analysis on the limitations of suffixing in an online environment. </title> <booktitle> In SIGIR '87 , 1987. </booktitle>
Reference-contexts: Stemming can often reduce the number of terms to be considered, but it is computationally intensive and requires an online dictionary. Donna Harman <ref> [Har87] </ref> found that stemming makes little difference in the performance of IR systems. I do not stem any words for my experiments. The terms were weighted with a combination of local log and global entropy weighting (see Section 2.2.3) as suggested by Dumais [Dum91].
Reference: [Har92] <author> Donna Harman. </author> <title> Evaluation issues in information retrieval. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 28(4):439 - 440, </volume> <month> July August </month> <year> 1992. </year>
Reference-contexts: Systems are often evaluated by the recall and precision values [Sal92, TS92] for standard queries on standard document collections such as Cranfield <ref> [Har92] </ref> or MEDLARS [Dum91]. Table 2.1 summarizes characteristics of databases composed of articles from: Time magazine, abstracts in the U.S. National Library of Medicine (MED), abstracts about aeronautics from the Cranfield collection, abstracts about library science and related fields (CISI and ADI), and the collection I use for my tests.
Reference: [Hen90] <editor> Diane Henderson, editor. </editor> <booktitle> Information in the Year 2000: From Research to Applications Proceedings of the 53rd ASIS Annual Meeting, volume 27, </booktitle> <address> Toronto, Ontario, </address> <month> 4 - 8 November </month> <year> 1990. </year> <title> Learned Information, </title> <publisher> Inc. </publisher>
Reference: [Hor83] <author> Mark R. Horton. </author> <title> Standard for interchange of USENET messages. Request for Comments 850, </title> <institution> Internet Engineering Task Force, </institution> <month> June </month> <year> 1983. </year> <note> Updated by RFC 1036 [HA87]. </note>
Reference-contexts: I have represented the news articles as two logical units: 1) the text body 2) a header containing information about the message and the delivery system used for it. In the case of my data, the `References' field in a header makes an explicit connection between articles <ref> [Hor83, HA87] </ref>. Figure 3.1 shows a typical message header with its structural links highlighted. Semantic links make connections between documents based on the similarity of their information content that was implicit in the original data collection. The semantic links in my implementation connect articles that contain the same technical terms. <p> II-1 What format does the raw data have? News postings (like those found in the following figures) consist of two parts: 1) a message consisting of text and 2) a header containing information about the message and the delivery system used for it <ref> [Hor83, HA87] </ref>. The header often contains lines tagged `Subject', `Summary', and `Keywords' to aid in indexing. However, in practice these lines are seldom useful because they are maintained haphazardly.
Reference: [HR88] <author> Udo Hahn and Ulrich Reimer. </author> <title> Automatic Generation of Hypertext Knowledge Bases. </title> <booktitle> In Proceedings of the ACM Conference on Office Information Systems , pages 182 - 188, </booktitle> <year> 1988. </year>
Reference-contexts: Programs to convert from a variety of structured formats to a marked-up form are available [BS93]. 2.1.2 The AI Approach The papers illustrating the AI approach <ref> [CRM89, Rai87, HR88] </ref> deal with text from highly restricted domains. They employ semantic parsers to find relationships in the texts. Among other problems, such systems require grammatically correct texts. These systems use knowledge bases to aid in the parsing of such texts. <p> It searches for citations that satisfy certain relationships between concepts that have been specified by the user. This is a significant improvement over systems that use only keywords. The approach used in the TOPIC system <ref> [Rai87, HR88] </ref> depends on the use of a semantic parser and explicit domain-specific knowledge bases. TOPIC represents the topical structure of the text in a hierarchical graph, with links based on a semantic understanding of thematically coherent chunks of the text. <p> The link-making part of TOPIC has been tested on a small number of texts (20 texts, each about 2000-3000 text tokens long). The authors note that their system has not been developed to create links within collections of thematically varied texts <ref> [HR88, p.188] </ref>. 2.1.3 The IR Approach My text consists of hundreds of files from many authors. An average file contains about 200 words. Also, the authors of these files did not restrict themselves to a small semantic domain. Files topics range from mathematical models to industrial rumours.
Reference: [JF87] <author> William P. Jones and George W. Furnas. </author> <title> Pictures of relevance: A geometric analysis of similarity measures. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 38(6):420 - 442, </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: The similarity of two documents in the vector model is computed by determining the distance between their corresponding vectors. Since the indexed terms are not necessarily independent the computed similarity measures may not be meaningful. They are often used in practice however. William Jones and George Furnas <ref> [JF87, p. 420] </ref> made a geometric analysis of similarity measures which they intended to complement an empirical analysis of similarity measures. They attempted to characterize the iso-similarity contours of the measures to determine the behaviour of the measures. The analyses in Subsection 2.3.1 summarize some of their work. <p> The Overlap Measure The overlap measure is used in the SMART system <ref> [JF87] </ref>. It is very sensitive to document length, long and short documents will both produce improper results. <p> Savoy [Sav93] created an IR system augmented with some hypertext links based on document to document similarity. He uses the poorly performing <ref> [JF87, p. 432] </ref> Dice measure to judge the similarity of documents and measures performance in terms of recall and precision. I considered several methods for evaluating the quality of the hypertext graphs that my program produced. I compare the semantic similarity and link distance between all pairs of documents.
Reference: [Jon90] <author> Kevin P. Jones. </author> <title> Natural-language processing and automatic indexing: a reply. </title> <journal> The Indexer, </journal> <volume> 17(2):114 - 115, </volume> <month> October </month> <year> 1990. </year>
Reference-contexts: The vocabulary problem in IR is that different people will often choose different words with which to express the same concept [FLGD87, KN90] or to index the same document <ref> [Jon90, KN90] </ref>. Systems are often evaluated by the recall and precision values [Sal92, TS92] for standard queries on standard document collections such as Cranfield [Har92] or MEDLARS [Dum91]. Table 2.1 summarizes characteristics of databases composed of articles from: Time magazine, abstracts in the U.S.
Reference: [KN90] <author> C. Korycinski and Alan F. Newell. </author> <title> Natural-language processing and automatic indexing. </title> <journal> The Indexer, </journal> <volume> 17(1):21 - 29, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: The vocabulary problem in IR is that different people will often choose different words with which to express the same concept <ref> [FLGD87, KN90] </ref> or to index the same document [Jon90, KN90]. Systems are often evaluated by the recall and precision values [Sal92, TS92] for standard queries on standard document collections such as Cranfield [Har92] or MEDLARS [Dum91]. <p> The vocabulary problem in IR is that different people will often choose different words with which to express the same concept [FLGD87, KN90] or to index the same document <ref> [Jon90, KN90] </ref>. Systems are often evaluated by the recall and precision values [Sal92, TS92] for standard queries on standard document collections such as Cranfield [Har92] or MEDLARS [Dum91]. Table 2.1 summarizes characteristics of databases composed of articles from: Time magazine, abstracts in the U.S.
Reference: [Kro92] <author> Ed Krol. </author> <title> The Whole Internet User's Guide & Catalog. </title> <publisher> O'Reilly & Associates, Inc., </publisher> <year> 1992. </year>
Reference-contexts: Some hypertext systems provide visual browsers to show the user a graphical representation of the database they are querying. The tkWWW program is a browser designed for use with the World Wide Web hypertext project <ref> [Kro92, chapter 13] </ref>.
Reference: [LH85] <author> Margaret E. Lundy and Richard A. Harshman. </author> <title> Reference Manual for the PARAFAC Analysis Package. </title> <publisher> Scientific Software Associates, </publisher> <address> 48 Wilson Avenue, London, Ontario, N6H 1X3, Canada, </address> <year> 1985. </year> <note> PARAFAC is available via anonymous ftp from phobos.sscl.uwo.ca. </note>
Reference: [MB93] <author> Elli Mylonas and Mark Bernstein. </author> <title> A literary apprentice. </title> <note> Submitted to Computing in the Humanities, 1993. 59 </note>
Reference-contexts: Conclusion In practice anything that normalizes for document length, e.g., the cosine measure is acceptable [Ber93]. Susan Dumais [Dum93] has found empirically that the cosine measure works best. I use the cosine measure for my experiments. 2.3.2 Other Methods Bloom Filter Apprentice Bernstein <ref> [Ber90, MB93] </ref> developed a novel method for using Bloom filters [Blo70] to compute the similarity of passages in monographs (see also Section 2.1.3). His `link apprentice' is part of an interactive program for reading texts, e.g., high school level history textbooks.
Reference: [Mea92] <author> Charles T. Meadow. </author> <title> Text Information Retrieval Systems. </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: The higher the magnitude of the product the greater the similarity. He also normalizes the results with respect to passage length [Ber93]. 2.2 Information Retrieval Most information retrieval work is done with bibliographic databases in mind <ref> [Mea92] </ref>. IR is usually thought of as a process of query refinement, in which a user queries a computerized library catalogue to find records of interest. <p> What are the Fundamental Concepts?' [A panel discussion] W. Bruce Comparison Relevance Feedback 2.2.1 Performance Evaluation There is not yet a satisfactory measure of the indexing effectiveness of an IR system <ref> [Mea92, p. 286] </ref>. Systems however are often evaluated using measures of precision and recall [Sha86, Sal68] or measures derived from them [Swe80, Mea92]. Recall is defined as the number of relevant database entries that match the query divided by the number of all database entries. <p> Bruce Comparison Relevance Feedback 2.2.1 Performance Evaluation There is not yet a satisfactory measure of the indexing effectiveness of an IR system [Mea92, p. 286]. Systems however are often evaluated using measures of precision and recall [Sha86, Sal68] or measures derived from them <ref> [Swe80, Mea92] </ref>. Recall is defined as the number of relevant database entries that match the query divided by the number of all database entries. Precision is the number of found records relevant to the query divided by the number of records found.
Reference: [MS88] <author> William Mendenhall and Terry Sincich. </author> <title> Statistics for the Engineering and Computer Sciences. </title> <publisher> Dellen Publishing Company, </publisher> <address> second edition, </address> <year> 1988. </year>
Reference-contexts: 33 semantic similarity computed with LSA 34 semantic similarity computed with LSA 35 semantic similarity computed with LSA 36 4.3 Evaluation Using Correlation Measures `The Pearson product moment coefficient of correlation r [Equation 4.3-1] is a measure of the strength of the linear relationship between two variables x and y.' <ref> [MS88, p. 445] </ref> It is used to estimate the true correlation for a population from a sample population.
Reference: [Nel87] <author> Theodor Holm Nelson. </author> <title> Literary Machines. </title> <booktitle> Privately published, The Distributors, </booktitle> <address> 702 South Michigan, South Bend IN 46618 USA, 87.1 edition, </address> <year> 1987. </year>
Reference-contexts: Introduction Hypertext documents are a way of presenting textual information <ref> [Con87, Nel87, Eas90] </ref>. Unlike traditional documents, such as books, hypertext documents are interactive and record explicitly the relationships between different parts of their text [DeR89]. Hypertext was first described as part of a machine called memex [Bus45] in 1945. <p> Unlike traditional documents, such as books, hypertext documents are interactive and record explicitly the relationships between different parts of their text [DeR89]. Hypertext was first described as part of a machine called memex [Bus45] in 1945. The term hypertext was coined by Theodore Nelson <ref> [Nel87] </ref> in the 1960's. 1.1 Advantages of Hypertext It is much easier to search for text in a hypertext document than in a book [ELK + 91]. To find details in a traditional book, one reads all of it or searches through the index.
Reference: [NTB84] <author> G.Th. Niedermair, G. Thurmair, and I. Buttel. </author> <title> Mars: A retrieval tool on the basis of morphological analysis. </title> <booktitle> In Research and Development in Information Retrieval , 1984. </booktitle>
Reference-contexts: The best known method is called stemming or morphological analysis. Using this method, `computer' and `computing' would both be indexed by `compute' <ref> [NTB84, Sal89] </ref>. Salton et al. have developed a method for choosing which words and groups of words to use for indexing a document collection [SYY74].
Reference: [Rab86] <editor> Fausto Rabitti, editor. </editor> <booktitle> 1986 | ACM Conference on Research and Development in Information Retrieval. ACM-SIGIR, ACM, </booktitle> <month> 8 - 10 September </month> <year> 1986. </year>
Reference: [Rai87] <author> Rainer Hammwohner and Ulrich Thiel. </author> <title> Content Oriented Relations between Text Units | a Structural Model for Hypertexts. </title> <booktitle> In Hypertext '87 Papers , pages 155 - 176, </booktitle> <year> 1987. </year>
Reference-contexts: In such cases, the links between words and entries have already been made by humans, based on an understanding of the text. Although there has been great interest in the automatic conversion of general texts into hypertext documents, little work has been done to date <ref> [Rai87, CRM89, Ber90] </ref>. Chapter 2 Background 2.1 Previous Work Automatic conversion of text into a hypertext document is an interesting open problem [Bal93]. <p> Programs to convert from a variety of structured formats to a marked-up form are available [BS93]. 2.1.2 The AI Approach The papers illustrating the AI approach <ref> [CRM89, Rai87, HR88] </ref> deal with text from highly restricted domains. They employ semantic parsers to find relationships in the texts. Among other problems, such systems require grammatically correct texts. These systems use knowledge bases to aid in the parsing of such texts. <p> It searches for citations that satisfy certain relationships between concepts that have been specified by the user. This is a significant improvement over systems that use only keywords. The approach used in the TOPIC system <ref> [Rai87, HR88] </ref> depends on the use of a semantic parser and explicit domain-specific knowledge bases. TOPIC represents the topical structure of the text in a hierarchical graph, with links based on a semantic understanding of thematically coherent chunks of the text.
Reference: [RT87] <author> Darrell R. Raymond and Frank Wm. Tompa. </author> <title> Hypertext and the New Oxford English Dictionary. </title> <booktitle> In Hypertext '87 Papers , pages 143 - 153, </booktitle> <year> 1987. </year>
Reference-contexts: I identify three different approaches: 1) working with only highly structured text; 2) an AI artificial intelligence approach employing semantic parsers and knowledge bases; and 3) an information retrieval (IR) approach involving statistics derived from the text. 2.1.1 Highly Structured Documents The Oxford English Dictionary (OED) Project <ref> [RT87, RT88] </ref> and the conversion of The Engineering Data Compendium [Glu89] are examples of the creation of hypertext links in existing highly structured traditional documents.
Reference: [RT88] <author> Darrell R. Raymond and Frank Wm. Tompa. </author> <title> Hypertext and the Oxford English Dictionary. </title> <journal> Communications of the ACM, </journal> <volume> 31(7):871 - 879, </volume> <month> July </month> <year> 1988. </year>
Reference-contexts: I identify three different approaches: 1) working with only highly structured text; 2) an AI artificial intelligence approach employing semantic parsers and knowledge bases; and 3) an information retrieval (IR) approach involving statistics derived from the text. 2.1.1 Highly Structured Documents The Oxford English Dictionary (OED) Project <ref> [RT87, RT88] </ref> and the conversion of The Engineering Data Compendium [Glu89] are examples of the creation of hypertext links in existing highly structured traditional documents. <p> Also they claim that if the system is fast enough, then users would benefit more by exploring many alternative paths through the document than by following the guidance provided by a few well-chosen links <ref> [RT88, pp. 877-8] </ref>. Other researchers are working with electronic versions of text that have already been structured for use with computers. Part of the MAESTRO Project [Fah88, FB90] is to make links between texts marked-up in SGML [Gol90]. <p> The longest path in the graph might be an important indicator of the quality of the hypertext document. Shorter maximal paths will permit more browsing as suggested by Raymond and Tompa <ref> [RT88, pp. 877-8] </ref>. 5.3 Which Method Is Best An examination of Tables 4.1 - 4.3 shows that the link distances in hypertext graphs built using a combination of Entropy and local log weights corresponds best with the measure of semantic similarity.
Reference: [RW83] <author> Vijay V. Raghavan and S.K.M. Wong. </author> <title> A critical analysis of vector space model for information retrieval. </title> <booktitle> In Research and Development in Information Retrieval , 1983. </booktitle>
Reference-contexts: The filters are built by hashing on each individual word of each page. Bernstein reports great success with the apprentice but has not reported any formal analysis of its performance. Latent Semantic Analysis V. Raghavan and S. K. M. Wong <ref> [RW83, RW86] </ref> have analysed the theoretical underpinnings of the vector space model. In the model, a collection of documents is represented as a matrix of vectors of terms weights within documents. They suggest that a true matrix 15 model would allow arithmetic manipulation of the vectors to form new matrices.
Reference: [RW86] <author> Vijay V. Raghavan and S.K.M. Wong. </author> <title> A critical analysis of vector space model for information retrieval. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 37(5):279 - 287, </volume> <month> September </month> <year> 1986. </year>
Reference-contexts: Although the vector model does not have a strong theoretical basis <ref> [RW86, WZW85] </ref>, it has been used extensively [Sal91]. The model is often used with the implicit assumption that terms are independent [Dum91, WZW85], so jT j terms can be represented by jT j orthogonal dimensions. Tree Models Documents may be described by trees, i.e., directed acyclic graphs. <p> The filters are built by hashing on each individual word of each page. Bernstein reports great success with the apprentice but has not reported any formal analysis of its performance. Latent Semantic Analysis V. Raghavan and S. K. M. Wong <ref> [RW83, RW86] </ref> have analysed the theoretical underpinnings of the vector space model. In the model, a collection of documents is represented as a matrix of vectors of terms weights within documents. They suggest that a true matrix 15 model would allow arithmetic manipulation of the vectors to form new matrices.
Reference: [Sal68] <author> Gerard Salton. </author> <title> Automatic Information Organization and Retrieval. </title> <publisher> McGraw-Hill computer science. McGraw-Hill Book Company, </publisher> <year> 1968. </year>
Reference-contexts: What are the Fundamental Concepts?' [A panel discussion] W. Bruce Comparison Relevance Feedback 2.2.1 Performance Evaluation There is not yet a satisfactory measure of the indexing effectiveness of an IR system [Mea92, p. 286]. Systems however are often evaluated using measures of precision and recall <ref> [Sha86, Sal68] </ref> or measures derived from them [Swe80, Mea92]. Recall is defined as the number of relevant database entries that match the query divided by the number of all database entries. Precision is the number of found records relevant to the query divided by the number of records found.
Reference: [Sal89] <author> G. Salton. </author> <title> Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year> <month> 60 </month>
Reference-contexts: He was not able however to perform rigorous testing of the idea. G. Salton and Chris Buckley [SB90] propose using clustering as part of a linking strategy to improve relevance. 2.2.3 IR Techniques Choosing Terms G. Salton, C. S. Yang, and C. T. Yu <ref> [Sal89, SYY74] </ref> have developed methods for choosing words for indexing documents. The following explanation is derived from Salton [Sal89, Chapter 9]. For a list of individual words in all documents, the method is to select words and collections of words, that discriminate between the documents. <p> Salton and Chris Buckley [SB90] propose using clustering as part of a linking strategy to improve relevance. 2.2.3 IR Techniques Choosing Terms G. Salton, C. S. Yang, and C. T. Yu [Sal89, SYY74] have developed methods for choosing words for indexing documents. The following explanation is derived from Salton <ref> [Sal89, Chapter 9] </ref>. For a list of individual words in all documents, the method is to select words and collections of words, that discriminate between the documents. Words with very high or very low frequency are eliminated from consideration as indexing terms at the beginning of the process. <p> For the purposes of indexing, low frequency words are replaced by their equivalent word in a thesaurus. For example, `acquiescence' and `compliance' might both be indexed as though they were `consent' <ref> [Sal89, after Table 9.5] </ref>. Some high frequency words are combined to form low frequency phrases that can be used to better distinguish between documents. The selection of terms is used to model the document as a boolean vector. <p> weighting when she as signed each term in a collection D of documents the weight of dlog 2 jDje dlog 2 f t D e + 1 (2.2-1) where 2 f t D is the frequency of term t in the collection of jDj documents. * Inverse Document Frequency (Idf) <ref> [Dum91, Sal89] </ref> Frequency-based indexing models are based on the premise that the best indexing terms `are those that occur frequently in individual documents but rarely in the remainder of the collection' [Sal89, p. 280]. <p> t D is the frequency of term t in the collection of jDj documents. * Inverse Document Frequency (Idf) [Dum91, Sal89] Frequency-based indexing models are based on the premise that the best indexing terms `are those that occur frequently in individual documents but rarely in the remainder of the collection' <ref> [Sal89, p. 280] </ref>. <p> This is the inspiration for the inverse document frequency weighting: 2 A summary of notation used throughout this document begins on page xi. 11 jD t j + 1 (2.2-2) * GfIdf [Dum91] f t jD t j * entropy [Dum91] The entropy weighting scheme was developed from information theory <ref> [Sal89] </ref>. 1 d d D d D log (jDj) * term-discrimination value (TDV) [Sal89, pp.281 - 4] The term-discrimination value is a sophisticated measurement of the distinguishing power of each term. <p> of notation used throughout this document begins on page xi. 11 jD t j + 1 (2.2-2) * GfIdf [Dum91] f t jD t j * entropy [Dum91] The entropy weighting scheme was developed from information theory [Sal89]. 1 d d D d D log (jDj) * term-discrimination value (TDV) <ref> [Sal89, pp.281 - 4] </ref> The term-discrimination value is a sophisticated measurement of the distinguishing power of each term. <p> Local Weightings Some local weighting schemes are: * raw term frequency <ref> [Dum91, Har86, Sal89] </ref>: f t * Binary or Boolean [Dum91] 8 &gt; &gt; &gt; &lt; 1 if f t 0 otherwise (2.2-6) log (f t 12 Salton and Buckley [SB91, p. 24] have produced weighting formulae for use with an electronic copy of an encyclopaedia. <p> The best known method is called stemming or morphological analysis. Using this method, `computer' and `computing' would both be indexed by `compute' <ref> [NTB84, Sal89] </ref>. Salton et al. have developed a method for choosing which words and groups of words to use for indexing a document collection [SYY74].
Reference: [Sal91] <author> Gerard Salton. </author> <title> Developments in Automatic Text Retrieval. </title> <journal> Science, </journal> <volume> 253, </volume> <month> 30 August </month> <year> 1991. </year>
Reference-contexts: Although the vector model does not have a strong theoretical basis [RW86, WZW85], it has been used extensively <ref> [Sal91] </ref>. The model is often used with the implicit assumption that terms are independent [Dum91, WZW85], so jT j terms can be represented by jT j orthogonal dimensions. Tree Models Documents may be described by trees, i.e., directed acyclic graphs. <p> Clustering is a method by which documents are grouped into classes by topic <ref> [Sal91] </ref>. This is accomplished by comparing documents to the pseudo-document that is a perfect representative of a class. This is similar to the vector space concept of a centroid, a single document that represents the average values of all the documents in the collection. Frank B. <p> S. K. M. Wong and Y. Y. Yao [WY92] performed a more thorough analysis and testing than Sparck Jones and concluded that a probabilistic logarithmic measure performs even better. Their formulae are exceedingly complex and are not reproduced here. Salton notes that such methods are rarely practical <ref> [Sal91, p. 975] </ref>. Term weighting schemes are used to increase recall and precision [Sha86]. Typically a term's weight in a document is computed from the product of the measure of the term's importance in the document (its local weight) and its importance in the entire collection (its global weight).
Reference: [Sal92] <author> Gerard Salton. </author> <title> The state of retrieval system evaluation. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 28(4):441 - 449, </volume> <month> July August </month> <year> 1992. </year>
Reference-contexts: The vocabulary problem in IR is that different people will often choose different words with which to express the same concept [FLGD87, KN90] or to index the same document [Jon90, KN90]. Systems are often evaluated by the recall and precision values <ref> [Sal92, TS92] </ref> for standard queries on standard document collections such as Cranfield [Har92] or MEDLARS [Dum91]. Table 2.1 summarizes characteristics of databases composed of articles from: Time magazine, abstracts in the U.S.
Reference: [Sav93] <author> Jacques Savoy. </author> <title> Effectiveness of information retrieval systems used in a hypertext environment. </title> <journal> Hypermedia, </journal> <volume> 5(1):23 - 46, </volume> <year> 1993. </year>
Reference-contexts: Preliminary experiments with graphs of differing degrees showed little improvement when more than 5 links were allowed from a document. This conclusion was also reached by Jacques Savoy <ref> [Sav93, p. 42] </ref> in a different experiment (see Section 3.1.3). First I chose the terms that the links would be formed upon. <p> For example, dp (0:9) is the smallest number of links the user must be allowed to traverse, to ensure a 90% probability of navigating from an image d 1 to an image d 2 . Savoy <ref> [Sav93] </ref> created an IR system augmented with some hypertext links based on document to document similarity. He uses the poorly performing [JF87, p. 432] Dice measure to judge the similarity of documents and measures performance in terms of recall and precision. <p> A method for weighting terms must be chosen and the number of out-links to permit each document to have must be decided upon. Savoy <ref> [Sav93, p. 41] </ref> reports that allowing a document to have five out-links produces better performance than restricting the number of out-links to one. This is in keeping with my findings.
Reference: [SB90] <author> Gerard Salton and Chris Buckley. </author> <title> Approaches to global text analysis. </title> <booktitle> In ASIS '90 Proceedings of the 51st Annual Meeting , pages 228 - 233, </booktitle> <year> 1990. </year>
Reference-contexts: Frank B. Baker [Bak62] proposed using a probabilistic approach for this. He was not able however to perform rigorous testing of the idea. G. Salton and Chris Buckley <ref> [SB90] </ref> propose using clustering as part of a linking strategy to improve relevance. 2.2.3 IR Techniques Choosing Terms G. Salton, C. S. Yang, and C. T. Yu [Sal89, SYY74] have developed methods for choosing words for indexing documents. The following explanation is derived from Salton [Sal89, Chapter 9].
Reference: [SB91] <author> Gerard Salton and Chris Buckley. </author> <title> Automatic text structuring and retrieval | experiments in automatic encyclopedia searching. </title> <booktitle> In SIGIR '91 , pages 21 - 30, </booktitle> <year> 1991. </year>
Reference-contexts: Local Weightings Some local weighting schemes are: * raw term frequency [Dum91, Har86, Sal89]: f t * Binary or Boolean [Dum91] 8 &gt; &gt; &gt; &lt; 1 if f t 0 otherwise (2.2-6) log (f t 12 Salton and Buckley <ref> [SB91, p. 24] </ref> have produced weighting formulae for use with an electronic copy of an encyclopaedia. Their formulae are neither local nor global but a combination of the two. They use different formulae for comparing sentences, paragraphs and whole entries.
Reference: [Sha86] <author> W. M. Shaw, Jr. </author> <title> The foundation of evaluation. </title> <journal> Journal of the American Society For Information Science, </journal> <volume> 37(5):346 - 348, </volume> <month> September </month> <year> 1986. </year>
Reference-contexts: What are the Fundamental Concepts?' [A panel discussion] W. Bruce Comparison Relevance Feedback 2.2.1 Performance Evaluation There is not yet a satisfactory measure of the indexing effectiveness of an IR system [Mea92, p. 286]. Systems however are often evaluated using measures of precision and recall <ref> [Sha86, Sal68] </ref> or measures derived from them [Swe80, Mea92]. Recall is defined as the number of relevant database entries that match the query divided by the number of all database entries. Precision is the number of found records relevant to the query divided by the number of records found. <p> Their formulae are exceedingly complex and are not reproduced here. Salton notes that such methods are rarely practical [Sal91, p. 975]. Term weighting schemes are used to increase recall and precision <ref> [Sha86] </ref>. Typically a term's weight in a document is computed from the product of the measure of the term's importance in the document (its local weight) and its importance in the entire collection (its global weight).
Reference: [Spa72] <author> Karen Sparck Jones. </author> <title> A statistical interpretation of term specificity and its application in retrieval. </title> <journal> Journal of Documentation, </journal> <volume> 28(1):11 - 21, </volume> <month> March </month> <year> 1972. </year>
Reference-contexts: The use of real valued term weights provides a better way to distinguish between documents. The weights provide a representation of documents in space. Similarity measures are used to determine the distance between vectors in that space. 10 Weighting Terms Assigning weights to terms improves performance considerably <ref> [Spa72, Har86] </ref>. Karen Sparck Jones [Spa72] was the first to assign weights to terms by their frequency in the document collection. She found substantial improvement over boolean weighting when she assigned each term in a collection D of documents the weight defined in Equation 2.2-1. S. K. M. <p> The weights provide a representation of documents in space. Similarity measures are used to determine the distance between vectors in that space. 10 Weighting Terms Assigning weights to terms improves performance considerably [Spa72, Har86]. Karen Sparck Jones <ref> [Spa72] </ref> was the first to assign weights to terms by their frequency in the document collection. She found substantial improvement over boolean weighting when she assigned each term in a collection D of documents the weight defined in Equation 2.2-1. S. K. M. Wong and Y. Y. <p> The global frequency of term t is denoted by f t D . The number of documents in the collection is jDj. The number of terms in the collection is jT j. Global Weightings Some global weighting schemes are: * Specificity <ref> [Spa72] </ref> Sparck Jones found substantial improvement over boolean weighting when she as signed each term in a collection D of documents the weight of dlog 2 jDje dlog 2 f t D e + 1 (2.2-1) where 2 f t D is the frequency of term t in the collection of
Reference: [SRA90] <editor> N. Streitz, A. Rizk, and J. Andre, editors. </editor> <booktitle> Hypertext: Concepts, Systems and Applications, The Cambridge Series on Electronic Publishing. </booktitle> <address> INRIA, France, </address> <publisher> Cambridge University Press, </publisher> <month> November </month> <year> 1990. </year> <booktitle> Proceedings of the First European Conference on Hypertext. </booktitle>
Reference: [Swe80] <author> John A. Swets. </author> <title> Effectiveness of information retrieval methods. </title> <editor> In Belver C. Griffith, editor, </editor> <booktitle> Key Papers in Information Science, </booktitle> <pages> pages 349 - 367. </pages> <publisher> Knowledge Industry Publications, Inc., </publisher> <address> White Plains, New York, </address> <year> 1980. </year>
Reference-contexts: Bruce Comparison Relevance Feedback 2.2.1 Performance Evaluation There is not yet a satisfactory measure of the indexing effectiveness of an IR system [Mea92, p. 286]. Systems however are often evaluated using measures of precision and recall [Sha86, Sal68] or measures derived from them <ref> [Swe80, Mea92] </ref>. Recall is defined as the number of relevant database entries that match the query divided by the number of all database entries. Precision is the number of found records relevant to the query divided by the number of records found.
Reference: [SYY74] <author> G. Salton, C.S. Yang, and C.T. Yu. </author> <title> A theory of term importance in automatic text analysis. </title> <type> TR 74-208, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> July </month> <year> 1974. </year>
Reference-contexts: He was not able however to perform rigorous testing of the idea. G. Salton and Chris Buckley [SB90] propose using clustering as part of a linking strategy to improve relevance. 2.2.3 IR Techniques Choosing Terms G. Salton, C. S. Yang, and C. T. Yu <ref> [Sal89, SYY74] </ref> have developed methods for choosing words for indexing documents. The following explanation is derived from Salton [Sal89, Chapter 9]. For a list of individual words in all documents, the method is to select words and collections of words, that discriminate between the documents. <p> The best known method is called stemming or morphological analysis. Using this method, `computer' and `computing' would both be indexed by `compute' [NTB84, Sal89]. Salton et al. have developed a method for choosing which words and groups of words to use for indexing a document collection <ref> [SYY74] </ref>. They use the discrimination value analysis (Equation 2.2-5). 2.3 Similarity Measures Similarity measures in IR are usually computed between a query and a document but can also be computed between documents. Most of these methods work with the vector model.
Reference: [Tan89] <author> Andrew S. Tanenbaum. </author> <title> Computer Networks. </title> <publisher> Prentice Hall, </publisher> <address> second edition, </address> <year> 1989. </year> <month> 61 </month>
Reference-contexts: Without proper indexing and searching techniques, the information is difficult to find and use. In 1989, Andrew Tanenbaum estimated that Usenet was probably the largest computer network in the world <ref> [Tan89] </ref>. Messages sent over Usenet form part of an important information resource. With an average of 16 Mb of new messages available from Usenet every day, it is clear that anyone wanting to find material will need some form of cross-indexing and searching.
Reference: [TS92] <author> Jean Tague-Sutcliffe. </author> <title> The pragmatics of inforamtion retrieval experimentation, revisited. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 28(4):467 - 490, </volume> <month> July August </month> <year> 1992. </year>
Reference-contexts: The vocabulary problem in IR is that different people will often choose different words with which to express the same concept [FLGD87, KN90] or to index the same document [Jon90, KN90]. Systems are often evaluated by the recall and precision values <ref> [Sal92, TS92] </ref> for standard queries on standard document collections such as Cranfield [Har92] or MEDLARS [Dum91]. Table 2.1 summarizes characteristics of databases composed of articles from: Time magazine, abstracts in the U.S.
Reference: [TW86] <author> Randall H. Trigg and Mark Weisner. TEXTNET: </author> <title> A Network-Based Approach to Text Handling. </title> <journal> ACM Transactions on Office Information Systems, </journal> <volume> 4(1):1 - 23, </volume> <month> January </month> <year> 1986. </year>
Reference-contexts: Part of the MAESTRO Project [Fah88, FB90] is to make links between texts marked-up in SGML [Gol90]. Legal documents with numbered sections, paragraphs, and clauses are a prime example of highly structured text [ACG91]. Some attention has been given to turning text marked-up in *roff format into hypertext <ref> [TW86, FPS89] </ref> with keywords identified by text highlighting, e.g., italicized words. Programs to convert from a variety of structured formats to a marked-up form are available [BS93]. 2.1.2 The AI Approach The papers illustrating the AI approach [CRM89, Rai87, HR88] deal with text from highly restricted domains.
Reference: [vR84] <editor> C.J. van Rijsbergen, editor. </editor> <booktitle> Research and Development in Information Retrieval, British Computer Society Workshop, </booktitle> <address> King's College, Cambridge, </address> <month> 2 - 6 July </month> <year> 1984. </year> <booktitle> Proceedings of the third joint BCS and ACM symposium. </booktitle>
Reference: [Wan92] <author> Joseph Wang. </author> <note> ANNOUNCING: tkWWW 0.3 Alpha. Usenet Message-ID h1992Aug30.232318.13727@athena.mit.edui, cross-posted to comp.lang.tcl, </note> <author> alt.hypertext, comp.windows.x and comp.lang.sgml, </author> <month> August </month> <year> 1992. </year>
Reference-contexts: The tkWWW program is a browser designed for use with the World Wide Web hypertext project [Kro92, chapter 13]. The World Wide Web project `seeks to build a world wide network of hypertext links.' <ref> [Wan92] </ref> Figure I.1 contains the list of commands available from a non-graphical interface to the World Wide Web system. 46 47 (1) Next (2) Back (3) go Up (4) Find page page to previous keywords in document WWW index (5) List (6) Recall (7) go to Top (8) go to End
Reference: [WY92] <author> S.K.M. Wong and Y.Y. Yao. </author> <title> An information-theoretic measure of term speci-fity. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 43(1):54 - 61, </volume> <month> January </month> <year> 1992. </year>
Reference-contexts: She found substantial improvement over boolean weighting when she assigned each term in a collection D of documents the weight defined in Equation 2.2-1. S. K. M. Wong and Y. Y. Yao <ref> [WY92] </ref> performed a more thorough analysis and testing than Sparck Jones and concluded that a probabilistic logarithmic measure performs even better. Their formulae are exceedingly complex and are not reproduced here. Salton notes that such methods are rarely practical [Sal91, p. 975].
Reference: [WZW85] <author> S. K. M. Wong, Wojciech Ziarki, and Patrick C. N. Wong. </author> <title> Generalized vector space model in information retrieval. </title> <booktitle> In SIGIR '85 , 1985. </booktitle>
Reference-contexts: Although the vector model does not have a strong theoretical basis <ref> [RW86, WZW85] </ref>, it has been used extensively [Sal91]. The model is often used with the implicit assumption that terms are independent [Dum91, WZW85], so jT j terms can be represented by jT j orthogonal dimensions. Tree Models Documents may be described by trees, i.e., directed acyclic graphs. <p> Although the vector model does not have a strong theoretical basis [RW86, WZW85], it has been used extensively [Sal91]. The model is often used with the implicit assumption that terms are independent <ref> [Dum91, WZW85] </ref>, so jT j terms can be represented by jT j orthogonal dimensions. Tree Models Documents may be described by trees, i.e., directed acyclic graphs. Tree structures are a natural way to represent the hierarchical nature of many documents.
Reference: [You90] <author> Laura De Young. </author> <title> Linking Considered Harmful. In Hypertext: </title> <booktitle> Concepts, Systems and Applications , pages 238 - 249, </booktitle> <year> 1990. </year>
Reference-contexts: IR techniques are also applied to full text databases in which queries are evaluated with respect to entire documents. This process is illustrated in Figure 2.1 1 . There is a good deal of overlap between hypertext and IR, but the research areas are quite different <ref> [Hal88, CNBKO90, You90, Bas91] </ref>. Information Problem Documents Text Representation Query Retrieved Documents Representation Documents Indexed Figure adapted from: Figure 1 of `Hypertext and Information Retrieval: Croft, panel proposer. (Hypertext: Concepts, Systems and Applications, 1990). What are the Fundamental Concepts?' [A panel discussion] W.

References-found: 89

