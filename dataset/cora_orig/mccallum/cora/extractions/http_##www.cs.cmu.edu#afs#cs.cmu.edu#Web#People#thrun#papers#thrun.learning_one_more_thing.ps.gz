URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/thrun.learning_one_more_thing.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/full.html
Root-URL: http://www.cs.cmu.edu
Title: Learning One More Thing  
Author: Sebastian Thrun Tom M. Mitchell 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: September 1994  
Pubnum: CMU-CS-94-184  
Abstract: Most research on machine learning has focused on scenarios in which a learner faces a single, isolated learning task. The lifelong learning framework assumes instead that the learner encounters a multitude of related learning tasks over its lifetime, providing the opportunity for the transfer of knowledge. This paper studies lifelong learning in the context of binary classification. It presents the invariance approach, in which knowledge is transferred via a learned model of the invariances of the domain. Results on learning to recognize objects from color images demonstrate superior generalization capabilities if invariances are learned and used to bias subsequent learning. This research is sponsored in part by the National Science Foundation under award IRI-9313367, and by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel Command, USAF, and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330. Views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing official policies or endorsements, either expressed or implied, of NSF, Wright Laboratory or the United States Government. 
Abstract-found: 1
Intro-found: 1
Reference: [ Atkeson, 1991 ] <author> Christopher A. Atkeson. </author> <title> Using locally weighted regression for robot learning. </title> <booktitle> In Proceedings of the 1991 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 958-962, </pages> <address> Sacramento, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: A variety of approaches aim to change the bias of an inductive function approximator in a more direct way. For example, Sutton [ Sutton, 1992 ] describes an approach that employs Kalman filters to determine optimal learning rates. Atkeson <ref> [ Atkeson, 1991 ] </ref> proposes techniques for optimizing the distance metric in a nearest neighbor generalizer. In [ Maron and Moore, 1994 ] , an incremental method for the selection of nearest neighbor models is described.
Reference: [ Barto et al., to appear ] <author> Andrew G. Barto, Steven J. Bradtke, and Satinder P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <note> to appear. </note>
Reference-contexts: A popular method for learning control is reinforcement learning <ref> [ Barto et al., to appear ] </ref> , [ Sutton, 1990 ] , [ Watkins and Dayan, 1992 ] . Reinforcement learning constructs value function that can be used to select optimal actions.
Reference: [ Caruana and Freitag, 1994 ] <author> Rich Caruana and Dayne Freitag. </author> <title> Greedy attribute selection. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: All these approaches develop better internal representations of the data by considering multiple functions in F with the goal of improving generalization. 14 * Spotting relevant features. Another approach, which bears close resemblance to learning invariances and learning representations, is to spot irrelevant features [ Littlestone, 1987 ] , <ref> [ Caruana and Freitag, 1994 ] </ref> . If the set of target functions F is such thatacross the boardonly a subset the features is relevant (e.g., the time of day may not matter for object recognition), a learning system can employ support sets to find the most relevant features.
Reference: [ Caruana, 1993 ] <author> Richard Caruana. </author> <title> Multitask learning: A knowledge-based of source of inductive bias. </title> <editor> In Paul E. Utgoff, editor, </editor> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 41-48, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 18 </pages>
Reference-contexts: In order to improve these results, we applied a learning technique that focusses learning by incorporating additional training information, adopted from [ Suddarth and Kergosien, 1990 ] , <ref> [ Caruana, 1993 ] </ref> . Their technique rests on the assumption that in addition to the learning task of interest, some related learning tasks, using the same input representation and the same training data (with different target values), are available. <p> In his approach, multiple, related tasks are trained simultaneously in a single neural network, forcing the networks to share hidden units. He reports that hidden internal representations are developed which lead to improved generalization <ref> [ Caruana, 1993 ] </ref> . Notice that these results match our findings when training the invariance network. All these approaches develop better internal representations of the data by considering multiple functions in F with the goal of improving generalization. 14 * Spotting relevant features.
Reference: [ Hild and Waibel, 1993 ] <author> Hermann Hild and Alex Waibel. </author> <title> Multi-speaker/speaker-independent architectures for the multi-state time delay neural network. </title> <booktitle> In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages II 255-258. </pages> <publisher> IEEE, </publisher> <month> April </month> <year> 1993. </year>
Reference-contexts: This is the case, for example, in approaches to speaker adaptation. Speaker adaptation comprises a family of techniques studied in speech recognition, in which a computer quickly adapts to the accent, voice, pitch, or speed of an individual speaker (see <ref> [ Hild and Waibel, 1993 ] </ref> for an example). Typically, speech is translated to a more machine-understandable speech by a user-specific module that allows quick adaptation. Speaker adaptation is an example of an approach in which training data X is adapted to fit previously learned modules.
Reference: [ Littlestone, 1987 ] <author> Nick Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1987. </year>
Reference-contexts: All these approaches develop better internal representations of the data by considering multiple functions in F with the goal of improving generalization. 14 * Spotting relevant features. Another approach, which bears close resemblance to learning invariances and learning representations, is to spot irrelevant features <ref> [ Littlestone, 1987 ] </ref> , [ Caruana and Freitag, 1994 ] .
Reference: [ Maron and Moore, 1994 ] <author> Oded Maron and Andrew W. Moore. </author> <title> Hoeffding races: Accelerating model selection search for classification and function approximation. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, Sutton [ Sutton, 1992 ] describes an approach that employs Kalman filters to determine optimal learning rates. Atkeson [ Atkeson, 1991 ] proposes techniques for optimizing the distance metric in a nearest neighbor generalizer. In <ref> [ Maron and Moore, 1994 ] </ref> , an incremental method for the selection of nearest neighbor models is described. Starting with a hypothesis set H, this technique uses training data and cross-validation to gradually reduce H.
Reference: [ Mitchell and Thrun, 1993 ] <author> Tom M. Mitchell and Sebastian B. Thrun. </author> <title> Explanation-based neural network learning for robot control. </title> <editor> In S. J. Hanson, J. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 287-294, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Based on these points the learner might generate the hypothesis h 1 . If the slopes are also known, the learner can do much better: h 2 . Prop algorithm [ Simard et al., 1992 ] and the explanation-based neural network learning (EBNN) algorithm <ref> [ Mitchell and Thrun, 1993 ] </ref> . Here we will refer to it as EBNN. Suppose we are given a training set X, and an invariance network which has been trained using a collection of support sets Y . We are now interested in learning f fl . <p> For example, control learning may benefit from training data collected in a classification domain, although the control function operates over different input and output spaces. For example, in <ref> [ Mitchell and Thrun, 1993 ] </ref> and [ Thrun, 1994 ] control learning is studied in a lifelong learning framework. <p> Reinforcement learning constructs value function that can be used to select optimal actions. Embedded in a lifelong learning framework, a control learning agent may face a variety of control learning tasks over its lifetime. As shown in <ref> [ Mitchell and Thrun, 1993 ] </ref> , learning action models, which are functions of the type g : S ! S; can significantly reduce the amount of training required for subsequent control learning tasks.
Reference: [ Mitchell et al., 1994 ] <author> Tom M. Mitchell, Joseph O'Sullivan, and Sebastian B. Thrun. </author> <title> Explanation-based learning for mobile robot perception. In Workshop on Robot Learning, </title> <booktitle> Eleventh Conference on Machine Learning, </booktitle> <year> 1994. </year>
Reference-contexts: This strategy, which has been found to be useful across a variety of domains <ref> [ Mitchell et al., 1994 ] </ref> , [ Thrun, 1994 ] resulted about equivalent performance (74.1%) in the object recognition domain. The reader should notice that all these results refer to the classification accuracy after 10,000 training epochs, using just one positive and one negative training example.
Reference: [ Pomerleau, 1989 ] <author> D. A. Pomerleau. ALVINN: </author> <title> an autonomous land vehicle in a neural network. </title> <type> Technical Report CMU-CS-89-107, </type> <institution> Computer Science Dept. Carnegie Mellon University, </institution> <address> Pitts-burgh PA, </address> <year> 1989. </year>
Reference-contexts: Simard and colleagues [ Simard et al., 1992 ] employed the Tangent-Prop algorithm for recognizing hand-printed letters. Since letter recognition should be invariant to translation and rotation, they manually provided zero-valued target slopes in the directions of these invariances, very much like those automatically generated by EBNN. Pomerleau <ref> [ Pomerleau, 1989 ] </ref> reports a similar technique that was used for training an autonomous vehicle. Based on knowledge about the relation of camera images and steering direction, he constructed additional training data that were used when training the network.
Reference: [ Pratt, 1993 ] <author> Lori Y. Pratt. </author> <title> Discriminability-based transfer between neural networks. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Other researchers report techniques to develop more appropriate hidden layer representations from multiple tasks. For example, Pratt proposed a method which transfered information by using an internal representation that was developed in earlier learning tasks <ref> [ Pratt, 1993 ] </ref> . A similar technique has been proposed in [ Sharkey and Sharkey, 1992 ] . A second example of learning internal representations using multiple target functions is Caruana's multi-task learning algorithm.
Reference: [ Quinlan, 1986 ] <author> J. Ross Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: For example, the set H could be the set of all artificial neural networks with 20 hidden units [ Rumelhart et al., 1986 ] , or, alternatively, the set of all decision trees with depth less than 10 <ref> [ Quinlan, 1986 ] </ref> . Throughout this paper, let us make the simplifying assumption that all functions in F are binary classifiers, by restricting the output to O = f0; 1g.
Reference: [ Rumelhart et al., 1986 ] <author> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing. </booktitle> <volume> Vol. I + II. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The learner has a set of hypotheses that it can consider, denoted by H, which might or might not be different from F . For example, the set H could be the set of all artificial neural networks with 20 hidden units <ref> [ Rumelhart et al., 1986 ] </ref> , or, alternatively, the set of all decision trees with depth less than 10 [ Quinlan, 1986 ] . <p> In the experiments described below, is approximated by training an artificial neural network using the Backpropagation algorithm <ref> [ Rumelhart et al., 1986 ] </ref> . Once has been learned, one way to mimic f fl is to pick an arbitrary positive training instance in X, and to use for classification, as described above.
Reference: [ Sharkey and Sharkey, 1992 ] <author> Noel E. Sharkey and Amanda J.C. Sharkey. </author> <title> Adaptive generalization and the transfer of knowledge. </title> <booktitle> In Proceedings of the Second Irish Neural Networks Conference, </booktitle> <address> Belfast, </address> <year> 1992. </year>
Reference-contexts: Other researchers report techniques to develop more appropriate hidden layer representations from multiple tasks. For example, Pratt proposed a method which transfered information by using an internal representation that was developed in earlier learning tasks [ Pratt, 1993 ] . A similar technique has been proposed in <ref> [ Sharkey and Sharkey, 1992 ] </ref> . A second example of learning internal representations using multiple target functions is Caruana's multi-task learning algorithm. In his approach, multiple, related tasks are trained simultaneously in a single neural network, forcing the networks to share hidden units.
Reference: [ Simard et al., 1992 ] <author> Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. </author> <title> Tangent prop a formalism for specifying selected invariances in an adaptive network. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 895-903, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Based on these points the learner might generate the hypothesis h 1 . If the slopes are also known, the learner can do much better: h 2 . Prop algorithm <ref> [ Simard et al., 1992 ] </ref> and the explanation-based neural network learning (EBNN) algorithm [ Mitchell and Thrun, 1993 ] . Here we will refer to it as EBNN. <p> Consequently, instead of fitting training examples of the type hi; f fl (i)i, EBNN fits training examples of the type hi; f fl (i); r i f fl (i)i: Gradient descent can be used to fit training examples of this type, as explained in <ref> [ Simard et al., 1992 ] </ref> . Fig. 1 illustrates the utility of this additional slope information in function fitting. Notice if multiple positive instances are available in X, slopes can be derived for each one of 5 1. <p> However, many of the approaches listed here have not been proposed in the context of learning more than one task. It should be noted that the invariance approach bears some resemblance to training schemes found in the context of autonomous driving and letter recognition. Simard and colleagues <ref> [ Simard et al., 1992 ] </ref> employed the Tangent-Prop algorithm for recognizing hand-printed letters. Since letter recognition should be invariant to translation and rotation, they manually provided zero-valued target slopes in the directions of these invariances, very much like those automatically generated by EBNN.
Reference: [ Suddarth and Kergosien, 1990 ] <author> Steven C. Suddarth and Y. L. Kergosien. </author> <title> Rule-injection hints as a means of improving network performance and learning time. </title> <booktitle> In Proceedings of the EURASIP Workshop on Neural Networks, </booktitle> <address> Sesimbra, Portugal, </address> <month> Feb </month> <year> 1990. </year> <note> EURASIP. 19 </note>
Reference-contexts: When applied to the two remaining unseen objects, the shoe and the glasses, the best invariance network classified only 53.2% of all image pairs correctly. In order to improve these results, we applied a learning technique that focusses learning by incorporating additional training information, adopted from <ref> [ Suddarth and Kergosien, 1990 ] </ref> , [ Caruana, 1993 ] . Their technique rests on the assumption that in addition to the learning task of interest, some related learning tasks, using the same input representation and the same training data (with different target values), are available.
Reference: [ Sutton, 1990 ] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <month> June </month> <year> 1990, </year> <pages> pages 216-224, </pages> <year> 1990. </year>
Reference-contexts: A popular method for learning control is reinforcement learning [ Barto et al., to appear ] , <ref> [ Sutton, 1990 ] </ref> , [ Watkins and Dayan, 1992 ] . Reinforcement learning constructs value function that can be used to select optimal actions. Embedded in a lifelong learning framework, a control learning agent may face a variety of control learning tasks over its lifetime.
Reference: [ Sutton, 1992 ] <author> Richard S. Sutton. </author> <title> Adapting bias by gradient descent: An incremental version of delta-bar-delta. </title> <booktitle> In Proceeding of Tenth National Conference on Artificial Intelligence AAAI-92, </booktitle> <pages> pages 171-176, </pages> <address> Menlo Park, CA, July 1992. </address> <publisher> AAAI, AAAI Press/The MIT Press. </publisher>
Reference-contexts: They differ in the way bias is represented, and in the assumption they make on the underlying function class F . A variety of approaches aim to change the bias of an inductive function approximator in a more direct way. For example, Sutton <ref> [ Sutton, 1992 ] </ref> describes an approach that employs Kalman filters to determine optimal learning rates. Atkeson [ Atkeson, 1991 ] proposes techniques for optimizing the distance metric in a nearest neighbor generalizer.
Reference: [ Thrun and Mitchell, 1993 ] <author> Sebastian B. Thrun and Tom M. Mitchell. </author> <title> Lifelong robot learning. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <year> 1993. </year> <note> (to appear). Also appeared as Technical Report IAI-TR-93-7, </note> <institution> University of Bonn, Dept. of Computer Science III. </institution>
Reference-contexts: Because these invariances are common to all functions in F , images showing other objects can provide additional information for learning about these invariances, and hence augment the training set X. This example illustrates the lifelong learning framework <ref> [ Thrun and Mitchell, 1993 ] </ref> . In lifelong learning, a collection of related learning problems is encountered over the lifetime of the system.
Reference: [ Thrun, 1994 ] <author> Sebastian B. Thrun. </author> <title> A lifelong learning perspective for mobile robot control. </title> <booktitle> In Proceedings of the IEEE/RSJ/GI International Conference on Intelligent Robots and Systems, </booktitle> <month> September </month> <year> 1994. </year> <note> (to appear). </note>
Reference-contexts: This strategy, which has been found to be useful across a variety of domains [ Mitchell et al., 1994 ] , <ref> [ Thrun, 1994 ] </ref> resulted about equivalent performance (74.1%) in the object recognition domain. The reader should notice that all these results refer to the classification accuracy after 10,000 training epochs, using just one positive and one negative training example. <p> For example, control learning may benefit from training data collected in a classification domain, although the control function operates over different input and output spaces. For example, in [ Mitchell and Thrun, 1993 ] and <ref> [ Thrun, 1994 ] </ref> control learning is studied in a lifelong learning framework.
Reference: [ Watkins and Dayan, 1992 ] <author> Christopher J.C.H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year> <month> 20 </month>
Reference-contexts: A popular method for learning control is reinforcement learning [ Barto et al., to appear ] , [ Sutton, 1990 ] , <ref> [ Watkins and Dayan, 1992 ] </ref> . Reinforcement learning constructs value function that can be used to select optimal actions. Embedded in a lifelong learning framework, a control learning agent may face a variety of control learning tasks over its lifetime.
References-found: 21

