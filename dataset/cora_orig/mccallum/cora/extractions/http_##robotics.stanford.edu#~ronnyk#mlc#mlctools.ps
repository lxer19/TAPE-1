URL: http://robotics.stanford.edu/~ronnyk/mlc/mlctools.ps
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: mlc@CS.Stanford.EDU  
Title: MLC A Machine Learning Library in C  
Author: Ron Kohavi George John Richard Long David Manley Karl Pfleger 
Address: Stanford, CA 94305  
Affiliation: Computer Science Department Stanford University  
Note: Appears in Tools with AI '94  
Abstract: We present MLC ++ , a library of C ++ classes and tools for supervised Machine Learning. While MLC ++ provides general learning algorithms that can be used by end users, the main objective is to provide researchers and experts with a wide variety of tools that can accelerate algorithm development, increase software reliability, provide comparison tools, and display information visually. More than just a collection of existing algorithms, MLC ++ is an attempt to extract commonalities of algorithms and decompose them for a unified view that is simple, coherent, and extensible. In this paper we discuss the problems MLC ++ aims to solve, the design of MLC ++ , and the current functionality. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. W. Aha, D. Kibler, and M. K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: To generate mul-tivariate trees with perceptrons at nodes, the induction algorithm can put perceptron categoriz ers at the nodes. Induction algorithms Induction algorithms induce categorizers. The library currently provides a majority inducer, a nearest-neighbor inducer <ref> [1] </ref>, an ID3-like decision tree inducer [16], and an inducer for oblivious read-once decision graphs [9]. Visualization tools From the outset, one of our top priorities was to provide visualization tools to the user. Graphical displays of datasets and induced concepts can provide key insights.
Reference: [2] <author> Dana Angluin. </author> <title> Computational learning theory: Survey and selected bibliography. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 351-369. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction Newton said he saw farther because he stood on the shoulders of giants. Computer programmers stand on each other's toes | James Coggins [5] In supervised machine learning <ref> [18, 2] </ref>, one tries to find a rule (a categorizer) that can be used to accurately predict the class label of novel instances. During the last decade, the machine learning community has developed a plethora of algorithms for this task.
Reference: [3] <author> Leo Breiman, Jerome H. Friedman, Richard A. Ol-shen, and Charles J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: Display of information In order to understand the problem better, and perhaps bias a learning algorithm, it is helpful to view the resulting structures (e.g., decision tree), or the data. Even commercially available programs such as C4.5 [17] and CART <ref> [3] </ref> give only a rudimentary display of an induced tree. A library providing a common framework and basic tools for implementing learning algorithms would alleviate the pain involved in programming from scratch.
Reference: [4] <author> Frederick P. Brooks. </author> <title> No silver bullets. </title> <editor> In H. J. Ku-gler, editor, </editor> <booktitle> Information Processing. </booktitle> <publisher> Elsevier Science Publishers, North Holland, </publisher> <year> 1986. </year> <note> Reprinted in Unix Review November 1987. </note>
Reference-contexts: Profiling is done regularly to improve efficiency. 4 Summary The most radical possible solution for constructing software is not to construct it at all. : : : The key issue, of course, is applicability. Can I use an avail able off-the-shelf package to perform my task? | Frederick Brooks <ref> [4] </ref> MLC ++ is an attempt at providing such an off-the-shelf package to researchers and users of machine learning algorithms. We have described several problems researchers in machine learning currently face, and we believe that these problems can be solved with the right tool.
Reference: [5] <author> James Coggins. </author> <title> Designing C++ libraries. </title> <journal> The C++ Journal, </journal> <volume> 1(1) </volume> <pages> 25-32, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Newton said he saw farther because he stood on the shoulders of giants. Computer programmers stand on each other's toes | James Coggins <ref> [5] </ref> In supervised machine learning [18, 2], one tries to find a rule (a categorizer) that can be used to accurately predict the class label of novel instances. During the last decade, the machine learning community has developed a plethora of algorithms for this task.
Reference: [6] <author> E. R. Gansner, E. Koutsofios, S. C. North, and K. P. Vo. </author> <title> A technique for drawing directed graphs. </title> <journal> In IEEE Transactions on Software Engineering, </journal> <pages> pages 214-230, </pages> <year> 1993. </year>
Reference-contexts: We attempt to use as much educational and public domain software as possible for this part of MLC ++ . For example, the graph manipulations are done using LEDA (Library of Efficient Data Structures) written by Stefan Naher [15], and dot from AT&T <ref> [6] </ref>. Core classes These are the basic tools that are shared by many algorithms in supervised machine learning. They further divide into three types of functionality: Input/Output Classes for reading and writing data files. <p> Decision trees and decision graphs are excellent examples of interpretable structures, and MLC ++ interfaces the excellent graph-drawing programs dot and dotty provided by AT&T <ref> [6] </ref>. Visualization of discrete data For viewing datasets and induced concepts, we have implemented General Logic Diagrams [20], a method for diagrammatic visualization of discrete data. After running an induction algorithm, users may gain insight about the induced concept by inspecting the GLD.
Reference: [7] <author> Robert C. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 63-90, </pages> <year> 1993. </year>
Reference-contexts: the number of runs varies considerably, as does the ratio of the sizes of the training and test set. : : : it is virtually certain that some papers reporting results on a dataset have used slightly different versions of the dataset than others : : : | Robert Holte <ref> [7] </ref> Although many experimental results have appeared in the literature, the field seems to be in a state of disarray. There are too many algorithms and variations of algorithms, each claiming to do better on a few datasets.
Reference: [8] <author> George John, Ron Kohavi, and Karl Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 121-129. </pages> <publisher> Morgan Kauf-mann, </publisher> <month> July </month> <year> 1994. </year> <note> Available by anonymous ftp from: starry.Stanford.EDU:pub/ronnyk/ml94.ps. </note>
Reference-contexts: We described MLC ++ , our attempt at building such a tool. Much work has already been done on MLC ++ , and it has already aided some of us in our research <ref> [8, 9, 10, 11, 12] </ref>. We trust that other researchers will also enjoy productivity gains when using MLC ++ , and will contribute to this effort. Acknowledgments The MLC ++ project is partly funded by ONR grant N00014-94-1-0448 and NSF grant IRI-9116399.
Reference: [9] <author> Ron Kohavi. </author> <title> Bottom-up induction of oblivious, read-once decision graphs : strengths and limitations. </title> <booktitle> In Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 613-618, </pages> <month> July </month> <year> 1994. </year> <note> Available by anonymous ftp from Starry.Stanford.EDU:pub/ronnyk/aaai94.ps. </note>
Reference-contexts: Induction algorithms Induction algorithms induce categorizers. The library currently provides a majority inducer, a nearest-neighbor inducer [1], an ID3-like decision tree inducer [16], and an inducer for oblivious read-once decision graphs <ref> [9] </ref>. Visualization tools From the outset, one of our top priorities was to provide visualization tools to the user. Graphical displays of datasets and induced concepts can provide key insights. <p> We described MLC ++ , our attempt at building such a tool. Much work has already been done on MLC ++ , and it has already aided some of us in our research <ref> [8, 9, 10, 11, 12] </ref>. We trust that other researchers will also enjoy productivity gains when using MLC ++ , and will contribute to this effort. Acknowledgments The MLC ++ project is partly funded by ONR grant N00014-94-1-0448 and NSF grant IRI-9116399.
Reference: [10] <author> Ron Kohavi. </author> <title> Feature subset selection as search with probabilistic estimates. </title> <booktitle> In AAAI Fall Symposium on Relevance, </booktitle> <pages> pages 122-126, </pages> <month> November </month> <year> 1994. </year> <note> Available by anonymous ftp from: starry.Stanford.EDU: pub/ronnyk/aaaiSymposium94.ps. </note>
Reference-contexts: We described MLC ++ , our attempt at building such a tool. Much work has already been done on MLC ++ , and it has already aided some of us in our research <ref> [8, 9, 10, 11, 12] </ref>. We trust that other researchers will also enjoy productivity gains when using MLC ++ , and will contribute to this effort. Acknowledgments The MLC ++ project is partly funded by ONR grant N00014-94-1-0448 and NSF grant IRI-9116399.
Reference: [11] <author> Ron Kohavi. </author> <title> A third dimension to rough sets. </title> <booktitle> In Third International Workshop on Rough Sets and Soft Computing, </booktitle> <pages> pages 244-251, </pages> <month> November </month> <year> 1994. </year> <note> Available by anonymous ftp from: starry.Stanford.EDU:pub/ronnyk/roughOODG.ps. </note>
Reference-contexts: We described MLC ++ , our attempt at building such a tool. Much work has already been done on MLC ++ , and it has already aided some of us in our research <ref> [8, 9, 10, 11, 12] </ref>. We trust that other researchers will also enjoy productivity gains when using MLC ++ , and will contribute to this effort. Acknowledgments The MLC ++ project is partly funded by ONR grant N00014-94-1-0448 and NSF grant IRI-9116399.
Reference: [12] <author> Ron Kohavi. </author> <title> Useful feature subsets and rough set reducts. </title> <booktitle> In Third International Workshop on Rough Sets and Soft Computing, </booktitle> <pages> pages 310-317, </pages> <month> November </month> <year> 1994. </year> <note> Available by anonymous ftp from: starry.Stanford.EDU:pub/ronnyk/rough.ps. </note>
Reference-contexts: We described MLC ++ , our attempt at building such a tool. Much work has already been done on MLC ++ , and it has already aided some of us in our research <ref> [8, 9, 10, 11, 12] </ref>. We trust that other researchers will also enjoy productivity gains when using MLC ++ , and will contribute to this effort. Acknowledgments The MLC ++ project is partly funded by ONR grant N00014-94-1-0448 and NSF grant IRI-9116399.
Reference: [13] <author> Mike Meyer. Statlib. </author> <note> Available at lib.stat.cmu.edu. </note>
Reference-contexts: We do not intend to duplicate any of this effort; in fact, we use their data formats as much as possible. There is also a large repository of data used by statisticians in the StatLib archive, created and maintained by Meyer <ref> [13] </ref>. Mooney has a collection of a few machine learning algorithms implemented in Lisp at UCI [14], but they are not an integrated environment, and are not very efficient. StatLog [19] is an ESPRIT project studying the behavior of over twenty algorithms (mostly in the ML-Toolbox), on over twenty datasets.
Reference: [14] <author> Patrick M. Murphy and David W. Aha. </author> <title> UCI repository of machine learning databases. For information contact ml-repository@ics.uci.edu, </title> <year> 1994. </year>
Reference-contexts: Below, we mention previous projects addressing similar concerns. An extensive collection of over 100 datasets has been collected by Murphy and Aha at the University of California at Irvine <ref> [14] </ref>. We do not intend to duplicate any of this effort; in fact, we use their data formats as much as possible. There is also a large repository of data used by statisticians in the StatLib archive, created and maintained by Meyer [13]. <p> There is also a large repository of data used by statisticians in the StatLib archive, created and maintained by Meyer [13]. Mooney has a collection of a few machine learning algorithms implemented in Lisp at UCI <ref> [14] </ref>, but they are not an integrated environment, and are not very efficient. StatLog [19] is an ESPRIT project studying the behavior of over twenty algorithms (mostly in the ML-Toolbox), on over twenty datasets.
Reference: [15] <author> Stefan Naeher. LEDA: </author> <title> A Library of Efficient Data Types and Algorithms. </title> <institution> Max-Planck-Institut fuer Informatik, </institution> <address> IM Stadtwald, D-66123 Saarbruecken, FRG, 3.0 edition, </address> <year> 1992. </year> <note> Available by anonymous ftp in ftp.cs.uni-sb.de:LEDA. </note>
Reference-contexts: We attempt to use as much educational and public domain software as possible for this part of MLC ++ . For example, the graph manipulations are done using LEDA (Library of Efficient Data Structures) written by Stefan Naher <ref> [15] </ref>, and dot from AT&T [6]. Core classes These are the basic tools that are shared by many algorithms in supervised machine learning. They further divide into three types of functionality: Input/Output Classes for reading and writing data files.
Reference: [16] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year> <note> Reprinted in Shavlik and Dietterich (eds.) Readings in Machine Learning. </note>
Reference-contexts: Categorizers are built recursively; for example, in a decision tree categorizer, the branching nodes are categorizers themselves (mapping the set of instances into the set of children of that node), and the induction algorithm can use any categorizer, including the possibility of recursive decision trees. ID3 <ref> [16] </ref> always uses attribute categorizers for nominal attributes and threshold categorizers for real attributes. To generate mul-tivariate trees with perceptrons at nodes, the induction algorithm can put perceptron categoriz ers at the nodes. Induction algorithms Induction algorithms induce categorizers. <p> To generate mul-tivariate trees with perceptrons at nodes, the induction algorithm can put perceptron categoriz ers at the nodes. Induction algorithms Induction algorithms induce categorizers. The library currently provides a majority inducer, a nearest-neighbor inducer [1], an ID3-like decision tree inducer <ref> [16] </ref>, and an inducer for oblivious read-once decision graphs [9]. Visualization tools From the outset, one of our top priorities was to provide visualization tools to the user. Graphical displays of datasets and induced concepts can provide key insights.
Reference: [17] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1993. </year>
Reference-contexts: Display of information In order to understand the problem better, and perhaps bias a learning algorithm, it is helpful to view the resulting structures (e.g., decision tree), or the data. Even commercially available programs such as C4.5 <ref> [17] </ref> and CART [3] give only a rudimentary display of an induced tree. A library providing a common framework and basic tools for implementing learning algorithms would alleviate the pain involved in programming from scratch.
Reference: [18] <author> Jeffrey C. Schlimmer and Pat Langley. </author> <title> Learning, Machine. </title> <editor> In Stuart Shapiro and David Eckroth, editors, </editor> <booktitle> The Encyclopedia of Artificial Intelligence, </booktitle> <pages> pages 785-805. </pages> <publisher> Wiley-Interscience, </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Newton said he saw farther because he stood on the shoulders of giants. Computer programmers stand on each other's toes | James Coggins [5] In supervised machine learning <ref> [18, 2] </ref>, one tries to find a rule (a categorizer) that can be used to accurately predict the class label of novel instances. During the last decade, the machine learning community has developed a plethora of algorithms for this task.
Reference: [19] <editor> C.C. Taylor, D. Michie, and D.J. Spiegalhalter. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Paramount Publishing International, </publisher> <year> 1994. </year>
Reference-contexts: Mooney has a collection of a few machine learning algorithms implemented in Lisp at UCI [14], but they are not an integrated environment, and are not very efficient. StatLog <ref> [19] </ref> is an ESPRIT project studying the behavior of over twenty algorithms (mostly in the ML-Toolbox), on over twenty datasets. StatLog is an instance of a good experimental study, but does not provide the tools to aid researchers in performing similar studies.
Reference: [20] <author> Janusz Wnek and Ryszard S. Michalski. </author> <title> Hypothesis-driven constructive induction in AQ17-HCI : A method and experiments. </title> <journal> Machine Learning, </journal> <volume> 14(2) </volume> <pages> 139-168, </pages> <year> 1994. </year>
Reference-contexts: Decision trees and decision graphs are excellent examples of interpretable structures, and MLC ++ interfaces the excellent graph-drawing programs dot and dotty provided by AT&T [6]. Visualization of discrete data For viewing datasets and induced concepts, we have implemented General Logic Diagrams <ref> [20] </ref>, a method for diagrammatic visualization of discrete data. After running an induction algorithm, users may gain insight about the induced concept by inspecting the GLD.
References-found: 20

