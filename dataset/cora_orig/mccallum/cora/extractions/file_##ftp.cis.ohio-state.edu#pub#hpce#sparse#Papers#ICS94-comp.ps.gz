URL: file://ftp.cis.ohio-state.edu/pub/hpce/sparse/Papers/ICS94-comp.ps.gz
Refering-URL: http://www.cis.ohio-state.edu/~chh/Publication/sparse-papers.html
Root-URL: 
Title: On Sparse Matrix Reordering for Parallel Factorization  
Author: B. Kumar, P. Sadayappan, C.-H. Huang 
Keyword: Spectral Partitioning, Parallel Sparse Factorization, Distributed Memory Machine, Nested Dissection, Matrix Reordering  
Address: Columbus, OH 43210.  
Affiliation: Department of Computer and Information Science The Ohio State University  
Abstract: To minimize the amount of computation and storage for parallel sparse factorization, sparse matrices have to be reordered prior to factorization. We show that none of the popular ordering heuristics proposed before, namely, multiple minimum degree and nested dissection, perform consistently well over a range of matrices arising in diverse application domains. Spectral partitioning has been previously proposed as a means of generating small vertex separators for nested dissection of sparse matrices, so that the resulting ordering is amenable to efficient distributed parallel factorization with good load balance and low inter-processor communication. We show that nested dissection using spectral partitioning performs well for matrices arising from finite-element discretizations, but results in excessive fill compared to the minimum degree ordering for unstructured matrices such as power matrices and those arising from circuit simulation. The relative effectiveness of these two ordering schemes for parallel factorization is shown to vary widely for matrices arising from different application domains. We present an ordering strategy that performs consistently well for all matrix types. Its ordering is comparable or better than either minimum degree or nested dissection for all matrices evaluated. Performance results on the Intel iPSC/860 are reported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Ashcraft. </author> <title> The Fan-Both Family of Column-Based Distributed Cholesky Factorization Algorithms. </title> <type> Technical report, </type> <institution> Boeing Computer Services, </institution> <month> Dec. </month> <year> 1992. </year>
Reference: [2] <author> K. Eswar, P. Sadayappan, C.-H. Huang, and V. Vis-vanathan. </author> <title> Supernodal Sparse Cholesky Factorization On Distributed-Memory Multiprocessors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume 3, </volume> <pages> pages 18-22, </pages> <year> 1993. </year>
Reference-contexts: Total work is the the total number of operations required to normalize and update the columns of the matrix. Factorization time is the time required for generating the columns of the Cholesky factor L, and includes inter-processor communication time. Factorization is performed using a kji aggregated supernodal algorithm <ref> [2] </ref>. This algorithm has the low message volume of the fan-in algorithm and the good cache behavior of the supernodal fan-out algorithm, while avoiding the high idle times suffered by the fan-in algorithm. It has been shown to consistently outperform both the supernodal fan-in and supernodal fan-out algorithms.
Reference: [3] <author> K. Eswar, P. Sadayappan, and V. Visvanathan. </author> <title> Mul-tifrontal Factorization of Sparse Matrices on Shared-Memory Multiprocessors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 159-166, </pages> <year> 1991. </year>
Reference: [4] <author> A. George. </author> <title> Nested Dissection of a Regular Finite Element Mesh. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 10 </volume> <pages> 345-363, </pages> <year> 1973. </year>
Reference-contexts: Also, the components produced should be of roughly equal size, so that the elimination tree that is obtained is balanced and hence can be efficiently mapped onto processors for parallel factorization. George <ref> [4] </ref> showed that for a regular k fi k grid corresponding to a 9-point stencil, the fill-in introduced can be identified precisely and is O (k 2 lgk). Further, the operation count is O (k 3 ).
Reference: [5] <author> A. George and J.-W.H. Liu. </author> <title> The Evolution of the Minimum Degree Algorithm. </title> <journal> SIAM Review, </journal> <volume> 31 </volume> <pages> 1-19, </pages> <year> 1989. </year>
Reference-contexts: The problem of finding an optimal ordering to minimize the amount of fill-in has been shown to be NP-complete. There exist a number of heuristics for the factorization process, the more popular being the minimum degree and nested dissection orderings. The minimum degree algorithm <ref> [5] </ref> involves choosing at each stage of the factorization process, the node that has the least degree among all the remaining nodes. <p> The minimum degree algorithm has been shown to generate low-fill orderings over a wide range of matrices. It is optimal for those matrices whose graphs are trees, since it introduces no fill. George et al. <ref> [5] </ref> have developed an efficient implementation of this algorithm that reduces the number of degree updates that need to be performed. It is referred to as the multiple minimum degree (MMD) algorithm.
Reference: [6] <author> A.J. Hoffman, M.S. Martin, and D.J. Rose. </author> <title> Complexity Bounds for Regular Finite Difference and Finite Element Grids. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 10 </volume> <pages> 364-369, </pages> <year> 1973. </year>
Reference-contexts: George [4] showed that for a regular k fi k grid corresponding to a 9-point stencil, the fill-in introduced can be identified precisely and is O (k 2 lgk). Further, the operation count is O (k 3 ). Hoffman et al. <ref> [6] </ref> showed that the algorithm is optimal for such grids in the sense that all other orderings require at least O (k 3 ) operations. Spectral partitioning [9] has been proposed as an efficient method to perform nested dissection.
Reference: [7] <author> E.L. Lawler. </author> <title> Combinatorial Optimization: Networks and Matroids. </title> <publisher> Holt, Rinehart, and Winston, </publisher> <year> 1976. </year>
Reference-contexts: This initial partition gives an edge separator E, from which a vertex separator can be generated. In our implementation, we use the Dulmage-Mendelsohn reduction <ref> [7] </ref> to compute a minimal vertex cover of E, which corresponds to the smallest vertex separator associated with E.
Reference: [8] <author> J.W.H. Liu. </author> <title> The Role of Elimination Trees in Sparse Factorization. </title> <journal> SIAM Journal of Matrix Analysis and Applications, </journal> <volume> 11 </volume> <pages> 134-172, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: It is the graph of A with all the fill edges added, and hence is a supergraph of G (A). The elimination tree plays an important role in the sparse matrix factorization process. It provides structural information relevant to the factorization <ref> [8] </ref>. If the sparse matrix A is factorized into LL T , the elimination tree of A is defined using the structure of L as follows. <p> In the following sections, we assume that A is irreducible, and hence T (A) is a tree. The concept of elimination tree is illustrated in Figure 2. G (A) is the graph of a matrix A which is factorized by eliminating nodes in order of increasing number. Property 2.1 <ref> [8] </ref> For i &gt; j, the numerical values of column L fli depend on column L flj iff l ij 6= 0. Property 2.2 [8] If l ij 6= 0 and i &gt; j, then the node i is an ancestor of node j in the elimination tree. <p> G (A) is the graph of a matrix A which is factorized by eliminating nodes in order of increasing number. Property 2.1 <ref> [8] </ref> For i &gt; j, the numerical values of column L fli depend on column L flj iff l ij 6= 0. Property 2.2 [8] If l ij 6= 0 and i &gt; j, then the node i is an ancestor of node j in the elimination tree.
Reference: [9] <author> A. Pothen, H.D. Simon, and K.-P. Liou. </author> <title> Partitioning Sparse Matrices with Eigenvectors of Graphs. </title> <journal> SIAM Journal of Matrix Analysis and Applications, </journal> <volume> 11(3) </volume> <pages> 430-452, </pages> <month> Jul. </month> <year> 1990. </year>
Reference-contexts: Further, the operation count is O (k 3 ). Hoffman et al. [6] showed that the algorithm is optimal for such grids in the sense that all other orderings require at least O (k 3 ) operations. Spectral partitioning <ref> [9] </ref> has been proposed as an efficient method to perform nested dissection. Let G = (V; E) be an Table 1: Incomplete Nested Dissection vs. Minimum Degree Ordering (Structural Analysis Matrices) Matrix Depth Tree Ht. Total Work Fact. Time Idle Time # Msg. Msg. Vol.
Reference: [10] <author> A. Pothen, H.D. Simon, L. Wang, and S.T. Barnard. </author> <title> Towards a Fast Implementation of Spectral Nested Dissection. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 42-51, </pages> <year> 1992. </year>
Reference-contexts: Experiments were conducted on an Intel iPSC/860 hypercube with 32 processors. Incomplete nested dissection was done using the multilevel spectral partitioning code developed by Simon and Barnard <ref> [10] </ref>. Nested dissection was carried out to varying depths, with the components generated at the last step being ordered using the MMD algorithm. Nested dissection to depth 0 is equivalent to performing an ordering using MMD on the entire Table 2: Incomplete Nested Dissection vs.
Reference: [11] <author> P. Sadayappan and V. Visvanathan. </author> <title> Distributed Sparse Factorization of Circuit Matrices via Recursive E-tree Partitioning. In SIAM Symposium on Sparse Matrices, </title> <month> May </month> <year> 1989. </year>
Reference: [12] <author> M. Yannakakis. </author> <title> Computing the Minimum Fill-in is NP-complete. </title> <journal> SIAM Journal of Algebraic and Discrete Methods, </journal> <volume> 2 </volume> <pages> 77-79, </pages> <year> 1981. </year>
References-found: 12

