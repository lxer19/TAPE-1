URL: ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/nameUsenix92.ps.Z
Refering-URL: http://www.cs.berkeley.edu/projects/sprite/sprite.papers.html
Root-URL: 
Title: A Trace-Driven Analysis of Name and Attribute Caching in a Distributed System  
Author: Ken W. Shirriff John K. Ousterhout 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division Electrical Engineering and Computer Sciences University of California at Berkeley  
Abstract: This paper presents the results of simulating file name and attribute caching on client machines in a distributed file system. The simulation used trace data gathered on a network of about 40 workstations. Caching was found to be advantageous: a cache on each client containing just 10 directories had a 91% hit rate on name lookups. Entry-based name caches (holding individual directory entries) had poorer performance for several reasons, resulting in a maximum hit rate of about 83%. File attribute caching obtained a 90% hit rate with a cache on each machine of the attributes for 30 files. The simulations show that maintaining cache consistency between machines is not a significant problem; only 1 in 400 name component lookups required invalidation of a remotely cached entry. Process migration to remote machines had little effect on caching. Caching was less successful in heavily shared and modified directories such as /tmp, but there weren't enough references to /tmp overall to affect the results significantly. We estimate that adding name and attribute caching to the Sprite operating system could reduce server load by 36% and the number of network packets by 30%. 
Abstract-found: 1
Intro-found: 1
Reference: [BHK91] <author> M. Baker, J. Hartman, M. Kupfer, K. Shirriff and J. Ousterhout, </author> <title> Measurements of a Distributed File System, </title> <booktitle> Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <month> Oct. </month> <year> 1991, </year> <pages> 198-212. </pages>
Reference-contexts: The trace data We collected eight one-day traces of activity on the Sprite system. These traces consisted of log records of file system activity, collected on our file servers. More information on the trace data is available in <ref> [BHK91] </ref>. Table 2 gives an overview of the trace data we used for this study. There were three types of trace records used in this study. The most important was the lookup record, which logged a path name lookup.
Reference: [DoO91] <author> F. Douglis and J. Ousterhout, </author> <title> Transparent Process Migration: Design Alternatives and the Sprite Implementation, </title> <journal> SoftwarePractice & Experience 21, </journal> <month> 8 (Aug. </month> <year> 1991), </year> <pages> 757-785. </pages>
Reference-contexts: To our surprise, we found that process migration does not have a significant effect on name and attribute caching, even though migrated processes account for an average of 19% of lookups. (Process migration is a mechanism in Sprite used to move processes to idle machines for parallel execution <ref> [DoO91] </ref>.) There was almost no difference in cache performance between simulations with process migration and without migration. Consistency overhead was small; only a small amount of network traffic was required to keep the caches consistent across multiple machines, as shown by the low invalidation rate in Table 1. <p> We wished to examine the effect of this sharing on name and attribute caching. Sprite provides a process migration facility, which allows processes to be moved across the network to idle machines <ref> [DoO91] </ref>. This permits users to take advantage of the processing power of several machines at once. There are currently two main uses of process migration in Sprite: parallel compilation and large simulations.
Reference: [Flo86] <author> R. Floyd, </author> <title> Directory Reference Patterns in a UNIX environment, </title> <type> Technical Report 179, </type> <institution> Computer Science Department, The University of Rochester, Rochester, </institution> <address> NY, </address> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: If client-level name caches are combined with caches of file data, they may even allow a client machine to continue operating when the file server is unavailable [KiS91]. Unfortunately, there has been little data published on the measured performance of name caches in distributed file systems. Floyd et al. <ref> [Flo86, FlE89] </ref> and Sheltzer et al. [SLP86] performed trace-driven studies of name cache performance, and both concluded that relatively small name caches produce relatively high hit ratios.
Reference: [FlE89] <author> R. Floyd and C. Ellis, </author> <title> Directory Reference Patterns in Hierarchical File Systems, </title> <journal> IEEE Transactions on Knowledge and Data Engineering 1, </journal> <month> 2 (June </month> <year> 1989), </year> <pages> 238-247. </pages>
Reference-contexts: If client-level name caches are combined with caches of file data, they may even allow a client machine to continue operating when the file server is unavailable [KiS91]. Unfortunately, there has been little data published on the measured performance of name caches in distributed file systems. Floyd et al. <ref> [Flo86, FlE89] </ref> and Sheltzer et al. [SLP86] performed trace-driven studies of name cache performance, and both concluded that relatively small name caches produce relatively high hit ratios. <p> However, an examination of some common directories shows them to be only a few kilobytes.) The size of a directory cache is noteworthy in comparison to file system data caches in Sprite, which may hold several megabytes of data. Our measurements correlate well with those in <ref> [FlE89] </ref>, which found a 10 directory cache was equivalent to about 14 kilobytes. 3. Simulations of name and attribute caching 3.1. About the simulator We constructed a simulator that used the trace data to estimate the effectiveness of various caching schemes for file names and attributes. <p> However, the average path actually had 82% chance of being entirely in the cache. This graph shows averages over all eight traces. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Our name cache performance results are close to those of other papers, even though our computing environment is different. For instance, Floyd et al. <ref> [FlE89] </ref> found an 85% hit rate with a 10 directory cache, and a 95% hit rate on a 30 directory cache. These hit rates are close to ours, even though their measurements were on a single, multi-user machine. <p> This graph shows the time since the last reference, for references to items in the cache. For all three cache types, the majority of references to cached entries happened no more than a second after the previous reference. This corresponds well with the single-machine results in <ref> [FlE89] </ref>, which found that half of the inter-reference times were under 1/4 second. Cache entries no older than a minute accounted for over 90% of the cache hits.
Reference: [HeP90] <author> J. Hennessy and D. Patterson, </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: In our cache simulations, each name component that was found in the client's cache was counted as a cache hit. If the component was not found, it was counted as a miss. We divided misses into several categories (based on the categories used in <ref> [HeP90] </ref>): g Compulsory misses are misses that would occur in an arbitrarily large cache: the first access to each direc tory or entry causes a compulsory miss. g Capacity misses are name references that are misses due to the size of the cache; they would have been hits in a suitably
Reference: [Hil87] <author> M. Hill, </author> <title> Aspects of Cache Memory and Instruction Buffer Performance, </title> <type> Report No. UCB/CSD 87/381, PhD Thesis, </type> <institution> Computer Science Division, UC Berkeley, Berkeley, </institution> <address> CA, </address> <month> Nov. </month> <year> 1987. </year>
Reference-contexts: When an entry was modified, it was invalidated from the caches of all other machines. We used several techniques to keep the simulation to a reasonable size and run time. The simulator used a stack-based model <ref> [Hil87] </ref> in order to simulate multiple cache sizes in one simulation run. To keep the simulation state from growing excessively, we pruned the LRU lists at regular intervals.
Reference: [HBM89] <author> A. Hisgen, A. Birrell, T. Mann, M. Schroeder and G. Swart, </author> <title> Availability and Consistency Tradeoffs in the Echo Distributed File System, </title> <booktitle> Proceedings of the Second Workshop on Workstation Operating Systems (WWOS-II), </booktitle> <month> Sep. </month> <year> 1989, </year> <pages> 49-54. </pages>
Reference-contexts: Name lookup is an even larger problem in distributed systems, where a client machine may have to contact a file server across the network to perform the name lookup. Most network file systems cache naming information on client workstations as well as on servers <ref> [HBM89, HKM88, WPE83] </ref>. This allows clients to perform most name lookups without contacting the server, which improves the speed of lookups by as much as an order of magnitude. In addition, client-level name caching reduces the load on the server and the network.
Reference: [HKM88] <author> J. Howard, M. Kazar, S. Menees, D. Nichols, M. Satyanarayanan, R. Sidebotham and M. West, </author> <title> Scale and Performance in a Distributed File System, </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (Feb. </month> <year> 1988), </year> <pages> 51-81. </pages>
Reference-contexts: Name lookup is an even larger problem in distributed systems, where a client machine may have to contact a file server across the network to perform the name lookup. Most network file systems cache naming information on client workstations as well as on servers <ref> [HBM89, HKM88, WPE83] </ref>. This allows clients to perform most name lookups without contacting the server, which improves the speed of lookups by as much as an order of magnitude. In addition, client-level name caching reduces the load on the server and the network. <p> We assumed that the name and attribute caches were kept strongly consistent. That is, the caches never were allowed to contain stale data. We simulated a callback mechanism similar to the one used in Andrew <ref> [HKM88] </ref> to maintain consistency. For the callback mechanism, the server keeps a record of what data is cached on each client. When another client modifies data, it must inform the server, which then calls back all clients with cached copies of that data. The clients then invalidate their stale data.
Reference: [KhL90] <author> D. Khorramabadi and C. Lowery, </author> <title> Analysis of Network Traffic in the Sprite Remote Procedure Call System, </title> <institution> Computer Science 262 Project Report, Computer Science Division, UC Berkeley, Berkeley, </institution> <address> CA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Combining these figures, we estimate that total file server kernel load could be reduced by 36%. Caching would also reduce network traffic. A previous study of Sprite network traffic <ref> [KhL90] </ref> found that about 1/3 of Sprite remote procedure call (RPC) packets were for open, stat, and fstat operations.
Reference: [KiS91] <author> J. Kistler and M. Satyanarayanan, </author> <title> Disconnected Operation in the Coda File System, </title> <booktitle> Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <month> Oct. </month> <year> 1991, </year> <pages> 213-225. </pages>
Reference-contexts: In addition, client-level name caching reduces the load on the server and the network. If client-level name caches are combined with caches of file data, they may even allow a client machine to continue operating when the file server is unavailable <ref> [KiS91] </ref>. Unfortunately, there has been little data published on the measured performance of name caches in distributed file systems. Floyd et al. [Flo86, FlE89] and Sheltzer et al. [SLP86] performed trace-driven studies of name cache performance, and both concluded that relatively small name caches produce relatively high hit ratios.
Reference: [LKM84] <author> S. Leffler, M. Karels and M. McKusick, </author> <title> Measuring and Improving the Performance of 4.2BSD, </title> <booktitle> Proceedings of the 1984 USENIX Summer Conference, </booktitle> <month> June </month> <year> 1984, </year> <pages> 237-252. </pages>
Reference-contexts: 1. Introduction Operating systems spend much of their time performing path name lookups to convert symbolic path names to file identifiers. In order to reduce the cost of name lookups, many systems have implemented name caching schemes. For instance, Leffler et al <ref> [LKM84] </ref> measured performance on a single-machine system running 4.2BSD Unix and found path name translation to be the single most expensive function performed by the kernel, requiring 19% of kernel CPU cycles. When name caching was added to 4.3BSD Unix, it reduced name translation costs by 35%. <p> We did kernel name lookup timing measurements, which show that about 20% of the time the file server spent in the kernel was spent handling name lookups. (This corresponds well with the single-machine measurements in <ref> [LKM84] </ref>, which found the kernel spends 19% of its time performing name lookups.) We estimate that another 20% of the time was spent handling file opens. Given a relatively small name and attribute cache on each client, we could eliminate 90% of name lookups and file opens.
Reference: [Nel88] <author> M. Nelson, </author> <title> Physical Memory Management in a Network Operating System, </title> <type> Report No. UCB/CSD 88/471, PhD Thesis, </type> <institution> Computer Science Division, UC Berkeley, Berkeley, </institution> <address> CA, </address> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: Earlier measurements of Sprite's performance in <ref> [Nel88] </ref>, estimated that server utilization and network utilization in Sprite could be reduced by a factor of 2 with local name caching. However, since that estimate was an upper bound based on performance measurements on a set of benchmarks, we believe the figures here to be more realistic. 4.
Reference: [OCD88] <author> J. Ousterhout, A. Cherenson, F. Douglis, M. Nelson and B. Welch, </author> <title> The Sprite Network Operating System, </title> <booktitle> IEEE Computer 21, </booktitle> <month> 2 (Feb. </month> <year> 1988), </year> <pages> 23-36. </pages>
Reference-contexts: We performed eight traces; these results are averages. 2. Collection of data 2.1. The Sprite system We performed our name and attribute cache tracing on Sprite, a network-based operating system <ref> [OCD88] </ref>. Sprite provides a Unix-like environment in a network of about 40 workstations. Files are stored on one of several file servers and may be cached on the clients, with full consistency maintained among the cached copies.
Reference: [SGK85] <author> R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh and B. Lyon, </author> <title> Design and Implementation of the Sun Network Filesystem, </title> <booktitle> Proceedings of the 1985 USENIX Summer Conference, </booktitle> <month> June </month> <year> 1985, </year> <pages> 119-130. </pages>
Reference-contexts: In the Echo system, on the upper levels (close to the root) of the file system, clients invalidate cached name data after several hours. Until the data is invalidated, clients can access inconsistent data. As another alternative, some NFS implementations <ref> [SGK85] </ref> uses a probabilistic scheme, in which cached names and attributes are considered invalid after a certain length of time. This time varies between 3 and 60 seconds, and is selected based on prior reference patterns of the file.
Reference: [SLP86] <author> A. Sheltzer, R. Lindell and G. Popek, </author> <title> Name Service Locality and Cache Design in a Distributed Operating System, </title> <booktitle> Proceedings of the 6th International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1986, </year> <pages> 515-522. </pages>
Reference-contexts: Unfortunately, there has been little data published on the measured performance of name caches in distributed file systems. Floyd et al. [Flo86, FlE89] and Sheltzer et al. <ref> [SLP86] </ref> performed trace-driven studies of name cache performance, and both concluded that relatively small name caches produce relatively high hit ratios. However, Floyd studied a single time-shared system, and Sheltzer studied a small collection of networked time-shared machines where many accesses were to local files. <p> For instance, Floyd et al. [FlE89] found an 85% hit rate with a 10 directory cache, and a 95% hit rate on a 30 directory cache. These hit rates are close to ours, even though their measurements were on a single, multi-user machine. Sheltzer et al. <ref> [SLP86] </ref> found a 15-directory cache reduced the whole-path miss ratio from 71% to 11%. (Even without caching, many of the path name lookups could be completed locally because each machine in the 15-site VAX network stored part of the file system.) The hit ratio with 40 cached directories ranged from 87% <p> For the remainder of the time, usually only one remote machine needs to be invalidated. For the attribute cache, a remote machine needed to be invalidated about 21% of the time. These consistency overhead measurements are similar to those found on Locus by Sheltzer et al. <ref> [SLP86] </ref>. Sheltzer found that with a 60-directory cache, only 0.051% of the references required cache invalidation. This invalidation rate is about a fifth of the rate we measured on our system. The probable cause is that since we have 40 machines, compared to 15 in [SLP86], we have a higher chance <p> Locus by Sheltzer et al. <ref> [SLP86] </ref>. Sheltzer found that with a 60-directory cache, only 0.051% of the references required cache invalidation. This invalidation rate is about a fifth of the rate we measured on our system. The probable cause is that since we have 40 machines, compared to 15 in [SLP86], we have a higher chance of requiring invalidation. Sheltzer also found 7% of invalidations resulted in more than a single invalidation, compared to our rate of 11% of whole-directory name cache modifications affecting more than the locally cached copy. 3.8.

References-found: 15

