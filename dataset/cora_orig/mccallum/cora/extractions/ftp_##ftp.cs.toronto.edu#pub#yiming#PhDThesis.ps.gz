URL: ftp://ftp.cs.toronto.edu/pub/yiming/PhDThesis.ps.gz
Refering-URL: http://www.cs.toronto.edu/~yiming/
Root-URL: 
Title: SENSOR PLANNING FOR OBJECT SEARCH  
Author: by Yiming Ye 
Degree: A thesis submitted in conformity with the requirements for the degree of Doctor of Philosophy  
Note: c Copyright by Yiming Ye 1997  
Address: Toronto  
Affiliation: Graduate Department of Computer Science University of  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A.L. Abbott. </author> <title> Active surface reconstruction by integrating focus,vergence, stereo, and camera calibration. </title> <booktitle> In Third International Conference on Computer Vision, </booktitle> <pages> pages 489-491, </pages> <address> Hilton Head, SC., </address> <month> December </month> <year> 1991. </year>
Reference-contexts: Each cell in the final occupancy map contains its occupancy status (unknown, empty, or occupied) with an associated certainty factor, using the following convention: unknown 0; empty <ref> [1; 0); occupied (0; 1] </ref>. A cell is considered unknown if no information concerning it is available. Cells can be empty with a certainty factor ranging from 0 to 1 and occupied with a certainty factor ranging from 0 to 1. <p> After the application of the operation f 1 , the distribution is denoted by p <ref> [1] </ref> (c 1 ); p [1] (c 2 ); : : : ; p [1] (c n ); p [1] (c o ). <p> After the application of the operation f 1 , the distribution is denoted by p <ref> [1] </ref> (c 1 ); p [1] (c 2 ); : : : ; p [1] (c n ); p [1] (c o ). <p> After the application of the operation f 1 , the distribution is denoted by p <ref> [1] </ref> (c 1 ); p [1] (c 2 ); : : : ; p [1] (c n ); p [1] (c o ). <p> After the application of the operation f 1 , the distribution is denoted by p <ref> [1] </ref> (c 1 ); p [1] (c 2 ); : : : ; p [1] (c n ); p [1] (c o ). Generally, after the application of the operation f i , the distribution is denoted by p [i] (c 1 ); p [i] (c 2 ); : : : ; p [i] (c n ); p [i] (c o ), where 1 i q. <p> So, this assumption does not influence the NP-complete property of the task Chapter 2. Problem Formulation 26 The values of A [i] (1 i n, or i = o) satisfy A [0] + A <ref> [1] </ref> + + A [n] = 1 Each A [i] 0 is assumed to be a rational number A [i] = q i where p i 2 Z + and q i 2 Z + . <p> Steps: m * (7) |- (14) Explain- Array F [0 : m] gives the ordered set of actions selected as the effort allocation. len gives the number of actions in the effort allocation. The ordered actions for the effort allocation are: hF <ref> [1] </ref>; : : : ; F [len]i. <p> We uniformally divide [0; w 2 ] into n w 1 parts. Finally we get the tessellation <ref> [ 0 ; 1 ] </ref>, [ n w 2 ; n w 1 ], [ n w 1 ; w 2 + ff i ]. Similarly we can tessellate . <p> The Proof of the Properties 143 Proof (1) When k = 1, from update rule 2.9 we have p <ref> [1] </ref> (c) = p [0] (c o ) + i=1 p [0] (c i )(1 b (c i ; f )) p [0] (c)(1 b (c; f 1 )) i=1 p [0] (c i ) i=1 p [0] (c i )b (c i ; f )) p [0] (c)(1 b (c; <p> [0] (c)(1 b (c; f 1 )) i=1 p [0] (c i ) i=1 p [0] (c i )b (c i ; f )) p [0] (c)(1 b (c; f 1 )) Since b (c; f 1 ) = 0 when c 2 O (f 1 ), we have p <ref> [1] </ref> (c) = 1 P (f 1 ) So, the result is true when k = 1. (2) Suppose when k = r, we have p [r] (c) = (1 P (f 1 ))(1 P (f 2 )) : : :; (1 P (f r )) (3) When k = r <p> Q [r] T o 1 k1 X ( c2 1i 1 &lt;i 2 &lt;:::&lt;i r k1 Q [r] T o ) 1 k1 X T r (A.4) Where T 0 = c2 Q [0] T o = c2 Q [0] T o T 1 = c2 1i 1 k1 Q <ref> [1] </ref> T o = i 1 =1 c2 Q [1] T o i 1 =k1 X X n k1 (f 1 :::f i 1 :::f k1 ) (f k ) p [0] (c)[1 b (c; f 1 )] : : :[1 b (c; f i 1 1 )][1 b (c; f <p> 1i 1 &lt;i 2 &lt;:::&lt;i r k1 Q [r] T o ) 1 k1 X T r (A.4) Where T 0 = c2 Q [0] T o = c2 Q [0] T o T 1 = c2 1i 1 k1 Q <ref> [1] </ref> T o = i 1 =1 c2 Q [1] T o i 1 =k1 X X n k1 (f 1 :::f i 1 :::f k1 ) (f k ) p [0] (c)[1 b (c; f 1 )] : : :[1 b (c; f i 1 1 )][1 b (c; f i 1 +1 )] : : :[1 b (c; <p> (f 1 :::f i 1 :::f k1 ) (f k ) p [0] (c)[1 b (c; f 1 )] : : :[1 b (c; f i 1 1 )][1 b (c; f i 1 +1 )] : : :[1 b (c; f k1 )]b (c; f k ) Note: n <ref> [1] </ref> T o T n [1] T o above transformation is true. . . . <p> :::f k1 ) (f k ) p [0] (c)[1 b (c; f 1 )] : : :[1 b (c; f i 1 1 )][1 b (c; f i 1 +1 )] : : :[1 b (c; f k1 )]b (c; f k ) Note: n <ref> [1] </ref> T o T n [1] T o above transformation is true. . . . <p> As an illustration, we only calculate p [0] (c)b (c; f 1 )b (c; f k ). Others are quite similar. The part contributed by T 0 is c2 Q [0] T o The part contributed by T 1 is c2 2i 1 k1 Q <ref> [1] </ref> T o : : : The part contributed by T r is c2 2i 1 &lt;i 2 &lt;:::&lt;i r k1 Q [r] T o The part contributed by T k1 is 0. The sum of the total contribution is c2 Q [0] T o c2 2i 1 k1 Q [1] <p> <ref> [1] </ref> T o : : : The part contributed by T r is c2 2i 1 &lt;i 2 &lt;:::&lt;i r k1 Q [r] T o The part contributed by T k1 is 0. The sum of the total contribution is c2 Q [0] T o c2 2i 1 k1 Q [1] T o : : : X n S T o X n k1 (f 1 f 2 :::f k1 ) (f k ) p [0] (c)b (c; f 1 )b (c; f k ) X T p [0] (c)b (c; f 1 )b (c; f k ) (A.5) Chapter A. <p> The part contributed by T 0 is (1) r c2 Q [0] T o p [0] (c)b (c; f 1 ) : : :b (c; f r )b (c; f k ) The part contributed by T 1 is (1) r c2 r+1i 1 k1 Q <ref> [1] </ref> T o p [0] (c)b (c; f 1 ) : : : b (c; f r )b (c; f k ) . . . <p> (c; f r )b (c; f k ) The total contribution is: (1) r c2 Q [0] T o p [0] (c)b (c; f 1 ) : : : b (c; f r )b (c; f k ) + : : : + (1) r c2 r+1i 1 k1 Q <ref> [1] </ref> T o p [0] (c)b (c; f 1 ) : : : b (c; f r )b (c; f k ) + : : : + (1) r c2 Q [kr1] T o p [0] (c)b (c; f 1 ) : : : b (c; f r )b (c; f
Reference: [2] <author> A.L. Abbott and N. Ahuja. </author> <title> Surface reconstruction by dynamic integration of focus, camera vergence, and stereo. </title> <booktitle> In Second International Conference on Computer Vision, </booktitle> <pages> pages 532-543, </pages> <address> Florida, USA, </address> <month> December </month> <year> 1988. </year>
Reference-contexts: of [t b ; t e ] and the slice on the sphere whose tilt is within the range of [ t e ; t b ]. (a) Let t t e ff (b) Let pan 2arctanf sin ( ff 2 ) 2 )+ ff (c) Use pan to divide <ref> [0; 2] </ref> for the given slice. We obtain a series of intervals [p b ; p e ] as follows:. [0; pan ], [ pan ; 2 pan ], : : :, [k pan ; 2]. <p> We obtain a series of intervals [p b ; p e ] as follows:. [0; pan ], [ pan ; 2 pan ], : : :, <ref> [k pan ; 2] </ref>. Note: the length of the last interval may not be pan . (d) For each division, let p p b +p e 2 . Then perform S candidate = S candidate S and S candidate = S candidate S 6. Let t e t b 7.
Reference: [3] <author> A.L.Abbott. </author> <title> Selective fixation control for machine vision: A survey. </title> <booktitle> In IEEE Intervna-tional Conference on System, Man and Cybernetics, </booktitle> <pages> pages 1-6, </pages> <month> August </month> <year> 1991. </year>
Reference: [4] <author> Y. Aloimonous, I. Weiss, and A. Bandyopadhyay. </author> <title> Active vision. </title> <booktitle> In First International Conference on Computer Vision, </booktitle> <pages> pages 35-54, </pages> <address> London, England, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Furthermore, there are some tasks (e.g., model based object recognition) which are naturally suited to active vision and which conventional vision systems find very difficult to perform. It has been shown <ref> [4] </ref> that an active observer can solve basic vision problems (e.g., shape from shading, shape from contour, shape from texture and structure from motion) in a much more efficient way than a passive observer.
Reference: [5] <author> Firshid Arman. </author> <title> Model-based object recognition in dense-range images |- a review. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(1) </volume> <pages> 5-45, </pages> <year> 1993. </year>
Reference-contexts: Finally we discuss the concept of detection function in the general situation. 3.1 Object Recognition Object recognition is the task of finding and labeling parts of a two-dimensional image of a scene that correspond to an object in the scene [14] [68] <ref> [5] </ref> [86]. To perform the object recognition task, we need to establish models, or general descriptions of each object to be recognized. A model may include shape, texture, and context knowledge about the occurrence of such objects in a scene.
Reference: [6] <author> R. </author> <title> Bajcsy. Active perception vs. passive perception. </title> <booktitle> In Third IEEE Workshop on Vision, </booktitle> <pages> pages 55-59, </pages> <address> Bellaire, </address> <year> 1985. </year>
Reference-contexts: This is the sensor planning problem for object search. The second is the manipulation of the hardware so that the sensing operators can reach the state specified by the planner. This is possible because of the recent availability of sophisticated hardware for sensor/platform control and real-time image processing ([77], <ref> [6] </ref>, [87]). The third is search for the target within the image. This is the object recognition and localization problem, which attracts a lot of attention within the computer vision community (see ([19]) for a review). <p> In other words, a very small error in the input might result in a catastrophic error in the output. Bajcsy <ref> [6] </ref> noticed that there are some major issues that are not addressed in Marr's paradigm, one of them being the effect of the perceiver's behavior. She proposed that a passive sensor be used in an active fashion, purposefully changing the sensor's state parameters according to sensing strategies.
Reference: [7] <author> R. </author> <title> Bajcsy. Perception with feedback. </title> <booktitle> In Image Understanding Workshop, </booktitle> <pages> pages 279-288, </pages> <year> 1988. </year>
Reference: [8] <author> R. Bajcsy and M. Campos. </author> <title> Active and exploratory perception. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 56(1) </volume> <pages> 31-40, </pages> <year> 1992. </year>
Reference: [9] <author> Andrew Balke and Alan Yuille. </author> <title> Active Vision. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1992. </year>
Reference: [10] <author> D. H. Ballard and A. Ozcandarli. </author> <title> Eye fixation and early vision: Kinetic depth. </title> <booktitle> In Second International Conference on Computer Vision, </booktitle> <pages> pages 524-531, </pages> <address> Florida, USA, </address> <month> December </month> <year> 1988. </year>
Reference: [11] <author> Dana H. Ballard. </author> <title> Task frames in visuo-motor coordination. </title> <booktitle> In Proceedings IEEE Workshop on Computer Vision: Representation and Control, </booktitle> <pages> pages 3-10, </pages> <address> Bellaire MI., </address> <year> 1985. </year>
Reference-contexts: The effective region for =4 is <ref> [11; 27] </ref>. (a) (b) Chapter 9. <p> See Figure 9.2 for an illustration. Since the diameter of the region is 75, only three angle sizes 0.785398 (45 o ),0.386876 (22:166346 0 ), 0.164481 (9:424083 o ) are needed in the search process. Their effective ranges are <ref> [11; 27] </ref>, [27; 66], [66; 162], respectively. It is assumed that we do not know the probability distribution at the beginning, and we do not know whether the object is within the room or outside the room.
Reference: [12] <author> D.H. Ballard. </author> <title> Animate vision. </title> <journal> Artificial Intelligence, </journal> <volume> 48 </volume> <pages> 57-86, </pages> <year> 1991. </year> <note> 169 BIBLIOGRAPHY 170 </note>
Reference: [13] <author> A. Bandyopadhyay, B. Chandra, and D. Ballard. </author> <title> Active navigation: Tracking an environmental point considered beneficial. </title> <booktitle> In Proceedings of IEEE Workshop on Motion: Representation and Analysis, </booktitle> <pages> pages 23-28, </pages> <address> Charleston,SC, </address> <month> May </month> <year> 1986. </year>
Reference: [14] <author> Paul J. Besl and Ramesh C. Jain. </author> <title> Three-dimensional object recognition. </title> <journal> ACM Computing Surveys, </journal> <volume> 17(1) </volume> <pages> 74-145, </pages> <year> 1985. </year>
Reference-contexts: Finally we discuss the concept of detection function in the general situation. 3.1 Object Recognition Object recognition is the task of finding and labeling parts of a two-dimensional image of a scene that correspond to an object in the scene <ref> [14] </ref> [68] [5] [86]. To perform the object recognition task, we need to establish models, or general descriptions of each object to be recognized. A model may include shape, texture, and context knowledge about the occurrence of such objects in a scene.
Reference: [15] <author> R.M. Bolle, A. Califano, and R. Kjeldsen. </author> <title> Data and model driven foveation. </title> <type> Technical Report RC 15096, </type> <institution> IBM Research Division, T.J. Watson Research Center, </institution> <address> NY, </address> <month> October </month> <year> 1989. </year>
Reference-contexts: In Rimey and Brown's task-oriented system T EA [63], the low resolution camera provides a peripheral image of the entire field of view, while the high resolution camera can be moved within the field of view to examine the object in detail and to do verification. Bolle et al. <ref> [15] </ref> examine the problem of how to combine information from the high resolution fovea-like window and information from the coarse resolution of the whole image so as to extract features that cannot be detected at either coarse or fine resolution alone.
Reference: [16] <author> M. Brand. </author> <title> An eye for design: Why, where and how to look for causal structure in visual scenes. </title> <booktitle> SPIE.Intelligent Robots and Computer Vision, </booktitle> <address> 1825(XI):180-188, </address> <year> 1992. </year>
Reference: [17] <author> Rodney Brooks. </author> <title> A layered intelligent control system for a mobile robot. </title> <journal> IEEE Journal Robotics and Automation, </journal> <volume> (RA-2):14-23, </volume> <month> April </month> <year> 1986. </year>
Reference-contexts: The cycle repeats as the robot ventures forth once again on its quest for cans. Although the task of the robot is to collect soda cans, the purpose of Connell's research is to test Brooks's subsumption architecture [60] <ref> [17] </ref> rather than to study object search. Connell's system is unusual in that it is controlled by a collection of independent behaviors rather than a centralized program, and because it uses a minimal amount of state.
Reference: [18] <author> C.M. Brown. </author> <title> Issues in selective perception. </title> <booktitle> In 11th International Conference on Pattern Recognition, </booktitle> <pages> pages 21-30, </pages> <year> 1992. </year>
Reference: [19] <author> R. Chin and C. Dyer. </author> <title> Model-based recognition in robot vision. </title> <journal> Computing Serveys, </journal> <volume> 18(1) </volume> <pages> 67-108, </pages> <month> March </month> <year> 1986. </year>
Reference: [20] <author> Connel. </author> <title> An Artificial Creature. </title> <type> PhD thesis, </type> <institution> AI Lab, MIT, </institution> <year> 1989. </year>
Reference-contexts: Although sensor planning for object search is very important in practice, it is interesting to note that research on this topic within the computer vision community is limited ([77], [89], [29], [54], [63]). Connell et al. <ref> [20] </ref> have constructed a robot that roams an area searching for and collecting soda cans. The planning is very simple since the robot just follows the walls of the room and the sensor only searches the area immediately in front of the robot. <p> Thus, the next position for the robot can be selected from this basic set of candidate positions. 8.2 Comparison with Wixson's Work The existing works that are directly concerned with the task of searching for a 3D object within a 3D environment are: Connell et al.'s work <ref> [20] </ref> on searching for and collecting soda cans by an artificial creature, Garvey's proposal [29] for indirect search and Wixson et. al's work [88] 82 Chapter 8. Discussion 83 on efficiency analysis of direct and indirect search and on viewpoint selection for object search. Connell [20] describes a real, fully-functional mobile <p> are: Connell et al.'s work <ref> [20] </ref> on searching for and collecting soda cans by an artificial creature, Garvey's proposal [29] for indirect search and Wixson et. al's work [88] 82 Chapter 8. Discussion 83 on efficiency analysis of direct and indirect search and on viewpoint selection for object search. Connell [20] describes a real, fully-functional mobile robot which operates in an unstructured environment to collect empty soda cans. The robot has infrared proximity sensors on the body for obstacle avoidance, a flux-gate compass for navigation and a high resolution laser range finder for locating and recognizing objects.
Reference: [21] <author> Cregg K. Cowan and Peter D. Kovesi. </author> <title> Automatic sensor placement from vision task requirements. </title> <journal> IEEE Transactions on Pattern analysis and Machine Intelligence, </journal> <volume> 10(3) </volume> <pages> 407-416, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: He uses a method based on maximizing the magnitude of the intensity gradient as the criterion function. The Fibonacci search technique is used to optimally locate the mode of the criterion function and then the thick-lens law is used to compute the distance. Cowan et al. <ref> [21] </ref> present an approach to automatically generating the possible camera locations for observing an object.
Reference: [22] <author> S.M. Culhane and J.K. Tsotsos. </author> <title> An attentional prototype for early vision. </title> <booktitle> In Second European Conference on Computer Vision, </booktitle> <pages> pages 551-560, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> May </month> <year> 1992. </year>
Reference: [23] <author> S.M. Culhane and J.K. Tsotsos. </author> <title> A prototype for data-driven visual attention. </title> <booktitle> In 11th International Conference on Pattern Recognition, </booktitle> <pages> pages 36-40, </pages> <year> 1992. </year>
Reference: [24] <author> T. Dean. </author> <title> Sequential decision making for active perception. </title> <booktitle> In Image Understanding Workshop, </booktitle> <pages> pages 889-894, </pages> <address> Pennsylvania, USA, </address> <month> September </month> <year> 1990. </year>
Reference: [25] <author> C. Durieu, H. Clergeot, and F. Monteil. </author> <title> Localization of a mobile robot with beacons taking erroneous data into account. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 1062-1068, </pages> <address> New York, U.S.A, </address> <year> 1989. </year>
Reference-contexts: Active beacons are a very common method of navigation for both aircraft and shipping, and have been used for mobile robot localization. 75 Chapter 7. Where to Move Next 76 Durieu et al. <ref> [25] </ref> analyze the operation and performances of a navigation system for a mobile robot using ultrasonic/infra-red telemeter with active beacons, intended for a mobile inspection robot application.
Reference: [26] <author> A. Elfes. </author> <title> A sonar-based mapping and navigation system. </title> <booktitle> In IEEE International Conference on Robotics and Automation, </booktitle> <address> San Francisco, CA. U.S.A., </address> <month> April </month> <year> 1986. </year> <note> BIBLIOGRAPHY 171 </note>
Reference-contexts: The solidity properties are updated whenever the robot gets to a new position. The probabilities are updated whenever a visual operation is conducted. This is a process of environmental mapping. The idea to tessellate the space into grids and assign each grid with property values is not new. Elfes <ref> [26] </ref> has studied the problem of acquiring and handling information about the existence and localization of solid objects and empty spaces by composing information coming from multiple sonar readings, and has built a coherent world-model that reflects the information acquired. <p> Cells can be empty with a certainty factor ranging from 0 to 1 and occupied with a certainty factor ranging from 0 to 1. The final map is computed from two separate arrays derived from the empty and occupied certainty distribution introduced above. One important step of Elfes <ref> [26] </ref> approach is the combination of the constraints from individual sonar readings with the sonar map.
Reference: [27] <author> M. Garey and D. Johnson. </author> <title> Computers and intractability: A guide to the theory of NP-completeness. W.H. </title> <publisher> Freeman and Co., </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Proof: Chapter 2. Problem Formulation 31 We prove the theorem by restriction. The KN AP SACK problem is a well known NP-Complete problem <ref> [27] </ref>. We will restrict the object search task to KN AP SACK by allowing instances in which any two operations in O = ff 1 ; f 2 ; : : : ; f m g do not have a common influence region. <p> The effective region for =4 is <ref> [11; 27] </ref>. (a) (b) Chapter 9. <p> See Figure 9.2 for an illustration. Since the diameter of the region is 75, only three angle sizes 0.785398 (45 o ),0.386876 (22:166346 0 ), 0.164481 (9:424083 o ) are needed in the search process. Their effective ranges are <ref> [11; 27] </ref>, [27; 66], [66; 162], respectively. It is assumed that we do not know the probability distribution at the beginning, and we do not know whether the object is within the room or outside the room. <p> See Figure 9.2 for an illustration. Since the diameter of the region is 75, only three angle sizes 0.785398 (45 o ),0.386876 (22:166346 0 ), 0.164481 (9:424083 o ) are needed in the search process. Their effective ranges are [11; 27], <ref> [27; 66] </ref>, [66; 162], respectively. It is assumed that we do not know the probability distribution at the beginning, and we do not know whether the object is within the room or outside the room.
Reference: [28] <author> T.D. Garvey. </author> <title> Perceptual strategies for purposive vision. </title> <type> Technical Report Tech. Note 117, </type> <institution> SRI International, </institution> <year> 1976. </year>
Reference: [29] <author> Thomas D. Garvey. </author> <title> Perceptual strategies for purposive vision. </title> <type> Technical Report Technical Note 117, </type> <institution> SRI International, </institution> <year> 1976. </year>
Reference-contexts: For the above two examples, the solution to the problem of where to efficiently direct the sensor is critical. Although sensor planning for object search is very important in practice, it is interesting to note that research on this topic within the computer vision community is limited ([77], [89], <ref> [29] </ref>, [54], [63]). Connell et al. [20] have constructed a robot that roams an area searching for and collecting soda cans. The planning is very simple since the robot just follows the walls of the room and the sensor only searches the area immediately in front of the robot. <p> Probability of presence is used in their system, but the purpose of the sensor planning is mainly answering average queries that can be posed on a Bayes Net rather than searching. Chapter 1. Introduction 5 The indirect search mechanism proposed by Garvey <ref> [29] </ref> first directs the sensor to search for an "intermediate" object that commonly participates in a spatial relationship with the target, and then directs the sensor to examine the restricted region specified by this relationship. <p> this basic set of candidate positions. 8.2 Comparison with Wixson's Work The existing works that are directly concerned with the task of searching for a 3D object within a 3D environment are: Connell et al.'s work [20] on searching for and collecting soda cans by an artificial creature, Garvey's proposal <ref> [29] </ref> for indirect search and Wixson et. al's work [88] 82 Chapter 8. Discussion 83 on efficiency analysis of direct and indirect search and on viewpoint selection for object search. Connell [20] describes a real, fully-functional mobile robot which operates in an unstructured environment to collect empty soda cans. <p> The navigation is also simple, as the robot is constrained to only move along the walls, and it is not required that the robot be able to examine every part of the region. Although the strategy of indirect search is proposed by Garvey <ref> [29] </ref>, it is Wixson and Ballard who first study the theory of efficiency of direct and indirect search. From a certain point of view, Wixson is the first to seriously study the task of 3D object search and to propose the formula for search efficiency.
Reference: [30] <author> J.J. Gibson. </author> <title> The perception of the visual world. </title> <address> Houghton-Mi*in, Boston, </address> <year> 1950. </year>
Reference: [31] <author> J.J. Gibson. </author> <title> The Ecological Approach to Visual Perception. </title> <address> Houghton-Mi*in, Boston, </address> <year> 1979. </year>
Reference: [32] <author> W.E.L. </author> <title> Grimson. The combinatorics of local constraints in model-based recognition and localization from sparse data. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 33(4) </volume> <pages> 658-686, </pages> <year> 1986. </year>
Reference: [33] <author> D.H. Hubel. </author> <title> Eye, Brain and Vision. </title> <publisher> Volume 22 of Scientific American Library W.H.Freeman and Company, </publisher> <year> 1988. </year>
Reference: [34] <author> K. </author> <title> Ikeuchi. Precompiling a geometrical model into an interpretation tree for object recognition in bin-picking tasks. </title> <booktitle> In Image Understanding Workshop, </booktitle> <pages> pages 321-330, </pages> <address> California, </address> <month> February </month> <year> 1987. </year>
Reference-contexts: An aspect graph with nodes being assigned values representing "goodness" of view is suggested to guide the motion of the camera on the sphere. Ikeuchi <ref> [34] </ref> precompiles a geometrical model into an interpretation tree which distinguishes among viewpoints based on the set of visible faces. This tree is then used to determine attitude by using the optimal features at each determining process.
Reference: [35] <author> Chris L. Jackins and Steven L. Tanimoto. </author> <title> Oct-trees and their use in representing three-dimensional objects. </title> <journal> Computer Graphics and Image Processing, </journal> (14):249-270, 1980. 
Reference-contexts: Here we assume independence between actions In our approach, the environment is tessellated into identical unit cubes. There are many other approaches to model a three-dimensional environment or a three-dimensional object. Chapter 2. Problem Formulation 20 Among them, octree representation <ref> [35] </ref> [59] is an economical approaches in terms of memory space. It provides a representation method that falls into the "recursive" category. The environment represented by an oct-tree is assumed to be 2 u fi 2 u fi 2 u arrays of unit cubes.
Reference: [36] <author> R. Jain, S.L. Bartlett, and N. O'Brien. </author> <title> Motion stereo using ego-motion complex logarithmic mapping. </title> <journal> IEEE Transactions on Pattern analysis and Machine Intelligence, </journal> <volume> PAMI-9(3):356-369, </volume> <month> May </month> <year> 1987. </year>
Reference: [37] <author> Tarabanis K., P. Allen, and R. Tsai. </author> <title> A survey of sensor planning in computer vision. </title> <type> Technical report, </type> <institution> Comp. Sci. Dept., Columbia University, </institution> <year> 1992. </year>
Reference: [38] <author> J.R. Kender and Freudenstein. </author> <title> What is a "degenerate" view? In Image Understanding Workshop, </title> <address> pages 589-598, California, </address> <month> February </month> <year> 1987. </year>
Reference: [39] <author> H.S. Kim, R.C Jain, and R. A. Volz. </author> <title> Object recognition using multiple views. </title> <booktitle> In 1985 IEEE Intervnational Conference on Robotics and Automation, </booktitle> <pages> pages 28-33, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Maver and Bajcsy [53][54] develop a strategy to determine the sequence of different views in order to see a portion of the visual field otherwise hidden. Kim <ref> [39] </ref> has studied the problem of determining the camera viewpoints for successive views looking for distinguishing features of object. The distance of the camera to the object is determined by the size of the object and the size of the feature.
Reference: [40] <author> L. Kirousis and C. Papadimitriou. </author> <title> The complexity of recognizing polyhedral scenes. </title> <booktitle> In 26 Annual Symposium on Foundations of Computer Science, </booktitle> <address> Portland, Ore., </address> <year> 1985. </year>
Reference-contexts: Chapter 2. Problem Formulation 25 Tsotsos [78] also ties the concept of active perception to attentive processing in general, and to his complexity level analysis of visual search, and proves that active unbounded visual search is NP-Complete. Kirousis and Papadimitriou <ref> [40] </ref> show that the problem of polyhedral scene labeling is inherently NP-Complete. Many other vision researchers ([32],[58], etc.) routinely provide an analysis of the complexity of their proposed algorithms.
Reference: [41] <author> J. Koenderink and A. van Doorn. </author> <title> The internal representation of solid shape with respect to vision. </title> <journal> Biological Cybernetics, </journal> <volume> 32 </volume> <pages> 211-216, </pages> <year> 1979. </year> <note> BIBLIOGRAPHY 172 </note>
Reference: [42] <author> B. O. Koopman. </author> <title> Search and Screen: general principles with historical applications. </title> <publisher> Perga-man Press, </publisher> <address> Elmsford, N.Y, </address> <year> 1980. </year>
Reference-contexts: Furthermore, there is no sensor planning involved in searching the environment for the "intermediate" object or in searching the restricted region for the target. It is interesting to note that the operational research community has done a lot of research on optimal search <ref> [42] </ref>. Their purpose is to determine how to allocate effort to search for a target, such as a lost submarine in the ocean or an oil field within a certain region.
Reference: [43] <author> David J. Kriegman, Ernst Triendl, and Thomas O. Binford. </author> <title> Stereo vision and navigation in building for mobile robots. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 5(6) </volume> <pages> 792-803, </pages> <month> December </month> <year> 1989. </year>
Reference: [44] <author> E. Krotkov. </author> <title> Active computer vision by cooperative focus and stereo. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989. </year>
Reference: [45] <author> Eric Krotkov. </author> <title> Focusing. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1 </volume> <pages> 223-237, </pages> <year> 1987. </year>
Reference: [46] <author> K.N. Kutulakos and Dyer. </author> <title> Recovering shape by purposive viewpoint adjustment. </title> <booktitle> In Proceedings of Computer Vision and Pattern Recognition, </booktitle> <pages> pages 16-22, </pages> <address> Illinois,USA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Previous Work on Sensor Planning 57 defined to be a position at which the lengths of two non-parallel object lines are maximized. They propose a set of behaviors that can move the camera to the standard view based on the low level image data. Kutulakos and Dyer <ref> [46] </ref> present a sensor planning approach in which the observer can purposefully change viewpoint in order to achieve a well-defined geometric relationship with respect to a 3D shape prior to its recognition.
Reference: [47] <author> H.J. Levesque. </author> <booktitle> What is planning in the presence of sensing? In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1996. </year>
Reference: [48] <author> H.J. Levesque, R. Reiter, Y. Lespirance, F. Lin, and R. Scherl. Golog: </author> <title> A logic programming language for dynamic domains. </title> <journal> Journal of Logic Programming, </journal> <note> Special issue on Reasoning about Action and Change, to appear. </note>
Reference: [49] <author> S. Li. </author> <title> Realizing active vision by a mobile robot. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 205-209, </pages> <year> 1991. </year>
Reference: [50] <author> D. Marr. Vision. W. H. Freeman, </author> <year> 1982. </year>
Reference-contexts: These techniques were used to segment 2-D images based on gray value intensities. In general, engineering approaches to vision require ideal input data or specific domain dependent assumptions. These approaches were rejected by David Marr <ref> [50] </ref>, because they were often ad hoc, with no sound theoretical basis relating the type of processing to the actual visual content. As a result, these approaches could not be generalized to arbitrary scene interpretation.
Reference: [51] <author> Massone, Sandini, and Tagliasco. </author> <title> "form-invariant" topological strategy for 2d shape recognition. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 30 </volume> <pages> 169-188, </pages> <year> 1985. </year>
Reference: [52] <author> L. Mathies and S. A. Shafer. </author> <title> Error modeling in stereo navigation. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> RA-3(3):239-248, </volume> <year> 1987. </year>
Reference: [53] <author> J. Maver and R. </author> <title> Bajcsy. Occlusions and the next view planning. </title> <booktitle> In IEEE 1992 International Conference on Robotics and Automation, </booktitle> <pages> pages 1806-1811, </pages> <month> April </month> <year> 1992. </year>
Reference: [54] <editor> Jasna Maver and Ruzena Bajcsy. </editor> <title> How to decide from the first view where to look next. </title> <booktitle> In Proceedings of the DARPA Image Understanding Workshop, </booktitle> <month> September </month> <year> 1990. </year>
Reference-contexts: Although sensor planning for object search is very important in practice, it is interesting to note that research on this topic within the computer vision community is limited ([77], [89], [29], <ref> [54] </ref>, [63]). Connell et al. [20] have constructed a robot that roams an area searching for and collecting soda cans. The planning is very simple since the robot just follows the walls of the room and the sensor only searches the area immediately in front of the robot.
Reference: [55] <author> Merriam-Webster. </author> <title> Webster's Ninth New Collegiate Dictionary. </title> <address> Merriam-Webster, Spring-field, Mass., </address> <year> 1985. </year>
Reference-contexts: Navigation is a fundamental requirement of autonomous mobile robots. It is defined as "the science of getting ships, aircraft, or spacecraft from place to place; esp: the method of determining position, course, and distance traveled" <ref> [55] </ref>. It is surprising to find that position estimation for mobile robots is still problematic, given the available knowledge and experience in the land, marine and aerospace communities. One reason might be a lack of knowledge regarding the navigation literature from other disciplines.
Reference: [56] <author> Randal C. Nelson. Imtroduction: </author> <title> Vision as intelligent behavior: An introduction to machine vision at the university of rochester. </title> <journal> International Journal Computer Vision, </journal> <volume> 7(1), </volume> <year> 1991. </year> <note> BIBLIOGRAPHY 173 </note>
Reference: [57] <author> S.B. Nickerson, M. Jenkin, E. Milios, B. Down, P. Jasiobedzki, A. Jepson, D. Terzopoulos, J. Tsotsos, D. Wilkes, N. Bains, and K. Tran. </author> <title> Ark: Autonomous navigation of a mobile robot in a known environment. </title> <booktitle> In Intelligent Autonomous Systems-3, </booktitle> <pages> pages 288-296, </pages> <address> Pennsylvania, USA, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: The mirrors are used to ensure collinearity of effective optical axes of the camera lens and the range finder. The robot can move freely in an office-like environment or a known industrial environment. The laser can be "emitted" from the center of the camera (see <ref> [57] </ref> for detail) in any direction so as to measure the distance of objects in the environment to the center of the camera along this direction.
Reference: [58] <author> W. H. Plantinga and C. R. Dyer. </author> <title> An algorithm for constructing the aspect graph. </title> <booktitle> In 27 Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 123-131, </pages> <address> Toronto, Ontario, </address> <year> 1986. </year>
Reference: [59] <author> Michael Potmesil. </author> <title> Generating octree models of 3d objects from their silhouettes in a sequence of images. </title> <journal> Computer Graphics and Image Processing, </journal> (40):1-29, 1987. 
Reference-contexts: Here we assume independence between actions In our approach, the environment is tessellated into identical unit cubes. There are many other approaches to model a three-dimensional environment or a three-dimensional object. Chapter 2. Problem Formulation 20 Among them, octree representation [35] <ref> [59] </ref> is an economical approaches in terms of memory space. It provides a representation method that falls into the "recursive" category. The environment represented by an oct-tree is assumed to be 2 u fi 2 u fi 2 u arrays of unit cubes.
Reference: [60] <author> R.A.Brooks and J.H.Connell. </author> <title> Asynchronous distributed control system for a mobile robot. </title> <booktitle> In Mobile Robots:Proc. SPIE 727, </booktitle> <address> California, </address> <year> 1987. </year>
Reference-contexts: The cycle repeats as the robot ventures forth once again on its quest for cans. Although the task of the robot is to collect soda cans, the purpose of Connell's research is to test Brooks's subsumption architecture <ref> [60] </ref> [17] rather than to study object search. Connell's system is unusual in that it is controlled by a collection of independent behaviors rather than a centralized program, and because it uses a minimal amount of state.
Reference: [61] <author> Raymond D. Rimey. </author> <title> Controlling eye movements with hidden markov models. </title> <journal> International Journal Computer Vision, </journal> <volume> 7(1), </volume> <year> 1991. </year>
Reference: [62] <author> R.D. Rimey. </author> <title> Task-specific utility in a general bayes net vision system. </title> <booktitle> In Proceedings of Computer Vision and Pattern Recognition, </booktitle> <pages> pages 142-147, </pages> <address> Illinois,USA, </address> <month> June </month> <year> 1992. </year>
Reference: [63] <author> R.D. Rimey and C.M. Brown. </author> <title> Where to look next using a bayes net: Incorporating geometric relations. </title> <booktitle> In Second European Conference on Computer Vision, </booktitle> <pages> pages 542-550, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Although sensor planning for object search is very important in practice, it is interesting to note that research on this topic within the computer vision community is limited ([77], [89], [29], [54], <ref> [63] </ref>). Connell et al. [20] have constructed a robot that roams an area searching for and collecting soda cans. The planning is very simple since the robot just follows the walls of the room and the sensor only searches the area immediately in front of the robot. <p> This may not be very efficient since the likely presence of the target is not considered when the robot is roaming. Rimey and Brown <ref> [63] </ref> used composite Bayes net and utility decision rules to plan the sensor in their task-oriented system, TEA. The sensor is directed to the center of mass of the expected area for a certain object based on the belief value of the net. <p> Swain et al. [69][70] propose a strategy of using colour as a low resolution cue to guide the fovea (corresponding to the small viewing angle size) in order to find an object whose identity is known. In Rimey and Brown's task-oriented system T EA <ref> [63] </ref>, the low resolution camera provides a peripheral image of the entire field of view, while the high resolution camera can be moved within the field of view to examine the object in detail and to do verification.
Reference: [64] <author> M.P. Robins. </author> <title> Free-ranging automatic guided-vehicle system. </title> <journal> GEC Reviews, </journal> <volume> 2(2), </volume> <month> February </month> <year> 1986. </year>
Reference-contexts: Passive beacons have the advantages of robustness and reliability, because they require no power and are completely inert. They are also cheaper, since a single active transmitter on the vehicle can be served by many passive beacons. Robins <ref> [64] </ref> describes a automatic guided vehicles (AGV) developed by GEC Electrical Projects Ltd and been launched as a full product at the GEC Centenary Exhibition in 1986. <p> Although differing in principle, all these methods provide the AGV with a line to follow, and the localization problem can be solved by recognizing special markers placed on or in the floor <ref> [64] </ref>. In this case, we can predefine a basic set of candidate positions such that the robot can view the whole search space without occlusion from these positions.
Reference: [65] <author> G. </author> <title> Sandini. Active tracking strategy for monocular depth inference over multiple frames. </title> <journal> IEEE trans. on PAMI, </journal> <volume> 12 </volume> <pages> 13-27, </pages> <year> 1990. </year>
Reference-contexts: Using camera movement to constrain degrees of freedom through a fixation point can facilitate the computation of kinetic depth ([10]). When an observer is able to track an environmental point, the task of navigation and the task of depth computation will benefit ([13], <ref> [65] </ref>). Another benefit of purposefully moving cameras is to get a better view so as to make the task at hand easier. Wilkes and Tsotsos [87] propose the concept of active object recognition.
Reference: [66] <author> Bernt Schiele and James L. Crowley. </author> <title> Where to look next and what to look for. </title> <booktitle> In The 4th International Symposium on Intelligent Robotics Systems, </booktitle> <pages> pages 139-145, </pages> <address> Lisbon, </address> <month> July </month> <year> 1996. </year>
Reference-contexts: The concept of detection function is a function used to describe the statistical detection result with a given set of camera parameters and a given recognition when the target orientation and the background situation is allowed to change. It is interesting to note that Schiele and Crowley <ref> [66] </ref> recently propose a statistical approach to answer the question "where to look next". Their approach is based on the use of multidimensional histograms of local neighborhood operators and a network of salient points. <p> See Figure 9.2 for an illustration. Since the diameter of the region is 75, only three angle sizes 0.785398 (45 o ),0.386876 (22:166346 0 ), 0.164481 (9:424083 o ) are needed in the search process. Their effective ranges are [11; 27], <ref> [27; 66] </ref>, [66; 162], respectively. It is assumed that we do not know the probability distribution at the beginning, and we do not know whether the object is within the room or outside the room. <p> See Figure 9.2 for an illustration. Since the diameter of the region is 75, only three angle sizes 0.785398 (45 o ),0.386876 (22:166346 0 ), 0.164481 (9:424083 o ) are needed in the search process. Their effective ranges are [11; 27], [27; 66], <ref> [66; 162] </ref>, respectively. It is assumed that we do not know the probability distribution at the beginning, and we do not know whether the object is within the room or outside the room.
Reference: [67] <author> A. Shmuel and Werman. </author> <title> Active vision:3d from an image sequence. </title> <booktitle> In 10th International Conference on Pattern Recognition, </booktitle> <pages> pages 48-54, </pages> <address> New Jersey, USA, </address> <month> June </month> <year> 1990. </year>
Reference: [68] <author> Paul Suetens. </author> <title> Computational strategies for object recognition. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(1) </volume> <pages> 5-61, </pages> <year> 1992. </year>
Reference-contexts: Finally we discuss the concept of detection function in the general situation. 3.1 Object Recognition Object recognition is the task of finding and labeling parts of a two-dimensional image of a scene that correspond to an object in the scene [14] <ref> [68] </ref> [5] [86]. To perform the object recognition task, we need to establish models, or general descriptions of each object to be recognized. A model may include shape, texture, and context knowledge about the occurrence of such objects in a scene. <p> The computational strategies used by previous work in object recognition may be roughly classified into four classes according to two main characteristics: their suitability for complex image Chapter 3. The Detection Function 39 data and their suitability for complex models <ref> [68] </ref>. The first class is the "Feature Vector Classification" method. Feature vector methods rely on a trivial model of an object's image characteristics and are typically applied only to simple data. The second class is the "Fitting Models to Photometry" method.
Reference: [69] <author> Michael J. Swain. </author> <title> Low resolution cues for guiding saccadic eye movements. </title> <booktitle> In Proceedings of Computer Vision and Pattern Recognition, </booktitle> <pages> pages 737-740, </pages> <address> Illinois, USA, </address> <month> June </month> <year> 1992. </year> <note> BIBLIOGRAPHY 174 </note>
Reference: [70] <author> Michael J. Swain and D. Ballard. </author> <title> Color indexing. </title> <journal> International Journal Computer Vision, </journal> <volume> 7(1) </volume> <pages> 11-32, </pages> <year> 1991. </year>
Reference: [71] <author> M.J. Swain and M. Stricker. </author> <title> Promising directions in active vision. </title> <type> Technical Report CS 91-27, </type> <institution> Univ. Of Chicago Technical Report, </institution> <year> 1991. </year>
Reference: [72] <author> Konstantinos Tarabanis, Roger Y. Tsai, and Peter K. Allen. </author> <title> Automatic sensor placement from vision task requirements. CVGIP: Image Understanding, </title> <address> 10.(3):340-358, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Their approach converts each sensing requirement into a geometric constraint on the sensor location, from which the three-dimensional region of viewpoints that satisfy the constraint is computed. The intersection of these regions is the space where a sensor may be located. Tarabanis <ref> [72] </ref> also considers the above problems by analytically characterizing the domain of admissible camera locations, orientations, and optical settings. Their concept of a viewpoint is defined in a broader sense that includes not only viewer orientation and position, but also the optical settings associated with the viewpoint at hand.
Reference: [73] <author> C. E. Thorpe, M. Hebert, T. Kanade, and S. Shafer. </author> <title> Vision and navigation for the carnegie-mellon navlab. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 10(3) </volume> <pages> 362-373, </pages> <month> May </month> <year> 1988. </year>
Reference: [74] <author> J.K Tsotsos. </author> <title> A complexity level analysis of immediate vision. </title> <journal> International Journal Computer Vision, </journal> <volume> 4(1) </volume> <pages> 303-320, </pages> <year> 1988. </year>
Reference: [75] <author> J.K. Tsotsos. </author> <title> Analyzing vision at the complexity level. </title> <journal> The behavioral and brain science, </journal> <volume> 13 </volume> <pages> 423-469, </pages> <year> 1990. </year>
Reference-contexts: Complexity considerations are commonplace in the biological and computational vision literature. For example, Tsotsos <ref> [75] </ref> shows that the general problem of visual search (search for a target within an image) is computationally intractable in a formal, complexity-theoretic sense. Chapter 2.
Reference: [76] <author> J.K. Tsotsos. </author> <title> Active verses passive perception, which is more efficient? Behavioral and brain sciences, </title> <address> 14:4:770-773, </address> <year> 1991. </year>
Reference: [77] <author> J.K. Tsotsos. </author> <title> 3d search strategy. Internal ARK Working Paper, </title> <institution> Department of Computer Science, Univrsity of Toronto, </institution> <year> 1992. </year>
Reference-contexts: Although the results are elegant in a mathematical sense, they cannot be directly applied here because the searcher model is too abstract and general, and there is no sensor planning involved in their approach. 1.4 Thesis Statement and Overview As an elaboration of Tsotsos' work on 3D object search <ref> [77] </ref> and work concerning the efficiency of active and passive visual search [78], this thesis provides the first formalization of the sensor planning task for object search and a practical sensor planning strategy. <p> Each solid angle is associated with a radius which is the length of the emitting line in the direction of the central axis of the solid angle. The environment can thus be represented by the union of these solid angles. This representation is called the sensed sphere <ref> [77] </ref>. The sensed sphere can be constructed either by a laser or by stereo. To use stereo to construct the sensed sphere, we can first construct a sparse depth sampling of the environment by existing stereo algorithms [43][52][73][81] and then transform this into the representation of a sensed sphere.
Reference: [78] <author> J.K. Tsotsos. </author> <title> Active verses passive perception, which is more efficient? IJCV, </title> <year> 1992. </year>
Reference-contexts: Some problems that are ill-posed, nonlinear or unstable for a passive observer can be converted into well-posed, linear or stable problems by an active observer. This improvement is due to the availability of extra constraints from the additional images obtained in the active vision process. As summarized by Tsotsos <ref> [78] </ref>, active vision can provide dynamic changes in the image acquisition process and is useful in at least the following ways: * to see a portion of the visual field otherwise hidden; * to compensate for spatial non-uniformity of a processing mechanism; * to increase spatial resolution; * to disambiguate aspects <p> Tsotsos <ref> [78] </ref> pointed out that in order to obtain the above advantages, there are significant hurdles to overcome, one of them being efficiency. <p> In fact, we must consider the cost imposed on a perceptual system if it must actively manage the sensor. As listed in <ref> [78] </ref>, these costs include: Chapter 1. <p> be directly applied here because the searcher model is too abstract and general, and there is no sensor planning involved in their approach. 1.4 Thesis Statement and Overview As an elaboration of Tsotsos' work on 3D object search [77] and work concerning the efficiency of active and passive visual search <ref> [78] </ref>, this thesis provides the first formalization of the sensor planning task for object search and a practical sensor planning strategy. <p> Instead of examining different aspects of object search individually and in some degree of isolation, the formulation emphasizes more the relationship between action and image processing and integrates the two into a complete system. As motivated by Tsotsos' discussion of incremental active perception <ref> [78] </ref>, our formulation combines the influence of the search agent's initial knowledge and the influence of the performance of the available recognition algorithms together and formalizes the task as an optimization problem, thus transforms Tsotsos' general notion about the cost and efficiency of a general active vision system into the corresponding <p> The region is determined by the camera's position, the viewing axis and the visual angle size, etc. This task is similar to the visual attention problem where limited resources are focused on a part of an image (2D). Thus, the object search problem is also an attention problem <ref> [78] </ref>. 2.2 The Search Agent The search agent has two components. The first is a sensor, which is used to detect the target and to sense the environment. Here, the sensor is a camera with zoom capabilities. <p> i=1 n X p (c i ; t f 2 )b (c i ; f 2 )] + f j=1 n X p (c i ; t f j )b (c i ; f j )] g fi [ i=1 and the total time for applying this allocation is (following <ref> [78] </ref>): T [F] = f 2F The task of object search is to find an allocation F O which satisfies T (F) K and maximizes P [F]. Chapter 2. Problem Formulation 17 2.4.3 The Environment The environment is assumed to be an office-like space or an industrial workspace. <p> Complexity considerations are commonplace in the biological and computational vision literature. For example, Tsotsos [75] shows that the general problem of visual search (search for a target within an image) is computationally intractable in a formal, complexity-theoretic sense. Chapter 2. Problem Formulation 25 Tsotsos <ref> [78] </ref> also ties the concept of active perception to attentive processing in general, and to his complexity level analysis of visual search, and proves that active unbounded visual search is NP-Complete. Kirousis and Papadimitriou [40] show that the problem of polyhedral scene labeling is inherently NP-Complete.
Reference: [79] <author> J.K. Tsotsos. </author> <title> On the relative complexity of active vs. passive visual search. </title> <journal> International Journal of Computer Vision, </journal> <volume> 7 </volume> <pages> 127-141, </pages> <year> 1992. </year>
Reference-contexts: Tsotsos [78] pointed out that in order to obtain the above advantages, there are significant hurdles to overcome, one of them being efficiency. It can be shown <ref> [79] </ref> that if a problem can be solved by both active and passive methods, then the active approach will be more efficient only under certain constraints. In fact, we must consider the cost imposed on a perceptual system if it must actively manage the sensor. <p> As a typical active vision task, an object search system must take all of the above factors into consideration. The work reported in this thesis can be viewed as a logical extension and elaboration of Tsotsos' work <ref> [79] </ref> concerning the efficiency of active and passive visual search. 1.3 Motivation Object search is a task that is particularly well suited to the active vision approach. Thus, the study of object search may produce results that will lead to an understanding of various aspects of an active vision system.
Reference: [80] <author> John K. Tsotsos, S. Dickinson, M. Jenkin, E. Milios, A. Jepson, B. Down, E. Amdur, S. Stevenson, M. Black, D. Metaxas, J. Cooperstock, S. Culhane, F. Nuflo, G. Verghese, W. Wai, D. Wilkes, and Yiming Ye. </author> <title> The playbot project. In IJCAI95: Workshop for handicapped Children, </title> <address> Montreal, Canada, </address> <year> 1995. </year>
Reference: [81] <author> M. A. Turk, D. G. Morgenthaler, K. D. Gremban, and M. Marra. </author> <title> Vits |- a vision system for autonomous land vehicle navigation. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 10(3) </volume> <pages> 362-373, </pages> <month> May </month> <year> 1988. </year>
Reference: [82] <author> W.E.Grimson. </author> <title> Sensing strategies for disambiguating among multiple objects in known poses. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> RA-2(4):196-213, </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: Ikeuchi [34] precompiles a geometrical model into an interpretation tree which distinguishes among viewpoints based on the set of visible faces. This tree is then used to determine attitude by using the optimal features at each determining process. Grimson <ref> [82] </ref> proposes an optimal sensing strategy for disambiguating among alternative interpretations of scenes containing multiple polyhedral objects (multiple objects in known poses or a single object with several consistent poses).
Reference: [83] <author> C.J. Westelius. </author> <title> Preattentive gaze control for robot vision. </title> <type> Technical Report Thesis No. 322, </type> <institution> Linkoping Studies in Science and Technology, </institution> <year> 1992. </year> <note> BIBLIOGRAPHY 175 </note>
Reference: [84] <author> P. Whaite. </author> <title> Uncertain views. </title> <booktitle> In Proceedings of Computer Vision and Pattern Recognition, </booktitle> <pages> pages 3-9, </pages> <address> Illinois,USA, </address> <month> June </month> <year> 1992. </year>
Reference: [85] <author> P. Whaite and F. P. Ferrie. </author> <title> From uncertainty to visual exploratory. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(10) </volume> <pages> 1308-1049, </pages> <year> 1991. </year>
Reference: [86] <author> D. Wilkes. </author> <title> Active object recognition. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1994. </year>
Reference-contexts: Due to the camera's limited field of view, occlusion, the limited depth of field of lenses, the lighting conditions and other factors, it is often necessary to obtain many images using many camera configurations and viewpoints (see <ref> [86] </ref> for further discussion). Also, a robot often needs to search for known landmarks in order to estimate its own position and navigate in an industrial environment. For the above two examples, the solution to the problem of where to efficiently direct the sensor is critical. <p> Finally we discuss the concept of detection function in the general situation. 3.1 Object Recognition Object recognition is the task of finding and labeling parts of a two-dimensional image of a scene that correspond to an object in the scene [14] [68] [5] <ref> [86] </ref>. To perform the object recognition task, we need to establish models, or general descriptions of each object to be recognized. A model may include shape, texture, and context knowledge about the occurrence of such objects in a scene. <p> The Detection Function 38 is the object identity. There are several desirable properties that a recognizer should have <ref> [86] </ref>. The first is that when an object in the system's known universe is present, the system should emit a label for the object; when the object is not present, its label should not be emitted. <p> The second is that the recognizer should exhibit competence in a wide variety of circumstances. The third is that a recognizer should exhibit a pleasing tradeoff, with competence and robustness on the one hand, and computational resources used on the other <ref> [86] </ref>. Object recognition is difficult. One of the difficulties comes from the process of identifying objects in the image. Because a combination of factors must be used in the identification process.
Reference: [87] <author> D. Wilkes and J.K. Tsotsos. </author> <title> Active object recognition. </title> <booktitle> In Proceedings of Computer Vision and Pattern Recognition, </booktitle> <pages> pages 136-141, </pages> <address> Illinois, USA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The second is the manipulation of the hardware so that the sensing operators can reach the state specified by the planner. This is possible because of the recent availability of sophisticated hardware for sensor/platform control and real-time image processing ([77], [6], <ref> [87] </ref>). The third is search for the target within the image. This is the object recognition and localization problem, which attracts a lot of attention within the computer vision community (see ([19]) for a review). <p> When an observer is able to track an environmental point, the task of navigation and the task of depth computation will benefit ([13], [65]). Another benefit of purposefully moving cameras is to get a better view so as to make the task at hand easier. Wilkes and Tsotsos <ref> [87] </ref> propose the concept of active object recognition. <p> The study of object search may lead to more insight into existing vision problems. For example, the problem of object recognition is less difficult when the image is intelligently grabbed by a controlled camera, and the image 134 Chapter 11. Conclusion 135 analysis is interpreted within specific contexts <ref> [87] </ref>. Of course, the simplicity obtained in the image analysis phase is at the cost of the complexity encountered in the camera controlling phase. Much work has been done on image analysis. It is now time to study the problems related to the control of the camera.
Reference: [88] <author> Lambert Ernest Wixson. </author> <title> Gaze Selection for Visual Search. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Rochester, </institution> <year> 1994. </year>
Reference-contexts: Wixson's Work The existing works that are directly concerned with the task of searching for a 3D object within a 3D environment are: Connell et al.'s work [20] on searching for and collecting soda cans by an artificial creature, Garvey's proposal [29] for indirect search and Wixson et. al's work <ref> [88] </ref> 82 Chapter 8. Discussion 83 on efficiency analysis of direct and indirect search and on viewpoint selection for object search. Connell [20] describes a real, fully-functional mobile robot which operates in an unstructured environment to collect empty soda cans.
Reference: [89] <author> L.E. Wixson. </author> <title> Exploiting world structure to efficiently search for objects. </title> <type> Technical Report Technical Report 434, </type> <institution> Dept. Comp. Sci, Univ. of Rochester, </institution> <year> 1992. </year>
Reference-contexts: For the above two examples, the solution to the problem of where to efficiently direct the sensor is critical. Although sensor planning for object search is very important in practice, it is interesting to note that research on this topic within the computer vision community is limited ([77], <ref> [89] </ref>, [29], [54], [63]). Connell et al. [20] have constructed a robot that roams an area searching for and collecting soda cans. The planning is very simple since the robot just follows the walls of the room and the sensor only searches the area immediately in front of the robot. <p> In previous work on sensor planning for object search, this knowledge is also used to varying extents. In the indirect search approach ([29], <ref> [89] </ref>), although this knowledge is not explicitly represented, it is implicitly used, because the target has a high probability of presence within the region specified by the geometric relation between the intermediate object and the target. <p> Whaite and Ferrie [84][85] propose a method that can use the spatial structure of uncertainty to guide the acquisition of additional data in a way that improves the knowledge of the scene. Wixson and Ballard [90] <ref> [89] </ref> study the benefits of combining a high resolution image (corresponding to wide camera angle size) with a low resolution image (corresponding to small camera angle size) in the task of object search.
Reference: [90] <author> L.E. Wixson. </author> <title> Looking near one object for another. </title> <booktitle> SPIE.Intelligent Robots and Computer Vision, </booktitle> <address> 1825(XI):159-167, </address> <year> 1992. </year>
Reference-contexts: Whaite and Ferrie [84][85] propose a method that can use the spatial structure of uncertainty to guide the acquisition of additional data in a way that improves the knowledge of the scene. Wixson and Ballard <ref> [90] </ref> [89] study the benefits of combining a high resolution image (corresponding to wide camera angle size) with a low resolution image (corresponding to small camera angle size) in the task of object search.
Reference: [91] <author> L.E. Wixson and D. Ballard. </author> <title> Using intermediate object to improve the efficiency of visual search. </title> <journal> International Journal on Computer Vision, </journal> <volume> 18(3) </volume> <pages> 209-230, </pages> <year> 1994. </year>
Reference-contexts: Chapter 1. Introduction 5 The indirect search mechanism proposed by Garvey [29] first directs the sensor to search for an "intermediate" object that commonly participates in a spatial relationship with the target, and then directs the sensor to examine the restricted region specified by this relationship. Wixson and Ballard <ref> [91] </ref> presented a mathematical model of search efficiency and predicted that indirect search can improve efficiency in many situations.
Reference: [92] <author> Yiming Ye and John K. Tsotsos. </author> <title> Tracking with pan, tilt, </title> <journal> and zoomable camera. </journal> <note> In In preparation. </note>
Reference: [93] <author> Yiming Ye and John K. Tsotsos. </author> <title> Sensor planning for object search. </title> <type> Technical Report RBCV-TR-94-47, </type> <institution> Computer Science Department, University of Toronto, </institution> <year> 1994. </year>
Reference: [94] <author> Yiming Ye and John K. Tsotsos. </author> <title> The detection function in object search. </title> <booktitle> In Proceedings of the fourth international conference for young computer scientists, </booktitle> <pages> pages 868-873, </pages> <address> Beijing, PRC, </address> <month> August </month> <year> 1995. </year>
Reference: [95] <author> Yiming Ye and John K. Tsotsos. </author> <title> Sensor planning in 3d object search: its formulation and complexity. </title> <booktitle> In The 4th International Symposium on Artificial Intelligence and Mathematics, </booktitle> <address> Florida, U.S.A., </address> <month> January </month> <year> 1995. </year>
Reference: [96] <author> Yiming Ye and John K. Tsotsos. </author> <title> Where to look next in 3d object search. </title> <booktitle> In IEEE International Symposium for Computer Vision, </booktitle> <pages> pages 539-544, </pages> <address> Florida, U.S.A., </address> <month> November </month> <year> 1995. </year>
Reference: [97] <author> Yiming Ye and John K. Tsotsos. </author> <title> On the collaborative object search team: a formulation. In ICMAS-96 workshop on Learning, Interactions and Organizations in Multiagent Environment, </title> <address> Kyoto, Japan, </address> <month> December </month> <year> 1996. </year> <note> BIBLIOGRAPHY 176 </note>
Reference: [98] <author> Yiming Ye and John K. Tsotsos. </author> <title> A robot agent that can search. </title> <booktitle> In Proceedings of the sixth annual CAS conference, </booktitle> <institution> IBM Canada Ltd. Laboratory Centre for Advanced Studie, Toronto, </institution> <address> Ontario, Canada, </address> <month> November </month> <year> 1996. </year>
Reference: [99] <author> Yiming Ye and John K. Tsotsos. </author> <title> Sensor planning in 3d object search. </title> <booktitle> In The 4th INTERNATIONAL SYMPOSIUM on INTELLIGENT ROBOTIC SYSTEMS '96, </booktitle> <address> Lisbon, Portugal, </address> <month> July </month> <year> 1996. </year>
Reference: [100] <author> Yiming Ye and John K. Tsotsos. </author> <title> Knowledge difference and its influence on a search agent. </title> <booktitle> In First International Conference on Autonomous Agents, </booktitle> <address> Marina del Rey, California, </address> <month> February </month> <year> 1997. </year>
Reference: [101] <author> Yiming Ye and John K. Tsotsos. </author> <title> On the collaborative object search team: a formulation. </title> <booktitle> In Learning in DAI Systems, Gerhard Weiss edited, Lecture Notes in Artificial Intelligence, </booktitle> <publisher> Springer Verlag, </publisher> <year> 1997. </year>
Reference: [102] <author> Yiming Ye, John K. Tsotsos, and Karen Bennet. </author> <title> Live world wide web and its agents: the cas lwww project. </title> <booktitle> In Second International Conference on Multiagent Systems: </booktitle> <address> ICMAS'96, Kyoto, Japan, </address> <month> December </month> <year> 1996. </year>
Reference: [103] <author> Seungku Yi. </author> <title> Automatic sensor and light source positioning for machine vision. </title> <booktitle> In 10th International Conference on Pattern Recognition, </booktitle> <pages> pages 55-59, </pages> <address> New Jersey, USA, </address> <month> June </month> <year> 1990. </year>
Reference: [104] <author> J.Y. Zheng, F. Kishino, Q. Chen, and S. Tsuji. </author> <title> Active camera controlling for manipulation. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition, </booktitle> <address> Washington, </address> <year> 1991. </year> <note> BIBLIOGRAPHY 177 </note>
Reference: [9] <institution> [44] [33] [30] [31] [75] [79] [36] [74] [56] [61] [70] [51] [82] [45] [83] [71] [28] [15] [60] [53] </institution>
Reference: [39] <institution> [41] [8] [85] [16] [12] [37] [42] [57] [58] [77] [78] [75] [87] [86] [93] [80] [94] [95] [96] [99] [102] BIBLIOGRAPHY 178 </institution>
Reference-contexts: Maver and Bajcsy [53][54] develop a strategy to determine the sequence of different views in order to see a portion of the visual field otherwise hidden. Kim <ref> [39] </ref> has studied the problem of determining the camera viewpoints for successive views looking for distinguishing features of object. The distance of the camera to the object is determined by the size of the object and the size of the feature.
References-found: 106

