URL: ftp://ftp.cs.arizona.edu/reports/1998/TR98-08.ps
Refering-URL: http://www.cs.arizona.edu/research/reports.html
Root-URL: http://www.cs.arizona.edu
Title: Scalable Web Server Design for Distributed Data Management  
Author: Scott M. Baker Bongki Moon 
Address: Tucson, AZ 85721  
Affiliation: Department of Computer Science The University of Arizona  
Note: August  
Pubnum: Technical Report  
Email: fbakers,bkmoong@cs.arizona.edu  
Date: 98-8  1998  
Abstract: Traditional techniques for a distributed web server design rely on manipulation of central resources, such as routers or DNS services, to distribute requests designated for a single IP address to multiple web servers. The goal of the Distributed Cooperative Web Server (DCWS) system development is to explore application-level techniques for distributing web content. We achieve this by dynamically manipulating the hyperlinks stored within the web documents themselves. The DCWS system effectively eliminates the bottleneck of centralized resources, while balancing the load among distributed web servers. DCWS servers may be located in different networks, or even different continents and still balance load effectively. DCWS system design is fully compatible with existing HTTP protocol semantics and existing web client software products. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Virgilio Almeida, Azer Bestavros, Mark Crovella, and Adriana de Oliveira. </author> <booktitle> Characterizing reference locality in the WWW. In the 4th International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 92-103, </pages> <address> Miami Beach, FL, </address> <month> December </month> <year> 1996. </year>
Reference-contexts: The benchmark is intended to correspond to the real-world behavior that most clients exhibit while they are accessing the web. Web clients running browsers typically maintain a client-side cache. This client side cache effects access patterns significantly and reduces temporal locality <ref> [1] </ref>. With the custom client benchmark, we sought to simulate this caching behavior by building a client-side cache into the benchmark program. The cache is maintained for the duration of each simulated access sequence (1-25 document requests) of the benchmark and reset after each sequence.
Reference: [2] <author> Eric Anderson, Dave Patterson, and Eric Brewer. </author> <title> The MagicRouter: An application of fast packet interposing. </title> <note> Submitted for publication. </note>
Reference-contexts: It is intended to be a general purpose solution, capable of handling other services in addition to the web. No discussion is given in the white paper as to what extent the LocalDirector is a bottleneck of the system. 2 Fast Packet Interposing is a user-level technique developed in <ref> [2] </ref> and is used by the MagicRouter to distribute load. The MagicRouter is used to make a cluster of servers appear to have a single IP address without modifications to any servers. Fast Packet Interposing is used to modify network addresses within the data packets that pass through the MagicRouter.
Reference: [3] <author> D. Andresen, T. Yang, O. Egecioglu, O. Ibarra, and T. Smith. </author> <title> Scalability issues for high performance digital libraries on the world wide web. </title> <booktitle> In Proceedings of ADL'96 Forum on Research and Technology Advances in Digital Libraries, </booktitle> <pages> pages 91-100, </pages> <address> Washington D.C., </address> <month> May </month> <year> 1996. </year>
Reference-contexts: A potential problem with DNS rotation is the development of "hot spots", which lead to serious load imbalances. A detailed study of the techniques for an online digital library was reported in <ref> [3] </ref>. Dynamic server selection [10] is proposed as a client-based solution, in which clients automatically determine the best server for a given file without a priori knowledge of server performance. The technique relies on replication of web documents by proxy servers.
Reference: [4] <author> Daniel Andresen, Tao Yang, and Oscar H. Ibarra. </author> <title> Toward a scalable distributed WWW server on workstation clusters. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 42 </volume> <pages> 91-100, </pages> <year> 1997. </year>
Reference-contexts: Probabilistic and deterministic algorithms based on adaptive TTL (time-to-live) approach have been proposed [7]. Lower TTL values are assigned when the DNS chooses a less capable server or an address mapping request comes from a hot client. Another solution proposed in <ref> [4] </ref> attempts to develop a distributed scheduling heuristic based on a multi-variate cost function (CPU, disk and network utilization), which helps make the decision on task migration. Two techniques are used for load balancing: DNS rotation and HTTP URL redirection.
Reference: [5] <author> Martin F. Arlitt and Carey L. Williamson. </author> <title> Internet web servers: Workload characterization and performance implications. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 5(5) </volume> <pages> 631-645, </pages> <month> October </month> <year> 1997. </year>
Reference-contexts: It also enables the web servers to take advantage of geographic caching of documents <ref> [5] </ref>. The distributed cooperative web server solution poses the following benefits over traditional systems based on packet-level manipulation, or domain name services (DNS) and distributed file systems: * Network or packet level manipulation is not necessary. <p> As we expected, in contrast, the CPS measures were observed in the reverse order. Since real-world web transactions are fairly small <ref> [5] </ref>. we chose to use CPS as a balancing metric rather than BPS in Section 4. On the other hand, it is possible that in a system which uses significantly larger file sizes (such as the Sequoia storage benchmark data set), BPS may be a better load balancing metric.
Reference: [6] <author> Azer Bestavros, Mark Crovella, Jun Liu, and David Martin. </author> <title> Distributed packet rewriting and its application to scalable server architectures. </title> <type> Technical Report TR-98-003, </type> <institution> Boston University, </institution> <address> Boston, MA, </address> <month> February </month> <year> 1998. </year>
Reference-contexts: Fault tolerance and Load Balancing are stressed by the paper. The MagicRouter is expected to be a bottleneck as all packets must arrive through it as a central resource. A technique called dynamic packet rewriting (DPR) <ref> [6] </ref> is used to distribute load. DPR attempts to route requests at the IP level, by manipulating the methods in which an IP address is mapped to a host. It is a distributed algorithm, and attempts to eliminate the bottleneck of a centralized solution, such as a centralized round-robin router.
Reference: [7] <author> Michele Colajanmi and Philip S. Yu. </author> <title> Adaptive TTL schemes for load balancing of distributed web servers. </title> <journal> ACM Sigmetrics Performance Evaluation Review, </journal> <volume> 25(2) </volume> <pages> 36-42, </pages> <month> September </month> <year> 1997. </year>
Reference-contexts: Two-tier round-robin DNS scheduling divides clients into two classes normal and hot to handle non-uniform distribution of client requests [8]. Probabilistic and deterministic algorithms based on adaptive TTL (time-to-live) approach have been proposed <ref> [7] </ref>. Lower TTL values are assigned when the DNS chooses a less capable server or an address mapping request comes from a hot client.
Reference: [8] <author> Michele Colajanmi, Philip S. Yu, and Daniel M. Dias. </author> <title> Scheduling algorithms for distributed web servers. </title> <booktitle> In Proceedings of the 17th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 169-176, </pages> <address> Baltimore, MD, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: Numerous variations of the round-robin DNS scheduling have been proposed for heterogenous web servers and non-uniform client distribution. Two-tier round-robin DNS scheduling divides clients into two classes normal and hot to handle non-uniform distribution of client requests <ref> [8] </ref>. Probabilistic and deterministic algorithms based on adaptive TTL (time-to-live) approach have been proposed [7]. Lower TTL values are assigned when the DNS chooses a less capable server or an address mapping request comes from a hot client.
Reference: [9] <institution> Standard Performance Evaluation Corporation. SPECweb96 benchmark. </institution> <address> http://www.specbench.org/- osg/web96/, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: url from its server if it is not in the cache. request all embedded images in parallel (using helper threads). wait until all the requested documents arrive. parse the document and select a new link from the document set current url new link end end Conventional benchmarks such as SPECweb96 <ref> [9] </ref> are not suitable as they are designed to request documents without regard to the hyperlinks contained within the documents. The benchmark is intended to correspond to the real-world behavior that most clients exhibit while they are accessing the web. Web clients running browsers typically maintain a client-side cache.
Reference: [10] <author> Mark E. Crovella and Robert L. Carter. </author> <title> Dynamic server selection in the internet. </title> <booktitle> In the Third IEEE Workshop on the Architecture and Implementation of High Performance Communication Subsystems (HPCS'95), </booktitle> <month> August </month> <year> 1995. </year> <note> Also available as TR-95-014, </note> <institution> Boston University. </institution>
Reference-contexts: A potential problem with DNS rotation is the development of "hot spots", which lead to serious load imbalances. A detailed study of the techniques for an online digital library was reported in [3]. Dynamic server selection <ref> [10] </ref> is proposed as a client-based solution, in which clients automatically determine the best server for a given file without a priori knowledge of server performance. The technique relies on replication of web documents by proxy servers.
Reference: [11] <author> D. M. Dias, W. Kish, R. Mukherjee, and R. Tewari. </author> <title> A scalable and highly available web server. </title> <booktitle> In Proceedings of IEEE COMPCON Conference on Technologies for the Information Superhighway, </booktitle> <pages> pages 85-92, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: The IBM scalable web server is built on an SP-2 parallel system, which is essentially a cluster of identical RS6000 workstations. The IBM web server uses a TCP router instead of DNS scheduling for improved load balancing <ref> [11] </ref>, but its use is limited to tightly coupled systems such as SP-2. The presence of heterogeneous web servers not only increase the complexity of the DNS scheduling, but also makes a simple round-robin scheduling not directly applicable.
Reference: [12] <author> Peter Eriksson. </author> <note> Phttpd amultithreaded web server. http://www.signum.se/phttpd. </note>
Reference-contexts: The more traditional pool-of-processes approach would have made sharing statistical data such as the Local Document Graph and Global Load Table difficult and inefficient. Some popular web servers, including Apache [14], are either multithreaded or have experimental multithreaded efforts. <ref> [15, 12] </ref>. A general purpose HTML parser has been used to build simple parse trees for HTML source files.
Reference: [13] <author> R. Fielding, J. Gettys, J. C. Mogul, H. Frystyk, L. Masinter, P. Leach, and T. Berners-Lee. </author> <title> Hypertext transfer protocol - HTTP/1.1. http://www.ietf.cnri.reston.va.us/internet-drafts/draft-ietf-http-v11-spec-rev-03.txt, March 1997. Internet Draft. </title>
Reference-contexts: The algorithmic procedure for the document selection is presented in Algorithm 1 in Figure 4. 2 These extension headers are ignored by any server which does not understand them, but may be interpreted by a server that does understand the extension header <ref> [13] </ref>. 6 It is typical that well-known entry points will provide users with an external view of the server. Users will use the well-known entry points to gain access to the server.
Reference: [14] <author> Roy T. Fielding and Gail E. Kaiser. </author> <title> The Apache HTTP server project. </title> <journal> IEEE Internet Computing, </journal> <volume> 1(4) </volume> <pages> 88-90, </pages> <month> July/August </month> <year> 1997. </year>
Reference-contexts: The more traditional pool-of-processes approach would have made sharing statistical data such as the Local Document Graph and Global Load Table difficult and inefficient. Some popular web servers, including Apache <ref> [14] </ref>, are either multithreaded or have experimental multithreaded efforts. [15, 12]. A general purpose HTML parser has been used to build simple parse trees for HTML source files.
Reference: [15] <author> Dean Gaudet. </author> <booktitle> Apache performance notes. </booktitle> <address> http://www.apache.org/docs/misc/perf-tuning.html. </address>
Reference-contexts: The more traditional pool-of-processes approach would have made sharing statistical data such as the Local Document Graph and Global Load Table difficult and inefficient. Some popular web servers, including Apache [14], are either multithreaded or have experimental multithreaded efforts. <ref> [15, 12] </ref>. A general purpose HTML parser has been used to build simple parse trees for HTML source files.
Reference: [16] <author> Eric Dean Katz, Michelle Butler, and Robert McGrath. </author> <title> A scalable HTTP server: The NCSA prototype. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 27 </volume> <pages> 155-164, </pages> <year> 1994. </year>
Reference-contexts: Web accessibility will be an essential component of the services that future digital libraries should provide for clients. This need has created a strong demand for database access capability through the internet [19], and high performance scalable web servers <ref> [16, 23] </ref>. <p> The packet router is expected to be a bottleneck as all packets must pass through it. On the other hand, the work load distribution may also be achieved by using a custom domain name service (DNS) to rotate the one host name amongst many IP addresses <ref> [16] </ref>. However, this is a very coarse-grained solution since DNS mappings may be cached by multiple levels within the hierarchy of services. DNS mappings do have a time-to-live (TTL) parameter which specifies how long the information is considered valid. <p> The NCSA scalable web server is built on a cluster of identically configured servers, and uses round-robin DNS scheduling and Andrew file system (AFS) for load sharing among the servers <ref> [16, 18] </ref>. The IBM scalable web server is built on an SP-2 parallel system, which is essentially a cluster of identical RS6000 workstations.
Reference: [17] <author> B. Krishnamurthy and C. E. Wills. </author> <title> Piggyback server invalidation for proxy cache coherency. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 30 </volume> <pages> 185-193, </pages> <month> April </month> <year> 1998. </year> <month> 16 </month>
Reference-contexts: Such a solution would be wasteful in a system where network bandwidth is an important resource. Instead, the solution that was chosen was to piggyback the load information onto existing HTTP transfers. The idea of piggybacking information has been used for cache coherency by server invalidation <ref> [17] </ref>. The HTTP protocol allows for inserting extension headers into the existing protocol semantics. 2 Extension headers may be included in both the HTTP request (client to server) and the response (server to client).
Reference: [18] <author> Thomas T. Kwan, Robert E. McGrath, and Daniel A. Reed. </author> <title> Ncsa's world wide web server: Design and performance. </title> <journal> IEEE Computer, </journal> <volume> 28(11) </volume> <pages> 68-74, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: The NCSA scalable web server is built on a cluster of identically configured servers, and uses round-robin DNS scheduling and Andrew file system (AFS) for load sharing among the servers <ref> [16, 18] </ref>. The IBM scalable web server is built on an SP-2 parallel system, which is essentially a cluster of identical RS6000 workstations. <p> raster data set that includes a hyperlink to each image file.. 5.3 Experimental Results Among many ways to evaluate the performance of web servers, there seems to have emerged a consensus that three most important measures are connections per second (CPS), bytes transferred per second (BPS), and round-trip time (RTT) <ref> [18, 20] </ref>. However, the third measure, round-trip time is difficult to measure for an operational web server and depends on various performance factors such as network overhead and bottleneck, which are not directly related to the web server itself.
Reference: [19] <author> Tam Nguyen and V. Srinivasan. </author> <title> Accessing relational databases from the world wide web. </title> <booktitle> In Proceedings of the 1996 ACM-SIGMOD Conference, </booktitle> <pages> pages 529-540, </pages> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Web accessibility will be an essential component of the services that future digital libraries should provide for clients. This need has created a strong demand for database access capability through the internet <ref> [19] </ref>, and high performance scalable web servers [16, 23].
Reference: [20] <author> Venkata N. Padmanabhan and Jeffrey C. Mogul. </author> <title> Improving HTTP latency. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 28 </volume> <pages> 25-35, </pages> <year> 1995. </year>
Reference-contexts: raster data set that includes a hyperlink to each image file.. 5.3 Experimental Results Among many ways to evaluate the performance of web servers, there seems to have emerged a consensus that three most important measures are connections per second (CPS), bytes transferred per second (BPS), and round-trip time (RTT) <ref> [18, 20] </ref>. However, the third measure, round-trip time is difficult to measure for an operational web server and depends on various performance factors such as network overhead and bottleneck, which are not directly related to the web server itself.
Reference: [21] <author> Michael Stonebraker, Jim Frew, Kenn Gardels, and Jeff Meredith. </author> <title> The SEQUOIA 2000 storage benchmark. </title> <booktitle> In Proceedings of the 1993 ACM-SIGMOD Conference, </booktitle> <pages> pages 2-11, </pages> <address> Washington, DC, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Data sets We have chosen four real-world data sets with different characteristics. The first three in the following are the authors' own creations, and they are available in http://www.cs.arizona.edu/dcws. The last data set is the Sequoia 2000 storage benchmark data <ref> [21] </ref> publicly available in http://epoch.cs.- berkeley.edu:8000/sequoia/benchmark/. 11 * MAPUG Mailing List Archive: The mailing list archive contains 1,534 documents, 28,998 links and 5,918 Kbytes aggregate size of all files. The mailing lists are topical discussions conducted via electronic mails.
Reference: [22] <author> Cisco System. </author> <title> Scaling the internet web servers. </title> <address> http://www.cisco.com/warp/public/751/lodir/- scale wp.htm, </address> <month> November </month> <year> 1997. </year> <note> White Paper. </note>
Reference-contexts: Any distributed system requires some techniques to distribute the work load across multiple server computers. In some scalable web server systems <ref> [22] </ref>, this work load distribution is done at the packet level by using some form of custom routing. The router must intercept inbound packets, translate the incoming IP address into an address of a server computer, and place the packet out on a local area network. <p> A hybrid buffer management algorithm [23] has been proposed to balance intra-cluster network traffic and disk access by dynamically controlling the amount of data replication. A centralized round-robin router is used to route requests amongst multiple servers. The Cisco LocalDirector Cisco systems <ref> [22] </ref> uses a virtual server to handle incoming requests at a virtual IP address. The LocalDirector functions as an intelligent router, routing requests from the virtual IP address to physical servers at real IP addresses.
Reference: [23] <author> Shivakumar Venkataraman, Miron Livny, and Jeffrey F. Naughton. </author> <title> Memory management for scalable web data servers. </title> <booktitle> In the 13th Inter. Conference on Data Engineering, </booktitle> <pages> pages 510-519, </pages> <address> Birmingham, UK, </address> <month> April </month> <year> 1997. </year> <month> 17 </month>
Reference-contexts: Web accessibility will be an essential component of the services that future digital libraries should provide for clients. This need has created a strong demand for database access capability through the internet [19], and high performance scalable web servers <ref> [16, 23] </ref>. <p> The technique relies on replication of web documents by proxy servers. It is assumed that a list of proxy servers exists which contain a given document. A hybrid buffer management algorithm <ref> [23] </ref> has been proposed to balance intra-cluster network traffic and disk access by dynamically controlling the amount of data replication. A centralized round-robin router is used to route requests amongst multiple servers.
References-found: 23

