URL: ftp://ftp.cs.ucsd.edu/pub/emj/papers/multiscale.ps.Z
Refering-URL: http://www.cs.ucsd.edu/users/emj/
Root-URL: http://www.cs.ucsd.edu
Title: Multiscale Optimization in Neural Nets  
Author: Eric Mjolsness, Charles Garrett, and Willard L. Miranker 
Abstract: One way to speed up convergence in a large optimization problem is to introduce a smaller, approximate version of the problem at a coarser scale and to alternate between relaxation steps for the fine-scale and the coarse-scale problems. We exhibit such an optimization method for neural networks governed by quite general objective functions. At the coarse scale there is a smaller approximating neural net which, like the original net, is nonlinear and has a nonquadratic objective function. The transitions and information flow from fine to coarse scale and back do not disrupt the optimization, and the user need only specify a partition of the original fine-scale variables. Thus the method can be applied easily to many problems and networks. We show positive experimental results including cost comparisons. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. J. </author> <title> Hopfield, "Neurons with graded response have collective computational properties like those of two-state neurons," </title> <booktitle> Proceedings of the National Academy of Sciences USA, </booktitle> <volume> vol. vol. 81, </volume> <pages> pp. 3088-3092, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: 1 Introduction A rather general neural net objective function for continuous neural variables v i is composed of a general sum of linear, quadratic and cubic terms, by which neurons are connected and can interact with each other, along with nonlinear potential functions of just one neural variable each <ref> [1] </ref>. All higher-order polynomial objectives can be reduced to this form [2]. If the cubic and potential function terms are absent then the objective is quadratic and analogous to many numerical problems, defined on geometric domains, for which multigrid methods are successfully used as fast relaxation algorithms [3, 4]. <p> Thus, we define and explore a multiscale optimization method appropriate for neural nets. 2 Theory The general neural net objective function that we will use for continuous neural variables v i is <ref> [1] </ref> E [v] = ijk X T ij v i v j i X i (v i ); (1) although many networks are designed without the cubic term T ijk v i v j v k . The potential function term is i (v i ). <p> We confine our discussion to singularites in i , such as those of the barrier functions that correspond to sigmoidal neural transfer functions v = g (u) through g 1 (v) = 0 (v) <ref> [1] </ref>. (See Figure 2 for a typical barrier function expressed as a sum of two simgular functions (v) = (v) + + (v).) It may be possible to handle some nonsigmoidal neural transfer functions in a similar way.
Reference: [2] <author> E. Mjolsness and C. Garrett, </author> <title> "Algebraic transformations of objective functions," </title> <booktitle> Neural Networks, </booktitle> <year> 1990. </year> <note> In press. </note>
Reference-contexts: All higher-order polynomial objectives can be reduced to this form <ref> [2] </ref>. If the cubic and potential function terms are absent then the objective is quadratic and analogous to many numerical problems, defined on geometric domains, for which multigrid methods are successfully used as fast relaxation algorithms [3, 4]. <p> Then summing over i and a as in equation (11) shows that (15) implies (13). Incidentally, the max i2 (a) operation is neurally implementable by means of a winner-take-all subnet <ref> [2] </ref>. For a maximum over n variables, O (n) connections and time proportional to O (log n) [8] is typically required by such an analog neural network. <p> This suggests using the multiscale method to speed up convergence within a continuation method for minimizing E. 3.2 Saddle Points As mentioned in the introduction, any polynomial summand of an objective function can be reduced to a cubic polynomial, so equation (1) is rather general <ref> [2] </ref>. But this reduction occurs at the expense of replacing minima with saddle points which have the characteristic that each variable is classified ahead of time as requiring maximization or minimization. <p> But a slight change of notation can bring this question into the domain of a circuit-design method that uses rewrite rules by which one objective function can be algebraically transformed into another, more implementable one <ref> [2] </ref>. We will describe this notation in detail in Appendix 2. 4 Experimental Results We have applied the proposed multiscale optimization techniques to several nonquadratic objective functions. <p> It is known how to reduce this to O (n 2 ) connections in a saddle-point objective function <ref> [2] </ref> (with different temporal behavior). But to avoid the complications of studying the multiscale algorithm in the context of saddle points, we will compare the multiscale and single-scale versions of the O (n 3 ) network just described. The multiscale method requires a partition of the M ffi variables.
Reference: [3] <author> W. Hackbusch, </author> <title> "On the multi-grid method applied to difference equations," </title> <journal> Computing, </journal> <volume> vol. 20, </volume> <pages> pp. 291-306, </pages> <year> 1978. </year>
Reference-contexts: All higher-order polynomial objectives can be reduced to this form [2]. If the cubic and potential function terms are absent then the objective is quadratic and analogous to many numerical problems, defined on geometric domains, for which multigrid methods are successfully used as fast relaxation algorithms <ref> [3, 4] </ref>. Such algorithms proceed by introducing a smaller, approximate version of the problem at a coarser scale (i.e. using a coarser mesh) and alternating between relaxation steps for the fine-scale and the coarse-scale problems.
Reference: [4] <author> W. L. Miranker, </author> <title> Numerical Methods for Stiff Equations. </title> <address> D. </address> <publisher> Reidel Publishing Co., </publisher> <year> 1981. </year>
Reference-contexts: All higher-order polynomial objectives can be reduced to this form [2]. If the cubic and potential function terms are absent then the objective is quadratic and analogous to many numerical problems, defined on geometric domains, for which multigrid methods are successfully used as fast relaxation algorithms <ref> [3, 4] </ref>. Such algorithms proceed by introducing a smaller, approximate version of the problem at a coarser scale (i.e. using a coarser mesh) and alternating between relaxation steps for the fine-scale and the coarse-scale problems.
Reference: [5] <author> D. Terzopoulos, </author> <title> "Regularization of inverse visual problems involving discontinuities," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. PAMI-8, </volume> <month> July </month> <year> 1986. </year> <note> See page 419. </note>
Reference-contexts: This is done recursively, at many scales, and it is the two-way passage of information between scales which is responsible for the unusual effectiveness of the technique. Previously, multigrid methods have been applied to nonlinear objective functions arising in computer vision and which are close to being neurally optimizable <ref> [5, 6] </ref>, but the coarse-scale objective functions were taken to be quadratic approximations of the full objective functions. <p> Another multiscale method has been applied to the minimization of this objective by Ter-zopoulos <ref> [5, 6] </ref>. The differences are illuminating: Terzopoulos's algorithm turns the nonquadratic objective into a quadratic one by holding the line process variables l fixed, as control parameters.
Reference: [6] <author> D. Terzopoulos, </author> <title> Multiresolution Computation of Visible-Surface Representations. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1984. </year>
Reference-contexts: This is done recursively, at many scales, and it is the two-way passage of information between scales which is responsible for the unusual effectiveness of the technique. Previously, multigrid methods have been applied to nonlinear objective functions arising in computer vision and which are close to being neurally optimizable <ref> [5, 6] </ref>, but the coarse-scale objective functions were taken to be quadratic approximations of the full objective functions. <p> Another multiscale method has been applied to the minimization of this objective by Ter-zopoulos <ref> [5, 6] </ref>. The differences are illuminating: Terzopoulos's algorithm turns the nonquadratic objective into a quadratic one by holding the line process variables l fixed, as control parameters.
Reference: [7] <author> F. Chatelin and W. L. Miranker, </author> <title> "Acceleration by aggregation of successive approximation methods," </title> <journal> Linear Algebra and its Applications, </journal> <volume> vol. 43, </volume> <pages> pp. 17-47, </pages> <year> 1982. </year>
Reference-contexts: user-supplied 3 partition of the original variables, take v i = v 0 P P ia = B ia z i (v 0 ) (4) P [V] i = v 0 X B ia @E ! V a : (5) (C.f. the form of the prolongation matrix P ia in <ref> [7] </ref>.) Notice that the prolongation (5), while linear, is not homogeneous. Here v 0 is the value of v obtained by the last fine-scale relaxation of E, just before the coarse-scale relaxation of ^ E. Note that v 0 is the image of V = 0 under P [V].
Reference: [8] <author> J. J. </author> <title> Hopfield, "The effectiveness of analogue `neural network' hardware," </title> <journal> Network, </journal> <volume> vol. 1, </volume> <pages> pp. 27-40, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Then summing over i and a as in equation (11) shows that (15) implies (13). Incidentally, the max i2 (a) operation is neurally implementable by means of a winner-take-all subnet [2]. For a maximum over n variables, O (n) connections and time proportional to O (log n) <ref> [8] </ref> is typically required by such an analog neural network.
Reference: [9] <author> A. Brandt, </author> <title> "Multi-level adaptive solutions to boundary value problems," </title> <journal> Math. Comp., </journal> <volume> vol. 31, </volume> <pages> pp. 333-390, </pages> <year> 1977. </year>
Reference-contexts: When the problem is derived from a partial differential equation on a spatial domain, the modal techniques of Fourier analysis are often used to prove speedup <ref> [9] </ref>. Much weaker assumptions are required by more recent speedup proofs [10], though a quadratic objective is still assumed. We know of no such proof for our method; it must simply be tried out.
Reference: [10] <author> C. C. Douglas and J. Douglas, Jr., </author> <title> "Abstract multilevel convergence theory requires almost no assumptions," </title> <type> Tech. Rep. </type> <institution> RC15853, IBM Yorktown Heights, </institution> <year> 1990. </year>
Reference-contexts: When the problem is derived from a partial differential equation on a spatial domain, the modal techniques of Fourier analysis are often used to prove speedup [9]. Much weaker assumptions are required by more recent speedup proofs <ref> [10] </ref>, though a quadratic objective is still assumed. We know of no such proof for our method; it must simply be tried out. <p> This recursive control scheme ensures that the smallest and cheapest networks are called upon most frequently in a completely serial algorithm, as discussed for example in <ref> [10] </ref>. When switching levels, information is passed using the prolongation and restriction maps (equations (5) and (6)) as appropriate.
Reference: [11] <author> Y. G. Leclerc, </author> <title> "Constructing simple stable descriptions for image partitioning," </title> <journal> International Journal of Computer Vision, </journal> <volume> vol. 3, </volume> <pages> pp. 73-102, </pages> <year> 1989. </year>
Reference-contexts: Spatial-domain multiscale techniques can also lead to better local minima for problems with nonquadratic and multimodal objective functions; for example, scale-space continuation methods in computer vision may have this desirable property <ref> [11, 12] </ref>. We do not expect such an improvement in the local minima reached by our multiscale method because it never takes any uphill steps in the original, multimodal E.
Reference: [12] <author> A. Witkin, D. Terzopoulos, and M. Kass, </author> <title> "Signal matching through scale space," </title> <journal> International Journal of Computer Vision, </journal> <volume> vol. 1, </volume> <pages> pp. 133-144, </pages> <year> 1987. </year>
Reference-contexts: Spatial-domain multiscale techniques can also lead to better local minima for problems with nonquadratic and multimodal objective functions; for example, scale-space continuation methods in computer vision may have this desirable property <ref> [11, 12] </ref>. We do not expect such an improvement in the local minima reached by our multiscale method because it never takes any uphill steps in the original, multimodal E.
Reference: [13] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability, </title> <editor> p. </editor> <volume> 209. </volume> <publisher> Freeman, </publisher> <year> 1979. </year> <title> Graph partitioning problem in A2.2 </title> . 
Reference-contexts: For example, one might preprocess the original network of equation (1) so as to group together neurons that are strongly connected (large jT ij j). Finding the best possible graph partition is an NP-complete problem 8 <ref> [13] </ref>, but nearly-optimal partitions may be found by another optimizing neural net such as the graph-partitioning networks studied in [14, 15]. A particularly cheap (and approximate) way to do this is to bisect the net into two "modules" with minimal inter-module connections, and recursively bisect the modules.
Reference: [14] <author> C. Peterson and B. Soderberg, </author> <title> "A new method for mapping optimization problems onto neural networks," </title> <journal> International Journal of Neural Systems, </journal> <volume> vol. 1, no. 3, </volume> <year> 1989. </year>
Reference-contexts: Finding the best possible graph partition is an NP-complete problem 8 [13], but nearly-optimal partitions may be found by another optimizing neural net such as the graph-partitioning networks studied in <ref> [14, 15] </ref>. A particularly cheap (and approximate) way to do this is to bisect the net into two "modules" with minimal inter-module connections, and recursively bisect the modules.
Reference: [15] <author> D. E. V. D. Bout, </author> <title> "Graph partitioning using annealed neural networks," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 1, </volume> <month> June </month> <year> 1990. </year> <month> 15 </month>
Reference-contexts: Finding the best possible graph partition is an NP-complete problem 8 [13], but nearly-optimal partitions may be found by another optimizing neural net such as the graph-partitioning networks studied in <ref> [14, 15] </ref>. A particularly cheap (and approximate) way to do this is to bisect the net into two "modules" with minimal inter-module connections, and recursively bisect the modules.
Reference: [16] <author> J. J. Hopfield and D. W. Tank, </author> <title> "Collective computation with continuous variables," </title> <booktitle> in Dis--ordered Systems and Biological Organization, </booktitle> <pages> pp. 155-170, </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: To illustrate the method's independence of a continuous spatial domain, we used an objective function for inexact graph-matching based on purely structural similarity of two graphs <ref> [16, 17] </ref>. This problem may have application to problems of model matching in high-level computer vision. The objective is related to the Traveling Salesman Problem objective of [18], and both suffer from a strong increase in the number of local minima as the problem size increases.
Reference: [17] <author> C. von der Malsburg and E. </author> <title> Bienenstock, "Statistical coding and short-term synaptic plasticity: A scheme for knowledge representation in the brain," </title> <booktitle> in Disordered Systems and Biological Organization, </booktitle> <pages> pp. 247-252, </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: To illustrate the method's independence of a continuous spatial domain, we used an objective function for inexact graph-matching based on purely structural similarity of two graphs <ref> [16, 17] </ref>. This problem may have application to problems of model matching in high-level computer vision. The objective is related to the Traveling Salesman Problem objective of [18], and both suffer from a strong increase in the number of local minima as the problem size increases.
Reference: [18] <author> J. J. Hopfield and D. W. Tank, </author> <title> "`Neural' computation of decisions in optimization problems," </title> <journal> Biological Cybernetics, </journal> <volume> vol. 52, </volume> <pages> pp. 141-152, </pages> <year> 1985. </year>
Reference-contexts: This problem may have application to problems of model matching in high-level computer vision. The objective is related to the Traveling Salesman Problem objective of <ref> [18] </ref>, and both suffer from a strong increase in the number of local minima as the problem size increases. So we also considered a less problematical but spatially-structured nonquadratic objective function from low-level vision [19].
Reference: [19] <author> C. Koch, J. Marroquin, and A. Yuille, </author> <title> "Analog "neuronal" networks in early vision," </title> <booktitle> Proceedings of the National Acadamy of Sciences USA, </booktitle> <volume> vol. 83, </volume> <month> June </month> <year> 1986. </year>
Reference-contexts: The objective is related to the Traveling Salesman Problem objective of [18], and both suffer from a strong increase in the number of local minima as the problem size increases. So we also considered a less problematical but spatially-structured nonquadratic objective function from low-level vision <ref> [19] </ref>. It may be used for smooth two-dimensional surface reconstruction from sparse data, modified by nonlinear discontinuity detection processes, all defined on a discretized two-dimensional grid.
Reference: [20] <author> J. R. Anderson and C. Peterson, </author> <title> "Applicability of mean field theory neural network methods ot the graph partitioning problem," </title> <type> Tech. Rep. </type> <institution> ACA-ST-064-88, Microelectronics and Computer Technology Corporation, </institution> <month> February </month> <year> 1988. </year>
Reference-contexts: Second, we considered the sparse two-dimensional graphs of <ref> [20] </ref>, obtained by independenly choosing n points from the unit square with the uniform probability distribution and connecting up all points within a distance d determined by the requirement that the average degree of connectivity be given by 4nd 2 = 3.
Reference: [21] <author> D. G. Luenberger, </author> <title> Linear and Nonlinear Programming, ch. 7. </title> <publisher> Addison-Wesley, </publisher> <year> 1984. </year> <month> 16 </month>
Reference-contexts: These interesting issues could be explored in further work. In our experiments the numerical relaxation step at any given scale consisted of repeated uni-variate minimization along the gradient direction (a "line search") <ref> [21] </ref>. This strategy is standard 11 in parallel optimization algorithms but slightly different from the continuous steepest methods often used in analog neural nets.
References-found: 21

