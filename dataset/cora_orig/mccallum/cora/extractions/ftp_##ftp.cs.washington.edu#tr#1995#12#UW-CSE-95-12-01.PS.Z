URL: ftp://ftp.cs.washington.edu/tr/1995/12/UW-CSE-95-12-01.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Email: fcraig, baerg@cs.washington.edu  
Title: On the Performance of a Bus-based Multiprocessor Cluster Architecture  
Author: Craig Anderson and Jean-Loup Baer 
Note: This work was supported in part by NSF Grants CCR-94-01689, CCR-91-23308 and by  
Date: December 4, 1995  
Address: Box 352350  Seattle, WA 98195-2350  
Affiliation: Department of Computer Science and Engineering  University of Washington  Apple Computer, Inc.  
Pubnum: TR UW-CSE-95-12-01  
Abstract: The focus of this paper is on the evaluation of a hierarchical cluster architecture where a cluster consists of a single bus shared-memory multiprocessor and where the interconnect is a tree hierarchy of busses. We first outline a cache coherence protocol for a UMA architecture. We then introduce a variation of the protocol for sector caches where the coherence and transfer units are not the same. We evaluate, through cycle by cycle simulation, the UMA architecture under these two protocols. We simulate six benchmarks with varied memory access patterns and show that the clustering concept is beneficial as long as architectures are balanced. The subblock protocol works well and is more stable than protocols with fixed block size that can behave very well on some applications and very poorly on others. In the second part of the paper, we modify the UMA architecture into a NUMA architecture. We discuss several ways to perform memory allocation in this new context. We test some of these allocations on two of the benchmarks and report that improvements are largest when the memory allocation does not induce much extra computations and when architectures are balanced. 
Abstract-found: 1
Intro-found: 1
Reference: [AB93] <author> Craig Anderson and Jean-Loup Baer. </author> <title> A multi-level hierarchical cache coherence protocol for multiprocessors. </title> <booktitle> In Proc. of 7th Int. Parallel Processing Symopo-sium, </booktitle> <pages> pages 142-148, </pages> <year> 1993. </year>
Reference-contexts: bus request, for the same reason given above that an upper bus transaction has priority over a lower bus transaction if they access the same block on the same cycle. 2.3 Evaluation methodology Our architectural evaluations are performed on a cycle by cycle execution driven simulator, a modification of Cerberus <ref> [BAD89, AB93] </ref>. The processor module simulates (at a crude level) the pipeline delays of a RISC processor. All instruction references and all private data references are assumed to hit in the processor cache with no additional delay.
Reference: [AB95] <author> Craig Anderson and Jean-Loup Baer. </author> <title> Two techniques for improving the performance of bus-based multiprocessors. </title> <booktitle> In International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 264-275, </pages> <year> 1995. </year>
Reference-contexts: The motivation for the second set of experiments is that the subblock protocol was found to be successful when applied within a cluster <ref> [AB95] </ref>. We first describe briefly this second protocol. 3.1 Subblock protocol for two level architecture The subblock protocol is intended to work with sector caches where blocks are subdivided into subblocks (in our case, blocks of 64 bytes are divided into 8 subblocks of 8 bytes each). <p> In the sector caches, both blocks and subblocks have states. At the intra-cluster level the protocol is similar to the one described in <ref> [AB95] </ref>. There are 4 block states and 4 subblock states. Cache to cache transfers are favored. On read misses, as many valid subblocks in the block as possible are transferred. On writes to clean subblocks and write misses, only the subblock to be written is invalidated. <p> For all configurations and both sizes of bus cache, the subblock protocol equaled or slightly exceeded the already excellent perfor mance of the 64b-protocol. The lone exception to this was the (unbalanced) 2X16, 128K bus cache organization where performance dropped by 1%. Like the case with single bus machines <ref> [AB95] </ref>, the subblock protocol's policy of using large transfers worked well in applications (like Gauss) with good spatial locality. The number of upper bus transfers under the Sb-protocol was nearly identical to the number used by the 64b-protocol, and the number of upper bus bytes transferred was less. <p> This is not a surprise, given the subblock protocol's performance on Cholesky was closer to the 8b than the 64b-protocol when using a single bus <ref> [AB95] </ref>. This stems from the nature of Cholesky's data access pattern to the main data structure. MP3D MP3D performed best when using a 64 byte block size. Speedups reached a maximum of about 10 under the 8b-protocol and about 15 under the 64b-protocol. (cf. Figure 3). <p> However, its behavior was not quite as good as it was for the single bus case <ref> [AB95] </ref>. Some explanations for this include: * The subblock protocol lacked a DIRTY SHARED subblock state at the bus cache level. This caused some level of increase in the number of cluster bus transactions. * The subblock protocol allocated cache space on a block (64 byte) by block basis. <p> Un--like the conventional protocol used with a fixed block size, its performance across the tested benchmarks was much more consistent. The overall performance improvements of the subblock protocol were relatively less impressive than in the case of single bus system <ref> [AB95] </ref>. One of the reasons is that we wanted to minimize the number of states in the bus cache in order to make the standard and subblock protocols more comparable cost-wise.
Reference: [And95] <author> Craig Anderson. </author> <title> Improving the Performance of Bus-Based Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1995. </year>
Reference-contexts: There are also transitional states that are needed for the correctness of the protocol (see below). The blocks in the bus caches are in one of the five following states (with, of course, the possibility that they are in transitional states; see <ref> [And95] </ref> for a complete protocol description): * INVALID: The block is not present in or below the cache. * READ SHARED: The block is present in the cache in a clean state and may exist elsewhere in the system in a clean state. 4 * VALID EXCLUSIVE: The block is present <p> Because upper bus bandwidth is usually the bottleneck in hierarchical bus systems, it should be saved in preference to saving lower bus bandwidth. Hence, we decided to discard the DIRTY SHARED subblock state. A complete protocol description for both levels can be found in <ref> [And95] </ref>. 3.2 Evaluation of the UMA architecture We simulated ten different combinations of processors and clusters, ranging in size (and cost) from the 2 clusters of 2 processors (2X2) configuration to the 8 clusters of 4 processors 7 configuration.
Reference: [BAD89] <author> Eugene D. Brooks III, Tim S. Axelrod, and Gregory A. Darmohray. </author> <title> The Cer-berus multiprocessor simulator. </title> <editor> In G. Rodrigue, editor, </editor> <booktitle> Parallel Processing for Scientific Computing, </booktitle> <pages> pages 384-390. </pages> <publisher> SIAM, </publisher> <year> 1989. </year>
Reference-contexts: bus request, for the same reason given above that an upper bus transaction has priority over a lower bus transaction if they access the same block on the same cycle. 2.3 Evaluation methodology Our architectural evaluations are performed on a cycle by cycle execution driven simulator, a modification of Cerberus <ref> [BAD89, AB93] </ref>. The processor module simulates (at a crude level) the pipeline delays of a RISC processor. All instruction references and all private data references are assumed to hit in the processor cache with no additional delay.
Reference: [BBW92] <author> Jonathan Bertoni, Jean-Loup Baer, and Wen-Hann Wang. </author> <title> Scaling shared-bus multiprocessors with multiple buses and shared caches: a performance study. </title> <journal> Microprocessors and Microsystems, </journal> <volume> 16(7) </volume> <pages> 339-50, </pages> <year> 1992. </year> <month> 20 </month>
Reference-contexts: Since it is unlikely that non-cluster busses will be shorter than cluster busses, clocking the non-cluster busses faster than the cluster busses will probably not be an option. Instead, wider busses (to improve bandwidth) or multiple (possibly interleaved) busses <ref> [BBW92] </ref> would allow more clusters (hence more processors) to be effectively connected in a hierarchical system. 19 The hierarchical subblock protocol performed relatively well on the six benchmarks. Un--like the conventional protocol used with a fixed block size, its performance across the tested benchmarks was much more consistent.
Reference: [BW88] <author> Jean-Loup Baer and Wen-Hann Wang. </author> <title> On the inclusion properties for multi-level cache hierarchies. </title> <booktitle> In Proc. of 15th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 73-80, </pages> <year> 1988. </year>
Reference-contexts: To reduce bus traffic, all caches in the system are write back caches. Bus caches satisfy the inclusion property, that is, each cache at a given level contains a superset of the contents of the caches below it in the hierarchy <ref> [BW88] </ref>. Inclusion is guaranteed through the actions of the protocol, as in [Wil87]. When a bus cache replaces a block, it sends a message down that forces all processor caches below it to invalidate the block.
Reference: [Con94] <author> Convex Computer Corporation. </author> <title> Convex Exemplar Scalable Parallel Processing System, </title> <year> 1994. </year>
Reference-contexts: Cache Coherence Top Snoopy Snoopy Snoopy Direc- Hybrid SCI None Cache tory Coherence Cluster Yes Dir.y Dir.y Yes Yesz Yes No Cache? Inclusion? Yes Yes Yes No N/A Yes N/A Memory UMA COMA COMA NUMA NUMA NUMA NUMA Organi zation Reference [Wil87] [HLH92] [FIR93] [LLJ + 92] [WTP + 92] <ref> [Con94] </ref> [VSLW91] yDir.: Only directory entries are cached, not data. zEach cluster shares a portion of main memory, which can be used as a cache of other clusters' pages. Several factors impact the scalability of this type of architecture.
Reference: [Dar88] <author> Gregory A. Darmohray. </author> <title> Gaussian techniques on shared-memory multiprocessors. </title> <type> Master's thesis, </type> <institution> University of California, Davis, </institution> <month> April </month> <year> 1988. </year>
Reference-contexts: We used 6 applications to test and evaluate the proposed designs. We chose these benchmarks because together they exhibit a wide variety of reference patterns. In all cases, we gathered statistics only for the parallel section of the program. The Gauss a gaussian elimination program without pivoting <ref> [Dar88] </ref>- and Cholesky - sparse matrix factorization from the SPLASH suite [SWG92] benchmarks both have good spatial locality and exhibit high hit rates. MP3D a 3-dimensional molecule simulator also from the SPLASH suite exhibits a great deal of true sharing, and was written to avoid false sharing.
Reference: [DN87] <author> Srinivas Devadas and A. Richard Newton. </author> <title> Topological optimization of multiple level array logic. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <month> November </month> <year> 1987. </year>
Reference-contexts: MP3D a 3-dimensional molecule simulator also from the SPLASH suite exhibits a great deal of true sharing, and was written to avoid false sharing. Both the Topopt topological optimization on VLSI circuits using a parallel simulated annealing algorithm <ref> [DN87] </ref> and Pverify a verification program which compares two circuits and reports if they are functionally equivalent [Egg89] [MDWS87] applications exhibit both true and false sharing, even using moderate block sizes.
Reference: [Egg89] <author> Susan Eggers. </author> <title> Simulation Analysis of Data Sharing in Shared Memory Multiprocessor. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1989. </year>
Reference-contexts: Both the Topopt topological optimization on VLSI circuits using a parallel simulated annealing algorithm [DN87] and Pverify a verification program which compares two circuits and reports if they are functionally equivalent <ref> [Egg89] </ref> [MDWS87] applications exhibit both true and false sharing, even using moderate block sizes.
Reference: [ENSO94] <author> Andrew Erlichson, Basem Nayfeh, Jaswinder Singh, and Kunle Olukotun. </author> <title> The benefits of clustering in shared address space multiprocessors: an applications-driven investigation. </title> <type> Technical Report CSL-TR-94-632, </type> <institution> Stanford University, </institution> <year> 1994. </year>
Reference-contexts: Caches were made coherent via a full directory scheme <ref> [ENSO94] </ref>. They found limited advantage to clustering because of contention and larger access times to the shared cache. In our case, we found that cluster allocation succeeded in reducing upper bus traffic on the two applications we simulated.
Reference: [FIR93] <author> S. Frank, H. Burkhardt III, and J. Rothnie. </author> <title> The KSR-1: bridging the gap between shared memory and MPPs. </title> <booktitle> In Proc. of Spring 1993 COMPCON, </booktitle> <pages> pages 285-294, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Snoopy Snoopy Snoopy Snoopy Snoopy Directory Snoopy Cache Coherence Top Snoopy Snoopy Snoopy Direc- Hybrid SCI None Cache tory Coherence Cluster Yes Dir.y Dir.y Yes Yesz Yes No Cache? Inclusion? Yes Yes Yes No N/A Yes N/A Memory UMA COMA COMA NUMA NUMA NUMA NUMA Organi zation Reference [Wil87] [HLH92] <ref> [FIR93] </ref> [LLJ + 92] [WTP + 92] [Con94] [VSLW91] yDir.: Only directory entries are cached, not data. zEach cluster shares a portion of main memory, which can be used as a cache of other clusters' pages. Several factors impact the scalability of this type of architecture.
Reference: [HLH92] <author> Erik Hagersten, Anders Landin, and Seif Haridi. </author> <title> DDM a cache-only memory architecture. </title> <journal> IEEE Computer, </journal> <volume> 25(9) </volume> <pages> 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Cluster Snoopy Snoopy Snoopy Snoopy Snoopy Directory Snoopy Cache Coherence Top Snoopy Snoopy Snoopy Direc- Hybrid SCI None Cache tory Coherence Cluster Yes Dir.y Dir.y Yes Yesz Yes No Cache? Inclusion? Yes Yes Yes No N/A Yes N/A Memory UMA COMA COMA NUMA NUMA NUMA NUMA Organi zation Reference [Wil87] <ref> [HLH92] </ref> [FIR93] [LLJ + 92] [WTP + 92] [Con94] [VSLW91] yDir.: Only directory entries are cached, not data. zEach cluster shares a portion of main memory, which can be used as a cache of other clusters' pages. Several factors impact the scalability of this type of architecture.
Reference: [LLJ + 92] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessy. </author> <title> The DASH prototype: Implementation and performance. </title> <booktitle> In Proc. 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Snoopy Snoopy Snoopy Snoopy Directory Snoopy Cache Coherence Top Snoopy Snoopy Snoopy Direc- Hybrid SCI None Cache tory Coherence Cluster Yes Dir.y Dir.y Yes Yesz Yes No Cache? Inclusion? Yes Yes Yes No N/A Yes N/A Memory UMA COMA COMA NUMA NUMA NUMA NUMA Organi zation Reference [Wil87] [HLH92] [FIR93] <ref> [LLJ + 92] </ref> [WTP + 92] [Con94] [VSLW91] yDir.: Only directory entries are cached, not data. zEach cluster shares a portion of main memory, which can be used as a cache of other clusters' pages. Several factors impact the scalability of this type of architecture.
Reference: [MDWS87] <author> H-K. T. Ma, S. Devadas, R. Wei, and A. Sangiovanni-Vincentelli. </author> <title> Logic verification algorithms and their parallel implementation. </title> <booktitle> In Proceedings of the 24th Design Automation Conference, </booktitle> <pages> pages 283-290, </pages> <year> 1987. </year>
Reference-contexts: Both the Topopt topological optimization on VLSI circuits using a parallel simulated annealing algorithm [DN87] and Pverify a verification program which compares two circuits and reports if they are functionally equivalent [Egg89] <ref> [MDWS87] </ref> applications exhibit both true and false sharing, even using moderate block sizes.
Reference: [SJGH93] <author> Jaswinder Pal Singh, Truman Joe, Anoop Gupta, and John L. Hennessy. </author> <title> An emprical comparison of the Kendall Square Research KSR-1 and Stanford DASH multiprocessors. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 214-225, </pages> <year> 1993. </year>
Reference-contexts: number of cluster bus reads thus saturation of cluster busses was reached more quickly (mostly in unbalanced cases like 2X16 configurations). 4.3 Previous work and summary Singh et al. examined the effects of data placement on the DASH, where the granularity of data distribution to home nodes is the page <ref> [SJGH93] </ref>. A general technique they tried was the first touch, in which data was allocated in the home cluster which first accessed it. This technique has the potential to work well only when data structures are initialized in parallel by the processor which is going to access it the most.
Reference: [SWG92] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stan-ford Parallel Applications for Shared Memory. </title> <booktitle> Computer Architecture News, </booktitle> <pages> pages 5-44, </pages> <month> March </month> <year> 1992. </year> <month> 21 </month>
Reference-contexts: We chose these benchmarks because together they exhibit a wide variety of reference patterns. In all cases, we gathered statistics only for the parallel section of the program. The Gauss a gaussian elimination program without pivoting [Dar88]- and Cholesky - sparse matrix factorization from the SPLASH suite <ref> [SWG92] </ref> benchmarks both have good spatial locality and exhibit high hit rates. MP3D a 3-dimensional molecule simulator also from the SPLASH suite exhibits a great deal of true sharing, and was written to avoid false sharing. <p> Cholesky Like Gauss, Cholesky performs best when using a large block size. Unlike Gauss, Cholesky did not achieve near linear speedup because of the limited size of the input matrix that restricts available concurrency <ref> [SWG92] </ref>. Maximum speedup of about 17.5 was obtained using the 64 byte block size and 32 processors (in the 8X4 or 4X8 organizations) and 128K bus cache size. The advantage of using the 64b-protocol over the 8b-protocol was 12-30% for large bus caches and much more for small bus caches.
Reference: [VSLW91] <author> Zvonko Vranesic, Micahel Stumm, David Lewis, and Ron White. Hector: </author> <title> A hier-archically structured shared-memory multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 24(1) </volume> <pages> 72-79, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Coherence Top Snoopy Snoopy Snoopy Direc- Hybrid SCI None Cache tory Coherence Cluster Yes Dir.y Dir.y Yes Yesz Yes No Cache? Inclusion? Yes Yes Yes No N/A Yes N/A Memory UMA COMA COMA NUMA NUMA NUMA NUMA Organi zation Reference [Wil87] [HLH92] [FIR93] [LLJ + 92] [WTP + 92] [Con94] <ref> [VSLW91] </ref> yDir.: Only directory entries are cached, not data. zEach cluster shares a portion of main memory, which can be used as a cache of other clusters' pages. Several factors impact the scalability of this type of architecture.
Reference: [Wil87] <author> Andrew Wilson Jr. </author> <title> Hierarchical cache/bus architecture for shared memory multiprocessors. </title> <booktitle> In Proc. of 14th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 244-252, </pages> <year> 1987. </year>
Reference-contexts: Ring Cluster Snoopy Snoopy Snoopy Snoopy Snoopy Directory Snoopy Cache Coherence Top Snoopy Snoopy Snoopy Direc- Hybrid SCI None Cache tory Coherence Cluster Yes Dir.y Dir.y Yes Yesz Yes No Cache? Inclusion? Yes Yes Yes No N/A Yes N/A Memory UMA COMA COMA NUMA NUMA NUMA NUMA Organi zation Reference <ref> [Wil87] </ref> [HLH92] [FIR93] [LLJ + 92] [WTP + 92] [Con94] [VSLW91] yDir.: Only directory entries are cached, not data. zEach cluster shares a portion of main memory, which can be used as a cache of other clusters' pages. Several factors impact the scalability of this type of architecture. <p> Bus caches satisfy the inclusion property, that is, each cache at a given level contains a superset of the contents of the caches below it in the hierarchy [BW88]. Inclusion is guaranteed through the actions of the protocol, as in <ref> [Wil87] </ref>. When a bus cache replaces a block, it sends a message down that forces all processor caches below it to invalidate the block. Each bus cache has a dual directory so that operations on both busses it is connected to can proceed in parallel. <p> Having different states for non-processor caches is usual for hierarchical architectures <ref> [Wil87] </ref>, [YTB92]. The introduction of the DIRTY SHARED state is motivated by its effect of reducing level 1 traffic in inter-cluster requests. Bus requests are generated by processor actions (read, write, swap) and cache states.
Reference: [WTP + 92] <author> Andrew Wilson, Marc Teller, Thoams Probert, Dyung Le, and Richard LaRowe. </author> <title> Lynx/Galactica Net: A distributed, cache coherent multiprocessor system. </title> <booktitle> In Proc. of the 25th Hawaii International Conference on System Sciences, </booktitle> <volume> volume 1, </volume> <pages> pages 416-426, </pages> <year> 1992. </year>
Reference-contexts: Snoopy Directory Snoopy Cache Coherence Top Snoopy Snoopy Snoopy Direc- Hybrid SCI None Cache tory Coherence Cluster Yes Dir.y Dir.y Yes Yesz Yes No Cache? Inclusion? Yes Yes Yes No N/A Yes N/A Memory UMA COMA COMA NUMA NUMA NUMA NUMA Organi zation Reference [Wil87] [HLH92] [FIR93] [LLJ + 92] <ref> [WTP + 92] </ref> [Con94] [VSLW91] yDir.: Only directory entries are cached, not data. zEach cluster shares a portion of main memory, which can be used as a cache of other clusters' pages. Several factors impact the scalability of this type of architecture.
Reference: [YTB92] <author> Q. Yang, G. Thangadurai, and L. Bhuyan. </author> <title> Design of an adaptive cache coherence protocol for large scale multiprocessors. </title> <journal> IEEE TPDS, </journal> <volume> 3(3) </volume> <pages> 281-293, </pages> <month> May </month> <year> 1992. </year> <month> 22 </month>
Reference-contexts: Having different states for non-processor caches is usual for hierarchical architectures [Wil87], <ref> [YTB92] </ref>. The introduction of the DIRTY SHARED state is motivated by its effect of reducing level 1 traffic in inter-cluster requests. Bus requests are generated by processor actions (read, write, swap) and cache states.
References-found: 21

