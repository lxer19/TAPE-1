URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS96-28.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fchlee, yfwang, tyangg@cs.ucsb.edu  
Title: Global Optimization for Mapping Parallel Image Processing Tasks on Distributed Memory Machines  
Author: Cheolwhan Lee Yuan-Fang Wang Tao Yang 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: Many parallel algorithms and library routines are available for performing computer vision and image processing (CVIP) tasks on distributed-memory multiprocessors. The typical image distribution may use column, row, and block based mapping. Integrating a set of library routines for a CVIP application requires a global optimization for determining the data mapping of individual tasks by considering inter-task communication. The main difficulty in deriving the optimal image data distribution for each task is that CVIP task computation may involve loops, and the number of processors available and the size of the input image may vary at the run time. In this paper, a CVIP application is modeled using a task chain with nested loops, specified by conventional visual languages such as Khoros and Explorer. A mapping algorithm is proposed that optimizes the average run-time performance for CVIP applications with nested loops by considering the data redistribution overheads and possible run-time parameter variations. A taxonomy of CVIP operations is provided and used for further reducing the complexity of the algorithm. Experimental results on both low-level image processing and high-level computer vision applications are presented to validate this approach.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, et al., </author> <title> LAPACK Users' Guide, Second Edition, </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1995. </year>
Reference: [2] <author> H. M. Alnuweiri and V. Prasanna, </author> <title> Parallel Architectures and Algorithms for Image Component Labeling, </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 14, No. 10, </volume> <month> Oct. </month> <year> 1992, pp.1014-1992. </year>
Reference: [3] <author> D. Banerjee and J. C. </author> <title> Browne Complete Parallelization of Computations: Integration of data partitioning and functional parallelism for dynamic data structures, </title> <booktitle> Proc. of IPPS' 96, IEEE, Hawaii, </booktitle> <pages> pp. 354-360. </pages>
Reference-contexts: The system will then analyze the corresponding task graphs automatically and determines suitable processor allocation and data partitioning schemes. Such an idea can be found in the graphical parallel programming tool research <ref> [4, 3] </ref> which uses coarse-grain computation graphs to model user applications. Here, we address a fundamental mapping problem arising in realizing such a parallel programming environment.
Reference: [4] <author> A. Beguelin, J.J. Dongarra, G.A. Geist, R. Manchek, </author> <title> V.S. Sunderam, Graphical development tools for network-based concurrent supercomputing, </title> <booktitle> Proc. of Supercomputing '91, IEEE, </booktitle> <year> 1991, </year> <month> pp.435-444. </month>
Reference-contexts: The system will then analyze the corresponding task graphs automatically and determines suitable processor allocation and data partitioning schemes. Such an idea can be found in the graphical parallel programming tool research <ref> [4, 3] </ref> which uses coarse-grain computation graphs to model user applications. Here, we address a fundamental mapping problem arising in realizing such a parallel programming environment.
Reference: [5] <author> A. L. Beguelin, et al., </author> <title> Solving Computational Grand Challenges using a Network of Heterogeneous Supercomputers, </title> <booktitle> Proceedings of Fifth SIAM Conference on Parallel Processing, </booktitle> <address> Philadelphia, </address> <year> 1992, </year> <pages> pp. 596-601. </pages>
Reference: [6] <author> R. F. Browne and R. M. Hodgson, </author> <title> Mapping Image Processing Operations onto Transputer Networks, </title> <journal> Microprocessors and Microsystems, </journal> <volume> Vol. 13 No. 3, </volume> <month> April </month> <year> 1989, </year> <month> pp.203-211 </month>
Reference-contexts: In this taxonomy we classify an operation as local or global. If an output pixel value of an operator generally depends upon the values of the whole input image, the operator is classified as a global operator <ref> [6] </ref>. Otherwise it is local. A local operation can be either a pixel operation or a masking operation. A pixel operator functions on a pixel-by-pixel basis while a masking operator draws its inputs from a small area in an image which is usually a small r fi s rectangular tile.
Reference: [7] <author> P. K. Biswas, J. Mukherjee, and B. N. Chatterji, </author> <title> Component Labeling in Pyramid Architecture, </title> <journal> Pattern Recognition, </journal> <volume> Vol. 26, No. 7, </volume> <year> 1993, </year> <pages> pp. 1099-1115. </pages>
Reference-contexts: 1 Introduction Computer Vision and Image Processing (CVIP) algorithms possess characteristics which are ideally suited for implementation on a variety of parallel architectures [29, 30]. Many efficient parallel CVIP algorithms have been proposed on distributed memory machines <ref> [7, 25] </ref>. However, a CVIP application usually employs many individual algorithms, and optimizing individual tasks in a processing pipeline comprising many tasks does not guarantee the optimal performance of the whole sequence of operations.
Reference: [8] <author> J. Choi, J. J. Dongarra, R. Pozo, and D. W. Walker, </author> <title> ScaLAPACK: A scalable linear algebra library for distributed memory concurrent computers, </title> <booktitle> Proceedings of the 4th Symp. on Massively Parallel Computing, </booktitle> <year> 1992, </year> <month> pp.120-127. </month>
Reference-contexts: The need for such software systems for effectively integrating and utilizing existing libraries of parallel algorithms has been recognized before. For example, This research direction has been demonstrated in the ScaLAPACK <ref> [8] </ref> for general scientific computing and in the DISC and other image library projects [13, 31]. In this paper, we have developed a mapping scheme as the first step to realize a parallel visual pro-gramming system based on Khoros [17, 18, 19, 23]. <p> The structure of the CS-2 data communication network is a multi-stage packet-switching fat-tree using a wormhole routing scheme. There are several message passing libraries available and the one we used is NX/2 (the Intel communication interface). And the NX/2 version ScaLAPACK <ref> [8] </ref> was employed as a parallel library. For system parameters, we estimated ff = 80sec, fi = 0:026sec=byte, and ! = 0:33sec=f lop using the NX/2 communication package on the MEIKO CS-2.
Reference: [9] <institution> IRIX Explorer User's Guide, Silicon Graphics, Inc., Mountain View, </institution> <address> CA. </address> <year> 1992. </year>
Reference: [10] <author> A. Gerasoulis and T. Yang, </author> <title> On the Granularity and Clustering of Directed Acylic Task Graphs, </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> Vol. 4, No. 6, </volume> <month> June </month> <year> 1993, </year> <pages> pp. 686-701. </pages>
Reference-contexts: For the block-cyclic partitioning, we assume that the blocking factor b in the cyclic pattern is fixed such that the computation time is greater than the communication time for an efficient parallel implementation <ref> [10] </ref>. <p> For system parameters, we estimated ff = 80sec, fi = 0:026sec=byte, and ! = 0:33sec=f lop using the NX/2 communication package on the MEIKO CS-2. The blocking factor, b, was selected to be eight so that the ratio of computation speed and communication speed is about one <ref> [10] </ref> and parallelism can be exploited without excessive communication overhead. 5.1 Image Enhancement and Noise Reduction Problem description We tested the scheduling scheme on a popular image processing operation, homomorphic filtering [11], for noise reduction and image enhancement.
Reference: [11] <author> R C. Gonzalez and R. E. Woods, </author> <title> Digital Image Processing, </title> <publisher> Addison-Wesley Pub. Company, </publisher> <year> 1992. </year>
Reference-contexts: b, was selected to be eight so that the ratio of computation speed and communication speed is about one [10] and parallelism can be exploited without excessive communication overhead. 5.1 Image Enhancement and Noise Reduction Problem description We tested the scheduling scheme on a popular image processing operation, homomorphic filtering <ref> [11] </ref>, for noise reduction and image enhancement. The input image was assumed to be corrupted by a multiplicative noise process and an additive noise process. The operation used histogram equalization to stretch the image intensities back to the original range, followed by median filtering to remove the "salt-and-pepper" additive noise. <p> The next node was a masking operation; hence, the node was expanded into six nodes in the cost graph using six different data partitioning schemes. The Fourier transform assumed two different implementations, both of which used the transpose algorithm for the parallel 1D FFT operation. For 2D FFT <ref> [11] </ref>, one library routine used the row partition for 1D FFT, followed by a transpose operation, and then by another row-wise 1D FFT. The other routine was similar but assumed the column partition for 1D FFT instead.
Reference: [12] <author> M. Gupta and P. Banerjee, </author> <title> Demonstration of Automatic Data Partitioning Techniques for Parallelizing Compilers on Multicomputers, </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> Vol. 3, No. 2, </volume> <month> Mar. </month> <year> 1992, </year> <pages> pp. 179-193. </pages>
Reference-contexts: Our model uses task graphs where each task is parallelizable. This model can be also found in the recent work for exploiting data and task parallelism, e.g. [22, 32]. Their work deals with DAGs of fixed data distribution and processor parameters, but not with graphs with loops. In <ref> [12] </ref>, techniques for optimizing the data distribution are presented for nested loops, which can be viewed as the optimization for one macro task.
Reference: [13] <author> L. H. Jamieson, E. J. Delp, J. N. Patel, C.-C. Wang, and A. A. Khokhar, </author> <title> A Library-Based Program Development Environment for Parallel Image Processing, </title> <booktitle> Proceedings of Scalable Parallel Libraries Conference, </booktitle> <institution> Mississippi State Univ., Mississippi, </institution> <month> Oct. </month> <year> 1993, </year> <pages> pp. 187-194. </pages>
Reference-contexts: The need for such software systems for effectively integrating and utilizing existing libraries of parallel algorithms has been recognized before. For example, This research direction has been demonstrated in the ScaLAPACK [8] for general scientific computing and in the DISC and other image library projects <ref> [13, 31] </ref>. In this paper, we have developed a mapping scheme as the first step to realize a parallel visual pro-gramming system based on Khoros [17, 18, 19, 23]. Khoros has been successfully utilized for many CVIP and scientific applications. <p> We do not want to imply that static scheduling is a feasible choice for data-dependent operations on distribution machines and it is indeed an open problem how to devise effective dynamic scheduling for efficient data-dependent computation on distributed memory machines. Library-based parallel system development tools were discussed in <ref> [13, 24] </ref>. Reeves [24] constructed a parallel library of CVIP routines, but he did not provide scheduling support for parallel execution. Jamieson, et al. [13] illustrated a user-friendly interface to a parallel library, which stores several implementations of a single algorithm, each optimized for a different parallel architecture. <p> Library-based parallel system development tools were discussed in [13, 24]. Reeves [24] constructed a parallel library of CVIP routines, but he did not provide scheduling support for parallel execution. Jamieson, et al. <ref> [13] </ref> illustrated a user-friendly interface to a parallel library, which stores several implementations of a single algorithm, each optimized for a different parallel architecture. This system was able to select the most efficient implementation of an algorithm by considering various system parameters. <p> Note that an algorithm may have different performance using different distribution patterns. For example, the 2D "row-column" parallel FFT algorithm using the row or column partition outperforms the 2D decimation FFT algorithm adopting a block partition for small problem-size/machine-size ratios in coarse-grained machines <ref> [13] </ref>. Since we will employ existing parallel algorithm libraries to execute individual tasks in a graph, the possibility that some tasks may be implemented with a few fixed choices of data distribution must be considered.
Reference: [14] <author> C. H. Koelbel, D. B. Loveman, R. S. Schreiber, G. L. Steele Jr., and M. E. Zosel, </author> <title> The High Performance Fortran Handbook, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1994. </year>
Reference-contexts: A typical mapping scheme for 2D image data is row, column, or block based, and we will assume the basic distribution strategies used in HPF <ref> [14] </ref>. Each task may employ a different mapping scheme to exploit data parallelism, as one mapping may be better than another in terms of individual task performance. <p> We assume that task T will use all p processors and T employs one of the following six data distribution schemes based on the High-Performance Fortran standard <ref> [14] </ref>: row, column, row-cyclic, column-cyclic, block, and block-cyclic partitions. The assignment of computation in each task to the p processors is based on the data distribution pattern assumed for that particular task. Note that an algorithm may have different performance using different distribution patterns. <p> Under some special conditions, CVIP algorithms involving data dependent operations can still be scheduled at the compilation time, we discuss these cases in Section 4. 3.3 Data Redistribution A task may employ one of the following six data partitioning schemes, which are also provided by the High-Performance Fortran <ref> [14] </ref>: row, column, row-cyclic, column-cyclic, block, and block-cyclic partitions. These data partitioning schemes are depicted in Figure 3 assuming that the size of the image is n fi n and the number of processors is p.
Reference: [15] <author> V. K. P. Kumar, et al., </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms, </title> <publisher> The Ben-jamin/Cummings Publishing Company, Inc., </publisher> <address> Redwood City, CA., </address> <year> 1994. </year>
Reference-contexts: In [17] we studied and tabulated the cost functions of data redistribution from one partition to another. The cost table is reproduced in Table 2. Note that the communication algorithms used to generate Table 2 may not be optimal and other algorithms for all-to-all communication could be used, e.g., <ref> [15, 27] </ref>.
Reference: [16] <author> S.-Y. Lee and J. K. Aggarwal, </author> <title> A System Design/Scheduling Strategy for Parallel Image Processing, </title> <journal> IEEE Trans. PAMI, </journal> <volume> Vol. 12, No. 2, </volume> <month> Feb. </month> <year> 1990, </year> <pages> pp. 194-204. </pages>
Reference-contexts: However, automated compilation without taking advantage of specific domain knowledge is still quite difficult. Several researchers presented preliminary designs of schedulers for parallel CVIP operations. For example, Lee and Aggarwal <ref> [16] </ref> discussed static and dynamic design/scheduling strategies 2 for image processing operations comprising a linear sequence of tasks. Although this research provided a useful design framework for parallel image processing, it is not applicable to message-passing architectures and does not consider the communication overheads for redistributing image data.
Reference: [17] <author> C. Lee, Y. F. Wang, and T. Yang, </author> <title> Static Global Scheduling for Optimal Computer Vision and Image Processing Operations on Distributed-Memory Multiprocessors, </title> <type> Technical Report TRCS94-23, </type> <institution> University of California, Santa Barbara, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: In this paper, we have developed a mapping scheme as the first step to realize a parallel visual pro-gramming system based on Khoros <ref> [17, 18, 19, 23] </ref>. Khoros has been successfully utilized for many CVIP and scientific applications. In Khoros, a user employs a visual programming language to specify CVIP operations as task graphs and the system employs existing library routines to process the required computation. <p> The mask size is assumed to be r fi s. as pixels needed in a masking operation may not be stored in the same processor. Communication costs of a masking operation using different data partitioning schemes are summarized in Table 1 (Details can be found in <ref> [17] </ref>). The communication and computation times of global operations can be quite complicated and do not assume any fixed pattern like they do for pixel and masking operations. We found no easy way to tabulate them in advance. <p> If two adjacent processing algorithms assume the same data partitioning scheme, then there should be no redistribution cost incurred. Thus only thirty nontrivial data redistributions assume non-zero costs. In <ref> [17] </ref> we studied and tabulated the cost functions of data redistribution from one partition to another. The cost table is reproduced in Table 2. Note that the communication algorithms used to generate Table 2 may not be optimal and other algorithms for all-to-all communication could be used, e.g., [15, 27].
Reference: [18] <author> C. Lee, Y. F. Wang, and T. Yang, </author> <title> Static Global Scheduling for Optimal Computer Vision and Image Processing Operations on Distributed-Memory Multiprocessors, </title> <booktitle> 6th International Conference on Computer Analysis of Images and Patterns, </booktitle> <address> Prague, Czeck, </address> <month> Sept. </month> <pages> 6-8, </pages> <year> 1995, </year> <pages> pp. 920-925. </pages>
Reference-contexts: This research thus aims at developing a global optimization scheduler-which takes into consideration the data shu*ing overheads in between processing stages and possible run-time variation of the task and resource parameters-to generate data partition and processor assignment schemes for optimizing an entire sequence of operations, instead of individual tasks <ref> [18, 19] </ref>. The need for such software systems for effectively integrating and utilizing existing libraries of parallel algorithms has been recognized before. For example, This research direction has been demonstrated in the ScaLAPACK [8] for general scientific computing and in the DISC and other image library projects [13, 31]. <p> In this paper, we have developed a mapping scheme as the first step to realize a parallel visual pro-gramming system based on Khoros <ref> [17, 18, 19, 23] </ref>. Khoros has been successfully utilized for many CVIP and scientific applications. In Khoros, a user employs a visual programming language to specify CVIP operations as task graphs and the system employs existing library routines to process the required computation.
Reference: [19] <author> C. Lee, Y. F. Wang, and T. Yang, </author> <title> Partitioning and Scheduling for Parallel Image Processing Operations, </title> <booktitle> Seventh IEEE Symposium on Parllel and Distributed Processing, </booktitle> <address> San Antonio, Texas, </address> <month> Oct. </month> <pages> 25-28, </pages> <year> 1995, </year> <pages> pp. 86-90. 25 </pages>
Reference-contexts: This research thus aims at developing a global optimization scheduler-which takes into consideration the data shu*ing overheads in between processing stages and possible run-time variation of the task and resource parameters-to generate data partition and processor assignment schemes for optimizing an entire sequence of operations, instead of individual tasks <ref> [18, 19] </ref>. The need for such software systems for effectively integrating and utilizing existing libraries of parallel algorithms has been recognized before. For example, This research direction has been demonstrated in the ScaLAPACK [8] for general scientific computing and in the DISC and other image library projects [13, 31]. <p> In this paper, we have developed a mapping scheme as the first step to realize a parallel visual pro-gramming system based on Khoros <ref> [17, 18, 19, 23] </ref>. Khoros has been successfully utilized for many CVIP and scientific applications. In Khoros, a user employs a visual programming language to specify CVIP operations as task graphs and the system employs existing library routines to process the required computation.
Reference: [20] <author> The MPI Forum, </author> <title> MPI:A Message Passing Interface, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <address> Portland, Or., pp.878--883. </address>
Reference: [21] <author> H. Murakami and B. V. K. Kumar, </author> <title> Efficient Calculation of Primary Images from a Set of Images, </title> <journal> IEEE T-PAMI, </journal> <volume> Vol. 4, No. 5, </volume> <month> Sep. </month> <year> 1982, </year> <pages> pp. 511-515. </pages>
Reference-contexts: into smaller subsets such that the MPD of the schedule in each parameter subset is less than a preset threshold. 5.2 Face Recognition using Eigen Images Problem description A second example was on parallelizing a high-level vision recognition algorithm based on the principal component analysis, which was first proposed in <ref> [21] </ref> and later made popular by [28]. The basic idea is to treat an image as a point in a very high dimensional space.
Reference: [22] <author> S. Ramaswamy, S. Sapatnekar, and P. Banerjee, </author> <title> A convex programming approach for exploiting data and functional parallelism. </title> <booktitle> In Proc. of 1994 Inter. Conf. on Parallel Processing, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 116-125. </pages>
Reference-contexts: These systems can benefit from the result of this paper. Our model uses task graphs where each task is parallelizable. This model can be also found in the recent work for exploiting data and task parallelism, e.g. <ref> [22, 32] </ref>. Their work deals with DAGs of fixed data distribution and processor parameters, but not with graphs with loops. In [12], techniques for optimizing the data distribution are presented for nested loops, which can be viewed as the optimization for one macro task.
Reference: [23] <author> J. Rasure and S. Kubica, </author> <title> The Khoros Application Development Environment, </title> <publisher> Khoral Research Inc., </publisher> <address> Albu-querque, New Mexico, </address> <year> 1992. </year>
Reference-contexts: In this paper, we have developed a mapping scheme as the first step to realize a parallel visual pro-gramming system based on Khoros <ref> [17, 18, 19, 23] </ref>. Khoros has been successfully utilized for many CVIP and scientific applications. In Khoros, a user employs a visual programming language to specify CVIP operations as task graphs and the system employs existing library routines to process the required computation.
Reference: [24] <author> A. P. Reeves, </author> <title> Parallel Programming for Computer Vision, </title> <journal> IEEE Software, </journal> <volume> Vol. 8, No. 6, </volume> <month> Nov. </month> <year> 1991, </year> <pages> pp. 51-59. </pages>
Reference-contexts: We do not want to imply that static scheduling is a feasible choice for data-dependent operations on distribution machines and it is indeed an open problem how to devise effective dynamic scheduling for efficient data-dependent computation on distributed memory machines. Library-based parallel system development tools were discussed in <ref> [13, 24] </ref>. Reeves [24] constructed a parallel library of CVIP routines, but he did not provide scheduling support for parallel execution. Jamieson, et al. [13] illustrated a user-friendly interface to a parallel library, which stores several implementations of a single algorithm, each optimized for a different parallel architecture. <p> Library-based parallel system development tools were discussed in [13, 24]. Reeves <ref> [24] </ref> constructed a parallel library of CVIP routines, but he did not provide scheduling support for parallel execution. Jamieson, et al. [13] illustrated a user-friendly interface to a parallel library, which stores several implementations of a single algorithm, each optimized for a different parallel architecture.
Reference: [25] <author> H. J. Siegel, J. B. Armstrong, and D. W. Watson, </author> <title> Mapping Computer-Vision-Related Tasks onto Reconfigurable Parallel-Processing Systems, </title> <journal> IEEE Computer, </journal> <volume> Vol. 25, No. 2, </volume> <month> Feb. </month> <year> 1992, </year> <month> pp.54-63. </month>
Reference-contexts: 1 Introduction Computer Vision and Image Processing (CVIP) algorithms possess characteristics which are ideally suited for implementation on a variety of parallel architectures [29, 30]. Many efficient parallel CVIP algorithms have been proposed on distributed memory machines <ref> [7, 25] </ref>. However, a CVIP application usually employs many individual algorithms, and optimizing individual tasks in a processing pipeline comprising many tasks does not guarantee the optimal performance of the whole sequence of operations.
Reference: [26] <author> G. Srellner, S. Lamberts, and T. Ludwig, </author> <title> NXLib Users' Guide, </title> <institution> Institut fur Informatik, Technische Universitat Munchen, Munchen, German, </institution> <month> Mar. </month> <year> 1994. </year>
Reference: [27] <author> R. Thakur and A. Choudhary, </author> <title> All-to-all communication on meshes with wormhole routing, </title> <booktitle> Proceeding of 1994 Inter. Parallel Processing Symposium, </booktitle> <pages> pp. 561-565. </pages>
Reference-contexts: In [17] we studied and tabulated the cost functions of data redistribution from one partition to another. The cost table is reproduced in Table 2. Note that the communication algorithms used to generate Table 2 may not be optimal and other algorithms for all-to-all communication could be used, e.g., <ref> [15, 27] </ref>.
Reference: [28] <author> M. Turk and A. Pentland, </author> <title> Eigenfaces for Recognition, </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> Vol. 3, No. 1, </volume> <year> 1991, </year> <month> pp.71-86. </month>
Reference-contexts: MPD of the schedule in each parameter subset is less than a preset threshold. 5.2 Face Recognition using Eigen Images Problem description A second example was on parallelizing a high-level vision recognition algorithm based on the principal component analysis, which was first proposed in [21] and later made popular by <ref> [28] </ref>. The basic idea is to treat an image as a point in a very high dimensional space. Then for a collection of images, the principal components of the image ensemble can be found by computing the eigenvectors of the covariance matrix of the set of images. <p> Then for a collection of images, the principal components of the image ensemble can be found by computing the eigenvectors of the covariance matrix of the set of images. These eigenvectors (or eigen images) then serve as a compact basis set to represent the image ensemble. In <ref> [28] </ref>, this eigen image paradigm was applied for human face recognition, and is fast gaining popularity. More specifically, an image I (x; y) is usually treated as a two-dimensional matrix of dimension, say, n fi n. <p> We need a computationally feasible method to find these eigenvectors. <ref> [28] </ref> suggested computing the eigenvectors and eigenvalues of A T A instead, which is of a much smaller dimension of M fi M . And it can be trivially shown that the eigenvectors of AA T and those of A T A are related by a multiplication by A. <p> The scheduling algorithm was used to generate a parallel version of this face recognition algorithm. We used the same image data set as that in <ref> [28] </ref>. The visual representation of the algorithm using Khoros 2.0.2 is shown in Figure 12. The corresponding task graph of this operation is shown in Figure 13 (a). Again, input/output routines were omitted in the task graph.
Reference: [29] <author> J. A. Webb, </author> <title> Steps Toward Architecture-independent Image Processing, </title> <journal> IEEE Computer, </journal> <volume> Vol. 25, No. 2, </volume> <month> Feb. </month> <year> 1992, </year> <pages> pp. 21-31. </pages>
Reference-contexts: 1 Introduction Computer Vision and Image Processing (CVIP) algorithms possess characteristics which are ideally suited for implementation on a variety of parallel architectures <ref> [29, 30] </ref>. Many efficient parallel CVIP algorithms have been proposed on distributed memory machines [7, 25]. However, a CVIP application usually employs many individual algorithms, and optimizing individual tasks in a processing pipeline comprising many tasks does not guarantee the optimal performance of the whole sequence of operations.
Reference: [30] <author> C. C. Weems, </author> <title> Parallel Processing in the DARPA Strategic Computing Vision Program, </title> <journal> IEEE Expert, </journal> <volume> Vol. 6, No. 5, </volume> <month> Oct. </month> <year> 1991, </year> <pages> pp. 23-38. </pages>
Reference-contexts: 1 Introduction Computer Vision and Image Processing (CVIP) algorithms possess characteristics which are ideally suited for implementation on a variety of parallel architectures <ref> [29, 30] </ref>. Many efficient parallel CVIP algorithms have been proposed on distributed memory machines [7, 25]. However, a CVIP application usually employs many individual algorithms, and optimizing individual tasks in a processing pipeline comprising many tasks does not guarantee the optimal performance of the whole sequence of operations.
Reference: [31] <author> F. Weil, L. H. Jamieson, and E. J. Delp, </author> <title> Dynamic Intelligent Scheduling and Control of Reconfigurable Parallel Architectures for Computer Vision/Image Processing, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 13, </volume> <year> 1991, </year> <pages> pp. 273-285. </pages>
Reference-contexts: The need for such software systems for effectively integrating and utilizing existing libraries of parallel algorithms has been recognized before. For example, This research direction has been demonstrated in the ScaLAPACK [8] for general scientific computing and in the DISC and other image library projects <ref> [13, 31] </ref>. In this paper, we have developed a mapping scheme as the first step to realize a parallel visual pro-gramming system based on Khoros [17, 18, 19, 23]. Khoros has been successfully utilized for many CVIP and scientific applications. <p> Although this research provided a useful design framework for parallel image processing, it is not applicable to message-passing architectures and does not consider the communication overheads for redistributing image data. DISC <ref> [31] </ref> uses dynamic scheduling in handling data-dependent tasks and conditional branches on a dynamic reconfigurable and repartitionable machine (PASM). Run-time dynamic scheduling is advantageous in dealing with data dependent and conditional operations. But dynamic scheduling also incurs a high control overhead on distributed memory machines. <p> Our scheduler, to be used at the compilation time, is well suited for tasks involving data-independent operations. Data-dependent operations pose more serious problems, as their behaviors cannot be predicted at the compilation time. The difficulty of mapping data-dependent operations was first recognized in the DISC project <ref> [31] </ref>. And some worst-case, best-case, or average-case analysis should be employed, but the optimality of such analyses cannot be guaranteed.
Reference: [32] <author> T. Yang and A. Gerasoulis, </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message Passing Multiprocessors, </title> <booktitle> Proceedings of 6th ACM Int. Conf. on Supercomputing, </booktitle> <address> Washington D.C., </address> <month> Jul. </month> <year> 1992, </year> <pages> pp. 428-437. 26 </pages>
Reference-contexts: These systems can benefit from the result of this paper. Our model uses task graphs where each task is parallelizable. This model can be also found in the recent work for exploiting data and task parallelism, e.g. <ref> [22, 32] </ref>. Their work deals with DAGs of fixed data distribution and processor parameters, but not with graphs with loops. In [12], techniques for optimizing the data distribution are presented for nested loops, which can be viewed as the optimization for one macro task.
References-found: 32

