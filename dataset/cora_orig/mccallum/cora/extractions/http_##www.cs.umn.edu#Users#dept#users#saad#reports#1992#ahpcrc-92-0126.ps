URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1992/ahpcrc-92-0126.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1992/
Root-URL: http://www.cs.umn.edu
Title: Krylov Subspace Methods in Distributed Computing Environments  
Author: Youcef Saad 
Affiliation: Department of Computer Science, University of Minnesota, Minneapo-lis, USA  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> O. Axelsson. </author> <title> A generalized conjugate gradient, least squares method. </title> <journal> Num. Math., </journal> <volume> 51 </volume> <pages> 209-227, </pages> <year> 1987. </year>
Reference-contexts: A number of projection processes on so-called Krylov subspaces have been proposed in recent years to solve such systems, see for example, <ref> [1, 7, 9, 8, 11, 10, 14, 22, 20, 21] </ref>. <p> Form the approximate solution: Compute x m = x 0 + Z m y m where y m = argmin y kfie 1 H m yk 2 and e 1 = <ref> [1; 0; : : : ; 0] </ref> T . 4.
Reference: [2] <author> X. C. Cai, W. D. Gropp, and D. E. Keyes. </author> <title> A comparison of some domain decomposition and ILU preconditioned iterative methods for nonsymmetric elliptic problems. </title> <note> Submitted, </note> <year> 1992. </year>
Reference-contexts: The superscript k related to the color indicates that we cannot modify x loc at step k before we have completed the modification at step k 1. An s step SOR iteration would simply consist of adding an outer loop to the above algorithm. In <ref> [2] </ref> an idea based on multicolor SOR was studied. It was termed `a multiplicative' Schwartz procedure and was found to be one of the best `practical' preconditioners tested by the authors. <p> It was termed `a multiplicative' Schwartz procedure and was found to be one of the best `practical' preconditioners tested by the authors. Note that we can also define a block-Jacobi type precon-ditioner similarly and that overlapping domains can also be used to enhance the performance of such preconditioners <ref> [2] </ref>. 10 Preconditioned iterations for interface points An interesting observation which can be made is that it is possible to write equations for interface points alone.
Reference: [3] <author> X. C. Cai and Y. Saad. </author> <title> Overlapping domain decomposition algorithms for general sparse matrices. </title> <type> Technical report, </type> <institution> Army High Performance Computing Research Center, </institution> <year> 1992. </year> <note> In preparation. </note>
Reference-contexts: In this paper we will restrict ourselves to discussing the main ideas in this context. Readers are referred to <ref> [3] </ref> and [16] for additional details. 2 Preconditioned Krylov Subspace Methods We consider a linear system of the form Ax = b; (1) where A is a large sparse nonsymmetric real matrix of size N.
Reference: [4] <author> T. A. Davis. </author> <title> A parallel algorithm for sparse unsymmetric LU factorizations. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana Champaign, Urbana, IL., </institution> <year> 1989. </year>
Reference-contexts: Finding such sets is relatively easy. An independent set is maximal if it cannot be augmented by elements in its complement to form a larger independent set. Independent set orderings have been mainly used for deriving parallel direct solution techniques for sparse linear systems <ref> [12, 13, 4] </ref> and multifrontal techniques [6] can be viewed as a particular case.
Reference: [5] <author> I. Duff, M. Marrone, and G. Radicati. </author> <title> A proposal for user level sparse BLAS. </title> <note> Preliminary report, </note> <year> 1992. </year>
Reference-contexts: Note also that the matrix by vector products in steps 2 and 3 can use any convenient data structure that will improve efficiency by exploiting the local architecture. For this reason, the work being done elsewhere on defining a user-lever sparse BLAS can be fully exploited <ref> [5] </ref>. 3.3 Preconditioning operations With the domain-decomposition data mapping outlined above, the usual ILU factorization cannot easily be exploited. Fortunately, several alternatives exist and we would like to describe just a few of them. ILU for subdomain ordered equations.
Reference: [6] <author> I. S. Duff, A. M. Erisman, and J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Claren-don Press, Oxford, </publisher> <year> 1986. </year>
Reference-contexts: An independent set is maximal if it cannot be augmented by elements in its complement to form a larger independent set. Independent set orderings have been mainly used for deriving parallel direct solution techniques for sparse linear systems [12, 13, 4] and multifrontal techniques <ref> [6] </ref> can be viewed as a particular case.
Reference: [7] <author> H. C. Elman. </author> <title> Iterative Methods for Large Sparse Nonsymmetric Systems of Linear Equations. </title> <type> PhD thesis, </type> <institution> Yale University, Computer Science Dept., </institution> <address> New Haven, CT., </address> <year> 1982. </year>
Reference-contexts: A number of projection processes on so-called Krylov subspaces have been proposed in recent years to solve such systems, see for example, <ref> [1, 7, 9, 8, 11, 10, 14, 22, 20, 21] </ref>.
Reference: [8] <author> R. Freund, M. H. Gutknecht, and N. M. Nachtigal. </author> <title> An implementation of the Look-Ahead Lanczos algorithm for non-Hermitian matrices, Part I. </title> <type> Technical Report 90-11, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts, </institution> <year> 1990. </year> <month> 16 </month>
Reference-contexts: A number of projection processes on so-called Krylov subspaces have been proposed in recent years to solve such systems, see for example, <ref> [1, 7, 9, 8, 11, 10, 14, 22, 20, 21] </ref>.
Reference: [9] <author> R. Freund and N. M. Nachtigal. </author> <title> An implementation of the look-ahead lanczos algorithm for non-Hermitian matrices, Part II. </title> <type> Technical Report 90-11, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts, </institution> <year> 1990. </year>
Reference-contexts: A number of projection processes on so-called Krylov subspaces have been proposed in recent years to solve such systems, see for example, <ref> [1, 7, 9, 8, 11, 10, 14, 22, 20, 21] </ref>.
Reference: [10] <author> A. L. Hageman and D. M. Young. </author> <title> Applied Iterative Methods. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: A number of projection processes on so-called Krylov subspaces have been proposed in recent years to solve such systems, see for example, <ref> [1, 7, 9, 8, 11, 10, 14, 22, 20, 21] </ref>.
Reference: [11] <author> K. C. Jea and D. M. Young. </author> <title> Generalized conjugate gradient acceleration of nonsym-metrizable iterative methods. </title> <journal> Linear Algebra Appl., </journal> <volume> 34 </volume> <pages> 159-194, </pages> <year> 1980. </year>
Reference-contexts: A number of projection processes on so-called Krylov subspaces have been proposed in recent years to solve such systems, see for example, <ref> [1, 7, 9, 8, 11, 10, 14, 22, 20, 21] </ref>.
Reference: [12] <author> R. Leuze. </author> <title> Independent set orderings for parallel matrix factorizations by Gaussian elimination. </title> <journal> Parallel Computing, </journal> <volume> 10 </volume> <pages> 177-191, </pages> <year> 1989. </year>
Reference-contexts: Finding such sets is relatively easy. An independent set is maximal if it cannot be augmented by elements in its complement to form a larger independent set. Independent set orderings have been mainly used for deriving parallel direct solution techniques for sparse linear systems <ref> [12, 13, 4] </ref> and multifrontal techniques [6] can be viewed as a particular case.
Reference: [13] <author> J. G. Lewis, B. W. Peyton, and A. Pothen. </author> <title> A fast algorithm for reordering sparse matrices for parallel factorizations. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 6 </volume> <pages> 1146-1173, </pages> <year> 1989. </year>
Reference-contexts: Finding such sets is relatively easy. An independent set is maximal if it cannot be augmented by elements in its complement to form a larger independent set. Independent set orderings have been mainly used for deriving parallel direct solution techniques for sparse linear systems <ref> [12, 13, 4] </ref> and multifrontal techniques [6] can be viewed as a particular case.
Reference: [14] <author> T. C. Oppe, W. Joubert, and D. R. Kincaid. </author> <title> NSPCG user's guide. a package for solving large linear systems by various iterative methods. </title> <type> Technical report, </type> <institution> The University of Texas at Austin, </institution> <year> 1988. </year>
Reference-contexts: A number of projection processes on so-called Krylov subspaces have been proposed in recent years to solve such systems, see for example, <ref> [1, 7, 9, 8, 11, 10, 14, 22, 20, 21] </ref>.
Reference: [15] <author> Y. Saad. </author> <title> A flexible inner-outer preconditioned GMRES algorithm. </title> <type> Technical Report 91-279, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, Minnesota, </institution> <year> 1991. </year>
Reference-contexts: when the preconditioner M is applied to the right, we will solve instead of (1), the preconditioned linear system (AM 1 )(M x) = b: (2) It is important in many applications to be able to allow the preconditioner to vary from step to step within the inner GMRES process <ref> [15] </ref>. <p> The approximate solution x m obtained from this modified algorithm minimizes the residual norm kb Ax m k 2 over x 0 + SpanfZ m g, <ref> [15] </ref>. In addition, if at a given step k, we have Az k = v k (i.e., if the preconditioning is `exact' at step k) and if the k fi k Hessenberg matrix H k = fh ij g i;j=1;:::;k is nonsingular then the approximation x k is exact.
Reference: [16] <author> Y. Saad. </author> <title> Algorithms and software tools for general purpose automatic domain decomposition. </title> <type> Technical report, </type> <institution> Army High Performance Computing Research Center, </institution> <year> 1992. </year> <note> In preparation. </note>
Reference-contexts: In this paper we will restrict ourselves to discussing the main ideas in this context. Readers are referred to [3] and <ref> [16] </ref> for additional details. 2 Preconditioned Krylov Subspace Methods We consider a linear system of the form Ax = b; (1) where A is a large sparse nonsymmetric real matrix of size N.
Reference: [17] <author> Y. Saad. </author> <title> Highly parallel preconditioners for general sparse matrices. </title> <type> Technical Report 92-087, </type> <institution> University of Minnesota, Army High Performance Computing Research Center, Minneapolis, Minnesota, </institution> <year> 1992. </year>
Reference-contexts: Multi-color k-step SOR/SSOR iterations There are a few simple greedy heuristic which allow to color the subdomains in such a way that no two neighboring sub-domains share the same color. This is a graph coloring problem and the reader is referred to <ref> [17] </ref>, for example, for algorithms and some references. Once the subdo-mains have been colored one can use a form of Multi-Color SOR or SSOR relaxation to precondition the global equations.
Reference: [18] <author> Y. Saad. ILUM: </author> <title> a parallel multi-elimination ILU preconditioner for general sparse matrices. </title> <type> Technical report, </type> <institution> University of Minnesota, Army High Performance Computing Research Center, Minneapolis, Minnesota, </institution> <year> 1992. </year> <note> In preparation. </note>
Reference-contexts: One such procedure has been recently developed and tested. The procedure recursively finds independent sets and defines coarser meshes on them. We will give a brief description of one step of the recursive coarsening process. First, we find a maximal independent set in the graph <ref> [18] </ref>. <p> Heuristics are proposed in <ref> [18] </ref> to find good orders of traversal, i.e., traversals that yield large independent sets. Once an independent set is found we need to define a coarse mesh on the set S.
Reference: [19] <author> Y. Saad. ILUT: </author> <title> a dual threshold incomplete ILU factorization. </title> <type> Technical Report 92-38, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, </institution> <year> 1992. </year>
Reference-contexts: However, it has been our experience that when more fill-in is used the preconditioner improves and the difference in quality narrows, and may even reverse in favor of the more parallel version <ref> [19] </ref>. Multi-color k-step SOR/SSOR iterations There are a few simple greedy heuristic which allow to color the subdomains in such a way that no two neighboring sub-domains share the same color.
Reference: [20] <author> Y. Saad and M. H. Schultz. </author> <title> Conjugate gradient-like algorithms for solving nonsymmetric linear systems. </title> <journal> Mathematics of Computation, </journal> <volume> 44(170) </volume> <pages> 417-424, </pages> <year> 1985. </year>
Reference-contexts: A number of projection processes on so-called Krylov subspaces have been proposed in recent years to solve such systems, see for example, <ref> [1, 7, 9, 8, 11, 10, 14, 22, 20, 21] </ref>.
Reference: [21] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: A number of projection processes on so-called Krylov subspaces have been proposed in recent years to solve such systems, see for example, <ref> [1, 7, 9, 8, 11, 10, 14, 22, 20, 21] </ref>. <p> A number of projection processes on so-called Krylov subspaces have been proposed in recent years to solve such systems, see for example, [1, 7, 9, 8, 11, 10, 14, 22, 20, 21]. Thus, the GMRES algorithm introduced in <ref> [21] </ref> for solving general sparse nonsymmetric linear systems of equations is a tehnique which minimizes the 2-norm of the residual vector b Ax over x in the Krylov subspace K m = Spanfr 0 ; Ar 0 ; : : : ; A m1 r 0 g; 2 where r 0 <p> simply observing that in the last step of the standard GMRES algorithm, the approximate solution is formed as a linear combination of the preconditioned vectors z i = M 1 v i ; i = 1; : : : ; m, where the v i 's are the Arnoldi vectors <ref> [21] </ref>. Since these vectors are all obtained by applying the same preconditioning matrix M 1 to the v's, we need not save them. We only need to apply M 1 to the linear combination of the v 0 s.
Reference: [22] <author> P. K. W. Vinsome. Orthomin, </author> <title> an iterative method for solving sparse sets of simultaneous linear equations. </title> <booktitle> In Proceedings of the Fourth Symposium on Resevoir Simulation, </booktitle> <pages> pages 149-159. </pages> <booktitle> Society of Petroleum Engineers of AIME, </booktitle> <year> 1976. </year> <month> 17 </month>
Reference-contexts: A number of projection processes on so-called Krylov subspaces have been proposed in recent years to solve such systems, see for example, <ref> [1, 7, 9, 8, 11, 10, 14, 22, 20, 21] </ref>.
References-found: 22

