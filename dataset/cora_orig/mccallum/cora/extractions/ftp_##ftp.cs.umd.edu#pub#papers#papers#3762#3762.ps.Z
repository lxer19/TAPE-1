URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3762/3762.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Understanding the Sources of Variation in Software Inspections combined effects of process inputs and process
Author: Adam Porter Harvey Siy Audris Mockus, Lawrence Votta A. Porter and H. Siy, 
Keyword: Categories and Subject Descriptors: D.2.5 [Software Engineering]: Testing and Debugging-Code inspections and walk-throughs General Terms: Experimentation Additional Key Words and Phrases: Statistical models, empirical studies, software inspection, software process  
Address: Park, MD 20742,  IL 60566,  
Affiliation: University of Maryland Bell Laboratories  Department of Computer Science, University of Maryland, College  Department, Bell Laboratories, Lucent Technologies, Naperville,  
Note: Submitted for publication to ACM Transactions on Software Engineering and Methodology.  The  to be identified.  This work is supported in part by a National Science Foundation Faculty Early Career Development Award, CCR-9501354. Dr. Siy was also partly supported by AT&T 's Summer Employment Program. Authors' addresses:  A. Mockus and L. Votta, Software Production Research  
Email: faporter,harveyg@cs.umd.edu;  faudris,vottag@research.bell-labs.com  
Date: January 6, 1997  
Abstract: In a previous experiment, we determined how various changes in three structural elements of the software inspection process (team size, and number and sequencing of sessions), altered effectiveness and interval. Our results showed that such changes did not significantly influence the defect detection rate, but that certain combinations of changes dramatically increased the inspection interval. We also observed a large amount of unexplained variance in the data, indicating that other factors must be affecting inspection performance. The nature and extent of these other factors now have to be determined to ensure that they had not biased our earlier results. Also, identifying these other factors might suggest additional ways to improve the efficiency of inspections. Acting on the hypothesis that the "inputs" into the inspection process (reviewers, authors, and code units) were significant sources of variation, we modeled their effects on inspection performance. We found that they were responsible for much more variation in defect detection than was process structure. This leads us to conclude that better defect detection techniques, not better process structures, are the key to improving inspection effectiveness. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Victor R. Basili and Harlan D. Mills. </author> <title> Understanding and documenting programs. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-8(3):270-283, </volume> <month> May </month> <year> 1982. </year>
Reference-contexts: Researchers should therefore concentrate on improving the small-team-single-session process by finding better techniques for reviewers to carry it out (e.g., systematic reading techniques <ref> [1] </ref> for the preparation step, meetingless techniques [10, 18, 12] for the collection step, etc.). 7 Future Work 7.1 Framework For Further Study Our study revealed a number of influences affecting variation in the data, some internal and some external to the inspection process.
Reference: [2] <author> Victor R. Basili and David M Weiss. </author> <title> A methodology for collecting valid software engineering data. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-10(6):728-738, </volume> <month> Nov. </month> <year> 1984. </year>
Reference-contexts: The entire data set may be examined online at http://www.cs.umd.edu/users/harvey/variance.html. 2.3 Self-Reported Data Self-reported data tend to contain systematic errors. Therefore we minimized the amount of self-reported data by employing direct observation [20] and interviews <ref> [2] </ref>. The IQE attended 125 of the 130 collection meetings 6 to make sure the meeting data was reported accurately and that reviewers do not mistakenly add to their preparation forms any issues that were not found until collection.
Reference: [3] <author> Richard A. Becker, John M. Chambers, and Allan R. Wilks. </author> <title> The New S Language. </title> <publisher> Wadsworth and Brooks/Cole, </publisher> <year> 1988. </year>
Reference-contexts: Model building involves formulating the model, fitting the model, and checking that the model adequately characterizes the process. We built the models in the S programming language <ref> [3, 7] </ref>.
Reference: [4] <author> George E. Box, William G. Hunter, and J. Stuart Hunter. </author> <title> Statistics for Experimenters. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1978. </year>
Reference-contexts: For each factor, we built a contingency table, showing the frequency of occurrence of each value of that factor within each treatment. We then used Pearson's 2 -test for independence <ref> [4, pp. 145-150] </ref>. If the result is significant, then the factor is not independently distributed across the treatments.
Reference: [5] <author> F. O. Buck. </author> <title> Indicators of quality inspections. </title> <type> Technical Report 21.802, </type> <institution> IBM, Kingston, </institution> <address> NY, </address> <month> Sep. </month> <year> 1981. </year>
Reference-contexts: is illustrated 11 While it is beside the goals of this study, we feel it is important to point out this circular cause-effect relationship in light of recommendations that making reviewers spend a minimum amount of time in preparation and in collection will result in the finding of more defects <ref> [5] </ref>. 20 of defects found per session in each 2sX2pN inspection. Each column represents one inspection.
Reference: [6] <author> John M. Chambers, William S. Cleveland, Beat Kleiner, and Paul A. Tuckey. </author> <title> Graphical Methods For Data Analysis. </title> <publisher> Chapman & Hall, </publisher> <year> 1983. </year>
Reference: [7] <author> John M. Chambers and Trevor J. Hastie, </author> <title> editors. Statistical Models in S. </title> <publisher> Wadsworth & Brooks, </publisher> <year> 1992. </year>
Reference-contexts: Model building involves formulating the model, fitting the model, and checking that the model adequately characterizes the process. We built the models in the S programming language <ref> [3, 7] </ref>. <p> following formula: 13 Def ects ~ T eamSize + Sessions + Repairs + P hase + Author + F unc + log (Size) + 12 The generalized linear model and the rationale for using it are explained in Appendix C. 13 We used S language notation to represent our models <ref> [7, pp. 24-31] </ref>. <p> R A + R B + R C + R D + R E + R F + R G + R H + R I + R J + R K (1) In this model, Functionality and Author are categorical variables represented in S as sets of dummy variables <ref> [7, pp. 20-22,32-36] </ref>. They have 7 and 5 degrees of freedom, respectively. Stepwise model selection heuristic 14 selected the following model. <p> To avoid overfitting the data, the number of parameters must always be kept small or the residual degrees of freedom high. To perform stepwise model selection we used the step () function in S <ref> [7, pp. 233-238] </ref>. 15 In S, increase the scale parameter of the step () function. 24 the model with the original values (a perfect fit would imply that everything is on the line y = x). There is a substantial correlation between the two (cor = 0.69).
Reference: [8] <author> Chris Chatfield. </author> <title> Model uncertainty, data mining and statistical inference. </title> <journal> Journal of the Royal Statistical Society, Series A, </journal> <volume> 158(3), </volume> <year> 1995. </year>
Reference: [9] <author> Bill Curtis. </author> <title> Substantiating programmer variability. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 69(7):846, </address> <month> July </month> <year> 1981. </year>
Reference-contexts: This has several implications for the design and analysis of industrial experiments. Past studies have cautioned that wide variation in the abilities of individual developers may mask effects due to experimental treatments <ref> [9] </ref>. However, even with our relatively crude models, we managed to devise a suitable means of accounting for individual variation when analyzing the experimental results. But ultimately, we will get better results only if we can identify and control for factors affecting reviewer and author performance.
Reference: [10] <author> Alan R. Dennis and Joseph S. Valacich. </author> <title> Computer brainstorms: More heads are better than one. </title> <journal> Journal of Applied Psychology, </journal> <volume> 78(4) </volume> <pages> 531-537, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Researchers should therefore concentrate on improving the small-team-single-session process by finding better techniques for reviewers to carry it out (e.g., systematic reading techniques [1] for the preparation step, meetingless techniques <ref> [10, 18, 12] </ref> for the collection step, etc.). 7 Future Work 7.1 Framework For Further Study Our study revealed a number of influences affecting variation in the data, some internal and some external to the inspection process.
Reference: [11] <author> Stephen G. Eick, Clive R. Loader, M. David Long, Lawrence G. Votta, and Scott Vander Wiel. </author> <title> Estimating software fault content before coding. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 59-65, </pages> <address> Melbourne, Australia, </address> <month> May </month> <year> 1992. </year>
Reference: [12] <author> Philip M. Johnson. </author> <title> An instrumented approach to improving software quality through formal technical review. </title> <booktitle> In Proceedings of the 16th International Conference on Software Engineering, </booktitle> <pages> pages 113-122, </pages> <address> Sorrento, Italy, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Researchers should therefore concentrate on improving the small-team-single-session process by finding better techniques for reviewers to carry it out (e.g., systematic reading techniques [1] for the preparation step, meetingless techniques <ref> [10, 18, 12] </ref> for the collection step, etc.). 7 Future Work 7.1 Framework For Further Study Our study revealed a number of influences affecting variation in the data, some internal and some external to the inspection process.
Reference: [13] <author> David A. Kenny. </author> <title> Correlation and Causality. </title> <publisher> John Wiley & Sons., </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Finally, we rejected models for which, based on our experience, we could not argue that their variables were causal agents of inspection performance. Specifically, there are four conditions that must be satisfied before factor A can be said to cause response B <ref> [13] </ref>: 1. A must occur before B. 2. A and B must be correlated. 3. There is no other factor C that accounts for the correlation between A and B. 4. A mechanism exists that explains how A affects B.
Reference: [14] <author> Brett Kyle. </author> <title> Successful Industrial Experimentation, chapter 5. </title> <publisher> VCH Publishers, Inc., </publisher> <year> 1995. </year>
Reference-contexts: It shows how these inputs interact with each process step. This is an example of a cause-and-effect diagram, similar to the ones used in practice <ref> [14] </ref>, but customized here for our use. The number and types of issues raised in the preparation step are influenced by the reviewers selected and by the number of defects originally in the code unit (which in turn may be affected by the author of the code unit).
Reference: [15] <author> David A. Ladd and J. Christopher Ramming. </author> <title> Software research and switch software. </title> <booktitle> In International Conference on Communications Technology, </booktitle> <address> Beijing, China, </address> <year> 1992. </year>
Reference: [16] <author> David A. Ladd and J. Christopher Ramming. </author> <title> Two application languages in software production. </title> <booktitle> In USENIX Symposium on Very-High-Level Languages, </booktitle> <month> Oct. </month> <year> 1994. </year>
Reference: [17] <author> P. McCullagh and J. A. Nelder. </author> <title> Generalized Linear Models. </title> <publisher> Chapman and Hall, </publisher> <address> 2nd edition, </address> <year> 1989. </year>
Reference: [18] <author> J.F. Nunamaker, Alan R. Dennis, Joseph S. Valacich, Douglas R. Vogel, and Joey F. George. </author> <title> Electronic meeting systems to support group work. </title> <journal> Communications of the ACM, </journal> <volume> 34(7) </volume> <pages> 40-61, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Researchers should therefore concentrate on improving the small-team-single-session process by finding better techniques for reviewers to carry it out (e.g., systematic reading techniques [1] for the preparation step, meetingless techniques <ref> [10, 18, 12] </ref> for the collection step, etc.). 7 Future Work 7.1 Framework For Further Study Our study revealed a number of influences affecting variation in the data, some internal and some external to the inspection process.
Reference: [19] <author> Dewayne E. Perry, Adam A. Porter, and Lawrence G. Votta. </author> <title> Experimental software engineering: A report on the state of the art. </title> <booktitle> In Proceedings of the 17th International Conference on Software Engineering, </booktitle> <pages> pages 277-279, </pages> <address> Seattle, WA, </address> <month> April </month> <year> 1995. </year> <note> Invited talk and short paper appear in the proceedings. </note>
Reference-contexts: As for the element of error, previous observational studies on time usage conducted in this environment have shown that although there are always inaccuracies in self-reported data, the self-reported data is generally within 20% of the observed data <ref> [19] </ref>. 2.4 Results of the Experiment Our experiment produced three general results: 1. Inspection interval and effectiveness of defect detection were not significantly affected by team size (large vs. small). 2. Inspection interval and effectiveness of defect detection were not significantly affected by number of sessions (single vs. multiple). 3.
Reference: [20] <author> Dewayne E. Perry, Nancy A. Staudenmayer, and Lawrence G. Votta. </author> <title> Understanding and improving time usage in software development. </title> <editor> In Alexander Wolf and Alfonso Fuggetta, </editor> <title> 38 editors, Software Process, </title> <booktitle> volume 5 of Trends in Software: Software Process. </booktitle> <publisher> John Wiley & Sons., </publisher> <year> 1995. </year>
Reference-contexts: The entire data set may be examined online at http://www.cs.umd.edu/users/harvey/variance.html. 2.3 Self-Reported Data Self-reported data tend to contain systematic errors. Therefore we minimized the amount of self-reported data by employing direct observation <ref> [20] </ref> and interviews [2]. The IQE attended 125 of the 130 collection meetings 6 to make sure the meeting data was reported accurately and that reviewers do not mistakenly add to their preparation forms any issues that were not found until collection.
Reference: [21] <author> Adam A. Porter, Lawrence G. Votta, Harvey P. Siy, and Carol A. Toman. </author> <title> An experiment to assess the cost-benefits of code inspections in large scale software development. </title> <booktitle> In The Third Symposium on the Foundations of Software Engineering, </booktitle> <address> Washington, D.C., </address> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: Therefore, we conducted a controlled experiment in which we manipulated the structure of the inspection process <ref> [21] </ref>. We adjusted the size of the team and the number of sessions. Defects were sometimes repaired in between multiple sessions and sometimes not.
Reference: [22] <author> Scott Vander Wiel and Lawrence G. Votta. </author> <title> Assessing software designs using capture-recapture methods. </title> <journal> IEEE Trans. on Software Engineering, </journal> 19(11) 1045-1054, Nov. 1993. <volume> 39 </volume>
References-found: 22

