URL: ftp://ftp.cs.dartmouth.edu/TR/TR95-267.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR95-267/
Root-URL: http://www.cs.dartmouth.edu
Title: An API for Choreographing Data Accesses  
Author: Elizabeth A. M. Shriver Leonard F. Wisniewski 
Date: November 8, 1995  
Address: New York University  College PCS-TR95-267  
Affiliation: Courant Institute of Mathematical Sciences  Department of Computer Science Dartmouth College Dartmouth  
Abstract: Current APIs for multiprocessor multi-disk file systems are not easy to use in developing out-of-core algorithms that choreograph parallel data accesses. Consequently, the efficiency of these algorithms is hard to achieve in practice. We address this deficiency by specifying an API that includes data-access primitives for data choreography. With our API, the programmer can easily access specific blocks from each disk in a single operation, thereby fully utilizing the parallelism of the underlying storage system. Our API supports the development of libraries of commonly-used higher-level routines such as matrix-matrix addition, matrix-matrix multiplication, and BMMC (bit-matrix-multiply/complement) permutations. We illustrate our API in implementations of these three high-level routines to demonstrate how easy it is to use.
Abstract-found: 1
Intro-found: 1
Reference: [ACFS94] <author> B. Alpern, L. Carter, E. Feig, and T. Selker. </author> <title> The uniform memory hierarchy model of computation. </title> <journal> Algorithmica, </journal> 12(2/3):72-109, August and September 1994. 
Reference-contexts: models or input/output (I/O) complexity models, that aim to represent the key features of computer memory hierarchies and data movement in order to present a suitable model for algorithm design and analysis at a feasible level of abstraction. 1 The interested reader can find full descriptions of these models in <ref> [ACFS94, AV88, VS90, VS94] </ref>. Using the I/O complexity models, algorithm designers have developed out-of-core algorithms that choreograph data movements to utilize all of the disks in the I/O subsystem concurrently. We feel that this approach is a viable solution because of the asymptotic performance gains achieved by I/O-efficient algorithms.
Reference: [AP94] <author> Alok Aggarwal and C. Greg Plaxton. </author> <title> Optimal parallel sorting in multi-level storage. </title> <booktitle> In Proceedings of the 5th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 659-668, </pages> <address> Arlington, VA, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: In particular, Vitter and Shriver's Parallel Disk Model (PDM) [VS94] provides a reasonable model for the design of I/O-efficient algorithms and analysis at a feasible level of abstraction. A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting <ref> [AP94, Arg95, NV93, VS94] </ref>, general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93].
Reference: [Arg95] <author> Lars Arge. </author> <title> The buffer tree: A new technique for optimal I/O-algorithms. </title> <booktitle> In 4th International Workshop on Algorithms and Data Structures (Proceedings), Lecture Notes in Computer Science, </booktitle> <volume> number 955, </volume> <pages> pages 334-345, </pages> <address> Kingston, Canada, </address> <month> August </month> <year> 1995. </year> <note> Springer-Verlag. </note>
Reference-contexts: In particular, Vitter and Shriver's Parallel Disk Model (PDM) [VS94] provides a reasonable model for the design of I/O-efficient algorithms and analysis at a feasible level of abstraction. A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting <ref> [AP94, Arg95, NV93, VS94] </ref>, general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93].
Reference: [AV88] <author> Alok Aggarwal and Jeffrey Scott Vitter. </author> <title> The input/output complexity of sorting and related problems. </title> <journal> Communications of the ACM, </journal> <volume> 31(9) </volume> <pages> 1116-1127, </pages> <month> Septem-ber </month> <year> 1988. </year>
Reference-contexts: models or input/output (I/O) complexity models, that aim to represent the key features of computer memory hierarchies and data movement in order to present a suitable model for algorithm design and analysis at a feasible level of abstraction. 1 The interested reader can find full descriptions of these models in <ref> [ACFS94, AV88, VS90, VS94] </ref>. Using the I/O complexity models, algorithm designers have developed out-of-core algorithms that choreograph data movements to utilize all of the disks in the I/O subsystem concurrently. We feel that this approach is a viable solution because of the asymptotic performance gains achieved by I/O-efficient algorithms. <p> Section 4 discusses the implemention of the I/O-optimal algorithms for performing matrix-matrix addition, matrix-matrix multiplication, and BMMC (bit-matrix-multiply/complement) permutations using our API. Section 5 contains some concluding remarks. 2 Background Researchers have developed theoretical models to capture important features of data movement between main memory and secondary storage <ref> [AV88, Flo72, VS90, VS94] </ref>. In particular, Vitter and Shriver's Parallel Disk Model (PDM) [VS94] provides a reasonable model for the design of I/O-efficient algorithms and analysis at a feasible level of abstraction.
Reference: [AVV95] <author> L. Arge, D. E. Vengroff, and J. S. Vitter. </author> <title> External-memory algorithms for processing line segments in geographic information systems. To appear, </title> <booktitle> Third European Symposium on Algorithms, </booktitle> <year> 1995. </year> <month> 22 </month>
Reference-contexts: designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms <ref> [AVV95, GTVV93] </ref>. In this section, we define the parameters and data layout for PDM.
Reference: [BBS + 94] <author> Robert Bennett, Kelvin Bryant, Alan Sussman, Raja Das, and Joel Saltz. Jo--vian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: A loosely synchronous computation is one in which all the participating processors alternate between phases of computation and I/O <ref> [BBS + 94] </ref>. We assume a striped file across the disks with a striping unit of one block per disk, i.e., the blocks are placed on disk in a round-robin fashion as shown in Figure 2. <p> The Panda run-time library [SCJ + 95, SW94] achieves performance improvements by pro 8 viding an API and more efficient layout alternatives for multidimensional array data. This approach takes advantage of the spatiotemporal locality of the array. Jovian <ref> [BBS + 94] </ref> and PASSION [CBH + 94] also provide support for efficient array data access, but abstract away direct disk access from the programmer.
Reference: [CBF93] <author> Peter F. Corbett, Sandra Johnson Baylor, and Dror G. Feitelson. </author> <title> Overview of the Vesta parallel file system. </title> <booktitle> In Proceedings of IPPS '93 Workshop on I/O in Parallel Computer Systems, </booktitle> <pages> pages 1-16, </pages> <month> April </month> <year> 1993. </year> <note> Reprinted in Computer Architecture News, </note> <month> December </month> <year> 1993. </year>
Reference-contexts: Another approach integrates special enhancements for I/O into the file system <ref> [CBF93, KS93] </ref>.
Reference: [CBH + 94] <author> Alok Choudhary, Rajesh Bordawekar, Michael Harry, Rakesh Krishnaiyer, Ravi Ponnusamy, Tarvinder Singh, and Rajeev Thakur. </author> <title> PASSION: parallel and scalable software for input-output. </title> <type> Technical Report SCCS-636, </type> <institution> ECE Dept., NPAC and CASE Center, Syracuse University, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: At the software level, considerable effort has been made to develop new languages and compiler features that support I/O parallelism and optimizations via data layout conversion [dBC93], compiler hints [PGS93], and preprocessing for out-of-core parallel code <ref> [CBH + 94, CC94] </ref>. Another approach integrates special enhancements for I/O into the file system [CBF93, KS93]. <p> The Panda run-time library [SCJ + 95, SW94] achieves performance improvements by pro 8 viding an API and more efficient layout alternatives for multidimensional array data. This approach takes advantage of the spatiotemporal locality of the array. Jovian [BBS + 94] and PASSION <ref> [CBH + 94] </ref> also provide support for efficient array data access, but abstract away direct disk access from the programmer.
Reference: [CC94] <author> Thomas H. Cormen and Alex Colvin. </author> <title> ViC*: A preprocessor for virtual-memory C*. </title> <type> Technical Report PCS-TR94-243, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: At the software level, considerable effort has been made to develop new languages and compiler features that support I/O parallelism and optimizations via data layout conversion [dBC93], compiler hints [PGS93], and preprocessing for out-of-core parallel code <ref> [CBH + 94, CC94] </ref>. Another approach integrates special enhancements for I/O into the file system [CBF93, KS93].
Reference: [CFF + 95] <author> Peter Corbett, Dror Feitelson, Sam Fineberg, Yarsun Hsu, Bill Nitzberg, Jean-Pierre Prost, Marc Snir, Bernard Traversat, and Parkson Wong. </author> <title> Overview of the MPI-IO parallel I/O interface. </title> <booktitle> In IPPS '95 Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 1-15, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: These APIs include extensions to the conventional Unix interface (e.g., [NK95]), modifications to the conventional Unix interface (e.g., [GS95]), and other interfaces which differ from Unix (e.g., <ref> [CFF + 95, CFPB93] </ref>). An I/O-efficient algorithm would not be easy to program using these low-level file system APIs since the programmer must map the high-level parallel disk accesses to low-level file system operations.
Reference: [CFPB93] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, and Sandra Johnson Baylor. </author> <title> Parallel access to files in the Vesta file system. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: These APIs include extensions to the conventional Unix interface (e.g., [NK95]), modifications to the conventional Unix interface (e.g., [GS95]), and other interfaces which differ from Unix (e.g., <ref> [CFF + 95, CFPB93] </ref>). An I/O-efficient algorithm would not be easy to program using these low-level file system APIs since the programmer must map the high-level parallel disk accesses to low-level file system operations.
Reference: [CGG + 95] <author> Yi-Jen Chiang, Michael T. Goodrich, Edward F. Grove, Roberto Tamassia, Darren Erik Vengroff, and Jeffrey Scott Vitter. </author> <title> External-memory graph algorithms. </title> <booktitle> In Proceedings of the Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 139-149, </pages> <address> San Francisco, CA, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms <ref> [CGG + 95] </ref>, and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM.
Reference: [CK94] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <type> Technical Report PCS-TR93-188, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> September </month> <year> 1994. </year> <note> Earlier version appeared in Proceedings of the 1993 DAGS/PC Symposium, </note> <editor> Hanover, </editor> <publisher> NH, </publisher> <pages> pages 64-74, </pages> <month> June </month> <year> 1993. </year> <month> 23 </month>
Reference-contexts: If the parallel disk system lays out consecutive logical blocks of a file in a physically consecutive fashion, consecutive access can result in small seek times when accessing a file sequentially (e.g., by memoryload access). 2.2 Parallel disk access in file systems and runtime libraries Cormen and Kotz <ref> [CK94] </ref> have identified several capabilities that a file system should support to enable high-performance implementations of the I/O-efficient algorithms. They point out that extensions of the traditional single-offset I/O interface for parallel file systems cannot allow independent access across disks.
Reference: [Cor92] <author> Thomas H. Cormen. </author> <title> Virtual Memory for Data-Parallel Computing. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <year> 1992. </year> <note> Available as Technical Report MIT/LCS/TR-559. </note>
Reference-contexts: A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations <ref> [Cor92, Cor93, CSW94, Wis95] </ref>, mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations <ref> [Cor92, Wis95] </ref>, matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose <ref> [Cor92, Cor93, CSW94, VS94] </ref>, FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM.
Reference: [Cor93] <author> Thomas H. Cormen. </author> <title> Fast permuting on disk arrays. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):41-57, January and February 1993. </note>
Reference-contexts: A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations <ref> [Cor92, Cor93, CSW94, Wis95] </ref>, mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose <ref> [Cor92, Cor93, CSW94, VS94] </ref>, FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM.
Reference: [CSW94] <author> Thomas H. Cormen, Thomas Sundquist, and Leonard F. Wisniewski. </author> <title> Asymptotically tight bounds for performing BMMC permutations on parallel disk systems. </title> <type> Technical Report PCS-TR94-223, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> July </month> <year> 1994. </year> <booktitle> Extended abstract appeared in Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Velen, Germany, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations <ref> [Cor92, Cor93, CSW94, Wis95] </ref>, mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose <ref> [Cor92, Cor93, CSW94, VS94] </ref>, FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> Cormen, Sundquist, and Wisniewski <ref> [CSW94] </ref> show how to perform any BMMC permutation in an asymptotically optimal number of parallel I/Os. Their algorithm decomposes the permutation into a series of permutations, each of which can be performed in one pass over the data.
Reference: [dBC93] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In Proceedings of the IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 56-70, </pages> <year> 1993. </year> <note> Shortened version published in Computer Architecture News, Decem-ber 1993. </note>
Reference-contexts: Mechanisms such as disk striping or interleaving [Kim86, SGM86], and RAID [PGK88] have achieved fine-grain parallelism at the physical disk level. At the software level, considerable effort has been made to develop new languages and compiler features that support I/O parallelism and optimizations via data layout conversion <ref> [dBC93] </ref>, compiler hints [PGS93], and preprocessing for out-of-core parallel code [CBH + 94, CC94]. Another approach integrates special enhancements for I/O into the file system [CBF93, KS93].
Reference: [EHKM94] <author> Christopher L. Elford, Jay Huber, Chris Kuszmaul, and Tara Madhyastha. </author> <title> Portable parallel file system detailed design. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Illinois, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: Unfortunately, none of these run-time libraries include routines for simultaneous, direct access to the disks. The API of PPFS provides routines that support independent access at the record level, not at the disk level <ref> [EHKM94, HER + 95] </ref>. This API provides multiple-offset routines that allow the user to access more than one range of data in a single request, but does not provide a disk abstraction that allows the user to request particular blocks from each disk.
Reference: [Flo72] <author> R. W. Floyd. </author> <title> Permuting information in idealized two-level storage. </title> <editor> In R. Miller and J. Thatcher, editors, </editor> <booktitle> Complexity of Computer Calculations, </booktitle> <pages> pages 105-109. </pages> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: Section 4 discusses the implemention of the I/O-optimal algorithms for performing matrix-matrix addition, matrix-matrix multiplication, and BMMC (bit-matrix-multiply/complement) permutations using our API. Section 5 contains some concluding remarks. 2 Background Researchers have developed theoretical models to capture important features of data movement between main memory and secondary storage <ref> [AV88, Flo72, VS90, VS94] </ref>. In particular, Vitter and Shriver's Parallel Disk Model (PDM) [VS94] provides a reasonable model for the design of I/O-efficient algorithms and analysis at a feasible level of abstraction.
Reference: [GS95] <author> Garth Gibson and Daniel Stodolsky. </author> <title> Issues arising in the SIO-OS low-level PFS API. </title> <booktitle> Presentation at the Scalable Input/Output for High Performance Computers Workshop at the Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: A number of existing file systems provide low-level API support for simultaneous, independent, direct access to the multiple disks provided by modern machine architectures. These APIs include extensions to the conventional Unix interface (e.g., [NK95]), modifications to the conventional Unix interface (e.g., <ref> [GS95] </ref>), and other interfaces which differ from Unix (e.g., [CFF + 95, CFPB93]). An I/O-efficient algorithm would not be easy to program using these low-level file system APIs since the programmer must map the high-level parallel disk accesses to low-level file system operations.
Reference: [GTVV93] <author> M. H. Goodrich, J.-J. Tsay, D. E. Vengroff, and J. S. Vitter. </author> <title> External-memory computational geometery. </title> <booktitle> In Proceedings of the 34th Annual Symposium on 24 Foundations of Computer Science, </booktitle> <pages> pages 714-723, </pages> <address> Palo Alto, CA, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms <ref> [AVV95, GTVV93] </ref>. In this section, we define the parameters and data layout for PDM.
Reference: [HER + 95] <author> Jay Huber, Christopher L. Elford, Daniel A. Reed, Andrew A. Chien, and David S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Unfortunately, none of these run-time libraries include routines for simultaneous, direct access to the disks. The API of PPFS provides routines that support independent access at the record level, not at the disk level <ref> [EHKM94, HER + 95] </ref>. This API provides multiple-offset routines that allow the user to access more than one range of data in a single request, but does not provide a disk abstraction that allows the user to request particular blocks from each disk.
Reference: [Kim86] <author> Michelle Y. Kim. </author> <title> Synchronized disk interleaving. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(11):978-988, </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: Author's e-mail address: wisnie@cs.dartmouth.edu. 1 The hardware solutions include methods to improve the rate of I/O throughput to unipro-cessor systems by introducing parallelism into the I/O subsystem. Mechanisms such as disk striping or interleaving <ref> [Kim86, SGM86] </ref>, and RAID [PGK88] have achieved fine-grain parallelism at the physical disk level.
Reference: [KS93] <author> Orran Krieger and Michael Stumm. </author> <title> HFS: A flexible file system for large-scale multiprocessors. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 6-14, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Another approach integrates special enhancements for I/O into the file system <ref> [CBF93, KS93] </ref>.
Reference: [NK95] <author> Nils Nieuwejaar and David Kotz. </author> <title> A multiprocessor extension to the conventional file system interface. </title> <type> Technical Report PCS-TR95-253, </type> <institution> Dartmouth College, </institution> <year> 1995. </year>
Reference-contexts: A number of existing file systems provide low-level API support for simultaneous, independent, direct access to the multiple disks provided by modern machine architectures. These APIs include extensions to the conventional Unix interface (e.g., <ref> [NK95] </ref>), modifications to the conventional Unix interface (e.g., [GS95]), and other interfaces which differ from Unix (e.g., [CFF + 95, CFPB93]).
Reference: [NV93] <author> Mark H. Nodine and Jeffrey Scott Vitter. </author> <title> Deterministic distribution sort in shared and distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 120-129, </pages> <address> Velen, Germany, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: In particular, Vitter and Shriver's Parallel Disk Model (PDM) [VS94] provides a reasonable model for the design of I/O-efficient algorithms and analysis at a feasible level of abstraction. A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting <ref> [AP94, Arg95, NV93, VS94] </ref>, general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93].
Reference: [PGK88] <author> David A. Patterson, Garth Gibson, and Randy H. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 109-116, </pages> <address> Chicago, IL, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Author's e-mail address: wisnie@cs.dartmouth.edu. 1 The hardware solutions include methods to improve the rate of I/O throughput to unipro-cessor systems by introducing parallelism into the I/O subsystem. Mechanisms such as disk striping or interleaving [Kim86, SGM86], and RAID <ref> [PGK88] </ref> have achieved fine-grain parallelism at the physical disk level. At the software level, considerable effort has been made to develop new languages and compiler features that support I/O parallelism and optimizations via data layout conversion [dBC93], compiler hints [PGS93], and preprocessing for out-of-core parallel code [CBH + 94, CC94].
Reference: [PGS93] <author> R. H. Patterson, G. A. Gibson, and M. Satyanarayanan. </author> <title> Informed prefetch-ing: Converting high throughput to low latency. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: At the software level, considerable effort has been made to develop new languages and compiler features that support I/O parallelism and optimizations via data layout conversion [dBC93], compiler hints <ref> [PGS93] </ref>, and preprocessing for out-of-core parallel code [CBH + 94, CC94]. Another approach integrates special enhancements for I/O into the file system [CBF93, KS93].
Reference: [SCJ + 95] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year> <note> To appear. 25 </note>
Reference-contexts: At a higher level of abstraction, run-time libraries achieve I/O-performance improvements on multiple disk systems, but typically lose the direct disk access in the abstraction. The Panda run-time library <ref> [SCJ + 95, SW94] </ref> achieves performance improvements by pro 8 viding an API and more efficient layout alternatives for multidimensional array data. This approach takes advantage of the spatiotemporal locality of the array.
Reference: [SGM86] <author> Kenneth Salem and Hector Garcia-Molina. </author> <title> Disk striping. </title> <booktitle> In IEEE 1986 Con--ference on Data Engineering, </booktitle> <pages> pages 336-342, </pages> <year> 1986. </year>
Reference-contexts: Author's e-mail address: wisnie@cs.dartmouth.edu. 1 The hardware solutions include methods to improve the rate of I/O throughput to unipro-cessor systems by introducing parallelism into the I/O subsystem. Mechanisms such as disk striping or interleaving <ref> [Kim86, SGM86] </ref>, and RAID [PGK88] have achieved fine-grain parallelism at the physical disk level.
Reference: [SW94] <author> K. E. Seamons and M. Winslett. </author> <title> An efficient abstract interface for multidimensional array I/O. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 650-659, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: At a higher level of abstraction, run-time libraries achieve I/O-performance improvements on multiple disk systems, but typically lose the direct disk access in the abstraction. The Panda run-time library <ref> [SCJ + 95, SW94] </ref> achieves performance improvements by pro 8 viding an API and more efficient layout alternatives for multidimensional array data. This approach takes advantage of the spatiotemporal locality of the array.
Reference: [SWC + 95] <author> Elizabeth A. M. Shriver, Leonard F. Wisniewski, Bruce G. Calder, David Green-berg, Ryan Moore, and David Womble. </author> <title> Parallel disk access using the Whiptail File System: Design and implementation. </title> <type> Manuscript, </type> <year> 1995. </year>
Reference-contexts: Our API is easy to implement; it has been implemented as the interface for the Whiptail File System, an experimental file system for I/O-efficient out-of-core problems <ref> [SWC + 95] </ref>. The outline of this paper is as follows. In Section 2, we describe the Parallel Disk Model and the types of data access used by out-of-core algorithms designed on this model. We also discuss the extent to which existing file system interfaces support these out-of-core algorithms. <p> It would be easiest to implement and would have the least performance overhead when being implemented on a file system with routines for direct disk or RAID access. Researchers at Sandia National Laboratories have developed the Whiptail File System (WFS) <ref> [SWC + 95] </ref>, the first implementation of our API. WFS is a prototype file system built on top of the Parallel File System (PFS) on the Intel Paragon. PFS provides direct access to each of the local RAIDs on the Intel Paragon.
Reference: [Ven94] <author> Darren Erik Vengroff. </author> <title> A transparent parallel I/O environment. </title> <booktitle> In Proceedings of the 1994 DAGS/PC Symposium, </booktitle> <pages> pages 117-134, </pages> <address> Hanover, NH, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: This approach takes advantage of the spatiotemporal locality of the array. Jovian [BBS + 94] and PASSION [CBH + 94] also provide support for efficient array data access, but abstract away direct disk access from the programmer. The Transparent Parallel I/O Environment (TPIE) <ref> [Ven94] </ref> provides a high-level access method interface (AMI) to the I/O paradigms that have already been developed, but does not allow the user to explicitly program disk accesses. Unfortunately, none of these run-time libraries include routines for simultaneous, direct access to the disks.
Reference: [VS90] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Optimal disk I/O with parallel block transfer. </title> <booktitle> In Proceedings of the 22nd Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 159-169, </pages> <address> Baltimore, MD, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: models or input/output (I/O) complexity models, that aim to represent the key features of computer memory hierarchies and data movement in order to present a suitable model for algorithm design and analysis at a feasible level of abstraction. 1 The interested reader can find full descriptions of these models in <ref> [ACFS94, AV88, VS90, VS94] </ref>. Using the I/O complexity models, algorithm designers have developed out-of-core algorithms that choreograph data movements to utilize all of the disks in the I/O subsystem concurrently. We feel that this approach is a viable solution because of the asymptotic performance gains achieved by I/O-efficient algorithms. <p> Section 4 discusses the implemention of the I/O-optimal algorithms for performing matrix-matrix addition, matrix-matrix multiplication, and BMMC (bit-matrix-multiply/complement) permutations using our API. Section 5 contains some concluding remarks. 2 Background Researchers have developed theoretical models to capture important features of data movement between main memory and secondary storage <ref> [AV88, Flo72, VS90, VS94] </ref>. In particular, Vitter and Shriver's Parallel Disk Model (PDM) [VS94] provides a reasonable model for the design of I/O-efficient algorithms and analysis at a feasible level of abstraction.
Reference: [VS94] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Algorithms for parallel memory I: Two-level memories. </title> <journal> Algorithmica, </journal> 12(2/3):110-147, August and Septem-ber 1994. 
Reference-contexts: models or input/output (I/O) complexity models, that aim to represent the key features of computer memory hierarchies and data movement in order to present a suitable model for algorithm design and analysis at a feasible level of abstraction. 1 The interested reader can find full descriptions of these models in <ref> [ACFS94, AV88, VS90, VS94] </ref>. Using the I/O complexity models, algorithm designers have developed out-of-core algorithms that choreograph data movements to utilize all of the disks in the I/O subsystem concurrently. We feel that this approach is a viable solution because of the asymptotic performance gains achieved by I/O-efficient algorithms. <p> Section 4 discusses the implemention of the I/O-optimal algorithms for performing matrix-matrix addition, matrix-matrix multiplication, and BMMC (bit-matrix-multiply/complement) permutations using our API. Section 5 contains some concluding remarks. 2 Background Researchers have developed theoretical models to capture important features of data movement between main memory and secondary storage <ref> [AV88, Flo72, VS90, VS94] </ref>. In particular, Vitter and Shriver's Parallel Disk Model (PDM) [VS94] provides a reasonable model for the design of I/O-efficient algorithms and analysis at a feasible level of abstraction. <p> Section 5 contains some concluding remarks. 2 Background Researchers have developed theoretical models to capture important features of data movement between main memory and secondary storage [AV88, Flo72, VS90, VS94]. In particular, Vitter and Shriver's Parallel Disk Model (PDM) <ref> [VS94] </ref> provides a reasonable model for the design of I/O-efficient algorithms and analysis at a feasible level of abstraction. A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus <p> In particular, Vitter and Shriver's Parallel Disk Model (PDM) [VS94] provides a reasonable model for the design of I/O-efficient algorithms and analysis at a feasible level of abstraction. A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting <ref> [AP94, Arg95, NV93, VS94] </ref>, general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. <p> In particular, Vitter and Shriver's Parallel Disk Model (PDM) <ref> [VS94] </ref> provides a reasonable model for the design of I/O-efficient algorithms and analysis at a feasible level of abstraction. A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting <ref> [VS94] </ref>, BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose <ref> [Cor92, Cor93, CSW94, VS94] </ref>, FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting <ref> [VS94] </ref>, BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> These last parameters are necessary for the routine to be called with submatrices as input. 15 4.2 Matrix-matrix multiplication There are several approaches to performing out-of-core matrix-matrix multiplication. These approaches include the standard recursive divide-and-conquer method presented in <ref> [VS94] </ref> and a method similar to the LU factorization algorithm presented in [WGWR93], which divides one of the matrices into groups of columns. We implement a variation of Vitter and Shriver's recursive out-of-core algorithm [VS94]. This algorithm is as follows: 1. If k M , multiply the matrices internally. <p> These approaches include the standard recursive divide-and-conquer method presented in <ref> [VS94] </ref> and a method similar to the LU factorization algorithm presented in [WGWR93], which divides one of the matrices into groups of columns. We implement a variation of Vitter and Shriver's recursive out-of-core algorithm [VS94]. This algorithm is as follows: 1. If k M , multiply the matrices internally. Otherwise do the following steps: 2. Subdivide A and B into eight k=2 fi k=2 submatrices: A 1;1 A 2;2 and B 1;1 -B 2;2 .
Reference: [WGWR93] <author> David Womble, David Greenberg, Stephen Wheat, and Rolf Riesen. </author> <title> Beyond core: Making parallel computer I/O practical. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 56-63, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition <ref> [WGWR93] </ref>, graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> These approaches include the standard recursive divide-and-conquer method presented in [VS94] and a method similar to the LU factorization algorithm presented in <ref> [WGWR93] </ref>, which divides one of the matrices into groups of columns. We implement a variation of Vitter and Shriver's recursive out-of-core algorithm [VS94]. This algorithm is as follows: 1. If k M , multiply the matrices internally. Otherwise do the following steps: 2.
Reference: [Wis95] <author> Leonard F. Wisniewski. </author> <title> Structured permuting in place on parallel disk systems. </title> <type> Technical Report PCS-TR95-265, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> September </month> <year> 1995. </year> <month> 26 </month>
Reference-contexts: A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations <ref> [Cor92, Cor93, CSW94, Wis95] </ref>, mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94], general permuting [VS94], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations <ref> [Cor92, Wis95] </ref>, matrix-matrix multiplication [VS94], matrix transpose [Cor92, Cor93, CSW94, VS94], FFT [VS94], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM.
References-found: 37

