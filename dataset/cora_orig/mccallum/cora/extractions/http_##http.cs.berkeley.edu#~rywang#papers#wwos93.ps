URL: http://http.cs.berkeley.edu/~rywang/papers/wwos93.ps
Refering-URL: http://http.cs.berkeley.edu/~rywang/papers/wwos93.html
Root-URL: 
Email: frywang,teag@cs.berkeley.edu  
Title: xFS: A Wide Area Mass Storage File System  
Author: Randolph Y. Wang and Thomas E. Anderson 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California  
Abstract: The current generation of file systems are inadequate in facing the new technological challenges of wide area networks and massive storage. xFS is a prototype file system we are developing to explore the issues brought about by these technological advances. xFS adapts many of the techniques used in the field of high performance multiprocessor design. It organizes hosts into a hierarchical structure so locality within clusters of workstations can be better exploited. By using an invalidation-based write back cache coherence protocol, xFS minimizes network usage. It exploits the file system naming structure to reduce cache coherence state. xFS also integrates different storage technologies in a uniform manner. Due to its intelligent use of local hosts and local storage, we expect xFS to achieve better performance and availability than current generation network file systems run in the wide area. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Baker and M. Sullivan. </author> <title> The Recovery Box: Using Fast Recovery to Provide High Availability in the Unix Environment. </title> <booktitle> USENIX Association Summer 1992 Conference Proceedings, </booktitle> <pages> pages 31-43, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: It has also been noted that such frequently updated and small amount of volatile state is a good candidate for inclusion in a stable memory that can survive machine crashes and the recovery cost can be kept to a minimum <ref> [1] </ref>. Another question we need to answer is how to deal with machines that do not respond to ownership revokes due to crashes or network partitions. There are three alternatives.
Reference: [2] <author> M. Blaze and R. Alonso. </author> <title> Toward Massive Distributed Systems. </title> <booktitle> Proceedings of the 3rd Workshop on Workstation Operating Systems, </booktitle> <pages> pages 48-51, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Some use manual migration and/or whole file migration. This is neither convenient nor efficient. Some offer ad hoc extensions to existing systems. This usually increases code complexity. While others have addressed some aspects of the problem <ref> [2, 5, 9, 11, 13] </ref>, there is yet no system that handles both WANs and mass storage. <p> Secondly, the hierarchical organization does a better job at taking advantage of potential locality within clusters of workstations. When a DASH processor references data cached in a local cluster, no remote communication is incurred. Similarly, one experiment <ref> [2] </ref> has shown that roughly two thirds of the files read by one workstation are already cached by neighboring clients. A study done by Dahlin [4] shows clients within a cluster can effectively serve each others' cache misses. <p> A number of recent studies have explored the idea of hierarchical organizations <ref> [2, 5] </ref>. One experiment [5] employed an intermediate data cache server and its hit rate was found to be surprisingly low.
Reference: [3] <author> S. Coleman and S. Miller. </author> <title> Mass Storage System Reference Model: Version 4. </title> <booktitle> IEEE Technical Committee on Mass Storage Systems and Technology, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Once the location of the desired data is discovered, data flows directly between the sender and the receiver. In a way, the separation of control messages 2 and data messages 3 in xFS is analogous to the approach taken in the Mass Storage Reference Model <ref> [3] </ref> where name services, file movement, and storage management all have distinct information flow paths. A number of recent studies have explored the idea of hierarchical organizations [2, 5]. One experiment [5] employed an intermediate data cache server and its hit rate was found to be surprisingly low.
Reference: [4] <author> M. Dahlin. </author> <title> Private Communication. </title> <year> 1993. </year>
Reference-contexts: When a DASH processor references data cached in a local cluster, no remote communication is incurred. Similarly, one experiment [2] has shown that roughly two thirds of the files read by one workstation are already cached by neighboring clients. A study done by Dahlin <ref> [4] </ref> shows clients within a cluster can effectively serve each others' cache misses. To exploit such locality, when an xFS client references data not present in its local cache, it first queries a consistency server on the cluster to see if any peer clients can supply the data.
Reference: [5] <author> D. Muntz and P. Honeyman. </author> <title> Multi-Level Caching in Distributed File System. </title> <booktitle> USENIX Association Winter 1992 Conference Proceedings, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: Some use manual migration and/or whole file migration. This is neither convenient nor efficient. Some offer ad hoc extensions to existing systems. This usually increases code complexity. While others have addressed some aspects of the problem <ref> [2, 5, 9, 11, 13] </ref>, there is yet no system that handles both WANs and mass storage. <p> A number of recent studies have explored the idea of hierarchical organizations <ref> [2, 5] </ref>. One experiment [5] employed an intermediate data cache server and its hit rate was found to be surprisingly low. <p> A number of recent studies have explored the idea of hierarchical organizations [2, 5]. One experiment <ref> [5] </ref> employed an intermediate data cache server and its hit rate was found to be surprisingly low.
Reference: [6] <author> J. Howard, M. Kazar, S. Menees, D. Nichols, M. Satyanarayanan, R. Sidebotham, and M. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-82, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Section 4 describes how xFS reduces the amount of cache coherence information maintained by hosts. Section 5 discusses the integration of multiple levels of storage. Section 6 discusses some additional issues including crash recovery. Section 7 concludes. 2 Hierarchical Client-server Or ganization Andrew <ref> [6] </ref>, NFS [17], and Sprite [20] all have a simple one-layer client-server hierarchy. <p> A client with local stable storage can store private data indefinitely. The ability of a client to operate more independently without server intervention should also result in better availability. NFS [17] writes through and offers little consistency guarantee for concurrent read write sharing. Andrew <ref> [6] </ref> provides a consistent view at file open time and writes back at close time. Sprite [14] enforces perfect consistency by disabling caching for write-shared files but still writes all dirty data to the server every 30 seconds for reliability reasons. <p> It may also choose to revoke the ownership of data it has previously granted to other hosts. A client that has the write ownership is obliged to transfer its dirty data back only upon receipt of such a revoking call. In existing stateful file systems <ref> [6, 14, 19] </ref>, an open involves requesting ownership; a close involves relinquishing ownership; and a callback is associated with revoking ownership. Many systems do not distinguish the notion of open and that of requesting ownership, or the notion of close and that of relinquishing ownership. <p> Fortunately, the Coda study has shown that such conflicts are extremely rare events and we do not expect ownership revokes to be frequent in xFS. xFS's approach of storing data locally and indefinitely also raises other concerns. Security is one. An-drew <ref> [6] </ref> treats servers as first class citizens and clients are deemed less trustworthy. In xFS, however, the line between clients and servers are blurred when clients are allowed to serve each other (section 2).
Reference: [7] <author> W. de Jonge, M. F. Kaashoek, and W. Hsieh. </author> <title> The Logical Disk: A New Approach to Improving File 7 System Performance. </title> <booktitle> Proceedings of the 14th Sym--posium on Operating Systems Principles, </booktitle> <month> Decem-ber </month> <year> 1993. </year> <note> To appear. </note>
Reference-contexts: This is similar to the approach taken by <ref> [7] </ref> where logical disk addresses are mapped to physical ones to allow a clean separation between file and disk management without sacrificing performance. The memory management analogy, unfortunately, does not apply for data layout.
Reference: [8] <author> B. Kaliski, Jr. </author> <title> The MD4 Message Digest Algorithm. </title> <booktitle> Workshop on the Theory and Application of Cryptographic Techniques Proceedings, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: In such an organization, the clusters can be treated as security domains where cluster members trust each other but foreign clusters that are not homes are deemed untrustworthy. A more paranoid approach would require the writer to compute a checksum. Kaliski <ref> [8] </ref> has devised an efficient algorithm which makes it computationally infeasible to find two messages with the same checksum or a message with a prespecified checksum. A reader, upon receipt of the data and the checksum, can run the same algorithm to verify that the data has not been altered.
Reference: [9] <author> J. Kohl and C. Staelin. HighLight: </author> <title> Using A Log-structured File System for Tertiary Storage Management. </title> <booktitle> USENIX Association Winter 1993 Conference Proceedings, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: Some use manual migration and/or whole file migration. This is neither convenient nor efficient. Some offer ad hoc extensions to existing systems. This usually increases code complexity. While others have addressed some aspects of the problem <ref> [2, 5, 9, 11, 13] </ref>, there is yet no system that handles both WANs and mass storage. <p> The second is how to lay out data on media in an efficient manner. To solve the problem of locating data in multiple levels of storage, some existing systems extend the UFS data structures to incorporate tertiary devices. For example, a block address in Highlight <ref> [9] </ref> can belong to the disk farm, some tertiary store, or the disk cache for tertiary storage. Given an hinode number, offseti pair, locating data involves locating the inode and indexing the inode. <p> Since many tertiary devices are append-only 6 and tertiary archival storage are 6 Most media deliver best performance when they are used as append-only devices. 5 mostly write-only, log-structured layout is a natural way of managing such devices. Highlight <ref> [9] </ref>, a descendant of LFS, bases its design on this observation. It employs a different cleaner process to migrate selected data blocks out to tertiary store, which is also written log-structured. When designing the xFS approach to locating data, we draw analogies from solutions to memory management problems.
Reference: [10] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In particular, xFS has the following features: * xFS organizes hosts into a more sophisticated hierarchical structure analogous to those found in some high performance multiprocessors such as DASH <ref> [10] </ref>. Requests are satisfied by a local cluster if possible to minimize remote communication. * xFS uses an invalidation-based write back cache coherence protocol pioneered by research done on multiprocessor caches. <p> In an attempt to solve these problems, xFS employs a hierarchy similar to those found in scalable shared memory multiprocessors (figure 2) 1 . To understand the motivation of such an organization, it is instructive to study the analogy between DASH <ref> [10] </ref> and xFS. The DASH system consists of a number of processor clusters. Communication among the clusters is provided by a mesh network. This is analogous to WAN links in xFS where bandwidth is limited and latency is high.
Reference: [11] <author> F. McClain. DataTree and UniTree: </author> <title> Software for File and Storage Management. </title> <booktitle> Digest of Papers, Proceedings of 10th IEEE Symposium on Mass Storage Systems, </booktitle> <pages> pages 126-128, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Some use manual migration and/or whole file migration. This is neither convenient nor efficient. Some offer ad hoc extensions to existing systems. This usually increases code complexity. While others have addressed some aspects of the problem <ref> [2, 5, 9, 11, 13] </ref>, there is yet no system that handles both WANs and mass storage.
Reference: [12] <author> J. Mogul. </author> <title> A Recovery Protocol for Spritely NFS. </title> <booktitle> Proceedings of the USENIX Workshop on File Systems, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Later when the server sends revoking calls to which the clients simply reply that they do not have it any more. A recovering server, however, has to recover the exact state it had before the crash. It appears that a server centric approach as done in <ref> [12] </ref> would work well. Under such a scheme, a recovering server directs its clients to help rebuild the lost server state.
Reference: [13] <author> J. Mott-Smith. </author> <title> The Jaquith Archive Server. </title> <type> UCB/CSD Report 92-701, </type> <institution> University of California, Berkeley, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Some use manual migration and/or whole file migration. This is neither convenient nor efficient. Some offer ad hoc extensions to existing systems. This usually increases code complexity. While others have addressed some aspects of the problem <ref> [2, 5, 9, 11, 13] </ref>, there is yet no system that handles both WANs and mass storage.
Reference: [14] <author> M. Nelson, B. Welch, and J. Ousterhout. </author> <title> Caching in the Sprite Network File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> Febru-ary </month> <year> 1988. </year>
Reference-contexts: The ability of a client to operate more independently without server intervention should also result in better availability. NFS [17] writes through and offers little consistency guarantee for concurrent read write sharing. Andrew [6] provides a consistent view at file open time and writes back at close time. Sprite <ref> [14] </ref> enforces perfect consistency by disabling caching for write-shared files but still writes all dirty data to the server every 30 seconds for reliability reasons. <p> It may also choose to revoke the ownership of data it has previously granted to other hosts. A client that has the write ownership is obliged to transfer its dirty data back only upon receipt of such a revoking call. In existing stateful file systems <ref> [6, 14, 19] </ref>, an open involves requesting ownership; a close involves relinquishing ownership; and a callback is associated with revoking ownership. Many systems do not distinguish the notion of open and that of requesting ownership, or the notion of close and that of relinquishing ownership. <p> Many systems do not distinguish the notion of open and that of requesting ownership, or the notion of close and that of relinquishing ownership. Even in a system that aggressively exploits caching such as Sprite <ref> [14] </ref>, a client merely passes the user system calls to the server (figure 3a).
Reference: [15] <author> M. Papamarcos and J. Patel. </author> <title> A Low Overhead Coherence Solution for Multiprocessors with Private Cache Memories. </title> <booktitle> Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 348-354, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: To reduce the number of bytes transferred over the wide area network, xFS uses a protocol based on multiprocessor style cache coherence <ref> [15] </ref>. An xFS host 4 requests ownership of a file or a directory from a higher level server. A client that possesses read ownership is allowed to cache data locally for reads 5 .
Reference: [16] <author> M. Rosenblum and J. Ousterhout. </author> <title> The Design and Implementation of a Log-Structured File System. </title> <journal> Operating Systems Review, </journal> <volume> 25(5) </volume> <pages> 1-15, </pages> <month> Octo-ber </month> <year> 1991. </year>
Reference-contexts: Such extensions usually complicate the code and force the new storage devices to inherit data structures that were originally designed for disks. A solution to the second problem, namely that of laying out data on media efficiently, is provided by the log-structured file system (LFS) <ref> [16] </ref>. LFS appends newly written data to a segmented log and it is optimized for write performance. As old data is deleted, LFS reclaims disk space by recopying sparsely populated segments to the tail of the log and marks those segments as clean.
Reference: [17] <author> R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, and B. Lyon. </author> <title> Design and Implementation of the Sun Network Filesystem. </title> <booktitle> Proceedings of the 1985 Summer USENIX Conference, </booktitle> <month> June </month> <year> 1985. </year>
Reference-contexts: Section 4 describes how xFS reduces the amount of cache coherence information maintained by hosts. Section 5 discusses the integration of multiple levels of storage. Section 6 discusses some additional issues including crash recovery. Section 7 concludes. 2 Hierarchical Client-server Or ganization Andrew [6], NFS <ref> [17] </ref>, and Sprite [20] all have a simple one-layer client-server hierarchy. <p> A client with local stable storage can store private data indefinitely. The ability of a client to operate more independently without server intervention should also result in better availability. NFS <ref> [17] </ref> writes through and offers little consistency guarantee for concurrent read write sharing. Andrew [6] provides a consistent view at file open time and writes back at close time.
Reference: [18] <author> M. Satyanarayanan, J. Kistler, P. Kumar, M. Okasaki, E. Siegel, and D. Steere. Coda: </author> <title> A Highly Available File System for a Distributed Workstation Environment. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 39(4) </volume> <pages> 447-459, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Clients with local stable storage can store private data indefinitely, consuming less wide area bandwidth with lower latency than if all modifications were written through to the server. For data cached locally, clients can operate on them without server attention. The Coda <ref> [18] </ref> study illustrates that this should offer better availability in case of server or network failure. * xFS exploits the file system naming structure to reduce the state needed to preserve cache coherence. Ownership of a directory and its descendents can be granted as a whole. <p> The demise of a single server or a failed network link usually renders the clients helpless. xFS hosts, on the other hand, after acquiring ownership of the proper data, can operate more independently. Furthermore, xFS hosts will have an easier time rein-tegrating after disconnection. A Coda <ref> [18] </ref> client, for example, is required to write the dirty data it has accumulated during disconnection back to the server during reintegration. In xFS, such writes are not necessary unless the dirty data is needed by others. There are a number of problems with this cache coherence scheme. <p> We adopt the third approach taken by Coda <ref> [18] </ref> which trades quality for availability. Fortunately, the Coda study has shown that such conflicts are extremely rare events and we do not expect ownership revokes to be frequent in xFS. xFS's approach of storing data locally and indefinitely also raises other concerns. Security is one.
Reference: [19] <author> V. Srinivasan and J. Mogul. Spritely NFS: </author> <title> Experience with Cache Consistency Protocols. </title> <booktitle> Proceedings of 12th Symposium on Operating Systems Principles, </booktitle> <pages> pages 45-57, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Furthermore, none of these file systems does effective directory caching; consequently, even after optimizing write back policies, there are still a large number of name related operations left <ref> [19] </ref>. Such unnecessary use of network traffic might have posed little problem on a LAN but will become a performance bottleneck in a wide area. <p> It may also choose to revoke the ownership of data it has previously granted to other hosts. A client that has the write ownership is obliged to transfer its dirty data back only upon receipt of such a revoking call. In existing stateful file systems <ref> [6, 14, 19] </ref>, an open involves requesting ownership; a close involves relinquishing ownership; and a callback is associated with revoking ownership. Many systems do not distinguish the notion of open and that of requesting ownership, or the notion of close and that of relinquishing ownership. <p> An xFS server needs to remember what ownerships it has granted to which clients. Even after applying the state compression techniques described in section 4, the amount of information would still be too large to keep entirely in memory as Sprite [20] and Spritely NFS <ref> [19] </ref> do. Part of it needs to be written to stable storage. On the other hand, we cannot afford to log every ownership change to disk. Consequently, we need to recover the memory resident information lost in a crash. The task is relatively simple for clients.
Reference: [20] <author> B. Welch. </author> <title> The Sprite Distributed File System. </title> <type> PhD Thesis, </type> <institution> Department of Electrical Engineering and Computer Science, University of California, Berke-ley, </institution> <month> March </month> <year> 1990. </year> <month> 8 </month>
Reference-contexts: Section 4 describes how xFS reduces the amount of cache coherence information maintained by hosts. Section 5 discusses the integration of multiple levels of storage. Section 6 discusses some additional issues including crash recovery. Section 7 concludes. 2 Hierarchical Client-server Or ganization Andrew [6], NFS [17], and Sprite <ref> [20] </ref> all have a simple one-layer client-server hierarchy. <p> An xFS server needs to remember what ownerships it has granted to which clients. Even after applying the state compression techniques described in section 4, the amount of information would still be too large to keep entirely in memory as Sprite <ref> [20] </ref> and Spritely NFS [19] do. Part of it needs to be written to stable storage. On the other hand, we cannot afford to log every ownership change to disk. Consequently, we need to recover the memory resident information lost in a crash. The task is relatively simple for clients.
References-found: 20

