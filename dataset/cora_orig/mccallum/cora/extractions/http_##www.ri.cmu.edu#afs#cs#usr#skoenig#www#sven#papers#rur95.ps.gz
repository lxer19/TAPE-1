URL: http://www.ri.cmu.edu/afs/cs/usr/skoenig/www/sven/papers/rur95.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs/usr/skoenig/www/sven/abstracts/rur95.html
Root-URL: 
Email: fskoenig,rich,reidsg@cs.cmu.edu  
Title: Robot Navigation with Markov Models: A Framework for Path Planning and Learning with Limited Computational Resources  
Author: Sven Koenig, Richard Goodwin, Reid G. Simmons 
Address: Pittsburgh, PA 15213-3890, USA  
Affiliation: Carnegie Mellon University,  
Abstract: Navigation methods for mobile robots need to take various sources of uncertainty into account in order to get robust performance. The ability to improve performance with experience and to adapt to new circumstances is equally important for long-term operation. Real-time constraints, limited computation and memory, as well as the cost of collecting training data also need to be accounted for. In this paper, we discuss our evolving architecture for mobile robot navigation that we use as a test-bed for evaluating methods for dealing with uncertainty in the face of real-time constraints and limited computational resources. The architecture is based on POMDP models that explicitly represent actuator uncertainty, sensor uncertainty, and approximate knowledge of the environment (such as uncertain metric information). Using this model, the robot is able to track its likely location as it navigates through a building. Here, we discuss additions to the architecture: a learning component that allows the robot to improve the POMDP model from experience, and a decision-theoretic path planner that takes into account the expected performance of the robot as well as probabilistic information about the state of the world. A key aspect of both additions is the efficient allocation of computational resources and their practical application to real-world robots.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A.R. Cassandra, </author> <title> L.P. Kaelbling, and M.L. Littman. Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> pages 1023-1028, </pages> <year> 1994. </year>
Reference-contexts: At present, however, it is infeasible to determine optimal POMDP solutions given our real-time constraints and the size of our state spaces (over 3000 states for one floor of our building) <ref> [6, 1] </ref>. Instead, our planner currently associates a directive d (s) 2 D with each Markov state.
Reference: 2. <author> R. Goodwin. </author> <title> Reasoning about when to start acting. </title> <booktitle> In Proceedings of the Second International Conference on Artificial Intelligence Planning Systems (AIPS-94), </booktitle> <pages> pages 86-91, </pages> <year> 1994. </year>
Reference-contexts: In making this decision, our meta-level controller also takes into account the fact that the robot can continue to optimize its plan and plan for more contingencies after it begins to execute a plan, see <ref> [2] </ref> for details. To decide which contingencies to plan for, the planner uses a simplified sensitivity analysis to identify the contingencies to which the range of expected plan execution times is most sensitive.
Reference: 3. <author> B. Hannaford and P. Lee. </author> <title> Hidden Markov model analysis of force/torque information in telemanipulation. </title> <journal> International Journal of Robotics Research, </journal> <volume> 10(5) </volume> <pages> 528-539, </pages> <month> 5 </month> <year> 1991. </year>
Reference-contexts: It uses a variant of the Baum-Welch algorithm, an expectation-maximization algorithm for learning partially observable Markov models from observations. The Baum-Welch algorithm is best known for its application to speech recognition, but it has also been applied in robotics, for example to interpret tele-operation commands <ref> [3, 12] </ref>. The Baum-Welch algorithm assumes that there is only one action available in each state.
Reference: 4. <author> S. Koenig and R.G. Simmons. </author> <title> Unsupervised learning of probabilistic models for robot navigation. </title> <booktitle> In Proceedings of the International Conference on Robotics and Automation (ICRA-96), </booktitle> <year> 1996. </year>
Reference-contexts: Our experience shows that the Baum-Welch algorithm can learn extremely good models of the environment with a small number of corridor traversals. Even with only one traversal of each corridor, the learned POMDPs are still very good, despite Xavier's noisy sensors. (Experimental results are reported in <ref> [4] </ref>.) 5 Decision-Theoretic Planning The use of Markov models for position estimation and action execution suggests using POMDP algorithms on the same models for goal-directed path planning.
Reference: 5. <author> M.L. Littman, A.R. Cassandra, </author> <title> and L.P. Kaelbling. Learning policies for partially observable environments: Scaling up. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning (ML-95), </booktitle> <pages> pages 362-370, </pages> <year> 1995. </year>
Reference-contexts: The action selection component chooses new directives greedily by selecting the directive with the highest total probability mass: arg max X p (s) : (4) This directive can be calculated very efficiently, making the action selection component very reactive. Recent advances in approximate algorithms for solving POMDP problems <ref> [5, 8] </ref> suggest that it might eventually become feasible to use them for path planning. The promise of such methods is that they can plan for information-gathering actions and that they allow the amount of computation to be traded off against the quality of the resulting plan.
Reference: 6. <author> W.S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28(1) </volume> <pages> 47-65, </pages> <year> 1991. </year>
Reference-contexts: At present, however, it is infeasible to determine optimal POMDP solutions given our real-time constraints and the size of our state spaces (over 3000 states for one floor of our building) <ref> [6, 1] </ref>. Instead, our planner currently associates a directive d (s) 2 D with each Markov state.
Reference: 7. <author> I. Nourbakhsh, R. Powers, and S. Birchfield. Dervish: </author> <title> An office-navigating robot. </title> <journal> AI Magazine, </journal> <volume> 16(2) </volume> <pages> 53-60, </pages> <year> 1995. </year>
Reference-contexts: Each forward transition after that is deterministic (modulo dead-reckoning uncertainty). Fig. 4. Markov model for a corridor of length 2 to 4 meters Doorways are modeled with a single exact-length Markov chain that leads through a door into a room. Similarly to <ref> [7] </ref>, doorways have an associated probability p that the door is open.
Reference: 8. <author> R. Parr and S. Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pages 1088-1094, </pages> <year> 1995. </year>
Reference-contexts: The action selection component chooses new directives greedily by selecting the directive with the highest total probability mass: arg max X p (s) : (4) This directive can be calculated very efficiently, making the action selection component very reactive. Recent advances in approximate algorithms for solving POMDP problems <ref> [5, 8] </ref> suggest that it might eventually become feasible to use them for path planning. The promise of such methods is that they can plan for information-gathering actions and that they allow the amount of computation to be traded off against the quality of the resulting plan.
Reference: 9. <author> L.R. Rabiner. </author> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-16, </pages> <month> 1 </month> <year> 1986. </year>
Reference-contexts: This two-step hill-climbing process can then be repeated with the same trace and the new Markov model. This process eventually converges to a Markov model which locally, but not necessarily globally, best fits the trace. See <ref> [9] </ref> for details. The Baum-Welch algorithm can be easily extended to account for the fact that the robot can choose from several actions in every state. It also has the property that the improved POMDPs always conform to the original topological map.
Reference: 10. <author> R. Simmons. </author> <title> Becoming increasingly reliable. </title> <booktitle> In Proceedings of the Second International Conference on Artificial Intelligence Planning Systems (AIPS-94), </booktitle> <pages> pages 152-157, </pages> <year> 1994. </year>
Reference-contexts: To decide how closely to approximate a solution, we use methods ranging from simple rules compiled into the planner to more sophisticated on-line strategies that monitor and direct the planning activities. The initial version of our planner was adapted from the planner used in our landmark-based navigation system <ref> [10] </ref>. It uses A* search in a topological map that is augmented with approximate metric information (an abstraction of the POMDP) to find a shortest path to the goal. The resulting path is used to assign preferred headings to the edges and nodes in the topological map.
Reference: 11. <author> R. Simmons and S. Koenig. </author> <title> Probabilistic robot navigation in partially observable environments. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pages 1080-1087, </pages> <year> 1995. </year>
Reference-contexts: In addition to these components, a model-learning component improves the initial POMDP, that was generated by the POMDP compiler from a topological map, initial action and sensor models, and uncertain knowledge about corridor distances and door states (open or closed). In <ref> [11] </ref>, we reported on the position estimation and action selection components. Here we describe the learning and planning components. Fig. 2. <p> For example, we use more than one group of Markov states to encode positional uncertainty in corridor junctions and are able to model foyers and rooms. See <ref> [11] </ref> for a more comprehensive discussion. 4 Model Learning The task of the model-learning component is to improve the initial distance estimates and to learn better action and sensor models.
Reference: 12. <author> J. Yang, Y. Xu, and C.S. Chen. </author> <title> Hidden Markov model approach to skill learning and its application to telerobotics. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10(5) </volume> <pages> 621-631, </pages> <month> 10 </month> <year> 1994. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: It uses a variant of the Baum-Welch algorithm, an expectation-maximization algorithm for learning partially observable Markov models from observations. The Baum-Welch algorithm is best known for its application to speech recognition, but it has also been applied in robotics, for example to interpret tele-operation commands <ref> [3, 12] </ref>. The Baum-Welch algorithm assumes that there is only one action available in each state.
References-found: 12

