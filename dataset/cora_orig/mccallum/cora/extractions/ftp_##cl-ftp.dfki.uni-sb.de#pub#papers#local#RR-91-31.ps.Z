URL: ftp://cl-ftp.dfki.uni-sb.de/pub/papers/local/RR-91-31.ps.Z
Refering-URL: http://cl-www.dfki.uni-sb.de/cl/papers/cl-abstracts.html
Root-URL: 
Phone: Tel.: 49 (631) 205-3211 Stuhlsatzenhausweg 3  Tel.: 49 (681) 302-5252  
Title: Feature-Based Inheritance Networks for Computational Lexicons  
Author: f ur K unstliche Hans-Ulrich Krieger and John Nerbonne f ur K unstliche Intelligenz 
Address: Postfach 20 80 67608 Kaiserslautern, FRG  66123 Saarbrucken, FRG  
Note: Deutsches Forschungszentrum  
Date: September 1991  
Affiliation: Deutsches Forschungszentrum  Intelligenz GmbH Research  GmbH  
Pubnum: Report RR-92-31  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J.H. Allen and H.B.Greenough. </author> <title> New Latin Grammar. </title> <publisher> Ginn and Company, </publisher> <address> Boston, </address> <year> 1903. </year>
Reference-contexts: Section 4), nor do we wish to advocate the inflectional analysis at this time. 12 But there are clearer examples of languages with passives which are paradigm elements, e.g. Latin (cf. Allen and Greenough <ref> [1] </ref>), and our primary goal is to establish that this (traditional) analysis is formulatable in a feature-based lexicon.
Reference: [2] <editor> Stephen R. Anderson. Inflection. In Michael Hammond and Michael Noonan, editors, </editor> <booktitle> Theoretical Morphology, </booktitle> <pages> pages 23-43. </pages> <publisher> Academic Press, </publisher> <address> Orlando, </address> <year> 1988. </year>
Reference-contexts: And it turns out that the wellformedness of forms with the suffix -bar cannot be predicted only on the basis of the component morphemes. We find patterns such as the following: 4 Cf. Matthews [35] pp.20ff for a much more thorough defense of this material; cf. Anderson <ref> [2] </ref>, [3] and Zwicky [49], [50] for more recent defenses of the lexeme-based view. 5 A great deal of what we analyze below may be found in a very thorough study of this process, Jindrich Toman's Wortbildung [47]. 4 morpheme lexeme 1 lexeme 2 meid- *meidbar vermeidbar `avoidable' lad- *ladbar aufladbar <p> Anderson <ref> [2] </ref>, p.147). It is our goal here to show how morphotactics may be subsumed into the lexicon, and we shall discuss this at lenghth in Sections 3 and 4 below. But we shall deliberately have very little to say about morphophonemics, which we do not intend to treat here.
Reference: [3] <author> Stephen R. Anderson. </author> <title> Morphological theory. </title> <editor> In Frederick J. Newmeyer, editor, </editor> <booktitle> Linguistics: The Cambridge Survey, </booktitle> <volume> volume 1, </volume> <pages> pages 146-91. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: And it turns out that the wellformedness of forms with the suffix -bar cannot be predicted only on the basis of the component morphemes. We find patterns such as the following: 4 Cf. Matthews [35] pp.20ff for a much more thorough defense of this material; cf. Anderson [2], <ref> [3] </ref> and Zwicky [49], [50] for more recent defenses of the lexeme-based view. 5 A great deal of what we analyze below may be found in a very thorough study of this process, Jindrich Toman's Wortbildung [47]. 4 morpheme lexeme 1 lexeme 2 meid- *meidbar vermeidbar `avoidable' lad- *ladbar aufladbar `loadable'
Reference: [4] <author> Emmon Bach. </author> <title> Categorial grammars as theories of language. </title> <editor> In Richard T. Oehrle, Emmon Bach, and Deirdre Wheeler, editors, </editor> <booktitle> Categorial Grammars and Natural Language Structures, </booktitle> <pages> pages 17-34. </pages> <publisher> Reidel, </publisher> <address> Dordrecht and Boston, </address> <year> 1988. </year>
Reference-contexts: In HPSG the type sign has an attribute SYNTAXjLOCALjSUBCAT which is restricted in value to lists of signs. This attribute encodes subcategorization information, which is lexically based in HPSG, much as it is in Categorial Grammar (Bach, <ref> [4] </ref>). Grammatical heads specify the syntactic and semantic restrictions they impose on their complements and adjuncts. For example, verbs and verb phrases bear a feature SUBCAT whose content is a (perhaps ordered) set of feature structures representing their unsatisfied subcategorization requirements.
Reference: [5] <author> Rolf Backofen, Lutz Euler, and Gunter Gorz. </author> <title> Towards the integration of functions, relations and types in an AI programming language. </title> <booktitle> In Proceedings of GWAI-90, </booktitle> <address> Berlin, 1990. </address> <publisher> Springer. </publisher>
Reference-contexts: These are descriptions of objects whose value is included in one of the disjuncts, i.e., it is FIRST or THIRD: fi AGRjPER fFIRST, THIRDg fl In order to link particular choices with formal elements, we make extensive use of distributed disjunctions, investigated by Backofen, Euler and Gorz <ref> [5] </ref> and Dorre and Eisele [14]. This technique was developed because it (normally) allows more efficient processing of disjunctions, since it obviates the need to expand them to disjunctive normal form. <p> Here is an example of two AVM descriptions which are compatible, and a further example of two which are not: 8 Cf. Backofen et al. <ref> [5] </ref> for a discussion of a third advantage of distributed disjunctions, namely a normal increase in processing efficiency. 7 fi fi PER THIRD fl fl fi fi fl fl PER THIRD NUM SG PER THIRD NUM PL u AGR NUM SG = ? (Note the incompatible specification of the value at
Reference: [6] <author> Steven Bird. </author> <title> Prosodic morphology and constraint-based phonology. </title> <institution> Research Paper EUCCSRP-38, Centre for Cognitive Science, University of Edinburgh, </institution> <year> 1990. </year>
Reference-contexts: Cf. Bird <ref> [6] </ref>. For efficiency reasons, however, a second path might be chosen, viz., the employment of a hybrid feature-based two-level morphology. This has efficiency advantages, and it is our intention to pursue this line. Cf. Trost [48].
Reference: [7] <author> Gosse Bouma. </author> <title> Defaults in unification grammar. </title> <booktitle> In Proceedings of the 28th Annual Meeting of the ACL, </booktitle> <pages> pages 165-172. </pages> <institution> Association for Computational Linguistics, </institution> <year> 1990. </year>
Reference-contexts: This has seemed suspicious within the context of feature systems because these were developed (in part) to allow monotonic processing of linguistic information, and the use of defaults leads to nonmonotonicity. 2 But, as Bouma <ref> [7] </ref>, p.169 points out, the use of lexical defaults is a fairly harmless form of nonmonotoncity, since the lexicon is nonmonotonic only with respect to lexical development|the syntactic use of information specified via lexical default leads to none of the problems associated with nonmonotonic reasoning; e.g., inferences about phrases never need <p> Cook, Hill and Canning [13]). The mechanism we shall employ for the default combination of lexical information is the default unification developed by Bouma <ref> [7] </ref>; we may employ this within the lexicon, even while eschewing its use for parsing and generation. The present work is closest to Pollard and Sag's in that it proceeds from a view of the lexicon in which feature structures bear the burden of linguistic description. <p> The semantics of (19) (the entry which was built regularly) is modified by using overwriting, default unification or other nonmonotonic mechanisms to enforce the standard reading. In this case, ebar (21) belongs to the class bar-comp-A, because all other properties remain the same. We would follow Bouma <ref> [7] </ref> in the use of default unification (as a basis for default inheritance). ebar stand 2 6 ?-A ^ :bar-comp-A SEM RELN safely-eat 0 SOURCE : : : MORPHS ??? 3 7 26 ebar stand 0 ebar nonstand RELN safely-eat 0 SOURCE : : : 2 6 bar-comp-A SEM 1 RELN
Reference: [8] <author> Joan Bresnan, </author> <title> editor. The Mental Representation of Grammatical Relations. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1982. </year>
Reference-contexts: Pollard and Sag (Chap.8.2) furthermore suggest a use of lexical rules which brings their work closer to standard linguistic views (e.g., LFG, <ref> [8] </ref>). <p> Our proposed construal of lexical rules as feature structures obviates any use of lexical rules as operations|also familiar from LFG (Bresnan, <ref> [8] </ref>) and PATR-II (Shieber et al., [44]), especially D-PATR (Karttunen, [26]). 3 We believe therefore that the present paper is a contribution to feature-based theories, as well, in that it shows how lexical rules can be construed in terms of feature-structure. <p> The same is true, if we move to other theories: f-structures differ in form (syntax) and interpretation (semantics) from lexical rules stated in LFG (cf. the articles in Bresnan, <ref> [8] </ref>). This same observation holds for HPSG [36], Ch. 8.2, 21 for the Alvey tools project [38], for the early days of HPSG [19], for the work of Flickinger [17], and also for Hoeksema's Categorial Morphology [23].
Reference: [9] <author> Jonathan Calder. </author> <title> Paradigmatic morphology. </title> <booktitle> In Proceedings of the 5th Annual Meeting of the European Association for Computational Linguistics, </booktitle> <pages> pages 58-65, </pages> <year> 1989. </year>
Reference-contexts: Flickinger, [17], pp.107-110), in the Alvey tools project (cf. Ritchie et al., [38], p.298), and in HPSG (cf. Pollard and Sag, [36], pp.209-213). Paradigmatic morphology improves upon these ideas by defining the paradigm as a sequence of lexical rules on which subsumption relations can be defined (cf. Calder, <ref> [9] </ref>), but the fundamental analytical tool is still the lexical rule. In this view, the inflectional paradigm above is described by postulating rules which relate one paradigm element to another (or relating it to another form), including perhaps a rule to derive first singular forms from infinitives. <p> One possible dimension of classification is, for instance, the distinction between procedural vs. declarative formulation of rules relating phonological, morphological and orthographic phenomena (cf. Calder, <ref> [9] </ref>). 19 Classifying specific treatments is subjective. Most linguists will interpret lexical rules procedurally.
Reference: [10] <author> Luca Cardelli and Peter Wegner. </author> <title> On understanding types, data abstraction, and polymorphism. </title> <journal> ACM Computing Surveys, </journal> <volume> 17(4) </volume> <pages> 471-522, </pages> <year> 1985. </year>
Reference-contexts: Thus, in contrast to the version of order-constituents in [36], our version of order-constituent is overloaded with respect to its argument|we employ an ad hoc polymorphism. Cf. Cardelli & Wegner <ref> [10] </ref>. Alternatively, we could specify a second function, order-morph-constituents. 21 Although Pollard & Sag [36] strictly type the attributes of feature structures in general, they do not explicitly state that principles as well as rules may also be regarded as types.
Reference: [11] <author> Bob Carpenter. </author> <title> The Logic of Typed Feature Structures. </title> <booktitle> Tracts in Theoretical Computer Science. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <note> to appear, </note> <year> 1992. </year>
Reference-contexts: Feature description languages thus provide a natural formalization for work in structured lexicons. A final aspect of modern feature theories that we shall have cause to exploit is their use of typing (cf. Carpenter, <ref> [11] </ref> for a presentation). A type system imposed on a system of feature structures has several tasks: first, provides a means of referring to classes of feature structures of a given restricted sort. We shall put this to good use, e.g., in representing derivational relationships as complex inheritance.
Reference: [12] <author> Bob Carpenter, Carl J. Pollard, and Alex Franz. </author> <title> The specification and implementation of constraint-based unification grammar. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1991. </year>
Reference-contexts: These assumptions lead us to postulate the following structure for -bar: 2 6 6 6 6 bar-suff MORPHjFORM "bar" SYNjLOC 2 LEX HEADjMAJ A SUBCAT bar-V 1 3 SEM OPERATOR 3 SCOPE 1 7 7 7 7 5 31 Under the assumption of Carpenter's "total well-typing" <ref> [12] </ref>, it may be useful to drop the attribute LEX in (15).
Reference: [13] <author> William Cook, Walt Hill, and Peter Canning. </author> <title> Inheritance is not subtyping. </title> <booktitle> In ACM SIGACTSIGPLAN Symposium on Principles of Programming Languages (POPL), </booktitle> <year> 1990. </year> <month> 34 </month>
Reference-contexts: Care needs to be taken that the two notions of hierarchy|the classes involved in the default inheritance relationship and the feature structure types defined there|not be confused (cf. Cook, Hill and Canning <ref> [13] </ref>). The mechanism we shall employ for the default combination of lexical information is the default unification developed by Bouma [7]; we may employ this within the lexicon, even while eschewing its use for parsing and generation.
Reference: [14] <author> Jochen Dorre and Andreas Eisele. </author> <title> Determining consistency of feature terms with distributed disjunctions. </title> <editor> In Dieter Metzing, editor, </editor> <booktitle> Proceedings of GWAI-89 (15th German Workshop on AI), </booktitle> <pages> pages 270-279, </pages> <address> Berlin, 1989. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: descriptions of objects whose value is included in one of the disjuncts, i.e., it is FIRST or THIRD: fi AGRjPER fFIRST, THIRDg fl In order to link particular choices with formal elements, we make extensive use of distributed disjunctions, investigated by Backofen, Euler and Gorz [5] and Dorre and Eisele <ref> [14] </ref>. This technique was developed because it (normally) allows more efficient processing of disjunctions, since it obviates the need to expand them to disjunctive normal form.
Reference: [15] <author> Jochen Dorre and Andreas Eisele. </author> <title> A comprehensive unification-based grammar formalism. </title> <type> Technical Report Deliverable R3.1.B, </type> <institution> DYANA, Centre for Cognitive Science, Edinburgh, </institution> <year> 1991. </year>
Reference-contexts: But when the instantiation of a word is done, all ANYs have to be removed (must be unified `out'), because ANY henceforth behaves like ?. (Dorre & Eisele, <ref> [15] </ref>, pp. 18, give a formalization of ANY in terms of so-called meta-constraints.) There will be a third possibility of repair, if the underlying (feature) logic allows us to state that certain (under- specified) types (classes) cannot be instantiated.
Reference: [16] <author> Roger Evans and Gerald Gazdar. </author> <title> The DATR papers. </title> <type> Technical Report SCRP 139, </type> <institution> School of Cognitive and Computing Sciences, University of Sussex, </institution> <year> 1990. </year>
Reference-contexts: 1 Introduction The best inheritance mechanisms for representing lexical information have been Flickinger, Pollard and Wasow's [19] work on "structured lexicons", and Evans and Gazdar's <ref> [16] </ref> work on DATR. y We thank Rolf Backofen, Stephan Busemann, Bob Carpenter, Bob Kasper, Andreas Kathol, Klaus Netter, Carl Pollard and Harald Trost for conversations about this work. <p> This brings us to the second point of connection between feature-based theories and lexicon theory. The work on "structured lexicons" cited in the introduction by Flickinger et al [19], Flickinger [17] and Evans and Gazdar <ref> [16] </ref> emphasized the value of lexicons in which specifications were as free as possible of redundancy, and these works eliminated redundancy by exploiting a 8 relation of inheritance between lexical classes, realized as a relation between nodes in a directed graph (inheritance hierarchy). <p> It is, however, compatible with various lexical structures, as is demonstrated by its use in the Alvey project, noted above [38]. The direct characterization of the paradigm has been the alternative approach both in linguistics (cf. Matthews, [34], Chap.IV) and in computational linguistics (cf. Evans and Gazdar's DATR, <ref> [16] </ref>, and Russell et al.'s ELU lexicon, [40]). The fundamental idea in our characterization is due to the work in DATR, in which paradigms are treated as alternative further specifications of abstract lexemes. <p> There are two similar proposals in inheritance- based, computational lexicons for analysing inflectional variation as the further specification of an abstract lexeme. We discuss these in the present section. DATR (cf. Evans and Gazdar <ref> [16] </ref>) is a graph description language (and inference engine) which can encode information about lexical items. <p> The use of default specifications thus obtains the same advantages in derivation that Flickinger et al. [19] and Evans & Gazdar <ref> [16] </ref> have shown in word-class definitions. Defaults, together with the possibility of overwriting defaults in more specific definitions may turn out to be even more important in connection with the analysis of derivational relationships, since these are notoriously irregular in morphological form, syntactic feature assignment, and semantics (cf.
Reference: [17] <author> Daniel Flickinger. </author> <title> Lexical Rules in the Hierarchical Lexicon. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1987. </year>
Reference-contexts: This brings us to the second point of connection between feature-based theories and lexicon theory. The work on "structured lexicons" cited in the introduction by Flickinger et al [19], Flickinger <ref> [17] </ref> and Evans and Gazdar [16] emphasized the value of lexicons in which specifications were as free as possible of redundancy, and these works eliminated redundancy by exploiting a 8 relation of inheritance between lexical classes, realized as a relation between nodes in a directed graph (inheritance hierarchy). <p> The deployment of lexical rules may be found in Flickinger's approach (cf. Flickinger, <ref> [17] </ref>, pp.107-110), in the Alvey tools project (cf. Ritchie et al., [38], p.298), and in HPSG (cf. Pollard and Sag, [36], pp.209-213). Paradigmatic morphology improves upon these ideas by defining the paradigm as a sequence of lexical rules on which subsumption relations can be defined (cf. <p> One advantage of representing paradigms in this fashion|as opposed to representations via lexical rules, as in Flickinger <ref> [17] </ref> or in the other rule-based approaches cited above|is that the paradigm is represented in the same formalism in which word classes and lexemes are represented. A paradigm may thus participate in the same inheritance relationships that relate word classes and individual lexemes. <p> This same observation holds for HPSG [36], Ch. 8.2, 21 for the Alvey tools project [38], for the early days of HPSG [19], for the work of Flickinger <ref> [17] </ref>, and also for Hoeksema's Categorial Morphology [23]. By its nature, an external lexical rule sets up a relation between two lexemes (or classes of lexemes)|or, in the case of feature-based theories, between two feature structure descriptions.
Reference: [18] <author> Daniel Flickinger and John Nerbonne. </author> <title> Inheritance and complementation: A case study of easy adjectives and related nouns. </title> <journal> Computational Linguistics, </journal> <volume> 18, </volume> <year> 1991. </year>
Reference-contexts: We shall employ default inheritance regularly, perhaps most crucially in the specification of derivational relations (cf. below and cf. Flickinger et al. [19]; and Gazdar [20], [21]; and Flickinger and Nerbonne <ref> [18] </ref> for arguments supporting the use of defaults in lexical specifications). <p> In this case, the SOURCE role contains the semantics of an underspecified NP, and the proposition within which it occurs holds whenever there is some value for the NP semantics for which the proposition holds (cf. Flickinger and Nerbonne <ref> [18] </ref> for a similar treatment of the semantics of the for phrase licensed by easy adjectives and Karttunen [26] for an analysis of the semantics of the passive by phrase along these lines).
Reference: [19] <author> Daniel Flickinger, Carl Pollard, and Thomas Wasow. </author> <title> Structure-sharing in lexical represen-tation. </title> <booktitle> In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1985. </year>
Reference-contexts: 1 Introduction The best inheritance mechanisms for representing lexical information have been Flickinger, Pollard and Wasow's <ref> [19] </ref> work on "structured lexicons", and Evans and Gazdar's [16] work on DATR. y We thank Rolf Backofen, Stephan Busemann, Bob Carpenter, Bob Kasper, Andreas Kathol, Klaus Netter, Carl Pollard and Harald Trost for conversations about this work. <p> In a system with default inheritance, these may be regarded not as anomalous, but rather as imperfectly regular, or regular within limits. We shall employ default inheritance regularly, perhaps most crucially in the specification of derivational relations (cf. below and cf. Flickinger et al. <ref> [19] </ref>; and Gazdar [20], [21]; and Flickinger and Nerbonne [18] for arguments supporting the use of defaults in lexical specifications). <p> This brings us to the second point of connection between feature-based theories and lexicon theory. The work on "structured lexicons" cited in the introduction by Flickinger et al <ref> [19] </ref>, Flickinger [17] and Evans and Gazdar [16] emphasized the value of lexicons in which specifications were as free as possible of redundancy, and these works eliminated redundancy by exploiting a 8 relation of inheritance between lexical classes, realized as a relation between nodes in a directed graph (inheritance hierarchy). <p> DATR (cf. Evans and Gazdar [16]) is a graph description language (and inference engine) which can encode information about lexical items. DATR provides a formally clean version of the sort of default inheritance first advocated in de Smedt [45] and Flickinger et al. <ref> [19] </ref>, and as such represents a significant advance in the understanding of default lexical inheritance. <p> This same observation holds for HPSG [36], Ch. 8.2, 21 for the Alvey tools project [38], for the early days of HPSG <ref> [19] </ref>, for the work of Flickinger [17], and also for Hoeksema's Categorial Morphology [23]. By its nature, an external lexical rule sets up a relation between two lexemes (or classes of lexemes)|or, in the case of feature-based theories, between two feature structure descriptions. <p> The use of default specifications thus obtains the same advantages in derivation that Flickinger et al. <ref> [19] </ref> and Evans & Gazdar [16] have shown in word-class definitions.
Reference: [20] <author> Gerald Gazdar. </author> <title> Linguistic applications of default inheritance mechanisms. </title> <editor> In Peter Whitelock, Mary McGee Wood, Harald L. Somers, Rod Johnson, and P.Bennett, editors, </editor> <booktitle> Linguistic Theory and Computer Applications. </booktitle> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1987. </year>
Reference-contexts: In a system with default inheritance, these may be regarded not as anomalous, but rather as imperfectly regular, or regular within limits. We shall employ default inheritance regularly, perhaps most crucially in the specification of derivational relations (cf. below and cf. Flickinger et al. [19]; and Gazdar <ref> [20] </ref>, [21]; and Flickinger and Nerbonne [18] for arguments supporting the use of defaults in lexical specifications).
Reference: [21] <author> Gerald Gazdar. </author> <title> An introduction to DATR. </title> <editor> In Roger Evans and Gerald Gazdar, editors, </editor> <booktitle> The DATR Papers, number SCRP 139 in Cognitive Science Research Reports, </booktitle> <pages> pages 1-14. </pages> <institution> School of Cognitive and Computing Sciences, University of Sussex, </institution> <year> 1990. </year>
Reference-contexts: In a system with default inheritance, these may be regarded not as anomalous, but rather as imperfectly regular, or regular within limits. We shall employ default inheritance regularly, perhaps most crucially in the specification of derivational relations (cf. below and cf. Flickinger et al. [19]; and Gazdar [20], <ref> [21] </ref>; and Flickinger and Nerbonne [18] for arguments supporting the use of defaults in lexical specifications).
Reference: [22] <author> Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan Sag. </author> <title> Generalized Phrase Structure Grammar. </title> <publisher> Harvard University Press, </publisher> <year> 1985. </year>
Reference-contexts: Kay's [30] FUG included an ANY value, which defaulted to ? unless unified with, in which case it was &gt;; Shieber [43], p.59 presents a scheme for the default interpretation of feature structure templates; Kaplan [25] presents nonmonotonic "constraining equations" for LFG; and Gazdar et al. <ref> [22] </ref>, p.29 et passim propose nonmonotonic feature specification defaults. 2 of lexical structure, but rather as a framework within which such theories may be formulated, i.e., a tool for lexical description. <p> detail, it is worth reminding ourselves that the developing research paradigm of computational lexicology|within which this paper might be located|differs from computational morphology (e.g., two-level or finite-state morphology) in that the former takes a lexeme-based view of lexical variation (such 3 And nearly everywhere else: GPSG metarules (Gazdar et al., <ref> [22] </ref>) and Categorial Morphology treatments (Hoeksema, [23]) are quite similar in treating lexical rules as fundamentally distinct from lexical entries. 3 as inflection and derivation), while the latter generally takes a morpheme-based view.
Reference: [23] <author> Jack Hoeksema. </author> <title> Categorial Morphology. </title> <publisher> Garland, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: that the developing research paradigm of computational lexicology|within which this paper might be located|differs from computational morphology (e.g., two-level or finite-state morphology) in that the former takes a lexeme-based view of lexical variation (such 3 And nearly everywhere else: GPSG metarules (Gazdar et al., [22]) and Categorial Morphology treatments (Hoeksema, <ref> [23] </ref>) are quite similar in treating lexical rules as fundamentally distinct from lexical entries. 3 as inflection and derivation), while the latter generally takes a morpheme-based view. <p> This same observation holds for HPSG [36], Ch. 8.2, 21 for the Alvey tools project [38], for the early days of HPSG [19], for the work of Flickinger [17], and also for Hoeksema's Categorial Morphology <ref> [23] </ref>. By its nature, an external lexical rule sets up a relation between two lexemes (or classes of lexemes)|or, in the case of feature-based theories, between two feature structure descriptions.
Reference: [24] <author> Mark Johnson. </author> <title> Attribute Value Logic and the Theory of Grammar. Center for the Study of Language and Information, </title> <publisher> Stanford, </publisher> <year> 1988. </year>
Reference-contexts: What we have written above are AVM's or feature descriptions|they describe the abstract objects (feature structures) we use to model linguistic phenomena. Attribute-value descriptions such as the ones above are standardly interpreted in one of two ways: either directly, as descriptions of linguistic objects (cf. Johnson <ref> [24] </ref>; Smolka [46]), or algebraically, as specifications of feature structures (cf. Pollard and Sag, [36] Chap. 2), which then may be regarded as models of the linguistic objects.
Reference: [25] <author> Ronald Kaplan and Joan Bresnan. </author> <title> Lexical-functional grammar: A formal system for gram-matical representation. </title> <editor> In Joan Bresnan, editor, </editor> <booktitle> The Mental Representation of Grammatical Relations, </booktitle> <pages> pages 173-281. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Mass, </address> <year> 1982. </year>
Reference-contexts: Kay's [30] FUG included an ANY value, which defaulted to ? unless unified with, in which case it was &gt;; Shieber [43], p.59 presents a scheme for the default interpretation of feature structure templates; Kaplan <ref> [25] </ref> presents nonmonotonic "constraining equations" for LFG; and Gazdar et al. [22], p.29 et passim propose nonmonotonic feature specification defaults. 2 of lexical structure, but rather as a framework within which such theories may be formulated, i.e., a tool for lexical description.
Reference: [26] <author> Lauri Karttunen. D-PATR: </author> <title> A development environment for unifiaction-based grammars. </title> <type> Technical Report CSLI-86-61, </type> <institution> Center for the Study of Language and Information, Stanford University, </institution> <year> 1986. </year>
Reference-contexts: Our proposed construal of lexical rules as feature structures obviates any use of lexical rules as operations|also familiar from LFG (Bresnan, [8]) and PATR-II (Shieber et al., [44]), especially D-PATR (Karttunen, <ref> [26] </ref>). 3 We believe therefore that the present paper is a contribution to feature-based theories, as well, in that it shows how lexical rules can be construed in terms of feature-structure. <p> While nothing would prohibit a lexical rule from operating on abstract stems to create forms, this was seldom done (cf. Karttunen's LFG-style treatment of passive in D-PATR, Karttunen, <ref> [26] </ref>, pp.12-14 for an exception). <p> Lexical rules in PATR-II [44] or D-PATR <ref> [26] </ref>, e.g., look like feature structures and are represented via a collection of path equations, but their interpretation is completely different from that of (normal) feature structures. <p> Flickinger and Nerbonne [18] for a similar treatment of the semantics of the for phrase licensed by easy adjectives and Karttunen <ref> [26] </ref> for an analysis of the semantics of the passive by phrase along these lines). The intention behind this approach can be appreciated in a concrete example: the sentence Das Buch ist lesbar `The book is readable' doesn't explicitly state for whom it is possible to read the book.
Reference: [27] <author> Robert T. Kasper and William C. </author> <title> Rounds. A logical semantics for feature structures. </title> <booktitle> In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 257-266, </pages> <institution> Columbia University, </institution> <year> 1986. </year>
Reference-contexts: The formulation of the principles presupposes that the underlying feature logic (along the lines of Kasper & Rounds <ref> [27] </ref>, [39]; cf. the section on feature structures and HPSG) is extended by adding functional dependencies (functionally dependent values; for a motivation, cf. Pollard & Sag [36], pp. 48-49; for a formal definition, cf. Reape [37], pp. 73ff).
Reference: [28] <author> Andreas Kathol. </author> <title> Verbal and adjectival passives in german. </title> <booktitle> In MIT Working Papers in Linguistics, </booktitle> <volume> volume 14. </volume> <publisher> MIT, </publisher> <year> 1991. </year>
Reference-contexts: Any putative distinction related to the two types of representation would have to be explained in another fashion. 12 Cf. Kathol <ref> [28] </ref> for a third, perhaps most interesting analysis, under which a single participial form serves in both active and passive voices, so that passive is neither an inflection, nor a derivational alternation. 14 2 6 6 6 6 6 6 6 6 6 6 4 MORPH 2 STEM PREFIX f $1
Reference: [29] <author> Martin Kay. </author> <title> Functional unification grammar: A formalism for machine translation. </title> <booktitle> In Proceedings of COLING, </booktitle> <pages> pages 75-78, </pages> <year> 1984. </year>
Reference-contexts: Our proposals are couched in HPSG terms, both because this is often readily understandable but also because HPSG|and to a lesser extent, FUG (Kay, <ref> [29] </ref>, [30])|have most vigorously explored the hypothesis that feature structures are a sufficient representation scheme for all linguistic knowledge. The idea is that feature structures (also called "functional structures" in FUG, "attribute-value matrices" in HPSG) can encode not only syntactic information, but also semantic, pragmatic, morphological and lexical information.
Reference: [30] <author> Martin Kay. </author> <title> Parsing in functional unification grammar. </title> <editor> In David Dowty and Lauri Karttunen, editors, </editor> <booktitle> Natural Language Parsing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1985. </year>
Reference-contexts: Pollard and Sag [36], p.194, Note 4; Sag and Pollard [42], p.24; and Shieber [43], pp.59-61. 2 It is probably worth noting that there have nonetheless been several attempts at using nonmonotonic inference rules in unification-based systems. Kay's <ref> [30] </ref> FUG included an ANY value, which defaulted to ? unless unified with, in which case it was &gt;; Shieber [43], p.59 presents a scheme for the default interpretation of feature structure templates; Kaplan [25] presents nonmonotonic "constraining equations" for LFG; and Gazdar et al. [22], p.29 et passim propose nonmonotonic
Reference: [31] <author> James Kilbury, Petra Naerger, and Ingrid Renz. </author> <title> DATR as a lexical component for PATR. </title> <booktitle> In Proceedings of the 6th Annual Meeting of the European Chapter of the Association for Computational Linguistics, </booktitle> <year> 1991. </year> <month> 35 </month>
Reference-contexts: Most linguists will interpret lexical rules procedurally. But lexical rules can also be regarded purely declaratively, even if the procedural view is the most prominent one. 20 15 For a rather bleak view of prospects for interfaces between DATR and feature formalisms, cf. the proposal in Kilbury et al. <ref> [31] </ref>. 16 What makes this point subtle is that DATR does allow the expression of "path equivalences", e.g., a statement such as &lt; agr &gt;==&lt; subject agr &gt;.
Reference: [32] <author> Kimmo Koskenniemi. </author> <title> Two-level model for morphological analysis. </title> <booktitle> In Proceedings of the Eighth Annual Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 683-685, </pages> <year> 1983. </year>
Reference-contexts: The two-level approach to inflection may be found in Koskenniemi <ref> [32] </ref> (for an interesting extension in the direction of feature-based processing, cf. Trost [48]). This differs from the current proposal in being morpheme-based. It is, however, compatible with various lexical structures, as is demonstrated by its use in the Alvey project, noted above [38].
Reference: [33] <author> Hans-Ulrich Krieger. </author> <title> Eliminating complex non-reversible functions in derivational morphol-ogy. </title> <type> Technical Report xx, </type> <institution> Deutsches Forschungsinstitut fur Kunstliche Intelligenz, Saarbrucken, Germany, </institution> <year> 1991. </year>
Reference-contexts: Likewise, morphological head features may be specified under MORPHjHEAD which will seem preferable to those to whom it seems unnatural to specify the category of, e.g., -bar as (unsaturated) adjective. This approach is investigated in Krieger <ref> [33] </ref>. 20 and headed-structure is replaced by morph-head-struct (morphologically headed structure) which has at least the attributes HEAD-MORPH and COMP-MORPH. The symbol complex (complex word) corresponds to the type of the same name in the subsumption lattice (cf. Fig. 2).
Reference: [34] <author> P.H.Matthews. </author> <title> Inflectional Morphology: A Theoretical Study Based on Aspects of Latin Verb Conjugation. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1972. </year>
Reference-contexts: Trost [48]). This differs from the current proposal in being morpheme-based. It is, however, compatible with various lexical structures, as is demonstrated by its use in the Alvey project, noted above [38]. The direct characterization of the paradigm has been the alternative approach both in linguistics (cf. Matthews, <ref> [34] </ref>, Chap.IV) and in computational linguistics (cf. Evans and Gazdar's DATR, [16], and Russell et al.'s ELU lexicon, [40]). The fundamental idea in our characterization is due to the work in DATR, in which paradigms are treated as alternative further specifications of abstract lexemes.
Reference: [35] <author> P.H.Matthews. </author> <title> Morphology. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1974. </year>
Reference-contexts: Wellformedness is one such important property. And it turns out that the wellformedness of forms with the suffix -bar cannot be predicted only on the basis of the component morphemes. We find patterns such as the following: 4 Cf. Matthews <ref> [35] </ref> pp.20ff for a much more thorough defense of this material; cf. <p> There are, e.g., verbs whose paradigms differ from those of their component morphemes, e.g., the weak verbs handhaben and veranlassen, which are derived from the strong haben and lassen. If inflection depends on morphemes, these examples must be analyzed as involving distinct pairs of morphemes. 7 Matthews <ref> [35] </ref>, Chap. <p> In general, in representing a paradigm, the exponents of the paradigm are listed as alternatives under [MORPHjENDING f $n : : :g], while their associated properties appear elsewhere within the feature structure under the same distributed disjunction `f $n : : :g' (for this terminology, cf Matthews, <ref> [35] </ref>). One advantage of representing paradigms in this fashion|as opposed to representations via lexical rules, as in Flickinger [17] or in the other rule-based approaches cited above|is that the paradigm is represented in the same formalism in which word classes and lexemes are represented.
Reference: [36] <author> Carl Pollard and Ivan Sag. </author> <title> An Information-Based Theory of Syntax and Semantics, </title> <publisher> Vol.I. CSLI, Stanford, </publisher> <year> 1987. </year>
Reference-contexts: PATR-II present lexicons via a collection of templates or macros, which are purely syntactic abbreviations for feature-structure descriptions. Pollard and Sag's (1987) sketch of the lexicon in Head-Driven Phrase Structure Grammar (hence HPSG), presented in Pollard and Sag <ref> [36] </ref> and Sag and Pollard [42], extended these ideas by interpreting lexical definitions as feature-structure descriptions and inheritance specifications as subsumption statements, rather than treating them in a purely syntactic fashion. <p> Lexical rules are thus an emergent phenomenon in language, ultimately reducible to feature-structures (cf. Section 4 for a discussion of alternative views of lexical rules). Our purpose in this paper is to present the feature-based lexicon not as a linguistic theory 1 But cf. Pollard and Sag <ref> [36] </ref>, p.194, Note 4; Sag and Pollard [42], p.24; and Shieber [43], pp.59-61. 2 It is probably worth noting that there have nonetheless been several attempts at using nonmonotonic inference rules in unification-based systems. <p> We commence from the treatment of the lexicon in HPSG I (Pollard and Sag, <ref> [36] </ref>, Chap.8) as an inheritance structure, which we accept (with minor modifications noted below). <p> Attribute-value descriptions such as the ones above are standardly interpreted in one of two ways: either directly, as descriptions of linguistic objects (cf. Johnson [24]; Smolka [46]), or algebraically, as specifications of feature structures (cf. Pollard and Sag, <ref> [36] </ref> Chap. 2), which then may be regarded as models of the linguistic objects. The distinction is mathematically interesting, but it will not be pursued here, since it is irrelevant to grammars and lexicons written in this notation. <p> The deployment of lexical rules may be found in Flickinger's approach (cf. Flickinger, [17], pp.107-110), in the Alvey tools project (cf. Ritchie et al., [38], p.298), and in HPSG (cf. Pollard and Sag, <ref> [36] </ref>, pp.209-213). Paradigmatic morphology improves upon these ideas by defining the paradigm as a sequence of lexical rules on which subsumption relations can be defined (cf. Calder, [9]), but the fundamental analytical tool is still the lexical rule. <p> inflectional variation|this one based on constrained relations. 19 Our briefly sketched treatment of derivation (and inflection) in the introductory section is in its essence declarative, because linguistic knowledge is encoded in terms of feature structures only and unification is the sole information-building operation. 20 Interestingly to note, Pollard & Sag, <ref> [36] </ref>, pp. 209, suggest a third interpretation of lexical rules coming directly from the field of many-sorted abstract data types|an algebraic perspective on lexical rules. 16 4.1 External vs. <p> The same is true, if we move to other theories: f-structures differ in form (syntax) and interpretation (semantics) from lexical rules stated in LFG (cf. the articles in Bresnan, [8]). This same observation holds for HPSG <ref> [36] </ref>, Ch. 8.2, 21 for the Alvey tools project [38], for the early days of HPSG [19], for the work of Flickinger [17], and also for Hoeksema's Categorial Morphology [23]. <p> Under the assumption that a lexical rule can in principle be typed and resides in the lexicon, this type ought to be a subtype of lexical-sign according to Pollard & Sag <ref> [36] </ref> and ought to have exactly the three top-level attributes PHON, SYN, and SEM|but this isn't the case. 22 One can quibble about our choice of terminology here. <p> If this is the case, one may ask, why does HPSG treat word formation via external lexical rules rather than in a purely feature-based way? Why not formulate rules and principles for word grammar similar to those stated by Pollard & Sag <ref> [36] </ref> for phrasal and sentential grammar? We think, there are at least three replies this question might provoke: 1. HPSG is a theory capable only of capturing the form and meaning of sentences in feature structure descriptions, but incapable of describing morphotactical aspects of language. 2. <p> Trying to handle lexical structures (morphotactics) in terms of feature structure descriptions (i.e., via rules and principles) only leads to inefficient implementations. 3. HPSG is a conglomerate of different formalisms and theories (cf. [42], p. 1, and <ref> [36] </ref>, p. 1), saying little or nothing about morphotactics. In the theories from which HPSG borrows, morphotactics were stated in form of external lexical rules. HPSG assumed this view somewhat nonreflectively, because HPSG's primary purpose is the description of phrasal and sentential syntax and semantics. <p> Summing up, we think it's a promising task to approach derivation purely in terms of feature structure descriptions|just in the spirit of HPSG. Recall the two equations in <ref> [36] </ref>, p. 147, UG = P 1 u ::: u P n (2) English = P 1 u : : : u P n+m u (L 1 t : : : t L p t R 1 t : : : t R q )(3) These may be understood in the <p> These fundamental equations define an HPSG grammatical theory for phrases and sentences, and we propose to apply a similar methodology to derivation, relying extensively on rules, principles, and unification-based inheritance (for an explanation of (2) and (3), cf. Pollard & Sag, <ref> [36] </ref>, p. 147). In contrast to inflection, derivation cannot rely on naive inheritance alone. <p> Treating derivation in our approach will lead to complex morphs (e.g., words) consisting of a head daughter HEAD-MORPH and a complement daughter COMP-MORPH (e.g. (5)). The task of the `morphological-daughters' feature is to encode morphological structure, similarly to how HEAD-DTR and COMP-DTRS do this on the phrasal level (cf. <ref> [36] </ref>). <p> Fig. 2). HEAD-MORPH and COMP-MORPH are put together under the label MORPHS (cf. DTRS). With these assertions in mind, we postulate, in analogy to the phrasal `rules' in HPSG (cf. <ref> [36] </ref>, pp. 149), the following morphological rule schema: [LEX +] ! H; C [LEX +](6) or more formally as a typed feature structure ('' is used for definitional expressions): MHCR 2 6 6 complex SYNjLOCjLEX + MORPHS 2 morph-head-struct HEAD-MORPH [affix ] COMP-MORPH [part-of-speech ] 3 3 7 7 (7) Just <p> The formulation of the principles presupposes that the underlying feature logic (along the lines of Kasper & Rounds [27], [39]; cf. the section on feature structures and HPSG) is extended by adding functional dependencies (functionally dependent values; for a motivation, cf. Pollard & Sag <ref> [36] </ref>, pp. 48-49; for a formal definition, cf. Reape [37], pp. 73ff). <p> If the argument is of type morph-head-struct (see below), the function is being applied in derivation instead of working on the sentence level. Thus, in contrast to the version of order-constituents in <ref> [36] </ref>, our version of order-constituent is overloaded with respect to its argument|we employ an ad hoc polymorphism. Cf. Cardelli & Wegner [10]. Alternatively, we could specify a second function, order-morph-constituents. 21 Although Pollard & Sag [36] strictly type the attributes of feature structures in general, they do not explicitly state that <p> Thus, in contrast to the version of order-constituents in <ref> [36] </ref>, our version of order-constituent is overloaded with respect to its argument|we employ an ad hoc polymorphism. Cf. Cardelli & Wegner [10]. Alternatively, we could specify a second function, order-morph-constituents. 21 Although Pollard & Sag [36] strictly type the attributes of feature structures in general, they do not explicitly state that principles as well as rules may also be regarded as types.
Reference: [37] <author> Mike Reape. </author> <title> An introduction to the semantics of unification-based grammar formalisms. </title> <type> Technical Report R3.2.A, </type> <institution> DYANA, University of Edinburgh, </institution> <year> 1991. </year>
Reference-contexts: Pollard & Sag [36], pp. 48-49; for a formal definition, cf. Reape <ref> [37] </ref>, pp. 73ff).
Reference: [38] <author> Graeme D. Ritchie, Stephen G. Pulman, Alan W. Black, and Graham Russell. </author> <title> A computa-tional framework for lexical description. </title> <booktitle> Computational Linguistics, </booktitle> <address> 13(3-4):290-307, </address> <year> 1987. </year>
Reference-contexts: The deployment of lexical rules may be found in Flickinger's approach (cf. Flickinger, [17], pp.107-110), in the Alvey tools project (cf. Ritchie et al., <ref> [38] </ref>, p.298), and in HPSG (cf. Pollard and Sag, [36], pp.209-213). Paradigmatic morphology improves upon these ideas by defining the paradigm as a sequence of lexical rules on which subsumption relations can be defined (cf. Calder, [9]), but the fundamental analytical tool is still the lexical rule. <p> Trost [48]). This differs from the current proposal in being morpheme-based. It is, however, compatible with various lexical structures, as is demonstrated by its use in the Alvey project, noted above <ref> [38] </ref>. The direct characterization of the paradigm has been the alternative approach both in linguistics (cf. Matthews, [34], Chap.IV) and in computational linguistics (cf. Evans and Gazdar's DATR, [16], and Russell et al.'s ELU lexicon, [40]). <p> The same is true, if we move to other theories: f-structures differ in form (syntax) and interpretation (semantics) from lexical rules stated in LFG (cf. the articles in Bresnan, [8]). This same observation holds for HPSG [36], Ch. 8.2, 21 for the Alvey tools project <ref> [38] </ref>, for the early days of HPSG [19], for the work of Flickinger [17], and also for Hoeksema's Categorial Morphology [23]. By its nature, an external lexical rule sets up a relation between two lexemes (or classes of lexemes)|or, in the case of feature-based theories, between two feature structure descriptions.
Reference: [39] <author> William C. Rounds and Robert T. Kasper. </author> <title> A complete logical calculus for record structures representing linguistic information. </title> <booktitle> In Proceedings of the 15th Annual Symposium of the on Logic in Computer Science, </booktitle> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: The formulation of the principles presupposes that the underlying feature logic (along the lines of Kasper & Rounds [27], <ref> [39] </ref>; cf. the section on feature structures and HPSG) is extended by adding functional dependencies (functionally dependent values; for a motivation, cf. Pollard & Sag [36], pp. 48-49; for a formal definition, cf. Reape [37], pp. 73ff).
Reference: [40] <author> Graham Russell, John Carroll, and Susan Warwick. </author> <title> Multiple default inheritance in a unification-based lexicon. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1991. </year>
Reference-contexts: The direct characterization of the paradigm has been the alternative approach both in linguistics (cf. Matthews, [34], Chap.IV) and in computational linguistics (cf. Evans and Gazdar's DATR, [16], and Russell et al.'s ELU lexicon, <ref> [40] </ref>). The fundamental idea in our characterization is due to the work in DATR, in which paradigms are treated as alternative further specifications of abstract lexemes. We express the same fundamental idea in feature structures by defining paradigms as large disjunctions which subsume appropriate lexical nodes. <p> The feature-based model using ditributed disjunctions is freer: dependent properties may be distributed in various positions in a feature structure. The ELU lexicon (Russell et al., <ref> [40] </ref>) uses equations describing feature structures for the most part, but the equations are divided into a "main set" and a "variant set", the latter of which (monotonically) describe inflectional variants. <p> Lexical rules are simply a particular kind of feature structure description. 17 presented here for the field of derivation. Another approach, which is also distinguished by its use of internal lexical rules, can be found in the ELU system <ref> [40] </ref>. 23 4.2 Our Treatment of Derivation In HPSG-I linguistic knowledge about word formation is encoded through a family of lexical rules, which are not feature structures, but rather essentially external operators working on feature structures. <p> We are convinced, however, that this approach is not strong enough for derivation (cf. below). 18 ELU's treatment of derivation (cf. <ref> [40] </ref>, p. 218) is done in such a way, and it may be a reasonable tack to take for the treatment at hand, that of the German separable prefixes.
Reference: [41] <author> Jerry Sadock. </author> <title> Autolexical syntax: A proposal for the treatment of noun incorporation and similar phenomena. </title> <booktitle> Natural Language and Linguistic Theory, </booktitle> <volume> 3 </volume> <pages> 379-439, </pages> <year> 1985. </year>
Reference-contexts: in two central lexical topics: the treatment of inflection and derivation. (There is, of course, an influential school of linguistic thought which denies the significance of this distinction, and our tools do not presuppose the distinction|we too can generalize all lexical rules to "word formation" a la Sadock's "autolexical syntax" <ref> [41] </ref>.
Reference: [42] <author> Ivan Sag and Carl Pollard. </author> <title> Head-driven phrase structure: An informal synopsis. </title> <type> Technical Report CSLI-87-89, </type> <institution> Center for the Study of Language and Information, Stanford University, </institution> <year> 1987. </year>
Reference-contexts: PATR-II present lexicons via a collection of templates or macros, which are purely syntactic abbreviations for feature-structure descriptions. Pollard and Sag's (1987) sketch of the lexicon in Head-Driven Phrase Structure Grammar (hence HPSG), presented in Pollard and Sag [36] and Sag and Pollard <ref> [42] </ref>, extended these ideas by interpreting lexical definitions as feature-structure descriptions and inheritance specifications as subsumption statements, rather than treating them in a purely syntactic fashion. Pollard and Sag (Chap.8.2) furthermore suggest a use of lexical rules which brings their work closer to standard linguistic views (e.g., LFG, [8]). <p> Section 4 for a discussion of alternative views of lexical rules). Our purpose in this paper is to present the feature-based lexicon not as a linguistic theory 1 But cf. Pollard and Sag [36], p.194, Note 4; Sag and Pollard <ref> [42] </ref>, p.24; and Shieber [43], pp.59-61. 2 It is probably worth noting that there have nonetheless been several attempts at using nonmonotonic inference rules in unification-based systems. <p> Trying to handle lexical structures (morphotactics) in terms of feature structure descriptions (i.e., via rules and principles) only leads to inefficient implementations. 3. HPSG is a conglomerate of different formalisms and theories (cf. <ref> [42] </ref>, p. 1, and [36], p. 1), saying little or nothing about morphotactics. In the theories from which HPSG borrows, morphotactics were stated in form of external lexical rules. HPSG assumed this view somewhat nonreflectively, because HPSG's primary purpose is the description of phrasal and sentential syntax and semantics.
Reference: [43] <author> Stuart Shieber. </author> <title> An Introduction to Unification-Based Approaches to Grammar. Center for the Study of Language and Information, </title> <institution> Stanford University, </institution> <year> 1986. </year>
Reference-contexts: The present proposal draws both from the work above on default inheritance networks and from the lexical ideas of feature-based theories in general (cf. the section on PATR-II lexicons in Shieber, <ref> [43] </ref>, pp.54-61). PATR-II present lexicons via a collection of templates or macros, which are purely syntactic abbreviations for feature-structure descriptions. <p> Section 4 for a discussion of alternative views of lexical rules). Our purpose in this paper is to present the feature-based lexicon not as a linguistic theory 1 But cf. Pollard and Sag [36], p.194, Note 4; Sag and Pollard [42], p.24; and Shieber <ref> [43] </ref>, pp.59-61. 2 It is probably worth noting that there have nonetheless been several attempts at using nonmonotonic inference rules in unification-based systems. Kay's [30] FUG included an ANY value, which defaulted to ? unless unified with, in which case it was &gt;; Shieber [43], p.59 presents a scheme for the <p> and Pollard [42], p.24; and Shieber <ref> [43] </ref>, pp.59-61. 2 It is probably worth noting that there have nonetheless been several attempts at using nonmonotonic inference rules in unification-based systems. Kay's [30] FUG included an ANY value, which defaulted to ? unless unified with, in which case it was &gt;; Shieber [43], p.59 presents a scheme for the default interpretation of feature structure templates; Kaplan [25] presents nonmonotonic "constraining equations" for LFG; and Gazdar et al. [22], p.29 et passim propose nonmonotonic feature specification defaults. 2 of lexical structure, but rather as a framework within which such theories may be formulated, i.e., <p> Shieber <ref> [43] </ref> is the standard introductory reference to feature-based grammars, and we assume basic familiarity with this sort of analysis; here we review only the bare essentials needed for the lexical analysis below. Feature structures are mathematical objects which model linguistic entities such as the utterance tokens of words and phrases.
Reference: [44] <author> Stuart Shieber, Hans Uszkoreit, J. Robinson, and M. Tyson. </author> <title> The formalism and implemen-tation of PATR-II. In Research on Interactive Acquisition and Use of Knowledge. </title> <institution> AI Center, SRI International, Menlo Park, Cal., </institution> <year> 1983. </year>
Reference-contexts: Our proposed construal of lexical rules as feature structures obviates any use of lexical rules as operations|also familiar from LFG (Bresnan, [8]) and PATR-II (Shieber et al., <ref> [44] </ref>), especially D-PATR (Karttunen, [26]). 3 We believe therefore that the present paper is a contribution to feature-based theories, as well, in that it shows how lexical rules can be construed in terms of feature-structure. <p> Lexical rules in PATR-II <ref> [44] </ref> or D-PATR [26], e.g., look like feature structures and are represented via a collection of path equations, but their interpretation is completely different from that of (normal) feature structures.
Reference: [45] <author> Koenraad De Smedt. </author> <title> Using object-oriented knowledge representation techniques in morphol-ogy and syntax programming. </title> <booktitle> In Proceedings of the 1984 European Conference on Artificial Intelligence, </booktitle> <pages> pages 181-184, </pages> <year> 1984. </year>
Reference-contexts: We discuss these in the present section. DATR (cf. Evans and Gazdar [16]) is a graph description language (and inference engine) which can encode information about lexical items. DATR provides a formally clean version of the sort of default inheritance first advocated in de Smedt <ref> [45] </ref> and Flickinger et al. [19], and as such represents a significant advance in the understanding of default lexical inheritance.
Reference: [46] <author> Gert Smolka. </author> <title> A feature logic with subsorts. </title> <type> Technical Report 33, </type> <address> WT LILOG-IBM Germany, </address> <year> 1988. </year>
Reference-contexts: What we have written above are AVM's or feature descriptions|they describe the abstract objects (feature structures) we use to model linguistic phenomena. Attribute-value descriptions such as the ones above are standardly interpreted in one of two ways: either directly, as descriptions of linguistic objects (cf. Johnson [24]; Smolka <ref> [46] </ref>), or algebraically, as specifications of feature structures (cf. Pollard and Sag, [36] Chap. 2), which then may be regarded as models of the linguistic objects. The distinction is mathematically interesting, but it will not be pursued here, since it is irrelevant to grammars and lexicons written in this notation.
Reference: [47] <author> Jindrich Toman. Wortsyntax: </author> <title> Eine Diskussion ausgewahlter Probleme deutscher Wortbildung. </title> <type> Niemeyer, </type> <institution> Tubingen, </institution> <year> 1983. </year>
Reference-contexts: Anderson [2], [3] and Zwicky [49], [50] for more recent defenses of the lexeme-based view. 5 A great deal of what we analyze below may be found in a very thorough study of this process, Jindrich Toman's Wortbildung <ref> [47] </ref>. 4 morpheme lexeme 1 lexeme 2 meid- *meidbar vermeidbar `avoidable' lad- *ladbar aufladbar `loadable' hab- *habbar handhabbar `manageable' stell- *stellbar einstellbar, `adjustable' vorstellbar `imaginable' me- mebar *bemebar `measurable' arbeit- *arbeitbar bearbeitbar `workable' This table demonstrates that the wellformedness of -bar derivatives cannot be predicted on the basis of the stem <p> Defaults, together with the possibility of overwriting defaults in more specific definitions may turn out to be even more important in connection with the analysis of derivational relationships, since these are notoriously irregular in morphological form, syntactic feature assignment, and semantics (cf. Toman's book-length study on -bar adjectives <ref> [47] </ref> for ample illustration). The typed approach to -bar suffixation allows us to prevent ill-formed -bar adjectives; e.g., we have to rule out the combination of haben (to have) together with -bar.
Reference: [48] <author> Harald Trost. </author> <title> The application of two-level morphology to non-concatenative german mor-phology. </title> <booktitle> In Proceedings of the 13th International Conference on Computational Linguistics (COLING), </booktitle> <year> 1990. </year>
Reference-contexts: Cf. Bird [6]. For efficiency reasons, however, a second path might be chosen, viz., the employment of a hybrid feature-based two-level morphology. This has efficiency advantages, and it is our intention to pursue this line. Cf. Trost <ref> [48] </ref>. A third possibility would be to simply anticipate the effects of morphophonemics in the specification of paradigms (even if this results in the multiplication of paradigms). 10 though we are convinced that a more abstract characterization (e.g., in feature structures) is necessary. <p> The two-level approach to inflection may be found in Koskenniemi [32] (for an interesting extension in the direction of feature-based processing, cf. Trost <ref> [48] </ref>). This differs from the current proposal in being morpheme-based. It is, however, compatible with various lexical structures, as is demonstrated by its use in the Alvey project, noted above [38]. The direct characterization of the paradigm has been the alternative approach both in linguistics (cf.
Reference: [49] <author> Arnold M. </author> <title> Zwicky. How to describe inflection. </title> <booktitle> In Proceedings of the 11th Annual Meeting of the Berkeley Linguistics Society, </booktitle> <pages> pages 372-86, </pages> <year> 1985. </year>
Reference-contexts: We find patterns such as the following: 4 Cf. Matthews [35] pp.20ff for a much more thorough defense of this material; cf. Anderson [2], [3] and Zwicky <ref> [49] </ref>, [50] for more recent defenses of the lexeme-based view. 5 A great deal of what we analyze below may be found in a very thorough study of this process, Jindrich Toman's Wortbildung [47]. 4 morpheme lexeme 1 lexeme 2 meid- *meidbar vermeidbar `avoidable' lad- *ladbar aufladbar `loadable' hab- *habbar handhabbar
Reference: [50] <author> Arnold M. </author> <title> Zwicky. Inflectional morphology as a subcomponent of grammar. </title> <booktitle> In Proceedings of the 3rd International Morphology Meeting, </booktitle> <year> 1988. </year> <month> 36 </month>
Reference-contexts: We find patterns such as the following: 4 Cf. Matthews [35] pp.20ff for a much more thorough defense of this material; cf. Anderson [2], [3] and Zwicky [49], <ref> [50] </ref> for more recent defenses of the lexeme-based view. 5 A great deal of what we analyze below may be found in a very thorough study of this process, Jindrich Toman's Wortbildung [47]. 4 morpheme lexeme 1 lexeme 2 meid- *meidbar vermeidbar `avoidable' lad- *ladbar aufladbar `loadable' hab- *habbar handhabbar `manageable'
References-found: 50

