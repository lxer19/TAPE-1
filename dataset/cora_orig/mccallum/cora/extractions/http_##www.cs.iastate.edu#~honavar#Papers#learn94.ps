URL: http://www.cs.iastate.edu/~honavar/Papers/learn94.ps
Refering-URL: http://www.cs.iastate.edu/~cs474/weekly.html
Root-URL: 
Title: Toward Learning Systems That Integrate Different Strategies and Representations TR93-22  
Author: Vasant Honavar 
Address: 226 Atanasoff Ames, IA 50011  
Affiliation: Iowa State University of Science and Technology Department of Computer Science  
Date: September 14, 1993  
Abstract-found: 0
Intro-found: 1
Reference: <author> Arbib, M. A. </author> <year> (1993). </year> <title> Schema Theory: Cooperative Computation for Brain Theory and Distributed AI. </title> <booktitle> In this volume. </booktitle>
Reference-contexts: the exception of some recent work in connectionist architectures for variable binding and unification in slot and filler type representations (Smolensky, 1990; Hendler, 1989; Sun, 1993), very little systematic study has been done on the representation and use (especially in learning) of structured objects such as frames (Minsky, 1975), schemas <ref> (Arbib, 1993) </ref> and conceptual graphs (Sowa, 1984) in connectionist networks. One possibility is to use structured object representations of the sort developed in the context of syntactic pattern recognition (see below) along with suitably defined similarity metrics with generative learning algorithms.
Reference: <author> Ballard, D. H. </author> <year> (1987). </year> <title> Modular Learning in Neural Networks. </title> <booktitle> In: Proceedings of the Sixth National Conference on Artificial Intelligence. </booktitle> <address> Boston, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference: <author> Booker, L. E., Goldberg, D. E., & Holland, J. H. </author> <year> (1990). </year> <title> In: Machine Learning Paradigms and Methods. </title> <editor> Carbonell, J. (Ed). </editor> <address> Boston, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference-contexts: If the actions recommended by different competing classifiers are contradictory, the classifier that has the largest bid has the greatest probability of being selected to fire. The bid of each activated classifier is proportional to its fitness or strength <ref> (Booker, Goldberg, & Holland, 1990) </ref>.
Reference: <author> Booker, L. E., Riolo, R. L., & Holland, J. H. </author> <year> (1993). </year> <title> Learning and Representation in Classifier Systems. </title> <booktitle> In this volume. </booktitle>
Reference-contexts: The representation based on bit-strings provide an effective and efficient basis for classification and abstraction whenever the bits in fact correspond to characteristic features. It is often possible to translate symbolic attribute-value based instance representation into bit-string representation and vice-versa <ref> (Booker, Riolo, & Holland, 1993) </ref>. Abstract symbolic representations (e.g., semantic networks) can be translated into a system of classifiers (Forrest, 1991). None of this should be particularly surprising since rule-based production systems are Turing-equivalent (Post, 1943).
Reference: <author> Buchanan, B. G., & Wilkins, D. C. </author> <year> (1993). </year> <title> Readings in Knowledge Acquisition and Learning. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Carpenter, G., & Grossberg, S. </author> <year> (1991). </year> <title> (Ed). Pattern Recognition by Self-Organizing Neural Networks. </title> <address> Cambridge, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference: <author> Devijver, P. & Kittler, J. </author> <year> (1982). </year> <title> Pattern Recognition: A Statistical Approach. </title> <address> Engle-wood Cliffs, New Jersey: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Fahlman, S. E., & Lebiere, C. </author> <year> (1990). </year> <booktitle> The Cascade-Correlation Architecture. In: Advances in Neural Information Processing Systems Vol. </booktitle> <volume> 2. </volume> <editor> Touretzky, D. S. (Ed). </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fodor, J. </author> <year> (1976). </year> <booktitle> The Language of Thought. </booktitle> <address> Boston, MA: </address> <publisher> Harvard University Press. </publisher>
Reference: <author> Fogel, L. J., Owens, A. J., & Walsh, M. J. </author> <year> (1966). </year> <title> Artificial Intelligence Through Simulated Evolution. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: Stochastic extensions of attributed grammars offer additional interesting possibilities that remain to be explored. 6 Learning in Evolutionary Systems Genetic algorithms and classifier systems (Holland, 1975; Booker, Goldberg, & Holland, 1990; Goldberg, 1989), evolutionary programming <ref> (Fogel, Owens, & Walsh, 1966) </ref>, and genetic programming (Koza, 1992) are all related techniques that find use in machine learning. They are all inspired by processes that appear to be used in biological evolution and the working of the immune system.
Reference: <author> Fu, K. S. </author> <year> (1982). </year> <title> Syntactic Pattern Recognition and Applications. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Various tradeoffs can be made between the descriptive power of the language and the efficiency of syntax analysis. It is often possible to approximate languages of interest by regular (finite state) languages or to use domain-specific syntactic or semantic constraints to facilitate efficient parsing <ref> (Fu, 1982) </ref>. 5.2 Pattern description languages Representation of patterns by linear strings permits the encoding of only a single relation between the pattern primitives (i.e., a pattern primitive can only precede or succeed another). <p> Inference of stochastic grammars can be accomplished by a two-stage process: First, a non-stochastic grammar G is inferred based on the sample strings from the unknown grammar. Then the probabilities p r associated with each rule r 2 P are estimated from the samples using standard statistical estimation techniques <ref> (Fu, 1982) </ref>. This is a good example of a learning scheme wherein both syntactic and statistical paradigms go hand in hand.
Reference: <author> Fukunaga, K. </author> <year> (1990). </year> <title> Introduction to Statistical Pattern Recognition. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Gallant, S. </author> <year> (1993). </year> <title> Neural Network Learning and Expert Systems. </title> <address> Cambridge, Mas-sachusetts: </address> <publisher> MIT Press. </publisher>
Reference: <author> Gennari J., Langley, P., & Fisher, D. </author> <year> (1990). </year> <title> Models of Incremental Concept Formation. In: Machine Learning Paradigms and Methods. </title> <editor> Carbonell, J. (Ed). </editor> <address> Boston, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference: <author> Gentner, D. </author> <year> (1993). </year> <title> The Mechanisms of Analogical Learning. In: Readings in Knowledge Acquisition and Learning. Buchanan, </title> <editor> B. G., & Wilkins, D. C. (Ed). </editor> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. 23 Ginsberg, </publisher> <editor> M. </editor> <booktitle> (1993). Essentials of Artificial Intelligence. </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Analogy refers to a mapping between two entities (objects, events, problems, behaviors, etc.). Relatively little is known about the formation of analogical mappings or analogical inference <ref> (but see Gentner, 1993) </ref>.
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization, </title> <booktitle> and Machine Learning. </booktitle> <address> New York: </address> <publisher> Addison Wesley. </publisher>
Reference: <author> Goldfarb, L. </author> <year> (1990). </year> <title> An Evolving Model for Pattern Learning. </title> <booktitle> Pattern Recognition 23, </booktitle> <pages> 595-616. </pages>
Reference: <author> Gonzalez, R. C., & Thomason, M. G. </author> <year> (1978). </year> <title> Syntactic Pattern Recognition: An Introduction. </title> <address> Reading, Massachusetts: </address> <publisher> Addison Wesley. </publisher>
Reference: <editor> Hendler, J. </editor> <booktitle> (1989). The design and Implementation of Marker-Passing Systems. Connection Science 1 17-40. </booktitle>
Reference: <author> Hinton, G. E. </author> <year> (1990). </year> <title> Connectionist Learning Procedures. In: Machine Learning Paradigms and Methods. </title> <editor> Carbonell, J. (Ed). </editor> <address> Boston, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <booktitle> Adaptation in Natural and Artificial Systems. </booktitle> <address> Ann Arbor, Michi-gan: </address> <publisher> University of Michigan Press. </publisher>
Reference-contexts: It can be shown that evolutionary process of the sort outlined above simulates a highly opportunistic and exploitative randomized search algorithm that explores high-dimensional search spaces rather efficiently <ref> (Holland, 1975) </ref>. Since most learning tasks can be formulated as essentially search problems, it is natural to apply evolutionary algorithms in machine learning. <p> This is the approach taken in (Smith, 1980). Holland (1975) takes a different approach: the solution to a problem is a coupled set of rules but the population consists of individual rules (see below). 19 6.1 Representation and Learning in Classifier Systems Classifier systems <ref> (Holland, 1975) </ref> are adaptive rule-based production systems in which the rules of the form if x then y are restricted to have extremely simple syntax. For obvious reasons, such rules are called classifiers. <p> failure in solving the problem, a heuristically motivated bucket-brigade algorithm is used to change the strength of a classifier by apportioning credit or blame to it as a function of the environmental reward or punishment obtained at the end of a sequence of rule firings in which the classifier participated <ref> (Holland, 1975) </ref>. Genetic operators are applied to a population of classifiers to generate new classifiers periodically which then participate in the bidding and credit assignment process.
Reference: <author> Honavar, V. & Uhr, L. </author> <year> (1988). </year> <title> A Network of Neuron-Like Units that Learns to Perceive by Generation as well as Reweighting of its Links. </title> <booktitle> Proceedings of 1988 Connectionist Models Summer School, </booktitle> <editor> Touretzky, D., Hinton, G and Sejnowski T. (Eds.), </editor> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Honavar, V. & Uhr, L. </author> <year> (1989a). </year> <title> Brain-Structured Connectionist Networks that Perceive and Learn, </title> <booktitle> Connection Science 1, </booktitle> <pages> pp. 139-159. </pages>
Reference: <author> Honavar, V. & Uhr, L. </author> <year> (1989b). </year> <title> Generation, Local Receptive Fields, and Global Convergence Improve Perceptual Learning in Connectionist Networks. </title> <booktitle> In: Proceedings of the 1989 International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Honavar, V. & Uhr, L. </author> <year> (1990). </year> <title> Coordination and Control Structures and Processes: Possibilities for Connectionist Networks. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence 2 277-302. </journal>
Reference: <author> Honavar, V. </author> <year> (1992a). </year> <title> Inductive Learning Using Generalized Distance Measures. </title> <booktitle> In: Proceedings of the SPIE Conference on Adaptive and Learning Systems. </booktitle> <address> Orlando, Florida. </address>
Reference-contexts: Such systems would extract, abstract, and encode instances of complex structured objects as necessary and use parameter modification algorithms to fine-tune the representations thus constructed <ref> (Honavar, 1992a) </ref>. The use of finite automata (regular language recognizers) and other more powerful microcircuits in place of the extremely simple processing elements used in connectionist networks suggest other possibilities. <p> learning algorithms developed in the context of connectionist networks (see above) to add new structured templates through a 17 feedback-guided process of extraction and abstraction of patterns and subpatterns from the environmental input once suitable similarity or distance measures are defined and efficient algorithms for computing such distances are specified <ref> (Honavar, 1992a) </ref>. Many of the generalization and abstraction algorithms used in symbol processing approaches can find use in the modification of acquired templates.
Reference: <author> Honavar, V. </author> <year> (1992b). </year> <title> Some Biases for Efficient Learning of Spatial, Temporal, and Spatio-Temporal Patterns. </title> <booktitle> In: Proceedings of the International Joint Conference on Neural Networks. </booktitle> <address> Beijing, China. </address>
Reference-contexts: For a discussion of a number of alternative generative learning algorithms for connectionist networks, see (Honavar & Uhr, 1993); and for an example of their use in task-driven construction of multi-resolution representations of 2-dimensional images, see <ref> (Honavar, 1992b) </ref>.
Reference: <author> Honavar, V. </author> <year> (1992c). </year> <title> Learning Parsimonious epresentations of Three-Dimensional Shapes. </title> <booktitle> In: NATO Advanced Research Workshop on Mathematical Representations of Shape, </booktitle> <address> Drieber-gen, Netherlands. </address> <note> To appear. </note>
Reference: <author> Honavar, V. & Uhr, L. </author> <title> (1993) Generative Learning Structures and Processes for Generalized Connectionist Networks. </title> <journal> Information Sciences (Special Issue on Artificial Intelligence and Neural Networks) 70 75-108. </journal>
Reference-contexts: In addition, they include mechanisms to ensure that a neuron thus recruited in fact reduces the residual error to yield a better approximation. For a discussion of a number of alternative generative learning algorithms for connectionist networks, see <ref> (Honavar & Uhr, 1993) </ref>; and for an example of their use in task-driven construction of multi-resolution representations of 2-dimensional images, see (Honavar, 1992b).
Reference: <author> Honavar, V., & Uhr, L. </author> <year> (1993a). </year> <title> Symbol Processing Systems, Connectionist Networks, and Beyond. In: Intelligent Hybrid Systems. </title> <editor> Goonatilake, S. & Khebbal, S. (Ed). </editor> <address> London: </address> <note> Wiley. To appear. </note>
Reference: <author> Honavar, V., & Uhr, L. </author> <year> (1993b). </year> <title> Toward Integration of Symbol Processing and Connectionist Systems in Artificial Intelligence and Cognitive Modelling. In: Computational 24 Architectures for Integrating Symbolic and Neural Processes. Sun, </title> <editor> R., & Bookman, L. (Ed). </editor> <address> New York: </address> <note> Kluwer. To appear. </note>
Reference: <author> Jacobs, R. A., Jordan, M. I., & Barto, A. G. </author> <title> Task Decomposition Through Competition in a Modular Connectionist Architecture: The What and Where Vision Tasks. </title> <type> Tech. Rep. 90-27. </type> <institution> Amherst, Massachusetts: Department of Computer and Information Science, University of Massachusetts. </institution>
Reference: <author> Knuth, D. E. </author> <year> (1968). </year> <title> Semantics of Context-Free Languages. </title> <note> Journal of Mathematical System Theory 2 127-146. </note>
Reference-contexts: Borrowing from developments in the theory of programming languages <ref> (Knuth, 1968) </ref> and linguistics, we can add semantic attributes to each substructure-in the case of grammars, to each terminal or auxiliary element as follows: Suppose G = (V T ; V N ; P; S) is a context-free grammar with the production rules P of the form n ! ffi where
Reference: <author> Kohonen, T. </author> <year> (1989). </year> <title> Self-Organization and Associative Memory. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: <editor> Koza, J. </editor> <booktitle> (1992). Genetic Programming. </booktitle> <address> Boston, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Stochastic extensions of attributed grammars offer additional interesting possibilities that remain to be explored. 6 Learning in Evolutionary Systems Genetic algorithms and classifier systems (Holland, 1975; Booker, Goldberg, & Holland, 1990; Goldberg, 1989), evolutionary programming (Fogel, Owens, & Walsh, 1966), and genetic programming <ref> (Koza, 1992) </ref> are all related techniques that find use in machine learning. They are all inspired by processes that appear to be used in biological evolution and the working of the immune system. <p> Their simplicity and generality is appealing. However, it is neither always necessary nor always particularly desirable to restrict the representations used in such evolutionary systems to simple rules of the form used in classifier systems. We will return to this question later. But first, we examine genetic programming <ref> (Koza, 1992) </ref> which demonstrates the application of evolutionary learning methods to breed populations of LISP programs. 6.2 Representation and Learning in Genetic Programming Sys tems Genetic programming (Koza, 1992) is an example of the application of evolutionary learning using populations of LISP programs. <p> We will return to this question later. But first, we examine genetic programming <ref> (Koza, 1992) </ref> which demonstrates the application of evolutionary learning methods to breed populations of LISP programs. 6.2 Representation and Learning in Genetic Programming Sys tems Genetic programming (Koza, 1992) is an example of the application of evolutionary learning using populations of LISP programs. The use of syntactically correct LISP programs to represent potential solutions reduces the learning problem to the task of efficiently searching the space of LISP programs for an acceptable solution. <p> Crossover on the other hand, results in exchange of sub-trees (functions, constants, or parameters) between the participating functions. The shape of the resulting trees (offspring) can in general be different from that of the parents. Genetic programming has been applied to a variety of machine learning tasks <ref> (Koza, 1992) </ref> but its scalability to problems of realistic complexity remains to be established. However, it offers an intriguing approach to learning in domains where neither the size nor the shape of the solution is known a-priori.
Reference: <author> Kung, S. Y. </author> <year> (1993). </year> <title> Digital Neural Networks. </title> <address> New York: </address> <publisher> Prentice Hall. </publisher>
Reference: <author> Laird, J. E., Rosenbloom, P. S. </author> & <title> Newell (1986). Chunking in SOAR: The Anatomy of a General Learning Mechanism. </title> <booktitle> Machine Learning 1, </booktitle> <pages> 11-46. </pages>
Reference: <author> Langley, P., & Zytkow, J. M. </author> <year> (1990). </year> <title> Data-Driven Approaches to Empirical Discovery. In: Machine Learning Paradigms and Methods. </title> <editor> Carbonell, J. (Ed). </editor> <address> Boston, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference: <author> Lenat, D. B. </author> <year> (1982). </year> <title> AM: Discovery as Heuristic Search. In: Knowledge-based Systems Davis, </title> <editor> R., & Lenat, D. B. (Ed). </editor> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Levenshtein, A. </author> <year> (1966). </year> <title> Binary Codes Capable of Correcting Deletions, </title> <journal> Insertions, and Reversals. Soviet Physics - Doklady 10, </journal> <pages> 703-710. </pages>
Reference-contexts: reduces to the acquisition of a suitable collection of templates (and perhaps selection of suitable similarity metrics) adequate for the task. 5.7 Distance Measures for Structured Patterns If patterns are represented by strings, an intuitively appealing distance measure is the Lev-enshtein distance, originally proposed in the context of error-correcting codes <ref> (Levenshtein, 1966) </ref>. The Levenshtein distance (also called edit distance) from a string ff to another string fi is measured by the minimum cost of transforming ff into fi.
Reference: <author> Michalewicz, Z. </author> <year> (1992). </year> <title> Genetic Algorithms + Data Structures = Evolution Programs. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> Theory and Methodology of Inductive Learning. In: Machine Learning AnArtificial Intelligence Approach. </title> <editor> Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Ed). </editor> <address> Palo Alto, California: </address> <publisher> Tioga. </publisher>
Reference: <author> Michalski, R. S. </author> <year> (1993). </year> <title> Toward a Unified Theory of Learning: Multi-Strategy Task-Adaptive Learning. In: Readings in Knowledge Acquisition and Learning. Buchanan, </title> <editor> B. G., & Wilkins, D. C. </editor> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In short, Learning = Inference + Memorization <ref> (Michalski, 1993) </ref>. This takes place in the context of background knowledge that the learner has, the environmental input, and the goals or needs of the learner.
Reference: <author> Miclet, L. </author> <year> (1986). </year> <title> Structural Methods in Pattern Recognition. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Minsky, M. </author> <year> (1975). </year> <title> A Framework for Representing Knowledge. In: The Psychology of Computer Vision. </title> <editor> Winston, P. H. (Ed). </editor> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: With the exception of some recent work in connectionist architectures for variable binding and unification in slot and filler type representations (Smolensky, 1990; Hendler, 1989; Sun, 1993), very little systematic study has been done on the representation and use (especially in learning) of structured objects such as frames <ref> (Minsky, 1975) </ref>, schemas (Arbib, 1993) and conceptual graphs (Sowa, 1984) in connectionist networks. One possibility is to use structured object representations of the sort developed in the context of syntactic pattern recognition (see below) along with suitably defined similarity metrics with generative learning algorithms.
Reference: <author> Minton, S., Carbonell, J. G., Knoblock, C. A., Kuokka, D. R., & Gil, Y. </author> <title> (1990) In: Machine Learning Paradigms and Methods. </title> <editor> Carbonell, J. (Ed). </editor> <address> Boston, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference-contexts: A commonly used analytical learning strategy is explanation-based learning <ref> (Minton et al., 1990) </ref>. For example, given a solution to a particular problem in integral calculus (perhaps discovered with time-consuming search of a suitably represented state-space), explanation-based learning mechanism generates an explanation using background knowledge (general rules of integration) showing how the solution deductively follows from what the system knows.
Reference: <author> Mitchell, T. </author> <year> (1982). </year> <title> Generalization as Search. </title> <booktitle> Artificial Intelligence 18 203-226. </booktitle>
Reference-contexts: Alternatively, one may specialize from overly general descriptions so as to exclude counter-examples of the concept. It is possible to employ bi-directional search of the space of concept descriptions so that both generalization and specialization go hand in hand <ref> (Mitchell, 1982) </ref>.
Reference: <author> Natarajan, B. K. </author> <year> (1992). </year> <title> Machine Learning: A Theoretical Approach. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The amount of search effort necessary is a good measure of the difficulty of learning, and this is invariably a function of the representations at the disposal of the learner. Recent results in computational learning theory <ref> (Natarajan, 1992) </ref> reinforce this intuition.
Reference: <author> Newell, A., & Simon, H. A. </author> <year> (1972). </year> <title> Human Problem-Solving. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Newell, A. </author> <year> (1980). </year> <title> Symbol Systems. </title> <booktitle> Cognitive Science 4 135-183. </booktitle>
Reference: <author> Omlin, C. W., & Giles, C. L. </author> <year> (1993). </year> <title> Extraction and Insertion of Symbolic Information in Recurrent Neural Networks. </title> <note> In this volume. 25 Parekh, </note> <author> R. G., & Honavar, V. </author> <year> (1993a). </year> <title> Efficient Learning of Regular Languages Using Teacher-Supplied Positive Samples and Learner-Generated Queries. </title> <booktitle> In: Proceedings of the UNB Artificial Intelligence Symposium. </booktitle> <address> Fredericton, Canada. </address> <note> To appear. </note>
Reference-contexts: on the efficient inference of a regular grammar G given a set of samples from L (G) and the teacher's responses to a finite number of membership queries on strings generated by the learner, see (Parekh & Honavar, 1993a; 1993b); and for examples of connectionist approaches to grammar induction, see <ref> (Omlin & Giles, 1993) </ref>. 5.4 Attributed Grammars In many practical problems, it becomes necessary to supplement classical grammars which are purely syntactic in nature with semantic information.
Reference: <author> Parekh, R. G., & Honavar, V. </author> <year> (1993b). </year> <title> Grammar Induction for Machine Learning. </title> <note> In preparation. </note>
Reference: <author> Pavlidis, T. </author> <year> (1977). </year> <title> Structural Pattern Recognition. </title> <address> New York: </address> <publisher> Springer. </publisher>
Reference: <author> Post, E. </author> <year> (1943). </year> <title> Formal Reduction of the General Combinatorial Decision Problem. </title> <journal> American Journal of Mathematics 65 197-268. </journal>
Reference-contexts: Abstract symbolic representations (e.g., semantic networks) can be translated into a system of classifiers (Forrest, 1991). None of this should be particularly surprising since rule-based production systems are Turing-equivalent <ref> (Post, 1943) </ref>. The fact that structured symbolic representations can be translated into classifier systems does not necessarily imply that learning has to be restricted to operate at the level of classifier systems.
Reference: <author> Rosenfeld, A. </author> <year> (1981). </year> <title> Picture Languages. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Rosenfeld, A. </author> <year> (1984). </year> <title> (Ed). Multi-Resolution Image Processing and Analysis. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Rueckl, J. G., Cave, K. R., & Kosslyn, S. M. </author> <year> (1989). </year> <title> Why Are "What" and "Where" Processed by Separate Visual Systems? A Computational Investigation. </title> <journal> Journal of Cognitive Neuroscience 1 171-186. </journal>
Reference: <editor> Rumelhart, D. E., & McClelland, J. L. (1986) (Ed). </editor> <booktitle> Parallel Distributed Processing, Vol. </booktitle> <address> I-II, Cambridge, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference: <author> Shavlik, J. W., & Dietterich, T. G. </author> <year> (1990). </year> <editor> (Ed). </editor> <booktitle> Readings in Machine Learning. </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Shavlik, J. W. </author> <year> (1993). </year> <title> A Framework for Combining Symbolic and Neural Learning. </title> <booktitle> In this volume. </booktitle>
Reference-contexts: Such knowledge can take a variety of forms. A number of researchers have investigated techniques of initializing a network with knowledge available in the form of propositional rules <ref> (Shavlik, 1993) </ref>. The network's initial knowledge is gradually refined with the use of training examples that fine-tune the weights in the network.
Reference: <author> Shrager, J., & Langley, P. </author> <year> (1990). </year> <title> Computational Models of Scientific Discovery and Theory Formation. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Simon, H. A. </author> <year> (1983). </year> <title> Why Should Machines Learn? In: Machine Learning AnArtificial Intelligence Approach. </title> <editor> Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Ed). </editor> <address> Palo Alto, California: </address> <publisher> Tioga. </publisher>
Reference: <author> Sowa, J. F. </author> <year> (1984). </year> <title> Conceptual Structures: </title> <booktitle> Information Processing in Mind and Machine. </booktitle> <address> Reading, Massachusetts: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: work in connectionist architectures for variable binding and unification in slot and filler type representations (Smolensky, 1990; Hendler, 1989; Sun, 1993), very little systematic study has been done on the representation and use (especially in learning) of structured objects such as frames (Minsky, 1975), schemas (Arbib, 1993) and conceptual graphs <ref> (Sowa, 1984) </ref> in connectionist networks. One possibility is to use structured object representations of the sort developed in the context of syntactic pattern recognition (see below) along with suitably defined similarity metrics with generative learning algorithms.
Reference: <author> Smith, S. F. </author> <year> (1980). </year> <title> A Learning System Based on Genetic Operators. </title> <type> Ph.D. Dissertation. </type> <institution> University of Pittsburgh. </institution>
Reference-contexts: The most straightforward scheme would be to encode the solution space of the learning problem in the form of bit-strings that constitute potential solutions (individuals in the population). This is the approach taken in <ref> (Smith, 1980) </ref>.
Reference: <author> Smolensky, P. </author> <year> (1990). </year> <title> Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems. </title> <booktitle> Artificial Intelligence 46 159-216. </booktitle>
Reference: <author> Sun, R. </author> <year> (1993). </year> <title> Logic and Variables in Connectionist Models: A Brief Overview. </title> <booktitle> In this volume. </booktitle>
Reference: <author> Tanimoto, S. L., & Klinger, A. </author> <year> (1980). </year> <title> Structured Computer Vision: Machine Perception Through Hierarchical Computation Structures. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Tesauro, G. </author> <year> (1989). </year> <note> Neurogammon Wins Computer Olympiad. Neural Computation 1 321-323. </note>
Reference: <author> Uhr, L., & Vossler, C. </author> <year> (1963). </year> <title> A Pattern Recognition Program that Generates, Evaluates, and Adjusts its Own Pperators. In: Computers and Thought. </title> <editor> Feigenbaum, E., & Feldman, J. (Ed). </editor> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Uhr, L. </author> <year> (1973). </year> <title> Pattern Recognition, Learning, </title> <booktitle> and Thought. </booktitle> <address> New York: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Uhr, L. </author> <title> (1987) (Ed). </title> <booktitle> Parallel Computer Vision. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A Theory of the Learnable. </title> <journal> Communications of the ACM 27 1134-1142. </journal>
Reference: <author> Waibel, A. </author> <year> (1989). </year> <title> Modular Construction of Time-Delay Neural Networks for Speech Recognition. </title> <booktitle> Neural Computation 1 39-46. </booktitle>
Reference: <author> Waterman, D. A. </author> <year> (1985). </year> <title> A Guide to Expert Systems. </title> <address> Reading, Massachusetts: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Elaborate knowledge-bases are constructed through a process of knowledge engineering <ref> (Waterman, 1985) </ref> a complex task that entails making explicit the knowledge and working methods of the human expert, which are usually implicit and difficult to characterize. That is, the human expert must have learned, and be able to make explicit, all the knowledge needed, rather than having the system learn.
Reference: <author> Winston, P. H. </author> <title> (1975) (Ed). </title> <booktitle> The Psychology of Computer Vision. </booktitle> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Winston, P. H. </author> <year> (1992). </year> <booktitle> Artificial Intelligence. </booktitle> <address> Boston, Massachusetts: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Yang, J. & Honavar, V. </author> <year> (1993). </year> <title> A Hierarchical Representation Scheme for Three-Dimensional Object Recognition and Description. </title> <booktitle> In: Proceedings of FLAIRS-93, </booktitle> <address> Fort Lauderdale, Florida. </address> <month> 27 </month>
References-found: 78

