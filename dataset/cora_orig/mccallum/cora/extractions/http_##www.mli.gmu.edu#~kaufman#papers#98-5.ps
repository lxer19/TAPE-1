URL: http://www.mli.gmu.edu/~kaufman/papers/98-5.ps
Refering-URL: http://www.mli.gmu.edu/kpubs.html
Root-URL: 
Title: Machine Learning and Data Mining: Methods and Applications, Data Mining and Knowledge Discovery: A Review
Author: Edited by R.S. Michalski, I. Bratko and M. Kubat Ryszard S. Michalski and Kenneth A. Kaufman 
Note: 1997 John Wiley Sons Ltd 2  2.1 INTRODUCTION  
Abstract: An enormous proliferation of databases in almost every area of human endeavor has created a great demand for new, powerful tools for turning data into useful, task-oriented knowledge. In efforts to satisfy this need, researchers have been exploring ideas and methods developed in machine learning, pattern recognition, statistical data analysis, data visualization, neural nets, etc. These efforts have led to the emergence of a new research area, frequently called data mining and knowledge discovery. The first part of this chapter is a compendium of ideas on the applicability of symbolic machine learning methods to this area. The second part describes a multistrategy methodology for conceptual data exploration, by which we mean the derivation of high-level concepts and descriptions from data through symbolic reasoning involving both data and background knowledge. The methodology, which has been implemented in the INLEN system, combines machine learning, database and knowledge-based technologies. To illustrate the systems capabilities, we present results from its application to a problem of discovery of economic and demographic patterns in a database containing facts and statistics about the countries of the world. The presented results demonstrate a high potential utility of the methodology for assisting in solving practical data mining and knowledge discovery tasks. 
Abstract-found: 1
Intro-found: 1
Reference: [Bai82] <author> Baim, P.W. </author> <title> The PROMISE Method for Selecting Most Relevant Attributes for Inductive Learning Systems. </title> <type> Report No. </type> <institution> UIUCDCS-F-82-898, Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1982. </year>
Reference-contexts: By removing less relevant attributes, the representation space is reduced, and the problem becomes simpler. Thus, such a process can be viewed as a form of improving the representation space. Some methods for finding the most relevant attributes are described in [Zag72] and <ref> [Bai82] </ref>. MICHALSKI & KAUFMAN1 0 In many applications, the attributes originally given may be only weakly or indirectly relevant to the problem at hand. In such situations, there is a need for generating new, more relevant attributes that may be functions of the original attributes. <p> This can be done by applying one of many methods for attribute selection, such as Gain Ratio [Qui93] or Promise level <ref> [Bai82] </ref>. MICHALSKI & KAUFMAN1 6 Generating new attributes: The problem is to generate additional columns that correspond to new attributes generated by a constructive induction procedure. These new attributes are created by using the problems background knowledge and/or special heuristic procedures as described in papers on constructive induction, e.g., [BWM93]. <p> For instance, one such operator climbs an attributes generalization hierarchy to build more general decision rules [KM96]. GENATR operators generate new attribute sets by creating new attributes [BM96], selecting the most representative attributes from the original set <ref> [Bai82] </ref>, or by abstracting attributes [Ker92]. <p> To this end, one may use many different criteria for evaluating the relevance of an attribute for a given classification problem, such as gain ratio [Qui93]; gini index [BFOS84], PROMISE <ref> [Bai82] </ref>, and chisquare analysis [Har84], [Min89]. These criteria evaluate attributes on the basis of their expected global performance, which means that attributes with the highest ability to discriminate among all classes are selected as most relevant. <p> This way, the recognition of Q will require more tests than necessary, but at no expense to the recognition of other letters. INLEN supports both global and local attribute evaluation criteria for selecting the most relevant attributes. The former is based on the PROMISE methodology <ref> [Bai82] </ref>, while the latter employs a variation of PROMISE that is oriented toward the maximum performance of some attribute value, rather than on the attributes global performance. 2.9.2 Generating New Attributes When the original representation space is weakly relevant to the problem at hand, or the concept to be learned is
Reference: [BMR87] <author> Bentrup, J.A., Mehler, G.J. and Riedesel, J.D. </author> <title> INDUCE 4: A Program for Incrementally Learning Structural Descriptions From Examples. Reports of the Intelligent Systems Group, </title> <type> ISG 87-2. </type> <institution> UIUCDCS-F-87-958, Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1987. </year>
Reference-contexts: AQ15 learns attributional descriptions of entities, i.e., descriptions involving only their attributes. More general descriptions, structural or relational., also involve relationships among components of the entities, the attributes of the components, and quantifiers. Such descriptions are produced, for example, by the INDUCE module of EMERALD [Lar77], <ref> [BMR87] </ref>. Constructing structural descriptions requires a more complex description DATA MINING AND KNOWLEDGE DISCOVERY 5 language that includes multi-argument predicates, for example, PROLOG, or Annotated Predicate Calculus [Mic83], [BMK97].
Reference: [BMMZ92] <author> Bergadano, F., Matwin, S., Michalski, R.S., and Zhang, J. </author> <title> Learning Two-Tiered Descriptions of Flexible Concepts: The POSEIDON System. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> pp. 5-43, </pages> <year> 1992. </year> <note> MICHALSKI & KAUFMAN3 6 </note>
Reference-contexts: Sections 2.5 and 2.8 present consistent and complete example solutions from the inductive concept learning program AQ15c [WKBM95]. In some applications, especially those involving learning rules from noisy data or learning flexible concepts [Mic90], it is may be advantageous to learn descriptions that are incomplete and/or inconsistent <ref> [BMMZ92] </ref>. Attributional descriptions can be visualized by mapping them into a planar representation of a discrete multidimensional space (a diagram) spanned over the given attributes [Mic78], [WSWM90]. For example, Figure 2.2 shows a diagrammatic visualization of the rules from DIAV [WSWM90], [Wne95]. <p> Learning flexible concepts, i.e., concepts that inherently lack precise definition and whose meaning is context-dependent; some ideas concerned with this topic include fuzzy sets (e.g., [Zad65], [DPY93]), two-tiered concept representations (e.g., [Mic90], <ref> [BMMZ92] </ref>), and rough sets (e.g., [Paw91], [Slo92], [Zia94]). Learning concepts at different levels of generality, i.e., learning descriptions that involve concepts from different levels of generalization hierarchies representing background knowledge (e.g., [KM96]).
Reference: [BMM96] <author> Bloedorn, E., Mani, I. and MacMillan, T.R. </author> <title> Machine Learning of User Profiles: Representational Issues. </title> <booktitle> Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: While this chapter concentrates on methods for extracting knowledge from numeric and symbolic data, many DATA MINING AND KNOWLEDGE DISCOVERY 3 techniques can also be useful when applied to text, speech or image data (e.g., <ref> [BMM96] </ref>, [Uma97], [CGCME97], [MRDMZ97]). The second part of this chapter describes a methodology for conceptual data exploration, by which we mean the derivation of high-level concepts and descriptions from data.
Reference: [BM96] <author> Bloedorn, E., and Michalski, </author> <title> R.S. The AQ17-DCI System for Data-Driven Constructive Induction and Its Application to the Analysis of World Economics. </title> <booktitle> Proceedings of the 9th International Symposium on Methodologies for Intelligent Systems, </booktitle> <address> Zakopane, Poland. </address> <year> 1996. </year>
Reference-contexts: In this program, the process of generating new attributes is done by combining initial attributes by mathematical and/or logical operators and selecting the best combinations, and/or by obtaining advice from an expert [BWM93] <ref> [BM96] </ref>. 2.2.4 Selection of the Most Representative Examples When a database is very large, determining general patterns or rules characterizing different concepts may be very time-consuming. <p> The determination of such relationships (rules) can be guided by different rule quality criteria, for example, simplicity, cost, predictive accuracy, etc. In the INLEN system, the AQ learning method was applied due to the simplicity and the high comprehensibility of decision rules it generates [WKBM95], <ref> [BM96] </ref>. Determining timedependent patterns: This problem concerns the detection of temporal patterns in sequences of data arranged along the time dimension in a GDT (Figure 2.5). Among the novel ideas that could be applied for analyzing such timedependent data is a multi-model method for qualitative prediction [DM86], [MKC85], [MKC86]. <p> Among the novel ideas that could be applied for analyzing such timedependent data is a multi-model method for qualitative prediction [DM86], [MKC85], [MKC86]. Another novel idea is a temporal constructive induction technique that can generate new attributes that are designed to capture time dependent patterns [Dav81], <ref> [BM96] </ref>. Example selection: The problem is to select rows from the table that correspond to the most representative examples of different classes. When a datatable is very large, is it important to concentrate the analysis on a representative sample. <p> For instance, one such operator climbs an attributes generalization hierarchy to build more general decision rules [KM96]. GENATR operators generate new attribute sets by creating new attributes <ref> [BM96] </ref>, selecting the most representative attributes from the original set [Bai82], or by abstracting attributes [Ker92]. <p> This is done by a constructive induction operator based on the program AQ17-DCI <ref> [BM96] </ref>. In the case of a database that contains information on objects changing over time, one needs a mechanism for constructive induction that can take advantage of the time data ordering. <p> As described above, these rules apply only in the context of predominantly Islamic countries, and are based on the assumption that that determination has already been made. 2.10.5 Experiment 4: Applying Constructive Induction Operators An experiment chronicled by Bloedorn and Michalski <ref> [BM96] </ref> demonstrates the power of utilizing constructive induction as a knowledge discovery operator.
Reference: [BWM93] <author> Bloedorn, E., Wnek, J. and Michalski, </author> <title> R.S. Multistrategy Constructive Induction. </title> <booktitle> Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <address> Harpers Ferry, WV, </address> <pages> pp. 188-203, </pages> <year> 1993. </year>
Reference-contexts: A learning process that consists of two (intertwined) phases, one concerned with the construction of the best representation space, and the second concerned with generating the best hypothesis in the found space is called constructive induction [Mic78], [Mic83], [WM94]. An example of a constructive induction program is AQ17 <ref> [BWM93] </ref>, which performs all three types of improvements of the original representation space. In this program, the process of generating new attributes is done by combining initial attributes by mathematical and/or logical operators and selecting the best combinations, and/or by obtaining advice from an expert [BWM93] [BM96]. 2.2.4 Selection of the <p> constructive induction program is AQ17 <ref> [BWM93] </ref>, which performs all three types of improvements of the original representation space. In this program, the process of generating new attributes is done by combining initial attributes by mathematical and/or logical operators and selecting the best combinations, and/or by obtaining advice from an expert [BWM93] [BM96]. 2.2.4 Selection of the Most Representative Examples When a database is very large, determining general patterns or rules characterizing different concepts may be very time-consuming. <p> MICHALSKI & KAUFMAN1 6 Generating new attributes: The problem is to generate additional columns that correspond to new attributes generated by a constructive induction procedure. These new attributes are created by using the problems background knowledge and/or special heuristic procedures as described in papers on constructive induction, e.g., <ref> [BWM93] </ref>. Clustering: The problem is to automatically partition the rows of the table into groups that correspond to conceptual clusters, that is, sets of entities with high conceptual cohesiveness [MSD81]. Such a clustering operator will generate an additional column in the table that corresponds to a new attribute cluster name.
Reference: [Bon70] <author> Bongard, N. </author> <title> Pattern Recognition. Spartan Books, New York (a translation from Russian), </title> <year> 1970. </year>
Reference-contexts: These functions may be simple, e.g., a product or sum of a set of the original attributes, or very complex, e.g., a Boolean attribute based on the presence or absence of a straight line or circle in an image <ref> [Bon70] </ref>. Finally, in some situations, it will be desirable to abstract some attributes, that is, to group some attribute values into units, and thus reduce the attributes range of possible values. A quantization of continuous attributes is an example of such an operation.
Reference: [BKKPS96] <author> Brachman, R.J., Khabaza, T., Kloesgen, W., Piatetsky-Shapiro, G. and Simoudis, E. </author> <title> Mining Business Databases. </title> <journal> Communications of the ACM, </journal> <volume> 39:11, </volume> <pages> pp. 42-48, </pages> <year> 1996. </year>
Reference-contexts: These and related efforts have led to the emergence of a new research area, frequently called data mining and knowledge discovery, e.g., [Lbo81], [MBS82], [ZG89], [Mic91b], [Zag91], [MKKR92], [VHMT93], [FPSU96], [EH96], <ref> [BKKPS96] </ref>, and [FHS96]. The first part of this chapter is a compendium of ideas on the applicability of symbolic machine learning methods to data mining and knowledge discovery.
Reference: [BMK97] <author> Bratko, I., Muggleton, S. and Karalic, A. </author> <title> Applications of Inductive Logic Programming. </title> <editor> In Michalski, R.S., Bratko, I. and Kubat, M. (eds.), </editor> <booktitle> Machine Learning and Data Mining: Methods and Applications, </booktitle> <address> London, </address> <publisher> John Wiley & Sons, </publisher> <year> 1997. </year>
Reference-contexts: Such descriptions are produced, for example, by the INDUCE module of EMERALD [Lar77], [BMR87]. Constructing structural descriptions requires a more complex description DATA MINING AND KNOWLEDGE DISCOVERY 5 language that includes multi-argument predicates, for example, PROLOG, or Annotated Predicate Calculus [Mic83], <ref> [BMK97] </ref>. For database exploration, attributional descriptions appear to be the most important and the easiest to implement, because typical databases characterize entities in terms of attributes, not relations. One simple and popular form of attributional description is a decision or classification tree.
Reference: [BFOS84] <author> Breiman, L., Friedman, J.H., Olshen, R.A. and Stone, C.J. </author> <title> Classification and Regression Trees. </title> <address> Belmont, CA, </address> <publisher> Wadsworth Int. Group, </publisher> <year> 1984. </year>
Reference-contexts: To this end, one may use many different criteria for evaluating the relevance of an attribute for a given classification problem, such as gain ratio [Qui93]; gini index <ref> [BFOS84] </ref>, PROMISE [Bai82], and chisquare analysis [Har84], [Min89]. These criteria evaluate attributes on the basis of their expected global performance, which means that attributes with the highest ability to discriminate among all classes are selected as most relevant.
Reference: [CR95a] <author> Carpineto, C. and Romano, G. </author> <title> Some Results on Lattice-based Discovery in Databases. </title> <booktitle> Workshop on Statistics, Machine Learning and Knowledge Discovery in Databases, Heraklion, </booktitle> <pages> pp. 216-221, </pages> <year> 1995. </year>
Reference-contexts: An example of conceptual clustering is presented in Section 2.5. DATA MINING AND KNOWLEDGE DISCOVERY 9 A Some new ideas on employing conceptual clustering for structuring text databases and creating concept lattices for discovering dependencies in data are in <ref> [CR95a] </ref> and [CR95b].
Reference: [CR95b] <author> Carpineto, C. and Romano, G. </author> <title> Automatic Construction of Navigable Concept Networks Characterizing Text Databases. </title> <editor> Gori, M. and Soda, G. (eds.), </editor> <booktitle> Topics in Artificial Intelligence, </booktitle> <publisher> LNAI 992-Springer, </publisher> <pages> pp. 67-78, </pages> <year> 1995. </year>
Reference-contexts: An example of conceptual clustering is presented in Section 2.5. DATA MINING AND KNOWLEDGE DISCOVERY 9 A Some new ideas on employing conceptual clustering for structuring text databases and creating concept lattices for discovering dependencies in data are in [CR95a] and <ref> [CR95b] </ref>.
Reference: [CGCME97] <author> Cavalcanti, R.B., Guadagnin, R., Cavalcanti, C.G.B., Mattos, S.P. and Estuqui, </author> <title> V.R. A Contribution to Improve Biological Analyses of Water Through Automatic Image Recognition. Pattern Recognition and Image Analysis, </title> <booktitle> 7:1, </booktitle> <pages> pp. 18-23, </pages> <year> 1997. </year>
Reference-contexts: While this chapter concentrates on methods for extracting knowledge from numeric and symbolic data, many DATA MINING AND KNOWLEDGE DISCOVERY 3 techniques can also be useful when applied to text, speech or image data (e.g., [BMM96], [Uma97], <ref> [CGCME97] </ref>, [MRDMZ97]). The second part of this chapter describes a methodology for conceptual data exploration, by which we mean the derivation of high-level concepts and descriptions from data. The methodology, stemming mainly from various efforts in machine learning, applies diverse methods and tools for determining task-oriented data characterizations and generalizations.
Reference: [CM81] <author> Collins, A. and Michalski, </author> <title> R.S. Toward a Formal Theory of Human Plausible Reasoning. </title> <booktitle> Proceedings of the Third Annual Conference of the Cognitive Science Society, </booktitle> <address> Berkeley, CA, </address> <year> 1981. </year>
Reference-contexts: An interesting approach to this problem is to apply a multi-line reasoning, based the core theory of human plausible reasoning <ref> [CM81] </ref>, [Don88], [CM89]. Determining decision structures from declarative knowledge (decision rules): Suppose that a set of general decision rules (a declarative form of knowledge) has been hypothesized for a given data set (GDT).
Reference: [CM89] <author> Collins, A. and Michalski, </author> <title> R.S. The Logic of Plausible Reasoning: A Core Theory. </title> <journal> Cognitive Science, </journal> <volume> 13, </volume> <pages> pp. 1-49, </pages> <year> 1989. </year>
Reference-contexts: An interesting approach to this problem is to apply a multi-line reasoning, based the core theory of human plausible reasoning [CM81], [Don88], <ref> [CM89] </ref>. Determining decision structures from declarative knowledge (decision rules): Suppose that a set of general decision rules (a declarative form of knowledge) has been hypothesized for a given data set (GDT). <p> GENEVE operators generate events, facts or examples that satisfy given rules, select the most representative events from a given set [ML78], determine examples that are similar to a given example <ref> [CM89] </ref>, or predict the value of a given variable using an expert system shell or a decision structure. ANALYZE operators analyze various relationships that exist in the data, e.g., determining the degree of similarity between two examples, checking if there is an implicative relationship between two variables, etc.
Reference: [DW80] <author> Daniel, C. and Wood, </author> <title> F.S. Fitting Equations to Data. </title> <address> New York, </address> <publisher> John Wiley & Sons, </publisher> <year> 1980. </year>
Reference-contexts: Data analysis techniques that have been traditionally used for such tasks include regression analysis, cluster analysis, numerical taxonomy, multidimensional analysis, other multivariate statistical methods, stochastic models, time series analysis, nonlinear estimation techniques, and others (e.g., <ref> [DW80] </ref>, [Tuk86], [MT89], [Did89], and [Sha96]). These techniques have been widely used for solving many practical problems. They are, however, primarily oriented toward the extraction of quantitative and statistical data characteristics, and as such have inherent limitations.
Reference: [Dav81] <author> Davis, J. CONVART: </author> <title> A Program for Constructive Induction on Time-Dependent Data. M.S. </title> <type> Thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1981. </year> <title> DATA MINING AND KNOWLEDGE DISCOVERY 3 7 </title>
Reference-contexts: Qualitative prediction, i.e., discovering patterns in sequences or processes and using these patterns to qualitatively predict the possible continuation of the given sequences or processes (e.g., <ref> [Dav81] </ref>, [MKC85], [MKC86], [DM86]). MICHALSKI & KAUFMAN8 Each of these problems is relevant to the derivation of useful knowledge from a collection of data (static or dynamic). <p> Among the novel ideas that could be applied for analyzing such timedependent data is a multi-model method for qualitative prediction [DM86], [MKC85], [MKC86]. Another novel idea is a temporal constructive induction technique that can generate new attributes that are designed to capture time dependent patterns <ref> [Dav81] </ref>, [BM96]. Example selection: The problem is to select rows from the table that correspond to the most representative examples of different classes. When a datatable is very large, is it important to concentrate the analysis on a representative sample. <p> Inherent in a timestamped representation are many attributes that can be generated through constructive induction, for example, date of the highest temperature, the minimum population growth rate during some period, weediness on date of planting, etc. CONVART <ref> [Dav81] </ref> uses user-provided and default system suggestions to search for useful timedependent attributes that are added to the representation space. It uses the items on the suggestion list to generate new attributes and to test them for likely relevance to the problem.
Reference: [Did89] <editor> Diday, E. (ed.) </editor> <booktitle> Proceedings of the Conference on Data Analysis, Learning Symbolic and Numeric Knowledge. </booktitle> <publisher> Nova Science Publishers, Inc., </publisher> <address> Antibes, </address> <year> 1989. </year>
Reference-contexts: Data analysis techniques that have been traditionally used for such tasks include regression analysis, cluster analysis, numerical taxonomy, multidimensional analysis, other multivariate statistical methods, stochastic models, time series analysis, nonlinear estimation techniques, and others (e.g., [DW80], [Tuk86], [MT89], <ref> [Did89] </ref>, and [Sha96]). These techniques have been widely used for solving many practical problems. They are, however, primarily oriented toward the extraction of quantitative and statistical data characteristics, and as such have inherent limitations. For example, a statistical analysis can determine covariances and correlations between variables in data.
Reference: [DM86] <author> Dieterrich, T. and Michalski, </author> <title> R.S. Learning to Predict Sequences. </title> <booktitle> Chapter in Machine Learning: An Artificial Intelligence Approach Vol. </booktitle> <volume> 2. </volume> <editor> Michalski, R.S., Carbonell, J.G. and Mitchell, T.M. (eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 63-106, </pages> <year> 1986. </year>
Reference-contexts: Qualitative prediction, i.e., discovering patterns in sequences or processes and using these patterns to qualitatively predict the possible continuation of the given sequences or processes (e.g., [Dav81], [MKC85], [MKC86], <ref> [DM86] </ref>). MICHALSKI & KAUFMAN8 Each of these problems is relevant to the derivation of useful knowledge from a collection of data (static or dynamic). <p> But is there a consistent pattern? To determine such a pattern, one can employ different descriptive models, and instantiate the models to fit the particular sequence. The instantiated model that best fits the data is then used for prediction. Such a method is described in <ref> [DM86] </ref>. The method employs three descriptive modelsperiodic, decomposition and DNF. The periodic model is used to detect repeating patterns in a sequence. For example, general, there can be also periodic sequences within the periodic sequences. <p> Determining timedependent patterns: This problem concerns the detection of temporal patterns in sequences of data arranged along the time dimension in a GDT (Figure 2.5). Among the novel ideas that could be applied for analyzing such timedependent data is a multi-model method for qualitative prediction <ref> [DM86] </ref>, [MKC85], [MKC86]. Another novel idea is a temporal constructive induction technique that can generate new attributes that are designed to capture time dependent patterns [Dav81], [BM96]. Example selection: The problem is to select rows from the table that correspond to the most representative examples of different classes.
Reference: [Don88] <author> Dontas, K. APPLAUSE: </author> <title> An Implementation of the Collins-Michalski Theory of Plausible Reasoning. M.S. </title> <type> Thesis, </type> <institution> Computer Science Department, The University of Tennessee, Knoxville, TN, </institution> <year> 1988. </year>
Reference-contexts: These problems are important to learning from complex real-world observations, where there is always some amount of noise. Learning from incomplete data, i.e., learning from examples in which the values of some attributes are unknown (e.g., <ref> [Don88] </ref>, [LHGS96]). Learning from distributed data, i.e., learning from separate collections of data that must be brought together if the patterns within them are to be exposed (e.g., [RKK95]). <p> An interesting approach to this problem is to apply a multi-line reasoning, based the core theory of human plausible reasoning [CM81], <ref> [Don88] </ref>, [CM89]. Determining decision structures from declarative knowledge (decision rules): Suppose that a set of general decision rules (a declarative form of knowledge) has been hypothesized for a given data set (GDT).
Reference: [DPY93] <editor> Dubois, D., Prade, H. and Yager, R.R. (eds.). </editor> <booktitle> Readings in Fuzzy Sets and Intelligent Systems. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Learning from biased data, i.e., learning from a data set that does not reflect the actual distribution of events (e.g., [Fee96]). Learning flexible concepts, i.e., concepts that inherently lack precise definition and whose meaning is context-dependent; some ideas concerned with this topic include fuzzy sets (e.g., [Zad65], <ref> [DPY93] </ref>), two-tiered concept representations (e.g., [Mic90], [BMMZ92]), and rough sets (e.g., [Paw91], [Slo92], [Zia94]). Learning concepts at different levels of generality, i.e., learning descriptions that involve concepts from different levels of generalization hierarchies representing background knowledge (e.g., [KM96]).
Reference: [EH96] <editor> Evangelos S. and Han, J. (eds.) </editor> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining. </booktitle> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: These and related efforts have led to the emergence of a new research area, frequently called data mining and knowledge discovery, e.g., [Lbo81], [MBS82], [ZG89], [Mic91b], [Zag91], [MKKR92], [VHMT93], [FPSU96], <ref> [EH96] </ref>, [BKKPS96], and [FHS96]. The first part of this chapter is a compendium of ideas on the applicability of symbolic machine learning methods to data mining and knowledge discovery.
Reference: [FM90] <author> Falkenhainer, B.C. and Michalski, </author> <title> R.S. Integrating Quantitative and Qualitative Discovery in the ABACUS System. </title> <editor> In Kodratoff, Y. and Michalski, R.S. (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach Vol. III, </booktitle> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 153-190, </pages> <year> 1990. </year>
Reference-contexts: Integrating qualitative and quantitative discovery, i.e., determining sets of equations that fit a given set of data points, and qualitative conditions for the application of these equations (e.g., <ref> [FM90] </ref>). Qualitative prediction, i.e., discovering patterns in sequences or processes and using these patterns to qualitatively predict the possible continuation of the given sequences or processes (e.g., [Dav81], [MKC85], [MKC86], [DM86]). <p> This is an example of quantitative discovery. The application of machine learning to quantitative discovery was pioneered by the BACON system [LBS83], and then explored by many systems since, such as COPER [Kok86], FAHRENHEIT [Zyt87], and ABACUS <ref> [FM90] </ref>. Similar problems have been explored independently by Zagoruiko [Zag72] under the name of empirical prediction. Some equations may not apply directly to data, because of an inappropriate value of a constant, or different equations may apply under different qualitative conditions. <p> A ball falling through some sort of fluid will reach a terminal velocity dependent on the radius and mass of the ball and the viscosity of the fluid. A program ABACUS [Gre88], <ref> [FM90] </ref>, [Mic91a] is able to determine quantitative laws under different qualitative conditions. It partitions the data into example sets, each of which adheres to a different equation determined by a quantitative discovery module. <p> Individual branches may be assigned a set of attribute values. Leaves may be assigned a set of decisions [IM93], [Ima95]. GENEQ operators generate equations characterizing numerical data sets and qualitatively describing the conditions under which these equations apply (e.g., <ref> [FM90] </ref>). GENHIER operators build conceptual clusters or hierarchies. They are based on the program CLUSTER methodology [MSD81]. The operator in INLEN is based on the reimplementation in C of the program CLUSTER/2 [Ste84].
Reference: [FHS96] <author> Fayyad, U., Haussler, D. and Stolorz, P. </author> <title> Mining Scientific Data. </title> <journal> Communications of the ACM, </journal> <volume> 39:11, </volume> <pages> pp. 51-57, </pages> <year> 1996. </year>
Reference-contexts: These and related efforts have led to the emergence of a new research area, frequently called data mining and knowledge discovery, e.g., [Lbo81], [MBS82], [ZG89], [Mic91b], [Zag91], [MKKR92], [VHMT93], [FPSU96], [EH96], [BKKPS96], and <ref> [FHS96] </ref>. The first part of this chapter is a compendium of ideas on the applicability of symbolic machine learning methods to data mining and knowledge discovery.
Reference: [FPS96] <author> Fayyad, U., Piatetsky-Shapiro, G. and Smyth, P. </author> <title> Knowledge Discovery and Data Mining: Toward a Unifying Framework. </title> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Portland, OR, </address> <pages> pp. 82-88, </pages> <year> 1996. </year>
Reference-contexts: The INLEN methodology for intelligent data exploration directly reflects the aims of the current research on data mining and knowledge discovery. In this context, it may be useful to explain the distinction between the concepts of data mining and knowledge discovery, as proposed in <ref> [FPS96] </ref>.
Reference: [FPSU96] <editor> Fayyad, U.M. Piatetsky-Shapiro, G. Smyth, P. and Uhturusamy, R. (eds.) </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <address> San Mateo, CA, </address> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: These and related efforts have led to the emergence of a new research area, frequently called data mining and knowledge discovery, e.g., [Lbo81], [MBS82], [ZG89], [Mic91b], [Zag91], [MKKR92], [VHMT93], <ref> [FPSU96] </ref>, [EH96], [BKKPS96], and [FHS96]. The first part of this chapter is a compendium of ideas on the applicability of symbolic machine learning methods to data mining and knowledge discovery.
Reference: [Fee96] <author> Feelders, A. </author> <title> Learning from Biased Data Using Mixture Models. </title> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Portland, OR, </address> <pages> pp. 102-107, </pages> <year> 1996. </year>
Reference-contexts: Learning from biased data, i.e., learning from a data set that does not reflect the actual distribution of events (e.g., <ref> [Fee96] </ref>). Learning flexible concepts, i.e., concepts that inherently lack precise definition and whose meaning is context-dependent; some ideas concerned with this topic include fuzzy sets (e.g., [Zad65], [DPY93]), two-tiered concept representations (e.g., [Mic90], [BMMZ92]), and rough sets (e.g., [Paw91], [Slo92], [Zia94]).
Reference: [FR86] <author> Forsyth, R. and Rada, R. </author> <title> Machine Learning: </title> <booktitle> Applications in Expert Systems and Information Retrieval. </booktitle> <publisher> Pittman, </publisher> <year> 1986. </year>
Reference-contexts: Methods for performing the above operations on data tables have been implemented in various machine learning programs (e.g., [MCM83], [MCM86], <ref> [FR86] </ref>, [Kod88], and [KM90]).
Reference: [Gre88] <author> Greene, G. </author> <title> The Abacus.2 System for Quantitative Discovery: Using Dependencies to Discover NonLinear Terms. Reports of the Machine Learning and Inference Laboratory, MLI 88-4, Machine Learning and Inference Laboratory, </title> <institution> George Mason University, Fairfax, VA, </institution> <year> 1988. </year>
Reference-contexts: A ball falling through some sort of fluid will reach a terminal velocity dependent on the radius and mass of the ball and the viscosity of the fluid. A program ABACUS <ref> [Gre88] </ref>, [FM90], [Mic91a] is able to determine quantitative laws under different qualitative conditions. It partitions the data into example sets, each of which adheres to a different equation determined by a quantitative discovery module.
Reference: [Har84] <author> Hart, A. </author> <title> Experience in the Use of an Inductive System in Knowledge Engineering. </title> <editor> In M Bramer (Ed.), </editor> <booktitle> Research and Developments in Expert Systems, </booktitle> <address> Cambridge, </address> <publisher> Cambridge University Press, </publisher> <year> 1984. </year>
Reference-contexts: To this end, one may use many different criteria for evaluating the relevance of an attribute for a given classification problem, such as gain ratio [Qui93]; gini index [BFOS84], PROMISE [Bai82], and chisquare analysis <ref> [Har84] </ref>, [Min89]. These criteria evaluate attributes on the basis of their expected global performance, which means that attributes with the highest ability to discriminate among all classes are selected as most relevant. When determining a declarative knowledge representation, such as decision rules, the goal is somewhat different.
Reference: [HMM86] <author> Hong, J., Mozetic, I. and Michalski, </author> <title> R.S. AQ15: Incremental Learning of Attribute-Based Descriptions from Examples: The Method and Users Guide. </title> <booktitle> MICHALSKI & KAUFMAN3 8 Reports of the Intelligent Systems Group, </booktitle> <address> ISG 86-5, UIUCDCS-F-86-949, </address> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1986. </year>
Reference-contexts: Each of these programs is directly applicable to conceptual data exploration. For example, the rules in Figure 2.1 were generated by the AQ15 rule module [MMHL86], <ref> [HMM86] </ref> from a set of positive and negative examples of Class 1 of robot-figures. AQ15 learns attributional descriptions of entities, i.e., descriptions involving only their attributes. More general descriptions, structural or relational., also involve relationships among components of the entities, the attributes of the components, and quantifiers. <p> To do so, an incremental learning program must be applied to synthesize the prior knowledge with the new information. The incremental learning process may be full-memory, partial-memory, or no-memory, depending on how much of the original training data is maintained in the incremental learning process <ref> [HMM86] </ref>, [RM88], [MM95]. Searching for approximate patterns in (imperfect) data: For some GDTs, it may not be possible (or useful) to find complete and consistent descriptions. In such cases, it is important to determine patterns that hold for a large number of cases, but not necessarily for all.
Reference: [HMS66] <author> Hunt, E., Marin, J. and Stone, P. </author> <title> Experiments in Induction. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1966. </year>
Reference-contexts: In the conventional machine learning approach, decision trees are learned directly from training examples, thus avoiding the step of first creating rules <ref> [HMS66] </ref>, [Qui86], [Qui93]. Learning a decision tree directly from examples, however, may have serious disadvantages in practice. A decision tree is a form of procedural knowledge. Once it has been constructed, it is not easy to modify it to accommodate changes in the decision-making conditions.
Reference: [Ima95] <author> Imam, </author> <title> I.F. Discovering Task-Oriented Decision Structures from Decision Rules. </title> <type> Ph.D. dissertation, </type> <institution> School of Information Technology and Engineering, George Mason University, Fairfax, VA, </institution> <year> 1995. </year>
Reference-contexts: Such rules can be often simplified by detecting superfluous conditions in them (e.g., [Qui93]). The opposite process of transforming a ruleset into a decision tree is not so direct <ref> [Ima95] </ref>, because a rule representation is more powerful than a tree representation. The term more powerful means in this context that a decision tree representing a given ruleset may require superfluous conditions (e.g., [Mic90]). <p> A methodology for doing this and arguments for and against using such an approach (as opposed to the traditional method of learning of decision trees directly from examples) are discussed in [IM93], <ref> [Ima95] </ref>, and [MI97]. Methods for performing the above operations on data tables have been implemented in various machine learning programs (e.g., [MCM83], [MCM86], [FR86], [Kod88], and [KM90]). <p> A decision structure is a generalization of the concept of a decision tree in which nodes can be assigned an attribute or a function of attributes. Individual branches may be assigned a set of attribute values. Leaves may be assigned a set of decisions [IM93], <ref> [Ima95] </ref>. GENEQ operators generate equations characterizing numerical data sets and qualitatively describing the conditions under which these equations apply (e.g., [FM90]). GENHIER operators build conceptual clusters or hierarchies. They are based on the program CLUSTER methodology [MSD81]. <p> Another weakness of decision trees is that they may become unwieldy and incomprehensible because of their limited knowledge representational power. To overcome the above limitations, a new approach has been developed that creates task-oriented decision structures from decision rules <ref> [Ima95] </ref>, [MI97].
Reference: [IM93] <author> Imam, I.F. and Michalski, </author> <title> R.S. Should Decision Trees be Learned from Examples or From Decision Rules? Proceedings of the Seventh International Symposium on Methodologies for Intelligent Systems (ISMIS-93), </title> <address> Trondheim, Norway, </address> <year> 1993. </year>
Reference-contexts: A methodology for doing this and arguments for and against using such an approach (as opposed to the traditional method of learning of decision trees directly from examples) are discussed in <ref> [IM93] </ref>, [Ima95], and [MI97]. Methods for performing the above operations on data tables have been implemented in various machine learning programs (e.g., [MCM83], [MCM86], [FR86], [Kod88], and [KM90]). <p> DATA MINING AND KNOWLEDGE DISCOVERY 1 9 GENTREE operators build a decision structure from a given set of decision rules (e.g., <ref> [IM93] </ref>), or from examples (e.g., [Qui93]). A decision structure is a generalization of the concept of a decision tree in which nodes can be assigned an attribute or a function of attributes. Individual branches may be assigned a set of attribute values. Leaves may be assigned a set of decisions [IM93], <p> <ref> [IM93] </ref>), or from examples (e.g., [Qui93]). A decision structure is a generalization of the concept of a decision tree in which nodes can be assigned an attribute or a function of attributes. Individual branches may be assigned a set of attribute values. Leaves may be assigned a set of decisions [IM93], [Ima95]. GENEQ operators generate equations characterizing numerical data sets and qualitatively describing the conditions under which these equations apply (e.g., [FM90]). GENHIER operators build conceptual clusters or hierarchies. They are based on the program CLUSTER methodology [MSD81]. <p> a predictive accuracy of 91.8% (when AQDT built an equivalent decision structure by combining some branches, the number of leaves was reduced to 8), while the decision tree learned by C4.5 from the same set of training examples had 8 nodes and 15 leaves, with a predictive accuracy of 85.7% <ref> [IM93] </ref>. This methodology directly fits the philosophy of INLEN.
Reference: [Kau94] <author> Kaufman, K.A. </author> <title> Comparing International Development Patterns Using Multi-Operator Learning and Discovery Tools. </title> <booktitle> Proceedings of AAAI-94 Workshop on Knowledge Discovery in Databases, </booktitle> <address> Seattle, WA, </address> <pages> pp. 431-440, </pages> <year> 1994. </year>
Reference-contexts: One experiment focused on distinguishing between development patterns in Eastern Europe and East Asia, first by identifying such patterns, and then by generating discriminant rules <ref> [Kau94] </ref>. A conceptual clustering operator determined a way of grouping the countries, based on each countrys change in the percentage of its population in the labor force between 1980 and 1990. In this classification, the typical Eastern European country and the typical East Asian country fell into separate groups.
Reference: [KM93] <author> Kaufman, K.A. and Michalski, </author> <title> R.S. EMERALD: An Integrated System of Machine Learning and Discovery Programs to Support Education and Experimental Research. Reports of the Machine Learning and Inference Laboratory, MLI 93-10, Machine Learning and Inference Laboratory, </title> <institution> George Mason University, Fairfax, VA, </institution> <year> 1993. </year>
Reference-contexts: To illustrate this point, Figure 2.1 gives an example of a disjunctive description of a class of robot-figures in EMERALD (a large system for demonstrating machine learning and discovery capabilities <ref> [KM93] </ref>). <p> The EMERALD system, mentioned above, combines five programs that display different kinds of learning capabilities <ref> [KM93] </ref>. These capabilities include rule learning from examples (using program AQ15), learning distinctions between structures (INDUCE), conceptual clustering (CLUSTER/2), prediction of object sequences (SPARC), and derivation of equations and rules characterizing data about physical processes (ABACUS). Each of these programs is directly applicable to conceptual data exploration.
Reference: [KM96] <author> Kaufman, K.A. and Michalski, </author> <title> R.S. A Method for Reasoning with Structured and Continuous Attributes in the INLEN-2 Multistrategy Knowledge Discovery System. </title> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Portland, OR, </address> <pages> pp. 232-237, </pages> <year> 1996. </year>
Reference-contexts: Learning concepts at different levels of generality, i.e., learning descriptions that involve concepts from different levels of generalization hierarchies representing background knowledge (e.g., <ref> [KM96] </ref>). Integrating qualitative and quantitative discovery, i.e., determining sets of equations that fit a given set of data points, and qualitative conditions for the application of these equations (e.g., [FM90]). <p> The type defines the ordering (if any) of the values in the domain. For example, the AQ15 learning program [MMHL86] allows four types of attributes: nominal (no order), linear (total order), cyclic (cyclic total order), and structured (hierarchical order; see <ref> [KM96] </ref>). The attribute type determines the kinds of operations that are allowed on this attributes values during a learning process. Entries in each row are values of the attributes for the entity associated with the row. Typically, each row corresponds to a single entity. <p> TRANSFORM operators perform various transformations on the knowledge segments, e.g., generalization or specialization, abstraction or concretion, optimization of given rules, etc. according to user-provided criteria. For instance, one such operator climbs an attributes generalization hierarchy to build more general decision rules <ref> [KM96] </ref>. GENATR operators generate new attribute sets by creating new attributes [BM96], selecting the most representative attributes from the original set [Bai82], or by abstracting attributes [Ker92]. <p> To provide some mechanism for capturing such preferences, INLEN allows a user to define anchor nodes in a generalization hierarchy. Such nodes should reflect the interests of a given application <ref> [KM96] </ref>. To illustrate this idea, consider Figure 2.10 again. In this hierarchy, vanilla and rocky road are kinds of ice cream; ice cream is a frozen dessert, which is a dessert, which is a type of food. <p> This attribute was presented as a nominal attribute in the initial dataset. In order to examine how the structuring of attributes affects knowledge discovery, INLEN was applied to identical data sets with and without the Religion attribute being structured <ref> [KM96] </ref>. A portion of the attribute domain structure is shown in Figure 2.12.
Reference: [KMK91] <author> Kaufman, K.A., Michalski, R.S., and Kerschberg, L. </author> <title> Mining for Knowledge in Databases: Goals and General Description of the INLEN System. </title> <editor> In Piatetsky-Shapiro, G. and Frawley, W.J. (eds.), </editor> <title> Knowledge Discovery in Databases, </title> <publisher> AAAI press, </publisher> <address> Menlo Park, CA, </address> <pages> pp. 449-462, </pages> <year> 1991. </year>
Reference-contexts: This idea underlies the INLEN system <ref> [KMK91] </ref>, [MKKR92]. The name INLEN is derived from inference and learning. The system integrates machine learning programs, statistical data analysis tools, a database, a knowledge base, inference procedures, and various supporting programs under a unified architecture and graphical interface. <p> The diagrammatic visualization method, DIAV [Wne95] is used for displaying the effects of symbolic learning operations on data. The KGOs are the heart of the INLEN system. To facilitate their use, the concept of a knowledge segment was introduced <ref> [KMK91] </ref>. A knowledge segment is a structure that links one or more relational tables from the database with one or more structures from the knowledge base.
Reference: [Ker92] <author> Kerber, R. ChiMerge: </author> <title> Discretization of Numeric Attributes. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI-92), </booktitle> <address> San Jose, CA, </address> <pages> pp. 123-127, </pages> <year> 1992. </year>
Reference-contexts: For instance, one such operator climbs an attributes generalization hierarchy to build more general decision rules [KM96]. GENATR operators generate new attribute sets by creating new attributes [BM96], selecting the most representative attributes from the original set [Bai82], or by abstracting attributes <ref> [Ker92] </ref>. GENEVE operators generate events, facts or examples that satisfy given rules, select the most representative events from a given set [ML78], determine examples that are similar to a given example [CM89], or predict the value of a given variable using an expert system shell or a decision structure.
Reference: [Kod88] <author> Kodratoff, Y. </author> <title> Introduction to Machine Learning. </title> <publisher> Pittman, </publisher> <year> 1988. </year>
Reference-contexts: Methods for performing the above operations on data tables have been implemented in various machine learning programs (e.g., [MCM83], [MCM86], [FR86], <ref> [Kod88] </ref>, and [KM90]).
Reference: [Kok86] <author> Kokar, </author> <title> M.M. Coper: A Methodology for Learning Invariant Functional Descriptions. Chapter in Machine Learning: A Guide to Current Research, </title> <editor> Michalski, R.S., Mitchell, T.M and Carbonell, J.G. (Eds.), </editor> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA 1986. </address>
Reference-contexts: This is an example of quantitative discovery. The application of machine learning to quantitative discovery was pioneered by the BACON system [LBS83], and then explored by many systems since, such as COPER <ref> [Kok86] </ref>, FAHRENHEIT [Zyt87], and ABACUS [FM90]. Similar problems have been explored independently by Zagoruiko [Zag72] under the name of empirical prediction. Some equations may not apply directly to data, because of an inappropriate value of a constant, or different equations may apply under different qualitative conditions.
Reference: [KM90] <editor> Kodratoff, Y. and Michalski, R.S. (eds.) </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Vol. III. </booktitle> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann., </publisher> <year> 1990 </year>
Reference-contexts: Methods for performing the above operations on data tables have been implemented in various machine learning programs (e.g., [MCM83], [MCM86], [FR86], [Kod88], and <ref> [KM90] </ref>).
Reference: [LHGS96] <author> Lakshminarayan, K., Harp, S.A., Goldman, R. and Samad, T. </author> <title> Imputation of Missing Data Using Machine Learning Techniques. </title> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining. </booktitle> <address> Portland, OR, </address> <pages> pp. 140-145, </pages> <year> 1996. </year> <title> DATA MINING AND KNOWLEDGE DISCOVERY 3 9 </title>
Reference-contexts: These problems are important to learning from complex real-world observations, where there is always some amount of noise. Learning from incomplete data, i.e., learning from examples in which the values of some attributes are unknown (e.g., [Don88], <ref> [LHGS96] </ref>). Learning from distributed data, i.e., learning from separate collections of data that must be brought together if the patterns within them are to be exposed (e.g., [RKK95]).
Reference: [LBS83] <author> Langley, P., Bradshaw G.L. and Simon, H.A. </author> <title> Rediscovering Chemistry with the BACON System. Machine Learning: An Artificial Intelligence Approach, </title> <editor> Michalski, R.S., Carbonell, J.G. and Mitchell, T.M. (Eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> pp. 307-329, </pages> <year> 1983. </year>
Reference-contexts: This is an example of quantitative discovery. The application of machine learning to quantitative discovery was pioneered by the BACON system <ref> [LBS83] </ref>, and then explored by many systems since, such as COPER [Kok86], FAHRENHEIT [Zyt87], and ABACUS [FM90]. Similar problems have been explored independently by Zagoruiko [Zag72] under the name of empirical prediction.
Reference: [Lar77] <author> Larson, J.B. INDUCE-1: </author> <title> An Interactive Inductive Inference Program in VL21 Logic System. </title> <type> Report No. 876, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1977. </year>
Reference-contexts: AQ15 learns attributional descriptions of entities, i.e., descriptions involving only their attributes. More general descriptions, structural or relational., also involve relationships among components of the entities, the attributes of the components, and quantifiers. Such descriptions are produced, for example, by the INDUCE module of EMERALD <ref> [Lar77] </ref>, [BMR87]. Constructing structural descriptions requires a more complex description DATA MINING AND KNOWLEDGE DISCOVERY 5 language that includes multi-argument predicates, for example, PROLOG, or Annotated Predicate Calculus [Mic83], [BMK97].
Reference: [Lbo81] <author> Lbov, G.S. </author> <title> Mietody Obrabotki Raznotipnych Ezperimentalnych Danych (Methods for Analysis of Multitype Experimental Data). </title> <institution> Akademia Nauk USSR, Sibirskoje Otdielenie, Institut Matiematikie, Izdatielstwo Nauka, Novosibirsk, </institution> <year> 1981. </year>
Reference-contexts: These and related efforts have led to the emergence of a new research area, frequently called data mining and knowledge discovery, e.g., <ref> [Lbo81] </ref>, [MBS82], [ZG89], [Mic91b], [Zag91], [MKKR92], [VHMT93], [FPSU96], [EH96], [BKKPS96], and [FHS96]. The first part of this chapter is a compendium of ideas on the applicability of symbolic machine learning methods to data mining and knowledge discovery.
Reference: [MM95] <author> Maloof, M.A. and Michalski, </author> <title> R.S. Learning Evolving Concepts Using Partial Memory Approach. </title> <booktitle> Working Notes of the 1995 AAAI Fall Symposium on Active Learning, </booktitle> <address> Boston, MA, </address> <pages> pp. 70-73, </pages> <year> 1995. </year>
Reference-contexts: For example, the area of interest of a user is often an evolving concept (e.g., [WK96]). Learning concepts from data arriving over time, i.e., incremental learning in which currently held hypotheses characterizing concepts may need to be updated to account for the new data (e.g., <ref> [MM95] </ref>). Learning from biased data, i.e., learning from a data set that does not reflect the actual distribution of events (e.g., [Fee96]). <p> To do so, an incremental learning program must be applied to synthesize the prior knowledge with the new information. The incremental learning process may be full-memory, partial-memory, or no-memory, depending on how much of the original training data is maintained in the incremental learning process [HMM86], [RM88], <ref> [MM95] </ref>. Searching for approximate patterns in (imperfect) data: For some GDTs, it may not be possible (or useful) to find complete and consistent descriptions. In such cases, it is important to determine patterns that hold for a large number of cases, but not necessarily for all.
Reference: [Mic91a] <author> Michael, J. </author> <title> Validation, Verification and Experimentation with Abacus2. Reports of the Machine Learning and Inference Laboratory, MLI 91-8, Machine Learning and Inference Laboratory, </title> <institution> George Mason University, Fairfax, VA, </institution> <year> 1991. </year>
Reference-contexts: A ball falling through some sort of fluid will reach a terminal velocity dependent on the radius and mass of the ball and the viscosity of the fluid. A program ABACUS [Gre88], [FM90], <ref> [Mic91a] </ref> is able to determine quantitative laws under different qualitative conditions. It partitions the data into example sets, each of which adheres to a different equation determined by a quantitative discovery module.
Reference: [Mic78] <author> Michalski, </author> <title> R.S. A Planar Geometrical Model for Representing MultiDimensional Discrete Spaces and Multiple-Valued Logic Functions. ISG Report No. </title> <type> 897, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1978. </year>
Reference-contexts: Attributional descriptions can be visualized by mapping them into a planar representation of a discrete multidimensional space (a diagram) spanned over the given attributes <ref> [Mic78] </ref>, [WSWM90]. For example, Figure 2.2 shows a diagrammatic visualization of the rules from DIAV [WSWM90], [Wne95]. Each cell in the diagram represents one specific combination of values of the attributes. <p> The four shaded areas marked Class1 (A) represent rule A, and the shaded area marked Class 1 (B) represents rule B. In such a diagram, conjunctive rules correspond to certain regular arrangements of cells that are easy to recognize <ref> [Mic78] </ref>. MICHALSKI & KAUFMAN6 The diagrammatic visualization can be used for displaying the target concept (the concept to be learned), the training examples (the examples and counterexamples of the concept), and the actual concept learned by a method. <p> A learning process that consists of two (intertwined) phases, one concerned with the construction of the best representation space, and the second concerned with generating the best hypothesis in the found space is called constructive induction <ref> [Mic78] </ref>, [Mic83], [WM94]. An example of a constructive induction program is AQ17 [BWM93], which performs all three types of improvements of the original representation space. <p> To this end, INLEN supports the visualization of data and knowledge through the diagrammatic visualization method implemented in the DIAV program <ref> [Mic78] </ref>, [Wne95]. clustering operator. Let us illustrate the method with the hard disk classification problem presented in the previous section.
Reference: [Mic80] <author> Michalski, </author> <title> R.S. Inductive Learning as Rule-Guided Generalization and Conceptual Simplification of Symbolic Descriptions: Unifying Principles and a Methodology. </title> <booktitle> Workshop on Current Developments in Machine Learning, </booktitle> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1980. </year>
Reference-contexts: Such attributes have value sets ordered into hierarchies <ref> [Mic80] </ref>. In order to take advantage of the properties of structured attributes in executing inductive learning, new inductive generalization rules have been defined. An inductive generalization rule (or transmutation) takes an input statement and relevant background knowledge, and hypothesizes a more general statement [Mic80], [Mic83], [Mic94]. <p> attributes have value sets ordered into hierarchies <ref> [Mic80] </ref>. In order to take advantage of the properties of structured attributes in executing inductive learning, new inductive generalization rules have been defined. An inductive generalization rule (or transmutation) takes an input statement and relevant background knowledge, and hypothesizes a more general statement [Mic80], [Mic83], [Mic94]. For example, removing a condition from the premise of a decision rule is a generalization transmutation (this is called a dropping condition generalization rule), since if the premise has fewer conditions, a larger set of instances can satisfy it.
Reference: [Mic83] <author> Michalski, </author> <title> R.S. A Theory and Methodology of Inductive Learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> pp. 111-161, </pages> <year> 1983. </year>
Reference-contexts: Such descriptions are produced, for example, by the INDUCE module of EMERALD [Lar77], [BMR87]. Constructing structural descriptions requires a more complex description DATA MINING AND KNOWLEDGE DISCOVERY 5 language that includes multi-argument predicates, for example, PROLOG, or Annotated Predicate Calculus <ref> [Mic83] </ref>, [BMK97]. For database exploration, attributional descriptions appear to be the most important and the easiest to implement, because typical databases characterize entities in terms of attributes, not relations. One simple and popular form of attributional description is a decision or classification tree. <p> In some methods, BK may include more information, e.g., constraints on the interrelationship between various attributes, rules for generating higher level concepts, new attributes, as well as some initial hypothesis <ref> [Mic83] </ref>. Learned rules are usually consistent and complete with regard to the input data. This means that they completely and correctly classify all the original training examples. Sections 2.5 and 2.8 present consistent and complete example solutions from the inductive concept learning program AQ15c [WKBM95]. <p> Such differences are expressed in the form of rules that define properties that characterize one group but not the other. These operators are based on programs for learning discriminant concept descriptions. Section 2.5 will illustrate these two types of descriptions. For more details and their definitions see <ref> [Mic83] </ref>. Basic methods for concept learning assume that examples do not have errors, that all attributes have a specified value in them, that all examples are located in the same database, and that concepts to be learned have a precise (crisp) description that does not change over time. <p> A learning process that consists of two (intertwined) phases, one concerned with the construction of the best representation space, and the second concerned with generating the best hypothesis in the found space is called constructive induction [Mic78], <ref> [Mic83] </ref>, [WM94]. An example of a constructive induction program is AQ17 [BWM93], which performs all three types of improvements of the original representation space. <p> Each rule shows all characteristics common to a given category, that is, it represents a characteristic description of a category <ref> [Mic83] </ref>. (Note that some of the conditions in these rules appear to be redundant. For example, the last condition of the Class 2 rule says that Loaners is yes or no. <p> To create a description that points out the most significant distinctions, one needs to apply the operator that creates discriminant descriptions <ref> [Mic83] </ref>. The operator (GENRULE) is applied to the extended data table in Figure 2.7, using the Group column as its output attribute. <p> In order to take advantage of the properties of structured attributes in executing inductive learning, new inductive generalization rules have been defined. An inductive generalization rule (or transmutation) takes an input statement and relevant background knowledge, and hypothesizes a more general statement [Mic80], <ref> [Mic83] </ref>, [Mic94]. For example, removing a condition from the premise of a decision rule is a generalization transmutation (this is called a dropping condition generalization rule), since if the premise has fewer conditions, a larger set of instances can satisfy it. <p> any additional conditions), then the extension of R1 against R 2 along dimension x i C R1 | R2 /x i produces a new rule R3: [x i B e], which is a consistent generalization of R1, that is, a generalization that does not intersect logically with R 2 [MM71], <ref> [Mic83] </ref>. The value of the parameter e controls the degree of generalization. If e is (the empty set), then R 3 is the maximal consistent generalization of R1.
Reference: [Mic90] <author> Michalski, </author> <title> R.S. Learning Flexible Concepts: Fundamental Ideas and a Method Based on Two-tiered Representation. </title> <editor> In Kodratoff, Y. and Michalski, R.S. (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Vol. III, </booktitle> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 63-102, </pages> <year> 1990. </year>
Reference-contexts: The opposite process of transforming a ruleset into a decision tree is not so direct [Ima95], because a rule representation is more powerful than a tree representation. The term more powerful means in this context that a decision tree representing a given ruleset may require superfluous conditions (e.g., <ref> [Mic90] </ref>). The input to an attributional learning program consists of a set of examples of individual classes and background knowledge (BK) relevant to the given learning problem. The examples (cases of decisions) are in the form of vectors of attribute-value pairs associated with some decision class. <p> This means that they completely and correctly classify all the original training examples. Sections 2.5 and 2.8 present consistent and complete example solutions from the inductive concept learning program AQ15c [WKBM95]. In some applications, especially those involving learning rules from noisy data or learning flexible concepts <ref> [Mic90] </ref>, it is may be advantageous to learn descriptions that are incomplete and/or inconsistent [BMMZ92]. Attributional descriptions can be visualized by mapping them into a planar representation of a discrete multidimensional space (a diagram) spanned over the given attributes [Mic78], [WSWM90]. <p> Learning flexible concepts, i.e., concepts that inherently lack precise definition and whose meaning is context-dependent; some ideas concerned with this topic include fuzzy sets (e.g., [Zad65], [DPY93]), two-tiered concept representations (e.g., <ref> [Mic90] </ref>, [BMMZ92]), and rough sets (e.g., [Paw91], [Slo92], [Zia94]). Learning concepts at different levels of generality, i.e., learning descriptions that involve concepts from different levels of generalization hierarchies representing background knowledge (e.g., [KM96]).
Reference: [Mic91b] <author> Michalski, </author> <title> R.S. Searching for Knowledge in a World Flooded with Facts. Applied Stochastic Models and Data Analysis, </title> <journal> Vol. </journal> <volume> 7, </volume> <pages> pp. </pages> <address> 153-l63, </address> <month> January, </month> <year> 1991. </year>
Reference-contexts: These and related efforts have led to the emergence of a new research area, frequently called data mining and knowledge discovery, e.g., [Lbo81], [MBS82], [ZG89], <ref> [Mic91b] </ref>, [Zag91], [MKKR92], [VHMT93], [FPSU96], [EH96], [BKKPS96], and [FHS96]. The first part of this chapter is a compendium of ideas on the applicability of symbolic machine learning methods to data mining and knowledge discovery.
Reference: [Mic94] <author> Michalski, </author> <title> R.S. Inferential Theory of Learning: Developing Foundations for Multistrategy Learning. </title> <editor> In Michalski, R.S. and Tecuci, G. (eds.), </editor> <title> Machine Learning: A Multistrategy Approach, </title> <address> San Francisco, </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 3-61, </pages> <year> 1994. </year>
Reference-contexts: In order to take advantage of the properties of structured attributes in executing inductive learning, new inductive generalization rules have been defined. An inductive generalization rule (or transmutation) takes an input statement and relevant background knowledge, and hypothesizes a more general statement [Mic80], [Mic83], <ref> [Mic94] </ref>. For example, removing a condition from the premise of a decision rule is a generalization transmutation (this is called a dropping condition generalization rule), since if the premise has fewer conditions, a larger set of instances can satisfy it.
Reference: [MBS82] <author> Michalski, </author> <title> R.S., Baskin, A.B. and Spackman, K.A. A Logic-based Approach to Conceptual Database Analysis. </title> <booktitle> Sixth Annual Symposium on Computer Applications in Medical Care (SCAMC-6), </booktitle> <institution> George Washington University, Medical Center, </institution> <address> Washington, DC, </address> <pages> pp. 792-796, </pages> <year> 1982. </year> <note> MICHALSKI & KAUFMAN4 0 </note>
Reference-contexts: These and related efforts have led to the emergence of a new research area, frequently called data mining and knowledge discovery, e.g., [Lbo81], <ref> [MBS82] </ref>, [ZG89], [Mic91b], [Zag91], [MKKR92], [VHMT93], [FPSU96], [EH96], [BKKPS96], and [FHS96]. The first part of this chapter is a compendium of ideas on the applicability of symbolic machine learning methods to data mining and knowledge discovery.
Reference: [MCM83] <editor> Michalski, R.S., Carbonell, J.G. and Mitchell, T.M. (eds.) </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Palo Alto, CA, </address> <publisher> Tioga Publishing, </publisher> <year> 1983. </year>
Reference-contexts: Methods for performing the above operations on data tables have been implemented in various machine learning programs (e.g., <ref> [MCM83] </ref>, [MCM86], [FR86], [Kod88], and [KM90]).
Reference: [MCM86] <editor> Michalski, R.S., Carbonell, J.G. and Mitchell, T.M. (eds.) </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach Vol. 2. </booktitle> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: Methods for performing the above operations on data tables have been implemented in various machine learning programs (e.g., [MCM83], <ref> [MCM86] </ref>, [FR86], [Kod88], and [KM90]).
Reference: [MI97] <author> Michalski, </author> <title> R.S. and Imam, I.F. On Learning Decision Structures. </title> <journal> Fundamenta Matematicae, Polish Academy of Sciences, </journal> <note> 1997 (in press). </note>
Reference-contexts: A methodology for doing this and arguments for and against using such an approach (as opposed to the traditional method of learning of decision trees directly from examples) are discussed in [IM93], [Ima95], and <ref> [MI97] </ref>. Methods for performing the above operations on data tables have been implemented in various machine learning programs (e.g., [MCM83], [MCM86], [FR86], [Kod88], and [KM90]). <p> Another weakness of decision trees is that they may become unwieldy and incomprehensible because of their limited knowledge representational power. To overcome the above limitations, a new approach has been developed that creates task-oriented decision structures from decision rules [Ima95], <ref> [MI97] </ref>. <p> by AQDT-2 for a wind bracing design problem had 5 nodes and 9 leaves, with a predictive accuracy of 88.7% when tested against a new set of data, while the decision tree generated by the popular program C4.5 had 17 nodes and 47 leaves with a predictive accuracy of 84% <ref> [MI97] </ref>.
Reference: [MKW91] <author> Michalski, R.S., Kaufman, K. and Wnek, J. </author> <title> The AQ Family of Learning Programs: A Review of Recent Developments and an Exemplary Application. Reports of the Machine Learning and Inference Laboratory, MLI 91-11, Machine Learning and Inference Laboratory, </title> <institution> George Mason University, Fairfax, VA, </institution> <year> 1991. </year>
Reference-contexts: In many situations one or more of these assumptions may not hold. This leads to a variety of more complex machine learning and data mining problems: Learning from incorrect data, i.e., learning from examples that contain a certain amount of errors or noise (e.g., [Qui90], <ref> [MKW91] </ref>). These problems are important to learning from complex real-world observations, where there is always some amount of noise. Learning from incomplete data, i.e., learning from examples in which the values of some attributes are unknown (e.g., [Don88], [LHGS96]).
Reference: [MKKR92] <author> Michalski, R.S., Kerschberg, L., Kaufman, K. and Ribeiro, J. </author> <title> Mining for Knowledge in Databases: The INLEN Architecture, Initial Implementation and First Results. </title> <journal> Journal of Intelligent Information Systems: Integrating AI and Database Technologies, </journal> <volume> 1, </volume> <pages> pp. 85-113, </pages> <year> 1992. </year>
Reference-contexts: These and related efforts have led to the emergence of a new research area, frequently called data mining and knowledge discovery, e.g., [Lbo81], [MBS82], [ZG89], [Mic91b], [Zag91], <ref> [MKKR92] </ref>, [VHMT93], [FPSU96], [EH96], [BKKPS96], and [FHS96]. The first part of this chapter is a compendium of ideas on the applicability of symbolic machine learning methods to data mining and knowledge discovery. <p> The main constraint is that these descriptions should be easy to understand and interpret by an expert in the given domain, i.e., they should satisfy the principle of comprehensibility [Mic93]. Our first efforts in developing a methodology for multistrategy data exploration have been implemented in the system INLEN <ref> [MKKR92] </ref>. The system combines a range of machine learning methods and tools with more traditional data analysis techniques. These tools provide a user with the capability to make different kinds of data explorations and to derive different kinds of knowledge from a database. <p> This idea underlies the INLEN system [KMK91], <ref> [MKKR92] </ref>. The name INLEN is derived from inference and learning. The system integrates machine learning programs, statistical data analysis tools, a database, a knowledge base, inference procedures, and various supporting programs under a unified architecture and graphical interface.
Reference: [MKC85] <author> Michalski, R. S., Ko H. and Chen, K. SPARC/E(V.2), </author> <title> An Eleusis Rule Generator and Game Player. </title> <journal> Reports of the Intelligent Systems Group, </journal> <volume> ISG No. </volume> <pages> 85-11, </pages> <institution> UIUCDCS-F-85-941, Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1985. </year>
Reference-contexts: Qualitative prediction, i.e., discovering patterns in sequences or processes and using these patterns to qualitatively predict the possible continuation of the given sequences or processes (e.g., [Dav81], <ref> [MKC85] </ref>, [MKC86], [DM86]). MICHALSKI & KAUFMAN8 Each of these problems is relevant to the derivation of useful knowledge from a collection of data (static or dynamic). <p> Determining timedependent patterns: This problem concerns the detection of temporal patterns in sequences of data arranged along the time dimension in a GDT (Figure 2.5). Among the novel ideas that could be applied for analyzing such timedependent data is a multi-model method for qualitative prediction [DM86], <ref> [MKC85] </ref>, [MKC86]. Another novel idea is a temporal constructive induction technique that can generate new attributes that are designed to capture time dependent patterns [Dav81], [BM96]. Example selection: The problem is to select rows from the table that correspond to the most representative examples of different classes.
Reference: [MKC86] <author> Michalski, R.S., Ko, H. and Chen, K. </author> <title> Qualitative Prediction: A Method and a Program SPARC/G. Chapter in Expert Systems, </title> <editor> Guetler, C. (ed.), </editor> <publisher> Academic Press, Inc., </publisher> <address> London, </address> <year> 1986. </year>
Reference-contexts: Qualitative prediction, i.e., discovering patterns in sequences or processes and using these patterns to qualitatively predict the possible continuation of the given sequences or processes (e.g., [Dav81], [MKC85], <ref> [MKC86] </ref>, [DM86]). MICHALSKI & KAUFMAN8 Each of these problems is relevant to the derivation of useful knowledge from a collection of data (static or dynamic). <p> The third model, the DNF (disjunctive normal form) or catchall model, tries to capture general properties characterizing the whole sequence. For example, for the sequence in The program SPARC/G <ref> [MKC86] </ref> employs these three descriptive models to detect patterns in a sequence of arbitrary objects, and then uses the patterns to predict a plausible continuation for the sequence. <p> Determining timedependent patterns: This problem concerns the detection of temporal patterns in sequences of data arranged along the time dimension in a GDT (Figure 2.5). Among the novel ideas that could be applied for analyzing such timedependent data is a multi-model method for qualitative prediction [DM86], [MKC85], <ref> [MKC86] </ref>. Another novel idea is a temporal constructive induction technique that can generate new attributes that are designed to capture time dependent patterns [Dav81], [BM96]. Example selection: The problem is to select rows from the table that correspond to the most representative examples of different classes. <p> A specific operator may generate rules characterizing a set of facts, discriminating between groups of facts, characterizing a sequence of events, and determining differences between sequences, based on programs such as AQ15c [WKBM95] and SPARC/G <ref> [MKC86] </ref>. A KGO for learning rules can usually work in either incremental or batch mode.
Reference: [ML78] <author> Michalski, R.S. and Larson, J.B. </author> <title> Selection of Most Representative Training Examples and Incremental Generation of VL1 Hypotheses: The Underlying Methodology and the Description of Programs ESEL and AQ11. </title> <type> Report No. 867, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1978. </year>
Reference-contexts: Most such cases are those that are either most typical or most extreme (assuming that there is not too much noise in the data). One method for determining the latter ones, the socalled method of outstanding representatives, is described in <ref> [ML78] </ref>. 2.2.5 Integration of Qualitative and Quantitative Discovery In a database that contains numerical attributes, a useful discovery might be an equation binding these attributes. <p> When a datatable is very large, is it important to concentrate the analysis on a representative sample. The method of outstanding representatives selects examples (tuples) that are most different from the other examples <ref> [ML78] </ref>. Attribute selection: When there are many columns (attributes) in the GDT, it is often desirable to reduce the data table by removing columns that correspond to the least relevant attributes for a designated learning task. <p> GENATR operators generate new attribute sets by creating new attributes [BM96], selecting the most representative attributes from the original set [Bai82], or by abstracting attributes [Ker92]. GENEVE operators generate events, facts or examples that satisfy given rules, select the most representative events from a given set <ref> [ML78] </ref>, determine examples that are similar to a given example [CM89], or predict the value of a given variable using an expert system shell or a decision structure.
Reference: [MM71] <author> Michalski, R.S. and McCormick, B.H. </author> <title> Interval Generalization of Switching Theory. </title> <booktitle> Proceedings of the 3rd Annual Houston Conference on Computer and System Science, </booktitle> <address> Houston, TX, </address> <year> 1971. </year>
Reference-contexts: for any additional conditions), then the extension of R1 against R 2 along dimension x i C R1 | R2 /x i produces a new rule R3: [x i B e], which is a consistent generalization of R1, that is, a generalization that does not intersect logically with R 2 <ref> [MM71] </ref>, [Mic83]. The value of the parameter e controls the degree of generalization. If e is (the empty set), then R 3 is the maximal consistent generalization of R1.
Reference: [MMHL86] <author> Michalski, R. S., Mozetic, I, Hong, J. and Lavrac, N. </author> <title> The AQ15 Inductive Learning System: An Overview and Experiments. </title> <type> ISG Report 86-20, </type> <institution> UIUCDCS-R-86-1260, Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1986. </year>
Reference-contexts: Each of these programs is directly applicable to conceptual data exploration. For example, the rules in Figure 2.1 were generated by the AQ15 rule module <ref> [MMHL86] </ref>, [HMM86] from a set of positive and negative examples of Class 1 of robot-figures. AQ15 learns attributional descriptions of entities, i.e., descriptions involving only their attributes. More general descriptions, structural or relational., also involve relationships among components of the entities, the attributes of the components, and quantifiers. <p> Each attribute is assigned a domain and a type. The domain specifies the set of all legal values that the attribute can be assigned in the table. The type defines the ordering (if any) of the values in the domain. For example, the AQ15 learning program <ref> [MMHL86] </ref> allows four types of attributes: nominal (no order), linear (total order), cyclic (cyclic total order), and structured (hierarchical order; see [KM96]). The attribute type determines the kinds of operations that are allowed on this attributes values during a learning process.
Reference: [MRDMZ97] <author> Michalski, R.S., Rosenfeld, A., Duric, Z., Maloof, M. and Zhang, Q. </author> <title> Application of Machine Learning in Computer Vision. </title> <editor> In Michalski, R.S., Bratko, I. and Kubat, M. (eds.), </editor> <booktitle> Machine Learning and Data Mining: Methods and Applications, </booktitle> <address> London, </address> <publisher> John Wiley & Sons, </publisher> <year> 1997. </year>
Reference-contexts: While this chapter concentrates on methods for extracting knowledge from numeric and symbolic data, many DATA MINING AND KNOWLEDGE DISCOVERY 3 techniques can also be useful when applied to text, speech or image data (e.g., [BMM96], [Uma97], [CGCME97], <ref> [MRDMZ97] </ref>). The second part of this chapter describes a methodology for conceptual data exploration, by which we mean the derivation of high-level concepts and descriptions from data. The methodology, stemming mainly from various efforts in machine learning, applies diverse methods and tools for determining task-oriented data characterizations and generalizations.
Reference: [MSD81] <author> Michalski, R. S., Stepp , R. and Diday, E. </author> <title> A Recent Advance in Data Analysis: Clustering Objects into Classes Characterized by Conjunctive Concepts. </title> <editor> Chapter in Kanal, L. and Rosenfeld, A. (eds.), </editor> <booktitle> Progress in Pattern Recognition, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 33-55, </pages> <year> 1981. </year> <title> DATA MINING AND KNOWLEDGE DISCOVERY 4 1 </title>
Reference-contexts: A classification quality criterion used in conceptual clustering may involve a variety of factors, such as the fit of a cluster description to the data (called sparseness), the simplicity of the description, and other properties of the entities or the concepts that describe them <ref> [MSD81] </ref>. An example of conceptual clustering is presented in Section 2.5. DATA MINING AND KNOWLEDGE DISCOVERY 9 A Some new ideas on employing conceptual clustering for structuring text databases and creating concept lattices for discovering dependencies in data are in [CR95a] and [CR95b]. <p> Clustering: The problem is to automatically partition the rows of the table into groups that correspond to conceptual clusters, that is, sets of entities with high conceptual cohesiveness <ref> [MSD81] </ref>. Such a clustering operator will generate an additional column in the table that corresponds to a new attribute cluster name. The values of this attribute for each tuple in the table indicate the assigned class of the entity. <p> Leaves may be assigned a set of decisions [IM93], [Ima95]. GENEQ operators generate equations characterizing numerical data sets and qualitatively describing the conditions under which these equations apply (e.g., [FM90]). GENHIER operators build conceptual clusters or hierarchies. They are based on the program CLUSTER methodology <ref> [MSD81] </ref>. The operator in INLEN is based on the reimplementation in C of the program CLUSTER/2 [Ste84]. TRANSFORM operators perform various transformations on the knowledge segments, e.g., generalization or specialization, abstraction or concretion, optimization of given rules, etc. according to user-provided criteria.
Reference: [Min89] <author> Mingers, J. </author> <title> An Empirical Comparison of Selection Measures for Decision-Tree Induction. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> pp. 319-342, </pages> <year> 1989. </year>
Reference-contexts: To this end, one may use many different criteria for evaluating the relevance of an attribute for a given classification problem, such as gain ratio [Qui93]; gini index [BFOS84], PROMISE [Bai82], and chisquare analysis [Har84], <ref> [Min89] </ref>. These criteria evaluate attributes on the basis of their expected global performance, which means that attributes with the highest ability to discriminate among all classes are selected as most relevant. When determining a declarative knowledge representation, such as decision rules, the goal is somewhat different.
Reference: [MT89] <author> Morgenthaler, S. and Tukey, J.W. </author> <title> The Next Future of Data Analysis. </title> <editor> In Diday, E. (ed.), </editor> <booktitle> Proceedings of the Conference on Data Analysis, Learning Symbolic and Numeric Knowledge, </booktitle> <publisher> Nova Science Publishers, Inc., </publisher> <address> Antibes, </address> <year> 1989. </year>
Reference-contexts: Data analysis techniques that have been traditionally used for such tasks include regression analysis, cluster analysis, numerical taxonomy, multidimensional analysis, other multivariate statistical methods, stochastic models, time series analysis, nonlinear estimation techniques, and others (e.g., [DW80], [Tuk86], <ref> [MT89] </ref>, [Did89], and [Sha96]). These techniques have been widely used for solving many practical problems. They are, however, primarily oriented toward the extraction of quantitative and statistical data characteristics, and as such have inherent limitations. For example, a statistical analysis can determine covariances and correlations between variables in data.
Reference: [Paw91] <author> Pawlak, Z. </author> <title> Rough Sets: Theoretical Aspects of Reasoning about Data. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, </address> <year> 1991. </year>
Reference-contexts: Learning flexible concepts, i.e., concepts that inherently lack precise definition and whose meaning is context-dependent; some ideas concerned with this topic include fuzzy sets (e.g., [Zad65], [DPY93]), two-tiered concept representations (e.g., [Mic90], [BMMZ92]), and rough sets (e.g., <ref> [Paw91] </ref>, [Slo92], [Zia94]). Learning concepts at different levels of generality, i.e., learning descriptions that involve concepts from different levels of generalization hierarchies representing background knowledge (e.g., [KM96]).
Reference: [Qui86] <author> Quinlan, J.R. </author> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> pp. 81-106, </pages> <year> 1986. </year>
Reference-contexts: One simple and popular form of attributional description is a decision or classification tree. In such a tree, nodes correspond to attributes, branches stemming from the nodes correspond to attribute values, and leaves correspond to individual classes (e.g., <ref> [Qui86] </ref>). A decision tree can be transformed into a set of decision rules (a ruleset) by traversing all paths from the root to individual leaves. Such rules can be often simplified by detecting superfluous conditions in them (e.g., [Qui93]). <p> In the conventional machine learning approach, decision trees are learned directly from training examples, thus avoiding the step of first creating rules [HMS66], <ref> [Qui86] </ref>, [Qui93]. Learning a decision tree directly from examples, however, may have serious disadvantages in practice. A decision tree is a form of procedural knowledge. Once it has been constructed, it is not easy to modify it to accommodate changes in the decision-making conditions. <p> For example, if an attribute (test) assigned to a high-level node in the tree is impossible or too costly to measure, the decision tree offers no alternative course of action other than probabilistic reasoning <ref> [Qui86] </ref>. In contrast, a human making the decision would probably search for alternative tests to perform. People can do this because they typically store decision knowledge in a declarative form.
Reference: [Qui90] <author> Quinlan, J.R. </author> <title> Probabilistic Decision Trees. </title> <editor> In Y. Kodratoff and R.S. Michalski, (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Volume III, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> pp. 140-152, </pages> <year> 1990. </year>
Reference-contexts: In many situations one or more of these assumptions may not hold. This leads to a variety of more complex machine learning and data mining problems: Learning from incorrect data, i.e., learning from examples that contain a certain amount of errors or noise (e.g., <ref> [Qui90] </ref>, [MKW91]). These problems are important to learning from complex real-world observations, where there is always some amount of noise. Learning from incomplete data, i.e., learning from examples in which the values of some attributes are unknown (e.g., [Don88], [LHGS96]).
Reference: [Qui93] <author> Quinlan, J.R. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1993. </year>
Reference-contexts: A decision tree can be transformed into a set of decision rules (a ruleset) by traversing all paths from the root to individual leaves. Such rules can be often simplified by detecting superfluous conditions in them (e.g., <ref> [Qui93] </ref>). The opposite process of transforming a ruleset into a decision tree is not so direct [Ima95], because a rule representation is more powerful than a tree representation. The term more powerful means in this context that a decision tree representing a given ruleset may require superfluous conditions (e.g., [Mic90]). <p> This can be done by applying one of many methods for attribute selection, such as Gain Ratio <ref> [Qui93] </ref> or Promise level [Bai82]. MICHALSKI & KAUFMAN1 6 Generating new attributes: The problem is to generate additional columns that correspond to new attributes generated by a constructive induction procedure. <p> DATA MINING AND KNOWLEDGE DISCOVERY 1 9 GENTREE operators build a decision structure from a given set of decision rules (e.g., [IM93]), or from examples (e.g., <ref> [Qui93] </ref>). A decision structure is a generalization of the concept of a decision tree in which nodes can be assigned an attribute or a function of attributes. Individual branches may be assigned a set of attribute values. Leaves may be assigned a set of decisions [IM93], [Ima95]. <p> In the conventional machine learning approach, decision trees are learned directly from training examples, thus avoiding the step of first creating rules [HMS66], [Qui86], <ref> [Qui93] </ref>. Learning a decision tree directly from examples, however, may have serious disadvantages in practice. A decision tree is a form of procedural knowledge. Once it has been constructed, it is not easy to modify it to accommodate changes in the decision-making conditions. <p> To this end, one may use many different criteria for evaluating the relevance of an attribute for a given classification problem, such as gain ratio <ref> [Qui93] </ref>; gini index [BFOS84], PROMISE [Bai82], and chisquare analysis [Har84], [Min89]. These criteria evaluate attributes on the basis of their expected global performance, which means that attributes with the highest ability to discriminate among all classes are selected as most relevant.
Reference: [Rei84] <author> Reinke, R.E. </author> <title> Knowledge Acquisition and Refinement Tools for the ADVISE Meta-Expert System. </title> <type> Masters Thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1984. </year>
Reference-contexts: These operators can also be used to apply the rules to any given situation to determine a decision. The TEST operator implemented in INLEN is based on the ATEST program <ref> [Rei84] </ref>. VISUALIZE operators are used to present data and/or knowledge to the user in a convenient, easy-to-understand format [Wne95]. Summarizing, INLEN integrates a large set of operators for performing various types of operations on the data base, on the knowledge base, or the data and knowledge bases combined.
Reference: [RM88] <author> Reinke, R.E. and Michalski, </author> <title> R.S. Incremental Learning of Concept Descriptions: A Method and Experimental Results. </title> <journal> Machine Intelligence, </journal> <volume> 11, </volume> <pages> pp. 263-288, </pages> <year> 1988. </year>
Reference-contexts: To do so, an incremental learning program must be applied to synthesize the prior knowledge with the new information. The incremental learning process may be full-memory, partial-memory, or no-memory, depending on how much of the original training data is maintained in the incremental learning process [HMM86], <ref> [RM88] </ref>, [MM95]. Searching for approximate patterns in (imperfect) data: For some GDTs, it may not be possible (or useful) to find complete and consistent descriptions. In such cases, it is important to determine patterns that hold for a large number of cases, but not necessarily for all.
Reference: [RKK95] <author> Ribeiro, J.S., Kaufman, K.A. and Kerschberg, L. </author> <title> Knowledge Discovery From Multiple Databases. </title> <booktitle> Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Montreal, PQ, </address> <pages> pp. 240-245, </pages> <year> 1995. </year>
Reference-contexts: Learning from incomplete data, i.e., learning from examples in which the values of some attributes are unknown (e.g., [Don88], [LHGS96]). Learning from distributed data, i.e., learning from separate collections of data that must be brought together if the patterns within them are to be exposed (e.g., <ref> [RKK95] </ref>). Learning drifting or evolving concepts, i.e., learning concepts that are not stable but changing over time, randomly or in a certain general direction. For example, the area of interest of a user is often an evolving concept (e.g., [WK96]).
Reference: [RMGJB76] <author> Rosch, E., Mervis, C., Gray, W., Johnson, D. and Boyes-Braem, P. </author> <title> Basic Objects in Natural Categories. </title> <journal> Cognitive Psychology, </journal> <volume> 8, </volume> <pages> pp. 382-439, </pages> <year> 1976. </year>
Reference-contexts: Nodes marked by + and are values occurring in positive and negative examples, respectively. Cognitive scientists have noticed that people prefer certain nodes in a generalization hierarchy (concepts) over other nodes when creating descriptions (e.g., <ref> [RMGJB76] </ref>). Factors that influence the choice of a concept (node) include the concept typicality (how common are a concepts features among its sibling concepts), and the context in which the concept is being used.
Reference: [Sha96] <author> Sharma, S. </author> <title> Applied Multivariate Techniques. </title> <publisher> London, John Wiley & Sons, Inc., </publisher> <year> 1996. </year>
Reference-contexts: Data analysis techniques that have been traditionally used for such tasks include regression analysis, cluster analysis, numerical taxonomy, multidimensional analysis, other multivariate statistical methods, stochastic models, time series analysis, nonlinear estimation techniques, and others (e.g., [DW80], [Tuk86], [MT89], [Did89], and <ref> [Sha96] </ref>). These techniques have been widely used for solving many practical problems. They are, however, primarily oriented toward the extraction of quantitative and statistical data characteristics, and as such have inherent limitations. For example, a statistical analysis can determine covariances and correlations between variables in data.
Reference: [Slo92] <author> Slowinski, R. (ed.) </author> <title> Intelligent Decision Support: Handbook of Applications and Advances of the Rough Sets Theory. </title> <publisher> Dordrecht/Boston/London, Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Learning flexible concepts, i.e., concepts that inherently lack precise definition and whose meaning is context-dependent; some ideas concerned with this topic include fuzzy sets (e.g., [Zad65], [DPY93]), two-tiered concept representations (e.g., [Mic90], [BMMZ92]), and rough sets (e.g., [Paw91], <ref> [Slo92] </ref>, [Zia94]). Learning concepts at different levels of generality, i.e., learning descriptions that involve concepts from different levels of generalization hierarchies representing background knowledge (e.g., [KM96]).
Reference: [Ste84] <author> Stepp, R. </author> <title> A Description and Users Guide for CLUSTER/2, A Program for Conjunctive Conceptual Clustering. </title> <type> Report No. </type> <institution> UIUCDCS-R-83-1084, Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1984. </year>
Reference-contexts: GENHIER operators build conceptual clusters or hierarchies. They are based on the program CLUSTER methodology [MSD81]. The operator in INLEN is based on the reimplementation in C of the program CLUSTER/2 <ref> [Ste84] </ref>. TRANSFORM operators perform various transformations on the knowledge segments, e.g., generalization or specialization, abstraction or concretion, optimization of given rules, etc. according to user-provided criteria. For instance, one such operator climbs an attributes generalization hierarchy to build more general decision rules [KM96]. <p> The first operator is realized by the CLUSTER/2 program for conceptual clustering <ref> [Ste84] </ref>. The second operator is realized by the AQ15c rule learning program [WKBM95]. This section illustrates these operators through an application to a datatable characterizing hard drives (Figure 2.7). The datatable is based on information published in the October, 1994 issue of MacUser.
Reference: [Tuk86] <author> Tukey, J.W. </author> <title> The Collected Works of John W. </title> <journal> Tukey, </journal> <volume> Vol. </volume> <booktitle> V, Philosophy and Principles of Data Analysis: </booktitle> <pages> 1965-1986. </pages> <editor> Jones, L.V. (ed.), </editor> <publisher> Wadsworth & Brooks/Cole, </publisher> <address> Monterey, CA, </address> <year> 1986. </year>
Reference-contexts: Data analysis techniques that have been traditionally used for such tasks include regression analysis, cluster analysis, numerical taxonomy, multidimensional analysis, other multivariate statistical methods, stochastic models, time series analysis, nonlinear estimation techniques, and others (e.g., [DW80], <ref> [Tuk86] </ref>, [MT89], [Did89], and [Sha96]). These techniques have been widely used for solving many practical problems. They are, however, primarily oriented toward the extraction of quantitative and statistical data characteristics, and as such have inherent limitations. For example, a statistical analysis can determine covariances and correlations between variables in data.
Reference: [Uma97] <author> Umann, E. </author> <title> Phons in Spoken Speech: A Contribution to the Computer Analysis of Spoken Texts. Pattern Recognition and Image Analysis, </title> <booktitle> 7:1, </booktitle> <pages> pp. 138-144, </pages> <year> 1997. </year> <note> MICHALSKI & KAUFMAN4 2 </note>
Reference-contexts: While this chapter concentrates on methods for extracting knowledge from numeric and symbolic data, many DATA MINING AND KNOWLEDGE DISCOVERY 3 techniques can also be useful when applied to text, speech or image data (e.g., [BMM96], <ref> [Uma97] </ref>, [CGCME97], [MRDMZ97]). The second part of this chapter describes a methodology for conceptual data exploration, by which we mean the derivation of high-level concepts and descriptions from data.
Reference: [VHMT93] <editor> Van Mechelen, I., Hampton, J., Michalski, R.S. and Theuns, P. (eds.) </editor> <title> Categories and Concepts: Theoretical Views and Inductive Data Analysis. </title> <publisher> London, Academic Press, </publisher> <year> 1993. </year>
Reference-contexts: These and related efforts have led to the emergence of a new research area, frequently called data mining and knowledge discovery, e.g., [Lbo81], [MBS82], [ZG89], [Mic91b], [Zag91], [MKKR92], <ref> [VHMT93] </ref>, [FPSU96], [EH96], [BKKPS96], and [FHS96]. The first part of this chapter is a compendium of ideas on the applicability of symbolic machine learning methods to data mining and knowledge discovery.
Reference: [WK96] <author> Widmer, G. and Kubat, M. </author> <title> Learning in the Presence of Concept Drift and Hidden Concepts. </title> <journal> Machine Learning, </journal> <volume> 23, </volume> <pages> pp. 69-101, </pages> <year> 1996. </year>
Reference-contexts: Learning drifting or evolving concepts, i.e., learning concepts that are not stable but changing over time, randomly or in a certain general direction. For example, the area of interest of a user is often an evolving concept (e.g., <ref> [WK96] </ref>). Learning concepts from data arriving over time, i.e., incremental learning in which currently held hypotheses characterizing concepts may need to be updated to account for the new data (e.g., [MM95]).
Reference: [Wne95] <author> Wnek, J. </author> <title> DIAV 2.0 User Manual, Specification and Guide through the Diagrammatic Visualization System. Reports of the Machine Learning and Inference Laboratory, </title> <type> MLI 95-5, </type> <institution> George Mason University, Fairfax, VA, </institution> <year> 1995. </year>
Reference-contexts: Attributional descriptions can be visualized by mapping them into a planar representation of a discrete multidimensional space (a diagram) spanned over the given attributes [Mic78], [WSWM90]. For example, Figure 2.2 shows a diagrammatic visualization of the rules from DIAV [WSWM90], <ref> [Wne95] </ref>. Each cell in the diagram represents one specific combination of values of the attributes. For example, the cell marked by an X represents the vector: (HeadShape= S quare, Holding= S word, JacketColor= R ed, IsSmiling= F alse). <p> These operators perform symbolic and numerical data exploration tasks. They are based on various machine learning and inference programs, on conventional data exploration techniques, and on visualization operators for displaying graphically the results of exploration. The diagrammatic visualization method, DIAV <ref> [Wne95] </ref> is used for displaying the effects of symbolic learning operations on data. The KGOs are the heart of the INLEN system. To facilitate their use, the concept of a knowledge segment was introduced [KMK91]. <p> These operators can also be used to apply the rules to any given situation to determine a decision. The TEST operator implemented in INLEN is based on the ATEST program [Rei84]. VISUALIZE operators are used to present data and/or knowledge to the user in a convenient, easy-to-understand format <ref> [Wne95] </ref>. Summarizing, INLEN integrates a large set of operators for performing various types of operations on the data base, on the knowledge base, or the data and knowledge bases combined. <p> To this end, INLEN supports the visualization of data and knowledge through the diagrammatic visualization method implemented in the DIAV program [Mic78], <ref> [Wne95] </ref>. clustering operator. Let us illustrate the method with the hard disk classification problem presented in the previous section.
Reference: [WKBM95] <author> Wnek, J., Kaufman, K., Bloedorn, E. and Michalski, </author> <title> R.S. Selective Induction Learning System AQ15c: The Method and Users Guide. Reports of the Machine Learning and Inference Laboratory, MLI 95-4, Machine Learning and Inference Laboratory, </title> <institution> George Mason University, Fairfax, VA, </institution> <year> 1995. </year>
Reference-contexts: Learned rules are usually consistent and complete with regard to the input data. This means that they completely and correctly classify all the original training examples. Sections 2.5 and 2.8 present consistent and complete example solutions from the inductive concept learning program AQ15c <ref> [WKBM95] </ref>. In some applications, especially those involving learning rules from noisy data or learning flexible concepts [Mic90], it is may be advantageous to learn descriptions that are incomplete and/or inconsistent [BMMZ92]. <p> The determination of such relationships (rules) can be guided by different rule quality criteria, for example, simplicity, cost, predictive accuracy, etc. In the INLEN system, the AQ learning method was applied due to the simplicity and the high comprehensibility of decision rules it generates <ref> [WKBM95] </ref>, [BM96]. Determining timedependent patterns: This problem concerns the detection of temporal patterns in sequences of data arranged along the time dimension in a GDT (Figure 2.5). Among the novel ideas that could be applied for analyzing such timedependent data is a multi-model method for qualitative prediction [DM86], [MKC85], [MKC86]. <p> A specific operator may generate rules characterizing a set of facts, discriminating between groups of facts, characterizing a sequence of events, and determining differences between sequences, based on programs such as AQ15c <ref> [WKBM95] </ref> and SPARC/G [MKC86]. A KGO for learning rules can usually work in either incremental or batch mode. <p> The first operator is realized by the CLUSTER/2 program for conceptual clustering [Ste84]. The second operator is realized by the AQ15c rule learning program <ref> [WKBM95] </ref>. This section illustrates these operators through an application to a datatable characterizing hard drives (Figure 2.7). The datatable is based on information published in the October, 1994 issue of MacUser.
Reference: [WM94] <author> Wnek, J. and Michalski, </author> <title> R.S. Hypothesis-driven Constructive Induction in AQ17-HCI: A Method and Experiments. </title> <journal> Machine Learning, </journal> <volume> 14, </volume> <pages> pp. 139-168, </pages> <year> 1994. </year>
Reference-contexts: A learning process that consists of two (intertwined) phases, one concerned with the construction of the best representation space, and the second concerned with generating the best hypothesis in the found space is called constructive induction [Mic78], [Mic83], <ref> [WM94] </ref>. An example of a constructive induction program is AQ17 [BWM93], which performs all three types of improvements of the original representation space. <p> Columns in the tables correspond to attributes used to characterize entities associated with the rows. These may be initial attributes, given a priori, or additional ones generated through a process of constructive induction (e.g., <ref> [WM94] </ref>). Each attribute is assigned a domain and a type. The domain specifies the set of all legal values that the attribute can be assigned in the table. The type defines the ordering (if any) of the values in the domain.
Reference: [WSWM90] <author> Wnek, J., Sarma, J., Wahab, A., Michalski, </author> <title> R.S. Comparing Learning Paradigms via Diagrammatic Visualization: A Case Study in Single Concept Learning using Symbolic, Neural Net and Genetic Algorithm Methods. Reports of the Machine Learning and Inference Laboratory, MLI 90-2, Machine Learning and Inference Laboratory, </title> <institution> George Mason University, Fairfax, VA, </institution> <year> 1990. </year>
Reference-contexts: Attributional descriptions can be visualized by mapping them into a planar representation of a discrete multidimensional space (a diagram) spanned over the given attributes [Mic78], <ref> [WSWM90] </ref>. For example, Figure 2.2 shows a diagrammatic visualization of the rules from DIAV [WSWM90], [Wne95]. Each cell in the diagram represents one specific combination of values of the attributes. <p> Attributional descriptions can be visualized by mapping them into a planar representation of a discrete multidimensional space (a diagram) spanned over the given attributes [Mic78], <ref> [WSWM90] </ref>. For example, Figure 2.2 shows a diagrammatic visualization of the rules from DIAV [WSWM90], [Wne95]. Each cell in the diagram represents one specific combination of values of the attributes. For example, the cell marked by an X represents the vector: (HeadShape= S quare, Holding= S word, JacketColor= R ed, IsSmiling= F alse). <p> By comparing the target concept with the learned concept, one can determine the error area, i.e., the area containing all examples that would be incorrectly classified by the learned concept. Such a diagrammatic visualization method can illustrate any kind of attributional learning process <ref> [WSWM90] </ref>.
Reference: [Zad65] <author> Zadeh, L. </author> <title> Fuzzy Sets. </title> <journal> Information and Control, </journal> <volume> 8, </volume> <pages> pp. 338-353, </pages> <year> 1965. </year>
Reference-contexts: Learning from biased data, i.e., learning from a data set that does not reflect the actual distribution of events (e.g., [Fee96]). Learning flexible concepts, i.e., concepts that inherently lack precise definition and whose meaning is context-dependent; some ideas concerned with this topic include fuzzy sets (e.g., <ref> [Zad65] </ref>, [DPY93]), two-tiered concept representations (e.g., [Mic90], [BMMZ92]), and rough sets (e.g., [Paw91], [Slo92], [Zia94]). Learning concepts at different levels of generality, i.e., learning descriptions that involve concepts from different levels of generalization hierarchies representing background knowledge (e.g., [KM96]).
Reference: [Zag72] <author> Zagoruiko, N.G. </author> <title> Recognition Methods and Their Application. </title> <booktitle> Sovietsky Radio, Moscow (in Russian), </booktitle> <year> 1972. </year>
Reference-contexts: By removing less relevant attributes, the representation space is reduced, and the problem becomes simpler. Thus, such a process can be viewed as a form of improving the representation space. Some methods for finding the most relevant attributes are described in <ref> [Zag72] </ref> and [Bai82]. MICHALSKI & KAUFMAN1 0 In many applications, the attributes originally given may be only weakly or indirectly relevant to the problem at hand. In such situations, there is a need for generating new, more relevant attributes that may be functions of the original attributes. <p> This is an example of quantitative discovery. The application of machine learning to quantitative discovery was pioneered by the BACON system [LBS83], and then explored by many systems since, such as COPER [Kok86], FAHRENHEIT [Zyt87], and ABACUS [FM90]. Similar problems have been explored independently by Zagoruiko <ref> [Zag72] </ref> under the name of empirical prediction. Some equations may not apply directly to data, because of an inappropriate value of a constant, or different equations may apply under different qualitative conditions.
Reference: [Zag91] <author> Zagoruiko, N.G. </author> <title> Ekspertnyie Sistemy I Analiz Dannych (Expert Systems and Data Analysis). </title> <institution> Wychislitielnyje Sistemy, N.144, Akademia Nauk USSR, Sibirskoje Otdielenie, Institut Matiematikie, Novosibirsk, </institution> <year> 1991. </year>
Reference-contexts: These and related efforts have led to the emergence of a new research area, frequently called data mining and knowledge discovery, e.g., [Lbo81], [MBS82], [ZG89], [Mic91b], <ref> [Zag91] </ref>, [MKKR92], [VHMT93], [FPSU96], [EH96], [BKKPS96], and [FHS96]. The first part of this chapter is a compendium of ideas on the applicability of symbolic machine learning methods to data mining and knowledge discovery.
Reference: [ZG89] <author> Zhuravlev, Y.I. and Gurevitch, </author> <title> I.B. Pattern Recognition and Image Recognition. Chapter in Zhuravlev, Y.I. (ed.), Pattern Recognition, Classification, Forecasting: Mathematical Techniques and their Application. </title> <note> Issue 2, Nauka, Moscow, pp. 5-72 (in Russian), </note> <year> 1989. </year>
Reference-contexts: These and related efforts have led to the emergence of a new research area, frequently called data mining and knowledge discovery, e.g., [Lbo81], [MBS82], <ref> [ZG89] </ref>, [Mic91b], [Zag91], [MKKR92], [VHMT93], [FPSU96], [EH96], [BKKPS96], and [FHS96]. The first part of this chapter is a compendium of ideas on the applicability of symbolic machine learning methods to data mining and knowledge discovery.
Reference: [Zia94] <author> Ziarko, </author> <title> W.P. (ed.) Rough Sets, Fuzzy Sets and Knowledge Discovery. </title> <publisher> Berlin, Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Learning flexible concepts, i.e., concepts that inherently lack precise definition and whose meaning is context-dependent; some ideas concerned with this topic include fuzzy sets (e.g., [Zad65], [DPY93]), two-tiered concept representations (e.g., [Mic90], [BMMZ92]), and rough sets (e.g., [Paw91], [Slo92], <ref> [Zia94] </ref>). Learning concepts at different levels of generality, i.e., learning descriptions that involve concepts from different levels of generalization hierarchies representing background knowledge (e.g., [KM96]).
Reference: [Zyt87] <author> Zytkow, J.M. </author> <title> Combining Many Searches in the FAHRENHEIT Discovery System. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <address> Irvine, CA, </address> <pages> pp. 281-287, </pages> <year> 1987. </year>
Reference-contexts: This is an example of quantitative discovery. The application of machine learning to quantitative discovery was pioneered by the BACON system [LBS83], and then explored by many systems since, such as COPER [Kok86], FAHRENHEIT <ref> [Zyt87] </ref>, and ABACUS [FM90]. Similar problems have been explored independently by Zagoruiko [Zag72] under the name of empirical prediction. Some equations may not apply directly to data, because of an inappropriate value of a constant, or different equations may apply under different qualitative conditions.
References-found: 94

