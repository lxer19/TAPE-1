URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/uwem/WWW/work/papers/96.icassp.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/uwem/WWW/work/papers.html
Root-URL: 
Email: uwe@ira.uka.de, huerst@ira.uka.de, paul d@ollie.mit.edu  
Phone: 1  2  
Title: ADAPTIVE BIMODAL SENSOR FUSION FOR AUTOMATIC SPEECHREADING  
Author: Uwe Meier Wolfgang Hurst Paul Duchnowski ; 
Address: Germany  Cambridge MA, USA  
Affiliation: Interactive Systems Laboratories University of Karlsruhe, Karlsruhe,  Massachusetts Institute of Technology  
Abstract: We present recent work on improving the performance of automated speech recognizers by using additional visual information (Lip-/Speechreading), achieving error reduction of up to 50%. This paper focuses on different methods of combining the visual and acoustic data to improve the recognition performance. We show this on an extension of an existing state-of-the-art speech recognition system, a modular MS-TDNN. We have developed adaptive combination methods at several levels of the recognition network. Additional information such as estimated signal-to-noise ratio (SNR) is used in some cases. The results of the different combination methods are shown for clean speech and data with artificial noise (white, music, motor). The new combination methods adapt automatically to varying noise conditions making hand-tuned parameters unnecessary. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. McGurk and J. MacDonald. </author> <title> Hearing lips and seeing voices. </title> <booktitle> Nature, </booktitle> <year> 1976. </year>
Reference-contexts: It is well known that hearing-impaired listeners and those listening in adverse acoustic environments rely heavily on the visual input to disambiguate among acoustically con-fusable speech elements. The usefullness of lip movement information stems in large part from its rough complemen-tariness to the acoustic signal <ref> [1, 2, 3] </ref>. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept was published by other researchers in [4, 5, 6, 7, 8, 9]. Our own work in this area has been previously reported in [10, 11, 12].
Reference: [2] <author> M. McGrath and Q. Summerfield. </author> <title> Intermodal tim ing relations and audio-visual speech recognition by normal-hearing adults. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 77(2) </volume> <pages> 678-685, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: It is well known that hearing-impaired listeners and those listening in adverse acoustic environments rely heavily on the visual input to disambiguate among acoustically con-fusable speech elements. The usefullness of lip movement information stems in large part from its rough complemen-tariness to the acoustic signal <ref> [1, 2, 3] </ref>. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept was published by other researchers in [4, 5, 6, 7, 8, 9]. Our own work in this area has been previously reported in [10, 11, 12].
Reference: [3] <author> A.A. Montgomery, B. Walden, and R. Prosek. </author> <title> Effects of consonantal context on vowel lipreading. </title> <journal> Journal of Speech and Hearing Research, </journal> <volume> 30 </volume> <pages> 50-59, </pages> <year> 1987. </year>
Reference-contexts: It is well known that hearing-impaired listeners and those listening in adverse acoustic environments rely heavily on the visual input to disambiguate among acoustically con-fusable speech elements. The usefullness of lip movement information stems in large part from its rough complemen-tariness to the acoustic signal <ref> [1, 2, 3] </ref>. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept was published by other researchers in [4, 5, 6, 7, 8, 9]. Our own work in this area has been previously reported in [10, 11, 12].
Reference: [4] <author> A.J. Goldschen. </author> <title> Continuous Automatic Speech Recog nition by Lipreading. </title> <type> Dissertation, </type> <institution> The School of Engineering and Applied Science of The George Washington University, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: The usefullness of lip movement information stems in large part from its rough complemen-tariness to the acoustic signal [1, 2, 3]. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept was published by other researchers in <ref> [4, 5, 6, 7, 8, 9] </ref>. Our own work in this area has been previously reported in [10, 11, 12]. In this paper we focus on combining the acoustic and visual input data to improve recognition performance.
Reference: [5] <author> J. R. Movellan. </author> <title> Visual speech recognition with stochas tic networks. </title> <booktitle> NIPS 94, </booktitle> <year> 1994. </year>
Reference-contexts: The usefullness of lip movement information stems in large part from its rough complemen-tariness to the acoustic signal [1, 2, 3]. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept was published by other researchers in <ref> [4, 5, 6, 7, 8, 9] </ref>. Our own work in this area has been previously reported in [10, 11, 12]. In this paper we focus on combining the acoustic and visual input data to improve recognition performance.
Reference: [6] <author> P.L. Silsbee and A.C. Bovic. </author> <title> Audio-visual speech recog nition for a vowel discrimination task. </title> <booktitle> SPIE, </booktitle> 2049 84-95. 
Reference-contexts: The usefullness of lip movement information stems in large part from its rough complemen-tariness to the acoustic signal [1, 2, 3]. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept was published by other researchers in <ref> [4, 5, 6, 7, 8, 9] </ref>. Our own work in this area has been previously reported in [10, 11, 12]. In this paper we focus on combining the acoustic and visual input data to improve recognition performance.
Reference: [7] <author> E.D. Petajan. </author> <title> Automatic lipreading to enhance speech recognition. </title> <booktitle> Proc. IEEE Communications Society Global Telecommunications Conference, </booktitle> <year> 1984. </year>
Reference-contexts: The usefullness of lip movement information stems in large part from its rough complemen-tariness to the acoustic signal [1, 2, 3]. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept was published by other researchers in <ref> [4, 5, 6, 7, 8, 9] </ref>. Our own work in this area has been previously reported in [10, 11, 12]. In this paper we focus on combining the acoustic and visual input data to improve recognition performance.
Reference: [8] <author> K. Mase and A. Pentland. </author> <title> Automantic lipreading by optical-flow analysis. </title> <journal> Systems and Computers in Japan, </journal> <volume> 22(6) </volume> <pages> 67-76, </pages> <year> 1991. </year>
Reference-contexts: The usefullness of lip movement information stems in large part from its rough complemen-tariness to the acoustic signal [1, 2, 3]. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept was published by other researchers in <ref> [4, 5, 6, 7, 8, 9] </ref>. Our own work in this area has been previously reported in [10, 11, 12]. In this paper we focus on combining the acoustic and visual input data to improve recognition performance.
Reference: [9] <author> D.G. Stork, G. Wolff, and E. Levine. </author> <title> Neural net work lipreading system for improved speech recognition. </title> <booktitle> IJCNN, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: The usefullness of lip movement information stems in large part from its rough complemen-tariness to the acoustic signal [1, 2, 3]. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept was published by other researchers in <ref> [4, 5, 6, 7, 8, 9] </ref>. Our own work in this area has been previously reported in [10, 11, 12]. In this paper we focus on combining the acoustic and visual input data to improve recognition performance.
Reference: [10] <author> C. Bregler, H. Hild, S. Manke, and A. Waibel. </author> <title> Improv ing connected letter recognition by lipreading. </title> <booktitle> Proc. ICASSP, 1993. </booktitle> <address> Minneapolis. </address>
Reference-contexts: Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept was published by other researchers in [4, 5, 6, 7, 8, 9]. Our own work in this area has been previously reported in <ref> [10, 11, 12] </ref>. In this paper we focus on combining the acoustic and visual input data to improve recognition performance. The merging of the two information sources is very important for the final results. With only visual input our recognizer obtains recognition rates of up to 55%.
Reference: [11] <author> P. Duchnowski, M. Hunke, D. Busching, U. Meier, and A. Waibel. </author> <title> Toward movement-invariant automatic lipreading and speech recognition. </title> <booktitle> Proc. ICASSP, </booktitle> <year> 1995. </year>
Reference-contexts: Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept was published by other researchers in [4, 5, 6, 7, 8, 9]. Our own work in this area has been previously reported in <ref> [10, 11, 12] </ref>. In this paper we focus on combining the acoustic and visual input data to improve recognition performance. The merging of the two information sources is very important for the final results. With only visual input our recognizer obtains recognition rates of up to 55%.
Reference: [12] <author> P. Duchnowski, U. Meier, and A. Waibel. </author> <title> See me, hear me: Integrating automatic speech recognition and lipreading. </title> <booktitle> International Conference on Spoken Language Processing, ICSLP, </booktitle> <pages> pages 547-550, </pages> <year> 1994. </year>
Reference-contexts: Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept was published by other researchers in [4, 5, 6, 7, 8, 9]. Our own work in this area has been previously reported in <ref> [10, 11, 12] </ref>. In this paper we focus on combining the acoustic and visual input data to improve recognition performance. The merging of the two information sources is very important for the final results. With only visual input our recognizer obtains recognition rates of up to 55%.
Reference: [13] <author> A. Waibel, T. Hanazawa, G. Hinton, and K. Shikano. </author> <title> Phoneme recognition using time-delay neural networks. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 37(3) </volume> <pages> 328-339, </pages> <year> 1989. </year>
Reference-contexts: Words in our database are 8 letters long on average. noise signal-to-noise ratio clean 33 dB white noise 16 dB and 8 dB music 20 dB and 16 dB motor 25 dB and 10 dB Table 1. Acoustic environments tested (dB SNR). A modular MS-TDNN <ref> [13, 14] </ref> is used to perform the recognition. Combining visual and acoustic data is done on the phonetic layer (Fig. 1) or on lower levels (Fig. 3). As visual input we use Linear Discriminant Analysis coefficients of the gray-scale pictures of the lip region. (top 32 coefficients per image frame).
Reference: [14] <author> Hermann Hild and Alex Waibel. </author> <title> Speaker-Independent Connected Letter Recognition With a Multi-State Time Delay Neural Network. </title> <booktitle> In 3rd European Conference on Speech, Communication and Technology (EU-ROSPEECH) 93, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: Words in our database are 8 letters long on average. noise signal-to-noise ratio clean 33 dB white noise 16 dB and 8 dB music 20 dB and 16 dB motor 25 dB and 10 dB Table 1. Acoustic environments tested (dB SNR). A modular MS-TDNN <ref> [13, 14] </ref> is used to perform the recognition. Combining visual and acoustic data is done on the phonetic layer (Fig. 1) or on lower levels (Fig. 3). As visual input we use Linear Discriminant Analysis coefficients of the gray-scale pictures of the lip region. (top 32 coefficients per image frame).
Reference: [15] <author> H. Gunther Hirsch. </author> <title> Estimation of Noise Spectrum and its Application to SNR-Estimation and Speech Enhancement. </title> <type> Technical Report, </type> <institution> International Computer Science Institute, Berkeley, California, USA. </institution>
Reference-contexts: Fig. 2 shows on an example from the test set, the values of the weights as they vary with the estimated SNR which is shown on top (for more information about this algorithm see <ref> [15] </ref>). 3.3. Learning the weights Another approach is to use a neural network to compute the combination weights at the phoneme level. This method differs form the previous in two ways. First the combination weights are learned from training data and not calculated during the recognition progress.
References-found: 15

