URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn9.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: A PARALLEL IMPLEMENTATION OF THE INVARIANT SUBSPACE DECOMPOSITION ALGORITHM FOR DENSE SYMMETRIC MATRICES*  
Author: STEVEN HUSS-LEDERMAN ANNA TSAO and GUODONG ZHANG 
Date: March 22-24, 1993.  
Note: Appeared in Proceedings, Sixth SIAM Conference on Parallel Processing for Scientific Computing, Norfolk, Virginia,  
Abstract: We give an overview of the Invariant Subspace Decomposition Algorithm for dense symmetric matrices (SYISDA) by first describing the algorithm, followed by a discussion of a parallel implementation of SYISDA on the Intel Delta. Our implementation utilizes an optimized parallel matrix multiplication implementation we have developed. Load balancing in the costly early stages of the algorithm is accomplished without redistribution of data between stages through the use of the block scattered decomposition. Computation of the invariant subspaces at each stage is done using a new tridiagonalization scheme due to Bischof and Sun. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Anderson, E., Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, & D. Sorensen, </author> <title> LAPACK: A portable linear algebra library for high-performance computers, </title> <booktitle> Proceedings, Supercomputing `90, </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1990, </year> <pages> pp. 2-11. </pages>
Reference-contexts: If eigenvectors are desired, initialize the eigenvector matrix Z to be the identity matrix. 1. Compute upper and lower bounds on the spectrum of A and use these bounds to form `(A), where ` is a linear function that maps the spectrum of A into <ref> [0; 1] </ref> so that its mean eigenvalue is mapped to 1=2. 2. Perform the iteration, A 0 = `(A); where the polynomials P k ; k = 1; 2; 3; : : : are designed so that the eigenvalues of the sequence of iterates tend either toward 0 or 1. <p> This is usually achieved with a so-called "rank-revealing orthogonal factorization" [7, 11, 12]; the "QR-factorization with column pivoting" [18], used in our previously implemented sequential code, is the most commonly implemented strategy <ref> [14, 1] </ref>. For computers with a memory hierarchy and distributed-memory machines, variations of this strategy have been devised [6, 5] which limit the adverse effect of the data-dependent column exchanges on the data reference locality and synchronization costs.
Reference: 2. <author> Anderson, E., Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, & D. Sorensen, </author> <title> LAPACK Users' Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: In this way, machine-dependent communications are relegated as much as possible to these primitives, localizing their impact. Efforts to attain performance can then be devoted to optimizing these functions on the target machine. This philosophy is analogous to the reliance of packages such as LAPACK <ref> [2] </ref> and ScaLAPACK [13] on optimized BLAS. General parallel design issues for SYISDA on square meshes are discussed in Section 3. In particular, we describe how good load balancing in the critical early stages of SYISDA can be achieved by spreading the data for each subproblem over the entire mesh.
Reference: 3. <author> Auslander, L. & A. Tsao, </author> <title> On parallelizable eigensolvers, </title> <journal> Adv. Appl. Math. </journal> <volume> 13 (1992), </volume> <pages> 253-261, </pages> <note> (also appeared as Technical Report SRC-TR-91-028, Supercomputing Research Center). 7 </note>
Reference-contexts: We present preliminary results on parallel studies of the Symmetric Invariant Subspace Decomposition Algorithm (SYISDA). In particular, we present results from work-in-progress on an implementation of SYISDA for square meshes. SYISDA is a divide-and-conquer algorithm which derives most of its parallelism from performing matrix multiplication <ref> [3] </ref>. Recall the algorithm in [25]. fl This paper is PRISM Working Note #9, available via anonymous ftp to ftp.super.org in the directory pub/prism. y Supercomputing Research Center, 17100 Science Drive, Bowie, MD 20715 z CONVEX Computer Corporation, 3000 Waterview Parkway, Richardson, TX 75083-3851.
Reference: 4. <author> Berry, M. & A. Sameh, </author> <title> Parallel algorithms for the singular value and dense symmetric eigenvalue problem, </title> <journal> CSRD (1988), </journal> <volume> no. </volume> <pages> 761. </pages>
Reference-contexts: The prevalence of distributed memory machines has spurred very active investigation into algorithms which map more readily onto these architectures. Promising algorithms that have been investigated include bisection/multisection coupled with inverse iteration, Divide and Conquer, Jacobi methods, and homotopy methods <ref> [29, 23, 4, 26] </ref>. The work presented in this paper is part of the Parallel Research on Invariant Subspace Methods (PRISM) project, composed of researchers from Argonne National Laboratory, Supercomputing Research Center, University of California, Berkeley, and University of Kentucky, which is investigating scalable algorithms for solving eigenproblems.
Reference: 5. <author> Bischof, C. H., </author> <title> A block QR factorization algorithm using restricted pivoting., </title> <booktitle> Proceedings SUPERCOMPUTING '89, </booktitle> <publisher> ACM Press, </publisher> <address> Baltimore, Md., </address> <year> 1989, </year> <pages> pp. 248-256. </pages>
Reference-contexts: For computers with a memory hierarchy and distributed-memory machines, variations of this strategy have been devised <ref> [6, 5] </ref> which limit the adverse effect of the data-dependent column exchanges on the data reference locality and synchronization costs.
Reference: 6. <author> Bischof, C., </author> <title> A parallel QR factorization with controlled local pivoting, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 12 (1991), </volume> <pages> 36-57. </pages>
Reference-contexts: For computers with a memory hierarchy and distributed-memory machines, variations of this strategy have been devised <ref> [6, 5] </ref> which limit the adverse effect of the data-dependent column exchanges on the data reference locality and synchronization costs.
Reference: 7. <author> Bischof, C. H. & P. C. Hansen, </author> <title> A block algorithm for computing rank-revealing QR factorizations, also MCS-P251-0791, </title> <booktitle> Numerical Algorithms 2 (1992), </booktitle> <volume> no. </volume> <pages> 3-4, 371-392. </pages>
Reference-contexts: To decompose the eigenproblem for A into two smaller ones, we must compute the range and null spaces of M . This is usually achieved with a so-called "rank-revealing orthogonal factorization" <ref> [7, 11, 12] </ref>; the "QR-factorization with column pivoting" [18], used in our previously implemented sequential code, is the most commonly implemented strategy [14, 1].
Reference: 8. <author> Bischof, C., M. Marques, & X. Sun, </author> <title> Parallel bandreduction and tridiagonalization, </title> <booktitle> Proceedings, Sixth SIAM Conference on Parallel Processing for Scientific Computing (Norfolk, </booktitle> <address> Virginia, </address> <month> March 22-24, </month> <editor> 1993) (R. F. Sincovec, eds.), </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993, </year> <note> (also PRISM Working Note #8). </note>
Reference-contexts: Optimal load balancing occurs when each node has the same amount of data for each matrix. The implementation guarantees that all matrix operands end up in place. Bischof and Sun <ref> [9, 10, 8] </ref> have developed a new method for accomplishing Step 3. The application of the sequence of polynomials P k in Step 2 produces a matrix M all of whose eigenvalues are either zero or one. <p> Bischof and Sun, on the other hand, are developing a parallel implementation for square meshes of the band reduction paradigm described in [10]. The reduction of a full matrix to tridiagonal form is a special case covered by this paradigm. This approach is described in more detail in <ref> [8] </ref> in these proceedings. 3. Parallel Issues for SYISDA 3.1. The early stages. For large problems, the early stages of SYISDA are dominated by the computation time for performing the matrix multiplications in Step 2 and the divide in Step 3.
Reference: 9. <author> Bischof, C. & X. Sun, </author> <title> A divide-and-conquer method for tridiagonalizing symmetric matrices with repeated eigenvalues, </title> <note> (also PRISM working note #1), Argonne National Laboratory (1992). </note>
Reference-contexts: Optimal load balancing occurs when each node has the same amount of data for each matrix. The implementation guarantees that all matrix operands end up in place. Bischof and Sun <ref> [9, 10, 8] </ref> have developed a new method for accomplishing Step 3. The application of the sequence of polynomials P k in Step 2 produces a matrix M all of whose eigenvalues are either zero or one.
Reference: 10. <author> Bischof, C. & X. Sun, </author> <title> A framework for symmetric band reduction and tridiagonalization, </title> <type> Preprint MCS-P298-0392, </type> <institution> Argonne National Laboratory (1992), </institution> <note> (also PRISM Working Note #3). </note>
Reference-contexts: Optimal load balancing occurs when each node has the same amount of data for each matrix. The implementation guarantees that all matrix operands end up in place. Bischof and Sun <ref> [9, 10, 8] </ref> have developed a new method for accomplishing Step 3. The application of the sequence of polynomials P k in Step 2 produces a matrix M all of whose eigenvalues are either zero or one. <p> Bischof and Sun, on the other hand, are developing a parallel implementation for square meshes of the band reduction paradigm described in <ref> [10] </ref>. The reduction of a full matrix to tridiagonal form is a special case covered by this paradigm. This approach is described in more detail in [8] in these proceedings. 3. Parallel Issues for SYISDA 3.1. The early stages.
Reference: 11. <author> Chan, T. F. & P. C. Hansen, </author> <title> Some applications of the rank revealing QR factorization, </title> <journal> SIAM Journal on Scientific and Statistical Computing 13 (1992), </journal> <volume> no. 3, </volume> <pages> 727-741. </pages>
Reference-contexts: To decompose the eigenproblem for A into two smaller ones, we must compute the range and null spaces of M . This is usually achieved with a so-called "rank-revealing orthogonal factorization" <ref> [7, 11, 12] </ref>; the "QR-factorization with column pivoting" [18], used in our previously implemented sequential code, is the most commonly implemented strategy [14, 1].
Reference: 12. <author> Chandrasekaran, S. & I. Ipsen, </author> <title> On rank-revealing QR factorizations, </title> <type> Technical Report YALEU/DCS/RR-880, </type> <institution> Yale University, Department of Computer Science, </institution> <year> 1991. </year>
Reference-contexts: To decompose the eigenproblem for A into two smaller ones, we must compute the range and null spaces of M . This is usually achieved with a so-called "rank-revealing orthogonal factorization" <ref> [7, 11, 12] </ref>; the "QR-factorization with column pivoting" [18], used in our previously implemented sequential code, is the most commonly implemented strategy [14, 1].
Reference: 13. <author> Choi, J., J. J. Dongarra, R. Pozo, & D. W. Walker, </author> <title> ScaLAPACK: A scalable linear algebra library for distributed memory concurrent computers, </title> <booktitle> Proceedings, Fourth Symposium on the Frontiers of Massively Parallel Computation (McLean, </booktitle> <address> Virginia, </address> <month> October 19-21, </month> <title> 1992), </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1992, </year> <pages> pp. 120-127. </pages>
Reference-contexts: In this way, machine-dependent communications are relegated as much as possible to these primitives, localizing their impact. Efforts to attain performance can then be devoted to optimizing these functions on the target machine. This philosophy is analogous to the reliance of packages such as LAPACK [2] and ScaLAPACK <ref> [13] </ref> on optimized BLAS. General parallel design issues for SYISDA on square meshes are discussed in Section 3. In particular, we describe how good load balancing in the critical early stages of SYISDA can be achieved by spreading the data for each subproblem over the entire mesh. <p> In particular, we describe how good load balancing in the critical early stages of SYISDA can be achieved by spreading the data for each subproblem over the entire mesh. This is accomplished on square meshes using the block scattered decomposition <ref> [13] </ref> and eliminates the need for costly and complicated data movement between stages. In the late stages of the algorithm, when communication overheads become much more significant, a one-time global data redistribution is performed to finish the small amount of remaining work.
Reference: 14. <author> Dongarra, J., C. B. Moler, J. R. Bunch, & G. W. Stewart, </author> <title> LINPACK User's Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: This is usually achieved with a so-called "rank-revealing orthogonal factorization" [7, 11, 12]; the "QR-factorization with column pivoting" [18], used in our previously implemented sequential code, is the most commonly implemented strategy <ref> [14, 1] </ref>. For computers with a memory hierarchy and distributed-memory machines, variations of this strategy have been devised [6, 5] which limit the adverse effect of the data-dependent column exchanges on the data reference locality and synchronization costs.
Reference: 15. <author> Dongarra, J. & R. van de Geijn, </author> <title> Reduction to condensed form for the eigenvalue problem on distributed-memory architectures, </title> <type> Technical Report ORNL/TM-12006, </type> <institution> Oak Ridge National Laboratory, Engineering Physics and Mathematics Division (January 1992). </institution>
Reference-contexts: The reduction of M to T and the accumulation of the transformation matrix Q dominate the invariant subspace computation. Several parallel tridiagonalization implementations have been suggested in the literature based either on the standard Householder tridiagonalization procedure (e.g., [20]) or a block version of this algorithm (e.g., <ref> [15] </ref>). Bischof and Sun, on the other hand, are developing a parallel implementation for square meshes of the band reduction paradigm described in [10]. The reduction of a full matrix to tridiagonal form is a special case covered by this paradigm.
Reference: 16. <author> Dunigan, T. H., </author> <title> Communication Performance of the Intel Touchstone Delta Mesh, </title> <type> Technical Report ORNL/TM-11983, </type> <institution> Oak Ridge National Laboratory (January, </institution> <year> 1992). </year>
Reference-contexts: Third, the Delta's separate message-passing chips (MRCs) and wormhole routing protocol reduce communication delays enough to be able to assume that "hops" are almost free. More details on Delta communications can be found in [27], <ref> [16] </ref>, [28], and [21]. The final important feature of the Delta we exploited is that the assembly-coded version of DGEMM can sustain a peak rate of 36:5 Mflops/sec per node of a possible 40 Mflops/sec (peak rate for one addition and one multiplication) for sufficiently large matrices.
Reference: 17. <author> Fox, G., S. Otto, & A. Hey, </author> <title> Matrix algorithms on a hypercube I: matrix multiplication, </title> <booktitle> Parallel Computing 4 (1987), </booktitle> <pages> 17-31. </pages>
Reference-contexts: Because SYISDA works with dense matrices, it is significantly more expensive from a computational complexity point of view than the widely-used QR algorithm. SYISDA is nonetheless a promising algorithm, since matrix multiplication is a preferred primitive on a wide variety of architectures <ref> [17, 24, 19] </ref>. For instance, on machines such as the Cray-2, Cray-YMP, and IBM RS6000/550, where optimized vendor-supplied BLAS are supplied, actual running times for SYISDA are usually within a factor of 3-6 of the symmetric QR algorithm, even when symmetry is not exploited. <p> This generalization, virtual two-dimensional panelled torus wrap, is discussed in another paper in these proceedings [22]. We implemented a variant of Broadcast-Multiply-Roll <ref> [17] </ref> in C using communication primitives highly suited to the Delta. In particular, since the Delta does not effectively overlap communication and computation, the algorithm we chose is highly synchronous. In addition, the algorithm maintains a large granularity so that the single node assembly-coded matrix multiplication could be exploited.
Reference: 18. <author> Golub, G. H., </author> <title> Numerical methods for solving linear least squares problems, </title> <booktitle> Numerische Mathematik 7 (1965), </booktitle> <pages> 206-216. </pages>
Reference-contexts: To decompose the eigenproblem for A into two smaller ones, we must compute the range and null spaces of M . This is usually achieved with a so-called "rank-revealing orthogonal factorization" [7, 11, 12]; the "QR-factorization with column pivoting" <ref> [18] </ref>, used in our previously implemented sequential code, is the most commonly implemented strategy [14, 1].
Reference: 19. <author> Golub, G. & C. F. Van Loan, </author> <title> Matrix Computations, 2nd ed., </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: Because SYISDA works with dense matrices, it is significantly more expensive from a computational complexity point of view than the widely-used QR algorithm. SYISDA is nonetheless a promising algorithm, since matrix multiplication is a preferred primitive on a wide variety of architectures <ref> [17, 24, 19] </ref>. For instance, on machines such as the Cray-2, Cray-YMP, and IBM RS6000/550, where optimized vendor-supplied BLAS are supplied, actual running times for SYISDA are usually within a factor of 3-6 of the symmetric QR algorithm, even when symmetry is not exploited.
Reference: 20. <author> Hendrikson, B. & D. Womble, </author> <title> The torus-wrap mapping for dense matrix calculations on massively parallel computers, </title> <institution> SAND92-0792, Sandia National Laboratories (1992). </institution>
Reference-contexts: The reduction of M to T and the accumulation of the transformation matrix Q dominate the invariant subspace computation. Several parallel tridiagonalization implementations have been suggested in the literature based either on the standard Householder tridiagonalization procedure (e.g., <ref> [20] </ref>) or a block version of this algorithm (e.g., [15]). Bischof and Sun, on the other hand, are developing a parallel implementation for square meshes of the band reduction paradigm described in [10]. The reduction of a full matrix to tridiagonal form is a special case covered by this paradigm.
Reference: 21. <author> Huss-Lederman, S., E. M. Jacobson, A. Tsao, & G. Zhang, </author> <title> Optimizing Communication Primitives on the Intel Touchstone Delta, </title> <note> Technical Note, Supercomputing Research Center (1993) (to appear). </note>
Reference-contexts: Third, the Delta's separate message-passing chips (MRCs) and wormhole routing protocol reduce communication delays enough to be able to assume that "hops" are almost free. More details on Delta communications can be found in [27], [16], [28], and <ref> [21] </ref>. The final important feature of the Delta we exploited is that the assembly-coded version of DGEMM can sustain a peak rate of 36:5 Mflops/sec per node of a possible 40 Mflops/sec (peak rate for one addition and one multiplication) for sufficiently large matrices.
Reference: 22. <author> Huss-Lederman, S., E. M. Jacobson, A. Tsao, & G. Zhang, </author> <title> Matrix Multiplication on the Intel Touchstone Delta, </title> <booktitle> Proceedings, Sixth SIAM Conference on Parallel Processing for Scientific Computing (Norfolk, </booktitle> <address> Virginia, </address> <month> March 22-24, </month> <editor> 1993) (R. F. Sincovec, eds.), </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993, </year> <note> (summary of results also appeared in FY 1991-1992 Annual Report of the The Concurrent Supercomputing Consortium; expanded version to appear as Technical Report, Supercomputing Research Center, 1993; also PRISM Working Note #7). </note>
Reference-contexts: This in turn provided the motivation for a new generalization of the block scattered decomposition on non-square meshes that also allowed for good performance in distributed matrix multiplication. This generalization, virtual two-dimensional panelled torus wrap, is discussed in another paper in these proceedings <ref> [22] </ref>. We implemented a variant of Broadcast-Multiply-Roll [17] in C using communication primitives highly suited to the Delta. In particular, since the Delta does not effectively overlap communication and computation, the algorithm we chose is highly synchronous. <p> Our code can deal with a variety of different blocking schemes, by means of a user-passed function providing block size information. More detailed information about our implementation is given in <ref> [22] </ref>. The code was developed with square matrices in mind and performs best when the matrix elements are spread out as uniformly as possible. Optimal load balancing occurs when each node has the same amount of data for each matrix. <p> Throughout this section, we assume that the computation is being done on a p fi p mesh. Because we chose a two-dimensional matrix multiplication algorithm, the two data layouts considered here are two-dimensional contiguous blocking and the block scattered decomposition (e.g., <ref> [22] </ref>). Let us briefly review both data layout strategies on a p fi p mesh. Let ! be the block size. For simplicity, assume that ! divides the matrix dimension, n.
Reference: 23. <author> Ipsen, I. & E. Jessup, </author> <title> Solving the symmetric tridiagonal eigenvalue problem on the hypercube, </title> <type> Tech. Rep. </type> <institution> RR-548, Yale University (1987). </institution>
Reference-contexts: The prevalence of distributed memory machines has spurred very active investigation into algorithms which map more readily onto these architectures. Promising algorithms that have been investigated include bisection/multisection coupled with inverse iteration, Divide and Conquer, Jacobi methods, and homotopy methods <ref> [29, 23, 4, 26] </ref>. The work presented in this paper is part of the Parallel Research on Invariant Subspace Methods (PRISM) project, composed of researchers from Argonne National Laboratory, Supercomputing Research Center, University of California, Berkeley, and University of Kentucky, which is investigating scalable algorithms for solving eigenproblems.
Reference: 24. <author> Johnsson, S. L. & C.-T. Ho, </author> <title> Matrix multiplication on boolean cubes using generic communication primitives, Parallel Processing and Medium Scale Multiprocessors, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1987. </year>
Reference-contexts: Because SYISDA works with dense matrices, it is significantly more expensive from a computational complexity point of view than the widely-used QR algorithm. SYISDA is nonetheless a promising algorithm, since matrix multiplication is a preferred primitive on a wide variety of architectures <ref> [17, 24, 19] </ref>. For instance, on machines such as the Cray-2, Cray-YMP, and IBM RS6000/550, where optimized vendor-supplied BLAS are supplied, actual running times for SYISDA are usually within a factor of 3-6 of the symmetric QR algorithm, even when symmetry is not exploited.
Reference: 25. <author> Lederman, S., A. Tsao, & T. Turnbull, </author> <title> A parallelizable eigensolver for real diagonalizable matrices with real eigenvalues, </title> <type> Technical Report TR-91-042, </type> <institution> Supercomputing Research Center (1991). </institution>
Reference-contexts: In particular, we present results from work-in-progress on an implementation of SYISDA for square meshes. SYISDA is a divide-and-conquer algorithm which derives most of its parallelism from performing matrix multiplication [3]. Recall the algorithm in <ref> [25] </ref>. fl This paper is PRISM Working Note #9, available via anonymous ftp to ftp.super.org in the directory pub/prism. y Supercomputing Research Center, 17100 Science Drive, Bowie, MD 20715 z CONVEX Computer Corporation, 3000 Waterview Parkway, Richardson, TX 75083-3851. <p> We believe that SYISDA is a potentially useful algorithm for large problems because of its scalable primitives. As discussed in <ref> [25] </ref>, Step 2 consists entirely of matrix multiplication and dominates the cost of the sequential algorithm. Because SYISDA works with dense matrices, it is significantly more expensive from a computational complexity point of view than the widely-used QR algorithm. <p> As an extreme example, when the spectral bounds in Step 1 indicate that a particular subproblem has clustered eigenvalues <ref> [25] </ref>, no iterations at all are required. In the early stages of SYISDA, the run time differences caused by poor load balancing could have an extremely adverse effect on performance, making dynamic load balancing problematic.
Reference: 26. <author> Li, T.-Y., H. Zhang, & X.-H. Sun, </author> <title> Parallel homotopy algorithm for symmetric tridiagonal eigenvalue problems, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 12 (1991), no. 3, </volume> <pages> 469-87. </pages>
Reference-contexts: The prevalence of distributed memory machines has spurred very active investigation into algorithms which map more readily onto these architectures. Promising algorithms that have been investigated include bisection/multisection coupled with inverse iteration, Divide and Conquer, Jacobi methods, and homotopy methods <ref> [29, 23, 4, 26] </ref>. The work presented in this paper is part of the Parallel Research on Invariant Subspace Methods (PRISM) project, composed of researchers from Argonne National Laboratory, Supercomputing Research Center, University of California, Berkeley, and University of Kentucky, which is investigating scalable algorithms for solving eigenproblems.
Reference: 27. <author> Regnier, G., </author> <title> Intel Touchstone Delta Message Passing Performance, Intel Supercomputing Systems Division (August, 1991), </title> <type> preprint. </type>
Reference-contexts: Third, the Delta's separate message-passing chips (MRCs) and wormhole routing protocol reduce communication delays enough to be able to assume that "hops" are almost free. More details on Delta communications can be found in <ref> [27] </ref>, [16], [28], and [21]. The final important feature of the Delta we exploited is that the assembly-coded version of DGEMM can sustain a peak rate of 36:5 Mflops/sec per node of a possible 40 Mflops/sec (peak rate for one addition and one multiplication) for sufficiently large matrices.
Reference: 28. <author> Regnier, G., </author> <title> Delta Message Passing Protocol, presentation overheads, </title> <booktitle> Proceedings, First Intel Delta Applications Workshop, </booktitle> <month> CCSF-14-92 (February, </month> <year> 1992), </year> <title> Caltech Concurrent Supercomputing Facilities, </title> <address> Pasadena, California, </address> <year> 1992, </year> <pages> pp. 173-178. </pages>
Reference-contexts: Third, the Delta's separate message-passing chips (MRCs) and wormhole routing protocol reduce communication delays enough to be able to assume that "hops" are almost free. More details on Delta communications can be found in [27], [16], <ref> [28] </ref>, and [21]. The final important feature of the Delta we exploited is that the assembly-coded version of DGEMM can sustain a peak rate of 36:5 Mflops/sec per node of a possible 40 Mflops/sec (peak rate for one addition and one multiplication) for sufficiently large matrices.
Reference: 29. <author> Schreiber, R., </author> <title> Solving eigenvalue and singular value problems on an undersized systolic array, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 7 (1986), no. 2, </volume> <pages> 441-451. </pages>
Reference-contexts: The prevalence of distributed memory machines has spurred very active investigation into algorithms which map more readily onto these architectures. Promising algorithms that have been investigated include bisection/multisection coupled with inverse iteration, Divide and Conquer, Jacobi methods, and homotopy methods <ref> [29, 23, 4, 26] </ref>. The work presented in this paper is part of the Parallel Research on Invariant Subspace Methods (PRISM) project, composed of researchers from Argonne National Laboratory, Supercomputing Research Center, University of California, Berkeley, and University of Kentucky, which is investigating scalable algorithms for solving eigenproblems.
References-found: 29

