URL: http://www.cs.brown.edu/courses/cs295-8/dtree.ps
Refering-URL: http://www.cs.brown.edu/courses/cs295-8/
Root-URL: http://www.cs.brown.edu/
Email: E-mail: shanir@math.tau.ac.il.  
Title: Diffracting Trees  
Author: Nir Shavit yz Asaph Zemach 
Address: Tel-Aviv 69978, Israel.  
Affiliation: Department of Computer Science, Tel-Aviv University,  
Note: A preliminary version of this work appeared in the Proceedings of the Annual Symposium on Parallel Algorithms and Architectures (SPAA),  Contact Author:  
Date: August 15, 1996  June 1994.  
Abstract: Shared counters are among the most basic coordination structures in multiprocessor computation, with applications ranging from barrier synchronization to concurrent-data-structure design. This paper introduces diffracting trees, novel data structures for shared counting and load balancing in a distributed parallel environment. Empirical evidence, collected on a simulated distributed shared-memory machine and several simulated message passing architectures, shows that diffracting trees scale better and are more robust than both combining trees and counting networks, currently the most effective known methods for implementing concurrent counters in software. The use of a randomized coordination method together with a combinatorial data structure overcomes the resiliency drawbacks of combining trees. Our simulations show that to handle the same load, diffracting trees and counting networks should have a similar width w, yet the depth of a diffracting tree is O(log w), whereas counting networks have depth O(log 2 Diffracting trees have already been used to implement highly efficient producer/consumer queues, and we believe diffraction will prove to be an effective alternative paradigm to combining and queue locking in the design of many concurrent data structures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal and M. Cherian. </author> <title> Adaptive Backoff Synchronization Techniques. </title> <booktitle> In the Proceedings of the 16th International Symposium on Computer Architecture, </booktitle> <pages> pp. 396-406, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Given that the majority of current multiprocessor architectures do not provide specialized hardware support for efficient counting, there is a growing need to develop effective software-based counting methods. The simplest way to implement a counter is to place it in a spin-lock protected critical section, adding an exponential-back-off mechanism <ref> [1, 6, 23] </ref> or a queue lock as devised by Anderson [6] and Mellor-Crummey and Scott [40] to reduce contention [20, 49]. Unfortunately, such centralized methods are inherently non-parallel and cannot hope to scale well. <p> In both to be at balancer b 0 . If the first compare-and-swap fails, it follows that some other processor has already managed to diffract p, so p is directed to the b-&gt;next <ref> [1] </ref> balancer (Line 11). <p> phase 1 **********************/ 3 location [mypid] := b 4 place := random (1,b-&gt;size) 5 him := register_to_memory_swap (b-&gt;prism [place],mypid) 6 if not_empty (him) then 7 if compare_and_swap (location [mypid],b,EMPTY) then 8 if compare_and_swap (location [him],b,EMPTY) then 9 return b-&gt;next [0] 10 else location [mypid] := b 11 else return b-&gt;next <ref> [1] </ref> 12 endif /********************** phase 2 **********************/ 13 while true 14 repeat b-&gt;spin times 15 if location [mypid] &lt;&gt; b then 16 return b-&gt;next [1] 17 endrepeat 18 if test_and_set (b-&gt;lock) then 19 if compare_and_swap (location [mypid],b,EMPTY) 20 then 21 i := b-&gt;toggle 22 b-&gt;toggle := not (i) 23 b-&gt;lock := <p> 7 if compare_and_swap (location [mypid],b,EMPTY) then 8 if compare_and_swap (location [him],b,EMPTY) then 9 return b-&gt;next [0] 10 else location [mypid] := b 11 else return b-&gt;next <ref> [1] </ref> 12 endif /********************** phase 2 **********************/ 13 while true 14 repeat b-&gt;spin times 15 if location [mypid] &lt;&gt; b then 16 return b-&gt;next [1] 17 endrepeat 18 if test_and_set (b-&gt;lock) then 19 if compare_and_swap (location [mypid],b,EMPTY) 20 then 21 i := b-&gt;toggle 22 b-&gt;toggle := not (i) 23 b-&gt;lock := FALSE 24 return b-&gt;next [i] 25 else 26 b-&gt;lock := FALSE 27 return b-&gt;next [1] 28 endif 29 endif 30 endwhile 31 end 9 <p> if location [mypid] &lt;&gt; b then 16 return b-&gt;next <ref> [1] </ref> 17 endrepeat 18 if test_and_set (b-&gt;lock) then 19 if compare_and_swap (location [mypid],b,EMPTY) 20 then 21 i := b-&gt;toggle 22 b-&gt;toggle := not (i) 23 b-&gt;lock := FALSE 24 return b-&gt;next [i] 25 else 26 b-&gt;lock := FALSE 27 return b-&gt;next [1] 28 endif 29 endif 30 endwhile 31 end 9 p's PID from prism time to diffract p. The amount of time is dependent on the value of the spin field of each balancer. A higher spin value indicates more time is spent waiting to be diffracted. <p> If location [p] could not be erased it follows that some other processor already collided with p, and p exits the balancer, being diffracted to b-&gt;next <ref> [1] </ref> (Lines 26-27). If the lock could not be seized, processor p resumes spinning. Notice that before accessing the toggle bit or trying to diffract, p clears location [p] using a compare-and-swap operation. <p> This protects us from situations where some processor q is diffracted by p without noticing. The construction works because it assures that for every processor being diffracted left (to b-&gt;next [0]), there is exactly one processor diffracted right (to b-&gt;next <ref> [1] </ref>). Since all other processors go through the toggle bit a balance is maintained. A formal proof is given in Section 5.2. 3.1 Some Implementation Details The following discussion assumes an implementation on a machine that supports a globally addressable, physically distributed memory model. <p> The choice of these parameters is obviously architecture dependent. In our simulations we used for the variable size the values 8,4,2,2 and 1, for levels 0; : : : ; 4 of a width 32 tree respectively. We also used a form of adaptive (exponential) back-off <ref> [1] </ref> on the spin to facilitate rapid access to the toggle bit in reduced load situations. Each processor kept a local copy of the tree's spin variables and used them as initial values for the back-off. <p> spin: integer end location: global array [1..NUMPROCS] of ptr to balancer 1 function diff-bal (b: ptr to balancer): ptr to balancer 2 begin /**************** phase 1 is unchanged ****************/ /********************** phase 2 **********************/ 13 while true 14 repeat b-&gt;spin times 15 if location [mypid] &lt;&gt; b then 16 return b-&gt;next <ref> [1] </ref> 17 endrepeat 18 if test_and_set (b-&gt;lock) then 19 if compare_and_swap (location [mypid],b,EMPTY) 20 then 21 i := b-&gt;toggle 22 b-&gt;toggle := not (i) 23 b-&gt;lock := FALSE 24 if b-&gt;spin &gt; 1 then b-&gt;spin := b-&gt;spin / 2 25 return b-&gt;next [i] 26 else 27 b-&gt;lock := FALSE 28 return <p> endrepeat 18 if test_and_set (b-&gt;lock) then 19 if compare_and_swap (location [mypid],b,EMPTY) 20 then 21 i := b-&gt;toggle 22 b-&gt;toggle := not (i) 23 b-&gt;lock := FALSE 24 if b-&gt;spin &gt; 1 then b-&gt;spin := b-&gt;spin / 2 25 return b-&gt;next [i] 26 else 27 b-&gt;lock := FALSE 28 return b-&gt;next <ref> [1] </ref> 29 endif 30 endif 31 if b-&gt;spin &lt; MAXSPIN then b-&gt;spin := b-&gt;spin * 2 32 endwhile 33 end 13 type balancer is begin size: integer spin: integer prism: array [1..size] of integer toggle: boolean next: array [0..1] of ptr to balancer end location: global array [1..NUMPROCS] of ptr to <p> phase 1 **********************/ 3 location [mypid] := b 4 place := random (1,b-&gt;size) 5 him := register_to_memory_swap (b-&gt;prism [place],mypid) 6 if not_empty (him) then 7 if compare_and_swap (location [mypid],b,EMPTY) then 8 if compare_and_swap (location [him],b,EMPTY) then 9 return b-&gt;next [0] 10 else location [mypid] := b 11 else return b-&gt;next <ref> [1] </ref> 12 endif /********************** phase 2 **********************/ 13 repeat b-&gt;spin times 14 if location [mypid] &lt;&gt; b then 15 return b-&gt;next [1] 16 endrepeat 17 if compare_and_swap (location [mypid],b,EMPTY) then 18 i := fetch_and_complement (b-&gt;toggle) 19 return b-&gt;next [i] 20 else 21 return return b-&gt;next [1] 22 endif 23 end 14 <p> not_empty (him) then 7 if compare_and_swap (location [mypid],b,EMPTY) then 8 if compare_and_swap (location [him],b,EMPTY) then 9 return b-&gt;next [0] 10 else location [mypid] := b 11 else return b-&gt;next <ref> [1] </ref> 12 endif /********************** phase 2 **********************/ 13 repeat b-&gt;spin times 14 if location [mypid] &lt;&gt; b then 15 return b-&gt;next [1] 16 endrepeat 17 if compare_and_swap (location [mypid],b,EMPTY) then 18 i := fetch_and_complement (b-&gt;toggle) 19 return b-&gt;next [i] 20 else 21 return return b-&gt;next [1] 22 endif 23 end 14 not perform complete hardware simulations. <p> b 11 else return b-&gt;next <ref> [1] </ref> 12 endif /********************** phase 2 **********************/ 13 repeat b-&gt;spin times 14 if location [mypid] &lt;&gt; b then 15 return b-&gt;next [1] 16 endrepeat 17 if compare_and_swap (location [mypid],b,EMPTY) then 18 i := fetch_and_complement (b-&gt;toggle) 19 return b-&gt;next [i] 20 else 21 return return b-&gt;next [1] 22 endif 23 end 14 not perform complete hardware simulations. Instead, operations which are local (do not interact with the parallel environment) are run directly on the simulating machine's CPU and memory. <p> Canceling tokens leave the balancer through b-&gt;next [0] the first output wire, canceled tokens leave through b-&gt;next <ref> [1] </ref> the second output wire. Toggling tokens may leave through either wire. Lemma 5.9 In a quiescent state the toggling tokens have the step property.
Reference: [2] <author> E. Aharonson and H. Attiya. </author> <title> Counting networks with arbitrary fan out. </title> <booktitle> In the Proceedings of the 3 rd ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <address> Orlando, Florida, </address> <month> January </month> <year> 1992. </year> <title> Also: </title> <type> Technical Report 679, </type> <institution> The Technion, </institution> <month> June </month> <year> 1991. </year> <month> 45 </month>
Reference-contexts: There is a wide body of theoretical research analyzing the performance of counting networks and attempting to improve on their O (log 2 w) depth <ref> [2, 5, 7, 12, 13, 18, 27, 32, 33] </ref>. The most effective is the elegant combinatorial design due to Klugerman and Plaxton [32, 33] of depth close to O (log w). Unfortunately, the "exponentially large" constants involved make these constructions impractical.
Reference: [3] <author> A. Agarwal, D. Chaiken, K. Johnson, D. Krantz, J. Kubiatowicz, K. Kurihara, B. Lim, G. Maa, and D. Nussbaum. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. In Scalable Shared Memory Multiprocessors, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> Also as MIT Technical Report MIT/LCS/TM-454, </note> <month> June </month> <year> 1991. </year>
Reference-contexts: The Proteus Parallel Hardware Simulator [10, 11] of Brewer, Dellarocas, Colbrook and Weihl was used to evaluate performance in a shared memory architecture similar to the Alewife machine of Agarwal, Chaiken, Johnson, Krantz, Kubia-towicz, Kurihara, Lim, Maa, and Nussbaumet <ref> [3] </ref>. Netsim, part of the Rice Parallel Processing Testbed [15, 29] developed by Covington, Dwarkadas, Jump, Sinclair, and Madala was used for testing in message passing architectures. <p> All three operations can be implemented in a lock-free [25] manner using the load-linked/store-conditional operations available on many modern architectures [16, 39]. On machines like the MIT Alewife <ref> [3] </ref> that support full-empty bits in hardware, the compare and swap operations can be directly replaced by loads and stores that interact/are-conditioned on the bit [4]. The code translates into the following sequence of operations (illustrated in Figure 5) performed by a process shepherding a token through a balancer. <p> proof that the implementation in Figure 7 is wait-free is given in Lemma 5.19. 3.3 Performance We evaluated the performance of diffracting trees relative to other known methods by running a collection of benchmarks on a simulated distributed-shared-memory multiprocessor similar to the MIT Alewife machine developed by Agarwal et. al. <ref> [3] </ref>. Our simulations were performed using Proteus 2 , a multiprocessor simulator developed by Brewer et. al. [11]. Proteus simulates parallel code by multiplexing several parallel threads on a single CPU. <p> The graphs in Figures 8 and 9 show the throughput and latency of the various counting methods. Our performance graphs for the known methods other than Diffracting trees conform with previous findings and in particular, agree with the results of Herlihy, Lim and Shavit [24] on ASIM <ref> [3] </ref>, the Alewife machine hardware simulator 3 . It is clear from these graphs that the MCS lock and the lock with exponential backoff do not 3 To confirm our findings we reproduced their experiments with Proteus and got nearly identical results.
Reference: [4] <author> Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, Godfrey D'Souza, and Mike Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Large-Scale Multiprocessors. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 48-61, </pages> <month> June </month> <year> 1993 </year>
Reference-contexts: The left-hand side of Figure 1 shows such an execution on a Binary <ref> [4] </ref> type counting tree (width 4) which we define formally below. As can be seen, the network moves input tokens to output wires in increasing order modulo w. <p> On machines like the MIT Alewife [3] that support full-empty bits in hardware, the compare and swap operations can be directly replaced by loads and stores that interact/are-conditioned on the bit <ref> [4] </ref>. The code translates into the following sequence of operations (illustrated in Figure 5) performed by a process shepherding a token through a balancer. In Phase 1 processor p announces the arrival of its token at balancer b 0 , by writing b 0 to location [p] (Line 3). <p> In order to facilitate fast simulations, Proteus does 1 For this purpose a hardware fetch-and-complement is planned to be added to the next version of the Alewife's Sparcle processor <ref> [4] </ref> as a conditional store operation on a location with a full/empty bit.
Reference: [5] <author> B. Aiello, R. Venkatesan, and M. Yung. </author> <title> Coins, Weights and Contention in Balancing Networks. </title> <booktitle> In the Proceedings of the Thirteenth ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 193-214, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: There is a wide body of theoretical research analyzing the performance of counting networks and attempting to improve on their O (log 2 w) depth <ref> [2, 5, 7, 12, 13, 18, 27, 32, 33] </ref>. The most effective is the elegant combinatorial design due to Klugerman and Plaxton [32, 33] of depth close to O (log w). Unfortunately, the "exponentially large" constants involved make these constructions impractical. <p> Finally, it would be interesting to extend the use of diffraction to other forms of counting networks such as those of Felten, LaMarca, and Ladner [18], Aiello, Venkatesan, and Yung <ref> [5] </ref>, and Busch and Mavronicolas [13, 14]. 7 Acknowledgments We wish to thank Dan Touitou for his many insightful observations and the anonymous referees for their many valuable comments. Thanks are also due to Allan Fekete for his careful proof-reading of the final manuscript.
Reference: [6] <author> T.E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Given that the majority of current multiprocessor architectures do not provide specialized hardware support for efficient counting, there is a growing need to develop effective software-based counting methods. The simplest way to implement a counter is to place it in a spin-lock protected critical section, adding an exponential-back-off mechanism <ref> [1, 6, 23] </ref> or a queue lock as devised by Anderson [6] and Mellor-Crummey and Scott [40] to reduce contention [20, 49]. Unfortunately, such centralized methods are inherently non-parallel and cannot hope to scale well. <p> The simplest way to implement a counter is to place it in a spin-lock protected critical section, adding an exponential-back-off mechanism [1, 6, 23] or a queue lock as devised by Anderson <ref> [6] </ref> and Mellor-Crummey and Scott [40] to reduce contention [20, 49]. Unfortunately, such centralized methods are inherently non-parallel and cannot hope to scale well. This is true also of hardware supported fetch-and-increment operations unless the hardware itself employs one of the parallel methods described below. <p> To make the comparisons fair, the code for each method below was optimized, as was the distribution of the data structures in the machine's memory. The methods are: ExpBackoff A counter protected by a lock using test-and-test-and-set with exponential backoff <ref> [6, 23] </ref>. MCS A counter protected by the queue-lock of Mellor-Crummey and Scott [40]. Processors waiting for the lock form a linked list, each pointing to its predecessor. At the "head" of the list is the processor who has the lock. <p> The wires/pointers from one balancer to another are cached locally by processors, while the toggle bit in shared memory is protected by a spin-lock with exponential backoff <ref> [6, 23] </ref>. Each output wire ends in a local counter implemented using a short critical section protected by a test-and-test-and-set lock with exponential backoff [6, 23]. <p> wires/pointers from one balancer to another are cached locally by processors, while the toggle bit in shared memory is protected by a spin-lock with exponential backoff <ref> [6, 23] </ref>. Each output wire ends in a local counter implemented using a short critical section protected by a test-and-test-and-set lock with exponential backoff [6, 23]. The counting network width of 64 was chosen based on preliminary testing that showed it provides the best throughput/average latency over a range of up to 256 processors.
Reference: [7] <author> J. Aspnes, M.P. Herlihy, and N. Shavit. </author> <title> Counting Networks and Multi-Processor Coordination. </title> <booktitle> In the Proceedings of the 23rd ACM Annual Symposium on Theory of Computing, </booktitle> <pages> pp. 348-358, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The combining trees of Yew, Tzeng, and Lawrie [49] and Goodman, Vernon, and Woest [21], and the counting networks of Aspnes, Herlihy, and Shavit <ref> [7] </ref>, both meet the above criteria, and indeed were found to be the most effective methods for concurrent counting in software. A combining tree is a distributed binary-tree based data structure with a shared counter at its root. <p> However, this throughput is highly dependent on processor timings, and a single processor's delay or failure can delay all others indefinitely. A Bitonic counting network <ref> [7] </ref> is a distributed data structure having a layout isomorphic to Batcher's Bitonic sorting network [8], with a "local counter" at the end of each output wire. <p> There is a wide body of theoretical research analyzing the performance of counting networks and attempting to improve on their O (log 2 w) depth <ref> [2, 5, 7, 12, 13, 18, 27, 32, 33] </ref>. The most effective is the elegant combinatorial design due to Klugerman and Plaxton [32, 33] of depth close to O (log w). Unfortunately, the "exponentially large" constants involved make these constructions impractical. <p> Unfortunately, the "exponentially large" constants involved make these constructions impractical. This paper introduces diffracting trees, a new distributed technique for shared counting, enjoying the benefits of the above methods and avoiding many of their drawbacks. Diffracting trees, like counting networks <ref> [7] </ref>, are constructed from simple one-input two-output computing elements called balancers that are connected to one another by wires to form a balanced binary tree. Tokens arrive on the balancer's input wire at arbitrary times, and are output on its output wires. <p> contains formal correctness proofs for all our constructions, and Section 6 concludes this paper and lists areas of further research. 2 Trees that Count We begin by introducing the abstract notion of a counting tree, a special form of the counting network data structures introduced by Aspnes, Herlihy, and Shavit <ref> [7] </ref>. A counting tree balancer is a computing element with one input wire and two output wires. Tokens arrive on the balancer's input wire at arbitrary times, and are output on its output wires. <p> Formal definitions of the properties of balancing networks can be found elsewhere <ref> [7] </ref>. On a shared memory multiprocessor one can implement a balancing tree as a shared data structure, where balancers are records, and wires are pointers from one record to another. <p> CNet The Bitonic counting network of Aspnes, Herlihy, and Shavit <ref> [7] </ref> of width 64. A Bitonic counting network is a network of two-input-two-output balancers having a layout isomorphic to a Bitonic sorting network [8]. <p> One would hope this means 20 that a network of width w = 16 could deliver top throughput performance of 16=t cnet , for, say 1=2w (log w)(1 + log w) = 160 processors. Unfortunately, as empirical testing shows <ref> [7, 24] </ref>, if the counting network is loaded to that extent, t cnet for each balancer tends to degrade (grow) rapidly due to contention and sequential bottlenecking. <p> Our formal model for multiprocessor computation follows <ref> [7, 36] </ref>. First a formal description of a balancer is given, then it is shown that any Binary counting tree counts, that is, its outputs have the step property. <p> Let the state of a balancer at a given time be defined as the collection of tokens on its input and output wires <ref> [7] </ref>. We denote by x the number of input tokens ever received on the balancer's input wire, and by y i ; i 2 f0; 1g the number of tokens ever output on its ith output wire. <p> We present the following useful lemmas due to Aspnes, Herlihy, and Shavit <ref> [7] </ref>. Lemma 5.1 If y 0 ; : : :; y w1 is a sequence of non-negative integers, the following statements are equivalent: 1. For any i &lt; j, 0 y i y j 1. <p> A counter can be constructed from a Binary [w] tree by adding local counters to the tree's output wires. We paraphrase a lemma <ref> [7] </ref> which equates counting with balancing. Lemma 5.6 Consider a Binary [w] tree based shared counter as described above. Let x be the largest number returned by any increment operation on the counter.
Reference: [8] <author> K.E. Batcher. </author> <title> Sorting Networks and their Applications. </title> <booktitle> In the Proceedings of AFIPS Joint Computer Conference, </booktitle> <pages> pages 338-334, </pages> <year> 1968. </year>
Reference-contexts: However, this throughput is highly dependent on processor timings, and a single processor's delay or failure can delay all others indefinitely. A Bitonic counting network [7] is a distributed data structure having a layout isomorphic to Batcher's Bitonic sorting network <ref> [8] </ref>, with a "local counter" at the end of each output wire. Unlike queue-locks and combining trees which are based on a single counter location handing out indices, counting networks have a collection of w separate counter locations. <p> CNet The Bitonic counting network of Aspnes, Herlihy, and Shavit [7] of width 64. A Bitonic counting network is a network of two-input-two-output balancers having a layout isomorphic to a Bitonic sorting network <ref> [8] </ref>. Each processor performing an increment operation travels 16 through the network from input wires to output wires toggling the shared bits in the balancers along its path.
Reference: [9] <author> R.D. Blumofe, and C.E. Leiserson. </author> <title> Sheduling Multithreaded Computations by Work Stealing. </title> <booktitle> In the Proceeding of the 35th Symposium on Foundations of Computer Science (FOCS '94), </booktitle> <pages> pp. 356-368, </pages> <month> Nov. </month> <year> 1994. </year>
Reference: [10] <author> E.A. Brewer, C.N. Dellarocas. </author> <title> Proteus User Documentation. MIT, 545 Technology Square, </title> <address> Cambridge, MA 02139, 0.5 edition, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: Diffracting trees thus combine the the high degree of parallelism and fault-tolerance of counting networks with the beneficial utilization of "collisions" of a combining tree. We compared the performance of diffracting trees to the above methods in simulated shared memory and message passing environments. The Proteus Parallel Hardware Simulator <ref> [10, 11] </ref> of Brewer, Dellarocas, Colbrook and Weihl was used to evaluate performance in a shared memory architecture similar to the Alewife machine of Agarwal, Chaiken, Johnson, Krantz, Kubia-towicz, Kurihara, Lim, Maa, and Nussbaumet [3].
Reference: [11] <author> E.A. Brewer, C.N. Dellarocas, A. Colbrook and W.E. Weihl. Proteus: </author> <title> A High-Performance Parallel-Architecture Simulator. </title> <type> MIT Technical Report /MIT/LCS/TR-561, </type> <month> September </month> <year> 1991. </year>
Reference-contexts: Diffracting trees thus combine the the high degree of parallelism and fault-tolerance of counting networks with the beneficial utilization of "collisions" of a combining tree. We compared the performance of diffracting trees to the above methods in simulated shared memory and message passing environments. The Proteus Parallel Hardware Simulator <ref> [10, 11] </ref> of Brewer, Dellarocas, Colbrook and Weihl was used to evaluate performance in a shared memory architecture similar to the Alewife machine of Agarwal, Chaiken, Johnson, Krantz, Kubia-towicz, Kurihara, Lim, Maa, and Nussbaumet [3]. <p> Our simulations were performed using Proteus 2 , a multiprocessor simulator developed by Brewer et. al. <ref> [11] </ref>. Proteus simulates parallel code by multiplexing several parallel threads on a single CPU. Each thread runs on its own virtual CPU with accompanying local memory, cache and communications hardware, keeping track of how much time is spent using each component.
Reference: [12] <author> C. Busch and M. Mavronicolas. </author> <title> A Combinatorial Treatment of Balancing Networks. </title> <booktitle> In Thirteenth ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 206-215, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: There is a wide body of theoretical research analyzing the performance of counting networks and attempting to improve on their O (log 2 w) depth <ref> [2, 5, 7, 12, 13, 18, 27, 32, 33] </ref>. The most effective is the elegant combinatorial design due to Klugerman and Plaxton [32, 33] of depth close to O (log w). Unfortunately, the "exponentially large" constants involved make these constructions impractical.
Reference: [13] <author> C. Busch and M. Mavronicolas. </author> <title> A Logarithmic Depth Counting Network. </title> <booktitle> Annouced in Four-teeth ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 274, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: There is a wide body of theoretical research analyzing the performance of counting networks and attempting to improve on their O (log 2 w) depth <ref> [2, 5, 7, 12, 13, 18, 27, 32, 33] </ref>. The most effective is the elegant combinatorial design due to Klugerman and Plaxton [32, 33] of depth close to O (log w). Unfortunately, the "exponentially large" constants involved make these constructions impractical. <p> Finally, it would be interesting to extend the use of diffraction to other forms of counting networks such as those of Felten, LaMarca, and Ladner [18], Aiello, Venkatesan, and Yung [5], and Busch and Mavronicolas <ref> [13, 14] </ref>. 7 Acknowledgments We wish to thank Dan Touitou for his many insightful observations and the anonymous referees for their many valuable comments. Thanks are also due to Allan Fekete for his careful proof-reading of the final manuscript.
Reference: [14] <author> C. Busch and M. Mavronicolas. </author> <title> Load Balancing Networks. </title> <booktitle> Annouced in Fourteeth ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 275, </pages> <month> August </month> <year> 1995. </year> <month> 46 </month>
Reference-contexts: Finally, it would be interesting to extend the use of diffraction to other forms of counting networks such as those of Felten, LaMarca, and Ladner [18], Aiello, Venkatesan, and Yung [5], and Busch and Mavronicolas <ref> [13, 14] </ref>. 7 Acknowledgments We wish to thank Dan Touitou for his many insightful observations and the anonymous referees for their many valuable comments. Thanks are also due to Allan Fekete for his careful proof-reading of the final manuscript.
Reference: [15] <author> R. G. Covington, S. Dwarkadas, J. R. Jump, J. B. Sinclair, S. Madala. </author> <title> The Efficient Simulation of Parallel Computer Systems. </title> <journal> International Journal in Computer Simulations, </journal> <volume> Vol. 1, </volume> <pages> pp. 31-58, </pages> <year> (1991). </year>
Reference-contexts: The Proteus Parallel Hardware Simulator [10, 11] of Brewer, Dellarocas, Colbrook and Weihl was used to evaluate performance in a shared memory architecture similar to the Alewife machine of Agarwal, Chaiken, Johnson, Krantz, Kubia-towicz, Kurihara, Lim, Maa, and Nussbaumet [3]. Netsim, part of the Rice Parallel Processing Testbed <ref> [15, 29] </ref> developed by Covington, Dwarkadas, Jump, Sinclair, and Madala was used for testing in message passing architectures. We found that, in shared-memory systems, diffracting trees substantially outperform both combining trees and counting networks, currently the most effective known methods for shared counting. <p> Netsim is a generic network simulator, developed as part of the Rice Parallel Processing Testbed <ref> [15] </ref>. The simulation is event driven, implying that time progresses from event to event, operations performed between events, which do not interact with the simulated network, take no time.
Reference: [16] <author> Digital Equipment Corporation. </author> <title> Alpha system reference manual. </title>
Reference-contexts: All three operations can be implemented in a lock-free [25] manner using the load-linked/store-conditional operations available on many modern architectures <ref> [16, 39] </ref>. On machines like the MIT Alewife [3] that support full-empty bits in hardware, the compare and swap operations can be directly replaced by loads and stores that interact/are-conditioned on the bit [4].
Reference: [17] <author> C. Dwork, M. P. Herlihy, and O. Waarts. </author> <title> Contention in shared memory algorithms. </title> <booktitle> In the Proceedings of the 25th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 174-183, </pages> <month> May </month> <year> 1993. </year> <note> Expanded version: Digital Equipment Corporation Technical Report CRL 93/12. </note>
Reference-contexts: It would also be interesting to formally analyze diffracting tree behavior using newly developed models of contention such as that of Dwork, Herlihy, and Waarts <ref> [17] </ref>.
Reference: [18] <author> E.W. Felten, A. LaMarca, R. Ladner. </author> <title> Building Counting Networks from Larger Balancers. </title> <institution> University of Washington T.R. #93-04-09. </institution>
Reference-contexts: There is a wide body of theoretical research analyzing the performance of counting networks and attempting to improve on their O (log 2 w) depth <ref> [2, 5, 7, 12, 13, 18, 27, 32, 33] </ref>. The most effective is the elegant combinatorial design due to Klugerman and Plaxton [32, 33] of depth close to O (log w). Unfortunately, the "exponentially large" constants involved make these constructions impractical. <p> The counting network width of 64 was chosen based on preliminary testing that showed it provides the best throughput/average latency over a range of up to 256 processors. We note that Felten, LaMarca, and Ladner <ref> [18] </ref> show network designs using higher fan-in/out balancers which can get up to a 25% performance improvement over the Bitonic network. DTree A Diffracting Tree of width 32. The graphs in Figures 8 and 9 show the throughput and latency of the various counting methods. <p> Finally, it would be interesting to extend the use of diffraction to other forms of counting networks such as those of Felten, LaMarca, and Ladner <ref> [18] </ref>, Aiello, Venkatesan, and Yung [5], and Busch and Mavronicolas [13, 14]. 7 Acknowledgments We wish to thank Dan Touitou for his many insightful observations and the anonymous referees for their many valuable comments. Thanks are also due to Allan Fekete for his careful proof-reading of the final manuscript.
Reference: [19] <author> E. Freudenthal and A. Gottlieb. </author> <title> Process Coordination with Fetch-and-Increment. </title> <booktitle> In the Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), SIGOPS Operating Systems Review Special Issue, </booktitle> <pages> page 260, </pages> <address> April 1991, Santa Clara, California. </address>
Reference-contexts: to imagine a program that doesn't count something, and indeed, on multiprocessor machines shared counters are the key to solving a variety of coordination problems such as barrier synchronization [40], index distribution, shared program counters [41] and the design of concurrent data structures such as queues and stacks (see also <ref> [19, 22, 47] </ref>). In its purest form, a counter is an object that holds an integer value and provides a fetch-and-increment operation, incrementing the counter and returning its previous value.
Reference: [20] <author> D. Gawlick. </author> <title> Processing 'hot spots' in high performance systems. </title> <booktitle> In the Proceedings IEEE COMPCON'85, </booktitle> <month> Feb. </month> <year> 1985. </year>
Reference-contexts: The simplest way to implement a counter is to place it in a spin-lock protected critical section, adding an exponential-back-off mechanism [1, 6, 23] or a queue lock as devised by Anderson [6] and Mellor-Crummey and Scott [40] to reduce contention <ref> [20, 49] </ref>. Unfortunately, such centralized methods are inherently non-parallel and cannot hope to scale well. This is true also of hardware supported fetch-and-increment operations unless the hardware itself employs one of the parallel methods described below. <p> This means that it can support the same kind of throughput to w independent counters with much 2 lower latency. However, it seems that we are back to square one since the root of the tree will be a "hot spot" <ref> [20, 42] </ref> and a sequential bottleneck that is no better than a centralized counter implementation. This would indeed be true if one were to use the accepted (counting network) implementation of a balancer a single location with a bit toggled by each passing token.
Reference: [21] <author> J.R. Goodman, M.K. Vernon, and P.J. Woest. </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent multiprocessors. </title> <booktitle> In the Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 64-75, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The combining trees of Yew, Tzeng, and Lawrie [49] and Goodman, Vernon, and Woest <ref> [21] </ref>, and the counting networks of Aspnes, Herlihy, and Shavit [7], both meet the above criteria, and indeed were found to be the most effective methods for concurrent counting in software. A combining tree is a distributed binary-tree based data structure with a shared counter at its root. <p> The code was taken directly from Mellor-Crummey and Scott's article [40] and implemented using atomic register-to-memory-swap and compare-and-swap operations. CTree A counter at the root of an optimal width combining tree using the protocol of Goodman et al. <ref> [21] </ref> as modified by Herlihy, Lim, and Shavit [24]. A combining tree is a distributed data structure with the layout of a binary tree. Optimal width means that when n processors participate in the simulation, a tree of width n=2 is used [24].
Reference: [22] <author> A. Gottlieb, B.D. Lubachevsky, and L. Rudolph. </author> <title> Basic techniques for the efficient coordination of very large numbers of cooperating sequential processors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(2) </volume> <pages> 164-189, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: to imagine a program that doesn't count something, and indeed, on multiprocessor machines shared counters are the key to solving a variety of coordination problems such as barrier synchronization [40], index distribution, shared program counters [41] and the design of concurrent data structures such as queues and stacks (see also <ref> [19, 22, 47] </ref>). In its purest form, a counter is an object that holds an integer value and provides a fetch-and-increment operation, incrementing the counter and returning its previous value.
Reference: [23] <author> G. Graunke and S. Thakkar. </author> <title> Synchronization Algorithms for Shared-Memory Multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 60-70, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Given that the majority of current multiprocessor architectures do not provide specialized hardware support for efficient counting, there is a growing need to develop effective software-based counting methods. The simplest way to implement a counter is to place it in a spin-lock protected critical section, adding an exponential-back-off mechanism <ref> [1, 6, 23] </ref> or a queue lock as devised by Anderson [6] and Mellor-Crummey and Scott [40] to reduce contention [20, 49]. Unfortunately, such centralized methods are inherently non-parallel and cannot hope to scale well. <p> To make the comparisons fair, the code for each method below was optimized, as was the distribution of the data structures in the machine's memory. The methods are: ExpBackoff A counter protected by a lock using test-and-test-and-set with exponential backoff <ref> [6, 23] </ref>. MCS A counter protected by the queue-lock of Mellor-Crummey and Scott [40]. Processors waiting for the lock form a linked list, each pointing to its predecessor. At the "head" of the list is the processor who has the lock. <p> The wires/pointers from one balancer to another are cached locally by processors, while the toggle bit in shared memory is protected by a spin-lock with exponential backoff <ref> [6, 23] </ref>. Each output wire ends in a local counter implemented using a short critical section protected by a test-and-test-and-set lock with exponential backoff [6, 23]. <p> wires/pointers from one balancer to another are cached locally by processors, while the toggle bit in shared memory is protected by a spin-lock with exponential backoff <ref> [6, 23] </ref>. Each output wire ends in a local counter implemented using a short critical section protected by a test-and-test-and-set lock with exponential backoff [6, 23]. The counting network width of 64 was chosen based on preliminary testing that showed it provides the best throughput/average latency over a range of up to 256 processors.
Reference: [24] <author> M. Herlihy, B.H. Lim and N. Shavit. </author> <title> Low Contention Load Balancing on Large Scale Multiprocessors. </title> <booktitle> In the Proceedings of the 3rd Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <address> July 1992, San Diego, CA. </address> <note> Full version available as a DEC TR. </note>
Reference-contexts: Unfortunately, such centralized methods are inherently non-parallel and cannot hope to scale well. This is true also of hardware supported fetch-and-increment operations unless the hardware itself employs one of the parallel methods described below. A recent survey of counting techniques by Herlihy, Lim, and Shavit <ref> [24] </ref> suggests that scalable counting can only be achieved by methods that are distributed and therefore have low contention on memory and interconnect, and are parallel, and thus allow many requests to be dealt with concurrently. <p> Two benchmarks were used to test the performance of diffracting trees: index-distribution and job queues. 3.3.1 Index Distribution Benchmark Index-distribution is a load balancing technique, in which processors dynamically choose loop iterations to execute in parallel. As mentioned elsewhere <ref> [24] </ref>, a simple example of index distribution is the problem of rendering the Mandelbrot Set. Each loop iteration covers a rectangle in the screen. <p> The code was taken directly from Mellor-Crummey and Scott's article [40] and implemented using atomic register-to-memory-swap and compare-and-swap operations. CTree A counter at the root of an optimal width combining tree using the protocol of Goodman et al. [21] as modified by Herlihy, Lim, and Shavit <ref> [24] </ref>. A combining tree is a distributed data structure with the layout of a binary tree. Optimal width means that when n processors participate in the simulation, a tree of width n=2 is used [24]. <p> tree using the protocol of Goodman et al. [21] as modified by Herlihy, Lim, and Shavit <ref> [24] </ref>. A combining tree is a distributed data structure with the layout of a binary tree. Optimal width means that when n processors participate in the simulation, a tree of width n=2 is used [24]. Every node of the tree (including the leaves) contains a spin-lock, and the root contains a local counter. Each pair of processors is accorded a leaf. In order to reach the counter at the root, a processor's request to increment the counter must ascend the tree from a leaf. <p> The graphs in Figures 8 and 9 show the throughput and latency of the various counting methods. Our performance graphs for the known methods other than Diffracting trees conform with previous findings and in particular, agree with the results of Herlihy, Lim and Shavit <ref> [24] </ref> on ASIM [3], the Alewife machine hardware simulator 3 . It is clear from these graphs that the MCS lock and the lock with exponential backoff do not 3 To confirm our findings we reproduced their experiments with Proteus and got nearly identical results. <p> The scalable throughput of diffracting trees is to a large extent a result of their ability to withstand high loads with low contention as explained above, coupled with their low depth. To see why this is so consider the following "back of the envelope" calculation. Optimal depth combining trees <ref> [24] </ref> have a depth of log n=2 where n is the number of processes. <p> One would hope this means 20 that a network of width w = 16 could deliver top throughput performance of 16=t cnet , for, say 1=2w (log w)(1 + log w) = 160 processors. Unfortunately, as empirical testing shows <ref> [7, 24] </ref>, if the counting network is loaded to that extent, t cnet for each balancer tends to degrade (grow) rapidly due to contention and sequential bottlenecking. <p> The combining tree is severely affected by fluctuations in arrival times (see also <ref> [24] </ref>) and scales poorly. As seen in Figure 8 the diffracting tree shows a drop in performance when the number of processors goes from 224 to 256. This suggests the need to increase the size of the tree if more processors are to be used. <p> If t has the special value BLOCK, the routine waits indefinitely. send message (t,m) Sends the message m to the thread t. random (a,b) Returns a random integer in the interval [a,b]. Section 5.3 contains the correctness proof for this implementation. 4.2 Measuring Performance It has been shown <ref> [24] </ref> that for the cache-coherent Alewife architecture on which our shared memory counting methods were tested, message passing implementations are significantly faster than shared memory ones. <p> Since Netsim, unlike Proteus, is an event based simulator, there is no need to take in to account start-up times all processors are ready at the same time. Since it has already been shown <ref> [24] </ref> that centralized counting methods do not scale well, we compare only the three distributed-parallel counting methods: CNet [w] A message based Bitonic counting network, implemented in the obvious way: balancers are threads of control and tokens are messages. <p> For combining trees, in all the network architectures we tested, as the range of work between counter accesses grew, variations in the arrival rates of requests made combining more difficult, and performance degraded. This conforms with the observations of Herlihy, Lim, and Shavit <ref> [24] </ref> that combining trees perform poorly (lower percentages of requests are combined) when the load drops. A dramatic example of this can be seen in the tests on the torus mesh network with single wire switches (the same network used for Figure 18) under different workloads (Figures 19 through 20).
Reference: [25] <author> M.P. Herlihy. </author> <title> A methodology for implementing highly concurrent data structures. </title> <booktitle> In the Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 197-206, </pages> <address> Seattle, WA, </address> <month> March 14-16 </month> <year> 1990. </year>
Reference-contexts: All three operations can be implemented in a lock-free <ref> [25] </ref> manner using the load-linked/store-conditional operations available on many modern architectures [16, 39]. On machines like the MIT Alewife [3] that support full-empty bits in hardware, the compare and swap operations can be directly replaced by loads and stores that interact/are-conditioned on the bit [4].
Reference: [26] <author> M.P. Herlihy. </author> <title> Wait-Free Synchronization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(1) </volume> <pages> 123-149, </pages> <month> January </month> <year> 1991. </year> <month> 47 </month>
Reference-contexts: They scale better, giving higher throughput over a large number of processors, and are more robust in terms of their ability to handle unexpected latencies and differing loads. Note also that like counting networks but unlike combining trees, diffracting trees can be implemented in a wait-free <ref> [26] </ref> manner (given the appropriate hardware primitives). By this we mean that for each increment operation termination is guaranteed in a bounded number of steps independently of the pace or even a possible halting failure of all other processors. <p> In fact, replacing the toggling operation with a hardware fetch-and-complement operation would make the diffracting tree and counting network implementations wait-free <ref> [26] </ref>. That is, the number of steps needed to increment the shared counter is bounded by a constant, regardless of the actions of other processors.
Reference: [27] <author> M.P. Herlihy, N. Shavit, and O. Waarts. </author> <title> Linearizable Counting Networks. </title> <booktitle> In the Proceedings of the 32 nd Annual Symposium on Foundations of Computer Science, </booktitle> <address> San Juan, Puerto Rico, </address> <month> October </month> <year> 1991, </year> <pages> pp. 526-535. </pages> <note> Detailed version with empirical results appeared as MIT/LCS technical manuscript 459, </note> <month> November </month> <year> 1991. </year>
Reference-contexts: There is a wide body of theoretical research analyzing the performance of counting networks and attempting to improve on their O (log 2 w) depth <ref> [2, 5, 7, 12, 13, 18, 27, 32, 33] </ref>. The most effective is the elegant combinatorial design due to Klugerman and Plaxton [32, 33] of depth close to O (log w). Unfortunately, the "exponentially large" constants involved make these constructions impractical.
Reference: [28] <author> K. Hwang. </author> <title> Advanced Computer Architecture. </title> <publisher> McGraw-Hill Computer Engineering Series. </publisher> <address> ISBN 0-07-031622-8. </address>
Reference-contexts: This type of modeling reflects current trends in computer architecture, where network speeds dominate scalability since they do not improve as fast as processor speeds <ref> [28] </ref>. Our experiments included four types of networks: a torus mesh network with single wire switches, a torus mesh network with crossbar switches, a butterfly network with crossbar switches, and an n fi n crossbar network.
Reference: [29] <author> J. R. </author> <title> Jump Netsim Reference Manual. </title> <institution> Rice University. </institution> <note> Available by ftp from titan.cs.rice.edu as /public/parallel/sim.tar.Z. </note>
Reference-contexts: The Proteus Parallel Hardware Simulator [10, 11] of Brewer, Dellarocas, Colbrook and Weihl was used to evaluate performance in a shared memory architecture similar to the Alewife machine of Agarwal, Chaiken, Johnson, Krantz, Kubia-towicz, Kurihara, Lim, Maa, and Nussbaumet [3]. Netsim, part of the Rice Parallel Processing Testbed <ref> [15, 29] </ref> developed by Covington, Dwarkadas, Jump, Sinclair, and Madala was used for testing in message passing architectures. We found that, in shared-memory systems, diffracting trees substantially outperform both combining trees and counting networks, currently the most effective known methods for shared counting. <p> (mybalancer-&gt;spin) 6 if (message1 &lt;&gt; NULL) then /* perform diffraction */ 7 dispatch_message (mybalancer-&gt;next [0],message0) 8 dispatch_message (mybalancer-&gt;next [1],message1) 9 else /* send to toggle */ 10 send_message (mybalancer-&gt;toggle,message0) 11 endif 12 end 26 We tested the performance of the message passing diffracting trees in simulated network envi-ronments using Netsim <ref> [29] </ref>. Netsim is a generic network simulator, developed as part of the Rice Parallel Processing Testbed [15]. The simulation is event driven, implying that time progresses from event to event, operations performed between events, which do not interact with the simulated network, take no time.
Reference: [30] <author> S. Kirkpatrick and C. D. Gelatt and M. P. Vecchi. </author> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> Vol. 220, </volume> <year> 1983, </year> <pages> pages 671-680. </pages>
Reference-contexts: While a counting network's layout can also be optimized (though to a lesser extent than a combining tree), the dynamic flow patterns of diffracting trees make layout optimization much less effective. In our experiments we used the simulated annealing algorithm <ref> [30] </ref> to attempt to minimize the average distance traveled per message for each data structure.
Reference: [31] <author> J. Kubiatowicz. </author> <title> Personal communication (February 1995). </title>
Reference-contexts: The machines' concurrent logging mechanism will have all processors repeatedly write blocks of logged operations onto multiple disks. The counter handing out next-available-disk-block locations will be a test case for a diffracting counter solution to what would otherwise be a hot-spot and a sequential bottleneck <ref> [31] </ref>. As mentioned earlier, given the hardware fetch-and-complement operation to be added to the Sparcle chip's set of conditional load/store operations, one will be able to implement a shared 44 memory diffracting-tree based counter in a wait-free manner, that is, without any locks.
Reference: [32] <author> M. Klugerman and C.G. Plaxton. </author> <title> Small-depth Counting Networks. </title> <booktitle> In the Proceedings of the 24th ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pp. 417-428, </pages> <year> 1992. </year>
Reference-contexts: There is a wide body of theoretical research analyzing the performance of counting networks and attempting to improve on their O (log 2 w) depth <ref> [2, 5, 7, 12, 13, 18, 27, 32, 33] </ref>. The most effective is the elegant combinatorial design due to Klugerman and Plaxton [32, 33] of depth close to O (log w). Unfortunately, the "exponentially large" constants involved make these constructions impractical. <p> There is a wide body of theoretical research analyzing the performance of counting networks and attempting to improve on their O (log 2 w) depth [2, 5, 7, 12, 13, 18, 27, 32, 33]. The most effective is the elegant combinatorial design due to Klugerman and Plaxton <ref> [32, 33] </ref> of depth close to O (log w). Unfortunately, the "exponentially large" constants involved make these constructions impractical. This paper introduces diffracting trees, a new distributed technique for shared counting, enjoying the benefits of the above methods and avoiding many of their drawbacks.
Reference: [33] <author> M. Klugerman, </author> <title> Small-Depth Counting Networks. </title> <type> Ph.D. Thesis, </type> <institution> MIT, </institution> <year> 1994. </year>
Reference-contexts: There is a wide body of theoretical research analyzing the performance of counting networks and attempting to improve on their O (log 2 w) depth <ref> [2, 5, 7, 12, 13, 18, 27, 32, 33] </ref>. The most effective is the elegant combinatorial design due to Klugerman and Plaxton [32, 33] of depth close to O (log w). Unfortunately, the "exponentially large" constants involved make these constructions impractical. <p> There is a wide body of theoretical research analyzing the performance of counting networks and attempting to improve on their O (log 2 w) depth [2, 5, 7, 12, 13, 18, 27, 32, 33]. The most effective is the elegant combinatorial design due to Klugerman and Plaxton <ref> [32, 33] </ref> of depth close to O (log w). Unfortunately, the "exponentially large" constants involved make these constructions impractical. This paper introduces diffracting trees, a new distributed technique for shared counting, enjoying the benefits of the above methods and avoiding many of their drawbacks.
Reference: [34] <author> Nancy A. Lynch. </author> <title> Distributed Algorithms. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, Calif., </address> <year> 1996. </year>
Reference-contexts: We assume the machine's shared memory to be a collection of "memory locations," each of which follows the specification of of an atomic register <ref> [34] </ref>. The operations on each memory location (and therefore the values it takes) can be ordered chronologically, and atomicity assures us that this ordering is well defined. Thus one can draw a time-line of events for each of the memory locations making up the location array.
Reference: [35] <author> R. Luling, and B. Monien. </author> <title> A Dynamic Distributed Load Balancing Algorithm with Provable Good Performance. </title> <booktitle> In the Proceedings of the 5rd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 164-173, </pages> <month> June </month> <year> 1993. </year>
Reference: [36] <author> N.A. Lynch and M.R. Tuttle. </author> <title> Hierarchical Correctness Proofs for Distributed Algorithms. </title> <booktitle> In Sixth ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 137-151, </pages> <month> August </month> <year> 1987. </year> <note> Full version available as MIT Technical Report MIT/LCS/TR-387. </note>
Reference-contexts: Our formal model for multiprocessor computation follows <ref> [7, 36] </ref>. First a formal description of a balancer is given, then it is shown that any Binary counting tree counts, that is, its outputs have the step property.
Reference: [37] <author> B.H. Lim and A. Agarwal. </author> <title> Reactive Synchronization Algorithms for Multiprocessors. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pp. 25-35, </pages> <year> 1994. </year>
Reference-contexts: However, since the interval in which a given width is optimal increases with tree size, the wider tree can usually be used without fear. Also, the application of an adaptive scheme for changing diffracting tree size "on the fly" (see for example <ref> [37] </ref>) will most likely not result in frequent changes among different width trees.
Reference: [38] <author> Udi Manber. </author> <title> On maintaining dynamic information in a concurrent environment SIAM J. </title> <booktitle> Computing 15(4), </booktitle> <pages> pages 1130-1142, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: We are also developing a version of diffracting trees for non-coherent shared memory machines such as the Cray T3D [48]. A recent paper by Shavit and Touitou [44] introduces "Elimination Trees," a new form of Diffracting trees that can be used to create highly parallel producer/consumer pools and stacks <ref> [38, 43] </ref>. The algorithms provide superior response (on average just a few machine instructions) under high loads with a guaranteed logarithmic (in w) number of steps under sparse request patterns.
Reference: [39] <institution> MIPS Computer Company. The MIPS RISC Architecture. </institution>
Reference-contexts: All three operations can be implemented in a lock-free [25] manner using the load-linked/store-conditional operations available on many modern architectures <ref> [16, 39] </ref>. On machines like the MIT Alewife [3] that support full-empty bits in hardware, the compare and swap operations can be directly replaced by loads and stores that interact/are-conditioned on the bit [4].
Reference: [40] <author> J.M. Mellor-Crummey and M.L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <type> Technical Report 342, </type> <institution> University of Rochester, Rochester, </institution> <address> NY 14627, </address> <month> April </month> <year> 1990. </year> <month> 48 </month>
Reference-contexts: 1 Introduction It is hard to imagine a program that doesn't count something, and indeed, on multiprocessor machines shared counters are the key to solving a variety of coordination problems such as barrier synchronization <ref> [40] </ref>, index distribution, shared program counters [41] and the design of concurrent data structures such as queues and stacks (see also [19, 22, 47]). <p> The simplest way to implement a counter is to place it in a spin-lock protected critical section, adding an exponential-back-off mechanism [1, 6, 23] or a queue lock as devised by Anderson [6] and Mellor-Crummey and Scott <ref> [40] </ref> to reduce contention [20, 49]. Unfortunately, such centralized methods are inherently non-parallel and cannot hope to scale well. This is true also of hardware supported fetch-and-increment operations unless the hardware itself employs one of the parallel methods described below. <p> The methods are: ExpBackoff A counter protected by a lock using test-and-test-and-set with exponential backoff [6, 23]. MCS A counter protected by the queue-lock of Mellor-Crummey and Scott <ref> [40] </ref>. Processors waiting for the lock form a linked list, each pointing to its predecessor. At the "head" of the list is the processor who has the lock. To free the lock, the head processor hands ownership to its successor, and so on, down the list. <p> While waiting for the lock, processors spin locally on their own node in the linked list. The lock has a single "tail" pointer which directs new processors wishing to acquire the lock to the end of the queue. The code was taken directly from Mellor-Crummey and Scott's article <ref> [40] </ref> and implemented using atomic register-to-memory-swap and compare-and-swap operations. CTree A counter at the root of an optimal width combining tree using the protocol of Goodman et al. [21] as modified by Herlihy, Lim, and Shavit [24]. <p> This is not surprising, since both are methods for eliminating contention but do not support parallelism. Our results for the MCS lock differ from those of Mellor-Crummey and Scott <ref> [40] </ref> due to differences in machine architecture. In their BBN Butterfly experiments if two read-modify-write operations are performed on the same memory location (such as register-to-memory-swap on the lock's tail pointer) one will succeed immediately and the other is blocked and retried later.
Reference: [41] <author> J.M. Mellor-Crummey and T.J. LeBlanc. </author> <title> A software instruction counter. </title> <booktitle> In the Proceedings of the 3rd ACM International Conference On Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 78-86, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: 1 Introduction It is hard to imagine a program that doesn't count something, and indeed, on multiprocessor machines shared counters are the key to solving a variety of coordination problems such as barrier synchronization [40], index distribution, shared program counters <ref> [41] </ref> and the design of concurrent data structures such as queues and stacks (see also [19, 22, 47]). In its purest form, a counter is an object that holds an integer value and provides a fetch-and-increment operation, incrementing the counter and returning its previous value.
Reference: [42] <author> G.H. Pfister and A. Norton. </author> <title> `Hot Spot' contention and combining in multistage interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(11):933-938, </volume> <month> November </month> <year> 1985. </year>
Reference-contexts: This means that it can support the same kind of throughput to w independent counters with much 2 lower latency. However, it seems that we are back to square one since the root of the tree will be a "hot spot" <ref> [20, 42] </ref> and a sequential bottleneck that is no better than a centralized counter implementation. This would indeed be true if one were to use the accepted (counting network) implementation of a balancer a single location with a bit toggled by each passing token.
Reference: [43] <author> L. Rudolph, M. Slivkin, and E. Upfal. </author> <title> A Simple Load Balancing Scheme for Task Allocation in Parallel Machines. </title> <booktitle> In the Proceedings of the 3rd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 237-245, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: We are also developing a version of diffracting trees for non-coherent shared memory machines such as the Cray T3D [48]. A recent paper by Shavit and Touitou [44] introduces "Elimination Trees," a new form of Diffracting trees that can be used to create highly parallel producer/consumer pools and stacks <ref> [38, 43] </ref>. The algorithms provide superior response (on average just a few machine instructions) under high loads with a guaranteed logarithmic (in w) number of steps under sparse request patterns.
Reference: [44] <author> N. Shavit, and D. Touito. </author> <title> Elimination Trees and the Construction of Pools and Stacks. </title> <booktitle> To appear in the Proceedings of the Fifteenth ACM Symposium on Principles of Distributed Computing, </booktitle> <address> Philadelphia, </address> <month> May 23-26, </month> <year> 1996. </year>
Reference-contexts: The machine is due to become operational in 1996. We are also developing a version of diffracting trees for non-coherent shared memory machines such as the Cray T3D [48]. A recent paper by Shavit and Touitou <ref> [44] </ref> introduces "Elimination Trees," a new form of Diffracting trees that can be used to create highly parallel producer/consumer pools and stacks [38, 43].
Reference: [45] <author> N. Shavit, E. Upfal, and A. Zemach. </author> <title> A Steady-State Analysis of Diffracting Trees. </title> <booktitle> To appear in the Proceedings of the Eight Annual Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <address> Padua, Italy, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: On the more theoretical side, combining trees have the advantage of offering a general fetch-and- operation, and it would be interesting to find out if a variant of diffracting could provide such a property. A recent paper by Shavit, Upfal, and Zemach <ref> [45] </ref> provides a combinatorial model and steady-state analysis that confirm some of the empirical results observed in this paper and offer a collection of improvements, among them a more "stable" diffracting tree algorithm.
Reference: [46] <author> N. Shavit and A. Zemach. </author> <title> Diffracting Trees. </title> <booktitle> In the Proceedings of the Fifth Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <month> June </month> <year> 1994. </year>
Reference: [47] <author> H.S. Stone. </author> <title> Database applications of the fetch-and-add instruction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-33(7):604-612, </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: to imagine a program that doesn't count something, and indeed, on multiprocessor machines shared counters are the key to solving a variety of coordination problems such as barrier synchronization [40], index distribution, shared program counters [41] and the design of concurrent data structures such as queues and stacks (see also <ref> [19, 22, 47] </ref>). In its purest form, a counter is an object that holds an integer value and provides a fetch-and-increment operation, incrementing the counter and returning its previous value.
Reference: [48] <author> Cray Research. </author> <title> CRAY T3D System Architecture Overview. </title> <note> Available via WWW as http://www.cray.com/PUBLIC/product-info/mpp/T3D Architecture Over. </note>
Reference-contexts: Further enhancements to Alewife's LimitLess protocol will hopefully allow to improve performance even further. The machine is due to become operational in 1996. We are also developing a version of diffracting trees for non-coherent shared memory machines such as the Cray T3D <ref> [48] </ref>. A recent paper by Shavit and Touitou [44] introduces "Elimination Trees," a new form of Diffracting trees that can be used to create highly parallel producer/consumer pools and stacks [38, 43].
Reference: [49] <author> P.C Yew, N.F. Tzeng, and D.H. Lawrie. </author> <title> Distributing Hot-Spot Addressing in Large-Scale Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 388-395, </pages> <month> April </month> <year> 1987. </year> <month> 49 </month>
Reference-contexts: The simplest way to implement a counter is to place it in a spin-lock protected critical section, adding an exponential-back-off mechanism [1, 6, 23] or a queue lock as devised by Anderson [6] and Mellor-Crummey and Scott [40] to reduce contention <ref> [20, 49] </ref>. Unfortunately, such centralized methods are inherently non-parallel and cannot hope to scale well. This is true also of hardware supported fetch-and-increment operations unless the hardware itself employs one of the parallel methods described below. <p> The combining trees of Yew, Tzeng, and Lawrie <ref> [49] </ref> and Goodman, Vernon, and Woest [21], and the counting networks of Aspnes, Herlihy, and Shavit [7], both meet the above criteria, and indeed were found to be the most effective methods for concurrent counting in software.
References-found: 49

