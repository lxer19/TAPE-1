URL: http://www.cs.rice.edu:80/~ychu/ps/aspc-ipps.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~ychu/newpapers.html
Root-URL: 
Email: ychu@cs.rice.edu  
Title: Optimal All-to-Some Personalized Communication on Hypercubes nication step, a hypercube node can send n elements
Author: Y. Charlie Hu 
Address: 6100 Main Street Houston, TX 77005-1892  
Affiliation: Department of Computer Science Rice University  
Note: To appear in the 12th International Parallel Processing Symposium  N to 4(log M M N 1). At one commu  
Abstract: In a hypercube multiprocessor with distributed memory, each data element has a street address and an apartment number (i.e. a hypercube node address and a local memory address). We describe an optimal algorithm for performing the all-to-some personalized communication (ASPC) on Boolean n-cubes, defined as (ijj) ! (i 2 j jj), i 2 [0; 2 n 1], j 2 [0; n 1], where (ijj) denote the data element on node i and location j. The algorithm also gives an optimal schedule for emulating PM2I networks on hy-percubes under the binary-reflected Gray code encoding. We also study an important class of parallel algorithms, called 2 b -descend, which perform log M iterations on an M -element input a[0 : M 1]. For b = log M 1; :::; 0, iteration b computes new values of each a[i] as a function of a[i]; a[i + 2 b ]; a[i 2 b ]. For large applications, the problem size M is typically much larger than the number of nodes N . We show that on hypercubes, the optimal ASPC algorithm devised in this paper can be used in combination with pipelining communication and computation in 2 b - descend computations to reduce the communication steps from 2 log N M 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Chen and Y. Hu. </author> <title> Optimizations for compiling itera tive spatial loops to massively parallel machines. </title> <booktitle> In Lan guages and Compilers for Parallel Computing, Lecture Notes in Computer Science 757. </booktitle> <address> SpringerVerlag, </address> <year> 1993. </year>
Reference-contexts: Schedule i (j) for n = 3. * Step 2: Repeat Step 1, except j 2 <ref> [1; n 1] </ref>. The route that element (ijj) takes is (ijj) ! (i i (j) jj) ! ((i i (j) ) i i (j) (j) jj) = (i + 2 j jj). starting at node i at location j are indicated by i j . <p> The above algorithm can be effectively derived via loop transformations on the original 2 b -descend algorithm after first splitting the node address index loop and the local memory address index loop, and can therefore be automated by a compiler, as we have previously demonstrated in <ref> [1] </ref>. 5. Conclusions We have described an optimal algorithm for performing the all-to-some personalized communication (ASPC) on Boolean n-cubes with all-port communication. The algorithm effectively gives an optimal schedule for emulating PM2I networks on hypercubes under the binary-reflected Gray code encoding.
Reference: [2] <author> R. W. Hockney and C. Jesshope. </author> <title> Parallel Computers. </title> <editor> Adam Hilger, </editor> <year> 1981. </year>
Reference-contexts: Another interesting example is Parallel Cyclic Reduction (PCR) <ref> [2] </ref> which is a well-known method for solving tridiagonal systems. In [4], a general algorithm for the class of 2 b -descend computations requiring O (log N ) communication steps on SIMD hypercubes was developed.
Reference: [3] <author> S. L. Johnsson. </author> <title> Communication efficient basic linear alge bra computations on hypercube architectures. </title> <journal> J. Parallel Dis tributed Computing, </journal> <volume> 4(2):133172, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: On such hypercubes, it has been shown that a 2 b permutation can be performed in O (1) steps, using a Gray-code mapping of data elements to the hypercube nodes <ref> [3] </ref>. The algorithm involves concurrent communications on different nodes along different links, although one link at a time. Straightforward use of this algorithm for ASPC would require O (n) communication steps.
Reference: [4] <author> D. Nassimi. </author> <title> Parallel algorithms for the classes of 2 b DE SEND and ASEND computations on SIMD hypercube. </title> <journal> IEEE Trans. Para. and Dist. Sys., </journal> <volume> 4(12):1372 1381, </volume> <month> December </month> <year> 1993. </year>
Reference-contexts: The inverse is called a 2 b permutation. If we restrict to always using the same links of the different nodes links that change the same bit of the node addresses (called SIMD hypercubes in <ref> [4] </ref>) at a time, it has been proved that at least log N b communication steps are required for a 2 b permutation no matter what mapping mechanism is used to embed the data elements [4]. <p> nodes links that change the same bit of the node addresses (called SIMD hypercubes in <ref> [4] </ref>) at a time, it has been proved that at least log N b communication steps are required for a 2 b permutation no matter what mapping mechanism is used to embed the data elements [4]. On SIMD hypercubes, a hypercube node can send one element along one of its n links at one communication step. <p> The algorithm effectively gives an opti mal schedule for emulating PM2I networks on hypercubes under the binary-reflected Gray code encoding. We also study an important class of parallel algorithms, called 2 b -descend <ref> [4] </ref>, which perform log M iterations on an M -element input a [0 : M 1]. <p> For b = log M 1; :::; 0, iteration b computes new values of each a [i] as a function of a [i]; a [i + 2 b ], and a [i 2 b ]. In <ref> [4] </ref>, an algorithm requiring O (log N ) communication steps on SIMD hypercubes was developed. However, the algorithm assumes that the number of data elements is the same as the number of hypercube nodes. Furthermore, the algorithm performs three times more redundant computations at each iteration. <p> Furthermore, the algorithm performs three times more redundant computations at each iteration. For large applications, the problem size M is typically much larger than the number of nodes N . Directly applying the method in <ref> [4] </ref> thus will result in 2 log N M N computation steps, plus the redundant computations. <p> Note that the schedule can be precomputed and each node only needs to store its n entries. 4. Application of ASPC to 2 b -Descend Com putations In this section, we consider an important class of parallel algorithms called 2 b -descend <ref> [4] </ref>, and show how to pipeline communication and computation to expose the ASPC communication pattern. We then apply our optimal ASPC algorithm to reduce the communication complexity of these algorithms. The class of 2 b -descend computations is defined in [4] in the following generic form. <p> an important class of parallel algorithms called 2 b -descend <ref> [4] </ref>, and show how to pipeline communication and computation to expose the ASPC communication pattern. We then apply our optimal ASPC algorithm to reduce the communication complexity of these algorithms. The class of 2 b -descend computations is defined in [4] in the following generic form. <p> Algorithm PM2B-DESEND 1) begin 2) for b = log N 1 downto 0 do 3) a [i] f b;i (a [i]; a [i + 2 b modN ], a [i2 b modN ]); 0 i N 1 4) end Examples of 2 b -descend include those studied in <ref> [4] </ref>, such as odd-even merge, cyclic-shift permutation, and parallel-prefix computation. Another interesting example is Parallel Cyclic Reduction (PCR) [2] which is a well-known method for solving tridiagonal systems. In [4], a general algorithm for the class of 2 b -descend computations requiring O (log N ) communication steps on SIMD hypercubes <p> ], a [i2 b modN ]); 0 i N 1 4) end Examples of 2 b -descend include those studied in <ref> [4] </ref>, such as odd-even merge, cyclic-shift permutation, and parallel-prefix computation. Another interesting example is Parallel Cyclic Reduction (PCR) [2] which is a well-known method for solving tridiagonal systems. In [4], a general algorithm for the class of 2 b -descend computations requiring O (log N ) communication steps on SIMD hypercubes was developed. <p> For large applications on modern massively parallel processors, the problem size M is typically much larger than the number of nodes N . Directly applying the method in <ref> [4] </ref> thus will result in 2 log N M N computation steps, in addition to the redundant computations. In the following, we describe how to pipeline the communication and computation for the elements of a node, and make use of the ASPC algorithm developed in the previous section. <p> In <ref> [4] </ref>, the class of 2 b -ascend computations was also defined to be simply reversing the iteration order. It was claimed to be harder than 2 b -descend computations, and an O (log 2 N= log log N ) algorithm was developed.
Reference: [5] <author> E. Reingold, J. Nievergelt, and N. Deo. </author> <title> Combinatorial Algo rithms. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs. NJ, </address> <year> 1977. </year>
Reference-contexts: Each node has n outgoing links and n incoming links. Let link k be the link that changes the kth bit of the end node addresses. 2.2. Binary-Reflected Gray Code The binary-reflected gray code sequence <ref> [5] </ref> derives its name from the fact the second half of the code is identical to the first half in reverse order.
Reference: [6] <author> H. J. Siegel. </author> <title> Interconnection Networks for Large Scale Par allel Processing. </title> <publisher> Lexington Books, </publisher> <year> 1985. </year> <month> 5 </month>
Reference-contexts: We represent the address of the jth element on node i as (ijj). We define the all-to-some personalized communication (ASPC) pattern to be (ijj) ! (i2 j jj), i 2 [0; 2 n 1], j 2 [0; n1]. This pattern reflects the inter-connection of PM2I networks <ref> [6] </ref>. It can also be viewed as performing n concurrent 2 b permutations and n concurrent 2 b permutations on an n-dimensional hypercube with b 2 [0; :::; n 1].
References-found: 6

