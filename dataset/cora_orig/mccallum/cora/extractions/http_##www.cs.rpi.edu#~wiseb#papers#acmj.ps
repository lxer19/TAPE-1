URL: http://www.cs.rpi.edu/~wiseb/papers/acmj.ps
Refering-URL: http://www.cs.rpi.edu/~wiseb/mybib.html
Root-URL: http://www.cs.rpi.edu
Email: glinert@cs.rpi.edu, kliner@cs.rpi.edu, gormsby@gunfight.austin.ibm.com, wiseb@cs.rpi.edu.  
Title: UnWindows: Bringing Multimedia Computing to Users with Disabilities an ongoing effort whose ultimate goal is
Author: Ephraim P. Glinert Richard L. Kline Gary R. Ormsby G. Bowden Wise 
Note: The UnWindows project is  This research was supported, in part, by the National Science Foundation under contracts CDA-8805910, CDA-9015249, CDA-9214887, CDA-9214892. and IRI-9213823. Authors' internet e-mail addresses:  Ormsby's current address: IBM Corporation, Internal  
Address: Troy, NY 12180  9372, 11400 Burnet Road, Austin, TX 78758.  
Affiliation: Computer Science Department Rensselaer Polytechnic Institute  Zip  
Abstract: In this paper, we first discuss design considerations relating to the visual and aural modalities, both in general and for people with disabilities. We then review the recently introduced metawidget technology, which we've adopted for the second stage of UnWindows, now underway. We next present a historical overview and progress report on UnWindows, and conclude with plans for the future of the project. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Bowe. </author> <title> Personal Computers and Special Needs. </title> <publisher> Sybex Computer Books, </publisher> <year> 1984. </year>
Reference-contexts: If tasks demand separate resources, performance of two simultaneous tasks x In reality, people who can see even minimally will strongly resist nonvisual alternatives. Indeed, according to Bowe <ref> [1] </ref>, only about 10% of the blind or visually impaired are proficient at reading Braille. Some degree of magnification is certainly helpful to these users, but too much is counterproductive because it drastically reduces the computer-to-human communications bandwidth.
Reference: [2] <author> J. Elkind. </author> <title> The Incidence of Disabilities in the United States. </title> <booktitle> Human Factors, </booktitle> <volume> 32(4) </volume> <pages> 397-405, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: No problem, we'll provide a utility to automatically and uniformly magnify all output." Not quite! x Is it worth the effort to accommodate the special needs of disabled users? Humanitarian considerations aside, this is a huge market. By analyzing data from the 1990 census and other sources, Elkind <ref> [2] </ref> concluded that there are approximately 92 million people in the United States alone with an impairment of some sort.
Reference: [3] <author> C. D. Wickens. </author> <title> Engineering Psychology and Human Performance. </title> <editor> Charles E. Merrill, </editor> <address> Columbus, OH, </address> <year> 1984. </year>
Reference-contexts: Graphical marks: points, lines, areas. Graphical relationships: Positional: 1-D, 2-D, 3-D. Temporal: animation. Retinal: color, shape, size, saturation, texture, orientation. sentation Graphics. (i.e., time-sharing) will be more efficient. Wick-ens <ref> [3] </ref> has suggested that separate resource capacities are available for modality, processing (spatial vs. verbal), and stage of processing (early vs. late). Task interference occurs when the same resources are called upon simultaneously.
Reference: [4] <author> H. A. Rollins and R. Hendricks. </author> <title> Processing of Words Presented Simultaneously to Eye and Ear. </title> <journal> J. Experimental Psychology, </journal> <volume> 6(1) </volume> <pages> 99-109, </pages> <year> 1980. </year>
Reference-contexts: Wick-ens [3] has suggested that separate resource capacities are available for modality, processing (spatial vs. verbal), and stage of processing (early vs. late). Task interference occurs when the same resources are called upon simultaneously. Results from numerous studies using a dual-task paradigm indicate <ref> [4, 5] </ref> that intermodal time-sharing (division of attention), as when using eye and ear simultaneously to do two things, is more successful than intramodal (e.g., using the eye to do two things at once).
Reference: [5] <author> S. M. Forbes, M. M. Taylor, and P. H. Lindsay. </author> <title> Cue Timing in a Multi-Dimensional Detection Task. Perceptual and Motor Skills, </title> <booktitle> 25 </booktitle> <pages> 113-120, </pages> <year> 1967. </year>
Reference-contexts: Wick-ens [3] has suggested that separate resource capacities are available for modality, processing (spatial vs. verbal), and stage of processing (early vs. late). Task interference occurs when the same resources are called upon simultaneously. Results from numerous studies using a dual-task paradigm indicate <ref> [4, 5] </ref> that intermodal time-sharing (division of attention), as when using eye and ear simultaneously to do two things, is more successful than intramodal (e.g., using the eye to do two things at once).
Reference: [6] <author> J. Mackinlay. </author> <title> Automating the Design of Graphical Presentations of Relational Information. </title> <journal> ACM Trans. on Graphics, </journal> <volume> 5(2) </volume> <pages> 110-141, </pages> <month> April </month> <year> 1986. </year> <month> 8 </month>
Reference-contexts: Unfortunately, people with disabilities may not have this option. 2.1 The Visual Modality The manner in which information is graphically presented to users can profoundly influence its comprehensibility and utility. A "vocabulary" established by Bertin and cited by Mackinlay <ref> [6] </ref> is shown in Fig. 1. The first two retinal relationships are useful primarily for conveying quantitative information, the last two for conveying qualitative information, and the middle two for both purposes. The retinal constituents are known to not be of equal importance, and also to not be independent.
Reference: [7] <author> R. C. Carter. </author> <title> Visual Search with Color. </title> <journal> J. Exper--imental Psychology: Human Perception and Performance, </journal> <volume> 8(1) </volume> <pages> 127-136, </pages> <year> 1982. </year>
Reference-contexts: It is well established that color can be incorporated into a display to aid in the location of objects, and that it is one of the best ways to cue in a discrimination task <ref> [7] </ref> because it is, as a rule, identified more accurately than size, saturation or shape.
Reference: [8] <author> W. L. Nicholson and R. J. Littlefield. </author> <title> Interactive Color Graphics for Multivariate Data. </title> <booktitle> In Proc. 14th Annual Symposium on the Interface in Computer Science and Statistics, </booktitle> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY, </address> <pages> pages 211-219, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: Color can be used to cue depth, so change in color can indicate motion towards or away from the viewer <ref> [8] </ref>. 2 The importance of appropriate use of styliza-tion, redundancy and animation in the display has also been well documented in the literature [9].
Reference: [9] <editor> E. P. Glinert, editor. </editor> <booktitle> Tutorial|Visual Programming Environments; Volume 1: Paradigms and Systems; Volume 2: Applications and Issues. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <address> Washington, DC, </address> <year> 1990. </year>
Reference-contexts: Color can be used to cue depth, so change in color can indicate motion towards or away from the viewer [8]. 2 The importance of appropriate use of styliza-tion, redundancy and animation in the display has also been well documented in the literature <ref> [9] </ref>.
Reference: [10] <author> T. W. Malone. </author> <title> What Makes Things Fun To Learn? A Study of Intrinsically Motivating Computer Games. </title> <type> PhD Thesis, </type> <institution> Dept. of Psychology, Stanford University, </institution> <year> 1980. </year>
Reference-contexts: To cite but one specific example, in an especially elegant investigation Malone <ref> [10] </ref> found that one version of a video game was played four times as often as another, merely because the score was displayed redundantly (that is, both textually and graphically). 2.2 The Aural Modality Interfaces and electronic documents of all kinds, in which text and graphics are augmented with recorded voice
Reference: [11] <author> M. M. Blattner, D. A. Sumikawa, and R. M. Greenberg. Earcons and Icons: </author> <title> Their Structure and Common Design Principles. </title> <journal> Human-Computer Interaction, </journal> <volume> 4(1) </volume> <pages> 11-44, </pages> <year> 1989. </year>
Reference-contexts: What about synthesized nonspeech audio? Can complex information be presented usefully in an auditory pattern? Can auditory and visual information be combined to facilitate the processing of information? Recent work strongly supports a positive answer to these questions <ref> [11, 12, 13] </ref>. Nonspeech audio can serve as an indicator of event occurrence [14], as a guide to object proximity [15], and (in analogy to the Geiger counter) as an indication of the density of values in a specified range during navigation through a representation [16].
Reference: [12] <author> W. H. Watkins and C. E. Feehrer. </author> <title> Acoustic Facilitation of Visual Detection. </title> <journal> J. Experimental Psychology, </journal> <volume> 70(3) </volume> <pages> 332-333, </pages> <year> 1965. </year>
Reference-contexts: What about synthesized nonspeech audio? Can complex information be presented usefully in an auditory pattern? Can auditory and visual information be combined to facilitate the processing of information? Recent work strongly supports a positive answer to these questions <ref> [11, 12, 13] </ref>. Nonspeech audio can serve as an indicator of event occurrence [14], as a guide to object proximity [15], and (in analogy to the Geiger counter) as an indication of the density of values in a specified range during navigation through a representation [16].
Reference: [13] <author> N. E. Loveless, J. Brebner, and P. </author> <title> Hamilton. </title> <journal> Bisensory Presentaiton of Information. Psychological Bulletin, </journal> <volume> 73(3) </volume> <pages> 161-195, </pages> <year> 1970. </year>
Reference-contexts: What about synthesized nonspeech audio? Can complex information be presented usefully in an auditory pattern? Can auditory and visual information be combined to facilitate the processing of information? Recent work strongly supports a positive answer to these questions <ref> [11, 12, 13] </ref>. Nonspeech audio can serve as an indicator of event occurrence [14], as a guide to object proximity [15], and (in analogy to the Geiger counter) as an indication of the density of values in a specified range during navigation through a representation [16].
Reference: [14] <author> W. W. Gaver, R. B. Smith, and T. O'Shea. </author> <title> Effective Sounds in Complex Systems: The ARKola Experiment. </title> <booktitle> In Conf. Proc., CHI'91: Human Factors in Computing Systems, </booktitle> <address> New Orleans, </address> <pages> pages 85-90, </pages> <address> April 27-May 2, </address> <year> 1991. </year>
Reference-contexts: Nonspeech audio can serve as an indicator of event occurrence <ref> [14] </ref>, as a guide to object proximity [15], and (in analogy to the Geiger counter) as an indication of the density of values in a specified range during navigation through a representation [16].
Reference: [15] <author> E. M. Wenzel, F. L. Wightman, and S. H. </author> <note> Foster. </note>
Reference-contexts: Nonspeech audio can serve as an indicator of event occurrence [14], as a guide to object proximity <ref> [15] </ref>, and (in analogy to the Geiger counter) as an indication of the density of values in a specified range during navigation through a representation [16].
References-found: 15

