URL: ftp://dec002.cmpe.boun.edu.tr/people/ethem/papers/aca.ps.Z
Refering-URL: http://www.cmpe.boun.edu.tr/~ethem/
Root-URL: 
Title: Comparison of Statistical and Neural Classifiers and Their Applications to Optical Character Recognition and Speech Classification  
Author: Ethem Alpaydn, Fikret Gurgen 
Keyword: Statistical pattern recognition, artificial neural networks, optical character recog nition, speech recognition, Bayes decision theory, nonparametric estimation.  
Note: Neural Network Systems Techniques and Applications (in print) C. T. Leondes (Ed.), c flACADEMIC Press  
Address: TR-80815 Istanbul Turkey  
Affiliation: Department of Computer Engineering Bogazi~ci University  
Email: falpaydin,gurgeng@boun.edu.tr  
Date: October 24, 1996  
Abstract: We give a review of basic statistical and neural techniques for classification. Statistical techniques are based on the idea of estimating class-conditional likelihoods and using Bayes rule to convert these to posterior class probabilities whereas neural techniques estimate directly the posteriors. Statistical techniques include (i) Parametric (Gaussian) Bayes classifiers, (ii) Non-parametric kernel-based density estimators like k-nearest neighbor and Parzen windows, and (iii) mixtures of (Gaussian) densities (a special case of which is the Learning Vector Quantization). As neural classifiers, we include simple perceptrons and multilayer perceptrons with sigmoid and Gaussian hidden units. The neural and statistical techniques are quite similar in many respects and many approaches have been discovered independently twice, once in 1960s by statisticians and once in 1980s by the neural network researchers. One of the aims of this article is to make this link more apparent. We also discuss two, most popular, pattern recognition applications: Optical character recognition and speech recognition. Though they seem different, in many respects, the two applications are similar and in the past, almost the same techniques have been applied for their implementation. We implement the well known statistical and neural classification techniques for two datasets of these applications and compare them in terms of generalization accuracy, memory requirement and learning time. We especially advise to take into account statistics of the sample even if a neural classifier is to be used. The similarity between statistical and neural techniques is greater than generally agreed and simple statistical methods like k-NN perform generally quite well and much of the functionality of neural networks like distributed parallel computation can be obtained by such methods without requiring complicated computation and precise error minimization procedures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alpaydn, E. </author> <title> (1994) "GAL: Networks that Grow when they Learn and Shrink when they Forget," </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 8(1), </volume> <pages> 391-414. </pages>
Reference-contexts: Depending on the shape of K, one can have various estimators [45]. One disadvantage of kernel estimators is the requirement of storing the whole sample. One possibility is to selectively discard patterns that do not convey much information <ref> [1] </ref>. Another is to cluster data and keep reference vectors that represent clusters of patterns instead of the patterns themselves. The semiparametric mixture models discussed in Section 4.3 correspond to this idea. 4.2.1 K-nearest neighbor (k-NN).
Reference: [2] <author> Alpaydn, E., Aratma, S., Yagc, M. </author> <title> (1994) "Recognition of Handwritten Digits using Neural Networks," </title> <journal> ELEKTR _ IK, Turkish Journal of Electrical Engineering and Computer Sciences, </journal> <volume> 2(1), </volume> <pages> 20-31. </pages>
Reference-contexts: A review of the task and several neural and conventional approaches is given by Senior [44]. Comparison of distance-based classifiers, single and multilayer perceptrons and radial basis function networks is given in <ref> [2] </ref>. For the task of optical handwritten character recognition, a significant step was the production of a CDROM (Special Database 3) by the National Institute of Standards and Technology (NIST) [16] which includes a large set of digitized character images and computer subroutines that process them.
Reference: [3] <author> Alpaydn, E., Gurgen, F. </author> <title> (1995) "Comparison of Kernel Estimators, Perceptrons and Radial-Basis Functions for OCR and Speech Classification," </title> <journal> Neural Computing and Applications, </journal> <volume> 3, </volume> <pages> 38-49. </pages>
Reference-contexts: In a training process, given a training sample, the weights that minimize the difference between network outputs and required outputs is computed. This article has the aim of comparing these two approaches and extends a previous study <ref> [3] </ref>. In Section 2, we define the two applications that we are concerned with in this study, namely optical character recognition and speech recognition.
Reference: [4] <author> Alpaydn, E., Jordan, M. I. </author> <title> (1996) "Local Linear Perceptrons for Classification," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(3), </volume> <pages> 788-792. </pages>
Reference-contexts: The generalization ability of RBF can be extended by having the weight of each hidden unit, W jh , not a scalar but a linear function of the input <ref> [4] </ref>.
Reference: [5] <author> Berger, J. O. </author> <title> (1980) Statistical Decision Theory and Bayesian Analysis, 2nd edition, </title> <publisher> Springer. </publisher>
Reference-contexts: A classifier is then a mapping from &lt; d to fC 1 ; : : : ; C m ; C rej g. In statistical decision theory, actions have costs and the decision having the minimum cost is made <ref> [5] </ref>. <p> Assuming that correct decisions have no cost, all incorrect decisions have equal cost and no rejects, for minimum expected cost (or risk), the so-called Bayes' decision rule states that a given input should be assigned to the class with the highest posterior probability <ref> [12, 5, 31] </ref>: c = arg max P (C j jx) (1) The posteriors are almost never exactly known and thus are estimated.
Reference: [6] <author> Blue, J. L., Candela, G. T., Grother, P. J., Chellappa, R., Wilson, C. L. </author> <title> (1994) "Evaluation of Pattern Classifiers for Fingerprint and OCR Applications," </title> <journal> Pattern Recognition, </journal> <volume> 27(4), </volume> <pages> 485-501. </pages>
Reference-contexts: Many of the above-mentioned work use this dataset or its predecessor. It is available by writing to NIST. A comparison of four statistical and three neural network classifiers is given by Blue et al. <ref> [6] </ref> for optical character recognition and a similar task, fingerprint recognition (for which a similar CDROM was also made available by NIST). Researchers from NIST made several studies using this dataset and technical reports can be accessed over the internet.
Reference: [7] <author> Bourlard, H. A., Morgan, N. </author> <title> (1994) Connectionist Speech Recognition: A Hybrid Approach, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: There is then a layer of hidden units that extract features from the input. This is followed by the layer of output units where in classification each output corresponds to one class. There are a number of advantages to using neural network type classifiers for pattern recognition <ref> [7] </ref>: 1. They can learn, i.e., given a sufficiently large labelled training set, the parameters can be computed to optimize a given error criterion. 2. They can generate any kind of nonlinear function of the input. 3. <p> In some recurrent architectures, a separate set of units is designated as feedback units containing the hidden or output values generated by the preceding input. In theory, the current state of the whole network will nonlinearly depend on a combination of the previous network state and the current input <ref> [7] </ref>. A comparison of different recurrent architectures and learning rules is given in [20]. Furui [15] discusses various methods for speech recognition. Lippmann [27] and Waibel and Hampshire [52] give two reviews on using neural networks for speech recognition. <p> Comparison of distance-based classifiers, single and multilayer perceptrons and radial basis function networks for phoneme recognition is given in [19]. The recent book by Bourlard and Morgan <ref> [7] </ref> discuss in more detail neural approaches to speech classification. Currently the most efficient approach for speech recognition is accepted to be Hidden Markov Models (HMMs) [7]. An HMM models speech as a sequence of discrete stationary states with instantaneous transition between states. <p> The recent book by Bourlard and Morgan <ref> [7] </ref> discuss in more detail neural approaches to speech classification. Currently the most efficient approach for speech recognition is accepted to be Hidden Markov Models (HMMs) [7]. An HMM models speech as a sequence of discrete stationary states with instantaneous transition between states. At any state, there is a stochastic output process that describes the probability of occurrence of some feature vectors and a stochastic state-transition matrix conditioned on the input. <p> Generally there is one HMM for every word and states correspond to phonemes, syllables or demi-syllables. HMMs are also used to recognize individual phonemes where states correspond to substructures. Bourlard and Morgan <ref> [7] </ref> give a detailed discussion of HMM models and their use in speech recognition. They also show [34] how HMMs and multilayer networks can be combined for continuous speech recognition where the network estimates the emission probabilities for HMMs.
Reference: [8] <author> Cherkassky, V., Friedman, J. H., Wechsler, H. </author> <title> (1994) From Statistics to Neural Networks: Theory and Pattern Recognition Applications, </title> <booktitle> NATO ASI Series F, </booktitle> <volume> vol. 136, </volume> <publisher> Springer. </publisher>
Reference-contexts: Many of the neural techniques are either identical or bear much resemblance to previously proposed statistical techniques. For a good discussion of neural networks from statisticians' point of view and vice versa, see the collection of articles in <ref> [8] </ref>. The recent interest in neural networks did much to revive interest in the old field of statistical pattern recognition [42].
Reference: [9] <author> Lee, C.-H., Soong, F. K., Paliwal, K. K. </author> <title> (1996) Automatic Speech and Speaker Recognition: Advanced Topics, </title> <publisher> Kluwer Academic Publishers. [10] le Cun, </publisher> <editor> Y., Boser, B., Denker, J.S., Henderson, D., Howard, R. E, Hubbard, W., Jackel, L.D. </editor> <title> (1990) "Handwritten Digit Recognition with a Back-Propagation Network," </title> <editor> In Touretzky, D. (Ed.) </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> 396-404, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: They also show [34] how HMMs and multilayer networks can be combined for continuous speech recognition where the network estimates the emission probabilities for HMMs. A recent reference on current speech recognition methodologies is <ref> [9] </ref>. 7 Simulation Results For optical character recognition (OCR) experiments, we used the set of programs recently made available by NIST [16] to generate a database on which to test the algorithms we discuss. 44 people have filled in forms which are scanned and processed to get 32 fi 32 matrices
Reference: [11] <author> Dempster, A. P., Laird, N. M., Rubin, D. B. </author> <title> (1977) "Maximum Likelihood from Incomplete Data via the EM Algorithm," </title> <journal> Journal of Royal Statistical Society B, </journal> <volume> 39, </volume> <pages> 1-38. </pages>
Reference-contexts: and the mixing proportions, that maximizes the likelihood of a given iid sample X j of class j: n j X log p (x i j j ) X log h This does not have an analytical solution but an iterative procedure exists based on the Expectation Maximization (EM) algorithm <ref> [11, 40] </ref>.
Reference: [12] <author> Duda, R. O., Hart, P. E. </author> <title> (1973) Pattern Classification and Scene Analysis, </title> <publisher> Wiley. </publisher>
Reference-contexts: feature extraction methods concentrate also on auditory modelling and time-frequency representation of speech signals. 4 Statistical Classifiers In pattern recognition, we are asked to assign a multidimensional input x 2 &lt; d to one of a set of classes C j ; j = 1; : : : ; m <ref> [12, 31] </ref>. Sometimes the additional action of reject is added to choose 4 when no class or more than one class is probable. A classifier is then a mapping from &lt; d to fC 1 ; : : : ; C m ; C rej g. <p> Assuming that correct decisions have no cost, all incorrect decisions have equal cost and no rejects, for minimum expected cost (or risk), the so-called Bayes' decision rule states that a given input should be assigned to the class with the highest posterior probability <ref> [12, 5, 31] </ref>: c = arg max P (C j jx) (1) The posteriors are almost never exactly known and thus are estimated. <p> There are three approaches: 1. Parametric Methods. These assume that class-conditional densities have a certain parametric form, e.g., normal, whose sufficient statistics are estimated from the data <ref> [12, 31] </ref>. These methods generally reduce to distance-based methods where depending on assumptions made on the data, the good distance metric is chosen. 2. Non-parametric Methods. When no such assumptions can be done, the densities need be estimated directly from the data. <p> These methods generally reduce to distance-based methods where depending on assumptions made on the data, the good distance metric is chosen. 2. Non-parametric Methods. When no such assumptions can be done, the densities need be estimated directly from the data. These are also known as kernel-based estimators <ref> [12, 45, 46] </ref>. 3. Semi-parametric Methods. The densities are written as a mixture model whose parameters are estimated [12, 40, 50, 36, 48]. <p> Non-parametric Methods. When no such assumptions can be done, the densities need be estimated directly from the data. These are also known as kernel-based estimators [12, 45, 46]. 3. Semi-parametric Methods. The densities are written as a mixture model whose parameters are estimated <ref> [12, 40, 50, 36, 48] </ref>. <p> jx) j (x) = p (xjC j )P (C j ) j (x) = log p (xjC j ) + log P (C j ) (5) 4.1 Parametric Bayes classifiers The shape of decision regions defined by a Bayes classifier depends on the assumed form for p (xjC j ) <ref> [12, 31] </ref>. Most frequently, it is taken to be multivariate normal which assumes that examples from a class are noisy versions of an ideal class member. <p> Small or large h leads to a decrease in success. When h is small, there are few samples and when it is large, there are too many. The good h value is to be found using cross-validation on a separate set. For the k-nearest neighbor, it has been shown <ref> [12] </ref> that the performance of the 1-nearest neighbor in classification is never worse than twice the Bayesian risk where complete knowledge of the distributions are assumed. It can thus be said that at least half of this knowledge is provided by the nearest neighbor. <p> For example in character recognition while writing `7' one prototype may be a seven with a horizontal middle bar (European version) and one without (American version). A mixture density defines the class-conditional density as a sum of a small number of densities <ref> [12, 36, 31] </ref>: ^p (xjC j ) = h=1 10 where the conditional densities p (xj! jh ; C j ) are called the component densities and the prior prob-abilities P (! jh ) are called the mixing parameters.
Reference: [13] <author> Fukushima, K. </author> <title> (1988) "Neocognitron: A Hierarchical Neural Network Capable of Visual Pattern Recognition," </title> <booktitle> Neural Networks, </booktitle> <volume> 1, </volume> <pages> 119-130. </pages>
Reference-contexts: This is because it is a significant application of evident economic utility and also because it is a test bed before more complicated visual pattern recognition applications are attempted. One of the earliest neural network based system for handwritten character recognition is the neocognitron of Fukushima <ref> [13] </ref>. Significant amount of work on optical recognition of postal ZIP codes is done by a group at AT&T Bell Labs by Le Cun and others [10, 30]. The system uses a multilayered network with local connections and weight sharing trained with back-propagation for classification.
Reference: [14] <author> Funahashi, K. </author> <title> (1989) "On the Approximate Realization of Continuous Mapping by Neural Networks," </title> <booktitle> Neural Networks, </booktitle> <volume> 2, </volume> <pages> 183-192. </pages>
Reference-contexts: It has been shown <ref> [14, 22] </ref> that this type of a neural network is a universal approximator, i.e., can approximate any continuous function with desired accuracy. It has also been shown [43] that in the large sample case, multilayer perceptrons estimate posterior probabilities thus building a link between multilayer networks and statistical classifiers.
Reference: [15] <author> Furui, S. </author> <title> (1989) Digital Speech Processing, Synthesis and Recognition, </title> <publisher> Marcel Dekker. </publisher>
Reference-contexts: In theory, the current state of the whole network will nonlinearly depend on a combination of the previous network state and the current input [7]. A comparison of different recurrent architectures and learning rules is given in [20]. Furui <ref> [15] </ref> discusses various methods for speech recognition. Lippmann [27] and Waibel and Hampshire [52] give two reviews on using neural networks for speech recognition.
Reference: [16] <author> Garris, M. D., J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet and C. L. </author> <title> Wilson (1994). NIST Form-Based Handprint Recognition System, </title> <type> NISTIR 5469, </type> <institution> National Institute of Standards and Technology, Computer Systems Laboratory, Advanced Systems Division, Gaithersburg, MD, USA. </institution>
Reference-contexts: Typically a resolution of 300 dpi (dots per inch) is used. With smaller resolutions and smaller character images, dots and accents may be lost or images may get connected. In most applications, characters are written in special places on pre-printed forms <ref> [38, 16] </ref>. So the first step after scanning is registration which is that of determining how the original form is translated and rotated to get the final image. This is done by matching a number of points in the input form to the original blank form. <p> Comparison of distance-based classifiers, single and multilayer perceptrons and radial basis function networks is given in [2]. For the task of optical handwritten character recognition, a significant step was the production of a CDROM (Special Database 3) by the National Institute of Standards and Technology (NIST) <ref> [16] </ref> which includes a large set of digitized character images and computer subroutines that process them. This allowed many researchers a common testbed of significant size and quality on which to compare their approaches. Many of the above-mentioned work use this dataset or its predecessor. <p> A recent reference on current speech recognition methodologies is [9]. 7 Simulation Results For optical character recognition (OCR) experiments, we used the set of programs recently made available by NIST <ref> [16] </ref> to generate a database on which to test the algorithms we discuss. 44 people have filled in forms which are scanned and processed to get 32 fi 32 matrices of handwritten digits. These matrices are then low-pass filtered and undersampled to 8 by 8 to decrease dimensionality.
Reference: [17] <author> Geman, S., Bienenstock, E., Doursat, R. </author> <title> (1992) "Neural Networks and the Bias/Variance Dilemma," </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 1-58. 22 </pages>
Reference-contexts: Large k takes into account the effect of very distant samples and thus is not good either. When h (or k) is decreased to decrease bias, variance increases and vice versa. This can intuitively be explained as follows <ref> [17] </ref>. When h (or k) is large, ^p is the weighted average of many samples and thus does not change much from one sample set to another.
Reference: [18] <author> Guyon, I., Poujoud, I., Personnaz, L., Dreyfus, G., Denker, J., le Cun, Y. </author> <title> (1989) "Comparing Different Neural Architectures for Classifying Handwritten Digits," </title> <booktitle> International Joint Conference on Neural Networks 1989, </booktitle> <volume> vol. 2, </volume> <pages> 127-132, </pages> <address> Washington USA. </address>
Reference-contexts: This is necessary if characters are touching in such a way that they cannot be segmented by a straightforward segmentation procedure. Several comparitive studies have also been done; either by fixing the dataset and varying the methods or by also using a number of datasets. Guyon et al. <ref> [18] </ref> is an early reference where simple and multilayer perceptrons are compared with statistical distance-based classifiers like k-NN in recognizing handwritten digits for automatic reading of postal codes. A comparison of k-NN, multilayer perceptron and radial basis functions in recognizing handwritten digits is given by Lee [26].
Reference: [19] <author> Gurgen, F., Alpaydn, R., Unluakn, U., Alpaydn, E. </author> <title> (1994) "Distributed and Local Neural Classifiers for Phoneme Recognition," </title> <journal> Pattern Recognition Letters, </journal> <volume> 15, </volume> <pages> 1111-1118. </pages>
Reference-contexts: Lee and Lippmann [25], and Ng and Lippmann [35] for the same two artificial and two speech tasks compare a large number of conventional and neural pattern classifiers. Comparison of distance-based classifiers, single and multilayer perceptrons and radial basis function networks for phoneme recognition is given in <ref> [19] </ref>. The recent book by Bourlard and Morgan [7] discuss in more detail neural approaches to speech classification. Currently the most efficient approach for speech recognition is accepted to be Hidden Markov Models (HMMs) [7].
Reference: [20] <author> Gurgen, F., ~Shmanoglu, M., Alpaydn, E. </author> <title> (1996) "Learning Speech Dynamics by Neural Networks with Delay Elements," </title> <booktitle> ICT'96, International Conference on Telecommunications, </booktitle> <editor> B. Sankur (Ed.), </editor> <month> 156-161. </month>
Reference-contexts: In theory, the current state of the whole network will nonlinearly depend on a combination of the previous network state and the current input [7]. A comparison of different recurrent architectures and learning rules is given in <ref> [20] </ref>. Furui [15] discusses various methods for speech recognition. Lippmann [27] and Waibel and Hampshire [52] give two reviews on using neural networks for speech recognition.
Reference: [21] <author> Hertz, J., Krogh, A., Palmer, R. </author> <title> (1991) An Introduction to the Theory of Neural Computation, </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: the input agree and we moved the mean away from the input if they disagree: jc = +(x jc ) if x 2 C j (x jc ) otherwise (20) 5 Neural Classifiers An artificial neural network is a network of simple processing units that are interconnected through weighted connections <ref> [24, 21, 42] </ref>. The interconnection topology between the units and the weights of the connections define the operation of the network. We are generally interested in feedforward networks where a set of units are designated as the input units through which input features are fed to the network. <p> This technique is called backpropagation of errors <ref> [21] </ref>. Note that because of the dependence introduced through softmax, a given sample is used to train the discriminants of all classes. <p> They extract nonlinear input combinations to be able to define nonlinear discriminants <ref> [21] </ref>. Usually, the hidden units implement perceptrons passed through the sigmoid function: h (x) = (T T 1 h x 0 ] Connection weights of both layers, T and W , are trained in a supervised manner by gradient descent over a cost function like the cross-entropy. <p> In the neural network terminology, these are named recurrent networks which oppose to feedforward networks by having also connections between units in the same layer or connections to units of a preceding layer <ref> [21] </ref>. For short sequences, a recurrent network can be converted into an equivalent feedforward network by unfolding it over time. This is another way of mapping time into spaces, the difference being that now copies of the whole network is done.
Reference: [22] <author> Hornik, K., Stinchcombe, M., White, H. </author> <title> (1989) "Multilayer Feedforward Networks are Universal Approximators," </title> <booktitle> Neural Networks, </booktitle> <volume> 2, </volume> <pages> 359-366. </pages>
Reference-contexts: It has been shown <ref> [14, 22] </ref> that this type of a neural network is a universal approximator, i.e., can approximate any continuous function with desired accuracy. It has also been shown [43] that in the large sample case, multilayer perceptrons estimate posterior probabilities thus building a link between multilayer networks and statistical classifiers.
Reference: [23] <author> Keeler, J., Rumelhart, D. E. </author> <title> (1992) "A Self-Organizing Integrated Segmentation and Recognition Neural Net," </title> <editor> in J. E. Moody, S. J. Hanson, R. P. Lippmann (Eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> 496-503, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: An extensive study of backpropagation for optical recognition of both handwritten letters and digits is given by Martin and Pittman [28]. Keeler, Martin and others at MCC work on combining segmentation and recognition in one integrated system <ref> [23, 29] </ref>. This is necessary if characters are touching in such a way that they cannot be segmented by a straightforward segmentation procedure. Several comparitive studies have also been done; either by fixing the dataset and varying the methods or by also using a number of datasets.
Reference: [24] <author> Kohonen, T. </author> <title> (1988) Self-Organization and Associative Memory, </title> <publisher> Springer. </publisher>
Reference-contexts: These are also known as kernel-based estimators [12, 45, 46]. 3. Semi-parametric Methods. The densities are written as a mixture model whose parameters are estimated [12, 40, 50, 36, 48]. In the case of normal mixtures, this approach is equivalent to cluster-based classification strategies like LVQ of Kohonen <ref> [24] </ref> and is similar to Gaussian radial basis function networks [32]. 5 A decision rule as given in Eq. (1) has the effect of dividing the input space into mutually exclusive regions called the decision regions where each region is assigned to one of the m classes. <p> The rationale of this method is that by moving the mean closer to the sample we increase the likelihood of seeing that sample. 11 4.3.1 Learning Vector Quantization Kohonen <ref> [24] </ref> proposed Learning Vector Quantization which also moves means of wrong classes away. <p> the input agree and we moved the mean away from the input if they disagree: jc = +(x jc ) if x 2 C j (x jc ) otherwise (20) 5 Neural Classifiers An artificial neural network is a network of simple processing units that are interconnected through weighted connections <ref> [24, 21, 42] </ref>. The interconnection topology between the units and the weights of the connections define the operation of the network. We are generally interested in feedforward networks where a set of units are designated as the input units through which input features are fed to the network.
Reference: [25] <author> Lee Y., Lippmann, R. </author> <title> (1990) "Practical Characteristics of Neural Network and Conventional Pattern Classifiers on Artificial and Speech Problems," </title> <editor> in D. Touretzky (Ed.) </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> 168-177, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Early work used recurrent neural networks for representation of temporal context but after the introduction of time delay neural networks by Waibel at al. [51], feedforward networks are also used for phoneme recognition. Lee and Lippmann <ref> [25] </ref>, and Ng and Lippmann [35] for the same two artificial and two speech tasks compare a large number of conventional and neural pattern classifiers. Comparison of distance-based classifiers, single and multilayer perceptrons and radial basis function networks for phoneme recognition is given in [19].
Reference: [26] <author> Lee, Y. </author> <title> (1991) "Handwritten Digit Recognition Using K-Nearest-Neighbor, Radial-Basis Function, and Backpropagation Neural Networks," </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 440-449. </pages>
Reference-contexts: Guyon et al. [18] is an early reference where simple and multilayer perceptrons are compared with statistical distance-based classifiers like k-NN in recognizing handwritten digits for automatic reading of postal codes. A comparison of k-NN, multilayer perceptron and radial basis functions in recognizing handwritten digits is given by Lee <ref> [26] </ref>. A review of the task and several neural and conventional approaches is given by Senior [44]. Comparison of distance-based classifiers, single and multilayer perceptrons and radial basis function networks is given in [2].
Reference: [27] <author> Lippmann, R. P. </author> <title> (1989) "Review of Neural Networks for Speech Recognition," </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 1-38. </pages>
Reference-contexts: The best existing systems perform well only on artificially constrained tasks. Performance is better if samples are provided for all speakers/writers, when words are spoken/letters are written in isolation, when the vocabulary size is small and when restricted language models are used to constrain allowable sequences <ref> [27, 38] </ref>. 3 3 Data Acquisition and Preprocessing 3.1 Optical Character Recognition In optical character recognition, preprocessing should be done before individual character images can be fed to the classifier. Depending on how small the characters are, a suitable resolution should be chosen first for scanning. <p> In theory, the current state of the whole network will nonlinearly depend on a combination of the previous network state and the current input [7]. A comparison of different recurrent architectures and learning rules is given in [20]. Furui [15] discusses various methods for speech recognition. Lippmann <ref> [27] </ref> and Waibel and Hampshire [52] give two reviews on using neural networks for speech recognition. Early work used recurrent neural networks for representation of temporal context but after the introduction of time delay neural networks by Waibel at al. [51], feedforward networks are also used for phoneme recognition.
Reference: [28] <author> Martin, G. L, Pittman, J. A. </author> <title> (1990) "Recognizing Hand-Printed Letters and Digits Using Backpropagation Learning," </title> <editor> in D. Touretzky (Ed.) </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> 405-414, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This implements a hierarchical cone where simpler local features are extracted in parallel which combine to form higher-level, less local features and which finally define the digits. An extensive study of backpropagation for optical recognition of both handwritten letters and digits is given by Martin and Pittman <ref> [28] </ref>. Keeler, Martin and others at MCC work on combining segmentation and recognition in one integrated system [23, 29]. This is necessary if characters are touching in such a way that they cannot be segmented by a straightforward segmentation procedure.
Reference: [29] <author> Martin, G. L., Rashid, M. </author> <title> (1992) "Recognizing Overlapping Hand-Printed Characters by Centered-Object Integrated Segmentation and Recognition," </title> <editor> in J. E. Moody, S. J. Hanson, R. P. Lippmann (Eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> 504-511. </pages>
Reference-contexts: An extensive study of backpropagation for optical recognition of both handwritten letters and digits is given by Martin and Pittman [28]. Keeler, Martin and others at MCC work on combining segmentation and recognition in one integrated system <ref> [23, 29] </ref>. This is necessary if characters are touching in such a way that they cannot be segmented by a straightforward segmentation procedure. Several comparitive studies have also been done; either by fixing the dataset and varying the methods or by also using a number of datasets.
Reference: [30] <author> Matan, O., Baird, H.S., Bromley, J., Burges, C. J. C., Denker, J. S., Jackel, L. D., le Cun, Y., Pednault, E. P. D., Satterfield, W. D., Stenard, C. E., Thompson, T. J. </author> <title> (1992) "Reading Handwritten Digits: A Zip Code Recognition System," </title> <journal> IEEE Computer, </journal> <volume> 25(7), </volume> <pages> 59-62. </pages>
Reference-contexts: One of the earliest neural network based system for handwritten character recognition is the neocognitron of Fukushima [13]. Significant amount of work on optical recognition of postal ZIP codes is done by a group at AT&T Bell Labs by Le Cun and others <ref> [10, 30] </ref>. The system uses a multilayered network with local connections and weight sharing trained with back-propagation for classification. This implements a hierarchical cone where simpler local features are extracted in parallel which combine to form higher-level, less local features and which finally define the digits.
Reference: [31] <author> McLachlan, G. J. </author> <title> (1992) Discriminant Analysis and Statistical Pattern Recognition, </title> <publisher> Wiley. </publisher>
Reference-contexts: feature extraction methods concentrate also on auditory modelling and time-frequency representation of speech signals. 4 Statistical Classifiers In pattern recognition, we are asked to assign a multidimensional input x 2 &lt; d to one of a set of classes C j ; j = 1; : : : ; m <ref> [12, 31] </ref>. Sometimes the additional action of reject is added to choose 4 when no class or more than one class is probable. A classifier is then a mapping from &lt; d to fC 1 ; : : : ; C m ; C rej g. <p> Assuming that correct decisions have no cost, all incorrect decisions have equal cost and no rejects, for minimum expected cost (or risk), the so-called Bayes' decision rule states that a given input should be assigned to the class with the highest posterior probability <ref> [12, 5, 31] </ref>: c = arg max P (C j jx) (1) The posteriors are almost never exactly known and thus are estimated. <p> There are three approaches: 1. Parametric Methods. These assume that class-conditional densities have a certain parametric form, e.g., normal, whose sufficient statistics are estimated from the data <ref> [12, 31] </ref>. These methods generally reduce to distance-based methods where depending on assumptions made on the data, the good distance metric is chosen. 2. Non-parametric Methods. When no such assumptions can be done, the densities need be estimated directly from the data. <p> jx) j (x) = p (xjC j )P (C j ) j (x) = log p (xjC j ) + log P (C j ) (5) 4.1 Parametric Bayes classifiers The shape of decision regions defined by a Bayes classifier depends on the assumed form for p (xjC j ) <ref> [12, 31] </ref>. Most frequently, it is taken to be multivariate normal which assumes that examples from a class are noisy versions of an ideal class member. <p> The common covariance matrix can even be forced to be diagonal if there is even less data available providing further regularization <ref> [31] </ref>. There are also techniques to decrease the dimensionality. One is subset selection which means choosing the most important p dimensions from d, ignoring the dp dimensions. Principal component analysis (PCA) chooses the most important p linear combinations of the d dimensions. 7 discriminant. 8 one nearest neighbor. <p> For example in character recognition while writing `7' one prototype may be a seven with a horizontal middle bar (European version) and one without (American version). A mixture density defines the class-conditional density as a sum of a small number of densities <ref> [12, 36, 31] </ref>: ^p (xjC j ) = h=1 10 where the conditional densities p (xj! jh ; C j ) are called the component densities and the prior prob-abilities P (! jh ) are called the mixing parameters. <p> In classification, we know that outputs are probabilities and that they sum up to one. This can be enforced using the softmax model [42]: h j = P (23) and the error measure to be minimized is the cross-entropy between the two distributions <ref> [31, 42] </ref>: E = i j where r ij = 1 if x i 2 C j and 0 otherwise.
Reference: [32] <author> Moody, J., Darken, C. J. </author> <title> (1989) "Fast Learning in Networks of Locally-Tuned Processing Units," </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 281-294. </pages>
Reference-contexts: Semi-parametric Methods. The densities are written as a mixture model whose parameters are estimated [12, 40, 50, 36, 48]. In the case of normal mixtures, this approach is equivalent to cluster-based classification strategies like LVQ of Kohonen [24] and is similar to Gaussian radial basis function networks <ref> [32] </ref>. 5 A decision rule as given in Eq. (1) has the effect of dividing the input space into mutually exclusive regions called the decision regions where each region is assigned to one of the m classes. <p> These theorems do not tell how many hidden units are necessary, so one should test several alternatives on a cross-validation set and choose the best. 5.3 Radial Basis Functions A radial basis function network (RBF) <ref> [32, 39] </ref> is another type of feedforward, multilayer network where the basis function is a Gaussian: h (x) = (kT h xk) = exp 2 2 Sometimes () are normalized to sum up to one. RBF is also a universal approximator. <p> Training can be done in one of two ways: In the uncoupled version, originally proposed by Moody and Darken <ref> [32] </ref>, the Gaussian centers are trained in an unsupervised manner, e.g., using k-means. , the spread of Gaussians, is computed as a factor of the average of inter-center distances. The second layer of W is a single layer perceptron and is trained using gradient-descent rule in a supervised manner.
Reference: [33] <author> Morgan, D. P., Christopher, L. S. </author> <title> (1991) Neural Networks and Speech Processing, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: A more expensive preprocessing is to have small local kernels with which all parts of the image are convolved, to detect line segments, corners, etc. 3.2 Speech Recognition Digitized speech samples are obtained by an antialiasing filter and an analog-to-digital converter (A/D) <ref> [33, 41] </ref>. A low-pass anti-aliasing filter should be set below the Nyquist frequency (half of the sample rate) so that the Fourier transform of the signal will be band limited. An A/D converter commonly consists of a sampling circuit and a hold circuit. <p> The required segments of each utterance are manually, or using an accurate segmentation algorithm, endpointed and processed into frames. A linear predictive coding (LPC) based analysis procedure <ref> [33, 41, 34] </ref> can be used to obtain desired features such as cepstrum coefficients, fast Fourier transform (FFT) coefficients.
Reference: [34] <author> Morgan, N., Bourlard, H. </author> <title> (1995) "Continuous Speech Recognition: An Introduction to the Hybrid HMM/Connectionist approach," </title> <journal> IEEE Signal Processing Magazine, </journal> <pages> 25-42. 23 </pages>
Reference-contexts: The required segments of each utterance are manually, or using an accurate segmentation algorithm, endpointed and processed into frames. A linear predictive coding (LPC) based analysis procedure <ref> [33, 41, 34] </ref> can be used to obtain desired features such as cepstrum coefficients, fast Fourier transform (FFT) coefficients. <p> Generally there is one HMM for every word and states correspond to phonemes, syllables or demi-syllables. HMMs are also used to recognize individual phonemes where states correspond to substructures. Bourlard and Morgan [7] give a detailed discussion of HMM models and their use in speech recognition. They also show <ref> [34] </ref> how HMMs and multilayer networks can be combined for continuous speech recognition where the network estimates the emission probabilities for HMMs.
Reference: [35] <author> Ng, K, Lippmann, R. </author> <title> (1991) "Practical Characteristics of Neural Network and Conventional Pattern Classifiers," </title> <editor> in R. Lippmann, J. Moody, D. Touretzky (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> 970-976, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Early work used recurrent neural networks for representation of temporal context but after the introduction of time delay neural networks by Waibel at al. [51], feedforward networks are also used for phoneme recognition. Lee and Lippmann [25], and Ng and Lippmann <ref> [35] </ref> for the same two artificial and two speech tasks compare a large number of conventional and neural pattern classifiers. Comparison of distance-based classifiers, single and multilayer perceptrons and radial basis function networks for phoneme recognition is given in [19].
Reference: [36] <author> Nowlan, S.J. </author> <title> (1991) Soft Competitive Adaptation: Neural Network Learning Algorithms based on Fitting Statistical Mixtures, </title> <type> PhD Thesis, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: Non-parametric Methods. When no such assumptions can be done, the densities need be estimated directly from the data. These are also known as kernel-based estimators [12, 45, 46]. 3. Semi-parametric Methods. The densities are written as a mixture model whose parameters are estimated <ref> [12, 40, 50, 36, 48] </ref>. <p> For example in character recognition while writing `7' one prototype may be a seven with a horizontal middle bar (European version) and one without (American version). A mixture density defines the class-conditional density as a sum of a small number of densities <ref> [12, 36, 31] </ref>: ^p (xjC j ) = h=1 10 where the conditional densities p (xj! jh ; C j ) are called the component densities and the prior prob-abilities P (! jh ) are called the mixing parameters. <p> Neural networks based on mixture models have also been proposed. Nowlan <ref> [36] </ref> considers them as "soft" variants of competitive approaches when used for vector quantization. Traven [50] proposes to use a mixture of Gaussians and calls this a "a neural network approach" and uses EM to optimize parameters without saying so.
Reference: [37] <author> Omohundro, S. M. </author> <title> (1987) "Efficient Algorithms with Neural Network Behavior," </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 273-347. </pages>
Reference-contexts: For a good discussion of neural networks from statisticians' point of view and vice versa, see the collection of articles in [8]. The recent interest in neural networks did much to revive interest in the old field of statistical pattern recognition [42]. Omohundro <ref> [37] </ref> discusses how nonparametric kernel estimators can be implemented as neural networks (by representing each sample with a Gaussian centered at the sample) and also discusses efficient data structures for the purpose.
Reference: [38] <author> Pavlidis, T., Mori, S. </author> <title> (1992) Special Issue on Optical Character Recognition, </title> <booktitle> Proceedings of the IEEE, </booktitle> <pages> 80(7). </pages>
Reference-contexts: Recognition is done while writing takes place and is called on-line. A special journal issue on different approaches to character recognition, edited by Pavlidis and Mori <ref> [38] </ref>, recently appeared. In speech recognition, there is no analogy to printed character recognition as the sound to be recognized is natural, never synthetic. Speech is captured using a microphone and is digitized to get a set of samples in time from the sound waveform. <p> The best existing systems perform well only on artificially constrained tasks. Performance is better if samples are provided for all speakers/writers, when words are spoken/letters are written in isolation, when the vocabulary size is small and when restricted language models are used to constrain allowable sequences <ref> [27, 38] </ref>. 3 3 Data Acquisition and Preprocessing 3.1 Optical Character Recognition In optical character recognition, preprocessing should be done before individual character images can be fed to the classifier. Depending on how small the characters are, a suitable resolution should be chosen first for scanning. <p> Typically a resolution of 300 dpi (dots per inch) is used. With smaller resolutions and smaller character images, dots and accents may be lost or images may get connected. In most applications, characters are written in special places on pre-printed forms <ref> [38, 16] </ref>. So the first step after scanning is registration which is that of determining how the original form is translated and rotated to get the final image. This is done by matching a number of points in the input form to the original blank form. <p> of a piecewise constant approximation. 15 6 Literature Survey 6.1 Optical Character Recognition Optical character recognition is one of the most popular pattern recognition applications and many systems have been proposed in the past towards this aim; see the special journal issue edited by Pavlidis and Mori for a review <ref> [38] </ref>. This is because it is a significant application of evident economic utility and also because it is a test bed before more complicated visual pattern recognition applications are attempted. One of the earliest neural network based system for handwritten character recognition is the neocognitron of Fukushima [13].
Reference: [39] <author> Poggio, T, Girosi, F. </author> <title> (1990) "Networks for Approximation and Learning," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78, </volume> <pages> 1481-1497. </pages>
Reference-contexts: These theorems do not tell how many hidden units are necessary, so one should test several alternatives on a cross-validation set and choose the best. 5.3 Radial Basis Functions A radial basis function network (RBF) <ref> [32, 39] </ref> is another type of feedforward, multilayer network where the basis function is a Gaussian: h (x) = (kT h xk) = exp 2 2 Sometimes () are normalized to sum up to one. RBF is also a universal approximator.
Reference: [40] <author> Redner, R. A., Walker, H. F. </author> <title> (1984) "Mixture Densities, Maximum Likelihood and the EM Algorithm," </title> <journal> SIAM Review, </journal> <volume> 26(2), </volume> <pages> 195-239. </pages>
Reference-contexts: Non-parametric Methods. When no such assumptions can be done, the densities need be estimated directly from the data. These are also known as kernel-based estimators [12, 45, 46]. 3. Semi-parametric Methods. The densities are written as a mixture model whose parameters are estimated <ref> [12, 40, 50, 36, 48] </ref>. <p> and the mixing proportions, that maximizes the likelihood of a given iid sample X j of class j: n j X log p (x i j j ) X log h This does not have an analytical solution but an iterative procedure exists based on the Expectation Maximization (EM) algorithm <ref> [11, 40] </ref>.
Reference: [41] <author> Rabiner, L., Juang, B.-H. </author> <title> (1993) Fundamentals of Speech Recognition, </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: For a more detailed analysis, refer to the book by Rabiner and Juang <ref> [41] </ref>. The two tasks of character recognition and speech recognition have a number of common problems. One is the segmentation of characters or phonemes from a stream of text image or speech. <p> A more expensive preprocessing is to have small local kernels with which all parts of the image are convolved, to detect line segments, corners, etc. 3.2 Speech Recognition Digitized speech samples are obtained by an antialiasing filter and an analog-to-digital converter (A/D) <ref> [33, 41] </ref>. A low-pass anti-aliasing filter should be set below the Nyquist frequency (half of the sample rate) so that the Fourier transform of the signal will be band limited. An A/D converter commonly consists of a sampling circuit and a hold circuit. <p> The required segments of each utterance are manually, or using an accurate segmentation algorithm, endpointed and processed into frames. A linear predictive coding (LPC) based analysis procedure <ref> [33, 41, 34] </ref> can be used to obtain desired features such as cepstrum coefficients, fast Fourier transform (FFT) coefficients.
Reference: [42] <author> Ripley, B. D. </author> <title> (1994) "Neural networks and related methods for classification," </title> <journal> Journal of Royal Statistical Society B, </journal> <volume> 56, </volume> <pages> 409-456. </pages>
Reference-contexts: the input agree and we moved the mean away from the input if they disagree: jc = +(x jc ) if x 2 C j (x jc ) otherwise (20) 5 Neural Classifiers An artificial neural network is a network of simple processing units that are interconnected through weighted connections <ref> [24, 21, 42] </ref>. The interconnection topology between the units and the weights of the connections define the operation of the network. We are generally interested in feedforward networks where a set of units are designated as the input units through which input features are fed to the network. <p> In classification, we know that outputs are probabilities and that they sum up to one. This can be enforced using the softmax model <ref> [42] </ref>: h j = P (23) and the error measure to be minimized is the cross-entropy between the two distributions [31, 42]: E = i j where r ij = 1 if x i 2 C j and 0 otherwise. <p> In classification, we know that outputs are probabilities and that they sum up to one. This can be enforced using the softmax model [42]: h j = P (23) and the error measure to be minimized is the cross-entropy between the two distributions <ref> [31, 42] </ref>: E = i j where r ij = 1 if x i 2 C j and 0 otherwise. <p> For a good discussion of neural networks from statisticians' point of view and vice versa, see the collection of articles in [8]. The recent interest in neural networks did much to revive interest in the old field of statistical pattern recognition <ref> [42] </ref>. Omohundro [37] discusses how nonparametric kernel estimators can be implemented as neural networks (by representing each sample with a Gaussian centered at the sample) and also discusses efficient data structures for the purpose.
Reference: [43] <author> Ruck, D. W., Rogers, S. K, Kabrisky, M., Oxley, M. E, Suter, B. W. </author> <title> (1990) "The Multi-layer Perceptron as an Approximation to a Bayes Optimal Discriminant Function," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1, </volume> <pages> 296-298. </pages>
Reference-contexts: It has been shown [14, 22] that this type of a neural network is a universal approximator, i.e., can approximate any continuous function with desired accuracy. It has also been shown <ref> [43] </ref> that in the large sample case, multilayer perceptrons estimate posterior probabilities thus building a link between multilayer networks and statistical classifiers.
Reference: [44] <author> Senior, A. W. </author> <title> (1992) Off-line Handwriting Recognition: A Review and Experiments, </title> <institution> Cambridge University Engineering Department, </institution> <address> CUED/F-INFENG/TR 105. </address>
Reference-contexts: A comparison of k-NN, multilayer perceptron and radial basis functions in recognizing handwritten digits is given by Lee [26]. A review of the task and several neural and conventional approaches is given by Senior <ref> [44] </ref>. Comparison of distance-based classifiers, single and multilayer perceptrons and radial basis function networks is given in [2].
Reference: [45] <author> Silverman, B. W. </author> <title> (1986) Density Estimation for Statistics and Data Analysis, </title> <publisher> Chapman & Hall. </publisher>
Reference-contexts: These methods generally reduce to distance-based methods where depending on assumptions made on the data, the good distance metric is chosen. 2. Non-parametric Methods. When no such assumptions can be done, the densities need be estimated directly from the data. These are also known as kernel-based estimators <ref> [12, 45, 46] </ref>. 3. Semi-parametric Methods. The densities are written as a mixture model whose parameters are estimated [12, 40, 50, 36, 48]. <p> The kernel estimator is given as ^p (xjC j ) = n j h d i=1 h (10) h is the window width or the smoothing parameter. Depending on the shape of K, one can have various estimators <ref> [45] </ref>. One disadvantage of kernel estimators is the requirement of storing the whole sample. One possibility is to selectively discard patterns that do not convey much information [1]. Another is to cluster data and keep reference vectors that represent clusters of patterns instead of the patterns themselves.
Reference: [46] <editor> Specht, D. F. </editor> <booktitle> (1990) "Probabilistic Neural Networks," Neural Networks, </booktitle> <volume> 3, </volume> <pages> 109-118. </pages>
Reference-contexts: These methods generally reduce to distance-based methods where depending on assumptions made on the data, the good distance metric is chosen. 2. Non-parametric Methods. When no such assumptions can be done, the densities need be estimated directly from the data. These are also known as kernel-based estimators <ref> [12, 45, 46] </ref>. 3. Semi-parametric Methods. The densities are written as a mixture model whose parameters are estimated [12, 40, 50, 36, 48]. <p> Omohundro [37] discusses how nonparametric kernel estimators can be implemented as neural networks (by representing each sample with a Gaussian centered at the sample) and also discusses efficient data structures for the purpose. One example is the probabilistic neural network of Specht <ref> [46] </ref> which is a neural network implementation of Parzen windows.
Reference: [47] <author> Srihari, S. N. </author> <year> (1992). </year> <title> High-performance reading machines. </title> <booktitle> Proceedings of the IEEE 80, </booktitle> <pages> 1120-1132. </pages>
Reference-contexts: Researchers from NIST made several studies using this dataset and technical reports can be accessed over the internet. Recently with the reduction of cost of computing power and memory, it has been possible to have multiple systems for the same task which are then combined to improve accuracy <ref> [47, 49] </ref>. One approach is to have parallel models and then take a vote.
Reference: [48] <author> Streit, R. L., Luginbuhl, T. E. </author> <title> (1994) "Maximum Likelihood Training of Probabilistic Neural Networks," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(5), </volume> <pages> 764-783. </pages>
Reference-contexts: Non-parametric Methods. When no such assumptions can be done, the densities need be estimated directly from the data. These are also known as kernel-based estimators [12, 45, 46]. 3. Semi-parametric Methods. The densities are written as a mixture model whose parameters are estimated <ref> [12, 40, 50, 36, 48] </ref>. <p> Note that here we have one mixture model for each class leading to an overall mixture of mixtures <ref> [48] </ref>.
Reference: [49] <author> Suen, C. Y., C. Nadal, R. Legault, T. A. Mai and L. </author> <title> Lam (1992). Computer recognition of unconstrained handwritten numerals. </title> <booktitle> Proceedings of the IEEE 80, </booktitle> <pages> 1162-1180. </pages>
Reference-contexts: Researchers from NIST made several studies using this dataset and technical reports can be accessed over the internet. Recently with the reduction of cost of computing power and memory, it has been possible to have multiple systems for the same task which are then combined to improve accuracy <ref> [47, 49] </ref>. One approach is to have parallel models and then take a vote.
Reference: [50] <author> Traven, H. </author> <title> G.C. (1991) "A Neural Network Approach to Statistical Pattern Classification by `Semiparametric' Estimation of Probability Density Functions," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(3), </volume> <pages> 366-377. </pages>
Reference-contexts: Non-parametric Methods. When no such assumptions can be done, the densities need be estimated directly from the data. These are also known as kernel-based estimators [12, 45, 46]. 3. Semi-parametric Methods. The densities are written as a mixture model whose parameters are estimated <ref> [12, 40, 50, 36, 48] </ref>. <p> Neural networks based on mixture models have also been proposed. Nowlan [36] considers them as "soft" variants of competitive approaches when used for vector quantization. Traven <ref> [50] </ref> proposes to use a mixture of Gaussians and calls this a "a neural network approach" and uses EM to optimize parameters without saying so. Statistics can also be used to improve the performance of neural techniques.
Reference: [51] <author> Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., Lang, K. J. </author> <title> (1989) "Phoneme Recognition Using Time-Delay Neural Networks," </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 37, </volume> <pages> 328-339. </pages>
Reference-contexts: Classifiers we have considered up to now are static, i.e., assume that the whole input feature vector is available for classification. To use a static classifier for a dynamic task, a time delay approach is used <ref> [51] </ref>. This uses an input layer with tapped delay lines and can be used if the input buffer is large enough to accommodate the longest possible sequence or if a resampling is done to normalize length. <p> Furui [15] discusses various methods for speech recognition. Lippmann [27] and Waibel and Hampshire [52] give two reviews on using neural networks for speech recognition. Early work used recurrent neural networks for representation of temporal context but after the introduction of time delay neural networks by Waibel at al. <ref> [51] </ref>, feedforward networks are also used for phoneme recognition. Lee and Lippmann [25], and Ng and Lippmann [35] for the same two artificial and two speech tasks compare a large number of conventional and neural pattern classifiers.
Reference: [52] <author> Waibel, A., Hampshire II, J. B. </author> <title> (1991) Neural Network Applications to Speech, </title> <editor> in P. Antognetti & V. Milutinovic (Eds). </editor> <booktitle> Neural Networks: Concepts, Applications, and Implementations 1, </booktitle> <pages> 54-76, </pages> <publisher> Prentice Hall. </publisher> <pages> 24 </pages>
Reference-contexts: A comparison of different recurrent architectures and learning rules is given in [20]. Furui [15] discusses various methods for speech recognition. Lippmann [27] and Waibel and Hampshire <ref> [52] </ref> give two reviews on using neural networks for speech recognition. Early work used recurrent neural networks for representation of temporal context but after the introduction of time delay neural networks by Waibel at al. [51], feedforward networks are also used for phoneme recognition.
References-found: 51

