URL: http://www.icsi.berkeley.edu/ftp/global/pub/techreports/1994/tr-94-025.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/ftp/global/pub/techreports/1994/
Root-URL: http://www.icsi.berkeley.edu
Title: Fast and Efficient Parallel Algorithms for Problems in Control Theory  
Author: Bruno Codenotti Biswa N. Datta Karabi Datta Mauro Leoncini B. N. Datta 
Note: Partly sup ported by ESPRIT Basic Research Action, Project 9072 "GEPPCOM".  The work of  was supported by a NSF grant under contract number DMS -9212629.  
Affiliation: Istituto di Elaborazione dell'Informazione, Consiglio Nazionale delle Ricerche, Pisa (Italy).  Department of Mathematical Sciences, Northern Illinois University, Decalb, Illinois.  
Date: August 1994  
Pubnum: TR-94-025  
Abstract: Remarkable progress has been made in both theory and applications of all important areas of control. On the other hand, progress in computational aspects of control theory, especially in the area of large-scale and parallel computations, has been painfully slow. In this paper we address some central problems arising in control theory, namely the controllability and the eigenvalue assignment problems, and the solution of the Lyapunov and Sylvester observer matrix equations. For all these problems we give parallel algorithms that run in almost linear time on a Parallel Random Access Machine model. The algorithms make efficient use of the processors and are scalable, which makes them of practical worth also in the case of limited parallelism. fl This paper is in part based on an invited talk delivered to the 1992 American Control Conference in Chicago, Il. x Dipartimento di Informatica, Universita di Pisa, Pisa (Italy). Part of this work was done while the author was visiting the "International Computer Science Institute", Berkeley, CA. Supported by ESPRIT Basic Research Action, Project 9072 "GEPPCOM", and by M.U.R.S.T., 40% and 60% funds. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Barnett, </author> <title> Introduction to Mathematical Control Theory, </title> <publisher> Clarendon, Oxford, </publisher> <year> 1975. </year>
Reference-contexts: Linear control systems give rise to a variety of interesting problems, such as * controllability and osservability, * eigenvalue assignment (also called pole assignment), * stability and inertia, * matrix equations (Sylvester, Lyapunov, Riccati), just to name a few. These, in turn, give rise to many linear algebra problems <ref> [1, 7] </ref>. In this paper we focus on the parallel complexity of a number of algorithms that solve some of the above mentioned problems. In particular, we consider the controllability and the eigenvalue assignment problems, and the solution of the Lyapunov and Sylvester observer matrix equations. <p> Suppose that a parallel algorithm P for a problem does work w on a p processors PRAM. Then Brent's scheduling principle states that there are implementations of P that do essentially the same work on any q processor PRAM, for any value of q 2 <ref> [1; p] </ref>. Essentially, it is based on the simulation idea that any processor of a q processor machine can execute one parallel step of the algorithm being simulated in dp=qe &lt; p=q + 1 parallel steps. <p> In particular, if a size dependent (independent) parallel algorithm P is work efficient, then there are work efficient size independent implementations of P on any q processors PRAM with q in the range <ref> [1; p (n)] </ref> ([1; p]). 2.1 Basic linear algebra algorithms In this section we recall some known results about the complexity of certain basic problems that we will be widely using throughout the rest of the paper. <p> As an application of the principle of down scalability, consider the algorithm for matrix multiplication. It follows from the principle and from Theorem 4 that p processors PRAM implementations running in parallel time O (n 3 =p) are available for any value of p in the range <ref> [1; n 3 = log n] </ref>. The algorithms in Theorem 3 and 4 are both optimal from the viewpoint of parallel time. They are also work efficient with respect to the classical sequential algorithms for matrix-vector multiplication and matrix multiplication.
Reference: [2] <author> R. H. Bartels and G. W. Stewart, </author> <title> Solution of the Matrix Equation AX + XB = C, </title> <journal> Comm. Ass. Comput. Mach. </journal> <volume> 15 </volume> <pages> 820-826, </pages> <year> 1972. </year>
Reference-contexts: A variety of methods exist for the solution of the Lyapunov matrix equation. One that is most effective from a numerical point of view is the method of Bartels and Stewart <ref> [2] </ref>. The method is based on the reduction of the matrix A to the real Schur form (instead of Hessemberg form), with the subsequent solution of the reduced problem. Unfortunately, the method appears not to be suitable for parallel implementations. There are at least two reasons for this.
Reference: [3] <author> S. P. Bhattacharyya and E DeSouza, </author> <title> Pole Assignment via Sylvester's Equation, </title> <journal> Systems Control Letters 1 </journal> <pages> 261-263, </pages> <year> 1982. </year>
Reference-contexts: Thus EAP is very important in the process of designing a control system. There are various sequential methods available for solving the pole placement problem. Among the best known are: * the implicit QR methods [26, 27]; * the matrix equation approach <ref> [3] </ref>; * the solution via the real Schur form [31]. Some of these approaches do not seem to be suitable for parallel processing. For instance, in the implicit QR-type methods the eigenvalues are assigned one at the time.
Reference: [4] <author> D. Bini, </author> <title> Parallel Solution of Certain Toeplitz Linear Systems, </title> <journal> SIAM J. Comput. </journal> <volume> 13 </volume> <pages> 268-276, </pages> <year> 1984. </year>
Reference: [5] <author> C. Bischof, B. N. Datta, and A. Purkayastha, </author> <title> A Parallel Algorithm for the Multi-input Sylvester-Osbserver Equation, </title> <type> Argonne Technical Report MCS-P274-1191, revision 1, </type> <year> 1994. </year>
Reference-contexts: For the Sylvester-Observer equation a widely used approach is the Observer-Hessenberg method of Van Dooren [16]. Both methods appear not to be easily parallelizable. On the other hand, the method discussed here is suitable for large scale parallel machines. The algorithm has been first presented in <ref> [5] </ref>; here we prove that optimal (theoretical) speedup is possible for a wide range of values of the number of processors available. Consider again the equation (14). Clearly, a matrix L satisfying the conditions 1 and 2 stated above could be chosen in the class of diagonal matrices. <p> Let X be partitioned conformally, i.e. X = (X 1 ; : : : ; X k ), where each X j is n fi r, and let F = (f 1 ; : : : ; f r ). The following algorithm is adapted from <ref> [5] </ref> in order to exploit maximal parallelism. Algorithm S. 1. Solve the n linear systems (i) (i) 2. Compute the n values ff j = l=1 (i) (i) 3. Compute the columns x (1) i of X 1 as follows x i = j=1 (i) (i) 4. <p> For i = 1; : : : ; k 1 do (computation of the remaining blocks) (a) ^ X i+1 = HX i X i fl ii (b) fl i+1;i = diag (i+1) (i+1) (c) X i+1 = ^ X i+1 fl 1 Theorem 18 <ref> [5] </ref> Algorithm S above computes the solution X to the Sylvester-Observer equation (14). Algorithm S is well-suited to fast parallel implementations. Cost analysis of Algorithm S. 1. To complete this step we can choose either the polylog fast algorithm or the linear time algorithm for linear system solution.
Reference: [6] <author> D. Bini and V. Pan, </author> <title> Polynomial and Matrix Computations: Fundamental Algorithms, </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1994. </year>
Reference-contexts: Theorem 6 Solving the system of linear equations Ax = b, computing det (A), and computing rank (A) can all be done in parallel time O (log 2 n) provided that O (nM (n)) processors are available. For a proof of Theorem 6 see, e.g., <ref> [6] </ref>. Since n 3 =(nM (n)) ! 0, it follows that the presently available polylogarithmic time parallel linear system solvers are not work efficient.
Reference: [7] <author> C. T. Chen, </author> <title> Linear System Theory and Design, </title> <publisher> Holt, Rinehart and Wilson, </publisher> <year> 1984. </year>
Reference-contexts: Linear control systems give rise to a variety of interesting problems, such as * controllability and osservability, * eigenvalue assignment (also called pole assignment), * stability and inertia, * matrix equations (Sylvester, Lyapunov, Riccati), just to name a few. These, in turn, give rise to many linear algebra problems <ref> [1, 7] </ref>. In this paper we focus on the parallel complexity of a number of algorithms that solve some of the above mentioned problems. In particular, we consider the controllability and the eigenvalue assignment problems, and the solution of the Lyapunov and Sylvester observer matrix equations.
Reference: [8] <author> B. Codenotti and M. Leoncini, </author> <title> Parallel Complexity of Linear System Solution, World-Scientific Pu. </title> <publisher> Co., </publisher> <address> Singapore, </address> <year> 1991. </year>
Reference-contexts: These nice properties follow from the fact that the algorithms discussed reduce essentially to basic linear algebra computations. The latter are among the most studied parallel computations, for which there is a wide body of literature on both complexity and implementation issues (see, e.g. <ref> [20, 8] </ref>). We are also interested in the parallel complexity of the problems investigated. In this respect we are able to prove a quadratic logarithmic upper bound on the parallel time complexity of the controllability and eigenvalue assignment problems. <p> More precisely, we adopt the arithmetic PRAM model, in which we assume that any arithmetic operation on real numbers can be performed at unit cost (see <ref> [8] </ref>). The crucial performance parameters for a PRAM algorithm are the parallel time and the maximum number of processors operating in parallel during a computation. Clearly, parallel time is a function of both the size of the input problem and the number of processors available.
Reference: [9] <author> D. Coppersmith and S. Winograd, </author> <title> Matrix Multiplication via Arithmetic Progression, </title> <booktitle> 19th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1987, </year> <pages> 1-6. 21 </pages>
Reference-contexts: A key example is given by the standard matrix multiplication algorithm with respect to the asymptotically fastest algorithm by Coppersmith and Winograd <ref> [9] </ref>. 2 rithm as the product of time and number of processors 3 : w p (n) = t (n)p: We say that a size independent parallel algorithm using p processors is work or processor efficient (with respect to a given sequential algorithm with running time T (n)) if w p <p> They are also work efficient with respect to the classical sequential algorithms for matrix-vector multiplication and matrix multiplication. The matrix multiplication algorithm, however, is not work efficient with respect to the fast sequential algorithms for matrix multiplication <ref> [29, 9] </ref>. In the rest of this paper we will use M (n) to denote the minimum number of processors that support O (log n) matrix multiplication. <p> In the rest of this paper we will use M (n) to denote the minimum number of processors that support O (log n) matrix multiplication. This is related to the best sequential running time for matrix multiplication, which is currently n ff , with 2 &lt; ff 2:38 (see <ref> [9] </ref>). From Theorem 4 and the remarks in the previous paragraph we easily obtain the following result. Corollary 5 The first n powers of A, i.e.
Reference: [10] <author> B. N. Datta, </author> <title> A New Criterion of Controllability, </title> <journal> IEEE Trans. Aut. Control AC-29:444--446, </journal> <year> 1984. </year>
Reference-contexts: When the conditions above are satisfied (A Hessemberg and m &lt;< n) the sequential cost of the practical algorithms for solving the controllability problem is O (n 3 ). Our parallel algorithm for testing the controllability of the pair (A; B) is based on the criterion introduced in <ref> [10] </ref>. In the multi input case, i.e. when m &gt; 1, this requires that the matrix A be further brought to the companion form. In the cases when this transformation is numerically stable, the resulting method will be both accurate and fast. <p> : : : 0 1 1 C C C A and a vector b, consider the matrix X = X C (b) whose rows x T i are defined in the following way: i = b T if i = n i+1 C c i+1 x T (5) Theorem 7 <ref> [10] </ref> Let (C; B) be a system in dual phase variable canonical form. Let b i denote the i-th column of B, i = 1; : : :; m. <p> In this case, the reduction to the companion form is not required. To test the controllability of the pair (A; B), where A is lower Hessemberg, it is sufficient to test the singularity of the matrix X, with rows x i , defined in the following way (see <ref> [10] </ref>): i = B T if i = n; a i;i+1 x T i+1 : : : a n;i+1 x T From the above recurrence, and proceeding as in the multi input case with companion matrices, it is easy to design either almost linear time efficient parallel algorithms, or O (log
Reference: [11] <author> B. N. Datta, </author> <title> Parallel Algorithms for the Lyapunov Matrix Equation, </title> <type> manuscript. </type>
Reference-contexts: This is much better from the viewpoint of parallelism, but has numerical limitations due to the diagonal reduction process (see [19]). In this section, based on the works <ref> [14, 11] </ref>, we consider two different algorithms for the solution of the equation (7) in case C is a rank-one symmetric positive semidefinite matrix. Both algorithms are characterized by running time O (n log n), provided that sufficiently many processors are available. <p> Such a small difference could still justify using Algorithm L2, in case this would guarantee a better numerical accuracy (cf. the numerical experiments reported in <ref> [11, 14] </ref>). 6 Sylvester-Observer Matrix Equation In the general Sylvester matrix equation AX XB = C (13) the matrices A, B, and C are given, and we are asked to find a matrix X which satisfies (13).
Reference: [12] <author> B. N. Datta and K. Datta, </author> <title> An Algorithm for Computing Powers of a Hessemberg Matrix and its Applications, </title> <journal> Linear Algebra Appl. </journal> <volume> 14 </volume> <pages> 273-284, </pages> <year> 1976. </year>
Reference-contexts: U A + AU 's first n 1 rows are zero). Moreover, if (x) is the characteristic polynomial of A, then r T = (1) n u T Lemma 15 <ref> [12, 22] </ref> Let H be an unreduced Hessemberg matrix, and let g (x) and h (x) be two polynomials of same degree such that e T 1 g (H) = e T 1 h (H). Then g (H) = h (H).
Reference: [13] <author> B. N. Datta and K. Datta, </author> <title> On Parallel Arithmetic Complexities of Some Linear Algebra Problems, </title> <type> Manuscript, </type> <institution> Northern Illinois University, Dekalb, Illinois, </institution> <year> 1984. </year>
Reference-contexts: For instance, in the implicit QR-type methods the eigenvalues are assigned one at the time. In this section we present a processor efficient parallel algorithm for the single input case, i.e. when B = b is a vector. The algorithm is based on one presented in <ref> [13] </ref> and runs in almost linear time. Moreover, differently from other algorithms that appear in the literature, it does not require that the matrix A be transformed into Hessemberg form.
Reference: [14] <author> B. N. Datta and K. Datta, </author> <title> A Fast Solution Method for Positive Semidefinite Lyapunov Matrix Equation, </title> <booktitle> Proc. 2nd Latin American Conference on Applied Mathematics, </booktitle> <address> Rio de Janeiro, </address> <pages> 83-104. </pages>
Reference-contexts: This is much better from the viewpoint of parallelism, but has numerical limitations due to the diagonal reduction process (see [19]). In this section, based on the works <ref> [14, 11] </ref>, we consider two different algorithms for the solution of the equation (7) in case C is a rank-one symmetric positive semidefinite matrix. Both algorithms are characterized by running time O (n log n), provided that sufficiently many processors are available. <p> Actually, it can be proved that the matrix P in (11) coincides with (H), where (x) is the characteristic polynomial of H. Now, the computation of (H) can be performed very efficiently using the results of the following two lemmas. Lemma 14 <ref> [14] </ref> Given a matrix A, there always exists a matrix U , with rows u i , i = 1; : : : ; n, such that U A + AU = e n r T (i.e. U A + AU 's first n 1 rows are zero). <p> Such a small difference could still justify using Algorithm L2, in case this would guarantee a better numerical accuracy (cf. the numerical experiments reported in <ref> [11, 14] </ref>). 6 Sylvester-Observer Matrix Equation In the general Sylvester matrix equation AX XB = C (13) the matrices A, B, and C are given, and we are asked to find a matrix X which satisfies (13).
Reference: [15] <author> B. N. Datta and K. Datta, </author> <title> The Matrix Equation XA = A T X and an Associated Algorithm for Solving the Inertia and Stability Problems, </title> <journal> Linear Algebra Appl. </journal> <volume> 97 </volume> <pages> 103-119, </pages> <year> 1987. </year>
Reference-contexts: However, the construction of the solution matrix X does not use the recurrences (10). We begin with a lemma that tells us how to solve the simple equation SA = A T S. 16 Lemma 16 <ref> [15] </ref> Let A be given, and let q be an arbitrary vector.
Reference: [16] <author> P. Van Dooren, </author> <title> Reduced Order Observers: a New Algorithm and Proofs, </title> <journal> Systems and Control Letters 4 </journal> <pages> 243-251, </pages> <year> 1984. </year>
Reference-contexts: A well-known strategy for solving the Sylvester equation is the Hessenberg-Schur approach due to Golub, Nash, and Van Loan [18]. For the Sylvester-Observer equation a widely used approach is the Observer-Hessenberg method of Van Dooren <ref> [16] </ref>. Both methods appear not to be easily parallelizable. On the other hand, the method discussed here is suitable for large scale parallel machines.
Reference: [17] <author> J. Von zur Gathen, </author> <title> Parallel Linear Algebra. </title> <editor> In: J. Reif (ed.), </editor> <title> Synthesis of Parallel Algorithm, </title> <publisher> Morgan and Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1993, </year> <pages> 573-617. </pages>
Reference-contexts: The algorithms that achieve this bound are mostly algebraic in nature and need numerical sophistications to be implemented in practice. Moreover, they are not efficient from the viewpoint of processor utilization. This study is therefore mainly of theoretical interest, but it is nonetheless important <ref> [17, 30] </ref>. The rest of the paper is organized as follows.
Reference: [18] <author> G. H. Golub, S. Nash, and C. Van Loan, </author> <title> A Hessenberg-Schur Method for the Problem AX + XB = C, </title> <journal> IEEE Trans. Aut. Control 24 </journal> <pages> 209-213, </pages> <year> 1979. </year>
Reference-contexts: A well-known strategy for solving the Sylvester equation is the Hessenberg-Schur approach due to Golub, Nash, and Van Loan <ref> [18] </ref>. For the Sylvester-Observer equation a widely used approach is the Observer-Hessenberg method of Van Dooren [16]. Both methods appear not to be easily parallelizable. On the other hand, the method discussed here is suitable for large scale parallel machines.
Reference: [19] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: In fact P i = I v T v for a certain vector v, and thus a multiplication by P i can be easily reduced essentially to the computation of a matrix-vector and an external vector products (for the details see, e.g., <ref> [19] </ref>). Performing n 2 such stages requires therefore O (n log n) time on an O (n 2 = log n) processor PRAM. We now address the question of whether an unreduced Hessemberg matrix can be trans formed by similarity to its companion form. <p> A different algorithm, based on the reduction of the matrix A to diagonal form, has been recently developed by Hodel and Poola for the sparse Lyapunov equation problem [21]. This is much better from the viewpoint of parallelism, but has numerical limitations due to the diagonal reduction process (see <ref> [19] </ref>). In this section, based on the works [14, 11], we consider two different algorithms for the solution of the equation (7) in case C is a rank-one symmetric positive semidefinite matrix. Both algorithms are characterized by running time O (n log n), provided that sufficiently many processors are available.
Reference: [20] <author> D. Heller, </author> <title> A Survey of Parallelism in Numerical Linear Algebra, </title> <journal> SIAM Rev. </journal> <volume> 20 </volume> <pages> 740-777, </pages> <year> 1978. </year>
Reference-contexts: These nice properties follow from the fact that the algorithms discussed reduce essentially to basic linear algebra computations. The latter are among the most studied parallel computations, for which there is a wide body of literature on both complexity and implementation issues (see, e.g. <ref> [20, 8] </ref>). We are also interested in the parallel complexity of the problems investigated. In this respect we are able to prove a quadratic logarithmic upper bound on the parallel time complexity of the controllability and eigenvalue assignment problems.
Reference: [21] <author> A. S. Hodel and K. Poola, </author> <title> Parallel Solution of Large Lyapunov Equations, </title> <journal> SIAM J. Matrix Analysis Appl. </journal> <volume> 13 </volume> <month> 1189-1203 </month> <year> 1992. </year>
Reference-contexts: A different algorithm, based on the reduction of the matrix A to diagonal form, has been recently developed by Hodel and Poola for the sparse Lyapunov equation problem <ref> [21] </ref>. This is much better from the viewpoint of parallelism, but has numerical limitations due to the diagonal reduction process (see [19]).
Reference: [22] <author> C. P. Huang, </author> <title> Computing Powers of Arbitrary Hessenberg Matrices, </title> <journal> Linear Algebra Appl. </journal> <volume> 21 </volume> <pages> 123-134, </pages> <year> 1978. </year>
Reference-contexts: U A + AU 's first n 1 rows are zero). Moreover, if (x) is the characteristic polynomial of A, then r T = (1) n u T Lemma 15 <ref> [12, 22] </ref> Let H be an unreduced Hessemberg matrix, and let g (x) and h (x) be two polynomials of same degree such that e T 1 g (H) = e T 1 h (H). Then g (H) = h (H).
Reference: [23] <author> R. E. </author> <title> Kalman, On the General Theory of Control Systems, </title> <booktitle> Proc. 1st IFAC Congress, </booktitle> <volume> vol. 1, </volume> <year> 1960, </year> <pages> 481-491. </pages>
Reference-contexts: If m is close to n or the matrix A is not in Hessemberg form, all these methods are characterized by an operation count as high as n 4 , and are therefore impractical. For instance, Kalman's criterion <ref> [23] </ref> requires that rank BjABj : : : jA n1 B We therefore assume that A is an nfin lower Hessember matrix while B is any nfim matrix, with m &lt;< n. The assumption that A be Hessemberg is not a great loss of generality.
Reference: [24] <author> R. M. Karp and V. Ramachandran, </author> <title> Parallel Algorithms for Shared Memory Machines. </title> <editor> In: J. van Leeuwen (ed.), </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity, </booktitle> <publisher> The MIT Press/Elsevier, </publisher> <year> 1990, </year> <pages> 869-941. </pages>
Reference-contexts: range of values of the processor bound p. 2 Preliminaries Conforming to a huge body of literature, we adopt as the underlying computation model the Parallel Random Access Machines (PRAM), a shared memory multiprocessor which has proved to be effective for the design and analysis of parallel algorithms (see, e.g., <ref> [24, 25] </ref>). More precisely, we adopt the arithmetic PRAM model, in which we assume that any arithmetic operation on real numbers can be performed at unit cost (see [8]). <p> The relationships among size dependent and size independent parallel algorithms can be further explained in light of the well-known Brent's scheduling principle (see, for instance, <ref> [24] </ref>). Suppose that a parallel algorithm P for a problem does work w on a p processors PRAM. Then Brent's scheduling principle states that there are implementations of P that do essentially the same work on any q processor PRAM, for any value of q 2 [1; p].
Reference: [25] <author> C. P. Kruskal, L. Rudolph, and M. </author> <title> Snir A Complexity Theory of Efficient Parallel Algorithms Theoret. </title> <journal> Comput. Sci. </journal> <volume> 71 </volume> <pages> 95-132, </pages> <year> 1990. </year> <month> 22 </month>
Reference-contexts: range of values of the processor bound p. 2 Preliminaries Conforming to a huge body of literature, we adopt as the underlying computation model the Parallel Random Access Machines (PRAM), a shared memory multiprocessor which has proved to be effective for the design and analysis of parallel algorithms (see, e.g., <ref> [24, 25] </ref>). More precisely, we adopt the arithmetic PRAM model, in which we assume that any arithmetic operation on real numbers can be performed at unit cost (see [8]). <p> On the other hand, the number of processors can be regarded either as a free parameter or as a function of the input size. When the number p of processors is a free parameter we have size independent parallel algorithms (see <ref> [25] </ref>). The running time in this case is denoted by t p (n). On the other hand, in the context of size dependent parallel algorithms, we assume an (in principle) unbounded availability of processors and try to design algorithms that minimize time.
Reference: [26] <author> G. Minimis and C. C. Paige, </author> <title> An Algorithm for Pole Assignment of Time Invariant Linear Systems, </title> <journal> International Journal on Control 35 </journal> <pages> 130-138, </pages> <year> 1981. </year>
Reference-contexts: Thus EAP is very important in the process of designing a control system. There are various sequential methods available for solving the pole placement problem. Among the best known are: * the implicit QR methods <ref> [26, 27] </ref>; * the matrix equation approach [3]; * the solution via the real Schur form [31]. Some of these approaches do not seem to be suitable for parallel processing. For instance, in the implicit QR-type methods the eigenvalues are assigned one at the time.
Reference: [27] <author> R. V. Patel and P. Misra, </author> <title> Numerical Algorithms for Eigenvalue Assignment by State Feedback, </title> <booktitle> Proc. of the IEEE 17 </booktitle> <pages> 1755-1764, </pages> <year> 1984. </year>
Reference-contexts: Thus EAP is very important in the process of designing a control system. There are various sequential methods available for solving the pole placement problem. Among the best known are: * the implicit QR methods <ref> [26, 27] </ref>; * the matrix equation approach [3]; * the solution via the real Schur form [31]. Some of these approaches do not seem to be suitable for parallel processing. For instance, in the implicit QR-type methods the eigenvalues are assigned one at the time.
Reference: [28] <author> A. H. Sameh and D. J. </author> <title> Kuck On Stable Parallel Linear System Solvers, </title> <journal> J. Assoc. Comput. Mach. </journal> <volume> 25 </volume> <pages> 81-91, </pages> <year> 1978. </year>
Reference-contexts: Therefore X C (b) can be computed in 3 (n 1) parallel steps using 2n 1 processors. To test whether X C (b) is singular, parallel algorithms exist which requires O (n) time on O (n 2 ) processors (see, e.g., <ref> [28] </ref>). It easily follows from Theorem 7 and Lemma 8 that the controllability of a system in dual phase variable canonical form can be tested in parallel time O (n) using O (mn 2 ) processors. <p> The same bounds apply (exactly for the same reasons) also to step 2. Step 3 requires the solution of a linear system, which can be done in linear time, provided that O (n 2 ) processors are available <ref> [28] </ref>. Finally, the computation of the columns 2 through n of the matrix Y (step 4) can be done according to (10) in time O (n log n) on O (n 2 = log n) processors. <p> To complete this step we can choose either the polylog fast algorithm or the linear time algorithm for linear system solution. In the latter case we can exploit the Hessemberg structure of H to further bound the processor demand of the linear system solver presented in <ref> [28] </ref>. In fact, when the input matrix is Hessemberg, any Givens rotation can be performed in costant time, provided that O (n) processors are available. It follows that the linear time solution requires only O (n) processors. 2.
Reference: [29] <author> V. </author> <title> Strassen Gaussian Elimination is not Optimal Numer. </title> <journal> Math. </journal> <volume> 13 </volume> <pages> 354-356, </pages> <year> 1969. </year>
Reference-contexts: They are also work efficient with respect to the classical sequential algorithms for matrix-vector multiplication and matrix multiplication. The matrix multiplication algorithm, however, is not work efficient with respect to the fast sequential algorithms for matrix multiplication <ref> [29, 9] </ref>. In the rest of this paper we will use M (n) to denote the minimum number of processors that support O (log n) matrix multiplication.
Reference: [30] <author> V. </author> <title> Strassen Algebraic Complexity Theory. </title> <editor> In: J. van Leeuwen (ed.), </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity, </booktitle> <publisher> The MIT Press/Elsevier, </publisher> <year> 1990, </year> <pages> 633-672. </pages>
Reference-contexts: The algorithms that achieve this bound are mostly algebraic in nature and need numerical sophistications to be implemented in practice. Moreover, they are not efficient from the viewpoint of processor utilization. This study is therefore mainly of theoretical interest, but it is nonetheless important <ref> [17, 30] </ref>. The rest of the paper is organized as follows.
Reference: [31] <author> A. Varga, </author> <title> A Schur Method for Pole Assignment, </title> <journal> IEEE Trans. Aut. Control 26 </journal> <pages> 517-519, </pages> <year> 1981. </year>
Reference-contexts: There are various sequential methods available for solving the pole placement problem. Among the best known are: * the implicit QR methods [26, 27]; * the matrix equation approach [3]; * the solution via the real Schur form <ref> [31] </ref>. Some of these approaches do not seem to be suitable for parallel processing. For instance, in the implicit QR-type methods the eigenvalues are assigned one at the time.
References-found: 31

