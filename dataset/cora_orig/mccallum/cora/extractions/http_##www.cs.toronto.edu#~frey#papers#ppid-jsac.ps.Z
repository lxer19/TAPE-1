URL: http://www.cs.toronto.edu/~frey/papers/ppid-jsac.ps.Z
Refering-URL: http://www.cs.toronto.edu/~frey/papers/ppid-jsac.abs.html
Root-URL: http://www.cs.toronto.edu
Title: Iterative Decoding of Compound Codes by Probability Propagation in Graphical Models  
Author: Frank R. Kschischang, Member, IEEE and Brendan J. Frey 
Date: 1, JAN. 1998 1  
Note: IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 16, NO.  
Abstract: We present a unified graphical model framework for describing compound codes and deriving iterative decoding algorithms. After reviewing a variety of graphical models (Markov random fields, Tanner graphs, and Bayesian networks), we derive a general distributed marginalization algorithm for functions described by factor graphs. From this general algorithm, Pearl's belief propagation algorithm is easily derived as a special case. We point out that recently developed iterative decoding algorithms for various codes, including turbo decoding of parallel-concatenated convolutional codes, may be viewed as probability propagation in a graphical model of the code. We focus on Bayesian network descriptions of codes, which give a natural input/state/output/channel description of a code and channel, and we indicate how iterative decoders can be developed for parallel- and serially-concatenated coding systems, product codes, and low-density parity-check codes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Berrou, A. Glavieux, and P. Thitimajshima, </author> <title> Near Shannon limit error-correcting coding and decoding: Turbo codes, </title> <booktitle> in Proc. IEEE Int. Conf. Commun. </booktitle> <address> (ICC), (Geneva, Switzerland), </address> <pages> pp. 10641070, </pages> <year> 1993. </year>
Reference-contexts: The turbo decoding algorithm uses the forward-backward algorithm [35,36] (or an approximation to it) to process each constituent trellis. The algorithm uses extrinsic information <ref> [1, 7] </ref> produced by the previous step, when processing the next trellis. This is the information that is passed from one trellis to the other through the information symbols. <p> Nevertheless, the networks are broken into tractable KSCHISCHANG AND FREY: ITERATIVE DECODING BY PROBABILITY PROPAGATION 11 subnetworks, each describing a constituent code and in which probability propagation can be applied. Iterating over these constituent decoders can result in excellent decoding performance in practice, as demonstrated by Berrou, et al. <ref> [1] </ref>. We have shown that many recently proposed iterative decoders can be described as message passing in a graphical code model.
Reference: [2] <author> G. D. Forney, Jr., </author> <title> Concatenated Codes. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1966. </year>
Reference-contexts: A serially concatenated convolutional compound code was proposed by Benedetto and Montorsi [3]. Their system is essentially the same as Forney's concatenated codes <ref> [2] </ref>, with convolutional inner and outer codes. The Bayesian network for a non-systematic rate 1/2 compound code of this sort is shown in Fig. 9 (b), along with its two cycle-free constituent networks.
Reference: [3] <author> S. Benedetto and G. Montorsi, </author> <title> Serial concatenation of block and convolutional codes, </title> <journal> Electronics Letters, </journal> <volume> vol. 32, </volume> <pages> pp. 887888, </pages> <year> 1996. </year>
Reference-contexts: Bayesian networks for three compound codes and their constituent codes: (a) A systematic rate 1/3 turbo code; (b) A rate 1/2 serially concatenated convolutional code; (c) A low-density parity check code; (d) a (15,9) product code. A serially concatenated convolutional compound code was proposed by Benedetto and Montorsi <ref> [3] </ref>. Their system is essentially the same as Forney's concatenated codes [2], with convolutional inner and outer codes. The Bayesian network for a non-systematic rate 1/2 compound code of this sort is shown in Fig. 9 (b), along with its two cycle-free constituent networks.
Reference: [4] <author> S. Benedetto and G. Montorsi, </author> <title> Iterative decoding of serially concatenated convolutional codes, </title> <journal> Electronics Letters, </journal> <volume> vol. 32, </volume> <pages> pp. 11861188, </pages> <year> 1996. </year>
Reference-contexts: This graphical framework unifies several iterative decoding algorithms. The turbo decoding algorithm, the separable MAP filter algorithm [6], the new iterative decoding algorithm <ref> [4] </ref> used for decoding serially concatenated convolutional codes, and, as first pointed out by MacKay and Neal [16], Gallager's algorithm for decoding low-density parity check codes [5] are all a form of probability propagation in the compound code networks shown in Fig. 9.
Reference: [5] <author> R. G. Gallager, </author> <title> Low-Density Parity-Check Codes. </title> <address> Cambridge, MA: </address> <publisher> M.I.T. Press, </publisher> <year> 1963. </year>
Reference-contexts: Only the output of the inner convolutional code is transmitted over the channel. Fig. 9 (c) shows an example of a Gallager low-density parity check code <ref> [5] </ref>. <p> This graphical framework unifies several iterative decoding algorithms. The turbo decoding algorithm, the separable MAP filter algorithm [6], the new iterative decoding algorithm [4] used for decoding serially concatenated convolutional codes, and, as first pointed out by MacKay and Neal [16], Gallager's algorithm for decoding low-density parity check codes <ref> [5] </ref> are all a form of probability propagation in the compound code networks shown in Fig. 9. The overall decoding procedure essentially consists of applying probability propagation while ignoring the graph cycles. The procedure can be broken down into a series of processing cycles.
Reference: [6] <author> J. Lodge, R. Young, P. Hoeher, and J. Hagenauer, </author> <title> Separable MAP `filters' for the decoding of product and concatenated codes, </title> <booktitle> in Proceedings of IEEE International Conference on Communications, </booktitle> <pages> pp. 17401745, </pages> <year> 1993. </year>
Reference-contexts: This graphical framework unifies several iterative decoding algorithms. The turbo decoding algorithm, the separable MAP filter algorithm <ref> [6] </ref>, the new iterative decoding algorithm [4] used for decoding serially concatenated convolutional codes, and, as first pointed out by MacKay and Neal [16], Gallager's algorithm for decoding low-density parity check codes [5] are all a form of probability propagation in the compound code networks shown in Fig. 9.
Reference: [7] <author> J. Hagenauer, E. Offer, and L. Papke, </author> <title> Iterative decoding of binary block and convolutional codes, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 429445, </pages> <year> 1996. </year>
Reference-contexts: The turbo decoding algorithm uses the forward-backward algorithm [35,36] (or an approximation to it) to process each constituent trellis. The algorithm uses extrinsic information <ref> [1, 7] </ref> produced by the previous step, when processing the next trellis. This is the information that is passed from one trellis to the other through the information symbols.
Reference: [8] <author> B. J. Frey and F. R. Kschischang, </author> <title> Probability propagation and iterative decoding, </title> <booktitle> in Proc. 34th Annual Allerton Conf. on Communication, Control, and Computing, (Allerton House, </booktitle> <address> Monticello, </address> <publisher> Illinois), </publisher> <pages> pp. 482493, </pages> <month> October </month> <year> 1996. </year>
Reference: [9] <author> B. J. Frey, </author> <title> Bayesian Networks for Pattern Classification, Data Compression and Channel Coding. </title> <institution> Toronto, Canada: Department of Electrical and Computer Engineering, University of Toronto, </institution> <year> 1997. </year> <note> Doctoral dissertation available at http://www.cs.utoronto.ca/frey. </note>
Reference: [10] <author> J. Pearl, </author> <title> Fusion, propagation, and structuring in belief networks, </title> <journal> Artificial Intelligence, </journal> <volume> vol. 29, </volume> <pages> pp. 241288, </pages> <year> 1986. </year>
Reference: [11] <author> S. L. Lauritzen and D. J. Spiegelhalter, </author> <title> Local computations with probabilities on graphical structures and their application to expert systems, </title> <journal> J. of the Royal Statistical Society, Series B, </journal> <volume> vol. 50, </volume> <pages> pp. 157224, </pages> <year> 1988. </year>
Reference: [12] <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year> <note> Revised second printing. </note>
Reference: [13] <author> R. E. </author> <title> Neapolitan, Probabilistic Reasoning in Expert Systems: Theory and Algorithms. </title> <publisher> Toronto: John Wiley & Sons, </publisher> <year> 1990. </year>
Reference: [14] <author> F. V. Jensen, </author> <title> An Introduction to Bayesian Networks. </title> <address> New York: </address> <publisher> Springer Verlag, </publisher> <year> 1996. </year>
Reference: [15] <author> S. L. Lauritzen, </author> <title> Graphical Models. </title> <publisher> Oxford University Press, </publisher> <year> 1996. </year>
Reference: [16] <author> D. J. C. MacKay and R. M. Neal, </author> <title> Good codes based on very sparse matrices, in Cryptography and Coding. </title> <booktitle> 5th IMA Conference (C. </booktitle> <editor> Boyd, ed.), </editor> <volume> no. </volume> <booktitle> 1025 in Lecture Notes in Computer Science, </booktitle> <pages> pp. 100111, </pages> <address> Berlin Ger-many: </address> <publisher> Springer, </publisher> <year> 1995. </year>
Reference-contexts: The channel outputs are indicated below the row of codeword symbols. MacKay and Neal <ref> [16] </ref> were the first to describe Gallager's codes using Bayesian networks. <p> This graphical framework unifies several iterative decoding algorithms. The turbo decoding algorithm, the separable MAP filter algorithm [6], the new iterative decoding algorithm [4] used for decoding serially concatenated convolutional codes, and, as first pointed out by MacKay and Neal <ref> [16] </ref>, Gallager's algorithm for decoding low-density parity check codes [5] are all a form of probability propagation in the compound code networks shown in Fig. 9. The overall decoding procedure essentially consists of applying probability propagation while ignoring the graph cycles.
Reference: [17] <author> D. J. C. MacKay and R. M. Neal, </author> <title> Near Shannon limit performance of low density parity check codes, </title> <journal> Electronics Letters, </journal> <volume> vol. 32, no. 18, </volume> <pages> pp. 1645 1646, </pages> <year> 1996. </year> <note> Reprinted in vol. 33, </note> <month> March </month> <year> 1997, </year> <pages> pp. 457458. </pages>
Reference: [18] <author> D. J. C. MacKay, </author> <title> Good error-correcting codes based on very sparse matrices. </title> <note> Submitted to IEEE Transactions on Information Theory, </note> <year> 1997. </year>
Reference: [19] <author> R. J. McEliece, D. J. C. MacKay, and J.-F. Cheng, </author> <title> Turbo decoding as an instance of Pearl's `belief propagation' algorithm. </title> <journal> To appear in IEEE J. Selected Areas in Commun., </journal> <month> Jan. </month> <year> 1998. </year>
Reference-contexts: Usually the cyclic procedure is allowed to iterate until some termination criterion is satisfied. Then, the information symbols are detected, usually in the fashion of the maximum a posteriori (MAP) symbol probability decoding rule. C. Turbo Decoding: Probability Propagation for Turbo Codes For example, as shown explicitly in <ref> [19, this issue] </ref>, this probability propagation algorithm, when applied to turbo codes, is the standard turbo decoding algorithm. The turbo decoding algorithm uses the forward-backward algorithm [35,36] (or an approximation to it) to process each constituent trellis.
Reference: [20] <author> N. Wiberg, H.-A. Loeliger, and R. Kotter, </author> <title> Codes and iterative decoding on general graphs, </title> <journal> European Trans. on Telecommun., </journal> <volume> vol. 6, </volume> <pages> pp. 513525, </pages> <address> Sep./Oct. </address> <year> 1995. </year>
Reference-contexts: Whereas the upper chain di rectly uses the information sequence u, the lower chain uses a permuted sequence, obtained by applying an interleaver. The systematic codeword component has been included with the up per constituent code. Wiberg, et al. <ref> [20, 21] </ref> were probably the first to describe turbo codes using this type of graphical model. u (a) (b) u (c) (d) Fig. 9.
Reference: [21] <author> N. Wiberg, </author> <title> Codes and Decoding on General Graphs. </title> <type> PhD thesis, </type> <institution> Linkoping University, Sweden, </institution> <year> 1996. </year>
Reference-contexts: Whereas the upper chain di rectly uses the information sequence u, the lower chain uses a permuted sequence, obtained by applying an interleaver. The systematic codeword component has been included with the up per constituent code. Wiberg, et al. <ref> [20, 21] </ref> were probably the first to describe turbo codes using this type of graphical model. u (a) (b) u (c) (d) Fig. 9.
Reference: [22] <author> R. M. Tanner, </author> <title> A recursive approach to low complexity codes, </title> <journal> IEEE Trans. on Inform. Theory, </journal> <volume> vol. IT-27, </volume> <pages> pp. 533547, </pages> <month> Sept. </month> <year> 1981. </year>
Reference: [23] <author> G. D. Forney, Jr., </author> <title> Trellises old and new, in Communications and Cryptography: Two Sides of One Tapestry (R. </title> <editor> E. Blahut, D. J. Costello, Jr., U. Maurer, and T. Mittelholzer, eds.), pp. </editor> <volume> 115128, </volume> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference: [24] <author> G. D. Forney, Jr., </author> <title> The forward-backward algorithm, </title> <booktitle> in Proc. 34th Annual Allerton Conf. on Communication, Control, and Computing, </booktitle> <publisher> (Aller-ton House, Monticello, Illinois), </publisher> <pages> pp. 432446, </pages> <month> October </month> <year> 1996. </year>
Reference: [25] <author> R. Kindermann and J. L. Snell, </author> <title> Markov Random Fields and their Applications. </title> <address> Providence, Rhode Island: </address> <publisher> American Mathematical Society, </publisher> <year> 1980. </year>
Reference: [26] <author> C. J. Preston, </author> <title> Gibbs States on Countable Sets. </title> <publisher> Cambridge University Press, </publisher> <year> 1974. </year>
Reference: [27] <author> V. Isham, </author> <title> An introduction to spatial point processes and Markov random fields, </title> <journal> Int. Stat. Rev., </journal> <volume> vol. 49, </volume> <pages> pp. 2143, </pages> <year> 1981. </year>
Reference: [28] <author> G. E. Hinton and T. J. Sejnowski, </author> <title> Learning and relearning in Boltzmann machines, in Parallel Distributed Processing: Explorations in the Mi-crostructure of Cognition (D. </title> <editor> E. Rumelhart and J. L. McClelland, eds.), </editor> <volume> vol. I, </volume> <pages> pp. 282317, </pages> <address> Cambridge MA.: </address> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference: [29] <author> S. M. Aji and R. J. </author> <title> McEliece, A general algorithm for distributing information on a graph, </title> <booktitle> in Proc. 1997 IEEE Int. Symp. on Inform. Theory, </booktitle> <address> (Ulm, Germany), p. 6, </address> <month> July </month> <year> 1997. </year>
Reference: [30] <author> S. Verdu and H. V. </author> <title> Poor, Abstract dynamic programming models under commutativity conditions, </title> <journal> SIAM J. on Control and Optimization, </journal> <volume> vol. 25, </volume> <pages> pp. 9901006, </pages> <month> July </month> <year> 1987. </year>
Reference: [31] <author> R. J. </author> <title> McEliece, On the BJCR trellis for linear block codes, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 10721092, </pages> <month> July </month> <year> 1996. </year>
Reference: [32] <author> E. Schrodinger, </author> <title> Statistical Thermodynamics. </title> <publisher> Cambridge University Press, </publisher> <year> 1962. </year>
Reference: [33] <author> B. J. Frey, F. R. Kschischang, and P. G. Gulak, </author> <title> Concurrent turbo-decoding, </title> <booktitle> in Proc. 1997 IEEE Int. Symp. on Inform. Theory, </booktitle> <address> (Ulm, Ger-many), p. 431, </address> <year> 1997. </year>
Reference: [34] <author> D. Bertsekas and R. Gallager, </author> <title> Data Networks. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice-Hall, </publisher> <editor> 2nd ed., </editor> <year> 1992. </year>
Reference: [35] <author> L. E. Baum and T. Petrie, </author> <title> Statistical inference for probabilistic functions of finite state Markov chains, </title> <journal> Annals of Mathematical Statistics, </journal> <volume> vol. 37, </volume> <pages> pp. 15591563, </pages> <year> 1966. </year>

References-found: 35

