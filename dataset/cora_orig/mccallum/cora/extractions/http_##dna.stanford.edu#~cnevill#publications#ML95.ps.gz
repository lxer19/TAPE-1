URL: http://dna.stanford.edu/~cnevill/publications/ML95.ps.gz
Refering-URL: http://dna.stanford.edu/~cnevill/resume.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: -srg1,sallyjo,geoff,cgn,ihw-@cs.waikato.ac.nz  
Title: Applying a Machine Learning Workbench: Experience with Agricultural Databases  
Author: Stephen R. Garner, Sally Jo Cunningham, Geoffrey Holmes, Craig G. Nevill-Manning and Ian H. Witten 
Address: Hamilton, New Zealand.  
Affiliation: Computer Science Department, University of Waikato,  
Abstract: This paper reviews our experience with the application of machine learning techniques to agricultural databases. We have designed and implemented a machine learning workbench, WEKA, which permits rapid experimentation on a given dataset using a variety of machine learning schemes, and has several facilities for interactive investigation of the data: preprocessing attributes, evaluating and comparing the results of different schemes, and designing comparative experiments to be run offline. We discuss the partnership between agricultural scientist and machine learning researcher that our experience has shown to be vital to success. We review in some detail a particular agricultural application concerned with the culling of dairy herds. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cunningham, S.J. </author> <title> (1995) Machine learning and statistics: A matter of perspective.. </title> <type> Working Paper Series 95/11, </type> <institution> Department of Computer Science, University of Waikato (Hamilton, </institution> <address> New Zealand). </address>
Reference-contexts: One useful technique is to stress the commonalities between statistics and machine learning, presenting them as complementary rather than promoting machine learning as a replacement for statistical analysis <ref> (Cunningham 1995) </ref>. A second is to point out that machine learning is useful for generating hypotheses, which can then be supported by further statistical analysis or scientific testing. In the initial visits to an agricultural institute, we emphasise the need to establish a partnership in analysing the data.
Reference: <author> Fu, </author> <title> K.S. (1968) Sequential methods in pattern recognition and machine learning New York: </title> <publisher> Academic Press. </publisher>
Reference: <author> Gaines, B.R. </author> <title> (1991) The tradeoff between knowledgte and data in knowledge acquisition. In Knowledge discovery in databases, edited by GT. </title> <editor> Piatetsky-Shapiro and W.J. Frawley. </editor> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <pages> pp. 491505. </pages>
Reference-contexts: The precise details of output translation varies for the individual schemes. For FOIL , which produces its output as rules already, it is a simple matter to convert the rule format to PREval. I NDUCT <ref> (Gaines 1991) </ref>, which produces rules in a special rippledown structure, provides an option to produce its output directly in the PRE val format. For C4.5, both decision trees and rules are converted automatically into PREval rules.
Reference: <author> Harner, E.J., and Galfalvy, H.C. </author> <year> (1995) </year> <month> OmegaStat: </month> <title> an environment for implementing intelligenct modeling strategies. </title> <booktitle> Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale (Florida, USA), </address> <pages> pp. </pages> <month> .252-258. </month>
Reference: <author> Holmes, G., Donkin, A., and Witten, I.H. </author> <year> (1994) </year> <month> Weka: </month> <title> a machine learning workbench. </title> <booktitle> Proceedings of the 1994 Second Australian and New Zealand Conference on Intelligent Information Systems, </booktitle> <address> Brisbane, Australia, </address> <pages> pp. 357 - 361. </pages>
Reference: <author> Holmes, G., and Nevill-Manning, </author> <title> C.G. (1995) Feature selection via the discovery of simple classification rules. </title> <booktitle> To appear in Proceedings of Symposium on Intelligent Data Analysis (IDA95), </booktitle> <address> Baden-Baden, Germany, </address> <month> August, </month> <year> 1995. </year>
Reference-contexts: FSS is a forward sequential selection algorithm which iteratively adds attributes and tests their effectiveness/relevance with an induction algorithm. It produces a list of what the induction algorithm considers to be the most relevant attributes. Runic <ref> (Smith and Holmes 1995) </ref>, an algorithm for subset selection that examines rough numeric dependencies in the data. An attribute is considered potentially relevant if a particular subrange of its values can be used to predict the value of another feature more accurately than by pure chance.
Reference: <author> Holte, </author> <title> R.C. (1993) Very simple classification rules perform well on most commonly-used datasets. </title> <booktitle> Machine Learning 11, </booktitle> <pages> pp. 6391. </pages>
Reference-contexts: Preselection of potentially relevant attributes is critical, as omitting a key feature or including irrelevant ones can both lead to poor classification accuracy. We run the data through one or more of the following schemes to select the most relevant features: 1R, the single-attribute classifier developed by Holte <ref> (Holte 1993) </ref>. While Holte uses 1R as a standalone learning scheme, we view it as a feature selector. Applying 1R iteratively with each of the attributes in the raw data allows us to rank attributes by their classificatory power. <p> It is a more general implementation of Holtes experimental paradigm <ref> (Holte 1993) </ref>. A user specifies the ratio of the size of test and training sets and the number of runs required (for example, Holte uses a 1/3:2/3 split of data and 25 runs).
Reference: <author> Kohavi, R., John, G., Long, R., Manley, D. and Pfleger, K. </author> <year> (1994) </year> <month> MLC++: </month> <title> A Machine Learning Library in C++, </title> <type> Tech Report, </type> <institution> Computer Science Dept, Stanford University. </institution>
Reference: <author> McQueen, R.J., Garner, S.R., Nevill-Manning, C.G., and Witten, I.H. </author> <title> (1994) Applying machine learning to agricultural data. </title> <note> In Press, Journal of Computing and Electronics in Agriculture. Also available as Working Paper Series 94/3 Department of Computer Science, </note> <institution> University of Waikato (Hamilton, </institution> <address> New Zealand). </address>
Reference: <author> McQueen, R.J., Neal, D.L., DeWar, R.E., and Nevill - Manning, </author> <title> C.G. (1994) The WEKA machine learning workbench: its application to a real world agricultural database. </title> <booktitle> Proceedings of the Canadian Machine Learning Workshop, </booktitle> <address> Banff, Alberta, Canada. </address>
Reference: <author> Quinlan, J.R. </author> <year> (1992) </year> <month> C4.5: </month> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Applying 1R iteratively with each of the attributes in the raw data allows us to rank attributes by their classificatory power. C4.5, a learning scheme that has proven in practice to be quite effective in winnowing out irrelevant attributes <ref> (Quinlan 1992) </ref>. We apply C4.5 directly to the initial data table, and eliminate attributes that do not play a significant role in the resulting decision tree. As will be discussed in Section 4, we also gain quick insight into possible problems with the format of key attributes. <p> When the data was originally denormalized into a single table, the link and join relations duplicated some attributes (under different names). These were identified and deleted. The principal tools used to select potentially relevant attributes were C4.5 <ref> (Quinlan 1992) </ref> and FOIL. The initial flat table was run through C 4.5 on the workbench. Cows were classified on their fate code attribute, which can take the values sold, dead, lost and unknown. The resulting tree, shown in Figure 5, proved disappointing.
Reference: <author> Quinlan, J.R. and Cameron-Jones, </author> <title> R.M. (1993) FOIL: a midterm report, </title> <booktitle> Proc European Conf on Machine Learning, </booktitle> <pages> pp 320. </pages> <publisher> Springer Verlag. </publisher>
Reference-contexts: If these relationships are identified as patterns by the system, they will not be considered interesting by the user. Most machine learning tools will introduce these two problems because they only work with a single relation, though some techniques, for example FOIL <ref> (Quinlan and Cameron-Jones, 1993) </ref>, use first-order logic and work with multiple relations. Once the database is in a single relation, each attribute must be examined to determine its data typefor example, whether it contains numeric or symbolic information.
Reference: <author> Smith, T.C., and Holmes, G. </author> <title> (1995) Subset selection using rough numeric dependency. </title> <booktitle> To appear in Proceedings of Symposium on Intelligent Data Analysis (IDA95), Baden - Baden, </booktitle> <address> Germany, </address> <month> August, </month> <year> 1995.. </year>
Reference-contexts: FSS is a forward sequential selection algorithm which iteratively adds attributes and tests their effectiveness/relevance with an induction algorithm. It produces a list of what the induction algorithm considers to be the most relevant attributes. Runic <ref> (Smith and Holmes 1995) </ref>, an algorithm for subset selection that examines rough numeric dependencies in the data. An attribute is considered potentially relevant if a particular subrange of its values can be used to predict the value of another feature more accurately than by pure chance.
Reference: <author> Ting, K.M. </author> <title> (1994) The problem of small disjuncts: its remedy in decision trees. </title> <editor> In: Elio, R. (Editor), </editor> <booktitle> Proc. Tenth Canadian Conference on Artificial Intelligence; pp 9197. Canadian Society for Computational Studies of Intelligence. </booktitle>
Reference-contexts: One approach is to reduce the number of instances of the larger class in the training set while maintaining the numbers of instances in the smaller classes. Another is to construct a hybrid scheme by augmenting C4.5 with an instance-based learner <ref> (Ting 1994) </ref>. 2.2 ATTRIBUTE ANALYSISFROM CLEAN TO USEFUL DATA At this point, we have cleaned up rows (data instances) and now need to determine the columns that are most likely to provide information and the one to be used for classification.
Reference: <author> Witten, I.H., Cunningham, S.J., Holmes, G., McQueen, R., and Smith, L. </author> <title> (1993) Practical Machine Learning and its Application to Problems in Agriculture. </title> <booktitle> Proceedings of the New Zealand Computer Society Conference , Auckland, </booktitle> <address> New Zealand, </address> <pages> pp. 308-325. </pages>
Reference-contexts: Papers describing the workbench have already appeared in the literature <ref> (Witten 1993) </ref>.
Reference: <author> Young, F.W., and Lubinsky, </author> <title> D.J. (1995) Learning from data by guiding the analyst: on the representation, use, and creation of visual statistical strategies. </title> <booktitle> Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale (Florida, USA), </address> <pages> pp. 531-539. </pages>
References-found: 16

