URL: http://www.cs.princeton.edu/~skadron/papers/tradeoffs.ps
Refering-URL: http://www.cs.princeton.edu/~skadron/pub_list.html
Root-URL: http://www.cs.princeton.edu
Email: fskadron,psa,dougg@cs.princeton.edu, mrm@ee.princeton.edu  
Title: Branch Prediction, Instruction-Window Size, and Cache Size: Performance Tradeoffs and Sampling Techniques  
Author: Kevin Skadron, Pritpal S. Ahuja, Margaret Martonosi Douglas W. Clark 
Note: This work was supported in part by NSF grant CCR-94-23123, NSF Career Award CCR-95-02516 (Martonosi), and an NDSEG Graduate Fellowship (Skadron).  
Address: Princeton University, Princeton, NJ 08540.  
Affiliation: Departments of Computer Science and Electrical Engineering  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P. S. Ahuja, K. Skadron, M. Martonosi, and D. W. Clark. </author> <title> Multi-path execution: Opportunities and limits. </title> <booktitle> In Proc. 12th ICS, </booktitle> <month> July </month> <year> 1998. </year>
Reference-contexts: The primary bottleneck remains branch prediction. As architects attack the branch-prediction bottleneck with more sophisticated hardware schemes and alternative techniquestrace caches [44], compiler-enhanced hardware prediction [2], [33], predication [22], [43], and multi-path execution <ref> [1] </ref>, [28], [54], [56]larger RUUs will become attractive. L1 data-miss penalties will always inspire innovations, but caches are now so big and sophisticated that future work should perhaps focus on the most latency-intolerant misses. We also consider sampling techniques to allow shorter but full-detail simulations.
Reference: [2] <author> D. I. August, D. A. Connors, J. C. Gyllenhaal, and W. W. Hwu. </author> <title> Architectural support for compiler-synthesized dynamic branch prediction strategies: Rationale and initial results. </title> <booktitle> In Proc. HPCA-3, </booktitle> <pages> pages 8493, </pages> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: Because potential for further improvements with such methods is dwindling [8], attention is turning to more sophisticated techniques. Young, Gloy, and Smith [60], [61] have demonstrated compiler-based methods for correlated branch prediction; Mahlke and Natara-jan [33] and August et al. <ref> [2] </ref> have examined branch prediction synthesized in the compiler. A different approach has been taken by Jacobsen, Rotenberg, Smith, et al, who examine confidence ratings attached to branch predictors [20]. <p> The primary bottleneck remains branch prediction. As architects attack the branch-prediction bottleneck with more sophisticated hardware schemes and alternative techniquestrace caches [44], compiler-enhanced hardware prediction <ref> [2] </ref>, [33], predication [22], [43], and multi-path execution [1], [28], [54], [56]larger RUUs will become attractive. L1 data-miss penalties will always inspire innovations, but caches are now so big and sophisticated that future work should perhaps focus on the most latency-intolerant misses.
Reference: [3] <author> D. Burger. </author> <type> Personal communication, </type> <month> Mar. </month> <year> 1998. </year>
Reference-contexts: When sampling like we do, the simulator must be warmed up until such initial effects have passed; fortunately, this can be done fairly quickly using fast-mode simulation. SimpleScalar will soon offer a checkpointing facility that removes even the need for fast-mode simulation <ref> [3] </ref>. Results can change significantly when the 50M-instruction simulation window does not avoid an initial phase. For example, Figure 13 plots abbreviated versions of our 3D graphs for go, gcc, compress, and perl, for both regular and 100% branch prediction.
Reference: [4] <author> D. Burger, T. M. Austin, and S. Bennett. </author> <title> Evaluating future microprocessors: the SimpleScalar tool set. </title> <type> Tech. Report TR-1308, </type> <institution> Univ. of Wisconsin-Madison Computer Sciences Dept., </institution> <month> July </month> <year> 1996. </year>
Reference: [5] <author> B. Calder and D. Grunwald. </author> <title> Fast & accurate instruction fetch and branch prediction. </title> <booktitle> In Proc. ISCA-21, </booktitle> <pages> pages 211, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Calder and Grunwald observe that the BTB and the direction-predictor should be decoupled; this both prevents not-taken branches from polluting the BTB and also removes the need for tags in the direction-predictor <ref> [5] </ref>. Such hardware branch prediction mechanisms have been widely incorporated into commercial designs [16], [17], [37]. Because potential for further improvements with such methods is dwindling [8], attention is turning to more sophisticated techniques.
Reference: [6] <author> B. Calder and D. Grunwald. </author> <title> Reducing indirect function call overhead in C++ programs. </title> <booktitle> In Proc. POPL-21, </booktitle> <pages> pages 397408, </pages> <month> Jan. </month> <year> 1994. </year> <note> DRAFT 35 P.-Y. </note> <author> Chang, E. Hao, and Y. N. Patt. </author> <title> Target prediction for indirect jumps. </title> <booktitle> In Proc. ISCA-24, </booktitle> <pages> pages 274-83, </pages> <month> Jun. </month> <year> 1997. </year>
Reference-contexts: Predicting branch targets is important, too. To better understand the performance effect of BTB misses, Michaud, Seznec, and Uhlig measure compulsory BTB misses [36] for all types of branches. Calder and Grunwald <ref> [6] </ref>, Chang, Hao, and Patt [7], and Driesen and Holzle [10] have all examined ways to augment the BTB by taking prior branch-target history into account. None of these papers explicitly treat predicting return-instruction targets, for which return-address stacks DRAFT 33 can virtually eliminate mispredictions.
Reference: [8] <author> I.-C. Chen, J. T. Coffey, and T. N. Mudge. </author> <title> Analysis of branch prediction via data compression. </title> <booktitle> In Proc. ASPLOS-VII, </booktitle> <pages> pages 12837, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Such hardware branch prediction mechanisms have been widely incorporated into commercial designs [16], [17], [37]. Because potential for further improvements with such methods is dwindling <ref> [8] </ref>, attention is turning to more sophisticated techniques. Young, Gloy, and Smith [60], [61] have demonstrated compiler-based methods for correlated branch prediction; Mahlke and Natara-jan [33] and August et al. [2] have examined branch prediction synthesized in the compiler.
Reference: [9] <author> P. J. Denning. </author> <title> The working set model for program behavior. </title> <journal> CACM, 11(5):323333. </journal>
Reference-contexts: Tables VII and VIII present data-cache miss ratios for the baseline and 100% predictors respectively. Recall that caches are 2-way set-associative. Plotting performance as a function of cache size, as in Figures 1 and 2, also permits us to estimate these programs' working sets <ref> [9] </ref>. Some applications have a hierarchy of working sets [46]; Table IX only identifies those that fit in 2 M or less. Knowledge of working set sizes like this can be useful in three ways.
Reference: [10] <author> K. Driesen and U. Holzle. </author> <title> Accurate indirect branch prediction. </title> <booktitle> In Proc. </booktitle> <address> ISCA-25, </address> <month> July </month> <year> 1998. </year>
Reference-contexts: Predicting branch targets is important, too. To better understand the performance effect of BTB misses, Michaud, Seznec, and Uhlig measure compulsory BTB misses [36] for all types of branches. Calder and Grunwald [6], Chang, Hao, and Patt [7], and Driesen and Holzle <ref> [10] </ref> have all examined ways to augment the BTB by taking prior branch-target history into account. None of these papers explicitly treat predicting return-instruction targets, for which return-address stacks DRAFT 33 can virtually eliminate mispredictions.
Reference: [11] <author> J. Emer. </author> <type> Personal communication, </type> <month> June </month> <year> 1997. </year>
Reference-contexts: B. Benchmarks We examine the SPEC integer benchmarks [53], summarized in Tables II and III, and use the provided reference inputs. Like any other suite, SPEC has its shortcomings: for example, many of the programs fit in small I-caches, they have high prediction accuracies compared to many commercial programs <ref> [11] </ref>, and for many programs just a few branches are responsible for most branch mispredictions. All benchmarks are compiled using gcc -O3 -funroll-loops (-O3 includes inlining), although Fortran benchmarks are first translated to C using f2c.
Reference: [12] <author> J. Emer. </author> <type> Personal communication, </type> <month> Mar. </month> <year> 1998. </year>
Reference: [13] <author> J. Emer and N. Gloy. </author> <title> A language for describing predictors and its application to automatic synthesis. </title> <booktitle> In Proc. ISCA-24, </booktitle> <pages> pages 30414, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Gloy and Emer have developed a general language for describing predictors, and they show how it can be used for automated synthesis of predictors. While resulting structures can be complex, this model may yield insight into avenues for further improvement <ref> [13] </ref>. Rotenberg et al. take an even more aggressive tack: they reorganize the processor around traces, groups of basic blocks which have been coalesced into a single unit.
Reference: [14] <author> K. I. Farkas, P. Chow, N. P. Jouppi, and Z. Vranesic. </author> <title> Memory-system design considerations for dynamically-scheduled processors. </title> <booktitle> In Proc. ISCA-24, </booktitle> <pages> pages 13343, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: Prefetching helps tolerate load latencies, but under OOE must take into account that many misses are adequately tolerated without prefetching. Most techniques [38], [41] were developed with simpler processor models in mind, but [47], for example, discusses data prefetching for the HP PA-8000. Farkas et al. <ref> [14] </ref> have recently provided some insights regarding memory system design for dynamically-scheduled processors, and Johnson and Hwu [23] discuss a cache allocation mechanism to prevent rarely accessed data from displacing frequently accessed lines, but these papers do not touch on the relationship of branch prediction to cache design as our paper
Reference: [15] <author> J. A. Fisher and S. M. Freudenberger. </author> <title> Predicting conditional branch directions from previous runs of a program. </title> <booktitle> In Proc. ASPLOS-V, </booktitle> <pages> pages 8595, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: DRAFT 9 Fig. 1. IPC as a function of data cache size and RUU size with the baseline, hybrid branch predictor. A more accurate indicator factors in basic block size, too, to obtain mispredictions per instruction <ref> [15] </ref>. Table IV tabulates these rates. It shows that ijpeg benefits from further increments of RUU despite its low prediction rate because it has one of the lowest misprediction-per-instruction rates. Table IV begs other questions.
Reference: [16] <author> L. Gwennap. </author> <title> Intel's P6 uses decoupled superscalar design. </title> <type> Microprocessor Report, </type> <pages> pages 915, </pages> <month> Feb. 16, </month> <year> 1995. </year>
Reference-contexts: Calder and Grunwald observe that the BTB and the direction-predictor should be decoupled; this both prevents not-taken branches from polluting the BTB and also removes the need for tags in the direction-predictor [5]. Such hardware branch prediction mechanisms have been widely incorporated into commercial designs <ref> [16] </ref>, [17], [37]. Because potential for further improvements with such methods is dwindling [8], attention is turning to more sophisticated techniques.
Reference: [17] <author> L. Gwennap. </author> <title> Digital 21264 sets new standard. </title> <type> Microprocessor Report, </type> <pages> pages 1116, </pages> <month> Oct. 28, </month> <year> 1996. </year>
Reference-contexts: During renaming, instructions use a register mapping table to determine whether their operands reside in the RUU's rename registers or have been committed to architectural state. A separate active list, issue queue, and register file, as in the MIPS R10000 [37] or DEC Alpha 21264 <ref> [17] </ref> can be simulated as well, but using an RUU eliminates artifacts that arise from interactions between active-list size and issue-queue size, and reduces the already large number of variables we examine. Except for the use of an RUU, our baseline configuration resembles the reported configuration of an Alpha 21264 [17]. <p> <ref> [17] </ref> can be simulated as well, but using an RUU eliminates artifacts that arise from interactions between active-list size and issue-queue size, and reduces the already large number of variables we examine. Except for the use of an RUU, our baseline configuration resembles the reported configuration of an Alpha 21264 [17]. Our experiments vary RUU size, first-level data and instruction cache sizes, and branch predictor accuracy. B. Benchmarks We examine the SPEC integer benchmarks [53], summarized in Tables II and III, and use the provided reference inputs. <p> Calder and Grunwald observe that the BTB and the direction-predictor should be decoupled; this both prevents not-taken branches from polluting the BTB and also removes the need for tags in the direction-predictor [5]. Such hardware branch prediction mechanisms have been widely incorporated into commercial designs [16], <ref> [17] </ref>, [37]. Because potential for further improvements with such methods is dwindling [8], attention is turning to more sophisticated techniques.
Reference: [18] <author> D. Hunt. </author> <title> Advanced performance features of the 64-bit PA-8000. </title> <booktitle> In CompCon '95, </booktitle> <pages> pages 12328, </pages> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: SimpleScalar calls the unified active list, issue queue, and physical register file DRAFT 6 an RUU, for register update unit; it is similar to the Metaflow DRIS (deferred-scheduling, register-renaming instruction shelf) [40] and the HP PA-8000 IRB (instruction reorder buffer) <ref> [18] </ref>. During renaming, instructions use a register mapping table to determine whether their operands reside in the RUU's rename registers or have been committed to architectural state.
Reference: [19] <author> V. S. Iyengar and L. H. Trevillyan. </author> <title> Evaluation and generation of reduced traces for benchmarks. </title> <institution> IBM Research Report RC 20610, </institution> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Their technique incorporates first- and second-level cache and TLB behavior as well as branch-prediction behavior, but because it uses traces, important mis-speculation effects are omitted <ref> [19] </ref>. To accurately choose our simulation window, we have measured interval branch-misprediction rates for each of the benchmarks: the misprediction rate computed only over the branch predictions for the chosen interval. The measurements are taken over million-instruction intervals.
Reference: [20] <author> E. Jacobsen, E. Rotenberg, and J. E. Smith. </author> <title> Assigning confidence to conditional branch predictions. </title> <booktitle> In Proc. Micro-29, </booktitle> <pages> pages 14252, </pages> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: A different approach has been taken by Jacobsen, Rotenberg, Smith, et al, who examine confidence ratings attached to branch predictors <ref> [20] </ref>. Gloy and Emer have developed a general language for describing predictors, and they show how it can be used for automated synthesis of predictors. While resulting structures can be complex, this model may yield insight into avenues for further improvement [13].
Reference: [21] <author> Q. Jacobsen, E. Rotenberg, and J. E. Smith. </author> <title> Path-based next trace prediction. </title> <booktitle> In Proc. Micro-30, </booktitle> <pages> pages 1423, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: When the fetch engine hits in the trace cache, it can provide several basic blocks every cycle without the need for merging cache blocks, multiple-branch or multi-ported predictors, or a multi-ported instruction cache <ref> [21] </ref>, [45]. While branch prediction is a well-known performance lever, its relationship to cache design decisions has not previously been quantitatively evaluated. Jouppi and Ranganathan do find in [24] that branch prediction is a stronger limitation on performance than memory latency or bandwidth. Predicting branch targets is important, too. <p> Jourdan et al. [26] and Skadron et al. [48] both focus on return-address-stack design, especially on mechanisms for repairing the return-address-stack after it has been modified by mis-speculated instructions. Jacobsen et al. discuss return-address prediction for trace processors in <ref> [21] </ref>. Finally, latency tolerance for cache design has been a key issue with processor architects for several years now. Many papers study the tradeoffs between L1 cache size and speed; the most recent, which simulates a MIPS R10000 model, is by Wilson and Olukotun [57].
Reference: [22] <author> R. Johnson and M. Schlansker. </author> <title> Analysis techniques for predicated code. </title> <booktitle> In Proc. Micro-29, </booktitle> <pages> pages 10013, </pages> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: The primary bottleneck remains branch prediction. As architects attack the branch-prediction bottleneck with more sophisticated hardware schemes and alternative techniquestrace caches [44], compiler-enhanced hardware prediction [2], [33], predication <ref> [22] </ref>, [43], and multi-path execution [1], [28], [54], [56]larger RUUs will become attractive. L1 data-miss penalties will always inspire innovations, but caches are now so big and sophisticated that future work should perhaps focus on the most latency-intolerant misses. We also consider sampling techniques to allow shorter but full-detail simulations.
Reference: [23] <author> T. L. Johnson and W. W. Hwu. </author> <title> Run-time adaptive cache hierarchy management via reference analysis. </title> <booktitle> In Proc. ISCA-24, </booktitle> <pages> pages 31526, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Most techniques [38], [41] were developed with simpler processor models in mind, but [47], for example, discusses data prefetching for the HP PA-8000. Farkas et al. [14] have recently provided some insights regarding memory system design for dynamically-scheduled processors, and Johnson and Hwu <ref> [23] </ref> discuss a cache allocation mechanism to prevent rarely accessed data from displacing frequently accessed lines, but these papers do not touch on the relationship of branch prediction to cache design as our paper does. VIII.
Reference: [24] <author> N. P. Jouppi and P. Ranganathan. </author> <title> The relative importance of memory latency, bandwidth, and branch limits to performance. In The Workshop on Mixing Logic and DRAM: Chips that Compute and Remember, </title> <month> June </month> <year> 1997. </year> <note> http://ayer.CS.Berkeley.EDU/isca97-workshop. </note>
Reference-contexts: While branch prediction is a well-known performance lever, its relationship to cache design decisions has not previously been quantitatively evaluated. Jouppi and Ranganathan do find in <ref> [24] </ref> that branch prediction is a stronger limitation on performance than memory latency or bandwidth. Predicting branch targets is important, too. To better understand the performance effect of BTB misses, Michaud, Seznec, and Uhlig measure compulsory BTB misses [36] for all types of branches.
Reference: [25] <author> N. P. Jouppi and D. W. Wall. </author> <title> Available instruction-level parallelism for superscalar and superpipelined machines. </title> <booktitle> In Proc. ASPLOS-III, </booktitle> <pages> pages 27282, </pages> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: We give a sampling of relevant work here. Some of the most basic work in the field of superscalar processors has focused on identifying the limits of ILP in different applications. For example, Wall and Jouppi have examined this issue for a variety of realistic and idealized machine configurations <ref> [25] </ref>, [55]. Other work by Smith, Johnson, and Horowitz has also explored the limits on instruction issue afforded by a suite of integer and scalar floating point applications [50]. These works have focused on inherent parallelism in the application.
Reference: [26] <author> S. Jourdan, J. Stark, T.-H. Hsing, and Y. N. Patt. </author> <title> Recovery requirements of branch prediction storage structures in the presence of mispredicted-path execution. </title> <booktitle> Int'l J. Parallel Programming, </booktitle> <address> 25(5):36383, </address> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: Return-address-stack accuracy can be an especially strong lever on performance. If the stack is not repaired, wrong-path instructions can perform invalid pops or pushes that corrupt the stack, as Jourdan et al. have pointed out <ref> [26] </ref>. They propose a sophisticated return-address stack that saves popped entries to avoid overwriting them with future mis-speculated pushes. So long as the stack does not overflow, this structure can return to any prior state. <p> None of these papers explicitly treat predicting return-instruction targets, for which return-address stacks DRAFT 33 can virtually eliminate mispredictions. Jourdan et al. <ref> [26] </ref> and Skadron et al. [48] both focus on return-address-stack design, especially on mechanisms for repairing the return-address-stack after it has been modified by mis-speculated instructions. Jacobsen et al. discuss return-address prediction for trace processors in [21].
Reference: [27] <author> R. E. Kessler, M. D. Hill, and D. A. Wood. </author> <title> A Comparison of Trace-Sampling Techniques for Multi-Megabyte Caches. </title> <type> Technical Report 1048, </type> <institution> Univ. of Wisconsin Computer Sciences Department, </institution> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: Our work simply runs the simulator in a fast, priming mode, but other work has studied analytic models for estimating cache miss rates during the unprimed portion of the sample [59], <ref> [27] </ref>, or described means for bounding errors by adjusting simulation lengths [34]. Iyengar and Trevillyan have derived the R-metric for measuring the representativeness of a trace; they generate DRAFT 25 traces by scaling basic-block transition counts and adjusting selected instructions to minimize the R-metric.
Reference: [28] <author> A. Klauser, V. Paithankar, and D. Grunwald. </author> <title> Selective eager execution on the PolyPath Architecture. </title> <booktitle> In Proc. </booktitle> <address> ISCA-25, </address> <month> July </month> <year> 1998. </year>
Reference-contexts: The primary bottleneck remains branch prediction. As architects attack the branch-prediction bottleneck with more sophisticated hardware schemes and alternative techniquestrace caches [44], compiler-enhanced hardware prediction [2], [33], predication [22], [43], and multi-path execution [1], <ref> [28] </ref>, [54], [56]larger RUUs will become attractive. L1 data-miss penalties will always inspire innovations, but caches are now so big and sophisticated that future work should perhaps focus on the most latency-intolerant misses. We also consider sampling techniques to allow shorter but full-detail simulations.
Reference: [29] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In Proc. ISCA-8, </booktitle> <pages> pages 8187, </pages> <month> June </month> <year> 1981. </year>
Reference: [30] <author> S. Laha, J. H. Patel, and R. K. Iyer. </author> <title> Accurate Low-Cost Methods for Performance Evaluation of Cache Memory Systems. </title> <journal> IEEE Trans. on Computers, </journal> <pages> pages 13251336, </pages> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: This makes it possible to simulate many different configurations or many benchmarks in a short period of time. Sampling schemes are widely used in architecture studies and several pieces of prior work have investigated sampling methodologies, particularly related to cache memory simulations. For example, Laha et al. <ref> [30] </ref> studied the accuracy of memory reference trace sampling using caches that were 128 KB and smaller. Their study concluded that sampling allows accurate estimates of cache miss rates, but their results were presented for fairly unaggressive sampling techniques; they simulated 60% of all memory references.
Reference: [31] <author> M. S. Lam and R. P. Wilson. </author> <title> Limits of control flow on parallelism. </title> <booktitle> In Proc. ISCA-19, </booktitle> <pages> pages 4657, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: These works have focused on inherent parallelism in the application. Lam and Wilson have explored the impact of control flow and showed how relaxing control dependence constraints potentially improves performance DRAFT 32 <ref> [31] </ref>. Woo et al. have characterized the behavior of the SPLASH-2 suite in terms of sensitivity to various parameters and discussed methodological issues for simulating parallel applications [58]. Other fundamental research has focused on understanding and improving branch prediction accuracy via both hardware and software means.
Reference: [32] <author> C.-C. Lee, I.-C. K. Chen, and T. N. Mudge. </author> <title> The bi-mode branch predictor. </title> <booktitle> In Proc. Micro-30, </booktitle> <pages> pages 413, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: Other fundamental research has focused on understanding and improving branch prediction accuracy via both hardware and software means. In this area, work by Yeh and Patt [52], Pan et al. [39], McFarling [35], Sprangle et al. [51], and Lee et al. <ref> [32] </ref> have proposed hardware mechanisms for keeping multi-level branch predictors and for tracking correlations between branches. Calder and Grunwald observe that the BTB and the direction-predictor should be decoupled; this both prevents not-taken branches from polluting the BTB and also removes the need for tags in the direction-predictor [5].
Reference: [33] <author> S. Mahlke and B. Natarajan. </author> <title> Compiler synthesized dynamic branch prediction. </title> <booktitle> In Proc. Micro-29, </booktitle> <pages> pages 153 164, </pages> <month> Dec. </month> <year> 1996. </year> <note> DRAFT 36 </note>
Reference-contexts: Because potential for further improvements with such methods is dwindling [8], attention is turning to more sophisticated techniques. Young, Gloy, and Smith [60], [61] have demonstrated compiler-based methods for correlated branch prediction; Mahlke and Natara-jan <ref> [33] </ref> and August et al. [2] have examined branch prediction synthesized in the compiler. A different approach has been taken by Jacobsen, Rotenberg, Smith, et al, who examine confidence ratings attached to branch predictors [20]. <p> The primary bottleneck remains branch prediction. As architects attack the branch-prediction bottleneck with more sophisticated hardware schemes and alternative techniquestrace caches [44], compiler-enhanced hardware prediction [2], <ref> [33] </ref>, predication [22], [43], and multi-path execution [1], [28], [54], [56]larger RUUs will become attractive. L1 data-miss penalties will always inspire innovations, but caches are now so big and sophisticated that future work should perhaps focus on the most latency-intolerant misses.
Reference: [34] <author> M. Martonosi, A. Gupta, and T. Anderson. </author> <title> Effectiveness of Trace Sampling for Performance Debugging Tools. </title> <booktitle> In Proc. ACM SIGMETRICS Conf. on Meas. and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Our work simply runs the simulator in a fast, priming mode, but other work has studied analytic models for estimating cache miss rates during the unprimed portion of the sample [59], [27], or described means for bounding errors by adjusting simulation lengths <ref> [34] </ref>. Iyengar and Trevillyan have derived the R-metric for measuring the representativeness of a trace; they generate DRAFT 25 traces by scaling basic-block transition counts and adjusting selected instructions to minimize the R-metric.
Reference: [35] <author> S. McFarling. </author> <title> Combining branch predictors. </title> <type> Tech. </type> <note> Note TN-36, DEC WRL, </note> <month> June </month> <year> 1993. </year>
Reference-contexts: DRAFT 5 We have added to SimpleScalar a branch target buffer (BTB) and a decoupled pattern history table (PHT) that uses a McFarling-style hybrid predictor <ref> [35] </ref> and combines two 2-level prediction mechanisms [52] with a selector that chooses between them. In our model, this selector is a table of 2-bit counters, indexed by either history or address. <p> Other fundamental research has focused on understanding and improving branch prediction accuracy via both hardware and software means. In this area, work by Yeh and Patt [52], Pan et al. [39], McFarling <ref> [35] </ref>, Sprangle et al. [51], and Lee et al. [32] have proposed hardware mechanisms for keeping multi-level branch predictors and for tracking correlations between branches.
Reference: [36] <author> P. Michaud, A. Seznec, and R. Uhlig. </author> <title> Trading conflict and capacity aliasing in conditional branch predictors. </title> <booktitle> In Proc. ISCA-24, </booktitle> <pages> pages 292303, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Jouppi and Ranganathan do find in [24] that branch prediction is a stronger limitation on performance than memory latency or bandwidth. Predicting branch targets is important, too. To better understand the performance effect of BTB misses, Michaud, Seznec, and Uhlig measure compulsory BTB misses <ref> [36] </ref> for all types of branches. Calder and Grunwald [6], Chang, Hao, and Patt [7], and Driesen and Holzle [10] have all examined ways to augment the BTB by taking prior branch-target history into account.
Reference: [37] <institution> MIPS Technologies. </institution> <note> MIPS R10000 Microprocessor User's Manual, </note> <month> Jun. </month> <year> 1995. </year> <note> Version 1.0. </note>
Reference-contexts: During renaming, instructions use a register mapping table to determine whether their operands reside in the RUU's rename registers or have been committed to architectural state. A separate active list, issue queue, and register file, as in the MIPS R10000 <ref> [37] </ref> or DEC Alpha 21264 [17] can be simulated as well, but using an RUU eliminates artifacts that arise from interactions between active-list size and issue-queue size, and reduces the already large number of variables we examine. <p> Calder and Grunwald observe that the BTB and the direction-predictor should be decoupled; this both prevents not-taken branches from polluting the BTB and also removes the need for tags in the direction-predictor [5]. Such hardware branch prediction mechanisms have been widely incorporated into commercial designs [16], [17], <ref> [37] </ref>. Because potential for further improvements with such methods is dwindling [8], attention is turning to more sophisticated techniques. Young, Gloy, and Smith [60], [61] have demonstrated compiler-based methods for correlated branch prediction; Mahlke and Natara-jan [33] and August et al. [2] have examined branch prediction synthesized in the compiler.
Reference: [38] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proc. ASPLOS-V, </booktitle> <pages> pages 6273, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Many papers study the tradeoffs between L1 cache size and speed; the most recent, which simulates a MIPS R10000 model, is by Wilson and Olukotun [57]. Prefetching helps tolerate load latencies, but under OOE must take into account that many misses are adequately tolerated without prefetching. Most techniques <ref> [38] </ref>, [41] were developed with simpler processor models in mind, but [47], for example, discusses data prefetching for the HP PA-8000.
Reference: [39] <author> S.-T. Pan, K. So, and J. Rahmeh. </author> <title> Improving the accuracy of dynamic branch prediction using branch correlation. </title> <booktitle> In Proc. ASPLOS-V, </booktitle> <pages> pages 7684, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Other fundamental research has focused on understanding and improving branch prediction accuracy via both hardware and software means. In this area, work by Yeh and Patt [52], Pan et al. <ref> [39] </ref>, McFarling [35], Sprangle et al. [51], and Lee et al. [32] have proposed hardware mechanisms for keeping multi-level branch predictors and for tracking correlations between branches.
Reference: [40] <author> V. Popescu, M. Schultz, J. Spracklen, G. Gibson, B. Lightner, and D. Isaman. </author> <booktitle> The Metaflow architecture. IEEE Micro, </booktitle> <pages> pages 1013, 6373, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: SimpleScalar calls the unified active list, issue queue, and physical register file DRAFT 6 an RUU, for register update unit; it is similar to the Metaflow DRIS (deferred-scheduling, register-renaming instruction shelf) <ref> [40] </ref> and the HP PA-8000 IRB (instruction reorder buffer) [18]. During renaming, instructions use a register mapping table to determine whether their operands reside in the RUU's rename registers or have been committed to architectural state.
Reference: [41] <author> A. K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice Univ., </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Many papers study the tradeoffs between L1 cache size and speed; the most recent, which simulates a MIPS R10000 model, is by Wilson and Olukotun [57]. Prefetching helps tolerate load latencies, but under OOE must take into account that many misses are adequately tolerated without prefetching. Most techniques [38], <ref> [41] </ref> were developed with simpler processor models in mind, but [47], for example, discusses data prefetching for the HP PA-8000.
Reference: [42] <author> C. Price. </author> <title> MIPS IV Instruction Set, Revision 3.1. MIPS Technologies, </title> <publisher> Inc., </publisher> <address> Mountain View, CA, </address> <month> Jan. </month> <year> 1995. </year>
Reference: [43] <author> B. R. Rau, D. W. L. Yen, W. Yen, and R. A. Towle. </author> <title> The Cydra 5 departmental supercomputer: Design philosophies, decisions, and trade-offs. </title> <booktitle> Computer, </booktitle> <pages> pages 1235, </pages> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: The primary bottleneck remains branch prediction. As architects attack the branch-prediction bottleneck with more sophisticated hardware schemes and alternative techniquestrace caches [44], compiler-enhanced hardware prediction [2], [33], predication [22], <ref> [43] </ref>, and multi-path execution [1], [28], [54], [56]larger RUUs will become attractive. L1 data-miss penalties will always inspire innovations, but caches are now so big and sophisticated that future work should perhaps focus on the most latency-intolerant misses. We also consider sampling techniques to allow shorter but full-detail simulations.
Reference: [44] <author> E. Rotenberg, S. Bennett, and J. E. Smith. </author> <title> Trace cache: a low latency approach to high bandwidth instruction fetching. </title> <booktitle> In Proc. Micro-29, </booktitle> <pages> pages 2434, </pages> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: The primary bottleneck remains branch prediction. As architects attack the branch-prediction bottleneck with more sophisticated hardware schemes and alternative techniquestrace caches <ref> [44] </ref>, compiler-enhanced hardware prediction [2], [33], predication [22], [43], and multi-path execution [1], [28], [54], [56]larger RUUs will become attractive. L1 data-miss penalties will always inspire innovations, but caches are now so big and sophisticated that future work should perhaps focus on the most latency-intolerant misses.
Reference: [45] <author> E. Rotenberg, Q. Jacobsen, Y. Sazeides, and J. E. Smith. </author> <title> Trace processors. </title> <booktitle> In Proc. Micro-30, </booktitle> <pages> pages 138148, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: When the fetch engine hits in the trace cache, it can provide several basic blocks every cycle without the need for merging cache blocks, multiple-branch or multi-ported predictors, or a multi-ported instruction cache [21], <ref> [45] </ref>. While branch prediction is a well-known performance lever, its relationship to cache design decisions has not previously been quantitatively evaluated. Jouppi and Ranganathan do find in [24] that branch prediction is a stronger limitation on performance than memory latency or bandwidth. Predicting branch targets is important, too.
Reference: [46] <author> E. Rothberg, J. P. Singh, and A. Gupta. </author> <title> Working sets, cache sizes, and node granularity issues for large-scale multiprocessors. </title> <booktitle> In Proc. ISCA-20, </booktitle> <pages> pages 1425, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Recall that caches are 2-way set-associative. Plotting performance as a function of cache size, as in Figures 1 and 2, also permits us to estimate these programs' working sets [9]. Some applications have a hierarchy of working sets <ref> [46] </ref>; Table IX only identifies those that fit in 2 M or less. Knowledge of working set sizes like this can be useful in three ways.
Reference: [47] <author> V. Santhanam, E. H. Gornish, and W.-C. Hsu. </author> <title> Data prefetching on the HP PA-8000. </title> <booktitle> In Proc. ISCA-24, </booktitle> <pages> pages 26473, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Prefetching helps tolerate load latencies, but under OOE must take into account that many misses are adequately tolerated without prefetching. Most techniques [38], [41] were developed with simpler processor models in mind, but <ref> [47] </ref>, for example, discusses data prefetching for the HP PA-8000.
Reference: [48] <author> K. Skadron, P. S. Ahuja, M. Martonosi, and D. W. Clark. </author> <title> Improving prediction for procedure returns with return-address-stack repair mechanisms. </title> <type> Tech. Report TR-577-98, </type> <institution> Princeton Dept. of Comp. Sci., </institution> <month> Mar. </month> <year> 1998. </year>
Reference-contexts: Instead, we find that simply saving the current top-of-stack pointer (TOSP) at the time of each branch prediction, and restoring it after a misprediction, reduces by 5093% return-address mispredictions from wrong-path corruption. Saving the top-of-stack contents (TOSC) along with the TOSP virtually eliminates return-address mispredictions <ref> [48] </ref>. All the simulations in this paper have assumed TOSP-and-TOSC repair. C. Branch Resolution Delays Even though a branch can move from decode to writeback in 5 cycles in our model, branches must wait for operands and then arbitrate for issue. <p> None of these papers explicitly treat predicting return-instruction targets, for which return-address stacks DRAFT 33 can virtually eliminate mispredictions. Jourdan et al. [26] and Skadron et al. <ref> [48] </ref> both focus on return-address-stack design, especially on mechanisms for repairing the return-address-stack after it has been modified by mis-speculated instructions. Jacobsen et al. discuss return-address prediction for trace processors in [21]. Finally, latency tolerance for cache design has been a key issue with processor architects for several years now.
Reference: [49] <author> K. Skadron and D. W. Clark. </author> <title> Design issues and tradeoffs for write buffers. </title> <booktitle> In Proc. HPCA-3, </booktitle> <pages> pages 144155, </pages> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: The model also assumes perfect write buffering (i.e., stores consume memory-hierarchy bandwidth but never cause stalls), which should have minimal impact on these results <ref> [49] </ref>. A load-store queue (LSQ) disambiguates memory references: stores may only pass preceding memory references whose addresses are known not to conflict.
Reference: [50] <author> M. D. Smith, M. Johnson, and M. Horowitz. </author> <title> Limits on Multiple Instruction Issue. </title> <booktitle> In Proc. ASPLOS-III, </booktitle> <pages> pages 290302, </pages> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: For example, Wall and Jouppi have examined this issue for a variety of realistic and idealized machine configurations [25], [55]. Other work by Smith, Johnson, and Horowitz has also explored the limits on instruction issue afforded by a suite of integer and scalar floating point applications <ref> [50] </ref>. These works have focused on inherent parallelism in the application. Lam and Wilson have explored the impact of control flow and showed how relaxing control dependence constraints potentially improves performance DRAFT 32 [31].
Reference: [51] <author> E. Sprangle, R. S. Chappell, M. Alsup, and Y. N. Patt. </author> <title> The agree predictor: A mechanism for reducing negative branch history interference. </title> <booktitle> In Proc. ISCA-24, </booktitle> <pages> pages 28491, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Other fundamental research has focused on understanding and improving branch prediction accuracy via both hardware and software means. In this area, work by Yeh and Patt [52], Pan et al. [39], McFarling [35], Sprangle et al. <ref> [51] </ref>, and Lee et al. [32] have proposed hardware mechanisms for keeping multi-level branch predictors and for tracking correlations between branches.
Reference: [52] <author> T.-Y. Teh and Y. N. Patt. </author> <title> A comparison of dynamic branch predictors that use two levels of branch history. </title> <booktitle> In Proc. ISCA-20, </booktitle> <pages> pages 25766, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: DRAFT 5 We have added to SimpleScalar a branch target buffer (BTB) and a decoupled pattern history table (PHT) that uses a McFarling-style hybrid predictor [35] and combines two 2-level prediction mechanisms <ref> [52] </ref> with a selector that chooses between them. In our model, this selector is a table of 2-bit counters, indexed by either history or address. <p> Other fundamental research has focused on understanding and improving branch prediction accuracy via both hardware and software means. In this area, work by Yeh and Patt <ref> [52] </ref>, Pan et al. [39], McFarling [35], Sprangle et al. [51], and Lee et al. [32] have proposed hardware mechanisms for keeping multi-level branch predictors and for tracking correlations between branches.
Reference: [53] <institution> The Standard Performance Evaluation Corporation. </institution> <note> WWW Site. http://www.specbench.org, Dec. </note> <year> 1996. </year>
Reference-contexts: Except for the use of an RUU, our baseline configuration resembles the reported configuration of an Alpha 21264 [17]. Our experiments vary RUU size, first-level data and instruction cache sizes, and branch predictor accuracy. B. Benchmarks We examine the SPEC integer benchmarks <ref> [53] </ref>, summarized in Tables II and III, and use the provided reference inputs.
Reference: [54] <author> A. Uht and V. Sindagi. </author> <title> Disjoint eager execution: An optimal form of speculative execution. </title> <booktitle> In Proc. Micro-28, </booktitle> <pages> pages 31325, </pages> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: The primary bottleneck remains branch prediction. As architects attack the branch-prediction bottleneck with more sophisticated hardware schemes and alternative techniquestrace caches [44], compiler-enhanced hardware prediction [2], [33], predication [22], [43], and multi-path execution [1], [28], <ref> [54] </ref>, [56]larger RUUs will become attractive. L1 data-miss penalties will always inspire innovations, but caches are now so big and sophisticated that future work should perhaps focus on the most latency-intolerant misses. We also consider sampling techniques to allow shorter but full-detail simulations.
Reference: [55] <author> D. W. Wall. </author> <title> Limits of Instruction-Level Parallelism. </title> <booktitle> In Proc. ASPLOS-IV, </booktitle> <pages> pages 17688, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Some of the most basic work in the field of superscalar processors has focused on identifying the limits of ILP in different applications. For example, Wall and Jouppi have examined this issue for a variety of realistic and idealized machine configurations [25], <ref> [55] </ref>. Other work by Smith, Johnson, and Horowitz has also explored the limits on instruction issue afforded by a suite of integer and scalar floating point applications [50]. These works have focused on inherent parallelism in the application.
Reference: [56] <author> S. Wallace, B. Calder, and D. M. Tullsen. </author> <title> Threaded multiple path execution. </title> <booktitle> In Proc. </booktitle> <address> ISCA-25, </address> <month> July </month> <year> 1998. </year>
Reference: [57] <author> K. M. Wilson and K. Olukotun. </author> <title> Designing high bandwidth on-chip caches. </title> <booktitle> In Proc. ISCA-24, </booktitle> <pages> pages 12132, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Finally, latency tolerance for cache design has been a key issue with processor architects for several years now. Many papers study the tradeoffs between L1 cache size and speed; the most recent, which simulates a MIPS R10000 model, is by Wilson and Olukotun <ref> [57] </ref>. Prefetching helps tolerate load latencies, but under OOE must take into account that many misses are adequately tolerated without prefetching. Most techniques [38], [41] were developed with simpler processor models in mind, but [47], for example, discusses data prefetching for the HP PA-8000.
Reference: [58] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proc. ISCA-22, </booktitle> <pages> pages 2436, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: It ensures that when choosing a single data cache size for some other type of study (e.g. a branch prediction study), the chosen operating point will not have an artificially high miss rate. Second, as discussed in <ref> [58] </ref>, it helps establish a suitable range of data-cache sizes for cache-related studies. Knowledge of working set sizes is also necessary when scaling cache or problem sizes. C. Instruction Cache and Data Cache Interactions We have examined interactions between data-cache size and RUU size, and instruction-cache size and RUU size. <p> Lam and Wilson have explored the impact of control flow and showed how relaxing control dependence constraints potentially improves performance DRAFT 32 [31]. Woo et al. have characterized the behavior of the SPLASH-2 suite in terms of sensitivity to various parameters and discussed methodological issues for simulating parallel applications <ref> [58] </ref>. Other fundamental research has focused on understanding and improving branch prediction accuracy via both hardware and software means.
Reference: [59] <author> D. A. Wood, M. D. Hill, and R. E. Kessler. </author> <title> A Model for Estimating Trace-Sample Miss Ratios. </title> <booktitle> In Proc. ACM SIGMETRICS Conf. on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 7989, </pages> <month> June </month> <year> 1991. </year> <note> DRAFT 37 </note>
Reference-contexts: Our work simply runs the simulator in a fast, priming mode, but other work has studied analytic models for estimating cache miss rates during the unprimed portion of the sample <ref> [59] </ref>, [27], or described means for bounding errors by adjusting simulation lengths [34]. Iyengar and Trevillyan have derived the R-metric for measuring the representativeness of a trace; they generate DRAFT 25 traces by scaling basic-block transition counts and adjusting selected instructions to minimize the R-metric.
Reference: [60] <author> C. Young, N. Gloy, and M. D. Smith. </author> <title> A comparative analysis of schemes for correlated branch prediction. </title> <booktitle> In Proc. ISCA-22, </booktitle> <pages> pages 27686, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Such hardware branch prediction mechanisms have been widely incorporated into commercial designs [16], [17], [37]. Because potential for further improvements with such methods is dwindling [8], attention is turning to more sophisticated techniques. Young, Gloy, and Smith <ref> [60] </ref>, [61] have demonstrated compiler-based methods for correlated branch prediction; Mahlke and Natara-jan [33] and August et al. [2] have examined branch prediction synthesized in the compiler. A different approach has been taken by Jacobsen, Rotenberg, Smith, et al, who examine confidence ratings attached to branch predictors [20].
Reference: [61] <author> C. Young and M. D. Smith. </author> <title> Improving the accuracy of static branch prediction using branch correlation. </title> <booktitle> In Proc. ASPLOS-VI, </booktitle> <pages> pages 23241, </pages> <month> Oct. </month> <year> 1994. </year> <note> DRAFT </note>
Reference-contexts: Such hardware branch prediction mechanisms have been widely incorporated into commercial designs [16], [17], [37]. Because potential for further improvements with such methods is dwindling [8], attention is turning to more sophisticated techniques. Young, Gloy, and Smith [60], <ref> [61] </ref> have demonstrated compiler-based methods for correlated branch prediction; Mahlke and Natara-jan [33] and August et al. [2] have examined branch prediction synthesized in the compiler. A different approach has been taken by Jacobsen, Rotenberg, Smith, et al, who examine confidence ratings attached to branch predictors [20].
References-found: 60

