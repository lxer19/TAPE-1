URL: http://glen.lcs.mit.edu/~devadas/pubs/compress.ps
Refering-URL: http://glen.lcs.mit.edu/~devadas/pubs/compiler.html
Root-URL: 
Title: Code Density Optimization for Embedded DSP Processors Using Data Compression Techniques  
Author: Stan Y. Liao Srinivas Devadas Kurt Keutzer 
Keyword: Figure 1. Heterogeneous System-on-a-Chip  
Address: Cambridge, Mass. Cambridge, Mass. Mountain View, Calif.  
Affiliation: MIT Dept. of EECS MIT Dept. of EECS Synopsys, Inc.  
Date: 1995  
Note: Revised Version for Paper in ARVLSI  1: Introduction  
Abstract: We address the problem of code size minimization in VLSI systems with embedded DSP processors. Reducing code size reduces the production cost of embedded systems. We use data compression methods to develop code size minimization strategies. We present a framework for code size minimization where the compressed data consists of a dictionary and a skeleton. The dictionary can be computed using popular text compression algorithms. We describe two methods to execute the compressed code that have varying performance characteristics and varying degrees of freedom in compressing the code. Experimental results obtained with a TMS320C25 code generator are presented. An increasingly common micro-architecture for embedded systems is to integrate a microprocessor or microcontroller, a ROM and an ASIC all on a single integrated circuit (Figure 1). Such a micro-architecture can currently be found in such diverse embedded systems as FAX modems, laser printers and cellular telephones. To justify such a level of integration these embedded system designs must be sold in very large volumes and, as a result, they are also very cost sensitive. The cost of the IC is most closely linked to the size of the IC and that is derived from the final circuit area. Surprisingly, it is not unusual for the bulk of the area of such ICs to be devoted to the ROM storing the program code for the microprocessor. In 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1986. </year>
Reference-contexts: However, programming in a HLL can incur a penalty in code size. One reason for this is that compiler optimization techniques (for examples see <ref> [1] </ref>) have classically focussed on code speed and not code density. Also, most available compilers optimize primarily for speed of execution. Although some optimizing transforms such as common subexpression elimination can improve both speed and size at the same time, in many cases there is a speed-size tradeoff. <p> the dictionary entries are processed to produce the dictionary, and each symbolic pointer is replaced by the appropriate instruction or instructions (e.g., CALL for Method I and CALD for Method II). 5.1: Generation of Dictionary Entries The instruction stream is first divided into basic blocks, using the algorithm described in <ref> [1, p. 529] </ref>. Then each block is compared with every other block, as well as itself, for common substrings. A threshold on the minimum length T (for example, 3) of substrings is prescribed, so that only potentially beneficial substrings are extracted.
Reference: [2] <author> G. J. Chaitin. </author> <title> A Theory of Program Size Formally Identical to Information Theory. </title> <journal> J. ACM, </journal> <volume> 22(3) </volume> <pages> 329-340, </pages> <month> July </month> <year> 1975. </year>
Reference-contexts: We are currently developing efficient algorithms to identify potentially useful extended blocks. At a higher level of compilation, the code generator can help out the code compression process by generating assembly code that has lower entropy. Chaitin <ref> [2] </ref> proved that the minimum size of a program is formally identical to the entropy of the sequence it generates. Thus, regardless of what compression model is used, programs of lower entropy can be made smaller.
Reference: [3] <author> J. G. Ganssle. </author> <title> The Art of Programming Embedded Systems. </title> <address> San Diego, CA: </address> <publisher> Academic Press, Inc., </publisher> <year> 1992. </year>
Reference-contexts: In many embedded system projects, the ROM space estimated at the beginning becomes insufficient later in the development phase or during maintenance. Designers usually have to work diligently to reduce the code size in order to avoid excessive design modification <ref> [3, p. 18] </ref>. Consequently, diminishing code size may result in a significant productivity gain as well. As the complexity of embedded systems grows, programming in assembly language and optimization by hand are no longer deemed practical or economical, except for time-critical portions of the program that absolutely require it.
Reference: [4] <institution> Texas Instruments. </institution> <note> TMS320C2x User's Guide. January 1993. Revision C. </note>
Reference-contexts: NOPs can be inserted so that the extended blocks will have this property. Although this may have an adverse effect on the dictionary size, it can potentially result in greater compression. The exact trade-offs are dependent on the application itself. We use the TMS320C25 architecture <ref> [4] </ref> to exemplify the method. The hardware modifications are shown in Figure 3 (for simplicity, much of the data path is not shown). An S-R flip-flop, a counter, an AND gate, two OR gates, and a link register have been added to the base processor.
Reference: [5] <author> A. Mayne and E. B. James. </author> <title> Information compression by factoring common strings. </title> <journal> Computer Journal, </journal> <volume> 18(2) </volume> <pages> 157-160, </pages> <year> 1975. </year>
Reference-contexts: A heuristic algorithm for generating a dictionary was presented in Mayne and James <ref> [5] </ref>.
Reference: [6] <author> J. A. Storer and T. G. Szymanski. </author> <title> Data Compression via Textual Substitution. </title> <journal> J. ACM, </journal> <volume> 29(4) </volume> <pages> 928-951, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: The concept of data compression, however, inspired us to use a scheme that can achieve better results than mere conventional optimization. This scheme is based on a compression method which Storer and Szymanski <ref> [6] </ref> called an external pointer macro (EPM) model. In this model, compressed data consists of a dictionary and a skeleton. The dictionary contains substrings that occur frequently in the original data. The skeleton contains symbols from the alphabet of the original data, interspersed with pointers to the dictionary. <p> with a small degree of modification to take into account the control structures in programs, and our framework will benefit from any progress in such text compression techniques. 3: Preliminaries 3.1: Data Compression We will briefly review the basic terminology of the macro model of data compression, as defined in <ref> [6] </ref>. The source data is treated as a finite string over some alphabet. In the external pointer macro (EPM) model, the compressed form of the source data consists of a dictionary and a skeleton. The dictionary is a string. <p> A heuristic algorithm for generating a dictionary was presented in Mayne and James [5]. Storer and Szymanski <ref> [6] </ref> showed that the problem of deciding whether the length of the shortest possible compressed form is less than k is NP-complete. 3.2: Definitions, Conventions, and Assumptions We will model our system as a machine with a programmable processor, a program ROM, and some application-specific integrated circuit (ASIC).
Reference: [7] <author> R. A. Wagner. </author> <title> Common Phrases and Minimum-Space Text Storage. </title> <journal> Comm. of ACM, </journal> <volume> 16(3) </volume> <pages> 148-152, </pages> <month> March </month> <year> 1973. </year>
Reference-contexts: The decoding process is straightforward: we simply scan through the skeleton, replacing each pointer by its reference to the dictionary with the indicated length. Wagner <ref> [7] </ref> gave an algorithm based on dynamic programming for optimally parsing the source into a skeleton given a collection of phrases (similar to a dictionary, though less flexible), but did not show how the phrases were best generated. <p> ) = i=1 j=1 = 2 1 (l 2 2 + + l 2 n 2 5.2: Substitution In the current implementation, a fast, greedy algorithm is used to replace occurrences of dictionary entries in the instruction stream by appropriate pointers to the dictionary. (The dynamic programming method suggested in <ref> [7] </ref> may yield improved results.) After dictionary entries are generated, we sort them according to their length. We then proceed to replace occurrences of each entry in the instruction stream by a symbolic pointer, beginning with the longest entry.
Reference: [8] <author> A. Wolfe and A. Chanin. </author> <title> Executing Compressed Programs on an Embedded RISC Archi tecture. </title> <booktitle> In Proceedings of Micro-25, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: If we were to execute the program from its compressed form, we would need some mechanism of random access, which is impractical, if not impossible, for these compression techniques. Recently, a novel RISC architecture which compresses instructions and then decompresses them in the cache was proposed in <ref> [8] </ref>. This approach, however, requires a significant redesign of the microprocessor architecture which is a stumbling block to its broad utilization. The concept of data compression, however, inspired us to use a scheme that can achieve better results than mere conventional optimization.
Reference: [9] <author> J. Ziv and A. Lempel. </author> <title> A Universal Algorithm for Sequential Data Compression. </title> <journal> IEEE Trans. on Info. Theory, </journal> <volume> IT-23(3):337-343, </volume> <month> May </month> <year> 1977. </year>
Reference-contexts: Experimental results obtained with a TMS320C25 code generator are presented in Section 6. We conclude in Section 7 with directions for future research. 2: Approaches to Code Compression High-performance data compression techniques, such as <ref> [9] </ref>, have been applied to reduce the size of executable programs. However, programs to be compressed are treated as sequential data, and before compressed programs can be executed, they are first decompressed and loaded into RAM. Thus, savings are achieved only on secondary storage (i.e., disks).
References-found: 9

