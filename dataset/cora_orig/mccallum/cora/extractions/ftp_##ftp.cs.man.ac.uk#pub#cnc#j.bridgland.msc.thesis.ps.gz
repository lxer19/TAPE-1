URL: ftp://ftp.cs.man.ac.uk/pub/cnc/j.bridgland.msc.thesis.ps.gz
Refering-URL: http://www.cs.man.ac.uk/cnc/arena/publication.html
Root-URL: http://www.cs.man.ac.uk
Title: PERSISTENT STORE IN A DYNAMIC RESOURCE MANAGEMENT ENVIRONMENT  By  
Author: James Bridgland 
Degree: A thesis submitted to the University of Manchester for the degree of Master of Science in the Faculty of Science and Engineering  
Affiliation: Department of Computer Science  
Date: October 1994  
Abstract-found: 0
Intro-found: 1
Reference: <author> Assenmacher, H., Breibach, T., Buhler, P., Hubsch, V., Peine, H., & Schwarz, R. </author> <year> 1994. </year> <title> Meeting the Application in User Space. </title> <booktitle> Pages 82-87 of: Proceedings of the Sixth ACM SIGOPS European Workshop. </booktitle>
Reference-contexts: In the second user-level approach, resource management is accomplished via libraries. Psyche is an example of a `nanokernel' which uses user-level libraries to implement resource management policies (Scott & LeBlanc, 1988). PANDA is a more recent system which supports user-level resource management via the provision of run-time packages <ref> (Assenmacher et al., 1994) </ref>. The PANDA kernel only provides protection and prevents monopolisation of resources. All other services are provided at user-level via the run-time packages, which may be tailored to an application's requirements.
Reference: <author> Baker, M., Asami, S., E.Deprit, Ousterhout, J., & Seltzer, M. </author> <year> 1992. </year> <title> Non-Volatile Memory for Fast, Reliable File Systems. </title> <booktitle> Pages 10-22 of: Fifth International Conference on Architectural Support for Programming Languages and Operating Systems. </booktitle>
Reference: <author> Baker, M.G., Hartman, J.H., Kupfer, M.D., Shirriff, K.W., & Ousterhout, J.K. </author> <year> 1991. </year> <title> Measurements of a Distributed File System. </title> <booktitle> Pages 198-212 of: Proceedings of the Thirteenth ACM Symposium on Operating System. </booktitle>
Reference-contexts: Consistency is thus guaranteed. Concurrent write sharing has an obvious impact on efficiency because client caching is disabled. However, tests performed on Sprite showed that only about 1% of client-server traffic was due to concurrent write sharing <ref> (Baker et al., 1991) </ref>. Caching at User-Level Cao et al. (1994) described a system which allowed the cache to be controlled at user-level. Their reason for placing the cache at user-level was based on two observations. Firstly, data in the cache can be accessed without the cost of a trap. <p> The design of Sprite showed how virtual memory pages should be treated with preference over cache pages <ref> (Baker et al., 1991) </ref>. In Sprite, a page owned by the virtual memory system can only be transferred to a file cache page if it has not been accessed for at least 20 minutes.
Reference: <author> Bernstein, P.A., Hadzilacos, V., & Goodman, N. </author> <year> 1987. </year> <title> Concurrency Control and Recovery In Database Systems. </title> <publisher> Addison Wesley. </publisher>
Reference-contexts: PERSISTENT STORE DESIGN 31 finer control over flushing. An example is a DBMS writing an intention list 12 to the disk <ref> (Bernstein et al., 1987) </ref>. Here the block containing the commit flag must be written after all the other blocks in the list. Stonebraker (1981) stated, "The service required from an OS buffer manager is selected force out".
Reference: <author> Bershad, B.N., Chambers, C., Eggers, S., Maeda, C., McNamee, D., Pardyak, P., Savage, S., & Sirer, E.G. </author> <year> 1994. </year> <title> SPIN An Extensible Microkernel for Application-specific Operating System Services. </title> <booktitle> Pages 68-71 of: Proceedings of the Sixth ACM SIGOPS European Workshop. </booktitle>
Reference-contexts: A custom system, supporting specific hardware or a particular application, is constructed using actual implementations of these classes. The SPIN microkernel is an example of a kernel-level dynamically customis-able system. SPIN allows `code sequences' to be added to the kernel at run-time <ref> (Bershad et al., 1994) </ref>. This allows the provision of different interfaces and alternative implementations of existing interfaces. System integrity is not compromised by the addition of code sequences because they are created by a `trusted' compiler. An application can therefore alter system services to achieve maximum performance.
Reference: <author> Black, D.L., Golub, D.B., Julin, D.P., Rashid, F.R., Draves, R.P., Dean, D.W., Forin, A., Barrera, J., Tokuda, H., Malan, G., & Bohman, D. </author> <year> 1992. </year> <title> Microker-nel Operating System Architecture and Mach. </title> <booktitle> Pages 11-30 of: Proceedings 131 REFERENCES 132 of the USENIX Workshop on Micro-Kernel and Other Kernel Architectures. </booktitle>
Reference-contexts: In a microkernel system, much of the policy is moved out of the kernel and into CHAPTER 1. INTRODUCTION 17 user-level servers. Examples of microkernel systems include Amoeba (Tanenbaum et al., 1990), Mach <ref> (Black et al., 1992) </ref> and Chorus (Rozier et al., 1988). Placing policy in user-level servers allows resource management policies to be altered by changing servers. In this sense, microkernel operating systems can be thought of as customisable. In general, customisation is achieved at either the kernel or user level. <p> CHAPTER 2. PERSISTENT STORE DESIGN 43 2.4.9 Single Level Store An alternative to providing a kernel-based file system is to map both primary and secondary storage into virtual memory, thus creating a single level store. This approach was used in the design of Mach <ref> (Black et al., 1992) </ref>. Golub et al. (1990) showed how a conventional file system could be implemented upon such a system. An emulation library maps the file into the processes address space on an open system call. Read, write and lseek system calls are then emulated by the library. <p> The operating system controls which pages are resident in main memory and which are `paged' to disk. Many operating systems support virtual memory: UNIX (Gingell et al., 1987; Moran, 1988); Mach <ref> (Black et al., 1992) </ref>; Sprite (Nelson et al., 1988). The virtual memory system must operate efficiently.
Reference: <author> Bryce, C., Issarny, V., Muller, G., & Pauut, I. </author> <year> 1994. </year> <title> Towards Safe and Efficient Customization in Distributed Systems (Position Paper). </title> <booktitle> Pages 57-61 of: Proceedings of the Sixth ACM SIGOPS European Workshop. </booktitle>
Reference: <author> Cao, P., Felten, E.W., & Li, K. </author> <year> 1993 </year> <month> (January). </month> <title> User Level File Caching Policies. </title> <type> Tech. </type> <institution> rept. CS-TR-445-94. Princeton University. </institution>
Reference: <author> Carey, M.J., Jauhari, R., & Livny, M. </author> <year> 1989. </year> <title> Piority in DBMS Resource Scheduling. </title> <booktitle> Pages 397-410 of: Proceedings of the Fiftennth Internltional Conference on Very Large Data Bases. </booktitle>
Reference: <author> Cheriton, D.R., & Duda, K.J. </author> <year> 1994. </year> <title> A Caching Model of Operating System Kernel Functionality. </title> <booktitle> Pages 88-91 of: Proceedings of the Sixth ACM SIGOPS European Workshop. </booktitle>
Reference: <author> Chou, H., & Kim, W. </author> <year> 1986. </year> <title> A Unifying Framework for Version Control in a CAD Environment. </title> <booktitle> In: Proceeding of the Twelfth International Conference on Very Large Databases. </booktitle>
Reference: <author> Dean, D., & Zippel, R. </author> <year> 1994. </year> <title> Matching Data Storage to Application Needs. </title> <booktitle> Pages 29-34 of: Proceedings of the Sixth ACM SIGOPS European Workshop. del Rosario, J.M., & Choudary, A.N. </booktitle> <year> 1994. </year> <title> High-Performance I/O for Massively Parallel Computers: Problems and Prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3), </volume> <pages> 59-68. </pages>
Reference: <author> Douglis, F., & Ousterhout, J. </author> <year> 1991. </year> <title> Transparent Process Migration: Design Alternatives and the Sprite Implementation. </title> <journal> Software Practice and Experience, </journal> <volume> 21(8), </volume> <pages> 757-785. </pages> <note> REFERENCES 133 Engler, </note> <author> D., Kaashoek, M.F., & O'Toole, J. </author> <year> 1994. </year> <title> The Operating System Kernel as a Secure Programmable Machine. </title> <booktitle> Pages 62-67 of: Proceedings of the Sixth ACM SIGOPS European Workshop. </booktitle>
Reference-contexts: The exception to this is when the file system is used to implement process migration. For example, in Sprite a process address space is `paged' to an ordinary file on a shared file system <ref> (Douglis & Ousterhout, 1991) </ref>. In UNIX, a separate disk partition is used by the virtual memory system for paged data. CHAPTER 3. DESIGN 52 3.2.3 Support for Disk Caching As described in section 2.4.1, caching plays an important role in improving I/O performance.
Reference: <editor> Gingell, R.A., Moran, J.P., & Shannon, </editor> <address> W.A. </address> <year> 1987. </year> <title> Virtual Momory Architecture in SunOS. </title> <booktitle> Pages 81-94 of: USENIX Summer Conference Proceedings. </booktitle>
Reference: <author> Golub, D., Dean, R., Forin, A., & Rashid, R. </author> <year> 1990. </year> <title> UNIX as an Application Program. </title> <booktitle> Pages 87-95 of: USENIX Summer Conference Proceedings. </booktitle>
Reference: <author> Hartman, J.H., & Ousterhout, J.K. </author> <year> 1992. </year> <title> Zebra: A Striped Network File System. </title> <booktitle> In: USENIX File Systems Workshop Proceedings. </booktitle>
Reference-contexts: As the cost of small disks continues to decrease, it is becoming viable for a system to have hundreds or even thousands of disks. A number of disks are then combined into an array. By "striping" <ref> (Hartman & Ousterhout, 1992) </ref> data across the array, the bandwidth is increased because sections of the data are read or written concurrently. If independent data items are stored on individual disks, the throughput can be increased because separate requests on different disk drives can be serviced concurrently.
Reference: <author> Hennessy, J.L., & Patterson, D.A. </author> <year> 1990. </year> <title> Computer Architecture : A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers Inc. </publisher>
Reference: <author> Howard, J.H., Kazar, M.L., Menees, S.G., Nichols, D.A., Satyanarayanan, M., Sidebotham, R.N., & West, M.J. </author> <year> 1988. </year> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1), </volume> <pages> 51-81. </pages>
Reference: <author> Istavrinos, P., & Borrman, L. </author> <year> 1990. </year> <note> LNCS 457. Springer-Verlag. </note> <author> Chap. </author> <title> A Process and Memory Model for a Parallel Distributed Machine. </title>
Reference: <author> J.Bacon, Moody, K., Thomson, S., & Wilson, T. </author> <year> 1991. </year> <title> A Multi-Service Storage Architecture. </title> <journal> ACM SIGOPS, </journal> <volume> 25(4), </volume> <pages> 47-65. </pages>
Reference: <author> Klahold, P., Schlageter, G., & Wilkes, W. </author> <year> 1986. </year> <title> A General Model for Version Management in Databases. </title> <booktitle> In: Proceeding of the Twelfth International Conference on Very Large Databases. </booktitle>
Reference: <author> Kotz, D. </author> <year> 1992. </year> <title> Multiprocessor File System Interfaces. Pages 149-150 of: USENIX File System Workshop Proceedings. </title> <note> REFERENCES 134 Krieger, </note> <author> O., Stumm, M., & Unrau, R. </author> <year> 1994. </year> <title> The Alloc Stream Facility A Redesign of Application Level Stream I/O. </title> <journal> IEEE Computer, </journal> <volume> 27(3). </volume>
Reference-contexts: By the time teraflop machines with terabyte primary memories become available, the storage requirements may reach petabytes 2 . Scientific applications on uniprocessor computers typically access large files sequentially <ref> (Kotz, 1992) </ref>. 2.1.2 Transaction System Store Typical transaction systems include automatic bank teller and airline reservation systems. The access in these systems usually comprises a large number of concurrent requests operating on small data items (Ousterhout & Douglis, 1989).
Reference: <editor> Le*er, S.J., KcKusick, M.K., Karels, M.J., & Quarterman, J.S. </editor> <year> 1989. </year> <title> The Design and Implementation of the 4.3 BSD UNIX Operating System. </title> <publisher> Addison Wesley. </publisher>
Reference: <author> Levy, E., & Silberschatz, A. </author> <year> 1990. </year> <title> Distributed File Systems: Concepts and Example. </title> <journal> ACM Computing Surveys, </journal> <volume> 22(4), </volume> <pages> 321-374. </pages>
Reference-contexts: CHAPTER 2. PERSISTENT STORE DESIGN 39 than a more realistic LRU policy. In a client-server model, NVRAM can also be used as a write buffer at the server. In a system such as NFS <ref> (Levy & Silberschatz, 1990) </ref> this is particularly beneficial due to the synchronous nature of writes. A client only has to wait for the data to be copied into the server's buffer rather than waiting for the completion of the I/O.
Reference: <author> Li, K., & Hudak, P. </author> <year> 1986. </year> <title> Memory Coherency in Shared Virtual Memory Systems. </title> <booktitle> Pages 229-239 of: Proceedings of the fifth ACM Symposium on the Principles of Distributed Computing. </booktitle>
Reference: <author> LoVerso, S.J., Hesheim, W., Isman, M., Milne, E.D., Nanopoulos, A., & Wheeler, R. </author> <year> 1993. </year> <title> sfs: A Parallel File System for the CM-5. </title> <booktitle> Pages 291-305 of: USENIX Conference Proceedings, </booktitle> <address> Cincinnati. </address>
Reference-contexts: A number of different RAID organisations exist, each giving different performance characteristics. The amount of usable storage varies among organisations. An example of a system using a RAID is the CM-5 <ref> (LoVerso et al., 1993) </ref>. CHAPTER 2. PERSISTENT STORE DESIGN 43 2.4.9 Single Level Store An alternative to providing a kernel-based file system is to map both primary and secondary storage into virtual memory, thus creating a single level store.
Reference: <author> Mayes, K.R. </author> <year> 1993. </year> <title> Trends in Operating Systems Towards Dynamic User-Level Policy Provision. </title> <type> Tech. </type> <institution> rept. UMCS-93-9-1. University of Manchester. </institution>
Reference: <author> Mayes, K.R., & Bridgland, J. </author> <year> 1994. </year> <title> Parallel Information Systems. UNICOM. In press. Chap. Customisable Resource Management for Parallel Database Systems. </title>
Reference: <author> Mayes, K.R., Quick, S., Bridgland, J., & Nisbet, A. </author> <year> 1994. </year> <title> Language- and Application- Oriented Resource Management for Parallel Architectures. </title> <booktitle> In: Proceedings of the 6th ACM SIGOPS European Workshop. </booktitle> <publisher> In press. </publisher>
Reference: <editor> McKusick, M.K., Joy, W.N., Le*er, S.J., & Fabry, R.S. </editor> <year> 1984. </year> <title> A Fast File System for UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3), </volume> <pages> 181-197. </pages> <address> Microplis. </address> <month> Super-Capacity: </month> <title> Drive Specifications. </title> <type> REFERENCES 135 Moran, </type> <institution> J.P. </institution> <year> 1988. </year> <title> SunOS Virtual Memory Implementation. </title> <booktitle> In: Proceedings of European UNIX User Group, </booktitle> <address> London. </address>
Reference-contexts: Sprite Log-Structured File System Sprite is a modern operating system which uses a LFS to implement its file system. The Sprite LFS (Rosenblum & Ousterhout, 1991) was designed to match or exceed the read performance of the UNIX Fast File System (FFS) <ref> (McKusick et al., 1984) </ref> whilst offering the increased write performance of a LFS. To enable free space to be managed effectively the disk is divided into fixed size extents CHAPTER 2. PERSISTENT STORE DESIGN 41 called segments. A segment is always written sequentially in its entirety.
Reference: <author> Morris, D., & Ibbett, R.N. </author> <year> 1979. </year> <title> The MU5 Computer Systems. </title> <publisher> Macmillan Computer Science Series. </publisher>
Reference-contexts: Kernel level customisation can be either static or dynamic. Some systems, for example the Kernel Tool Kit (Mukherjee & Schwan, 1993), are customisable both statically and dynamically. The operating system of the MU5 is an early example of a statically customis-able operating system <ref> (Morris & Ibbett, 1979) </ref>. The system was decomposed into a number of modules, for example store, process and device modules. The operating system was then built by selecting one module for each of the system components.
Reference: <author> Mosberger, D. </author> <year> 1993. </year> <title> Memory Consisteny Models. </title> <journal> ACM SIGOPS, </journal> <volume> 27(1), </volume> <pages> 18-26. </pages>
Reference: <author> Mukherjee, B., & Schwan, K. </author> <year> 1993. </year> <title> Experimentation with a Reconfigurable Micro-Kernel. </title> <booktitle> In: Proceedings of the USENIX Symposium on Microkernels and Other Kernel Architectures. </booktitle>
Reference-contexts: In this sense, microkernel operating systems can be thought of as customisable. In general, customisation is achieved at either the kernel or user level. Kernel level customisation can be either static or dynamic. Some systems, for example the Kernel Tool Kit <ref> (Mukherjee & Schwan, 1993) </ref>, are customisable both statically and dynamically. The operating system of the MU5 is an early example of a statically customis-able operating system (Morris & Ibbett, 1979). The system was decomposed into a number of modules, for example store, process and device modules.
Reference: <author> Mullender, S.J. </author> <year> 1985. </year> <title> A Distributed File Service Based on Optimistic Concur-rency Control. </title> <booktitle> Pages 51-62 of: Proceedings of the Tenth ACM Symposium on Operating System Principles, </booktitle> <volume> vol. </volume> <pages> 19. </pages>
Reference-contexts: Each read access is guaranteed to receive the value of the last modification. On the other hand, weak coherence ensures that the view will be consistent but only after a certain period of time or after an explicit synchronisation call. Amoeba ensures consistency through the use of immutable files <ref> (Mullender, 1985) </ref>. Sprite implements a more complex mechanism (Nelson et al., 1988) where a file may be cached at any number of clients providing it is only opened for reading at each client.
Reference: <author> Nelson, M.N., Welch, B.B., & Ousterhout, J.K. </author> <year> 1988. </year> <title> Caching in the Sprite Network File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1), </volume> <pages> 134-154. </pages>
Reference-contexts: This gain in performance is at the cost of fault tolerance if the system crashes, all the dirty data will be lost. Many operating systems such as UNIX (Le*er et al., 1989) and Sprite <ref> (Nelson et al., 1988) </ref> attempt a compromise between performance and fault tolerance. A flush-back policy allows a dirty block to reside in the cache for a certain period of time before it is flushed back to the disk. In UNIX the interval is 30 seconds. <p> On the other hand, weak coherence ensures that the view will be consistent but only after a certain period of time or after an explicit synchronisation call. Amoeba ensures consistency through the use of immutable files (Mullender, 1985). Sprite implements a more complex mechanism <ref> (Nelson et al., 1988) </ref> where a file may be cached at any number of clients providing it is only opened for reading at each client. <p> The operating system controls which pages are resident in main memory and which are `paged' to disk. Many operating systems support virtual memory: UNIX (Gingell et al., 1987; Moran, 1988); Mach (Black et al., 1992); Sprite <ref> (Nelson et al., 1988) </ref>. The virtual memory system must operate efficiently. It does not need to use the file system in the same way as an application does it does not need to use a text name for data and does not require data to be cached.
Reference: <author> Ng, R., Faloutsos, C., & Sellis, T. </author> <year> 1991. </year> <title> Flexible Buffer Allocation Based on Marginal Gains. </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <volume> 20(2), </volume> <pages> 387-396. </pages>
Reference: <author> Ousterhout, J.K, & Douglis, F. </author> <year> 1989. </year> <title> Beating the I/O Bottleneck: A Case for Log-Structured File Systems. </title> <journal> ACM SIGOPS, </journal> <volume> 23(1), </volume> <pages> 11-28. </pages>
Reference-contexts: Scientific applications on uniprocessor computers typically access large files sequentially (Kotz, 1992). 2.1.2 Transaction System Store Typical transaction systems include automatic bank teller and airline reservation systems. The access in these systems usually comprises a large number of concurrent requests operating on small data items <ref> (Ousterhout & Douglis, 1989) </ref>. The operations are typically of short duration and require a fast response time (Yoo & Rogers, 1993). The data involved in each operation is usually distinct from that in other, concurrent, operations. <p> However, a write-through policy does not allow the possibility of new data being deleted or over-written before an I/O is issued and therefore cannot reduce the number of write operations. "The BSD study" 9 <ref> (Ousterhout & Douglis, 1989) </ref> showed that approximately 30% of block accesses were writes. If the cache implements a write-through policy, at least this number of I/Os will be performed. 9 The trace was gathered on an office/engineering type system CHAPTER 2. <p> PERSISTENT STORE DESIGN 30 The BSD study further showed that 20-30% of newly-written data was deleted within 30 seconds and approximately 50% was deleted within 5 minutes. A later study <ref> (Ousterhout & Douglis, 1989) </ref> 10 found over 40% of bytes lived 11 for less than 30 seconds and 90% lived for less than a day.
Reference: <author> Ousterhout, J.K., Da Costa, H, Kunze, J.A., Kupfer, M., & Thompson, J.G. </author> <year> 1985. </year> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System. </title> <booktitle> Pages 15-24 of: Proceedings of the Tenth ACM Symposium on Operating System Principles. </booktitle> <address> REFERENCES 136 Patterson, </address> <note> D.A., </note> <author> Gibson, G., & Katz, R.H. </author> <year> 1988. </year> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID). </title> <booktitle> In: ACM SIGMOD International Conference on Management of Data. </booktitle>
Reference: <author> Patterson, R.H., Gibson, G.A., & Satyanarayanan, M. </author> <year> 1993. </year> <title> A Status Report on Research in Transparent Informed Prefetching. </title> <journal> ACM SIGOPS, </journal> <volume> 17(2), </volume> <pages> 21-34. </pages>
Reference: <author> Rosenblum, M., & Ousterhout, J.K. </author> <year> 1991. </year> <title> The Design and Implementation of a Log-Structured File System. </title> <booktitle> Pages 1-15 of: Proceedings of the Thirteenth ACM Symposium on Operating Systems. </booktitle>
Reference-contexts: A client only has to wait for the data to be copied into the server's buffer rather than waiting for the completion of the I/O. Baker et al. (1992) showed the use of NVRAM in conjunction with the Sprite log structured file system <ref> (Rosenblum & Ousterhout, 1991) </ref>. Traces showed that fsync () system call caused 10-25% of data segments to be written to the persistent store before they were full. In the worst case observed, this figure rose to 92%. Using a NVRAM buffer would eliminate the need to write these partial segments. <p> There must be some mechanism for grouping the free space into large contiguous areas for the LFS to operate efficiently. Sprite Log-Structured File System Sprite is a modern operating system which uses a LFS to implement its file system. The Sprite LFS <ref> (Rosenblum & Ousterhout, 1991) </ref> was designed to match or exceed the read performance of the UNIX Fast File System (FFS) (McKusick et al., 1984) whilst offering the increased write performance of a LFS.
Reference: <author> Rozier, M., Abrossimov, V., Armand, F., Boule, I., Gien, M., Guillemont, M., Hermann, F., Kaiser, C., Langlois, S., Leonard, P., & Neuhausser, W. </author> <year> 1988. </year> <title> CHORUS Distributed Operating Systems. </title> <type> Tech. </type> <institution> rept. CS/TR-88-76. Chorus Systems. </institution>
Reference-contexts: In a microkernel system, much of the policy is moved out of the kernel and into CHAPTER 1. INTRODUCTION 17 user-level servers. Examples of microkernel systems include Amoeba (Tanenbaum et al., 1990), Mach (Black et al., 1992) and Chorus <ref> (Rozier et al., 1988) </ref>. Placing policy in user-level servers allows resource management policies to be altered by changing servers. In this sense, microkernel operating systems can be thought of as customisable. In general, customisation is achieved at either the kernel or user level.
Reference: <author> Ruemmler, C., & Wilkes, J. </author> <year> 1993a. </year> <title> Modeling Disks. </title> <type> Tech. </type> <institution> rept. Hewlett-Packard Laboratories. </institution>
Reference: <author> Ruemmler, C., & Wilkes, J. </author> <year> 1993b. </year> <title> UNIX Disk Access Patterns. </title> <booktitle> Pages 405-420 of: USENIX Winter Conference Proceedings, </booktitle> <address> San Diego. </address>
Reference-contexts: While processor speeds continue to increase rapidly (Patterson et al., 1988), disk access times continue to improve much more slowly at about 7% each year <ref> (Ruemmler & Wilkes, 1993b) </ref>. Without finding other mechanisms to close the increasing performance gap, the overall gain in performance will be limited by the disks. This can be seen from Amdahl's law.
Reference: <author> Russo, V., Johnston, G., & Campbell, R. </author> <year> 1988. </year> <title> Process Management and Exception Handling in Multiprocessor Operating Systems Using Object-Oriented Design Techniques. </title> <booktitle> Pages 248-258 of: OOPSLA '88 Proceedings. </booktitle>
Reference-contexts: The system was decomposed into a number of modules, for example store, process and device modules. The operating system was then built by selecting one module for each of the system components. Choices, constructed using C++, is another example of a customis-able operating system <ref> (Russo et al., 1988) </ref>. Abstract classes are used to represent system components. A custom system, supporting specific hardware or a particular application, is constructed using actual implementations of these classes. The SPIN microkernel is an example of a kernel-level dynamically customis-able system.
Reference: <author> Sacco, G.M., & Schkolnick, M. </author> <year> 1982. </year> <title> A Mechanism for Managing the Buffer Pool in a Relational Database System Using the Hot Set Model. </title> <booktitle> Pages 257-262 of: Proceedings of the Eighth International Conference on Very Large Data Bases. </booktitle> <address> REFERENCES 137 Scott, </address> <note> M.L., </note> & <author> LeBlanc, T.J. </author> <year> 1988. </year> <title> Design Rational for Psyche, A General Purpose Multiprocessor Operating System. </title> <type> Tech. </type> <institution> rept. University of Rochester. </institution>
Reference: <author> Seltzer, M., Chen, P., & Ousterhout, J. </author> <year> 1990. </year> <title> Disk Scheduling Revisited. </title> <booktitle> Pages 313-324 of: USENIX Washington D.C. Conference Proceedings. </booktitle>
Reference: <author> Staelin, C., & Garcia-Molina, H. </author> <year> 1991. </year> <title> Smart Filesystems. </title> <booktitle> Pages 45-51 of: USENIX Dallas Conference Proceedings. </booktitle>
Reference-contexts: The read has been "hidden" beneath the computation and the overall time for the computation has been reduced. The UNIX operating system prefetches blocks of a file when it detects sequential access is occurring (Stonebraker et al., 1983). In iPcress, small files are prefetched into the cache on open <ref> (Staelin & Garcia-Molina, 1991) </ref>. Stone-braker (1981) states that the INGRES database management system can nearly always predict which block it will access next. This is not always the next logical block and therefore UNIX would not be able to prefetch this block.
Reference: <author> Stonebraker, M. </author> <year> 1981. </year> <title> Operating System Support for Database Management. </title> <journal> Communications of the ACM, </journal> <volume> 24(7), </volume> <pages> 412-418. </pages>
Reference-contexts: PERSISTENT STORE DESIGN 29 However, LRU does not perform well for database management systems (DBMSs) <ref> (Stonebraker, 1981) </ref>.
Reference: <author> Stonebraker, M., Woodfill, J., Ranstorm, J., Murphy, M., Meyer, M., & Allman, E. </author> <year> 1983. </year> <title> Performance Enhancements to a Relational Database System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(2), </volume> <pages> 167-185. </pages>
Reference-contexts: The read has been "hidden" beneath the computation and the overall time for the computation has been reduced. The UNIX operating system prefetches blocks of a file when it detects sequential access is occurring <ref> (Stonebraker et al., 1983) </ref>. In iPcress, small files are prefetched into the cache on open (Staelin & Garcia-Molina, 1991). Stone-braker (1981) states that the INGRES database management system can nearly always predict which block it will access next.
Reference: <author> Stroustrup, B. </author> <year> 1986. </year> <title> The C++ Programming Language. </title> <publisher> Addison Wesley. </publisher>
Reference-contexts: The design of the disk object is presented in section 3.5.1 and the system structure is described in section 3.5.2. 3.5.1 The Disk Object In this model, data are stored as objects on the disk. In a similar way to objects in languages such as C++ <ref> (Stroustrup, 1986) </ref>, a disk object comprises some state and a set of access functions or methods. Each disk object is of one type, or class, 4 The creation of new PSMs in this model is to implement new policies.
Reference: <author> Tanenbaum, A.S., van Renesse, R., van Staveren, H., Sharp, G.J., Mullender, S.J, Jansen, J., & van Rossum, G. </author> <year> 1990. </year> <title> Experiences with the Amoeba Distributed Operation System. </title> <journal> Communications of the ACM, </journal> <volume> 33(12). </volume>
Reference-contexts: The size of the kernel can also be large because each abstraction must be supported behind the trap interface. In a microkernel system, much of the policy is moved out of the kernel and into CHAPTER 1. INTRODUCTION 17 user-level servers. Examples of microkernel systems include Amoeba <ref> (Tanenbaum et al., 1990) </ref>, Mach (Black et al., 1992) and Chorus (Rozier et al., 1988). Placing policy in user-level servers allows resource management policies to be altered by changing servers. In this sense, microkernel operating systems can be thought of as customisable.
Reference: <author> Vongsathorn, P., & Carson, S.D. </author> <year> 1990. </year> <title> A System for Adaptive Disk Rearrangement. </title> <journal> Software Practice and Experience, </journal> <volume> 20(3), </volume> <pages> 225-242. </pages>
Reference: <author> Ward, M., & Townsend, P. </author> <year> 1990. </year> <note> CONPAR 90 - VAPP IV:LNCS 457. Springer-Verlag. </note> <editor> Chap. </editor> <booktitle> EDS Hardware Architecture. </booktitle>
Reference-contexts: CHAPTER 1. INTRODUCTION 19 3. To facilitate the subsequent porting to other hardware platforms. 4. To allow dynamic switching of policy at run-time. 5. To facilitate re-use of existing resource management code via object-oriented techniques. The initial target for Arena is a European Declarative System (EDS) hardware <ref> (Ward & Townsend, 1990) </ref>. The EDS machine is a hybrid multiprocessor/multicomputer. It comprises a number of nodes interconnected via a total connectivity network. Each node comprises two Sparc processors and a shared memory. Two potential problems may arise in the design of Arena.
Reference: <author> Welch, B. </author> <year> 1991. </year> <title> The File System Belongs in the Kernel. </title> <booktitle> Pages 233-249 of: Proceedings of the USENIX Mach Symposium. </booktitle>
Reference: <author> Wilkes, J. </author> <year> 1992. </year> <title> DataMesh research project, phase 1. Pages 63-69 of: USENIX File System Workshop Proceedings. </title> <note> REFERENCES 138 Wulf, </note> <author> W., Cohen, E., Corwin, W., Jones, A., Levin, R., Pierson, C., & Pol-lack, F. </author> <year> 1974. </year> <title> HYDRA: The Kernel of a Multiprocessor Operating System. </title> <journal> Communications of the ACM, </journal> <volume> 17(6), </volume> <pages> 337-345. </pages>
Reference-contexts: Communication can occur between local or distributed threads on a distributed machine. * Persistent Store Manager (PSM). The PSM implements all the policy relating to the persistent store such as caching and prefetching. In a similar way, policy was implemented in managers in DataMesh <ref> (Wilkes, 1992) </ref>. Each manager was optimised for a particular set of requirements. * Process Manager (PM). The PM determines the factors which control schedul ing of threads of control. The process structure of an application is deter mined entirely at user-level. * Store Manager (SM).
Reference: <author> Yoo, H., & Rogers, T. </author> <year> 1993. </year> <title> UNIX Kernel Support for OLTP Performance. </title> <booktitle> Pages 241-247 of: USENIX Winter Conference Proceedings, </booktitle> <address> San Diego. </address>
Reference-contexts: However, other operating system policies may still be inappropriate for the DBMS. CHAPTER 1. INTRODUCTION 16 An alternative method is to modify parts of a general purpose operating system to give the desired functionality. For example, the UNIX kernel has been modified to support on-line transaction processing <ref> (Yoo & Rogers, 1993) </ref>. The advantage of this technique is that only selected parts of the operating system need to be modified. The disadvantage is that the operating system must be modified on every hardware platform supporting the DBMS. Operating systems may also be specifically designed to support a DBMS. <p> The access in these systems usually comprises a large number of concurrent requests operating on small data items (Ousterhout & Douglis, 1989). The operations are typically of short duration and require a fast response time <ref> (Yoo & Rogers, 1993) </ref>. The data involved in each operation is usually distinct from that in other, concurrent, operations. For example, in a bank teller system it would be unusual for two operations on the same account to occur simultaneously because an account is likely to have just one owner.
Reference: <author> Young, M., Tevanian, A., Rashid, R., Golub, D., Eppinger, J., Chew, J., Bolosky, W., Black, D., & Baron, R. </author> <year> 1987. </year> <title> The Duality of Memory and Communication in the Implementation of Mulitprocessor Operating Systems. </title> <booktitle> Pages 63-76 of: Proceedings of the Eleventh ACM Syposium of Operating System Principles, </booktitle> <volume> vol. </volume> <pages> 21. </pages>
References-found: 57

