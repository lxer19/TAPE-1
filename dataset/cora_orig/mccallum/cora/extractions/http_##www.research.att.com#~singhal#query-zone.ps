URL: http://www.research.att.com/~singhal/query-zone.ps
Refering-URL: http://www.csi.uottawa.ca/~debruijn/irbib.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: singhal@research.att.com  mitra@cs.cornell.edu  chrisb@sabir.com  
Title: Learning Routing Queries in a Query Zone  
Author: Amit Singhal yfl Mandar Mitra Chris Buckley yy 
Address: University;  
Affiliation: Department of Computer Science, Cornell  
Note: AT&T Labs Research;  yy Sabir Research, Inc.;  
Abstract: Word usage is domain dependent. A common word in one domain can be quite infrequent in another. In this study we exploit this property of word usage to improve document routing. We show that routing queries (profiles) learned only from the documents in a query domain are better than the routing profiles learned when query domains are not used. We approximate a query domain by a query zone. Experiments show that routing profiles learned from a query zone are 8-12% more effective than the profiles generated when no query zoning is used. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Allan. </author> <type> Personal Communication. </type>
Reference-contexts: only considers the top R non-relevant articles to learn a routing query, where the query has R relevant articles in the training set. [3] This is motivated by the need to have a balance between the number of the positive and the negative examples in Rocchio's learning of feedback queries. <ref> [1] </ref> We experimented with a similar strategy which selects the top k fi R non-relevant (where k is an integer, we used 1, 2, 4, 6, 8, 10, 12, 14, and 16) articles for a query that has R relevant articles in the training set.
Reference: [2] <author> J. Allan. </author> <title> Incremental relevance feedback for information filtering. </title> <booktitle> In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 270-278. </pages> <institution> Association for Computing Machinery, </institution> <address> New York, </address> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: This learned profile, also known as the routing or the feedback query since it is obtained using user's relevance feedback, is then matched against all the new articles that a system encounters (for example any new news stories, : : : ). <ref> [5, 28, 2, 9] </ref> If a new article matches the user profile adequately, then this article is assumed to be of potential interest to the user and is routed to the user.
Reference: [3] <author> J. Allan, L. Ballesteros, J. Callan, W. Croft, and Z. Lu. </author> <title> Recent experiments with INQUERY. </title> <booktitle> In Proceedings of the Fourth Text REtrieval Conference (TREC-4), </booktitle> <pages> pages 49-64. </pages> <note> NIST Special Publication 500-236, Octo-ber 1996. </note>
Reference-contexts: The matching algorithm used by most systems to match a new article to a user profile is relatively straight-forward. Most current IR systems use their standard text matching algorithms to match new articles to a profile. <ref> [8, 20, 3, 16] </ref> For example, the Smart system uses the standard vector fl This study was done when the primary author was a doctoral candidate at Cornell, and was supported in part by the National Science Foundation under grant IRI-9300124. inner-product similarity computation to match user profiles (which are term <p> Allan et al. use a technique which only considers the top R non-relevant articles to learn a routing query, where the query has R relevant articles in the training set. <ref> [3] </ref> This is motivated by the need to have a balance between the number of the positive and the negative examples in Rocchio's learning of feedback queries. [1] We experimented with a similar strategy which selects the top k fi R non-relevant (where k is an integer, we used 1, 2,
Reference: [4] <author> N. Belkin and W. Croft. </author> <title> Information filtering and information retrieval: </title> <journal> Two sides of the same coin? Communications of the ACM, </journal> <volume> 35(12) </volume> <pages> 29-38, </pages> <year> 1992. </year>
Reference-contexts: This problem has also been called as selective dissemination of information or information filtering. <ref> [4] </ref> Most current state of the art routing algorithms first learn a user profile from the training examples, i.e., the articles marked relevant by the user and the non-relevant articles.
Reference: [5] <author> C. Buckley and G. Salton. </author> <title> Optimization of relevance feedback weights. </title> <booktitle> In Proceedings of the Eighteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 351-357. </pages> <institution> Association for Computing Machinery, </institution> <address> New York, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: This learned profile, also known as the routing or the feedback query since it is obtained using user's relevance feedback, is then matched against all the new articles that a system encounters (for example any new news stories, : : : ). <ref> [5, 28, 2, 9] </ref> If a new article matches the user profile adequately, then this article is assumed to be of potential interest to the user and is routed to the user. <p> Most current routing algorithms also assign weights to the features in a user profile. These weights indicate the relative importance of the features in predicting relevance of an article. <ref> [6, 5, 21] </ref> To learn the features and their weights, most routing algorithms usually use the probability of occurrence (or some variation of it) of a feature in the articles marked relevant by a user and the non-relevant articles in the training corpus. [22, 14] The central idea of this scheme <p> If some relevant document does not pass the similarity threshold, it is included in the query zone. Parameters involved: ff, fi, fl, and S. * QZ-3: Dynamic query zoning. This strategy is motivated by Buckley and Salton's dynamic feedback optimization (DFO) technique. <ref> [5] </ref> DFO aims at improving feedback weights obtained from a feedback technique (like Rocchio) by changing individual term weights and studying the effect of the change, retrospectively, on the training set of documents. <p> Motivated by dynamic feedback optimization of Buckley and Salton, <ref> [5] </ref> we try to "learn" a good query zone size on a per query basis from the training data. To do this, we generate various profiles for a user query by using query zones of different sizes.
Reference: [6] <author> C. Buckley, G. Salton, and J. Allan. </author> <title> The effect of adding relevance information in a relevance feedback environment. </title> <booktitle> In Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 292-300. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Most current routing algorithms also assign weights to the features in a user profile. These weights indicate the relative importance of the features in predicting relevance of an article. <ref> [6, 5, 21] </ref> To learn the features and their weights, most routing algorithms usually use the probability of occurrence (or some variation of it) of a feature in the articles marked relevant by a user and the non-relevant articles in the training corpus. [22, 14] The central idea of this scheme
Reference: [7] <author> C. Buckley, A. Singhal, and M. Mitra. </author> <title> Using query zoning and correlation within SMART : TREC 5. </title> <booktitle> In Proceedings of the Fifth Text REtrieval Conference (TREC-5). NIST Special Publication, </booktitle> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: Rank the training data according to the original query, and assume that the top K (say 5,000) documents form the query zone. If some relevant document is ranked below the top K documents, include it in the query zone. This strategy was used in our TREC-5 participation. <ref> [7] </ref> Parameters involved: ff, fi, fl, and K. * QZ-2: All documents with similarity to the orig inal query greater than some threshold (S). Queries can have narrow or broad domains. A query from a very narrow domain should have a smaller query zone than a broad query.
Reference: [8] <author> C. Buckley, A. Singhal, M. Mitra, and G. Salton. </author> <title> New retrieval approaches using SMART : TREC 4. </title> <booktitle> In Proceedings of the Fourth Text REtrieval Conference (TREC-4), </booktitle> <pages> pages 25-48. </pages> <note> NIST Special Publication 500-236, </note> <month> October </month> <year> 1996. </year>
Reference-contexts: The matching algorithm used by most systems to match a new article to a user profile is relatively straight-forward. Most current IR systems use their standard text matching algorithms to match new articles to a profile. <ref> [8, 20, 3, 16] </ref> For example, the Smart system uses the standard vector fl This study was done when the primary author was a doctoral candidate at Cornell, and was supported in part by the National Science Foundation under grant IRI-9300124. inner-product similarity computation to match user profiles (which are term
Reference: [9] <author> J. Callan. </author> <title> Information filtering with inference networks. </title> <booktitle> In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 262-269. </pages> <institution> Association for Computing Machinery, </institution> <address> New York, </address> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: This learned profile, also known as the routing or the feedback query since it is obtained using user's relevance feedback, is then matched against all the new articles that a system encounters (for example any new news stories, : : : ). <ref> [5, 28, 2, 9] </ref> If a new article matches the user profile adequately, then this article is assumed to be of potential interest to the user and is routed to the user.
Reference: [10] <author> D. T. Davis and J.-N. Hwang. </author> <title> Attentional focus training by boundary region data selection. </title> <booktitle> In International Joint Conference on Neural Networks, pages I-676 to I-681, </booktitle> <address> Baltimore, MD, </address> <month> June 7-11 </month> <year> 1992. </year>
Reference-contexts: Most Sampling techniques in machine learning aim at reducing the size of the training set [19] and are not motivated by improving the classification accuracy (except for a few like <ref> [10] </ref>). Our zoning techniques, on the other hand, are specifically aimed at improving the routing effectiveness and are not motivated by the size of the training corpus.
Reference: [11] <author> D. K. </author> <title> Harman. </title> <booktitle> Overview of the third Text REtrieval Conference (TREC-3). In Proceedings of the Third Text REtrieval Conference (TREC-3), </booktitle> <pages> pages 1-19. </pages> <note> NIST Special Publication 500-225, </note> <month> April </month> <year> 1995. </year>
Reference-contexts: Parameters involved: ff, fi, fl, and a list of rank cut offs. 4 This situation arises in TREC because pooling is used for relevance judgments. <ref> [11, 12] </ref> We assume that all the unjudged documents are non-relevant. 5 Related Work In [15] Hull, and in [28] Schutze, Hull, and Pedersen want to reduce the dimensionality of the feature space dramatically for use with strong learning methods. <p> our experiments | the TREC-3 routing task, queries: TREC topics 101-150, training corpus: disks 1 and 2, test corpus: disk 3; and the TREC-4 routing task, queries: fifty TREC topics, training corpus: disks 1, 2, and 3, test corpus: 804 Mbytes of text from Ziff, FR 1994, and the Internet. <ref> [11, 12] </ref> We select a reasonable set of Rocchio parameters (ff, fi, fl) for No-QZ. To avoid multi-dimensional tuning of parameters, we select a good set of Rocchio parameters for K = 5; 000 for QZ-1 and use the same ff, fi, fl across all query zoning strategies.
Reference: [12] <author> D. K. </author> <title> Harman. </title> <booktitle> Overview of the fourth Text REtrieval Conference (TREC-4). In Proceedings of the Fourth Text REtrieval Conference (TREC-4), </booktitle> <pages> pages 1-24. </pages> <note> NIST Special Publication 500-236, </note> <month> October </month> <year> 1996. </year>
Reference-contexts: 1 Background Document routing is an important problem in the field of information retrieval. <ref> [12] </ref> When a user has marked several articles as relevant to his/her information need, a system should be able to automatically learn the user's "profile" and should be able to route (send) new, potentially interesting, articles to the user. <p> Parameters involved: ff, fi, fl, and a list of rank cut offs. 4 This situation arises in TREC because pooling is used for relevance judgments. <ref> [11, 12] </ref> We assume that all the unjudged documents are non-relevant. 5 Related Work In [15] Hull, and in [28] Schutze, Hull, and Pedersen want to reduce the dimensionality of the feature space dramatically for use with strong learning methods. <p> our experiments | the TREC-3 routing task, queries: TREC topics 101-150, training corpus: disks 1 and 2, test corpus: disk 3; and the TREC-4 routing task, queries: fifty TREC topics, training corpus: disks 1, 2, and 3, test corpus: 804 Mbytes of text from Ziff, FR 1994, and the Internet. <ref> [11, 12] </ref> We select a reasonable set of Rocchio parameters (ff, fi, fl) for No-QZ. To avoid multi-dimensional tuning of parameters, we select a good set of Rocchio parameters for K = 5; 000 for QZ-1 and use the same ff, fi, fl across all query zoning strategies. <p> The test set does have 5 This is one of the main reasons why the general performance in the TREC-4 and the TREC-5 routing tasks is much lower than the performance for the TREC-3 routing task. <ref> [12, 13] </ref> K (rank cut-off) 1,000 2,000 4,000 6,000 8,000 10,000 TREC-3 Average 0.4368 0.4415 0.4326 0.4279 0.4235 0.4213 Precision TREC-3 vs. No-QZ +10.2% +11.4% + 9.2% + 8.0% + 6.9% + 6.3% TREC-4 Average 0.3223 0.3535 0.3735 0.3777 0.3776 0.3773 Precision TREC-4 vs.
Reference: [13] <author> D. K. </author> <title> Harman. </title> <booktitle> Overview of the fifth Text REtrieval Conference (TREC-5). In Proceedings of the Fifth Text REtrieval Conference (TREC-5), 1997 (to appear). </booktitle>
Reference-contexts: The test set does have 5 This is one of the main reasons why the general performance in the TREC-4 and the TREC-5 routing tasks is much lower than the performance for the TREC-3 routing task. <ref> [12, 13] </ref> K (rank cut-off) 1,000 2,000 4,000 6,000 8,000 10,000 TREC-3 Average 0.4368 0.4415 0.4326 0.4279 0.4235 0.4213 Precision TREC-3 vs. No-QZ +10.2% +11.4% + 9.2% + 8.0% + 6.9% + 6.3% TREC-4 Average 0.3223 0.3535 0.3735 0.3777 0.3776 0.3773 Precision TREC-4 vs.
Reference: [14] <author> D. Harper. </author> <title> Relevance Feedback in Document Retrieval Systems. </title> <type> PhD thesis, </type> <institution> University of Cambridge, Eng-land, </institution> <year> 1980. </year>
Reference-contexts: features in predicting relevance of an article. [6, 5, 21] To learn the features and their weights, most routing algorithms usually use the probability of occurrence (or some variation of it) of a feature in the articles marked relevant by a user and the non-relevant articles in the training corpus. <ref> [22, 14] </ref> The central idea of this scheme is that if a feature occurs with a high probability in the relevant articles but with a low probability in the non-relevant articles, then it is a good indicator of relevance and should be assigned a high weight in the profile.
Reference: [15] <author> D. Hull. </author> <title> Improving text retrieval for the routing problem using latent semantic indexing. </title> <booktitle> In Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 282-291. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Parameters involved: ff, fi, fl, and a list of rank cut offs. 4 This situation arises in TREC because pooling is used for relevance judgments. [11, 12] We assume that all the unjudged documents are non-relevant. 5 Related Work In <ref> [15] </ref> Hull, and in [28] Schutze, Hull, and Pedersen want to reduce the dimensionality of the feature space dramatically for use with strong learning methods. <p> Since SVD is extremely computationally intensive, they reduce the number of documents they use in their document space, and instead work in a "local region". In <ref> [15] </ref> Hull uses only the relevant documents for a query to learn the LSI factors. In [28] Schutze, Hull, and Pedersen use the top 2,000 documents retrieved by a query generated using Rocchio's feedback to learn the LSI factors needed in their classifiers.
Reference: [16] <author> K. Kwok and L. Grunfeld. TREC-4 ad-hoc, </author> <title> routing retrieval and filtering experiments using PIRCS. </title> <booktitle> In Proceedings of the Fourth Text REtrieval Conference (TREC-4), </booktitle> <pages> pages 145-152. </pages> <note> NIST Special Publication 500-236, </note> <month> October </month> <year> 1996. </year>
Reference-contexts: The matching algorithm used by most systems to match a new article to a user profile is relatively straight-forward. Most current IR systems use their standard text matching algorithms to match new articles to a profile. <ref> [8, 20, 3, 16] </ref> For example, the Smart system uses the standard vector fl This study was done when the primary author was a doctoral candidate at Cornell, and was supported in part by the National Science Foundation under grant IRI-9300124. inner-product similarity computation to match user profiles (which are term
Reference: [17] <author> K. Kwok and L. Grunfeld. </author> <title> TREC-5 english and chinese retrieval experiments using PIRCS. </title> <booktitle> In Proceedings of the Fifth Text REtrieval Conference (TREC-5), 1997 (to appear). </booktitle>
Reference-contexts: Recently Kwok and Grunfeld have used a sampling technique based on genetic algorithms that selects the best training subset of the relevant articles to be used in creation of a feedback query. <ref> [17] </ref> Since we have a small number of relevant articles (positive examples) for a typical routing query, we did not consider ignoring any of those in our training phase. Also Kwok and Grunfeld did not obtain very encouraging results by ignoring some of the relevant articles. [17] It might be possible <p> of a feedback query. <ref> [17] </ref> Since we have a small number of relevant articles (positive examples) for a typical routing query, we did not consider ignoring any of those in our training phase. Also Kwok and Grunfeld did not obtain very encouraging results by ignoring some of the relevant articles. [17] It might be possible to combine sampling techniques for the relevant articles (like Kwok's) and our techniques for sampling the non-relevant articles to obtain an even richer set of training data. ff = 8 fi = 16 0.3870 0.3872 0.3794 | fi = 64 0.3860 0.3914 0.3967 0.3962 Table 2:
Reference: [18] <author> D. Lewis and W. Gale. </author> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 3-12. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Our techniques can also be thought of as sampling techniques that selectively use non-relevant documents for training. Sampling techniques are well studied in the machine learning community. Our techniques come closest to uncertainty sampling of Lewis and Gale <ref> [18] </ref>, which is motivated by the query by committee technique of Seung, Opper, and Sompolinsky. [29] In uncertainty sampling, only the training examples whose class membership is uncertain (given the current classifier) are proposed to a user for membership judgment.
Reference: [19] <author> M. Plutowski and H. White. </author> <title> Selecting concise training sets from clean data. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(2) </volume> <pages> 305-318, </pages> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: Most Sampling techniques in machine learning aim at reducing the size of the training set <ref> [19] </ref> and are not motivated by improving the classification accuracy (except for a few like [10]). Our zoning techniques, on the other hand, are specifically aimed at improving the routing effectiveness and are not motivated by the size of the training corpus.
Reference: [20] <author> S. Robertson, S. Walker, M. Hancock-Beaulieu, M. Gat-ford, and A. Payne. </author> <booktitle> Okapi at TREC-4. In Proceedings of the Fourth Text REtrieval Conference (TREC-4), </booktitle> <pages> pages 73-96. </pages> <note> NIST Special Publication 500-236, Oc-tober 1996. </note>
Reference-contexts: The matching algorithm used by most systems to match a new article to a user profile is relatively straight-forward. Most current IR systems use their standard text matching algorithms to match new articles to a profile. <ref> [8, 20, 3, 16] </ref> For example, the Smart system uses the standard vector fl This study was done when the primary author was a doctoral candidate at Cornell, and was supported in part by the National Science Foundation under grant IRI-9300124. inner-product similarity computation to match user profiles (which are term
Reference: [21] <author> S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. </author> <booktitle> Okapi at TREC-3. In Proceedings of the Third Text REtrieval Conference (TREC-3), </booktitle> <pages> pages 109-126. </pages> <note> NIST Special Publication 500-225, </note> <month> April </month> <year> 1995. </year>
Reference-contexts: Most current routing algorithms also assign weights to the features in a user profile. These weights indicate the relative importance of the features in predicting relevance of an article. <ref> [6, 5, 21] </ref> To learn the features and their weights, most routing algorithms usually use the probability of occurrence (or some variation of it) of a feature in the articles marked relevant by a user and the non-relevant articles in the training corpus. [22, 14] The central idea of this scheme
Reference: [22] <author> J. Rocchio. </author> <title> Document Retrieval Systems-Optimization and Evaluation. </title> <type> PhD thesis, </type> <institution> Harvard Computational Laboratory, </institution> <address> Cambridge, MA, </address> <year> 1966. </year>
Reference-contexts: features in predicting relevance of an article. [6, 5, 21] To learn the features and their weights, most routing algorithms usually use the probability of occurrence (or some variation of it) of a feature in the articles marked relevant by a user and the non-relevant articles in the training corpus. <ref> [22, 14] </ref> The central idea of this scheme is that if a feature occurs with a high probability in the relevant articles but with a low probability in the non-relevant articles, then it is a good indicator of relevance and should be assigned a high weight in the profile. <p> of entire non-relevant corpus. indicate the "moving away" of the query from the non-relevant documents and towards the relevant articles. 2 Rocchio's Algorithm A feedback query creation algorithm developed by Joe Roc-chio in the mid-1960's has, over the years, proven to be one of the most successful profile learning algorithms. <ref> [22, 23] </ref> Roc-chio's algorithm was developed in the framework of the vector space model. [27] The algorithm is based upon the fact that if the relevance for a query is known, an optimal 1 query vector will maximize the average query-document similarity for the relevant articles, and will simultaneously minimize the
Reference: [23] <author> J. Rocchio. </author> <title> Relevance feedback in information retrieval. </title> <booktitle> In The SMART Retrieval System|Experiments in Automatic Document Processing, </booktitle> <pages> pages 313-323, </pages> <address> Engle-wood Cliffs, NJ, 1971. </address> <publisher> Prentice Hall, Inc. </publisher>
Reference-contexts: of entire non-relevant corpus. indicate the "moving away" of the query from the non-relevant documents and towards the relevant articles. 2 Rocchio's Algorithm A feedback query creation algorithm developed by Joe Roc-chio in the mid-1960's has, over the years, proven to be one of the most successful profile learning algorithms. <ref> [22, 23] </ref> Roc-chio's algorithm was developed in the framework of the vector space model. [27] The algorithm is based upon the fact that if the relevance for a query is known, an optimal 1 query vector will maximize the average query-document similarity for the relevant articles, and will simultaneously minimize the <p> Also, coefficients have been introduced in Rocchio's formulation which control the contribution of the original query, the relevant articles, and the non-relevant articles to the feedback 1 See <ref> [23] </ref> (page 315) for Rocchio's definition of an optimal query. query. <p> in the training set, will also do a good job of differentiating relevance from non-relevance in the new set of articles. 3 Hypothesis Rocchio's optimal query is designed to maximize the average query-document similarity for all relevant articles, and to minimize the average query-document similarity for all known non-relevant articles. <ref> [23] </ref> The interesting part of this algorithm is its use of all known non-relevant articles, which means that an article which is completely unrelated to the user query also has a say in what the final query vector would be. This effect has its shortcomings.
Reference: [24] <author> G. Salton and C. Buckley. </author> <title> Term-weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24(5) </volume> <pages> 513-523, </pages> <year> 1988. </year>
Reference-contexts: results for the TREC-3 task (average precision 0.4440) are marginally better than the best parameter setting in QZ-1 (average precision 0.4415 for K = 2; 000) as well as in QZ-2 (aver 6 In the course of our experiments, we also used the well-known cosine normalization scheme for the queries, <ref> [24] </ref> but sum-based normalization yields more consistent results across query sets. age precision 0.4408 for S = 0:25).
Reference: [25] <author> G. Salton and C. Buckley. </author> <title> Improving retrieval performance by relevance feedback. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(4) </volume> <pages> 288-297, </pages> <year> 1990. </year>
Reference-contexts: Also, coefficients have been introduced in Rocchio's formulation which control the contribution of the original query, the relevant articles, and the non-relevant articles to the feedback 1 See [23] (page 315) for Rocchio's definition of an optimal query. query. These modifications yield the following query refor mulation function: <ref> [25] </ref> ~ Q new = fffi ~ Q orig +fi fi 1 X ~ Dfl fi N R D62Rel The feedback query created by Rocchio's query reformulation process (using the documents marked relevant by a user) is now considered as the user profile.
Reference: [26] <author> G. Salton and M. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw Hill Book Co., </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: fl This study was done when the primary author was a doctoral candidate at Cornell, and was supported in part by the National Science Foundation under grant IRI-9300124. inner-product similarity computation to match user profiles (which are term vectors in Smart) to the new articles (which are also term vectors). <ref> [26] </ref> The effectiveness with which a system routes new articles to users is, then, largely dependent upon the quality of the profile generated by the system. Profile creation becomes the most important step in the routing process.
Reference: [27] <author> G. Salton, A. Wong, and C. Yang. </author> <title> A vector space model for information retrieval. </title> <journal> Communications of the ACM, </journal> <volume> 18(11) </volume> <pages> 613-620, </pages> <month> November </month> <year> 1975. </year>
Reference-contexts: documents and towards the relevant articles. 2 Rocchio's Algorithm A feedback query creation algorithm developed by Joe Roc-chio in the mid-1960's has, over the years, proven to be one of the most successful profile learning algorithms. [22, 23] Roc-chio's algorithm was developed in the framework of the vector space model. <ref> [27] </ref> The algorithm is based upon the fact that if the relevance for a query is known, an optimal 1 query vector will maximize the average query-document similarity for the relevant articles, and will simultaneously minimize the average query-document similarity for the non-relevant documents.
Reference: [28] <author> H. Schutze, D. Hull, and J. Pedersen. </author> <title> A comparison of classifiers and document representations for the routing problem. </title> <booktitle> In Proceedings of the Eighteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 229-237. </pages> <institution> Association for Computing Machinery, </institution> <address> New York, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: This learned profile, also known as the routing or the feedback query since it is obtained using user's relevance feedback, is then matched against all the new articles that a system encounters (for example any new news stories, : : : ). <ref> [5, 28, 2, 9] </ref> If a new article matches the user profile adequately, then this article is assumed to be of potential interest to the user and is routed to the user. <p> Parameters involved: ff, fi, fl, and a list of rank cut offs. 4 This situation arises in TREC because pooling is used for relevance judgments. [11, 12] We assume that all the unjudged documents are non-relevant. 5 Related Work In [15] Hull, and in <ref> [28] </ref> Schutze, Hull, and Pedersen want to reduce the dimensionality of the feature space dramatically for use with strong learning methods. They use singular valued decomposition (SVD) of the document space (actually the documents-to-features matrix) to obtain a small number of LSI factors to be used with their learning methods. <p> Since SVD is extremely computationally intensive, they reduce the number of documents they use in their document space, and instead work in a "local region". In [15] Hull uses only the relevant documents for a query to learn the LSI factors. In <ref> [28] </ref> Schutze, Hull, and Pedersen use the top 2,000 documents retrieved by a query generated using Rocchio's feedback to learn the LSI factors needed in their classifiers. <p> The main motivation in these studies is to reduce the size of the training data for it to be usable with computation-ally intensive SVD methods. In <ref> [28] </ref> they mention that local LSI also has the advantage of using the non-relevant articles that are most difficult to distinguish from the relevant documents (see [28] page 233), but have not explicitly measured the advantages from their local LSI technique vs. a global one. <p> In <ref> [28] </ref> they mention that local LSI also has the advantage of using the non-relevant articles that are most difficult to distinguish from the relevant documents (see [28] page 233), but have not explicitly measured the advantages from their local LSI technique vs. a global one.
Reference: [29] <author> H. Seung, M. Opper, and H. Sompolinsky. </author> <title> Query by committee. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 287-294. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: Sampling techniques are well studied in the machine learning community. Our techniques come closest to uncertainty sampling of Lewis and Gale [18], which is motivated by the query by committee technique of Seung, Opper, and Sompolinsky. <ref> [29] </ref> In uncertainty sampling, only the training examples whose class membership is uncertain (given the current classifier) are proposed to a user for membership judgment.
Reference: [30] <author> A. Singhal, C. Buckley, and M. Mitra. </author> <title> Pivoted document length normalization. </title> <booktitle> In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 21-29. </pages> <institution> Association for Computing Machinery, </institution> <address> New York, </address> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: with its use of all the non-relevant articles to learn a feedback query, does a good job of differ 2 Here the query domain is simulated by considering the top 5,000 documents retrieved by the original query. 3 Documents are weighted using the Ltu weighting scheme of the Smart system. <ref> [30] </ref> entiating the articles in the general domain of a query from the articles that are not in the query domain; but it fails to make the finer distinctions between the relevant and the non-relevant articles in the query domain. <p> To avoid multi-dimensional tuning of parameters, we select a good set of Rocchio parameters for K = 5; 000 for QZ-1 and use the same ff, fi, fl across all query zoning strategies. Document vectors are individually weighted using the Ltu weighting scheme of the Smart system. <ref> [30] </ref> Table 2 shows the average precision results obtained by using various ff, fi, fl values for the TREC-3 routing task when no query zoning is used. Table 3 shows the same tuning for the TREC-4 routing task. <p> Query document similarity is dependent on the document length as well as the query length, and we do normalize the document vectors for length, <ref> [30] </ref> for a similarity threshold based scheme to be effective across queries, we will need to length-normalize the query vectors as well.
References-found: 30

