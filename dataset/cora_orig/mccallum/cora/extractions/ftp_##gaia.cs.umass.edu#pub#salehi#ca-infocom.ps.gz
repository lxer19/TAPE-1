URL: ftp://gaia.cs.umass.edu/pub/salehi/ca-infocom.ps.gz
Refering-URL: http://www.cs.umass.edu/~salehi/home.html
Root-URL: 
Email: -salehi,kurose,towsley-@cs.umass.edu  
Title: The Effectiveness of Affinity-Based Scheduling in Multiprocessor Networking  
Author: James D. Salehi, James F. Kurose, and Don Towsley 
Address: Amherst MA 01003, USA  
Affiliation: Computer Science Department, University of Massachusetts,  
Date: March 1996  
Note: To appear in Proc. IEEE INFOCOM,  
Abstract: Techniques for avoiding the high memory overheads found on many modern shared-memory multiprocessors are of increasing importance in the development of high-performance multiprocessor protocol implementations. One such technique is processor-cache affinity scheduling, which can significantly lower packet latency and substantially increase protocol processing throughput [20]. In this paper, we evaluate several aspects of the effectiveness of affinity-based scheduling in multiprocessor network protocol processing, under packet-level and connection-level par-allelization approaches. Specifically, we evaluate the performance of the scheduling technique 1) when a large number of streams are concurrently supported, 2) when processing includes copying of uncached packet data, 3) as applied to send-side protocol processing, and 4) in the presence of stream burstiness and source locality, two well-known properties of network traffic. We find that affinity-based scheduling performs well under these conditions, emphasizing its robustness and general effectiveness in multiprocessor network processing. In addition, we explore a technique which improves the caching behavior and available packet-level concurrency under connection-level parallelism, and find performance improves dramatically. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Anderson, E. Lazowska and H. Levy. </author> <title> The Performance Implications of Thread Management Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: To illustrate a performance lower bound, we consider a global organization managed either MRU or LRU, under which thread stacks tend to migrate among processors. (As an aside, note that while the load-balancing and synchronization benefits of per-processor thread pools have been explored <ref> [1] </ref>, the cache affinity benefits have not previously been evaluated.) * Stream affinity (i.e., avoidance of coherence-based misses on initial writes to per-stream data structures) is gained by wiring streams to processorsrequiring all protocol processing for a given stream to be performed on the same processor.
Reference: [2] <author> M. Bjorkman and P. Gunningberg. </author> <title> Locking Effects in Multiprocessor Implementations of Protocols. </title> <booktitle> Proc. ACM SIGCOMM, p. </booktitle> <pages> 74-83, </pages> <month> Sep. </month> <year> 1993. </year>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [2, 5, 11, 15, 18, 23, 24] </ref>). The use of parallelism in protocol processing is motivated by the development of high-speed networks, such as ATM, capable of delivering gigabit-range bandwidth to individual machines. Emerging large-scale server applications, such as digital multimedia information respositories, require application-level access to high network bandwidth.
Reference: [3] <author> P. Danzig, S. Jamin, R. Caceres, D. Mitzel and D. Estrin. </author> <title> An Empirical Workload Model for Driving Wide-Area TCP/IP Network Simulations. </title> <journal> Journal of Internetworking: Research and Experience, </journal> <volume> 3(1) </volume> <pages> 1-26, </pages> <year> 1992. </year> <title> 8 (a) B = 1 (b) B = 2 (c) B = 8 (a) B = 1 (b) B = 2 (c) B = 8 </title>
Reference-contexts: Free memory affinity scheduling increases the maximum number of supportable streams by 25%. 7 Stream burstiness and source locality We now refine the stream model. It is well-known that network traffic is generally not Poisson (e.g., <ref> [3, 6, 9, 12, 17] </ref>). The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic [9, 14].
Reference: [4] <author> M. Devarakonda and A. Mukherjee. </author> <booktitle> Issues in Implementation of Cache-Affinity Scheduling.Proc. Winter USENIX Conference, p. </booktitle> <pages> 345-357, </pages> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [4, 7, 13, 20, 26, 28] </ref>. On modern shared-memory machines, the time required to access an uncached memory location is typically much larger than the time to access one cached locally. <p> While previous studies have explored affinity-based scheduling of non-network-related application processing, <ref> [4, 7, 13, 26, 28] </ref>, our work is the first to apply the technique to operating system network processing. In [20, 21] we evaluated the benefits of affinity-based scheduling of receive-side UDP/IP/FDDI processing, for a shared-memory multiprocessor concurrently executing a background workload of non-protocol activity.
Reference: [5] <author> A. Garg. </author> <title> Parallel STREAMS: A Multi-Processor Implementation.Proc. </title> <booktitle> Winter 1990 USENIX Conference, p. </booktitle> <pages> 163-176, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [2, 5, 11, 15, 18, 23, 24] </ref>). The use of parallelism in protocol processing is motivated by the development of high-speed networks, such as ATM, capable of delivering gigabit-range bandwidth to individual machines. Emerging large-scale server applications, such as digital multimedia information respositories, require application-level access to high network bandwidth.
Reference: [6] <author> R. Gusella. </author> <title> A Measurement Study of Diskless Workstation Traffic on an Ethernet IEEE Transactions on Communications,38(9):1557-1568,Sep. </title> <year> 1990. </year>
Reference-contexts: Free memory affinity scheduling increases the maximum number of supportable streams by 25%. 7 Stream burstiness and source locality We now refine the stream model. It is well-known that network traffic is generally not Poisson (e.g., <ref> [3, 6, 9, 12, 17] </ref>). The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic [9, 14].
Reference: [7] <author> A. Gupta, A. Tucker and S. Urushibara. </author> <title> The Impact of Operating System Scheduling Policies And SynchronizationMethods on the Performance of Parallel Applications. </title> <booktitle> Proc. ACM SIGMETRICS, p. </booktitle> <pages> 120-132, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [4, 7, 13, 20, 26, 28] </ref>. On modern shared-memory machines, the time required to access an uncached memory location is typically much larger than the time to access one cached locally. <p> While previous studies have explored affinity-based scheduling of non-network-related application processing, <ref> [4, 7, 13, 26, 28] </ref>, our work is the first to apply the technique to operating system network processing. In [20, 21] we evaluated the benefits of affinity-based scheduling of receive-side UDP/IP/FDDI processing, for a shared-memory multiprocessor concurrently executing a background workload of non-protocol activity.
Reference: [8] <author> N. Hutchinson and L. Peterson. </author> <title> The x-Kernel: An Architecture for Implementing Network Protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: To establish the results, we follow the research methodology developed in our earlier work, extending the software infrastructure as necessary. Our development environment consists of a parallelized x-kernel <ref> [8, 16] </ref> running on a SGI Challenge multiprocessor. We conduct a set of multiprocessor experiments with the x-kernel's UDP/IP/FDDI protocol stack, measuring packet processing times under specific conditions of cache state.
Reference: [9] <author> R. Jain and S. Routhier. </author> <title> Packet Trains: Measurements and a New Model for Computer Network Traffic, </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 4(6) </volume> <pages> 986-995, </pages> <month> Sep. </month> <year> 1986. </year>
Reference-contexts: The benefit of affinity-based scheduling remains significant. 4) We evaluate performance as a function of stream burstiness and source locality, two well-known properties of network traffic <ref> [9, 14] </ref>, by modeling individual streams with the Packet-Train source model [9]. We find that the performance of affinity-based scheduling is relatively insensitive to these source characteristics. 5) We explore a technique for improving the caching behavior and available packet-level concurrency under CLP. <p> The benefit of affinity-based scheduling remains significant. 4) We evaluate performance as a function of stream burstiness and source locality, two well-known properties of network traffic [9, 14], by modeling individual streams with the Packet-Train source model <ref> [9] </ref>. We find that the performance of affinity-based scheduling is relatively insensitive to these source characteristics. 5) We explore a technique for improving the caching behavior and available packet-level concurrency under CLP. <p> Protocol processing does not include data-touching operations, such as copying or checksumming; these are considered in Section 5. Packet arrivals are modeled with an exponential interarrival time; the more realistic Packet-Train model <ref> [9] </ref>, which captures stream burstiness and source locality, is considered in Section 7. 4.1 PLP Figures 4 and 5 show the performance of affinity-based scheduling under PLP as the number of admitted streams varies. <p> Free memory affinity scheduling increases the maximum number of supportable streams by 25%. 7 Stream burstiness and source locality We now refine the stream model. It is well-known that network traffic is generally not Poisson (e.g., <ref> [3, 6, 9, 12, 17] </ref>). The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic [9, 14]. <p> The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic <ref> [9, 14] </ref>. How do burstiness and source locality impact the performance of our results? To capture these workload characteristics, we refine the per-stream arrival process to the Packet-Train model developed by Jain and Routhier [9]. See Figure 12 for an illustration. <p> How do burstiness and source locality impact the performance of our results? To capture these workload characteristics, we refine the per-stream arrival process to the Packet-Train model developed by Jain and Routhier <ref> [9] </ref>. See Figure 12 for an illustration. Each stream is modeled as sending bursts of packets, where the burst size in packets has distribution B with mean B.
Reference: [10] <author> J. Kay and J. Pasquale. </author> <title> The Importance of Non-Data Touching Processing Overheads in TCP/IP. </title> <booktitle> Proc. ACM SIGCOMM, p. </booktitle> <pages> 259-268, </pages> <month> Sep. </month> <year> 1993. </year>
Reference-contexts: examine this alternative in Section 8. 5 Data-touching operations So far, we have evaluated affinity-based scheduling of protocol processing which does not include data-touching operations (e.g., copying, checksumming), motivated by the fact that protocol processing time in many real environments is dominated by non-data-touching operations with generally fixed per-packet overheads <ref> [10] </ref> (see [19, 22] for discussion). Yet there are arguments for explicitly incorporating data-copying overheads. Most implementations copy packet data between user and kernel space. <p> To incorporate this per-byte overhead into our simulation model, we need a packet-size distribution. We use the empirical UDP/- IP/FDDI distribution published by Kay and Pasquale <ref> [10] </ref>. This distribution was gathered by the authors from a traffic trace taken from their departmental FDDI LAN, supporting mostly workstations and file servers. <p> Ninety percent of the packets in this FDDI trace are UDP packets, primarily from NFS; thus, the empirical data is well-matched to the protocols involved in our study. The packet-size distribution, taken from Figure 5b of <ref> [10] </ref>, is shown in Table 2. In formulating this distribution, we have assumed that each 8KB UDP packet is sent as two 4KB FDDI frames. We modified our multiprocessor simulation to sample the distribution independently for each packet sent.
Reference: [11] <author> T. La Porta and M. Schwartz. </author> <title> Performance Analysis of MSP: Feature-Rich High-Speed Transport Protocol. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(6) </volume> <pages> 740-753, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [2, 5, 11, 15, 18, 23, 24] </ref>). The use of parallelism in protocol processing is motivated by the development of high-speed networks, such as ATM, capable of delivering gigabit-range bandwidth to individual machines. Emerging large-scale server applications, such as digital multimedia information respositories, require application-level access to high network bandwidth.
Reference: [12] <author> W. Leland, M. Taqqu, W. Willinger and D. Wilson. </author> <title> On the Self-Similar Nature of Ethernet Traffic (Extended Version). </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 2(1) </volume> <pages> 1-15, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: Free memory affinity scheduling increases the maximum number of supportable streams by 25%. 7 Stream burstiness and source locality We now refine the stream model. It is well-known that network traffic is generally not Poisson (e.g., <ref> [3, 6, 9, 12, 17] </ref>). The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic [9, 14].
Reference: [13] <author> E. Markatos and T. LeBlanc. </author> <title> Using Processor Affinity in Loop Scheduling on Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [4, 7, 13, 20, 26, 28] </ref>. On modern shared-memory machines, the time required to access an uncached memory location is typically much larger than the time to access one cached locally. <p> While previous studies have explored affinity-based scheduling of non-network-related application processing, <ref> [4, 7, 13, 26, 28] </ref>, our work is the first to apply the technique to operating system network processing. In [20, 21] we evaluated the benefits of affinity-based scheduling of receive-side UDP/IP/FDDI processing, for a shared-memory multiprocessor concurrently executing a background workload of non-protocol activity.
Reference: [14] <author> J. Mogul. </author> <title> Network Locality at the Scale of Processes. </title> <booktitle> Proc. ACM SIGCOMM, p. </booktitle> <pages> 273-284, </pages> <month> Sep. </month> <year> 1991. </year>
Reference-contexts: The benefit of affinity-based scheduling remains significant. 4) We evaluate performance as a function of stream burstiness and source locality, two well-known properties of network traffic <ref> [9, 14] </ref>, by modeling individual streams with the Packet-Train source model [9]. We find that the performance of affinity-based scheduling is relatively insensitive to these source characteristics. 5) We explore a technique for improving the caching behavior and available packet-level concurrency under CLP. <p> The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic <ref> [9, 14] </ref>. How do burstiness and source locality impact the performance of our results? To capture these workload characteristics, we refine the per-stream arrival process to the Packet-Train model developed by Jain and Routhier [9]. See Figure 12 for an illustration.
Reference: [15] <author> E. Nahum, D. Yates, J. Kurose and D. Towsley. </author> <title> Performance Issues in Par-allelized Network Protocols. </title> <booktitle> Proc. 1 st USENIX Symposium on Operating Systems Design and Implementation, p. </booktitle> <pages> 125-137, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [2, 5, 11, 15, 18, 23, 24] </ref>). The use of parallelism in protocol processing is motivated by the development of high-speed networks, such as ATM, capable of delivering gigabit-range bandwidth to individual machines. Emerging large-scale server applications, such as digital multimedia information respositories, require application-level access to high network bandwidth.
Reference: [16] <author> S. O'Malley and L. Peterson. </author> <title> A Dynamic Network Architecture. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(2) </volume> <pages> 110-143, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: To establish the results, we follow the research methodology developed in our earlier work, extending the software infrastructure as necessary. Our development environment consists of a parallelized x-kernel <ref> [8, 16] </ref> running on a SGI Challenge multiprocessor. We conduct a set of multiprocessor experiments with the x-kernel's UDP/IP/FDDI protocol stack, measuring packet processing times under specific conditions of cache state.
Reference: [17] <author> V. Paxson and S. Floyd. </author> <title> Wide Area Traffic: The Failure of Poisson Modeling. </title> <booktitle> Proc. ACM SIGCOMM, p. </booktitle> <pages> 257-268, </pages> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: Free memory affinity scheduling increases the maximum number of supportable streams by 25%. 7 Stream burstiness and source locality We now refine the stream model. It is well-known that network traffic is generally not Poisson (e.g., <ref> [3, 6, 9, 12, 17] </ref>). The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic [9, 14].
Reference: [18] <author> D. Presotto. </author> <title> Multiprocessor STREAMS for Plan 9. United Kingdom UNIX User's Group, </title> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [2, 5, 11, 15, 18, 23, 24] </ref>). The use of parallelism in protocol processing is motivated by the development of high-speed networks, such as ATM, capable of delivering gigabit-range bandwidth to individual machines. Emerging large-scale server applications, such as digital multimedia information respositories, require application-level access to high network bandwidth.
Reference: [19] <author> J. Salehi, J. Kurose and D. Towsley. </author> <title> Further Results in Affinity-Based Scheduling of Parallel Networking. </title> <type> Technical Report 95-46, </type> <institution> Computer Science Department, U. Massachusetts, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: alternative in Section 8. 5 Data-touching operations So far, we have evaluated affinity-based scheduling of protocol processing which does not include data-touching operations (e.g., copying, checksumming), motivated by the fact that protocol processing time in many real environments is dominated by non-data-touching operations with generally fixed per-packet overheads [10] (see <ref> [19, 22] </ref> for discussion). Yet there are arguments for explicitly incorporating data-copying overheads. Most implementations copy packet data between user and kernel space. <p> In order to capture the impact of the copy on cached protocol state and therefore on protocol processing time, we assess the overhead experimentally. See <ref> [19, 22] </ref> for a discussion of the checksumming operation, and why we have chosen not to incorporate it into the measurements. We modified our x-kernel implementation to include a copy of B bytes of uncached packet data, with B ranging between 1 and 4KB. <p> For PLP, compare Figures 8 and 4; for CLP, 9 and 6. Although the copy increases the mean packet delay by about 30s, the benefit of affinity scheduling remains significant. This holds for thread and free-memory affinity scheduling as well; see <ref> [19] </ref> or [22] for the complete set of graphs (omitted here due to space limitations). Throughout the rest of the paper, the overhead of copying uncached data is incorporated into all results. 6 Send-side processing Until now, we have explored affinity-based scheduling of receive-side UDP/IP/FDDI processing. <p> The goal was to identify the protocol and stream-specific data structures written during a send operation, as a first step toward developing send-side PLP and CLP implementations. In contrast to the receive-side path, send-side UDP/IP/FDDI processing references only read-only protocol-specific and stream-specific data structures (see <ref> [19] </ref> for discussion). Thus, there is no need to explicitly enable parallelism through software locks (PLP) or through data structure replication (CLP), and the un-parallelized implementation immediately supports concurrency. Thus, there is no PLP/CLP distinction with regard to send-side UDP/IP/FDDI processing. <p> We then performed a set of experiments measuring processing time in the instrumented x-kernel. See <ref> [19, 22] </ref> for a description of the experimental design. The resulting timing measurements are shown in Table 3, with corresponding receive-side numbers from [20] presented for comparison purposes. The timings suggest the importance of code affinity scheduling of send-side processing. <p> We assume the mean time between bursts is several orders of magnitude larger than the mean time between packets within a burst, in line with Jain and Routhier's observations. In <ref> [19, 22] </ref> we evaluate the impact of increases in mean burst size B on the performance of all four affinity scheduling policies (code, stream, thread, free-memory) under PLP, CLP, and for send-side processing.
Reference: [20] <author> J. Salehi, J. Kurose and D. Towsley. </author> <title> The Performance Impact of Scheduling for Cache Affinity in Parallel Network Processing, </title> <booktitle> Fourth IEEE International Symposium on High-PerformanceDistributed Computing, p. </booktitle> <pages> 66-77, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [4, 7, 13, 20, 26, 28] </ref>. On modern shared-memory machines, the time required to access an uncached memory location is typically much larger than the time to access one cached locally. <p> While previous studies have explored affinity-based scheduling of non-network-related application processing, [4, 7, 13, 26, 28], our work is the first to apply the technique to operating system network processing. In <ref> [20, 21] </ref> we evaluated the benefits of affinity-based scheduling of receive-side UDP/IP/FDDI processing, for a shared-memory multiprocessor concurrently executing a background workload of non-protocol activity. We found that the scheduling technique can significantly reduce packet latency, and in some cases substantially increase protocol processing throughput. <p> By matching the number of independent protocol stacks to the number of admitted streams, the performance of CLP improves dramatically, due to 1 In <ref> [20] </ref>, we referred to these approachesas Locking and Independent Protocol Stacks (IPS), respectively. Here, we adopt terms which reflect the level of protocol parallelism enabled. 2 Both PLP and CLP apply to connection-oriented (e.g., TCP) and connection-less (e.g., UDP) protocol stacks. <p> New performance results are presented in Sections 4-8, and summarized in Section 9. Due to space limitations, we omit a review of the related work in affinity-based scheduling; see <ref> [20, 22] </ref> for discussion. 2 Problem formulation In this section, we review the problem formulation developed in our earlier work [20]. <p> New performance results are presented in Sections 4-8, and summarized in Section 9. Due to space limitations, we omit a review of the related work in affinity-based scheduling; see [20, 22] for discussion. 2 Problem formulation In this section, we review the problem formulation developed in our earlier work <ref> [20] </ref>. <p> Data is not actually received from, or sent on, the actual network. To parallelize this x-kernel implementation, we first identified the non-read-only data structures referenced during protocol processing (see <ref> [20] </ref> for details). Consider the following two par-allelization approaches. * Packet Level Parallelism (PLP) is achieved by protecting each non-read-only data structure with its own software lock. <p> Below, we summarize the salient aspects of the approach (see <ref> [20] </ref> for details). To facilitate presentation, we focus throughout this section on receive-side processing. The same general methodology is employed in obtaining the send-side results (Section 6). 3.1 Simulation of multiprocessor networking Consider the simulation of PLP. There are N processors and N protocol threads. <p> See <ref> [20] </ref> for details. To acquire the t hot; hot , t cold; hot , and t cold; cold values, we conduct experiments on our multiprocessor in which we vary the scheduling of protocol threads and explicitly manipulate the processor caches. The receive-side experimental design and resulting measurements are presented in [20]; <p> <ref> [20] </ref> for details. To acquire the t hot; hot , t cold; hot , and t cold; cold values, we conduct experiments on our multiprocessor in which we vary the scheduling of protocol threads and explicitly manipulate the processor caches. The receive-side experimental design and resulting measurements are presented in [20]; the send-side analysis appears in Section 6 of this paper. To summarize, we perform experiments specifically reflecting the parallelization alternative and particular combination of migration overheads, to determine t hot; hot , t cold; hot , and t cold; cold . <p> The computed packet processing time t (x) is then used in the multiprocessor simulation model. We now turn to the first set of performance results. number of streams varies, without data-touching operations. 4 Large numbers of streams In <ref> [20] </ref>, we fixed the number of streams at eight, and examined the performance of receive-side UDP/IP/FDDI processing while varying the mean packet rate of the individual streams. <p> Thus affinity-based scheduling remains beneficial under CLP as the number of streams varies, as with PLP. However, varying the number of streams diminishes the impact of the x-kernel's fast-path optimizations, raising packet processing time (see <ref> [20] </ref> for further discussion). While this is unavoidable under PLP since there is essentially a single protocol stack, one way to improve performance under CLP is to scale the number of independent stacks with the number of streams, thereby ensuring that the fast-path optimizations are realized with every packet. <p> To ensure the data is uncached, the referenced memory locations are written by another processor prior to protocol processing for the packet. We repeated the timing experiments, varying B from 1 to 4KB, and found that the receive-side timings presented in <ref> [20] </ref>, as well as the send-side timings presented in Section 6, scale up nearly linearly at a rate of 0.04s per byte. To incorporate this per-byte overhead into our simulation model, we need a packet-size distribution. We use the empirical UDP/- IP/FDDI distribution published by Kay and Pasquale [10]. <p> We then performed a set of experiments measuring processing time in the instrumented x-kernel. See [19, 22] for a description of the experimental design. The resulting timing measurements are shown in Table 3, with corresponding receive-side numbers from <ref> [20] </ref> presented for comparison purposes. The timings suggest the importance of code affinity scheduling of send-side processing. <p> On the other hand, with B sufficiently large, PLP offers lower mean packet delay under code affinity scheduling. If the mean inter-packet delay I were smaller (here it is 80s), the delay under CLP would rise above PLP even sooner with respect to increasing B. This is observed in <ref> [20] </ref>, where we considered Compound Poisson arrivals (i.e., the I = 0 case). 8 Stream-scaled CLP We now turn to our last set of performance results. So far, we have considered processor-scaled CLP, in which the number of independent protocol stacks matches the number of processors.
Reference: [21] <author> J. Salehi, J. Kurose and D. Towsley. </author> <title> Scheduling for Cache Affinity in Paral-lelized Communication Protocols (Extended Abstract). </title> <booktitle> Proc. ACM SIGMET-RICS, p. </booktitle> <pages> 311-312, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: While previous studies have explored affinity-based scheduling of non-network-related application processing, [4, 7, 13, 26, 28], our work is the first to apply the technique to operating system network processing. In <ref> [20, 21] </ref> we evaluated the benefits of affinity-based scheduling of receive-side UDP/IP/FDDI processing, for a shared-memory multiprocessor concurrently executing a background workload of non-protocol activity. We found that the scheduling technique can significantly reduce packet latency, and in some cases substantially increase protocol processing throughput.
Reference: [22] <author> J. Salehi, J. Kurose and D. Towsley. </author> <title> The Effectiveness of Affinity-Based Scheduling in Multiprocessor Networking (Extended Version). </title> <journal> Submitted to IEEE/ACM Transactions on Networking, </journal> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: New performance results are presented in Sections 4-8, and summarized in Section 9. Due to space limitations, we omit a review of the related work in affinity-based scheduling; see <ref> [20, 22] </ref> for discussion. 2 Problem formulation In this section, we review the problem formulation developed in our earlier work [20]. <p> alternative in Section 8. 5 Data-touching operations So far, we have evaluated affinity-based scheduling of protocol processing which does not include data-touching operations (e.g., copying, checksumming), motivated by the fact that protocol processing time in many real environments is dominated by non-data-touching operations with generally fixed per-packet overheads [10] (see <ref> [19, 22] </ref> for discussion). Yet there are arguments for explicitly incorporating data-copying overheads. Most implementations copy packet data between user and kernel space. <p> In order to capture the impact of the copy on cached protocol state and therefore on protocol processing time, we assess the overhead experimentally. See <ref> [19, 22] </ref> for a discussion of the checksumming operation, and why we have chosen not to incorporate it into the measurements. We modified our x-kernel implementation to include a copy of B bytes of uncached packet data, with B ranging between 1 and 4KB. <p> For PLP, compare Figures 8 and 4; for CLP, 9 and 6. Although the copy increases the mean packet delay by about 30s, the benefit of affinity scheduling remains significant. This holds for thread and free-memory affinity scheduling as well; see [19] or <ref> [22] </ref> for the complete set of graphs (omitted here due to space limitations). Throughout the rest of the paper, the overhead of copying uncached data is incorporated into all results. 6 Send-side processing Until now, we have explored affinity-based scheduling of receive-side UDP/IP/FDDI processing. <p> We then performed a set of experiments measuring processing time in the instrumented x-kernel. See <ref> [19, 22] </ref> for a description of the experimental design. The resulting timing measurements are shown in Table 3, with corresponding receive-side numbers from [20] presented for comparison purposes. The timings suggest the importance of code affinity scheduling of send-side processing. <p> We assume the mean time between bursts is several orders of magnitude larger than the mean time between packets within a burst, in line with Jain and Routhier's observations. In <ref> [19, 22] </ref> we evaluate the impact of increases in mean burst size B on the performance of all four affinity scheduling policies (code, stream, thread, free-memory) under PLP, CLP, and for send-side processing.
Reference: [23] <author> D. Schmidt and T. Suda. </author> <title> Measuring the Impact of Alternative Parallel Pro cess Architectures on Communication Subsystem Performance. </title> <booktitle> Proc. 4 th International Workshop on Protocols for High-Speed Networks, </booktitle> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [2, 5, 11, 15, 18, 23, 24] </ref>). The use of parallelism in protocol processing is motivated by the development of high-speed networks, such as ATM, capable of delivering gigabit-range bandwidth to individual machines. Emerging large-scale server applications, such as digital multimedia information respositories, require application-level access to high network bandwidth. <p> We also compared the performance of packet-level parallelism (PLP) and connection-level parallelism (CLP). 1 PLP enables unrestricted packet-level concurrency, whereas CLP allows concurrent processing only of packets from different streams. 2 These parallelization approaches are known to perform best on RISC-based shared-memory platforms <ref> [23, 24] </ref>, yet exhibit very different caching behaviors. CLP can deliver much lower message latency and significantly higher message throughput, due primarily to caching effects, but PLP exhibits better intra-stream scalability and more robust response to intra-stream burstiness, due to its greater packet-level concurrency.
Reference: [24] <author> D. Schmidt and T. Suda. </author> <title> Measuring the Performance of Parallel Message-based Process Architectures. </title> <booktitle> Proc. IEEE INFOCOM, p. </booktitle> <pages> 624-633, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [2, 5, 11, 15, 18, 23, 24] </ref>). The use of parallelism in protocol processing is motivated by the development of high-speed networks, such as ATM, capable of delivering gigabit-range bandwidth to individual machines. Emerging large-scale server applications, such as digital multimedia information respositories, require application-level access to high network bandwidth. <p> We also compared the performance of packet-level parallelism (PLP) and connection-level parallelism (CLP). 1 PLP enables unrestricted packet-level concurrency, whereas CLP allows concurrent processing only of packets from different streams. 2 These parallelization approaches are known to perform best on RISC-based shared-memory platforms <ref> [23, 24] </ref>, yet exhibit very different caching behaviors. CLP can deliver much lower message latency and significantly higher message throughput, due primarily to caching effects, but PLP exhibits better intra-stream scalability and more robust response to intra-stream burstiness, due to its greater packet-level concurrency.
Reference: [25] <author> J. Singh, H. Stone and D. Thiebaut. </author> <title> A Model of Workloads and its Use in Miss-Rate Prediction for Fully Associative Caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7) </volume> <pages> 811-825, </pages> <month> Jul. </month> <year> 1992. </year>
Reference-contexts: We use a multiprocessor simulation model that closely follows the behavior of Figure 1. An analytic model of packet processing time, which we derive from established analytic results developed by other researchers <ref> [25, 27] </ref>, is used to capture the displacement of the cached protocol state by the background workload. This model is formulated to reflect the specific cache architecture and organization of our Silicon Graphics machine. <p> packet processing time, t (x), as t (x) = G 1 (x)t hot; hot + (1 G 1 (x))G 2 (x)t cold; hot + (1 G 1 (x)))(1 G 2 (x))t cold; cold : (1) We formulate F 1 (x) and F 2 (x) from validated analytic results given in <ref> [25, 27] </ref>, in a way that reflects the cache organization (line size, cache size, associativity) of our SGI architecture. See [20] for details.
Reference: [26] <author> M. Squillante and E. Lazowska. </author> <title> Using Processor Cache Affinity Information in Shared-Memory Multiprocessor Scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-143, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [4, 7, 13, 20, 26, 28] </ref>. On modern shared-memory machines, the time required to access an uncached memory location is typically much larger than the time to access one cached locally. <p> While previous studies have explored affinity-based scheduling of non-network-related application processing, <ref> [4, 7, 13, 26, 28] </ref>, our work is the first to apply the technique to operating system network processing. In [20, 21] we evaluated the benefits of affinity-based scheduling of receive-side UDP/IP/FDDI processing, for a shared-memory multiprocessor concurrently executing a background workload of non-protocol activity.
Reference: [27] <author> D. Thiebaut and H. Stone. </author> <title> Footprints in the Cache. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(4) </volume> <pages> 305-329, </pages> <month> Nov. </month> <year> 1987. </year>
Reference-contexts: We use a multiprocessor simulation model that closely follows the behavior of Figure 1. An analytic model of packet processing time, which we derive from established analytic results developed by other researchers <ref> [25, 27] </ref>, is used to capture the displacement of the cached protocol state by the background workload. This model is formulated to reflect the specific cache architecture and organization of our Silicon Graphics machine. <p> packet processing time, t (x), as t (x) = G 1 (x)t hot; hot + (1 G 1 (x))G 2 (x)t cold; hot + (1 G 1 (x)))(1 G 2 (x))t cold; cold : (1) We formulate F 1 (x) and F 2 (x) from validated analytic results given in <ref> [25, 27] </ref>, in a way that reflects the cache organization (line size, cache size, associativity) of our SGI architecture. See [20] for details.
Reference: [28] <author> R. Vaswani and J. Zahorjan. </author> <title> The Implications of Cache Affinity on Processor Scheduling for Multiprogrammed, Shared Memory Multiprocessors. </title> <booktitle> Proc. 13 th Symposium on Operating Systems Principles, p. </booktitle> <pages> 26-40, </pages> <month> Oct. </month> <year> 1991. </year> <month> 9 </month>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [4, 7, 13, 20, 26, 28] </ref>. On modern shared-memory machines, the time required to access an uncached memory location is typically much larger than the time to access one cached locally. <p> While previous studies have explored affinity-based scheduling of non-network-related application processing, <ref> [4, 7, 13, 26, 28] </ref>, our work is the first to apply the technique to operating system network processing. In [20, 21] we evaluated the benefits of affinity-based scheduling of receive-side UDP/IP/FDDI processing, for a shared-memory multiprocessor concurrently executing a background workload of non-protocol activity.
References-found: 28

