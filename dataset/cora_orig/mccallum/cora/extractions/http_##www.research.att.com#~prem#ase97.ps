URL: http://www.research.att.com/~prem/ase97.ps
Refering-URL: 
Root-URL: 
Email: prem,stubblebine@research.att.com  
Title: Research directions for automated software verification: Using trusted hardware  
Author: Prem Devanbu Stuart Stubblebine 
Address: Florham Park, NJ 07932, USA  
Affiliation: Information Systems and Services Research Center, AT&T Labs Research  
Abstract: Service providers hosting software on servers at the request of content providers need assurance that the hosted software has no undesirable properties. This problem applies to browsers which host applets, networked software which can host software agents, etc. The hosted software's properties are currently verified by testing and/or verification processes by the hosting computer. This increases cost, causes delay, and leads to difficulties in version control. By furnishing content providers with a physically secure computing device with an embedded certified private key, such properties can be verified and/or enforced by the secure computing device at the content provider's site; the secure device can verify such properties, statically whenever possible, and by inserting checks into the executable binary when necessary. The resulting binary is attested by a trusted signature, and can be hosted with confidence. This position paper is a preliminary report that outlines our scientific and engineering goals in this project. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> ActiveX Consortium. </institution> <note> http:/www.activex.org. </note>
Reference-contexts: Finally, the full proof, and any invariants that contribute towards the proof will have to be released to H for checking; this may lead to unwanted loss of intellectual property. 2.3 Trusted Builder Signatures the ActiveX model The ActiveX model <ref> [1] </ref> also allows for the possibility of automating the development of trust in software that is received over the internet. ActiveX is based on the assumption that software built by well-known individuals and companies can be trusted. Authenticity of software is established by an attached cryptographic signature.
Reference: [2] <author> M. Blum, W. Evans, P. Gemmell, S. Kannan, and M. Noar. </author> <title> Checking the correctness of memories. </title> <journal> Algorithmica, </journal> <note> 12(2/3):225244, 1994. Originally appeared in FOCS 91. </note>
Reference-contexts: This is achieved by storing a constant or at most a logarithmic number of bits in the PSC 1 . Our techniques do not aspire to the information-theoretic level of security of memory checking protocols described in <ref> [2] </ref>. In this application, the adversary (H) enjoys just a constant level of speed up over the memory checker, and lower levels of security appear adequate.
Reference: [3] <author> DARPA. </author> <title> Workshop on foundations of secure mobile code, </title> <month> March </month> <year> 1997. </year> <note> http://www.cs.nps.navy.mil/- research/languages/wkshp.html. </note>
Reference-contexts: All these options have hurdles, including difficulties in verification, unwanted information disclosure, a priori human costs, administrative difficulties, or runtime penalties. 2 Related Work The issue of mobile code has received a great deal of attention lately <ref> [3] </ref>. In addition to the naive testing approach described in the previous section, there are approaches based on formal program verification. Verification has to be carried out by the hosting computer, since in general, any type of tool that runs on P's machine cannot be trusted.
Reference: [4] <author> P. Devanbu and S. G. Stubblebine. </author> <title> Cryptographic verification of test coverage claims. </title> <booktitle> In Proceedings of The Fifth ACM/SIGSOFT Symposium on the foundations of software engineering, </booktitle> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: There are any number of aspects that can be a secrecy concern to the software producer including the fact that a product has been submitted for testing, the results of the testing, and information revealed about the code during testing. Other approaches, not involving third party testing <ref> [4] </ref>, could be used. Testing approaches, however, have a fundamental limitation: it is impossible to test programs under all possible conditions.
Reference: [5] <author> E. Felten. </author> <title> Princeton safe internet programming java/activex faq, </title> <note> 1997. http://www.CS.Princeton.EDU/sip/java-vs-activex.html. </note>
Reference-contexts: The trust in this case in entirely based on proper key management, and proper use of the keys in signatures. If the keys get stolen or misused, signed (and therefore completely trusted) software could wreak havoc. To quote the Princeton Safe Internet Programming group <ref> [5] </ref>: ActiveX security relies entirely on human judgement.
Reference: [6] <author> C. Gunter, P. Homeier, and S. Nettles. </author> <title> Infrastructure for proof-referencing code. </title> <booktitle> In Proceedings, Workshop on Foundations of Secure Mobile Code, </booktitle> <month> March </month> <year> 1997. </year> <note> http://www.cs.nps.navy.mil/- research/languages/wkshp.html. </note>
Reference-contexts: The burden on H is now reduced to the far easier task of proof checking, which can often be carried out very fast. Gunter et al <ref> [6] </ref> recommend an allied approach they call proof referencing code, which involves the development and reuse of logical frameworks constructed as HOL theories. Such frameworks reduce the proof construction burden on P. However, it is important to bear in mind that the proof-carrying code approach is not always applicable.
Reference: [7] <author> G. McGraw and E. Felten. </author> <title> Java Security: Hostile Applets, Holes & Antidotes. </title> <publisher> John Wiley & Sons, </publisher> <year> 1997. </year>
Reference-contexts: Java applets constructed by potentially hostile parties can be hosted in the carefully controlled Java runtime environment. Java security is based on a number of principles <ref> [7] </ref>, one of which is type safety. Verifying type safety involves a great deal of information about a program; for this reason, Java byte codes contain exactly the same information as Java source code. <p> There are several difficulties associated with this approach. First, for large applets, there can be a significant overhead to doing byte code verification. Second, since the safety property checks (in this case, type checking) are done at the hosting party's site, exploitable weaknesses (several are described in <ref> [7] </ref>) in the checking process must be addressed by propagating new versions of the verifier to every user of the web client. The large, diverse population of web users makes it unlikely that timely and orderly update of faulty versions of the verifier will occur. <p> There is also another significant advantage. Security weaknesses in the Java type system or the byte code verifier have been discovered on several occasions <ref> [7] </ref>. When such weaknesses are discovered, the only way currently for browsers to secure themselves from attacks that exploit a known weakness is to download and install a new version of the browser (or applet viewer).
Reference: [8] <author> G. Necula. </author> <title> Proof-carrying code. </title> <booktitle> In Proceedings of POPL 97. ACM SIGPLAN, </booktitle> <year> 1997. </year>
Reference-contexts: Typically such proofs are large, cumbersome, and make use of powerful proof techniques such as induction. There are formidable obstacles to the automated construction of such proofs. It would be unrealistic for H to construct such proofs about every hosted software C. Necula <ref> [8] </ref> advocates an appealing variant of the formal verification approach. The proof is constructed by P, and is shipped to H along with the binary program C. <p> Approaches suggested by Necula <ref> [8] </ref> et al work on the binary; intellectual property is much harder to steal from binaries. However, in the proof-carrying code approach, the binary is accompanied by annotations and a proof. <p> These are needed by the code consumer to generate both a safety verification condition, and to check that this condition is showed by the proof. Unfortunately, the annotations and the proof can disclose a lot of valuable information about design and implementation. For example, the example given in <ref> [8] </ref> discloses the full layout of the datastructures used.
Reference: [9] <author> R. Rivest, A. Shamir, and L. Adleman. </author> <title> A Method for obtaining digital signatures and public key cryptosystems. </title> <journal> Communications of the ACM, </journal> <year> 1978. </year>
Reference: [10] <author> J. Rushby. </author> <title> Critical system properties: Survey and taxonomy. </title> <type> Technical Report CSL-93-01, </type> <institution> Stanford Research Institute, </institution> <year> 1993. </year>
Reference-contexts: Such frameworks reduce the proof construction burden on P. However, it is important to bear in mind that the proof-carrying code approach is not always applicable. Not all safety properties can be cast as a verification condition <ref> [10, 12] </ref>. These approaches are subject to some of the same issues discussed in x 2.1. First proof checking can still be a substantial task, depending on the size of the proof; Necula reports that a type-safety proof for a 730 byte binary program was checked in 1.9 ms.
Reference: [11] <author> S. G. Stubblebine. </author> <title> Recent-secure authentication: Enforcing revocation in distributed systems. </title> <booktitle> In IEEE Computer Society Symposium on Security and Privacy, </booktitle> <address> Oakland, California, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: When a flaw is discovered in a version of the bytecode verifier, the keys corresponding to that version of the verifier can be revoked by the manufacturer; a certified revocation message can be passed on the browsers using pull or push approaches <ref> [11] </ref>. 4.2 Formal Verification Java bytecode verification is possible only because Java bytecodes are essentially source code. Approaches suggested by Necula [8] et al work on the binary; intellectual property is much harder to steal from binaries.
Reference: [12] <author> C.-R. Tsai, V. D. Gligor, and C. S. Chandersekaran. </author> <title> On the identification of covert storage channels in secure systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <year> 1990. </year>
Reference-contexts: Such frameworks reduce the proof construction burden on P. However, it is important to bear in mind that the proof-carrying code approach is not always applicable. Not all safety properties can be cast as a verification condition <ref> [10, 12] </ref>. These approaches are subject to some of the same issues discussed in x 2.1. First proof checking can still be a substantial task, depending on the size of the proof; Necula reports that a type-safety proof for a 730 byte binary program was checked in 1.9 ms.
Reference: [13] <author> H.-P. V. Vliet. Mocha java bytecode decompiler, </author> <year> 1996. </year> <note> http://web.inter.nl.net/users/H.P.van.Vliet /mocha.htm. </note>
Reference-contexts: The large, diverse population of web users makes it unlikely that timely and orderly update of faulty versions of the verifier will occur. Finally, Java byte code is source code; bytecode decompiler tools such as mocha <ref> [13] </ref> have amply demonstrated this fact. This naturally raises intellectual property protection concerns: full implementation details are revealed to every user . 2.2 Proof-carrying Code The traditional formal verification approach involves constructing (given a piece of software) a formal proof that certain safety properties hold.
Reference: [14] <author> R. Wahbe, S. Lucco, T. Anderson, and S. Graham. </author> <title> Efficient software-based fault isolation. </title> <booktitle> In Proceedings of the Symposium on Operating Systems Principles, </booktitle> <year> 1993. </year>
Reference-contexts: H tests C under many different possible conditions. 3. If C passes tests, H agrees to host software. Other options for H include formal verification, transforming the program in some way (by inserting asserts that fail when safety properties are violated, e.g.,), or sandbox-ing <ref> [14] </ref> the software in a runtime environment. All these options have hurdles, including difficulties in verification, unwanted information disclosure, a priori human costs, administrative difficulties, or runtime penalties. 2 Related Work The issue of mobile code has received a great deal of attention lately [3].
Reference: [15] <author> B. Yee and D. Tygar. </author> <title> Secure coprocessors in electronic commerce applications. </title> <booktitle> In Proceedings of The First USENIX Workshop on Electronic Commerce, </booktitle> <address> New York, New York, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Independent agents can develop trust in these computations, subject to signatures appended to the results of the computations conducted by the PSC. Details of physically secure co-processors, and many different applications (other than the one discussed herein) can be found in <ref> [15] </ref>. 4 Tools Resident in Physically Secure co processors. Our approach is to establish trust in software using tools resident in a PSC installed at the P's site. Specifically, we envision tools resident in PSC devices.
References-found: 15

