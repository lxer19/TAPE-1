URL: http://www.cs.umn.edu/Users/dept/users/kumar/webace.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: WebACE: A Web Agent for Document Categorization and Exploration  
Author: Eui-Hong (Sam) Han Daniel Boley Maria Gini Robert Gross Kyle Hastings George Karypis Vipin Kumar Bamshad Mobasher Jerome Moore 
Affiliation: Department of Computer Science and Engineering University of Minnesota  
Abstract: We propose an agent for exploring and categorizing documents on the World Wide Web. The heart of the agent is an automatic categorization of a set of documents, combined with a process for generating new queries used to search for new related documents and filtering the resulting documents to extract the set of documents most closely related to the starting set. The document categories are not given a-priori. We present the overall architecture and describe two novel algorithms which provide significant improvement over traditional clustering algorithms and form the basis for the query generation and search component of the agent. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Ackerman et al. </author> <title> Learning probabilistic user profiles. </title> <journal> AI Magazine, </journal> <volume> 18(2) </volume> <pages> 47-56, </pages> <year> 1997. </year>
Reference-contexts: A few recent examples of such agents include WebWatcher [3], Syskill & Webert, and others. Syskill & Webert <ref> [1] </ref> utilizes a user profile and learns to rate Web pages of interest using a Bayesian classifier. Bal-abanovic [4] uses a single well-defined profile to find similar web documents. Candidate web pages are located using best-first search. <p> The number of times a user visits a document and the total amount of time a user spends viewing a document are just a few methods for determining user interest <ref> [1, 3, 4] </ref>. Once WebACE has recorded a sufficient number of interesting documents, each document is reduced to a document vector and the document vectors are passed to the clustering modules.
Reference: [2] <author> A. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A.I. Verkamo. </author> <title> Fast discovery of association rules. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307-328. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: This method first finds set of items that occur frequently together in transactions using association rule discovery methods <ref> [2] </ref>. These frequent item sets are then used to group items into hypergraph edges, and a hyper-graph partitioning algorithm [17] is used to find the item clusters. The similarity among items is captured implicitly by the frequent item sets. <p> For example, if fd 1 ; d 2 ; d 3 g is a frequent item set, then the hypergraph contains a hyperedge that connects d 1 , d 2 and d 3 . The weight of a hyperedge is calculated as the average confidence <ref> [2] </ref> of all the association rules involving the related documents of the hyperedge. <p> Similarly, this method can be applied to word clustering. In this setting, each word corresponds to an item and each document corresponds to a transaction. This method uses the Apriori algorithm <ref> [2] </ref> which has been shown to be very efficient in finding frequent item sets and HMETIS [17] which can partition very large hy-pergraphs (of size &gt; 100K nodes) in minutes on personal computers.
Reference: [3] <author> R. Armstrong, D. Freitag, T. Joachims, and T. Mitchell. WebWatcher: </author> <title> A learning apprentice for the world wide web. </title> <booktitle> In Proc. AAAI Spring Symposium on Information Gathering from Heterogeneous, Distributed Environments. </booktitle> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference-contexts: Personalized Web Agents Another group of Web agents includes those that obtain or learn user preferences and discover Web information sources that correspond to these preferences, and possibly those of other individuals with similar interests (using collaborative filtering). A few recent examples of such agents include WebWatcher <ref> [3] </ref>, Syskill & Webert, and others. Syskill & Webert [1] utilizes a user profile and learns to rate Web pages of interest using a Bayesian classifier. Bal-abanovic [4] uses a single well-defined profile to find similar web documents. Candidate web pages are located using best-first search. <p> The number of times a user visits a document and the total amount of time a user spends viewing a document are just a few methods for determining user interest <ref> [1, 3, 4] </ref>. Once WebACE has recorded a sufficient number of interesting documents, each document is reduced to a document vector and the document vectors are passed to the clustering modules.
Reference: [4] <author> Marko Balabanovic, Yoav Shoham, and Yeogirl Yun. </author> <title> An adaptive agent for automated Web browsing. Journal of Visual Communication and Image Representation, </title> <type> 6(4), </type> <year> 1995. </year>
Reference-contexts: A few recent examples of such agents include WebWatcher [3], Syskill & Webert, and others. Syskill & Webert [1] utilizes a user profile and learns to rate Web pages of interest using a Bayesian classifier. Bal-abanovic <ref> [4] </ref> uses a single well-defined profile to find similar web documents. Candidate web pages are located using best-first search. The system needs to keep a large dictionary and is limited to a single user. 3 WebACE Architecture WebACE's architecture is shown in Figure 1. <p> The number of times a user visits a document and the total amount of time a user spends viewing a document are just a few methods for determining user interest <ref> [1, 3, 4] </ref>. Once WebACE has recorded a sufficient number of interesting documents, each document is reduced to a document vector and the document vectors are passed to the clustering modules.
Reference: [5] <author> C. Berge. </author> <title> Graphs and Hypergraphs. </title> <publisher> American Elsevier, </publisher> <year> 1976. </year>
Reference-contexts: A frequent item sets found using the association rule discovery algorithm corresponds to a set of documents that have a sufficiently large number of features in common. These frequent item sets are mapped into hyperedges in a hypergraph. A hypergraph <ref> [5] </ref> H = (V; E) consists of a set of vertices V and a set of hyperedges E. A hypergraph is an extension of a graph in the sense that each hyperedge can connect more than two vertices.
Reference: [6] <author> M. W. Berry, S. T. Dumais, and Gavin W. O'Brien. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37 </volume> <pages> 573-595, </pages> <year> 1995. </year>
Reference-contexts: The cluster with the largest such scatter value is selected next. This method differs from that of Latent Semantic Indexing (LSI) <ref> [6] </ref> in many ways. First of all, LSI was originally formulated for a different purpose, namely as a method to reduce the dimensionality of the search space for the purpose of handling queries: retrieving some documents given a set of search terms.
Reference: [7] <author> D.L. Boley. </author> <title> Principal Direction Divisive Partitioning. </title> <type> Technical Report TR-97-056, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1997. </year>
Reference-contexts: This feature is particularly useful for clustering large document sets which are returned by standard search engines using keyword queries. 4.2 Principal Component Divisive Partitioning The method of Principal Component Divisive Partitioning <ref> [7] </ref> is a top down clustering method. Starting with a "root" cluster encompassing the entire document set, each unsplit cluster is split into two "child" clusters until a desired number of clusters is reached.
Reference: [8] <author> P. Cheeseman and J. Stutz. </author> <title> Bayesian classification (Autoclass): Theory and results. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: If the dimensionality is high, then the calculated mean values do not differ significantly from one cluster to the next. Hence the clustering based on these mean values does not always produce very good clusters. Similarly, probabilistic methods such as Bayesian classification used in AutoClass <ref> [8] </ref>, do not perform well when the size of the feature space is much larger than the size of the sample set. This type of data distribution seems to be characteristic of document categorization applications on the Web, such as categorizing a bookmark file. <p> In contrast to traditional clustering methods, our proposed methods are linearly scalable, an advantage which makes these methods particularly suitable for use in Web retrieval and categorization agents. For our evaluation, we compare these algorithms to two well-known methods: Bayesian classification as used by Au-toClass <ref> [8] </ref> and hierarchical agglomeration clustering (HAC) based on the use of a distance function [10]. AutoClass is based on the probabilistic mixture modeling [26], and given a data set it finds maximum parameter values for a specific probability distribution functions of the clusters.
Reference: [9] <author> R. B. Doorenbos, O. Etzioni, and D. S. Weld. </author> <title> A scalable comparison shopping agent for the World Wide Web. </title> <type> Technical Report 96-01-03, </type> <institution> University of Washington, Dept. of Computer Science and Engineering, </institution> <year> 1996. </year>
Reference-contexts: For example, agents such as FAQ-Finder [14], Information Manifold [18], and OC-CAM [19] rely either on pre-specified and domain specific information about particular types of documents, or on hard coded models of the information sources to retrieve and interpret documents. Other agents, such as ShopBot <ref> [9] </ref> and ILA [23], attempt to interact with and learn the structure of unfamiliar information sources. Information Filtering/Categorization A number of Web agents use various information retrieval techniques [12] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize.
Reference: [10] <author> Richard O. Duda and Peter E. Hart. </author> <title> Pattern Classification and scene analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: For our evaluation, we compare these algorithms to two well-known methods: Bayesian classification as used by Au-toClass [8] and hierarchical agglomeration clustering (HAC) based on the use of a distance function <ref> [10] </ref>. AutoClass is based on the probabilistic mixture modeling [26], and given a data set it finds maximum parameter values for a specific probability distribution functions of the clusters. The clustering results provide the full description of each cluster in terms of probability distribution of each attributes.
Reference: [11] <author> W. B. Frakes. </author> <title> Stemming algorithms. </title> <editor> In W. B. Frakes and R. Baeza-Yates, editors, </editor> <booktitle> Information Retrieval Data Structures and Algorithms, </booktitle> <pages> pages 131-160. </pages> <publisher> Pren-tice Hall, </publisher> <year> 1992. </year>
Reference-contexts: This ensures a stable data sample since some pages are fairly dynamic in content. The word lists from all documents were filtered with a stop-list and "stemmed" using Porter's suffix-stripping algorithm [24] as implemented by <ref> [11] </ref>. We derived 10 experiments (according to the method used for feature selection) and clustered the documents using the four algorithms described earlier. The objective of feature selection was to reduce the dimensionality of the clustering problem while retain the important features of the documents.
Reference: [12] <author> W. B. Frakes and R. Baeza-Yates. </author> <title> Information Retrieval Data Structures and Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: Other agents, such as ShopBot [9] and ILA [23], attempt to interact with and learn the structure of unfamiliar information sources. Information Filtering/Categorization A number of Web agents use various information retrieval techniques <ref> [12] </ref> and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize. For example, HyPursuit [27] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents. <p> The lifespan of these request threads is short, i.e. the duration of one HTTP request, Conversely, the browser listener thread persists for the duration of the application. 4 Clustering Methods Existing approaches to document clustering are generally based on either probabilistic methods, or distance and similarity measures (see <ref> [12] </ref>). Distance-based methods such as k-means analysis, hierarchical clustering [16] and nearest-neighbor clustering [20] use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space.
Reference: [13] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins Univ. Press, 3rd edition, </publisher> <year> 1996. </year>
Reference-contexts: The computation of u is the most expensive step in this whole process. We have used a fast Lanczos-based solver for the singular values of the matrix of documents in the cluster <ref> [13] </ref>. This algorithm is able to take full advantage of the fact that less than 4% of all the entries in the document vectors are nonzero.
Reference: [14] <author> K. Hammond, R. Burke, C. Martin, and S. Lytinen. FAQ-Finder: </author> <title> A case-based approach to knowledge navigation. </title> <booktitle> In Working Notes of the AAAI Spring Symposium: Information Gathering from Heterogeneous, Distributed Environments. </booktitle> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference-contexts: For example, agents such as FAQ-Finder <ref> [14] </ref>, Information Manifold [18], and OC-CAM [19] rely either on pre-specified and domain specific information about particular types of documents, or on hard coded models of the information sources to retrieve and interpret documents.
Reference: [15] <author> E.H. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Clustering based on association rule hypergraphs. </title> <booktitle> In Workshop on Research Issues on Data Mining and Knowledge Discovery, </booktitle> <pages> pages 9-13, </pages> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference-contexts: In HAC, the features in each document vector is usually weighted using the TFIDF function, which is an increasing function of the feature's text frequency and its inverse document frequency in the document space. 4.1 Association Rule Hypergraph Partitioning Algorithm In <ref> [15] </ref>, a new method was proposed for clustering related items in transaction-based databases, such as supermarket bar code data, using association rules and hypergraph partitioning. This method first finds set of items that occur frequently together in transactions using association rule discovery methods [2].
Reference: [16] <author> A.K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Distance-based methods such as k-means analysis, hierarchical clustering <ref> [16] </ref> and nearest-neighbor clustering [20] use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space.
Reference: [17] <author> G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar. </author> <title> Multilevel hypergraph partitioning: Application in VLSI domain. </title> <booktitle> In Proceedings ACM/IEEE Design Automation Conference, </booktitle> <year> 1997. </year>
Reference-contexts: This method first finds set of items that occur frequently together in transactions using association rule discovery methods [2]. These frequent item sets are then used to group items into hypergraph edges, and a hyper-graph partitioning algorithm <ref> [17] </ref> is used to find the item clusters. The similarity among items is captured implicitly by the frequent item sets. In document clustering, each document corresponds to an item and each possible feature corresponds to a transaction. <p> Similarly, this method can be applied to word clustering. In this setting, each word corresponds to an item and each document corresponds to a transaction. This method uses the Apriori algorithm [2] which has been shown to be very efficient in finding frequent item sets and HMETIS <ref> [17] </ref> which can partition very large hy-pergraphs (of size &gt; 100K nodes) in minutes on personal computers. An additional advantage of ARHP is that it can be used to filter out non-relevant documents while clustering a document space, and thus improving the quality of the document clusters.
Reference: [18] <author> T. Kirk, A. Y. Levy, Y. Sagiv, and D. Srivastava. </author> <title> The information manifold. </title> <booktitle> In Working Notes of the AAAI Spring Symposium: Information Gathering from Heterogeneous, Distributed Environments. </booktitle> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference-contexts: For example, agents such as FAQ-Finder [14], Information Manifold <ref> [18] </ref>, and OC-CAM [19] rely either on pre-specified and domain specific information about particular types of documents, or on hard coded models of the information sources to retrieve and interpret documents.
Reference: [19] <author> C. Kwok and D. Weld. </author> <title> Planning to gather information. </title> <booktitle> In Proc. 14th National Conference on AI, </booktitle> <pages> pages 32-39, </pages> <year> 1996. </year>
Reference-contexts: For example, agents such as FAQ-Finder [14], Information Manifold [18], and OC-CAM <ref> [19] </ref> rely either on pre-specified and domain specific information about particular types of documents, or on hard coded models of the information sources to retrieve and interpret documents. Other agents, such as ShopBot [9] and ILA [23], attempt to interact with and learn the structure of unfamiliar information sources.
Reference: [20] <author> S.Y. Lu and K.S. Fu. </author> <title> A sentence-to-sentence clustering procedure for pattern analysis. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 8 </volume> <pages> 381-389, </pages> <year> 1978. </year>
Reference-contexts: Distance-based methods such as k-means analysis, hierarchical clustering [16] and nearest-neighbor clustering <ref> [20] </ref> use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space.
Reference: [21] <author> Y. S. Maarek and I.Z. Ben Shaul. </author> <title> Automatically organizing bookmarks per content. </title> <booktitle> In Proc. of 5th International World Wide Web Conference, </booktitle> <year> 1996. </year>
Reference-contexts: For example, HyPursuit [27] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents. BO (Bookmark Organizer) <ref> [21] </ref> combines hierarchical clustering techniques and user interaction to organize a collection of Web documents based on conceptual information.
Reference: [22] <author> J. Moore, E. Han, D. Boley, M. Gini, R. Gross, K. Hast-ings, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Web page categorization and feature selection using association rule and principal component clustering. </title> <booktitle> In 7th Workshop on Information Technologies and Systems, </booktitle> <month> Dec </month> <year> 1997. </year>
Reference-contexts: It should be noted that the conclusions drawn in the above discussion have been confirmed by another experiment using a totally independent set of documents <ref> [22] </ref>. 5 Search for and Categorization of Similar Documents One of the main tasks of the agent is to search the Web for documents that are related to the clusters of documents. <p> For the categorization component, our experiments have shown that the ARHP algorithm and the PDDP algorithm are capable of extracting higher quality clusters while operating much faster compared to classical algorithms such as HAC or AutoClass. This is consistent with our previous results <ref> [22] </ref>. The ARHP algorithm is also capable of filtering out documents by setting a support threshold. To search for similar documents queries are formed by extending the characteristic word sets for each cluster.
Reference: [23] <author> M. Perkowitz and O. Etzioni. </author> <title> Category translation: learning to understand information on the internet. </title> <booktitle> In Proc. 15th International Joint Conference on AI, </booktitle> <pages> pages 930-936, </pages> <address> Montral, Canada, </address> <year> 1995. </year>
Reference-contexts: For example, agents such as FAQ-Finder [14], Information Manifold [18], and OC-CAM [19] rely either on pre-specified and domain specific information about particular types of documents, or on hard coded models of the information sources to retrieve and interpret documents. Other agents, such as ShopBot [9] and ILA <ref> [23] </ref>, attempt to interact with and learn the structure of unfamiliar information sources. Information Filtering/Categorization A number of Web agents use various information retrieval techniques [12] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize.
Reference: [24] <author> M. F. Porter. </author> <title> An algorithm for suffix stripping. </title> <booktitle> Program, </booktitle> <volume> 14(3) </volume> <pages> 130-137, </pages> <year> 1980. </year>
Reference-contexts: The labeling facilitates an entropy calculation and subsequent references to any page were directed to the archive. This ensures a stable data sample since some pages are fairly dynamic in content. The word lists from all documents were filtered with a stop-list and "stemmed" using Porter's suffix-stripping algorithm <ref> [24] </ref> as implemented by [11]. We derived 10 experiments (according to the method used for feature selection) and clustered the documents using the four algorithms described earlier. The objective of feature selection was to reduce the dimensionality of the clustering problem while retain the important features of the documents.
Reference: [25] <author> Gerard Salton and Michael J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: Some words are more frequent in a document than other words. Simple frequency of the occurrence of words is not adequate, as some documents are larger than others. Furthermore, some words may occur frequently across documents. Techniques such as TFIDF <ref> [25] </ref> have been proposed precisely to deal with some of these problems. Secondly, the number of all the words in all the documents can be very large. Distance-based schemes generally require the calculation of the mean of document clusters. <p> This algorithm is able to take full advantage of the fact that less than 4% of all the entries in the document vectors are nonzero. We remark that we did not use TFIDF scaling <ref> [25] </ref> because (a) the quality of the PDDP algorithm results was not noticeably different, and (b) TFIDF fills in all the zero entries with nonzero values, substantially increasing the costs.
Reference: [26] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: For our evaluation, we compare these algorithms to two well-known methods: Bayesian classification as used by Au-toClass [8] and hierarchical agglomeration clustering (HAC) based on the use of a distance function [10]. AutoClass is based on the probabilistic mixture modeling <ref> [26] </ref>, and given a data set it finds maximum parameter values for a specific probability distribution functions of the clusters. The clustering results provide the full description of each cluster in terms of probability distribution of each attributes.
Reference: [27] <author> Ron Weiss, Bienvenido Velez, Mark A. Sheldon, Chanathip Nemprempre, Peter Szilagyi, Andrzej Duda, and David K. Gifford. Hypursuit: </author> <title> A hierarchical network search engine that exploits content-link hypertext clustering. </title> <booktitle> In Seventh ACM Conference on Hypertext, </booktitle> <month> March </month> <year> 1996. </year>
Reference-contexts: Information Filtering/Categorization A number of Web agents use various information retrieval techniques [12] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize. For example, HyPursuit <ref> [27] </ref> uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents. BO (Bookmark Organizer) [21] combines hierarchical clustering techniques and user interaction to organize a collection of Web documents based on conceptual information.
Reference: [28] <author> Marilyn R. Wulfekuhler and William F. Punch. </author> <title> Finding salient features for personal Web page categories. </title> <booktitle> In Proc. of 6th International World Wide Web Conference, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: BO (Bookmark Organizer) [21] combines hierarchical clustering techniques and user interaction to organize a collection of Web documents based on conceptual information. Pattern recognition methods and word clustering using the Hartigan's K-means partitional clustering algorithm are used in <ref> [28] </ref> to discover salient HTML document features (words) that can be used in finding similar HTML documents on the Web.
References-found: 28

