URL: http://www.cs.cmu.edu/~dandre/papers/rat.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/dandre/www/papers/papers.html
Root-URL: 
Phone: (412) 268-7123  
Title: Automatically Choosing the Number of Fitness Cases: The Rational Allocation of Trials  
Author: Astro Teller David Andre 
Date: (510)-643-6228  
Address: Pittsburgh, PA 15213  Berkeley, CA 94720  
Affiliation: Department of Computer Science Carnegie Mellon University  Division of Computer Science University of California at Berkeley  
Abstract: For many problems to which genetic programming has been applied, choosing the number of fitness cases with which to evaluate the individuals is a crucial decision. If too few fitness cases are used, overfitting may occur, and the measured fitness of an individual may not be representative of its true fitness. On the other hand, if too many fitness cases are used, a great deal of computer time can be wasted. This paper presents a method for the Rational Allocation of Trials (RAT) that dynamically allocates a boundedly optimal number of fitness cases for each individual. RAT allocates individuals to tournaments prior to their evaluation, and then, borrowing from previous work in model selection, allocates trials (fitness cases) only to those individuals for whom the cost of evaluating another fitness case is outweighed by the expected utility that the new information will provide. For most evolutionary computation approaches, including genetic programming, and for most problems, the RAT algorithm will provide significant time savings at minimal additional system complexity.
Abstract-found: 1
Intro-found: 1
Reference: [ Box et al., 1978 ] <author> G. E. Box, W. G. Hunter, and J. S. Hunter. </author> <title> Statistics for Experimenters. </title> <publisher> Wiley, </publisher> <year> 1978. </year>
Reference-contexts: That individual might easily be highly represented in the next generation. 5 The BRACE Algorithm The BRACE algorithm, described in [ Moore and Lee, 1994 ] , deals with the problem of RACEing similar models with high variance by appealing to the Blocking statistical technique <ref> [ Box et al., 1978 ] </ref> . The Blocking insight is that, by confining sample comparisons to within "blocks" (groups of more homogeneous data), greater precision can often be obtained. Equations 7, 8, and 9 are the relevant Blocking formalisms for BRACE.
Reference: [ Gratch et al., 1993 ] <author> J. Gratch, S. Chien, and G. De-Jong. </author> <title> Learning search control knowledge for deep space network scheduling. </title> <booktitle> In Proceedings of the 10th International Conference on Machine Learning. </booktitle> <publisher> Mor-gan Kaufman, </publisher> <year> 1993. </year>
Reference: [ Greiner and Jurisica, 1992 ] <author> R. Greiner and I. Jurisica. </author> <title> A statistical approach to solving the ebl utility problem. </title> <booktitle> In Proceedings of the 10th International Conference on Artificial Intelligence. </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference: [ Holland, 1992 ] <author> J. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: This is another way of addressing the issue of the number of training cases, but in this work there is no attempt at optimality and brood selection inherently uses a separate process of fitness attribution for the children in the weeding-out step of the selection process. Chapter 5 of <ref> [ Holland, 1992 ] </ref> is entitled "The Optimal Allocation of Trials" and while the theory is described in the context of the k-armed bandit problem, the next chapter shows its application to genetic algorithms.
Reference: [ Maxwell, 1994 ] <author> Sidney R. Maxwell. </author> <title> Experiments with a coroutine model for genetic programming. </title> <booktitle> In Proceedings of the 1994 IEEE World Congress on Computational Intelligence, </booktitle> <address> Orlando, Florida, USA, volume 1, pages 413-417a, Orlando, Florida, USA, 27-29 June 1994. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: Because this paper uses genetic programming as the example domain for RAT, we will give two examples of related work from that field. <ref> [ Maxwell, 1994 ] </ref> describes how, under certain conditions, it is possible to allow evolving programs to run for varying lengths of time and to give each a fitness score per execution time.
Reference: [ Moore and Lee, 1994 ] <author> A. Moore and M. Lee. </author> <title> Efficient algorithms for minimizing cross validation error. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: RAT uses the simple mathematics of sample statistics to calculate the utility of evaluating another fitness cases for an individual, and is based on previous work in the machine learning literature on model selection <ref> [ Moore and Lee, 1994 ] </ref> . Given some simple assumptions, the RAT algorithm results in a boundedly optimal 1 allocation of fitness cases for individuals. <p> As is described in section 7, the utility of a particular piece of information in the evolutionary context is a complex issue. 4 The RACE Algorithm The RACE algorithm is described in <ref> [ Moore and Lee, 1994 ] </ref> . The goal of the RACE algorithm is to save time in the process of model selection. <p> Keep in mind that in RAT we will not be "throwing out" a population individual, but only ceasing to improve our approximation to its actual fitness. That individual might easily be highly represented in the next generation. 5 The BRACE Algorithm The BRACE algorithm, described in <ref> [ Moore and Lee, 1994 ] </ref> , deals with the problem of RACEing similar models with high variance by appealing to the Blocking statistical technique [ Box et al., 1978 ] . <p> 1 k X ((e i (l) e j (l)) ^ h Now, as k goes from 1 to T , we can eliminate a model M i on iteration k when: Prob (h fl ij &lt; fl) &lt; ffi (10) for some model M j on this same iteration k. <ref> [ Moore and Lee, 1994 ] </ref> present these two algorithms as methods for quickly choosing a single best model. Evolutionary computation has a similar problem to that of model selection picking multiple better individuals from a population.
Reference: [ Tackett, 1994 ] <author> W. Tackett. </author> <title> The unique implications of brood selection for genetic programming. </title> <booktitle> In Proceedings of the First IEEE International Conference on Evolutionary Computation, </booktitle> <pages> pages 160-3. </pages> <publisher> IEEE Press, </publisher> <year> 1994. </year>
Reference-contexts: how people in the evolutionary computation community have, in the past, attempted to address issues of training time, though in this case the question is "How long should I let a program run on a particular training case" and not directly, "How many training cases should I give this program?" <ref> [ Tackett, 1994 ] </ref> describes a process called "Brood Selection" in which many children are created and then a pre-selection goes on in an attempt to weed out all but the most fit children of recombination.
References-found: 7

