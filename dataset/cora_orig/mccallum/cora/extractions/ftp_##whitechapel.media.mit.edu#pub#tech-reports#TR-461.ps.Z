URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-461.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Title: CONJOINT PROBABILISTIC SUBBAND MODELING  Architecture and Planning,  of Computers and Communications  
Author: by Ashok Chhabedia Popat Rosalind W. Picard Stephen A. Benton 
Degree: in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  All rights reserved Signature of Author: Program in Media Arts and Sciences  Certified by:  Thesis Supervisor Accepted by:  Chair, Departmental Committee on Graduate Students  
Date: September 1997  August 8, 1997  
Note: 1990 Submitted to the Program in  c 1997  NEC Career Development Professor  Program in  
Address: 1986  
Affiliation: S.B. Electrical Engineering, Massachusetts Institute of Technology,  S.M. Electrical Engineering, Massachusetts Institute of Technology,  Media Arts and Sciences, School of  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Massachusetts Institute of Technology  Media Arts and Sciences  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> K. Abend, T. J. Harley, and L. N. Kanal. </author> <title> Classification of binary patterns. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-11:538-544, </volume> <year> 1980. </year>
Reference-contexts: Markov random mesh models based on parametric conditional densities have been studied extensively for use in texture and image modeling. In 1965 Abend et al. <ref> [1] </ref> applied such models to binary images. Besag [8] studied consistency requirements to allow the use of conditional rather than joint probability models to specify a two-dimensional stochastic process, and considered several exponential-family parametric conditional models.
Reference: [2] <author> Robert B. Ash. </author> <title> Real Analysis and Probability. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: It is assumed that Prob [ ] satisfies the usual requirements of a probability measure: nonnegativity, normalization, and countable additivity <ref> [2] </ref>.
Reference: [3] <author> Andrew R. Barron and Thomas M. </author> <title> Cover. Minimum complexity density estimation. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> 37(4) </volume> <pages> 1034-1054, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The minimum-description length (MDL) principle chooses the proportions that result in the shortest possible exact coded representation of the observed data, using an agreed upon prefix-free code for the empirical distribution itself <ref> [3, 109] </ref>. See [137], p. 106 for an interesting criticism of this approach.
Reference: [4] <author> Andrew R. Barron, Laszlo Gyorfi, and Edward C. van der Meulen. </author> <title> Distribution estimation consistent in total variation and in two types of information divergence. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(5) </volume> <pages> 1437-1454, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Parameter estimates obtained by optimizing regularized criteria can, in certain cases, be shown to have desirable asymptotic theoretical properties, such as consistency <ref> [4, 110] </ref>. However, unpenalized ML is also fine asymptotically, but notoriously unreliable on small samples, so it is not clear exactly what consistency per se really buys us in a practical setting. An important and somewhat different approach, applicable to semiparametric models, is cross-validation.
Reference: [5] <author> W.R. Bennett. </author> <title> Spectra of quantized signals. </title> <journal> Bell System Technical Journal, </journal> <volume> 27 </volume> <pages> 446-472, </pages> <month> July </month> <year> 1948. </year>
Reference-contexts: Approximations of this type were first developed by Bennett <ref> [5] </ref>.
Reference: [6] <author> Toby Berger. </author> <title> Rate Distortion Theory: A Mathematical Basis for Data Compression. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1971. </year>
Reference-contexts: The approachable bound on performance in the lossy setting is given by the rate-distortion function (RDF) [125], which is known analytically for comparatively few sources and distortion measures <ref> [41, 6] </ref>, and in general must be computed numerically [10]. Therefore, it would be difficult to make a strong case for the crossvalidated likelihood criterion (or any other criterion) by attempting to relate it to actual or even best-case system performance in any sort of general way. <p> A more feasible alternative would be to divide the image into non-overlapping equal-size blocks and apply VQ to each one. Unlike the scheme to be presented, VQ can always approach the rate-distortion bound in the limit of large vector dimension <ref> [6] </ref>, which is the optimum achievable curve (distortion-measure dependent) in the rate vs. distortion plane. However, VQ can be complex to design and, at the encoding end, to implement. <p> Gaussian random variables, such that future V 's are always independent of current and previous Y 's <ref> [6] </ref>. For simplicity, it is assumed that the V 's have zero mean and unit variance. <p> It is useful to compare performance with Shannon's distortion-rate function. For a Gaussian autoregressive source of order m and assuming mean-squared distortion, this limiting R-D curve is given parametrically by the following expressions <ref> [47, 6] </ref>: D = 2 h 1 i and 1 Z max 0; 2 1 i where g (!) = fi m X a i e ji! fi 2 Varying from zero (corresponding to zero distortion) to infinity (zero rate) traces out the rate-distortion bound.
Reference: [7] <author> Toby Berger. </author> <title> Minimum entropy quantizers and permutation codes. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-28(2):149-156, </volume> <month> March </month> <year> 1982. </year>
Reference-contexts: It is well-known that under an entropy constraint, uniformly spaced thresholds are asymptotically optimal for small levels of distortion with respect to several measures (including MSE), for a broad class of i.i.d. sources <ref> [46, 7] </ref>. Uniform spacing has also been found experimentally to be nearly optimal at all rates for several important sources and distortion measures [94]. Accordingly, we assume a uniform cell width of .
Reference: [8] <author> Julian Besag. </author> <title> Spatial interaction and the statistical analysis of lattice systems. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 36 </volume> <pages> 192-236, </pages> <year> 1974. </year>
Reference-contexts: Markov random mesh models based on parametric conditional densities have been studied extensively for use in texture and image modeling. In 1965 Abend et al. [1] applied such models to binary images. Besag <ref> [8] </ref> studied consistency requirements to allow the use of conditional rather than joint probability models to specify a two-dimensional stochastic process, and considered several exponential-family parametric conditional models.
Reference: [9] <author> Christopher M. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: An alternative view of mixtures is as general-purpose semiparametric density estimators, not necessarily reflective of the structure of the data source [37, p. 118ff]. This interpretation, which is the one adopted here, has been steadily growing in popularity in recent years, particularly among researchers interested in artificial neural networks <ref> [51, 9] </ref>. However, the important distinction between density estimation and function approximation is often blurred in that literature; see [95] for a brief discussion. <p> A possible numerical approach is gradient search [72], but the gradients involved are complex enough to make this technique computationally unattractive for large M . 2.6 Mixture Refinement via Expectation-Maximization The EM algorithm <ref> [9, 31, 104] </ref> provides an alternative gradient-free means of locally maximizing the likelihood over the mixture parameters, and has come to be a standard method of parameter estimation in mixtures and related models. <p> This soft assignment constitutes what is called the expectation step. The other step, maximization, involves re-estimating all of the parameters. Let f ^ P (i) (i) 2 (i) m;d g denote the mixture parameters at the beginning of the i th iteration. The EM update rule <ref> [104, 9] </ref> can be written as m = (i) Q D z ^ f mix (z d j ^ m;d ; ^ m;d ) m 0 =1 (i) Q D z ^ f mix (z d j ^ m 0 ;d ; ^ m 0 ;d ) ^ m;d = n=1 <p> Thus, any direct search would have to be constrained or regularized in some way. An approach based on a three-layer neural network implementation of a mixture is considered in <ref> [9] </ref>. It requires careful parameterization of the mixture parameters, and its apparent complexity indicates that it may not scale well to large problems. Since the EM algorithm is well-suited to mixture estimation for joint densities, it is reasonable to ask whether it might be applied fruitfully to conditional density estimation. <p> As mentioned in Section 2.7, there has been some prior work in direct optimization of mixtures for conditional density estimation using gradient-based techniques. The technique described in <ref> [9] </ref> implements the mixture as a three-layer neural network, and involves a carefully chosen parameterization of the weights. The resulting gradient expressions appear to be computationally complex, perhaps limiting the applicability of that technique to mixtures having relatively few components and to low-order conditional densities.
Reference: [10] <author> Richard E. Blahut. </author> <title> Computation of channel capacity and rate-distortion functions. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-18(4):460-473, </volume> <month> July </month> <year> 1972. </year>
Reference-contexts: The approachable bound on performance in the lossy setting is given by the rate-distortion function (RDF) [125], which is known analytically for comparatively few sources and distortion measures [41, 6], and in general must be computed numerically <ref> [10] </ref>. Therefore, it would be difficult to make a strong case for the crossvalidated likelihood criterion (or any other criterion) by attempting to relate it to actual or even best-case system performance in any sort of general way. <p> A filter length of unity corresponds to no preprocessing, while a dimension of unity corresponds to Lloyd-Max quantization. For reference, the rate-distortion bound (computed via the Blahut algorithm <ref> [10] </ref>) is also shown. Note that preprocessing helps performance when the simplest VQ is used, but is actually harmful when higher dimensionality is permitted.
Reference: [11] <author> M.B. Brahmanandam, S. Panchanathan, and M. Goldberg. </author> <title> An affine transform based image vector quantizer. </title> <booktitle> In Proceedings of the SPIE Visual Communications and Image Processing '93 Conference, volume 2094, </booktitle> <pages> pages 1639-1648, </pages> <address> Cambridge, MA, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Such an optimization can be feasible if the system is sufficiently constrained; for example, Brahmanandam et al. <ref> [11] </ref> have considered jointly optimizing an affine block transform and a vector quantizer.
Reference: [12] <author> Leo Breiman. </author> <title> Simplicity vs. accuracy | Heisenberg's principle in statistics. Abstract from a talk given at the 1996 the AMS-IMS-SIAM Joint Summer Research Conference on Adaptive Selection of Models and Statistical Procedures, </title> <month> June </month> <year> 1996. </year> <note> Available by request from this author. </note>
Reference-contexts: Indeed, some of the most successful techniques for regression and classification currently in use involve extremely complex semiparametric models and procedures that do not seem to lend themselves well to human interpretation <ref> [12] </ref>. There is no reason to suppose that the situation should be any different for density estimation. Our goal in modeling subbands is clear: to improve the performance of compression and other systems that operate in the subband domain.
Reference: [13] <author> Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth& Brooks/Cole, </publisher> <address> Pacific Grove, CA, </address> <year> 1984. </year>
Reference-contexts: Although this is also known as the "leave-one-out method;" we adopt the terminology used by Breiman et al. <ref> [13] </ref> because we use their notation elsewhere extensively. Each such trained model for that M is then evaluated for performance, taking as T the single observation that was deleted in training. The results of these jLj evaluations are then averaged to obtain the overall evaluation criterion for M . <p> Recalling the close relationship between likelihood and entropy discussed in Section 2.3, an approximately equivalent formulation is: find a partition for which the conditional entropy of y is minimized. This formulation is suggestive of a partioning technique proposed by Breiman, Friedman, Olshen, and Stone <ref> [13] </ref> as part of a tree-structured approach to classification and regression. In their methodology, here generically termed CART R fl , a fine partition is initially grown by recursively splitting cells to achieve the greatest stepwise reduction in a specified empirical measure of impurity. <p> The use of such trees to estimate posteriors 52 for categorical data is actually discussed in some detail by Breiman et al. <ref> [13] </ref> (see Section 3.9 of this thesis), and a nearly identical approach is presented by Rissanen [107] in the context of conditional PMF estimation for universal data compression. <p> In the early stages of development of the CART classification methodology, the empirical entropy was used as a substitute splitting criterion for the empirical misclassification rate, because the latter was found to be ill-behaved in a number of respects <ref> [13, pp. 94-98] </ref>. Later, other substitute criteria (notably, the Gini index) were used and even preferred. In at least one respect, then, matters are simpler for tree-structured density estimation than for classification: The natural criterion, empirical entropy, is known to be well-behaved and therefore a substitute need not be sought. <p> First, some useful notation and terminology specific to trees will be introduced. 3.1 Trees: Basic Concepts, Notation, and Terminology Notation will closely follow the conventions established in <ref> [13] </ref>, with some minor additions and modifications. Towards making the presentation self-contained, discussion begins with some basic definitions. <p> Once this complexity has been established, the leaf complexities can then be optimized independently. To determine the best tree complexity, we prune T max back to a subtree using minimum cost-complexity pruning, as described in <ref> [13] </ref>, taking crossvalidated likelihood as the cost criterion. Since the leaf mixtures have not yet been determined, we must approximate the likelihood with an estimate, just as when splitting. <p> We can use these histograms, with regularization, to obtain a cross-validated likelihood estimate by evaluating the histograms on the leaf populations corresponding to T . We then apply minimum cost-complexity pruning in the usual way <ref> [13, pp. 66-71] </ref>, using the negative crossvalidated likelihood for cost. The tree with the best crossvalidated likelihood is selected. The process is illustrated best by an example. Consider the six-component mixture of isotropic Gaussians having the parameters given in Table 3.4.2. <p> The most popular existing techniques have already been discussed in the early sections of this chapter. The most obvious relevant prior work is the use of CART trees to estimate the posterior class-membership probabilities, which is discussed in the original work by Breiman et al. <ref> [13, pp. 121-126] </ref>. There, the objective adopted was mean-squared error between the true and estimated PMFs, i.e., a discrete version of the MISE criterion.
Reference: [14] <author> P. Brodatz. </author> <title> Textures: A Photographic Album for Artists and Designers. </title> <publisher> Dover, </publisher> <address> New York, </address> <year> 1966. </year>
Reference: [15] <author> Judith C. Brown. </author> <title> Cluster-based probability model for musical instrument identification. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 102(2), </volume> <month> May </month> <year> 1997. </year> <month> 125 </month>
Reference-contexts: Application of our multiresolution method of texture synthesis to sound textures in joint work with Nicolas Saint-Arnaud [115] has already been noted in Chapter 5. Recently, Judith Brown <ref> [15] </ref> adapted our conjoint texture classification technique [96] to the problem of identifying musical instruments from sound samples, with encouraging results [16].
Reference: [16] <author> Judith C. Brown. </author> <title> private communication. </title> <publisher> MIT Media Lab, </publisher> <month> January </month> <year> 1997. </year>
Reference-contexts: Application of our multiresolution method of texture synthesis to sound textures in joint work with Nicolas Saint-Arnaud [115] has already been noted in Chapter 5. Recently, Judith Brown [15] adapted our conjoint texture classification technique [96] to the problem of identifying musical instruments from sound samples, with encouraging results <ref> [16] </ref>.
Reference: [17] <author> Robert W. Buccigrossi and Eero P. Simoncelli. </author> <title> Progresive wavelet image coding based on a conditional probability model. </title> <booktitle> In ICASSP-97: 1997 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Munich, Germany, </address> <month> April </month> <year> 1997. </year> <note> IEEE. </note>
Reference-contexts: In recent years, there has been growing interest in taking advantage of the nonlinear statistical dependence among subband pixels <ref> [17, 25, 55, 89, 88, 122, 126] </ref>. Recall from Section 2.1 that a simple but effective traditional approach to subband image coding is to employ entropy-coded uniform-threshold scalar quantization on the subbands, maintaining the same stepsize when quantizing all of the pixels in all of the subbands.
Reference: [18] <author> James A. Bucklew and Gary L. Wise. </author> <title> Multidimensional asymptotic quantization theory with rth power distortion measures. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> IT-28:239-247, </volume> <month> March </month> <year> 1982. </year>
Reference-contexts: For tractability, we further assume that there are infinitely many cells, and that the representation levels are the midpoints of the cells. The latter assumption incurs some loss in MSE-performance, generally significant only at low rates. If is small and if f is continuous and smooth (see <ref> [18, 80] </ref> for rigorous statements of these conditions), the quantization error at high rates will be nearly uniform on the interval [=2; =2], so that the MSE will be very close to 2 =12, largely independent of the actual rate. Approximations of this type were first developed by Bennett [5].
Reference: [19] <author> Edwin A. Burtt. </author> <booktitle> The Metaphysical Foundations of Modern Physical Science. </booktitle> <publisher> Humanities Press, </publisher> <address> Atlantic Highlands, NJ, </address> <year> 1980. </year>
Reference-contexts: In engineering, statistics, and science in general, one of the chief goals of modeling is prediction, and some models that have been extremely and consistently predictively accurate have come to be termed laws. Maxwell's equations, for instance, are considered "laws" of electromagnetism, though their justification is wholly empirical <ref> [19] </ref>.
Reference: [20] <author> Philip A. Chou. </author> <title> Optimal partitioning for classification and regression trees. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(4) </volume> <pages> 340-354, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: It is interesting to note that if M H t were constant, then the function ^ (t; d; t ) could be used to estimate information-divergence cost, which is listed by Chou <ref> [20, Appendix A] </ref> as satisfying conditions necessary for a particular method of fast search more general than the search for thresholds considered here. <p> However, only the leaf-conditional separability of x and y (from each other) seems essential in the formulation of the methodology. As mentioned in Section 3.3, the regularized histogram splitting/pruning cost function used in PCDT appears to have the necessary properties that Chou <ref> [20] </ref> and Chou et al. [22] list as allowing efficient and more general tree growing and pruning strategies to be used. In particular, it may be feasible to allow splitting by arbitrary hyperplanes instead of restricting the splits to hyperplanes that are perpendicular to the coordinate axes.
Reference: [21] <author> Philip A. Chou and Tom Lookabaugh. </author> <title> Conditional entropy-constrained vector quantization of linear predictive coefficients. </title> <booktitle> In Proceedings of the 1990 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 197-200, </pages> <address> Albuquerque, NM, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Connections can be drawn between the proposed conjoint scalar quantization system and several variations on VQ, including predictive, multistage, and nonlinear interpolative VQ [45]. Perhaps the most relevant prior work is the conditional entropy-constrained VQ technique proposed by Chou and Lookabaugh <ref> [21] </ref>. In that technique, entropy coding of VQ codewords is carried out conditionally using a first-order Markov model estimated from codeword histograms (occurrence counts). The authors report a significant improvement using this technique over unconditional entropy coded VQ in compressing linear predictive coefficients for speech.
Reference: [22] <author> Philip A. Chou, Tom Lookabaugh, and Robert M. Gray. </author> <title> Optimal pruning with applications to tree-structured source coding and modeling. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 35(2) </volume> <pages> 299-315, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: We can therefore use the same criterion in pruning as was used in splitting, but with crossvalidation. Crossvalidation will also be used later in determining the best complexities for each leaf. Note that ^ can be made to correspond to the mutual information functional listed by Chou et al. <ref> [22, Table I] </ref> by making a suitable choice of probability measures (namely, the regularized histogram estimates). Consequently, it satisfies the conditions necessary for the more general pruning algorithm described there to be used, wherein tree-complexity measures other than the number of leaf nodes can be employed. <p> However, only the leaf-conditional separability of x and y (from each other) seems essential in the formulation of the methodology. As mentioned in Section 3.3, the regularized histogram splitting/pruning cost function used in PCDT appears to have the necessary properties that Chou [20] and Chou et al. <ref> [22] </ref> list as allowing efficient and more general tree growing and pruning strategies to be used. In particular, it may be feasible to allow splitting by arbitrary hyperplanes instead of restricting the splits to hyperplanes that are perpendicular to the coordinate axes.
Reference: [23] <author> Ronald Christensen. </author> <title> Foundations of Inductive Reasoning. </title> <publisher> Entropy Ltd., Lincoln, </publisher> <address> Mass., </address> <year> 1980. </year>
Reference-contexts: The term "random object" will be defined later in this section. Density estimation is an example of a problem in inductive inference, and inherits all of the well-known foundational difficulties attendant to that class of problem <ref> [23, 105, 136, 139] </ref>. In particular, induction is not deduction: it is not possible on the basis of observing a finite sample to show analytically that a particular density estimate is correct or even reasonable, unless strong assumptions are made that reach beyond the information present in the available data.
Reference: [24] <author> Ronald A. Christensen, Kris Popat, and Thomas A. Reichert. </author> <title> Expert resolution using maximum entropy. </title> <booktitle> Management Science, </booktitle> <year> 1997. </year> <note> in preparation. </note>
Reference-contexts: A philosophically similar technique, without the hierarchical component, was proposed independently in [98]. Recently the same idea has been developed and analyzed extensively in the discrete case by Christensen et al. <ref> [24] </ref>; its extension to the continuous case, where its performance with PCDT could be compared, is a topic for possible future research. An important nonparametric technique in multivariate density estimation that has not yet been mentioned is projection pursuit [57, 39].
Reference: [25] <author> Pamela C. Cosman, Robert M. Gray, and Martin Vetterli. </author> <title> Vector quantization of image subbands: a survey. </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> 5(2) </volume> <pages> 202-225, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: One approach is to group subband pixels into disjoint sets and to quantize the pixels in each set jointly, ignoring any interdependence between sets. This approach to subband coding is an active area of research; see the recent paper by Cosman et al. <ref> [25] </ref> for an extensive survey. Also described there are techniques that we might call conjoint, except that, as far as I am aware, the conditional distribution is either modeled only implicitly as in [126], or explicitly but using fairly restrictive models as in [73]. <p> Perceptually optimized rate allocation has been considered by Jayant et al. [61] and Wu and Gersho [144]. As mentioned previously, subband pixels exhibit a great deal of statistical dependence despite their near-uncorrelatedness. This dependence may be exploited for coding gain in a number of ways <ref> [25] </ref>. In Chapter 5, we will consider exploiting the dependence by conjoint processing; specifically, by sequential scalar quantization with conditional entropy coding. 2.2 Density Estimation This section introduces ideas and notation related to probabilistic modeling in general and to density estimation in particular. <p> In recent years, there has been growing interest in taking advantage of the nonlinear statistical dependence among subband pixels <ref> [17, 25, 55, 89, 88, 122, 126] </ref>. Recall from Section 2.1 that a simple but effective traditional approach to subband image coding is to employ entropy-coded uniform-threshold scalar quantization on the subbands, maintaining the same stepsize when quantizing all of the pixels in all of the subbands.
Reference: [26] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley and Sons, </publisher> <year> 1991. </year>
Reference-contexts: His subsequent work generalizes the MDL principle considerably, removing the strict model/data separation, and allowing variable precision representation of the model param 1 In other words, proportional to the cardinality of the type class <ref> [26] </ref>. 20 eters [110]. It should be emphasized that the immediate goal of the MDL principle is inference, not data compression: the hope is that the lie turns out to be a more accurate description of the remaining contents of the box than the truth. <p> between P and ^ P , which is denoted and defined in the discrete case as D (P jj ^ P ) = z P (z) : (2:3:4) The relative entropy is not a true metric between densities because it is not symmetric and does not satisfy the triangle inequality <ref> [26] </ref>. On the other hand, it behaves in many ways like a metric and has a similar geometric interpretation in many problems [27]. <p> with the approximation R ln 1 = ln + h (Z) + D (f jj ^ f ); where h (Z) = R 1 1 f (z) ln f (z)dz is the differential entropy of Z, and D (f jj ^ f ) is the continuous version of relative entropy <ref> [26] </ref>, defined as D (f jj ^ f ) = z: ^ f (z)6=0 f (z) ln ^ f (z) dz if Probfz : ^ f (z) = 0 jf g = 0 +1 otherwise. (2:3:6) As in the discrete case, the difference in average log likelihoods for two competing density
Reference: [27] <author> Imre Csiszar. </author> <title> Information type measures of difference of probability distributions and indirect observations. </title> <journal> Studia Sci. Math. Hungar., </journal> <volume> 2 </volume> <pages> 229-318, </pages> <year> 1967. </year>
Reference-contexts: On the other hand, it behaves in many ways like a metric and has a similar geometric interpretation in many problems <ref> [27] </ref>.
Reference: [28] <author> Imre Csiszar and Janos Korner. </author> <title> Information Theory: Coding Theorems for Discrete Memo-ryless Systems. </title> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: is slightly ironic that a notion of a true governing probability law is unnecessary for the term "density estimation" to be meaningful, but perhaps no more so than in those other situations in inference and coding that can be approached without positing a true distribution or even a probability space <ref> [28, 30, 77, 108] </ref>. 2.4 Histogram, Kernel, and Mixture Density Models A probabilistic model should both generalize and summarize the training sequence.
Reference: [29] <author> Lee D. Davisson. </author> <title> Universal noiseless coding. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> IT-19:783-795, </volume> <month> November </month> <year> 1973. </year>
Reference-contexts: This observation provides a basis for two-part universal coding <ref> [29] </ref> as well as an intuitive justification of Rissanen's stochastic complexity measure of M 2 log jLj [110].
Reference: [30] <author> A.P. Dawid. </author> <title> Present position and potential developments: some personal views, statistical theory, the prequential approach. </title> <journal> Journal of the Royal Statistical Society, Series A, </journal> <volume> 147, Part 2 </volume> <pages> 278-292, </pages> <year> 1984. </year>
Reference-contexts: The dictionary definition of "conjoint" is not much different from that of "joint;" both can be taken in our context to mean roughly "taking the other values into account." Since "joint" is already claimed, we instead appropriate the term "conjoint" for our purpose. A related term coined by Dawid <ref> [30] </ref> is prequential (for predictive sequential), but it has come to be used in a relatively narrow statistical sense, rather than to describe a general style of processing information. <p> Such models tend to be semiparametric or nonparametric, and relatively complex. 6 Besides providing a descriptive framework, sequential probabilistic modeling can also serve as the basis for a powerful inferential framework <ref> [30, 108] </ref>. 30 2.3 Evaluation Criteria for Density Estimation As discussed Section 2.2, in a practical setting there is no analytic justification of preferring one density estimate or estimation procedure over another. Therefore, any such justification must be empirical. <p> is slightly ironic that a notion of a true governing probability law is unnecessary for the term "density estimation" to be meaningful, but perhaps no more so than in those other situations in inference and coding that can be approached without positing a true distribution or even a probability space <ref> [28, 30, 77, 108] </ref>. 2.4 Histogram, Kernel, and Mixture Density Models A probabilistic model should both generalize and summarize the training sequence. <p> Sardo and Kittler have recently investigated efficient methods for determining the appropriate complexities of separable-Gaussian mixture density estimates, particularly for small samples. In a recent paper [118], they invoke Dawid's notion <ref> [30] </ref> of calibration of the predictive performance of a model specifically to detect underfitting. In addition to avoiding the computational burden of crossvali-dation, their approach does not seem to suffer from the propensity towards oversimplistic models that is sometimes associated with MDL and related information criteria.
Reference: [31] <author> A. P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Stat. Soc., </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year> <month> 126 </month>
Reference-contexts: A possible numerical approach is gradient search [72], but the gradients involved are complex enough to make this technique computationally unattractive for large M . 2.6 Mixture Refinement via Expectation-Maximization The EM algorithm <ref> [9, 31, 104] </ref> provides an alternative gradient-free means of locally maximizing the likelihood over the mixture parameters, and has come to be a standard method of parameter estimation in mixtures and related models. <p> Recently, Tony Jebara has proposed a fixed-point maximization technique [63, 64] which seems to show promise for conditional density estimation, again primarily in the case of low-complexity mixtures. The development mirrors that of ordinary EM <ref> [31] </ref>, but the conditional density is substituted for the joint density when lower-bounding the per-iteration likelihood improvement via Jensen's inequality.
Reference: [32] <author> Pierre A. Devijver and Josef Kittler. </author> <title> Pattern Recognition: a Statistical Approach. </title> <booktitle> Prentice--Hall International, </booktitle> <address> Engelwood Cliffs, NJ, </address> <year> 1982. </year>
Reference-contexts: While tenable for an i.i.d. model of pixel differences, this assumption is arguably too restrictive in a system that is allowed to adapt. In any case, this approach adapts the problem to the Context algorithm, not the other way around. 76 Devijver and Kittler <ref> [32, chapter 11] </ref>. Briefly, it involves sequencing through the eigenvectors of the covariance matrix for the data, proceeding from largest to smallest, partitioning space with eigenvector-perpendicular splits between modes whenever they are detected.
Reference: [33] <author> Luc Devroye and Laszlo Gyorfi. </author> <title> Nonparametric Density Estimation: the L 1 View. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Specific Criteria: Likelihood and Relative Entropy We now discuss the choice of functional form of the evaluation criterion for a density estimate ^ f of f based on a particular training sequence, considering joint density estimation first. In theoretical analyses of density estimation <ref> [33, 120] </ref>, where the training sequence and therefore ^ f are considered random, the usual L p norms for functions are often used, where L p ( ^ f ; f ) = fi fi fi dz : (2:3:1) The most frequently used values of p are 1 and 2, with <p> Kernel density estimation is an important technique with a well-developed supporting theory [34, 49, 120], much of it focusing on the asymptotic properties as the training sequence becomes arbitrarily large <ref> [33] </ref>.
Reference: [34] <author> Luc Devroye, Laszlo Gyorfi, and Gabor Lugosi. </author> <title> A Probabilistic Theory of Pattern Recognition. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: to be discussed involve complex, data-driven procedures such as tree-growing and -pruning, for which a theoretical analysis would be difficult, though considerable progress has been made in this general area | see the recent work by Lugosi and Nobel [74], Nobel [81], Nobel and Olshen [82] and Devroye et al. <ref> [34, Chapters 20-23] </ref>. Second, the techniques and analyses to be presented were arrived at through practically motivated, somewhat heuristic reasoning, and it is desirable from a pedagogical standpoint not to obscure this path through post hoc theorizing. <p> The histogram functions by explicitly grouping observations into like occurrences, essentially transforming the continuous problem into a discrete one. A good reference for the asymptotic properties of histogram and other estimates is the recent book by Devroye et al. <ref> [34] </ref>. Another important semiparametric density model is the finite mixture, defined in the vector case 4 Not all models are specified analytically; some are defined by computer programs; e.g., regression trees. <p> Estimate (c) is a mix ture density estimate also having 128 components, trained using the generalized Lloyd algorithm as described in Section 2.5. Kernel density estimation is an important technique with a well-developed supporting theory <ref> [34, 49, 120] </ref>, much of it focusing on the asymptotic properties as the training sequence becomes arbitrarily large [33]. <p> It was mentioned in Chapter 2 that there has been significant progress recently by Lugosi and Nobel [74], Nobel [81], Nobel and Olshen [82] and Devroye et al. <ref> [34, Chapters 20-23] </ref> in establishing various asymptotic properties of tree-structured and other data-driven statistical techniques. Typically, the proofs involved in demonstrating such properties require that the cells in the partition shrink to zero volume as the sample size gets large.
Reference: [35] <author> Richard O. Duda and Peter E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Henceforth, mixtures components will be assumed to be separable Gaussian densities unless otherwise stated. Estimating the mixture components and mixing probabilities is in general a complex task. Several approaches have been proposed over the years <ref> [35, 40, 76, 104, 135] </ref>, and the topic continues to be an active area of research today. Much of the work has focused on low-complexity mixtures. When mixtures of high complexity are required or allowed, as they are here, robustness and computational complexity of the training process become important considerations.
Reference: [36] <author> Michelle Effros, Philip A. Chou, and Robert M. Gray. </author> <title> Weighted universal image compression. </title> <note> Submitted for publication. </note>
Reference-contexts: Such a technique and variants of it are termed universal. Recently, several strategies for universal coding of images were proposed and investigated in <ref> [36] </ref>; these techniques appear to hold considerable promise. 84 The proposed method of lossy encoding is uniform scalar quantization with conditional entropy coding, wherein both the quantizer and the entropy coder are adapted on the basis of previously decoded pixels. The system is shown in Figure 4.2.1.
Reference: [37] <author> B.S. </author> <title> Everitt and D.J. Hand. Finite Mixture Distributions. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: An alternative view of mixtures is as general-purpose semiparametric density estimators, not necessarily reflective of the structure of the data source <ref> [37, p. 118ff] </ref>. This interpretation, which is the one adopted here, has been steadily growing in popularity in recent years, particularly among researchers interested in artificial neural networks [51, 9].
Reference: [38] <author> Terrence L. </author> <title> Fine. Theories of Probability; an Examination of Foundations. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: as little consolation when the designer of the code claims (even if correctly) that the expected code length is only 200 bits. 3 Although there is now nearly universal agreement about what the axioms in a mathematical theory of probability ought to be (though there remains some dissent | see <ref> [38] </ref>), there is comparatively little agreement about what the interpretation of probability ought to be. The frequentist and subjectivist interpretations, which underlie traditional and Bayesian statistics respectively, are the two most familiar opposing views; there exist others as well.
Reference: [39] <author> Jerome H. Friedman, Werner Stuetzle, and Ann Schroeder. </author> <title> Projection pursuit density estimation. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 79(387) </volume> <pages> 599-608, </pages> <month> September </month> <year> 1984. </year>
Reference-contexts: An important nonparametric technique in multivariate density estimation that has not yet been mentioned is projection pursuit <ref> [57, 39] </ref>. In that technique, a multivariate (i.e., joint) density is expressed as a background density (which must be assumed) multiplied by the product of M univariate functions of linear combinations of the coordinates.
Reference: [40] <author> K. Fukunaga and T.E. Flick. </author> <title> Estimation of the parameters of a gaussian mixture using the method of moments. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5(4):410-416, </volume> <month> July </month> <year> 1983. </year>
Reference-contexts: Henceforth, mixtures components will be assumed to be separable Gaussian densities unless otherwise stated. Estimating the mixture components and mixing probabilities is in general a complex task. Several approaches have been proposed over the years <ref> [35, 40, 76, 104, 135] </ref>, and the topic continues to be an active area of research today. Much of the work has focused on low-complexity mixtures. When mixtures of high complexity are required or allowed, as they are here, robustness and computational complexity of the training process become important considerations.
Reference: [41] <author> Robert G. Gallager. </author> <title> Information Theory and Reliable Communication. </title> <publisher> Wiley, </publisher> <year> 1968. </year>
Reference-contexts: The main Bayesian leap (from Fisher's point of view, not Rissanen's) is to view the proportions themselves (or more generally, anything that isn't known with certainty) as being described by a probability distribution. Rissanen uses a prefix-free code, which amounts to the same thing via the Kraft inequality <ref> [41] </ref>. It is even possible to argue coherently for estimating a lower proportion of red than blue. A rule that does so is termed counterinductive [136]. One way to justify such an estimate would be on Bayesian grounds, using a suitable prior. <p> Interestingly, hidden Markov models were discussed in the context of source coding by Gallager <ref> [41, Chapter 3] </ref> more than a decade before they became popular in speech processing. For the remainder of this chapter, successive observations will be assumed to be independent. This is not as restrictive as it may sound, because the observations will be allowed to be multivari-ate and high-dimensional. <p> The approachable bound on performance in the lossy setting is given by the rate-distortion function (RDF) [125], which is known analytically for comparatively few sources and distortion measures <ref> [41, 6] </ref>, and in general must be computed numerically [10]. Therefore, it would be difficult to make a strong case for the crossvalidated likelihood criterion (or any other criterion) by attempting to relate it to actual or even best-case system performance in any sort of general way.
Reference: [42] <author> Robert G. Gallager. </author> <title> Source coding for subband coding. </title> <type> Unpublished manuscript, </type> <institution> Codex Corp., </institution> <month> August </month> <year> 1985. </year>
Reference-contexts: Using the same stepsize for all subband pixels can be shown to result in a nearly optimal implicit rate allocation with respect to mean-square error <ref> [42, 93] </ref>. A frequency-weighted mean-square error criterion can be easily accommodated by varying the stepsizes across subbands, as is implicitly done in JPEG [101]. Perceptually optimized rate allocation has been considered by Jayant et al. [61] and Wu and Gersho [144]. <p> Subband regions of high activity or variance will cause the quantizer to have greater output entropy, resulting in greater rate being implicitly allocated to those regions and subbands. As mentioned in Section 2.1, it can be shown that such a simple strategy results in a nearly MSE-optimal rate allocation <ref> [42, 93] </ref>, without the need for a computationally intensive rate-allocation procedure. Of course this assumes that the probabilistic model upon which the entropy coding is performed is accurate.
Reference: [43] <author> Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin. </author> <title> Bayesian Data Analysis. </title> <publisher> Chapman and Hall, </publisher> <year> 1995. </year>
Reference-contexts: This assumption of independence (or exchangeability in Bayesian parlance <ref> [43] </ref>) is reasonable if the box had been shaken before you reached in, and if the number of marbles in it is in fact very large. Relaxing this assumption will be considered later. <p> In a Bayesian setting, choosing ~ can be shown equivalent to selecting parameters for a Dirichlet prior on ^ P t <ref> [43] </ref>, though such an interpretation is not essential. More will be said about ~ later. 59 the ML estimator.
Reference: [44] <author> Neil Gershenfeld, Bernd Schoner, and Eric Metois. </author> <title> Cluster-weighted modeling for time-series prediction and characterisation. </title> <type> Preprint, </type> <year> 1997. </year>
Reference-contexts: Specifically, this occurs when the tail of a far-away large-variance component becomes greater than that of a nearby narrow-variance one. The same phenomenon occurs in the version of Jacob and Jordan's mixtures-of-experts architecture [59] proposed by Gershenfeld et al. <ref> [44] </ref>. Although consistent with the view of conditioning space consisting of multiple classes, each characterized by a Gaussian density, this type of weighting is inconsistent with the locality-of-experts motivation that underlies much of 112 the original mixture-of-experts framework. <p> Recently, Judith Brown [15] adapted our conjoint texture classification technique [96] to the problem of identifying musical instruments from sound samples, with encouraging results [16]. Eric Metois [78] and Bernd Schoner [119] trace the development of their Cluster-Weighted Modeling work <ref> [44] </ref> with Neil Gershenfeld to ideas gleaned from our papers on the 116 application of mixture models to high-order conditional density estimation for texture synthesis [96] and compression [98]. 117 APPENDIX A Economized EM Algorithm for Mixture Estimation This appendix presents a computationally economized, approximate version of the expectation-maximization (EM) algorithm
Reference: [45] <author> Allen Gersho and Robert M. Gray. </author> <title> Vector Quantization and Signal Compression. </title> <publisher> Kluwer academic publishers, </publisher> <year> 1992. </year>
Reference-contexts: In data compression, for example, the advantage of joint processing results from the ability to exploit dependence among the data, the ability to adapt to the distribution of the data, and the ability to fill space efficiently with representative points <ref> [45, 71] </ref>. It is generally much easier and often more natural to process data sequentially than jointly. It is of interest, therefore, to find ways of making the performance of sequential processing approach that of joint processing to the extent possible. <p> At the very least, it must be assumed that the relevant statistics across boxes can be learned by looking at the contents of 22 only a single box; roughly speaking, this is to assume that the process has the ergodic property <ref> [45] </ref>. In addition, in practice some sort of assumption regarding stationarity (invariance of statistics to shifts in time) and the allowed type of interdependence among observations must be conceded. <p> Selecting a set of representative points for a given set of data is precisely the problem of codebook design in vector quantization. It is natural therefore to consider the adaptation of algorithms normally used for codebook design to the task of mixture density estimation. The generalized Lloyd algorithm (GLA) <ref> [45] </ref>, which is the best known 10 and perhaps most widely used of these algorithms, involves iterating the following two steps after initializing the codebook: (1) Assign each training point to the nearest entry in the current codebook, and (2) update the codebook by replacing each entry with the centroid of <p> The word "nearest" is used in the Euclidean sense. It can be shown that the sum of the squared distances between training points and their nearest codebook entries does not increase as the algorithm progresses, and in fact converges (to some value) in a finite number of iterations <ref> [45, pp. 363-368] </ref>. Once the codebook has been designed, it can be used for mixture density estimation in a straightforward way. <p> However, the term "generalized Lloyd algorithm" may be more appropriate, for the reasons cited in <ref> [45, p. 362] </ref>. 44 the degree of improvement was usually slight, e.g., not more than about two tenths of a bit per pixel when applied to the lossless compression of natural images. <p> The main difficulty we encountered in running the GLA was the well-known problem of the assignment step occasionally resulting in one or more empty cells. Two methods described in <ref> [45] </ref> for handling this situation were tried. Both involved replacing the empty cell's codebook entry with a slightly perturbed version of another entry whose corresponding cell was deemed to be a good candidate for splitting into two. <p> Neither method worked perfectly in all cases, but the latter was found to be the more reliable of the two in getting the algorithm to use all of the allocated components. A partial distance search <ref> [45, pp. 479-480] </ref> technique was employed to expedite the assignment step, resulting in significant computational savings (typically, a factor of 2) for large M . <p> Two commonly used initialization strategies are random assignment to training points and LBG splitting <ref> [45] </ref>. We have found that random assignment often leads to empty cells, possibly because the initial clusters are occasionally insufficiently spread out. Traditional LBG splitting is appropriate when the number of clusters is a power of two, because each split doubles the number of clusters. <p> A method for taking advantage of this potential savings is now proposed. Computationally inexpensive discovery of which terms are insignificant may be accomplished in a manner analogous to the partial search mentioned briefly in Section 2.5. In that technique <ref> [45] </ref>, to speed up the search for the codebook entry or cluster nearest to a given training point, the partial Euclidean distance is tested after each coordinate's contribution is accumulated into the sum, and a candidate-nearest cluster is eliminated from further consideration as soon as it is discovered that the partial <p> The predictor really comes into play at low rates. In many (but not all) versions of predictive coding, the prediction is a linear combination of previously decoded values <ref> [45, 62] </ref>. Such prediction is termed linear. From this point of view, the prediction made by the proposed system is potentially highly nonlinear, since it can be an arbitrary functional of the estimated conditional distribution. <p> There is a vast amount of literature on lossy compression. Connections can be drawn between the proposed conjoint scalar quantization system and several variations on VQ, including predictive, multistage, and nonlinear interpolative VQ <ref> [45] </ref>. Perhaps the most relevant prior work is the conditional entropy-constrained VQ technique proposed by Chou and Lookabaugh [21]. In that technique, entropy coding of VQ codewords is carried out conditionally using a first-order Markov model estimated from codeword histograms (occurrence counts). <p> This is the part of the economized algorithm that effectively generalizes the partial distance method <ref> [45] </ref> of fast codebook lookup to the probabilistic setting, as alluded to in Section 2.6. 120 A remaining issue concerns those training points which are initially outliers for every cluster.
Reference: [46] <author> H. Gish and J.N. Pierce. </author> <title> Asymptotically efficient quantization. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:676-683, </volume> <month> September </month> <year> 1968. </year>
Reference-contexts: It is well-known that under an entropy constraint, uniformly spaced thresholds are asymptotically optimal for small levels of distortion with respect to several measures (including MSE), for a broad class of i.i.d. sources <ref> [46, 7] </ref>. Uniform spacing has also been found experimentally to be nearly optimal at all rates for several important sources and distortion measures [94]. Accordingly, we assume a uniform cell width of .
Reference: [47] <author> Robert M. Gray. </author> <title> Information rates of autoregressive processes. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-16:412-421, </volume> <year> 1970. </year>
Reference-contexts: It is useful to compare performance with Shannon's distortion-rate function. For a Gaussian autoregressive source of order m and assuming mean-squared distortion, this limiting R-D curve is given parametrically by the following expressions <ref> [47, 6] </ref>: D = 2 h 1 i and 1 Z max 0; 2 1 i where g (!) = fi m X a i e ji! fi 2 Varying from zero (corresponding to zero distortion) to infinity (zero rate) traces out the rate-distortion bound.
Reference: [48] <author> Robert M. Gray. </author> <title> Probability, Random Processes, and Ergodic Properties. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: A final complication arises when the order in which the marbles are observed is considered to be significant; that is, when successive observations are regarded as dependent. In such cases, it is natural to view the sequence of observations as constituting a sample path of a random process <ref> [48, 84] </ref>. In this framework, the marbles are assumed always to come out of a given box in the same fixed sequence, but the box itself is assumed to have been selected at random from a possibly infinite collection or ensemble of boxes, according to some probability distribution.
Reference: [49] <author> D.J. </author> <title> Hand. Kernel Discriminant Analysis. </title> <publisher> Research Studies Press, </publisher> <year> 1982. </year> <month> 127 </month>
Reference-contexts: Mixture models will be discussed in greater detail later, particularly in Sections 2.4 2.6. Finally, a model having a number of degrees of freedom that varies with the amount of training data is generally termed nonparametric. 5 An important example of such a model is the kernel density estimate <ref> [49, 120] </ref>, which in the scalar case is given by ^ f (z) = hjLj jLj X K h ; (2:2:5) where h &gt; 0 is called the bandwidth parameter (usually chosen to depend on jLj), and typically the kernel K is itself selected to be a valid PDF. <p> Estimate (c) is a mix ture density estimate also having 128 components, trained using the generalized Lloyd algorithm as described in Section 2.5. Kernel density estimation is an important technique with a well-developed supporting theory <ref> [34, 49, 120] </ref>, much of it focusing on the asymptotic properties as the training sequence becomes arbitrarily large [33].
Reference: [50] <author> M. Hassner and J. Sklansky. </author> <title> The use of Markov random fields as models of texture. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 12 </volume> <pages> 357-370, </pages> <year> 1980. </year>
Reference-contexts: In 1980, Kanal [66] summarized and reinterpreted the earlier approach of Abend et al., and traced the early development of Markov 98 random field (MRF) models for image processing. The explicit use of MRFs for textures, again using parametric conditional models, was proposed in 1980 by Hassner and Sklansky <ref> [50] </ref>. Much of the research in MRFs has focused on the problem of estimating model parameters; see for example the 1987 paper by Pickard [91].
Reference: [51] <author> Simon Haykin. </author> <title> Neural Networks: A Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <year> 1994. </year>
Reference-contexts: An alternative view of mixtures is as general-purpose semiparametric density estimators, not necessarily reflective of the structure of the data source [37, p. 118ff]. This interpretation, which is the one adopted here, has been steadily growing in popularity in recent years, particularly among researchers interested in artificial neural networks <ref> [51, 9] </ref>. However, the important distinction between density estimation and function approximation is often blurred in that literature; see [95] for a brief discussion.
Reference: [52] <author> David J. Heeger and James R. Bergen. </author> <title> Pyramid-based texture analysis/synthesis. </title> <booktitle> In Proceedings of the IEEE International Conference on Image Processing, </booktitle> <volume> volume 3, </volume> <pages> pages 648-651, </pages> <address> Washington, DC, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: However, the conditional models are not required to be of the simple parametric form assumed throughout the MRF literature. A texture synthesis method using pyramid structures has recently been proposed by Heeger and Bergen <ref> [53, 52] </ref>. In their approach, a texture is synthesized by iteratively transforming the marginal distributions of subbands in a steerable-pyramid decomposition [133] of a pseudorandom noise image to match those of a given texture sample. The marginal distributions are estimated by histograms. <p> The results obtained by Heeger and Bergen <ref> [52, 53] </ref> suggest that using different filters | specifically, oriented ones | may result in an improvement over the filtered synthesis results shown here while using relatively simple probabilistic models for the subbands.
Reference: [53] <editor> D.J. Heeger and J.R. Bergen. Pyramid-based texture analysis/synthesis. </editor> <booktitle> In Proceedings of SIGGRAPH '95, </booktitle> <pages> pages 229-238, </pages> <address> Los Angeles, CA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: However, the conditional models are not required to be of the simple parametric form assumed throughout the MRF literature. A texture synthesis method using pyramid structures has recently been proposed by Heeger and Bergen <ref> [53, 52] </ref>. In their approach, a texture is synthesized by iteratively transforming the marginal distributions of subbands in a steerable-pyramid decomposition [133] of a pseudorandom noise image to match those of a given texture sample. The marginal distributions are estimated by histograms. <p> The results obtained by Heeger and Bergen <ref> [52, 53] </ref> suggest that using different filters | specifically, oriented ones | may result in an improvement over the filtered synthesis results shown here while using relatively simple probabilistic models for the subbands.
Reference: [54] <author> E.G. Henrichon and K.S. Fu. </author> <title> On mode estimation in pattern recognition. </title> <booktitle> In Proceedings of the 7th Symposium on Adaptive Processes, pages 3-a-1. UCLA, </booktitle> <year> 1968. </year>
Reference-contexts: This issue aside, the main point to be made here is that the technique proposed by Shang [123] focuses on the joint rather than conditional density estimation problem, and seems to do so by what amounts to an adaptive-bin histogram. In 1968 Henrichon and Fu <ref> [54] </ref> proposed a recursive partitioning technique for mode separation which is similar in several respects to both CART and projection pursuit (see the final paragraph in this section), and therefore deserves mention.
Reference: [55] <author> Henry Holtzman. </author> <title> Increasing resolution using pyramid coding and pattern matching. </title> <note> Available by request from this author, </note> <month> May </month> <year> 1990. </year>
Reference-contexts: In recent years, there has been growing interest in taking advantage of the nonlinear statistical dependence among subband pixels <ref> [17, 25, 55, 89, 88, 122, 126] </ref>. Recall from Section 2.1 that a simple but effective traditional approach to subband image coding is to employ entropy-coded uniform-threshold scalar quantization on the subbands, maintaining the same stepsize when quantizing all of the pixels in all of the subbands. <p> By generating the higher-frequency input subbands in a way that is consistent with a statistical model that chacterizes interdependence both within and across subbands, realistic detail can be "invented" in the synthesized image. This general idea was first suggested by Pentland and explored by Holtzman <ref> [55] </ref>, who used implicit models determined by VQ codebooks to increase the resolution of natural images.
Reference: [56] <author> J.-Y. Huang and P.M. Schultheiss. </author> <title> Block quantization of correlated Gaussian random variables. </title> <journal> IEEE Transactions on Communications, </journal> <volume> CS-11:289-296, </volume> <month> September </month> <year> 1963. </year>
Reference-contexts: Subband-based lossy compression is termed subband coding. Subband coding was first applied to speech [62] in the seventies and extended to images in the mid-eighties [143]. Much of the underlying motivation and 17 theory carries over directly from an older technique called transform coding <ref> [56] </ref>. As mentioned above, a block transform is a special case of subband decomposition. The basic idea behind image subband coding is now reviewed briefly.
Reference: [57] <author> Peter J. Huber. </author> <title> Projection pursuit. </title> <journal> The Annals of Statistics, </journal> <volume> 13(2) </volume> <pages> 435-475, </pages> <year> 1985. </year>
Reference-contexts: An important nonparametric technique in multivariate density estimation that has not yet been mentioned is projection pursuit <ref> [57, 39] </ref>. In that technique, a multivariate (i.e., joint) density is expressed as a background density (which must be assumed) multiplied by the product of M univariate functions of linear combinations of the coordinates.
Reference: [58] <author> Toshihide Ibaraki and Naoki Katoh. </author> <title> Resource Allocation Problems : Algorithmic Approaches. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: In such situations, an optimal iterative rate allocation procedure can be used instead <ref> [58, 127] </ref>, but a simple formula for the improvement in MSE no longer exists in general. 18 Alternatively, rate can be allocated implicitly by entropy-coded uniform-threshold scalar quan-tization (see Section 2.3).
Reference: [59] <author> R.A. Jacobs, M.I. Jordan, and S.J. Nowlan. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1) </volume> <pages> 79-87, </pages> <year> 1991. </year>
Reference-contexts: Specifically, this occurs when the tail of a far-away large-variance component becomes greater than that of a nearby narrow-variance one. The same phenomenon occurs in the version of Jacob and Jordan's mixtures-of-experts architecture <ref> [59] </ref> proposed by Gershenfeld et al. [44]. Although consistent with the view of conditioning space consisting of multiple classes, each characterized by a Gaussian density, this type of weighting is inconsistent with the locality-of-experts motivation that underlies much of 112 the original mixture-of-experts framework.
Reference: [60] <author> Anil K. Jain. </author> <title> Fundamentals of Digital Image Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1989. </year>
Reference-contexts: Such a model is readily generalized to two-dimensional sequences by the notion of Markov random field, which has been used extensively in texture and image processing <ref> [60] </ref>. Relaxation of strict stationarity can be introduced in a controlled and extremely useful manner via the concept of the hidden Markov source [102] and its various generalizations, which have found application in speech processing, gesture recognition, and other important areas.
Reference: [61] <author> N. Jayant, J. Johnston, and R. Safranek. </author> <title> Signal compression based on models of human perception. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(10) </volume> <pages> 1385-1422, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: A frequency-weighted mean-square error criterion can be easily accommodated by varying the stepsizes across subbands, as is implicitly done in JPEG [101]. Perceptually optimized rate allocation has been considered by Jayant et al. <ref> [61] </ref> and Wu and Gersho [144]. As mentioned previously, subband pixels exhibit a great deal of statistical dependence despite their near-uncorrelatedness. This dependence may be exploited for coding gain in a number of ways [25].
Reference: [62] <author> Nuggehally S. Jayant and Peter Noll. </author> <title> Digital Coding of Waveforms : Principles and Applications to Speech and Video. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1984. </year>
Reference-contexts: For an orthogonal block transform, diagonalization of the covariance matrix occurs as a by-product of maximizing energy compaction subject to the constraint of constant total energy <ref> [62] </ref>. Therefore, to the extent that an orthogonal subband decomposition achieves energy compaction, pixels are uncorrelated across subbands. 16 Approximate uncorrelatedness within subbands can be easily understood in the frequency domain. <p> Subband coding of images Subband and subband-pyramid decomposition are often used in lossy compression. Subband-based lossy compression is termed subband coding. Subband coding was first applied to speech <ref> [62] </ref> in the seventies and extended to images in the mid-eighties [143]. Much of the underlying motivation and 17 theory carries over directly from an older technique called transform coding [56]. As mentioned above, a block transform is a special case of subband decomposition. <p> per-pixel quantization error for each subset can be approximated as * 2 i 2 2R i , where R i is the average rate allocated to subset i and * is a performance factor which depends on both the type of quantizer and on the marginal distribution of the pixels <ref> [62] </ref>. <p> In this case, the improvement in mean-square error (MSE) over quantizing the original pixels can be approximated by the ratio of the arithmetic to the geometric mean of the variances <ref> [62] </ref>. Neither (2:1:4) nor this formula for the improvement is valid in the important case where one of more of the variances are nearly zero, as this would result in negative rate allocations which would violate the assumption behind the use of Lagrange multipliers. <p> The predictor really comes into play at low rates. In many (but not all) versions of predictive coding, the prediction is a linear combination of previously decoded values <ref> [45, 62] </ref>. Such prediction is termed linear. From this point of view, the prediction made by the proposed system is potentially highly nonlinear, since it can be an arbitrary functional of the estimated conditional distribution.
Reference: [63] <author> Tony Jebara. </author> <title> Conditionally maximized density estimation for incomplete data via the CEM method. Class project report, MIT Course 9.390: </title> <institution> Computational Laboratory in Cognitive Science, </institution> <year> 1997. </year>
Reference-contexts: Its application to large mixtures should therefore be investigated, and the results compared with those obtained with PCDT. Recently, Tony Jebara has proposed a fixed-point maximization technique <ref> [63, 64] </ref> which seems to show promise for conditional density estimation, again primarily in the case of low-complexity mixtures. The development mirrors that of ordinary EM [31], but the conditional density is substituted for the joint density when lower-bounding the per-iteration likelihood improvement via Jensen's inequality. <p> The method has been applied in the context of computer vision to learn a mapping between facial feature locations and image data, yielding results superior to those obtained using EM-estimated joint densities <ref> [63] </ref>. However, convergence even to a local optimum is not reliable; typically several initializations must be tried before convergence is obtained. It is not known whether convergence can always be obtained by repeatedly retrying with a different initialization.
Reference: [64] <author> Tony Jebara and Kris Popat. </author> <title> Methods for conditionally maximized likelihood density estimation. </title> <type> Technical report, </type> <institution> M.I.T. Media Laboratory, </institution> <year> 1997. </year> <note> In preparation. </note>
Reference-contexts: Its application to large mixtures should therefore be investigated, and the results compared with those obtained with PCDT. Recently, Tony Jebara has proposed a fixed-point maximization technique <ref> [63, 64] </ref> which seems to show promise for conditional density estimation, again primarily in the case of low-complexity mixtures. The development mirrors that of ordinary EM [31], but the conditional density is substituted for the joint density when lower-bounding the per-iteration likelihood improvement via Jensen's inequality.
Reference: [65] <author> Michael I. Jordan. </author> <title> private communication. </title> <publisher> MIT, </publisher> <month> April </month> <year> 1997. </year>
Reference-contexts: It is therefore not clear what expectation should be computed in the E-step. A number of ad hoc modifications to the E-step have been tried in joint work with Tony Jebara with limited success; see Section 3.9. Based on these experiences and on discussions with Michael Jordan <ref> [65] </ref>, it seems to me unlikely that a completely satisfactory EM algorithm for conditional-density mixture estimation will be found, but more research is required. A plausible approach to the conditional-density mixture estimation problem is suggested if we take a step back and reconsider the problem once again in abstract terms.
Reference: [66] <editor> Laveen N. Kanal. </editor> <title> Markov mesh models. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 12 </volume> <pages> 371-375, </pages> <year> 1980. </year>
Reference-contexts: In 1965 Abend et al. [1] applied such models to binary images. Besag [8] studied consistency requirements to allow the use of conditional rather than joint probability models to specify a two-dimensional stochastic process, and considered several exponential-family parametric conditional models. In 1980, Kanal <ref> [66] </ref> summarized and reinterpreted the earlier approach of Abend et al., and traced the early development of Markov 98 random field (MRF) models for image processing. The explicit use of MRFs for textures, again using parametric conditional models, was proposed in 1980 by Hassner and Sklansky [50].
Reference: [67] <author> R. L. Kashyap. </author> <title> Characterization and estimation of two-dimensional ARMA models. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> IT-30(5):736-745, </volume> <year> 1984. </year> <month> 128 </month>
Reference-contexts: A simultaneous autoregressive (SAR) model <ref> [67] </ref> was trained on each of these using a 21 fi 21 estimation window. The resulting SAR model was then used to synthesize the texture shown in the middle column. The order of the SAR model was 12, corresponding to a 5 fi 5 neighborhood. <p> The neighborhood shown in Figure 4.1.2 (d) was used, so the conditioning order of the model was 10. For comparison, also shown is the result of a 12 th order simultaneous autoregressive model 18 (SAR) synthesis <ref> [67] </ref> which corresponds to a filtering of white noise. The proposed nonlinear technique based on high-order conditional density estimation is able to capture much more of the characteristics of the textures that are visually important than is the SAR model.
Reference: [68] <author> Glen G. Langdon. </author> <title> An introduction to arithmetic coding. </title> <institution> IBM J. Res. Develop., </institution> <month> Mar. </month> <year> 1984. </year>
Reference-contexts: This is particularly easy to show in the case of lossless compression of a sequence of independent observations of a discrete-valued random vector Z having PMF P (Z). The number of bits produced by an arithmetic coder <ref> [68, 112] </ref> is likely to be close to the ideal Shannon code length of n=1 ^ P (z (n) ) (2:3:3) bits, which is simply the negative log likelihood.
Reference: [69] <author> Glen G. Langdon and Jorma J. Rissanen. </author> <title> Compression of black-white images with arithmetic coding. </title> <journal> IEEE Trans. Comm., </journal> <volume> COM-29:858-867, </volume> <month> June </month> <year> 1981. </year>
Reference-contexts: The PMF can be conditioned on anything that the decoder will have access to prior to the time of decoding. One of the first published applications of arithmetic coding was a scheme for lossless compression of binary images <ref> [69] </ref>, which is depicted in Figure 4.1.1. Pixels (which can be either black or white) are scanned in raster order and fed to an arithmetic coder, along with a corresponding estimate of the conditional PMF. <p> Several variations of such a count-based estimation procedure are described in <ref> [69] </ref>. Count-based PMF estimation is not appropriate for greyscale images, because the number of conditioning states becomes prohibitively large for even moderate-size neighborhoods. Also, that method of estimation when applied to scalar observations does not exploit the relationship between nearby amplitude levels.
Reference: [70] <author> S.P. Lloyd. </author> <title> Least squares quantization in PCM. </title> <institution> Unpublished Bell Laboratories Technical Note, </institution> <year> 1957. </year> <note> Reprinted in IEEE Transactions on Information Theory, vol. IT-28, </note> <month> March </month> <year> 1982, </year> <pages> pp. 127-135., </pages> <year> 1957. </year>
Reference-contexts: If i r such attempts fail, then the Lloyd algorithm is aborted, and the complexity 15 Not "generalized" here because it is applied to one-dimensional data, which was considered by Lloyd <ref> [70] </ref>. 66 for a detailed description and for the parameters values used in most of the experiments reported in this thesis. search is also terminated. Usually, i r is taken to be 10.
Reference: [71] <author> Tom D. Lookabaugh and Robert M. Gray. </author> <title> High-resolution quantization theory and the vector quantizer advantage. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> IT-35:1020-1033, </volume> <year> 1989. </year>
Reference-contexts: In data compression, for example, the advantage of joint processing results from the ability to exploit dependence among the data, the ability to adapt to the distribution of the data, and the ability to fill space efficiently with representative points <ref> [45, 71] </ref>. It is generally much easier and often more natural to process data sequentially than jointly. It is of interest, therefore, to find ways of making the performance of sequential processing approach that of joint processing to the extent possible. <p> The ability to model high-order conditional densities effectively allows a system to quantize and encode pixels sequentially while still retaining some (though not all) of the advantages of processing the pixels jointly. Specifically, the spatial interdepence among pixels can be exploited, as can the shape of their joint distribution <ref> [71] </ref>.
Reference: [72] <author> David G. Luenberger. </author> <title> Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: Direct solution of the mixture likelihood equations appears to be hopelessly intractable, even for relatively small mixtures in low dimensions [104]. A possible numerical approach is gradient search <ref> [72] </ref>, but the gradients involved are complex enough to make this technique computationally unattractive for large M . 2.6 Mixture Refinement via Expectation-Maximization The EM algorithm [9, 31, 104] provides an alternative gradient-free means of locally maximizing the likelihood over the mixture parameters, and has come to be a standard method
Reference: [73] <author> Mark R. Luettgen. </author> <title> Image Processing with Multiscale Stochastic Models. </title> <type> PhD thesis, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Also described there are techniques that we might call conjoint, except that, as far as I am aware, the conditional distribution is either modeled only implicitly as in [126], or explicitly but using fairly restrictive models as in <ref> [73] </ref>. It would be interesting to see whether an explicit, powerful conditional density model such as that described in Chapter 3 might be used advantageously in such systems. This question is considered in Chapter 5.
Reference: [74] <author> Gabor Lugosi and Andrew Nobel. </author> <title> Consistency of data-driven histogram methods for density estimation and classification. </title> <type> Technical Report UIUC-BI-93-01, </type> <institution> Beckman Institute, University of Illinois at Urbana-Champaign, </institution> <year> 1993. </year>
Reference-contexts: First, the density estimation techniques to be discussed involve complex, data-driven procedures such as tree-growing and -pruning, for which a theoretical analysis would be difficult, though considerable progress has been made in this general area | see the recent work by Lugosi and Nobel <ref> [74] </ref>, Nobel [81], Nobel and Olshen [82] and Devroye et al. [34, Chapters 20-23]. Second, the techniques and analyses to be presented were arrived at through practically motivated, somewhat heuristic reasoning, and it is desirable from a pedagogical standpoint not to obscure this path through post hoc theorizing. <p> Even if there is not, it may be possible to find some sort of kernel estimate which shares the fast recomputability property of the histogram. It was mentioned in Chapter 2 that there has been significant progress recently by Lugosi and Nobel <ref> [74] </ref>, Nobel [81], Nobel and Olshen [82] and Devroye et al. [34, Chapters 20-23] in establishing various asymptotic properties of tree-structured and other data-driven statistical techniques.
Reference: [75] <author> Stephen P. Luttrell. </author> <title> The partitioned mixture distribution: multiple overlapping density models. </title> <booktitle> In Proceedings of the 14th International Maximum Entropy Workshop, </booktitle> <address> Malvern, U.K., </address> <year> 1994. </year> <institution> Defense Research Agency. </institution> <type> Preprint. </type>
Reference-contexts: The first, dubbed partitioned mixture distributions, pertains potentially to any situation wherein a large array of separate mixture models are required to be represented simultaneously. Specifically, in <ref> [75] </ref>, a scheme is presented whereby the individual mixtures are made to share structure (interpreting the mixtures as three-layer neural networks), thereby saving representation cost and presumably improving training reliability (at some expense of modeling flexibility).
Reference: [76] <author> Geoffrey J. McLachlan and Kaye E. Basford. </author> <title> Mixture Models: Inference and Applications to Clustering. </title> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Historically, the main use of mixture models has been in classification and clustering, wherein the structure of the model is regarded as reflective of the physical process by which the data were generated or observed <ref> [76] </ref>. An alternative view of mixtures is as general-purpose semiparametric density estimators, not necessarily reflective of the structure of the data source [37, p. 118ff]. <p> Henceforth, mixtures components will be assumed to be separable Gaussian densities unless otherwise stated. Estimating the mixture components and mixing probabilities is in general a complex task. Several approaches have been proposed over the years <ref> [35, 40, 76, 104, 135] </ref>, and the topic continues to be an active area of research today. Much of the work has focused on low-complexity mixtures. When mixtures of high complexity are required or allowed, as they are here, robustness and computational complexity of the training process become important considerations.
Reference: [77] <author> N. Merhav and M. Feder. </author> <title> Universal schemes for sequential decision from individual data sequences. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(4) </volume> <pages> 1280-1292, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: is slightly ironic that a notion of a true governing probability law is unnecessary for the term "density estimation" to be meaningful, but perhaps no more so than in those other situations in inference and coding that can be approached without positing a true distribution or even a probability space <ref> [28, 30, 77, 108] </ref>. 2.4 Histogram, Kernel, and Mixture Density Models A probabilistic model should both generalize and summarize the training sequence.
Reference: [78] <author> Eric Metois. </author> <title> Musical Sound Information: Musical gesture and Embedding synthesis. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: Recently, Judith Brown [15] adapted our conjoint texture classification technique [96] to the problem of identifying musical instruments from sound samples, with encouraging results [16]. Eric Metois <ref> [78] </ref> and Bernd Schoner [119] trace the development of their Cluster-Weighted Modeling work [44] with Neil Gershenfeld to ideas gleaned from our papers on the 116 application of mixture models to high-order conditional density estimation for texture synthesis [96] and compression [98]. 117 APPENDIX A Economized EM Algorithm for Mixture Estimation
Reference: [79] <author> James R. Munkres. </author> <title> Topology: A First Course. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1975. </year>
Reference-contexts: This relationship is indicated symbolically as t &lt; t 0 . Note that the root node is an ancestor of every other node, and that the ancestor relation &lt; is a strict partial order on T (see <ref> [79] </ref>, p. 69ff). The depth of a node is defined to be the length of the path from the root to that node. A finite complete K-ary tree in which all of the leaves have the same depth is termed balanced.
Reference: [80] <author> S. Na and D.L. Neuhoff. </author> <title> Bennett's integral for vector quantizers. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 41(4) </volume> <pages> 886-900, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: For tractability, we further assume that there are infinitely many cells, and that the representation levels are the midpoints of the cells. The latter assumption incurs some loss in MSE-performance, generally significant only at low rates. If is small and if f is continuous and smooth (see <ref> [18, 80] </ref> for rigorous statements of these conditions), the quantization error at high rates will be nearly uniform on the interval [=2; =2], so that the MSE will be very close to 2 =12, largely independent of the actual rate. Approximations of this type were first developed by Bennett [5].
Reference: [81] <author> Andrew Nobel. </author> <title> Histogram regression estimation using data-dependent partitions. </title> <type> Technical Report UIUC-BI-94-01, </type> <institution> Beckman Institute, University of Illinois at Urbana-Champaign, </institution> <year> 1994. </year>
Reference-contexts: First, the density estimation techniques to be discussed involve complex, data-driven procedures such as tree-growing and -pruning, for which a theoretical analysis would be difficult, though considerable progress has been made in this general area | see the recent work by Lugosi and Nobel [74], Nobel <ref> [81] </ref>, Nobel and Olshen [82] and Devroye et al. [34, Chapters 20-23]. Second, the techniques and analyses to be presented were arrived at through practically motivated, somewhat heuristic reasoning, and it is desirable from a pedagogical standpoint not to obscure this path through post hoc theorizing. <p> Even if there is not, it may be possible to find some sort of kernel estimate which shares the fast recomputability property of the histogram. It was mentioned in Chapter 2 that there has been significant progress recently by Lugosi and Nobel [74], Nobel <ref> [81] </ref>, Nobel and Olshen [82] and Devroye et al. [34, Chapters 20-23] in establishing various asymptotic properties of tree-structured and other data-driven statistical techniques. Typically, the proofs involved in demonstrating such properties require that the cells in the partition shrink to zero volume as the sample size gets large.
Reference: [82] <author> Andrew Nobel and Richard Olshen. </author> <title> Termination and continuity of greedy growing for tree-structured vector quantizers. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42(1) </volume> <pages> 191-205, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: First, the density estimation techniques to be discussed involve complex, data-driven procedures such as tree-growing and -pruning, for which a theoretical analysis would be difficult, though considerable progress has been made in this general area | see the recent work by Lugosi and Nobel [74], Nobel [81], Nobel and Olshen <ref> [82] </ref> and Devroye et al. [34, Chapters 20-23]. Second, the techniques and analyses to be presented were arrived at through practically motivated, somewhat heuristic reasoning, and it is desirable from a pedagogical standpoint not to obscure this path through post hoc theorizing. <p> Even if there is not, it may be possible to find some sort of kernel estimate which shares the fast recomputability property of the histogram. It was mentioned in Chapter 2 that there has been significant progress recently by Lugosi and Nobel [74], Nobel [81], Nobel and Olshen <ref> [82] </ref> and Devroye et al. [34, Chapters 20-23] in establishing various asymptotic properties of tree-structured and other data-driven statistical techniques. Typically, the proofs involved in demonstrating such properties require that the cells in the partition shrink to zero volume as the sample size gets large.
Reference: [83] <author> Bruno Olshausen. </author> <title> private communication. </title> <publisher> MIT Media Lab, </publisher> <month> July </month> <year> 1996. </year>
Reference-contexts: It would not be quite enough to match the histogram in the subbands prior to synthesis; an additional requirement imposed in their method is that re-analysis into subbands maintain that marginal distribution. As suggested to me by Olshausen <ref> [83] </ref>, this may have the effect of forcing the subband phases to line up to match the original phase after synthesis as well.
Reference: [84] <author> Athanasios Papoulis. </author> <title> Probability, Random Variables, and Stochastic Processes. </title> <publisher> McGraw-Hill, </publisher> <address> 129 New York, </address> <year> 1991. </year>
Reference-contexts: To show that the correlation of pixels in the subband is nearly zero, it is sufficient to show that after subsampling, the magnitude spectrum is nearly flat across the whole spectrum <ref> [84] </ref>. <p> A final complication arises when the order in which the marbles are observed is considered to be significant; that is, when successive observations are regarded as dependent. In such cases, it is natural to view the sequence of observations as constituting a sample path of a random process <ref> [48, 84] </ref>. In this framework, the marbles are assumed always to come out of a given box in the same fixed sequence, but the box itself is assumed to have been selected at random from a possibly infinite collection or ensemble of boxes, according to some probability distribution.
Reference: [85] <author> Emanuel Parzen. </author> <title> Modern Probability Theory and its Applications. </title> <publisher> Wiley, </publisher> <year> 1960. </year>
Reference-contexts: Fisher's celebrated maximum likelihood (ML) principle gives the proportions as 9=10 for red, 1=10 for blue, and zero for any other color. To be useful in practice, this principle must usually be modified; often this is accomplished by adding a penalty term [129, 130]. Laplace's rule of succession <ref> [85] </ref> requires that you know the possible colors beforehand; if there are (say) five of them, then his rule results in the estimated proportions 2=3 for red, 2=15 for blue, and 1=15 for each of the three remaining unseen colors. <p> The choice ~ = 1 corresponds to a straightforward generalization of Laplace's rule of succession <ref> [85] </ref> to the multinomial setting, while the choice ~ = 1=2 corresponds to what has come to be known as the Krichevski-Trofimov estimator in the universal coding literature [142].
Reference: [86] <author> R. </author> <title> Pasco. Source coding algorithms for fast data compression. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1976. </year>
Reference-contexts: Thus, the problem of designing a lossless compressor is actually equivalent to that of estimating the governing probability law. This was long known to be true in principle, but became true in practice primarily through the advent of arithmetic coding in the late 1970s <ref> [113, 86] </ref>.
Reference: [87] <author> W.B. Pennebaker and J.L. Mitchell. </author> <title> Probability estimation for the q-coder. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 32(6) </volume> <pages> 737-752, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: The probability model ^ P for the arithmetic coder makes use of the numerical meaning of the quantizer output, which is an important feature not usually achieved by models used with arithmetic coding (e.g., that employed by the Q-coder <ref> [87] </ref>). The true function of the predictor is a bit subtle here. See the main text for an explanation. uniformly spaced representation levels, as is henceforth assumed, the simulation of quantization can be accomplished by adding i.i.d. noise uniformly distributed on [=2; =2] to y [n].
Reference: [88] <author> Alex Pentland. </author> <title> Fractal-based image compression and interpolation. </title> <type> U.S. Patent No. </type> <institution> 5,148,497, </institution> <year> 1992. </year>
Reference-contexts: In recent years, there has been growing interest in taking advantage of the nonlinear statistical dependence among subband pixels <ref> [17, 25, 55, 89, 88, 122, 126] </ref>. Recall from Section 2.1 that a simple but effective traditional approach to subband image coding is to employ entropy-coded uniform-threshold scalar quantization on the subbands, maintaining the same stepsize when quantizing all of the pixels in all of the subbands.
Reference: [89] <author> Alex Pentland and Bradley Horowitz. </author> <title> A practical approach to fractal-based image compression. </title> <booktitle> In Proc. IEEE Data Comp. Conf., </booktitle> <address> Utah, </address> <year> 1991. </year>
Reference-contexts: In recent years, there has been growing interest in taking advantage of the nonlinear statistical dependence among subband pixels <ref> [17, 25, 55, 89, 88, 122, 126] </ref>. Recall from Section 2.1 that a simple but effective traditional approach to subband image coding is to employ entropy-coded uniform-threshold scalar quantization on the subbands, maintaining the same stepsize when quantizing all of the pixels in all of the subbands.
Reference: [90] <author> K.O Perlmutter, S.M. Perlmutter, R.M. Gray, R.A. Olshen, and K.L. Oehler. </author> <title> Bayes risk weighted vector quantization with posterior estimation for image compression and classification. </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> 5(2) </volume> <pages> 347-360, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Recently, class probability trees have been adapted for use in a system that simultaneously performs compression 75 and classification by Perlmutter et al. <ref> [90] </ref>, where the criterion adopted was a weighted sum of Bayes risk and MSE on the observations. The Context algorithm for universal lossless coding proposed by Rissanen in [107] is similar to the CART class-probability tree, but is adaptive (i.e., "on-line").
Reference: [91] <author> David K. Pickard. </author> <title> Inference for discrete Markov fields: the simplest nontrivial case. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 82(397) </volume> <pages> 90-96, </pages> <year> 1987. </year>
Reference-contexts: The explicit use of MRFs for textures, again using parametric conditional models, was proposed in 1980 by Hassner and Sklansky [50]. Much of the research in MRFs has focused on the problem of estimating model parameters; see for example the 1987 paper by Pickard <ref> [91] </ref>. The texture synthesis methods described here fit into the Markov random mesh framework in the sense that textures are generated according to locally conditioned probability models. However, the conditional models are not required to be of the simple parametric form assumed throughout the MRF literature.
Reference: [92] <author> Ashok Popat, Wei Li, and Murat Kunt. </author> <title> Numerical design of parallel multiresolution filter banks for image coding applications. </title> <booktitle> In Proceedings of the SPIE 1991 International Symposium on Optical Applied Science and Engineering, </booktitle> <volume> volume 1567, </volume> <pages> pages 341-353, </pages> <address> San Diego, California, </address> <month> July </month> <year> 1991. </year> <booktitle> SPIE-the International Society for Optical Engineering, SPIE. </booktitle>
Reference-contexts: A jointly localizing, quasi-perfect-reconstruction, critically sampled analysis filter bank designed using the method described in <ref> [92] </ref> was used. The non-DC bands have been shifted and scaled in amplitude for display. Strong spatial and spectral interdependence of subband pixels is evident, yet the empirical correlation is nearly zero among pairs of non-DC pixels for a variety of displacements, both within and across subbands. <p> An alternative joint-localization criterion directly applicable to the discrete case was proposed by Slepian [134], and one we specifically devised for the design of nonuniform filterbanks for image processing was proposed in <ref> [92] </ref>. The latter criterion is quite complex in its generality, but simplifies greatly in the case of a uniform two-band orthogonal filter bank, becoming a weighted sum of the spatial and spectral second central moments. A numerical procedure for designing such filter banks is summarized below. <p> The symmetry constraint (B.2) is necessary for the filters to have zero phase, which is generally desirable in image processing applications. Since the gradient of (B.5) is readily computable, the required optimization search is straightforward. The adaptive hill-descending technique described in <ref> [92] </ref> was used to obtain filter banks having the low-pass analysis impulse responses tabulated in Table B.6. These responses are also presented graphically in Figure 5.2.2, while the corresponding magnitude-frequency responses are shown in Figure 5.2.1.
Reference: [93] <author> Ashok Popat, Andre Nicoulin, Andrea Basso, Wei Li, and Murat Kunt. </author> <title> Subband coding of video using energy-adaptive arithmetic coding and statistical feedback-free rate control. </title> <booktitle> In Proceedings of the Visual Communications and Image Processing '91 Conference, </booktitle> <volume> volume 1605, </volume> <pages> pages 940-953, </pages> <address> Boston, Massachusetts, </address> <month> November </month> <year> 1991. </year> <booktitle> SPIE-the International Society for Optical Engineering, SPIE. </booktitle>
Reference-contexts: Using the same stepsize for all subband pixels can be shown to result in a nearly optimal implicit rate allocation with respect to mean-square error <ref> [42, 93] </ref>. A frequency-weighted mean-square error criterion can be easily accommodated by varying the stepsizes across subbands, as is implicitly done in JPEG [101]. Perceptually optimized rate allocation has been considered by Jayant et al. [61] and Wu and Gersho [144]. <p> Subband regions of high activity or variance will cause the quantizer to have greater output entropy, resulting in greater rate being implicitly allocated to those regions and subbands. As mentioned in Section 2.1, it can be shown that such a simple strategy results in a nearly MSE-optimal rate allocation <ref> [42, 93] </ref>, without the need for a computationally intensive rate-allocation procedure. Of course this assumes that the probabilistic model upon which the entropy coding is performed is accurate.
Reference: [94] <author> Ashok C. Popat. </author> <title> Scalar quantization with arithmetic coding. </title> <type> Master's thesis, </type> <institution> Dept. of Elec. Eng. and Comp. Science, M.I.T., </institution> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: Uniform spacing has also been found experimentally to be nearly optimal at all rates for several important sources and distortion measures <ref> [94] </ref>. Accordingly, we assume a uniform cell width of . For tractability, we further assume that there are infinitely many cells, and that the representation levels are the midpoints of the cells. The latter assumption incurs some loss in MSE-performance, generally significant only at low rates.
Reference: [95] <author> Kris Popat and Rosalind Picard. </author> <title> Cluster-based probability model and its application to image and texture processing. </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> 6(2) </volume> <pages> 268-284, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: This interpretation, which is the one adopted here, has been steadily growing in popularity in recent years, particularly among researchers interested in artificial neural networks [51, 9]. However, the important distinction between density estimation and function approximation is often blurred in that literature; see <ref> [95] </ref> for a brief discussion. <p> It has been found that slightly better performance (likelihood) often results by setting ^ 2 m;d to be slightly larger than the sample variance; see <ref> [95, Appendix B] </ref> for a possible explanation. The GLA was the real "work horse" algorithm for most of our earlier work in applying mixture models to texture and image processing applications [96, 97, 95]. <p> The GLA was the real "work horse" algorithm for most of our earlier work in applying mixture models to texture and image processing applications <ref> [96, 97, 95] </ref>. Later, an improvement was gener ally obtained in all applications by following the GLA with a few iterations of the computationally more expensive expectation-maximization (EM) algorithm (to be described in Section 2.6), but 10 In the clustering literature, this algorithm has come to be known as k-means.
Reference: [96] <author> Kris Popat and Rosalind W. </author> <title> Picard. A novel cluster-based probability model for texture synthesis, classification, and compression. </title> <booktitle> In Proc. SPIE Visual Communications '93, </booktitle> <pages> pages 756-768, </pages> <address> Cambridge, Mass., </address> <year> 1993. </year>
Reference-contexts: The GLA was the real "work horse" algorithm for most of our earlier work in applying mixture models to texture and image processing applications <ref> [96, 97, 95] </ref>. Later, an improvement was gener ally obtained in all applications by following the GLA with a few iterations of the computationally more expensive expectation-maximization (EM) algorithm (to be described in Section 2.6), but 10 In the clustering literature, this algorithm has come to be known as k-means. <p> The effect is that this is perpetuated through all the subsequent levels of synthesis, resulting in the anomalies in the figure. We first presented the above methods of texture synthesis in 1993 using a discretized mixture of Gaussians trained by the GLA <ref> [96] </ref>. The results shown here use a continuous model trained 94 by EM, allowing less complex models for comparable visual quality in the multiresolution case, and resulting in noticeably better visual quality in the single-resolution case. <p> Application of our multiresolution method of texture synthesis to sound textures in joint work with Nicolas Saint-Arnaud [115] has already been noted in Chapter 5. Recently, Judith Brown [15] adapted our conjoint texture classification technique <ref> [96] </ref> to the problem of identifying musical instruments from sound samples, with encouraging results [16]. <p> Eric Metois [78] and Bernd Schoner [119] trace the development of their Cluster-Weighted Modeling work [44] with Neil Gershenfeld to ideas gleaned from our papers on the 116 application of mixture models to high-order conditional density estimation for texture synthesis <ref> [96] </ref> and compression [98]. 117 APPENDIX A Economized EM Algorithm for Mixture Estimation This appendix presents a computationally economized, approximate version of the expectation-maximization (EM) algorithm for mixture density estimation, based on the standard version presented in Section 2.6.
Reference: [97] <author> Kris Popat and Rosalind W. </author> <title> Picard. Cluster-based probability model applied to image restoration and compression. </title> <booktitle> In ICASSP-94: 1994 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 381-384, </pages> <address> Adelaide, Australia, </address> <month> April </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: The GLA was the real "work horse" algorithm for most of our earlier work in applying mixture models to texture and image processing applications <ref> [96, 97, 95] </ref>. Later, an improvement was gener ally obtained in all applications by following the GLA with a few iterations of the computationally more expensive expectation-maximization (EM) algorithm (to be described in Section 2.6), but 10 In the clustering literature, this algorithm has come to be known as k-means.
Reference: [98] <author> Kris Popat and Rosalind W. </author> <title> Picard. Exaggerated consensus in lossless image compression. </title> <booktitle> In ICIP-94: 1994 International Conference on Image Processing, </booktitle> <pages> pages 846-850, </pages> <address> Austin, TX, </address> <month> Nov. </month> <year> 1994. </year> <journal> IEEE. </journal> <volume> 130 </volume>
Reference-contexts: This would be another means for obtaining 78 a high-order conditional density estimate that would be interesting to compare performance-wise to PCDT. A philosophically similar technique, without the hierarchical component, was proposed independently in <ref> [98] </ref>. Recently the same idea has been developed and analyzed extensively in the discrete case by Christensen et al. [24]; its extension to the continuous case, where its performance with PCDT could be compared, is a topic for possible future research. <p> Eric Metois [78] and Bernd Schoner [119] trace the development of their Cluster-Weighted Modeling work [44] with Neil Gershenfeld to ideas gleaned from our papers on the 116 application of mixture models to high-order conditional density estimation for texture synthesis [96] and compression <ref> [98] </ref>. 117 APPENDIX A Economized EM Algorithm for Mixture Estimation This appendix presents a computationally economized, approximate version of the expectation-maximization (EM) algorithm for mixture density estimation, based on the standard version presented in Section 2.6.
Reference: [99] <author> Kris Popat and Kenneth Zeger. </author> <title> Robust quantization of memoryless sources using dispersive FIR filters. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 40(11) </volume> <pages> 1670-1674, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Surprisingly, it can achieve objective coding gain in its intended domain of applicability. 5.1 One-band Subband Coding The one-band subband coding system shown in Figure 5.1.1 was first proposed as a means for robust quantization in <ref> [99] </ref>.
Reference: [100] <author> P. Pudil, J. Novovicova, and J. Kittler. </author> <title> Simultaneous learning of decision rules and important attributes for classification problems in image analysis. </title> <journal> Image and Vision Computing, </journal> <volume> 12(3) </volume> <pages> 193-198, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: These considerations suggest that for efficient density estimation in the classification setting, the estimation criteria should target discrimination rather than representation. A good example of this is the work by Pudil et al. <ref> [100] </ref>, wherein the dimensions providing the greatest discriminatory power are modeled separately for each class, while those dimensions with less discriminatory power are modeled by a common "background" distribution. 38 In concluding this section, it is worthwhile to compare the ontological status of f with that of ^ f .
Reference: [101] <author> Majid Rabbani and Paul W. Jones. </author> <title> Digital Image Compression Techniques. </title> <publisher> SPIE Optical Engineering Press, Bellingham, </publisher> <address> Washington, </address> <year> 1991. </year>
Reference-contexts: Using the same stepsize for all subband pixels can be shown to result in a nearly optimal implicit rate allocation with respect to mean-square error [42, 93]. A frequency-weighted mean-square error criterion can be easily accommodated by varying the stepsizes across subbands, as is implicitly done in JPEG <ref> [101] </ref>. Perceptually optimized rate allocation has been considered by Jayant et al. [61] and Wu and Gersho [144]. As mentioned previously, subband pixels exhibit a great deal of statistical dependence despite their near-uncorrelatedness. This dependence may be exploited for coding gain in a number of ways [25].
Reference: [102] <author> Lawrence R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-286, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Such a model is readily generalized to two-dimensional sequences by the notion of Markov random field, which has been used extensively in texture and image processing [60]. Relaxation of strict stationarity can be introduced in a controlled and extremely useful manner via the concept of the hidden Markov source <ref> [102] </ref> and its various generalizations, which have found application in speech processing, gesture recognition, and other important areas. Interestingly, hidden Markov models were discussed in the context of source coding by Gallager [41, Chapter 3] more than a decade before they became popular in speech processing.
Reference: [103] <author> Ronald H. Randles and Douglas A. Wolfe. </author> <title> Introduction to the Theory of Nonparametric Statistics. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: No attempt will be made to prove the latter conjecture however. 3.9 Discussion of New Work Relative to Prior Work Semiparametric and nonparametric statistical testing traces its roots to work done by Wilcoxon, Mann, and Whitney in the 1940's, as recounted in <ref> [103] </ref>; however, application of the ideas to multi-variate density estimation became conceivable only with the advent of powerful digital computers in the 1960's. The most popular existing techniques have already been discussed in the early sections of this chapter.
Reference: [104] <author> Richard A. Redner and Homer F. Walker. </author> <title> Mixture densities, maximum likelihood, and the EM algorithm. </title> <journal> SIAM Review, </journal> <volume> 26(2) </volume> <pages> 195-239, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: Henceforth, mixtures components will be assumed to be separable Gaussian densities unless otherwise stated. Estimating the mixture components and mixing probabilities is in general a complex task. Several approaches have been proposed over the years <ref> [35, 40, 76, 104, 135] </ref>, and the topic continues to be an active area of research today. Much of the work has focused on low-complexity mixtures. When mixtures of high complexity are required or allowed, as they are here, robustness and computational complexity of the training process become important considerations. <p> Direct solution of the mixture likelihood equations appears to be hopelessly intractable, even for relatively small mixtures in low dimensions <ref> [104] </ref>. <p> A possible numerical approach is gradient search [72], but the gradients involved are complex enough to make this technique computationally unattractive for large M . 2.6 Mixture Refinement via Expectation-Maximization The EM algorithm <ref> [9, 31, 104] </ref> provides an alternative gradient-free means of locally maximizing the likelihood over the mixture parameters, and has come to be a standard method of parameter estimation in mixtures and related models. <p> This soft assignment constitutes what is called the expectation step. The other step, maximization, involves re-estimating all of the parameters. Let f ^ P (i) (i) 2 (i) m;d g denote the mixture parameters at the beginning of the i th iteration. The EM update rule <ref> [104, 9] </ref> can be written as m = (i) Q D z ^ f mix (z d j ^ m;d ; ^ m;d ) m 0 =1 (i) Q D z ^ f mix (z d j ^ m 0 ;d ; ^ m 0 ;d ) ^ m;d = n=1 <p> However, when this is done, it is no longer clear precisely what criterion EM is attempting to optimize. A completely satisfactory treatment of this problem does not appear in the literature, and in light of the comments in Section 3 of <ref> [104] </ref> regarding the conceptual and practical difficulties associated with ML mixture estimation, perhaps one should not be expected. The approach settled upon in this thesis was pragmatic.
Reference: [105] <author> Nicholas Rescher. </author> <title> Induction, an Essay on the Justification of Inductive Reasoning. </title> <institution> University of Pittsburgh Press, </institution> <year> 1980. </year>
Reference-contexts: The term "random object" will be defined later in this section. Density estimation is an example of a problem in inductive inference, and inherits all of the well-known foundational difficulties attendant to that class of problem <ref> [23, 105, 136, 139] </ref>. In particular, induction is not deduction: it is not possible on the basis of observing a finite sample to show analytically that a particular density estimate is correct or even reasonable, unless strong assumptions are made that reach beyond the information present in the available data.
Reference: [106] <author> Jorma J. Rissanen. </author> <title> Modeling by shortest desciption length. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: This particular version of MDL assumes a simple two-part model/message lossless encoding of the observed data, and corresponds to Rissanen's earliest work on MDL <ref> [106] </ref>. His subsequent work generalizes the MDL principle considerably, removing the strict model/data separation, and allowing variable precision representation of the model param 1 In other words, proportional to the cardinality of the type class [26]. 20 eters [110].
Reference: [107] <author> Jorma J. Rissanen. </author> <title> A universal data compression system. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> IT-29(5):656-664, </volume> <month> September </month> <year> 1983. </year>
Reference-contexts: The use of such trees to estimate posteriors 52 for categorical data is actually discussed in some detail by Breiman et al. [13] (see Section 3.9 of this thesis), and a nearly identical approach is presented by Rissanen <ref> [107] </ref> in the context of conditional PMF estimation for universal data compression. The difficulty with such a strategy is that by quantizing the dependent variable and treating it as categorical, advantage is not taken of the meaning of its value. <p> The Context algorithm for universal lossless coding proposed by Rissanen in <ref> [107] </ref> is similar to the CART class-probability tree, but is adaptive (i.e., "on-line"). However, as with class-probability trees, it is also inherently discrete. <p> histogram. 17 The estimated bit rate for lossless compression of the 512 fi 512 monochrome (Y component) version of Lenna using this scheme was 4.26 bpp, which is only about 0.05 bpp worse than that recently reported by Weinberger et al. [141] for a modified version of Rissanen's Context algorithm <ref> [107] </ref>. Table 4.1.4: Lossless Compression Rates (bpp) for the cman Image Neighborhood EM-128 PCDT-S LIN/PCDT (a) 7.96 7.94 7.94 (c) 5.24 5.22 5.21 4.2 Conjoint Scalar Quantization We next consider how the modeling techniques described in Chapter 3 might be applied to lossy compression. <p> The lossless compression scheme of Weinberger et al. [141] is based on explicit probability estimation and therefore deserves mention. Their technique is inherently adaptive owing to its use of Rissanen's Context algorithm <ref> [107] </ref>. That algorithm is count-based and hence requires categorical data. In an attempt to make use of prior knowledge that the underlying probability law is smooth for natural greyscale images, pixel differences are formed and used for the conditioning contexts.
Reference: [108] <author> Jorma J. Rissanen. </author> <title> A predictive inference principle for estimation. </title> <type> Technical report, </type> <institution> IBM Research Lab, </institution> <address> San Jose, CA, </address> <year> 1984. </year> <title> This report was no longer on file at IBM at the time of this writing. In case of difficulty in obtaining one, </title> <note> a copy is available by request from this author. </note>
Reference-contexts: This approach to processing information has been advocated by Rissanen <ref> [108, 111] </ref>. Our choice of term is somewhat arbitrary, but it is convenient to have a short way of describing this style of processing. <p> Such models tend to be semiparametric or nonparametric, and relatively complex. 6 Besides providing a descriptive framework, sequential probabilistic modeling can also serve as the basis for a powerful inferential framework <ref> [30, 108] </ref>. 30 2.3 Evaluation Criteria for Density Estimation As discussed Section 2.2, in a practical setting there is no analytic justification of preferring one density estimate or estimation procedure over another. Therefore, any such justification must be empirical. <p> is slightly ironic that a notion of a true governing probability law is unnecessary for the term "density estimation" to be meaningful, but perhaps no more so than in those other situations in inference and coding that can be approached without positing a true distribution or even a probability space <ref> [28, 30, 77, 108] </ref>. 2.4 Histogram, Kernel, and Mixture Density Models A probabilistic model should both generalize and summarize the training sequence.
Reference: [109] <author> Jorma J. Rissanen. Minimum-description-length principle. In Samuel Kotz, Norman Lloyd Johnson, and Campbell B. </author> <title> Read, editors, </title> <journal> Encyclopedia of Statistical Sciences, </journal> <volume> volume 5, </volume> <pages> pages 523-527. </pages> <publisher> Wiley, </publisher> <year> 1985. </year>
Reference-contexts: The minimum-description length (MDL) principle chooses the proportions that result in the shortest possible exact coded representation of the observed data, using an agreed upon prefix-free code for the empirical distribution itself <ref> [3, 109] </ref>. See [137], p. 106 for an interesting criticism of this approach.
Reference: [110] <author> Jorma J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1989. </year>
Reference-contexts: His subsequent work generalizes the MDL principle considerably, removing the strict model/data separation, and allowing variable precision representation of the model param 1 In other words, proportional to the cardinality of the type class [26]. 20 eters <ref> [110] </ref>. It should be emphasized that the immediate goal of the MDL principle is inference, not data compression: the hope is that the lie turns out to be a more accurate description of the remaining contents of the box than the truth. <p> It will be assumed throughout that all vector 2 Such assumptions can be self-serving, as Rissanen points out in Chapter 2 of <ref> [110] </ref>. <p> Regularization is the basis for every approach mentioned in Section 2.2, as well as one not mentioned there: Akaike's information criterion [116], which will not be discussed except to say that it is similar to MDL <ref> [110] </ref>. Parameter estimates obtained by optimizing regularized criteria can, in certain cases, be shown to have desirable asymptotic theoretical properties, such as consistency [4, 110]. <p> Parameter estimates obtained by optimizing regularized criteria can, in certain cases, be shown to have desirable asymptotic theoretical properties, such as consistency <ref> [4, 110] </ref>. However, unpenalized ML is also fine asymptotically, but notoriously unreliable on small samples, so it is not clear exactly what consistency per se really buys us in a practical setting. An important and somewhat different approach, applicable to semiparametric models, is cross-validation. <p> This observation provides a basis for two-part universal coding [29] as well as an intuitive justification of Rissanen's stochastic complexity measure of M 2 log jLj <ref> [110] </ref>. A similar dependence might hold for continuous densities. 42 example is shown in Figure 2.4.2 (c), where the components and mixing parameters have been selected in the manner to be described in Section 2.6.
Reference: [111] <author> Jorma J. Rissanen. </author> <title> Stochastic complexity and its applications. </title> <note> Unpublished report, available by request from this author, </note> <year> 1996. </year>
Reference-contexts: This approach to processing information has been advocated by Rissanen <ref> [108, 111] </ref>. Our choice of term is somewhat arbitrary, but it is convenient to have a short way of describing this style of processing.
Reference: [112] <author> Jorma J. Rissanen and Glen G. Langdon. </author> <title> Arithmetic coding. </title> <journal> IBM J. Res. Develop., </journal> <volume> 23(2) </volume> <pages> 149-162, </pages> <month> March </month> <year> 1979. </year>
Reference-contexts: This is particularly easy to show in the case of lossless compression of a sequence of independent observations of a discrete-valued random vector Z having PMF P (Z). The number of bits produced by an arithmetic coder <ref> [68, 112] </ref> is likely to be close to the ideal Shannon code length of n=1 ^ P (z (n) ) (2:3:3) bits, which is simply the negative log likelihood. <p> Our main motivation for considering lossless compression is that it serves as a particularly pure test of probabilistic modeling, as was discussed in Section 2.3. When a model is explicitly available, arithmetic coding <ref> [112] </ref> provides a means of using that model directly to perform lossless compression. The performance achieved is nearly perfect with respect to the model; if the model happens to match the source, then the actual bit rate will be very close to the entropy rate.
Reference: [113] <author> Jorma J. Rissanen and Glen G. Langdon. </author> <title> Universal modeling and coding. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> IT-27:12-23, </volume> <year> 1981. </year>
Reference-contexts: Thus, the problem of designing a lossless compressor is actually equivalent to that of estimating the governing probability law. This was long known to be true in principle, but became true in practice primarily through the advent of arithmetic coding in the late 1970s <ref> [113, 86] </ref>.
Reference: [114] <author> E.B. Saff and A.D. Snider. </author> <title> Fundamentals of Complex Analysis for Mathematics, </title> <booktitle> Science, and Engineering. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1976. </year>
Reference-contexts: By using the fact <ref> [114] </ref> that K1 X e j 2 K if n 0 (modK); 0 otherwise, we can express the z-transform of v [m] in terms of that of u [n] as V (z) = K k=0 K k z 1=K ); which corresponds to a discrete-time Fourier transform of V (!) =
Reference: [115] <author> Nicolas Saint-Arnaud and Kris Popat. </author> <title> Analysis and synthesis of sound textures. </title> <booktitle> In Proceed 131 ings of IJCAI-95 Two-Day Workshop on Computational Auditory Scene Analysis, </booktitle> <address> Montreal, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Conjoint subband texture synthesis is not limited to visual textures. In joint work with Nico-las Saint-Arnaud <ref> [115] </ref>, a one-dimensional version of the technique described above was used to synthesize sound textures. <p> Finally, we note that the research described in this thesis has begun to find application in research done by other groups within the Media Lab. Application of our multiresolution method of texture synthesis to sound textures in joint work with Nicolas Saint-Arnaud <ref> [115] </ref> has already been noted in Chapter 5. Recently, Judith Brown [15] adapted our conjoint texture classification technique [96] to the problem of identifying musical instruments from sound samples, with encouraging results [16].
Reference: [116] <author> Yosiyuki Sakamoto. </author> <title> Akaike Information Criterion Statistics. </title> <publisher> KTK Scientific Publishers, </publisher> <address> Hingham, MA, </address> <year> 1986. </year>
Reference-contexts: Regularization is the basis for every approach mentioned in Section 2.2, as well as one not mentioned there: Akaike's information criterion <ref> [116] </ref>, which will not be discussed except to say that it is similar to MDL [110]. Parameter estimates obtained by optimizing regularized criteria can, in certain cases, be shown to have desirable asymptotic theoretical properties, such as consistency [4, 110].
Reference: [117] <author> Lucia Sardo and Josef Kittler. </author> <title> Complexity analysis of RBF networks for pattern recognition. </title> <booktitle> In Proceedings of the 1996 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 574-579, </pages> <address> San Fransisco, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: In addition to avoiding the computational burden of crossvali-dation, their approach does not seem to suffer from the propensity towards oversimplistic models that is sometimes associated with MDL and related information criteria. In another recent paper <ref> [117] </ref>, the same authors focus on the situation in which the data are correlated, and develop a maximum penalized likelihood criterion with a free parameter termed a relaxation factor.
Reference: [118] <author> Lucia Sardo and Josef Kittler. </author> <title> Predictive performance for the detection of underfitting in density estimation. </title> <booktitle> In Proceedings of the IEE International Conference on Artificial Neural Networks, </booktitle> <pages> pages 24-28, </pages> <year> 1997. </year>
Reference-contexts: In many instances, simplicity is thought to be consistent with the goal of predictive accuracy, but there is no reason to suppose that the putative consistency is necessary <ref> [118] </ref>. Indeed, some of the most successful techniques for regression and classification currently in use involve extremely complex semiparametric models and procedures that do not seem to lend themselves well to human interpretation [12]. There is no reason to suppose that the situation should be any different for density estimation. <p> An important problem is determining the appropriate complexity of mixture models more efficiently and in a more general setting. Sardo and Kittler have recently investigated efficient methods for determining the appropriate complexities of separable-Gaussian mixture density estimates, particularly for small samples. In a recent paper <ref> [118] </ref>, they invoke Dawid's notion [30] of calibration of the predictive performance of a model specifically to detect underfitting.
Reference: [119] <author> Bernd Schoner. </author> <title> private communication. </title> <publisher> MIT Media Lab, </publisher> <month> August </month> <year> 1997. </year>
Reference-contexts: Recently, Judith Brown [15] adapted our conjoint texture classification technique [96] to the problem of identifying musical instruments from sound samples, with encouraging results [16]. Eric Metois [78] and Bernd Schoner <ref> [119] </ref> trace the development of their Cluster-Weighted Modeling work [44] with Neil Gershenfeld to ideas gleaned from our papers on the 116 application of mixture models to high-order conditional density estimation for texture synthesis [96] and compression [98]. 117 APPENDIX A Economized EM Algorithm for Mixture Estimation This appendix presents a
Reference: [120] <author> David W. Scott. </author> <title> Multivariate Density Estimation. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: In the continuous case, it is the volume of space that grows exponentially rather than the number of possible combinations, but the effect is the same. The general term used to refer to difficulties ensuing from this exponential growth is the curse of dimensionality (see <ref> [120] </ref>, pp. 196-206 for a good discussion). A similar problem plagues conditional density estimation, which refers to the estimation of proportions within a subpopulation satisfying a specified condition. <p> Mixture models will be discussed in greater detail later, particularly in Sections 2.4 2.6. Finally, a model having a number of degrees of freedom that varies with the amount of training data is generally termed nonparametric. 5 An important example of such a model is the kernel density estimate <ref> [49, 120] </ref>, which in the scalar case is given by ^ f (z) = hjLj jLj X K h ; (2:2:5) where h &gt; 0 is called the bandwidth parameter (usually chosen to depend on jLj), and typically the kernel K is itself selected to be a valid PDF. <p> Extension to the multivariate case is straightforward; see Section 2.4 or, for a more general approach, <ref> [120] </ref> pp. 150-155. Histogram, kernel, and mixture estimates are compared heuristically in Section 2.4. Some general observations can be made regarding the relative complexity, estimability, and expressive power of models in each of these three categories. <p> In particular, the distinction between semipara-metric and nonparametric models is not always made; instead, the term "nonparametric" is often used for both. Moreover, the dividing line between "parametric" and "nonparametric" is itself not always clear. See <ref> [120] </ref>, p. 44 for a discussion of this latter point. 29 The role of probabilistic modeling Semiparametric and nonparametric density estimation are very much products of the computer age, owing to their representational and computational complexity, but their recent emergence also reveals an evolution in thought about the role of probabilistic <p> Specific Criteria: Likelihood and Relative Entropy We now discuss the choice of functional form of the evaluation criterion for a density estimate ^ f of f based on a particular training sequence, considering joint density estimation first. In theoretical analyses of density estimation <ref> [33, 120] </ref>, where the training sequence and therefore ^ f are considered random, the usual L p norms for functions are often used, where L p ( ^ f ; f ) = fi fi fi dz : (2:3:1) The most frequently used values of p are 1 and 2, with <p> A related criterion that is often easier to manipulate <ref> [120] </ref> is the average of the ISE, called mean integrated square error (MISE): MISE = E L n Z h i 2 o Although MISE can be viewed as a special case of L 2 with an appropriate choice of integration measure, we follow the convention used in Scott's book [120] <p> <ref> [120] </ref> is the average of the ISE, called mean integrated square error (MISE): MISE = E L n Z h i 2 o Although MISE can be viewed as a special case of L 2 with an appropriate choice of integration measure, we follow the convention used in Scott's book [120] of taking L p to be with respect to Lebesgue measure, so that MISE is distinct. <p> In doing so, it fails to make good use of prior knowledge of the smoothness of the underlying distribution, and this failure is exacerbated in high dimensions. While various modifications have been proposed to mitigate this problem <ref> [120] </ref>, none seems entirely satisfactory, and consequently the appropriateness of the histogram and its close variants seems restricted to the scalar case. (a) z 2 0 255 255 0 z 1 (c) z 2 0 255 bins have been defined implicitly by the 8-bit amplitude resolution of the source image. <p> Estimate (c) is a mix ture density estimate also having 128 components, trained using the generalized Lloyd algorithm as described in Section 2.5. Kernel density estimation is an important technique with a well-developed supporting theory <ref> [34, 49, 120] </ref>, much of it focusing on the asymptotic properties as the training sequence becomes arbitrarily large [33]. <p> training sequence that are not also empty in its test sequence, leading to an overly conservative and grossly suboptimal choice for M H t . 14 Moreover, the application of simple rules to determine M H t directly from jL t j independently of the data, such as Sturges' rule <ref> [120] </ref> M H work when jL t j is sufficiently large, but this is precisely when they are not needed.
Reference: [121] <author> D.W. Scott and J.R. Thompson. </author> <title> Probability density estimation in higher dimensions. </title> <editor> In J.E. Gentle, editor, </editor> <booktitle> Computer Science and Statistics: Proceedings of the Fifteenth Symposium on the Interface, </booktitle> <pages> pages 173-179, </pages> <address> Amsterdam, 1983. </address> <publisher> North-Holland. </publisher>
Reference-contexts: The problem is worse in higher dimensions; for example, if triples of pixels were used instead of pairs, then at most about 1% of the bins could be nonempty. This manifestation of the curse of dimensionality has been dubbed the empty space phenomenon by Scott and Thompson <ref> [121] </ref>. An alternative would be to use larger bins so that more observations fall into each, or equivalently, coarser quantization. This would result in better generalization, but at the expense of resolution, and the tradeoff achievable in this way by the histogram generally becomes unacceptable in higher dimensions.
Reference: [122] <author> Takanori Senoo and Bernd Girod. </author> <title> Vector quantization for entropy coding of image subbands. </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> 1 </volume> <pages> 526-533, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: In recent years, there has been growing interest in taking advantage of the nonlinear statistical dependence among subband pixels <ref> [17, 25, 55, 89, 88, 122, 126] </ref>. Recall from Section 2.1 that a simple but effective traditional approach to subband image coding is to employ entropy-coded uniform-threshold scalar quantization on the subbands, maintaining the same stepsize when quantizing all of the pixels in all of the subbands.
Reference: [123] <author> Nong Shang. </author> <title> Tree-structured density estimation and dimensionality reduction. </title> <editor> In J. Sall and A. Lehman, editors, </editor> <booktitle> Proceedings of 26th Symposium on the Interface of Computing Science and Statistics (INTERFACE '94), </booktitle> <volume> volume 26, </volume> <pages> pages 172-176, </pages> <address> Fairfax Station, VA, </address> <month> June </month> <year> 1994. </year> <title> Interface Found. </title> <publisher> North America. </publisher>
Reference-contexts: Adapting it to continuous sources in a way that exploits prior knowledge of smoothness (as is done naturally by mixtures in PCDT) has apparently not yet been done. 16 Nong Shang, a former student of Breiman, has proposed applying the CART paradigm to the general problem of multivariate density estimation <ref> [123] </ref>. In that approach, constant probability is assigned to the cells of a CART tree; that is, the density itself plays the role of the dependent variable. Each leaf model is therefore represented by a constant, and these constants are ultimately determined by the relative populations of the cells. <p> Specifically, the derivation of the roughness criterion involves several steps which seem to make strong and restrictive but unstated assumptions about the true underlying probability law. This issue aside, the main point to be made here is that the technique proposed by Shang <ref> [123] </ref> focuses on the joint rather than conditional density estimation problem, and seems to do so by what amounts to an adaptive-bin histogram.
Reference: [124] <author> Claude E. Shannon. </author> <title> A mathematical theory of communication. </title> <journal> Bell Sys. Tech. Journal, </journal> <volume> 27, </volume> <year> 1948. </year>
Reference-contexts: It is well known that the best lossless encoding will produce on average a number of bits about equal to the minus log of the joint probability that has been assigned to the message <ref> [124] </ref>. However, it is difficult to work with the set of all messages, even if we restrict consideration to those of a fixed moderate length.
Reference: [125] <author> Claude E. Shannon. </author> <title> Coding theorems for a discrete source with a fidelity criterion. </title> <booktitle> IRE National Convention Record, </booktitle> <volume> Part 4, </volume> <pages> pages 142-163, </pages> <year> 1959. </year>
Reference-contexts: The approachable bound on performance in the lossy setting is given by the rate-distortion function (RDF) <ref> [125] </ref>, which is known analytically for comparatively few sources and distortion measures [41, 6], and in general must be computed numerically [10].
Reference: [126] <author> Jerome M. Shapiro. </author> <title> Embedded image coding using zerotrees of wavelet coefficients. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 41(12) </volume> <pages> 3345-3462, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Also described there are techniques that we might call conjoint, except that, as far as I am aware, the conditional distribution is either modeled only implicitly as in <ref> [126] </ref>, or explicitly but using fairly restrictive models as in [73]. It would be interesting to see whether an explicit, powerful conditional density model such as that described in Chapter 3 might be used advantageously in such systems. This question is considered in Chapter 5. <p> In recent years, there has been growing interest in taking advantage of the nonlinear statistical dependence among subband pixels <ref> [17, 25, 55, 89, 88, 122, 126] </ref>. Recall from Section 2.1 that a simple but effective traditional approach to subband image coding is to employ entropy-coded uniform-threshold scalar quantization on the subbands, maintaining the same stepsize when quantizing all of the pixels in all of the subbands.
Reference: [127] <author> Y. Shoham and A Gersho. </author> <title> Efficient bit allocation for an arbitrary set of quantizers. </title> <journal> IEEE Transactions on Acoustics Speech and Signal Processing, </journal> <volume> ASSP-36(9):1445-1453, </volume> <month> September </month> <year> 1988. </year>
Reference-contexts: In such situations, an optimal iterative rate allocation procedure can be used instead <ref> [58, 127] </ref>, but a simple formula for the improvement in MSE no longer exists in general. 18 Alternatively, rate can be allocated implicitly by entropy-coded uniform-threshold scalar quan-tization (see Section 2.3).
Reference: [128] <author> William McC. Siebert. </author> <title> Circuits, Signals, and Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: The uncertainty principle for continuous signals <ref> [128] </ref> limits the degree of localization that can be achieved simultaneously in the spectral and spatial (or temporal, in the case of audio signals) domains.
Reference: [129] <author> B. W. Silverman. </author> <title> Penalized maximum likelihood estimation. </title> <editor> In Samuel Kotz, Norman Lloyd Johnson, and Campbell B. Read, editors, </editor> <booktitle> Encyclopedia of Statistical Sciences, </booktitle> <volume> volume 6, </volume> <pages> pages 664-667. </pages> <publisher> Wiley, </publisher> <year> 1982. </year>
Reference-contexts: Fisher's celebrated maximum likelihood (ML) principle gives the proportions as 9=10 for red, 1=10 for blue, and zero for any other color. To be useful in practice, this principle must usually be modified; often this is accomplished by adding a penalty term <ref> [129, 130] </ref>. Laplace's rule of succession [85] requires that you know the possible colors beforehand; if there are (say) five of them, then his rule results in the estimated proportions 2=3 for red, 2=15 for blue, and 1=15 for each of the three remaining unseen colors.
Reference: [130] <author> B. W. Silverman. </author> <title> Density Estimation for Statistics and Data Analysis. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1986. </year> <month> 132 </month>
Reference-contexts: Fisher's celebrated maximum likelihood (ML) principle gives the proportions as 9=10 for red, 1=10 for blue, and zero for any other color. To be useful in practice, this principle must usually be modified; often this is accomplished by adding a penalty term <ref> [129, 130] </ref>. Laplace's rule of succession [85] requires that you know the possible colors beforehand; if there are (say) five of them, then his rule results in the estimated proportions 2=3 for red, 2=15 for blue, and 1=15 for each of the three remaining unseen colors.
Reference: [131] <author> Eero P. Simoncelli and Edward H. Adelson. </author> <title> Subband transforms. </title> <booktitle> In Subband Image Coding, </booktitle> <pages> pages 143-192. </pages> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: We will restrict consideration to pyramids obtained by using the same two-band critically sampled filter bank to carry out all of the analyses. Moreover, the filter banks considered will be orthogonal in the sense defined by Simoncelli and Adelson <ref> [131] </ref>. 15 Horizontal Frequency Vertical Frequency Horizontal Frequency Vertical Frequency Horizontal Frequency Vertical Frequency (a) (b) (c) subbands as shown in (a) by applying a two-band critically sampled filter bank separately in the horizontal and vertical directions. These four subbands constitute the first level of the pyramid. <p> The synthesis filters in these filter banks are the same as the analysis filters. The initialization for the optimization procedure used in generating these filters was the 9-tap filter bank proposed by Simoncelli and Adelson <ref> [131] </ref>, and like that filter bank, the subsampling phases differ for the low- and high-frequency bands. in Section 4.2, with independent quantization achieved by taking the conditioning dimension in the model to be zero. The conditional models themselves were EM-trained mixtures of 256 separable Gaussian components. <p> See the main text for details. The leftmost texture has been synthesized without filtering, in the multiresolution fashion described in the previous chapter. The middle and right textures were synthesized using the two-point DCT and the 9-tap Simoncelli-Adelson filters <ref> [131] </ref>, respectively. The spatial dispersion factors for these filters are: zero (subsampling), 0.25 (DCT), and 0.48 (Simoncelli-Adelson). 5.3 Conjoint Subband Texture Synthesis We now consider the potential role of preprocessing in multiresolution texture synthesis. <p> These responses are also presented graphically in Figure 5.2.2, while the corresponding magnitude-frequency responses are shown in Figure 5.2.1. To speed the optimization and to reduce the chances of falling into an undesirable local minimum, the orthogonal nine-tap filters proposed in <ref> [131] </ref> were used as initial guesses after appropriately padding by zeroes. 123 Table B.6: Numerically Computed Low-Band Filter Bank Coefficients n ff = 5 ff = 1 ff = 0:5 ff = 0:1 -8 -0.001779 -0.000272 0.000956 0.006392 -6 -0.005156 -0.002772 -0.004758 -0.017299 -4 -0.009002 0.010479 0.021303 0.037582 -2 -0.068062 -0.070450
Reference: [132] <author> Eero P. Simoncelli and Edward H. Adelson. </author> <title> Noise removal via Bayesian wavelet coring. </title> <booktitle> In Proceedings of the International Conference on Image Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 379-382, </pages> <address> Lausanne, Switzerland, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: Multiresolution restoration seems particularly promising in light of the recent work by Simoncelli and Adelson on coring <ref> [132] </ref>, wherein small-amplitude subband pixels are set to zero in a manner that the authors interpret in terms of a statistical model of the nonlinear interdependence of subband pixels. 113 6.3 Subband Coding As mentioned in Chapter 5, the performance of a conjoint subband coding system depends on many factors, including
Reference: [133] <author> Eero P. Simoncelli, William T. Freeman, Edward H. Adelson, and David J. Heeger. </author> <title> Shiftable multi-scale transforms. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38 </volume> <pages> 587-607, </pages> <year> 1992. </year>
Reference-contexts: A texture synthesis method using pyramid structures has recently been proposed by Heeger and Bergen [53, 52]. In their approach, a texture is synthesized by iteratively transforming the marginal distributions of subbands in a steerable-pyramid decomposition <ref> [133] </ref> of a pseudorandom noise image to match those of a given texture sample. The marginal distributions are estimated by histograms.
Reference: [134] <author> D. Slepian. </author> <title> Prolate spheroidal wave functions, Fourier analysis and uncertainty - V: the discrete case. </title> <journal> Bell System Technical Journal, </journal> <volume> 58 </volume> <pages> 1371-1430, </pages> <month> May-June </month> <year> 1978. </year>
Reference-contexts: More importantly for present purposes, given the absence of a corresponding localization criterion, there does not seem to be a natural way of incorporating the objective of near-Gaussianity into a design procedure for filter banks. An alternative joint-localization criterion directly applicable to the discrete case was proposed by Slepian <ref> [134] </ref>, and one we specifically devised for the design of nonuniform filterbanks for image processing was proposed in [92].
Reference: [135] <author> S.T. Sum and B.J. Oommen. </author> <title> Mixture decomposition for distributions from the exponential family using a generalized method of moments. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 25(7) </volume> <pages> 1139-1149, </pages> <year> 1995. </year>
Reference-contexts: Henceforth, mixtures components will be assumed to be separable Gaussian densities unless otherwise stated. Estimating the mixture components and mixing probabilities is in general a complex task. Several approaches have been proposed over the years <ref> [35, 40, 76, 104, 135] </ref>, and the topic continues to be an active area of research today. Much of the work has focused on low-complexity mixtures. When mixtures of high complexity are required or allowed, as they are here, robustness and computational complexity of the training process become important considerations.
Reference: [136] <author> Richard Swinburne. </author> <title> The Justification of Induction. </title> <publisher> Oxford University Press, </publisher> <year> 1974. </year>
Reference-contexts: The term "random object" will be defined later in this section. Density estimation is an example of a problem in inductive inference, and inherits all of the well-known foundational difficulties attendant to that class of problem <ref> [23, 105, 136, 139] </ref>. In particular, induction is not deduction: it is not possible on the basis of observing a finite sample to show analytically that a particular density estimate is correct or even reasonable, unless strong assumptions are made that reach beyond the information present in the available data. <p> Rissanen uses a prefix-free code, which amounts to the same thing via the Kraft inequality [41]. It is even possible to argue coherently for estimating a lower proportion of red than blue. A rule that does so is termed counterinductive <ref> [136] </ref>. One way to justify such an estimate would be on Bayesian grounds, using a suitable prior. Another justification, valid in the case of a small total marble population, would be on the grounds that more red marbles than blue have been used up by the observation sequence. <p> Here, the "ways that count" are determined by the flexibility of the model, which is under our control, while the assertion "will resemble the past" is up to nature and can only be hypothesized <ref> [136] </ref>. 39 z 1 Brodatz texture D1. The 150 2 300 148 = 22; 052 vectors for which both coordinates fell within the subimage boundaries were retained. Three different density estimates for the vectors are shown in Figure 2.4.2.
Reference: [137] <author> Vladimir N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: The minimum-description length (MDL) principle chooses the proportions that result in the shortest possible exact coded representation of the observed data, using an agreed upon prefix-free code for the empirical distribution itself [3, 109]. See <ref> [137] </ref>, p. 106 for an interesting criticism of this approach. <p> In these, a sequence of estimates of pixel values is needed, rather than a sequence of estimates of the distribution of values. While knowledge of the latter is generally sufficient for computing the former, it is seldom necessary for doing so, and (to paraphrase Vapnik <ref> [137] </ref>) a performance penalty may result from attempting to solve a harder problem (density estimation) as an intermediate step towards solving an easier one (function approximation). The same comment can be seen to apply to classification.
Reference: [138] <author> Martin Vetterli and Jelena Kovacevic. </author> <title> Wavelets and Subband Coding. </title> <publisher> Prentice-Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1995. </year>
Reference-contexts: The division into subbands is called analysis and the recombination is called synthesis. A good general reference on the subject is the textbook by Vetterli and Kovacevic <ref> [138] </ref>. We shall briefly summarize the essentials, explain why subband pixels tend to be approximately uncorrelated, and summarize how subband decompositions have traditionally been used in image compression. <p> In this case, when the analysis is carried out separably in the vertical and horizontal dimensions, the decomposition is termed a subband pyramid, or just a pyramid (see Figure 2.1.2). Such a structure can also be used to implement certain discrete wavelet transforms <ref> [138] </ref>. We will restrict consideration to pyramids obtained by using the same two-band critically sampled filter bank to carry out all of the analyses.
Reference: [139] <author> Georg Henrik von Wright. </author> <title> The Logical Problem of Induction. </title> <publisher> Macmillan, </publisher> <year> 1957. </year>
Reference-contexts: The term "random object" will be defined later in this section. Density estimation is an example of a problem in inductive inference, and inherits all of the well-known foundational difficulties attendant to that class of problem <ref> [23, 105, 136, 139] </ref>. In particular, induction is not deduction: it is not possible on the basis of observing a finite sample to show analytically that a particular density estimate is correct or even reasonable, unless strong assumptions are made that reach beyond the information present in the available data.
Reference: [140] <author> Roy Weatherford. </author> <title> Philosophical Foundations of Probability Theory. </title> <publisher> Routledge and Kegan Paul, </publisher> <address> London, </address> <year> 1982. </year>
Reference-contexts: The frequentist and subjectivist interpretations, which underlie traditional and Bayesian statistics respectively, are the two most familiar opposing views; there exist others as well. For an overview of viewpoints and issues, the interested reader is referred to the engaging book by Weatherford <ref> [140] </ref>. The variety in viewpoints regarding the interpretation of probability closely reflects those regarding statistical inference, some of which are touched upon elsewhere in this section. 24 outcomes lie in Euclidean space of the appropriate dimension.
Reference: [141] <author> Marcelo J. Weinberger, Jorma J. Rissanen, and Ronald B. </author> <title> Arps. Applications of universal context modeling to lossless compression of gray-scale images. </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> 5(4) </volume> <pages> 575-586, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: In 1968 Henrichon and Fu [54] proposed a recursive partitioning technique for mode separation which is similar in several respects to both CART and projection pursuit (see the final paragraph in this section), and therefore deserves mention. Their technique is summarized by 16 An attempt is made in <ref> [141] </ref>, but the approach makes use of the application-specific assumption that image pixel differences are Laplacian-distributed. While tenable for an i.i.d. model of pixel differences, this assumption is arguably too restrictive in a system that is allowed to adapt. <p> on a set of images having a different combined histogram. 17 The estimated bit rate for lossless compression of the 512 fi 512 monochrome (Y component) version of Lenna using this scheme was 4.26 bpp, which is only about 0.05 bpp worse than that recently reported by Weinberger et al. <ref> [141] </ref> for a modified version of Rissanen's Context algorithm [107]. <p> Therefore, we consider only the prior work in these areas that seem to relate to the approaches considered here. The lossless compression scheme of Weinberger et al. <ref> [141] </ref> is based on explicit probability estimation and therefore deserves mention. Their technique is inherently adaptive owing to its use of Rissanen's Context algorithm [107]. That algorithm is count-based and hence requires categorical data.
Reference: [142] <author> Frans M.J. Willems, Yuri M. Shtarkov, and Tjalling J. Tjalkens. </author> <title> The context-tree weighting method: basic properties. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> 41(3) </volume> <pages> 653-664, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: The choice ~ = 1 corresponds to a straightforward generalization of Laplace's rule of succession [85] to the multinomial setting, while the choice ~ = 1=2 corresponds to what has come to be known as the Krichevski-Trofimov estimator in the universal coding literature <ref> [142] </ref>. In a Bayesian setting, choosing ~ can be shown equivalent to selecting parameters for a Dirichlet prior on ^ P t [43], though such an interpretation is not essential. More will be said about ~ later. 59 the ML estimator.
Reference: [143] <author> J.W. Woods and S.D. O'Neil. </author> <title> Subband coding of images. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-34:1278-1288, </volume> <month> October </month> <year> 1986. </year>
Reference-contexts: Subband coding of images Subband and subband-pyramid decomposition are often used in lossy compression. Subband-based lossy compression is termed subband coding. Subband coding was first applied to speech [62] in the seventies and extended to images in the mid-eighties <ref> [143] </ref>. Much of the underlying motivation and 17 theory carries over directly from an older technique called transform coding [56]. As mentioned above, a block transform is a special case of subband decomposition. The basic idea behind image subband coding is now reviewed briefly.
Reference: [144] <author> S.W. Wu and A. Gersho. </author> <title> Rate-constrained picture-adaptive quantization for jpeg baseline coders. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 389-392, </pages> <address> Minneapolis, MN, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: A frequency-weighted mean-square error criterion can be easily accommodated by varying the stepsizes across subbands, as is implicitly done in JPEG [101]. Perceptually optimized rate allocation has been considered by Jayant et al. [61] and Wu and Gersho <ref> [144] </ref>. As mentioned previously, subband pixels exhibit a great deal of statistical dependence despite their near-uncorrelatedness. This dependence may be exploited for coding gain in a number of ways [25].
Reference: [145] <author> Song Chun Zhu, Yingnian Wu, and David Mumford. </author> <title> Filters, random fields, and maximum entropy (FRAME) | towards a unified theory for texture modeling. </title> <type> Technical Report 95-02, </type> <institution> Harvard University Robotics Laboratory, </institution> <address> Cambridge, MA 02138, </address> <year> 1995. </year> <month> 133 </month>
Reference-contexts: As suggested to me by Olshausen [83], this may have the effect of forcing the subband phases to line up to match the original phase after synthesis as well. Zhu et al. <ref> [145] </ref> have proposed a technique similar to that of Bergen and Heeger, invoking the principle of maximum entropy to relate the joint and marginal distributions.
References-found: 145

