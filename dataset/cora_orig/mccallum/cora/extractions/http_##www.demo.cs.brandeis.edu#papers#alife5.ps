URL: http://www.demo.cs.brandeis.edu/papers/alife5.ps
Refering-URL: http://www.demo.cs.brandeis.edu/papers/long.html
Root-URL: http://www.cs.brandeis.edu
Email: mland@cs.ucsd.edu -pollack,blair-@cs.brandeis.edu  
Title: Coevolution of a Backgammon Player wasting computer cycles on TD-Gammon (Tesauro, 1992). Letting a machine
Author: Jordan B. Pollack Alan D. Blair Mark Land Samuels checker player (Samuel, ). Donald 
Date: May 1996, Nara, Japan. Revised November  
Note: 1.0 Introduction It took great chutzpah for Gerald Tesauro to start  In  Originally in Proceedings of the Fifth Artificial Life Conference,  1996.  
Address: La Jolla, CA 92093 Waltham, MA 02254  
Affiliation: Computer Science Department Computer Science Department Volen Center for Complex Systems University of California, San Diego Brandeis University  
Abstract: One of the persistent themes in Artificial Life research is the use of co-evolutionary arms races in the development of specific and complex behaviors. However, other than Simss work on artificial robots, most of the work has attacked very simple games of prisoners dilemma or predator and prey. Following Tesauros work on TD-Gammon, we used a 4000 parameter feed-forward neural network to develop a competitive backgammon evaluation function. Play proceeds by a roll of the dice, application of the network to all legal moves, and choosing the move with the highest evaluation. However, no back-propagation, reinforcement or temporal difference learning methods were employed. Instead we apply simple hill-climbing in a relative fitness environment. We start with an initial champion of all zero weights and proceed simply by playing the current champion network against a slightly mutated challenger, changing weights when the challenger wins. Our results show co-evolution to be a powerful machine learning method, even when coupled with simple hill-climbing, and suggest that the surprising success of Tesauros program had more to do with the co-evolutionary structure of the learning task and the dynamics of the backgammon game itself, than to sophistication in the learning techniques. Michie initiated machine learning work on reinforcement with his MENACE tic-tac-toe learner using matchboxes with the positions drawn on them (Michie, 1961). However such self-organizing systems had generally been fraught with problems of scale and representation and abandoned by the field of AI. Self-playing game learners often learn weird and brittle strategies which allow them to draw each other, yet play poorly against humans and other programs. Yet after millions of iterations of self-play, Tesauros program has become one of the best backgammon players in the world (Tesauro, 1995) and his weights are viewed by his corporation as significant enough intellectual property to keep as a trade secret except to leverage sales of their minority operating system. (International Business Machines, 1995). However, Tesauro has published the weights for a player using a linear evaluation function called PUBEVAL, which we made use of as a yardstick. Others have replicated this TD result both for research purposes (Boyan, 1992) and reportedly in a commercial product called Jellyfish. How is this success to be understood, explained, and replicated in other domains? Is TD-Gammon unbridled good news about the reinforcement learning method? For the idea of conditioning a machine with rewards and punishments has been rejected by modern cognitive science as part of the associationist paradigm based on its weak or non-existent internal representations. It has been brought back to life in modern machine learning form through work initiated by Klopf, 1982, Barto et al., 1983, and Sutton, 1984. Similarly, there is a lot of work in learning in neural networks following the explosive success of BackPropagation at overcoming some limitations of the Perceptron(Rumelhart et al., 1986). However, with respect to the goal of a self-organizing learning machine which starts from a minimal specification and rises to great sophistication, TD-Gammon stands quite alone in both the reinforcement and neural network literature. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angeline, P. J. </author> <year> (1994). </year> <title> An alternate interpretation of the iterated prisoners dilemma and the evolution of non-mutual cooperation. 3. Further work, incorporating look-ahead, might employ two networks: net A (for pruning) would be tuned to discriminate siblings in the game tree, while net B (for leaf evaluation) is tuned for global position comparison. </title>
Reference-contexts: The problem is particularly prevalent in self-play for deterministic games such as chess or tic-tac-toe. We have worked on using a population to get around it <ref> (Angeline and Pollack, 1994) </ref>. Schraudolph et al., 1994 added non-determinism to the game of Go by choosing moves according to the Boltzmann distribution of statistical mechanics. Others, such as Fogel, 1993, expanded exploration by forcing initial moves. <p> Using the idea of co-evolution in learning recognizes the difference between an optimization based on absolute fitness and one based on relative fitness (with respect to the rest of the population). This was explored by Hillis (Hillis, 1992) on the sorting problem, by Angeline & Pollack <ref> (Angeline and Pollack, 1994) </ref> on genetically programmed Tic-Tac-Toe players, on predator/prey games, e.g. (Cliff and Miller, 1995, Reynolds, 1994), and by Juille & Pollack on the intertwined spirals problem (Juille and Pollack, 1995). Rosin & Belew applied competitive fitness to several games (Rosin and Belew, 1995).
Reference: <author> Angeline, P. J. and Pollack, J. B. </author> <year> (1994). </year> <title> Competitive environments evolve better solutions for complex tasks. </title> <editor> In Forrest, S., editor, </editor> <booktitle> Genetic Algorithms: Proceedings of the Fifth International Conference.. </booktitle>
Reference-contexts: The problem is particularly prevalent in self-play for deterministic games such as chess or tic-tac-toe. We have worked on using a population to get around it <ref> (Angeline and Pollack, 1994) </ref>. Schraudolph et al., 1994 added non-determinism to the game of Go by choosing moves according to the Boltzmann distribution of statistical mechanics. Others, such as Fogel, 1993, expanded exploration by forcing initial moves. <p> Using the idea of co-evolution in learning recognizes the difference between an optimization based on absolute fitness and one based on relative fitness (with respect to the rest of the population). This was explored by Hillis (Hillis, 1992) on the sorting problem, by Angeline & Pollack <ref> (Angeline and Pollack, 1994) </ref> on genetically programmed Tic-Tac-Toe players, on predator/prey games, e.g. (Cliff and Miller, 1995, Reynolds, 1994), and by Juille & Pollack on the intertwined spirals problem (Juille and Pollack, 1995). Rosin & Belew applied competitive fitness to several games (Rosin and Belew, 1995).
Reference: <author> Axelrod, R. </author> <year> (1984). </year> <title> The evolution of cooperation. </title> <publisher> Basic Books, </publisher> <address> New York. </address>
Reference-contexts: By playing them 1000 games against each other and showing the dominance relations with arrows, we can see many relative expertise cycles, albeit with small margins of victory, such as [45,000 beats 70,000 beats 85,000 beats 45,000]. In spatial studies of iterated prisoners dilemma following <ref> (Axelrod, 1984) </ref>, a stable population of tit for tat can be invaded by all cooperate which then allows exploitation by all defect.
Reference: <author> Barto, A., Sutton, R., and Anderson, C. </author> <year> (1983). </year> <title> Neuron-like adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13. </volume>
Reference: <author> Boyan, J. A. </author> <year> (1992). </year> <title> Modular neural networks for learning context-dependent game strategies. </title> <type> Masters thesis, </type> <institution> Computer Speech and Language Processing, Cambridge University. </institution>
Reference-contexts: However, Tesauro has published the weights for a player using a linear evaluation function called PUBEVAL, which we made use of as a yardstick. Others have replicated this TD result both for research purposes <ref> (Boyan, 1992) </ref> and reportedly in a commercial product called Jellyfish.
Reference: <author> Cliff, D. and Miller, G. </author> <year> (1995). </year> <title> Tracking the red queen: Measurements of adaptive progress in co-evolutionary simulations. </title> <booktitle> In Third European Conference on Artificial Life, </booktitle> <pages> pages 200218. </pages>
Reference-contexts: This was explored by Hillis (Hillis, 1992) on the sorting problem, by Angeline & Pollack (Angeline and Pollack, 1994) on genetically programmed Tic-Tac-Toe players, on predator/prey games, e.g. <ref> (Cliff and Miller, 1995, Reynolds, 1994) </ref>, and by Juille & Pollack on the intertwined spirals problem (Juille and Pollack, 1995). Rosin & Belew applied competitive fitness to several games (Rosin and Belew, 1995).
Reference: <author> Epstein, S. L. </author> <year> (1994). </year> <title> Toward an ideal trainer. </title> <journal> Machine Learning, </journal> <volume> 15(3). </volume>
Reference: <author> Fogel, D. B. </author> <year> (1993). </year> <title> Using evolutionary programming to create neural networks that are capable of playing tic-tac-toe. </title> <booktitle> In International conference on Neural Networks, </booktitle> <pages> pages 875880. </pages> <publisher> IEEE Press. </publisher>
Reference: <author> Hillis, D. </author> <year> (1992). </year> <title> Co-evolving parasites improves simulated evolution as an optimization procedure. </title> <editor> In C. Langton, C. Taylor, J. F. and Rasmussen, S., editors, </editor> <booktitle> Artificial Life II. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: Using the idea of co-evolution in learning recognizes the difference between an optimization based on absolute fitness and one based on relative fitness (with respect to the rest of the population). This was explored by Hillis <ref> (Hillis, 1992) </ref> on the sorting problem, by Angeline & Pollack (Angeline and Pollack, 1994) on genetically programmed Tic-Tac-Toe players, on predator/prey games, e.g. (Cliff and Miller, 1995, Reynolds, 1994), and by Juille & Pollack on the intertwined spirals problem (Juille and Pollack, 1995).
Reference: <author> Holland, J. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> The University of Michigan Press. </publisher>
Reference-contexts: The idea of machine learning based on evolution is most often thought of in terms of the genetic algorithm field pioneered by Holland <ref> (Holland, 1975) </ref>. Much of this work however has become focused on optimization to a fixed goal expressed as an absolute fitness function.
Reference: <author> Holland, J. H. </author> <year> (1994). </year> <title> Echoing emergence. </title> <editor> In Cowan, G., Pines, D., and Meltzer, D., editors, </editor> <title> Complexity: Metaphors, Models, </title> <booktitle> and Reality, </booktitle> <pages> pages 309342. </pages> <month> Addi-son-Wesley. </month>
Reference-contexts: In Artificial Life, there have been many recent results on formal and computational ecology models which appear to have arms race dynamics <ref> (Holland, 1994, Kauffman, 1993, Ray, 1992, Lindgren, 1992) </ref>. The idea of machine learning based on evolution is most often thought of in terms of the genetic algorithm field pioneered by Holland (Holland, 1975).
Reference: <author> Juille, H. and Pollack, J. </author> <year> (1995). </year> <title> Massively parallel genetic programming. </title> <editor> In Kinnear, P. A. . K., editor, </editor> <booktitle> Advances in Genetic Programming II. </booktitle> <publisher> MIT Press, cam-bridge. </publisher>
Reference-contexts: This was explored by Hillis (Hillis, 1992) on the sorting problem, by Angeline & Pollack (Angeline and Pollack, 1994) on genetically programmed Tic-Tac-Toe players, on predator/prey games, e.g. (Cliff and Miller, 1995, Reynolds, 1994), and by Juille & Pollack on the intertwined spirals problem <ref> (Juille and Pollack, 1995) </ref>. Rosin & Belew applied competitive fitness to several games (Rosin and Belew, 1995).
Reference: <author> Kauffman, S. A. </author> <year> (1993). </year> <title> The origins of Order: Self-Organization and Selection in Evolution. </title> <publisher> Oxford University Press. </publisher>
Reference: <author> Klopf, A. H. </author> <year> (1982). </year> <title> The Hedonistic Neuron. </title> <publisher> Hemisphere Publishing Corporation, </publisher> <address> Washington, D.C. </address>
Reference: <author> Lindgren, K. </author> <year> (1992). </year> <title> Evolutionary phenomena in simple dynamics. </title> <editor> In C. Langton, C. Taylor, J. F. and Ras-mussen, S., editors, </editor> <booktitle> Artificial Life II. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference: <author> Littman, M. L. </author> <year> (1994). </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 157163. </pages> <note> Morgan Kaufmann. International Business Machines, Press Release (Sept. 12, 1995). IBMs family funpak for OS/2 Warp hits retail shelves. </note>
Reference-contexts: In spatial studies of iterated prisoners dilemma following (Axelrod, 1984), a stable population of tit for tat can be invaded by all cooperate which then allows exploitation by all defect. This kind of relative expertise dynamics, which can be seen clearly in the simple game of rock/paper/scissors <ref> (Littman, 1994) </ref> might initially seen as very bad for self-play learning, because what looks like an advance might actually lead to a cycle of mediocrity. A small group of champions in a dominance circle might arise and hold a temporal monopoly preventing further advance.
Reference: <author> Michie, D. </author> <year> (1961). </year> <title> Trial and error. </title> <booktitle> In Science Survey, </booktitle> <volume> part 2, </volume> <pages> pages 129145. </pages> <publisher> Penguin. </publisher>
Reference-contexts: Donald Michie initiated machine learning work on reinforcement with his MENACE tic-tac-toe learner using matchboxes with the positions drawn on them <ref> (Michie, 1961) </ref>. However such self-organizing systems had generally been fraught with problems of scale and representation and abandoned by the field of AI. Self-playing game learners often learn weird and brittle strategies which allow them to draw each other, yet play poorly against humans and other programs.
Reference: <author> Mitchell, M., Hraber, P. T., and Crutchfield, J. P. </author> <year> (1993). </year> <title> Revisiting the edge of chaos: Evolving cellular automata to perform computations. </title> <journal> Complex Systems, </journal> <volume> 7. </volume>
Reference: <author> Packard, N. </author> <year> (1988). </year> <title> Adaptation towards the edge of chaos. </title> <editor> In Kelso, J. A. S., Mandell, A. J., and Shlesinger, M. F., editors, </editor> <booktitle> Dynamic patterns in complex systems, </booktitle> <pages> pages 293301. </pages> <publisher> World Scientific. </publisher>
Reference-contexts: The success came from the setup of co-evolutionary self-play biased by the dynamics of backgammon. Our result is thus similar to the bias found by Mitchell, Crutchfield & Graber in Packards evolution of Cellular Automata to the edge of chaos <ref> (Packard, 1988, Mitchell et al., 1993) </ref>.
Reference: <author> Ray, T. </author> <year> (1992). </year> <title> An approach to the synthesis of life. </title> <editor> In C. Langton, C. Taylor, J. F. and Rasmussen, S., editors, </editor> <booktitle> Artificial Life II. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference: <author> Reynolds, C. </author> <year> (1994). </year> <title> Competition, coevolution, and the game of tag. </title> <booktitle> In Proceedings 4th Artificial Life Conference. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Rosin, C. D. and Belew, R. K. </author> <year> (1995). </year> <title> Methods for competitive co-evolution: finding opponents worth beating. </title> <booktitle> In Proceedings of the 6th international conference on Genetic Algorithms, </booktitle> <pages> pages 373380. </pages> <publisher> Morgan Kauf-man. </publisher>
Reference-contexts: Rosin & Belew applied competitive fitness to several games <ref> (Rosin and Belew, 1995) </ref>.
Reference: <author> Rumelhart, D., Hinton, G., and Williams, R. </author> <year> (1986). </year> <title> Learning representations by back-propagating errors. Nature, </title> <publisher> 323:533536. </publisher>
Reference-contexts: Similarly, there is a lot of work in learning in neural networks following the explosive success of BackPropagation at overcoming some limitations of the Perceptron <ref> (Rumelhart et al., 1986) </ref>. However, with respect to the goal of a self-organizing learning machine which starts from a minimal specification and rises to great sophistication, TD-Gammon stands quite alone in both the reinforcement and neural network literature.
Reference: <author> Samuel, A. L. </author> <year> (1959). </year> <title> some studies of machine learning using the game of checkers. </title> <institution> IBM Joural of Research and Development. </institution>
Reference-contexts: Letting a machine learning program play itself backgammon in the hopes of bettering itself, indeed! After all, the dream of computers mastering a domain by self-play or introspection had been around since the early days of AI, forming part of Samuels checker player <ref> (Samuel, 1959) </ref>. Donald Michie initiated machine learning work on reinforcement with his MENACE tic-tac-toe learner using matchboxes with the positions drawn on them (Michie, 1961). However such self-organizing systems had generally been fraught with problems of scale and representation and abandoned by the field of AI.
Reference: <author> Schraudolph, N. N., Dayan, P., and Sejnowski, T. J. </author> <year> (1994). </year> <title> Temporal difference learning of position evaluation in the game of go. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6, </volume> <pages> pages 817824. </pages> <publisher> Morgan Kauffman. </publisher>
Reference: <author> Sims, K. </author> <year> (1994). </year> <title> Evolving 3d morphology and behavior by competition. </title> <booktitle> In Proceedings 4th Artificial Life Conference. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Rosin & Belew applied competitive fitness to several games (Rosin and Belew, 1995). However, besides Tesauros TD-Gammon, which has not to date been viewed as an instance of co-evolutionary learning, Sims artificial robot game <ref> (Sims, 1994) </ref> is the only other domain as complex as Backgammon to have substantial learning success. 3.2 Learnability and Unlearnability Learnability can be formally defined as a time constraint over a search space.
Reference: <author> Sutton, R. </author> <year> (1984). </year> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massa-chusetts, Amherst. </institution>
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. Machine Learning, </title> <publisher> 3:944. </publisher>
Reference-contexts: As noted in 2. For example, in a prisoners dilemma, if the payoff for temptation 7 instead of 5, the long term bonus for alternating defections (3.5) would be more rational than cooperating (3.0). See Angeline, 1994 for related discussion <ref> (Sutton, 1988) </ref>, the goals of good game playing and accurate position evaluation are not quite the same. In backgammon, the opponents stable configuration is immutable in the course of a single move.
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. Machine Learning, </title> <publisher> 8:257277. </publisher>
Reference-contexts: 1.0 Introduction It took great chutzpah for Gerald Tesauro to start wasting computer cycles on TD-Gammon <ref> (Tesauro, 1992) </ref>. Letting a machine learning program play itself backgammon in the hopes of bettering itself, indeed! After all, the dream of computers mastering a domain by self-play or introspection had been around since the early days of AI, forming part of Samuels checker player (Samuel, 1959).
Reference: <author> Tesauro, G. </author> <year> (1995). </year> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <volume> 38(3):58 68. </volume>
Reference-contexts: Self-playing game learners often learn weird and brittle strategies which allow them to draw each other, yet play poorly against humans and other programs. Yet after millions of iterations of self-play, Tesauros program has become one of the best backgammon players in the world <ref> (Tesauro, 1995) </ref> and his weights are viewed by his corporation as significant enough intellectual property to keep as a trade secret except to leverage sales of their minority operating system. (International Business Machines, 1995).
References-found: 30

