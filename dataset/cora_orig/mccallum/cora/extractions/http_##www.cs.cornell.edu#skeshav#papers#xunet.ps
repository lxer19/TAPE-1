URL: http://www.cs.cornell.edu/skeshav/papers/xunet.ps
Refering-URL: http://www.cs.cornell.edu/skeshav/papers.html
Root-URL: http://www.cs.brown.edu/
Title: Xunet 2: Lessons from an Early Wide-Area ATM Testbed  
Author: C. R. Kalmanek S. Keshav W. T. Marshall S. P. Morgan R. C. Restrick 
Abstract: This paper is a retrospective on the design of Xunet 2, one of the earliest functional wide-area ATM networks. Work on Xunet 2 began in 1989 and the network, consisting of experimental ATM switches, IP routers, and 45 Mbps transmission lines, has been operational since October 1991. The network serves as a ``laboratory without walls'' for eight research groups across the United States. While Xunet 2 has only a small number of nodes, it was designed as a prototype of a nationwide ATM network. This paper reviews some of the design decisions and lessons learned in the project and points out the research directions motivated by this work, focusing on the areas of traffic management, ATM switch design, network control, and the implementation of an IP router. 
Abstract-found: 1
Intro-found: 1
Reference: [AHU96] <author> R. Ahuja, S. Keshav and H. Saran, </author> <title> "Design, Implementation, and Performance of a Native-mode ATM Transport Protocol," </title> <booktitle> Proc. IEEE INFOCOM'96, </booktitle> <month> March </month> <year> 1996, </year> <pages> pp. 206-214. </pages>
Reference-contexts: The orc driver does protocol demultiplexing using the virtual circuit identifier when receiving from the network. Application programs can communicate using virtual circuits directly via a slight modification to the socket library <ref> [AHU96] </ref>. 5.5. Lessons The Xunet router has been in service since April 1993.
Reference: [ANER93] <author> N. G. Aneroussis, C. R. Kalmanek, V. E. Kelly, </author> <title> "Implementing OSI Management Facilities on the Xunet ATM Testbed", </title> <booktitle> Proc. 4th IFIP/IEEE Workshop on Distributed Systems: Operations and Management, </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: Determination of the appropriate threshold settings is often based on trial and error, and an OSI agent was developed for the switch which allowed the alarm thresholds to be changed by a remote manager <ref> [ANER93] </ref>. The fault management software logs status information on each switch and the log files are uploaded to New Jersey over the network each night. The status history for the past 15 minutes can be accessed at a virtual console. The signaling module supports connection control of simplex virtual circuits. <p> Finally, it forwards the positive response upstream toward the connection originator. Originally, the signaling module was only a client: it did not offer any services. Later, it was modified to allow an OSI agent to collect statistics such as connection blocking from the signaling module <ref> [ANER93] </ref>, and a central network manager used the blocking statistics to control the capacity of virtual paths so as to minimize call blocking and bandwidth requirements [ANER95a]. We designed a simple resource management scheme for CBR bandwidth reservation and ABR static buffer allocation.
Reference: [ANER95a] <author> N. G. Aneroussis and A. A. Lazar, </author> <title> "Managing Virtual Paths on Xunet III: Architecture, Experimental Platform, and Performance," </title> <booktitle> Proc. IFIP/IEEE International Symposium on Integrated Network Management, </booktitle> <address> Santa Barbara, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Later, it was modified to allow an OSI agent to collect statistics such as connection blocking from the signaling module [ANER93], and a central network manager used the blocking statistics to control the capacity of virtual paths so as to minimize call blocking and bandwidth requirements <ref> [ANER95a] </ref>. We designed a simple resource management scheme for CBR bandwidth reservation and ABR static buffer allocation. During the hop-by-hop connection setup described above, a Reserve operation is invoked at each switch as a connection request proceeds towards the destination (forward pass). <p> Our software organization puts performance-critical code in the kernel, primarily the code that is called during packet handling, while code that is not performance critical is kept in user space. We partitioned the functionality between user space and kernel so that we could do experiments that involved signaling <ref> [ANER95a] </ref> or multiplexing policy without touching the kernel. The remainder of this section focuses on the kernel code. Consider a packet which arrives on FDDI that is to be routed over Xunet.
Reference: [ANE95b] <author> N. G. Aneroussis, A. A. Lazar and D. E. Pen-darakis, </author> <title> "Taming Xunet III," </title> <journal> ACM Computer Communications Review, </journal> <volume> Vol. 25, No. 3, </volume> <pages> pp. 44-65, </pages> <month> October </month> <year> 1995. </year> <title> [ANSA93] "ANSAware 4.1: Application Programming in ANSAware," Architecture Projects Management Limited, </title> <publisher> Poseidon House, </publisher> <address> Castle Park, CB3 0RD, Cambridge UK, </address> <month> February </month> <year> 1993. </year>
Reference: [BERE93] <author> A. Berenbaum, J. Dixon, A. Iyengar, and S. Keshav, </author> <title> "A Flexible ATM Host-Interface for Xunet II," </title> <journal> IEEE Network, </journal> <volume> Vol. 7, No. 4, </volume> <month> July </month> <year> 1993, </year> <pages> pp. 18-23. </pages>
Reference-contexts: This approach eliminates the need to manage multiple reassembly buffers on the host adaptor or the need to manage partial state for the AAL5 CRC computation. The adaptor design <ref> [BERE93] </ref> is based on a RISC CPU, the AT&T Hobbit, acting as an intelligent DMA engine (Figure 10). Fiber transceivers operating at 200 Mbps interface to the link encoder/decoders which then feed a 16 KB receive fifo buffer and a 4 KB transmit fifo buffer.
Reference: [BONO95] <author> F. Bonomi and K. W. Fendick, </author> <title> "The Rate-Based Flow Control Framework for the Available Bit Rate ATM Service," </title> <journal> IEEE Network, </journal> <volume> Vol. 9, No. 2, </volume> <month> March-April </month> <year> 1995, </year> <pages> pp. 35-39. </pages>
Reference-contexts: believe that we have gained some valuable insights. 5 We first focus on three aspects of ABR service: scheduling, buffer management, and flow control, before returning to other service classes. __________________ 5 The term "ABR" has come to refer to the rate-based flow control framework defined by the ATM Forum <ref> [BONO95, FEND96] </ref>. We use the term more generically to refer to the service model that we describe in this section, unless the work of the ATM Forum is specifically referred to. We believe that ABR service will be used to support computer traffic with widely divergent characteristics and requirements. <p> In our view, host computers that attach directly to ATM networks should run Packet-Pair or some equivalent rate adaption algorithm. If the network provides explicit rate feedback through the use of Resource Management cells, as specified in the ATM Forum ABR service <ref> [BONO95, FEND96] </ref>, a host would simply use this rate instead of making its own rate estimates. When ATM networks are used to interconnect conventional local area networks, hosts will be running conventional TCP/IP. <p> The simplest and potentially most cost effective approach would give the host responsibility for transmission scheduling of one KByte or larger "transmission blocks." While Packet-Pair flow control allows this, the rate-based proposals <ref> [BONO95, FEND96] </ref> in the ATM Forum require a more complex and more expensive host adaptor with finer-grained scheduling and algorithms to modify the schedule based on feedback or an explicit rate allocation. The software organization on the router seems as valid today as when it was done.
Reference: [CACE92] <author> R. C aceres, </author> <title> "Multiplexing Traffic at the Entrance to Wide-Area Networks," </title> <type> PhD thesis, </type> <address> U. C. Berkeley, </address> <note> December 1992 (Report No. UCB/CSD 92/717). </note>
Reference-contexts: These issues were studied in conjunction with Xunet in <ref> [CACE92, SARA94A] </ref>. In order to provide a good quality LAN interconnection service, end systems and/or routers must avoid inducing congestion on the wide area network. Using simulation, we explored end-to-end performance when TCP's end-to-end congestion control scheme (TCP-Tahoe) is used in conjunction with an "edge-to-edge" congestion control scheme between routers. <p> When we began work on Xunet, AAL4 had been proposed for data service on ATM and we investigated the efficiency of IP encapsulation using this adaption layer. C aceres <ref> [CACE92] </ref> collected packet traces from our Internet gateway and we used the histogram of packet sizes to estimate efficiency. In some cases, IP encapsulation in AAL4 resulted in link utilizations as low as 65%.
Reference: [CAMP92] <author> R. H. Campbell et al., </author> <title> "Control Software for Virtual-Circuit Switches: Call Processing", </title> <booktitle> Future Tendencies in Computer Science, Control and Applied Mathematics, Lecture Notes in Computer Science 653, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin 1992, </address> <pages> pp. 175-186. </pages>
Reference-contexts: TDK [MCMA81] provided switch control for the Datakit switch and pioneered the notion of a "process-per-line" in which a lightweight process is associated with each half-connection during its transient states. Archos <ref> [CAMP92] </ref> explored the use of an object-oriented operating system for Datakit switch control. Finally, Milan Jukl in our group developed an early prototype for the Xunet control software which aimed for high performance by structuring the call processing software in a single process executing an event-driven finite state machine.
Reference: [CNRI95] <institution> On-line reference to the Gigabit Testbed Initiative at http://www.cnri.reston.va.us/ </institution>
Reference-contexts: In addition, AT&T upgraded some segments of the network to 622 Mbps to support the BLANCA project, one of five ``Gigabit Testbeds'' in the National Research and Education Network (NREN) initiative <ref> [CNRI95] </ref>. Xunet 2 consists of ten experimental ATM switches (two in Murray Hill) interconnected with 45 Mbps transmission lines (Figure 1). There is an ATM switch at each user site and switches at three AT&T central office locations: Oakland CA, Chicago IL and Newark NJ.
Reference: [DEME89] <author> A. Demers, S. Keshav, and S. Shenker, </author> <title> "Analysis and Simulation of a Fair Queueing Algorithm," </title> <booktitle> Proc. ACM SIGCOMM, </booktitle> <month> September </month> <year> 1989, </year> <pages> pp. </pages> <note> 1-12; also Journal of Internetworking Research and Experience, Vol. 1, No. 1, </note> <month> September </month> <year> 1990, </year> <pages> pp. 3-26. </pages>
Reference-contexts: Scheduling The importance of per-virtual-circuit queueing and round robin scheduling in data networks has been addressed elsewhere <ref> [NAGL86, KATA88, DEME89, HAHN91, MORG91] </ref>, but we briefly summarize the argument. A round robin scheduler is one in which each virtual circuit has its own, logically distinct, data queue and the scheduler serves non-empty data queues in turn.
Reference: [FEND96] <author> K. W. Fendick, </author> <title> "Evolution of Controls for the Available Bit Rate Service," </title> <journal> IEEE Communications, </journal> <volume> Vol. 34, No. 11, </volume> <month> November </month> <year> 1996, </year> <pages> pp. 35-39. </pages>
Reference-contexts: believe that we have gained some valuable insights. 5 We first focus on three aspects of ABR service: scheduling, buffer management, and flow control, before returning to other service classes. __________________ 5 The term "ABR" has come to refer to the rate-based flow control framework defined by the ATM Forum <ref> [BONO95, FEND96] </ref>. We use the term more generically to refer to the service model that we describe in this section, unless the work of the ATM Forum is specifically referred to. We believe that ABR service will be used to support computer traffic with widely divergent characteristics and requirements. <p> In our view, host computers that attach directly to ATM networks should run Packet-Pair or some equivalent rate adaption algorithm. If the network provides explicit rate feedback through the use of Resource Management cells, as specified in the ATM Forum ABR service <ref> [BONO95, FEND96] </ref>, a host would simply use this rate instead of making its own rate estimates. When ATM networks are used to interconnect conventional local area networks, hosts will be running conventional TCP/IP. <p> The simplest and potentially most cost effective approach would give the host responsibility for transmission scheduling of one KByte or larger "transmission blocks." While Packet-Pair flow control allows this, the rate-based proposals <ref> [BONO95, FEND96] </ref> in the ATM Forum require a more complex and more expensive host adaptor with finer-grained scheduling and algorithms to modify the schedule based on feedback or an explicit rate allocation. The software organization on the router seems as valid today as when it was done.
Reference: [FRAS83] <author> A. G. Fraser, </author> <title> "Towards a Universal Data Transport System", </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> Vol. 1, No. 5, </volume> <month> November </month> <year> 1983, </year> <pages> pp. 803-816. </pages>
Reference-contexts: The collaboration emphasized student research and the sites were linked by a network of Datakit Virtual Circuit Switches <ref> [FRAS83] </ref> joined by 1.544 Mbps transmission lines. This network has since become known as Xunet 1. The focus of the Xunet program was broadened in 1989 when AT&T started to plan a high-speed ATM network: Xunet 2. <p> The intuition behind the window schemes is that a source never needs a window size larger than its bandwidth-delay product since that is the largest amount of unacknowledged data that must be in transit to achieve the maximum throughput. The research version of the Datakit switch <ref> [FRAS83] </ref> provided static buffer allocation of a full round trip - 3 - window at the output queue of every switch for each virtual circuit. <p> Switch Architecture The Xunet switch was designed as a bus-based output queued switch with support for per-virtual-circuit queueing at the output ports, similar to the Datakit switch <ref> [FRAS83] </ref>. The switch architecture is shown in Figure 4. In the figure, each port controller consists of two cards: a queue handler and a line interface card.
Reference: [GOLE91] <author> S. J. Golestani, </author> <title> "Congestion-Free Communication in High-Speed Packet Networks," </title> <journal> IEEE - 16 - Transactions on Communications, </journal> <volume> Vol. 39, No. 12, </volume> <month> December </month> <year> 1991, </year> <pages> pp. 1802-1812. </pages>
Reference-contexts: Both the size of the elasticity buffer and of the switch buffers are of interest. One approach to controlling delay jitter involves retim-ing or reshaping of traffic within the network, as in Stop-and-Go Queueing <ref> [GOLE91] </ref> or Hierarchical Round Robin Scheduling (HRR) [KALM90, SARA94B]. These disciplines allow the network to control the burstiness of the traffic streams exiting a server, which affects the amount of delay jitter that can be introduced by the downstream switch.
Reference: [GROS95] <author> M. Grossglauser, S. Keshav, and D. Tse, "RCBR: </author> <title> A Simple and Efficient Service for Multiple Time-Scale Traffic," </title> <booktitle> Proc. ACM SIG-COMM '95, </booktitle> <address> Boston, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Our simulations show that video carried using the ABR service, with the coder's sending rate controlled using Packet-Pair, gives a perceptually high quality even in the presence of congestion. [SAFR95] shows how to adapt the sending rate for stored video. Second, <ref> [GROS95] </ref> describes a scheme based on ``fast'' renegotiation of a CBR rate. Renegotiation allows the network to extract statistical multiplexing gain on a time scale that is slow compared to the feedback-based rate adaption schemes such as Packet-Pair.
Reference: [HAHN91] <author> E. L. Hahne, </author> <title> "Round-Robin Scheduling for Max-Min Fairness in Data Networks", </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> Vol. 9, No. 7, </volume> <month> September </month> <year> 1991, </year> <pages> pp. 1024-1039. </pages>
Reference-contexts: Scheduling The importance of per-virtual-circuit queueing and round robin scheduling in data networks has been addressed elsewhere <ref> [NAGL86, KATA88, DEME89, HAHN91, MORG91] </ref>, but we briefly summarize the argument. A round robin scheduler is one in which each virtual circuit has its own, logically distinct, data queue and the scheduler serves non-empty data queues in turn.
Reference: [HAHN93] <author> E. L. Hahne, C. R. Kalmanek, and S. P. Mor-gan, </author> <title> "Dynamic Window Flow Control on a High-Speed, Wide-Area Data Network", </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> Vol. 26, No. 1, </volume> <month> September </month> <year> 1993, </year> <pages> pp. 29-41. </pages>
Reference-contexts: As the network becomes congested, however, the buffer allocation that is granted is reduced. When virtual circuits become idle, the buffer allocation returns to the default. In the dynamic buffer allocation scheme described by <ref> [HAHN93] </ref>, the total buffer requirement per output port is approximately B = W 0 log N + C , (2) where C is proportional to the renegotiation interval after which an idle circuit's allocation is returned to the default. <p> Using simulation, we explored end-to-end performance when TCP's end-to-end congestion control scheme (TCP-Tahoe) is used in conjunction with an "edge-to-edge" congestion control scheme between routers. The router-based flow control approach, based on the dynamic window flow control scheme of Section 2 <ref> [HAHN93] </ref>, insures that no packets are lost in the wide-area network due to congestion. As a result, packet losses only occur in the router or local area network. Losses can occur at the input router, but our work showed that the throughput is high with appropriate choice of parameters.
Reference: [KALM90] <author> C. R. Kalmanek, H. Kanakia, and S. Keshav, </author> <title> "Rate Controlled Servers for Very High-Speed Networks," </title> <booktitle> GLOBECOM '90, </booktitle> <address> San Diego, Cali-fornia, </address> <month> December </month> <year> 1990, </year> <pages> pp. 12-20. </pages>
Reference-contexts: Both the size of the elasticity buffer and of the switch buffers are of interest. One approach to controlling delay jitter involves retim-ing or reshaping of traffic within the network, as in Stop-and-Go Queueing [GOLE91] or Hierarchical Round Robin Scheduling (HRR) <ref> [KALM90, SARA94B] </ref>. These disciplines allow the network to control the burstiness of the traffic streams exiting a server, which affects the amount of delay jitter that can be introduced by the downstream switch. <p> It is commonly expected that CBR service will require hosts to implement a fine-grained cell transmission schedule, and adaptors can inexpensively implement a transmission schedule for this service using a variation of the HRR scheduler described in <ref> [KALM90] </ref>. In our view, however, an ABR service should be designed to allow a range of options on host adaptors.
Reference: [KALM92] <author> C. R. Kalmanek, B. Lyles, and W. T. Marshall, </author> <title> "Proposal for a Robust SEAL Protocol," Contribution to ANSI T1S1.5, </title> <address> Chicago, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: In some cases, IP encapsulation in AAL4 resulted in link utilizations as low as 65%. As a result of this study, we defined a payload type in the ATM cell header to mark the end of an IP packet and designed a frame-oriented adaption layer called AALX <ref> [KALM92] </ref> that eliminated the per-cell overhead of AAL4. Eliminating the per-cell overhead improved the efficiency for the same mix of packets from 65% to 85%, since many small packets would now fit in two cells rather than three. Both of these ideas have since been incorporated in standards.
Reference: [KANA93] <author> H. Kanakia, P. P. Mishra, and A. R. Reibman, </author> <title> "An Adaptive Congestion Control Scheme for Real-Time Packet Video Transport," </title> <journal> Computer Communication Review, </journal> <volume> Vol 23, No. 4, </volume> <month> Octo-ber </month> <year> 1993, </year> <pages> pp 20-31. </pages>
Reference-contexts: It is not clear to us that a leaky-bucket-controlled VBR service is of much use for traffic that is as bursty as variable bit rate video. Two recent approaches to carrying VBR video are attractive. First, <ref> [KANA93, LAKS97] </ref> demonstrate that it is possible to achieve good video quality by adapting the sending rate of a video coder in response to network congestion.
Reference: [KATA88] <author> M. G. H. Katavenis, </author> <title> "Fast Switching and Fair Control of Congested Flow in Broadband Networks," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> Vol. SAC-5, </volume> <month> October </month> <year> 1987, </year> <pages> pp. 1315-1326. </pages>
Reference-contexts: Scheduling The importance of per-virtual-circuit queueing and round robin scheduling in data networks has been addressed elsewhere <ref> [NAGL86, KATA88, DEME89, HAHN91, MORG91] </ref>, but we briefly summarize the argument. A round robin scheduler is one in which each virtual circuit has its own, logically distinct, data queue and the scheduler serves non-empty data queues in turn.
Reference: [KESH91] <author> S. Keshav, </author> <title> "A Control-Theoretic Approach to Flow Control," </title> <booktitle> Proc. ACM SIGCOMM '91, </booktitle> <month> September </month> <year> 1991, </year> <pages> pp. 3-15. </pages>
Reference-contexts: This would avoid congestion and is one of the key ideas behind the Packet-Pair flow control scheme <ref> [KESH91, KESH95] </ref>. With Packet-Pair, a source probes the network by sending packets in back-to-back pairs and measures the spacing of the acknowledgments to estimate the service rate and amount of data buffered at the bottleneck link.
Reference: [KESH95] <author> S. Keshav, </author> <note> "Packet-Pair Flow Control", submitted to ACM Transactions on Computer Systems. </note>
Reference-contexts: This would avoid congestion and is one of the key ideas behind the Packet-Pair flow control scheme <ref> [KESH91, KESH95] </ref>. With Packet-Pair, a source probes the network by sending packets in back-to-back pairs and measures the spacing of the acknowledgments to estimate the service rate and amount of data buffered at the bottleneck link.
Reference: [KESH97] <author> S. Keshav and S. P. Morgan, "SMART Retransmission: </author> <title> Performance with Overload and Random Losses," </title> <booktitle> Proc. IEEE INFOCOM'97, </booktitle> <month> April </month> <year> 1997, </year> <note> to appear. </note>
Reference-contexts: Simulations show that with this policy users who attempt to get more bandwidth at the bottleneck by "cheating" always end up hurting themselves, while users who correctly adapt their sending rate get good service <ref> [KESH97] </ref>. An example is shown in Figure 2. In Figure 2, ten statistically identical sources share a trunk whose speed determines the maximum speed of each virtual circuit. All of the sources share the same round-trip delay.
Reference: [LAKS97] <author> T. V. Lakshman, P. P. Mishra and K. K. Ramakrishnan, </author> <title> "Transporting Compressed Video over ATM Networks with Explicit Rate Feedback Control," </title> <booktitle> Proc. IEEE INFOCOM'97, </booktitle> <month> April </month> <year> 1997, </year> <note> to appear. </note>
Reference-contexts: It is not clear to us that a leaky-bucket-controlled VBR service is of much use for traffic that is as bursty as variable bit rate video. Two recent approaches to carrying VBR video are attractive. First, <ref> [KANA93, LAKS97] </ref> demonstrate that it is possible to achieve good video quality by adapting the sending rate of a video coder in response to network congestion.
Reference: [MCMA81] <author> L. E. McMahon, </author> <title> "An Experimental Software Organization for a Laboratory Data Switch," </title> <booktitle> Proc. ICC'81, IEEE International Conference on Communications, </booktitle> <volume> Vol. 2, </volume> <year> 1981, </year> <pages> pp. </pages> <month> 25.4.1-25.4.4. </month>
Reference-contexts: These considerations suggested a modular architecture in which pieces could evolve independently. The organization of software for call processing and switch control has been a focus of our research group for some time. TDK <ref> [MCMA81] </ref> provided switch control for the Datakit switch and pioneered the notion of a "process-per-line" in which a lightweight process is associated with each half-connection during its transient states. Archos [CAMP92] explored the use of an object-oriented operating system for Datakit switch control.
Reference: [MORG91] <author> S. P. Morgan, </author> <title> "Queueing Disciplines and Passive Congestion Control in Byte-Stream Networks", </title> <journal> IEEE Transactions on Communications, </journal> <volume> Vol. 39, No. 7, </volume> <month> July </month> <year> 1991, </year> <pages> pp. 1097-1106. </pages>
Reference-contexts: Scheduling The importance of per-virtual-circuit queueing and round robin scheduling in data networks has been addressed elsewhere <ref> [NAGL86, KATA88, DEME89, HAHN91, MORG91] </ref>, but we briefly summarize the argument. A round robin scheduler is one in which each virtual circuit has its own, logically distinct, data queue and the scheduler serves non-empty data queues in turn.
Reference: [NAGL86] <author> J. B. Nagle, </author> <title> "On Packet Switches with Infinite Storage," </title> <journal> IEEE Transactions on Communications, </journal> <volume> Vol. COM-35, No. 4, </volume> <month> April </month> <year> 1987, </year> <pages> pp. 435-438. </pages>
Reference-contexts: Scheduling The importance of per-virtual-circuit queueing and round robin scheduling in data networks has been addressed elsewhere <ref> [NAGL86, KATA88, DEME89, HAHN91, MORG91] </ref>, but we briefly summarize the argument. A round robin scheduler is one in which each virtual circuit has its own, logically distinct, data queue and the scheduler serves non-empty data queues in turn.
Reference: [OIE89] <author> Y. Oie, M. Murata, K. Kubota and H. Miyahara, </author> <title> "Effect of Speedup in Nonblocking Packet Switches," </title> <booktitle> IEEE International Conference on Communications, </booktitle> <month> June </month> <year> 1989, </year> <pages> pp. 410-414. </pages>
Reference-contexts: Since we operate these busses an order of magnitude faster than the access lines, input queues should occur infrequently even in a switch with a larger number of ports <ref> [OIE89] </ref>. 3.2.1. Header Translation Card The header translation card serves multiple functions. First, the card does the VCI translation needed for cell switching. In support of this function, a processor on the card audits the translation memory for consistency.
Reference: [ONU97] <author> P. Onufryk, "Euphony: </author> <title> An Embedded RISC Processor for Low-Cost ATM Networking and Signal Processing," </title> <booktitle> Proc. Supercon 97 - Digital Communications Design Conference, </booktitle> <address> Santa Clara, CA, </address> <month> January </month> <year> 1997. </year> <note> Also see http://www.research.att.com/pzo/euphony.html. </note>
Reference: [PARE94] <author> A. K. Parekh and R. G. Gallager, </author> <title> "A Generalized Processor Sharing Approach to Flow Control in Integrated Services Networks: The Multiple Node Case," </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> Vol. 2, No. 2, </volume> <month> April </month> <year> 1994, </year> <pages> pp. 137-150. </pages>
Reference-contexts: WRR is equivalent to a scheduling discipline known as Rate-Proportional Processor Scheduling <ref> [PARE94] </ref> in which the weight given to a VC is proportional to the fraction of link capacity to which it is entitled.
Reference: [RAMA88] <author> K. K. Ramakrishnan and R. Jain, </author> <title> ``A Binary Feedback Scheme for Congestion Avoidance in Computer Networks with a Connectionless Network Layer,'' </title> <booktitle> Proc. ACM SIGCOMM, </booktitle> <month> August </month> <year> 1988, </year> <pages> pp. 303-313. </pages>
Reference-contexts: This buffer management policy supports the dynamic window flow control scheme described in Section 2. The queue handler also includes specialized hardware for manipulating an eight-bit congestion field that we reserved in our ATM cell header. This feature was motivated by the DECbit scheme <ref> [RAMA88] </ref>, in which routers set a congestion bit to indicate congestion to sources. A schematic of the queue handler is shown in Figure 6. The card contains a large DRAM array which supports 64 K virtual queues implemented by a high-speed queue control state machine.
Reference: [RATH93] <author> E. P. Rathgeb, </author> <title> "Policing of Realistic VBR Video Traffic in an ATM Network," </title> <journal> International Journal of Digital and Analog Communications Systems, </journal> <volume> Vol. 6, </volume> <year> 1993, </year> <pages> pp. 213-226. </pages>
Reference-contexts: However, the choice of the leaky bucket parameters determines the amount of statistical multiplexing that can be achieved for a given cell loss rate, delay variation, and switch buffer pool. VBR service has been extensively discussed as a vehicle for carrying variable bit rate video traffic. However, measurements <ref> [VERB89, RATH93] </ref> of both teleconferencing and entertainment video traffic suggest a number of problems with a VBR video service based on leaky buckets. - 5 - One problem is that it may be difficult to pick reasonable leaky bucket parameters a priori for any particular video service.
Reference: [REIB92a] <author> A. R. Reibman and A. W. Berger, </author> <title> "Traffic Descriptors for VBR Video Teleconferencing over ATM Networks," </title> <booktitle> GLOBECOM '92, </booktitle> <address> Orlando, Florida, </address> <month> December </month> <year> 1992, </year> <journal> pp. 314-319; also IEEE/ACM Transactions on Networking, </journal> <volume> Vol. 3, No. 3, </volume> <month> June </month> <year> 1995, </year> <pages> pp. 329-339. </pages>
Reference-contexts: However, default choices may lead to a video quality that is no better than would be obtained by a CBR service with the same average rate. Leaky-bucket descriptors of unconstrained video encoder output require a large burst tolerance <ref> [REIB92a] </ref>, unless the average rate is nearly equal to the peak rate, which would not allow for much statistical multiplexing gain. The reason is that encoder output, especially for entertainment video, can have peak-rate bursts lasting for several seconds. <p> Moreover, large switch buffers cost. A final issue is that the amount of statistical multiplexing gain that can be achieved with video sources may be rather small. The video community now appears to expect potential gains in the range from 1.5:1 to 2:1 <ref> [REIB92a] </ref>, but it is hard to estimate this number because there are so many alternative video system configurations, and because there is no easy way to relate subjective judgments of video quality to quantities that can be computed in a network performance analysis.
Reference: [REIB92b] <author> A. R. Reibman and B. G. </author> <title> Haskell, "Constraints on Variable Bit-Rate Video for ATM Networks," </title> <journal> IEEE Transactions on Circuits and Systems for Video Technology, </journal> <volume> Vol 2, No. 4, </volume> <month> December </month> <year> 1992, </year> <pages> pp. 361-372. </pages>
Reference-contexts: Of course, one might imagine the user picking from a menu of "default" leaky bucket parameters provided by the encoder, since codecs can be designed <ref> [REIB92b] </ref> to comply with any given leaky-bucket parameters (CBR output is a special case). However, default choices may lead to a video quality that is no better than would be obtained by a CBR service with the same average rate.
Reference: [RFC1577] <author> M. Laubach, </author> <title> "Classical IP and ARP over ATM," Internet Network Working Group Request for Comments #1577, </title> <month> January </month> <year> 1994. </year>
Reference-contexts: We therefore got early experience with host computers that use both IP encapsulation and a native ATM protocol stack. 5.2. IP Service Architecture Our implementation of an IP service on Xunet is similar to the Classical Model of IP over ATM <ref> [RFC1577] </ref>. Hosts or routers which attach to Xunet are part of a logical IP subnetwork. When an IP packet arrives from a local area network, the router forwards it over an ATM virtual circuit to the egress router, as illustrated by the dashed line in Figure 9.
Reference: [ROBE92] <author> J. W. Roberts, </author> <title> editor, "Performance Evaluation and Design of Multiservice Networks," COST 224 Final Report, </title> <booktitle> Commission of the European Communities, Brussels (1992), </booktitle> <pages> pp. 111-147. </pages>
Reference-contexts: Admission control insures that sufficient capacity is available for CBR traffic. However, cell level queueing will occur as a result of the detailed arrival patterns of the input streams. Cell level queueing resulting from the superposition of deterministic streams having the same or different spacings has been extensively studied <ref> [ROBE92] </ref>. The simplest case occurs when N streams with the same spacing but different phases are multiplexed onto a single link. In an ideal model, the merged arrival process will be periodic, as will the output of the link. <p> The ensemble average queue length for a large number of deterministic streams arriving at the output queue of a switch can be approximated by the queue length distribution for an M/D/1 queue with the same utilization <ref> [ROBE92] </ref>. an M/D/1 queue at 90% utilization, as well as the cumulative delay distribution for a series of 10 independent but statistically identical M/D/1 queues, each at 90% utilization. In the figure, Q (x) is the probability that the queue length exceeds x cells.
Reference: [ROMA94] <author> A. Romanov and S. Floyd, </author> <title> "Dynamics of TCP Traffic over ATM Networks," </title> <booktitle> Proc. ACM SIG-COMM '94, </booktitle> <address> London, </address> <month> August 31 - September 2, </month> <year> 1994, </year> <pages> pp. 79-88. - 17 </pages> - 
Reference-contexts: As a result, packet losses only occur in the router or local area network. Losses can occur at the input router, but our work showed that the throughput is high with appropriate choice of parameters. Recent work on TCP performance in ATM networks has suggested Early Packet Discard policies <ref> [ROMA94, TURN96] </ref> which discard an entire TCP packet if a queue length threshold is exceeded, indicating congestion. However, this work relies only on end-to-end flow control and does not explore use of congestion control between routers at the edges of the ATM network.
Reference: [SAFR95] <author> R. Safranek, C. Kalmanek and R. Garg, </author> <title> "Methods for Matching Compressed Video to ATM Networks," </title> <booktitle> Proc. IEEE Workshop on Information Theory, Multiple Access and Queueing Theory, </booktitle> <address> St. Louis, MO, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: Our simulations show that video carried using the ABR service, with the coder's sending rate controlled using Packet-Pair, gives a perceptually high quality even in the presence of congestion. <ref> [SAFR95] </ref> shows how to adapt the sending rate for stored video. Second, [GROS95] describes a scheme based on ``fast'' renegotiation of a CBR rate. Renegotiation allows the network to extract statistical multiplexing gain on a time scale that is slow compared to the feedback-based rate adaption schemes such as Packet-Pair.
Reference: [SARA94A] <author> H. Saran and S. Keshav, </author> <title> "An Empirical Evaluation of Virtual Circuit Holding Times in IP," </title> <booktitle> Proc. IEEE INFOCOM'94, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: These issues were studied in conjunction with Xunet in <ref> [CACE92, SARA94A] </ref>. In order to provide a good quality LAN interconnection service, end systems and/or routers must avoid inducing congestion on the wide area network. Using simulation, we explored end-to-end performance when TCP's end-to-end congestion control scheme (TCP-Tahoe) is used in conjunction with an "edge-to-edge" congestion control scheme between routers. <p> The address resolution module also maintains an activity timer for each virtual circuit. When a connection has been inactive for one second, it calls up to the IP connection server. The IP connection server can choose whether to clear the connection <ref> [SARA94A] </ref>, in keeping with the idea that policy decisions are kept in user space. The interface presented by the orc driver to higher layers of the protocol stack does not depend on IP and Figure 11 shows a native ATM protocol stack.
Reference: [SARA94B] <author> H. Saran, S. Keshav and C. R. Kalmanek, </author> <title> "A Scheduling Discipline and Admission Control Policy for Xunet 2," </title> <journal> ACM Multimedia Systems Journal, </journal> <volume> Vol. 2, No. 3, </volume> <month> September </month> <year> 1994. </year>
Reference-contexts: Both the size of the elasticity buffer and of the switch buffers are of interest. One approach to controlling delay jitter involves retim-ing or reshaping of traffic within the network, as in Stop-and-Go Queueing [GOLE91] or Hierarchical Round Robin Scheduling (HRR) <ref> [KALM90, SARA94B] </ref>. These disciplines allow the network to control the burstiness of the traffic streams exiting a server, which affects the amount of delay jitter that can be introduced by the downstream switch. <p> We found in in-house measurements that two videocon-ferencing streams produced by the same H.261 codec differed in average rate by a factor of two, depending on whether the speaker was sedate or agitated. In <ref> [SARA94B] </ref>, we supposed that VBR video traffic would be carried through an HRR server, but again we had no way a priori of determining the HRR rate.
Reference: [TINA95] <author> M. Chapman and S. Montesi, </author> <title> "Overall Concepts and Principles of TINA," Document Label TB_MDC.018_1.0_94, </title> <month> February </month> <year> 1995, </year> <note> available from http://www.tinac.com/. </note>
Reference-contexts: We chose to use a commercial operating system to facilitate student research. In addition, with a commercial OS we could use commercial distributed systems software to explore the client-server approach to network control suggested by the ISO-ODP (Open Distributed Processing) community and in the TINA-C standards <ref> [TINA95] </ref>. Administration and maintenance of an experimental wide-area network pose unique challenges, particularly when hardware and software are still in flux. In addition, a goal of Xunet was to allow research students to use the network as a laboratory, perhaps running their own experimental software.
Reference: [TURN96] <author> J. S. Turner, </author> <title> "Maintaining High Throughput During Overload in ATM Switches," </title> <booktitle> Proc. IEEE INFOCOM'96, </booktitle> <month> March </month> <year> 1996, </year> <pages> pp. 287-295. </pages>
Reference-contexts: As a result, packet losses only occur in the router or local area network. Losses can occur at the input router, but our work showed that the throughput is high with appropriate choice of parameters. Recent work on TCP performance in ATM networks has suggested Early Packet Discard policies <ref> [ROMA94, TURN96] </ref> which discard an entire TCP packet if a queue length threshold is exceeded, indicating congestion. However, this work relies only on end-to-end flow control and does not explore use of congestion control between routers at the edges of the ATM network.
Reference: [VERB89] <author> W. Verbiest and L. Pinnoo, </author> <title> "A Variable Bit Rate Video Coder for Asynchronous Transfer Mode Networks," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> Vol. 7, No. 5, </volume> <month> June </month> <year> 1989, </year> <pages> pp. 761-770. </pages>
Reference-contexts: However, the choice of the leaky bucket parameters determines the amount of statistical multiplexing that can be achieved for a given cell loss rate, delay variation, and switch buffer pool. VBR service has been extensively discussed as a vehicle for carrying variable bit rate video traffic. However, measurements <ref> [VERB89, RATH93] </ref> of both teleconferencing and entertainment video traffic suggest a number of problems with a VBR video service based on leaky buckets. - 5 - One problem is that it may be difficult to pick reasonable leaky bucket parameters a priori for any particular video service.
Reference: [VERN88] <author> M. K. Vernon and U. Manber, </author> <title> "Distributed Round-Robin and First-Come-First-Serve Protocols and Their Application to Multiprocessor Bus Arbitration," </title> <booktitle> 15th IEEE International Symposium on Computer Architecture, </booktitle> <month> May 30 - June 2, </month> <year> 1988, </year> <pages> pp. 269-277. </pages>
Reference-contexts: The queue handlers arbitrate for access to the contention bus using a distributed group arbitration protocol <ref> [VERN88] </ref>. Arbitration is pipelined, so that queue handlers arbitrate during one time slot for permission to send a cell during the next time slot. During each arbitration cycle, a queue handler that wishes to transmit asserts its slot address on the arbitration bus.
Reference: [WEIN78] <author> C. J. Weinstein, </author> <title> "Fractional Speech Loss and Talker Activity Model for TASI and for Packet-Switched Speech," </title> <journal> IEEE Transactions on Communications, </journal> <volume> Vol. COM-26, No. 8, </volume> <month> August </month> <year> 1978, </year> <pages> pp. 1253-1257. </pages>
Reference-contexts: There are in principle two approaches to handling large burst tolerances. In the first approach, mean cell loss ratios are kept low by keeping the probability low that enough sources are in burst mode simultaneously to overload the link <ref> [WEIN78] </ref>. Since few sources are admitted into the network, this approach results in poor statistical multiplexing gain. A different approach increases the number of sources that could be accommodated at a given cell loss ratio.
References-found: 45

