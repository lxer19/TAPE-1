URL: ftp://archive.cis.ohio-state.edu/pub/neuroprose/wallisgm.temporalobjrec2.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00035.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email:  
Title: Using Spatio-Temporal Correlations to Learn Invariant Object Recognition  
Author: Guy Wallis 
Web: guy@mpik-tueb.mpg.de  
Address: 38, 72076 Tubingen, Germany.  
Affiliation: Max-Planck Institute fur biologische Kybernetik, Spemannstrae  
Abstract: A competitive network is described which learns to classify objects on the basis of temporal as well as spatial correlations. This is achieved by using a Hebb-like learning rule which is dependent upon prior as well as current neural activity. The rule is shown to be capable of outperforming a supervised rule on the cross-validation test of an invariant character recognition task, given a relatively small training set. It is also shown to outperform the supervised version of Fukushima's Neocognitron (1980), on a larger training set. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Attneave, F. </author> <year> 1959. </year> <title> Applications of Information Theory to Psychophysics: A Summary of Basic Concepts, Methods and Rsults. </title> <address> New York: </address> <publisher> Holt, Rinehart and Winston. </publisher>
Reference: <author> Foldiak, P. </author> <year> 1991. </year> <title> Learning invariance from transformation sequences. </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 194-200. </pages> <note> Learning Spatial Representations 12 Foldiak, </note> <author> P. </author> <year> 1992. </year> <title> Models of Sensory Coding. </title> <type> Tech. </type> <institution> rept. CUED/F-INFENG/TR 91. University of Cambridge, Department of Engineering. </institution>
Reference: <author> Fukushima, K. </author> <year> 1980. </year> <title> Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. </title> <journal> Biological Cybernetics, </journal> <volume> 36, </volume> <pages> 193-202. </pages>
Reference-contexts: Numerous ideas including weight sharing have been proposed to accommodate for such translations <ref> (Fukushima, 1980, for example) </ref>. This paper describes simulations of an unsupervised learning rule which requires information about the activity within a local region, but which otherwise acts locally within a cell, hence making it a more biologically relevant alternative.
Reference: <author> Fukushima, K. </author> <year> 1988. </year> <title> Neocognitron: A hierarchical neural network model capable of visual pattern recognition unaffected by shift in position. </title> <booktitle> Neural Networks, </booktitle> <volume> 1, </volume> <pages> 119-130. </pages>
Reference-contexts: The best performance quoted by Lovell & Tsoi for their version of the Neocognitron, using Fukushima's supervised training algorithm <ref> (Fukushima, 1988) </ref>, is 72.50%. The TraceNet's best performance using the trace rule is 74.5%. As a final comparison, the number of neurons in Fukushima's model is 11,320, with some 1.2 million connections.
Reference: <author> Fukushima, K. </author> <year> 1992. </year> <title> Character recognition with neural networks. </title> <journal> Neurocomputing, </journal> <volume> 4(5), </volume> <pages> 221-233. </pages>
Reference: <author> Goodale, M.A., & Milner, A.D. </author> <year> 1992. </year> <title> Separate visual pathways for perception and action. </title> <booktitle> Trends in Neurosciences, </booktitle> <volume> 15, </volume> <pages> 20-25. </pages>
Reference: <author> Hecht-Nielsen, R. </author> <year> 1990. </year> <title> Neurocomputing. </title> <address> Reading, Massachusetts: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Hertz, J., Krogh, A., & Palmer, R.G. </author> <year> 1990. </year> <title> Introduction to the theory of neural computation. </title> <address> Santa Fe Institute: </address> <publisher> Addison Wesley. </publisher>
Reference-contexts: The first layer consists of 256 neurons arranged in a 4x4 grid of 4x4 inhibitory pools. Each pool fully samples a corresponding 4x4 patch of a 16x16 input image. Competition within these pools is of the `winner take most' type, otherwise referred to as leaky learning <ref> (Hertz et al., 1990) </ref>. In the context of this network, this implies establishing which neuron within each pool is firing most strongly and electing it the winner.
Reference: <author> Klopf, A.H. </author> <year> 1988. </year> <title> A neuronal model of classical conditioning. </title> <journal> Psychobiology, </journal> <volume> 16, </volume> <pages> 85-125. </pages>
Reference: <author> LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., & Jackel, L.D. </author> <year> 1989. </year> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 541-551. </pages>
Reference: <author> Lovell, D.R., & Tsoi, A.C. </author> <year> 1992. </year> <title> The performance of the Neocognitron with various S-cell and C-cell transfer functions. Paper posted to the `Connectionists' internet mail group. </title> <institution> Produced in the Dept. Electrical Engineering, University of Queensland, Queensland 4072, Australia. </institution>
Reference: <author> Miyashita, Y. </author> <year> 1988. </year> <title> Neuronal correlate of visual associative long-term memory in the primate temporal cortex. </title> <journal> Nature, </journal> <volume> 335, </volume> <pages> 817-820. </pages>
Reference: <author> Rolls, E.T. </author> <year> 1992. </year> <title> Neurophysiological mechanisms underlying face processing within and beyond the temporal cortical areas. </title> <journal> Philosophical Transactions of the Royal Society, London [B], </journal> <volume> 335, </volume> <pages> 11-21. </pages>
Reference: <author> Rumelhart, D.E., Hinton, G.E., & Williams, R.J. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. Chap. 8 of: </title> <editor> Rumelhart, D.E., & McClelland, J.L. (eds), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> vol. 1: </volume> <booktitle> Foundations. </booktitle> <address> Cambridge, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference: <author> Sutton, R.S., & Barto, A.G. </author> <year> 1981. </year> <title> Towards a modern theory of adaptive networks: expectation and prediction. </title> <journal> Psychological Review, </journal> <volume> 88, </volume> <pages> 135-170. </pages> <note> Learning Spatial Representations 13 Tanaka, </note> <author> K., Saito, H., Fukada, Y., & Moriya, M. </author> <year> 1991. </year> <title> Coding visual images of objects in the inferotemporal cortex of the macaque monkey. </title> <journal> Journal of Neurophysiology, </journal> <volume> 66, </volume> <pages> 170-189. </pages>
Reference: <author> Wallis, G. </author> <year> 1994. </year> <title> Neural Mechanisms Underlying Processing in the Visual Areas of the Occipital and Temporal Lobes. </title> <type> Ph.D. thesis, </type> <institution> Department of Experimental Psychology, Oxford Univeristy. </institution> <address> www: ftp://ftp.mpik-tueb.mpg.de/pub/guy/all.ps.Z. </address>
Reference-contexts: The value 7 was found to be the optimal integer value in the range 1 to 10, optimally sparsifying activity within the network whilst stopping short of deactivating all other neurons within an inhibitory pool <ref> (Wallis, 1994) </ref>. As a test of recognition performance, digits from a character set used by LeCun et. al. (1989) were presented to the network. <p> As an aside, performance was also tested with set to zero. This corresponds to simple Hebbian learning in both the first and now also the second layer for which 2 The relationship between the value of and the length of stimulus presentation has been explored elsewhere <ref> (Wallis, 1994) </ref>. Learning Spatial Representations 5 validation data set note the considerably worse performance of the delta rule. learning is only affected by current neural activity.
Reference: <author> Wallis, G., & Rolls, E.T. </author> <year> 1995. </year> <title> A Model of Invariant Object Recognition in The Visual System. </title> <note> Submitted to Journal of Computational Neuroscience for Review. www: ftp://ftp.mpik-tueb.mpg.de/pub/guy/jcns7.ps.Z. </note>
Reference: <author> Wallis, G., Rolls, E.T., & Foldiak, P. </author> <year> 1993. </year> <title> Learning invariant responses to the natural transformations of objects. </title> <booktitle> Pages 1087-1090 of: International Joint Conference on Neural Networks, </booktitle> <volume> vol. </volume> <pages> 2. </pages>
Reference: <author> Widrow, B., & Hoff, M.E. </author> <year> 1960. </year> <title> Adaptive switching circuits. </title> <booktitle> Pages 96-104 of: 1960 IRE WESCON Convention Record, </booktitle> <volume> vol. </volume> <pages> 4. </pages> <address> New York: </address> <institution> Institute of Radio Engineers. </institution>
Reference-contexts: In addition, in order to gauge the relative performance of the trace rule, the same network is also trained using the standard delta rule <ref> (Widrow & Hoff, 1960) </ref>. The two layer network, referred to as the TraceNet was constructed with the intention that the first layer should serve to extract some local features which might appear comparable in transformed versions of the same digit.
References-found: 19

