URL: http://ftp.eecs.umich.edu/people/peter/dissertation.ps
Refering-URL: http://ftp.eecs.umich.edu/people/peter/
Root-URL: http://www.eecs.umich.edu
Title: ABSTRACT AUTOMATIC ACQUISITION OF WORD MEANING FROM CONTEXT  
Author: by Peter Mark Hastings 
Degree: Chair: Assistant Professor Steven Lytinen  
Abstract: This thesis presents an automatic, incremental lexical acquisition mechanism that uses the context of example sentences to guide inference of the meanings of unknown words. The goal of this line of research is to allow a Natural Language Processing (NLP) system to cope with words that it does not know | not just to gloss over them, but to try to infer what they mean. The environment within which this system operates is epitomized by the information extraction task: from virtually unconstrained text, elicit certain information that is deemed interesting. The knowledge acquisition bottleneck inherent in this task imposes constraints on the type of knowledge available for lexical inference. The main objective in this work is to infer as much information as possible about unknown words from context without requiring special-purpose knowledge. This was accomplished by extending the underlying NLP system to search its domain-specific concept representation for an appropriate concept to denote the meaning of the unknown word. The learning method is incremental, so every time the system encounters an example of an unfamiliar word, it adjusts its hypotheses. The basic system evolved through several different stages in order to improve its inferences. Then several variations to the basic system were made to capture especially difficult aspects of the acquisition task and to take advantage of discourse context. The approach was tested in two different domains. Target words were removed from the lexica and sentences containing them were processed by the system. The results were evaluated using measures taken from the field of Information Retrieval. When humans learn language, they are faced with a similar task: from a set of examples of a word's use, they must infer what that word means and how it is used. Not only is the task similar, but many of the behaviors and difficulties that the computational acquisition mechanism have encountered have also been described in the psycholinguistic literature. Although the system was not intended as a cognitive model, these parallels indicate strong constraints from the task itself, and therefore lend credence to viewing the system as a cognitive model. 
Abstract-found: 1
Intro-found: 1
Reference: [ Allen, 1981 ] <author> J. Allen. </author> <title> What's necessary to hide?: Modeling action verbs. </title> <booktitle> In Proceedings of the 19 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 77-81, </pages> <year> 1981. </year>
Reference-contexts: He considered what it would take to represent the verb " hide", and answer reasonable questions about it. Allen's answer to his question involved a temporal logic that could address "notions of belief, intention, and causality." <ref> [ Allen, 1981, p. 81 ] </ref> This work takes the same approach as Allen's, in effect rephrasing the previous question as, "In order to meet the functional requirements of the overall task, what does the NLP system need to know?" Instead of requiring the system to answer all possible questions about
Reference: [ Barwise and Etchemendy, 1989 ] <author> J. Barwise and J. Etchemendy. </author> <title> Model-theoretic semantics. </title> <editor> In M. Posner, editor, </editor> <booktitle> Foundations of Cognitive Science. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference: [ Behrend, 1990 ] <author> D. Behrend. </author> <title> The development of verb concepts: Children's use of verbs to label familiar and novel events. Child Development, </title> <booktitle> 61 </booktitle> <pages> 681-696, </pages> <year> 1990. </year>
Reference-contexts: This underscores the importance of the input in lexical acquisition. Along a slightly different vein, Behrend conducted in-depth studies of different types of verbs to compare children's comprehension and production among these various verbs <ref> [ Behrend, 1990 ] </ref> . The types of verbs that he studied were those that described actions (e.g. "squeeze", "pound"), results ("flatten", "break"), and instruments ("hammer"). He found that when labelling actions ("What is the person doing?"), children are more likely to use an instrument verb than an action verb. <p> Thus the children in the experiments were labeling the events with the most specific label possible. This contradicts the results found in acquisition of nouns, which demonstrate that "specific subordinate terms are used much less frequently than basic-level terms as labels for familiar 77 objects." <ref> [ Behrend, 1990, p. 694 ] </ref> . What could explain these psycholinguistic results? They suggest a difference between the internal mental structures that nouns and verbs map to. Gentner calls the basis for this difference the "referential / relational" distinction. <p> Under this assumption, it is clear why they learn nouns first: nouns refer to objects that they can see. Verbs refer to relations which tend to be less constrained by the physical world, so their meaning components "cut across all semantic fields." <ref> [ Behrend, 1990, p. 694 ] </ref> . What kind of mental representation can account for the differences that these data suggest? The representation that Gentner espouses is a semantic net, in which meanings are built up compositionally by referring to more basic elements of meaning.
Reference: [ Berwick, 1985 ] <author> R. Berwick. </author> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference: [ Bloom et al., 1980 ] <author> L. Bloom, K. Lifter, and J. Hafitz. </author> <title> Semantics of verbs and the development of verb inflection in child language. </title> <booktitle> Language, </booktitle> <volume> 56(2) </volume> <pages> 386-412, </pages> <year> 1980. </year>
Reference-contexts: They found significant differences between verb groups in the frequency of inflection use and the order of emergence of the inflections. They concluded that, "The semantics of the verbs that the children were learning was the major influence on their learning of verb inflections." <ref> [ Bloom et al., 1980, p. 404 ] </ref> This supports the general notion of the relationship of semantic structure to learning.
Reference: [ Bowerman, 1976 ] <author> M. Bowerman. </author> <title> Semantic factors in the acquisition of rules for word use and sentence construction. </title> <editor> In D. Morehead and A. Morehead, editors, </editor> <title> Normal and deficient child language. </title> <publisher> University Park Press, </publisher> <address> Baltimore, </address> <year> 1976. </year>
Reference-contexts: On the other hand, verbs tend to express relationships between objects or changes in those relationships. Relationships are more abstract and less easily perceived by humans. In fact, because there are basically an infinite number of imaginable relationships between any pair of objects or events <ref> [ Bowerman, 1976 ] </ref> , children must rely on linguistic input to inform them what relationships are culturally important. Because very young children have not fully developed this knowledge source, we expect them to focus on the more compelling perceptual aspects of their environment.
Reference: [ Bowerman, 1983 ] <author> M. Bowerman. </author> <title> How do children avoid constructing an overly general grammar in the absence of feedback about what is not a sentence? In Proceedings of Research on Childrens Language Development, </title> <booktitle> volume 22, </booktitle> <year> 1983. </year>
Reference-contexts: Bowerman, for example, cites evidence that 17 month old children can distinguish subtle variations in syntactic forms and apply them to their lexical acquisition task. <ref> [ Bowerman, 1983 ] </ref> Granger made a good start at defining the problem of automatic acquisition of lexical acquisition, and the research described in this thesis is in some ways an extension of his. <p> This allows it to quickly hypothesize meanings for words although the initial hypotheses may not be correct. 6.2 The No-Negative-Evidence problem When children learn language, they must induce the structure of the language relying almost entirely on examples of utterances which are within the language <ref> [ Bowerman, 1983 ] </ref> . They do not have the benefit of negative evidence that would tell them which possibilities 2 The question of which comes first, the concept or the lexeme, is an interesting one that will come up again in section 6.5.
Reference: [ Brent, 1991 ] <author> M. Brent. </author> <title> Automatic acquisition of subcategorization frames from untagged text. </title> <booktitle> In Proceedings of the 29 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 209-214, </pages> <year> 1991. </year>
Reference: [ Brent, 1993a ] <author> M. Brent. </author> <title> From grammar to lexicon: Unsupervised learning of lexical syntax. </title> <note> Computational Linguistics, 1993. in press. </note>
Reference: [ Brent, 1993b ] <author> M. Brent. </author> <title> Surface cues and robust inference as a basis for the early acquisition of subcategorization frames. </title> <type> Lingua, </type> <year> 1993. </year> <note> in press. </note>
Reference: [ Cardie, 1993 ] <author> C. Cardie. </author> <title> A case-based approach to knowledge acquisition for domain-specific sentence analysis. </title> <booktitle> In Proceedings of the 11 th National Conference on Artificial Intelligence, </booktitle> <pages> pages 798-803, </pages> <year> 1993. </year>
Reference-contexts: Thus by inferring a deeper definition for a word, Camille can actually reduce its memory load. 5.4.2 MayTag Recently Cardie developed another novel approach to lexical acquisition using a human trainer, not for presenting appropriate examples, but for developing a basis for bootstrapping <ref> [ Cardie, 1993 ] </ref> . Her system, MayTag, works within the environment of an information extraction task and uses the CIRCUS parser [ Lehnert, 1990 ] .
Reference: [ Carey and Bartlett, 1978 ] <author> S. Carey and E. Bartlett. </author> <title> Acquiring a single new word. Papers and reports on child language development (Department of Linguistics, </title> <publisher> Stanford University), </publisher> <pages> 15 17-29, </pages> <year> 1978. </year>
Reference-contexts: Thus instead of adding a node to the semantic hierarchy, the system could search for an unlabelled node. This approach would be consonant with psycholinguistic research that shows that children use unknown words to fill gaps in their lexicon <ref> [ Carey and Bartlett, 1978 ] </ref> . 17 It would conflict, however, with the conservative approach required by Camille's ambiguity recognition mechanism.
Reference: [ Carey, 1978 ] <author> S. Carey. </author> <title> The child as word learner. </title> <editor> In M. Halle, G. Miller, and J. Bresnan, editors, </editor> <booktitle> Linguistic theory and psychological reality, </booktitle> <pages> pages 264-293. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1978. </year>
Reference: [ Chinchor, 1992 ] <author> N. Chinchor. </author> <title> MUC-4 evaluation metrics. </title> <booktitle> In Proceedings of the Fourth Mes--sage Understanding Conference, </booktitle> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Chapter 5 contains a qualitative comparison of these systems with Camille. 28 The tradeoff between inferring correct hypotheses and limiting the number of hypothe-ses generated also resulted in the use of a more discriminating scoring mechanism, adapted from the MUC conferences <ref> [ Chinchor, 1992 ] </ref> . These measures, Recall and Precision (originally taken from the field of Information Retrieval), are defined below.
Reference: [ Chomsky, 1981 ] <author> N. Chomsky. </author> <title> Principles and parameters in syntactic theory. </title> <editor> In N. Hornstein and D. Lightfoot, editors, </editor> <title> Explanation in Linguistics: The Logical Problem of Language Acquisition. </title> <publisher> Longman, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: It was provided by an independent program that analyzed a stick-figure animation. The underlying linguistic structure was based on Universal Grammar <ref> [ Chomsky, 1981; Chomsky, 1985; Chomsky, 1986 ] </ref> . Specifically, Siskind encoded 12 principles into his system. Some of the principles defined Davra's basic abilities, for example that it had the ability to segment the input sentences and to comprehend the semantic representation.
Reference: [ Chomsky, 1985 ] <author> N. Chomsky. </author> <title> Knowledge of Language. </title> <publisher> Praeger Publications, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: It was provided by an independent program that analyzed a stick-figure animation. The underlying linguistic structure was based on Universal Grammar <ref> [ Chomsky, 1981; Chomsky, 1985; Chomsky, 1986 ] </ref> . Specifically, Siskind encoded 12 principles into his system. Some of the principles defined Davra's basic abilities, for example that it had the ability to segment the input sentences and to comprehend the semantic representation.
Reference: [ Chomsky, 1986 ] <author> N. Chomsky. </author> <title> Barriers. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: It was provided by an independent program that analyzed a stick-figure animation. The underlying linguistic structure was based on Universal Grammar <ref> [ Chomsky, 1981; Chomsky, 1985; Chomsky, 1986 ] </ref> . Specifically, Siskind encoded 12 principles into his system. Some of the principles defined Davra's basic abilities, for example that it had the ability to segment the input sentences and to comprehend the semantic representation.
Reference: [ Church and Hanks, 1990 ] <author> K. Church and P. Hanks. </author> <title> Word association norms, mutual information, and lexicography. </title> <journal> Computational Linguistics, </journal> <volume> 16, </volume> <year> 1990. </year>
Reference: [ Clark, 1987 ] <author> E. Clark. </author> <title> The principle of contrast: A constraint on language acquisition. </title> <editor> In B. MacWhinney, editor, </editor> <title> Mechanisms of Language Acquisition. </title> <publisher> Lawrence Erlbaum Inc, </publisher> <address> Hills-dale, NJ, </address> <year> 1987. </year>
Reference-contexts: This would reveal more about Camille's implementation and about its simulation of cognition in general. As mentioned in section 4.1, Clark proposed a reinterpretation of Mutual Exclusivity <ref> [ Clark, 1987 ] </ref> that separated it into three separate principles, some of which are abandoned after the initial learning period, and some of which are maintained. Camille's implementation can be extended to make the same distinctions.
Reference: [ Clark, 1989 ] <author> E. Clark. </author> <title> On the logic of contrast. </title> <journal> Journal of Child Language, </journal> <volume> 15 </volume> <pages> 317-335, </pages> <year> 1989. </year>
Reference: [ Corter and Gluck, 1992 ] <author> J. Corter and M. Gluck. </author> <title> Explaining basic categories: Feature predictability and information. </title> <journal> Psychological Bulletin, </journal> <volume> 111(2) </volume> <pages> 291-303, </pages> <year> 1992. </year>
Reference-contexts: This seemed like the least number of occurrences that would be likely to limit the probability of the instances coincidentally breaking into two groups. "Obvious groups" was defined along the lines of basic levels (see, for example, <ref> [ Waxman et al., 1991; Corter and Gluck, 1992 ] </ref> ). The psychological literature suggests that these are sets of culture-specific concepts which people are likely to use to name things, for example, Chair as opposed to Furniture and Lounge-Chair.
Reference: [ Cullingford, 1977 ] <author> R. Cullingford. </author> <title> Organizing World Knowledge for Story Understanding by Computer. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1977. </year>
Reference-contexts: In order to extend the system's knowledge | and thereby extend the inferences that it could make about unknown words | knowledge about sequences of actions was added to the semantics in the form of scripts <ref> [ Schank and Abelson, 1977; Cullingford, 1977 ] </ref> . Scripts specify common sequences of events or scenes. The classic example of a script describes what happens in a restaurant: the patron enters, is seated, gets a menu, orders, eats, pays, and leaves.
Reference: [ Dennett, 1978 ] <author> D. Dennett. </author> <title> Brainstorms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1978. </year>
Reference-contexts: Why are these similarities important? Dennett gives an answer from the philosophical standpoint: A good psychology of Martians, however unlike us they might be, would certainly yield general principles of psychology or epistemology applicable to human beings. <ref> [ Dennett, 1978, p. 113 ] </ref> Any agent which successfully processes the same sources of knowledge that humans do must have something to say about the properties of that knowledge that make it amenable to processing in general, and about how other agents must process it.
Reference: [ Fernald and Morikawa, 1993 ] <author> A. Fernald and H. Morikawa. </author> <title> Common themes and cultural variations in Japanese and American mothers' speech to infants. Child Development, </title> <booktitle> 64 </booktitle> <pages> 637-656, </pages> <year> 1993. </year>
Reference: [ Gentner, 1978 ] <author> D. Gentner. </author> <title> On relational meaning: The acquisition of verb meaning. Child Development, </title> <booktitle> 49 </booktitle> <pages> 988-998, </pages> <year> 1978. </year>
Reference: [ Gleitman, 1990 ] <author> L. Gleitman. </author> <title> The structural sources of verb meaning. Language Acquisition, </title> <address> I(1):3-55, </address> <year> 1990. </year>
Reference-contexts: The first theory, semantic bootstrapping [ Grimshaw, 1979; Grimshaw, 1981; Pinker, 1984 ] , suggests that children use their knowledge of what a sentence means in order to determine the syntax of that sentence, and by repetition, their language. Syntactic bootstrapping <ref> [ Gleitman, 1990 ] </ref> (see also section 6.3) states that children figure out grammar at an early age and use that knowledge to guide their acquisition of word meanings. Siskind's goal in developing Davra was to demonstrate that neither of these mechanisms is necessary for language acquisition.
Reference: [ Goldin-Meadow et al., 1976 ] <author> S. Goldin-Meadow, M. Seligman, and R. Gelman. </author> <title> Language in the two-year-old. </title> <journal> Cognition, </journal> <volume> 4 </volume> <pages> 189-202, </pages> <year> 1976. </year>
Reference: [ Golinkoff et al., 1987 ] <author> R. Golinkoff, P. Hirsh-Pasek, K. Cauley, and L. Gordon. </author> <title> The eyes have it: Lexical and syntactic comprehension in a new paradigm. </title> <journal> Journal of Child Language, </journal> <volume> 14 </volume> <pages> 23-46, </pages> <year> 1987. </year>
Reference: [ Gopnik and Choi, 1990 ] <author> A. Gopnik and S. Choi. </author> <title> Do linguistic differences lead to cognitive differences? a cross-linguistic study of semantic and cognitive development. </title> <booktitle> First Language, </booktitle> <volume> 10 </volume> <pages> 199-215, </pages> <year> 1990. </year>
Reference: [ Graesser et al., 1987 ] <author> A. Graesser, P. Hopkinson, and C. Schmid. </author> <title> Differences in interconcept organization between nouns and verbs. </title> <journal> Journal of Memory and Language, </journal> <volume> 26 </volume> <pages> 242-253, </pages> <year> 1987. </year>
Reference: [ Granger, 1977 ] <author> R. Granger. Foul-up: </author> <title> A program that figures out meanings of words from context. </title> <booktitle> In Proceedings of Fifth International Joint Conference on Artificial Intelligence, </booktitle> <year> 1977. </year>
Reference-contexts: These systems use a variety of mechanisms to infer the meanings. One is case-based, one uses a graph construction method, and one proposes patterns that a human must check. Some researchers have taken a knowledge-intensive approach to lexical acquisition <ref> [ Granger, 1977; Zernik, 1987a ] </ref> . These systems have somewhat coarse-grained semantic hierarchies along with additional information regarding causes and motives behind actions. This allows these systems to make powerful inferences about word meanings. <p> As will be described in Chapter 5, scripts were the primary knowledge source for lexical acquisition for two of the more prominent earlier systems, Foul-Up and Rina <ref> [ Granger, 1977; Zernik, 1987b ] </ref> . Unfortunately, neither Granger's work nor Zernik's was systematically applied to real-world texts. It is no surprise that these systems performed well when the authors wrote the scripts and the texts that their systems processed. <p> As an example, assume the system encountered the following sentences: Friday, a car swerved off Route 69. The car struck an elm. <ref> [ Granger, 1977, p. 173 ] </ref> If ELI didn't know the word "elm", it would complete processing by putting a place-holder into the OBJECT slot of a PROPEL frame. The place-holder recorded what the word was and that it had an indefinite article. <p> By using a much more powerful syntactic grammar, LINK can make fine distinctions in the syntactic structure of a sentence, and Camille can use these to influence word acquisition. Granger's learning mechanism was based on his "intuitions about how the analogous tasks are performed by people." <ref> [ Granger, 1977, p. 172 ] </ref> Unfortunately, he did little psychological analysis to back up his intuitions. His system is weak in terms of its psychological validity in several ways. First and foremost is the lack of discriminating information described above.
Reference: [ Grimshaw, 1979 ] <author> J. Grimshaw. </author> <title> Complement selection and the lexicon. </title> <journal> Linguistic Inquiry, </journal> <volume> 10 </volume> <pages> 279-326, </pages> <year> 1979. </year>
Reference-contexts: These theories both described strategies that children might use to apply what they already know to the task of learning the rest of their language. The first theory, semantic bootstrapping <ref> [ Grimshaw, 1979; Grimshaw, 1981; Pinker, 1984 ] </ref> , suggests that children use their knowledge of what a sentence means in order to determine the syntax of that sentence, and by repetition, their language.
Reference: [ Grimshaw, 1981 ] <author> J. Grimshaw. </author> <title> Form, function and the language acquisition device. </title> <editor> In C. L . Baker and J. J. McCarthy, editors, </editor> <title> The logical problem of language acquisition. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1981. </year>
Reference-contexts: These theories both described strategies that children might use to apply what they already know to the task of learning the rest of their language. The first theory, semantic bootstrapping <ref> [ Grimshaw, 1979; Grimshaw, 1981; Pinker, 1984 ] </ref> , suggests that children use their knowledge of what a sentence means in order to determine the syntax of that sentence, and by repetition, their language.
Reference: [ Hastings and Lytinen, 1991 ] <author> P. Hastings and S. Lytinen. </author> <title> Automatic acquisition of word meanings. </title> <editor> In D. Powers and L. Reeker, editors, </editor> <booktitle> Proceedings of the AAAI Spring Symposium on Machine Learning of Natural Language and Ontology, Document D-91-09, </booktitle> <institution> University of Kaiserslautern, </institution> <address> FRG, </address> <year> 1991. </year> <note> DFKI. </note>
Reference: [ Hastings and Lytinen, in press ] <author> P. Hastings and S. Lytinen. </author> <title> Acquiring new words from context. Heuristics: </title> <journal> The Journal of Knowledge Engineering, </journal> <note> in press. </note>
Reference: [ Hastings et al., 1991a ] <author> P. Hastings, S. Lytinen, and R. Lindsay. </author> <title> Learning words from context. </title> <editor> In L. Birnbaum and G. Collins, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 55-59, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Hastings et al., 1991b ] <author> P. Hastings, S. Lytinen, and R. Lindsay. </author> <title> Learning words: Computers and kids. </title> <editor> In K. Hammond and D. Gentner, editors, </editor> <booktitle> Proceedings of the 13th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 251-256, </pages> <address> Hillsdale, NJ, 1991. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: [ Hastings et al., 1991c ] <author> P. Hastings, S. Lytinen, and R. Lindsay. </author> <title> Psycholinguistic implications of a computational language-learning model. </title> <editor> In D. Powers, L. Reeker, and B. Humm, editors, </editor> <booktitle> Proceedings of the Workshop on Natural Language Learning of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference: [ Heibeck and Markman, 1987 ] <author> T. Heibeck and E. Markman. </author> <title> Word learning in children: An examination of fast mapping. Child Development, </title> <booktitle> 58 </booktitle> <pages> 1021-1034, </pages> <year> 1987. </year>
Reference: [ Hindle, 1990 ] <author> D. Hindle. </author> <title> Noun classification from predicate-argument structures. </title> <booktitle> In Proceedings of the 28 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 268-275, </pages> <year> 1990. </year>
Reference: [ Hirsh-Pasek and Golinkoff, 1993 ] <author> K. Hirsh-Pasek and R. Golinkoff. </author> <title> Skeletal supports for grammatical learning: What infants bring to the language learning task. </title> <booktitle> Advances in Infancy Research, </booktitle> <volume> 8 </volume> <pages> 299-338, </pages> <year> 1993. </year>
Reference: [ Hobbs et al., 1992 ] <author> J. Hobbs, D. Appelt, M. Tyson, J. Bear, and D Israel. </author> <title> SRI International: Description of the FASTUS system used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference, </booktitle> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Some rather successful systems turned assumptions about the knowledge required for NLP on their ear by dispensing entirely with lexica and grammars and concept representations. Instead they reduced the task to a simple pattern-matching problem (for example, SRI's FASTUS system <ref> [ Hobbs et al., 1992 ] </ref> ). When simple patterns were matched in the input, the appropriate part of the text was extracted. Unfortunately, because these systems do not have full grammar, it is fairly easy to come up with examples that the pattern-matchers cannot handle.
Reference: [ Huffman et al., 1993 ] <author> S. Huffman, C. Miller, and J. Laird. </author> <title> Learning from instruction: A knowledge-level capability within a unified theory of cognition. </title> <booktitle> In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <year> 1993. </year>
Reference-contexts: Attributes are defined as features 2 This brings up an interesting research issue: what would it take for a system to automatically learn word meanings that correspond to complex concepts, for example, sequences of actions? (This is a task that Huffman's system <ref> [ Huffman et al., 1993 ] </ref> is trained to do.) This will be left to future research. 90 of the concept which do not affect their set membership. For example, the color of a car may be important, but it doesn't affect whether or not it's labelled as a car.
Reference: [ Huttenlocher and Lui, 1979 ] <author> J. Huttenlocher and F. Lui. </author> <title> The semantic organization of some simple nouns and verbs. </title> <journal> Journal of verbal learning and verbal behavior, </journal> <volume> 18 </volume> <pages> 141-162, </pages> <year> 1979. </year> <month> 124 </month>
Reference: [ Huttenlocher et al., 1991 ] <author> J. Huttenlocher, W. Haight, A. Bryk, M. Seltzer, and T. Lyons. </author> <title> Early vocabulary growth: Relation to language input and gender. </title> <journal> Developmental Psychology, </journal> <volume> 27(2) </volume> <pages> 236-248, </pages> <year> 1991. </year>
Reference: [ Huyck, 1993 ] <author> C. Huyck. </author> <title> Efficient heuristic natural language parsing. </title> <booktitle> In Proceedings of the 11 th National Conference on Artificial Intelligence, </booktitle> <pages> pages 386-391, </pages> <year> 1993. </year>
Reference-contexts: The biggest problem was that the sentences were so complex that they were seldom completely parsed. Thus Camille was often forced to deal with missing or incorrect information about the examples. As will be further discussed below, Camille would benefit from a parsing system like that described in <ref> [ Huyck, 1993 ] </ref> that heuristically combines the constituents of a sentence. This would increase the probability that the information given to Camille would be correct and complete.
Reference: [ Jackendoff, 1983 ] <author> R. Jackendoff. </author> <title> Semantics and cognition. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: The input to the system consisted of simple sentences that were provided by a trainer, for example, "Bill ran from John." The system also received a set of possible meaning interpretations of an associated scene in the form of Jackendovian conceptual structures <ref> [ Jackendoff, 1983 ] </ref> , for example: ((BE (person3,AT (person1))) _ (BE (person3,AT (person2))) _ (GO (person3,[Path])) _ (GO (person3,FROM (person1))) _ . . . ) Unlike the other similar systems, this meaning representation was not hand-coded. It was provided by an independent program that analyzed a stick-figure animation.
Reference: [ Kaplan et al., 1990 ] <author> S. Kaplan, M. Weaver, and R. </author> <title> French. Active symbols and internal models: Towards a cognitive connectionism. </title> <publisher> Springer Verlag, </publisher> <address> London, </address> <year> 1990. </year> <booktitle> Springer Series on Artificial Intelligence and Society. </booktitle>
Reference-contexts: When no other parse was available, it could use the fact that its only parse had a sub-threshold score to signal the learning mechanism. Intuitively, this coincides with psychological theories that suggest that learning is more likely to occur in humans when their expectations are not met. <ref> [ Kaplan et al., 1990 ] </ref> 4.4 Camille 2.3: Expanding the domain knowledge The basic assumption of complete a priori concept knowledge is too restrictive. A lexical acquisition system should be able to learn concepts as well as word meanings. <p> The general framework consists of an IS-A inheritance hierarchy, a type of representation that is widely used in Artificial Intelligence. Various psychological studies support the existence of hierarchical structures in the brain ( <ref> [ Kaplan et al., 1990 ] </ref> and [ Keil, 1991 ] , for example). At the lowest level, this representation is clearly not "brain-like". It is highly unlikely that the brain uses such a rule-like arrangement for representing constraints. <p> The use of hierarchical mental structures in the brain is a topic of much discussion in the psychological literature ( <ref> [ Kaplan et al., 1990 ] </ref> and [ Keil, 1991 ] , for example). Oddly, most of the previously developed lexical acquisition systems, even those with cognitive goals, have used rather simplistic knowledge representation structures.
Reference: [ Katz and Fodor, 1963 ] <author> Jerrold J. Katz and Jerry A. Fodor. </author> <title> The structure of a semantic theory. </title> <booktitle> Language, </booktitle> <volume> 39, </volume> <year> 1963. </year>
Reference-contexts: One general point about knowledge representation, however, serves as an important introduction to this chapter. Intuitively it is clear (and it has been addressed by <ref> [ Katz and Fodor, 1963 ] </ref> among others) that there are many different aspects of conceptual knowledge. Consider what the word "Arson" brings to mind: techniques, instruments, likely targets, motivations of the actors.
Reference: [ Keil, 1991 ] <author> F. Keil. </author> <title> Theories, concepts, and the acquisition of word meaning. </title> <editor> In J. P. Byrnes and S. A. Gelman, editors, </editor> <booktitle> Perspectives on language and thought: Interrelations in development. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1991. </year>
Reference-contexts: The general framework consists of an IS-A inheritance hierarchy, a type of representation that is widely used in Artificial Intelligence. Various psychological studies support the existence of hierarchical structures in the brain ( [ Kaplan et al., 1990 ] and <ref> [ Keil, 1991 ] </ref> , for example). At the lowest level, this representation is clearly not "brain-like". It is highly unlikely that the brain uses such a rule-like arrangement for representing constraints. But the hierarchical structure has advantages that make it a powerful representation scheme for computers and humans. <p> The use of hierarchical mental structures in the brain is a topic of much discussion in the psychological literature ( [ Kaplan et al., 1990 ] and <ref> [ Keil, 1991 ] </ref> , for example). Oddly, most of the previously developed lexical acquisition systems, even those with cognitive goals, have used rather simplistic knowledge representation structures. Camille's concept hierarchy not only gives it representational economy, but also allows it to make fine distinctions in its lexical inferences.
Reference: [ Lebowitz, 1980 ] <author> M. Lebowitz. </author> <title> Generalization and memory in an integrated understanding system. Research Report No. </title> <type> 186, </type> <institution> Yale University, </institution> <address> New Haven, CN, </address> <month> October </month> <year> 1980. </year>
Reference: [ Lehnert, 1990 ] <author> W. Lehnert. </author> <title> Description of the CIRCUS system as used for MUC-3. </title> <booktitle> In Proceedings, Third Message Understanding Conference (MUC-3), </booktitle> <pages> pages 223-233, </pages> <address> San Diego, CA, 1990. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Then a human knowledgeable with the domain examines the proposed definitions and filters out erroneous ones. From 1500 answer keys and texts in the MUC Terrorism domain, AutoSlog produced 1237 definitions. In five hours, the human user winnowed this set down to 450 good definitions. The CIRCUS parser <ref> [ Lehnert, 1990 ] </ref> , using the AutoSlog definitions, produced Recall and Precision scores that were very close to those produced by the official UMass system which used a (painstakingly) hand-crafted pattern dictionary. Can this be considered lexical acquisition? In one sense it can. <p> Her system, MayTag, works within the environment of an information extraction task and uses the CIRCUS parser <ref> [ Lehnert, 1990 ] </ref> . CIRCUS, like Granger's and Zernik's systems, is a descendant of the Conceptual Dependency approach, and relies only minimally on the syntactic properties of the sentence. It works by designating certain concepts as triggers.
Reference: [ Lehnert, 1992 ] <author> W. Lehnert. </author> <title> University of Massachusetts: MUC-4 test results and analysis. Talk given at Fourth Message Understanding Conference (MUC-4), </title> <month> June </month> <year> 1992. </year>
Reference-contexts: As one group said after facing a second round of the massive development effort that they had undertaken the year before, "We either had to get some new grad students or automate our system." <ref> [ Lehnert, 1992 ] </ref> There has been an obvious trend in the evolution of NLP systems to incorporate some type of acquisition mechanism in order to reduce this most difficult aspect of porting an information extraction system to a new domain.
Reference: [ Lenat, 1990 ] <author> D. Lenat. </author> <title> Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1990. </year>
Reference-contexts: Even the CYC project <ref> [ Lenat, 1990 ] </ref> , which attempts to build a huge knowledge base of common sense information, cannot hope to represent low-level "features" that are very salient to humans like the sound of a robin's song or the color of a sunrise. 15 The nodes in LINK's concept hierarchy serve as <p> Artificial intelligence has not even approached an implementation that can accommodate this range. 1 Even an incredibly ambitious project like Cyc <ref> [ Lenat, 1990 ] </ref> , which is aimed at encoding a massive amount of common-sense knowledge, draws the line at some level of detail, relying on atoms to bridge the gap between perception and basic concepts.
Reference: [ Lytinen and Roberts, 1989a ] <author> S. Lytinen and S. Roberts. </author> <title> Lexical acquisition as a by-product of natural language processing. </title> <booktitle> In 11 th International Conference on Artificial Intelligence, 1989. Lexical Acquisition Workshop. </booktitle>
Reference-contexts: Thus additional examples of the word's use are quite likely to contradict the initial hypothesis, unless, of course, it was correct. As previously mentioned, learning nouns is much easier than learning verbs. Some early work in learning nouns with LINK is described in <ref> [ Lytinen and Roberts, 1989a ] </ref> . Additions to the noun-learning mechanism are described in section 4.3.1. Because semantic constraints are attached to the verbs, it is more difficult to learn unknown verbs, therefore verb acquisition is the primary focus of this thesis.
Reference: [ Lytinen and Roberts, 1989b ] <author> S. Lytinen and S. Roberts. </author> <title> Unifying linguistic knowledge. </title> <institution> AI Laboratory, Univ of Michigan, </institution> <address> Ann Arbor, MI 48109, </address> <year> 1989. </year>
Reference-contexts: Thus it is able to (and often does) make an incorrect initial guess about the meaning of the word and then recover based on additional examples of the word's use. The mechanism is implemented as an extension of the LINK Natural Language Processing (NLP) system <ref> [ Lytinen and Roberts, 1989b; Lytinen, 1991 ] </ref> . The specifics of LINK which relate to the lexical acquisition task are described in the next chapter.
Reference: [ Lytinen et al., 1992a ] <author> S. Lytinen, S. Bhattacharyya, R. Burridge, P. Hastings, C. Huyck, K. Lipinsky, E. McDaniel, and K. Terrell. </author> <title> The LINK system: MUC-4 test results and analysis. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference, </booktitle> <pages> pages 159-163, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [ Lytinen et al., 1992b ] <author> S. Lytinen, S. Bhattacharyya, R. Burridge, P. Hastings, C. Huyck, K. Lipinsky, E. McDaniel, and K. Terrell. </author> <title> Description of the LINK system used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference, </booktitle> <pages> pages 289-295, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [ Lytinen et al., in press ] <author> S. Lytinen, R. Burridge, P. Hastings, and C. Huyck. </author> <title> Description of the LINK system used for MUC-5. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference, </booktitle> <address> San Mateo, CA, </address> <publisher> in press. Morgan Kaufmann Publishers. </publisher> <pages> 125 </pages>
Reference: [ Lytinen, 1988 ] <author> S. Lytinen. </author> <title> Are vague words ambiguous? In S. </title> <editor> Small and G. Cottrell, editors, </editor> <booktitle> Lexical Ambiguity Resolution, </booktitle> <pages> pages 109-128. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: Very few of the other Terrorist actions have such "partner" actions. 4.3 Camille 2.2: Learning ambiguous words Lexical ambiguity has been a thorn in the side of NLP for a long time (for an overview of the difficulties caused by ambiguous words, see <ref> [ Lytinen, 1988 ] </ref> ). Much research has been devoted exclusively to this problem. The goal of most of this work, however, was to devise a mechanism for choosing between word senses. <p> That is, their contribution to the sentence was not 7 A more liberal approach will be discussed in section 4.4. 8 Like the word "others", some additional words in the lexicon were vague. (For a discussion of dealing with vague versus ambiguous words, see <ref> [ Lytinen, 1988 ] </ref> .) "Others" was the only vague word tested because it occurred prominently in such examples as, "11 others were wounded." 46 important to the information extraction task.
Reference: [ Lytinen, 1990 ] <author> S. Lytinen. </author> <title> Robust processing of terse text. </title> <booktitle> In Proceedings of the 1990 AAAI Symposium on Intelligent Text-based Systems, </booktitle> <pages> pages 10-14, </pages> <address> Stanford, CA, </address> <year> 1990. </year>
Reference-contexts: It also includes a summary of the cognitive implications of this work and some ways in which the model could be extended. 4 CHAPTER 2 THE FOUNDATION LINK The lexical acquisition techniques described herein have been implemented as an extension of the LINK NLP system <ref> [ Lytinen, 1990; Lytinen, 1991 ] </ref> . LINK uses a unification grammar (described later in this section) and extends the mechanism of chart parsing 1 by integrating syntactic and semantic processing.
Reference: [ Lytinen, 1991 ] <author> S. Lytinen. </author> <title> A unification-based, integrated natural language processing system. Computers and Mathematics with Applications, </title> <address> 23(6-9):403-418, </address> <year> 1991. </year>
Reference-contexts: Thus it is able to (and often does) make an incorrect initial guess about the meaning of the word and then recover based on additional examples of the word's use. The mechanism is implemented as an extension of the LINK Natural Language Processing (NLP) system <ref> [ Lytinen and Roberts, 1989b; Lytinen, 1991 ] </ref> . The specifics of LINK which relate to the lexical acquisition task are described in the next chapter. <p> It also includes a summary of the cognitive implications of this work and some ways in which the model could be extended. 4 CHAPTER 2 THE FOUNDATION LINK The lexical acquisition techniques described herein have been implemented as an extension of the LINK NLP system <ref> [ Lytinen, 1990; Lytinen, 1991 ] </ref> . LINK uses a unification grammar (described later in this section) and extends the mechanism of chart parsing 1 by integrating syntactic and semantic processing.
Reference: [ MacGregor, 1990 ] <author> R. MacGregor. </author> <title> The evolving technology of classification-based knowledge representation systems. </title> <editor> In J. Sowa, editor, </editor> <booktitle> Principles of Semantic Nets: Explorations in the Representation of Knowledge. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: This lack of semantic discrimination limits the types of inferences that those systems can make. Bananas and bulldogs are physically different and do different things. They should be treated differently. 5.5.1 Classification Systems Why is a classification system like LOOM <ref> [ MacGregor, 1990 ] </ref> included with these lexical acquisition systems? The purpose of this and other such systems is to represent concepts and the relationships between concepts, and to support inference on those concepts.
Reference: [ Mandler, 1988 ] <author> J. Mandler. </author> <title> How to build a baby: On the development of an accessible representational system. </title> <journal> Cognitive Development, </journal> <volume> 3 </volume> <pages> 113-136, </pages> <year> 1988. </year>
Reference: [ Mandler, 1992 ] <author> J. Mandler. </author> <title> How to build a baby: II. Conceptual primitives. </title> <journal> Psychological Review, </journal> <volume> 99 </volume> <pages> 587-604, </pages> <year> 1992. </year>
Reference: [ Markman, 1990 ] <author> E. Markman. </author> <title> Constraints children place on word meanings. </title> <journal> Cognitive Science, </journal> <volume> 14(1) </volume> <pages> 57-77, </pages> <month> Jan-Mar </month> <year> 1990. </year>
Reference-contexts: Later, more of the words tend to overlap in meaning, and therefore, Mutual Exclusivity may steer the learner away from a reasonable hypothesis. This is consonant with accounts of the use of Mutual Exclusivity in children <ref> [ Markman, 1991; Markman, 1990 ] </ref> , which suggest that children use this constraint for only a brief period in their development. Clark [ 1987 ] maintains that language users always follow her more general Principle of Contrast, which states that no two words are exact synonyms.
Reference: [ Markman, 1991 ] <author> E. Markman. </author> <title> The whole object, taxonomic, and mutual exclusivity assumptions as initial constraints on word meanings. </title> <editor> In J. P. Byrnes and S. A. Gelman, editors, </editor> <booktitle> Perspectives on language and thought: Interrelations in development. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1991. </year>
Reference-contexts: They are presented with a large number of words and a large number of possible referents of the words. Psycholinguistic researchers have suggested one mechanism that children might use to overcome this computational difficulty: the Mutual Exclusivity constraint <ref> [ Markman, 1991 ] </ref> . The theory behind this mechanism will be further addressed in section 6.6. <p> Later, more of the words tend to overlap in meaning, and therefore, Mutual Exclusivity may steer the learner away from a reasonable hypothesis. This is consonant with accounts of the use of Mutual Exclusivity in children <ref> [ Markman, 1991; Markman, 1990 ] </ref> , which suggest that children use this constraint for only a brief period in their development. Clark [ 1987 ] maintains that language users always follow her more general Principle of Contrast, which states that no two words are exact synonyms.
Reference: [ Marr, 1982 ] <author> D. Marr. </author> <title> Vision. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> San Francisco, CA, </address> <year> 1982. </year>
Reference-contexts: No silicon model is equivalent in every way to a neural model, so at the lowest level (Marr's mechanism level or Newell's physical level or Pylyshyn's "low road" <ref> [ Marr, 1982; Newell, 1990; Pylyshyn, 1989 ] </ref> ), there can be no absolute equivalence. Cognitive models must all, therefore, abstract to some level at which they assert that their process is the same or similar to human processing.
Reference: [ Mitchell, 1977 ] <author> T. Mitchell. </author> <title> Version spaces: A candidate elimination approach to rule learning. </title> <booktitle> In Proceedings of the Fifth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 305-309, </pages> <year> 1977. </year>
Reference-contexts: Thus, two types of information are supplied by example sentences: information which provides a lower bound on the level in the hierarchy of the meaning of an unknown word, and information which provides an upper bound. One might think that a least-commitment approach to learning, like Mitchell's candidate-elimination algorithm <ref> [ Mitchell, 1977 ] </ref> , would be the best way to approach this task. Mitchell's algorithm used version spaces to represent the set of candidate hypotheses, and slowly narrowed the version space depending on the additional constraints provided by new examples.
Reference: [ Naigles, 1990 ] <author> L. Naigles. </author> <title> Children use syntax to learn verb meanings. </title> <journal> Journal of Child Language, </journal> <volume> 17 </volume> <pages> 357-374, </pages> <year> 1990. </year>
Reference-contexts: In addition, she commented on the likelihood that learning does not occur solely on the basis of a single input, but is "gleaned from the presentation over time of the verb in its particular set of syntactic frames." <ref> [ Naigles, 1990, p. 371 ] </ref> Although at first glance, this might seem to conflict with the "fast-mapping" hypothesis, there is a reconciling explanation. As Carey and Bartlett suggest, children make a quick guess at what a word means.
Reference: [ Newell, 1990 ] <author> A. Newell. </author> <title> Unified Theories of Cognition. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: No silicon model is equivalent in every way to a neural model, so at the lowest level (Marr's mechanism level or Newell's physical level or Pylyshyn's "low road" <ref> [ Marr, 1982; Newell, 1990; Pylyshyn, 1989 ] </ref> ), there can be no absolute equivalence. Cognitive models must all, therefore, abstract to some level at which they assert that their process is the same or similar to human processing.
Reference: [ Pinker, 1984 ] <author> S. Pinker. </author> <title> Language learnability and Language Development. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1984. </year>
Reference-contexts: These theories both described strategies that children might use to apply what they already know to the task of learning the rest of their language. The first theory, semantic bootstrapping <ref> [ Grimshaw, 1979; Grimshaw, 1981; Pinker, 1984 ] </ref> , suggests that children use their knowledge of what a sentence means in order to determine the syntax of that sentence, and by repetition, their language.
Reference: [ Pylyshyn, 1989 ] <author> Z. </author> <title> Pylyshyn. </title> <booktitle> Computing in cognitive science. </booktitle> <editor> In M. Posner, editor, </editor> <booktitle> Foundations of Cognitive Science. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: No silicon model is equivalent in every way to a neural model, so at the lowest level (Marr's mechanism level or Newell's physical level or Pylyshyn's "low road" <ref> [ Marr, 1982; Newell, 1990; Pylyshyn, 1989 ] </ref> ), there can be no absolute equivalence. Cognitive models must all, therefore, abstract to some level at which they assert that their process is the same or similar to human processing.
Reference: [ Quinlan, 1992 ] <author> J. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: Based on examples of the decision trees which MayTag's C4.5 algorithm <ref> [ Quinlan, 1992 ] </ref> made (provided by Cardie), it was clear that the previous and fol 5 The system distinguishes between local and global semantics. 6 Note that there is a bootstrapping problem here.
Reference: [ Resnik, 1992 ] <author> P. </author> <title> Resnik. A class-based approach to lexical discovery. </title> <booktitle> In Proceedings of the 30 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 327-329, </pages> <year> 1992. </year> <month> 126 </month>
Reference: [ Riloff, 1993 ] <author> E. Riloff. </author> <title> Automatically constructing a dictionary for information extraction tasks. </title> <booktitle> In Proceedings of the 11 th National Conference on Artificial Intelligence, </booktitle> <pages> pages 811-816, </pages> <year> 1993. </year>
Reference-contexts: They are of interest because they delineate some of the boundaries of lexical acquisition. Furthermore, if the part of these systems that requires human intervention could be replaced to some extent by a machine learning system, then they could be considered full-fledged lexical acquisition systems. 5.4.1 Autoslog Autoslog <ref> [ Riloff, 1993 ] </ref> is not only geared toward the information extraction task, it exploits the knowledge base that the task provides. The system creates possible definitions of templates that might be useful for extracting text.
Reference: [ Salveter, 1979 ] <author> S. Salveter. </author> <title> Inferring conceptual graphs. </title> <journal> Cognitive Science, </journal> <volume> 3 </volume> <pages> 141-166, </pages> <year> 1979. </year>
Reference-contexts: This makes Camille ideally suited for the information extraction task. 3 Camille is further distinguished by the type of inference mechanism used. It is the only non-trained, incremental word-learning system. Unlike some of the other systems <ref> [ Salveter, 1979; Selfridge, 1986; Siskind, 1990 ] </ref> , Camille does not rely on a trainer to feed it sentences and give it a representation for the meanings of those sentences. It learns automatically, using only the linguistic information that is found in the text.
Reference: [ Salveter, 1980 ] <author> S. Salveter. </author> <title> Inferring conceptual graphs. </title> <booktitle> In Proceedings of the 18 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 13-15, </pages> <year> 1980. </year>
Reference: [ Schank and Abelson, 1977 ] <author> R. Schank and R. Abelson. </author> <title> Scripts, plans, goals, and understanding. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1977. </year>
Reference-contexts: In order to extend the system's knowledge | and thereby extend the inferences that it could make about unknown words | knowledge about sequences of actions was added to the semantics in the form of scripts <ref> [ Schank and Abelson, 1977; Cullingford, 1977 ] </ref> . Scripts specify common sequences of events or scenes. The classic example of a script describes what happens in a restaurant: the patron enters, is seated, gets a menu, orders, eats, pays, and leaves.
Reference: [ Schank, 1973 ] <author> R. Schank. </author> <title> Identification of conceptualizations underlying natural language. </title> <editor> In Roger Schank and K.M. Colby, editors, </editor> <booktitle> Computer Models of Thought and Language. W.H. </booktitle> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1973. </year>
Reference-contexts: Each script had certain trigger words defined for it, many of which were verbs. For example, trigger words for the restaurant script might be "went out", "dined", "restaurant", or "ordered". After a script was selected, SAM invoked a Conceptual Dependency <ref> [ Schank, 1973 ] </ref> analyzer on each sentence and matched the constituents of the sentence with the expectations of one of the scenes in the script. The integration process filled in the slots of the script and created pointers between the sentences in a form of anaphora resolution. <p> As the basis of Child's knowledge representation, he uses a simple sort of Conceptual Dependency <ref> [ Schank, 1973 ] </ref> formalism. This determines not only how domain knowledge is represented, but also how syntax is | or is not | used. <p> His system was called "Foul-Up" because when the NLP system encountered an unknown word, the parser could not continue without a special mechanism to doctor the parse structure. His program was implemented as an extension of a system called SAM, which was based on Schank's Conceptual Dependency framework <ref> [ Schank, 1973 ] </ref> and analyzed news reports using scripts [ Schank, 1981 ] so that they could be paraphrased. This section describes SAM's parsing process, the extensions that Granger made to acquire word meanings, and an analysis of the merits of his approach.
Reference: [ Schank, 1981 ] <author> R. Schank. </author> <title> Inside Computer Understanding. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1981. </year>
Reference-contexts: His program was implemented as an extension of a system called SAM, which was based on Schank's Conceptual Dependency framework [ Schank, 1973 ] and analyzed news reports using scripts <ref> [ Schank, 1981 ] </ref> so that they could be paraphrased. This section describes SAM's parsing process, the extensions that Granger made to acquire word meanings, and an analysis of the merits of his approach. As previously mentioned, SAM was based on Schank's Conceptual Dependency (CD) framework.
Reference: [ Selfridge, 1986 ] <author> Mallory Selfridge. </author> <title> A computer model of child language learning. </title> <journal> Artificial Intelligence, </journal> <volume> 29 </volume> <pages> 171-216, </pages> <year> 1986. </year>
Reference-contexts: This makes Camille ideally suited for the information extraction task. 3 Camille is further distinguished by the type of inference mechanism used. It is the only non-trained, incremental word-learning system. Unlike some of the other systems <ref> [ Salveter, 1979; Selfridge, 1986; Siskind, 1990 ] </ref> , Camille does not rely on a trainer to feed it sentences and give it a representation for the meanings of those sentences. It learns automatically, using only the linguistic information that is found in the text. <p> The visual input consists of an action and a set of features, for example: (PTRANS Actor (Father) Object (Ball) To (Top (Table)) Time (Past)) <ref> [ Selfridge, 1986, p. 196 ] </ref> . Child assumes this to be the meaning of the input sentence. The input sentence is also provided by the trainer and is either an imperative or a declarative description of an action. <p> Second, polysemy (multiple word senses) is not handled. It is interesting to note that Selfridge draws from Child the conclusion that "children use knowledge of known words to limit the hypotheses about an unknown word." <ref> [ Selfridge, 1986, p. 210 ] </ref> Camille does the same thing, but in a totally different manner. Child has for each word a set of attributes that add up to the meaning of the word.
Reference: [ Selfridge, 1991 ] <author> M. Selfridge. </author> <title> How do children learn to recognize ungrammatical sentences? In D. </title> <editor> Powers and L. Reeker, editors, </editor> <booktitle> Proceedings of the AAAI Spring Symposium on Machine Learning of Natural Language and Ontology, Document D-91-09. </booktitle> <institution> DFKI, University of Kaiserslautern, </institution> <address> FRG, </address> <year> 1991. </year>
Reference: [ Shatz and Ebeling, 1991 ] <author> M. Shatz and K. Ebeling. </author> <title> Patterns of language learning-related behaviours: evidence for self-help in acquiring grammar. </title> <journal> Journal of Child Language, </journal> <volume> 18 </volume> <pages> 295-313, </pages> <year> 1991. </year>
Reference: [ Sidner, 1979 ] <author> C. L. Sidner. </author> <title> Towards a Computational Theory of Definite Anaphora Comprehension in English Discourse. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1979. </year>
Reference-contexts: Knowledge about plans and goals (like that used by Wilensky [ 1978 ] ) can be used not only to make inferences about information not described in a text, but also about what is described if some important words are not known. Discourse information (e.g. <ref> [ Sidner, 1979 ] </ref> ) could add similar knowledge about texts. Instead of describing only sequences of actions like scripts do, discourse information could specify the communication goals of the text.
Reference: [ Siskind, 1990 ] <author> J. Siskind. </author> <title> Acquiring core meanings of words. </title> <booktitle> In Proceedings of the 28 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 143-156, </pages> <year> 1990. </year>
Reference-contexts: This makes Camille ideally suited for the information extraction task. 3 Camille is further distinguished by the type of inference mechanism used. It is the only non-trained, incremental word-learning system. Unlike some of the other systems <ref> [ Salveter, 1979; Selfridge, 1986; Siskind, 1990 ] </ref> , Camille does not rely on a trainer to feed it sentences and give it a representation for the meanings of those sentences. It learns automatically, using only the linguistic information that is found in the text. <p> that the key to success for the system (in the absence of one or both of the simplifying psycholinguistic strategies, presumably) was what he called "cross-situational learning." [ 1991, p. 159 ] The system required several different examples of a word's 3 The predecessor to Davra, Maimra, is described in <ref> [ Siskind, 1990 ] </ref> . 63 use and several different syntactic structures to be able to make its inferences. This underscores a weakness of the system.
Reference: [ Siskind, 1991 ] <author> J. Siskind. </author> <title> Dispelling myths about language bootstrapping. </title> <editor> In D. Powers and L. Reeker, editors, </editor> <booktitle> Proceedings of the AAAI Spring Symposium on Machine Learning of Natural Language and Ontology, Document D-91-09, </booktitle> <institution> University of Kaiserslautern, </institution> <address> FRG, </address> <year> 1991. </year> <note> DFKI. </note>
Reference: [ Spelke, 1982 ] <author> E. Spelke. </author> <title> Perceptual knowledge of objects in infancy. </title> <editor> In J. Mehler, E. Walker, and M. Garrett, editors, </editor> <title> Perspectives on mental representations. </title> <publisher> Lawrence Erlbaum Associates, Inc, </publisher> <address> Hillsdale, NJ, </address> <year> 1982. </year>
Reference: [ Sundheim, 1992 ] <author> B. Sundheim. </author> <title> Overview of the fourth message understanding evaluation and conference. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference, </booktitle> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: On the lowest end of the knowledge-use spectrum are the statistics-based methods described in [ Brent, 1991; Brent, 1993a; Brent, 1993b; Church and 1 For a general description of an information extraction task, see <ref> [ Sundheim, 1992 ] </ref> . For a description of the LINK implementation for MUC, see [ Lytinen et al., 1992a; Lytinen et al., 1992b; Lytinen et al., in press ] . 1 Hanks, 1990; Hindle, 1990; Resnik, 1992; Yarowsky, 1992; Zernik, 1991 ] . <p> of questions about a fixed set of actions | in short, the information extraction task. 1 The model proposed by Kaplan, Weaver, and French [ 1990 ] , however, does suggest a promising research direction. 89 What is required to successfully perform the information extraction task? In the MUC competitions <ref> [ Sundheim, 1992 ] </ref> , systems with greatly varying depths of knowledge representation performed at similar levels of efficiency. Some rather successful systems turned assumptions about the knowledge required for NLP on their ear by dispensing entirely with lexica and grammars and concept representations.
Reference: [ Tomasello, 1992 ] <author> M. Tomasello. </author> <title> First Verbs: A case study of early grammatical development. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1992. </year> <month> 127 </month>
Reference: [ Turing, 1950 ] <author> A. </author> <title> Turing. </title> <journal> Computing machinery and intelligence. Mind, </journal> <volume> 59 </volume> <pages> 433-460, </pages> <year> 1950. </year>
Reference: [ Waxman et al., 1991 ] <author> S. Waxman, E. Shipley, and B. Shepperson. </author> <title> Establishing new subcategories: The role of category labels and existing knowledge. Child Development, </title> <booktitle> 62 </booktitle> <pages> 127-138, </pages> <year> 1991. </year>
Reference-contexts: This seemed like the least number of occurrences that would be likely to limit the probability of the instances coincidentally breaking into two groups. "Obvious groups" was defined along the lines of basic levels (see, for example, <ref> [ Waxman et al., 1991; Corter and Gluck, 1992 ] </ref> ). The psychological literature suggests that these are sets of culture-specific concepts which people are likely to use to name things, for example, Chair as opposed to Furniture and Lounge-Chair.
Reference: [ Wilensky, 1978 ] <author> R. Wilensky. </author> <title> Understanding goal-based stories. </title> <type> Research Report 140, </type> <institution> Department of Computer Science, Yale University, </institution> <year> 1978. </year>
Reference: [ Winograd, 1987 ] <author> T. Winograd. </author> <title> Language as a Cognitive Process. Vol. 1: Syntax. </title> <publisher> Addison-Wesley Publishing, </publisher> <address> Reading, MA, </address> <year> 1987. </year>
Reference-contexts: Semantic nodes can also specify constraints on relationships between nodes. For example, the definition: (define-sem Arson is-a (Terrorist-Act) formulae (((Object) = Building))) specifies that Arson is a type of Terrorist-Act, and constrains its Object to be a type of Building. 1 See <ref> [ Winograd, 1987 ] </ref> for a description of chart parsing. 2 Appendix B contains a description of a second domain and complete testing results for cross-domain verification. 3 Throughout this document, the Sans Serif type style will be used to display the names of concepts.
Reference: [ Yarowsky, 1992 ] <author> D. Yarowsky. </author> <title> Word-sense disambiguation using statistical models of roget's categories trained on large corpora. </title> <booktitle> In Proceedings, COLING-92, </booktitle> <year> 1992. </year>
Reference: [ Zernik, 1987a ] <author> U. Zernik. </author> <title> Strategies in language acquisitions: Learning phrases from examples in context. </title> <type> Technical Report UCLA-AI-87-1, </type> <institution> UCLA, </institution> <month> January </month> <year> 1987. </year>
Reference-contexts: These systems use a variety of mechanisms to infer the meanings. One is case-based, one uses a graph construction method, and one proposes patterns that a human must check. Some researchers have taken a knowledge-intensive approach to lexical acquisition <ref> [ Granger, 1977; Zernik, 1987a ] </ref> . These systems have somewhat coarse-grained semantic hierarchies along with additional information regarding causes and motives behind actions. This allows these systems to make powerful inferences about word meanings.
Reference: [ Zernik, 1987b ] <author> Uri Zernik. </author> <title> How do machine language paradigms fare in language acquisition. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <address> Los Altos, CA, 1987. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As will be described in Chapter 5, scripts were the primary knowledge source for lexical acquisition for two of the more prominent earlier systems, Foul-Up and Rina <ref> [ Granger, 1977; Zernik, 1987b ] </ref> . Unfortunately, neither Granger's work nor Zernik's was systematically applied to real-world texts. It is no surprise that these systems performed well when the authors wrote the scripts and the texts that their systems processed.
Reference: [ Zernik, 1991 ] <author> U. Zernik. </author> <title> Train1 vs. train2: Tagging word senses in corpus. </title> <editor> In U. Zernik, editor, </editor> <title> Lexical Acquisition: Exploiting On-line Resources to Build a Lexicon. </title> <publisher> Lawrence Erlbaum Associates, Inc, </publisher> <address> Hillsdale, NJ, </address> <year> 1991. </year> <month> 128 </month>
References-found: 98

