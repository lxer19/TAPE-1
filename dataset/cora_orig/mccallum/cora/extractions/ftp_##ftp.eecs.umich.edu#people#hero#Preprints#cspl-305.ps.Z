URL: ftp://ftp.eecs.umich.edu/people/hero/Preprints/cspl-305.ps.Z
Refering-URL: http://www.eecs.umich.edu/~hero/det_est.html
Root-URL: http://www.cs.umich.edu
Title: Recursive Algorithms for Computing the Cramer-Rao Bound  Communication and Signal Processing  
Author: Alfred O. Hero, Mohammad Usman, Anne C. Sauve, and Jeffrey A. Fessler 
Note: 1 This work was supported in part by the National Science Foundation under grant BCS-9024370, a Government of Pakistan Postgraduate Fellowship, NIH grants CA-54362 and CA-60711, and DOE grant DE-FG02-87ER60561  
Address: Ann Arbor 48109-2122  
Affiliation: Department of EECS The University of Michigan  
Date: 305, Nov. 1996  
Pubnum: Technical Report No.  Laboratory (CSPL)  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. R. Kuruc, </author> <title> "Lower bounds on multiple-source direction finding in the presence of direction-dependent antenna-array-calibration errors," </title> <type> Technical Report 799, </type> <institution> M.I.T. Lincoln Laboratory, </institution> <month> Oct., </month> <year> 1989. </year>
Reference-contexts: For example in image analysis one may be primarily interested in a small q-pixel region of interest (ROI) corresponding to a tumor or lesion. In this case methods of sequential partitioning <ref> [1] </ref> can be used to calculate the q fi q submatrix of the inverse Fisher matrix yielding the CR bound for the ROI. However, while requiring fewer flops than required for direct full matrix inversion, this algorithm still requires the same order O (n 3 ) of flops. <p> elements f ij of the matrix F Y D p = Q + diag (jF Y Qj 1) : (4) where Q = ((f ij )) jijj&lt;p is a (2p 1)-diagonal matrix, jAj denotes a matrix whose elements are the absolute values of those of the matrix A, 1 = <ref> [1; : : : ; 1] </ref> T , and diag (x) is a diagonal matrix with the elements of the vector x along the diagonal. <p> Furthermore, for x = 1; (D p A)1 = 0, where 1 = <ref> [1; :::; 1] </ref> T . This implies that D p A has at least one zero eigenvalue, therefore D p A is rank deficient with rank at most n 1. 2
Reference: [2] <author> A. O. Hero and J. A. Fessler, </author> <title> "A recursive algorithm for computing CR-type bounds on estimator covariance," </title> <journal> IEEE Trans. on Inform. Theory, </journal> <volume> vol. 40, </volume> <pages> pp. 1205-1210, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: However, while requiring fewer flops than required for direct full matrix inversion, this algorithm still requires the same order O (n 3 ) of flops. In <ref> [2] </ref> and [3] a recursive method was presented for approximating columns of the CR bound for unbiased estimation of an element of the parameter vector and for non-singular FIM. This method was derived by specifying a FIM-dominating matrix which allowed a geometric series decomposition of the inverse FIM. <p> The main advantage of this algorithm is its monotone convergence which 2 guarantees a valid and improving lower bound on estimator covariance at each iteration. As will be shown in this paper, the price of monotonicity is slow convergence. In this paper we place the method of <ref> [2] </ref> in the setting of a general class of iterative algorithms, known as stationary and non-stationary linear equation solvers [4]. <p> The following iterative equation solvers are considered: monotone and non-monotone matrix splitting algorithms, such as the method of <ref> [2] </ref>, Jacobi, Gauss-Seidel, and preconditioned conjugate gradient algorithms. The extension of these algorithms to singular FIM is achieved by using matrix perturbation. We illustrate these algorithms for an important class of inverse problems arising in deconvolution, image restoration, and tomographic reconstruction. <p> the method of Gaussian elimination (2n 3 =3 flops) or by methods which exploit positive definiteness of F Y , such as the QR and Cholesky decompositions (n 3 =3 flops) [9, Sec. 4.2]. 3 Recursive CR Bound Algorithms for Non-Singular FIM Here we describe the monotonically convergent algorithm of <ref> [2] </ref> in the context of standard splitting iterations [9, Sec. 10.1], also known as stationary iterations [4], for approximating the solution to the linear equation (2). 3.1 Splitting Algorithms Let F and N be n fi n matrices which split F Y in the sense that F N = F Y <p> Then it can be shown in an analogous manner to <ref> [2, Eq. (9)] </ref> that (k+1) (k) = _m T F 1 2 NF 1 2 _m 0. <p> We can ensure that (F 1 N) = (I F 1 F Y ) &lt; 1 by selecting a preconditioning matrix F which dominates F Y in the sense that F F Y is non-negative definite <ref> [2] </ref>. In [2] a diagonal preconditioning matrix, denoted F EM , was specified as the FIM associated with the complete data for a complete/incomplete data formulation of the estimation problem. <p> We can ensure that (F 1 N) = (I F 1 F Y ) &lt; 1 by selecting a preconditioning matrix F which dominates F Y in the sense that F F Y is non-negative definite <ref> [2] </ref>. In [2] a diagonal preconditioning matrix, denoted F EM , was specified as the FIM associated with the complete data for a complete/incomplete data formulation of the estimation problem. <p> Under the standard assumption that for each detector the mean number of detected photons are strictly greater than zero the FIM is non-singular and has the form <ref> [2] </ref> F Y () = A T D 1 A: (10) where D = diag is a diagonal matrix containing the mean number of detected photons at each detector. <p> The monotone algorithm labeled EM uses the diagonal preconditioning matrix F = diag i fli 1= i given in <ref> [2] </ref>, where A fli is the i-th column of A and diag i (a i ) denotes a diagonal matrix with the scalars a i arranged along the diagonal.
Reference: [3] <author> A. O. Hero and J. A. Fessler, </author> <title> "A fast recursive algorithm for computing CR-type bounds for image reconstruction problems," </title> <booktitle> in Proc. of IEEE Nuclear Science Symposium, </booktitle> <pages> pp. 1188-1190, </pages> <address> Orlando, FA, </address> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: However, while requiring fewer flops than required for direct full matrix inversion, this algorithm still requires the same order O (n 3 ) of flops. In [2] and <ref> [3] </ref> a recursive method was presented for approximating columns of the CR bound for unbiased estimation of an element of the parameter vector and for non-singular FIM. This method was derived by specifying a FIM-dominating matrix which allowed a geometric series decomposition of the inverse FIM.
Reference: [4] <author> R. Barrett and etal, </author> <title> Templates for the solution of linear systems: building blocks for iterative methods, </title> <publisher> SIAM Press, </publisher> <address> Philadelphia, </address> <year> 1994. </year>
Reference-contexts: As will be shown in this paper, the price of monotonicity is slow convergence. In this paper we place the method of [2] in the setting of a general class of iterative algorithms, known as stationary and non-stationary linear equation solvers <ref> [4] </ref>. Using this setting we develop rapidly convergent CR bound approximation methods which can be applied to the cases of biased parameter estimation, estimation of a function of the parameters, and singular FIM. <p> of F Y , such as the QR and Cholesky decompositions (n 3 =3 flops) [9, Sec. 4.2]. 3 Recursive CR Bound Algorithms for Non-Singular FIM Here we describe the monotonically convergent algorithm of [2] in the context of standard splitting iterations [9, Sec. 10.1], also known as stationary iterations <ref> [4] </ref>, for approximating the solution to the linear equation (2). 3.1 Splitting Algorithms Let F and N be n fi n matrices which split F Y in the sense that F N = F Y . <p> the eigenvalue spread of F 1 F Y decreases, only a bound on the asymptotic rate of decrease of the norm of the residuals kr (k) k 2 = kfi (k) F 1 Y _mk F Y is available, where kuk 2 F Y = u T F Y u <ref> [4, Sec. 2.3.1] </ref>.
Reference: [5] <author> A. O. Hero, J. A. Fessler, and M. Usman, </author> <title> "Exploring bias-variance tradeoffs using the uniform CR bound," </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> vol. </volume> <booktitle> in review, </booktitle> , <year> 1994. </year>
Reference-contexts: The CR lower bound on the variance of the estimator ^ t (Y ) is <ref> [5, 6, 7] </ref> var ( ^ t) _m T F + where _m = r m is the column gradient vector [ @m @ 1 ; :::; @m Y denotes the Moore Penrose pseudo-inverse [8]. <p> When F Y is non-singular F + Y = F 1 Y the ordinary matrix inverse. Note that the pseudo-inverse form of the CR bound is generally not generally achievable unless the _m lies in the rangespace of F Y <ref> [5] </ref>. Throughout this paper we will be interested in calculating the right hand side of (1). The results are easily extended to calculation of the uniform lower bound presented in [5] which applies to estimators with unknown mean function m (). <p> form of the CR bound is generally not generally achievable unless the _m lies in the rangespace of F Y <ref> [5] </ref>. Throughout this paper we will be interested in calculating the right hand side of (1). The results are easily extended to calculation of the uniform lower bound presented in [5] which applies to estimators with unknown mean function m (). The results are easily extended to 3 estimation of vector-valued functions [t 1 (); : : : ; t q ()] T , (q t n) by repeated application of the algorithms given here.
Reference: [6] <author> C. R. Rao, </author> <title> Linear Statistical Inference and Its Applications, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: The CR lower bound on the variance of the estimator ^ t (Y ) is <ref> [5, 6, 7] </ref> var ( ^ t) _m T F + where _m = r m is the column gradient vector [ @m @ 1 ; :::; @m Y denotes the Moore Penrose pseudo-inverse [8].
Reference: [7] <author> B. Porat, </author> <title> Digital processing of random signals, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood-Cliffs N.J., </address> <year> 1994. </year>
Reference-contexts: The CR lower bound on the variance of the estimator ^ t (Y ) is <ref> [5, 6, 7] </ref> var ( ^ t) _m T F + where _m = r m is the column gradient vector [ @m @ 1 ; :::; @m Y denotes the Moore Penrose pseudo-inverse [8].
Reference: [8] <author> F. A. Graybill, </author> <title> Matrices with Applications in Statistics, </title> <publisher> Wadsworth Publishing Co., </publisher> <address> Belmont CA, </address> <year> 1983. </year>
Reference-contexts: The CR lower bound on the variance of the estimator ^ t (Y ) is [5, 6, 7] var ( ^ t) _m T F + where _m = r m is the column gradient vector [ @m @ 1 ; :::; @m Y denotes the Moore Penrose pseudo-inverse <ref> [8] </ref>. When F Y is non-singular F + Y = F 1 Y the ordinary matrix inverse. Note that the pseudo-inverse form of the CR bound is generally not generally achievable unless the _m lies in the rangespace of F Y [5].
Reference: [9] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations (2nd Edition), </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: Y x = _m: (2) For example, when F Y is non-singular the solution x can be found by the method of Gaussian elimination (2n 3 =3 flops) or by methods which exploit positive definiteness of F Y , such as the QR and Cholesky decompositions (n 3 =3 flops) <ref> [9, Sec. 4.2] </ref>. 3 Recursive CR Bound Algorithms for Non-Singular FIM Here we describe the monotonically convergent algorithm of [2] in the context of standard splitting iterations [9, Sec. 10.1], also known as stationary iterations [4], for approximating the solution to the linear equation (2). 3.1 Splitting Algorithms Let F and <p> flops) or by methods which exploit positive definiteness of F Y , such as the QR and Cholesky decompositions (n 3 =3 flops) [9, Sec. 4.2]. 3 Recursive CR Bound Algorithms for Non-Singular FIM Here we describe the monotonically convergent algorithm of [2] in the context of standard splitting iterations <ref> [9, Sec. 10.1] </ref>, also known as stationary iterations [4], for approximating the solution to the linear equation (2). 3.1 Splitting Algorithms Let F and N be n fi n matrices which split F Y in the sense that F N = F Y . <p> It is well known <ref> [10, 9] </ref> that if (F 1 N) &lt; 1 then fi (k) converges to the vector F 1 Y _m and the approximating sequence (k) converges to the CR bound _m T F 1 Y _m . <p> Many well known algorithms fall into the category of general splitting iterations, such as the Jacobi (J) and Gauss-Siedel (GS) iterations <ref> [9, Sec. 10.1] </ref>. Let the FIM have the additive decomposition F Y = D + U + L where D is diagonal and U and L are upper and lower triangular matrices with zero diagonal entries. <p> to minimizing the root convergence factor over all (2p 1)-diagonal preconditioning matrices F satisfying F F Y 0. 4 Preconditioned Conjugate Gradient Algorithm When the FIM F Y in the linear equation (2) is positive definite, the preconditioned conjugate gradient (CG) algorithm can be used to approximate the solution x <ref> [9, Sec. 10.3] </ref> and we 6 obtain an approximation to the CR bound _m T x. The CG algorithm can be interpreted as resulting from non-monotone and non-stationary acceleration of the splitting algorithm (3) via the introduction of time-varying acceleration parameters [9, sec 10.3.6]. <p> The CG algorithm can be interpreted as resulting from non-monotone and non-stationary acceleration of the splitting algorithm (3) via the introduction of time-varying acceleration parameters <ref> [9, sec 10.3.6] </ref>. The CG algorithm converges to the exact solution x in n iterations when run with infinite precision arithmetic. However, when run to termination it is not computationally competitive with Gaussian elimination. <p> The CG algorithm converges to the exact solution x in n iterations when run with infinite precision arithmetic. However, when run to termination it is not computationally competitive with Gaussian elimination. We will show that with proper preconditioning matrix F the following prematurely stopped preconditioned CG algorithm <ref> [9, Algorithm 10.3.1] </ref> is quite competitive with direct methods and has significantly faster convergence than MS iterations. 4.1 Preconditioned CG Recursion for CRB The following preconditioned CG iteration requires initialization of fi (0) and r (0) = _m F Y fi (0) : Solve: Fz (k) = r (k) ff (k) <p> is known that _m lies in the range space of F Y the CR bound _m T F + Y _m for singular F Y can be found in 4n 3 =3 flops using the QR factorization to solve for the min-norm solution x to F Y x = _m <ref> [9, Alg. 5.7.2] </ref>. However, typically the range space of F Y is unknown and much more computationally intensive algorithms are required, e.g. the singular value decomposition (SVD) (20n 3 flops). <p> that we have discussed the computational bottlenecks occur in computing the vector-matrix product F Y fi (k) (2n 2 flops) and, for non diagonal F, in computing the solution to the preconditioning equation Ffi (k) = u (np 2 + 8np flops for F = D p a p-diagonal preconditioner <ref> [9, Sec. 4.3.6] </ref>). However due to the simple sparse product form (10) of F Y the vector-matrix multiplication can be accomplished in two nested vector-matrix multiplications: F Y fi (k) = A T h i (4mn- flops). <p> or 5%, and * = 0:00074 was selected according to (9) as the average of the induced lower and upper bounds on *: * = [ 1 2 ffi min + 1 In a larger problem this minimum positive eigenvalue would have to be estimated, e.g. using successive power iterations <ref> [9] </ref>. below the true CR bound, which we calculated to be 43.5, by approximately 5%. Note that the GS algorithm does not converge to within 5% of the limit before 250 iterations.
Reference: [10] <author> R. A. Horn and C. R. Johnson, </author> <title> Matrix Analysis, </title> <address> Cambridge, </address> <year> 1985. </year>
Reference-contexts: It is well known <ref> [10, 9] </ref> that if (F 1 N) &lt; 1 then fi (k) converges to the vector F 1 Y _m and the approximating sequence (k) converges to the CR bound _m T F 1 Y _m . <p> P j6=k1;k+1 . . . . . . f n1;n P j6=n1 7 7 7 7 7 7 7 5 The following lemma follows directly from the diagonal dominance of the matrix D p F Y = diag (jF Y Qj 1) (F Y Q p ) <ref> [10, Corollary 7.2.1] </ref> and the easily verifiable fact that when F Y has non-negative entries: (D p F Y )1 = 0. We give a direct proof in Appendix B. Lemma 1 Assume that F Y is an n fi n symmetric matrix.
Reference: [11] <author> D. M. Young, </author> <title> Iterative solution of large linear systems, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: Therefore, the J iterations may not converge and are generally not monotone. In general to ensure convergence the Jacobi algorithm must be relaxed, corresponding to using N = (1 )D (U + L) in place of (U + L) where 2 (1; 2) is an over-relaxation parameter <ref> [11] </ref>. The GS iteration is obtained from the general splitting iteration by identifying F = D + L and N = U in (3). Like J iterations the GS iterations yield non-monotonic approximations. On the other hand, the GS iterations always converge for positive definite FIM.
Reference: [12] <author> S. D. Booth and J. A. Fessler, </author> <title> "Combined diagonal/fourier preconditioning methods for image reconstruction in emission tomography," </title> <booktitle> in IEEE Int. Conf. on Image Processing, </booktitle> <volume> vol. </volume> <pages> 3, </pages> <address> Crystal City, VA, </address> <month> Nov. </month> <year> 1995. </year> <month> 19 </month>
Reference-contexts: The conjugate gradient algorithm labeled CGDF uses a special preconditioner F consisting of a diagonal matrix, chosen to make F Y approximately circulant, followed by a Fourier-type preconditioner. The preconditioner used in CGDF is tailored to the spatially invariant PET application and is described in <ref> [12] </ref> in the context of fast least squares PET reconstruction algorithms. The GS algorithm shows very rapid convergence which is only slightly outdone by CGDF. On the 12 Alg. Asy. Conv.
References-found: 12

