URL: http://www.robotics.stanford.edu/~koller/papers/focs94.ps
Refering-URL: http://www.robotics.stanford.edu/~koller/papers/focs94.html
Root-URL: http://www.robotics.stanford.edu
Email: karger@lcs.mit.edu  daphne@cs.berkeley.edu  
Title: (De)randomized Construction of Small Sample Spaces in N C  
Author: David R. Karger Daphne Koller 
Address: Murray Hill, NJ 07974  Berkeley, CA 94720  
Affiliation: AT&T Bell Laboratories  Computer Science Division University of California  
Abstract: Koller and Megiddo introduced the paradigm of constructing compact distributions that satisfy a given set of constraints, and showed how it can be used to efficiently derandomize certain types of algorithm. In this paper, we significantly extend their results in two ways. First, we show how their approach can be applied to deal with more general expectation constraints. More importantly, we provide the first parallel (N C) algorithm for constructing a compact distribution that satisfies the constraints up to a small relative error. This algorithm deals with constraints over any event that can be verified by finite automata, including all independence constraints as well as constraints over events relating to the parity or sum of a certain set of variables. Our construction relies on a new and independently interesting parallel algorithm for converting a solution to a linear system into an almost basic approximate solution to the same system. We use these techniques in the first N C derandomization of an algorithm for constructing large independent sets in d-uniform hypergraphs for arbitrary d. We also show how the linear programming perspective suggests new proof techniques which might be useful in general probabilistic analysis. 
Abstract-found: 1
Intro-found: 1
Reference: [ABI86] <author> N. Alon, L. Babai, and A. Itai. </author> <title> A fast and simple randomized parallel algorithm for the maximal independent set problem. </title> <journal> Journal of Algorithms, </journal> <volume> 7 </volume> <pages> 567-583, </pages> <year> 1986. </year>
Reference-contexts: Furthermore, the emphasis has been on constructions in N C, so as to allow a derandomization of parallel algorithms. Most of these works construct a distribution that only approximately satisfies some of the required constraints. The early works <ref> [Jof74, Lub86, ABI86] </ref> generate distributions that err on the probabilities Pr (X i = b i ), but precisely satisfy the actual independence constraints such as those on Pr (X i = b i ; X j = b j ). <p> The fact that we can construct a distribution in N C allows us to derandomize parallel algorithms. We apply our techniques to the problem of finding a large independent set in a d-uniform hypergraph. The underlying randomized algorithm, described by Alon, Babai, and Itai <ref> [ABI86] </ref>, was derandomized in the same paper for fixed values of d. It was later derandomized also for d = O (poly log n) by Berger and Rompel [BR91] and Motwani, Naor, and Naor [MNN89]. <p> For illustration, we use the example of finding large independent sets in sparse hypergraphs. The problem description and the randomized algorithm for its solution are taken from <ref> [ABI86] </ref>. The analysis of the problem in terms of constraints is taken from [KM93]. We derandomize the algorithm, thus producing what is, to the best of our knowledge, the first N C algorithm that completely solves the problem.
Reference: [AGHP90] <author> N. Alon, O. Goldreich, J. Hastad, and R. Peralta. </author> <title> Simple constructions of almost k-wise independent random variables. </title> <booktitle> In Proceedings of the 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 544-553, </pages> <year> 1990. </year>
Reference-contexts: Therefore this approach can be used only if the desired d is constant. These bounds can be circumvented by allowing some error in the independence constraints. In <ref> [NN93, AGHP90, AMN92, EGL + 92] </ref>, the probabilities of the relevant events (as described above) are required to be within an additive factor of *. The size of the result ing distribution is polynomial in 1=*, so that we must choose * to be at least 1=poly (n).
Reference: [AMN92] <author> Y. Azar, R. Motwani, and J. Naor. </author> <title> Approximating arbitrary probability distributions using small sample spaces. </title> <type> Unpublished manuscript, </type> <year> 1992. </year>
Reference-contexts: Therefore this approach can be used only if the desired d is constant. These bounds can be circumvented by allowing some error in the independence constraints. In <ref> [NN93, AGHP90, AMN92, EGL + 92] </ref>, the probabilities of the relevant events (as described above) are required to be within an additive factor of *. The size of the result ing distribution is polynomial in 1=*, so that we must choose * to be at least 1=poly (n).
Reference: [BG92] <author> M. Blum and O. Goldreich. </author> <title> Towards a computational theory of statistical tests. </title> <booktitle> In Proceedings of the 33rd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 406-416, </pages> <year> 1992. </year>
Reference-contexts: It has also been looked at by Blum and Goldreich <ref> [BG92] </ref>, who take a more general look at the relationship between the power of a computational model and the pseudorandom sequences which can "fool" it. algorithm for approximate basis crashing, and use it to construct small distributions approximately satisfying the constraints. <p> We have already given methods for fooling finite automata; can we extend these techniques to fooling larger language classes such as logspace (two way finite automata)? Further investigation of the connection between our results and those of <ref> [Nis90, BG92] </ref> seems warranted. Another goal is to examine those randomized parallel algorithms which have so far resisted derandomization, and attempt to apply the techniques developed here.
Reference: [BM93] <author> P. A. Beling and N. Megiddo. </author> <title> Using fast matrix multiplication to find basic solutions. </title> <type> Technical Report RJ 9234, </type> <institution> IBM Research Division, </institution> <year> 1993. </year>
Reference-contexts: This basic solution corresponds to a distribution 0 whose support is precisely the set of positive entries in the basic solution. The process of constructing 0 from is known as basis crashing. The fastest basis crashing algorithm known is due to Beling and Megiddo <ref> [BM93] </ref>; it allows us to prove the following result: Theorem 2.3 : Given a sparse representation of a distribution satisfying C, it is possible to construct a size m distribution 0 using jS ()j evaluations of each F i in C, and O (jS ()j m 1:62 ) arithmetic operations. 3
Reference: [BR91] <author> B. Berger and J. Rompel. </author> <title> Simulating (log c n)-wise independence in NC. </title> <journal> Journal of the ACM, </journal> <volume> 38(4) </volume> <pages> 1026-1046, </pages> <year> 1991. </year>
Reference-contexts: The underlying randomized algorithm, described by Alon, Babai, and Itai [ABI86], was derandomized in the same paper for fixed values of d. It was later derandomized also for d = O (poly log n) by Berger and Rompel <ref> [BR91] </ref> and Motwani, Naor, and Naor [MNN89]. Koller and Megiddo [KM93] gave a derandomization of this algorithm for arbitrary d, but their derandomization is inherently sequential. <p> Alon, Babai, and Itai provide an N C algorithm for this problem by constructing a joint distribution of d-wise independent variables X i . This technique is effective only for constant d. The results of <ref> [BR91] </ref> and [MNN89] provide an N C algorithm for d = O (poly log n). However, they still maintain the general approach of searching the sample space of an almost d-wise independent distribution. Hence, this approach cannot be pushed any further in N C.
Reference: [CGH + 85] <author> B. Chor, O. Goldreich, J. Hastad, J. Fried-man, S. Rudich, and R. Smolensky. </author> <title> t-resilient functions. </title> <booktitle> In Proceedings of the 26th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 396-407, </pages> <year> 1985. </year>
Reference-contexts: This approach is inherently limited, since it was shown by Chor et al. <ref> [CGH + 85] </ref> that any sample space of n d-wise independent random bits has cardinality (n [d=2] ). Karloff and Mansour [KM94] extended this result to biased random bits, and showed that, in certain cases, the smallest sample space maintaining d-wise independence has size (n d ).
Reference: [EGL + 92] <author> G. Even, O. Goldreich, M. Luby, N. Nisan, and B. Velickovic. </author> <title> Approximations of general independent distributions. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 10-16, </pages> <year> 1992. </year>
Reference-contexts: Therefore this approach can be used only if the desired d is constant. These bounds can be circumvented by allowing some error in the independence constraints. In <ref> [NN93, AGHP90, AMN92, EGL + 92] </ref>, the probabilities of the relevant events (as described above) are required to be within an additive factor of *. The size of the result ing distribution is polynomial in 1=*, so that we must choose * to be at least 1=poly (n).
Reference: [ES73] <author> P. Erdos and J. Selfridge. </author> <title> On a combinatorial game. </title> <journal> Journal of Combinatorial Theory, series B, </journal> <volume> 14 </volume> <pages> 298-301, </pages> <year> 1973. </year>
Reference-contexts: The assumption that we have a conditional expectation oracle for each constraint is clearly crucial to this construction. A similar assumption appears in the method of conditional probabilities <ref> [ES73] </ref> (see also [Spe87]). The idea there is to perform a binary search of the sample space for a good point.
Reference: [Jof74] <author> A. Joffe. </author> <title> On a set of almost deterministic k-independent random variables. </title> <journal> Annals of Probability, </journal> <volume> 2 </volume> <pages> 161-162, </pages> <year> 1974. </year>
Reference-contexts: Furthermore, the emphasis has been on constructions in N C, so as to allow a derandomization of parallel algorithms. Most of these works construct a distribution that only approximately satisfies some of the required constraints. The early works <ref> [Jof74, Lub86, ABI86] </ref> generate distributions that err on the probabilities Pr (X i = b i ), but precisely satisfy the actual independence constraints such as those on Pr (X i = b i ; X j = b j ).
Reference: [KM93] <author> D. Koller and N. Megiddo. </author> <title> Finding small sample spaces satisfying given constraints. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on Theory of Com puting, </booktitle> <pages> pages 268-277, </pages> <year> 1993. </year> <note> To appear in SIAM Journal on Discrete Mathematics. </note>
Reference-contexts: This approach was initiated by Schulman [Sch92] and investigated in a more general way by Koller and Megiddo <ref> [KM93] </ref>. Koller and Megiddo viewed constraints as linear equations on variables representing the probabilities of the points in the sample space. This allows them to apply techniques from linear programming to the problem of derandomization. <p> Using the other approaches, independence must be enforced among all neighbors of a vertex; the sample space would then grow as 2 where is the maximum node degree. Using <ref> [KM93] </ref>, there is only one event per node, resulting in a sample space of size n (the number of nodes in the graph). In this example, the constraints depend on the edge structure of the input graph. This input-dependence is typical of the constraint-based approach. <p> In this example, the constraints depend on the edge structure of the input graph. This input-dependence is typical of the constraint-based approach. Therefore, unlike most previous constructions, the distribution cannot be precomputed. Furthermore, the inherently sequential nature of the <ref> [KM93] </ref> construction prevents their techniques from being applied to the derandomization of parallel algorithms. New results We extend the approach of [KM93] in several ways. <p> This input-dependence is typical of the constraint-based approach. Therefore, unlike most previous constructions, the distribution cannot be precomputed. Furthermore, the inherently sequential nature of the <ref> [KM93] </ref> construction prevents their techniques from being applied to the derandomization of parallel algorithms. New results We extend the approach of [KM93] in several ways. <p> All other constructions (including that of <ref> [KM93] </ref>) deal solely with independence constraints; the additional power can be quite important. <p> The underlying randomized algorithm, described by Alon, Babai, and Itai [ABI86], was derandomized in the same paper for fixed values of d. It was later derandomized also for d = O (poly log n) by Berger and Rompel [BR91] and Motwani, Naor, and Naor [MNN89]. Koller and Megiddo <ref> [KM93] </ref> gave a derandomization of this algorithm for arbitrary d, but their derandomization is inherently sequential. Our use of more generalized constraints allows us to reduce the number of constraints, and thus the size of the linear systems that must be solved, by a significant factor. <p> This demonstrates yet again the power inherent in viewing probabilistic constraints in the framework of linear systems. 2 Constraints as linear equations As we mentioned in the introduction, Koller and Megiddo <ref> [KM93] </ref> introduced the paradigm of constructing distributions that satisfy only the constraints actually required for the correctness of the algorithm. In this section and the next, we review their results and extend their applicability to a much larger class of constraints. <p> Definition 2.1: An expectation constraint over has the form E (F ) = fl, where F is an arbitrary random variable on . A probability distribution over satisfies if E (F) = fl. In <ref> [KM93] </ref>, Koller and Megiddo allowed only probabilistic constraints of the form Pr (Q) = , for Q . This is clearly a special case of our definition, since we can define F to be an indicator variable: F (x) = 1 for x 2 Q and 0 otherwise. <p> ), then a constraint [E (F i ) = fl i ] translates into a linear equation over these variables: P N j=1 F i (x j )v j = fl i . (Note that F i (x j ) is a known constant.) We can use the techniques of <ref> [KM93] </ref> to prove the following theorem: Theorem 2.2 : If a constraint set C is satisfied by some distribution , then there exists a size m distribution 0 satisfying C. <p> But in order to use this algorithm, we must already have a distribution which satisfies the constraints with a polynomial-size support; otherwise, our algorithm does not work in polynomial time. Our goal is to construct a distribution directly from the constraints that it must satisfy. As shown in <ref> [KM93] </ref>, even the problem of deciding whether a given set of constraints is consistent is N P-hard in general. We can circumvent this problem by requiring that the constraints be consistent with a fixed known independent distribution. <p> A function F can also be viewed as an indicator (or measurement) on the sample space, whose value we would like to maintain. The sequential construction of <ref> [KM93] </ref> works by "derandomizing" one variable X i at a time, using the basis crashing approach of the previous section as a subroutine. We take a similar approach for our more general expectation constraints. <p> Thus, the class of constraints on regular functions contains the class of independence constraints. Hence, our results in this section provide a parallel construction that works in all cases covered by the results of <ref> [KM93] </ref>. Furthermore, it covers a large number of other interesting cases, such as parity, sum modulo k, and threshold functions. Theorem 4.2 : Given an N C implementation of ReduceSupport, it is possible to construct in N C a polynomial size distribution which fools a set C of regular functions. <p> For illustration, we use the example of finding large independent sets in sparse hypergraphs. The problem description and the randomized algorithm for its solution are taken from [ABI86]. The analysis of the problem in terms of constraints is taken from <ref> [KM93] </ref>. We derandomize the algorithm, thus producing what is, to the best of our knowledge, the first N C algorithm that completely solves the problem. <p> However, they still maintain the general approach of searching the sample space of an almost d-wise independent distribution. Hence, this approach cannot be pushed any further in N C. The paradigm of looking only at the precise constraints imposed by the algorithm is crucial for making further progress. In <ref> [KM93] </ref>, Koller and Megiddo observe that d-wise independence, although sufficient for the analysis, is almost entirely redundant. Taking a close look, it is easy to see that far fewer constraints are actually required. As we mentioned, pairwise independence suffices for bounding the variance. <p> Since it is easy to construct expectation oracles for X and (X E (X)) 2 , we can reduce the number of constraints needed to m + 2. This in turn shrinks the linear systems which we must solve and thus improves the efficiency of the sequential derandomization of <ref> [KM93] </ref>. We can also apply our parallel techniques to the problem. Using Theorem 4.5, we can construct a distribution with a small support that approximately satisfies these constraints. <p> the problem: Theorem 6.2: There exists an N C algorithm which, given any constant * and an m fi n positive linear program, finds a solution with at most m log 1+o (1) m nonzero coefficients and value within (1 *) times optimal. 7 Conclusion Building on the work of <ref> [KM93] </ref>, we have further explored the problem of constructing small-size distributions by tailoring them to the specific needs of the algorithm and input at hand. We have extended the kinds of constraints which can be satisfied and have also given the first parallel algorithm for such a construction. <p> We hope that this different perspective will prove useful in solving other, similar problems. Acknowledgements Thanks to Yishay Mansour, Nimrod Megiddo, and Rajeev Motwani for helpful comments and discussions, and to Eyal Kushilevitz for asking whether the approach of <ref> [KM93] </ref> can be applied to parity constraints.
Reference: [KM94] <author> H. Karloff and Y. Mansour. </author> <title> On construction of k-wise independent random variables. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: This approach is inherently limited, since it was shown by Chor et al. [CGH + 85] that any sample space of n d-wise independent random bits has cardinality (n [d=2] ). Karloff and Mansour <ref> [KM94] </ref> extended this result to biased random bits, and showed that, in certain cases, the smallest sample space maintaining d-wise independence has size (n d ). Therefore this approach can be used only if the desired d is constant.
Reference: [Kus93] <author> E. Kushilevitz, </author> <year> 1993. </year> <title> private communication. </title>
Reference-contexts: For example, if we wish to maintain the distribution of the parity of X 1 + +X n , an exponential number of independence constraints are required. (Distributions satisfying parity constraints may be useful for cryptographic purposes <ref> [Kus93] </ref>.) Our construction deals with this as a single constraint. Since our sample space is polynomial whenever the number of constraints we wish to satisfy is polynomial, we are able to construct polynomial-size distributions where previous approaches could not. In addition, we are able to parallelize the construction.
Reference: [KUW86] <author> R. M. Karp, E. Upfal, and A. Wigder-son. </author> <title> Constructing a perfect matching is in random N C. </title> <journal> Combinatorica, </journal> <volume> 6(1) </volume> <pages> 35-48, </pages> <year> 1986. </year>
Reference-contexts: Another goal is to examine those randomized parallel algorithms which have so far resisted derandomization, and attempt to apply the techniques developed here. Since present proofs of correctness for parallel solutions to maximum matching <ref> [KUW86, MVV87] </ref> appear to rely on exponentially many constraints, this will of course require a reexamination of the proofs used or a development of new randomized algorithms for the problem. The approach of Koller and Megiddo utilized basis crashing for reducing the support of the distribution.
Reference: [LN93] <author> M. Luby and N. Nisan. </author> <title> A parallel approximation algorithm for positive linear programming. </title> <booktitle> In Proceedings of the 25 th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 448-457, </pages> <year> 1993. </year>
Reference-contexts: Parallel basis crashing is of independent interest and Section 6 can be read independent of the rest of the paper. Indeed, it has other applications. For example, our algorithm dovetails well with an N C algorithm of Luby and Nisan <ref> [LN93] </ref> for finding approximate solutions to positive (packing and covering) linear programs. <p> By applying our algorithm in a postprocessing step, we can arrange for the solution to the linear program to have at most m log 1+o (1) nonzero components, where m is the number of constraints, while maintaining the same accuracy as the original output of the <ref> [LN93] </ref> algorithm. The fact that we can construct a distribution in N C allows us to derandomize parallel algorithms. We apply our techniques to the problem of finding a large independent set in a d-uniform hypergraph. <p> This theorem is the foundation for the parallel support reduction procedure used in Section 4, and hence for our entire parallel construction of small distribution. However, it also has additional applications. Luby and Nisan <ref> [LN93] </ref> give an N C algorithm for approximately solving a positive (packing or covering) linear program.
Reference: [Lub86] <author> M. Luby. </author> <title> A simple parallel algorithm for the maximal independent set problem. </title> <journal> SIAM Journal on Computing, </journal> <volume> 15(4) </volume> <pages> 1036-1053, </pages> <year> 1986. </year>
Reference-contexts: Furthermore, the emphasis has been on constructions in N C, so as to allow a derandomization of parallel algorithms. Most of these works construct a distribution that only approximately satisfies some of the required constraints. The early works <ref> [Jof74, Lub86, ABI86] </ref> generate distributions that err on the probabilities Pr (X i = b i ), but precisely satisfy the actual independence constraints such as those on Pr (X i = b i ; X j = b j ).
Reference: [Meg94] <author> N. Megiddo, </author> <year> 1994. </year> <title> Private Communication. </title>
Reference-contexts: Motivated by the desire to parallelize their construction, we have developed a new approximate basis crashing algorithm. This is a significant contribution of independent interest, since, to our knowledge, there are no parallel algorithms for any type of significant support reduction. On the other hand, Megiddo <ref> [Meg94] </ref> has observed that the "base case" of basis crashing is as hard as the entire problem.
Reference: [MNN89] <author> R. Motwani, J. Naor, and M. Naor. </author> <title> The probabilistic method yields deterministic parallel algorithms. </title> <booktitle> In Proceedings of the 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 8-13, </pages> <year> 1989. </year> <note> To appear in Journal of Computer and System Sciences. Also available as: IBM Technical Report RJ 7173 (1989). </note>
Reference-contexts: The above approaches all generate a d-wise independent distribution, and then apply it uniformly to any algorithm for which d is an upper bound on the degree of independence required ([BR91] and <ref> [MNN89] </ref> also present a parallel algorithm for searching the support of certain implicitly constructed polylogn-wise independent distributions). In a different paradigm, it is possible to directly examine the constraints imposed by a specific algorithm and possibly even a specific input, and then construct a distribution to satisfy precisely those constraints. <p> The underlying randomized algorithm, described by Alon, Babai, and Itai [ABI86], was derandomized in the same paper for fixed values of d. It was later derandomized also for d = O (poly log n) by Berger and Rompel [BR91] and Motwani, Naor, and Naor <ref> [MNN89] </ref>. Koller and Megiddo [KM93] gave a derandomization of this algorithm for arbitrary d, but their derandomization is inherently sequential. Our use of more generalized constraints allows us to reduce the number of constraints, and thus the size of the linear systems that must be solved, by a significant factor. <p> We can compensate for this by sampling more often from points inside low-probability events. The second is that there is no point using a randomized algorithm as a subroutine in a derandomization procedure. We therefore "derandomize" our derandomization routine using the deterministic lattice approximation techniques developed by <ref> [MNN89] </ref>. Finally, the distribution 0 does not satisfy the constraints precisely, but only approximately. <p> Alon, Babai, and Itai provide an N C algorithm for this problem by constructing a joint distribution of d-wise independent variables X i . This technique is effective only for constant d. The results of [BR91] and <ref> [MNN89] </ref> provide an N C algorithm for d = O (poly log n). However, they still maintain the general approach of searching the sample space of an almost d-wise independent distribution. Hence, this approach cannot be pushed any further in N C. <p> We omit these proofs &gt;from this abstract, since we are actually interested in the derandomized version of this construction. The derandomization of this construction is based on the lattice approximation techniques of Motwani, Naor, and Naor <ref> [MNN89] </ref>. The lattice approximation algorithm takes as input an m fi h matrix C, with each entry c ij 2 [0; 1]; a vector ~r 2 [0; 1] h ; and a vec tor ~ d 2 IR m such that C~r = ~ d. <p> If we assign c ij = a ij y j =q j and ~ d = ~ b, the solution to the corresponding lattice approximation problem is precisely a derandomization of our construction above. The N C lattice approximation algorithm of <ref> [MNN89] </ref> allows us to derandomize our construction in N C (after appropriate transformation and rescaling of C).
Reference: [MVV87] <author> K. Mulmuley, U. V. Vazirani, and V. V. Vazirani. </author> <title> Matching is as easy as matrix inversion. </title> <journal> Combinatorica, </journal> <volume> 7(1) </volume> <pages> 105-113, </pages> <year> 1987. </year>
Reference-contexts: Another goal is to examine those randomized parallel algorithms which have so far resisted derandomization, and attempt to apply the techniques developed here. Since present proofs of correctness for parallel solutions to maximum matching <ref> [KUW86, MVV87] </ref> appear to rely on exponentially many constraints, this will of course require a reexamination of the proofs used or a development of new randomized algorithms for the problem. The approach of Koller and Megiddo utilized basis crashing for reducing the support of the distribution.
Reference: [Nis90] <author> N. Nisan. </author> <title> Pseudorandom generators for space-bounded computation. </title> <booktitle> In Proceedings of the 22nd Annual ACM Symposium on Theory of Computating, </booktitle> <pages> pages 204-212, </pages> <year> 1990. </year>
Reference-contexts: No parallel basis crashing algorithms were known previously. In Section 6 we present the first parallel 1 The problem of "looking random" to a group of testers has been studied by Nisan <ref> [Nis90] </ref>, who constructs a sample space of size n O (log (n)) that simultaneously satisfies the constraints associated with all polynomial size finite automata. <p> We have already given methods for fooling finite automata; can we extend these techniques to fooling larger language classes such as logspace (two way finite automata)? Further investigation of the connection between our results and those of <ref> [Nis90, BG92] </ref> seems warranted. Another goal is to examine those randomized parallel algorithms which have so far resisted derandomization, and attempt to apply the techniques developed here.
Reference: [NN93] <author> J. Naor and M. Naor. </author> <title> Small-bias probability spaces: Efficient constructions and applications. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22(4) </volume> <pages> 838-856, </pages> <year> 1993. </year>
Reference-contexts: Therefore this approach can be used only if the desired d is constant. These bounds can be circumvented by allowing some error in the independence constraints. In <ref> [NN93, AGHP90, AMN92, EGL + 92] </ref>, the probabilities of the relevant events (as described above) are required to be within an additive factor of *. The size of the result ing distribution is polynomial in 1=*, so that we must choose * to be at least 1=poly (n).
Reference: [Sch92] <author> L. J. Schulman. </author> <title> Sample spaces uniform on neighborhoods. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 17-25, </pages> <year> 1992. </year>
Reference-contexts: In a different paradigm, it is possible to directly examine the constraints imposed by a specific algorithm and possibly even a specific input, and then construct a distribution to satisfy precisely those constraints. This approach was initiated by Schulman <ref> [Sch92] </ref> and investigated in a more general way by Koller and Megiddo [KM93]. Koller and Megiddo viewed constraints as linear equations on variables representing the probabilities of the points in the sample space. This allows them to apply techniques from linear programming to the problem of derandomization.
Reference: [Spe87] <author> J. Spencer. </author> <title> Ten Lectures on the Probabilistic Method. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1987. </year>
Reference-contexts: Some of this work was done while at Stanford University. for example, Spencer <ref> [Spe87] </ref>). The underlying idea is as follows. Consider a finite set whose elements are classified as "good" and "bad." Suppose we wish to prove existence of at least one "good" element within . <p> The assumption that we have a conditional expectation oracle for each constraint is clearly crucial to this construction. A similar assumption appears in the method of conditional probabilities [ES73] (see also <ref> [Spe87] </ref>). The idea there is to perform a binary search of the sample space for a good point.
References-found: 23

