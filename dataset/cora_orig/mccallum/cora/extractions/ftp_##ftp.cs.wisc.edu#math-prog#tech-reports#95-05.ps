URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-05.ps
Refering-URL: http://www.cs.wisc.edu/math-prog/tech-reports/
Root-URL: 
Title: Hybrid Misclassification Minimization  
Author: Chunhui Chen O. L. Mangasarian 
Abstract: Mathematical Programming Technical Report 95-05 February 1995-Revised July & August 1995 Abstract Given two finite point sets A and B in the n-dimensional real space R n , we consider the NP-complete problem of minimizing the number of misclassified points by a plane attempting to divide R n into two halfspaces such that each open halfspace contains points mostly of A or B . This problem is equivalent to determining a plane fx j x T w = flg that maximizes the number of points x 2 A satisfying x T w &gt; fl, plus the number of points x 2 B satisfying x T w &lt; fl. A simple but fast algorithm is proposed that alternates between (i) minimizing the number of misclassified points by translation of the separating plane, and (ii) a rotation of the plane so that it minimizes a weighted average sum of the distances of the misclassified points to the separating plane. Existence of a global solution to an underlying hybrid minimization problem is established. Computational comparison with a parametric approach to solve the NP-complete problem indicates that our approach is considerably faster and appears to generalize better as determined by tenfold cross-validation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. P. Bennett and E. J. Bredensteiner. </author> <title> A parametric optimization method for machine learning. </title> <note> Department of Mathematical Sciences Report No. 217, </note> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1994. </year>
Reference-contexts: This problem was considered in [8], where a parametric minimization approach was proposed and implemented in <ref> [1] </ref>. Although the parametric procedure is effective, it is costly computationally, which is to be expected since the underlying problem is NP-complete. <p> Section 3 contains numerical results that indicate that the proposed hybrid algorithm is fast and appears to generalize better than the parametric algorithm misclassification minimization <ref> [1] </ref>. A word about our notation now. <p> This problem can be stated as the following misclassification minimization problem min e T (Aw + efl + e) fl + e T (Bw efl + e) fl : (6) In <ref> [8, 1] </ref> this problem was reformulated as a linear program with equilibrium constraints (LPEC) [6], that is a linear program with a single complementarity constraint. An implicitly exact penalty method as well as a parametric method were proposed for solving the LPEC in [8] and successfully implemented in [1]. <p> An implicitly exact penalty method as well as a parametric method were proposed for solving the LPEC in [8] and successfully implemented in <ref> [1] </ref>. Although effective, the parametric approach is costly, because for each value of the parameter, a nonconvex bilinear program need to be solved. We propose here an alternative hybrid approach that is considerably faster and which appears to generalize better than the parametric approach. <p> Although the HMM Algorithm does not necessarily solve the HMM Problem 3, it does terminate very quickly after two to five iterations at a solution that is about as good as that obtained by the more complex parametric misclassification algorithm <ref> [8, 1] </ref>. Furthermore the HMM Algo rithm appears to generalize better than the parametric algorithm, as indicated by the numerical computations given in the next section. We state now a finite termination result for the HMM Algorithm 5. 6. <p> For each data set, a separating plane was obtained by three methods: the parametric misclas-sification minimization (PMM) procedure of <ref> [8, 1] </ref>, the HMM Algorithm 5 of Section 2, and the robust linear program (RLP) algorithm [2], that is the linear program (3). In order to measure how well each separating plane generalizes to unseen data, we performed tenfold cross-validation on each data set [15]. <p> The time reported was the average time for the ten different subsets used for training. The parametric misclassification minimization procedure was coded in the modeling language AMPL [3] in <ref> [1] </ref> utilizing the MINOS [11] linear programming solver. The HMM Algorithm and the robust linear program algorithm were implemented using C and called MINOS as a subroutine to solve the linear programs. Table 1 gives a summary of the numerical results. <p> Total time 32.54 seconds PMM slowest in all 10 cases. Total time 1600.18 seconds (iv) Average of average number of LPs solved: RLP constant of 1 HMM average of 2.32 PMM average of 22.3 6 Table 1. Comparison of Hybrid Misclassification Minimization (HMM) with Parametric Misclassification Minimization (PMM) <ref> [8, 1] </ref> & Robust Linear Programming (RLP) [2] m Training Set Correctness Date Set k Testing Set Correctness n Time Seconds SPARCstation 20 Average LPs Solved HMM PMM RLP WBC Prognosis 119 72.24 71.33 66.048 32 0.71 10.65 0.501 239 97.87 98.57 97.73 WBCD 443 97.36 96.47 97.21 9 0.64 24.65
Reference: [2] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: This material is based on research supported by Air Force Office of Scientific Research Grant F49620-94-1-0036 and National Science Foundation Grant CCR-9322479. 1 plane (1) that minimizes a weighted average of the sum of the distances of the misclassified points to the plane <ref> [7, 2] </ref> as follows: min ( m e T z j Aw + y efl + e; Bw z efl e; y 0; z 0 (3) Here the rows of the matrices A 2 R mfin and B 2 R kfin represent the m points in A and the k points <p> In fact (7) is equivalent to maximizing the number of satisfied inequalities in (4), that is max e T (Aw efl) fl + e T (Bw + efl) fl (8) We note further, as in the case of robust linear programming separation <ref> [2] </ref> achieved by the linear program (3), when the null solution (w; fl) = (0; 1) 2 R n+1 gives a maximum value of maxfm; kg for (8), then it is never unique in w. <p> This is a useful property of (8), otherwise the null solution w = 0 would pose a computational difficulty similar to that addressed in <ref> [2] </ref>. 3 If we now assume that A and B have integer entries, then problem (8) belongs to the following class of problems which, we will show, is NP-complete. 1. Maximum Inequality Satisfiability (MIS). Let the matrix H 2 R pfiq have integer entries. <p> For each data set, a separating plane was obtained by three methods: the parametric misclas-sification minimization (PMM) procedure of [8, 1], the HMM Algorithm 5 of Section 2, and the robust linear program (RLP) algorithm <ref> [2] </ref>, that is the linear program (3). In order to measure how well each separating plane generalizes to unseen data, we performed tenfold cross-validation on each data set [15]. <p> Total time 1600.18 seconds (iv) Average of average number of LPs solved: RLP constant of 1 HMM average of 2.32 PMM average of 22.3 6 Table 1. Comparison of Hybrid Misclassification Minimization (HMM) with Parametric Misclassification Minimization (PMM) [8, 1] & Robust Linear Programming (RLP) <ref> [2] </ref> m Training Set Correctness Date Set k Testing Set Correctness n Time Seconds SPARCstation 20 Average LPs Solved HMM PMM RLP WBC Prognosis 119 72.24 71.33 66.048 32 0.71 10.65 0.501 239 97.87 98.57 97.73 WBCD 443 97.36 96.47 97.21 9 0.64 24.65 0.21 216 87.50 91.43 84.47 Cleveland Heart
Reference: [3] <author> R. Fourer, D. Gay, and B. Kernighan. </author> <title> AMPL. </title> <publisher> The Scientific Press, </publisher> <address> South San Francisco, California, </address> <year> 1993. </year>
Reference-contexts: The time reported was the average time for the ten different subsets used for training. The parametric misclassification minimization procedure was coded in the modeling language AMPL <ref> [3] </ref> in [1] utilizing the MINOS [11] linear programming solver. The HMM Algorithm and the robust linear program algorithm were implemented using C and called MINOS as a subroutine to solve the linear programs. Table 1 gives a summary of the numerical results.
Reference: [4] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability, A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: Note that in (9), H plays the role of the matrix " B e of (4). We now show that this problem is NP-complete. 2. Proposition The MIS Problem 1 is NP-complete. Proof The NP-complete Open Hemisphere (OH) Problem <ref> [4, page 246, problem MP6] </ref> is the problem of determining whether, for a positive integer r p, r of the inequalities Hx &gt; 0 can be satisfied by a rational vector x.
Reference: [5] <author> David Heath. </author> <title> A geometric Framework for Machine Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Johns Hopkins University-Baltimore, Maryland, </institution> <year> 1992. </year>
Reference-contexts: Now we show that MIS is NP-hard by reducing OH to an instance of MIS. Given a positive integer r, we solve MIS and obtain r for its maximum. The integer r solves OH if and only if r r. We note that Heath's NP-completeness result <ref> [5, Appendix C] </ref> is for a differently stated problem than ours.
Reference: [6] <author> Z.-Q. Luo, J.-S. Pang, D. Ralph, and S.-Q. Wu. </author> <title> Exact penalization and stationarity conditions of mathematical programs with equilibrium constraints. </title> <type> Technical Report 275, </type> <institution> Communications Research Laboratory, McMaster University, Hamilton, </institution> <address> Ontario, Hamilton, Ontario L8S 4K1, Canada, </address> <year> 1993. </year> <note> Mathematical Programming, to appear. </note>
Reference-contexts: This problem can be stated as the following misclassification minimization problem min e T (Aw + efl + e) fl + e T (Bw efl + e) fl : (6) In [8, 1] this problem was reformulated as a linear program with equilibrium constraints (LPEC) <ref> [6] </ref>, that is a linear program with a single complementarity constraint. An implicitly exact penalty method as well as a parametric method were proposed for solving the LPEC in [8] and successfully implemented in [1].
Reference: [7] <author> O. L. Mangasarian. </author> <title> Multi-surface method of pattern separation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:801-807, </volume> <year> 1968. </year>
Reference-contexts: This material is based on research supported by Air Force Office of Scientific Research Grant F49620-94-1-0036 and National Science Foundation Grant CCR-9322479. 1 plane (1) that minimizes a weighted average of the sum of the distances of the misclassified points to the plane <ref> [7, 2] </ref> as follows: min ( m e T z j Aw + y efl + e; Bw z efl e; y 0; z 0 (3) Here the rows of the matrices A 2 R mfin and B 2 R kfin represent the m points in A and the k points
Reference: [8] <author> O. L. Mangasarian. </author> <title> Misclassification minimization. </title> <journal> Journal of Global Optimization, </journal> <volume> 5 </volume> <pages> 309-323, </pages> <year> 1994. </year>
Reference-contexts: The problem of constructing a plane (1) such that the number of misclassified points is minimized, is considerably more difficult and in fact is NP-complete, as shown in Proposition 2 of Section 2 below. This problem was considered in <ref> [8] </ref>, where a parametric minimization approach was proposed and implemented in [1]. Although the parametric procedure is effective, it is costly computationally, which is to be expected since the underlying problem is NP-complete. <p> The cardinality of a set A will be denoted by card (A). The symbol " := " defines a quantity appearing on its left by a quantity appearing on its right. 2 2 The Hybrid Misclassification Minimization Problem We begin by defining the "pure" misclassification minimization problem as in <ref> [8] </ref> with the help of the step function () fl . <p> This problem can be stated as the following misclassification minimization problem min e T (Aw + efl + e) fl + e T (Bw efl + e) fl : (6) In <ref> [8, 1] </ref> this problem was reformulated as a linear program with equilibrium constraints (LPEC) [6], that is a linear program with a single complementarity constraint. An implicitly exact penalty method as well as a parametric method were proposed for solving the LPEC in [8] and successfully implemented in [1]. <p> An implicitly exact penalty method as well as a parametric method were proposed for solving the LPEC in <ref> [8] </ref> and successfully implemented in [1]. Although effective, the parametric approach is costly, because for each value of the parameter, a nonconvex bilinear program need to be solved. We propose here an alternative hybrid approach that is considerably faster and which appears to generalize better than the parametric approach. <p> Although the HMM Algorithm does not necessarily solve the HMM Problem 3, it does terminate very quickly after two to five iterations at a solution that is about as good as that obtained by the more complex parametric misclassification algorithm <ref> [8, 1] </ref>. Furthermore the HMM Algo rithm appears to generalize better than the parametric algorithm, as indicated by the numerical computations given in the next section. We state now a finite termination result for the HMM Algorithm 5. 6. <p> For each data set, a separating plane was obtained by three methods: the parametric misclas-sification minimization (PMM) procedure of <ref> [8, 1] </ref>, the HMM Algorithm 5 of Section 2, and the robust linear program (RLP) algorithm [2], that is the linear program (3). In order to measure how well each separating plane generalizes to unseen data, we performed tenfold cross-validation on each data set [15]. <p> Total time 32.54 seconds PMM slowest in all 10 cases. Total time 1600.18 seconds (iv) Average of average number of LPs solved: RLP constant of 1 HMM average of 2.32 PMM average of 22.3 6 Table 1. Comparison of Hybrid Misclassification Minimization (HMM) with Parametric Misclassification Minimization (PMM) <ref> [8, 1] </ref> & Robust Linear Programming (RLP) [2] m Training Set Correctness Date Set k Testing Set Correctness n Time Seconds SPARCstation 20 Average LPs Solved HMM PMM RLP WBC Prognosis 119 72.24 71.33 66.048 32 0.71 10.65 0.501 239 97.87 98.57 97.73 WBCD 443 97.36 96.47 97.21 9 0.64 24.65
Reference: [9] <author> O. L. Mangasarian, W. Nick Street, and W. H. Wolberg. </author> <title> Breast cancer diagnosis and prognosis via linear programming. </title> <journal> Operations Research, </journal> <volume> 43(4) </volume> <pages> 570-577, </pages> <month> July-August </month> <year> 1995. </year>
Reference-contexts: generalizes better. 5 3 Numerical Computation and Comparisons We report now on numerical results on the Wisconsin Breast Cancer Database (WBCD) and other data sets from the Irvine Machine Learning Database Repository [10] as well as the Star/Galaxy database collected by Odewahn [12] and the Wisconsin Breast Cancer Prognosis Database <ref> [9, 16] </ref>. For each data set, a separating plane was obtained by three methods: the parametric misclas-sification minimization (PMM) procedure of [8, 1], the HMM Algorithm 5 of Section 2, and the robust linear program (RLP) algorithm [2], that is the linear program (3).
Reference: [10] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, Irvine, California, </institution> <note> http://www.ics.uci.edu/AI/ML/MLDBRepository.html., 1992. </note>
Reference-contexts: a point seems to be as good as that obtained by a more complex and costly algorithm, and generalizes better. 5 3 Numerical Computation and Comparisons We report now on numerical results on the Wisconsin Breast Cancer Database (WBCD) and other data sets from the Irvine Machine Learning Database Repository <ref> [10] </ref> as well as the Star/Galaxy database collected by Odewahn [12] and the Wisconsin Breast Cancer Prognosis Database [9, 16].
Reference: [11] <author> B. A. Murtagh and M. A. Saunders. </author> <title> MINOS 5.0 user's guide. </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1983. </year> <note> MINOS 5.4 Release Notes, </note> <month> December </month> <year> 1992. </year> <month> 8 </month>
Reference-contexts: The time reported was the average time for the ten different subsets used for training. The parametric misclassification minimization procedure was coded in the modeling language AMPL [3] in [1] utilizing the MINOS <ref> [11] </ref> linear programming solver. The HMM Algorithm and the robust linear program algorithm were implemented using C and called MINOS as a subroutine to solve the linear programs. Table 1 gives a summary of the numerical results.
Reference: [12] <author> S. Odewahn, E. Stockwell, R. Pennington, R. Hummphreys, and W. Zumach. </author> <title> Automated star/galaxy discrimination with neural networks. </title> <journal> Astronomical Journal, </journal> <volume> 103(1) </volume> <pages> 318-331, </pages> <year> 1992. </year>
Reference-contexts: by a more complex and costly algorithm, and generalizes better. 5 3 Numerical Computation and Comparisons We report now on numerical results on the Wisconsin Breast Cancer Database (WBCD) and other data sets from the Irvine Machine Learning Database Repository [10] as well as the Star/Galaxy database collected by Odewahn <ref> [12] </ref> and the Wisconsin Breast Cancer Prognosis Database [9, 16].
Reference: [13] <author> H. E. Scarf. </author> <title> The Computation of Economic Equilibria. </title> <publisher> Yale University Press, </publisher> <address> New Haven, Conneticut, </address> <year> 1973. </year>
Reference-contexts: The process is repeated until no improvement in the number of misclassified points is possible. We term such a point as a stationary point. The idea of using different criteria to determine different parts of the solution (w; fl) is similar to that of finding equilibrium points <ref> [13] </ref> and solving multicriteria optimization problems [14].
Reference: [14] <author> R. E. Steuer. </author> <title> Multiple Criteria Optimization: Theory, Computation, and Application. </title> <publisher> John Wiley and Sons, </publisher> <year> 1986. </year>
Reference-contexts: We term such a point as a stationary point. The idea of using different criteria to determine different parts of the solution (w; fl) is similar to that of finding equilibrium points [13] and solving multicriteria optimization problems <ref> [14] </ref>.
Reference: [15] <author> M. Stone. </author> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 36 </volume> <pages> 111-147, </pages> <year> 1974. </year>
Reference-contexts: In the present approach we shall propose a fast alternative hybrid criterion that is quite effective in approximately minimizing the number of misclassified points as determined by tenfold cross-validation <ref> [15] </ref>. The basic idea is to minimize the number of misclassified points by translating the separating plane, and then rotating the plane in order to minimize a weighted average sum of the distances of misclassified points to a separating plane. <p> In order to measure how well each separating plane generalizes to unseen data, we performed tenfold cross-validation on each data set <ref> [15] </ref>. Specifically, we divided each data set into ten equal parts, obtained a separating plane for the combined nine parts (training) and tested the correctness of the plane (generalization) on the tenth set.
Reference: [16] <author> W. H. Wolberg, W. N. Street, D. N. Heisey, and O. L. Mangasarian. </author> <title> Computer-derived nuclear grade and breast cancer prognosis. Analytical and Quantitative Cytology and Histology. </title> <note> To appear. 9 </note>
Reference-contexts: generalizes better. 5 3 Numerical Computation and Comparisons We report now on numerical results on the Wisconsin Breast Cancer Database (WBCD) and other data sets from the Irvine Machine Learning Database Repository [10] as well as the Star/Galaxy database collected by Odewahn [12] and the Wisconsin Breast Cancer Prognosis Database <ref> [9, 16] </ref>. For each data set, a separating plane was obtained by three methods: the parametric misclas-sification minimization (PMM) procedure of [8, 1], the HMM Algorithm 5 of Section 2, and the robust linear program (RLP) algorithm [2], that is the linear program (3).
References-found: 16

