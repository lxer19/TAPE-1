URL: ftp://ftp.cs.virginia.edu/pub/WM/HPCA2.ps
Refering-URL: http://www.cs.virginia.edu/~wm/smc.html
Root-URL: http://www.cs.virginia.edu
Email: mckee@virginia.edu  
Title: Compiling for Efficient Memory Utilization Compiling for Efficient Memory Utilization  
Author: Sally A. McKee 
Web: http://www.cs.virginia.edu/~sam3a  
Affiliation: University of Virginia  
Abstract-found: 0
Intro-found: 1
Reference: [Aho88] <author> A.V. Aho, R. Sethi, J.D. Ullman, </author> <booktitle> Compilers: Principles, Techniques, and Tools, </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: This algorithm relies on induction variable detection, and is similar in complexity to strength reduction. Any good compiler textbook contains descriptions of these operations (e.g., Aho, Sethi, and Ullman <ref> [Aho88] </ref>). 1) Divide the loops memory accesses into partitions that reference disjoint sections of memory. If the proper partition is unknown for a particular reference, add that memory reference to all partitions.
Reference: [Ale93] <author> M.J. Alexander, M.W. Bailey, B.R. Childers, J.W. Davidson, S. Jinturkar, </author> <title> Memory Bandwidth Optimizations for Wide-Bus Machines, </title> <booktitle> Proc. 26th Hawaii International Conference on Systems Sciences (HICSS-26), </booktitle> <month> January </month> <year> 1993, </year> <pages> pages 466-475. </pages> <note> (incorrectly published under M.A. Alexander, et al.) </note>
Reference-contexts: Unrolling and scheduling accesses can be used to bring data into cache as efficiently as possible when the instruction set implements a block prefetching instruction. The technique 1. Memory access coalescing is a related compile-time optimization that attempts to minimize the number of bus transactions for wide-bus machines <ref> [Ale93] </ref>. Compiling for Efficient Memory Utilization 15 may be useful for systems that include hardware support for dynamic access ordering, as well. For instance, reordering can be used to exploit the stream buffers proposed by Jouppi [Jou90] or the Cray T3Ds read-ahead mechanism discussed in Section 3.3.
Reference: [Ben91] <author> M.E. Benitez, J. W. Davidson, </author> <title> Code Generation for Streaming: An Access/ Execute Mechanism, </title> <booktitle> Proc. Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <month> April </month> <year> 1991, </year> <pages> pages 132-141. </pages>
Reference: [Bro95] <author> J. Brooks, </author> <title> Single PE Optimization Techniques for the Cray T3D System, </title> <booktitle> Proc. 1st European T3D Workshop, </booktitle> <month> September, </month> <year> 1995. </year>
Reference-contexts: As Jeff Brooks explains, the rates you see in [a T3D] application depend highly on how the memory is referenced <ref> [Bro95] </ref>. This variance in performance occurs because the T3Ds DRAMs can perform some access sequences faster than others. <p> The direct-mapped, onchip data cache holds 256 lines of four words each, for a total capacity of 1024 words that can be accessed at a rate of one 64-bit word every cycle <ref> [Bro95] </ref>. This machine implements a read-ahead mode 1 that causes the next cache line after a miss to be prefetched into a special stream buffer.
Reference: [Cra95] <institution> Cray T3D Massively Parallel Processing System, Cray Research, Inc., </institution> <note> http:// www.cray.com/PUBLIC/product-info/mpp/CRAY_T3D.html, 1995. </note>
Reference-contexts: 1. Introduction As has become painfully obvious, processor speeds are increasing much faster than memory speeds. To illustrate the current problem, consider the multiprocessor Cray T3D <ref> [Cra95] </ref>. The peak Dynamic Random Access Memory (DRAM) read bandwidth for each 150 MHz DEC Alpha processor [DEC92] of this machine is 320 Mbytes/sec, or about one 64-bit word per four clock cycles. <p> Others allow some regions of memory to be designated as non-cacheable (as in the DEC Alpha [DEC92]). Still other systems include special-purpose hardware for streams (as in the Cray T3D <ref> [Cra95] </ref>, the WM [Wul92], the Stream Memory Controller [McK95c], or the hardware proposed by Valero, et al., for reordering vector accesses to avoid bank conicts [Val92]). On machines that enable access ordering optimizations, the performance benefits can be significant. <p> Experimental Platforms Our investigations of compile-time access ordering target three different platforms: a node of the Intel iPSC/860 [Int91], the University of Virginias Stream Memory Controller system [McK95c], and a node of the Cray T3D <ref> [Cra95] </ref>.
Reference: [DEC92] <institution> Digital Technical Journal, Digital Equipment Corporation, </institution> <note> 4(4), Special Issue, 1992, http://www.digital.com/info/DTJ/axp-toc.html. </note>
Reference-contexts: 1. Introduction As has become painfully obvious, processor speeds are increasing much faster than memory speeds. To illustrate the current problem, consider the multiprocessor Cray T3D [Cra95]. The peak Dynamic Random Access Memory (DRAM) read bandwidth for each 150 MHz DEC Alpha processor <ref> [DEC92] </ref> of this machine is 320 Mbytes/sec, or about one 64-bit word per four clock cycles. Unfortunately, the actual bandwidth may be as low as 28 Mbytes/sec in other words, the processors can perform up to 42 instructions in the time it takes to read a single DRAM location. <p> Although not a common feature, non-caching loads are supported by some processors (as in the Convex C-1 [Wal85] or Intel i860 [Int91]). Others allow some regions of memory to be designated as non-cacheable (as in the DEC Alpha <ref> [DEC92] </ref>). Still other systems include special-purpose hardware for streams (as in the Cray T3D [Cra95], the WM [Wul92], the Stream Memory Controller [McK95c], or the hardware proposed by Valero, et al., for reordering vector accesses to avoid bank conicts [Val92]).
Reference: [Don90] <author> J. Dongarra, J. DuCroz, I. Duff, S. Hammerling, </author> <title> A set of Level 3 Basic Linear Algebra Subprograms, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference: [Goo85] <author> J.R. Goodman, J. Hsieh, K. Liou, A.R. Pleszkun, P.B. Schechter, H.C. Young, </author> <title> PIPE: A VLSI Decoupled Architecture, </title> <booktitle> Proc. 12th International Symposium on Computer Architecture (ISCA), </booktitle> <month> June </month> <year> 1985, </year> <month> pages 20-27. </month> <title> Compiling for Efficient Memory Utilization 28 </title>
Reference: [IEE92] <author> Memory Catches Up, </author> <title> Special Report, </title> <journal> IEEE Spectrum, </journal> <volume> 29(10) </volume> <month> 34-53 October </month> <year> 1992. </year> <title> [Int91] i860 XP Microprocessor Data Book, </title> <publisher> Intel Corporation, </publisher> <year> 1991. </year>
Reference-contexts: For instance, nearly all current DRAMs (including the T3Ds) implement a form of fast-page mode operation <ref> [IEE92] </ref>. The sense amplifiers in fast-page mode devices behave much like a single line, or page, of cache on chip. A memory access falling outside the current page causes a new one to be loaded, making such page misses take three to five times as long as page hits. <p> Although the terminology is similar, DRAM pages should not be confused with virtual memory pages. Compiling for Efficient Memory Utilization 2 With an advertised bandwidth of 500 Mbytes per second, Rambus is another interesting new memory technology <ref> [IEE92] </ref>. These bus-based systems are capable of delivering a byte of data every 2 ns for a block of information up to 256 bytes long. Like page-mode DRAMs, Rambus devices use banks of sense amplifier latches to cache data on chip. <p> The order of requests strongly affects the performance of other common devices that offer speed-optimizing features (nibble-mode, static column mode, or a small amount of SRAM cache on chip) or exhibit novel organizations (Ramlink and the new synchronous DRAM designs) <ref> [IEE92] </ref>. For interleaved memory systems, the order of requests is important on another level, as well: accesses to different banks can be performed in parallel, and can be completed faster than successive accesses to the same bank.
Reference: [Jou90] <author> N.P. Jouppi, </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> Proc. 17th International Symposium on Computer Architecture (ISCA), </booktitle> <month> May </month> <year> 1990, </year> <pages> pages 364-373. </pages> <institution> Also Digital Equipment Corporation, Western Research Lab, </institution> <note> Technical Note TN-14, </note> <month> March </month> <year> 1990. </year>
Reference-contexts: Compiling for Efficient Memory Utilization 15 may be useful for systems that include hardware support for dynamic access ordering, as well. For instance, reordering can be used to exploit the stream buffers proposed by Jouppi <ref> [Jou90] </ref> or the Cray T3Ds read-ahead mechanism discussed in Section 3.3. Even though data is always fetched and cached in units of a line, register-level access ordering can be used to load the cache lines in an order that exploits the read-ahead hardware and the DRAMs page-mode.
Reference: [Kes93] <author> R.E. Kessler, J.L. Schwarzmeier, </author> <title> Cray T3D: A New Dimension for Cray Research, </title> <booktitle> Proc. </booktitle> <address> COMPCON93, San Francisco, </address> <pages> pages 176-182. </pages>
Reference: [Lam91] <author> M. Lam, E. Rothberg, and M.E. Wolf, </author> <title> The Cache Performance and Optimizations of Blocked Algorithms, </title> <booktitle> Proc. Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> , <month> April </month> <year> 1991, </year> <pages> pages 63-74. </pages>
Reference-contexts: conditions is it advantageous to avoid preloading entirely? Can we develop a model to bound performance for computations on this memory system, or to predict performance for a given computation and set of relative vector offsets? How much data reuse is required before the copying optimization of Lam, et al. <ref> [Lam91] </ref>, can profitably be used to reduce cache conicts for this machine? These are the kinds of questions that we hope to answer. Stay tuned to our web pages (http://www.cs.virginia.edu/~wm/) for future developments. 6.
Reference: [Lee91] <author> K. Lee, </author> <title> Achieving High Performance on the i860, </title> <type> Technical Report RNR-91-029, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> October </month> <year> 1991. </year>
Reference: [Lee92] <author> K. Lee, </author> <title> On the Floating Point Performance of the i860 Microprocessor, </title> <type> Technical Report 90-019, </type> <institution> NASA Ames Research Center, Moffett Field, CA, </institution> <note> October 1990 (my copy appears to have been revised, </note> <month> July </month> <year> 1992). </year>
Reference-contexts: Figure 9 depicts these limits graphically, along with a few empirical data points. We measured the performance for loading a 1024-element vector with blocking factors of 8 and 16. The daxpy performance is calculated from Lees timings for a 1024-element vector and blocking factor of 10 <ref> [Lee92] </ref>. In general, performance depends on block size, but is relatively independent of vector length. The limits for tridiag apply to any computation without read-modify-write vectors, including a single vector load.
Reference: [Lee93] <author> K. Lee, </author> <title> The NAS860 Library Users Manual, </title> <type> NAS Technical Report RND-93-003, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> March </month> <year> 1993. </year>
Reference: [McK95a] <author> S.A. McKee, </author> <title> Wm.A. Wulf, Access Ordering and Memory-Conscious Cache Utilization, </title> <booktitle> Proc. First International Symposium on High Performance Computer Architecture, </booktitle> <address> Raleigh, NC, </address> <month> January </month> <year> 1995, </year> <pages> pages 253-262. </pages>
Reference-contexts: Instead of reading all stream elements, as was done with the non-caching pipelined loads of the i860, it suffices to touch one element per line when loading the data to cache <ref> [McK95a] </ref>. Palacharla and Kessler examine the performance of both schemes, register-level reordering and preloading, for several vector kernels as well as whole benchmarks [Pal95]. Compile-time access ordering can still be useful for a system in which an external SMC has been added to enhance the performance of an existing processor.
Reference: [McK95b] <author> S.A. McKee, </author> <title> Maximizing Memory Bandwidth for Streamed Computations, </title> <type> Ph.D. Dissertation, </type> <institution> University of Virginia, </institution> <month> May, </month> <year> 1995. </year>
Reference: [McK95c] <author> S.A. McKee, C.W. Oliver, Wm.A. Wulf, K.L. Wright, J.H. Aylor, </author> <title> Evaluation of Dynamic Access Ordering Hardware, </title> <institution> University of Virginia, Department of Computer Science, </institution> <note> Technical Report CS-95-46, October 1995. See also http:// www.cs.virginia.edu/~wm/smc.html for related publications. </note>
Reference-contexts: Others allow some regions of memory to be designated as non-cacheable (as in the DEC Alpha [DEC92]). Still other systems include special-purpose hardware for streams (as in the Cray T3D [Cra95], the WM [Wul92], the Stream Memory Controller <ref> [McK95c] </ref>, or the hardware proposed by Valero, et al., for reordering vector accesses to avoid bank conicts [Val92]). On machines that enable access ordering optimizations, the performance benefits can be significant. <p> The graph on the right presents the same information in terms of the average number of cycles per stream access. 3. Experimental Platforms Our investigations of compile-time access ordering target three different platforms: a node of the Intel iPSC/860 [Int91], the University of Virginias Stream Memory Controller system <ref> [McK95c] </ref>, and a node of the Cray T3D [Cra95]. <p> Compile-time access ordering can still be useful for a system in which an external SMC has been added to enhance the performance of an existing processor. This organization is used in our prototype, proof-of-concept system <ref> [McK95c] </ref>. In this system, performance depends on processor bus utilization as well as memory utilization, thus the cost of switching between reading and writing should be amortized over as many accesses as possible.
Reference: [McM86] <author> F.H. McMahon, </author> <title> The Livermore Fortran Kernels: A Computer Test of the Numerical Performance Range, </title> <institution> Lawrence Livermore National Laboratory, UCRL-53745, </institution> <month> December </month> <year> 1986. </year>
Reference-contexts: On machines that enable access ordering optimizations, the performance benefits can be significant. As a simple example, consider the effect of executing the fifth Livermore Loop (tridiagonal elimination <ref> [McM86] </ref>) using non-caching accesses to reference a single bank of page-mode DRAMs. Figure 1 (a) represents the natural reference sequence for a straightforward translation of the computation . This computation occurs frequently in practice, especially in the solution of partial differential equations by finite difference or finite element methods.
Reference: [Mea92] <author> L. Meadows, S. Nakamoto, V. Schuster, </author> <title> A Vectorizing Software Pipelining Compiler for LIW and Superscalar Architectures, </title> <booktitle> Proc. </booktitle> <address> RISC92, </address> <month> pages 331-343. </month> <title> Compiling for Efficient Memory Utilization 29 </title>
Reference-contexts: The computation is then strip-mined to operate on the cached data blocks. Meadows, et al., describe a similar preloading scheme used by the PGI i860 compiler <ref> [Mea92] </ref>. Unrolling and scheduling accesses can be used to bring data into cache as efficiently as possible when the instruction set implements a block prefetching instruction. The technique 1. Memory access coalescing is a related compile-time optimization that attempts to minimize the number of bus transactions for wide-bus machines [Ale93].
Reference: [Moy91] <author> S.A. Moyer, </author> <title> Performance of the iPSC/860 Node Architecture, </title> <institution> University of Virginia Institute for Parallel Computation, Report IPC-91-07, </institution> <month> May </month> <year> 1991. </year> <note> (This and the following reports can be accessed from http://www.cs.virginia.edu/ research/techrep.html/) </note>
Reference: [Moy92a] <author> S.A. Moyer, </author> <title> Access Ordering Algorithms for a Single Module Memory, </title> <institution> University of Virginia Institute for Parallel Computation, Report IPC-92-02, </institution> <month> December </month> <year> 1992. </year>
Reference: [Moy92b] <author> S.A. Moyer, </author> <title> Access Ordering Algorithms for an Interleaved Memory, </title> <institution> University of Virginia Institute for Parallel Computation, Report IPC-92-12, </institution> <month> December </month> <year> 1992. </year>
Reference: [Moy92c] <author> S.A. Moyer, </author> <title> Access Ordering Algorithms for a Multicopy Memory, </title> <institution> University of Virginia Institute for Parallel Computation, Report IPC-92-13, </institution> <month> December </month> <year> 1992. </year>
Reference: [Moy93] <author> S.A. Moyer, </author> <title> Access Order and Effective Memory Bandwidth, </title> <type> Ph.D. Dissertation, </type> <institution> University of Virginia, </institution> <month> May, </month> <year> 1993. </year>
Reference: [Pal95] <author> S. Palacharla, R.E. Kessler, </author> <title> Program Restructuring to Exploit Page Mode and Read-Ahead Features of the Cray T3D, Cray Research Internal Report, </title> <year> 1995. </year>
Reference-contexts: Palacharla and Kessler examine the performance of both schemes, register-level reordering and preloading, for several vector kernels as well as whole benchmarks <ref> [Pal95] </ref>. Compile-time access ordering can still be useful for a system in which an external SMC has been added to enhance the performance of an existing processor. This organization is used in our prototype, proof-of-concept system [McK95c]. <p> To investigate the performance effects of such conicts when more than one stream is preloaded to cache, Palacharla and Kessler experiment with random array placements (175 in all) <ref> [Pal95] </ref>. They generate code to determine dynamically how much of each stream to preload. The overhead costs in this approach include computing the block size to preload and the extra reads to bring in each cache line. <p> cache). copy daxpy Compiling for Efficient Memory Utilization 26 5.3 Cray T3D To this authors knowledge, Palacharla and Kessler were the first to investigate unrolling and scheduling loads (with or without preloading the data to cache) to take advantage of the page-mode DRAMs and the read-ahead hardware of the T3D <ref> [Pal95] </ref>. Their method determines at run-time the cache conicts between vectors, and calculates how much of each vector should be preloaded; the same block size is not used for all vectors. Our work on the Cray T3D is still preliminary.
Reference: [Smi84] <author> J.E. Smith, </author> <title> Decoupled Access/Execute Architectures, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(4) </volume> <pages> 289-308, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: The ideas behind them are initially due to Wulf. Benitez and Davidson present versions of the algorithms developed for the WM, a novel superscalar architecture with hardware support for streaming [Wul92,Ben91]. Although designed for architectures with hardware support for the access/ execute model of computation <ref> [Smi84] </ref>, many of the techniques are applicable to stock microprocessors. Vectorizing compilers could be used to identify potential accesses for reordering, but stream detection is both simpler and more general than vectorization. Streaming code can often be generated for codes that are impossible to vectorize.
Reference: [Val92] <author> M. Valero, T. Lang, J.M. Llabera, M. Peiron, E. Ayguad, and J.J. Navarro, J.J., </author> <title> Increasing the Number of Strides for Conict-Free Vector Access, </title> <booktitle> Proc. 19th Annual International Symposium on Computer Architecture (ISCA), </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992, </year> <pages> pages 372-381. </pages>
Reference-contexts: Still other systems include special-purpose hardware for streams (as in the Cray T3D [Cra95], the WM [Wul92], the Stream Memory Controller [McK95c], or the hardware proposed by Valero, et al., for reordering vector accesses to avoid bank conicts <ref> [Val92] </ref>). On machines that enable access ordering optimizations, the performance benefits can be significant. As a simple example, consider the effect of executing the fifth Livermore Loop (tridiagonal elimination [McM86]) using non-caching accesses to reference a single bank of page-mode DRAMs.
Reference: [Wal85] <author> S. Wallach, </author> <title> The CONVEX C-1 64-bit Supercomputer, </title> <booktitle> Proc. Compcon Spring85, </booktitle> <month> February </month> <year> 1985. </year>
Reference-contexts: For instance, an instruction set that gives the user exibility in how memory is accessed makes it easier to exploit features of the memory system through access ordering. Although not a common feature, non-caching loads are supported by some processors (as in the Convex C-1 <ref> [Wal85] </ref> or Intel i860 [Int91]). Others allow some regions of memory to be designated as non-cacheable (as in the DEC Alpha [DEC92]).
Reference: [Wul92] <author> Wm. A. Wulf, </author> <title> Evaluation of the WM Architecture, </title> <booktitle> Proc. 19th Annual International Symposium on Computer Architecture (ISCA), </booktitle> <address> Gold Coast, Australia, </address> <booktitle> published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 20(2) </volume> <pages> 382-390, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Others allow some regions of memory to be designated as non-cacheable (as in the DEC Alpha [DEC92]). Still other systems include special-purpose hardware for streams (as in the Cray T3D [Cra95], the WM <ref> [Wul92] </ref>, the Stream Memory Controller [McK95c], or the hardware proposed by Valero, et al., for reordering vector accesses to avoid bank conicts [Val92]). On machines that enable access ordering optimizations, the performance benefits can be significant.
References-found: 30

