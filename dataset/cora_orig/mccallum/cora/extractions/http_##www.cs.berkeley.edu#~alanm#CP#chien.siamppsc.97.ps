URL: http://www.cs.berkeley.edu/~alanm/CP/chien.siamppsc.97.ps
Refering-URL: http://www.cs.berkeley.edu/~alanm/CP/bib.html
Root-URL: 
Email: fachien,pakin,lauria,mbbuchan,hane,giannini,prusakovg@cs.uiuc.edu  
Title: High Performance Virtual Machines (HPVM): Clusters with Supercomputing APIs and Performance Fast Messages (FM), MPI-FM,
Author: Andrew Chien Scott Pakin Mario Lauria Matt Buchanan Kay Hane Louis Giannini Jane Prusakova 
Note: Current elements of the HPVM project are: Illinois  This software is in use at dozens of sites.  
Date: December 21, 1996  
Affiliation: Department of Computer Science and National Center for Supercomputing Applications University of Illinois at Urbana-Champaign  
Abstract: The HPVM project provides software which enables high-performance computing on clusters of PCs and workstations using standard supercomputing APIs such as MPI, SHMEM Put/Get, and Global Arrays. HPVMs|High-Performance Virtual Machines|are surprisingly competitive with MPP systems, such as the IBM SP2 and Cray T3D. The Illinois HPVM achieves impressive low-level communication performance across the cluster: one-way latencies of around 11 sec and bandwidths &gt; 50 MBytes/sec|even for small packets (&lt; 256 bytes). Performance at higher levels, such as MPI, is expected to be approximately 17 sec latency and also &gt; 50 MByte/sec bandwidth. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. W. Felten, and J. Sandberg, </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer, </title> <booktitle> in Proceeding of the International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994, </year> <pages> pp. 142-153. </pages> <note> Available from http://www.cs.princeton.edu/shrimp/papers/isca94.paper.ps. </note>
Reference-contexts: Data transmission|the common case|is performed at user level. This approach is exemplified by Hamlyn [4], Cranium [15], U-Net [25], and SHRIMP <ref> [1] </ref>. Since operating systems are traditionally used for process protection, these systems all exploit the system's virtual memory hardware for protection, a much lower-overhead mechanism than system calls. With fast microprocessors, high-speed networks, and efficient software, PCs are an effective tool for high-performance computing.
Reference: [2] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. L. Seitz, J. N. Seizovic, and W. K. </author> <title> Su, </title> <journal> Myrinet|a gigabit-per-second local-area network, IEEE Micro, </journal> <volume> 15 (1995), </volume> <pages> pp. 29-36. </pages> <note> Available from http://www.myri.com/research/publications/Hot.ps. </note>
Reference-contexts: The primary remaining distinguishing feature of high-performance systems from distributed systems is the speed of communication and efficiency of coordination. While parallel machines typically exploit high-performance custom networks, high-performance commodity networks are becoming widely available at reasonable cost <ref> [2, 3, 7, 10] </ref>. While it may be some time before these networks are pervasive, their ready availability makes building machine clusters with high-performance interconnects feasible. For example, Myricom's Myrinet [2] hardware is capable of link speeds of 160 MBytes/sec and switch latencies of under a microsecond. <p> While parallel machines typically exploit high-performance custom networks, high-performance commodity networks are becoming widely available at reasonable cost [2, 3, 7, 10]. While it may be some time before these networks are pervasive, their ready availability makes building machine clusters with high-performance interconnects feasible. For example, Myricom's Myrinet <ref> [2] </ref> hardware is capable of link speeds of 160 MBytes/sec and switch latencies of under a microsecond. The advent of such high-performance networks coupled with gigaflop microprocessors makes high-performance 1 2 computing on clusters an attractive alternative. <p> However, 10 Mbits/sec is far too little bandwidth for network-intensive applications. Fortunately, paralleling the advances in microprocessor performance, a number of new, higher-bandwidth 3 "killer networks" have recently hit the market. These include FDDI [6], 100 Base-T Ethernet [7], FibreChannel [27], Myrinet <ref> [2] </ref>, and ATM/SONET [3] and currently run from 100 Mbits/sec to over 600 Mbits/sec, with the ability to scale to Gbit/sec bandwidths and beyond. Unfortunately, software for these new networks generally lags well behind the hardware in terms of performance.
Reference: [3] <author> J. Boudec, </author> <title> The Asynchronous Transfer Mode: A tutorial, Computer Networks and ISDN Systems, </title> <booktitle> 24 (1992), </booktitle> <pages> pp. 279-309. </pages>
Reference-contexts: The primary remaining distinguishing feature of high-performance systems from distributed systems is the speed of communication and efficiency of coordination. While parallel machines typically exploit high-performance custom networks, high-performance commodity networks are becoming widely available at reasonable cost <ref> [2, 3, 7, 10] </ref>. While it may be some time before these networks are pervasive, their ready availability makes building machine clusters with high-performance interconnects feasible. For example, Myricom's Myrinet [2] hardware is capable of link speeds of 160 MBytes/sec and switch latencies of under a microsecond. <p> However, 10 Mbits/sec is far too little bandwidth for network-intensive applications. Fortunately, paralleling the advances in microprocessor performance, a number of new, higher-bandwidth 3 "killer networks" have recently hit the market. These include FDDI [6], 100 Base-T Ethernet [7], FibreChannel [27], Myrinet [2], and ATM/SONET <ref> [3] </ref> and currently run from 100 Mbits/sec to over 600 Mbits/sec, with the ability to scale to Gbit/sec bandwidths and beyond. Unfortunately, software for these new networks generally lags well behind the hardware in terms of performance. Legacy software structures, designed more for portability/interoperability than for performance, are still prevalent.
Reference: [4] <author> G. Buzzard, D. Jacobson, S. Marovich, and J. Wilkes, Hamlyn: </author> <title> A high-performance network interface with sender-based memory management, </title> <booktitle> in Proceedings of the IEEE Hot Interconnects Symposium, </booktitle> <month> August </month> <year> 1995. </year> <note> Available from http://www.hpl.hp.com/personal/ John Wilkes/papers/HamlynHotIntIII.pdf. </note>
Reference-contexts: Data transmission|the common case|is performed at user level. This approach is exemplified by Hamlyn <ref> [4] </ref>, Cranium [15], U-Net [25], and SHRIMP [1]. Since operating systems are traditionally used for process protection, these systems all exploit the system's virtual memory hardware for protection, a much lower-overhead mechanism than system calls.
Reference: [5] <author> Cray Research, Inc., </author> <title> Cray T3D System Architecture Overview, </title> <month> March </month> <year> 1993. </year>
Reference-contexts: Machines such as the Intel Paragon [12], Thinking Machines CM-5 [24], IBM SP2 [11], and, more recently, SGI/Cray's T3D <ref> [5] </ref>, T3E [19], and Origin 2000 [20] are all based on microprocessors, and combine them in parallel configurations to achieve high performance.
Reference: [6] <institution> Fiber-distributed data interface (FDDI)|Token ring media access control (MAC). American National Standard for Information Systems ANSI X3.139-1987, </institution> <month> July </month> <year> 1987. </year> <institution> American National Standards Institute. </institution>
Reference-contexts: Today, most local area networks are interconnected via 10 Mbits/sec Ethernet [17]. However, 10 Mbits/sec is far too little bandwidth for network-intensive applications. Fortunately, paralleling the advances in microprocessor performance, a number of new, higher-bandwidth 3 "killer networks" have recently hit the market. These include FDDI <ref> [6] </ref>, 100 Base-T Ethernet [7], FibreChannel [27], Myrinet [2], and ATM/SONET [3] and currently run from 100 Mbits/sec to over 600 Mbits/sec, with the ability to scale to Gbit/sec bandwidths and beyond. Unfortunately, software for these new networks generally lags well behind the hardware in terms of performance.
Reference: [7] <author> L. Goldberg, 100Base-T4 transceiver simplifies adapter, repeater, </author> <title> and switch designs, </title> <institution> Elec tronic Design, </institution> <year> (1995), </year> <pages> pp. 155-160. </pages>
Reference-contexts: The primary remaining distinguishing feature of high-performance systems from distributed systems is the speed of communication and efficiency of coordination. While parallel machines typically exploit high-performance custom networks, high-performance commodity networks are becoming widely available at reasonable cost <ref> [2, 3, 7, 10] </ref>. While it may be some time before these networks are pervasive, their ready availability makes building machine clusters with high-performance interconnects feasible. For example, Myricom's Myrinet [2] hardware is capable of link speeds of 160 MBytes/sec and switch latencies of under a microsecond. <p> However, 10 Mbits/sec is far too little bandwidth for network-intensive applications. Fortunately, paralleling the advances in microprocessor performance, a number of new, higher-bandwidth 3 "killer networks" have recently hit the market. These include FDDI [6], 100 Base-T Ethernet <ref> [7] </ref>, FibreChannel [27], Myrinet [2], and ATM/SONET [3] and currently run from 100 Mbits/sec to over 600 Mbits/sec, with the ability to scale to Gbit/sec bandwidths and beyond. Unfortunately, software for these new networks generally lags well behind the hardware in terms of performance.
Reference: [8] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum, </author> <title> A high-performance, portable implementation of the MPI message passing interface standard. </title> <note> Available from http://www.mcs.anl.gov/mpi/ mpicharticle/paper.html or ftp://ftp.mcs.anl.gov/pub/mpi/mpicharticle.ps. </note>
Reference-contexts: To demonstrate this capability (and to create a high-performance implementation of a high-level API), we implemented two high-level APIs atop FM: MPI [16] and Global Arrays [18]. MPI-FM is based on the MPICH code base from Argonne and Mississippi State University <ref> [8] </ref>. MPICH is implemented as a two-level structure. The Abstract Device Interface (ADI) contains only a small number of functions (25), and the rest of MPI (125 functions) is built on top of that. To run MPI on FM, we needed only port MPICH's ADI to communicate with FM calls.
Reference: [9] <author> R. Gusella, </author> <title> A measurement study of diskless workstation traffic on Ethernet, </title> <journal> IEEE Transactions on Communications, </journal> <month> 38 </month> <year> (1990). </year>
Reference-contexts: This is an important result, because most network traffic is comprised of messages of about that size (see, for example, <ref> [9] </ref>). 3.2 Standard APIs While low-level layers such as Fast Messages can deliver hardware communication performance, higher-level layers offer greater functionality, application portability, and ease of use. The problem is that high-level layers add overhead to communication and generally perform significantly worse than low-level messaging layers.
Reference: [10] <author> R. Horst, TNet: </author> <title> A reliable system area network, </title> <booktitle> IEEE Micro, </booktitle> <year> (1995), </year> <pages> pp. 37-45. 8 </pages>
Reference-contexts: The primary remaining distinguishing feature of high-performance systems from distributed systems is the speed of communication and efficiency of coordination. While parallel machines typically exploit high-performance custom networks, high-performance commodity networks are becoming widely available at reasonable cost <ref> [2, 3, 7, 10] </ref>. While it may be some time before these networks are pervasive, their ready availability makes building machine clusters with high-performance interconnects feasible. For example, Myricom's Myrinet [2] hardware is capable of link speeds of 160 MBytes/sec and switch latencies of under a microsecond.
Reference: [11] <institution> IBM Corporation, Scalable POWERparallel System, </institution> <year> 1995. </year> <note> Available from http:// ibm.tc.cornell.edu/ibm/pps/sp2/sp2.html. </note>
Reference-contexts: As microprocessors have continued to increase in performance, they have approached the performance of the fastest vector processors and, because of their low cost, have become the building block of choice for high-performance parallel computing systems. Machines such as the Intel Paragon [12], Thinking Machines CM-5 [24], IBM SP2 <ref> [11] </ref>, and, more recently, SGI/Cray's T3D [5], T3E [19], and Origin 2000 [20] are all based on microprocessors, and combine them in parallel configurations to achieve high performance.
Reference: [12] <author> Intel Corporation, </author> <title> Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: As microprocessors have continued to increase in performance, they have approached the performance of the fastest vector processors and, because of their low cost, have become the building block of choice for high-performance parallel computing systems. Machines such as the Intel Paragon <ref> [12] </ref>, Thinking Machines CM-5 [24], IBM SP2 [11], and, more recently, SGI/Cray's T3D [5], T3E [19], and Origin 2000 [20] are all based on microprocessors, and combine them in parallel configurations to achieve high performance.
Reference: [13] <author> V. Karamcheti and A. Chien, </author> <title> Software overhead in messaging layers: Where does the time go?, </title> <booktitle> in Proceedings of the Sixth Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <year> 1994. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/asplos94.ps. </note>
Reference-contexts: What distinguishes FM from other messaging layers is not the surface API, but the underlying semantics|the service guarantees and control of the memory hierarchy that FM provides to software built atop FM. Analysis of the literature and our ongoing studies to support fine-grained parallel computing <ref> [13, 22] </ref> led to the conclusion that a low-level messaging layer should provide the following key guarantees, or higher-level messaging layers will suffer a performance loss: * Reliable delivery, * Ordered delivery, and 4 * Control over scheduling of communication work (decoupling).
Reference: [14] <author> M. J. Litzkow, M. Livny, and M. W. </author> <title> Mutka, Condor|a hunter of idle workstations, </title> <booktitle> in Proceedings of the 8th International Conference of Distributed Computing Systems, </booktitle> <month> June </month> <year> 1988, </year> <pages> pp. 104-111. </pages>
Reference-contexts: The resource stealing model, pioneered by systems such as Condor <ref> [14] </ref> and Utopia [28] and now commercialized in systems such as the Load Sharing Facility [23], exploit heterogeneous, shared resources connected by low-performance networks and harness them to achieve high throughput of sequential jobs.
Reference: [15] <author> N. R. McKenzie, K. Bolding, C. Ebeling, and L. Snyder, Cranium: </author> <title> An interface for message passing on adaptive packet routing networks, </title> <booktitle> in Proceedings of the 1994 Parallel Computer Routing and Communication Workshop, </booktitle> <month> May </month> <year> 1994. </year> <note> Available from ftp://shrimp.cs.washington.edu/pub/chaos/docs/cranium-pcrcw.ps.Z. </note>
Reference-contexts: Data transmission|the common case|is performed at user level. This approach is exemplified by Hamlyn [4], Cranium <ref> [15] </ref>, U-Net [25], and SHRIMP [1]. Since operating systems are traditionally used for process protection, these systems all exploit the system's virtual memory hardware for protection, a much lower-overhead mechanism than system calls. With fast microprocessors, high-speed networks, and efficient software, PCs are an effective tool for high-performance computing.
Reference: [16] <author> Message Passing Interface Forum, </author> <title> The MPI message passing interface standard, </title> <type> tech. rep., </type> <institution> University of Tennessee, Knoxville, </institution> <month> April </month> <year> 1994. </year> <note> Available from http://www.mcs.anl.gov/ mpi/mpi-report.ps. </note>
Reference-contexts: FM, and specifically, FM 2.0, is designed to reduce the performance gap between low- and high-level messaging layers. To demonstrate this capability (and to create a high-performance implementation of a high-level API), we implemented two high-level APIs atop FM: MPI <ref> [16] </ref> and Global Arrays [18]. MPI-FM is based on the MPICH code base from Argonne and Mississippi State University [8]. MPICH is implemented as a two-level structure.
Reference: [17] <author> R. Metcalfe and D. Boggs, </author> <title> Ethernet: Distributed packet-switching for local computer networks, </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 19 (1976), </volume> <pages> pp. 395-404. </pages>
Reference-contexts: Virtually all high-performance computing vendors now market systems based on ensembles of microprocessors. In addition to fast microprocessors, fast networks are also a necessity for efficient coordinated computation. Today, most local area networks are interconnected via 10 Mbits/sec Ethernet <ref> [17] </ref>. However, 10 Mbits/sec is far too little bandwidth for network-intensive applications. Fortunately, paralleling the advances in microprocessor performance, a number of new, higher-bandwidth 3 "killer networks" have recently hit the market.
Reference: [18] <author> J. Nieplocha, R. J. Harrison, and R. J. Littlefield, </author> <title> Global Arrays: A portable "shared-memory" programming model for distributed memory computers, </title> <booktitle> in Supercomputing '94, </booktitle> <year> 1994. </year> <note> Available from http://www.computer.org/conferen/sc94/neiploch.html. </note>
Reference-contexts: FM, and specifically, FM 2.0, is designed to reduce the performance gap between low- and high-level messaging layers. To demonstrate this capability (and to create a high-performance implementation of a high-level API), we implemented two high-level APIs atop FM: MPI [16] and Global Arrays <ref> [18] </ref>. MPI-FM is based on the MPICH code base from Argonne and Mississippi State University [8]. MPICH is implemented as a two-level structure. The Abstract Device Interface (ADI) contains only a small number of functions (25), and the rest of MPI (125 functions) is built on top of that.
Reference: [19] <author> S. L. Scott, </author> <title> Synchronization and communication in the T3E multiprocessor, </title> <booktitle> in Architec tural Support for Programming Languages and Operating Systems (ASPLOS-VII), </booktitle> <address> Cam-bridge, Massachusetts, </address> <month> October </month> <year> 1996, </year> <pages> pp. 26-36. </pages> <note> Available from http://reality.sgi.com/ sls craypark/Papers/asplos96.html. </note>
Reference-contexts: Machines such as the Intel Paragon [12], Thinking Machines CM-5 [24], IBM SP2 [11], and, more recently, SGI/Cray's T3D [5], T3E <ref> [19] </ref>, and Origin 2000 [20] are all based on microprocessors, and combine them in parallel configurations to achieve high performance.
Reference: [20] <author> Silicon Graphics, Inc., </author> <title> Origin Servers: Technical Overview of the Origin Family, </title> <note> 1996. Available from http://www.sgi.com/Products/hardware/servers/technology/ overview.html. </note>
Reference-contexts: Machines such as the Intel Paragon [12], Thinking Machines CM-5 [24], IBM SP2 [11], and, more recently, SGI/Cray's T3D [5], T3E [19], and Origin 2000 <ref> [20] </ref> are all based on microprocessors, and combine them in parallel configurations to achieve high performance.
Reference: [21] <author> P. G. Sobalvarro and W. E. Weihl, </author> <booktitle> Demand-based coscheduling of parallel jobs on multipro grammed multiprocessors, in Proceedings of the Parallel Job Scheduling Workshop at IPPS '95, </booktitle> <year> 1995. </year> <note> Available from http://www.psg.lcs.mit.edu/~pgs/papers/jsw-for-springer.ps. Also appears in Springer-Verlag Lecture Notes in Computer Science, Vol. 949. </note>
Reference-contexts: In this paper, we address the delivery of high performance communication. For a discussion of coordinated scheduling|another key aspect of HPVM|see <ref> [21] </ref>. Communication performance is a critical aspect of parallel computation, so we have built a series of low-level and high level communication layers which deliver high-performance communication for distributed resources. The lowest layer, Illinois Fast Messages (FM) achieves 11 sec latencies and bandwidths in excess of 50 MBytes/sec.
Reference: [22] <author> C. B. Stunkel and W. K. Fuchs, </author> <title> An analysis of cache performance for a hypercube multicom puter, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3 (1992), </volume> <pages> pp. 421-432. </pages>
Reference-contexts: What distinguishes FM from other messaging layers is not the surface API, but the underlying semantics|the service guarantees and control of the memory hierarchy that FM provides to software built atop FM. Analysis of the literature and our ongoing studies to support fine-grained parallel computing <ref> [13, 22] </ref> led to the conclusion that a low-level messaging layer should provide the following key guarantees, or higher-level messaging layers will suffer a performance loss: * Reliable delivery, * Ordered delivery, and 4 * Control over scheduling of communication work (decoupling).
Reference: [23] <author> J. Suplick, </author> <title> An analysis of load balancing technology. </title> <note> Available from http://platform.com/ products/lsf.comp.ps.Z, </note> <month> January </month> <year> 1994. </year>
Reference-contexts: The resource stealing model, pioneered by systems such as Condor [14] and Utopia [28] and now commercialized in systems such as the Load Sharing Facility <ref> [23] </ref>, exploit heterogeneous, shared resources connected by low-performance networks and harness them to achieve high throughput of sequential jobs. If parallel jobs are supported, they are efficient only if they are loosely-coupled (e.g. as in PVM).
Reference: [24] <author> Thinking Machines Corporation, </author> <title> The Connection Machine CM-5 Technical Summary, </title> <booktitle> 245 First Street, </booktitle> <address> Cambridge, MA 02154-1264, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: As microprocessors have continued to increase in performance, they have approached the performance of the fastest vector processors and, because of their low cost, have become the building block of choice for high-performance parallel computing systems. Machines such as the Intel Paragon [12], Thinking Machines CM-5 <ref> [24] </ref>, IBM SP2 [11], and, more recently, SGI/Cray's T3D [5], T3E [19], and Origin 2000 [20] are all based on microprocessors, and combine them in parallel configurations to achieve high performance.
Reference: [25] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels, U-Net: </author> <title> A user-level network interface for parallel and distributed computing, </title> <booktitle> in Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year> <note> Available from http://www.cs.cornell.edu/Info/ Projects/ATM/sosp.ps. </note>
Reference-contexts: Data transmission|the common case|is performed at user level. This approach is exemplified by Hamlyn [4], Cranium [15], U-Net <ref> [25] </ref>, and SHRIMP [1]. Since operating systems are traditionally used for process protection, these systems all exploit the system's virtual memory hardware for protection, a much lower-overhead mechanism than system calls. With fast microprocessors, high-speed networks, and efficient software, PCs are an effective tool for high-performance computing.
Reference: [26] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser, </author> <title> Active Messages: a mechanism for integrated communication and computation, </title> <booktitle> in Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: In the following subsections, we describe first the FM interfaces, and then several of the higher level communication layers built on top. 3.1 Fast Messages The FM interface traces its roots to Berkeley Active Messages <ref> [26] </ref> on the CM-5. The FM 1.1 API (Table 1) contains functions for sending long and short messages and for extracting messages from the network. Each message is associated with a handler, a function executing at the receiver and that stores or processes the message data.
Reference: [27] <author> X3T11 Technical Committee, </author> <title> Project 1119D, Fibre Channel Physical and Signalling Interface 3 (FC-PH-3), Revision 8.3, </title> <institution> American National Standards Institute, </institution> <month> January </month> <year> 1996. </year> <title> Working draft proposed American National Standard for Information Systems. </title> <note> Available from http://www.network.com/~ftp/FC/PH/FCPH3 83.ps. </note>
Reference-contexts: However, 10 Mbits/sec is far too little bandwidth for network-intensive applications. Fortunately, paralleling the advances in microprocessor performance, a number of new, higher-bandwidth 3 "killer networks" have recently hit the market. These include FDDI [6], 100 Base-T Ethernet [7], FibreChannel <ref> [27] </ref>, Myrinet [2], and ATM/SONET [3] and currently run from 100 Mbits/sec to over 600 Mbits/sec, with the ability to scale to Gbit/sec bandwidths and beyond. Unfortunately, software for these new networks generally lags well behind the hardware in terms of performance.
Reference: [28] <author> S. Zhou, J. Wang, X. Zheng, and P. Delisle, </author> <title> Utopia: A load sharing facility for large, heterogeneous distributed computer systems, </title> <journal> Software|Practice and Experience, </journal> <volume> 23 (1993), </volume> <pages> pp. 1305-1336. </pages> <note> Als appeared as Technical Report CSRI-257, April, 1992. Available from http://platform.com/products/lsf.paper.ps.Z. </note>
Reference-contexts: The resource stealing model, pioneered by systems such as Condor [14] and Utopia <ref> [28] </ref> and now commercialized in systems such as the Load Sharing Facility [23], exploit heterogeneous, shared resources connected by low-performance networks and harness them to achieve high throughput of sequential jobs. If parallel jobs are supported, they are efficient only if they are loosely-coupled (e.g. as in PVM).
References-found: 28

