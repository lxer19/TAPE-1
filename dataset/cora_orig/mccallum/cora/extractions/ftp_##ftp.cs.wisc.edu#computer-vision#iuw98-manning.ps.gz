URL: ftp://ftp.cs.wisc.edu/computer-vision/iuw98-manning.ps.gz
Refering-URL: http://www.cs.wisc.edu/computer-vision/pubs.html
Root-URL: http://www.cs.wisc.edu
Email: frmanningjdyerg@cs.wisc.edu  
Title: Interpolating View and Scene Motion by Dynamic View Morphing  
Author: Russell A. Manning Charles R. Dyer 
Note: This work is sponsored in part by the Defense Advanced Research Projects Agency (DARPA) and Rome Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-97-1-0138.  
Web: http://www.cs.wisc.edu/ frmanningjdyerg  
Address: Madison, WI 53706  
Affiliation: Department of Computer Sciences University of Wisconsin  
Abstract: We present a novel technique for interpolating between two views of a dynamic scene. Our approach extends the concept of view morphing introduced in [ Seitz and Dyer, 1996 ] and retains the relative advantages of that method. The interpolation will portray one possible physically-valid version of what transpired in the scene during the intervening time between views. The scene is assumed to consist of a small number of objects. Each object can undergo any motion during the time between views as long as the total movement is equivalent to a single, rigid translation. The dynamic view morphing technique can work with widely-spaced reference views, sparse point correspondences, and uncalibrated cameras. When the camera-to-camera transformation can be determined, the virtual objects can be portrayed moving along straight-line, constant-velocity trajectories. Methods are developed for determining the camera-to-camera transformation from information available in the reference views. It is shown that each moving object in a scene has a corresponding fundamental matrix and that the camera-to-camera transformation can be determined from two distinct fundamental matrices. Dynamic view morphing is developed for both pinhole and orthographic cameras, and the use of three or more reference views is discussed. Static view morphing is made more ver satile with respect to occlusion, and mosaicing is combined with dynamic view morphing for the case when both reference views share the same optical center. The resulting combination of techniques can be used to fill-in missing gaps in movies, perform "view hand-offs" between cameras at different locations, create movies from still images, perform movie stabilization and compression, track objects during periods of obstruction, and related tasks. 
Abstract-found: 1
Intro-found: 1
Reference: [ Chen and Williams, 1993 ] <author> Shenchang Eric Chen and Lance Williams. </author> <title> View interpolation for image synthesis. </title> <booktitle> In Proc. SIGGRAPH 93, </booktitle> <pages> pages 279-288, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction This paper presents a novel method for interpolating between two views of a dynamic scene. View interpolation <ref> [ Chen and Williams, 1993 ] </ref> is the process of creating a series of virtual views of a scene which, taken together, represent a continuous and physically accurate transition between two reference views of the scene. Most work on view interpolation is restricted to static scenes.
Reference: [ Manning and Dyer, 1998 ] <author> R. A. Manning and C. R. Dyer. </author> <title> Dynamic view morphing. </title> <type> Technical Report 1387, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <year> 1998. </year>
Reference-contexts: To make the sequence, the direction of the table edge was aligned in both views during the prewarp. The direction of the background object was also aligned. Why such alignment creates the desired effect is explained in <ref> [ Manning and Dyer, 1998 ] </ref> . 3.6 Orthographic Cameras The mathematical development for orthographic cameras is similar to that for pinhole cameras. However, except in special cases, no camera-to-camera transformation exists between the reference cameras. Hence it is typically impossible to guarantee linear motion for the virtual objects. <p> If the fundamental matrix can be determined for two objects in the scene and if the objects are not moving parallel to each other, then T AB can be determined directly from these two fundamental matrices. The previous fact is proven in <ref> [ Manning and Dyer, 1998 ] </ref> , which also gives a method for approximating T AB from two conjugate directions by making a reasonable assumption about the internal parameters of typical cameras. 5 Applications Dynamic view morphing has many potential applications; we list a few here: filling a missing gap in <p> From two distinct fundamental matrices, the camera-to-camera trans formation can be determined. The topics of this paper, as well as many additional topics and observations, are discussed in much greater detail in <ref> [ Manning and Dyer, 1998 ] </ref> .
Reference: [ Seitz and Dyer, 1996 ] <author> Steven M. Seitz and Charles R. Dyer. </author> <title> View morphing. </title> <booktitle> In Proc. SIGGRAPH 96, </booktitle> <pages> pages 21-30, </pages> <year> 1996. </year>
Reference-contexts: Therefore, view interpolation for dynamic scenes must portray a continuous change in viewpoint and a continuous change in the scene itself. Our work is based upon an earlier technique called view morphing <ref> [ Seitz and Dyer, 1996 ] </ref> which provides a method for interpolating between two widely-spaced views of a static scene (Fig. 1). The technique has several strengths which make it suitable for practical applica building stands at the intersection of two roads, and a truck is parked near the building. <p> By increasing the density of conjugate points, the virtual view can be made arbitrarily accurate. The prewarp is performed from information available in the fundamental matrix, which is calculated directly from the conjugate points. Complete details of the algorithm can be found in <ref> [ Seitz and Dyer, 1996 ] </ref> . 3 Dynamic View Morphing 3.1 Preliminary Concepts We assume the two reference views are captured at time t = 0 and time t = 1 through pinhole cameras, which are denoted camera A and camera B, respectively.
References-found: 3

