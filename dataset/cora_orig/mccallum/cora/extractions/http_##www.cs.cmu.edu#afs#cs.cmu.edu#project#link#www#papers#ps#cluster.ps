URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/link/www/papers/ps/cluster.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/link/www/homepage.html
Root-URL: 
Title: To appear in Maximum Entropy and Bayesian Methods, CLUSTER EXPANSIONS AND ITERATIVE SCALING FOR MAXIMUM
Author: K. Hanson and R. Silver JOHN D. LAFFERTY AND BERNHARD SUHM 
Address: 5000 Forbes Avenue Pittsburgh, PA 15217 USA  
Affiliation: School of Computer Science Carnegie Mellon University  
Note: (eds), Kluwer Academic Publishers, 1995.  
Abstract: The maximum entropy method has recently been successfully introduced to a variety of natural language applications. In each of these applications, however, the power of the maximum entropy method is achieved at the cost of a considerable increase in computational requirements. In this paper we present a technique, closely related to the classical cluster expansion from statistical mechanics, for reducing the computational demands necessary to calculate conditional maximum entropy language models. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> S. D. Pietra, V. D. Pietra, and J. Lafferty, </author> <title> "Inducing features of random fields," </title> <type> tech. rep., </type> <institution> CMU-CS-95-144, Department of Computer Science, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: 1. Introduction In this paper we present a computational technique that can enable faster calculation of maximum entropy models. The starting point for our method is an algorithm <ref> [1] </ref> for constructing maximum entropy distributions that is an extension of the generalized iterative scaling algorithm of Darroch and Ratcliff [2,3]. <p> While the restriction (2) on M can always be enforced by introducing a "slack variable," it can be inconvenient to do so for conditional maximum entropy language models that typically have hundreds of thousands of features. In <ref> [1] </ref> an algorithm was introduced that extends the Darroch-Ratcliff procedure by relaxing the assumption that M (h; w) is a constant. The updates for the improved algorithm are again given by equation (3), but with M now interpreted as a random variable. When (2) holds, the algorithms are identical.
Reference: 2. <author> J. Darroch and D. Ratcliff, </author> <title> "Generalized iterative scaling for log-linear models," </title> <journal> Ann. Math. Statistics, </journal> <volume> 43, </volume> <pages> pp. 1470-1480, </pages> <year> 1972. </year>
Reference-contexts: In addition, the use of conditional models is desirable for applications which process the input in a left-to-right fashion. 3. Iterative Scaling The generalized iterative scaling algorithm of Darroch and Ratcliff <ref> [2] </ref> is one method for calculating the maximum entropy distribution (1).
Reference: 3. <author> I. Csiszar, </author> <title> "A geometric interpretation of Darroch and Ratcliff's generalized iterative scaling," </title> <journal> The Annals of Statistics, </journal> <volume> 17, (3), </volume> <pages> pp. 1409-1413, </pages> <year> 1989. </year>
Reference: 4. <author> L. R. Bahl, F. Jelinek, and R. L. Mercer, </author> <title> "A maximum likelihood approach to continuous speech recognition," </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, PAMI-5, </journal> <volume> (2), </volume> <pages> pp. 179-190, </pages> <year> 1983. </year>
Reference-contexts: Language Modeling 2.1. LANGUAGE MODELS AS PRIORS FOR BAYESIAN DECODING Language modeling attempts to identify regularities in natural language and capture them in a statistical model. Language models are crucial ingredients in automatic speech recognition <ref> [4] </ref> and statistical machine translation [5] systems, where their use is naturally viewed in terms of the noisy channel model from information theory.
Reference: 5. <author> P. Brown, J. Cocke, S. D. Pietra, V. D. Pietra, F. Jelinek, J. Lafferty, R. Mercer, and P. Roosin, </author> <title> "A statistical approach to machine translation," </title> <journal> Computational Linguistics, </journal> <volume> 16, </volume> <pages> pp. 79-85, </pages> <year> 1990. </year>
Reference-contexts: Language Modeling 2.1. LANGUAGE MODELS AS PRIORS FOR BAYESIAN DECODING Language modeling attempts to identify regularities in natural language and capture them in a statistical model. Language models are crucial ingredients in automatic speech recognition [4] and statistical machine translation <ref> [5] </ref> systems, where their use is naturally viewed in terms of the noisy channel model from information theory.
Reference: 6. <author> E. T. Jaynes, </author> <title> Papers on Probability, Statistics, and Statistical Physics, </title> <address> D. </address> <publisher> Reidel Publishing, </publisher> <address> Dordrecht-Holland, </address> <year> 1983. </year>
Reference-contexts: The most common language models used in today's speech systems are the n-gram models, constructed in terms of simple word frequencies. 2.2. CONDITIONAL MAXIMUM ENTROPY LANGUAGE MODELS In the usual application of the maximum entropy principle <ref> [6] </ref>, prior information, typically in the form of frequencies, is represented as a set of constraints which collectively determine a unique maximum entropy distribution.
Reference: 7. <author> A. Berger, S. D. Pietra, and V. D. Pietra, </author> <title> "A maximum entropy approach to natural language processing," </title> <note> Computational Linguistics, to appear, </note> <year> 1995. </year>
Reference: 8. <author> R. Lau, R. Rosenfeld, and S. Roukos, </author> <title> "Adaptive language modeling using the maximum entropy principle," </title> <booktitle> in Proceedings of the ARPA Human Language Technology Workshop, </booktitle> <pages> pp. 108-113, </pages> <publisher> Morgan Kaufman Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Summary Our use of the cluster expansion for the language model presented in Section 5 demonstrates that this technique can be an important tool for reducing the computational burden of computing maximum entropy language models. The method also applies to higher order models such as "trigger models" <ref> [8] </ref>, where occurrences of words far back in the history can influence predictions by the use of long-distance bigram parameters. As a general technique, however, the method is limited in its usefulness.
Reference: 9. <author> R. P. </author> <title> Feynman, Statistical Mechanics: A Set of Lectures, </title> <editor> W. A. </editor> <publisher> Benjamin, </publisher> <address> Reading, MA, </address> <year> 1972. </year>
Reference-contexts: While this is then carried further to obtain a series expansion for the grand partition function, our use of the method will simply make use of the discrete analogues of the integrals b l for conditional models. For more details on the statistical physics calculations we refer to <ref> [9] </ref>. 4.2. CLUSTER EXPANSIONS FOR CONDITIONAL MAXENT MODELS The computation necessary to carry out the iterative scaling algorithm described in Section 3 is naturally divided into two parts.
Reference: 10. <author> P. C. Cheeseman, </author> <title> "A method for computing generalized Bayesian probability values for expert systems," </title> <booktitle> in Proc. Eighth International Conference on Artificial Intelligence, </booktitle> <pages> pp. 198-202, </pages> <year> 1983. </year>
Reference: 11. <author> S. A. Goldman, </author> <title> "Efficient methods for calculating maximum entropy distributions," </title> <type> tech. rep., </type> <institution> MIT Department of Electrical Engineering and Computer Science (Masters thesis), </institution> <year> 1987. </year>
Reference: 12. <author> J. Godfrey, E. Holliman, and M. McDaniel, </author> <title> "Switchboard: Telephone speech corpus for research development," </title> <booktitle> in Proc. ICASSP-92, </booktitle> <pages> pp. </pages> <address> I-517-520, </address> <year> 1992. </year>
Reference-contexts: Example: A Topic-Dependent Language Model In this section we describe the application of the cluster expansion to the training of a topic-dependent bigram model of the Switchboard corpus <ref> [12] </ref> for use in a speech recognition system. This corpus comprises approximately three million words of text, transcribed from more than 150 hours of speech collected from telephone conversations. An important aspect of the Switchboard corpus is that the conversations are restricted to 70 different topics.
References-found: 12

