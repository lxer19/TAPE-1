URL: http://polaris.cs.uiuc.edu/reports/1518.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Title: c  
Author: flCopyright by WILLIAM MORTON POTTENGER 
Date: 1997  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Zahira Ammarguellat and Luddy Harrison. </author> <title> Automatic Recognition of Induction & Recurrence Relations by Abstract Interpretation. </title> <booktitle> Proceedings of Sigplan 1990, </booktitle> <address> Yorktown Heights, </address> <month> 25(6) </month> <pages> 283-295, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [32, 12, 35, 42, 1, 26, 22, 44] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [12, 35]. <p> This approach has the advantage that complex conditional structures occurring in multiply-nested loops pose no particular problem. This approach is, however, inherently limited to the particular patterns programmed into the compiler. 2.2.2 Compile-time Solution of Recurrences Compile-time solutions include <ref> [1] </ref>, in which Harrison and Ammarguellat use abstract interpretation to map each variable assigned in a loop to a symbolic form, and match these against template patterns containing the closed-forms for commonly occurring recurrences. They are able to handle both induction variables and array-based recurrences in singly-nested loops. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 7 2.2.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [34, 12, 35, 42, 30, 1, 17, 47, 50, 44, 59] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication). <p> The [ ] enclose lists. The append operator takes two operands which are lists and creates and returns a new list by appending the second operand to the first. In the first case above, the list [2] is first appended to the list <ref> [1] </ref>, resulting in the list [1 2]. The list [3] is then appended to this list, resulting in the final list [1 2 3]. In the second case, the list [3] is first appended to the list [2], resulting in the list [2 3]. <p> The list [3] is then appended to this list, resulting in the final list [1 2 3]. In the second case, the list [3] is first appended to the list [2], resulting in the list [2 3]. This list is then appended to the list <ref> [1] </ref>, resulting in the same final list. The final result is identical in both cases even though the associative order of the operands differ. However, if we now consider a case where we attempt to commute the operands, the results will differ: append ([1] [2]) ) [1 2] append ([2] [1]) <p> <ref> [1] </ref>, resulting in the same final list. The final result is identical in both cases even though the associative order of the operands differ. However, if we now consider a case where we attempt to commute the operands, the results will differ: append ([1] [2]) ) [1 2] append ([2] [1]) ) [2 1] Clearly the append operator is not commutative. This has implications for the parallelization of output operations in that loops containing sequential output operations must be parallelized based on associativity alone. <p> In effect, we have represented the array a as a conglomerate operand rather than as multiple individual elements. As defined above, op&append is associative. This can be easily demonstrated in a way similar to append but with one important additional constraint: op&append ( op&append ( <ref> [1] </ref> [2]) [3]) ) op&append ( [1 (1 2)] [3]) ) [1 (1 2) ((1 2) 3)] op&append ( [1] op&append ( [2] [3])) ) op&append ( [1] [2 (2 3)]) ) [1 (1 2) (1 (2 3))] Here items in [ ] are lists, and ( ) are used to <p> As defined above, op&append is associative. This can be easily demonstrated in a way similar to append but with one important additional constraint: op&append ( op&append ( <ref> [1] </ref> [2]) [3]) ) op&append ( [1 (1 2)] [3]) ) [1 (1 2) ((1 2) 3)] op&append ( [1] op&append ( [2] [3])) ) op&append ( [1] [2 (2 3)]) ) [1 (1 2) (1 (2 3))] Here items in [ ] are lists, and ( ) are used to properly associate the operands of the binary operator . Note that if is associative, then op&append is associative. <p> This can be easily demonstrated in a way similar to append but with one important additional constraint: op&append ( op&append ( <ref> [1] </ref> [2]) [3]) ) op&append ( [1 (1 2)] [3]) ) [1 (1 2) ((1 2) 3)] op&append ( [1] op&append ( [2] [3])) ) op&append ( [1] [2 (2 3)]) ) [1 (1 2) (1 (2 3))] Here items in [ ] are lists, and ( ) are used to properly associate the operands of the binary operator . Note that if is associative, then op&append is associative. <p> The append operator takes two operands which are lists and creates a new list by appending the second operand to the first. In the first case above, the list [2] is first appended to the list <ref> [1] </ref>, resulting in the list [1 2]. The list [3] is then appended to this list, resulting in the final list [1 2 3]. In the second case, the list [3] is first appended to the list [2], resulting in the list [2 3]. <p> The list [3] is then appended to this list, resulting in the final list [1 2 3]. In the second case, the list [3] is first appended to the list [2], resulting in the list [2 3]. This list is then appended to the list <ref> [1] </ref>, resulting in the same final list. The final result is identical in both cases even though the associative order of the operands differ. However, as was demonstrated in Chapter 2, if we now consider a case where we attempt to commute the terms, the results will differ: append ([1] [2]) <p> The final result is identical in both cases even though the associative order of the operands differ. However, as was demonstrated in Chapter 2, if we now consider a case where we attempt to commute the terms, the results will differ: append (<ref> [1] </ref> [2]) ) [1 2] append ([2] [1]) ) [2 1] Clearly the append operator is not commutative. <p> For example: k = 0 50 k = k + 1 k = 0 enddo Assuming m &lt; n, the scalar induction variable k takes on the values <ref> [1; m] </ref> n=m times, resulting in n=m discontinuities in its sequence of values. <p> A third example occurs in the SPEC CFP95 benchmark su2cor which implements a lagged Fibonacci generator as described previously in this section. Lagged Fibonacci generators such as that implemented in su2cor take the form of a homogeneous linear recurrence. Such relations can be automatically detected using pattern recognition techniques <ref> [1] </ref>. General techniques for solving linear recurrences of this type are well known [38], and closed-forms for such recurrences can be computed at compile-time as discussed in section 4.3.1 above. <p> argc != 6) - cerr &lt;< "Usage: cSpace [ name num_procs input_file output_file"; cerr &lt;< " [ idx_file ] ]"n"; exit (1); - dbx_warn_overflow = 1; if (argc == 1) - ConceptSpace cs; String o_rname = "cout"; cs.compute (o_rname); - else if (argc == 5) - String name = argv <ref> [1] </ref>; int threads = atoi (argv [2]); String ifile = argv [3]; String ofile = argv [4]; String idx_ofile = ""; ConceptSpace cs (name, ifile, ofile, idx_ofile); String o_rname = ofile; cs.compute (o_rname, threads); - else if (argc == 6) - String name = argv [1]; int threads = atoi (argv <p> - String name = argv <ref> [1] </ref>; int threads = atoi (argv [2]); String ifile = argv [3]; String ofile = argv [4]; String idx_ofile = ""; ConceptSpace cs (name, ifile, ofile, idx_ofile); String o_rname = ofile; cs.compute (o_rname, threads); - else if (argc == 6) - String name = argv [1]; int threads = atoi (argv [2]); String ifile = argv [3]; String ofile = argv [4]; String idx_ofile = argv [5]; ConceptSpace cs (name, ifile, ofile, idx_ofile, sO_NOSTORE, dF_RAW, dF_INDEXED); String o_rname = ofile; cs.compute (o_rname, threads); - // HeapStats::stop (); // HeapStats::report (cout); // HeapStats::print_memory_leaks (cout); - // module
Reference: [2] <author> R. Asenjo, M. Ujaldon, and E. L. Zapata. </author> <title> SpLU Sparse LU Factorization. HPF-2. Scope of Activities and Motivating Applications. High Performance Fortran Forum, </title> <note> version 0.8 edition, </note> <month> November </month> <year> 1994. </year>
Reference-contexts: To understand this point, consider a simple example involving the lisp append operator: append (append ([1] <ref> [2] </ref>) [3]) ) append ([1 2] [3]) ) [1 2 3] append ([1] append ([2] [3])) ) append ([1] [2 3]) 16 ) [1 2 3] Here we are making a simple list of the numbers 1, 2, and 3. The [ ] enclose lists. <p> The [ ] enclose lists. The append operator takes two operands which are lists and creates and returns a new list by appending the second operand to the first. In the first case above, the list <ref> [2] </ref> is first appended to the list [1], resulting in the list [1 2]. The list [3] is then appended to this list, resulting in the final list [1 2 3]. In the second case, the list [3] is first appended to the list [2], resulting in the list [2 3]. <p> the first case above, the list <ref> [2] </ref> is first appended to the list [1], resulting in the list [1 2]. The list [3] is then appended to this list, resulting in the final list [1 2 3]. In the second case, the list [3] is first appended to the list [2], resulting in the list [2 3]. This list is then appended to the list [1], resulting in the same final list. The final result is identical in both cases even though the associative order of the operands differ. <p> The final result is identical in both cases even though the associative order of the operands differ. However, if we now consider a case where we attempt to commute the operands, the results will differ: append ([1] <ref> [2] </ref>) ) [1 2] append ([2] [1]) ) [2 1] Clearly the append operator is not commutative. This has implications for the parallelization of output operations in that loops containing sequential output operations must be parallelized based on associativity alone. <p> In effect, we have represented the array a as a conglomerate operand rather than as multiple individual elements. As defined above, op&append is associative. This can be easily demonstrated in a way similar to append but with one important additional constraint: op&append ( op&append ( [1] <ref> [2] </ref>) [3]) ) op&append ( [1 (1 2)] [3]) ) [1 (1 2) ((1 2) 3)] op&append ( [1] op&append ( [2] [3])) ) op&append ( [1] [2 (2 3)]) ) [1 (1 2) (1 (2 3))] Here items in [ ] are lists, and ( ) are used to properly <p> As defined above, op&append is associative. This can be easily demonstrated in a way similar to append but with one important additional constraint: op&append ( op&append ( [1] <ref> [2] </ref>) [3]) ) op&append ( [1 (1 2)] [3]) ) [1 (1 2) ((1 2) 3)] op&append ( [1] op&append ( [2] [3])) ) op&append ( [1] [2 (2 3)]) ) [1 (1 2) (1 (2 3))] Here items in [ ] are lists, and ( ) are used to properly associate the operands of the binary operator . Note that if is associative, then op&append is associative. <p> steps: * Divide the iteration space of L into p contiguous slices * Each processor computes the recurrence a i = a i1 fi i for its given slice, using as an initial value the identity for the coalescing loop operator * Serially compute the last values of each slice <ref> [2; p 1] </ref> * Each processor [2; p] sums the last value from the preceding slice into the elements in its slice 2 Here we have taken advantage of the indexable nature of the array a to treat each element as a scalar. <p> of L into p contiguous slices * Each processor computes the recurrence a i = a i1 fi i for its given slice, using as an initial value the identity for the coalescing loop operator * Serially compute the last values of each slice [2; p 1] * Each processor <ref> [2; p] </ref> sums the last value from the preceding slice into the elements in its slice 2 Here we have taken advantage of the indexable nature of the array a to treat each element as a scalar. This enables us to parallelize the loop without privatizing a. <p> It is summarized by the pseudo-code "Append reference to term to doc.terms in doc" above. To understand this point, consider a simple example involving the lisp append operator drawn from Chapter 2: append (append ([1] <ref> [2] </ref>) [3]) ) append ([1 2] [3]) ) [1 2 3] append ([1] append ([2] [3])) ) append ([1] [2 3]) ) [1 2 3] Here we are making a simple list of the numbers 1, 2, and 3. <p> The append operator takes two operands which are lists and creates a new list by appending the second operand to the first. In the first case above, the list <ref> [2] </ref> is first appended to the list [1], resulting in the list [1 2]. The list [3] is then appended to this list, resulting in the final list [1 2 3]. In the second case, the list [3] is first appended to the list [2], resulting in the list [2 3]. <p> the first case above, the list <ref> [2] </ref> is first appended to the list [1], resulting in the list [1 2]. The list [3] is then appended to this list, resulting in the final list [1 2 3]. In the second case, the list [3] is first appended to the list [2], resulting in the list [2 3]. This list is then appended to the list [1], resulting in the same final list. The final result is identical in both cases even though the associative order of the operands differ. <p> The final result is identical in both cases even though the associative order of the operands differ. However, as was demonstrated in Chapter 2, if we now consider a case where we attempt to commute the terms, the results will differ: append ([1] <ref> [2] </ref>) ) [1 2] append ([2] [1]) ) [2 1] Clearly the append operator is not commutative. <p> This led to the inclusion of the original C version of SpLU in the suite of HPF-2 motivating applications. The version of SpLU included in our benchmark suite is a Fortran implementation by the authors of the original HPF-2 version <ref> [2] </ref>. The sparse matrix lns 3937 from the Harwell-Boeing collection was used as input for the results reported in this chapter. 5.3 Analysis and Results In Chapter 4 we discussed several techniques that we found important in parallelizing sparse and irregular Fortran codes. <p> "Usage: cSpace [ name num_procs input_file output_file"; cerr &lt;< " [ idx_file ] ]"n"; exit (1); - dbx_warn_overflow = 1; if (argc == 1) - ConceptSpace cs; String o_rname = "cout"; cs.compute (o_rname); - else if (argc == 5) - String name = argv [1]; int threads = atoi (argv <ref> [2] </ref>); String ifile = argv [3]; String ofile = argv [4]; String idx_ofile = ""; ConceptSpace cs (name, ifile, ofile, idx_ofile); String o_rname = ofile; cs.compute (o_rname, threads); - else if (argc == 6) - String name = argv [1]; int threads = atoi (argv [2]); String ifile = argv [3]; <p> int threads = atoi (argv <ref> [2] </ref>); String ifile = argv [3]; String ofile = argv [4]; String idx_ofile = ""; ConceptSpace cs (name, ifile, ofile, idx_ofile); String o_rname = ofile; cs.compute (o_rname, threads); - else if (argc == 6) - String name = argv [1]; int threads = atoi (argv [2]); String ifile = argv [3]; String ofile = argv [4]; String idx_ofile = argv [5]; ConceptSpace cs (name, ifile, ofile, idx_ofile, sO_NOSTORE, dF_RAW, dF_INDEXED); String o_rname = ofile; cs.compute (o_rname, threads); - // HeapStats::stop (); // HeapStats::report (cout); // HeapStats::print_memory_leaks (cout); - // module ConceptSpace #include "ConceptSpace.h" #include &lt;strstream.h&gt; #include
Reference: [3] <author> Rafael Asenjo, Eladio Gutierrez, Yuan Lin, David Padua, Bill Pottenger, and Emilio Zapata. </author> <title> On the Automatic Parallelization of Sparse and Irregular Fortran Codes. </title> <type> Technical Report 1512, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: To understand this point, consider a simple example involving the lisp append operator: append (append ([1] [2]) <ref> [3] </ref>) ) append ([1 2] [3]) ) [1 2 3] append ([1] append ([2] [3])) ) append ([1] [2 3]) 16 ) [1 2 3] Here we are making a simple list of the numbers 1, 2, and 3. The [ ] enclose lists. <p> To understand this point, consider a simple example involving the lisp append operator: append (append ([1] [2]) <ref> [3] </ref>) ) append ([1 2] [3]) ) [1 2 3] append ([1] append ([2] [3])) ) append ([1] [2 3]) 16 ) [1 2 3] Here we are making a simple list of the numbers 1, 2, and 3. The [ ] enclose lists. <p> To understand this point, consider a simple example involving the lisp append operator: append (append ([1] [2]) <ref> [3] </ref>) ) append ([1 2] [3]) ) [1 2 3] append ([1] append ([2] [3])) ) append ([1] [2 3]) 16 ) [1 2 3] Here we are making a simple list of the numbers 1, 2, and 3. The [ ] enclose lists. <p> The append operator takes two operands which are lists and creates and returns a new list by appending the second operand to the first. In the first case above, the list [2] is first appended to the list [1], resulting in the list [1 2]. The list <ref> [3] </ref> is then appended to this list, resulting in the final list [1 2 3]. In the second case, the list [3] is first appended to the list [2], resulting in the list [2 3]. This list is then appended to the list [1], resulting in the same final list. <p> In the first case above, the list [2] is first appended to the list [1], resulting in the list [1 2]. The list <ref> [3] </ref> is then appended to this list, resulting in the final list [1 2 3]. In the second case, the list [3] is first appended to the list [2], resulting in the list [2 3]. This list is then appended to the list [1], resulting in the same final list. The final result is identical in both cases even though the associative order of the operands differ. <p> In effect, we have represented the array a as a conglomerate operand rather than as multiple individual elements. As defined above, op&append is associative. This can be easily demonstrated in a way similar to append but with one important additional constraint: op&append ( op&append ( [1] [2]) <ref> [3] </ref>) ) op&append ( [1 (1 2)] [3]) ) [1 (1 2) ((1 2) 3)] op&append ( [1] op&append ( [2] [3])) ) op&append ( [1] [2 (2 3)]) ) [1 (1 2) (1 (2 3))] Here items in [ ] are lists, and ( ) are used to properly associate <p> As defined above, op&append is associative. This can be easily demonstrated in a way similar to append but with one important additional constraint: op&append ( op&append ( [1] [2]) <ref> [3] </ref>) ) op&append ( [1 (1 2)] [3]) ) [1 (1 2) ((1 2) 3)] op&append ( [1] op&append ( [2] [3])) ) op&append ( [1] [2 (2 3)]) ) [1 (1 2) (1 (2 3))] Here items in [ ] are lists, and ( ) are used to properly associate the operands of the binary operator . <p> This can be easily demonstrated in a way similar to append but with one important additional constraint: op&append ( op&append ( [1] [2]) <ref> [3] </ref>) ) op&append ( [1 (1 2)] [3]) ) [1 (1 2) ((1 2) 3)] op&append ( [1] op&append ( [2] [3])) ) op&append ( [1] [2 (2 3)]) ) [1 (1 2) (1 (2 3))] Here items in [ ] are lists, and ( ) are used to properly associate the operands of the binary operator . Note that if is associative, then op&append is associative. <p> It is summarized by the pseudo-code "Append reference to term to doc.terms in doc" above. To understand this point, consider a simple example involving the lisp append operator drawn from Chapter 2: append (append ([1] [2]) <ref> [3] </ref>) ) append ([1 2] [3]) ) [1 2 3] append ([1] append ([2] [3])) ) append ([1] [2 3]) ) [1 2 3] Here we are making a simple list of the numbers 1, 2, and 3. <p> It is summarized by the pseudo-code "Append reference to term to doc.terms in doc" above. To understand this point, consider a simple example involving the lisp append operator drawn from Chapter 2: append (append ([1] [2]) <ref> [3] </ref>) ) append ([1 2] [3]) ) [1 2 3] append ([1] append ([2] [3])) ) append ([1] [2 3]) ) [1 2 3] Here we are making a simple list of the numbers 1, 2, and 3. <p> To understand this point, consider a simple example involving the lisp append operator drawn from Chapter 2: append (append ([1] [2]) <ref> [3] </ref>) ) append ([1 2] [3]) ) [1 2 3] append ([1] append ([2] [3])) ) append ([1] [2 3]) ) [1 2 3] Here we are making a simple list of the numbers 1, 2, and 3. The append operator takes two operands which are lists and creates a new list by appending the second operand to the first. <p> The append operator takes two operands which are lists and creates a new list by appending the second operand to the first. In the first case above, the list [2] is first appended to the list [1], resulting in the list [1 2]. The list <ref> [3] </ref> is then appended to this list, resulting in the final list [1 2 3]. In the second case, the list [3] is first appended to the list [2], resulting in the list [2 3]. This list is then appended to the list [1], resulting in the same final list. <p> In the first case above, the list [2] is first appended to the list [1], resulting in the list [1 2]. The list <ref> [3] </ref> is then appended to this list, resulting in the final list [1 2 3]. In the second case, the list [3] is first appended to the list [2], resulting in the list [2 3]. This list is then appended to the list [1], resulting in the same final list. The final result is identical in both cases even though the associative order of the operands differ. <p> For example, in the irregular Fortran benchmark DSMC3D (Discrete Simulation Monte Carlo), a similar operation is performed <ref> [3] </ref>. Instead of terms, however, the list contains molecules. Both of these operations can be modeled as coalescing operations in which the list is being reduced in size. In [3], we reduce sections of the list in parallel, and the actual recombination of the reduced sections takes place in a dofinal <p> For example, in the irregular Fortran benchmark DSMC3D (Discrete Simulation Monte Carlo), a similar operation is performed <ref> [3] </ref>. Instead of terms, however, the list contains molecules. Both of these operations can be modeled as coalescing operations in which the list is being reduced in size. In [3], we reduce sections of the list in parallel, and the actual recombination of the reduced sections takes place in a dofinal section which follows the doall execution of the doevery portion of the loop. <p> input_file output_file"; cerr &lt;< " [ idx_file ] ]"n"; exit (1); - dbx_warn_overflow = 1; if (argc == 1) - ConceptSpace cs; String o_rname = "cout"; cs.compute (o_rname); - else if (argc == 5) - String name = argv [1]; int threads = atoi (argv [2]); String ifile = argv <ref> [3] </ref>; String ofile = argv [4]; String idx_ofile = ""; ConceptSpace cs (name, ifile, ofile, idx_ofile); String o_rname = ofile; cs.compute (o_rname, threads); - else if (argc == 6) - String name = argv [1]; int threads = atoi (argv [2]); String ifile = argv [3]; String ofile = argv [4]; <p> [2]); String ifile = argv <ref> [3] </ref>; String ofile = argv [4]; String idx_ofile = ""; ConceptSpace cs (name, ifile, ofile, idx_ofile); String o_rname = ofile; cs.compute (o_rname, threads); - else if (argc == 6) - String name = argv [1]; int threads = atoi (argv [2]); String ifile = argv [3]; String ofile = argv [4]; String idx_ofile = argv [5]; ConceptSpace cs (name, ifile, ofile, idx_ofile, sO_NOSTORE, dF_RAW, dF_INDEXED); String o_rname = ofile; cs.compute (o_rname, threads); - // HeapStats::stop (); // HeapStats::report (cout); // HeapStats::print_memory_leaks (cout); - // module ConceptSpace #include "ConceptSpace.h" #include &lt;strstream.h&gt; #include "Collection/BaseMapIter.h" #include "Collection/Iterator.h" #include "Collection/KeyDatabase.h"
Reference: [4] <author> Richard Barrett, Michael Berry, Tony F. Chan, James Demmel, June Donato, Jack Dongarra, Victor Eijkhout, Roldan Pozo, Charles Romine, and Henk van der Vorst. </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1994. </year>
Reference-contexts: excerpt demonstrates: do j=1,a_nr r (j)=r (j)+ad (k)*q (ac (k),i) enddo enddo The matrix 1138 BUS of Harwell-Boeing collection was used as input for this benchmark. 5.2.6 MVPRODUCT MVPRODUCT is a set of basic sparse matrix operations including sparse matrix-vector multiplication and the product and sum of two sparse matrices <ref> [4, 24] </ref>. The representation of the sparse matrices employs two different schemes: compressed row storage (CRS) and compressed column storage (CCS) [52]. <p> [ idx_file ] ]"n"; exit (1); - dbx_warn_overflow = 1; if (argc == 1) - ConceptSpace cs; String o_rname = "cout"; cs.compute (o_rname); - else if (argc == 5) - String name = argv [1]; int threads = atoi (argv [2]); String ifile = argv [3]; String ofile = argv <ref> [4] </ref>; String idx_ofile = ""; ConceptSpace cs (name, ifile, ofile, idx_ofile); String o_rname = ofile; cs.compute (o_rname, threads); - else if (argc == 6) - String name = argv [1]; int threads = atoi (argv [2]); String ifile = argv [3]; String ofile = argv [4]; String idx_ofile = argv [5]; <p> [3]; String ofile = argv <ref> [4] </ref>; String idx_ofile = ""; ConceptSpace cs (name, ifile, ofile, idx_ofile); String o_rname = ofile; cs.compute (o_rname, threads); - else if (argc == 6) - String name = argv [1]; int threads = atoi (argv [2]); String ifile = argv [3]; String ofile = argv [4]; String idx_ofile = argv [5]; ConceptSpace cs (name, ifile, ofile, idx_ofile, sO_NOSTORE, dF_RAW, dF_INDEXED); String o_rname = ofile; cs.compute (o_rname, threads); - // HeapStats::stop (); // HeapStats::report (cout); // HeapStats::print_memory_leaks (cout); - // module ConceptSpace #include "ConceptSpace.h" #include &lt;strstream.h&gt; #include "Collection/BaseMapIter.h" #include "Collection/Iterator.h" #include "Collection/KeyDatabase.h" // KeyDatabase of Term objects
Reference: [5] <author> M. Berry, D. Chen, P. Koss, D. Kuck, L. Pointer, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <booktitle> Int'l. Journal of Supercomputer Applications, Fall 1989, </booktitle> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: In this section we provide a brief overview of these suites which include Grand Challenge codes from the National Center for Supercomputing Applications (NCSA), codes from the Standard Performance Evaluation Corporation (SPEC CFP95), and codes from the Perfect Club <ref> [5] </ref>. Table 4.1 gives a brief synopsis of each code, including its origin, the number of lines of code, and the serial execution time on a R4400-based SGI Challenge. <p> In addition, random () is called in INITIA DO2000 in the Perfect Club benchmark MDG <ref> [5] </ref>. The CHOLESKY benchmark in our sparse & irregular suite also calls the rand () library routine in an important loop. In three of these cases, the RNG call is the only factor preventing parallelization of the loop after application of the techniques implemented in the current Polaris restructurer. <p> [4]; String idx_ofile = ""; ConceptSpace cs (name, ifile, ofile, idx_ofile); String o_rname = ofile; cs.compute (o_rname, threads); - else if (argc == 6) - String name = argv [1]; int threads = atoi (argv [2]); String ifile = argv [3]; String ofile = argv [4]; String idx_ofile = argv <ref> [5] </ref>; ConceptSpace cs (name, ifile, ofile, idx_ofile, sO_NOSTORE, dF_RAW, dF_INDEXED); String o_rname = ofile; cs.compute (o_rname, threads); - // HeapStats::stop (); // HeapStats::report (cout); // HeapStats::print_memory_leaks (cout); - // module ConceptSpace #include "ConceptSpace.h" #include &lt;strstream.h&gt; #include "Collection/BaseMapIter.h" #include "Collection/Iterator.h" #include "Collection/KeyDatabase.h" // KeyDatabase of Term objects KeyDatabase&lt;String, Term&gt; Terms; // List
Reference: [6] <author> G.A. Bird. </author> <title> Molecular Gas Dynamics and the Direct Simulation of Gas Flows. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> England, </address> <year> 1994. </year>
Reference-contexts: The Harwell-Boeing matrix BCSSTK30 was used as input for this benchmark [15]. 68 5.2.2 DSMC3D DSMC3D is a modification of the DSMC (Direct Simulation Monte Carlo) benchmark in 3 dimensions. DSMC implements a simulation of the behavior of particles of a gas in space using the Monte Carlo method <ref> [6] </ref>.
Reference: [7] <author> Graeme Bird. </author> <title> Personal communication with author, </title> <year> 1996. </year>
Reference-contexts: However, when a molecule leaves the flow, it is deleted from the list and replaced by the last molecule in the list. This creates loop-carried dependences in the loop. However, the deletion of molecules can be deferred until after the entire list has been processed <ref> [66, 7] </ref>.
Reference: [8] <author> William Blume, Ramon Doallo, Rudolf Eigenmann, John Grout, Jay Hoeflinger, Thomas Lawrence, Jaejin Lee, David Padua, Yunheung Paek, Bill Pottenger, Lawrence Rauchwerger, and Peng Tu. </author> <title> Parallel Programming with Polaris. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 78-82, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: Here we have conditional expressions guarding updates to scalar variables. This is a classic case of a loop with reduction semantics. Such patterns commonly occur in computationally important loops in a wide range of codes <ref> [8] </ref>. 1 This example is drawn from the HPF-2 benchmark cholesky 4 Many frameworks have been developed for recognizing parallelism of this nature based on both syntactic and semantic schemes. <p> The specific techniques used to parallelize output operations of this nature are applicable generally in computer programs that perform output. These techniques can be applied to the automatic parallelization of computer programs in systems such as the Polaris restructurer <ref> [8] </ref>. 2.3.7 Loops with Dynamic Last Values When a loop writes to a variable which is "live-out", the last value of the variable must be preserved across loop exit. <p> Co-occurring terms are ranked in decreasing order of similarity, with the result that more general terms occur lower in the list of co-occurring terms. 3.2.4 The Implementation cSpace is based upon a collection hierarchy derived from the Polaris Project, a research project investigating the automatic parallelization of Fortran codes <ref> [8] </ref>. The collection hierarchy provides an extensive set of templatized data structures including lists, trees, and sets. The data structures are templatized in the sense that they may contain many different types of objects. Any object derived from the base class Listable may be referred to by a collection. <p> The 43 specific techniques used to parallelize output are applicable generally in computer programs that perform output. As noted in Chapter 2, these techniques can also be applied in the automatic parallelization of computer programs in systems such as the Polaris restructurer <ref> [8] </ref>. 3.4.3.1 Parallel Output Operations As discussed in section 3.3.2, the output operation is associative but not commutative. As a result, the parallelizing transformation must retain the original non-commuted order of execution. <p> During a recent review of the Polaris restructurer <ref> [8] </ref> approximately 500 loops from programs in the aforementioned suites were identified as serial. <p> We consider how well the parallelization techniques presented in Chapter 4 apply to this collection of codes. In conducting this work, we compare existing technology in the commercial parallelizer PFA from SGI with the Polaris restructurer <ref> [8] </ref>. <p> In the following chapter we consider the application of the techniques presented in Chapter 4 on Fortran application benchmarks containing outer time-stepping loops. 80 CHAPTER 6 Parallelism in Time-Stepping Loops 6.1 Introduction The Polaris restructurer recognizes much doall parallelism at both the outer and inner loop level <ref> [8] </ref>. However, few experiments have been conducted on actual application codes to determine whether effective advantage can be taken of additional non-doall parallelism.
Reference: [9] <author> B.R. Brooks, R.E. Bruccoleri, B.D. Olafson, D.J. States, S. Swaminathan, and M. Karplus. CHARMM: </author> <title> A Program for Macromolecular Energy, Minimization, and Dynamics Calculations. </title> <journal> J. Comp. Chem., </journal> <volume> 4 </volume> <pages> 187-217, </pages> <year> 1983. </year>
Reference-contexts: The matrix BCSSTK14 from the Harwell-Boeing collection has been used as input to this benchmark. 5.2.7 NBFC The calculation of non-bonded forces forms a key element of many molecular dynamics computations <ref> [9] </ref>. NBFC computes an electro-static interaction between particles where the forces acting on an atom are calculated from a list of neighboring atoms.
Reference: [10] <author> H. Chen and K. J. Lynch. </author> <title> Automatic Construction of Networks of Concepts Characterizing Document Databases. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 22(5) </volume> <pages> 885-902, </pages> <month> Septem-ber/October </month> <year> 1992. </year>
Reference-contexts: O (T DC) where T is the total number of terms, D is the number of documents in which T occurs, and C is the number of co-occurring terms in D. 3.2.3 The Similarity Function The similarity computation is based on an asymmetric "Cluster Function" developed by Chen and Lynch <ref> [10] </ref>. The authors show that the asymmetric cluster function represents term association better than the popular cosine function.
Reference: [11] <author> Hsinchun Chen, Bruce Schatz, Tobun Ng, Joanne Martinez, Amy Kirchhoff, and Chienting Lin. </author> <title> A Parallel Computing Approach to Creating Engineering Concept Spaces for Semantic Retrieval: The Illinois Digital Library Initiative Project. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <year> 1996. </year>
Reference-contexts: ordered, and 1 + qualifies the aggregation operation to mean that one or more objects are being aggregated. 31 32 3.2.2 The Algorithm Algorithms to compute Concept Spaces have been under development for several years. cSpace is a parallel C ++ shared-memory implementation based in part on algorithms described in <ref> [53, 19, 11] </ref>. The computation proceeds in phases. The first phase is symbolic in nature and accounts for less than 5% of the sequential execution time.
Reference: [12] <author> S. C. Chen, D. J. Kuck, and A. H. Sameh. </author> <title> Practical Parallel Band Triangular System Solvers. </title> <journal> ACM Trans. on Mathematical Software, </journal> <volume> 4(3) </volume> <pages> 270-277, </pages> <month> Sept., </month> <year> 1978. </year>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [32, 12, 35, 42, 1, 26, 22, 44] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [12, 35]. <p> The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., [32, 12, 35, 42, 1, 26, 22, 44]). Early work included parallel recurrence solvers which were implemented in hardware <ref> [12, 35] </ref>. More recently, techniques based on powerful symbolic analysis have been employed in the recognition and solution of recurrences statically at compile-time as well as dynamically at run-time. 2.2.1 Run-time Solution of Recurrences The run-time solution of recurrences also has a fairly rich history. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 7 2.2.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [34, 12, 35, 42, 30, 1, 17, 47, 50, 44, 59] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication).
Reference: [13] <author> Ronald Gary Cytron. </author> <title> Compile-Time Scheduling and Optimization for Asynchronous Machines. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Sci., </institution> <month> Oct., </month> <year> 1984. </year> <month> 142 </month>
Reference-contexts: this recurrence involves expansion and loop distribution as fol lows: a (0) = 0 a (i) = a (i 1) + b (i) enddo doall i = 1; n : : : = : : : a (i) : : : enddo A second, run-time solution is the doacross loop <ref> [13] </ref>: post (1) doacross i = 1; n P: wait (i) Q: a = a + b (i) T: post (i + 1) enddo Access to the variable a in statement S creates a loop-carried anti-dependence which can be resolved by privatizing a as follows: 53 post (1) doacross i =
Reference: [14] <author> Luiz DeRose, Kyle Gallivan, Bret Marsolf, David Padua, and Stratis Gallopoulos. </author> <title> FALCON: A MATLAB Interactive Restructuring Compiler. </title> <booktitle> Proceedings of the 8th International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Columbus, OH, pages 18.1-18.18, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: * Calls to Single-threaded Library Routines * Calls to Single-threaded User Routines Linear Congruential Generators Lagged Fibonacci Generators 4.4.2.1 Single-threaded Library Routines We have determined that calls to RNG library routines occur in two computationally important loops in a test suite used in the evaluation of the FALCON MATLAB compiler <ref> [14] </ref>. In addition, random () is called in INITIA DO2000 in the Perfect Club benchmark MDG [5]. The CHOLESKY benchmark in our sparse & irregular suite also calls the rand () library routine in an important loop.
Reference: [15] <author> Iain Duff, Nick Gould, John Reid, Jennifer Scott, and Linda Miles. </author> <title> Harwell Subroutine Library. </title> <institution> Technical Report http://www.rl.ac.uk/departments/ccd/numerical/hsl/hsl.html, Council for the Central Laboratory of the Research Councils, Department for Computation and Information, Advanced Research Computing Division. </institution>
Reference-contexts: The Harwell-Boeing matrix BCSSTK30 was used as input for this benchmark <ref> [15] </ref>. 68 5.2.2 DSMC3D DSMC3D is a modification of the DSMC (Direct Simulation Monte Carlo) benchmark in 3 dimensions. DSMC implements a simulation of the behavior of particles of a gas in space using the Monte Carlo method [6]. <p> This algorithm is somewhat slower than the MA48 code from Harwell Subroutine Library <ref> [15] </ref>, a left-looking standard benchmark for factorization. The motivation for developing a right-looking algorithm derived from the lack of significant parallelism in MA48. This led to the inclusion of the original C version of SpLU in the suite of HPF-2 motivating applications.
Reference: [16] <author> Rudolf Eigenmann and Siamak Hassanzadeh. </author> <title> Evaluating High-Performance Computer Technology through Industrially Significant Applications. </title> <journal> IEEE Computational Science & Engineering, </journal> <month> Spring </month> <year> 1996. </year>
Reference-contexts: The SPEChpc benchmark suite has been established under the auspices of the Standard Performance Evaluation Corporation (SPEC) in order to accomplish this goal <ref> [16] </ref>. SPEChpc is defined by a joint effort of industrial members, high-performance computer vendors, and academic institutions. The primary goal is to determine a set of industrially significant applications that can be used to characterize the performance of high-performance computers across a wide range of machine organizations.
Reference: [17] <author> A. Fisher and A. Ghuloum. </author> <title> Parallelizing Complex Scans and Reductions. </title> <booktitle> Proceedings of the SIGPLAN'94 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: Fisher and Ghuloum model loop bodies containing reductions and recurrences as 6 composable functions in <ref> [17] </ref>. They determine whether, for a given loop, the composition of the function representing the loop body yields a function isomorphic to the original model. From this, a parallel prefix solution of the reduction/recurrence is generated. They are able to handle both scalar and array-based recurrences in singly-nested loops. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 7 2.2.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [34, 12, 35, 42, 30, 1, 17, 47, 50, 44, 59] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication). <p> Recognition techniques based on the underlying semantics of reduction operations have been implemented in the Velour vectorizing compiler [30]. Similar to the techniques implemented in <ref> [17] </ref>, these approaches identify variables which are computed as recurrent associative functions derived from statements in the body of the loop.
Reference: [18] <author> Ian Foster, Rob Schreiber, and Paul Havlak. </author> <title> HPF-2 Scope of Activities and Motivating Applications. </title> <type> Technical Report CRPC-TR94492, </type> <institution> Rice University, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: The suite consists of a collection of sparse and irregular application programs as well as several kernels representing key computational elements present in sparse codes. Several of the benchmarks in our suite are derived from the set of motivating applications for the HPF-2 effort <ref> [18] </ref>. Exceptions include the kernels MVPRODUCT and LANCZOS which were developed as part of this project.
Reference: [19] <author> William B. Frakes and Ricardo Baeza-Yates. </author> <title> Information Retrieval Data Structures & Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1992. </year>
Reference-contexts: ordered, and 1 + qualifies the aggregation operation to mean that one or more objects are being aggregated. 31 32 3.2.2 The Algorithm Algorithms to compute Concept Spaces have been under development for several years. cSpace is a parallel C ++ shared-memory implementation based in part on algorithms described in <ref> [53, 19, 11] </ref>. The computation proceeds in phases. The first phase is symbolic in nature and accounts for less than 5% of the sequential execution time.
Reference: [20] <author> Dr. Sam Fuller. </author> <note> Seminar Presented at UIUC, </note> <month> October </month> <year> 1996. </year>
Reference-contexts: Wirt, automatic parallelization will play a crucial role in compilers for both existing and upcoming Intel-based multiprocessors. Echoed by Dr. Sam Fuller, Chief Scientist at Digital Equipment Corporation, this fact underscores a trend in multi-process computing which is expected to continue into the 21st century <ref> [20] </ref>. In recent years, research in the area of automatic parallelization has focused on numeric programs written in Fortran. However, new application areas which employ object-oriented programming models are evolving, and general methods which are applicable across language and application boundaries must be developed.
Reference: [21] <author> D. D. Gajski, D. J. Kuck, and D. A. Padua. </author> <title> Dependence Driven Computation. </title> <booktitle> Proceedings of the COMPCON 81 Spring Computer Conf., </booktitle> <pages> pages 168-172, </pages> <month> Feb., </month> <year> 1981. </year>
Reference: [22] <author> Michael P. Gerlek, Eric Stoltz, and Michael Wolfe. </author> <title> Beyond Induction Variables: Detecting and Classifying Sequences Using a Demand-driven SSA Form. </title> <note> To appear in TOPLAS. </note>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [32, 12, 35, 42, 1, 26, 22, 44] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [12, 35]. <p> Haghighat and Polychronopoulos symbolically execute loops and use finite difference methods in conjunction with interpolation to determine closed-forms for recurrences involving scalar variables [26]. Their approach is capable of multiple scalar transformations including induction variable substitution. Other compile-time solutions include <ref> [22] </ref> in which Wolfe et al derive relations between variables by matching against cycles in the SSA graph, and then use matrix inversion (among other methods) to determine closed-forms for induction variables. We also treat the solution of generalized induction variables at compile-time in [44].
Reference: [23] <author> Milind Baburao Girkar. </author> <title> Functional Parallelism Theoretical Foundations and Implementation. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> December </month> <year> 1991. </year>
Reference: [24] <author> G.H. Golub and C.F. van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1993. </year>
Reference-contexts: As a result, indirection occurs on the right-hand-side of the computed expressions. do nc=nintci,nintcf direc2 (nc)=bp (nc)*direc1 (nc) * -bs (nc)*direc1 (lcc (nc,1)) * -bw (nc)*direc1 (lcc (nc,4)) * -bl (nc)*direc1 (lcc (nc,5)) enddo 5.2.5 LANCZOS The lanczos algorithm with full reorthogonalization determines the eigenvalues of a symmetric matrix <ref> [24] </ref>. LANCZOS is an implementation of the lanczos algorithm for sparse matrices. The key computational elements are the calculation of a sparse matrix-vector product and the reorthogonalization of a dense work matrix. <p> excerpt demonstrates: do j=1,a_nr r (j)=r (j)+ad (k)*q (ac (k),i) enddo enddo The matrix 1138 BUS of Harwell-Boeing collection was used as input for this benchmark. 5.2.6 MVPRODUCT MVPRODUCT is a set of basic sparse matrix operations including sparse matrix-vector multiplication and the product and sum of two sparse matrices <ref> [4, 24] </ref>. The representation of the sparse matrices employs two different schemes: compressed row storage (CRS) and compressed column storage (CCS) [52].
Reference: [25] <author> Mark D. Guzzi, David A. Padua, Jay P. Hoeflinger, and Duncan H. Lawrie. </author> <title> Cedar Fortran and Other Vector and Parallel Fortran Dialects. </title> <journal> Journal of Supercomputing, </journal> <volume> 4(1) </volume> <pages> 37-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: This doevery section of the parallel loop is executed as a dependence-free doall loop <ref> [25] </ref>. Thus the associative transformation enables the execution of the bulk of the original serial loop consisting of an associative coalescing loop operator as a fully parallel doall loop. <p> as will be seen, the concepts presented in Chapters 2 and 3 carry over into this realm as well. 47 CHAPTER 4 Techniques for Solving Recurrences 4.1 Introduction Extensive analysis of applications from a number of benchmark suites has revealed the presence of many loops in which recurrences prevent doall <ref> [25] </ref> parallelization. During a recent review of the Polaris restructurer [8] approximately 500 loops from programs in the aforementioned suites were identified as serial. <p> Although the pipelining technique has been known for some time, no empirical study has decisively demonstrated the effectiveness of this technique on actual application codes. 4.3.6 Multi-level Parallel Execution Parallelism within loops can be loosely characterized as either doall <ref> [25] </ref>, doacross [25], or functional in nature. Loop pipelining, as discussed in the previous section, is an example of functional parallelism. Functional parallelism of various sorts has been a topic of study for some time ([21, 64, 23, 41]). <p> Although the pipelining technique has been known for some time, no empirical study has decisively demonstrated the effectiveness of this technique on actual application codes. 4.3.6 Multi-level Parallel Execution Parallelism within loops can be loosely characterized as either doall <ref> [25] </ref>, doacross [25], or functional in nature. Loop pipelining, as discussed in the previous section, is an example of functional parallelism. Functional parallelism of various sorts has been a topic of study for some time ([21, 64, 23, 41]).
Reference: [26] <author> Mohammad R. Haghighat and Constantine D. Polychronopoulos. </author> <title> Symbolic Program Analysis and Optimization for Parallelizing Compilers. </title> <booktitle> Presented at the 5th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August 3-5, </month> <year> 1992. </year>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [32, 12, 35, 42, 1, 26, 22, 44] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [12, 35]. <p> They are able to handle both induction variables and array-based recurrences in singly-nested loops. Haghighat and Polychronopoulos symbolically execute loops and use finite difference methods in conjunction with interpolation to determine closed-forms for recurrences involving scalar variables <ref> [26] </ref>. Their approach is capable of multiple scalar transformations including induction variable substitution. Other compile-time solutions include [22] in which Wolfe et al derive relations between variables by matching against cycles in the SSA graph, and then use matrix inversion (among other methods) to determine closed-forms for induction variables.
Reference: [27] <author> Luddy Harrison. </author> <title> Compiling Lisp for Evaluation on a Tightly Coupled Multiprocessor. </title> <type> Technical Report 565, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> Mar. 20, </month> <year> 1986. </year>
Reference-contexts: Similar to the techniques implemented in [17], these approaches identify variables which are computed as recurrent associative functions derived from statements in the body of the loop. Harrison also treats the parallelization of associative inductions, reductions, and recurrences in functional languages in <ref> [27] </ref>. 2.2.4 Commutative Operations In [51], Rinard and Diniz present a framework for parallelizing recursive function calls based on com-mutativity. <p> We address for the first time a case in which operands are array elements with loop-carried flow dependences between individual elements. 20 It is well known that recurrences are parallelizable when based on associative binary operators <ref> [27] </ref>. By treating the array used to contain the result as a conglomerate operand, associative recurrence relations can be modeled as associative coalescing loop operators.
Reference: [28] <author> John E. Hopcroft and Jeffrey D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1979. </year>
Reference: [29] <author> IBM. </author> <title> Parallel FORTRAN Language and Library Reference, </title> <month> March </month> <year> 1988. </year>
Reference-contexts: To understand these four steps, let's consider the following example: do i = 1; n enddo The following is the parallelized version of this example. The language used in this code is based on IBM's Parallel Fortran <ref> [29] </ref> with extensions which we have added to adapt the language to the special needs of the associative transformation. 13 parallel loop, block i = 1; n private sum p dofirst sum p = 0 doevery sum p = sum p + a (i) enddo dofinal, ordered lock sum = sum <p> The parallelizing transformation takes one of three forms: critical section, privatized, or expanded. Each approach is discussed and exemplified below. As before, the language used in the examples is based on IBM's Parallel Fortran <ref> [29] </ref>. * Critical Section 57 The first approach involves the insertion of synchronization primitives around each reduction state-ment, making the sum operation atomic.
Reference: [30] <author> P. Jouvelot and B. Dehbonei. </author> <title> A Unified Semantic Approach for the Vectorization and Parallelization of Generalized Reductions. </title> <booktitle> In Proceedings of the 1989 International Conference on Supercomputing, </booktitle> <address> Crete, Greece, </address> <month> June 5-9, </month> <year> 1989. </year> <note> ACM. </note>
Reference-contexts: In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 7 2.2.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [34, 12, 35, 42, 30, 1, 17, 47, 50, 44, 59] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication). <p> Recognition techniques based on the underlying semantics of reduction operations have been implemented in the Velour vectorizing compiler <ref> [30] </ref>. Similar to the techniques implemented in [17], these approaches identify variables which are computed as recurrent associative functions derived from statements in the body of the loop.
Reference: [31] <author> Jee Myeong Ku. </author> <title> The Design of an Efficient and Portable Interface Between a Parallelizing Compiler and its Target Machine. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <year> 1995. </year>
Reference-contexts: This pattern occurs commonly in many codes, both sparse and non-sparse, and is termed a histogram reduction <ref> [44, 31] </ref>. The parallelizing transformation takes one of three forms: critical section, privatized, or expanded. Each approach is discussed and exemplified below. <p> The Polaris parallelizing restructurer recognizes histogram reductions based on the techniques discussed in [44]. A more detailed study which compares and contrasts the performance of these three transformations is contained in <ref> [31] </ref>. 59 4.4.2 Random Number Generator Substitution One approach to breaking dependences caused by calls to pseudo-random number generators is to substitute thread-parallel generators which produce a robust stream of random numbers.
Reference: [32] <author> D. Kuck, P. Budnik, S-C. Chen, Jr. E. Davis, J. Han, P. Kraska, D. Lawrie, Y. Muraoka, R. Strebendt, and R. Towle. </author> <title> Measurements of Parallelism in Ordinary FORTRAN Programs. </title> <journal> Computer, </journal> <volume> 7(1) </volume> <pages> 37-46, </pages> <month> Jan., </month> <year> 1974. </year>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [32, 12, 35, 42, 1, 26, 22, 44] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [12, 35].
Reference: [33] <author> D. J. Kuck. </author> <title> The Structure of Computers and Computations,, volume I. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> NY, </address> <year> 1978. </year>
Reference-contexts: Similarly, Kuck has shown that simple expressions (e.g., right-hand-sides of assignment statements) can be reordered based on combinations of both associativity and commutativity in tree-height reduction <ref> [33] </ref>. 2.3 Associativity in Coalescing Loop Operators In this section we introduce the concept of a coalescing loop operator. <p> Simple operators such as + and fl are both commutative and associative and expressions involving such operators can often be parallelized based either on associativity, commutativity, or some combination thereof <ref> [33] </ref>. In the preceding sections, we've seen several examples of coalescing loop operators which were non-commutative. As a result, the question 26 remains open as to what role commutativity plays in parallelizing coalescing loop operators.
Reference: [34] <author> David Kuck and Yoichi Muraoka. </author> <title> Bounds on the Parallel Evaluation of Arithmetic Expressions Using Associativity and Commutativity. </title> <journal> Acta Informatica, </journal> <volume> 3, Fasc. 3 </volume> <pages> 203-216, </pages> <year> 1974. </year>
Reference-contexts: In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 7 2.2.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [34, 12, 35, 42, 30, 1, 17, 47, 50, 44, 59] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication).
Reference: [35] <author> David J. Kuck and Richard A. </author> <title> Stokes. The Burroughs Scientific Processor (BSP). </title> <journal> Special Issue on Supersystems, IEEE Trans. on Computers, </journal> <volume> C-31(5):363-376, </volume> <month> May, </month> <year> 1982. </year>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [32, 12, 35, 42, 1, 26, 22, 44] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [12, 35]. <p> The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., [32, 12, 35, 42, 1, 26, 22, 44]). Early work included parallel recurrence solvers which were implemented in hardware <ref> [12, 35] </ref>. More recently, techniques based on powerful symbolic analysis have been employed in the recognition and solution of recurrences statically at compile-time as well as dynamically at run-time. 2.2.1 Run-time Solution of Recurrences The run-time solution of recurrences also has a fairly rich history. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 7 2.2.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [34, 12, 35, 42, 30, 1, 17, 47, 50, 44, 59] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication).
Reference: [36] <author> D.H. Lehmer. </author> <title> Mathematical Methods in Large-scale Computing Units. </title> <booktitle> In 2nd Symposium on Large-Scale Digital Calculating Machinery, </booktitle> <pages> pages 141-146, </pages> <address> Cambridge, MA, 1951. </address> <publisher> Harvard University Press. </publisher>
Reference-contexts: The Perfect Club benchmark QCD, for example, contains the routines PRANF, LADD, and LMULT which together implement a linear congruential pseudo-random number generator <ref> [36] </ref>. Similarly, the DSMC3D benchmark in our sparse & irregular suite implements a linear congruential generator based on work described in [37]. A third example occurs in the SPEC CFP95 benchmark su2cor which implements a lagged Fibonacci generator as described previously in this section.
Reference: [37] <author> P.A.W. Lewis, A.S. Goodman, and J.M. Miller. </author> <title> A Pseudo-Random Number Generator for the System/360. </title> <journal> IBM Systems Journal, </journal> <volume> 8(2) </volume> <pages> 136-146, </pages> <month> May </month> <year> 1969. </year>
Reference-contexts: The Perfect Club benchmark QCD, for example, contains the routines PRANF, LADD, and LMULT which together implement a linear congruential pseudo-random number generator [36]. Similarly, the DSMC3D benchmark in our sparse & irregular suite implements a linear congruential generator based on work described in <ref> [37] </ref>. A third example occurs in the SPEC CFP95 benchmark su2cor which implements a lagged Fibonacci generator as described previously in this section. Lagged Fibonacci generators such as that implemented in su2cor take the form of a homogeneous linear recurrence.
Reference: [38] <author> G. Lueker. </author> <title> Some Techniques for Solving Recurrences. </title> <journal> Computing Surveys, </journal> <volume> Vol. 12, No. 4, </volume> <month> December </month> <year> 1980. </year>
Reference-contexts: Consider the following example: a (0) = 0 a (i) = a (i 1) + 1 enddo Here we have a recurrence involving the array a. Techniques for solving linear non-homogeneous recurrences of this nature are well known <ref> [38] </ref>. The closed-form for this recurrence is a (i) = i, resulting in the following parallel form: a (0) = 0 doall i = 1; n a (i) = i enddo This transformation has been found useful in the NCSA Grand Challenge code cmhog. <p> Lagged Fibonacci generators such as that implemented in su2cor take the form of a homogeneous linear recurrence. Such relations can be automatically detected using pattern recognition techniques [1]. General techniques for solving linear recurrences of this type are well known <ref> [38] </ref>, and closed-forms for such recurrences can be computed at compile-time as discussed in section 4.3.1 above.
Reference: [39] <author> Michael Mascagni and David Bailey. </author> <title> Requirements for a Parallel Pseudorandom Number Generator. </title> <note> Technical Report http://olympic.jpl.nasa.gov/SSTWG/lolevel.msgs.html, Center for Computing Sciences, </note> <institution> I.D.A. and NASA Ames Research Center, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Recent work by Bailey and Mascagni involves the development and standardization of thread-parallel pseudo-random number generators based on lagged Fibonacci series <ref> [48, 39] </ref>. As discussed in Chapter 2, the substitution of robust thread-parallel pseudo-random number generators can be considered to enable the parallel execution of loops based on the associativity of the operation.
Reference: [40] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol 9, No 1, </volume> <pages> pages 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The synchronization schemes employ busy-waiting. At a low level, all synchronization variables are mapped on cacheline boundaries. This provides clean busy-waiting for variables which are shared across processors (i.e., no false sharing of synchronization variables). As an optimization, exponential backoff is employed while processors spin-wait at synchronization points <ref> [40] </ref>. 2 Recall that updates to u are conditional 3 If, while p 2 was waiting for z, z was updated twice (i.e., z was updated again by another processor p 3 before p 2 could read it), p 2 reads uprev panel z 89 6.2.7 Multi-level Parallelism Section 6.2.5 depicted
Reference: [41] <author> Jose Eduardo Moreira. </author> <title> On the Implementation and Effectiveness of Autoscheduling for Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> February </month> <year> 1995. </year>
Reference: [42] <author> D. Padua and M. Wolfe. </author> <title> Advanced Compiler Optimization for Supercomputers. </title> <journal> CACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December, </month> <year> 1986. </year>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [32, 12, 35, 42, 1, 26, 22, 44] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [12, 35]. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 7 2.2.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [34, 12, 35, 42, 30, 1, 17, 47, 50, 44, 59] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication). <p> Due to the nature of the su2cor application, access to panels of the main data structure u involve potential loop-carried flow, anti, and output dependences. Flow dependences were resolved using explicit synchronization. Anti and output dependences were resolved using variable renaming <ref> [42] </ref>. The renamed variables in the transformed code depicted in section 6.2.5 are uprev and uprevprev, shorthand for the values of u in the preceding two timesteps. Here, a timestep is defined as one iteration of the original loop SU2COR do60.
Reference: [43] <author> D. A. Padua, D. J. Kuck, and D. H. Lawrie. </author> <title> High-Speed Multiprocessors and Compilation Techniques. </title> <journal> Special Issue on Parallel Processing, IEEE Trans. on Computers, </journal> <volume> C-29(9):763-776, </volume> <month> Sept., </month> <year> 1980. </year>
Reference-contexts: This same technique can be applied in the SPEC CFP95 benchmark turb3d. 4.3.5 Do-pipe Parallelization In the following example, the assignment a (0) = a (n) prevents doall parallelization of the outer i loop. However, a speedup can be achieved by pipelining the outer i loop <ref> [43] </ref>.
Reference: [44] <author> Bill Pottenger and Rudolf Eigenmann. </author> <title> Idiom Recognition in the Polaris Parallelizing Compiler. </title> <booktitle> Proceedings of the 9th ACM International Conference on Supercomputing, Barcelona, Spain, </booktitle> <pages> pages 444-448, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [32, 12, 35, 42, 1, 26, 22, 44] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [12, 35]. <p> From this, a parallel prefix solution of the reduction/recurrence is generated. They are able to handle both scalar and array-based recurrences in singly-nested loops. In our previous work <ref> [44] </ref> we take a general pattern-matching approach to the recognition and transformation of reductions. This approach has the advantage that complex conditional structures occurring in multiply-nested loops pose no particular problem. <p> Other compile-time solutions include [22] in which Wolfe et al derive relations between variables by matching against cycles in the SSA graph, and then use matrix inversion (among other methods) to determine closed-forms for induction variables. We also treat the solution of generalized induction variables at compile-time in <ref> [44] </ref>. A general pattern-matching approach is employed in the recognition phase of our induction solution algorithm. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 7 2.2.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [34, 12, 35, 42, 30, 1, 17, 47, 50, 44, 59] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication). <p> In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication). For example, in <ref> [44] </ref>, recurrence relations are solved using a run-time technique that is based on the associativity of the underlying operator within either a single reduction statement or a group of reduction statements which access the same reduction variable. <p> The original statement m = m + 1 has been deadcoded and all remaining uses of m have been substituted by m p . Finally, the last value of m is assigned at the loop exit. In order to simplify the presentation, the example assumes no zero-trip loops <ref> [44] </ref>. <p> These techniques have been implemented in a recognizer in the Polaris restructurer. However, these techniques can be readily extended to include the recognition of calls to intrinsic min and max functions. Once such intrinsics have been recognized, they can be automatically parallelized based on techniques described in <ref> [44] </ref>. In section 6.3 of Chapter 6 we discuss the parallelization of an intrinsic max reduction of this nature. 52 4.3.3 Semi-private Transformation In the following example loop-carried dependences on the scalar a prevent doall parallelization, and the loop-variant expression b (i) precludes a closed-form solution of the recurrence. <p> In addition, the use of a in the loop outside the reduction statement prevents use of a parallelizing reduction transformation such as described in <ref> [44] </ref>. do i = 1; n : : : = : : : a : : : enddo The traditional approach to solving this recurrence involves expansion and loop distribution as fol lows: a (0) = 0 a (i) = a (i 1) + b (i) enddo doall i = 1; <p> This pattern occurs commonly in many codes, both sparse and non-sparse, and is termed a histogram reduction <ref> [44, 31] </ref>. The parallelizing transformation takes one of three forms: critical section, privatized, or expanded. Each approach is discussed and exemplified below. <p> The parallelization of histogram reductions is based on a run-time technique which depends on the associativity of the operation being performed. The Polaris parallelizing restructurer recognizes histogram reductions based on the techniques discussed in <ref> [44] </ref>. A more detailed study which compares and contrasts the performance of these three transformations is contained in [31]. 59 4.4.2 Random Number Generator Substitution One approach to breaking dependences caused by calls to pseudo-random number generators is to substitute thread-parallel generators which produce a robust stream of random numbers. <p> This has the effect of removing anti-dependences via variable renaming. The remaining case in 1 This is the exact zero-trip test discussed in <ref> [44] </ref> 63 which m iv + (k (j + 1) k (j)) &gt; k (j + 1) and m iv k (j + 1) contains true loop-carried flow dependences and cannot be executed in a doall fashion. <p> There are two other loops which contain conditionally incremented induction variables, ENTER3 DO4 and INIT3 DO605. Together these loops account for approximately 7.5% of the sequential execution time. Both of the latter loops are parallelizable using techniques outlined in <ref> [44] </ref> for determining the closed-form of induction variables if the induction can be proven to be monotonically increasing. However, the conditional increment poses a problem in that monotonicity may not hold and the induction variable ranges may overlap as a result. <p> Nonetheless, the parallelization of such reductions can be accomplished based on the transformation described in <ref> [44] </ref>. An example of such a reduction occurs in loop-nest MAIN do80 in tomcatv.
Reference: [45] <author> Bill Pottenger and Bruce Schatz. cSpace: </author> <title> A Parallel C ++ Information Retrieval Benchmark. </title> <type> Technical Report 1511, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: We perform a case study of the semantic retrieval application cSpace <ref> [45] </ref> in order to evaluate the effectiveness of this model. The performance of cSpace is characterized on a modern shared-memory multi-processor in order to demonstrate the the applicability of this approach.
Reference: [46] <author> Bill Pottenger and Bruce Schatz. </author> <title> On the Evaluation of C ++ in a Parallel Programming Environment. </title> <type> Technical Report 1506, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: All experiments were conducted on a dedicated machine in single-user mode. Several interesting trends are revealed in Table 3.1. First, several runs resulted in super-linear speedups. This is an indirect result of the poor performance of multi-threaded dynamic memory allocation in C ++ on the SGI Power Challenge <ref> [46] </ref>. The parallel version of cSpace used in these experiments employs a customized memory manager which alleviates much of the overhead associated with multi-threaded dynamic memory allocation. However, this also provides an unexpected benefit in that the overhead of numerous calls to malloc (i.e., operator new) is entirely eliminated.
Reference: [47] <author> William Morton Pottenger. </author> <title> Induction Variable Substitution and Reduction Recognition in the Polaris Parallelizing Compiler. </title> <type> Master's thesis, </type> <institution> Univ of Illinois at Urbana-Champaign, Cntr for Supercomputing Res & Dev, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 7 2.2.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [34, 12, 35, 42, 30, 1, 17, 47, 50, 44, 59] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication). <p> Approximately 70% of the loops in this 35% subset were determined to be partially or fully parallelizable based on a manual inspection of the codes. This chapter extends the solution techniques discussed in <ref> [47] </ref> to include additional compile-time and run-time techniques for solving recurrences in parallel. <p> Each technique in the following list will be briefly discussed and exemplified. * Symbolic Computation of Closed-Forms 49 * Intrinsic Minimum & Maximum Reductions * Semi-private Transformation * Wrap-around Privatization * Do-pipe Parallelization * Multi-level Parallel Execution 4.3.1 Symbolic Computation of Closed-Forms <ref> [47] </ref> discusses a variety of techniques employed in the solution of reductions and inductions. These techniques include, for example, the use of computer algebra in the determination of closed-forms for scalar induction variables. Techniques of this nature can be extended to the solution of linear recurrences. <p> a proper guard on the last value assignment of m. 4.3.2 Intrinsic Minimum & Maximum Reductions The recognition of reductions of the general form: A (ff 1 ; ff 2 ; : : :) = A (ff 1 ; ff 2 ; : : :) + fi is discussed in <ref> [47] </ref>. Here fi represents an arbitrary expression and A may be a multi-dimensional array with subscript vector fff 1 ; ff 2 ; : : :g which may contain both loop-variant and invariant terms.
Reference: [48] <author> Daniel V. Pryor, Steven A. Cuccaro, Michael Mascagni, and M. L. Robinson. </author> <title> Implementation of a Portable and Reproducible Parallel Pseudorandom Number Generator. </title> <booktitle> Proceedings of Supercomputing '94, </booktitle> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Recent work by Bailey and Mascagni involves the development and standardization of thread-parallel pseudo-random number generators based on lagged Fibonacci series <ref> [48, 39] </ref>. As discussed in Chapter 2, the substitution of robust thread-parallel pseudo-random number generators can be considered to enable the parallel execution of loops based on the associativity of the operation. <p> This section of code has an explicit recurrence relation of the form a i = a i103 + a i250 mod 2 31 1. It is an implementation of a lagged-Fibonacci pseudorandom number generator with recursion parameters of (103, 250) <ref> [48] </ref>. Note that the conditional subtraction performs a modulo operation which results in an integer in the range [0; 2 31 2]. In theoretical terms, this is the representative residue class of the ring which contains equivalence classes of the integers modulo 2 31 1.
Reference: [49] <author> Lawrence Rauchwerger. </author> <title> Run-Time Parallelization: A Framework for Parallel Computation. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: If the loop operator is associative, the loop can be transformed using L t . In the latter case, transformation L t may still be applied with a slight modification. Following <ref> [49] </ref>, the doevery loop of L t can be strip-mined and executed for a pre-determined number of iterations after which the (accumulated) exit conditions for each slice can be checked. This can be repeated until the loop exits.
Reference: [50] <author> Lawrence Rauchwerger and David Padua. </author> <title> The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatization and Reduction Parallelization. </title> <booktitle> Proceedings of the SIGPLAN'95 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year> <month> 144 </month>
Reference-contexts: Suganuma, Komatsu, and Nakatani, for example, recognize and transform scalar reductions based on the detection of reduction semantics in the data dependence graph [59]. Rauchwerger and Padua test for the presence of privatizable arrays and reduction operations involving arrays in <ref> [50] </ref>. By treating individual array elements as scalars and recording at run-time whether a use of a given array element occurs outside the expanded reduction statement (or statements) involving the reduction variable, reduction operations can recognized and executed in parallel. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 7 2.2.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [34, 12, 35, 42, 30, 1, 17, 47, 50, 44, 59] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication).
Reference: [51] <author> Martin C. Rinard and Pedro C. Diniz. </author> <title> Commutativity Analysis: A New Analysis Framework for Parallelizing Compilers. </title> <booktitle> In Programming Language Implementation and Design (PLDI), </booktitle> <pages> pages 54-67. </pages> <publisher> ACM, </publisher> <year> 1996. </year>
Reference-contexts: Similar to the techniques implemented in [17], these approaches identify variables which are computed as recurrent associative functions derived from statements in the body of the loop. Harrison also treats the parallelization of associative inductions, reductions, and recurrences in functional languages in [27]. 2.2.4 Commutative Operations In <ref> [51] </ref>, Rinard and Diniz present a framework for parallelizing recursive function calls based on com-mutativity.
Reference: [52] <author> L.F. Romero and E.L. Zapata. </author> <title> Data Distributions for Sparse Matrix Vector Multiplication. </title> <journal> J. Parallel Computing, </journal> <volume> 21(4) </volume> <pages> 583-605, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: The representation of the sparse matrices employs two different schemes: compressed row storage (CRS) and compressed column storage (CCS) <ref> [52] </ref>. The access pattern is demonstrated by the following code abstract: 70 do i=1,a_nr do ja=ar (i),ar (i+1)-1 if (ac (ja).eq.br (jb)) THEN c (i,k)=c (i,k) endif enddo enddo enddo enddo Here indirection occurs on the right-hand-side of the computed expressions.
Reference: [53] <author> Dmitri Roussinov. </author> <title> Personal communication with author, </title> <year> 1996. </year>
Reference-contexts: ordered, and 1 + qualifies the aggregation operation to mean that one or more objects are being aggregated. 31 32 3.2.2 The Algorithm Algorithms to compute Concept Spaces have been under development for several years. cSpace is a parallel C ++ shared-memory implementation based in part on algorithms described in <ref> [53, 19, 11] </ref>. The computation proceeds in phases. The first phase is symbolic in nature and accounts for less than 5% of the sequential execution time.
Reference: [54] <author> James Rumbaugh, Michael Blaha, William Premerlani, Frederick Eddy, and William Lorensen. </author> <title> Object-Oriented Modeling and Design. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1991. </year>
Reference: [55] <author> B. Schatz, E. Johnson, P. Cochrane, and H. Chen. </author> <title> Interactive Term Suggestion for Users of Digital Libraries: Using Subject Thesauri and Co-occurrence Lists for Information Retrieval. </title> <booktitle> In 1st International ACM Conference on Digital Libraries, </booktitle> <pages> pages 126-133, </pages> <address> Bethesda, MD, 1996. </address> <publisher> ACM. </publisher>
Reference-contexts: The resulting map between terms is designated a Concept Space and is useful in the refinement of queries presented to the collection. Concept Spaces are used, for example, in interactive query sessions as part of the DLI testbed at the University of Illinois, Urbana-Champaign <ref> [55] </ref>. Algorithms to perform iterative search refinement which incorporate the computation of Concept Spaces are also under development as part of the Digital Library Research Program (DLRP) at Illinois. 3.2.1 The Object Model classes exist in the system: Term, Document, Cooccurrence, and ConceptSpace. The 3 symbol represents the aggregation operation.
Reference: [56] <author> Bruce Schatz. </author> <title> Interactive Retrieval in Information Spaces Distributed across a Wide-Area Network. </title> <type> PhD thesis, </type> <institution> University of Arizona Computer Science Department, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: The total size of this data set is 8,945 terms and the source file containing the documents is 639,483 bytes. This input was chosen specifically because this is the number of documents which is considered manageable in a response to a query to a bibliographic database <ref> [56] </ref>. The output size for this input data set is 2,665,339 bytes. 36 The second data set consists of 1505 documents and 53,776 terms and is dubbed the "Medium" data set. This data set is representative of a small personal repository [57].
Reference: [57] <author> Bruce Schatz and Hsinchun Chen. </author> <title> Building Large-Scale Digital Libraries. </title> <booktitle> IEEE Computer, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Developed as part of the federal NII Digital Library Initiative (DLI) at the University of Illinois <ref> [57] </ref>, cSpace is a hybrid symbolic/numeric application which determines relationships between terms in a collection of documents. The resulting map between terms is designated a Concept Space and is useful in the refinement of queries presented to the collection. <p> The output size for this input data set is 2,665,339 bytes. 36 The second data set consists of 1505 documents and 53,776 terms and is dubbed the "Medium" data set. This data set is representative of a small personal repository <ref> [57] </ref>. The document source file is 3,756,384 bytes in size and the output produced is 15,590,146 in size. The third data set consists of 8,787 documents and 710,431 terms. This set is dubbed the "Large" data set, and is representative of a collection of personal repositories.
Reference: [58] <institution> NRC Computer Science and Telecommunications Board. Computing The Future. National Academy Press, </institution> <address> Washington, D.C., </address> <year> 1992. </year>
Reference-contexts: As such, it represents a significant step towards the realization of the goals outlined in the National Research Council report Computing The Future <ref> [58] </ref>. In the following chapters we will turn our attention to the parallelization of scientific Fortran and hybrid Fortran/C codes.
Reference: [59] <author> T. Suganuma, H. Komatsu, and T. Nakatani. </author> <title> Detection and Global Optimization of Reduction Operations. </title> <booktitle> Proceedings of ICS'96, </booktitle> <address> Philadelphia, PA, USA, </address> <month> July </month> <year> 1996. </year>
Reference-contexts: Suganuma, Komatsu, and Nakatani, for example, recognize and transform scalar reductions based on the detection of reduction semantics in the data dependence graph <ref> [59] </ref>. Rauchwerger and Padua test for the presence of privatizable arrays and reduction operations involving arrays in [50]. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 7 2.2.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [34, 12, 35, 42, 30, 1, 17, 47, 50, 44, 59] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication).
Reference: [60] <author> Peiyi Tang, Pen-Chung Yew, and Chuan-Qi Zhu. </author> <title> Compiler Techniques for Data Synchronization in Nested Parallel Loops. </title> <booktitle> Proceedings of ICS'90, </booktitle> <address> Amsterdam, </address> <publisher> Holland, </publisher> <pages> 1 177-186, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In the past this problem has been addressed using timestamps to identify each write. Writes by processors executing iterations later than the current timestamp are permitted. Processors executing iterations earlier than the current timestamp are not permitted to update the shared variable <ref> [60] </ref>. This solution incurs additional overhead in terms of both space to maintain timestamps and computational time to achieve synchronized access to shared variables.
Reference: [61] <author> Peng Tu. </author> <title> Automatic Array Privatization and Demand-Driven Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: operator +=. 12 The following four steps are needed in order to transform a loop based on associativity alone: * Privatization of shared variables * Initialization of private variables * Block loop scheduling * Cross-processor reduction Privatization refers to the creation of thread or process-private copies of shared global variables <ref> [61] </ref>. The second step involves the initialization of the newly created private variables. In the third step, the iteration space of the loop is broken into contiguous slices, and each processor executes a slice of the original iteration space.
Reference: [62] <author> Peng Tu and David Padua. </author> <title> Automatic Array Privatization. </title> <editor> In Utpal Banerjee, David Gelernter, Alex Nicolau, and David Padua, editors, </editor> <booktitle> Proc. Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR. </address> <booktitle> Lecture Notes in Computer Science., </booktitle> <volume> volume 768, </volume> <pages> pages 500-521, </pages> <month> August 12-14, </month> <year> 1993. </year>
Reference-contexts: combined with the run-time estimation of the range of an induction variable is discussed in Chapter 5, section 5.3.8. 4.4.5 Copy-in and Copy-out It is often necessary to break loop-carried anti and output dependences by privatizing both scalar and array variables which are defined and used within a single iteration <ref> [62] </ref>. However, many such variables have initial values which must be copied into each processor's local copy of the variable prior to the start of parallel execution. This process is known as "copy-in".
Reference: [63] <author> Peng Tu and David Padua. </author> <title> Gated SSA-Based Demand-Driven Symbolic Analysis for Parallelizing Compilers. </title> <booktitle> Proceedings of the 9th ACM International Conference on Supercomputing, Barcelona, Spain, </booktitle> <pages> pages 414-423, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: The following is a high-level view of one form an algorithm for determining associativity in loops could take. It is based on the Gated-SSA representation of a program <ref> [63] </ref>: Given a loop L Transform L to GSA form For each function variable v at the header of L back substitute (variable e from loop-carried edge) Topologically sort functions into directed acyclic graph G based on non-loop carried flow-dependences 28 // This DAG is the operator representing the loop body
Reference: [64] <author> Alexander Veidenbaum. </author> <title> Compiler Optimizations and Architecture Design Issues for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Comput. Sci., </institution> <month> May </month> <year> 1985. </year>
Reference-contexts: Table 6.6 summarizes these results. The second transformation involved pipelining the outermost time-stepping loop into two stages. In conjunction with the partial privatization of two arrays, the granularity of synchronization was decreased in order to enable the execution of the two stages to be overlapped <ref> [64] </ref>. Performance results for do-pipe vs. privatized reductions also appear in Table 6.6. The speedup for the do-pipe technique exceeds that achieved for the parallel reductions.
Reference: [65] <author> Guhan Viswanathan and James R. Larus. </author> <title> User-defined Reductions for Efficient Communication in Data-Parallel Languages. </title> <type> Technical Report 1293, </type> <institution> Univ. of Wisconsin-Madison, Computer Sciences Department, </institution> <month> Aug. </month> <year> 1996. </year>
Reference: [66] <author> Dick Wilmoth. </author> <title> Personal communication with author, </title> <year> 1996. </year>
Reference-contexts: However, when a molecule leaves the flow, it is deleted from the list and replaced by the last molecule in the list. This creates loop-carried dependences in the loop. However, the deletion of molecules can be deferred until after the entire list has been processed <ref> [66, 7] </ref>.
Reference: [67] <author> Dr. Richard Wirt. </author> <note> Seminar Presented at UIUC, </note> <month> September </month> <year> 1996. </year>
Reference-contexts: As the discussion turned to the automatic parallelization of computer programs, Dr. Wirt emphasized the following point: The compiler IS the architecture <ref> [67] </ref> According to Dr. Wirt, automatic parallelization will play a crucial role in compilers for both existing and upcoming Intel-based multiprocessors. Echoed by Dr. Sam Fuller, Chief Scientist at Digital Equipment Corporation, this fact underscores a trend in multi-process computing which is expected to continue into the 21st century [20].

References-found: 67

