URL: ftp://ftp.cse.unsw.edu.au/pub/doc/papers/UNSW/9806.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/research/tr.html
Root-URL: http://www.cse.unsw.edu.au
Email: waleed@cse.unsw.edu.au  
Title: A General Architecture for Supervised Classification of Multivariate Time Series  
Author: Mohammed Waleed Kadous 
Date: September 1998  
Address: Sydney NSW 2052, Australia  
Affiliation: Department of Artificial Intelligence School of Computer Science Engineering University of New South Wales  
Pubnum: UNSW-CSE-TR-9806  
Abstract-found: 0
Intro-found: 1
Reference: [Ben96] <author> Yoshua Bengio. </author> <title> Neural Networks for Speech and Sequence Recognition. </title> <publisher> International Thomson Publishing Inc., </publisher> <year> 1996. </year>
Reference-contexts: As of yet, well-formed techniques for deciding appropriate values for these parameters are still developing. 7 Inputs Input Layer Hidden Layer Output Layer Outputs Context Layer * Their efficacy for learning long connected sequences is unproven. Attempts to learn long sequences of events, for example, <ref> [Ben96] </ref>, lasting for hundreds of samples, have not shown good results. They do seem capable of learning short sequences (tens of frames), such as learning individual phonemes, rather than whole words. * Extracting meaning from a normal feedforward neural networks has proved difficult.
Reference: [Dan98] <author> Andrea Danyluk. </author> <title> Predicting the future: AI approaches to time-series problems. Technical Report WS-98-07, </title> <publisher> AAAI Press, </publisher> <year> 1998. </year>
Reference-contexts: The work on context detection is about selecting which static classifier to use on a dynamic basis; whereas temporal classification is about classifying the dynamics themselves. Increasingly, this area has become a popular research topic. For example, a workshop held at AAAI '98 <ref> [Dan98] </ref> while focusing on temporal prediction, also contained several papers on learning from time series. For example, Keogh and Pazzani [KP98] looks at automated ways of clustering time series from ECG signals and Shuttle information, by using a piecewise model combined with segmentation and agglomerative clustering.
Reference: [DLM + 98] <author> Gautam Das, King-Ip Lin, Heikki Mannila, Gopal Renganathan, and Padhraic Smyth. </author> <title> Rule discovery from time series. </title> <booktitle> In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining (KDD-98). </booktitle> <publisher> AAAI Press, </publisher> <year> 1998. </year>
Reference-contexts: In their model, events are modelled not as a set of channels, but as a sequence of time-labelled events. Learning is accomplished by trying to find sequences of events and the relevant time constraints that fit the data. Das et al <ref> [DLM + 98] </ref> also worked on ways of extracting rule from univariate 10 data trying to extract rules of the form "A is followed by B".
Reference: [DM86] <author> T. G. Dietterich and R. S. Michalski. </author> <title> Machine Learning: </title> <booktitle> An Artificial Intelligence Approach, Volume II, chapter Learning to predict sequences, </booktitle> <pages> pages 63-106. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1986. </year>
Reference-contexts: These have included work on things such as sequence prediction (e.g. Dietterich and Michalski's work with Eleu-sis <ref> [DM86] </ref>), work with temporal logics and their applications to recognis-ing events, for example Kumar's work on temporal event conceptualisation [KM87] and the recent work in context detection and extraction for machine learning applications [Wid96, HHS98].
Reference: [FMR98] <author> Nir Friedman, Kevin Murphy, and Stuart Russell. </author> <title> Learning the structure of dynamic probabilistic networks. </title> <booktitle> In Proceeding Uncertainty in Artificial Intelligence Conference 1998 (UAI-98). </booktitle> <publisher> AAAI Press, </publisher> <year> 1998. </year>
Reference-contexts: The main problem with using dynamic Bayesian network is that while algorithms for learning the parameters of a Bayes net are well-advanced, learning the structure of Bayes nets has proved more difficult 1 . Friedman et al. <ref> [FMR98] </ref> are developing techniques for learning the structure of dynamic Bayes networks; it remains to be seen whether these techniques can be applied in temporal classification domains. 1 In many ways, dynamic Bayes nets suffer the same problems as HMMs.
Reference: [HHS98] <author> Michael Harries, Kim Horn, and Claude Sammut. </author> <title> Extracting hidden context. </title> <journal> Machine Learning, </journal> <volume> 32(2), </volume> <month> August </month> <year> 1998. </year>
Reference-contexts: Dietterich and Michalski's work with Eleu-sis [DM86]), work with temporal logics and their applications to recognis-ing events, for example Kumar's work on temporal event conceptualisation [KM87] and the recent work in context detection and extraction for machine learning applications <ref> [Wid96, HHS98] </ref>. While some of these areas bear some interesting relations to temporal classification, they differ in several regards. Sequence prediction is not about learning from labelled examples, nor, typically does it deal with multivariate data.
Reference: [JKP94] <author> George H. John, Ron Kohavi, and Karl Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the International Conference on Machile Learning 1994, </booktitle> <pages> pages 121-129, </pages> <year> 1994. </year>
Reference-contexts: At this stage, we can apply feature selection techniques to reduce the number of features that the learner must deal with. Of course, it is also possible to integrate the feature selection and learning stages, as for example occurs using "wrapper model" feature selection <ref> [JKP94] </ref>. 33 Rule 1: clusterA = Yes -&gt; class blue Rule 2: clusterA = No -&gt; class red Default class: blue 5.2.9 Conventional attribute-value learning This does attribute-value learning once the features have been selected. Almost any conventional learner can be used.
Reference: [Joh89] <author> Trevor Johnston. </author> <title> Auslan Dictionary: a Dictionary of the Sign Language of the Australian Deaf Community. </title> <institution> Deafness Resources Australia Ltd, </institution> <year> 1989. </year>
Reference-contexts: There are approximately 15,000 Auslan signers. A sign consists of a number of physical components, such as the handshape, location, palm orientation, movement of the palms and facial expression. Facial expression is minor in the formation of individual signs, but is fundamental in the construction of phrases <ref> [Joh89] </ref>. A small selection of isolated Auslan signs were captured using an instrumented glove called the PowerGlove.
Reference: [Kad95] <author> Mohammed Waleed Kadous. </author> <title> GRASP: Recognition of Australian sign language using instrumented gloves. </title> <type> Honours thesis, </type> <institution> School of Computer Science and Engineering, University of New South Wales, </institution> <year> 1995. </year>
Reference-contexts: No doubt, these problems can be converted to conventional classification tasks. However, techniques for doing so have generally been ad-hoc, labour-intensive and domain-specific. For example, Kadous <ref> [Kad95] </ref> does so for the sign language domain. In addition, such techniques do not make good use of the special temporal properties and heuristics that are likely to apply in such domains. <p> For example, with the sign language recognition domain, approximately 30 per cent accuracy can be obtained by looking at the minima and maxima of the x, y and z axes alone <ref> [Kad95] </ref>. However, they are usually not sufficient for doing high-accuracy classification. In addition, they are usually insufficient for producing meaningful descriptions. <p> In previous results <ref> [Kad95] </ref>, it was shown that by manually selecting a set of global attributes and extracting them and trying different learners, recognition with accuracies up to approximately 80 per cent on seen signers could be achieved with a vocabulary of 95 signs.
Reference: [KM87] <author> K. Kumar and A. Mukerjee. </author> <title> Temporal event conceptualization. </title> <booktitle> In Proc. of the 10th IJCAI, </booktitle> <pages> pages 472-475, </pages> <address> Milan, Italy, </address> <year> 1987. </year> <month> 49 </month>
Reference-contexts: These have included work on things such as sequence prediction (e.g. Dietterich and Michalski's work with Eleu-sis [DM86]), work with temporal logics and their applications to recognis-ing events, for example Kumar's work on temporal event conceptualisation <ref> [KM87] </ref> and the recent work in context detection and extraction for machine learning applications [Wid96, HHS98]. While some of these areas bear some interesting relations to temporal classification, they differ in several regards. Sequence prediction is not about learning from labelled examples, nor, typically does it deal with multivariate data.
Reference: [KP98] <author> Eamonn J. Keogh and Michael J. Pazzani. </author> <title> An enhanced repre-sentation of time series which allows fast and accurate classification, clustering and relevance feedback. In Predicting the Future: </title> <booktitle> AI Approaches to Time-Series Problems [Dan98], </booktitle> <pages> pages 44-51. </pages>
Reference-contexts: Increasingly, this area has become a popular research topic. For example, a workshop held at AAAI '98 [Dan98] while focusing on temporal prediction, also contained several papers on learning from time series. For example, Keogh and Pazzani <ref> [KP98] </ref> looks at automated ways of clustering time series from ECG signals and Shuttle information, by using a piecewise model combined with segmentation and agglomerative clustering.
Reference: [Man97] <author> Stefanos Manganaris. </author> <title> Supervised Classification with Temporal Data. </title> <type> PhD thesis, </type> <institution> Computer Science Department, School of Engineering, Vanderbilt University, </institution> <month> December </month> <year> 1997. </year>
Reference-contexts: He then applies the techniques to clinical domains. Paliouras [Pal97] discusses refinement of temporal parameters in an existing temporal expert system. Manganaris <ref> [Man97] </ref> developed a system for supervised classification of univariate signals using piecewise polynomial modelling combined with a scale-space analysis technique (i.e. a technique that allows the system to cope with the problem that patterns occur at different scales) and applies them to space shuttle data as well as an artificial dataset. <p> Thus, by applying the finding function, we can find all the events that appear to be an instance of an event primitive we are considering. Example PEPs are discussed below. Example PEP: A "straight-line" approximation for continuous-valued channels Manganaris <ref> [Man97] </ref> and Pednault [Ped89] discuss techniques for converting a sequence of continuous values into a piecewise polynomial model - in other words, you provide a continuous valued function, much like a continuous channel in our representation, and it provides you with a set of polynomials, that together, approximate the channel. <p> For example, the lines shown in figure 5.3 could be described as in table 5.1. The finding function used above is based on a similar heuristic to that used by Manganaris <ref> [Man97] </ref>. Giving a closed-form definition of this finding function is difficult.
Reference: [Mit97] <author> Thomas M. Mitchell. </author> <title> Machine Learning. </title> <publisher> McGraw-Hill, </publisher> <year> 1997. </year>
Reference-contexts: Thus for each class, the learner was used to build a binary classifier that returned true if the instance was a member of the class and false otherwise. The two learners we considered were: * A naive Bayes learner <ref> [Mit97] </ref> a learning technique which assumes that all the attributes are conditionally independent and makes an estimate of the probability that a particular training instance belongs to the class as the product of the probabilities that the each value came from the class.
Reference: [MR81] <author> C. S. Myers and L. R. Rabiner. </author> <title> A comparative study of several dynamic time-warping algorithms for connected word recognition. </title> <journal> The Bell System Technical Journal, </journal> <volume> 60(7) </volume> <pages> 1389-1409, </pages> <month> September </month> <year> 1981. </year>
Reference-contexts: They usually require many examples before converging, especially if there are many inputs or the hidden layer contains many nodes. 2.3 Dynamic time warping An older technique that been fallen out of favour since the advent of hidden Markov models is dynamic time warping <ref> [MR81] </ref>. In some senses, it is the temporal domain equivalent of instance-based learning with a complex distance function. In dynamic time warping, the problem is represented as finding the minimum distance between a set of template streams and the input stream. The class chosen is the "closest" template.
Reference: [MTV95] <author> Heikki Mannila, Hannu Toivonen, and A. Inkeri Verkamo. </author> <title> Discovering frequent episodes in sequences. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD-95), </booktitle> <pages> pages 210-215, </pages> <year> 1995. </year>
Reference-contexts: Mannila et al <ref> [MTV95] </ref> have also been looking at temporal classification problems, in particular applying it to network traffic analysis. In their model, events are modelled not as a set of channels, but as a sequence of time-labelled events.
Reference: [OJC98] <author> Tim Oates, David Jensen, and Paul R. Cohen. </author> <title> Discovering rules for clustering and predicting asynchronous events. In Predicting the Future: </title> <booktitle> AI Approaches to Time-Series Problems [Dan98], </booktitle> <pages> pages 73-79. </pages>
Reference-contexts: For example, Keogh and Pazzani [KP98] looks at automated ways of clustering time series from ECG signals and Shuttle information, by using a piecewise model combined with segmentation and agglomerative clustering. In Oates et al. <ref> [OJC98] </ref>, a system is applied to extracting patterns from network failures, by looking at all possible sequences of events and keeping tabs on the frequency of these events.
Reference: [Pal97] <author> Georgios Paliouras. </author> <title> Refinement of Temporal Constraints in an Event Recognition System using Small Datasets. </title> <type> PhD thesis, </type> <institution> University of Manchester, </institution> <year> 1997. </year>
Reference-contexts: Shahar [SM95] suggests an expert system architecture for knowledge-based temporal abstraction and also suggests that this system could be used for learning, though he does not actually do so. He then applies the techniques to clinical domains. Paliouras <ref> [Pal97] </ref> discusses refinement of temporal parameters in an existing temporal expert system.
Reference: [Ped89] <author> Edwin P. D. Pednault. </author> <title> Some experiments in applying inductive inference to surface reconstruction. </title> <booktitle> In AAAI Conference Proceedings, </booktitle> <pages> pages 1603-1608. </pages> <publisher> AAAI, </publisher> <year> 1989. </year>
Reference-contexts: Thus, by applying the finding function, we can find all the events that appear to be an instance of an event primitive we are considering. Example PEPs are discussed below. Example PEP: A "straight-line" approximation for continuous-valued channels Manganaris [Man97] and Pednault <ref> [Ped89] </ref> discuss techniques for converting a sequence of continuous values into a piecewise polynomial model - in other words, you provide a continuous valued function, much like a continuous channel in our representation, and it provides you with a set of polynomials, that together, approximate the channel.
Reference: [Qui93] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Almost any conventional learner can be used. For example, we may apply a rule-learning system to the dataset, a decision tree learner or any other attribute-value learner. We applied c4.5rules, a simple rule-learning system <ref> [Qui93] </ref>, to the Blues and Reds domain. It came up with the rules shown in figure 5.6. As can be seen, the rule is very simple. <p> Each class learner predicted the probability that the test instance was a member of the class. The binary classifiers were combined by looking for the classifier that returned the highest 43 probability that the instance was a member of its class. * C4.5 <ref> [Qui93] </ref>, a decision tree builder. Each class learner was asked to classify whether each test instance belonged to it or not. The classifiers produced by this technique were combined in the following way: If no binary classifier "claimed" the test instance, it was unclassified.
Reference: [RC98] <author> Michal T. Rosenstein and Paul R. Cohen. </author> <title> Concepts from time series. </title> <booktitle> In AAAI '98: Fifteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 739-745. </pages> <publisher> AAAI, AAAI Press, </publisher> <year> 1998. </year>
Reference-contexts: Furthermore, it is only the nature of these changes that makes classification possible. Examples of such domains include recognition of gestures, speech recognition, medical signal analysis, robots detecting temporal events from sensor readings <ref> [RC98] </ref>, data mining in temporal databases and many more. Consider the classification of spoken words. We can measure what are called the Cepstral coefficients (which try to model the human aural system) of the incoming speech signal. <p> Das et al [DLM + 98] also worked on ways of extracting rule from univariate 10 data trying to extract rules of the form "A is followed by B". Rosenstein and Cohen <ref> [RC98] </ref> used delay coordinates (a representation where the state at time t n is compared to its state at time t with n varied appropriately to give an appropriate "delay portrait").
Reference: [RJ86] <author> L. R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> IEEE Magazine on Accoustics, Speech and Signal Processing, </journal> <volume> 3(1) </volume> <pages> 4-16, </pages> <year> 1986. </year>
Reference-contexts: We already have some existing techniques which can perform well when the costs of manual adjustments bring enough benefits. The best example of this is hidden Markov models, the dominant means of speech recognition. Hidden Markov models (see section 2.1 and <ref> [RJ86] </ref>) can recognise speech with high rates of accuracy, but require a great deal of tweaking, not all of which is easily extractable from domain knowledge. <p> They have been applied for decades. Due to space considerations, an in-depth analysis of HMMs isn't possible. The interested reader may wish to have a look at <ref> [RJ86] </ref>. HMMs consist of states, possible transitions between states (and the probability of those transitions being taken) and a probability that in a particular state, a particular observation is made. An observation can be anything of interest.
Reference: [RM86] <author> D. E. Rumelhart and J. L. McClelland. </author> <title> Parallel Distributed Processing: Explorations in the microstructure of Cognition, </title> <booktitle> volume 1. Foundations. </booktitle> <publisher> MIT Press/Bradford Books, </publisher> <year> 1986. </year>
Reference-contexts: Through algorithms such as backpropagation, the weights of the neural net can be adjusted so as to produce an output on the appropriate unit when a particular pattern at the input is observed. The interested reader may wish to find out more in <ref> [RM86] </ref>. unit unit unit unit unit unit unit unit unit unit Input Layer Hidden Layers Output Layer A recurrent neural network (RNN) is a modification to this architecture to allow for temporal classification, as shown in figure 2.4.
Reference: [SM95] <author> Yuval Shahar and Mark A. Musen. </author> <title> Knowledge-based temporal abstraction in clinical domains. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <year> 1995. </year> <month> 50 </month>
Reference-contexts: In Oates et al. [OJC98], a system is applied to extracting patterns from network failures, by looking at all possible sequences of events and keeping tabs on the frequency of these events. Shahar <ref> [SM95] </ref> suggests an expert system architecture for knowledge-based temporal abstraction and also suggests that this system could be used for learning, though he does not actually do so. He then applies the techniques to clinical domains. Paliouras [Pal97] discusses refinement of temporal parameters in an existing temporal expert system.
Reference: [Wid96] <author> Gerhard Widmer. </author> <title> Recognition and exploitation of contextual clues via incremental meta-learning. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 525-533. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: Dietterich and Michalski's work with Eleu-sis [DM86]), work with temporal logics and their applications to recognis-ing events, for example Kumar's work on temporal event conceptualisation [KM87] and the recent work in context detection and extraction for machine learning applications <ref> [Wid96, HHS98] </ref>. While some of these areas bear some interesting relations to temporal classification, they differ in several regards. Sequence prediction is not about learning from labelled examples, nor, typically does it deal with multivariate data.
Reference: [ZR98] <author> Geoffrey Zweig and Stuart Russell. </author> <title> Speech recognition with dynamic Bayesian networks. </title> <booktitle> In Fifteenth National Conference on Artificial Intelligence (AAAI'98), </booktitle> <pages> pages 173-180. </pages> <publisher> AAAI Press, </publisher> <year> 1998. </year>
Reference-contexts: Another interesting development in recent years is the application of dynamic Bayesian networks to temporal classification tasks. While they are not specifically designed for temporal classification (they are more commonly used for prediction, or for estimating current state given the previous state estimate), Zweig and Russell <ref> [ZR98] </ref> have applied it to the task of speech recognition. The main problem with using dynamic Bayesian network is that while algorithms for learning the parameters of a Bayes net are well-advanced, learning the structure of Bayes nets has proved more difficult 1 .
References-found: 25

