URL: http://osl.cs.uiuc.edu/Papers/ipps-panwar.ps
Refering-URL: http://osl.cs.uiuc.edu/Papers/Parallel.html
Root-URL: http://www.cs.uiuc.edu
Email: Email: panwar@vnet.ibm.com  Email: f wooyoung j aghag@cs.uiuc.edu  
Title: Parallel Implementations of Irregular Problems using High-level Actor Language  
Author: R. B. Panwar W. Kim and G. A. Agha 
Address: San Jose, CA 95141, USA  Urbana, IL 61801, USA  
Affiliation: Application Dev. Technology Institute IBM Santa Teresa Labs  Open Systems Laboratory University of Illinois at Urbana-Champaign  
Abstract: In this paper we present our experience in implementing several irregular problems using a high-level actor language. The problems studied require dynamic computation of object placement and may result in load imbalance as the computation proceeds, thereby requiring dynamic load balancing. The algorithms are expressed as fine-grained computations providing maximal flexibility in adapting the computation load to arbitrary parallel architectures. Such an algorithm may be composed with different partitioning and distribution strategies (PDS's) to result in different performance characteristics. The PDS's are implemented for specific data structures or algorithms and are reusable for different parallel algorithms. We demonstrate how our methodology provides portability of algorithm specification, reusability and ease of expressibility. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: However, an unaided compiler may be less successful in more general cases. Because specifying a parallel algorithm in terms of actors does not introduce unnecessary sequentiality we use Actors as our computational model of concurrency <ref> [1] </ref>. Actors encapsulate data, procedures and a thread of control. Each actor has a mail address and a behavior. Mail addresses may be communicated, thereby providing a dynamic com-munication topology.
Reference: [2] <author> G. Agha, C. Houck, and R. Panwar. </author> <title> Distributed Execution of Actor Systems. </title> <editor> In D. Gelernter, T. Gross, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 1-17. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <note> LNCS 589. </note>
Reference-contexts: In response to a communication, an actor may: (i) asynchronously send a message to a specified actor, (ii) create an actor with the specified behavior, and (iii) change its local state. These basic actor primitives are supported as primitives in THAL. THAL is a descendent of HAL <ref> [6, 2, 3] </ref> and tailored for high-performance execution on stock-hardware distributed memory multicomputers, such as the CM-5. It extends create primitive to take an additional location argument to provide programmers with control over actor placement. In addition, migration for dynamic actor relocation is supported at the language level. <p> Since a C compiler may optimize the sequential intra-method computation, the compile-time optimizations of the THAL compiler concentrate on improving concurrent inter-method execution. In particular, the THAL compiler restores profitable concurrency that may be lost by some high-level language constructs <ref> [2] </ref>. Additional efficiency is supported by the runtime kernel [9] which provides a fast communication layer implemented using the CMAM [15].
Reference: [3] <author> G. Agha, W. Kim, and R. Panwar. </author> <title> Actor languages for specification of parallel computations. </title> <editor> In G. E. Blelloch, K. Mani Chandy, and S. Jagannathan, editors, </editor> <booktitle> DIMACS. Series in Discrete Mathematics and Theoretical Computer Science. </booktitle> <volume> vol 18. </volume> <booktitle> Specification of Parallel Algorithms, </booktitle> <pages> pages 239-258. </pages> <publisher> American Mathematical Society, </publisher> <year> 1994. </year>
Reference-contexts: In response to a communication, an actor may: (i) asynchronously send a message to a specified actor, (ii) create an actor with the specified behavior, and (iii) change its local state. These basic actor primitives are supported as primitives in THAL. THAL is a descendent of HAL <ref> [6, 2, 3] </ref> and tailored for high-performance execution on stock-hardware distributed memory multicomputers, such as the CM-5. It extends create primitive to take an additional location argument to provide programmers with control over actor placement. In addition, migration for dynamic actor relocation is supported at the language level.
Reference: [4] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed Memory Compiler Methods for Irregular Problems - Data Copy Reuse and Runtime Partitioning. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers and Run-Time Environments for Distributed Memory Machines. </title> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference-contexts: For irregular problems, such a priori determination of the necessary data distribution is not feasible. To address this problem in some limited cases, PARTI <ref> [4] </ref> and Kali [10] transform a user-defined for loop to an inspector/executor pair. In these languages the compiler assumes the entire responsibility to uncover concurrency characteristics. In many regular problems using dense matrices, compiler tools may exploit most of the useful parallelism.
Reference: [5] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran-D for MIMD Distributed Memory Machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: We present the evaluation results as well as what we have learned from the implementations. 2 Background Most of the previous work in specifying and optimizing placement of data in parallel programs has been based on extensions of sequential languages. In particular, Fortran-D <ref> [5] </ref> and High Performance Fortran (HPF) [11] allow explicit specification of data decomposition and distribution policies for regular problems to improve execution efficiency on distributed memory multicomputers. For irregular problems, such a priori determination of the necessary data distribution is not feasible.
Reference: [6] <author> C. Houck and G. Agha. HAL: </author> <title> A high-level actor language and its distributed implementation. </title> <booktitle> In Proceedings of the 21st International Conference on Parallel Processing (ICPP '92), </booktitle> <volume> volume II, </volume> <pages> pages 158-165, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: In response to a communication, an actor may: (i) asynchronously send a message to a specified actor, (ii) create an actor with the specified behavior, and (iii) change its local state. These basic actor primitives are supported as primitives in THAL. THAL is a descendent of HAL <ref> [6, 2, 3] </ref> and tailored for high-performance execution on stock-hardware distributed memory multicomputers, such as the CM-5. It extends create primitive to take an additional location argument to provide programmers with control over actor placement. In addition, migration for dynamic actor relocation is supported at the language level.
Reference: [7] <author> L. H. Jamieson. </author> <title> Characterizing parallel algorithms. </title> <editor> In R. J. Douglass L.H. Jamieson, D.B. Gannon, editor, </editor> <booktitle> The Characteristics of Parallel Algorithms, </booktitle> <pages> pages 65-100. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: 1 Introduction In parallel computing a set of operations and the partial order in which they may be carried out define an ideal algorithm <ref> [7] </ref>. The ideal algorithm may be specified without introducing any unnecessary sequentiality by using maximally concurrent objects, i.e., actors. In practice, limitations on computation and communication resources in practical architectures make implementations of a parallel algorithm use only part of all available parallelism in the ideal version of the algorithm.
Reference: [8] <author> D. Kahaner, C. Moler, and S. Nash. </author> <title> Numerical Methods and Software. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: The workers perform the integration in their subintervals in parallel and return the values to the master where they are combined to obtained the final result. Adaptive integration techniques <ref> [8] </ref> vary the step size used for the integration in a region based on the local error estimate. Thus the work available for each worker may dynamically increase as the computation proceeds.
Reference: [9] <author> W. Kim and G. Agha. </author> <title> Efficient Support of Location Transparency in Concurrent Object-Oriented Programming Languages. </title> <booktitle> In Supercomputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: Since a C compiler may optimize the sequential intra-method computation, the compile-time optimizations of the THAL compiler concentrate on improving concurrent inter-method execution. In particular, the THAL compiler restores profitable concurrency that may be lost by some high-level language constructs [2]. Additional efficiency is supported by the runtime kernel <ref> [9] </ref> which provides a fast communication layer implemented using the CMAM [15]. Despite the flexibility and support for dynamic computations, its performance on dense, regular problems is competitive with more restrictive, static languages [9]. 3 Methodology Fine-grained specification of parallel algorithms provides maximal flexibility in distributing the workload. <p> Additional efficiency is supported by the runtime kernel <ref> [9] </ref> which provides a fast communication layer implemented using the CMAM [15]. Despite the flexibility and support for dynamic computations, its performance on dense, regular problems is competitive with more restrictive, static languages [9]. 3 Methodology Fine-grained specification of parallel algorithms provides maximal flexibility in distributing the workload. THAL programs may exploit this flexibility to compose a parallel algorithm with different PDS's [14]. The PDS's themselves are designed for specific data structures or program structures.
Reference: [10] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling Global Name-space Parallel loops for Distributed Execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <year> 1991. </year>
Reference-contexts: For irregular problems, such a priori determination of the necessary data distribution is not feasible. To address this problem in some limited cases, PARTI [4] and Kali <ref> [10] </ref> transform a user-defined for loop to an inspector/executor pair. In these languages the compiler assumes the entire responsibility to uncover concurrency characteristics. In many regular problems using dense matrices, compiler tools may exploit most of the useful parallelism.
Reference: [11] <author> David. B. Loveman. </author> <title> High Performance Fortran. Parallel & Distributed Technology, </title> <journal> Systems & Applications, </journal> <volume> 1(1) </volume> <pages> 25-42, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: We present the evaluation results as well as what we have learned from the implementations. 2 Background Most of the previous work in specifying and optimizing placement of data in parallel programs has been based on extensions of sequential languages. In particular, Fortran-D [5] and High Performance Fortran (HPF) <ref> [11] </ref> allow explicit specification of data decomposition and distribution policies for regular problems to improve execution efficiency on distributed memory multicomputers. For irregular problems, such a priori determination of the necessary data distribution is not feasible.
Reference: [12] <author> K. Mani Chandy and Stephen Taylor. </author> <title> An Introduction to Parallel Programming. </title> <publisher> Jones and Bartlett Publishers, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: The algorithm we implemented computes the sum of elements available at each node of the tree (similar to the summation of leaves of a binary tree described in <ref> [12] </ref>). The values being summed are arrays of floating point numbers. The tree is generated as a search tree formed by inserting nodes containing a key. <p> PEs Time Time Without DLB With DLB 1 0.963 0.963 4 0.435 0.386 16 0.146 0.122 Table 3. Timing results (in seconds) for Adaptive Quadrature Algorithm. Quadrature Implementation 5.2 Unstructured Grid Problem The unstructured grid problem <ref> [12, 13] </ref> solves a set of differential equations for a given input domain with adaptive refinement of the grid used for solving the equations. The problem can be modeled using the same master worker configuration used in the adaptive quadrature problem.
Reference: [13] <editor> P. Mehrotra, J. Saltz, and R. Voigt, editors. </editor> <title> Unstructured Scientific Computation on Scalable Multiprocessors. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: PEs Time Time Without DLB With DLB 1 0.963 0.963 4 0.435 0.386 16 0.146 0.122 Table 3. Timing results (in seconds) for Adaptive Quadrature Algorithm. Quadrature Implementation 5.2 Unstructured Grid Problem The unstructured grid problem <ref> [12, 13] </ref> solves a set of differential equations for a given input domain with adaptive refinement of the grid used for solving the equations. The problem can be modeled using the same master worker configuration used in the adaptive quadrature problem.
Reference: [14] <author> R. Panwar and G. Agha. </author> <title> A Methodology for Programming Scalable Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 479-487, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: We adopt a programming methodology which uses fine-grained computation, asynchronous communication and dynamic object creation <ref> [14] </ref>. Such a methodology enables programmers to compose different partitioning and distribution strategies (PDS's) with ideal algorithms for better performance. A PDS which is defined for a particular data structure or an algorithm may be reused with different parallel algorithms. <p> THAL programs may exploit this flexibility to compose a parallel algorithm with different PDS's <ref> [14] </ref>. The PDS's themselves are designed for specific data structures or program structures. For example, more than one PDS's may be designed for a binary tree data structure. <p> Figure 1 shows how the separate specification of PDS and ideal algorithm may be combined. The glue code specifies details such as which algorithm behavior matches with which PDS behavior. This separate specification promotes modularity, portability and reusability <ref> [14] </ref>. Figure 2 gives the block diagram of the system used for our experiments. An ideal algorithm composed with a PDS using some glue code is translated to a single THAL program.
Reference: [15] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of International Symposium of Computer Architectures, </booktitle> <pages> pages 256-266, </pages> <year> 1992. </year>
Reference-contexts: In particular, the THAL compiler restores profitable concurrency that may be lost by some high-level language constructs [2]. Additional efficiency is supported by the runtime kernel [9] which provides a fast communication layer implemented using the CMAM <ref> [15] </ref>. Despite the flexibility and support for dynamic computations, its performance on dense, regular problems is competitive with more restrictive, static languages [9]. 3 Methodology Fine-grained specification of parallel algorithms provides maximal flexibility in distributing the workload.
References-found: 15

