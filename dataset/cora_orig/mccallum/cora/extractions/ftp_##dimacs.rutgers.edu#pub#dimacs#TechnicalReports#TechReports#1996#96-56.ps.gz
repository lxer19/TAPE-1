URL: ftp://dimacs.rutgers.edu/pub/dimacs/TechnicalReports/TechReports/1996/96-56.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Email: koiran@lip.ens-lyon.fr  sontag@hilbert.rutgers.edu  
Title: Dimension of Recurrent Neural Networks  
Author: by Pascal Koiran Eduardo D. Sontag 
Address: 46 allee d'Italie, 69364 Lyon Cedex 07 France  New Brunswick, NJ 08903 USA  
Affiliation: Laboratoire de l'Informatique du Parallelisme Ecole Normale Superieure de Lyon CNRS  Department of Mathematics Rutgers University  
Note: Vapnik-Chervonenkis  This research was carried out in part while visiting DIMACS and the Rutgers Center for Systems and Control (SYCON) at Rutgers University. This research was supported in part by US Air Force Grant AFOSR-94-0293. DIMACS is a partnership of Rutgers University, Princeton University, AT&T Research, Bellcore, and Bell Laboratories. DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology.  
Abstract: DIMACS Technical Report 96-56 December 1996 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E.B. Baum and D. Haussler, </author> <title> "What size net gives valid generalization?", </title> <journal> Neural Computation, </journal> <volume> 1(1989), </volume> <pages> pp. 151-160. </pages>
Reference-contexts: This trivial fact allows one to easily obtain estimates, based on those bounds which were developed (cf. <ref> [3, 1] </ref>, [6], [7]) for the feedforward case. Theorem 1 For recurrent architectures, with w weights receiving inputs of length k: 1. The VC dimension of threshold recurrent architectures is O (kw log kw). 2. <p> The result then follows from the Baum-Haussler bound <ref> [1] </ref>. 2 Proof of Theorem 2. Let S = fu 1 ; : : : ; u s g be a set of s inputs. We will bound the number of distinct transition functions of the architecture for inputs in S. <p> Next we define a family F of functions which shatters S. The functions in this family are indexed by - parameters w 1 ; :::; w - 2 <ref> [0; 1] </ref>. Each parameter is assumed to have a finite -bit binary expansion 0:w i1 : : : w i . <p> One can always convert a non-layered feedforward architecture into a layered one by introducing delays (identity gates). The following two lemmas from [8] are needed (the first one is well-known and easy to prove). Lemma 2 Let : <ref> [0; 1] </ref> ! [0; 1] be the logistic map (x) = 4x (1 x). For every n 1 and every * 2 f0; 1g n there exists x 1 2 [0; 1] such that the sequence (x k ) 1kn defined by x k+1 = (x k ) for k = <p> One can always convert a non-layered feedforward architecture into a layered one by introducing delays (identity gates). The following two lemmas from [8] are needed (the first one is well-known and easy to prove). Lemma 2 Let : <ref> [0; 1] </ref> ! [0; 1] be the logistic map (x) = 4x (1 x). For every n 1 and every * 2 f0; 1g n there exists x 1 2 [0; 1] such that the sequence (x k ) 1kn defined by x k+1 = (x k ) for k = 1; : : <p> Lemma 2 Let : <ref> [0; 1] </ref> ! [0; 1] be the logistic map (x) = 4x (1 x). For every n 1 and every * 2 f0; 1g n there exists x 1 2 [0; 1] such that the sequence (x k ) 1kn defined by x k+1 = (x k ) for k = 1; : : : ; n 1 satisfies the following property: 0 x k &lt; 1=2 if * k = 0 and and 1=2 &lt; x k 1 if *
Reference: [2] <author> Y. Bengio, </author> <title> Neural Networks for Speech and Sequence Recognition, </title> <publisher> Thompson Computer Press, </publisher> <address> Boston, </address> <year> 1996. </year>
Reference-contexts: They have been employed in the design of control laws for robotic manipulators (Jordan), as well as in speech recognition (Fallside, Kuhn), speaker identification (Anderson), formal language inference (Giles), and sequence extrapolation for time series prediction (Farmer). See for instance the book <ref> [2] </ref>, which emphasizes digital signal processing, or the work on language learning in [5], and the many references given in both citations. In both the areas of signal processing ([9]) and control ([10]), recurrent nets have been proposed as generic identification models or as prototype dynamic controllers.
Reference: [3] <author> T.M. </author> <title> Cover, "Capacity problems for linear machines", in: Pattern Recognition (L. </title> <editor> Kanal ed.), </editor> <publisher> Thompson Book Co., </publisher> <year> 1968, </year> <pages> pp. 283-289 </pages>
Reference-contexts: This trivial fact allows one to easily obtain estimates, based on those bounds which were developed (cf. <ref> [3, 1] </ref>, [6], [7]) for the feedforward case. Theorem 1 For recurrent architectures, with w weights receiving inputs of length k: 1. The VC dimension of threshold recurrent architectures is O (kw log kw). 2.
Reference: [4] <author> B. Dasgupta and E.D. Sontag, </author> <title> "Sample complexity for learning recurrent perceptron mappings," </title> <journal> IEEE Trans. Inform. Theory, </journal> <month> September </month> <year> 1996, </year> <note> to appear. (Summary in Advances in Neural Information Processing Systems 8 (NIPS95) (D.S. </note> <editor> Touretzky, M.C. Moser, and M.E. Hasselmo, eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996, </year> <pages> pp. 204-210.) - 14 </pages> - 
Reference-contexts: We are particularly interested in understanding the behavior of vc (A; k) as k ! 1, for various recurrent architectures, as well as the dependence of this quantity on the number of weights and the particular type of activation being used. In particular, we continue the work described in <ref> [4] </ref> (see also [16] for related work), which had obtained estimates of these quantities for architectures with identity activations. By a threshold recurrent architecture we mean a homogeneous one with = H. <p> The bounds would seem to be too conservative, since they completely disregard the fact that the weights in the different layers of the "unfolded" net are actually the same. The surprising aspect of the results to be stated next (and of the results in <ref> [4] </ref>) is that we obtain lower bounds which do not look much different. We first state two more upper bounds. The first one is interesting because for fixed w, it shows a log k dependence, rather than the k log k obtained by unfolding. <p> The VC dimension of recurrent architectures with activation , with w weights and receiving inputs of length k, is O (kw). Moreover, if is linear this bound can be improved to O (w log k). For a corresponding lower bound in the linear case, see <ref> [4] </ref>. We now turn to other lower bounds. Theorem 4 The VC dimension of threshold recurrent architectures, with w weights and receiving inputs of length k = (w), is (w log (k=w)). <p> Hence by [6] (Theorem 2.3) its VC dimension is O (w fi kw). 2 Interestingly, one can give a better upper bound for polynomial activation functions than for piecewise-polynomial activation functions. The linear case is included in <ref> [4] </ref>. Proof of Theorem 3. We denote by W the vector listing all weights in the three systems matrices ff; fi; fl, so that the parameter vector can be partitioned as (W; 0 ), where 0 lists the weights in ~.
Reference: [5] <author> C.L. Giles, G.Z. Sun, H.H. Chen, Y.C. Lee and D. Chen, </author> <title> "Higher order recurrent networks and grammatical inference", </title> <booktitle> in Advances in Neural Information Processing Systems 2, </booktitle> <editor> D.S. Touretzky (ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: See for instance the book [2], which emphasizes digital signal processing, or the work on language learning in <ref> [5] </ref>, and the many references given in both citations. In both the areas of signal processing ([9]) and control ([10]), recurrent nets have been proposed as generic identification models or as prototype dynamic controllers.
Reference: [6] <author> P. Goldberg and M. Jerrum, </author> <title> "Bounding the Vapnik-Chervonenkis dimension of concept classes parametrized by real numbers," </title> <booktitle> Machine Learning 18(1995), </booktitle> <pages> pp. 131-148. </pages>
Reference-contexts: This trivial fact allows one to easily obtain estimates, based on those bounds which were developed (cf. [3, 1], <ref> [6] </ref>, [7]) for the feedforward case. Theorem 1 For recurrent architectures, with w weights receiving inputs of length k: 1. The VC dimension of threshold recurrent architectures is O (kw log kw). 2. <p> Hence the whole computation requires O (kw) operations for inputs of length k. The architecture has w + n 2w programmable parameters, where n is the number of units in the network. Hence by <ref> [6] </ref> (Theorem 2.3) its VC dimension is O (w fi kw). 2 Interestingly, one can give a better upper bound for polynomial activation functions than for piecewise-polynomial activation functions. The linear case is included in [4]. Proof of Theorem 3. <p> The degree of P k in the programmable parameters is at most D = 2d k + P k1 By <ref> [6] </ref> (Theorem 2.2) the VC dimension is bounded by 2w log (8eD). (note that the degree in the input variables does not appear in this bound.) The theorem follows from the obvious observations: D = k + 1 for d = 1 and D &lt; 2d k+1 for d 2. 2
Reference: [7] <author> M. Karpinski and A. Macintyre, </author> <title> "Polynomial bounds for VC dimension of sigmoidal and general Pfaffian neural networks," J. Computer Sys. Sci., to appear. (Summary in "Polynomial bounds for VC dimension of sigmoidal neural networks," </title> <booktitle> in Proc. 27th ACM Symposium on Theory of Computing, 1995 , pp. </booktitle> <pages> 200-208.) </pages>
Reference-contexts: This trivial fact allows one to easily obtain estimates, based on those bounds which were developed (cf. [3, 1], [6], <ref> [7] </ref>) for the feedforward case. Theorem 1 For recurrent architectures, with w weights receiving inputs of length k: 1. The VC dimension of threshold recurrent architectures is O (kw log kw). 2. <p> By unfolding, the recurrent architecture can be simulated by a feedforward net with kn nodes, where n be the number of nodes in the original architecture, and the same number w of programmable parameters. By <ref> [7] </ref> there is a O ((kn) 2 w 2 ) upper bound on the VC dimension of that architecture. This is O (k 2 w 4 ) as claimed. <p> This is O (k 2 w 4 ) as claimed. Note: one can argue that the feedforward net has kw weights, but many of those weights are "shared" and there are only w + n 2w programmable parameters. The result in <ref> [7] </ref> explicitly allows such weight-sharing arrangements (see condition e in section 4.1 of their paper). 2 - 10 - 4.2 Lower Bounds Theorem 6 shows that the O (kw) upper bound of Theorem 3 is tight (for non-linear polynomials).
Reference: [8] <author> P. Koiran and E.D. Sontag, </author> <title> "Neural networks with quadratic VC dimension," </title> <journal> J. Computer Sys. </journal> <note> Sci., to appear. (Summary in Advances in Neural Information Processing Systems 8 (NIPS95) (D.S. </note> <editor> Touretzky, M.C. Moser, and M.E. Hasselmo, eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996, </year> <pages> pp. 197-203.) </pages>
Reference-contexts: Note also that the outputs at t = 1 and t = 2 are both 0 as needed. 2 Theorem 6 generalizes Theorem 5 to even more arbitrary activations. For this we need some of the machinery of <ref> [8] </ref>. In particular, we need to allow networks with multiplication and division gates. <p> We will use feedforward architectures as building blocks in our recurrent architectures. The necessary background is standard and can be found for instance in <ref> [8] </ref>. To be self-contained, we recall that the units of feedforward architecture are grouped into layers. We use the same type of units as in recurrent architectures (in particular, multiplication and division gates are allowed, as mentioned earlier in this section). <p> For synchronization reasons, such connections are to be avoided in recurrent nets. One can always convert a non-layered feedforward architecture into a layered one by introducing delays (identity gates). The following two lemmas from <ref> [8] </ref> are needed (the first one is well-known and easy to prove). Lemma 2 Let : [0; 1] ! [0; 1] be the logistic map (x) = 4x (1 x). <p> The next result is essentially Lemma 1 from <ref> [8] </ref>. <p> Since the network has to work only on a finite set of inputs, the construction will be correct if * is small enough (this can be justified as in <ref> [8] </ref>). Finally, the output unit accumulates the values of e, starting from the initial state o = 0. Note that these accumulated values are all (approximately) zero, except at most one of them. <p> Note that since the length of the longest path in the network increases by a constant factor, it is necessary to pad the input sequence with O (log w) dummy inputs. This changes only the implied constants in the symbols. 2 As in <ref> [8] </ref>, this result makes it possible to prove good VC dimension lower bounds for a wide class of transfer functions. The most important case is the following: Proof of Theorem 6. Linear and multiplication gates can be simulated by -gates as in [8]. <p> implied constants in the symbols. 2 As in <ref> [8] </ref>, this result makes it possible to prove good VC dimension lower bounds for a wide class of transfer functions. The most important case is the following: Proof of Theorem 6. Linear and multiplication gates can be simulated by -gates as in [8]. The input sequence must be padded by a small number of dummy inputs as in the proof of Theorem 9. 2 5 Final Remarks We have left several questions unanswered: 1.
Reference: [9] <author> M. Matthews, </author> <title> "A state-space approach to adaptive nonlinear filtering using recurrent neural networks," </title> <booktitle> Proc. 1990 IASTED Symp. on Artificial Intelligence Applications and Neural Networks, </booktitle> <address> Zurich, </address> <pages> pp. 197-200, </pages> <month> July </month> <year> 1990. </year>
Reference: [10] <editor> M.M. Polycarpou, and P.A. Ioannou, </editor> <title> "Neural networks and on-line approximators for adaptive control," </title> <booktitle> in Proc. Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pp. 93-798, </pages> <institution> Yale University, </institution> <year> 1992. </year>
Reference: [11] <author> H. Siegelmann and E.D. Sontag, </author> <title> "On the computational power of neural nets," </title> <journal> J. Comp. Syst. Sci. </journal> <volume> 50(1995): </volume> <pages> 132-150. </pages>
Reference: [12] <author> H. Siegelmann and E.D. Sontag, </author> <title> "Analog computation, neural networks, and circuits," </title> <journal> Theor. Comp. Sci. </journal> <volume> 131(1994): </volume> <pages> 331-360. </pages>
Reference: [13] <author> E.D. Sontag, </author> <title> Mathematical Control Theory: Deterministic Finite Dimensional Systems, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Often C is just a projection on some coordinates, that is, the components of y are simply a subset of the components of x. The linear systems customarily studied in control theory (see e.g. the textbook <ref> [13] </ref>) are precisely the homogeneous initialized recurrent nets with identity activation and x 0 = 0. u - B - h + - - - C - y r 6 To each initialized recurrent net (A; B; C; x 0 ; ) we associate a discrete time input/output behavior.
Reference: [14] <author> E.D. Sontag, </author> <title> "Neural nets as systems models and controllers," </title> <booktitle> in Proc. Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pp. 73-79, </pages> <institution> Yale University, </institution> <year> 1992. </year>
Reference: [15] <author> E.D. Sontag, </author> <title> "Feedforward nets for interpolation and classification," </title> <journal> J. Comp. Syst. Sci. </journal> <volume> 45(1992): </volume> <pages> 20-48. </pages>
Reference-contexts: In particular, we continue the work described in [4] (see also [16] for related work), which had obtained estimates of these quantities for architectures with identity activations. By a threshold recurrent architecture we mean a homogeneous one with = H. As in <ref> [15] </ref>, we say that : R ! R is sigmoidal , or a sigmoid , if: 1. is differentiable at some point x 0 where 0 (x 0 )6=0. 2. lim x!1 (x) = 0 and lim x!+1 (x) = 1. (the limits 0 and 1 can be replaced by any
Reference: [16] <author> A.M. Zador and B.A. Pearlmutter, </author> <title> "VC dimension of an integrate-and-fire neuron model," </title> <booktitle> Neural Computation 8(1996): </booktitle> <pages> 611-624. </pages>
Reference-contexts: In particular, we continue the work described in [4] (see also <ref> [16] </ref> for related work), which had obtained estimates of these quantities for architectures with identity activations. By a threshold recurrent architecture we mean a homogeneous one with = H.
References-found: 16

