URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/tara/public_html/thesis.ps.Z
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/tara/public_html/thesis.html
Root-URL: 
Title: c  
Author: flCopyright by Tara Maja Madhyastha 
Date: 1997  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> EOSDIS: </author> <title> EOS Data and Information System. </title> <institution> National Aeronautics and Space Administration, </institution> <year> 1992. </year>
Reference-contexts: HDF input/output performance is particularly important because HDF is the standard data format for all NASA EOS (Earth Observing System) data products. The goal of the Earth Observing System Data and Information System (EOSDIS) <ref> [1] </ref> is to manage data from NASA's Earth science research satellites, providing researchers with a uniform information resource that can be used to assess global change. The collective data rate from satellites in this project is expected to rise as high as 500 terabytes per day by the year 2000.
Reference: [2] <author> Anderson, T. </author> <title> The Case for Application-Specific Operating Systems. </title> <booktitle> In the Third Workshop on Workstation Operating Systems (1992). </booktitle>
Reference-contexts: The exokernel operating system simply defines a low-level interface to resources that handles only protection issues, not management, similar to the application-specific approach to operating systems proposed by Anderson <ref> [2] </ref>. A file system is a particular example of an operating system abstraction that can be restrictive when performance is an issue. Database systems have historically bypassed native file systems, seizing control of the raw resources in order to provide high performance.
Reference: [3] <author> Baker, M. G., Hartman, J. H., Kupfer, M. D., Shirriff, K. W., and Ouster-hout, J. K. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles (Oct. 1991), Association for Computing Machinery SIGOPS, </booktitle> <pages> pp. 198-212. </pages>
Reference-contexts: Uniprocessor workloads are well characterized. Studies of typical engineering workloads on workstations under UNIX 4.2 BSD [61] and the Sprite distributed file system <ref> [3] </ref> reveal that most file accesses are small, sequential, and read only. Scientific workloads on sequential supercomputers have also been extensively studied. Miller and Katz [55] characterized the behavior of seven computational fluid dynamics and climate applications.
Reference: [4] <author> Bassow, F. E. Installing, </author> <title> Managing, and Using the IBM AIX Parallel I/O File System. IBM Document Number SH34-6065-00, </title> <month> February </month> <year> 1995. </year> <institution> IBM Kingston, NY. </institution>
Reference-contexts: Unfortunately, this kind of guarantee usually restricts request concurrency and the overhead of synchronization can be expensive. Parallel file systems usually offer guaranteed atomicity as an option. For example, IBM PIOFS <ref> [4] </ref>, the product version of Vesta [17] 1 , provides two modes for opening a subfile: NORMAL and CAUTIOUS the latter manages overlapping reads writes by ensuring their serialization, and therefore performance is somewhat worse.
Reference: [5] <author> Baylor, S. J., and Wu, C. E. </author> <title> Parallel I/O Workload Characteristics Using Vesta. </title> <booktitle> In IPPS '95 Workshop on Input/Output in Parallel and Distributed Systems (April 1995), </booktitle> <pages> pp. 16-29. </pages>
Reference-contexts: This application uses several three-dimensional arrays that are allocated to processors using a (block,block,block) distribution across three dimensions. Baylor and Wu <ref> [6, 5] </ref> characterized four applications running on the IBM Vesta file system. They determined that the number of small requests was very large, yet there was a wide request size variation. There was significant file sharing within each application.
Reference: [6] <author> Baylor, S. J., and Wu, C. E. </author> <title> Parallel I/O Workload Characteristics Using Vesta. </title> <booktitle> In Input/Output in Parallel and Distributed Computer Systems, </booktitle> <editor> R. Jain, J. Werth, and J. C. Browne, Eds. </editor> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996, </year> <journal> ch. </journal> <volume> 7, </volume> <pages> pp. 167-185. </pages>
Reference-contexts: This application uses several three-dimensional arrays that are allocated to processors using a (block,block,block) distribution across three dimensions. Baylor and Wu <ref> [6, 5] </ref> characterized four applications running on the IBM Vesta file system. They determined that the number of small requests was very large, yet there was a wide request size variation. There was significant file sharing within each application.
Reference: [7] <author> Bennett, R., Bryant, K., Sussman, A., Das, R., and Saltz, J. Jovian: </author> <title> A Framework for Optimizing Parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference (October 1994), </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 10-20. </pages>
Reference-contexts: Many input/output libraries have been developed that rely upon collective input/output optimizations to improve performance of multiprocessor input/output systems. Jovian <ref> [7] </ref> is a run 13 time library for Single Process Multiple Data (SPMD) codes (processors execute the same code on different data). The programmer chooses the appropriate data distribution and input/output operations are implicitly collective. <p> Not all global access patterns are best optimized by recognition of sequential subsections. In particular, much research effort has been devoted to optimizing access to complex matrix decompositions through collective operations <ref> [13, 79, 7] </ref>. In this experiment we consider a benchmark based on access patterns exhibited by applications from Caltech for global climate modeling and modeling of the earth's interior [52].
Reference: [8] <author> Bershad, B. N., Savage, S., Pardyak, P., Sirer, E. G., Fiuczynski, M. E., Becker, D., Eggers, S., and Chambers, C. </author> <title> Extensibility, Safety and Performance in 142 the SPIN Operating System. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles (December 1995). </booktitle>
Reference-contexts: that utilize explicit policy control and access pattern information supplied at various levels of the application programming interface. 2.2.1 Explicit Policy Control Application specification of resource management policies has proven valuable in many domains, and many modern microkernels now support user control or extension of operating system services (e.g., SPIN <ref> [8] </ref> and exokernel [21]). SPIN achieves application resource control by allowing applications to define extension services that are dynamically linked into the kernel virtual address space.
Reference: [9] <author> Cao, P., Felten, E. W., and Li, K. </author> <title> Application-Controlled File Caching Policies. </title> <booktitle> In Proceedings of the 1994 Summer USENIX (June 1994). </booktitle>
Reference-contexts: Park et al suggest using custom virtual memory page replacement policies to optimize out-of-core input/output [63]. Cao et al recommend that applications use their own page replacement policy and describe an approach to buffer allocation to support application-controlled page replacement <ref> [9, 10] </ref>. Similarly, it is well-accepted that a parallel file system must be flexible to provide reasonable performance to a variety of applications. Several research file systems have been designed with explicit policy control in mind.
Reference: [10] <author> Cao, P., Felten, E. W., and Li, K. </author> <title> Implementation and Performance of Application-Controlled File Caching. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation (Nov. </booktitle> <year> 1994). </year>
Reference-contexts: Park et al suggest using custom virtual memory page replacement policies to optimize out-of-core input/output [63]. Cao et al recommend that applications use their own page replacement policy and describe an approach to buffer allocation to support application-controlled page replacement <ref> [9, 10] </ref>. Similarly, it is well-accepted that a parallel file system must be flexible to provide reasonable performance to a variety of applications. Several research file systems have been designed with explicit policy control in mind.
Reference: [11] <author> Charniak, E. </author> <title> Statistical Language Learning. </title> <publisher> The MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: A probabilistic model of file accesses can supply more detailed access pattern information for caching and prefetching policy decisions. To create such a model automatically, we use file access data from previous executions to train extended Markov models, or hidden Markov models (HMMs) <ref> [71, 11] </ref>. HMMs are commonly used in speech recognition software, where the goal is to identify sub-words, words, or syntax based on probabilistic models. At a very high level, access pattern classification is similar to speech recognition, and similar techniques apply.
Reference: [12] <author> Chen, Y., Foster, I., Nieplocha, J., and Winslett, M. </author> <title> Optimizing Collective I/O Performance on Parallel Computers: A Multisystem Study. </title> <booktitle> In Proceedings of the 11th ACM International Conference on Supercomputing (July 1997), </booktitle> <publisher> ACM Press. </publisher>
Reference-contexts: A variable number of coalescing processes aggregate the requests to perform them efficiently under the chosen distribution. PASSION [13, 87, 86] (Parallel And Scalable Software for Input-Output) is another runtime library targeted for SPMD applications with routines to efficiently perform out-of-core operations. The Panda <ref> [79, 12] </ref> library utilizes server-directed input/output and a collective interface to achieve high performance on array accesses. APIs can encapsulate information that allows optimizations that are impossible otherwise. For example, another example of an API-specified access pattern is the Read Any mode of the Portable Parallel File System (PPFS) [31].
Reference: [13] <author> Choudhary, A., Bordawekar, R., Harry, M., Krishnaiyer, R., Ponnusamy, R., Singh, T., and Thakur, R. </author> <title> PASSION: Parallel And Scalable Software for Input-Output. </title> <type> Tech. Rep. </type> <institution> SCCS-636, ECE Dept., NPAC and CASE Center, Syracuse University, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: The programmer chooses the appropriate data distribution and input/output operations are implicitly collective. A variable number of coalescing processes aggregate the requests to perform them efficiently under the chosen distribution. PASSION <ref> [13, 87, 86] </ref> (Parallel And Scalable Software for Input-Output) is another runtime library targeted for SPMD applications with routines to efficiently perform out-of-core operations. The Panda [79, 12] library utilizes server-directed input/output and a collective interface to achieve high performance on array accesses. <p> Not all global access patterns are best optimized by recognition of sequential subsections. In particular, much research effort has been devoted to optimizing access to complex matrix decompositions through collective operations <ref> [13, 79, 7] </ref>. In this experiment we consider a benchmark based on access patterns exhibited by applications from Caltech for global climate modeling and modeling of the earth's interior [52].
Reference: [14] <author> Corbett, P., Feitelson, D., Fineberg, S., Hsu, Y., Nitzberg, B., Prost, J.- P., Snir, M., Traversat, B., and Wong, P. </author> <title> Overview of the MPI-IO Parallel I/O Interface. </title> <booktitle> In IPPS '95 Workshop on Input/Output in Parallel and Distributed Systems (April 1995), </booktitle> <pages> pp. 1-15. </pages>
Reference-contexts: Reordering and parallelization of input/output requests is impossible if they are artificially serialized within the file system (e.g., by file pointers). This is one of the primary reasons that the UNIX file system interface, even with extensions, is generally deemed unsuitable as a parallel file system API <ref> [14, 39] </ref>, although it remains popular for compatibility reasons. For example, a program that can utilize subsections of a file in any order cannot express this flexibility through a file system with a byte stream interface.
Reference: [15] <author> Corbett, P., Prost, J.-P., Demetriou, C., Gibson, G., Riedel, E., Zeleka, J., Chen, Y., Felten, E., Li, K., Hartman, J., Peterson, L., Bershad, B., Wolman, A., and Aydt, R. </author> <title> Proposal for a Common Parallel File System Programming Interface Version 1.0, </title> <note> 1996. Available at http://www.cs.arizona.edu/sio. </note>
Reference-contexts: We can view an access pattern classification as an automatically generated hint, used to select and tune any variety of file system policies. In this chapter we describe how local and global classification could work in conjunction with the low level API (LL-API) <ref> [15] </ref> proposed by the Scalable I/O Initiative. This interface was developed by a consortium of universities, national laboratories, and industries involved in parallel input/output research. It includes functions that enable high performance to create a suitable foundation for writing higher-level application libraries.
Reference: [16] <author> Corbett, P. F., Baylor, S. J., and Feitelson, D. G. </author> <title> Overview of the Vesta Parallel File System. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems (1993), </booktitle> <pages> pp. 1-16. </pages> <note> Also published in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 7-14. </pages>
Reference-contexts: For example, Intel PFS [32] has file access modes that describe a variety of common global access patterns. A popular attempt to define a standard user-level application programming interface (API) called MPI-IO [91, 70] uses MPI-derived datatypes [24] to express the mapping of file data to processors. IBM's Vesta <ref> [16] </ref> (marketed as PIOFS) file system gives applications control over declustering that is geared towards two-dimensional distributions.
Reference: [17] <author> Corbett, P. F., and Feitelson, D. G. </author> <title> Design and Implementation of the Vesta Par--allel File System. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference (1994), </booktitle> <pages> pp. 63-70. </pages>
Reference-contexts: Unfortunately, this kind of guarantee usually restricts request concurrency and the overhead of synchronization can be expensive. Parallel file systems usually offer guaranteed atomicity as an option. For example, IBM PIOFS [4], the product version of Vesta <ref> [17] </ref> 1 , provides two modes for opening a subfile: NORMAL and CAUTIOUS the latter manages overlapping reads writes by ensuring their serialization, and therefore performance is somewhat worse.
Reference: [18] <author> Crandall, P. E., Aydt, R. A., Chien, A. A., and Reed, D. A. </author> <title> Characterization of a Suite of Input/Output Intensive Applications. </title> <booktitle> In Proceedings of Supercomputing '95 (Dec. </booktitle> <year> 1995). </year>
Reference-contexts: Input/output is due to compulsory reads of initialization data and writes to large checkpoint files. Therefore, sequential supercomputer input/output systems are typically optimized for high throughput. Uniprocessor input/output, whether from scientific applications or engineering workstations, is quite regular. In contrast, multiprocessor input/output access patterns exhibit greater variation <ref> [18, 81] </ref>, making it more difficult for file systems to optimize input/output. Furthermore, as processor speeds increase, it becomes computationally feasible to examine larger problems and new application areas, exacerbating the input/output bottleneck. <p> Furthermore, input/output requirements are a complex function of the interaction between system software and executing applications and may change unpredictably during program execution. 1.2 Application Input/Output Requirements Application level characterization studies <ref> [18, 81] </ref> demonstrate that there is wide variability in input/output access patterns, and moreover, performance is extremely sensitive to these variations. <p> Application areas include modeling of electron-molecule collisions, a 3-D numerical sim-ulation of the Navier-Stokes equations, an implementation of the Hartree-Fock self consistent field method to calculate the electron density around a molecule, and quantum chemical reaction dynamics <ref> [18, 81, 82] </ref>. These applications compose a snapshot of current input/output practices on scalable parallel machines and reflect the programmers' input/output design choices based on the capabilities of available input/output systems. They are but a small part of the Scalable Input/Output Initiative's (SIO) code suite [69]. <p> Experiments using large research codes on the Intel Paragon XP/S have shown that tuning PPFS file system policies to application needs, rather than forcing the application to use inappropriate and inefficient file access modes, is the key to performance <ref> [18] </ref>. Simple access pattern hints and cache policy controls yield large performance increases [31]. 4.1.2 PPFS Extensions The original PPFS interface provides the application with a rich set of manual file system policy controls and structured data access functions, but the rules guiding their use are ad hoc.
Reference: [19] <author> Crockett, T. W. </author> <title> File Concepts for Parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '89 (1989), </booktitle> <pages> pp. 574-579. </pages> <note> [20] del Rosario, </note> <author> J. M., Bordawekar, R., and Choudhary, A. </author> <title> Improved Parallel I/O via a Two-Phase Run-time Access Strategy. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems (1993), </booktitle> <pages> pp. 56-70. </pages> <note> Also published in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 31-38. </pages>
Reference-contexts: Unfortunately, scientific workload characterizations of supercomputers with sequential input/output do not automatically apply to multiprocessor systems, where the input/output requests themselves are parallelized over multiple disks. Furthermore, until relatively recently, little emphasis has been placed on characterizing parallel input/output access patterns. Crock-ett <ref> [19] </ref> and Kotz [43] have proposed taxonomies of access patterns (e.g., sequential, strided, and global equivalents), but experimental evidence to demonstrate the importance of these patterns is still scant. Few input/output bound multiprocessor applications exist due to inadequate file system performance; those that do exhibit architecture-centric access patterns.
Reference: [21] <author> Engler, D. R., Kaashoek, M. F., and Jr., J. O. Exokernel: </author> <title> An Operating System Architecture for Application-Level Resource Management. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles (December 1995). </booktitle>
Reference-contexts: policy control and access pattern information supplied at various levels of the application programming interface. 2.2.1 Explicit Policy Control Application specification of resource management policies has proven valuable in many domains, and many modern microkernels now support user control or extension of operating system services (e.g., SPIN [8] and exokernel <ref> [21] </ref>). SPIN achieves application resource control by allowing applications to define extension services that are dynamically linked into the kernel virtual address space.
Reference: [22] <author> Feitelson, D. G., Corbett, P. F., and Prost, J.-P. </author> <title> Performance of the Vesta Parallel File System. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium (April 1995), </booktitle> <pages> pp. 150-158. </pages>
Reference-contexts: At the other extreme, Vesta relies heavily upon buffer cache management to overcome temporal access pattern variations (e.g., aggregate read requests are recognized as sequential, enabling sequential prefetching, and out-of-order writes are buffered and written in larger segments) <ref> [22] </ref>. Caching and prefetching and dynamic recognition of global sequentiality are vital to Vesta performance. Asynchronous input/output gives the application the ability to overlap computation with input/output latency, by issuing input/output requests before they are needed. Intel PFS, IBM Vesta and MPI-IO all provide system calls for asynchronous input/output.
Reference: [23] <author> Feldman, G. C. </author> <title> SeaWiFS Project Homepage. </title> <institution> NASA Goddard Space Flight Center, </institution> <note> Available at http://seawifs.gsfc.nasa.gov/scripts/SEAWIFS.html, 1996. </note>
Reference-contexts: The goal of the SeaWiFS project is to provide a five year data set of global ocean color data to improve our understanding of the role of ocean primary production in the global carbon cycle <ref> [23] </ref>. Our input for this program is a simulated data set. The l0to1a application from the SeaWiFS processing software converts raw (level 0) satellite data to level 1A data products.
Reference: [24] <author> Forum, M. P. I. </author> <title> MPI: A Message-Passing Interface Standard, </title> <note> 1995. Available athttp://www.mcs.anl.gov/mpi/mpi-report-1.1/mpi-report.html. </note>
Reference-contexts: For example, Intel PFS [32] has file access modes that describe a variety of common global access patterns. A popular attempt to define a standard user-level application programming interface (API) called MPI-IO [91, 70] uses MPI-derived datatypes <ref> [24] </ref> to express the mapping of file data to processors. IBM's Vesta [16] (marketed as PIOFS) file system gives applications control over declustering that is geared towards two-dimensional distributions. <p> For example, the appropriate mode for a global sequential access pattern, mode M GLOBAL, combines multiple input/output requests to identical file bytes into a single request. MPI-IO uses MPI <ref> [24] </ref> derived datatypes to express data partitioning between the logical file layout and the processors. Server-side caching and prefetching are also important to performance, because they provide the ability to hide or reduce input/output latency without application intervention.
Reference: [25] <author> Griffioen, J., and Appleton, R. </author> <title> Reducing File System Latency Using a Predictive Approach. </title> <booktitle> In Proceedings of USENIX Summer Technical Conference (June 1994), </booktitle> <pages> pp. 197-207. </pages>
Reference-contexts: Some intelligent approaches to policy control have focused on construction of models of file access that guide prefetchers. Some approaches are purely statistical; in automatic prefetch-ing <ref> [25] </ref> the operating system creates a probability graph based on frequency counts to allow prefetching of files that are likely to be accessed soon after other files.
Reference: [26] <author> Grimshaw, A. S., and Loyot, Jr., E. C. </author> <title> ELFS: Object-oriented Extensible File Systems. </title> <booktitle> In Proceedings of the First International Conference on Parallel and Distributed Information Systems (December 1991), p. </booktitle> <volume> 177. </volume> <pages> 144 </pages>
Reference-contexts: File access patterns are thus an important factor in obtaining high throughput from both sequential and parallel file systems. Many studies have shown that file systems can utilize knowledge of input/output access patterns to improve input/output performance by selecting policies appropriate for the resource demands <ref> [26, 41, 68, 46] </ref>. <p> Ideally the programmer should not have to tune the request size for each platform. Studies have demonstrated that file system policies that exploit knowledge of application access patterns can provide better performance than a default set of strategies <ref> [26, 41, 68] </ref>. For example, on detection of random input/output access patterns, an adaptive file system might eschew the caching and sequential prefetching normally used for sequential accesses. <p> Many studies have shown this under different workloads and environments <ref> [41, 46, 47, 26] </ref>. Small input/output requests are best managed by aggregation, prefetching, caching, and write-behind, though large requests are better served by streaming data directly to or from storage devices and application buffers. 11 There are several very closely related approaches to application policy control. <p> The file system can invoke (possibly system dependent) policies that optimize input/output accordingly. For example, the ELFS system <ref> [26] </ref> proposes an object-oriented interface where object classes represent particular file access patterns. A particularly important kind of optimization that has been targeted by this approach is collective input/output.
Reference: [27] <author> Henderson, R. D. </author> <title> Unstructured Spectral Element Methods: Parallel Algorithms and Simulations. </title> <type> PhD thesis, </type> <month> June </month> <year> 1994. </year>
Reference-contexts: A classification for all processors for access to this file can be made within the first time window, at approximately 1500 seconds. 6.3.4.2 PRISM Computational fluid dynamics is another application area we have examined. PRISM is a parallel implementation of a 3-D numerical simulation of the Navier-Stokes equations <ref> [27, 28] </ref>. The 102 parallelization is by apportioning slides of the periodic domain to the nodes, with a combination of spectral elements and Fourier modes used to investigate the dynamics and transport properties of turbulent flow. (m16.rst, m16.rea and m16.mor); each file is accessed with a global sequential access pattern. <p> Our selected 112 applications are chosen from the Scalable Input/Output Initiative suite. Their input/output demands are characteristic of many scientific applications. 7.3.1 PRISM PRISM, a computational fluid dynamics code, is a parallel implementation of a 3-D numerical simulation of the Navier-Stokes equations <ref> [27, 28] </ref>. The parallelization is by apportioning the periodic domain to the processors, with a combination of spectral elements and Fourier modes used to investigate the dynamics and transport properties of turbulent flow. We focus on the first phase, in which every processor reads three initialization files.
Reference: [28] <author> Henderson, R. D., and Karniadakis, G. E. </author> <title> Unstructured Spectral Element Methods for Simulation of Turbulent Flows. </title> <journal> Journal of Computational Physics 122, </journal> <volume> 2 (1995), </volume> <pages> 191-217. </pages>
Reference-contexts: A classification for all processors for access to this file can be made within the first time window, at approximately 1500 seconds. 6.3.4.2 PRISM Computational fluid dynamics is another application area we have examined. PRISM is a parallel implementation of a 3-D numerical simulation of the Navier-Stokes equations <ref> [27, 28] </ref>. The 102 parallelization is by apportioning slides of the periodic domain to the nodes, with a combination of spectral elements and Fourier modes used to investigate the dynamics and transport properties of turbulent flow. (m16.rst, m16.rea and m16.mor); each file is accessed with a global sequential access pattern. <p> Our selected 112 applications are chosen from the Scalable Input/Output Initiative suite. Their input/output demands are characteristic of many scientific applications. 7.3.1 PRISM PRISM, a computational fluid dynamics code, is a parallel implementation of a 3-D numerical simulation of the Navier-Stokes equations <ref> [27, 28] </ref>. The parallelization is by apportioning the periodic domain to the processors, with a combination of spectral elements and Fourier modes used to investigate the dynamics and transport properties of turbulent flow. We focus on the first phase, in which every processor reads three initialization files.
Reference: [29] <institution> High Performance Computational Chemistry Group, Pacific Northwest National Laboratory. NWChem, </institution> <note> A Computational Chemistry Package for Parallel Compu ters, Version 1.1. </note> <institution> Richland, </institution> <address> Washington, 99352, USA, </address> <year> 1995. </year>
Reference-contexts: All of these applications run on the Intel Paragon. RENDER [53] is a parallel application that generates three-dimensional views of planetary surfaces from satellite data and terrain elevation data. HTF <ref> [29] </ref>, a version of the Hartree Fock algorithm, calculates interactions between subatomic particles. ESCAT [93] implements the Schwinger multichannel method for calculating low-energy electron-molecule collisions. <p> For example, two parallel implementations of the Hartree-Fock algorithm <ref> [29] </ref> exhibit this pattern. Hartree-Fock computes the electron density around a molecule by considering each electron in the molecule in the collective field of the others. The code iterates until all the electron fields are consistent.
Reference: [30] <author> Hinton, G. E. </author> <title> Connectionist Learning Procedures. </title> <booktitle> Artificial Intelligence 40 (1989), </booktitle> <volume> 185 - 234. </volume>
Reference-contexts: Exceptions will occur for any selected heuristics, and these will have to be identified and added by hand. Instead, we have investigated more sophisticated classification approaches that "learn" to classify access patterns. One approach is to train a feedforward artificial neural network <ref> [30] </ref> (ANN) 1 to classify patterns. We provide the neural network with examples of access patterns and their corresponding classifications; once trained, the network can correctly classify new access patterns. Our ANN classifier operates within certain domain-specific assumptions and constraints. <p> In other words, if the file has different classifications during separate executions, the probability of each classification must be maintained externally. 3.4 Neural Network Classification Our first classification approach was to train a feedforward artificial neural network <ref> [30] </ref> to classify patterns. Although neural networks are expensive to train initially, once training is complete, classification is very efficient.
Reference: [31] <author> Huber, J., Elford, C. L., Reed, D. A., Chien, A. A., and Blumenthal, D. S. </author> <title> PPFS: A High Performance Portable Parallel File System. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing (Barcelona, </booktitle> <month> July </month> <year> 1995), </year> <pages> pp. 385-394. </pages>
Reference-contexts: APIs can encapsulate information that allows optimizations that are impossible otherwise. For example, another example of an API-specified access pattern is the Read Any mode of the Portable Parallel File System (PPFS) <ref> [31] </ref>. Using this mode, an application specifies that the processors want to collectively read disjoint records of the entire file, but in no particular order. <p> Simple access pattern hints and cache policy controls yield large performance increases <ref> [31] </ref>. 4.1.2 PPFS Extensions The original PPFS interface provides the application with a rich set of manual file system policy controls and structured data access functions, but the rules guiding their use are ad hoc. <p> Significant speedups can be obtained by structuring this database so that processors can access any record that has not yet been requested, without regard to byte ordering, and by exploiting this information before issuing physical input/output requests <ref> [31] </ref>. 89 Although few experimental file systems currently support any form of "relaxed" file ordering, collective input/output interfaces are more commonplace.
Reference: [32] <institution> Paragon XP/S Product Overview. Intel Corporation, </institution> <year> 1991. </year>
Reference-contexts: In a global sequential interleaved access pattern, processors access disjoint interleaved blocks in node order. Hints may be specific input/output modes to describe common partitioning strategies, or they may be implicitly specified through the use of particular data distributions. For example, Intel PFS <ref> [32] </ref> has file access modes that describe a variety of common global access patterns. A popular attempt to define a standard user-level application programming interface (API) called MPI-IO [91, 70] uses MPI-derived datatypes [24] to express the mapping of file data to processors. <p> For example, IBM PIOFS [4], the product version of Vesta [17] 1 , provides two modes for opening a subfile: NORMAL and CAUTIOUS the latter manages overlapping reads writes by ensuring their serialization, and therefore performance is somewhat worse. Intel PFS <ref> [32] </ref> has analogous modes M ASYNC and M UNIX; in mode M UNIX reads and writes to the same file are atomic, but mode M ASYNC allows multiple readers and writers concurrent file access. <p> In a parallel file system, file data must be distributed over disks so as to provide the best performance for the global file access pattern; this is a function of balancing disk load and exploiting input/output bandwidth. Most modern parallel file systems simply stripe data cyclicly across the disks <ref> [32] </ref>, a strategy that usually results in balanced input/output loads. However, limiting the number and increasing the parallelism of input/output requests by matching the file declustering strategy to the access pattern can significantly improve performance. File systems vary greatly in the level of control they provide over declustering. <p> In 7.2 we test our experimental environment on simple global access pattern benchmarks. Finally, we examine performance of real applications in 7.3. 7.1 Experimental Environment Our experimental platform is the Intel Paragon; we use PPFS to perform physical input/output using PFS <ref> [32] </ref> as the underlying file system. PFS is a parallel file system that stripes data over disks on input/output nodes using a default 64 KB stripe size. In normal usage, applications provide access pattern information by specifying PFS modes and have limited control over input/output node buffering.
Reference: [33] <author> Joseph, D., and Grunwald, D. </author> <title> Prefetching using Markov Predictors. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture (June 1997), </booktitle> <pages> pp. 252-263. </pages>
Reference-contexts: The SEER system [49] correlates files by "semantic distance" to decide what files to cache in a portable computing environment. Although our focus is on file prefetching, a statistical model based on Markov predictors has been proposed for prefetching between on-chip and off-chip cache <ref> [33] </ref>. Memory references are modeled as a Markov process that is continually rebuilt as the program executes. Multiple memory references are prefetched at once. To simplify the model, the fan-out from each state is limited, and the transition probabilities are approximated using an LRU mechanism.
Reference: [34] <author> Kasabov, N. K. </author> <title> Foundations of Neural Networks, Fuzzy Systems, and Knowledge Engineering. </title> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: If it is not, or if most of the performance improvements made possible by dynamic adaptation can be expressed by simple rules, the information should be codified in simple ruleset. One method we are currently investigating for writing such a ruleset is fuzzy logic <ref> [96, 34] </ref>. Fuzzy variables and sets allow a ruleset to encapsulate approximate rules. For example, in the following rule "If accesses are small and sequential prefetch aggressively," the italicized concepts are fuzzy variables.
Reference: [35] <author> Kimbrel, T., Tomkins, A., Patterson, R. H., Bershad, B., Cao, P., Felten, E. W., Gibson, G. A., Karlin, A. R., and Li, K. </author> <title> A Trace-Driven Comparison of Algorithms for Parallel Prefetching and Caching. </title> <booktitle> In Proceedings of the USENIX Association Second Symposium on Operating Systems Design and Implementation (1996), </booktitle> <pages> pp. 19-34. </pages>
Reference-contexts: A cost-benefit analysis evaluates the reduction in input/output service time per buffer access to decide when to allocate buffers. This is an elegant approach for dealing with multiple applications that compete for shared caching and prefetching buffers. Kimbrel et al <ref> [35] </ref> and Tomkins et al [92] compare several integrated caching and prefetching policies using hints for systems with multiple disks. In parallel file systems, the way that data is partitioned among processors is closely linked to the access pattern. <p> Our policy selection methodology has treated caching and prefetching policies as separate entities; however, they are closely related to each other and are often fully integrated in current research literature that investigates prefetching for multiple disks <ref> [35, 92] </ref>. By using access pattern hints to determine the value of allocating prefetch buffers compared to cache buffers, prefetching decisions are tightly integrated with cache replacement policies.
Reference: [36] <author> Kleinrock, L. </author> <title> Queueing Systems, Vol. 1, Theory. </title> <publisher> John Wiley, </publisher> <year> 1975. </year>
Reference-contexts: In x3.5.2-x3.5.4 we describe our approach to modeling input/output access patterns using HMMs. In x3.5.5 we outline the HMM training process. Finally, in x3.5.6 we describe how to use HMMs for classification. 3.5.1 Hidden Markov Models A discrete-time Markov process <ref> [36] </ref> is a system that at any time is in one of N distinct states. At discrete times, the system changes states, or makes a transition, according to a set of probabilities associated with each state. Each state corresponds to a single observable event, or an observation symbol.
Reference: [37] <author> Korner, K. </author> <title> Intelligent Caching for Remote File Service. </title> <booktitle> In Proceedings of the 10th International Conference on Distributed Computing Systems (May 1990), </booktitle> <pages> pp. 220-226. </pages>
Reference-contexts: Fido is an example of a predictive cache that prefetches by using an associative memory to recognize access patterns over time [62]. Knowledge based caching has been proposed to enhance cache performance of remote file servers <ref> [37] </ref>. Mowry et al [56] propose a method for using input/output access patterns determined by the compiler to hide input/output latency by prefetching, assuming the application relies upon paged virtual memory for all input/output. These studies focus on application input/output, but the method can also be applied to implicit input/output.
Reference: [38] <author> Kotz, D. </author> <title> Prefetching and Caching Techniques in File Systems for MIMD Multiprocessors. </title> <type> PhD thesis, </type> <institution> Duke University, </institution> <month> April </month> <year> 1991. </year> <note> Available as technical report CS-1991-016. 145 </note>
Reference-contexts: For each of these input test patterns, we computed input/output statistics on windows of ten accesses. Kotz <ref> [38] </ref> suggests that a smaller number of accesses suffices to predict sequential runs; however, we wish to recognize strided and random patterns, implying that a larger number of accesses is necessary. At least 3-4 accesses are necessary to detect a strided access pattern.
Reference: [39] <author> Kotz, D. </author> <title> Multiprocessor File System Interfaces. </title> <booktitle> In Proceedings of the Second International Conference on Parallel and Distributed Information Systems (1993), </booktitle> <pages> pp. 194-201. </pages>
Reference-contexts: Reordering and parallelization of input/output requests is impossible if they are artificially serialized within the file system (e.g., by file pointers). This is one of the primary reasons that the UNIX file system interface, even with extensions, is generally deemed unsuitable as a parallel file system API <ref> [14, 39] </ref>, although it remains popular for compatibility reasons. For example, a program that can utilize subsections of a file in any order cannot express this flexibility through a file system with a byte stream interface.
Reference: [40] <author> Kotz, D. </author> <title> Disk-directed I/O for MIMD Multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation (November 1994), </booktitle> <pages> pp. 61-74. </pages> <note> Updated as Dartmouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference-contexts: Collective input/output operations alleviate this problem by requiring operations to be performed by every processor, permitting requests to be aggregated across processors and optimizing input/output accesses at the expense of explicit synchronization. Performance improvements have been demonstrated using collective input/output strategies including two-phase input/output [20] and disk-directed input/output <ref> [40] </ref>. In two-phase input/output, the application uses a particular interface to alert the file system to a collective operation. In a two-phase read, for example, data is read from disks in the most efficient manner and redistributed to the processors involved in the collective. <p> A variation of two-phase input/output called extended two-phase input/output [88] uses collective input/output in conjunction with dynamic partitioning of the input/output workload among processors to balance load and improve performance. Disk-directed input/output <ref> [40] </ref> is another method for optimizing collective requests. The complete collective request is passed to the input/output processors, which sort the list of transfers and use remote memory "put" and "get" messages to pipeline the transfer between application memory and disks. <p> Depending on the temporal and spatial distribution of the merged input/output requests, they can be reordered to obtain greater system throughput. This motivates optimizations such as two-phase [20] and collective input/output <ref> [40] </ref>; given global knowledge that some data must be read or written before all the processors can proceed, the input/output operations can occur as an efficient collective. Even when the collective operation is not performed explicitly, batching requests is a general principle that can be used to optimize input/output.
Reference: [41] <author> Kotz, D., and Ellis, C. S. </author> <title> Prefetching in File Systems for MIMD Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 1, </journal> <month> 2 (April </month> <year> 1990), </year> <pages> 218-230. </pages>
Reference-contexts: File access patterns are thus an important factor in obtaining high throughput from both sequential and parallel file systems. Many studies have shown that file systems can utilize knowledge of input/output access patterns to improve input/output performance by selecting policies appropriate for the resource demands <ref> [26, 41, 68, 46] </ref>. <p> Ideally the programmer should not have to tune the request size for each platform. Studies have demonstrated that file system policies that exploit knowledge of application access patterns can provide better performance than a default set of strategies <ref> [26, 41, 68] </ref>. For example, on detection of random input/output access patterns, an adaptive file system might eschew the caching and sequential prefetching normally used for sequential accesses. <p> Many studies have shown this under different workloads and environments <ref> [41, 46, 47, 26] </ref>. Small input/output requests are best managed by aggregation, prefetching, caching, and write-behind, though large requests are better served by streaming data directly to or from storage devices and application buffers. 11 There are several very closely related approaches to application policy control.
Reference: [42] <author> Kotz, D., and Ellis, C. S. </author> <title> Practical Prefetching Techniques for Parallel File Systems. </title> <booktitle> In Proceedings of the First International Conference on Parallel and Distributed Information Systems (December 1991), </booktitle> <pages> pp. 182-189. </pages>
Reference-contexts: Kotz has studied detecting more complicated access patterns exhibiting global sequentiality; this information is used to guide non-sequential prefetching within a file <ref> [42] </ref>. Fido is an example of a predictive cache that prefetches by using an associative memory to recognize access patterns over time [62]. Knowledge based caching has been proposed to enhance cache performance of remote file servers [37].
Reference: [43] <author> Kotz, D., and Ellis, C. S. </author> <title> Practical Prefetching Techniques for Multiprocessor File Systems. </title> <journal> Journal of Distributed and Parallel Databases 1, </journal> <month> 1 (January </month> <year> 1993), </year> <pages> 33-51. </pages>
Reference-contexts: Unfortunately, scientific workload characterizations of supercomputers with sequential input/output do not automatically apply to multiprocessor systems, where the input/output requests themselves are parallelized over multiple disks. Furthermore, until relatively recently, little emphasis has been placed on characterizing parallel input/output access patterns. Crock-ett [19] and Kotz <ref> [43] </ref> have proposed taxonomies of access patterns (e.g., sequential, strided, and global equivalents), but experimental evidence to demonstrate the importance of these patterns is still scant. Few input/output bound multiprocessor applications exist due to inadequate file system performance; those that do exhibit architecture-centric access patterns.
Reference: [44] <author> Kotz, D., and Nieuwejaar, N. </author> <title> Dynamic File-Access Characteristics of a Production Parallel Scientific Workload. </title> <booktitle> In Proceedings of Supercomputing '94 (November 1994), </booktitle> <pages> pp. 640-649. </pages>
Reference-contexts: Few input/output bound multiprocessor applications exist due to inadequate file system performance; those that do exhibit architecture-centric access patterns. Several studies attempt to address this dearth of characterization data by gathering application input/output access pattern information at the system and application level. The CHARISMA project <ref> [44, 60] </ref> has examined input/output accesses on the iPSC/860 Concurrent File System (CFS) and the CM5 Scalable Disk Array to obtain some generalizations of access patterns in parallel input/output workloads. They have observed predominantly write accesses, small request sizes, and generally sequential requests.
Reference: [45] <author> Krieger, O. </author> <title> HFS: A flexible file system for shared-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: Similarly, it is well-accepted that a parallel file system must be flexible to provide reasonable performance to a variety of applications. Several research file systems have been designed with explicit policy control in mind. HFS, the Hurricane File System <ref> [45, 47] </ref>, supports a variety of file structures, file system policies, and interfaces. HFS allows the file structure to be optimized for a particular access pattern, with commensurate prefetching, locking, and file cache management policies.
Reference: [46] <author> Krieger, O., and Stumm, M. </author> <title> HFS: A Flexible File System for large-scale Multiprocessors. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium (Hanover, </booktitle> <address> NH, </address> <month> June </month> <year> 1993), </year> <institution> Dartmouth Institute for Advanced Graduate Studies, </institution> <note> pp. 6-14. </note>
Reference-contexts: File access patterns are thus an important factor in obtaining high throughput from both sequential and parallel file systems. Many studies have shown that file systems can utilize knowledge of input/output access patterns to improve input/output performance by selecting policies appropriate for the resource demands <ref> [26, 41, 68, 46] </ref>. <p> Many studies have shown this under different workloads and environments <ref> [41, 46, 47, 26] </ref>. Small input/output requests are best managed by aggregation, prefetching, caching, and write-behind, though large requests are better served by streaming data directly to or from storage devices and application buffers. 11 There are several very closely related approaches to application policy control.
Reference: [47] <author> Krieger, O., and Stumm, M. </author> <title> HFS: A Performance-Oriented Flexible File System Based on Building-Block Compositions. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems (Philadelphia, </booktitle> <month> May </month> <year> 1996), </year> <pages> pp. 95-108. </pages>
Reference-contexts: Many studies have shown this under different workloads and environments <ref> [41, 46, 47, 26] </ref>. Small input/output requests are best managed by aggregation, prefetching, caching, and write-behind, though large requests are better served by streaming data directly to or from storage devices and application buffers. 11 There are several very closely related approaches to application policy control. <p> Similarly, it is well-accepted that a parallel file system must be flexible to provide reasonable performance to a variety of applications. Several research file systems have been designed with explicit policy control in mind. HFS, the Hurricane File System <ref> [45, 47] </ref>, supports a variety of file structures, file system policies, and interfaces. HFS allows the file structure to be optimized for a particular access pattern, with commensurate prefetching, locking, and file cache management policies.
Reference: [48] <author> Kroeger, T. M., and Long, D. D. E. </author> <title> Predicting File-System Actions From Prior Events. </title> <booktitle> In Proceedings of the USENIX 1996 Annual Technical Conference (Jan. </booktitle> <year> 1996), </year> <pages> pp. 319-328. </pages>
Reference-contexts: Some approaches are purely statistical; in automatic prefetch-ing [25] the operating system creates a probability graph based on frequency counts to allow prefetching of files that are likely to be accessed soon after other files. Kroeger and Long <ref> [48] </ref> use 15 a method based on data compression to create first, second, and higher order context models of file relationships. Lei and Duchamp [51] create file access trees for program executions and use recurrent access patterns heuristically to guide prefetching.
Reference: [49] <author> Kuenning, G. H. </author> <title> The Design of the SEER Predictive Caching System. </title> <booktitle> In Proceedings of Workshop on Mobile Computing Systems and Applications (Dec. </booktitle> <year> 1994), </year> <pages> pp. 37-43. </pages>
Reference-contexts: Kroeger and Long [48] use 15 a method based on data compression to create first, second, and higher order context models of file relationships. Lei and Duchamp [51] create file access trees for program executions and use recurrent access patterns heuristically to guide prefetching. The SEER system <ref> [49] </ref> correlates files by "semantic distance" to decide what files to cache in a portable computing environment. Although our focus is on file prefetching, a statistical model based on Markov predictors has been proposed for prefetching between on-chip and off-chip cache [33].
Reference: [50] <author> Kuppermann, A., and Wu, Y.-S. M. </author> <title> The Quantitative Prediction and Lifetime of a Pronounced Reactive Scattering Resonance. </title> <journal> Chemical Physics Letters 241 (1995), </journal> <pages> 229-240. </pages>
Reference-contexts: Total open time is comparable for both experiments, implying that the cost of reading and broadcasting a small initialization file is little more expensive than the cost of synchronizing the processors to open the file. 7.3.2 QCRD QCRD <ref> [94, 50] </ref> is a quantum chemical reaction dynamics code used to study elementary chemical reactions. The code uses the method of symmetrical hyperspherical coordinates and local hyperspherical surface functions to solve the Schrodinger equation for the cross sections of the scattering of an atom by a diatomic molecule.
Reference: [51] <author> Lei, H., and Duchamp, D. </author> <title> An Analytical Approach to File Prefetching. </title> <booktitle> In Proceedings of the USENIX 1997 Annual Technical Conference (Jan. </booktitle> <year> 1997), </year> <pages> pp. 275-288. </pages>
Reference-contexts: Kroeger and Long [48] use 15 a method based on data compression to create first, second, and higher order context models of file relationships. Lei and Duchamp <ref> [51] </ref> create file access trees for program executions and use recurrent access patterns heuristically to guide prefetching. The SEER system [49] correlates files by "semantic distance" to decide what files to cache in a portable computing environment.
Reference: [52] <author> Li, P. </author> <title> ESS Grand Challenge Projects: The Earth's Interior Modeling and the Global Climate Modeling. </title> <note> In HPCC ANNUAL REPORT (1995). available at http://olympic.jpl.nasa.gov/PERSONNEL/wangp/ping96one.html. </note>
Reference-contexts: In particular, much research effort has been devoted to optimizing access to complex matrix decompositions through collective operations [13, 79, 7]. In this experiment we consider a benchmark based on access patterns exhibited by applications from Caltech for global climate modeling and modeling of the earth's interior <ref> [52] </ref>. These applications read and write data from a two or three dimensional grid, partitioning data in blocks of contiguous particles among the processors. block contains cs 2 elements allocated to the processor with that number. Processors simultaneously read their allocated chunks in segments of the particle size.
Reference: [53] <author> Li, P., Curkendall, D., Duquette, W., and Henry, H. </author> <title> Remote Interactive Visualization and Analysis (RIVA) Using Parallel Supercomputers. </title> <booktitle> In Proceedings of the 1995 Parallel Rendering Symposium (October 1995). </booktitle>
Reference-contexts: All of these applications run on the Intel Paragon. RENDER <ref> [53] </ref> is a parallel application that generates three-dimensional views of planetary surfaces from satellite data and terrain elevation data. HTF [29], a version of the Hartree Fock algorithm, calculates interactions between subatomic particles. ESCAT [93] implements the Schwinger multichannel method for calculating low-energy electron-molecule collisions.
Reference: [54] <author> Michael Berry, e. a. </author> <title> The Perfect Club Benchmarks: Performance Evaluation of Supercomputers. </title> <journal> The International Journal of Supercomputer Applications 3, </journal> <volume> 3 (1989), </volume> <pages> 5-40. </pages> <month> Fall. </month>
Reference-contexts: Reddy and Banerjee [72] characterized implicit and explicit input/output activity of five par-allelized applications from the Perfect benchmark suite <ref> [54] </ref> and also found it to be mostly sequential; input/output itself was not parallelized.
Reference: [55] <author> Miller, E. L., and Katz, R. H. </author> <title> Input/Output Behavior of Supercomputer Applications. </title> <booktitle> In Proceedings of Supercomputing '91 (November 1991), </booktitle> <pages> pp. 567-576. </pages>
Reference-contexts: Because the workload is relatively uniform, a single set of policies provides good performance for the majority of accesses. Supercomputer input/output is dominated by accesses to very large, sequential files <ref> [55, 64] </ref>. Input/output is due to compulsory reads of initialization data and writes to large checkpoint files. Therefore, sequential supercomputer input/output systems are typically optimized for high throughput. Uniprocessor input/output, whether from scientific applications or engineering workstations, is quite regular. <p> Uniprocessor workloads are well characterized. Studies of typical engineering workloads on workstations under UNIX 4.2 BSD [61] and the Sprite distributed file system [3] reveal that most file accesses are small, sequential, and read only. Scientific workloads on sequential supercomputers have also been extensively studied. Miller and Katz <ref> [55] </ref> characterized the behavior of seven computational fluid dynamics and climate applications. File accesses were sequential and input/output was bursty due to the iterative nature of the underlying algorithms. They identified three main categories of input/output 9 requests: compulsory, cyclic checkpointing, and data staging. <p> We consider an abstract input/output architecture that consists of multiple input/output devices attached to separate processors on an interconnection network. Files may be declustered over many disks. We also limit our focus to compulsory file input/output. The Miller and Katz <ref> [55] </ref> study identified three reasons for input/output in scientific applications: compulsory (input/output to load data into memory for processing and write results), checkpointing data, and data staging. When staging data, the CPU may not need to access the data, because it is simply transferring it between devices.
Reference: [56] <author> Mowry, T. C., Demke, A. K., and Krieger, O. </author> <title> Automatic compiler-inserted I/O prefetching for out-of-core applications. </title> <booktitle> In Proceedings of the 1996 Symposium on Operating Systems Design and Implementation (October 1996), USENIX Association, </booktitle> <pages> pp. 3-17. </pages>
Reference-contexts: Fido is an example of a predictive cache that prefetches by using an associative memory to recognize access patterns over time [62]. Knowledge based caching has been proposed to enhance cache performance of remote file servers [37]. Mowry et al <ref> [56] </ref> propose a method for using input/output access patterns determined by the compiler to hide input/output latency by prefetching, assuming the application relies upon paged virtual memory for all input/output. These studies focus on application input/output, but the method can also be applied to implicit input/output.
Reference: [57] <author> NCSA. </author> <note> NCSA HDF, Version 2.0. </note> <institution> University of Illinois at Urbana-Champaign, National Center for Supercomputing Applications, </institution> <month> Feb. </month> <year> 1989. </year>
Reference-contexts: Many patterns are complex but repeatable, such as those generated by application calls to libraries such as the NCSA hierarchical data format (HDF) <ref> [57] </ref>. For example, often a process reads metadata from a structured file (small, localized accesses with irregular strides) and then 36 accesses a data array within the file (with large sequential or strided accesses). <p> One of the major reasons for complex structured access patterns is the use of higher level input/output libraries that hide file structure from the application. For example, NCSA's hierarchical data format (HDF) <ref> [57] </ref> is a platform independent data format for exchange of scientific data that has been adopted as a standard format for storing satellite data from NASA's Mission to Planet Earth project. <p> However, strided and irregular access patterns are common when input/output is encapsulated by an input/output library. In this section we examine access patterns produced using the NCSA Hierarchical Data Format (HDF) <ref> [57] </ref>. HDF performance is important in its own right and is a key factor in the satellite data processing applications that we examine in x5.4. 5.3.1 HDF and Satellite Data Processing HDF is a self-documenting physical file format that facilitates data transfer between machines and operating systems.
Reference: [58] <author> Needleman, S. B., and Wunsch, C. D. </author> <title> An Efficient Method Applicable to the Search for Similarities in the Amino Acid Sequences of Two Proteins. </title> <journal> Journal of Molecular Biology 48 (1970), </journal> <pages> 444-453. </pages>
Reference-contexts: Relaxing this constraint for applications can dramatically improve performance by allowing the file system to balance load and optimize throughput. Consider the Needleman, Wunch and Sellers (NWS) <ref> [58] </ref> dynamic programming algorithm for parallel genome matching, which is implemented by having each processor independently compare a test genome sequence against disjoint portions of the sequence database.
Reference: [59] <author> Nieuwejaar, N., and Kotz, D. </author> <title> The Galley Parallel File System. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing (May 1996). To appear. </booktitle> <pages> 147 </pages>
Reference-contexts: HFS, the Hurricane File System [45, 47], supports a variety of file structures, file system policies, and interfaces. HFS allows the file structure to be optimized for a particular access pattern, with commensurate prefetching, locking, and file cache management policies. Galley <ref> [59] </ref> is a flexible file system with a well defined low-level input/output interface, upon which multiple high-level input/output interfaces can be written. The second generation Galley file system goes a step further, allowing user code to be executed on input/output nodes.
Reference: [60] <author> Nieuwejaar, N., Kotz, D., Purakayastha, A., Ellis, C. S., and Best, M. </author> <title> File--Access Characteristics of Parallel Scientific Workloads. </title> <type> Tech. Rep. </type> <institution> PCS-TR95-263, Dept. of Computer Science, Dartmouth College, </institution> <month> August </month> <year> 1995. </year> <note> To appear in IEEE TPDS. </note>
Reference-contexts: Few input/output bound multiprocessor applications exist due to inadequate file system performance; those that do exhibit architecture-centric access patterns. Several studies attempt to address this dearth of characterization data by gathering application input/output access pattern information at the system and application level. The CHARISMA project <ref> [44, 60] </ref> has examined input/output accesses on the iPSC/860 Concurrent File System (CFS) and the CM5 Scalable Disk Array to obtain some generalizations of access patterns in parallel input/output workloads. They have observed predominantly write accesses, small request sizes, and generally sequential requests.
Reference: [61] <author> Ousterhout, J., and DaCosta, H. </author> <title> A Trace -Driven Analysis of the Unix 4.2BSD File System. </title> <booktitle> ACM Operating Systems Review 19, </booktitle> <month> 5 (December </month> <year> 1985), </year> <pages> 15-24. </pages>
Reference-contexts: Uniprocessor workloads are well characterized. Studies of typical engineering workloads on workstations under UNIX 4.2 BSD <ref> [61] </ref> and the Sprite distributed file system [3] reveal that most file accesses are small, sequential, and read only. Scientific workloads on sequential supercomputers have also been extensively studied. Miller and Katz [55] characterized the behavior of seven computational fluid dynamics and climate applications.
Reference: [62] <author> Palmer, M., and Zdonik, S. B. </author> <title> Fido: A Cache That Learns to Fetch. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Data Bases (Barcelona, </booktitle> <month> September </month> <year> 1991), </year> <pages> pp. 255-262. </pages>
Reference-contexts: Kotz has studied detecting more complicated access patterns exhibiting global sequentiality; this information is used to guide non-sequential prefetching within a file [42]. Fido is an example of a predictive cache that prefetches by using an associative memory to recognize access patterns over time <ref> [62] </ref>. Knowledge based caching has been proposed to enhance cache performance of remote file servers [37]. Mowry et al [56] propose a method for using input/output access patterns determined by the compiler to hide input/output latency by prefetching, assuming the application relies upon paged virtual memory for all input/output.
Reference: [63] <author> Park, Y., Scott, R., and Sechrest, S. </author> <title> Virtual Memory Versus File Interfaces for Large, </title> <booktitle> Memory-intensive Scientific Applications. In Proceedings of Supercomputing '96 (November 1996), </booktitle> <institution> ACM Press and IEEE Computer Society Press. </institution> <note> Also available as UH Department of Computer Science Research Report UH-CH-96-7. </note>
Reference-contexts: Database systems have historically bypassed native file systems, seizing control of the raw resources in order to provide high performance. Park et al suggest using custom virtual memory page replacement policies to optimize out-of-core input/output <ref> [63] </ref>. Cao et al recommend that applications use their own page replacement policy and describe an approach to buffer allocation to support application-controlled page replacement [9, 10]. Similarly, it is well-accepted that a parallel file system must be flexible to provide reasonable performance to a variety of applications.
Reference: [64] <author> Pasquale, B. K., and Polyzos, G. C. </author> <title> A Static Analysis of I/O Characteristics of Scientific Applications in a Production Workload. </title> <booktitle> In Proceedings of Supercomputing '93 (1993), </booktitle> <pages> pp. 388-397. </pages>
Reference-contexts: Because the workload is relatively uniform, a single set of policies provides good performance for the majority of accesses. Supercomputer input/output is dominated by accesses to very large, sequential files <ref> [55, 64] </ref>. Input/output is due to compulsory reads of initialization data and writes to large checkpoint files. Therefore, sequential supercomputer input/output systems are typically optimized for high throughput. Uniprocessor input/output, whether from scientific applications or engineering workstations, is quite regular. <p> File accesses were sequential and input/output was bursty due to the iterative nature of the underlying algorithms. They identified three main categories of input/output 9 requests: compulsory, cyclic checkpointing, and data staging. Pasquale and Polyzos <ref> [64, 65] </ref> analyzed input/output intensive applications on the Cray Y-MP and C90 at the San Diego Supercomputer Center; they concluded that their behavior over time was regular and cyclic.
Reference: [65] <author> Pasquale, B. K., and Polyzos, G. C. </author> <title> Dynamic I/O Characterization of I/O Intensive Scientific Applications. </title> <booktitle> In Proceedings of Supercomputing '94 (November 1994), </booktitle> <pages> pp. 660-669. </pages>
Reference-contexts: File accesses were sequential and input/output was bursty due to the iterative nature of the underlying algorithms. They identified three main categories of input/output 9 requests: compulsory, cyclic checkpointing, and data staging. Pasquale and Polyzos <ref> [64, 65] </ref> analyzed input/output intensive applications on the Cray Y-MP and C90 at the San Diego Supercomputer Center; they concluded that their behavior over time was regular and cyclic.
Reference: [66] <author> Patterson, and Hennessy. </author> <title> Computer Organization and Design: The Hardware/Software Interface. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1994. </year>
Reference-contexts: file are usually organized sequentially or close together on disk to avoid costly seeks between accesses. 2 Studies of typical UNIX workstation environments show that 90% of file accesses are to sequential data, 80% of file accesses are to files smaller than 10 KB, and 67% of accesses are reads <ref> [66] </ref>. Large caches can satisfy many of the small read only requests and sequential prefetching can hide input/output latency for this workload. Because the workload is relatively uniform, a single set of policies provides good performance for the majority of accesses.
Reference: [67] <author> Patterson, and Hennessy. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1996. </year>
Reference-contexts: Automatic classification and policy selection relieve the programmer of responsibility for input/output performance while increasing code and performance portability. 1.1 The Input/Output Problem Because of their cost effectiveness, magnetic disks have dominated secondary memory design since 1965 <ref> [67] </ref>. A magnetic hard disk consists of a set of platters that rotate on a spindle. Each disk surface is divided into concentric circles called tracks. An arm with a magnetic read/write head moves just above the surface to access data from the tracks. <p> Each disk surface is divided into concentric circles called tracks. An arm with a magnetic read/write head moves just above the surface to access data from the tracks. There are four main components to each disk access <ref> [67] </ref>. The first is seek time to position the head over the correct track. Once positioned, the head must wait until the the requested data moves beneath the head; this is the rotational latency. The third component is the time to transfer the data, or transfer time. <p> Therefore, we arbitrarily chose 1 MB to be the default cache size. File cache sizes can vary across operating systems from a small fixed percentage of main memory (e.g. 10% for Ultrix) to a dynamic allocation that can be as high as 74% for SunOS 4.1 <ref> [67] </ref>. Our control over memory allocated to a particular file's cache is limited to two sizes, small (the default size) and large.
Reference: [68] <author> Patterson, R. H., Gibson, G. A., Ginting, E., Stodolsky, D., and Zelenka, J. </author> <title> Informed Prefetching and Caching. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles (December 1995), </booktitle> <pages> pp. 79-95. </pages>
Reference-contexts: File access patterns are thus an important factor in obtaining high throughput from both sequential and parallel file systems. Many studies have shown that file systems can utilize knowledge of input/output access patterns to improve input/output performance by selecting policies appropriate for the resource demands <ref> [26, 41, 68, 46] </ref>. <p> Ideally the programmer should not have to tune the request size for each platform. Studies have demonstrated that file system policies that exploit knowledge of application access patterns can provide better performance than a default set of strategies <ref> [26, 41, 68] </ref>. For example, on detection of random input/output access patterns, an adaptive file system might eschew the caching and sequential prefetching normally used for sequential accesses. <p> Hints need not be specified in order for the file system to work, but they can improve performance if provided. Patterson et al demonstrate the success of providing hints to guide prefetching and caching of files that will be accessed in the future <ref> [68] </ref>. The application reveals future accesses through hints, allowing the file system to exploit input/output bandwidth. A cost-benefit analysis evaluates the reduction in input/output service time per buffer access to decide when to allocate buffers. <p> This advance knowledge of input/output requirements must be provided to the file system (e.g., as hints). This is the motivation behind Transparent Informed Prefetching (TIP) <ref> [68] </ref> and dynamic sets [85]; the application reveals future accesses allowing the file system to use parallelism within the request stream to exploit input/output bandwidth and improve performance. 87 The same principles apply for exploiting parallelism across multiple input/output streams.
Reference: [69] <author> Poole, J. T. </author> <title> Preliminary Survey of I/O Intensive Applications. </title> <type> Tech. Rep. CCSF-38, </type> <institution> Scalable I/O Initiative, Caltech Concurrent Supercomputing Facilities, Caltech, </institution> <year> 1994. </year> <month> 148 </month>
Reference-contexts: These applications compose a snapshot of current input/output practices on scalable parallel machines and reflect the programmers' input/output design choices based on the capabilities of available input/output systems. They are but a small part of the Scalable Input/Output Initiative's (SIO) code suite <ref> [69] </ref>. Thakur et al [90, 89] used Pablo to characterize the behavior of a three-dimensional parallel astrophysics code running on PIOFS on the IBM SP/2 and PFS on the Paragon. This application uses several three-dimensional arrays that are allocated to processors using a (block,block,block) distribution across three dimensions.
Reference: [70] <author> Prost, J.-P., Snir, M., Corbett, P., and Feitelson, D. </author> <title> MPI-IO, A Message-Passing Interface for Concurrent I/O. </title> <type> Tech. Rep. RC 19712 (87394), </type> <institution> IBM T.J. Watson Research Center, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: For example, Intel PFS [32] has file access modes that describe a variety of common global access patterns. A popular attempt to define a standard user-level application programming interface (API) called MPI-IO <ref> [91, 70] </ref> uses MPI-derived datatypes [24] to express the mapping of file data to processors. IBM's Vesta [16] (marketed as PIOFS) file system gives applications control over declustering that is geared towards two-dimensional distributions.
Reference: [71] <author> Rabiner, L. R. </author> <title> A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. </title> <booktitle> Proceedings of the IEEE 77, </booktitle> <month> 2 </month> <year> (1989). </year>
Reference-contexts: A probabilistic model of file accesses can supply more detailed access pattern information for caching and prefetching policy decisions. To create such a model automatically, we use file access data from previous executions to train extended Markov models, or hidden Markov models (HMMs) <ref> [71, 11] </ref>. HMMs are commonly used in speech recognition software, where the goal is to identify sub-words, words, or syntax based on probabilistic models. At a very high level, access pattern classification is similar to speech recognition, and similar techniques apply. <p> Such input/output patterns defy simple qualitative classification, but they have definite structure that does not change between executions. Qualitative classification does not lend itself to description of more complicated access patterns. Hidden Markov models <ref> [71] </ref> use data from previous executions to model access patterns probabilistically. HMMs are commonly used in speech recognition software, where the goal is to identify sub-words, words, or syntax based on probabilistic models. <p> Note, however, that there is no crossover between the patterns. To obtain a better descriptive HMM, it is possible to retrain the composite HMM (with transition matrix A) with all the access patterns, using the classic Baum-Welch algorithm for training HMMs <ref> [71] </ref>. This composite HMM is no longer equivalent to a Markov model and cannot be trained by simply calculating transition probabilities; the Baum-Welch algorithm is an iterative procedure that adjusts the model parameters to maximize the likelihood of generating each of the sequences given the model.
Reference: [72] <author> Reddy, A. L. N., and Banerjee, P. </author> <title> A Study of I/O Behavior of Perfect Benchmarks on a Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture (1990), </booktitle> <pages> pp. 312-321. </pages>
Reference-contexts: Pasquale and Polyzos [64, 65] analyzed input/output intensive applications on the Cray Y-MP and C90 at the San Diego Supercomputer Center; they concluded that their behavior over time was regular and cyclic. Reddy and Banerjee <ref> [72] </ref> characterized implicit and explicit input/output activity of five par-allelized applications from the Perfect benchmark suite [54] and also found it to be mostly sequential; input/output itself was not parallelized.
Reference: [73] <author> Reed, D. A. </author> <title> Scalable Performance Environments for Parallel Systems. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computing Conference, to appear (Apr. </booktitle> <year> 1991). </year>
Reference-contexts: The CHARISMA characterization effort focused on system-level input/output; however, in-depth study of application-level input/output is also necessary to understand programmer input/output requirements. Many researchers have addressed this problem, studying the application behavior of a variety of scientific parallel applications. Using the the Pablo <ref> [73, 74] </ref> performance analysis software, researchers have characterized the application level behavior of a wide variety of parallel applications on the Intel Paragon 10 XP/S.
Reference: [74] <author> Reed, D. A., Aydt, R. A., Noe, R. J., Roth, P. C., Shields, K. A., Schwartz, B. W., and Tavera, L. F. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <editor> A. Skjellum, Ed. </editor> <publisher> IEEE Computer Society, </publisher> <year> 1993, </year> <pages> pp. 104-113. </pages>
Reference-contexts: The CHARISMA characterization effort focused on system-level input/output; however, in-depth study of application-level input/output is also necessary to understand programmer input/output requirements. Many researchers have addressed this problem, studying the application behavior of a variety of scientific parallel applications. Using the the Pablo <ref> [73, 74] </ref> performance analysis software, researchers have characterized the application level behavior of a wide variety of parallel applications on the Intel Paragon 10 XP/S.
Reference: [75] <author> Rew, R. K. </author> <note> netCDF User's Guide, Version 1.0. </note> <institution> Unidata Program Center, University Corporation for Atmospheric Research, </institution> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: For example, NCSA's hierarchical data format (HDF) [57] is a platform independent data format for exchange of scientific data that has been adopted as a standard format for storing satellite data from NASA's Mission to Planet Earth project. The National Space Science Data Center's Common Data Format (CDF) <ref> [75] </ref> is another self-describing data abstraction for storage and manipulation of multidimensional data. CDF has been adopted by the space physics community as a standard for data sharing and analysis.
Reference: [76] <author> Ruemmler, C., and Wilkes, J. </author> <title> Modelling Disks. </title> <type> Tech. Rep. </type> <institution> HPL-93-68, Hewlett Packard Laboratories, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Improvements in disk access times have not kept pace with microprocessor performance, which has been improving by 50% or more per year. Although magnetic-media densities have increased, reducing disk transfer times by approximately 60-80% per year <ref> [76] </ref>, overall improvement in disk access times, which rely upon advances in mechanical systems, has been less than 10% per year.
Reference: [77] <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <title> Learning Internal Representations by Error Propagation. In Parallel Distributed Processing, </title> <editor> D. E. Rumelhart and J. L. McClelland, Eds., </editor> <volume> vol. 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: This error is used together with the output of the source unit to compute the changes to the link. Errors are propagated backward through the network according to the formulas below, which are those for standard backpropagation as introduced in <ref> [77, 78] </ref>.
Reference: [78] <author> Rumelhart, D. E., and McClelland, J. </author> <title> Parallel Distributed Processing. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This error is used together with the output of the source unit to compute the changes to the link. Errors are propagated backward through the network according to the formulas below, which are those for standard backpropagation as introduced in <ref> [77, 78] </ref>.
Reference: [79] <author> Seamons, K. E., Chen, Y., Jones, P., Jozwiak, J., and Winslett, M. </author> <title> Server-Directed Collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95 (December 1995). </booktitle>
Reference-contexts: A variable number of coalescing processes aggregate the requests to perform them efficiently under the chosen distribution. PASSION [13, 87, 86] (Parallel And Scalable Software for Input-Output) is another runtime library targeted for SPMD applications with routines to efficiently perform out-of-core operations. The Panda <ref> [79, 12] </ref> library utilizes server-directed input/output and a collective interface to achieve high performance on array accesses. APIs can encapsulate information that allows optimizations that are impossible otherwise. For example, another example of an API-specified access pattern is the Read Any mode of the Portable Parallel File System (PPFS) [31]. <p> Not all global access patterns are best optimized by recognition of sequential subsections. In particular, much research effort has been devoted to optimizing access to complex matrix decompositions through collective operations <ref> [13, 79, 7] </ref>. In this experiment we consider a benchmark based on access patterns exhibited by applications from Caltech for global climate modeling and modeling of the earth's interior [52].
Reference: [80] <author> Simitci, H., and Reed, D. A. </author> <title> A Comparison of Logical and Physical Parallel I/O Patterns. </title> <note> In submitted for publication (June 1997). 149 </note>
Reference-contexts: Current parallel file systems are usually optimized for some expected workloads, and even slight deviations from the expected can degrade performance by orders of magnitude <ref> [20, 80] </ref>. Often, to achieve acceptable performance, the programmer is forced to tune the size, order and frequency of input/output requests to match the idiosyncrasies of a specific input/output system.
Reference: [81] <author> Smirni, E., Aydt, R. A., Chien, A. A., and Reed, D. A. </author> <title> I/O Requirements of Scientific Applications: An Evolutionary View. </title> <booktitle> In Fifth International Symposium on High Performance Distributed Computing (1996), </booktitle> <pages> pp. 49-59. </pages>
Reference-contexts: Input/output is due to compulsory reads of initialization data and writes to large checkpoint files. Therefore, sequential supercomputer input/output systems are typically optimized for high throughput. Uniprocessor input/output, whether from scientific applications or engineering workstations, is quite regular. In contrast, multiprocessor input/output access patterns exhibit greater variation <ref> [18, 81] </ref>, making it more difficult for file systems to optimize input/output. Furthermore, as processor speeds increase, it becomes computationally feasible to examine larger problems and new application areas, exacerbating the input/output bottleneck. <p> Furthermore, input/output requirements are a complex function of the interaction between system software and executing applications and may change unpredictably during program execution. 1.2 Application Input/Output Requirements Application level characterization studies <ref> [18, 81] </ref> demonstrate that there is wide variability in input/output access patterns, and moreover, performance is extremely sensitive to these variations. <p> Application areas include modeling of electron-molecule collisions, a 3-D numerical sim-ulation of the Navier-Stokes equations, an implementation of the Hartree-Fock self consistent field method to calculate the electron density around a molecule, and quantum chemical reaction dynamics <ref> [18, 81, 82] </ref>. These applications compose a snapshot of current input/output practices on scalable parallel machines and reflect the programmers' input/output design choices based on the capabilities of available input/output systems. They are but a small part of the Scalable Input/Output Initiative's (SIO) code suite [69].
Reference: [82] <author> Smirni, E., and Reed, D. A. </author> <title> Workload Characterization of Input/Output Intensive Parallel Applications. In Modelling Techniques and Tools for Computer Performance Evaluation (June 1997). </title>
Reference-contexts: Application areas include modeling of electron-molecule collisions, a 3-D numerical sim-ulation of the Navier-Stokes equations, an implementation of the Hartree-Fock self consistent field method to calculate the electron density around a molecule, and quantum chemical reaction dynamics <ref> [18, 81, 82] </ref>. These applications compose a snapshot of current input/output practices on scalable parallel machines and reflect the programmers' input/output design choices based on the capabilities of available input/output systems. They are but a small part of the Scalable Input/Output Initiative's (SIO) code suite [69].
Reference: [83] <author> Smyth, P. </author> <title> Clustering Sequences with Hidden Markov Models. </title> <booktitle> In Advances in Neural Information Processing 9. </booktitle> <publisher> MIT Press, to appear. </publisher>
Reference-contexts: The probabilities of the transitions determine which of these is more likely. Figure 3.10 can also be viewed as a composite of two individual HMMs; in essence the HMM clusters sequences, as described in <ref> [83] </ref>. 3.5.4 Memory Requirements of Hidden Markov Models Our chosen access pattern representation raises the issue of state space explosion. Maintaining the transition probabilities for all possible transitions between n small blocks of a large file can require up to O (n 2 ) storage. <p> Only one application execution is necessary for training. When a file has several significantly different access patterns (e.g., Figure 3.10), the training process is more complicated. We apply a simplified version of a clustering algorithm proposed in <ref> [83] </ref>. First we train an HMM on each individual access pattern as described above, yielding some set of K unique HMMs, each with the n states. <p> A more sophisticated approach to clustering access pattern sequences that automatically determines the number of sequences required to fit the data is proposed in <ref> [83] </ref>.
Reference: [84] <author> Song, I., and Cho, Y. </author> <title> Page Prefetching Based on Fault History. </title> <booktitle> In USENIX MACH III Symposium (Santa Fe, </booktitle> <address> NM, </address> <year> 1993), </year> <pages> pp. 203-213. </pages>
Reference-contexts: These studies focus on application input/output, but the method can also be applied to implicit input/output. An automatic policy for page prefetching due to implicit input/output was proposed by Song and Cho <ref> [84] </ref>. This approach uses a history of previous page faults to predict subsequent page faults when an application repeatedly access pages in the same order. Some intelligent approaches to policy control have focused on construction of models of file access that guide prefetchers.
Reference: [85] <author> Steere, D. C. </author> <title> Using Dynamic Sets to Speed Search in World Wide Information Systems. </title> <type> Tech. Rep. </type> <institution> CMU-CS-95-174, School of Computer Science, Carnegie Mellon University, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: This advance knowledge of input/output requirements must be provided to the file system (e.g., as hints). This is the motivation behind Transparent Informed Prefetching (TIP) [68] and dynamic sets <ref> [85] </ref>; the application reveals future accesses allowing the file system to use parallelism within the request stream to exploit input/output bandwidth and improve performance. 87 The same principles apply for exploiting parallelism across multiple input/output streams.
Reference: [86] <author> Thakur, R. </author> <title> Runtime Support for In-Core and Out-of-Core Data-Parallel Programs. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, Syracuse University, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The programmer chooses the appropriate data distribution and input/output operations are implicitly collective. A variable number of coalescing processes aggregate the requests to perform them efficiently under the chosen distribution. PASSION <ref> [13, 87, 86] </ref> (Parallel And Scalable Software for Input-Output) is another runtime library targeted for SPMD applications with routines to efficiently perform out-of-core operations. The Panda [79, 12] library utilizes server-directed input/output and a collective interface to achieve high performance on array accesses.
Reference: [87] <author> Thakur, R., Bordawekar, R., Choudhary, A., Ponnusamy, R., and Singh, T. </author> <title> PASSION Runtime Library for Parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference (October 1994), </booktitle> <pages> pp. 119-128. </pages>
Reference-contexts: The programmer chooses the appropriate data distribution and input/output operations are implicitly collective. A variable number of coalescing processes aggregate the requests to perform them efficiently under the chosen distribution. PASSION <ref> [13, 87, 86] </ref> (Parallel And Scalable Software for Input-Output) is another runtime library targeted for SPMD applications with routines to efficiently perform out-of-core operations. The Panda [79, 12] library utilizes server-directed input/output and a collective interface to achieve high performance on array accesses.
Reference: [88] <author> Thakur, R., and Choudhary, A. </author> <title> An Extended Two-Phase Method for Accessing Sections of Out-of-Core Arrays. </title> <type> Tech. Rep. </type> <institution> CACR-103, Scalable I/O Initiative, Center for Advanced Computing Research, Caltech, </institution> <month> June </month> <year> 1995. </year> <month> Revised November </month> <year> 1995. </year>
Reference-contexts: In a two-phase read, for example, data is read from disks in the most efficient manner and redistributed to the processors involved in the collective. A variation of two-phase input/output called extended two-phase input/output <ref> [88] </ref> uses collective input/output in conjunction with dynamic partitioning of the input/output workload among processors to balance load and improve performance. Disk-directed input/output [40] is another method for optimizing collective requests.
Reference: [89] <author> Thakur, R., Gropp, W., and Lusk, E. </author> <title> An Experimental Evaluation of the Parallel I/O Systems of the IBM SP and Intel Paragon Using a Production Application. </title> <type> Tech. Rep. </type> <institution> MCS-P569-0296, Argonne National Laboratory, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: These applications compose a snapshot of current input/output practices on scalable parallel machines and reflect the programmers' input/output design choices based on the capabilities of available input/output systems. They are but a small part of the Scalable Input/Output Initiative's (SIO) code suite [69]. Thakur et al <ref> [90, 89] </ref> used Pablo to characterize the behavior of a three-dimensional parallel astrophysics code running on PIOFS on the IBM SP/2 and PFS on the Paragon. This application uses several three-dimensional arrays that are allocated to processors using a (block,block,block) distribution across three dimensions.
Reference: [90] <author> Thakur, R., Lusk, E., and Gropp, W. </author> <title> I/O Characterization of a Portable Astrophysics Application on the IBM SP and Intel Paragon. </title> <type> Tech. Rep. </type> <institution> MCS-P534-0895, Argonne National Laboratory, </institution> <month> August </month> <year> 1995. </year> <note> Revised October 1995. 150 </note>
Reference-contexts: These applications compose a snapshot of current input/output practices on scalable parallel machines and reflect the programmers' input/output design choices based on the capabilities of available input/output systems. They are but a small part of the Scalable Input/Output Initiative's (SIO) code suite [69]. Thakur et al <ref> [90, 89] </ref> used Pablo to characterize the behavior of a three-dimensional parallel astrophysics code running on PIOFS on the IBM SP/2 and PFS on the Paragon. This application uses several three-dimensional arrays that are allocated to processors using a (block,block,block) distribution across three dimensions.
Reference: [91] <author> The MPI-IO Committee. </author> <title> MPI-IO: A Parallel File I/O Interface for MPI, </title> <month> April </month> <year> 1996. </year> <note> Version 0.5. </note>
Reference-contexts: For example, Intel PFS [32] has file access modes that describe a variety of common global access patterns. A popular attempt to define a standard user-level application programming interface (API) called MPI-IO <ref> [91, 70] </ref> uses MPI-derived datatypes [24] to express the mapping of file data to processors. IBM's Vesta [16] (marketed as PIOFS) file system gives applications control over declustering that is geared towards two-dimensional distributions. <p> Intel PFS [32] has analogous modes M ASYNC and M UNIX; in mode M UNIX reads and writes to the same file are atomic, but mode M ASYNC allows multiple readers and writers concurrent file access. MPI-IO <ref> [91] </ref>, an attempt to define a high-level API, does not provide consistency by default, but provides a cautious mode if it is required.
Reference: [92] <author> Tomkins, A., Patterson, R. H., and Gibson, G. </author> <title> Informed Multi-Process Prefetching and Caching. </title> <booktitle> In Proceedings of the ACM International Conference on Measurement and Modeling of Computer Systems (June 1997). </booktitle>
Reference-contexts: A cost-benefit analysis evaluates the reduction in input/output service time per buffer access to decide when to allocate buffers. This is an elegant approach for dealing with multiple applications that compete for shared caching and prefetching buffers. Kimbrel et al [35] and Tomkins et al <ref> [92] </ref> compare several integrated caching and prefetching policies using hints for systems with multiple disks. In parallel file systems, the way that data is partitioned among processors is closely linked to the access pattern. <p> Our policy selection methodology has treated caching and prefetching policies as separate entities; however, they are closely related to each other and are often fully integrated in current research literature that investigates prefetching for multiple disks <ref> [35, 92] </ref>. By using access pattern hints to determine the value of allocating prefetch buffers compared to cache buffers, prefetching decisions are tightly integrated with cache replacement policies.
Reference: [93] <author> Winstead, C., and McKoy, V. </author> <title> Studies of Electron-Molecule Collisions on Massively Parallel Computers. In Modern Electronic Structure Theory, </title> <editor> D. R. Yarkony, Ed., </editor> <volume> vol. 2. </volume> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: All of these applications run on the Intel Paragon. RENDER [53] is a parallel application that generates three-dimensional views of planetary surfaces from satellite data and terrain elevation data. HTF [29], a version of the Hartree Fock algorithm, calculates interactions between subatomic particles. ESCAT <ref> [93] </ref> implements the Schwinger multichannel method for calculating low-energy electron-molecule collisions. We manually inspected each file trace to determine the "correct" classification for each window, partitioned the input/output statistics computed from every ten accesses into test files for the correct classification, and verified that the neural net output was identical. <p> This machine has 16 input/output nodes and unless otherwise specified was running OSF/1 1.3. 6.3.4.1 ESCAT The Schwinger-Multichannel Electron Scattering (ESCAT) quantum chemistry application calculates low-energy electron-molecule collisions <ref> [93] </ref>. Figure 6.6 shows the entire execution input/output timeline for one version of the code, executed on 128 processors. File names are listed along the y axis, and accesses are shown along the x axis in time. From an input/output perspective, there are four main execution phases.
Reference: [94] <author> Wu, Y.-S. M., Cuccaro, S. A., Hipes, P. G., and Kuppermann, A. </author> <title> Quantum Chemical Reaction Dynamics on a Highly Parallel Supercomputer. </title> <journal> Theoretica Chimica Acta 79 (1991), </journal> <pages> 225-239. </pages>
Reference-contexts: Total open time is comparable for both experiments, implying that the cost of reading and broadcasting a small initialization file is little more expensive than the cost of synchronizing the processors to open the file. 7.3.2 QCRD QCRD <ref> [94, 50] </ref> is a quantum chemical reaction dynamics code used to study elementary chemical reactions. The code uses the method of symmetrical hyperspherical coordinates and local hyperspherical surface functions to solve the Schrodinger equation for the cross sections of the scattering of an atom by a diatomic molecule.
Reference: [95] <author> Xu, S. </author> <type> Personal communication. NCSA, </type> <month> June </month> <year> 1995. </year>
Reference-contexts: We have used two file sizes: 10000 and 100000 records. This benchmark is based on a code fragment that was brought to the attention of HDF support staff by a programmer who did not understand why its performance was abysmal <ref> [95] </ref>. SDS routines are used exclusively to access the data set. We read or write all records along dimension 0 or 4. Access along dimension 4 produces a strided access pattern; this is the most inefficient way to read the SDS data.
Reference: [96] <author> Zadeh, L. A. </author> <title> Fuzzy Sets. </title> <booktitle> Information and Control 8, </booktitle> <month> 3 (June </month> <year> 1965), </year> <pages> 338-353. 151 </pages>
Reference-contexts: If it is not, or if most of the performance improvements made possible by dynamic adaptation can be expressed by simple rules, the information should be codified in simple ruleset. One method we are currently investigating for writing such a ruleset is fuzzy logic <ref> [96, 34] </ref>. Fuzzy variables and sets allow a ruleset to encapsulate approximate rules. For example, in the following rule "If accesses are small and sequential prefetch aggressively," the italicized concepts are fuzzy variables.
References-found: 95

