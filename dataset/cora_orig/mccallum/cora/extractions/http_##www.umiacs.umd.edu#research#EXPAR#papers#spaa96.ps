URL: http://www.umiacs.umd.edu/research/EXPAR/papers/spaa96.ps
Refering-URL: http://www.umiacs.umd.edu/research/EXPAR/papers/spaa96.html
Root-URL: 
Email: fhelman, dbader, josephg@umiacs.umd.edu  
Title: Parallel Algorithms for Personalized Communication and Sorting With an Experimental Study (Extended Abstract)  
Author: David R. Helman David A. Bader Joseph JaJa 
Keyword: Parallel Algorithms, Personalized Communication, Sorting, Sample Sort, Radix Sort, Parallel Performance.  
Note: The support by NASA Graduate Student Researcher Fellowship No. NGT-50951 is gratefully acknowledged. Supported in part by NSF grant No. CCR-9103135 and NSF HPCC/GCAG grant No. BIR-9318183.  
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies and Department of Electrical Engineering, University of Maryland,  
Abstract: A fundamental challenge for parallel computing is to obtain high-level, architecture independent, algorithms which execute efficiently on general-purpose parallel machines. With the emergence of message passing standards such as MPI, it has become easier to design efficient and portable parallel algorithms by making use of these communication primitives. While existing primitives allow an assortment of collective communication routines, they do not handle an important communication event when most or all processors have non-uniformly sized personalized messages to exchange with each other. We first present an algorithm for the h-relation personalized communication whose efficient implementation will allow high performance implementations of a large class of algorithms. We then consider how to effectively use these communication primitives to address the problem of sorting. Previous schemes for sorting on general-purpose parallel machines have had to choose between poor load balancing and irregular communication or multiple rounds of all-to-all personalized communication. In this paper, we introduce a novel variation on sample sort which uses only two rounds of regular all-to-all personalized communication in a scheme that yields very good load balancing with virtually no overhead. Another variation using regular sampling for choosing the splitters has similar performance with deterministic guaranteed bounds on the memory and communication requirements. Both of these variations efficiently handle the presence of duplicates without the overhead of tagging each element. The personalized communication and sorting algorithms presented in this paper have been coded in Split-C and run on a variety of platforms, including the Thinking Machines CM-5, IBM SP-2, Cray Research T3D, Meiko Scientific CS-2, and the Intel Paragon. Our experimental results are consistent with the theoretical analyses and illustrate the scalability and efficiency of our algorithms across different platforms. In fact, they seem to outperform all similar algorithms known to the authors on these platforms, and performance is invariant over the set of input distributions unlike previous efficient algorithms. Our sorting results also compare favorably with those reported for the simpler ranking problem posed by the NAS Integer Sorting (IS) Benchmark. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Alexandrov, M. Ionescu, K. Schauser, and C. Scheiman. LogGP: </author> <title> Incorporating Long Messages into the LogP Model One step closer towards a realistic model for parallel computation. </title> <booktitle> In 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 95-105, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year> <title> and problem size, on the Cray T3D and the IBM SP-2-TN </title>
Reference-contexts: This communication model is close to a number of similar models (e.g. the LogP [13], BSP [24], and LogGP <ref> [1] </ref> models) that have recently appeared in the literature and seems to be well-suited for designing parallel algorithms on current high performance platforms. We define the computation time T comp (n; p) as the maximum time taken by any processor to perform all of its local computation steps. <p> Table VI presents a comparison of our radix sort with another implementation of radix sort in Split-C by Alexan-drov et al. <ref> [1] </ref> This other implementation, which was tuned for the Meiko CS-2, is identified the table as AIS, while our algorithm is referred to as BHJ. The input [R] is random, [C] is cyclically sorted, and [N] is a random Gaussian approximation [4].
Reference: [2] <author> R.H. Arpaci, D.E. Culler, A. Krishnamurthy, S.G. Steinberg, and K. Yelick. </author> <title> Empirical Evaluation of the CRAY-T3D: A Compiler Perspective. </title> <publisher> In ACM Press, </publisher> <editor> editor, </editor> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 320-331, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference: [3] <author> D. Bader. </author> <title> Randomized and Deterministic Routing Algorithms for h-Relations. ENEE 648X Class Report, </title> <address> April 1, </address> <year> 1994. </year>
Reference-contexts: Our algorithm seems to achieve the best known experimental results for this problem. The general approach was independently described by Kaufmann et al. [18] and Ranka et al. [21] around the same time as our earlier draft <ref> [3] </ref> appeared, but our algorithm is simpler, has less overhead, and has a tighter bound size for the intermediate collective communication. Problem 2 (Sorting): Rearrange n equally distributed elements amongst p processors such that they appear in nondecreasing order starting from the smallest indexed processor.
Reference: [4] <author> D.A. Bader, D.R. Helman, and J. JaJa. </author> <title> Practical Parallel Algorithms for Personalized Communication and Integer Sorting. </title> <institution> CS-TR-3548 and UMIACS-TR-95-101 Technical Report, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> November </month> <year> 1995. </year> <note> To appear in ACM Journal of Experimental Al-gorithmics. </note>
Reference-contexts: Additionally, all of our algorithms efficiently handle the presence of duplicates without the overhead of tagging each element. 3 Personalized Communication For ease of presentation, we describe the personalized communication algorithm for the case when the input is initially evenly distributed amongst the processors. The reader is directed to <ref> [4] </ref> for the general case. Consider a set of n elements evenly distributed amongst p processors in such a manner that no processor holds more than n p elements. Each element consists of a pair hdata; desti, where dest is the location to where the data is to be routed. <p> Since we established that no bin can have more than n p 2 + p elements <ref> [4] </ref>, this is equivalent to performing a trans pose communication primitive with block size n p 2 + p * Step (3): Each processor P i rearranges the elements received in Step (2) into bins according to each ele ment's final destination. * Step (4): Each processor P i routes the <p> Since we established that no bin can have more than h p + p ments <ref> [4] </ref>, this is equivalent to performing a transpose primitive with block size h p + p The overall complexity of our algorithm is T comp (n; p) + T comm (n; p) = O (h) + 2 fi , which is asymptotically optimal with very small constants for p 2 n. <p> More results input data sets are given in <ref> [4] </ref>. The data sets used in these experiments are defined as follows. Our input of size n is initially distributed cyclically across the p processors such that each processor P i initially holds n p keys, for (0 i p 1). <p> The input [R] is random, [C] is cyclically sorted, and [N] is a random Gaussian approximation <ref> [4] </ref>. Additional performance results are given in Figure 5 and in [4]. <p> The input [R] is random, [C] is cyclically sorted, and [N] is a random Gaussian approximation <ref> [4] </ref>. Additional performance results are given in Figure 5 and in [4].
Reference: [5] <author> D.A. Bader and J. JaJa. </author> <title> Parallel Algorithms for Image Histogramming and Connected Components with an Experimental Study. </title> <booktitle> In Fifth ACM SIGPLAN Sym--posium of Principles and Practice of Parallel Programming, </booktitle> <pages> pages 123-133, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year> <note> To appear in Journal of Parallel and Distributed Computing. </note>
Reference-contexts: The cost of each of the collective communication primitives (see below) will be modeled by O (t + max (m; p)), where m is the maximum amount of data transmitted or received by a processor. Such a cost can be justified by using our earlier work <ref> [17, 6, 5] </ref>. Using this cost model, we can evaluate the communication time T comm (n; p) of an algorithm as a function of the input size n, the number of processors p , and the parameters t and .
Reference: [6] <author> D.A. Bader and J. JaJa. </author> <title> Practical Parallel Algorithms for Dynamic Data Redistribution, Median Finding, and Selection. </title> <institution> Technical Report CS-TR-3494 and UMIACS-TR-95-74, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> July </month> <year> 1995. </year> <booktitle> To be presented at the 10th International Parallel Processing Symposium, </booktitle> <address> Honolulu, HI, </address> <month> April 15-19, </month> <year> 1996. </year>
Reference-contexts: The cost of each of the collective communication primitives (see below) will be modeled by O (t + max (m; p)), where m is the maximum amount of data transmitted or received by a processor. Such a cost can be justified by using our earlier work <ref> [17, 6, 5] </ref>. Using this cost model, we can evaluate the communication time T comm (n; p) of an algorithm as a function of the input size n, the number of processors p , and the parameters t and . <p> Our algorithms are implemented in Split-C [11], an extension of C for distributed memory machines. The algorithms make use of MPI-like communication primitives but do not make any assumptions as to how these primitives are actually implemented. Our collective communication primitives, described in detail in <ref> [6] </ref>, are similar to those of MPI [20], the IBM POWERparallel [7], and the Cray MPP systems [10] and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows. <p> Step (8) requires O n time if we merge the sorted subse quences in a binary tree fashion. Steps (2), (5), and (7) call the communication primitives transpose, bcast, and transpose, respectively. The analysis of these primitives in <ref> [6] </ref> shows that with high probability these three steps require T comm (n; p) p 2 (p 1) , T comm (n; p) (t + p), and T comm (n; p) p 2 (p 1) , respec tively.
Reference: [7] <author> V. Bala, J. Bruck, R. Cypher, P. Elustondo, A. Ho, C.- T. Ho, S. Kipnis, and M. Snir. </author> <title> CCL: A Portable and Tunable Collective Communication Library for Scalable Parallel Computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6 </volume> <pages> 154-164, </pages> <year> 1995. </year>
Reference-contexts: The algorithms make use of MPI-like communication primitives but do not make any assumptions as to how these primitives are actually implemented. Our collective communication primitives, described in detail in [6], are similar to those of MPI [20], the IBM POWERparallel <ref> [7] </ref>, and the Cray MPP systems [10] and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows.
Reference: [8] <author> G.E. Blelloch, C.E. Leiserson, B.M. Maggs, C.G. Plax-ton, S.J. Smith, and M. Zagha. </author> <title> A Comparison of Sorting Algorithms for the Connection Machine CM-2. </title> <booktitle> In Proceedings of the ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 3-16, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: One way to choose the splitters is by randomly sampling the input elements at each processor - hence the name sample sort. Previous versions of sample sort <ref> [16, 8, 12] </ref> have randomly chosen s samples from the n p elements at each processor, routed them to a single processor, sorted them at that processor, and then selected every s th element as a splitter. <p> Moreover, such an irregular communication scheme cannot take advantage of the regular communication primitives proposed under the MPI standard [20]. The final difficulty with the original approach is that duplicate values are accommodated by tagging each item with some unique value <ref> [8] </ref>. This, of course, doubles the cost of both memory access and interprocessor communication. In our version of sample sort, we incur no overhead in obtaining n p 2 samples from each processor and in sorting these samples to identify the splitters.
Reference: [9] <author> W.W. Carlson and J.M. Draper. </author> <title> AC for the T3D. </title> <type> Technical Report SRC-TR-95-141, </type> <institution> Supercomputing Research Center, Bowie, MD, </institution> <month> February </month> <year> 1995. </year>
Reference: [10] <institution> Cray Research, Inc. </institution> <note> SHMEM Technical Note for C, October 1994. Revision 2.3. </note>
Reference-contexts: The algorithms make use of MPI-like communication primitives but do not make any assumptions as to how these primitives are actually implemented. Our collective communication primitives, described in detail in [6], are similar to those of MPI [20], the IBM POWERparallel [7], and the Cray MPP systems <ref> [10] </ref> and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows.
Reference: [11] <author> D.E. Culler, A. Dusseau, S.C. Goldstein, A. Krish-namurthy, S. Lumetta, S. Luna, T. von Eicken, and K. Yelick. </author> <title> Introduction to Split-C. </title> <institution> Computer Science Division - EECS, University of California, Berkeley, </institution> <note> version 1.0 edition, </note> <month> March 6, </month> <year> 1994. </year>
Reference-contexts: We define the computation time T comp (n; p) as the maximum time taken by any processor to perform all of its local computation steps. Our algorithms are implemented in Split-C <ref> [11] </ref>, an extension of C for distributed memory machines. The algorithms make use of MPI-like communication primitives but do not make any assumptions as to how these primitives are actually implemented.
Reference: [12] <author> D.E. Culler, A.C. Dusseau, R.P. Martin, and K.E. Schauser. </author> <title> Fast Parallel Sorting Under LogP: From Theory to Practice. In Portability and Performance for Parallel Processing, </title> <booktitle> chapter 4, </booktitle> <pages> pages 71-98. </pages> <publisher> John Wi-ley & Sons, </publisher> <year> 1993. </year>
Reference-contexts: One way to choose the splitters is by randomly sampling the input elements at each processor - hence the name sample sort. Previous versions of sample sort <ref> [16, 8, 12] </ref> have randomly chosen s samples from the n p elements at each processor, routed them to a single processor, sorted them at that processor, and then selected every s th element as a splitter.
Reference: [13] <author> D.E. Culler, R.M. Karp, D.A. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: The coefficient of t gives the total number of times collective communication primitives are used, and the coefficient of gives the maximum total amount of data exchanged between a processor and the remaining processors. This communication model is close to a number of similar models (e.g. the LogP <ref> [13] </ref>, BSP [24], and LogGP [1] models) that have recently appeared in the literature and seems to be well-suited for designing parallel algorithms on current high performance platforms.
Reference: [14] <author> D.R. Helman, D.A. Bader, and J. JaJa. </author> <title> A Parallel Sorting Algorithm With an Experimental Study. </title> <institution> Technical Report CS-TR-3549 and UMIACS-TR-95-102, UMI-ACS and Electrical Engineering, University of Mary-land, College Park, MD, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: We can establish the complexity of this algorithm with high probability that is with probability for some positive constant *. But before doing this, we need the results of the following theorem, whose proof has been omitted for brevity <ref> [14] </ref>. <p> Note that while we actually place the integers in sorted order, the benchmark only requires that they be ranked without actually reordering. See <ref> [14] </ref> for additional performance data and comparisons with other published results. 4.2 A New Sorting Algorithm by Regular Sampling A disadvantage of our random sample sort algorithm is that the performance bounds and the memory requirements can only be guaranteed with high probability.
Reference: [15] <author> D.R. Helman, D.A. Bader, and J. JaJa. </author> <title> A Parallel Regular Sorting Algorithm With an Experimental Study. </title> <type> Technical report, </type> <institution> UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> June </month> <year> 1996. </year> <note> In Preparation. </note>
Reference-contexts: The p consolidated subsequences are then merged to produce the i th column of the sorted array. Before establishing the complexity of this algorithm, we need the results of the following theorem, whose proof has been omitted for brevity <ref> [15] </ref>. Theorem 2: The number of elements exchanged by any two processors in Step (7) is at most p 2 + n . Consequently, at the completion of Step (7), no processor receives more than p + n elements, for n p 3 . <p> A hyphen indicates that that particular platform was unavailable to us. dence between the execution time and the total number of elements n. See <ref> [15] </ref> for additional performance data and comparisons with other published results. 4.3 An Efficient Radix Sort We now consider the problem of sorting n integer keys in the range [0; M 1] that are distributed equally over a p-processor distributed memory machine.
Reference: [16] <author> J.S. Huang and Y.C. Chow. </author> <title> Parallel Sorting and Data Partitioning by Sampling. </title> <booktitle> In Proceedings of the 7th Computer Software and Applications Conference, </booktitle> <pages> pages 627-631, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: One way to choose the splitters is by randomly sampling the input elements at each processor - hence the name sample sort. Previous versions of sample sort <ref> [16, 8, 12] </ref> have randomly chosen s samples from the n p elements at each processor, routed them to a single processor, sorted them at that processor, and then selected every s th element as a splitter.
Reference: [17] <author> J.F. JaJa and K.W. Ryu. </author> <title> The Block Distributed Memory Model for Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <pages> pages 752-756, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year> <note> To appear in IEEE Transactions on Parallel and Distributed Systems. </note>
Reference-contexts: The cost of each of the collective communication primitives (see below) will be modeled by O (t + max (m; p)), where m is the maximum amount of data transmitted or received by a processor. Such a cost can be justified by using our earlier work <ref> [17, 6, 5] </ref>. Using this cost model, we can evaluate the communication time T comm (n; p) of an algorithm as a function of the input size n, the number of processors p , and the parameters t and .
Reference: [18] <author> M. Kaufmann, J.F. Sibeyn, and T. Suel. </author> <title> Derandom-izing Algorithms for Routing and Sorting on Meshes. </title> <booktitle> In Proceedings of the 5th Symposium on Discrete Algorithms, </booktitle> <pages> pages 669-679. ACM-SIAM, </pages> <year> 1994. </year>
Reference-contexts: Significance: The importance of this problem has been stated in many papers (e.g. [24, 22]). Our algorithm seems to achieve the best known experimental results for this problem. The general approach was independently described by Kaufmann et al. <ref> [18] </ref> and Ranka et al. [21] around the same time as our earlier draft [3] appeared, but our algorithm is simpler, has less overhead, and has a tighter bound size for the intermediate collective communication.
Reference: [19] <author> X. Li, P. Lu, J. Schaeffer, J. Shillington, P.S. Wong, and H. Shi. </author> <title> On the Versatility of Parallel Sorting by Regular Sampling. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 1079-1103, </pages> <year> 1993. </year>
Reference-contexts: The alternative to this is to choose the samples by regular sampling. A previous version of regular sample sort <ref> [23, 19] </ref>, known as Parallel Sorting by Regular Sampling (PSRS), first sorts the n p elements at each processor and then selects every p 2 element as a sample. <p> This could be reduced by choosing more splitters, but this would also increase the overhead. And no matter what is done, previous workers have observed that the load balance would still deteriorate linearly with the number of duplicates <ref> [19] </ref>. The other difficulty is that no matter how the routing is scheduled, there exist inputs that give rise to large variations in the number of elements destined for different processors, and this in turn results in an inefficient use of the communication bandwidth.
Reference: [20] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, TN, </institution> <month> June </month> <year> 1995. </year> <note> Version 1.1. </note>
Reference-contexts: The first is the emergence of a dominant parallel architecture consisting of a number of powerful micropro cessors interconnected by either a proprietary interconnect or a standard off-the-shelf interconnect. The second factor is the emergence of standards, such as the message passing standard MPI <ref> [20] </ref>, for which machine builders and software developers will try to provide efficient support. Our work builds on these two developments by presenting a theoret ical and an experimental framework for designing parallel algorithms. In this abstract, we sketch our contributions in two important problems: personalized communication and sorting. <p> The algorithms make use of MPI-like communication primitives but do not make any assumptions as to how these primitives are actually implemented. Our collective communication primitives, described in detail in [6], are similar to those of MPI <ref> [20] </ref>, the IBM POWERparallel [7], and the Cray MPP systems [10] and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows. <p> Moreover, such an irregular communication scheme cannot take advantage of the regular communication primitives proposed under the MPI standard <ref> [20] </ref>. The final difficulty with the original approach is that duplicate values are accommodated by tagging each item with some unique value [8]. This, of course, doubles the cost of both memory access and interprocessor communication. <p> Moreover, such an irregular communication scheme cannot take advantage of the regular communication primitives proposed under the MPI standard <ref> [20] </ref>. In our algorithm, which is parameterized by a sampling ratio s p 2 , we guarantee that at the completion of sorting, each processor will have at most p + n ments, while incurring no overhead in gathering the samples to identify the splitters.
Reference: [21] <author> S. Ranka, R.V. Shankar, and K.A. Alsabti. </author> <title> Many-to-many Personalized Communication with Bounded Traffic. </title> <booktitle> In The Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 20-27, </pages> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Significance: The importance of this problem has been stated in many papers (e.g. [24, 22]). Our algorithm seems to achieve the best known experimental results for this problem. The general approach was independently described by Kaufmann et al. [18] and Ranka et al. <ref> [21] </ref> around the same time as our earlier draft [3] appeared, but our algorithm is simpler, has less overhead, and has a tighter bound size for the intermediate collective communication.
Reference: [22] <author> S. Rao, T. Suel, T. Tsantilas, and M. Goudreau. </author> <title> Efficient Communication Using Total-Exchange. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <pages> pages 544-550, </pages> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: Result: A new deterministic algorithm using two rounds of the transpose primitive, with optimal complexity and very small constant coefficients, is shown to be very efficient and scalable across different platforms and over different input distributions. Significance: The importance of this problem has been stated in many papers (e.g. <ref> [24, 22] </ref>). Our algorithm seems to achieve the best known experimental results for this problem.
Reference: [23] <author> H. Shi and J. Schaeffer. </author> <title> Parallel Sorting by Regular Sampling. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 361-372, </pages> <year> 1992. </year>
Reference-contexts: The alternative to this is to choose the samples by regular sampling. A previous version of regular sample sort <ref> [23, 19] </ref>, known as Parallel Sorting by Regular Sampling (PSRS), first sorts the n p elements at each processor and then selects every p 2 element as a sample.
Reference: [24] <author> L.G. Valiant. </author> <title> A Bridging Model for Parallel Computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: This communication model is close to a number of similar models (e.g. the LogP [13], BSP <ref> [24] </ref>, and LogGP [1] models) that have recently appeared in the literature and seems to be well-suited for designing parallel algorithms on current high performance platforms. <p> Result: A new deterministic algorithm using two rounds of the transpose primitive, with optimal complexity and very small constant coefficients, is shown to be very efficient and scalable across different platforms and over different input distributions. Significance: The importance of this problem has been stated in many papers (e.g. <ref> [24, 22] </ref>). Our algorithm seems to achieve the best known experimental results for this problem.
References-found: 24

