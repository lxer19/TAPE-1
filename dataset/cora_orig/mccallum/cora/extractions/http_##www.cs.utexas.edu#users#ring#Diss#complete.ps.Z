URL: http://www.cs.utexas.edu/users/ring/Diss/complete.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ring/Diss/index.html
Root-URL: http://www.cs.utexas.edu
Title: CONTINUAL LEARNING IN REINFORCEMENT ENVIRONMENTS  
Author: by MARK BISHOP RING, A.B., M.S.C.S. 
Degree: DISSERTATION Presented to the Faculty of the Graduate School of  in Partial Fulfillment of the Requirements for the Degree of DOCTOR OF PHILOSOPHY  
Date: August, 1994  
Address: Austin  AUSTIN  
Affiliation: The University of Texas at  THE UNIVERSITY OF TEXAS AT  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. S. Albus. </author> <title> Mechanisms of planning and problem solving in the brain. </title> <journal> Mathematical Biosciences, </journal> <volume> 45 </volume> <pages> 247-293, </pages> <year> 1979. </year>
Reference: [2] <author> Ethem Alpaydin. GAL: </author> <title> Networks that grow when they learn and shrink when they forget. </title> <type> Technical Report 91-032, </type> <institution> International Computer Science Institute, Berkeley, California, </institution> <month> May </month> <year> 1991. </year>
Reference: [3] <author> C. W. Anderson. </author> <title> Learning and Problem Solving with Multilayer Connectionist Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Sciences, University of Massachusetts, </institution> <year> 1986. </year>
Reference-contexts: Learning without this restriction has also been done <ref> [3, 54, 92, 116] </ref>. The only real restriction with the AHC (and with most other reinforcement-learning techniques as well) is that the learning algorithm used by the policy and critic modules must be capable of distinguishing the underlying environmental states.
Reference: [4] <author> Jonathan Richard Bachrach. </author> <title> Learning to represent state. </title> <type> Master's thesis, </type> <institution> Department of Computer and Information Sciences, University of Massachusetts, </institution> <address> Amherst, MA 01003, </address> <month> November </month> <year> 1988. </year>
Reference-contexts: The simplest recurrent networks look just like the feed-forward networks described in Chapter 3, except that some hidden units have single, recurrent self-connections, and there are no other recurrent connections. Examples of these networks will be described next: First, Mozer's Focused Back-Propagation network [65] and Bachrach's Sticky-bit network <ref> [4] </ref>; and Second, Fahlman's RCC (Recurrent Cascade Correlation) architecture [27]. After these comes a description of fully connected recurrent networks. 4.3.1 The Focused and Sticky-bit Architectures The focused architecture uses hidden units with adjustable connections in the hidden layer to encode a variable decay rate. <p> Since it is stable in two possible states, each hidden unit can detect the presence or absence of a single feature <ref> [4] </ref>. Learning in both networks is done by gradient descent in the error space with respect to the modifiable parameters w ij and d i for all i and j. The networks are more powerful 4.3. <p> Instead, the network could have just a 8.4. Future Work 103 single recurrent connection from each high-level unit to itself and then perform gradient descent with respect to these connections. This is the same elegant approach used by Bachrach <ref> [4] </ref> and by Mozer [65] (Section 4.3.1). Using Bachrach's approach, these units can keep their values for any arbitrary duration. This enables each high-level unit to tune in to precisely those signals that should cause it to change its activation value, regardless of when those signals occur. <p> Do we finish learning how to write or do research? Do we ever learn anything completely? Or do we just keep getting better than we were before? Appendix A Simulating a Queue With a Focused Network It is not difficult to devise an activation function that would allow a Bachrach <ref> [4] </ref> or Mozer [65] network to simulate a queue (i.e., to produce time-delayed output) given discrete inputs. One way to do this with differentiable transfer functions is as follows. Without loss of generality, assume the input to each input unit is binary. Allow one hidden unit for every input unit. <p> The total number of operations required per network cycle also scales with HN w . While this is better than the scaling behavior of the RTRL algorithm, it is not as good as Bachrach's, Mozer's, or Fahlman's algorithms <ref> [4, 27, 65] </ref> (Section 4.3).
Reference: [5] <author> Jonathan Richard Bachrach. </author> <title> Connectionist Modeling and Control of Finite State Environments. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Sciences, University of Massachusetts, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Table 4.1 lists the grammars learned. Some of these are extremely difficult. Even the first, the simplest, cannot be learned by a delay-line network, since the network must record whether it has seen a zero and remember this for an arbitrary period of time. Bachrach <ref> [5] </ref> designed a related architecture strictly for the purposes of learning finite-state automata in a connectionist system. His architecture was modeled after the finite-state-machine learning algorithm of Rivest and Schapire [84]. Bachrach's network is capable of learning some extremely difficult grammars from positive examples only. <p> In particular, Bachrach compared his system on difficult tasks to many traditional networks, including recurrent networks that used BackPropagation Through Time, and he found a great improvement. The standard networks were incapable of learning even his simplest tasks. Bachrach suggests in his thesis <ref> [5, p. 65] </ref>, that one of the key advantages of his network is the multiplicative property of the connections. <p> Gradient-following techniques for determining the optimal action when the actions are continuous have been explored by Werbos [116], Munro [68], Jordan and Jacobs [45], Schmidhuber [91, 93], Thrun, et al. [108], Thrun and Moller [107], Bachrach <ref> [5] </ref>, Linden and Weber [57], and others. These methods are all based on the concept of using a neural network to learn a model of the environment and then doing gradient descent in this differentiable model to improve the quality of the agent's decisions. <p> as input the agent's sensations and next action and produces as its output a measure of the quality of that action (e.g., the dfr of the agent's next state), gradient ascent can be performed in action space to maximize the model's output, thus producing the optimal action for that state <ref> [5, 57, 107, 108] </ref>. When the dfr is used to measure the quality of the agent's chosen action, as was done by Bachrach [5], the result is a continuous-action Q-learning algorithm. Of course, the same caveats apply here as apply just above for network inversion in general. <p> When the dfr is used to measure the quality of the agent's chosen action, as was done by Bachrach <ref> [5] </ref>, the result is a continuous-action Q-learning algorithm. Of course, the same caveats apply here as apply just above for network inversion in general. <p> See Pollack [75, Ch. 4] for the clearly related technique that inspired this one. Appendix B Equivalence of SLUG and Second-order Recurrent Networks The equivalence of SLUG and the other second-order recurrent networks presented in Section 4.3.4 becomes clear once SLUG is formalized. The following equation describes SLUG <ref> [5] </ref>: ~o (t) = W a (t) ~o (t 1); where ~o (t) is the vector of hidden-unit outputs at time t; a (t) is the single action taken at time t; and W a (t) is the weight matrix corresponding to action a (t).
Reference: [6] <author> L.C. Baird, III. </author> <title> Advantage updating. </title> <type> Technical report, </type> <institution> Wright Laboratory, Wright-Patterson Air Force Base, OH, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: In these situations it could be quite useful to distinguish between absolute and relative elevation. A similar but more sophisticated method, "Advantage Updating," was introduced by Baird <ref> [6] </ref>. Advantage updating has been proven to converge to the optimal policy (under appropriate conditions).
Reference: [7] <author> Avron Barr and Edward A. Feigenbaum. </author> <booktitle> The Handbook of Artificial Intelligence, </booktitle> <volume> volume 1. </volume> <publisher> William Kaufmann, Inc., </publisher> <address> Los Altos, California, </address> <year> 1981. </year>
Reference: [8] <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <title> Neuron-like elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 835-846, </pages> <year> 1983. </year>
Reference-contexts: Each w ij is therefore an eligibility trace <ref> [8, 50] </ref> of weight changes that decays exponentially. The trace constantly accrues weight changes over time | biased towards the most recent ones | but the changes are only applied to the weights when a reinforcement is received.
Reference: [9] <author> A. G. Barto, R. S. Sutton, and Christopher J. C. H. Watkins. </author> <title> Learning and sequential decision making. </title> <type> Technical Report COINS Technical Report 89-95, </type> <institution> University of Massachusetts at Amherst, Department of Computer and Information Science, </institution> <year> 1989. </year>
Reference-contexts: The reinforcement is either rewarding or punishing. The AHC is a general architecture that increases the probability of actions that lead to greatest reward and decreases the probability of actions that lead to punishment. More formally, the AHC attempts to maximize the agent's long-term reinforcement <ref> [9, 103, 105] </ref>. This architecture divides the agent into two modules: one that chooses an action and one that estimates the agent's future reinforcement given its current input. <p> Reinforcement Learning step to state y, will be 0:81, and so on. If all reinforcements are finite and 0 fl 1:0, then even if the agent continues forever, the discounted reward for every state is finite <ref> [9] </ref>. A distinction can be made between two types of tasks: those that terminate, and those that do not. In terminating tasks, the agent is stopped when it reaches a halting state, or when it executes the task for some predetermined maximum number of time steps. <p> Therefore, this process of alternatively improving the policy and the critic never produces a new policy that had been the policy previously. That is, the process of modifying the policy always increases the mean dfr until eventual convergence <ref> [9, x5.2] </ref>. In the final 1 How this adjustment is made depends on the mechanism for representing A (e.g., look-up table, neural network, etc.) 5.1. The Adaptive Heuristic Critic 37 policy, no state!action mapping can be changed to increase the policy's mean dfr . <p> Assuming the environment is deterministic and the discount factor is 0:9, the resulting landscape is presented in Figure 5.3b. The global maximum is positioned where the single goal is located. (Similar figures are given by Barto, et al. <ref> [9] </ref>.) If the agent has knowledge of its elevation on the landscape, it can immediately judge the quality of its moves and therefore learn the optimal policy. The goodness of a move is determined by the change in elevation plus the immediate reward.
Reference: [10] <author> Andrew G. Barto, Steven J. Bradtke, and Satinder P. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report 91-57, </type> <institution> Computer Science Department, University of Massachusetts at Amherst, </institution> <month> August </month> <year> 1991. </year>
Reference: [11] <author> Andrew G. Barto, Steven J. Bradtke, and Satinder P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <note> Submitted to AI Journal special issue on Computational Theories of Interaction and Agency, </note> <month> January </month> <year> 1993. </year>
Reference-contexts: These problems | where actions leading to a goal state result in a positive reinforcement and all other actions receive no reinforcement | are a subset of the dynamic-programming problems known as shortest-path problems <ref> [11, 12ff] </ref>[12]. Dynamic programming can apply to much more sophisticated reinforcement schemes than shortest-path problems, however, including cases where the environment's transitions are stochastic and where each action may receive positive or negative reinforcement. <p> If more than one action maximizes the equation, any such action can be chosen. Though an optimal policy can be determined from the optimal evaluation function, the converse is not the case. 2 For more information, see the excellent synthesis given by Barto, Bradtke, and Singh <ref> [11] </ref>. 40 Chapter 5. Reinforcement Learning One way to discover the optimal policy is through the AHC method discussed above, which approximates a form of dynamic programming known as policy iteration [105] in that it successively modifies the policy until the optimal policy is reached. <p> The advantage of policy iteration is that it can discover an optimal policy long before correctly calculating the optimal evaluation function. In contrast, value iteration solves Equation 5.9 without recognizing when an optimal policy has been determined <ref> [11] </ref>. This should not be construed, however, as indicating that value iteration offers only one way to solve the equation. Many different methods are described in the reinforcement-learning literature, each with its own advantages and disadvantages. One such method is RTDP (Real-Time Dynamic Programming) [11], which backs up the agent's current <p> an optimal policy has been determined <ref> [11] </ref>. This should not be construed, however, as indicating that value iteration offers only one way to solve the equation. Many different methods are described in the reinforcement-learning literature, each with its own advantages and disadvantages. One such method is RTDP (Real-Time Dynamic Programming) [11], which backs up the agent's current state together with as many other states as time permits at each step. Special cases of this are the Prioritized Sweeping [64] and the Queue-Dyna [71] algorithms, which at every step modify the values of those states where the modifications are the largest.
Reference: [12] <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year> <title> Secondary source, from [11]. Bibliography 119 </title>
Reference: [13] <author> Ulrich Bodenhausen and Alex Waibel. </author> <title> The Tempo 2 algorithm: adjusting time-delays by supervised learning. </title> <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 155-161, </pages> <address> San Mateo, California, 1991. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: As mentioned above, one such algorithm is Temporal Transition Hierarchies, which will be described in Section 6.2. Another is Bodenhausen and Waibel's system, Tempo 2 <ref> [13] </ref>. In Tempo 2, input units have time delays that can be learned as adjustable parameters. Each input unit, in fact, has three adjustable parameters for every incoming connection: the weight, time delay, and width of the time delay's receptive field.
Reference: [14] <author> Rodney A. Brooks. </author> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> RA-2(1):14-23, </volume> <month> March </month> <year> 1986. </year>
Reference: [15] <author> D. Chen, C. L. Giles, G.Z. Sun, H. H. Chen, Y. C. Lee, and M. W. Goudreau. </author> <title> Constructive learning of recurrent neural networks. </title> <booktitle> In IEEE Proceedings of the ICNN, </booktitle> <year> 1993. </year>
Reference-contexts: These networks should be able to learn any Markov-k task where k is initially unknown, since they are capable of storing an arbitrary amount of past history information. However, there are computations these networks cannot perform. This has been proven by Chen, et al. <ref> [15] </ref>, who showed that RCC networks using sigmoid or threshold activation functions are unable to learn certain classes of finite-state grammars. (This is due to the fact that none of the RCC hidden units have connections downwards, back to lower units | units closer to the inputs.) Another limitation of RCC <p> Those which implement incremental learning have a space complexity of O (n 4 ) and a time complexity of O (n 5 ) at each step, where n is the number of units. Constructive Fully Recurrent Networks. After describing the limitations of the RCC network, Chen, et al. <ref> [15] </ref>, went on to design a constructive network of their own. Theirs adds units as learning progresses to a fully connected, second-order recurrent network (that of Giles, et al. [34, 33], discussed above).
Reference: [16] <author> Lonnie Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI-92), </booktitle> <address> Cambridge, MA, 1992. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: One of these, McCallum's Utile Distinction Memory (UDM) [59] is based on Chris-man's Perceptual Distinctions Approach (PDA) <ref> [16] </ref>, which is a constructive Hidden Markov Model (HMM) method. Both approaches build units to represent states explicitly.
Reference: [17] <author> Axel Cleeremans, David Servan-Schreiber, and James L. McClelland. </author> <title> Finite state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 372-381, </pages> <year> 1989. </year>
Reference-contexts: Also shown are the corresponding values (when available) of other systems, quoted from reports published elsewhere (in most cases by the system's inventors). An Elman-type recurrent network was able to learn this task after 20,000 string presentations using 15 hidden units <ref> [17] </ref>. (The correctness criteria for the Elman net were 76 Chapter 7. <p> The results for the recurrent networks are quoted from other sources <ref> [17, 27] </ref>. The mean and/or best performance is shown when available. RTRL is the Real-Time Recurrent Learning algorithm [123]. <p> The system cannot remember events for any arbitrary duration, but only for specific, already-learned durations. This means that the network could not learn, for example, an embedded Reber grammar (a grammar in which the Reber grammar appears as a subcomponent <ref> [17] </ref>, and which requires information to be remembered for an indefinite duration). A variation on the algorithm that could conceivably solve this problem is discussed in the future-work section, Section 8.4. Another deficiency of transition hierarchies is their proclivity for creating new units.
Reference: [18] <author> Richard Dawkins. </author> <title> Hierarchical organisation: a candidate principle for ethology. </title> <editor> In P. P. G. Bateson and R. A. Hinde, editors, </editor> <booktitle> Growing Points in Ethology, </booktitle> <pages> pages 7-54, </pages> <address> Cambridge, 1976. </address> <publisher> Cambridge University Press. </publisher>
Reference: [19] <author> Shawn P. Day and Michael R. Davenport. </author> <title> Continuous-time temporal back-propagation with adaptable time delays. Submitted to: </title> <journal> IEEE Transactions on Neural Networks, </journal> <month> August </month> <year> 1991. </year>
Reference-contexts: Furthermore, though initially each unit responds to each input line through a single window, new windows can be created automatically when needed so that the units can respond to every input line at any number of time delays. A related architecture, designed by Day and Davenport <ref> [19] </ref>, responds to discrete intervals of time in the past rather than to intervals convolved with a Gaussian. Their architecture allows a prespecified number of adaptable time delays to appear for any connection in the network (not just connections to the input units).
Reference: [20] <author> Peter Dayan, </author> <month> May </month> <year> 1994. </year> <type> Personal Communication. </type>
Reference-contexts: It requires access to an intelligently constructed reinforcement landscape, however, which may be hard to supply. Perhaps the agent might eventually be allowed to construct its own system of short-term rewards in order to better teach itself skills that will bring it greater long-term reinforcement <ref> [20] </ref>. Even the layered-reinforcement method is limited, though, because it assumes a single (though possibly very complicated) task. This is the case in shaping, but since it is not the case in continual learning, a different approach may be needed.
Reference: [21] <author> Peter Dayan and Geoffrey E. Hinton. </author> <title> Feudal reinforcement learning. </title> <editor> In C. L. Giles, S. J. Hanson, and J. D. Cowan, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 271-278, </pages> <address> San Mateo, California, 1993. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The two greatest distinctions are: (1) it is context sensitive and can handle ambiguous perceptual information, and (2) the hierarchies are constructed automatically in a bottom-up fashion, instead of top-down and by hand. This is in marked contrast to the approaches of Dayan and Hinton <ref> [21] </ref>, Jameson [42], Kaelbling [47], 8.4.
Reference: [22] <author> Peter Dayan and Terrence J. Sejnowski. </author> <title> TD() converges with probability 1. </title> <note> Submitted to Machine Learning, </note> <year> 1993. </year>
Reference-contexts: Over successive trials, the estimates converge under appropriate conditions to the correct values <ref> [112, 22] </ref>, i.e., the left-hand side of Equation 5.1 will converge to the expected value of the right-hand side. Once a critic has been trained the (deterministic) policy can be improved by taking random non-policy actions and comparing the discounted future rewards of the states visited.
Reference: [23] <author> Kenji Doya. </author> <title> Universality of fully-connected recurrent neural networks. </title> <journal> Submitted to IEEE Transactions on Neural Networks, </journal> <year> 1993. </year>
Reference-contexts: Bachrach suggests in his thesis [5, p. 65], that one of the key advantages of his network is the multiplicative property of the connections. The conclusion to be drawn from this is that, even though many classes of recurrent networks can theoretically perform any computable temporal task <ref> [23, 61] </ref>, higher-order networks seem to be far superior learners of difficult finite-state grammars. Temporal Transition Hierarchies also have higher-order connections (though they're not recurrent). The fast learning times that will be demonstrated in Chapter 7 are in a large part due to these higher-order connections.
Reference: [24] <author> Gary L. Drescher. </author> <title> Made-Up Minds: A Constructivist Approach to Artificial Intelligence. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference: [25] <author> Jeffrey L. Elman. </author> <title> Finding structure in time. </title> <type> CRL Technical Report 8801, </type> <institution> University of California, San Diego, Center for Research in Language, </institution> <month> April </month> <year> 1988. </year>
Reference-contexts: Limited versions of these networks have 30 Chapter 4. Solving Temporal Problems with Neural Networks been devised by Jordan [43] and Elman <ref> [25] </ref>. These networks, though capable of computing functions not computable by focused and RCC networks, nevertheless have limitations in what they can learn, due to the fact that their learning algorithms only approximate gradient descent. <p> The learning algorithm would have to be modified to make use of the recurrent connections, but it is conceivable that the network might form long-lasting representations even without such modifications. The networks proposed by Jordan [43] and Elman <ref> [25] </ref>, for example, are capable of learning to store information over several time steps without performing gradient descent on their recurrent connections (Section 4.3.3).
Reference: [26] <author> Scott E. Fahlman. </author> <title> Faster learning variations on back-propagation: An empirical study. </title> <type> Technical Report CMU-CS-88-162, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <month> September </month> <year> 1988. </year> <note> 120 Bibliography </note>
Reference: [27] <author> Scott E. Fahlman. </author> <title> The recurrent cascade-correlation architecture. </title> <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 190-196, </pages> <address> San Mateo, California, 1991. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Examples of these networks will be described next: First, Mozer's Focused Back-Propagation network [65] and Bachrach's Sticky-bit network [4]; and Second, Fahlman's RCC (Recurrent Cascade Correlation) architecture <ref> [27] </ref>. After these comes a description of fully connected recurrent networks. 4.3.1 The Focused and Sticky-bit Architectures The focused architecture uses hidden units with adjustable connections in the hidden layer to encode a variable decay rate. <p> The results for the recurrent networks are quoted from other sources <ref> [17, 27] </ref>. The mean and/or best performance is shown when available. RTRL is the Real-Time Recurrent Learning algorithm [123]. <p> The results for the recurrent networks are quoted from other sources [17, 27]. The mean and/or best performance is shown when available. RTRL is the Real-Time Recurrent Learning algorithm [123]. RCC is the Recurrent Cascade Correlation algorithm <ref> [27] </ref>. slightly more stringent than those described in the previous paragraph: the output units corresponding to the one or two possible next arcs had to have an activation level of at least 0:3). <p> Recurrent Cascade Correlation (RCC) was able to learn this task using only two or three hidden units in an average of 25,000 string presentations <ref> [27] </ref>. The transition hierarchy system learned the task in an average of 167:7 strings, but had to be constrained not to create more than forty units. Had no constraint been imposed, the system would have continued to add new units in an effort to better predict the randomly selected arcs. <p> The total number of operations required per network cycle also scales with HN w . While this is better than the scaling behavior of the RTRL algorithm, it is not as good as Bachrach's, Mozer's, or Fahlman's algorithms <ref> [4, 27, 65] </ref> (Section 4.3).
Reference: [28] <author> Scott E. Fahlman and Christian Lebiere. </author> <title> The Cascade-Correlation learning architecture. </title> <type> Technical Report CMU-CS-90-100, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <month> February </month> <year> 1990. </year>
Reference: [29] <author> Gerald Fahner. </author> <title> A higher order unit that performs arbitrary boolean functions. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, volume III, </booktitle> <pages> pages 193-197, </pages> <year> 1990. </year>
Reference-contexts: If the input patterns are binary (even if the output is continuous), sigma-pi units can learn to compute any arbitrary mapping <ref> [29, 30] </ref>. It is appealing to use a single unit instead of an entire network of units, but a fully powerful sigma-pi unit would have 2 n terms if there are n units in the input vector. Two notable solutions have emerged to the exponential number-of-terms problem.
Reference: [30] <author> Gerald Fahner and Rolf Eckmiller. </author> <title> Structural adaptation of parsimonious higher order neural classifiers. </title> <type> Technical report, </type> <institution> Department of Neuroinformatics, University of Bonn, Bonn, Germany, </institution> <year> 1992. </year>
Reference-contexts: If the input patterns are binary (even if the output is continuous), sigma-pi units can learn to compute any arbitrary mapping <ref> [29, 30] </ref>. It is appealing to use a single unit instead of an entire network of units, but a fully powerful sigma-pi unit would have 2 n terms if there are n units in the input vector. Two notable solutions have emerged to the exponential number-of-terms problem. <p> Networks The second suggested solution to the number-of-terms problem with sigma-pi units is to carefully select the terms that generate a neuron's input. (This method is closest to the Temporal Transition Hierarchies method, which dynamically adds higher-order units one at a time.) This solution was suggested by Fahner and Eckmiller <ref> [30] </ref>, who discussed two approaches toward this end (both called "parsiHON", for "parsimonious Higher-Order Neuron"). The first approach is to build a sigma-pi unit with all possible terms, and then to eliminate unnecessary ones.
Reference: [31] <author> Marcus Frean. </author> <title> The Upstart algorithm: a method for constructing and training feed-forward neural networks. </title> <type> Preprint 89/469, </type> <institution> Department of Physics and Centre for Cognitive Science, Edinburgh University, </institution> <year> 1989. </year>
Reference: [32] <author> Stephen I. Gallant. </author> <title> Three constructive algorithms for network learning. </title> <booktitle> In The Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 652-660. </pages> <publisher> Lawrence Erlbaum, </publisher> <month> August </month> <year> 1986. </year>
Reference: [33] <author> C. L. Giles, C. B. Miller, D. Chen, G. Z. Sun, H. H. Chen, and Y. C. Lee. </author> <title> Extracting and learning an unknown grammar with recurrent neural networks. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 317-324, </pages> <address> San Mateo, California, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Giles, et al. <ref> [33] </ref>, and Watrous and Kuhn [113] both used Pollack's architecture, augmenting it by doing full gradient descent in the network's three-dimensional weight matrix with respect to the error generated over multiple time steps. <p> Solving Temporal Problems with Neural Networks 1* no odd zero strings after odd one strings no 000's pairwise, an even sum of 01's and 10's number of 11's number of 0's = 0 mod 3 0*1*0*1* Table 4.1: Regular languages from Tomita [109] used by Pollack [76], Giles, et al. <ref> [33] </ref>, and Watrous and Kuhn [113] for teaching higher-order recurrent networks to recognize finite-state grammars. <p> Constructive Fully Recurrent Networks. After describing the limitations of the RCC network, Chen, et al. [15], went on to design a constructive network of their own. Theirs adds units as learning progresses to a fully connected, second-order recurrent network (that of Giles, et al. <ref> [34, 33] </ref>, discussed above). Since it is fully connected, it does not suffer from the problems faced by RCC and is theoretically Turing equivalent, though it still suffers from the poor scaling behavior of second-order recurrent networks. 4.4.
Reference: [34] <author> C. L. Giles, G. Z. Sun, H. H. Chen, Y. C. Lee, and D. Chen. </author> <title> Higher order recurrent networks & grammatical inference. </title> <editor> In David S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, California, 1990. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Constructive Fully Recurrent Networks. After describing the limitations of the RCC network, Chen, et al. [15], went on to design a constructive network of their own. Theirs adds units as learning progresses to a fully connected, second-order recurrent network (that of Giles, et al. <ref> [34, 33] </ref>, discussed above). Since it is fully connected, it does not suffer from the problems faced by RCC and is theoretically Turing equivalent, though it still suffers from the poor scaling behavior of second-order recurrent networks. 4.4.
Reference: [35] <author> C. Lee Giles and Tom Maxwell. </author> <title> Learning, invariance, and generalization in high-order neural networks. </title> <journal> Applied Optics, </journal> <volume> 26(23) </volume> <pages> 4972-4978, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: The first solution is the reduction of order in the sigma-pi units. By limiting them to second-order terms only, functions impossible to compute with traditional units can be calculated while requiring only n 2 connection weights. This approach has been followed by Giles and Maxwell <ref> [35] </ref> and Pollack [74], described next. <p> This method is appropriate for much larger problem spaces than the first method and is similar to genetic algorithms [39] (which were incidentally suggested by Giles and Maxwell as a method of finding appropriate sigma-pi terms <ref> [35] </ref>). It is unclear how this method will scale, since the fraction of all possible terms that can actually be tested in a reasonable period of time shrinks exponentially with the size of the input space. Both parsiHON methods resulted in excellent generalization.
Reference: [36] <author> Mark W. Goudreau, C. Lee Giles, Srimat T. Chakradhar, and D. Chen. </author> <title> First-order vs. second-order single layer recurrent neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <year> 1993. </year>
Reference-contexts: In particular, higher-order recurrent networks can solve very difficult tasks while converging after fewer training examples than some classes of standard recurrent networks <ref> [36, 60, 74] </ref>. These networks not only have additive connections between units, but also have multiplicative connections. Though these networks do not have a great deal in common with the methods proposed in this dissertation, they are discussed here for completeness. The network described by Pollack [76] is given below.
Reference: [37] <author> G. R. Grimmett and D. R. Stirzaker. </author> <title> Probability and Random Processes. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1985. </year>
Reference: [38] <author> Stephen Grossberg. </author> <title> A theory of human memory: Self-organization and performance of sensory-motor codes, maps, and plans. </title> <editor> In R. Rosen and F. Snell, editors, </editor> <booktitle> Progress in Theoretical Biology, </booktitle> <volume> Vol. 5, </volume> <pages> pages 223-374. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1978. </year>
Reference: [39] <author> J. H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> The University of Michigan Press, </publisher> <address> Ann Arbor, Michigan, </address> <year> 1975. </year> <note> Bibliography 121 </note>
Reference-contexts: This method is appropriate for much larger problem spaces than the first method and is similar to genetic algorithms <ref> [39] </ref> (which were incidentally suggested by Giles and Maxwell as a method of finding appropriate sigma-pi terms [35]).
Reference: [40] <author> Kurt Hornik. </author> <title> Approximation capabilities of multilayer feedforward networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 4(2) </volume> <pages> 251-257, </pages> <year> 1991. </year>
Reference: [41] <author> Jenq-Neng Hwang and Chi H. Chan. </author> <title> Iterative constrained inversion of neural networks and its applications. </title> <booktitle> In The 24th Conference on Information Systems and Sciences, </booktitle> <pages> pages 754-759, </pages> <address> Princeton, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: Network inversion is the method of using gradient descent to do constraint satisfaction not on the network weights, but on the network inputs. This technique has been used by Williams [119], Kindermann and Linden [49], Linden and Weber [57], Hwang and Chan <ref> [41] </ref>, and others, many of whom have independently developed the same technique while trying to solve the inverse-kinematics problem. <p> The second problem, closely related to the first, is that there is often a many-to-one relationship between the input patterns and the target patterns. The inversion of the network averages among these global minima to produce an interpolation of those that generate this target <ref> [41, 44, 46, 56] </ref>. 5.4. Gradient Following Methods 41 The prediction of the model is then compared with the desired state, and the difference is back-propagated through the model and into the controller where the weights are changed. <p> One solution to these problems of multiple minima, proposed by Kindermann and Linden, is to "erase" spurious local minima by adding specially designed training patterns to the training set. Another solution is to impose extra constraints so that only a single valid input will be found <ref> [41] </ref>, or so that only one input can minimize all the imposed constraints (a technique used by Jordan [44] for controller modification but not network inversion). Most likely, the network inversion problem is inherently intractable: The problem it attempts to solve is NP-hard.
Reference: [42] <author> John W. Jameson. </author> <title> Reinforcement control with hierarchical backpropagated adaptive critics. Submitted to Neural Networks, </title> <month> March </month> <year> 1992. </year>
Reference-contexts: The two greatest distinctions are: (1) it is context sensitive and can handle ambiguous perceptual information, and (2) the hierarchies are constructed automatically in a bottom-up fashion, instead of top-down and by hand. This is in marked contrast to the approaches of Dayan and Hinton [21], Jameson <ref> [42] </ref>, Kaelbling [47], 8.4.
Reference: [43] <author> Michael I. Jordan. </author> <title> Serial order: A parallel distributed processing approach. </title> <type> ICS Report 8604, </type> <institution> Institute for Cognitive Science, University of California, </institution> <address> San Diego, </address> <month> May </month> <year> 1986. </year>
Reference-contexts: Limited versions of these networks have 30 Chapter 4. Solving Temporal Problems with Neural Networks been devised by Jordan <ref> [43] </ref> and Elman [25]. These networks, though capable of computing functions not computable by focused and RCC networks, nevertheless have limitations in what they can learn, due to the fact that their learning algorithms only approximate gradient descent. <p> The learning algorithm would have to be modified to make use of the recurrent connections, but it is conceivable that the network might form long-lasting representations even without such modifications. The networks proposed by Jordan <ref> [43] </ref> and Elman [25], for example, are capable of learning to store information over several time steps without performing gradient descent on their recurrent connections (Section 4.3.3).
Reference: [44] <author> Michael I. Jordan. </author> <title> Generic constraints on underspecified target trajectories. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 217-225, </pages> <year> 1989. </year>
Reference-contexts: The second problem, closely related to the first, is that there is often a many-to-one relationship between the input patterns and the target patterns. The inversion of the network averages among these global minima to produce an interpolation of those that generate this target <ref> [41, 44, 46, 56] </ref>. 5.4. Gradient Following Methods 41 The prediction of the model is then compared with the desired state, and the difference is back-propagated through the model and into the controller where the weights are changed. <p> Another solution is to impose extra constraints so that only a single valid input will be found [41], or so that only one input can minimize all the imposed constraints (a technique used by Jordan <ref> [44] </ref> for controller modification but not network inversion). Most likely, the network inversion problem is inherently intractable: The problem it attempts to solve is NP-hard. Network inversion can also be used in reinforcement-learning problems in a straightforward and elegant way. <p> Future actions by the controller should generate predictions from the model (and therefore reactions from the environment) closer to desired values. This technique has been used both for supervised learning tasks (training the controller to produce specific results in the environment) <ref> [44, 46, 69] </ref>, and for reinforcement-learning tasks (training the controller to maximize the predicted reinforcement) [45, 68, 91, 93, 116]. Several of these designs also include recurrent neural networks (see Chapter 4). <p> This has been done to model and then to minimize the agent's ignorance of its environment [57], to maximize a balance between the agent's curiosity with and boredom of its environment [91, 93, 108], and to maximize the smoothness, distinctiveness, and speed of a robot's movements <ref> [44] </ref>. The principal disadvantage of gradient descent techniques is that they are not guaranteed to converge to the optimal policy, unlike table-look-up dynamic programming. 5.5 Some Geometric Intuition It is common to display the reinforcement-learning process as the construction and utilization of a reinforcement landscape.
Reference: [45] <author> Michael I. Jordan and Robert A. Jacobs. </author> <title> Learning to control an unstable system with forward modeling. </title> <editor> In David S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 324-331, </pages> <address> San Mateo, California, 1990. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Gradient-following techniques for determining the optimal action when the actions are continuous have been explored by Werbos [116], Munro [68], Jordan and Jacobs <ref> [45] </ref>, Schmidhuber [91, 93], Thrun, et al. [108], Thrun and Moller [107], Bachrach [5], Linden and Weber [57], and others. <p> This technique has been used both for supervised learning tasks (training the controller to produce specific results in the environment) [44, 46, 69], and for reinforcement-learning tasks (training the controller to maximize the predicted reinforcement) <ref> [45, 68, 91, 93, 116] </ref>. Several of these designs also include recurrent neural networks (see Chapter 4). A taxonomy of many of the different neural-network architectures used for reinforcement learning is given by Lin in his thesis [55], where he describes many different permutations of critics, models, and policy modules.
Reference: [46] <author> Michael I. Jordan and David E. Rumelhart. </author> <title> Forward models: Supervised learning with a distal teacher. </title> <journal> Cognitive Science, </journal> <volume> 16 </volume> <pages> 307-354, </pages> <year> 1992. </year>
Reference-contexts: The second problem, closely related to the first, is that there is often a many-to-one relationship between the input patterns and the target patterns. The inversion of the network averages among these global minima to produce an interpolation of those that generate this target <ref> [41, 44, 46, 56] </ref>. 5.4. Gradient Following Methods 41 The prediction of the model is then compared with the desired state, and the difference is back-propagated through the model and into the controller where the weights are changed. <p> Controller modification techniques, also known as distal learning <ref> [46] </ref>, are similar to the method of Thrun, et al., in that a policy or controller module is trained by doing gradient descent in the model. The difference is that these techniques do not use gradient descent to choose actions; they back-propagate the error signal directly into the controller network. <p> Future actions by the controller should generate predictions from the model (and therefore reactions from the environment) closer to desired values. This technique has been used both for supervised learning tasks (training the controller to produce specific results in the environment) <ref> [44, 46, 69] </ref>, and for reinforcement-learning tasks (training the controller to maximize the predicted reinforcement) [45, 68, 91, 93, 116]. Several of these designs also include recurrent neural networks (see Chapter 4).
Reference: [47] <author> Leslie Pack Kaelbling. </author> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Machine Learning: Proceedings of the tenth International Conference, </booktitle> <pages> pages 167-173. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: The two greatest distinctions are: (1) it is context sensitive and can handle ambiguous perceptual information, and (2) the hierarchies are constructed automatically in a bottom-up fashion, instead of top-down and by hand. This is in marked contrast to the approaches of Dayan and Hinton [21], Jameson [42], Kaelbling <ref> [47] </ref>, 8.4. <p> A method that avoids retraining with changing goals is to train on all possible goals simultaneously (cf. Kaelbling <ref> [47, 48] </ref>). This method is highly promising in that it merges controller and model together into an expanded model of the environment. Models usually represent mappings of the form: (state, action) ! next-state: Given the current state and intended action, the next state is calculated.
Reference: [48] <author> Leslie Pack Kaelbling. </author> <title> Learning to achieve goals. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1094-1098, </pages> <address> Chambery, France, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A method that avoids retraining with changing goals is to train on all possible goals simultaneously (cf. Kaelbling <ref> [47, 48] </ref>). This method is highly promising in that it merges controller and model together into an expanded model of the environment. Models usually represent mappings of the form: (state, action) ! next-state: Given the current state and intended action, the next state is calculated.
Reference: [49] <author> J. Kindermann and A. Linden. </author> <title> Inversion of neural networks by gradient descent. </title> <journal> Journal of Parallel Computing, </journal> <volume> 14(3), </volume> <year> 1990. </year>
Reference-contexts: The first is that of network inversion, the second is that of controller modification. Network inversion is the method of using gradient descent to do constraint satisfaction not on the network weights, but on the network inputs. This technique has been used by Williams [119], Kindermann and Linden <ref> [49] </ref>, Linden and Weber [57], Hwang and Chan [41], and others, many of whom have independently developed the same technique while trying to solve the inverse-kinematics problem. <p> There are two unfortunate drawbacks of applying the inversion process, however. The first is that of spurious local minima <ref> [49] </ref>. The network may converge to an input that generates an output not particularly close to the desired output (just closer than any of its neighbors in input space).
Reference: [50] <author> A. H. Klopf. </author> <title> The Hedonistic Neuron: A Theory of Memory, Learning, </title> <booktitle> and Intelligence. </booktitle> <address> Hemisphere, Washington, </address> <year> 1982. </year>
Reference-contexts: Each w ij is therefore an eligibility trace <ref> [8, 50] </ref> of weight changes that decays exponentially. The trace constantly accrues weight changes over time | biased towards the most recent ones | but the changes are only applied to the weights when a reinforcement is received.
Reference: [51] <author> Benjamin J. Kuipers and Yung-Tai Byun. </author> <title> A robust, qualitative method for robot spatial learning. </title> <booktitle> In The Seventh National Conference on Artificial Intelligence, </booktitle> <year> 1988. </year>
Reference-contexts: Another approach is that of Kuipers and Byun <ref> [51] </ref>, whose agent builds an explicit topological map of its environment to distinguish among ambiguous perceptions.
Reference: [52] <author> John E. Laird, Paul S. Rosenbloom, and Alan Newell. </author> <title> Chunking in soar: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 11-46, </pages> <year> 1986. </year>
Reference: [53] <author> Thibault Langlois and Stephane Canu. </author> <title> Control of time-delay systems using reinforcement learning. </title> <editor> In Igor Alexsander and John Taylor, editors, </editor> <booktitle> Artificial Neural Networks, 2: Proceedings of the 1992 International conference on Artificial Neural Networks (ICANN-92), </booktitle> <pages> pages 607-610, </pages> <address> Amsterdam, 1992. </address> <publisher> North-Holland, Elsevier Science Publishing. 122 Bibliography </publisher>
Reference-contexts: Thus, in the case just given, action r will be reinforced every time it is chosen. This modification has been found to improve performance in at least some cases <ref> [53] </ref>. The AHC and the method just proposed have a weakness in that weight values reflect both how good an action is as well as how frequently the action was taken. One way to 44 Chapter 5.
Reference: [54] <author> Long-Ji Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 293-321, </pages> <year> 1992. </year>
Reference-contexts: Learning without this restriction has also been done <ref> [3, 54, 92, 116] </ref>. The only real restriction with the AHC (and with most other reinforcement-learning techniques as well) is that the learning algorithm used by the policy and critic modules must be capable of distinguishing the underlying environmental states. <p> Alternatively, the critic and policy modules are often implemented as neural networks. Lin <ref> [54] </ref>, for example, used one network for the critic and separate policy networks for each action. Each network takes the current sensory vector as input. The critic network is trained to predict the dfr for each input. <p> Lin <ref> [54] </ref>, for example, used a separate network for each action, updating them with the following learning rule: Q a t = r (s t ; a t ) + fl (max Q (s t+1 ; a) Q (s t ; a t )); (5:8) where Q a t is the error <p> The network is updated using the same Q values as given by Lin <ref> [54] </ref> (Equation 5.8). The nine mazes are shown in Figure 7.9. Barriers are in black and may not be entered by the agent. Nor may the agent move beyond the boundaries of the maze.
Reference: [55] <author> Long-Ji Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1993. </year> <note> Also appears as Technical Report CMU-CS-93-103. </note>
Reference-contexts: Several of these designs also include recurrent neural networks (see Chapter 4). A taxonomy of many of the different neural-network architectures used for reinforcement learning is given by Lin in his thesis <ref> [55] </ref>, where he describes many different permutations of critics, models, and policy modules. One of the benefits of using gradient-following methods to choose actions or to modify the controller is that the behavior of the system can be highly tailored simply through modification of the error function. <p> Lin <ref> [55] </ref>, Singh [100], and Wixson [126]. These methods also combine multiple policies, but they require complete state information and architectures pre-designed for a particular task. In all hierarchical architectures there is the issue of "vertical" credit assignment: assigning credit to the correct level of the hierarchy.
Reference: [56] <author> Alexander Linden. </author> <title> On discontinuous Q-functions in reinforcement learning. </title> <editor> In H. J. Ohlbach, editor, </editor> <booktitle> Proceedings of the German Workshop on Artificial Intelligence. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1993. </year>
Reference-contexts: The second problem, closely related to the first, is that there is often a many-to-one relationship between the input patterns and the target patterns. The inversion of the network averages among these global minima to produce an interpolation of those that generate this target <ref> [41, 44, 46, 56] </ref>. 5.4. Gradient Following Methods 41 The prediction of the model is then compared with the desired state, and the difference is back-propagated through the model and into the controller where the weights are changed. <p> Of course, the same caveats apply here as apply just above for network inversion in general. If the Q-values are linear with respect to the action space, this problem need not occur since the global minimum of a linear network can be found quickly (cf. Linden <ref> [56] </ref>). Thrun, et al. [108] extended this technique by teaching the actions, once they were determined through gradient descent, to a controller (policy) network which could then suggest these actions quickly and would improve over time such that gradient descent was no longer necessary.
Reference: [57] <author> Alexander Linden and Frank Weber. </author> <title> Implementing inner drive through competence reflection. </title> <editor> In J. A. Meyer, H. Roitblat, and S. Wilson, editors, </editor> <booktitle> From Animals to Ani-mats 2: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 321-326. </pages> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Gradient-following techniques for determining the optimal action when the actions are continuous have been explored by Werbos [116], Munro [68], Jordan and Jacobs [45], Schmidhuber [91, 93], Thrun, et al. [108], Thrun and Moller [107], Bachrach [5], Linden and Weber <ref> [57] </ref>, and others. These methods are all based on the concept of using a neural network to learn a model of the environment and then doing gradient descent in this differentiable model to improve the quality of the agent's decisions. <p> Network inversion is the method of using gradient descent to do constraint satisfaction not on the network weights, but on the network inputs. This technique has been used by Williams [119], Kindermann and Linden [49], Linden and Weber <ref> [57] </ref>, Hwang and Chan [41], and others, many of whom have independently developed the same technique while trying to solve the inverse-kinematics problem. <p> as input the agent's sensations and next action and produces as its output a measure of the quality of that action (e.g., the dfr of the agent's next state), gradient ascent can be performed in action space to maximize the model's output, thus producing the optimal action for that state <ref> [5, 57, 107, 108] </ref>. When the dfr is used to measure the quality of the agent's chosen action, as was done by Bachrach [5], the result is a continuous-action Q-learning algorithm. Of course, the same caveats apply here as apply just above for network inversion in general. <p> This has been done to model and then to minimize the agent's ignorance of its environment <ref> [57] </ref>, to maximize a balance between the agent's curiosity with and boredom of its environment [91, 93, 108], and to maximize the smoothness, distinctiveness, and speed of a robot's movements [44].
Reference: [58] <author> Michael L. Littman. </author> <title> An optimization-based categorization of reinforcement learning environments. </title> <editor> In J. A. Meyer, H. Roitblat, and S. W. Wilson, editors, </editor> <booktitle> From Animals to Animats 2: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 262-270. </pages> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference: [59] <author> R. Andrew McCallum. </author> <title> Overcoming incomplete perception with Utile Distinction Memory. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <pages> pages 190-196. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: In each state there are senses corresponding to the walls immediately surrounding that state. Thus, there are sixteen possible wall configurations and therefore sixteen unique senses, as shown in Figure 7.8. (Nearly the same sensory system was used by McCallum <ref> [59] </ref>.) Since "15" only occurs if the agent is completely boxed in, it is of little use and does not appear in any of the environments below. <p> One of these, McCallum's Utile Distinction Memory (UDM) <ref> [59] </ref> is based on Chris-man's Perceptual Distinctions Approach (PDA) [16], which is a constructive Hidden Markov Model (HMM) method. Both approaches build units to represent states explicitly. <p> Both use traditional HMM training methods (i.e., the Baum-Welch procedure [78]) to estimate state occupation probabilities and to adjust observation and state-transition probabilities, and both combine these methods with Q-learning to learn action-utility values and to choose actions. Results are given by McCallum <ref> [59] </ref> for his method in the environment shown in Figure 7.12. The Utile Distinction Memory approach consistently learned the maze in 2500 training steps. Transition hierarchies learned this task in an average of under 500 steps, indicating a factor of five speedup on this particular task. <p> not varying much, this indicates that predictions are close to correct or that the Q-values are small; these are places where improvements in prediction have little impact on performance, and where new units are less likely to be created. (This approach to unit creation is done more explicitly by McCallum <ref> [59] </ref>). The new units extend the agent's current repertoire of skills by modifying its behavior (i.e., revising the Q-values on which its policy is based) to take advantage of current contextual information.
Reference: [60] <author> Clifford B. Miller and C.L. Giles. </author> <title> Experimental comparison of the effect of order in recurrent neural networks. </title> <journal> International Journal of Pattern Recognition and Aritifi-cial Intelligence, 1993. Special Issue on Applications of Neural Networks to Pattern Recognition. </journal>
Reference-contexts: In particular, higher-order recurrent networks can solve very difficult tasks while converging after fewer training examples than some classes of standard recurrent networks <ref> [36, 60, 74] </ref>. These networks not only have additive connections between units, but also have multiplicative connections. Though these networks do not have a great deal in common with the methods proposed in this dissertation, they are discussed here for completeness. The network described by Pollack [76] is given below.
Reference: [61] <author> Marvin Minsky. </author> <title> Computation: Finite and Infinite Machines. </title> <publisher> Prentice-Hall, </publisher> <year> 1967. </year>
Reference-contexts: be done incrementally as new data is presented. (Comparisons with RCC are shown in Section 7.2.1.) 4.3.3 Fully Connected Recurrent Architectures In contrast to focused and RCC networks, certain classes of fully connected recurrent networks (networks that allow cycles in their connectivity besides just self-connections) have been shown by Minsky <ref> [61] </ref> to be Turing equivalent. Limited versions of these networks have 30 Chapter 4. Solving Temporal Problems with Neural Networks been devised by Jordan [43] and Elman [25]. <p> Bachrach suggests in his thesis [5, p. 65], that one of the key advantages of his network is the multiplicative property of the connections. The conclusion to be drawn from this is that, even though many classes of recurrent networks can theoretically perform any computable temporal task <ref> [23, 61] </ref>, higher-order networks seem to be far superior learners of difficult finite-state grammars. Temporal Transition Hierarchies also have higher-order connections (though they're not recurrent). The fast learning times that will be demonstrated in Chapter 7 are in a large part due to these higher-order connections.
Reference: [62] <author> Marvin L. Minsky and Seymour A. Papert. </author> <title> Perceptrons. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1969. </year>
Reference: [63] <author> John Moody and Christian Darken. </author> <title> Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 281-294, </pages> <year> 1989. </year>
Reference: [64] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13(1), </volume> <year> 1993. </year>
Reference-contexts: One such method is RTDP (Real-Time Dynamic Programming) [11], which backs up the agent's current state together with as many other states as time permits at each step. Special cases of this are the Prioritized Sweeping <ref> [64] </ref> and the Queue-Dyna [71] algorithms, which at every step modify the values of those states where the modifications are the largest.
Reference: [65] <author> Michael C. Mozer. </author> <title> A focused back-propagation algorithm for temporal pattern recognition. </title> <type> Technical Report CRG-TR-88-3, </type> <institution> Department of Psychology, University of Toronto, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: The simplest recurrent networks look just like the feed-forward networks described in Chapter 3, except that some hidden units have single, recurrent self-connections, and there are no other recurrent connections. Examples of these networks will be described next: First, Mozer's Focused Back-Propagation network <ref> [65] </ref> and Bachrach's Sticky-bit network [4]; and Second, Fahlman's RCC (Recurrent Cascade Correlation) architecture [27]. After these comes a description of fully connected recurrent networks. 4.3.1 The Focused and Sticky-bit Architectures The focused architecture uses hidden units with adjustable connections in the hidden layer to encode a variable decay rate. <p> Instead, the network could have just a 8.4. Future Work 103 single recurrent connection from each high-level unit to itself and then perform gradient descent with respect to these connections. This is the same elegant approach used by Bachrach [4] and by Mozer <ref> [65] </ref> (Section 4.3.1). Using Bachrach's approach, these units can keep their values for any arbitrary duration. This enables each high-level unit to tune in to precisely those signals that should cause it to change its activation value, regardless of when those signals occur. <p> learning how to write or do research? Do we ever learn anything completely? Or do we just keep getting better than we were before? Appendix A Simulating a Queue With a Focused Network It is not difficult to devise an activation function that would allow a Bachrach [4] or Mozer <ref> [65] </ref> network to simulate a queue (i.e., to produce time-delayed output) given discrete inputs. One way to do this with differentiable transfer functions is as follows. Without loss of generality, assume the input to each input unit is binary. Allow one hidden unit for every input unit. <p> The total number of operations required per network cycle also scales with HN w . While this is better than the scaling behavior of the RTRL algorithm, it is not as good as Bachrach's, Mozer's, or Fahlman's algorithms <ref> [4, 27, 65] </ref> (Section 4.3).
Reference: [66] <author> Michael C. Mozer. </author> <title> Induction of multiscale temporal structure. </title> <editor> In John E. Moody, Steven J. Hanson, and Richard P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 275-282, </pages> <address> San Mateo, California, 1992. </address> <publisher> Morgan Kaufmann Publishers. Bibliography 123 </publisher>
Reference-contexts: They have also been shown to be particularly poor at learning long temporal contingencies, i.e., long time delays. As will be seen in Section 7.2.2, Temporal Transition Hierarchies can learn long time delays very quickly. Multiscale Temporal Networks. To address the problem of learning long temporal contingencies, Mozer <ref> [66] </ref> created a network that integrated aspects of his focused network into a fully connected network. These networks added a special connection from each hidden unit to itself with a built in decay rate. The decay rates were fixed before training. <p> The Reber grammar [79] demonstrates the reliability of the algorithm when learning temporal dependencies in the face of noise. It has been used often in the neural network literature to demonstrate the relative power of various recurrent-network approaches. The gap task introduced by Mozer <ref> [66] </ref>, demonstrates the system's ability to learn long temporal dependencies reliably and quickly. 7.2.1 Reber Grammar The Reber grammar is a small finite-state grammar with one or two possible transitions from every state (Figure 7.2). Transitions from one node to the next are made by way of the labeled arcs. <p> In a non-stochastic environment with a fixed training set, a less chaotic graph emerges, as is shown in the next section. 7.2.2 The Gap Task Temporal Transition Hierarchies have also been tested on the gap tasks introduced by Mozer <ref> [66] </ref>. These tasks test the ability of a learning algorithm to bridge long time delays. 7.2. Supervised-Learning Tasks 79 RMS Error for Reber Grammar episode, the algorithm saw 166 strings. It predicted the last 100 strings correctly. <p> The inputs are locally encoded (one input unit for each unique sequence item). Results of the gap tasks are given in Table 7.4. The values for the standard recurrent network and for Mozer's own variation are quoted from Mozer's paper <ref> [66] </ref>. Mozer, whose network was specifically designed to learn long temporal dependencies (Section 4.3.3), reported results for gaps up to ten, and he also stated that the network had scaled linearly even up to gaps of 24. <p> 8 6 9830 992 8 16 10 &gt; 10000 1630 12 24 24 - 26 52 40 - 42 84 Table 7.4: The Temporal Transition Hierarchy network is compared on the "variable gap" task against a standard recurrent network [88] and a network devised specifically for learning long time delays <ref> [66] </ref>. The comparison values are quoted from Mozer [66], who reported results for gaps up to ten. RMS Error for the Gap Tasks behavior among all gap sizes. Training stopped once the entire sequence (except for the first element) was correctly predicted. 82 Chapter 7. <p> 10000 1630 12 24 24 - 26 52 40 - 42 84 Table 7.4: The Temporal Transition Hierarchy network is compared on the "variable gap" task against a standard recurrent network [88] and a network devised specifically for learning long time delays <ref> [66] </ref>. The comparison values are quoted from Mozer [66], who reported results for gaps up to ten. RMS Error for the Gap Tasks behavior among all gap sizes. Training stopped once the entire sequence (except for the first element) was correctly predicted. 82 Chapter 7.
Reference: [67] <author> Michael C. Mozer and Sreerupa Das. </author> <title> A connectionist symbol manipulator that discovers the structure of context-free languages. </title> <editor> In C. L. Giles, S. J. Hanson, and J. D. Cowan, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 863-870, </pages> <address> San Mateo, California, 1993. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Once such information is used it can immediately be forgotten. This kind of memorization can be implemented using some kind of push-down automaton. Neural-network algorithms that learn to use a stack <ref> [67, 101] </ref> do exist. Integrating these into a reinforcement learning environment would be a good next step. Making them amenable to continual learning will be more difficult. 8.4.3 The Changing-Reward Problem Another problem faced by CHILD in Chapter 7 was that the environments kept changing.
Reference: [68] <author> Paul Munro. </author> <title> A dual back-propagation scheme for scalar reward learning. </title> <booktitle> In The Ninth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 165-176, </pages> <address> Hillsdale, NJ, 1987. </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: Gradient-following techniques for determining the optimal action when the actions are continuous have been explored by Werbos [116], Munro <ref> [68] </ref>, Jordan and Jacobs [45], Schmidhuber [91, 93], Thrun, et al. [108], Thrun and Moller [107], Bachrach [5], Linden and Weber [57], and others. <p> This technique has been used both for supervised learning tasks (training the controller to produce specific results in the environment) [44, 46, 69], and for reinforcement-learning tasks (training the controller to maximize the predicted reinforcement) <ref> [45, 68, 91, 93, 116] </ref>. Several of these designs also include recurrent neural networks (see Chapter 4). A taxonomy of many of the different neural-network architectures used for reinforcement learning is given by Lin in his thesis [55], where he describes many different permutations of critics, models, and policy modules.
Reference: [69] <author> Derrick Nguyen and Bernard Widrow. </author> <title> The truck backer-upper: An example of self-learning in neural networks. </title> <editor> In W. Thomas Miller, III, Richard S. Sutton, and Paul J. Werbos, editors, </editor> <booktitle> Neural Networks for Control, chapter 12, </booktitle> <pages> pages 288-299. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Future actions by the controller should generate predictions from the model (and therefore reactions from the environment) closer to desired values. This technique has been used both for supervised learning tasks (training the controller to produce specific results in the environment) <ref> [44, 46, 69] </ref>, and for reinforcement-learning tasks (training the controller to maximize the predicted reinforcement) [45, 68, 91, 93, 116]. Several of these designs also include recurrent neural networks (see Chapter 4).
Reference: [70] <author> Athanasios Papoulis. </author> <title> Probability, Random Variables, and Stochastic Processes. </title> <booktitle> McGraw-Hill Series in Systems Science. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1965. </year>
Reference: [71] <author> Jing Peng and Ronald J. Williams. </author> <title> Efficient learning and planning within the Dyna framework. </title> <editor> In J. A. Meyer, H. Roitblat, and S. Wilson, editors, </editor> <booktitle> From Animals to Ani-mats 2: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 281-290. </pages> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: One such method is RTDP (Real-Time Dynamic Programming) [11], which backs up the agent's current state together with as many other states as time permits at each step. Special cases of this are the Prioritized Sweeping [64] and the Queue-Dyna <ref> [71] </ref> algorithms, which at every step modify the values of those states where the modifications are the largest.
Reference: [72] <author> David Pierce and Benjamin Kuipers. </author> <title> Learning hill-climbing functions as a strategy for generating behaviors in a mobile robot. </title> <editor> In J. A. Meyer and S. W. Wilson, editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 327-336. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference: [73] <author> David Pierce and Benjamin Kuipers. </author> <title> Learning to explore and build maps. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI-94), </booktitle> <address> Cambridge, MA, </address> <year> 1994. </year> <note> AAAI/MIT Press. To appear. </note>
Reference-contexts: As long as there is at least a single position in the environment where perceptual data is unambiguous, the agent can always theoretically determine when two positions are identical. This approach is promising but requires prior knowledge of the underlying sensorimotor apparatus, though Pierce <ref> [73] </ref> is making progress in removing this requirement. There are also other cases when the robot might require more information than its current sensory inputs. Even if there are no two locations in the environment with identical sensory values, the robot's task may generate ambiguities.
Reference: [74] <author> Jordan B. Pollack. </author> <title> Cascaded back-propagation on dynamic connectionist networks. </title> <booktitle> In The Ninth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 391-404, </pages> <year> 1987. </year>
Reference-contexts: The first solution is the reduction of order in the sigma-pi units. By limiting them to second-order terms only, functions impossible to compute with traditional units can be calculated while requiring only n 2 connection weights. This approach has been followed by Giles and Maxwell [35] and Pollack <ref> [74] </ref>, described next. <p> In particular, higher-order recurrent networks can solve very difficult tasks while converging after fewer training examples than some classes of standard recurrent networks <ref> [36, 60, 74] </ref>. These networks not only have additive connections between units, but also have multiplicative connections. Though these networks do not have a great deal in common with the methods proposed in this dissertation, they are discussed here for completeness. The network described by Pollack [76] is given below. <p> Though these networks do not have a great deal in common with the methods proposed in this dissertation, they are discussed here for completeness. The network described by Pollack [76] is given below. There are three differences between this network and its non-recurrent counterpart <ref> [74] </ref> described in Section 3.3. First, each input and output value is time indexed (e.g., O i (t) is the value of output unit O i at time t). Second, the context units of the previous version are now the output units from the preceding time step.
Reference: [75] <author> Jordan B. Pollack. </author> <title> On Connectionist Models of Natural Language Processing. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of Illinois, Urbana, IL, </institution> <year> 1987. </year>
Reference-contexts: See Pollack <ref> [75, Ch. 4] </ref> for the clearly related technique that inspired this one. Appendix B Equivalence of SLUG and Second-order Recurrent Networks The equivalence of SLUG and the other second-order recurrent networks presented in Section 4.3.4 becomes clear once SLUG is formalized.
Reference: [76] <author> Jordan B. Pollack. </author> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 227-252, </pages> <year> 1991. </year>
Reference-contexts: These networks not only have additive connections between units, but also have multiplicative connections. Though these networks do not have a great deal in common with the methods proposed in this dissertation, they are discussed here for completeness. The network described by Pollack <ref> [76] </ref> is given below. There are three differences between this network and its non-recurrent counterpart [74] described in Section 3.3. First, each input and output value is time indexed (e.g., O i (t) is the value of output unit O i at time t). <p> Solving Temporal Problems with Neural Networks 1* no odd zero strings after odd one strings no 000's pairwise, an even sum of 01's and 10's number of 11's number of 0's = 0 mod 3 0*1*0*1* Table 4.1: Regular languages from Tomita [109] used by Pollack <ref> [76] </ref>, Giles, et al. [33], and Watrous and Kuhn [113] for teaching higher-order recurrent networks to recognize finite-state grammars.
Reference: [77] <author> William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: The parameters used for the results in Table 7.2 were: = 0:06, L = 0:15, = 0:25, fi = 0:9, * = 0:0. Considerable efforts were made to optimize these values. The system was trained repeatedly on a fixed training set using a multidimensional direction set method (Powell's method <ref> [77, x10.5] </ref>) | an optimization technique for use in the absence of gradient information. The technique can be used to find very good parameter settings without the hassle and distress that tend to accompany a manual search for good values.
Reference: [78] <author> Lawrence R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77, </volume> <year> 1989. </year>
Reference-contexts: One of these, McCallum's Utile Distinction Memory (UDM) [59] is based on Chris-man's Perceptual Distinctions Approach (PDA) [16], which is a constructive Hidden Markov Model (HMM) method. Both approaches build units to represent states explicitly. Both use traditional HMM training methods (i.e., the Baum-Welch procedure <ref> [78] </ref>) to estimate state occupation probabilities and to adjust observation and state-transition probabilities, and both combine these methods with Q-learning to learn action-utility values and to choose actions. Results are given by McCallum [59] for his method in the environment shown in Figure 7.12.
Reference: [79] <author> A. S. Reber. </author> <title> Implicit learning of artificial grammars. </title> <journal> Journal of Verbal Learning and Verbal Behavior, </journal> <volume> 5 </volume> <pages> 855-863, </pages> <year> 1967. </year> <title> Secondary source, from [17]. 124 Bibliography </title>
Reference-contexts: After these parameters are specified, the interface is called to do the training or testing and can be queried afterwards for the results. 7.2 Supervised-Learning Tasks The two supervised-learning tasks reveal the strengths of Temporal Transition Hierarchies in two kinds of environments. The Reber grammar <ref> [79] </ref> demonstrates the reliability of the algorithm when learning temporal dependencies in the face of noise. It has been used often in the neural network literature to demonstrate the relative power of various recurrent-network approaches.
Reference: [80] <author> Mark B. </author> <title> Ring. Incremental development of complex behaviors through automatic construction of sensory-motor hierarchies. </title> <editor> In Lawrence A. Birnbaum and Gregg C. Collins, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <pages> pages 343-347. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> June </month> <year> 1991. </year>
Reference: [81] <author> Mark B. </author> <title> Ring. Learning sequential tasks by incrementally adding higher orders. </title> <editor> In C. L. Giles, S. J. Hanson, and J. D. Cowan, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 115-122, </pages> <address> San Mateo, California, 1993. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [82] <author> Mark B. </author> <title> Ring. Sequence learning with incremental higher-order neural networks. </title> <type> Technical Report AI 93-193, </type> <institution> Artificial Intelligence Laboratory, University of Texas at Austin, </institution> <month> January </month> <year> 1993. </year>
Reference: [83] <author> Mark B. </author> <title> Ring. Two methods for hierarchy learning in reinforcement environments. </title> <editor> In J. A. Meyer, H. Roitblat, and S. Wilson, editors, </editor> <booktitle> From Animals to Animats 2: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 148-155. </pages> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference: [84] <author> R. L. Rivest and R. E. Schapire. </author> <title> A new approach to unsupervised learning in deterministic environments. In Pat Langley, editor, </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pages 364-375, </pages> <address> Irvine, CA, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Bachrach [5] designed a related architecture strictly for the purposes of learning finite-state automata in a connectionist system. His architecture was modeled after the finite-state-machine learning algorithm of Rivest and Schapire <ref> [84] </ref>. Bachrach's network is capable of learning some extremely difficult grammars from positive examples only. Bachrach's network, named SLUG, has a different weight matrix for every input unit. (Only one 32 Chapter 4. <p> Therefore, the results that Bachrach showed for SLUG | including its relationship to the work of Rivest and Schapire <ref> [84] </ref> | apply to recurrent second-order networks in general. In particular, Bachrach compared his system on difficult tasks to many traditional networks, including recurrent networks that used BackPropagation Through Time, and he found a great improvement. The standard networks were incapable of learning even his simplest tasks.
Reference: [85] <author> A. J. Robinson and F. Fallside. </author> <title> The utility driven dynamic error propagation network. </title> <type> Technical Report CUED/F-INFENG/TR.1, </type> <institution> Cambridge University Engineering Department, </institution> <year> 1987. </year>
Reference-contexts: Williams and Peng [121] discuss the positive and negative consequences of this approach. A different, incremental method <ref> [85, 114, 123] </ref> calculates the complete gradient as a function of the derivatives computed at the previous time step.
Reference: [86] <author> H. L. </author> <title> Roitblat. A cognitive action theory of learning. </title> <editor> In J. Delacour and J. C. S Levy, editors, </editor> <booktitle> Systems with Learning and Memory Abilities, </booktitle> <pages> pages 13-26. </pages> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <year> 1988. </year>
Reference: [87] <author> H. L. </author> <title> Roitblat. Cognitive action theory as a control architecture. </title> <editor> In J. A. Meyer and S. W. Wilson, editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 444-450. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference: [88] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. V1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: On the other hand, the input to a second-order unit is the sum of second-order products (i.e., each term is the product of a weight and the value of two units): in i = B i + j k The general case is the sigma-pi unit <ref> [88] </ref>, where the products may be of any order: in i = j Y x k : 2 Transition hierarchies also have a temporal component not present in the feed-forward networks con sidered in this chapter. <p> Other networks do compute the complete gradient and are capable of learning as well as solving difficult problems. Back-Propagation Through Time and its variants <ref> [88, 115, 121] </ref> treat the temporal characteristics of sequential tasks spatially. These algorithms create a very large virtual network by replicating the real network once for every time step and then attaching these networks together. <p> Temporal Transition Recurrent Net Net Hierarchies Hierarchies 2 468 328 4 8 6 9830 992 8 16 10 &gt; 10000 1630 12 24 24 - 26 52 40 - 42 84 Table 7.4: The Temporal Transition Hierarchy network is compared on the "variable gap" task against a standard recurrent network <ref> [88] </ref> and a network devised specifically for learning long time delays [66]. The comparison values are quoted from Mozer [66], who reported results for gaps up to ten. RMS Error for the Gap Tasks behavior among all gap sizes.
Reference: [89] <author> Terence D. Sanger. </author> <title> A tree structured adaptive network for function approximation in high-dimensional spaces. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(2) </volume> <pages> 285-301, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: ParsiHON, in contrast, captures much higher-order information and learned the spiral pattern (which extends well beyond the area covered by the training set). Another technique that choses the number of terms during training and also seems to demonstrate quite good generalization is the tree-structured method introduced by Sanger <ref> [89] </ref>. This constructive approach creates new higher-order units during training which are combined to form basis functions over the input space. Though developed independently, Sanger's criteria for the construction of new units is much like that of Wynn-Jones (cf. Section 3.2) and of Temporal Transition Hierarchies (Section 6.2.4). <p> It then creates a new hidden unit to represent an entire area of this multidimensional space. In contrast, new units in the transition hierarchy network learn to correct only a single weight's error. Even more closely related is Sanger's (also feed-forward) network <ref> [89] </ref>, which occasionally creates a new unit to correct the single weight with the greatest variance. However, unlike temporal transition hierarchies, which can at every time step build units for all weights meeting a specific criterion (Equation 6.19), Sanger's network is trained over a fixed training set until convergence. <p> The major difference is that computation of the unit's activation values must begin at the highest unit and work top-down instead of bottom-up. This is necessary since the high-level units would otherwise have no effect on lower units. Without time dependencies, the algorithm strongly resembles that of Sanger <ref> [89] </ref> and Sanger et al. [90]. 8.4.2 Recurrent Connections One obvious drawback of Temporal Transition Hierarchies is that they can only learn Markov-k tasks (learning k in the process; see Section 2.2.3). <p> Appendix D Derivation of Learning Rule for Non-Temporal Network The following derivation is nearly identical to that given in Section 6.2 but has been altered to remove the temporal component. It can therefore be used for static classification tasks, and very closely resembles the network of Sanger <ref> [89] </ref>. The equations below have nearly a one-to-one correspondence with (and are therefore numbered to correspond with) those of Section 6.2 and may be viewed side by side with them for greater clarity.
Reference: [90] <author> Terence D. Sanger, Richard S. Sutton, and Christopher J. Matheus. </author> <title> Iterative construction of sparse polynomial approximations. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 1064-1071, </pages> <address> San Mateo, California, 1992. </address> <publisher> Morgan Kaufmann Publishers. Bibliography 125 </publisher>
Reference-contexts: This is necessary since the high-level units would otherwise have no effect on lower units. Without time dependencies, the algorithm strongly resembles that of Sanger [89] and Sanger et al. <ref> [90] </ref>. 8.4.2 Recurrent Connections One obvious drawback of Temporal Transition Hierarchies is that they can only learn Markov-k tasks (learning k in the process; see Section 2.2.3).
Reference: [91] <author> Jurgen Schmidhuber. </author> <title> Making the world differentiable: On using self-supervised fully recurrent neural networks for dynamic reinforcement learning and planning in nonstationary environments. </title> <type> Technical Report FKI-126-90 (revised), </type> <institution> Technische Univer-sitat Munchen, Institut fur Informatik, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: Gradient-following techniques for determining the optimal action when the actions are continuous have been explored by Werbos [116], Munro [68], Jordan and Jacobs [45], Schmidhuber <ref> [91, 93] </ref>, Thrun, et al. [108], Thrun and Moller [107], Bachrach [5], Linden and Weber [57], and others. <p> This technique has been used both for supervised learning tasks (training the controller to produce specific results in the environment) [44, 46, 69], and for reinforcement-learning tasks (training the controller to maximize the predicted reinforcement) <ref> [45, 68, 91, 93, 116] </ref>. Several of these designs also include recurrent neural networks (see Chapter 4). A taxonomy of many of the different neural-network architectures used for reinforcement learning is given by Lin in his thesis [55], where he describes many different permutations of critics, models, and policy modules. <p> This has been done to model and then to minimize the agent's ignorance of its environment [57], to maximize a balance between the agent's curiosity with and boredom of its environment <ref> [91, 93, 108] </ref>, and to maximize the smoothness, distinctiveness, and speed of a robot's movements [44].
Reference: [92] <author> Jurgen Schmidhuber. </author> <title> Networks adjusting networks. </title> <type> Technical Report FKI-125-90 (revised), </type> <institution> Technische Universitat Munchen, Institut fur Informatik, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: Learning without this restriction has also been done <ref> [3, 54, 92, 116] </ref>. The only real restriction with the AHC (and with most other reinforcement-learning techniques as well) is that the learning algorithm used by the policy and critic modules must be capable of distinguishing the underlying environmental states.
Reference: [93] <author> Jurgen Schmidhuber. </author> <title> Adaptive confidence and adaptive curiosity. </title> <type> Technical Report FKI-149-91 (revised), </type> <institution> Technische Universitat Munchen, Institut fur Informatik, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Gradient-following techniques for determining the optimal action when the actions are continuous have been explored by Werbos [116], Munro [68], Jordan and Jacobs [45], Schmidhuber <ref> [91, 93] </ref>, Thrun, et al. [108], Thrun and Moller [107], Bachrach [5], Linden and Weber [57], and others. <p> This technique has been used both for supervised learning tasks (training the controller to produce specific results in the environment) [44, 46, 69], and for reinforcement-learning tasks (training the controller to maximize the predicted reinforcement) <ref> [45, 68, 91, 93, 116] </ref>. Several of these designs also include recurrent neural networks (see Chapter 4). A taxonomy of many of the different neural-network architectures used for reinforcement learning is given by Lin in his thesis [55], where he describes many different permutations of critics, models, and policy modules. <p> This has been done to model and then to minimize the agent's ignorance of its environment [57], to maximize a balance between the agent's curiosity with and boredom of its environment <ref> [91, 93, 108] </ref>, and to maximize the smoothness, distinctiveness, and speed of a robot's movements [44].
Reference: [94] <author> Jurgen Schmidhuber. </author> <title> A possibility for implementing curiosity and boredom in model-building neural controllers. </title> <editor> In J. A. Meyer and S. W. Wilson, editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 15-21. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference: [95] <author> Jurgen Schmidhuber. </author> <title> A fixed size storage O(n 3 ) time complexity learning algorithm for fully recurrent continually running networks. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 243-248, </pages> <year> 1992. </year>
Reference-contexts: Other algorithms <ref> [95, 102, 121] </ref> have been proposed that attempt to reduce the number of computations per time step to O (n 3 ). Though the cost of using a fully recurrent network remains high, numerous studies have demonstrated very intelligent recurrent-network behavior learned via gradient descent [122, 123].
Reference: [96] <author> Jurgen Schmidhuber. </author> <title> Learning unambiguous reduced sequence descriptions. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 291-298, </pages> <address> San Mateo, California, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [97] <author> Jurgen Schmidhuber and Reiner Wahnsiedler. </author> <title> Planning simple trajectories using neural subgoal generators. </title> <editor> In J. A. Meyer, H. Roitblat, and S. Wilson, editors, </editor> <booktitle> From Animals to Animats 2: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 196-199. </pages> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference: [98] <author> Terrence J. Sejnowski and Charles R. Rosenberg. </author> <title> Parallel networks that learn to pronounce english text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168, </pages> <year> 1987. </year>
Reference-contexts: This can also be expressed as ~s (t t ) ffi ~s (t t + 1) ffi : : : ffi ~s (t); where "ffi" is the concatenation operator. This architecture was used, for example, by NetTalk <ref> [98] </ref>, which learned to map text to phonemes. The "sense" vector was a single binary-coded alphabetic character. In this system, t was equal to seven, so seven characters were given to the system as input at every time step.
Reference: [99] <author> Satinder Pal Singh. </author> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: One option is to train the agent to seek different goals in different contexts, i.e., to develop a different context-sensitive 8.4. Future Work 105 policy for each goal, provided the goal is specified in advance (like the tasks considered by Singh <ref> [99] </ref>). Another very promising option would be to use Temporal Transition Hierarchies to learn a model of the environment (Section 5.4), inverting the model to choose good actions or to train a controller .
Reference: [100] <author> Satinder Pal Singh, Andrew G. Barto, Roderic Grupen, and Christopher Connolly. </author> <title> Robust reinforcement learning in motion planning. </title> <editor> In Jack D. Cowan, Gerald Tesauro, and Joshua Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 655-662, </pages> <address> San Mateo, California, 1994. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Lin [55], Singh <ref> [100] </ref>, and Wixson [126]. These methods also combine multiple policies, but they require complete state information and architectures pre-designed for a particular task. In all hierarchical architectures there is the issue of "vertical" credit assignment: assigning credit to the correct level of the hierarchy.
Reference: [101] <author> G. Z. Sun, H. H. Chen, C. L. Giles, Y. C. Lee, and D. Chen. </author> <title> Connectionist pushdown automata that learn context-free grammars. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, pages I-577-580, </booktitle> <address> Hillsdale, NJ, 1990. </address> <publisher> Erlbaum Associates. 126 Bibliography </publisher>
Reference-contexts: Once such information is used it can immediately be forgotten. This kind of memorization can be implemented using some kind of push-down automaton. Neural-network algorithms that learn to use a stack <ref> [67, 101] </ref> do exist. Integrating these into a reinforcement learning environment would be a good next step. Making them amenable to continual learning will be more difficult. 8.4.3 The Changing-Reward Problem Another problem faced by CHILD in Chapter 7 was that the environments kept changing.
Reference: [102] <author> Guo-Zhen Sun, Hsing-Hen Chen, and Lee Yee-Chun. </author> <title> Green's function method for fast on-line learning algorithm of recurrent neural networks. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 333-340, </pages> <address> San Mateo, California, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Other algorithms <ref> [95, 102, 121] </ref> have been proposed that attempt to reduce the number of computations per time step to O (n 3 ). Though the cost of using a fully recurrent network remains high, numerous studies have demonstrated very intelligent recurrent-network behavior learned via gradient descent [122, 123].
Reference: [103] <author> Richard S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA 01003, </address> <year> 1984. </year>
Reference-contexts: This chapter contains a description of the most common reinforcement techniques, which attempt to resolve the issues surrounding dimensions 10 and 11 in Table 2.1. It begins with reviews of the AHC (adaptive heuristic critic) <ref> [103, 106] </ref>, and Q-learning [111], though they are both currently very popular and their details are known to many. Q-learning will be used in Section 7.3 to provide the framework for CHILD. <p> The reinforcement is either rewarding or punishing. The AHC is a general architecture that increases the probability of actions that lead to greatest reward and decreases the probability of actions that lead to punishment. More formally, the AHC attempts to maximize the agent's long-term reinforcement <ref> [9, 103, 105] </ref>. This architecture divides the agent into two modules: one that chooses an action and one that estimates the agent's future reinforcement given its current input.
Reference: [104] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: (s t ; a t ) is the reinforcement received for taking action a t in state s t ; and "A =B" means that A is modified so as to reduce jB Aj, the absolute difference between A and B. 1 This is a form of temporal difference learning <ref> [104] </ref>. Over successive trials, the estimates converge under appropriate conditions to the correct values [112, 22], i.e., the left-hand side of Equation 5.1 will converge to the expected value of the right-hand side.
Reference: [105] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <editor> In Bruce W. Porter and Ray J. Mooney, editors, </editor> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: The reinforcement is either rewarding or punishing. The AHC is a general architecture that increases the probability of actions that lead to greatest reward and decreases the probability of actions that lead to punishment. More formally, the AHC attempts to maximize the agent's long-term reinforcement <ref> [9, 103, 105] </ref>. This architecture divides the agent into two modules: one that chooses an action and one that estimates the agent's future reinforcement given its current input. <p> Reinforcement Learning One way to discover the optimal policy is through the AHC method discussed above, which approximates a form of dynamic programming known as policy iteration <ref> [105] </ref> in that it successively modifies the policy until the optimal policy is reached. The optimal policy can also be determined through value iteration methods, such as Q-learning, which solve Equation 5.9 by successive approximation. <p> The landscape reflects the "goodness" of each state. The greater a state's elevation on the landscape, the better that state is. The purpose of the critic module or evaluation function is to learn this landscape. Ideally, the landscape should reflect the environment's evaluation function. by Sutton <ref> [105] </ref>), the agent may move in one of four directions: up, down, left, or right. Assuming the environment is deterministic and the discount factor is 0:9, the resulting landscape is presented in Figure 5.3b.
Reference: [106] <author> Richard S. Sutton. </author> <title> Reinforcement learning architectures for animats. </title> <editor> In J. A. Meyer and S. W. Wilson, editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 288-296. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: This chapter contains a description of the most common reinforcement techniques, which attempt to resolve the issues surrounding dimensions 10 and 11 in Table 2.1. It begins with reviews of the AHC (adaptive heuristic critic) <ref> [103, 106] </ref>, and Q-learning [111], though they are both currently very popular and their details are known to many. Q-learning will be used in Section 7.3 to provide the framework for CHILD.
Reference: [107] <author> Sebastian B. Thrun and Knut Moller. </author> <title> Active exploration in dynamic environments. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 531-538, </pages> <address> San Mateo, California, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Gradient-following techniques for determining the optimal action when the actions are continuous have been explored by Werbos [116], Munro [68], Jordan and Jacobs [45], Schmidhuber [91, 93], Thrun, et al. [108], Thrun and Moller <ref> [107] </ref>, Bachrach [5], Linden and Weber [57], and others. These methods are all based on the concept of using a neural network to learn a model of the environment and then doing gradient descent in this differentiable model to improve the quality of the agent's decisions. <p> as input the agent's sensations and next action and produces as its output a measure of the quality of that action (e.g., the dfr of the agent's next state), gradient ascent can be performed in action space to maximize the model's output, thus producing the optimal action for that state <ref> [5, 57, 107, 108] </ref>. When the dfr is used to measure the quality of the agent's chosen action, as was done by Bachrach [5], the result is a continuous-action Q-learning algorithm. Of course, the same caveats apply here as apply just above for network inversion in general.
Reference: [108] <author> Sebastian B. Thrun, Knut Moller, and Alexander Linden. </author> <title> Planning with an adaptive world model. </title> <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <address> San Mateo, California, 1991. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Gradient-following techniques for determining the optimal action when the actions are continuous have been explored by Werbos [116], Munro [68], Jordan and Jacobs [45], Schmidhuber [91, 93], Thrun, et al. <ref> [108] </ref>, Thrun and Moller [107], Bachrach [5], Linden and Weber [57], and others. These methods are all based on the concept of using a neural network to learn a model of the environment and then doing gradient descent in this differentiable model to improve the quality of the agent's decisions. <p> as input the agent's sensations and next action and produces as its output a measure of the quality of that action (e.g., the dfr of the agent's next state), gradient ascent can be performed in action space to maximize the model's output, thus producing the optimal action for that state <ref> [5, 57, 107, 108] </ref>. When the dfr is used to measure the quality of the agent's chosen action, as was done by Bachrach [5], the result is a continuous-action Q-learning algorithm. Of course, the same caveats apply here as apply just above for network inversion in general. <p> If the Q-values are linear with respect to the action space, this problem need not occur since the global minimum of a linear network can be found quickly (cf. Linden [56]). Thrun, et al. <ref> [108] </ref> extended this technique by teaching the actions, once they were determined through gradient descent, to a controller (policy) network which could then suggest these actions quickly and would improve over time such that gradient descent was no longer necessary. <p> This has been done to model and then to minimize the agent's ignorance of its environment [57], to maximize a balance between the agent's curiosity with and boredom of its environment <ref> [91, 93, 108] </ref>, and to maximize the smoothness, distinctiveness, and speed of a robot's movements [44].
Reference: [109] <author> M. Tomita. </author> <title> Dynamic construction of finite-state automata from examples using hill-climbing. </title> <booktitle> In Proceedings of the Fourth Annual Cognitive Science Conference, </booktitle> <pages> pages 150-108, </pages> <address> Ann Arbor, MI, </address> <year> 1982. </year> <title> Secondary source, </title> <note> from [76]. </note>
Reference-contexts: Solving Temporal Problems with Neural Networks 1* no odd zero strings after odd one strings no 000's pairwise, an even sum of 01's and 10's number of 11's number of 0's = 0 mod 3 0*1*0*1* Table 4.1: Regular languages from Tomita <ref> [109] </ref> used by Pollack [76], Giles, et al. [33], and Watrous and Kuhn [113] for teaching higher-order recurrent networks to recognize finite-state grammars.
Reference: [110] <author> Alex Waibel. </author> <title> Modular construction of time-delay neural networks for speech recognition. </title> <journal> Neural Computation, </journal> <volume> 1(1) </volume> <pages> 39-46, </pages> <month> Spring </month> <year> 1989. </year>
Reference-contexts: The network learned to map the middle character ~s (t 4) to its phonemic category using the surrounding six characters as context. A more sophisticated method for dealing with delay lines is the TDNN (Time-Delay Neural Network) described by Waibel <ref> [110] </ref>. This network is a hierarchy of sampled input over many time steps. The hierarchy is constructed with multiple layers of hidden units.
Reference: [111] <author> Christopher J. C. H Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: This chapter contains a description of the most common reinforcement techniques, which attempt to resolve the issues surrounding dimensions 10 and 11 in Table 2.1. It begins with reviews of the AHC (adaptive heuristic critic) [103, 106], and Q-learning <ref> [111] </ref>, though they are both currently very popular and their details are known to many. Q-learning will be used in Section 7.3 to provide the framework for CHILD. The chapter then describes some research relating reinforcement learning to dynamic programming, followed by a discussion of reinforcement-learning research using gradient-descent methods. <p> Lin's tasks used a large, complicated sensory input which allowed many opportunities for generalization. 38 Chapter 5. Reinforcement Learning 5.2 Q-learning The AHC is capable of learning many difficult reinforcement tasks, but in its standard, simultaneous form, it has not been proven to converge to the optimal policy. Q-learning <ref> [111] </ref> is a reinforcement-learning method that has been proven to converge under certain conditions [112, 111]. It is used in Section 7.3 as the reinforcement learning component of CHILD. Q-learning is actually very similar to the AHC. <p> Reinforcement Learning 5.2 Q-learning The AHC is capable of learning many difficult reinforcement tasks, but in its standard, simultaneous form, it has not been proven to converge to the optimal policy. Q-learning [111] is a reinforcement-learning method that has been proven to converge under certain conditions <ref> [112, 111] </ref>. It is used in Section 7.3 as the reinforcement learning component of CHILD. Q-learning is actually very similar to the AHC. It, in a sense, combines the functions of the two AHC modules into a single module.
Reference: [112] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: Over successive trials, the estimates converge under appropriate conditions to the correct values <ref> [112, 22] </ref>, i.e., the left-hand side of Equation 5.1 will converge to the expected value of the right-hand side. Once a critic has been trained the (deterministic) policy can be improved by taking random non-policy actions and comparing the discounted future rewards of the states visited. <p> Reinforcement Learning 5.2 Q-learning The AHC is capable of learning many difficult reinforcement tasks, but in its standard, simultaneous form, it has not been proven to converge to the optimal policy. Q-learning [111] is a reinforcement-learning method that has been proven to converge under certain conditions <ref> [112, 111] </ref>. It is used in Section 7.3 as the reinforcement learning component of CHILD. Q-learning is actually very similar to the AHC. It, in a sense, combines the functions of the two AHC modules into a single module.
Reference: [113] <author> Raymond L. Watrous and Gary M. Kuhn. </author> <title> Induction of finite-state languages using second-order recurrent networks. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 309-316, </pages> <address> San Mateo, California, 1992. </address> <publisher> Morgan Kaufmann Publishers. Bibliography 127 </publisher>
Reference-contexts: Giles, et al. [33], and Watrous and Kuhn <ref> [113] </ref> both used Pollack's architecture, augmenting it by doing full gradient descent in the network's three-dimensional weight matrix with respect to the error generated over multiple time steps. <p> Networks 1* no odd zero strings after odd one strings no 000's pairwise, an even sum of 01's and 10's number of 11's number of 0's = 0 mod 3 0*1*0*1* Table 4.1: Regular languages from Tomita [109] used by Pollack [76], Giles, et al. [33], and Watrous and Kuhn <ref> [113] </ref> for teaching higher-order recurrent networks to recognize finite-state grammars.
Reference: [114] <author> Paul J. Werbos. </author> <title> Generalization of backpropagation with application to a recurrent gas market model. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 339-356, </pages> <year> 1988. </year>
Reference-contexts: Williams and Peng [121] discuss the positive and negative consequences of this approach. A different, incremental method <ref> [85, 114, 123] </ref> calculates the complete gradient as a function of the derivatives computed at the previous time step.
Reference: [115] <author> Paul J. Werbos. </author> <title> Backpropagation through time: What it is and how to do it. </title> <booktitle> Proceedings of the IEEE, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: Other networks do compute the complete gradient and are capable of learning as well as solving difficult problems. Back-Propagation Through Time and its variants <ref> [88, 115, 121] </ref> treat the temporal characteristics of sequential tasks spatially. These algorithms create a very large virtual network by replicating the real network once for every time step and then attaching these networks together.
Reference: [116] <author> Paul J. Werbos. </author> <title> A menu of designs for reinforcement learning over time. </title> <editor> In W. Thomas Miller, III, Richard S. Sutton, and Paul J. Werbos, editors, </editor> <booktitle> Neural Networks for Control, chapter 3, </booktitle> <pages> pages 67-95. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Learning without this restriction has also been done <ref> [3, 54, 92, 116] </ref>. The only real restriction with the AHC (and with most other reinforcement-learning techniques as well) is that the learning algorithm used by the policy and critic modules must be capable of distinguishing the underlying environmental states. <p> Gradient-following techniques for determining the optimal action when the actions are continuous have been explored by Werbos <ref> [116] </ref>, Munro [68], Jordan and Jacobs [45], Schmidhuber [91, 93], Thrun, et al. [108], Thrun and Moller [107], Bachrach [5], Linden and Weber [57], and others. <p> This technique has been used both for supervised learning tasks (training the controller to produce specific results in the environment) [44, 46, 69], and for reinforcement-learning tasks (training the controller to maximize the predicted reinforcement) <ref> [45, 68, 91, 93, 116] </ref>. Several of these designs also include recurrent neural networks (see Chapter 4). A taxonomy of many of the different neural-network architectures used for reinforcement learning is given by Lin in his thesis [55], where he describes many different permutations of critics, models, and policy modules.
Reference: [117] <author> Steven D. Whitehead and Dana H. Ballard. </author> <title> Active perception and reinforcement learning. </title> <journal> Neural Computation, </journal> <volume> 2(4) </volume> <pages> 409-419, </pages> <year> 1990. </year>
Reference-contexts: In many robotics tasks, however, the robot's sensory input alone is insufficient to determine the correct action. There may be locations in the environment where the sensory information is ambiguous (a condition termed "perceptual aliasing" by Whitehead and Ballard <ref> [117] </ref>, also known as the "hidden state" problem). If the environment is more complex than Markov-1, then its state, E (t) in Equation 2.5, is "hidden," since it cannot necessarily be deduced from the current sensory information alone. (Dimensions 4-10 of Table 2.1 are related to the environment's state).
Reference: [118] <author> B. Widrow and M. E. Hoff. </author> <title> Adaptive switching circuits. </title> <booktitle> In Institute of Radio Engineers, Western Electronic Show and Convention, Convention Record, </booktitle> <pages> pages 96-104, </pages> <address> New York, </address> <year> 1960. </year>
Reference-contexts: At t + 2; the agent has moved south, and the &lt;<ME, SC&gt;, MS&gt; behavior has completed, so the output value generated by both the MS and the &lt;<ME, SC&gt;, MS&gt; units is 1:0. 6.1.2 Learning The connection weights are adjusted with the delta rule <ref> [118] </ref> amplified by the reinforcement signal: w ij (t) = R (t + t i )out j (t)(T i (t) in i (t)) (6.1) The weight change at time t of the connection from unit j to unit i is equal to the product of the learning rate , the reward-level
Reference: [119] <author> Ronald J. Williams. </author> <title> Inverting a connectionist network mapping by back-propagation of error. </title> <booktitle> In The Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 859-865. </pages> <publisher> Lawrence Erlbaum, </publisher> <month> August </month> <year> 1986. </year>
Reference-contexts: The first is that of network inversion, the second is that of controller modification. Network inversion is the method of using gradient descent to do constraint satisfaction not on the network weights, but on the network inputs. This technique has been used by Williams <ref> [119] </ref>, Kindermann and Linden [49], Linden and Weber [57], Hwang and Chan [41], and others, many of whom have independently developed the same technique while trying to solve the inverse-kinematics problem.
Reference: [120] <author> Ronald J. Williams. </author> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 229-256, </pages> <year> 1992. </year>
Reference: [121] <author> Ronald J. Williams and Jing Peng. </author> <title> An efficient gradient-based algorithm for on-line training of recurrent network trajectories. </title> <journal> Neural Computation, </journal> <volume> 2(4) </volume> <pages> 490-501, </pages> <year> 1990. </year>
Reference-contexts: Other networks do compute the complete gradient and are capable of learning as well as solving difficult problems. Back-Propagation Through Time and its variants <ref> [88, 115, 121] </ref> treat the temporal characteristics of sequential tasks spatially. These algorithms create a very large virtual network by replicating the real network once for every time step and then attaching these networks together. <p> One way to train on infinite sequences is to back-propagate over only the past n time steps | where n is a carefully chosen integer for the task to be learned | and to ignore all previous time steps. Williams and Peng <ref> [121] </ref> discuss the positive and negative consequences of this approach. A different, incremental method [85, 114, 123] calculates the complete gradient as a function of the derivatives computed at the previous time step. <p> Other algorithms <ref> [95, 102, 121] </ref> have been proposed that attempt to reduce the number of computations per time step to O (n 3 ). Though the cost of using a fully recurrent network remains high, numerous studies have demonstrated very intelligent recurrent-network behavior learned via gradient descent [122, 123].
Reference: [122] <author> Ronald J. Williams and David Zipser. </author> <title> Experimental analysis of the real-time recurrent learning algorithm. </title> <journal> Connection Science, </journal> <volume> 1(1) </volume> <pages> 87-111, </pages> <year> 1989. </year>
Reference-contexts: Other algorithms [95, 102, 121] have been proposed that attempt to reduce the number of computations per time step to O (n 3 ). Though the cost of using a fully recurrent network remains high, numerous studies have demonstrated very intelligent recurrent-network behavior learned via gradient descent <ref> [122, 123] </ref>. These networks have been shown capable of solving very difficult temporal tasks, though generally extremely slowly. They have also been shown to be particularly poor at learning long temporal contingencies, i.e., long time delays.
Reference: [123] <author> Ronald J. Williams and David Zipser. </author> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 270-280, </pages> <year> 1989. </year>
Reference-contexts: Williams and Peng [121] discuss the positive and negative consequences of this approach. A different, incremental method <ref> [85, 114, 123] </ref> calculates the complete gradient as a function of the derivatives computed at the previous time step. <p> Williams and Peng [121] discuss the positive and negative consequences of this approach. A different, incremental method [85, 114, 123] calculates the complete gradient as a function of the derivatives computed at the previous time step. This approach, termed RTRL (Real Time Recurrent Learning) by Williams and Zipser <ref> [123] </ref>, does not require the large virtual network that grows according to the length of the sequence to be learned. <p> Other algorithms [95, 102, 121] have been proposed that attempt to reduce the number of computations per time step to O (n 3 ). Though the cost of using a fully recurrent network remains high, numerous studies have demonstrated very intelligent recurrent-network behavior learned via gradient descent <ref> [122, 123] </ref>. These networks have been shown capable of solving very difficult temporal tasks, though generally extremely slowly. They have also been shown to be particularly poor at learning long temporal contingencies, i.e., long time delays. <p> The results for the recurrent networks are quoted from other sources [17, 27]. The mean and/or best performance is shown when available. RTRL is the Real-Time Recurrent Learning algorithm <ref> [123] </ref>. RCC is the Recurrent Cascade Correlation algorithm [27]. slightly more stringent than those described in the previous paragraph: the output units corresponding to the one or two possible next arcs had to have an activation level of at least 0:3). <p> The k ij values must be maintained and calculated at every time step. They are similar to the p k ij values of the RTRL algorithm as presented by Williams and Zipser <ref> [123] </ref>; though not so many of these values are required, since the network is not fully recurrent. However, it is because of this extra overhead that the recurrent variation of the algorithm lacks the elegance of the Temporal Transition Hierarchies network.
Reference: [124] <author> S. W. Wilson. </author> <title> Hierarchical credit allocation in a classifier system. </title> <editor> In M. S. Elzas, T. I. Oren, and B. P. Zeigler, editors, </editor> <title> Modeling and Simulation Methodology. </title> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1989. </year>
Reference: [125] <author> Stewart W. Wilson. </author> <title> The animat path to AI. </title> <editor> In J. A. Meyer and S. W. Wilson, editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 15-21. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference: [126] <author> Lambert E. Wixson. </author> <title> Scaling reinforcement learning techniques via modularity. </title> <editor> In Lawrence A. Birnbaum and Gregg C. Collins, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <pages> pages 368-372. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: Lin [55], Singh [100], and Wixson <ref> [126] </ref>. These methods also combine multiple policies, but they require complete state information and architectures pre-designed for a particular task. In all hierarchical architectures there is the issue of "vertical" credit assignment: assigning credit to the correct level of the hierarchy.
Reference: [127] <author> Mike Wynn-Jones. </author> <title> Node splitting: A constructive algorithm for feed-forward neural networks. </title> <journal> Neural Computing and Applications, </journal> <volume> 1(1) </volume> <pages> 17-22, </pages> <year> 1993. </year>
Reference-contexts: Temporal Transition Hierarchies 61 A related method for adding new units, but in feed-forward neural-networks, was introduced by Wynne-Jones <ref> [127] </ref> and was described briefly in Section 3.2. This method, instead of simply monitoring the statistics for each connection individually, examines all the incoming connections to a particular unit to determine whether this group as a whole is being pulled in conflicting directions in the multidimensional weight space.
References-found: 127

