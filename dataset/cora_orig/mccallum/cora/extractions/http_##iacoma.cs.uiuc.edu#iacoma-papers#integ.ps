URL: http://iacoma.cs.uiuc.edu/iacoma-papers/integ.ps
Refering-URL: http://iacoma.cs.uiuc.edu/welcome.html
Root-URL: http://www.cs.uiuc.edu
Email: l-yang3,anguyen,torrella@cs.uiuc.edu  
Title: How Processor-Memory Integration Affects the Design of DSMs  
Author: Liuxi Yang, Anthony-Trung Nguyen and Josep Torrellas 
Date: May 1997  
Web: http://iacoma.cs.uiuc.edu/iacoma/  
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: The progressive integration of processor and memory has unexpected implications for the design of DSM systems. To exploit this integration best, we claim that we need to redesign the nodes of DSM systems and then reorganize the whole machine. In this paper, we propose a new DSM organization where processor nodes have their on-chip memories configured as caches and their directory controllers have been moved off-chip. The directory controllers are built with the same processor-memory chip as the computing nodes and, therefore, can be considered as non-computing nodes. The function of these non-computing nodes is to support the cache coherence protocol and to backup the application's data in their memories. Because off-the-shelf processors are so fast, these non-computing nodes manage the coherence operations and the storage of data in their memory in software. A high-level evaluation of the proposed architecture shows that it is significantly better than idealized versions of the traditional COMA and NUMA organizations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Blume, R. Eigenmann, K. Faigin, J. Grout, J. Hoe-flinger, D. Padua, P. Petersen, W. Pottenger, L. Rauch-werger, P. Tu, and S. Weatherford. </author> <title> Effective Automatic Parallelization with Polaris. </title> <journal> International Journal of Parallel Programming, </journal> <month> May </month> <year> 1995. </year>
Reference-contexts: FFT Complex 1-D fft / Splash2 64K points 3.4 2, 8 Radix Integer radix sort / Splash2 256K keys, radix 1024 3.6 2, 8 102.swim Weather prediction / Spec95 Reference 13.2 8, 64 101.tomcatv Fluid dynamics / Spec95 Reference 14.7 8, 64 normalized to NUMA. lelized by the Polaris compiler <ref> [1] </ref>. The table also shows the problem size and footprint of the applications, as well as the size of the two on-chip caches used in the experiments. Because the working sets of each of the threads of the applications are small, we use small caches.
Reference: [2] <institution> The Standard Performance Evaluation Corporation. </institution> <address> The Spec95fp Suite. URL: http://www.specbench.org. </address>
Reference-contexts: Handler Latency Occupancy Read 19-21 29-39 Read Exclusive 19-21 47-75 Acknowledge | 15-128 Write Back 24 128 For the experiments, we run 4 applications on a MINT-based [7] execution-driven simulator of the architectures. The applications, shown in Table 3, come from the Splash2 [8] and Spec95 <ref> [2] </ref> suites. They run with 32 threads. The Spec95 applications have been automatically paral Table 3: Applications used in the experiments.
Reference: [3] <institution> Silicon Graphics Inc. </institution> <note> Origin Servers. URL: http://www.sgi.com/Products/hardware/servers. </note>
Reference-contexts: To estimate the overhead of cache coherence protocol pro cessing in AGG, we wrote the protocol handlers. We then ran them on an SGI Origin 2000 <ref> [3] </ref> and used the performance counters to determine how many cycles each handler takes on a 4-issue processor. We ran each handler many times to eliminate the effects of the cache misses, therefore giving a lower bound in the execution time.
Reference: [4] <author> P. Kogge, S. Bass, J. Brockman, D. Chen, and E. Sha. Pursuing a Petaflop: </author> <title> Point Designs for 100 TF Computers Using PIM Technologies. </title> <booktitle> In Proceedings of the 1996 Frontiers of Massively Parallel Computation Symposium, </booktitle> <pages> pages 88-97, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: 1 Introduction With vast increases in the on-chip transistor count expected in the near future, a major trend in microprocessor design is the progressive integration of processor and memory to relieve the memory access bottleneck <ref> [4, 5, 6] </ref>. Traditional node organizations, where the processor is connected to the memory via a slow memory bus are expected to be replaced by more integrated organizations. Eventually, processor and main memory are likely to share the same chip.
Reference: [5] <author> D. Patterson, T. Anderson, N. Cardwell, R. Fromm, K. Keeton, C. Kozyrakis, R. Tomas, and K. Yelick. </author> <title> A Case for Intelligent DRAM. </title> <booktitle> In IEEE Micro, </booktitle> <pages> pages 33-44, </pages> <month> March/April </month> <year> 1997. </year>
Reference-contexts: 1 Introduction With vast increases in the on-chip transistor count expected in the near future, a major trend in microprocessor design is the progressive integration of processor and memory to relieve the memory access bottleneck <ref> [4, 5, 6] </ref>. Traditional node organizations, where the processor is connected to the memory via a slow memory bus are expected to be replaced by more integrated organizations. Eventually, processor and main memory are likely to share the same chip.
Reference: [6] <author> A. Saulsbury, F. Pong, and A. Nowatzyk. </author> <title> Missing the Memory Wall: The Case for Processor/Memory Integration. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 90-101, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: 1 Introduction With vast increases in the on-chip transistor count expected in the near future, a major trend in microprocessor design is the progressive integration of processor and memory to relieve the memory access bottleneck <ref> [4, 5, 6] </ref>. Traditional node organizations, where the processor is connected to the memory via a slow memory bus are expected to be replaced by more integrated organizations. Eventually, processor and main memory are likely to share the same chip.
Reference: [7] <author> J. Veenstra and R. Fowler. MINT: </author> <title> A Front End for Efficient Simulation of Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Second International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS'94), </booktitle> <pages> pages 201-207, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Table 2: Latencies and occupancies for the major types of protocol handlers. Handler Latency Occupancy Read 19-21 29-39 Read Exclusive 19-21 47-75 Acknowledge | 15-128 Write Back 24 128 For the experiments, we run 4 applications on a MINT-based <ref> [7] </ref> execution-driven simulator of the architectures. The applications, shown in Table 3, come from the Splash2 [8] and Spec95 [2] suites. They run with 32 threads. The Spec95 applications have been automatically paral Table 3: Applications used in the experiments.
Reference: [8] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Handler Latency Occupancy Read 19-21 29-39 Read Exclusive 19-21 47-75 Acknowledge | 15-128 Write Back 24 128 For the experiments, we run 4 applications on a MINT-based [7] execution-driven simulator of the architectures. The applications, shown in Table 3, come from the Splash2 <ref> [8] </ref> and Spec95 [2] suites. They run with 32 threads. The Spec95 applications have been automatically paral Table 3: Applications used in the experiments.
References-found: 8

