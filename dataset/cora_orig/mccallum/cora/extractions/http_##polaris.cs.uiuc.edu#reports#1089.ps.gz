URL: http://polaris.cs.uiuc.edu/reports/1089.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: ILL-CONDITIONING IN NEURAL NETWORK TRAINING PROBLEMS  
Author: S. SAARINEN R. BRAMLEY AND G. CYBENKO 
Abstract: The training problem for feedforward neural networks is nonlinear parameter estimation that can be solved by a variety of optimization techniques. Much of the literature on neural networks has focused on variants of gradient descent. The training of neural networks using such techniques is known to be a slow process with more sophisticated techniques not always performing significantly better. In this paper, we show that feedforward neural networks can have ill-conditioned Hessians and that this ill-conditioning can be quite common. The analysis and experimental results in this paper lead to the conclusion that many network training problems are ill-conditioned and may not be solved more efficiently by higher-order optimization methods. While our analyses are for completely connected layered networks, they extend to networks with sparse connectivity as well. Our results suggest that neural networks can have considerable redundancy in parameterizing the function space in a neighborhood of a local minimum, independently of whether or not the solution has a small residual. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bj orck and G. Golub, </author> <title> Numerical methods for computing angles between linear subspaces, </title> <journal> Mathematics of Computation, </journal> <volume> 27 (1973), </volume> <pages> pp. 579-594. </pages>
Reference-contexts: The 2-3-2 network in Figure 1 has 19 parameters or weights and so the Jacobian has 19 singular values. Our 2-dimensional input space is sampled on a uniform mesh in <ref> [0; 1] </ref> 2 with 400 training data points in each case presented and a maximum of 2000 function evaluations is allowed in the Levenberg-Marquardt algorithm unless indicated otherwise. The implementation of the Levenberg-Marquardt algorithm used here is that written by J. More and is available in MINPACK. <p> As an example of a larger network, Figure 8 shows the singular values of the initial Jacobian for four cases for a 5-7-2 network with weights chosen randomly in the region (1; 1) and t i sampled randomly in the cube <ref> [0; 1] </ref> 5 . From the graphs we can conclude that the Jacobian becomes more ill-conditioned as the norm of the weight vector increases. Considering that the initial conditions 13 Fig. 6. <p> The cosines of the canonical angles between range spaces are used to measure how closely two blocks J 1 and J 2 of columns of J come to spanning the same space (as is needed for Cases 2 and 3). The method for computing such cosines is from <ref> [1] </ref>. When some cosines are close to 1, then range (J 1 ) and range (J 2 ) are close to sharing a subspace spanned by the corresponding canonical vectors. The first column of Table 3 lists the first four dependency cases of Section 3.3.
Reference: [2] <author> M. Buhmann, </author> <title> Multivariate interpolation in odd dimensional euclidean spaces using multi-quadratics, </title> <type> Tech. Report DAMTP 1988/NA6, </type> <institution> University of Cambridge, Dept. of Appl. Math. and Theor. Physics, </institution> <year> 1988. </year>
Reference-contexts: In this paper we will use 1 (x), but the method in this paper can be used to derive similar results for other excitation functions as well (including radial basis functions <ref> [2, 12] </ref>). 7 Table 2 Summary of the Jacobian Jacobian element Formula Corresponding unknowns J i;(j1)(h+1)+1 0 (P j ) l x u+l x t+(l1)(p+1)+1+j 0 (Q l ) first layer offsets J i;(j1)(h+1)+1+k v k 0 (P j ) l x u+l x t+(l1)(p+1)+1+j 0 (Q l ) first layer
Reference: [3] <author> G. Cybenko, </author> <title> Approximations by superpositions of a single function, </title> <journal> Mathematics of Control, Signals and Systems, </journal> <volume> 2 (1989), </volume> <pages> pp. 303-314. </pages>
Reference-contexts: These choices were made because multilayer feedforward networks are commonly used by researchers and the classification problem is one for which neural nets are potentially suitable; see <ref> [3, 15, 8] </ref>. Furthermore, the training problems examined here are primarily overdetermined, that is, the number of training data points is greater than or equal to the number of network parameters. <p> It has been shown that under very general conditions on the activation functions, such classes of neural networks as in (7) are universal approximators <ref> [3] </ref>. The main reason for choosing the two functions in (8) is that they can approximate the hard delimiter step function and are continuously differentiable. <p> These networks have been successfully used in many neural network applications [17] and they can be used also as universal function approximators <ref> [3] </ref>. The form of the one hidden layer Jacobian is shown below with a numbering similar to that in Figure 1.
Reference: [4] <author> J. Dennis and R. Schnabel, </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations, </title> <publisher> Prentice-Hall, </publisher> <year> 1983. </year>
Reference-contexts: Quasi-Newton methods have a superlinear rate of convergence if, roughly speaking, H (x fl ) is nonsingular and the matrices B k are chosen so that J T J + B k approximates H (x fl ) along the search directions; see <ref> [4] </ref> for a more detailed description of both quasi-Newton methods and their convergence properties. Newton's method has a quadratic rate of convergence, provided that the Hessian is nonsingular at x fl .
Reference: [5] <author> C. Fraley, </author> <title> Solution of Nonlinear Least-Squares Problems, </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1987. </year> <note> also available as Technical Report # STAN-CS-87-1165, </note> <institution> Department of Computer 18 Fig. </institution> <month> 10. </month> <title> Computed singular values of the columns 10-17 (label + with a dashed line), 1-9 (label * with dotted line) and all columns (label o with solid line) of the Jacobian for the example discussed in the text and Table 3 (a 2-3-2 network used for the spiral problem). </title> <publisher> Science. </publisher>
Reference-contexts: Algorithms for minimizing (x) usually take advantage of the special structure of r (x) and H (x); the reader is referred to <ref> [5] </ref> for a survey of such algorithms. 3 Table 1 Search Directions Used by Various Optimization Methods Algorithm Search Direction Steepest Descent J T f Conjugate Gradient J T f + fi ~p; with ~p = previous search direction fi = a scalar Newton (J T J + P m Gauss-Newton
Reference: [6] <author> P. Gill, W. Murray, and M. Wright, </author> <title> Practical Optimization, </title> <publisher> Academic Press, </publisher> <address> London and New York, </address> <year> 1981. </year>
Reference-contexts: However, the computational determination of r is a difficult problem, and can have a dramatic effect on the resulting search direction (see the example in <ref> [6, page 136] </ref>). Furthermore the resulting search direction has nonzero components only in a subspace of dimension r. When r t n and the subspace changes slowly from iteration to iteration, the method can fail to make sufficient progress to a minimum. <p> When r t n and the subspace changes slowly from iteration to iteration, the method can fail to make sufficient progress to a minimum. Methods for circumventing this difficulty generally add a component in the orthogonal complement of the subspace of dimension r to the search direction, as in <ref> [6] </ref>. A second regularization approach adds a small multiple fl k x k of the norm of the weight vector to the objective function, possibly allowing fl to change on each iteration.
Reference: [7] <author> G. Golub and C. Van Loan, </author> <title> Matrix Computations, </title> <publisher> John Hopkins University Press, </publisher> <address> Baltimore, 2 ed., </address> <year> 1989. </year>
Reference-contexts: The eigenvalues of B are p (with multiplicity 1) and 0 (with multiplicity p 1), while the eigenvalues of E are no larger than p* by the Gersgorin Disk Theorem. An application of the Wielandt-Hoffman Theorem <ref> [7] </ref> shows that A T A has one eigenvalue p satisfying j p pj p*, and the other eigenvalues i satisfy j i j p*.
Reference: [8] <author> J. Hertz, A. Krogh, and R. G. Palmer, </author> <title> Introduction to the theory of neural networks, </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: These choices were made because multilayer feedforward networks are commonly used by researchers and the classification problem is one for which neural nets are potentially suitable; see <ref> [3, 15, 8] </ref>. Furthermore, the training problems examined here are primarily overdetermined, that is, the number of training data points is greater than or equal to the number of network parameters.
Reference: [9] <author> D. Luenberger, </author> <title> Introduction to Linear and Nonlinear Programming, </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Except for pattern search methods, the convergence properties of optimization algorithms for differentiable functions depend on properties of the first and/or second derivatives of the objective function <ref> [9] </ref>. For example, steepest descent explicitly requires the first derivative to define its search direction, and implicitly relies on the second derivative whose properties govern the rate of convergence. <p> Steepest descent has a q-linear rate of convergence (see [11] for a definition of q-linear) with an asymptotic error constant proportional to ( 1)=( + 1), where is the condition number of H (x fl ) <ref> [9] </ref>. Conjugate gradient methods generally have a linear rate of convergence, but their behaviour depends on the definition of the conjugacy scalar fi as well as the frequency of restart, i.e., reini-tialization of the algorithm [13].
Reference: [10] <author> J. J. Mor e, </author> <title> The Levenberg-Marquardt algorithm: Implementation and theory, in Numerical Analysis, </title> <booktitle> Lecture Notes in Mathematics, </booktitle> <editor> G. A. Watson, ed., </editor> <volume> vol. 630, </volume> <year> 1977, </year> <pages> pp. 105-116. </pages>
Reference-contexts: So if fl is prevented from decreasing to zero the quality of solution can suffer, while if fl is decreased to zero eventually the same ill-conditioning problems are encountered. Nevertheless the Levenberg-Marquardt algorithm used in Section 3.4 implicitly uses this second form of regularization (see <ref> [10] </ref>), and can provide adequate solutions in many cases. For most overdetermined nonlinear least squares problems, these considerations are minor. Generally the Jacobian is full rank (but ill-conditioning can occur) and it is only at exceptional points that J is rank-deficient, but even then the rank-deficiency is small. <p> Note that the differences in the graphs for large values of jxj are small. Figure 3 shows the graph for B (x; y) for x 2 <ref> [10; 10] </ref> and y 2 [20; 20]. Note that B (x; y) varies slowly with x. It is easy to see that sup x;y A (x; y) = 1 and sup x;y B (x; y) = 0:25 from the properties of and 0 .
Reference: [11] <author> J. M. Ortega and W. C. Rheinboldt, </author> <title> Iterative Solution of Nonlinear Equations in Several Variables, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Local convergence. The local convergence properties of the methods in Table 1 depend on the size of the residual f (x fl ) and the rank and condition number of H (x fl ), the Hessian at the solution. Steepest descent has a q-linear rate of convergence (see <ref> [11] </ref> for a definition of q-linear) with an asymptotic error constant proportional to ( 1)=( + 1), where is the condition number of H (x fl ) [9].
Reference: [12] <author> M. Powell, </author> <title> Radial basis functions for multivariable interpolation: a review, in IMA Conference on Algorithms for the Approximation of Functions and Data, </title> <publisher> Oxford University Press, </publisher> <year> 1987. </year>
Reference-contexts: In this paper we will use 1 (x), but the method in this paper can be used to derive similar results for other excitation functions as well (including radial basis functions <ref> [2, 12] </ref>). 7 Table 2 Summary of the Jacobian Jacobian element Formula Corresponding unknowns J i;(j1)(h+1)+1 0 (P j ) l x u+l x t+(l1)(p+1)+1+j 0 (Q l ) first layer offsets J i;(j1)(h+1)+1+k v k 0 (P j ) l x u+l x t+(l1)(p+1)+1+j 0 (Q l ) first layer
Reference: [13] <author> M. J. D. Powell, </author> <title> Restart procedures for the conjugate gradient method, </title> <journal> Mathematical Programming, </journal> <volume> 12 (1977), </volume> <pages> pp. </pages> <month> 241-254. </month> <title> [14] , Approximation theory and methods, </title> <publisher> Cambridge University Press, </publisher> <year> 1981. </year>
Reference-contexts: Conjugate gradient methods generally have a linear rate of convergence, but their behaviour depends on the definition of the conjugacy scalar fi as well as the frequency of restart, i.e., reini-tialization of the algorithm <ref> [13] </ref>.
Reference: [15] <author> D. E. Rumelhart and J. L. McClelland, </author> <title> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: These choices were made because multilayer feedforward networks are commonly used by researchers and the classification problem is one for which neural nets are potentially suitable; see <ref> [3, 15, 8] </ref>. Furthermore, the training problems examined here are primarily overdetermined, that is, the number of training data points is greater than or equal to the number of network parameters. <p> The input layer and output layer nodes do not apply a excitation function to their inputs. Therefore the inputs in description of neural networks in general can be found in <ref> [15] </ref>. We will next derive the functional form of the network function. Let the number of inputs to the network be h, the number of first layer nodes be p, and the number of second layer nodes be s.
Reference: [16] <author> S. Saarinen, R. Bramley, and G. Cybenko, </author> <title> Neural networks, backpropagation, and automatic differentiation, in Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. Corliss, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992, </year> <pages> pp. 31-42. </pages>
Reference-contexts: Singular values and eigenvalues were computed using Matlab 3.5 running on a Sun Sparcstation-1. Jacobian and Hessian matrices were computed with 48-bit mantissa arithmetic, using a form of automatic differentiation (see <ref> [16] </ref> for implementation details). The 2-3-2 network in Figure 1 has 19 parameters or weights and so the Jacobian has 19 singular values.

References-found: 15

