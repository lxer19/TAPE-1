URL: ftp://sls-ftp.lcs.mit.edu/pub/flammia/Eurospeech_95.ps.gz
Refering-URL: http://www.sls.lcs.mit.edu/flammia/publications.html
Root-URL: 
Email: fflammia; zueg@mit:edu  
Title: EMPIRICAL EVALUATION OF HUMAN PERFORMANCE AND AGREEMENT IN PARSING DISCOURSE CONSTITUENTS IN SPOKEN DIALOGUE 1  
Author: Giovanni Flammia and Victor Zue 
Address: Cambridge, Massachusetts 02139 USA  
Affiliation: Spoken Language Systems Group Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: This paper presents an empirical study on the annotation of discourse units in spoken dialogues. The goal of this research is to examine whether task-oriented human-human dialogues can be structured as sequences of a small number of individual discourse segments that can be reliably end-pointed. The data used for this study is a corpus of 18 orthographic transcriptions of actual telephone conversations between customers and travel agents or Yellow Pages operators. We propose the use of a general agreement metric derived from the kappa coefficient and we apply it to measuring the level of agreement among human coders in bracketing discourse segments. Despite the apparent difficulty of this annotation task, we show that a level of agreement around 60% can be reached among at least three out of five coders with variable levels of expertise, using a minimal and theory-neutral set of annotation instructions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bakeman R. and Gottman J.M. </author> <title> Observing Interaction: an introduction to sequential analysis Cambridge University Press, </title> <year> 1986. </year>
Reference-contexts: Recent empirical work on discourse coding has overcome this problem by either reporting the agreement on the most critical categories (i.e. Segment Boundary) [5] or by reporting the kappa coefficient, , a measure of agreement that is used in experimental psychology <ref> [10, 6, 1] </ref>. This measure corrects the observed agreement by subtracting the estimated chance agreement P c one expects from the distributions of the coded categories. The kappa coefficient is computed as follows [1]: = 1 P c bracketings of the same text. <p> This measure corrects the observed agreement by subtracting the estimated chance agreement P c one expects from the distributions of the coded categories. The kappa coefficient is computed as follows <ref> [1] </ref>: = 1 P c bracketings of the same text. Segments E, F , H, I and N do not have a match (N crosses brackets with E and F). <p> In general, let O i be the number of tokens in i that agree with classification j according to some criterion and let O j be the number of tokens in j that agree with classification i. Following <ref> [1] </ref>, define the observed agreement: P o = N i + N j where N i and N j are the number of tokens in i and j, respectively.
Reference: [2] <author> Black E. et al. </author> <title> "A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars" Proc. </title> <booktitle> Speech and Natural Language Workshop 1991 Morgan Kauffmann, </booktitle> <address> San Mateo CA. </address> <pages> pp. 306-311. </pages>
Reference-contexts: The corpora of telephone conversations used in this research are contributed by American Airlines SABRE Travel Information Network and by BellSouth Intelliven-tures. Penn Treebank has been instrumental in facilitating the comparison of several general English parsers <ref> [2] </ref>. As part of our research into the development of interactive conversational systems, i.e., systems that can understand and respond to verbal input, we have become increasingly aware of the importance of discourse and dialogue modelling, and the need for annotated data to aid research in this area. <p> The inter-coder agreement has been found to be 80-85% [3]. For comparing syntactic annotation agreement, the Penn Treebank project measures the percentage of times the brackets in one annotation cross those in another, as well as precision and recall with respect to a reference parse tree <ref> [2] </ref>. There are a number of ways to measure inter-coder agreement for discourse annotation [5, 6, 9, 10] As seen in considered structurally similar even if they are not identical. When the segmentations are linear the annotation task is reduced to a two-way classification of each utterance (Segment Boundary vs.
Reference: [3] <author> Cole R. et al. </author> <title> "Speech as Patterns on Paper" in Perception and Production of Fluent Speech eidted by R. </title> <publisher> Cole. Erlbaum Ass. </publisher> <year> 1980. </year> <pages> pp. 3-50. </pages>
Reference-contexts: MEASURING AGREEMENT Various metrics have been proposed to measure the agreement among coders for different linguistic annotations. For example, phonetic transcriptions have been compared in terms of a pairwise inter-coder agreement, taking into account insertion/deletion as well as substitution errors. The inter-coder agreement has been found to be 80-85% <ref> [3] </ref>. For comparing syntactic annotation agreement, the Penn Treebank project measures the percentage of times the brackets in one annotation cross those in another, as well as precision and recall with respect to a reference parse tree [2].
Reference: [4] <author> Flammia G. and Zue V., </author> <title> "N.b.: A graphical user interface for annotating spoken dialogue". AAAI'95 Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, </title> <institution> Stanford University, </institution> <month> March 27-30, </month> <year> 1995. </year> <pages> pp. 40-46. </pages>
Reference-contexts: To answer the first question, we developed N.b., a graphical annotation tool, which was described in an earlier paper <ref> [4] </ref>. To answer the second question, we conducted a set of experiments in which subjects were asked to provide discourse annotation through bracketing. The purpose of this paper is to describe these experiments, and to quantify the agreement among annotators using a set of metrics. <p> Next, we will develop a measure of inter-coder consistency using the kappa coefficient. Finally, we will apply this metric to the experiments and report the results. N.b.: A FLEXIBLE ANNOTATION TOOL To aid the process of efficiently producing discourse annotations, we developed N.b. (Nota Bene), a graphical annotation tool <ref> [4] </ref>. N.b. tries to meet the goal of being portable and ergonomic. Different annotation instructions and different theories about discourse interpretation and generation can be easily incorporated into the annotation process without the need tp change the graphical user interface. N.b. can be used to tag phrases, utterances and segments.
Reference: [5] <author> Hirschberg J., Nakatani C.H. and Grosz B. </author> <title> "Conveying Discourse Structure trough Intonation Variation" Proc. </title> <booktitle> ESCA Workshop on Spoken Dialogues Systems Vigso, </booktitle> <address> Denmark, </address> <month> June </month> <year> 1995. </year> <pages> pp. 189-192. </pages>
Reference-contexts: There are a number of ways to measure inter-coder agreement for discourse annotation <ref> [5, 6, 9, 10] </ref> As seen in considered structurally similar even if they are not identical. When the segmentations are linear the annotation task is reduced to a two-way classification of each utterance (Segment Boundary vs. <p> Recent empirical work on discourse coding has overcome this problem by either reporting the agreement on the most critical categories (i.e. Segment Boundary) <ref> [5] </ref> or by reporting the kappa coefficient, , a measure of agreement that is used in experimental psychology [10, 6, 1]. This measure corrects the observed agreement by subtracting the estimated chance agreement P c one expects from the distributions of the coded categories.
Reference: [6] <author> Isard A. and Carletta J. </author> <title> "Replicability of transaction and action coding in the Map Task Corpus" AAAI'95 Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, </title> <institution> Stanford University, </institution> <month> March 27-30, </month> <year> 1995. </year> <pages> pp. 60-66. </pages>
Reference-contexts: There are a number of ways to measure inter-coder agreement for discourse annotation <ref> [5, 6, 9, 10] </ref> As seen in considered structurally similar even if they are not identical. When the segmentations are linear the annotation task is reduced to a two-way classification of each utterance (Segment Boundary vs. <p> Recent empirical work on discourse coding has overcome this problem by either reporting the agreement on the most critical categories (i.e. Segment Boundary) [5] or by reporting the kappa coefficient, , a measure of agreement that is used in experimental psychology <ref> [10, 6, 1] </ref>. This measure corrects the observed agreement by subtracting the estimated chance agreement P c one expects from the distributions of the coded categories. The kappa coefficient is computed as follows [1]: = 1 P c bracketings of the same text.
Reference: [7] <author> Marcus M., Santorini S., and M. Marcinkiewicz, </author> <title> "Building a large annotated corpus of English: </title> <journal> the Penn Treebank" Computational Linguistics. </journal> <volume> Vol. 19. No. 2. </volume> <year> 1993. </year> <pages> pp. 313-330. </pages>
Reference-contexts: Therefore, corpora annotated by one site may not be useful to other sites, leading to duplication of effort and inhibiting cross-system comparisons. One approach to dealing with this problem is to provide a minimal set of theory-neutral annotations exemplified by the Penn Treebank <ref> [7] </ref>, in which the syntactic structure of sentences is implicitly described by bracketing major constituents without actually attaching labels to them. In fact, the 1 This research was supported by ARPA under Contract N6601-94-C-6040, monitored through the Office of Naval Command, Control and Ocean Surveillance Center.
Reference: [8] <author> Marcus M. </author> <type> personal communication. </type>
Reference-contexts: Experience from the Penn Treebank project indicated that inter-coder agreement increased drammatically when elaborate annotation instructions (a more than 300 pages document) had been provided, and the annotators had been extensively trained <ref> [8] </ref>. To increase the bracketing agreement and reduce the number of outliers, we believe that a small set of theory-neutral guidelines is necessary for handling the four most frequent types of disagreements.
Reference: [9] <author> Passoneau R.J. and Litman D.J. </author> <title> "Intention-based segmentation: Human reliability and correlation with linguistic cues" Proc. </title> <booktitle> 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1993. </year> <pages> pp. 148-155. </pages>
Reference-contexts: There are a number of ways to measure inter-coder agreement for discourse annotation <ref> [5, 6, 9, 10] </ref> As seen in considered structurally similar even if they are not identical. When the segmentations are linear the annotation task is reduced to a two-way classification of each utterance (Segment Boundary vs.
Reference: [10] <author> Rose C.P. </author> <title> "Conversation Acts, Interactional Structure and Conversational Outcomes" AAAI'95 Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, </title> <institution> Stanford University, </institution> <month> March 27-30, </month> <year> 1995. </year> <pages> pp. 132-136. </pages>
Reference-contexts: There are a number of ways to measure inter-coder agreement for discourse annotation <ref> [5, 6, 9, 10] </ref> As seen in considered structurally similar even if they are not identical. When the segmentations are linear the annotation task is reduced to a two-way classification of each utterance (Segment Boundary vs. <p> Recent empirical work on discourse coding has overcome this problem by either reporting the agreement on the most critical categories (i.e. Segment Boundary) [5] or by reporting the kappa coefficient, , a measure of agreement that is used in experimental psychology <ref> [10, 6, 1] </ref>. This measure corrects the observed agreement by subtracting the estimated chance agreement P c one expects from the distributions of the coded categories. The kappa coefficient is computed as follows [1]: = 1 P c bracketings of the same text.
References-found: 10

