URL: ftp://psyche.mit.edu/pub/jordan/boltzmann.trees.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/jordan.html
Root-URL: 
Title: Learning in Boltzmann Trees  
Author: Lawrence Saul and Michael Jordan 
Date: January 31, 1995  
Address: 79 Amherst Street, E10-243 Cambridge, MA 02139  
Affiliation: Center for Biological and Computational Learning Massachusetts Institute of Technology  
Abstract: We introduce a large family of Boltzmann machines that can be trained using standard gradient descent. The networks can have one or more layers of hidden units, with tree-like connectivity. We show how to implement the supervised learning algorithm for these Boltzmann machines exactly, without resort to simulated or mean-field annealing. The stochastic averages that yield the gradients in weight space are computed by the technique of decimation. We present results on the problems of N -bit parity and the detection of hidden symmetries.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley, D.H., Hinton, G.E., and Sejnowski, T.J. </author> <year> (1985), </year> <title> A Learning Algorithm for Boltzmann Machines, </title> <booktitle> Cognitive Science 9, </booktitle> <pages> 147-169 Binder, </pages> <editor> K., and Heerman, D.W. </editor> <year> (1988), </year> <title> Monte Carlo Simulation in Statistical Mechanics, </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference-contexts: 1 Introduction Boltzmann machines <ref> (Ackley, Hinton, & Sejnowski, 1985) </ref> have several compelling virtues. Unlike simple perceptrons, they can solve problems that are not linearly separable. The learning rule, simple and locally based, lends itself to massive parallelism. The theory of Boltzmann learning, moreover, has a solid foundation in statistical mechanics.
Reference: <author> Eggarter, </author> <title> T.P. (1974), Cayley Trees, the Ising Problem, and the Thermodynamic Limit, </title> <journal> Physical Review B 9, </journal> <pages> 2989-2992. </pages>
Reference: <author> Frean, M. </author> <year> (1990), </year> <title> The Upstart Algorithm: A Method for Constructing and Training Feedforward Neural Networks, </title> <booktitle> Neural Computation 2, </booktitle> <pages> 198-209. </pages>
Reference-contexts: Boltzmann trees with exact Boltzmann learning may present a viable option for problems in which the basic assumption behind mean-field learning| that the units in the network can be treated independently|does not hold. We know of constructive algorithms <ref> (Frean, 1990) </ref> for feed-forward nets that yield tree-like solutions; an analogous construction for Boltzmann machines has obvious appeal, in view of the potential for exact computations.
Reference: <author> Freund, Y. and Haussler, D. </author> <year> (1992), </year> <title> Unsupervised Learning of Distributions on Binary Vectors using Two Layer Networks, </title> <booktitle> In Advances in Neural Information Processing Systems IV (Denver 1992), </booktitle> <editor> ed. J. E. Moody, S. J. Hanson, </editor> <publisher> and R. </publisher>
Reference: <editor> P. Lippman, </editor> <address> 912-919. San Mateo: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Galland, C. C. </author> <year> (1993), </year> <title> The Limitations of Deterministic Boltzmann Machine Learning, Network: </title> <booktitle> Computation in Neural Systems 4, </booktitle> <pages> 355-379. </pages> <note> 10 Hertz, </note> <author> J., Krogh, A., and Palmer, R.G. </author> <year> (1991), </year> <title> Introduction to the Theory of Neural Computation, </title> <address> Redwood City: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: For many problems, this approximation works surprisingly well (Hinton, 1989), so that mean-field Boltzmann machines learn much more quickly than their stochastic counterparts. Under certain circumstances, however, the approximation breaks down, and the mean-field learning rule works badly if at all <ref> (Galland, 1993) </ref>. Another approach (Hopfield, 1987) is to focus on Boltz- mann machines with architectures simple enough to permit exact computations. Learning then proceeds by straightforward gradient descent on the cost function (Yair & Gersho, 1988), without the need for simulated or mean-field annealing.
Reference: <author> Hinton, G.E. </author> <year> (1989), </year> <title> Deterministic Boltzmann Learning Performs Steepest Descent in Weight Space, </title> <booktitle> Neural Computation 1, </booktitle> <pages> 143-150. </pages>
Reference-contexts: There have been efforts to overcome these difficulties. Peterson and Ander- son (1987) introduced a mean-field version of the original Boltzmann learning 1 shown) are fully connected to all the units in the tree. rule. For many problems, this approximation works surprisingly well <ref> (Hinton, 1989) </ref>, so that mean-field Boltzmann machines learn much more quickly than their stochastic counterparts. Under certain circumstances, however, the approximation breaks down, and the mean-field learning rule works badly if at all (Galland, 1993).
Reference: <author> Hopfield, J.J. </author> <year> (1987), </year> <title> Learning Algorithms and Probability Distributions in Feed-Forward and Feed-Back Networks, </title> <booktitle> Proceedings of the National Academy of Sciences, USA 84, </booktitle> <pages> 8429-8433. </pages>
Reference-contexts: For many problems, this approximation works surprisingly well (Hinton, 1989), so that mean-field Boltzmann machines learn much more quickly than their stochastic counterparts. Under certain circumstances, however, the approximation breaks down, and the mean-field learning rule works badly if at all (Galland, 1993). Another approach <ref> (Hopfield, 1987) </ref> is to focus on Boltz- mann machines with architectures simple enough to permit exact computations. Learning then proceeds by straightforward gradient descent on the cost function (Yair & Gersho, 1988), without the need for simulated or mean-field annealing.
Reference: <author> Itzykson, C. and Drouffe, J. </author> <year> (1991), </year> <title> Statistical Field Theory, </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference-contexts: There are also decimation rules for q-state (Potts) units, with q &gt; 2 <ref> (Itzykson & Drouffe, 1991) </ref>. The algorithm for Boltzmann trees raises a number of interesting questions. Some of these involve familiar issues in neural network design|for instance, how to choose the number of hidden layers and units.
Reference: <author> Kirkpatrick, S., Gellatt Jr, </author> <title> C.D. and Vecchi, </title> <publisher> M.P. </publisher> <year> (1983), </year> <title> Optimization by Simulated Annealing, </title> <booktitle> Science 220, </booktitle> <pages> 671-680. </pages>
Reference-contexts: The learning rule, simple and locally based, lends itself to massive parallelism. The theory of Boltzmann learning, moreover, has a solid foundation in statistical mechanics. Unfortunately, Boltzmann machines| as originally conceived|also have some serious drawbacks. In practice, they are relatively slow. Simulated annealing <ref> (Kirkpatrick, Gellat, & Vecchi, 1983) </ref>, though effective, entails a great deal of computation. Finally, compared to backpropagation networks (Rumelhart, Hinton, & Williams, 1986), where weight updates are computed by the chain rule, Boltzmann machines lack a certain degree of exactitude. <p> The main drawback of Boltzmann learning is that, in most networks, it is not possible to compute the gradients in weight space directly. Instead, one must resort to estimating the correlations hS i S j i by Monte Carlo simulation (Binder et al, 1988). The method of simulated annealing <ref> (Kirkpatrick et al, 1983) </ref> leads to accurate estimates but has the disadvantage of being very computation-intensive. A mean-field version of the algorithm (Peterson & An- derson, 1987) was proposed to speed up learning.
Reference: <author> Lauritzen, S. L. and Spiegelhalter, D. J. </author> <year> (1988), </year> <title> Local Computations with Probabilities on Graphical Structures and their Application to Expert Systems, </title> <journal> Journal of the Royal Statistical Society B 50, </journal> <pages> 157-224. </pages>
Reference-contexts: Finally, the tractability of Boltzmann trees is reminiscent of the tractability of tree-like belief networks, proposed by Pearl (1986, 1988); more sophisticated rules for computing probabilities in belief networks <ref> (Lauritzen & Spiegelhalter, 1988) </ref> may have useful counterparts in Boltzmann machines. These issues and others are left for further study. Acknowledgements The authors thank Mehran Kardar for useful discussions.
Reference: <author> Mtller, M. F. </author> <year> (1993), </year> <title> A Scaled Conjugate Gradient Algorithm for Fast Supervised Learning, </title> <booktitle> Neural Networks 6, </booktitle> <pages> 525-533. </pages>
Reference-contexts: The results show Boltzmann trees to be competitive with standard back-propagation networks <ref> (Mtller, 1993) </ref>. We also tested Boltzmann trees on the problem of detecting hidden symmetries. In the simplest version of this problem, the input patterns are square pixel arrays which have mirror symmetry about a fixed horizontal or vertical axis (but not both).
Reference: <author> Pearl, J. </author> <year> (1986), </year> <title> Fusion, Propagation, and Structuring in Belief Networks, </title> <booktitle> Artificial Intelligence 19, </booktitle> <pages> 241-288. </pages>
Reference: <author> Pearl, J. </author> <year> (1988), </year> <title> Probabilistic Reasoning in Intelligent Systems, </title> <address> San Mateo: </address> <publisher> Morgan Kauffman. </publisher>
Reference: <author> Peterson, C. and Anderson, J.R. </author> <year> (1987), </year> <title> A Mean Field Theory Learning Algorithm for Neural Networks, </title> <booktitle> Complex Systems 1, </booktitle> <pages> 995-1019. </pages>
Reference-contexts: Instead, one must resort to estimating the correlations hS i S j i by Monte Carlo simulation (Binder et al, 1988). The method of simulated annealing (Kirkpatrick et al, 1983) leads to accurate estimates but has the disadvantage of being very computation-intensive. A mean-field version of the algorithm <ref> (Peterson & An- derson, 1987) </ref> was proposed to speed up learning. It makes the approximation hS i S j i hS i ihS j i in the learning rule and estimates the magnetizations hS i i by solving a set of nonlinear equations. <p> The results, averaged over 100 separate trials, are shown in Figure 4. After 100 epochs, average performance was over 95% on the training set and over 85% on the test set. Finally, we investigated the use of the deterministic, or mean-field, learning rule <ref> (Peterson & Anderson, 1987) </ref> in Boltzmann trees. We repeated our experiments, substituting hS i ihS j i for hS i S j i in the update rule. Note that we computed the magnetizations hS i i exactly using decimation.
Reference: <author> Press, W.H., Flannery, B.P., Teukolsky, S.A, and Vetterling, W.T. </author> <year> (1986), </year> <title> Numerical Recipes, </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference-contexts: Monte Carlo estimates of stochastic averages (Binder & Heerman, 1988) are not sufficiently accurate to permit further refinements to the learning rule, such as quasi-Newton or conjugate-gradient techniques <ref> (Press, Flannery, Teukolsky, & Vetterling 1986) </ref>. There have been efforts to overcome these difficulties. Peterson and Ander- son (1987) introduced a mean-field version of the original Boltzmann learning 1 shown) are fully connected to all the units in the tree. rule. <p> Clearly, a number of techniques used in back-propagation networks, such as conjugate-gradient and quasi-Newton methods <ref> (Press et al, 1986) </ref>, could also be used to accelerate learning in Boltzmann trees. In this paper, we have considered the basic architecture in which a single output unit sits atop a tree of one or more hidden layers.
Reference: <author> Rumelhart, D.E., Hinton, G.E., and Williams, R.J. </author> <year> (1986), </year> <title> Learning Representations by Back-Propagating Errors, </title> <booktitle> Nature 323, </booktitle> <pages> 533-536. </pages>
Reference-contexts: The theory of Boltzmann learning, moreover, has a solid foundation in statistical mechanics. Unfortunately, Boltzmann machines| as originally conceived|also have some serious drawbacks. In practice, they are relatively slow. Simulated annealing (Kirkpatrick, Gellat, & Vecchi, 1983), though effective, entails a great deal of computation. Finally, compared to backpropagation networks <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>, where weight updates are computed by the chain rule, Boltzmann machines lack a certain degree of exactitude.
Reference: <author> Sejnowski, T.J. Kienker, P.K., and Hinton, G.E. </author> <year> (1986), </year> <title> Learning Symmetry Groups with Hidden Units, </title> <journal> Physica 22D, </journal> <pages> 260-275. </pages>
Reference-contexts: Again, we use recursive decimation to compute the relevant stochastic averages. We are thus able to implement the Boltzmann learning rule in an exact way. 4 Results We tested Boltzmann trees on two familiar problems: N -bit parity and the detection of hidden symmetries <ref> (Sejnowski et al, 1986) </ref>.
Reference: <author> Yair, E. and Gersho, A. </author> <year> (1988), </year> <title> The Boltzmann Perceptron Network: A MultiLayered Feed-Forward Network Equivalent to the Boltzmann Machine, </title> <booktitle> In Advances in Neural Information Processing Systems I (Denver 1988), </booktitle> <publisher> ed. D.S. </publisher>
Reference-contexts: Another approach (Hopfield, 1987) is to focus on Boltz- mann machines with architectures simple enough to permit exact computations. Learning then proceeds by straightforward gradient descent on the cost function <ref> (Yair & Gersho, 1988) </ref>, without the need for simulated or mean-field annealing. Hopfield (1987) wrote down the complete set of learning equations for a Boltz- mann machine with one layer of non-interconnected hidden units. Freund and Haussler (1992) derived the analogous equations for the problem of unsupervised learning.
Reference: <editor> Touretzky, </editor> <address> 116-123. San Mateo: </address> <publisher> Morgan Kaufman. </publisher> <pages> 11 </pages>
References-found: 20

