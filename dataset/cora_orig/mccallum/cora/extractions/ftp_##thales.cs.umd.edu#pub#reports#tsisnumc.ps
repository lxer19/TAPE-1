URL: ftp://thales.cs.umd.edu/pub/reports/tsisnumc.ps
Refering-URL: ftp://thales.cs.umd.edu/pub/biographical/xapp.html
Root-URL: 
Title: A Two-Stage Iteration for Solving Nearly Uncoupled Markov Chains  
Author: G. W. Stewart W. J. Stewart D. F. McAllister 
Note: This work was supported by the National Science Foundation under Grant MCS-83-00438.  
Address: Raleigh, NC 27650.  
Affiliation: Department of Computer Science, North Carolina State University,  
Date: April 1984 Reissued August 1991  
Pubnum: CSC TR-1348  
Abstract: This paper is concerned with an iteration for determining the steady-state probability vector of a nearly uncoupled Markov Chain. The states of these chains can be partitioned into aggregates with low probabilities of transitions between aggregates. The iteration consists of alternating block Gauss-Seidel iterations with Rayleigh-Ritz refinements. Under natural regularity conditions, the composite iteration reduces the error by a factor proportional to the size of the coupling between aggregates, so that the more loosely the chain is coupled, the faster the convergence. fl Department of Computer Science, University of Maryland, College Park, MD 20742. This work was supported by the Air Force Office of Sponsored Research under Grant AFOSR-82-0078. This report is available by anonymous ftp from thales.cs.umd.edu in the directory pub/reports. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.-J. Courtois. </author> <title> Decomposability. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: The diagonals of T z are usually found by solving eigenvalue problems associated with with the A ii . For example, Stewart [8] takes the ^z i to be the normalized left eigenvectors of the A ii ; Courtois <ref> [1] </ref> takes them to be the normalized left eigenvectors of stochastic approximations A fl ii to the A ii . Under appropriate assumptions, either method produces an O (*) approximation to y. <p> One approach is to assume a specific parameterization for A. For example, in <ref> [1] </ref> and [10] it is assumed that A can be written in the form A = A fl + *B; (2:1) where A fl is a block diagonal stochastic matrix. Although this approach works, it has two drawbacks.
Reference: [2] <author> J. J. Dongarra, J. R. Bunch, C. B. Moler, and G. W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: The preferred method is to compute an LU factorization of the matrix (I A j j) and use this factorization to solve the system <ref> [2] </ref>. Because the matrix does not change from iteration to iteration, the factorization need be done only once, which represents a great savings in computational effort.
Reference: [3] <author> S. Karlin and H. M. Taylor. </author> <title> A First Course in Stochastic Processes. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <note> second edition, </note> <year> 1975. </year>
Reference-contexts: Since A is stochastic, it satisfies Ax = x; (1:5) where x denotes the vector whose components are all one. We shall assume that the chain is irreducible and aperiodic <ref> [3, Ch. 2] </ref>, so that there is a unique, positive steady state vector y that satisfies y T A = y T (1:6) y T x = 1: (1:7) The purpose of this paper is to introduce a composite algorithm for computing y.
Reference: [4] <author> D. F. McAllister, G. W. Stewart, and W. J. Stewart. </author> <title> On a Rayleigh-Ritz refinement technique for nearly uncoupled stochastic matrices. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 60 </volume> <pages> 1-25, </pages> <year> 1984. </year>
Reference-contexts: Both steps are necessary, since the Gauss-Seidel step may reduce fl 2 only slowly, while the Rayleigh-Ritz step is stationary after one application (because T z = T ~z ). In this paper we shall analyze only the Gauss-Seidel step, since the Rayleigh-Ritz step has been analyzed elsewhere <ref> [4] </ref>. The latter is closely related to methods of aggregation, which were first proposed in this connection by Simon and Ando [5]. The idea is to determine a vector z such that T z is a good approximation to T y and then apply a Rayleigh-Ritz step to approximate y. <p> The chief drawback of the Two-Stage Iteration 5 algorithm is that it requires the solution of l eigenvalue problems to compute the ^z i . The paper is organized as follows. In the next section material from <ref> [4] </ref> on the structure of nearly uncoupled Markov chains is reviewed. Careful attention will be devoted to the conditions that must obtain as * approaches zero in order for our results to be valid. In x3 the Gauss-Seidel step is analyzed. <p> In x3 the Gauss-Seidel step is analyzed. The paper concludes with a section containing a discussion of the computational issues and an example. 2. The Structure of Nearly Uncoupled Markov Chains In this section we shall review some results found in <ref> [4] </ref> on the structure of nearly uncoupled Markov chains. The purpose is twofold. First, among the results is a precise statement of the behavior of the Rayleigh-Ritz refinement. Second, the notation and results are needed to establish the properties of the Gauss-Seidel step. <p> But it is easily verified that v T Thus (2.3) guards against the effects of asymptotic reducibility. The second condition requires some further notation. In <ref> [4] </ref> it is shown that there are matrices J i and K i such that (x i J i ) 1 = (^y i K i ) T (i = 1; 2; : : : ; l): (2:5) Moreover, for i = 1; 2; : : : ; l the norms <p> To see this, note that as * approaches zero, the eigenvalues of A approach those of the A ii , or equivalently those of the matrices (2.6), which by (2.5) are similar to the A ii . Now it is shown in <ref> [4] </ref> that g and h approach zero as * approaches zero, and it is easy to verify that the fi i approach one. Thus the smallest n l eigenvalues of A approach the eigenvalues of the C i . <p> The proof may be found in <ref> [4] </ref>. Two-Stage Iteration 8 Theorem 2.1. Let A satisfy Regularity Conditions 1 and 2. Then for all sufficiently small * the decomposition (2.14-2.15) exists. <p> This behavior of nearly uncoupled chains was first noted by Simon and Ando [5]. [We note in passing that a condition different from Regularity Condition 2 was used by the authors of this paper to establish the results in <ref> [4] </ref>. Two-Stage Iteration 9 However, that condition is implied by Regularity Condition 2 along with (2.20) and (2.21).] The spaces associated with the slow and fast transients are highly structured. <p> Regularity Condition 3. There is a constant M 3 such that k (I ~ A 2 ) 1 k &lt; M 3 * 1 (2:22) With the third regularity condition added to the first two, the following theorem is valid <ref> [4] </ref>. Theorem 2.2. Let A satisfy Regularity Conditions 1, 2, and 3. Let z &gt; 0 be given with kzk 1 = 1. Let fl i be defined by (1.17).
Reference: [5] <author> H. A. Simon and A. Ando. </author> <title> Aggregation of variables in dynamic systems. </title> <journal> Econometrica, </journal> <volume> 29 </volume> <pages> 111-138, </pages> <year> 1961. </year>
Reference-contexts: In this paper we shall analyze only the Gauss-Seidel step, since the Rayleigh-Ritz step has been analyzed elsewhere [4]. The latter is closely related to methods of aggregation, which were first proposed in this connection by Simon and Ando <ref> [5] </ref>. The idea is to determine a vector z such that T z is a good approximation to T y and then apply a Rayleigh-Ritz step to approximate y. The diagonals of T z are usually found by solving eigenvalue problems associated with with the A ii . <p> Consequently, A s 3 approaches zero more swiftly than A s 2 ; that is, Y 3 corresponds to a fast transient. This behavior of nearly uncoupled chains was first noted by Simon and Ando <ref> [5] </ref>. [We note in passing that a condition different from Regularity Condition 2 was used by the authors of this paper to establish the results in [4].
Reference: [6] <author> B. T. Smith, J. M. Boyle, J. J. Dongarra, B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide Lecture Notes in Computer Science. </title> <publisher> Springer-Verlag, </publisher> <address> New York, 2nd edition, </address> <year> 1976. </year>
Reference-contexts: are independent of the state of the network; [We choose = 0:03 and ffi = 0:95.] In our first experiment with this model, we choose values of N (6) and M (3) so that the resulting probability matrix is small enough (order 22) to analyze completely using the QR algorithm <ref> [6] </ref>. This permits us to compute the norms fl 1 and fl 2 of the errors along the slow and fast transient subspaces. Table 4.1 presents the results first of alternating Rayleigh-Ritz steps with Gauss-Seidel steps and then of following a Rayleigh-Ritz step with five Gauss-Seidel steps.
Reference: [7] <author> G. W. Stewart. </author> <title> Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year> <title> Two-Stage Iteration 19 </title>
Reference-contexts: For later use we shall write A = D + E; (1:2) where D = diag (A 11 ; A 22 ; : : : ; A ll ); (1:3) and set where k k denotes the spectral norm <ref> [7, Ch. 4] </ref>. Since A is stochastic, it satisfies Ax = x; (1:5) where x denotes the vector whose components are all one. <p> Again let z, the current approximation to y, be partitioned as in (1.9), and assume that the components of z are positive. Let ^z j = z j =kz j k 1 , where k k 1 denotes the 1-norm <ref> [7, Ch. 4] </ref>. The space from which the new approximation ~z is to be extracted is spanned by the columns of the matrix T z = B B @ 0 ^z 2 0 . . . 0 0 ^z l C C A The algorithm goes as follows. <p> (1 fi i ) 1 = O (* 1 ): (3:11) Since kg i k and kh i k are of order *, it follows from (3.10) and (3.11) that i ! O (1) I 1 fi i O (*) ! From (2.7) and standard perturbation theory for matrix inverses <ref> [7, Ch. 4] </ref>, [I C i + O ((*))] 1 = O (1). Hence from (3.12) i ! 1 O (1) O (1) ; (3:13) which establishes (3.8.1). <p> In the Rayleigh-Ritz step, the formation of B z will require approximately the same amount of work as one Gauss-Seidel step. Since only the dominant left eigenvector of B z is required and the corresponding eigenvalue is known, it should be computed by the inverse power method <ref> [7, Ch. 7] </ref>. This requires that an LU decomposition of I B z be calculated at a cost of l 3 =3 flops. <p> Thus w 1 = (1 0) T is one o (*) approximate null vector. We claim that there is a vector w 2 with k w 2 k = 1 such that w T 2 (I B 2 ) = o (*). For otherwise, the perturbation theory in <ref> [7, p. 295] </ref> would apply to show that there is a left eigenvector of I B y of the form (1 o (1) corresponding to an eigenvalue that is o (*) in size.
Reference: [8] <author> G. W. Stewart. </author> <title> Computable error bounds for aggregated Markov chains. </title> <journal> Journal of the ACM, </journal> <volume> 30 </volume> <pages> 271-285, </pages> <year> 1983. </year>
Reference-contexts: The diagonals of T z are usually found by solving eigenvalue problems associated with with the A ii . For example, Stewart <ref> [8] </ref> takes the ^z i to be the normalized left eigenvectors of the A ii ; Courtois [1] takes them to be the normalized left eigenvectors of stochastic approximations A fl ii to the A ii . Under appropriate assumptions, either method produces an O (*) approximation to y.
Reference: [9] <author> Y. Takahashi. </author> <title> A lumping method for numerical calculations of stationary distributions of Markov chains. </title> <institution> Research Report B-18, Department of Information Sciences, Tokyo Institute of Technology, </institution> <year> 1975. </year>
Reference-contexts: The drawback of this form of aggregation is that it produces a single result, which may or may not be accurate enough. To remedy this defect, two workers have proposed composite iterative schemes that reduce the error by a factor of order *. Takahashi <ref> [9] </ref> has proposed a method that turns out to be very close to the one analyzed here; so close, in fact, that the analysis of our method may be easily extended to prove the convergence of his. The method of Vantilborgh [10] is quite different in character. <p> We shall give some examples in the next section. The second observation is that the analysis extends with little modification to the method of Takahashi <ref> [9] </ref>. His method differs from ours in the way intermediate quantities are scaled in the Gauss-Seidel step.
Reference: [10] <author> H. Vantliborgh. </author> <title> The error of aggregation in decomposable systems. </title> <type> Report R 453, </type> <institution> Philips Research Laboratory, </institution> <address> Brussels, Belgium, </address> <year> 1981. </year>
Reference-contexts: Takahashi [9] has proposed a method that turns out to be very close to the one analyzed here; so close, in fact, that the analysis of our method may be easily extended to prove the convergence of his. The method of Vantilborgh <ref> [10] </ref> is quite different in character. He uses an approximation to y to construct stochastic matrices A fl ii , each with the property that its steady-state vector ^z i is an improved approximation to the corresponding part of y, suitably normalized. <p> One approach is to assume a specific parameterization for A. For example, in [1] and <ref> [10] </ref> it is assumed that A can be written in the form A = A fl + *B; (2:1) where A fl is a block diagonal stochastic matrix. Although this approach works, it has two drawbacks. <p> Two-Stage Iteration 15 that the Gauss-Seidel step and the Rayleigh-Ritz step will represent comparable computations. To illustrate the procedure we consider two examples, based on a model investigated by Vantilborgh in <ref> [10] </ref>, where further references may be found. The following is Vantliborgh's description of the model. A finite number N of active user terminals generates random requests for program execution.
Reference: [11] <author> R. S. Varga. </author> <title> Matrix Iterative Analysis. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1962. </year>
Reference-contexts: We shall now describe each step in detail. The block Gauss-Seidel step is the iteration commonly used to solve the large linear systems arising in the numerical treatment of partial differential equations <ref> [11] </ref>. In this case we apply it to the system y T (I A) = 0; (1:8) which is equivalent to (1.6).
References-found: 11

