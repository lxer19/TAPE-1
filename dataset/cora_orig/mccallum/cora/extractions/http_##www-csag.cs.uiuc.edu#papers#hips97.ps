URL: http://www-csag.cs.uiuc.edu/papers/hips97.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Title: Supporting High Level Programming with High Performance: The Illinois Concert System  
Author: Andrew Chien Julian Dolby Bishwaroop Ganguly Vijay Karamcheti Xingbin Zhang 
Keyword: concurrent languages, concurrent object-oriented programming, compiler optimization, runtime systems, object-oriented optimization  
Web: concert@red-herring.cs.uiuc.edu  
Address: Urbana, Illinois 61801  
Affiliation: Department of Computer Science University of Illinois  
Abstract: As high level features can potentially incur overhead, the Concert system employs a range of compiler and runtime optimization techniques to efficiently support the high level programming model. The compiler techniques include type inference, inlining and specialization; and the runtime techniques include caching, prefetching and hybrid stack/heap multithreading. The effectiveness of these techniques permits the construction of complex parallel applications that are flexible, enabling convenient application modification or tuning. We present performance results for a number of application programs which attain good speedups and absolute performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. America. POOL-T: </author> <title> A parallel object-oriented language. </title> <editor> In A. Yonezawa and M. Tokoro, editors, </editor> <booktitle> Object-Oriented Concurrent Programming, pages 199220. </booktitle> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Runtime optimizations contributing to good parallel performance. Actor-based languages <ref> [1, 20, 42, 29] </ref> are most similar in terms of high-level programming support, but have focused less [39, 27] on efficient implementation.
Reference: [2] <author> P. Beckman, D. Gannon, and E. Johnson. </author> <title> Portable parallel programming in HPC++. </title> <note> Available online at http://www.extreme.indiana.edu/ hpc%2b%2b/docs/ppphpc++/icpp.ps, </note> <year> 1996. </year>
Reference-contexts: Task-parallel object-oriented languages, mostly based on C++ extensions [16, 23, 6], support irregular parallelism and some location independence, but require programmer management of concurrency, storage-management, and task granularity which limits scalability and portability. Data-parallel object-oriented languages, such as pC++ [28], provide little support for expressing task-level parallelism. HPC++ <ref> [2] </ref> is similar, expressing concurrency primarily as parallel operations across homogenous collections. ICC++ expresses data parallelism as task-level concurrency, providing greater programming power, but making efficient implementation significantly more challenging. Concert differs from all the above systems in its focus on supporting high-level programming features with efficient implementation techniques.
Reference: [3] <author> B. Calder, D. Grunwald, and B. Zorn. </author> <title> Quantifying differences between C and C++ programs. </title> <type> Technical Report CU-CS-698-94, </type> <institution> University of Colorado, Boulder, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: The effect of object-oriented abstraction is to hide implementation details needed for efficient code e.g. concrete types of variables and object lifetimes beneath abstract interfaces, requiring program analysis to discover them. Additionally, the many interfaces in object-oriented code tend to break programs down into many small, dynamically-dispatched methods <ref> [3] </ref>. Two performance issues arise from this: Small dynamic methods both increase overhead by requiring function calls and reduce the effectiveness of standard intra-procedural optimizations by giving them smaller function bodies to work on.
Reference: [4] <author> S. Chakrabarti and K. Yelick. </author> <title> Implementing an irregular application on a distributed memory multiprocessor. </title> <booktitle> In Proceedings of the Fourth ACM/SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 169 179, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The applications exhibit good speedups ranging from 8.5 on 16 nodes for Grobner to 54.8 on 64 nodes for the force phase of FMM. These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [4, 35, 36, 21, 37] </ref>. <p> PROGRAM INPUT Grobner pavelle5 <ref> [4] </ref> Grobner basis IC-Cedar Myoglobin Molecular dynamics Radiosity Room [41] Hierarchical radiosity Barnes 16K bodies Hierarchical N-body FMM 32K bodies Hierarchical N-body FMM are only for the force phases. The speedup numbers are comparable to the best reported for low-level programming approaches.
Reference: [5] <author> C. Chambers. </author> <title> The Design and Implementation of the SELF Compiler, an Optimizing Compiler for Object-Oriented Programming Languages. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: ICC++ expresses data parallelism as task-level concurrency, providing greater programming power, but making efficient implementation significantly more challenging. Concert differs from all the above systems in its focus on supporting high-level programming features with efficient implementation techniques. This focus can be found in the context of sequential object-oriented languages <ref> [11, 19, 5, 10] </ref>, but our system additionally tackles the problems associated with concurrency, distribution and parallelism. With respect to parallel systems in general, a wide va riety of high-level approaches to portable programming are being actively pursued. Global address-space languages [14] minimally extend a low-level language with global pointers.
Reference: [6] <author> K. M. Chandy and C. Kesselman. </author> <title> Compositional C++: Compositional parallel programming. </title> <booktitle> In Proceedings of the Fifth Workshop on Compilers and Languages for Parallel Computing, </booktitle> <address> New Haven, Connecticut, </address> <year> 1992. </year> <note> YALEU/DCS/RR-915, Springer-Verlag Lecture Notes in Computer Science, </note> <year> 1993. </year>
Reference-contexts: Runtime optimizations contributing to good parallel performance. Actor-based languages [1, 20, 42, 29] are most similar in terms of high-level programming support, but have focused less [39, 27] on efficient implementation. Task-parallel object-oriented languages, mostly based on C++ extensions <ref> [16, 23, 6] </ref>, support irregular parallelism and some location independence, but require programmer management of concurrency, storage-management, and task granularity which limits scalability and portability. Data-parallel object-oriented languages, such as pC++ [28], provide little support for expressing task-level parallelism.
Reference: [7] <author> Chen and Cowie. </author> <title> Prototyping FORTRAN-90 compilers for massively parallel machines. </title> <booktitle> In Proceedings of SIGPLAN PLDI, </booktitle> <year> 1992. </year>
Reference-contexts: With respect to parallel systems in general, a wide va riety of high-level approaches to portable programming are being actively pursued. Global address-space languages [14] minimally extend a low-level language with global pointers. While efficiently implementable, they require programmer control of distribution, concurrency, and task granularity. Data parallel approaches <ref> [40, 7, 18] </ref> express parallelism across arrays, collections, or program constructs such as loops in the context of a single control flow model. Such programs achieve efficiency by grouping and scheduling operations on colocated data elements. However, they cannot easily express task-level or irregular con-currency. <p> Such programs achieve efficiency by grouping and scheduling operations on colocated data elements. However, they cannot easily express task-level or irregular con-currency. Further, with the exception of Fortran 90 <ref> [7] </ref>, data parallel languages provide no support for encapsulation and modularity. 6 Conclusions We have described the Concert System, an optimizing implementation for a concurrent object-oriented programming model. We detailed the features of our language, ICC++, that supports fine-grained concurrency and concurrent abstractions.
Reference: [8] <author> A. A. Chien. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively-Parallel Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: Concrete examples are provided in the next section. 2.4 Large Scale Concurrency: Collections Large scale concurrency often requires manipulation of large groups of objects, as well as the systematic elimination of any single points of serialization, to achieve good scalability <ref> [8] </ref>. To support clean encapsulation of large scale concurrency, ICC++ incorporates collections of objects. Objects within a collection are aware of the collection, and hence can co-operate to implement an abstraction with a concurrent interface.
Reference: [9] <author> A. A. Chien, U. S. Reddy, J. Plevyak, and J. Dolby. </author> <title> ICC++ a C++ dialect for high-performance parallel computation. </title> <booktitle> In Proceedings of the 2nd International Symposium on Object Technologies for Advanced Software, </booktitle> <month> March </month> <year> 1996. </year>
Reference-contexts: Thus, the state of each object is sequentially consistent. There is no consistency guaranteed between objects, but this capability can, naturally, be used to build arbitrary multi-object synchronization structures. These semantics were chosen both to provide programming convenience and allow compiler optimization of locking overhead <ref> [9, 34, 31] </ref>. Concrete examples are provided in the next section. 2.4 Large Scale Concurrency: Collections Large scale concurrency often requires manipulation of large groups of objects, as well as the systematic elimination of any single points of serialization, to achieve good scalability [8].
Reference: [10] <author> J. Dean, C. Chambers, and D. Grove. </author> <title> Selective specialization for object-oriented languages. </title> <booktitle> In Proceedings of the ACM SIGPLAN '95 Conference on Programmin g Language Design and Implementation, </booktitle> <pages> pages 93102, </pages> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: ICC++ expresses data parallelism as task-level concurrency, providing greater programming power, but making efficient implementation significantly more challenging. Concert differs from all the above systems in its focus on supporting high-level programming features with efficient implementation techniques. This focus can be found in the context of sequential object-oriented languages <ref> [11, 19, 5, 10] </ref>, but our system additionally tackles the problems associated with concurrency, distribution and parallelism. With respect to parallel systems in general, a wide va riety of high-level approaches to portable programming are being actively pursued. Global address-space languages [14] minimally extend a low-level language with global pointers.
Reference: [11] <author> L. P. Deutsch and A. M. Schiffman. </author> <title> Efficient implementation of the Smalltalk-80 system. </title> <booktitle> In Eleventh Symposium on Principles of Programming Languages, </booktitle> <pages> pages 297302. </pages> <publisher> ACM, </publisher> <year> 1984. </year>
Reference-contexts: ICC++ expresses data parallelism as task-level concurrency, providing greater programming power, but making efficient implementation significantly more challenging. Concert differs from all the above systems in its focus on supporting high-level programming features with efficient implementation techniques. This focus can be found in the context of sequential object-oriented languages <ref> [11, 19, 5, 10] </ref>, but our system additionally tackles the problems associated with concurrency, distribution and parallelism. With respect to parallel systems in general, a wide va riety of high-level approaches to portable programming are being actively pursued. Global address-space languages [14] minimally extend a low-level language with global pointers.
Reference: [12] <author> J. Dolby. </author> <title> Automatic inline allocation of objects. </title> <booktitle> In Proceedings of the 1997 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: This, in turn, causes two contexts to be created for print datum, as the targets now come from differing class contexts (effectively giving them differing types). This next pass of analysis results in the graph in Figure 3. 3.2 Static Optimizations The Concert system implements three interprocedural static optimizations <ref> [12, 34] </ref> to reduce object access overhead and enlarge thread granularity: object inlining, method inlining and access region expansion. First, we apply object inlining to inline allocate object within other objects. Inline allocation lowers object access costs because the inlined objects' consistency can be managed by the container object.
Reference: [13] <author> M. A. Ellis and B. Stroustrup. </author> <title> The Annotated C++ Reference Manual. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction The increasing complexity of software concomitantly increases the importance of tools which reduce, or aid the management of, such complexity. This trend is profoundly reshaping the mainstream programming world which focuses on sequential computing, producing a large scale movement toward object-oriented languages (e.g. C++ <ref> [13] </ref>, Smalltalk [15] and Java [38]) and object-based programming techniques (e.g. CORBA [30], DCOM, OLE, and a wealth of other object standards). Such techniques provide encapsulation which supports code reuse and modularity (separate design), enabling the construction of larger, more complex systems and speeding the development process.
Reference: [14] <editor> A. K. et al. </editor> <booktitle> Parallel programming in Split-C. In Proceedings of Supercomputing, </booktitle> <pages> pages 262273, </pages> <year> 1993. </year>
Reference-contexts: With respect to parallel systems in general, a wide va riety of high-level approaches to portable programming are being actively pursued. Global address-space languages <ref> [14] </ref> minimally extend a low-level language with global pointers. While efficiently implementable, they require programmer control of distribution, concurrency, and task granularity. Data parallel approaches [40, 7, 18] express parallelism across arrays, collections, or program constructs such as loops in the context of a single control flow model.
Reference: [15] <author> A. Goldberg and D. Robson. </author> <title> Smalltalk-80: The language and its implementation. </title> <publisher> Addison-Wesley, </publisher> <year> 1985. </year>
Reference-contexts: 1 Introduction The increasing complexity of software concomitantly increases the importance of tools which reduce, or aid the management of, such complexity. This trend is profoundly reshaping the mainstream programming world which focuses on sequential computing, producing a large scale movement toward object-oriented languages (e.g. C++ [13], Smalltalk <ref> [15] </ref> and Java [38]) and object-based programming techniques (e.g. CORBA [30], DCOM, OLE, and a wealth of other object standards). Such techniques provide encapsulation which supports code reuse and modularity (separate design), enabling the construction of larger, more complex systems and speeding the development process. <p> A global object namespace allows data to be accessed uniformly, factoring data and task placement from the program's functional specification. Implicit storage management frees the programmer from the details of memory management. Long a recognized benefit in sequential programs <ref> [22, 15] </ref> and recently further popularized by Java [38], implicit storage management simplifies concurrent programs significantly, particularly those with complex distributed data structures the Concert project's primary focus. 2.2 Expressing Concurrency ICC++ declares concurrency by annotating standard blocks (i.e. compound statements) and loops with the conc keyword.
Reference: [16] <author> A. Grimshaw. </author> <title> Easy-to-use object-oriented parallel processing with Mentat. </title> <journal> IEEE Computer, </journal> <volume> 5(26):3951, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Runtime optimizations contributing to good parallel performance. Actor-based languages [1, 20, 42, 29] are most similar in terms of high-level programming support, but have focused less [39, 27] on efficient implementation. Task-parallel object-oriented languages, mostly based on C++ extensions <ref> [16, 23, 6] </ref>, support irregular parallelism and some location independence, but require programmer management of concurrency, storage-management, and task granularity which limits scalability and portability. Data-parallel object-oriented languages, such as pC++ [28], provide little support for expressing task-level parallelism.
Reference: [17] <author> C. S. A. </author> <title> Group. The ICC++ reference manual. Concurrent Systems Architecture Group Memo. </title> <note> Available from http://www-csag.cs.uiuc.edu/, May 1996. </note>
Reference-contexts: ICC++ addresses the managing large-scale concur-rency with highly parallel object collections. We cover the salient features of ICC++ for each of these language features in order. Further details of ICC++ can be found in <ref> [17] </ref>. 2.1 General High Level Features ICC++ supports flexible, modular programming for concurrent programs in a style similar to that available for sequential programs.
Reference: [18] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler optimizations for FORTRAN D on MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <month> August </month> <year> 1992. </year>
Reference-contexts: With respect to parallel systems in general, a wide va riety of high-level approaches to portable programming are being actively pursued. Global address-space languages [14] minimally extend a low-level language with global pointers. While efficiently implementable, they require programmer control of distribution, concurrency, and task granularity. Data parallel approaches <ref> [40, 7, 18] </ref> express parallelism across arrays, collections, or program constructs such as loops in the context of a single control flow model. Such programs achieve efficiency by grouping and scheduling operations on colocated data elements. However, they cannot easily express task-level or irregular con-currency.
Reference: [19] <author> U. Holzle. </author> <title> Adaptive Optimization for SELF: Reconciling High Performance with Exporatory Programming. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: ICC++ expresses data parallelism as task-level concurrency, providing greater programming power, but making efficient implementation significantly more challenging. Concert differs from all the above systems in its focus on supporting high-level programming features with efficient implementation techniques. This focus can be found in the context of sequential object-oriented languages <ref> [11, 19, 5, 10] </ref>, but our system additionally tackles the problems associated with concurrency, distribution and parallelism. With respect to parallel systems in general, a wide va riety of high-level approaches to portable programming are being actively pursued. Global address-space languages [14] minimally extend a low-level language with global pointers.
Reference: [20] <author> C. Houck and G. Agha. HAL: </author> <title> A high-level actor language and its distributed implementation. </title> <booktitle> In Proceedings of the 21st International Conference on Parallel Processing, </booktitle> <pages> pages 158165, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Runtime optimizations contributing to good parallel performance. Actor-based languages <ref> [1, 20, 42, 29] </ref> are most similar in terms of high-level programming support, but have focused less [39, 27] on efficient implementation.
Reference: [21] <author> Y.-S. Hwang, R. Das, J. Saltz, B. Brooks, and M. Hodoffsffcek. </author> <title> Parallelizing molecular dynamics programs for distributed memory machines. </title> <journal> IEEE Computational Science and Engineering, </journal> <pages> pages 1829, </pages> <month> Summer </month> <year> 1995. </year>
Reference-contexts: The applications exhibit good speedups ranging from 8.5 on 16 nodes for Grobner to 54.8 on 64 nodes for the force phase of FMM. These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [4, 35, 36, 21, 37] </ref>.
Reference: [22] <author> G. L. S. Jr. </author> <title> Common LISP: The Language. </title> <note> Digital Press, second edition, </note> <year> 1990. </year>
Reference-contexts: A global object namespace allows data to be accessed uniformly, factoring data and task placement from the program's functional specification. Implicit storage management frees the programmer from the details of memory management. Long a recognized benefit in sequential programs <ref> [22, 15] </ref> and recently further popularized by Java [38], implicit storage management simplifies concurrent programs significantly, particularly those with complex distributed data structures the Concert project's primary focus. 2.2 Expressing Concurrency ICC++ declares concurrency by annotating standard blocks (i.e. compound statements) and loops with the conc keyword.
Reference: [23] <author> L. V. Kale and S. Krishnan. CHARM++: </author> <title> A portable concurrent object oriented system based on C++. </title> <booktitle> In Proceedings of OOPSLA'93, </booktitle> <pages> pages 91108, </pages> <year> 1993. </year>
Reference-contexts: Runtime optimizations contributing to good parallel performance. Actor-based languages [1, 20, 42, 29] are most similar in terms of high-level programming support, but have focused less [39, 27] on efficient implementation. Task-parallel object-oriented languages, mostly based on C++ extensions <ref> [16, 23, 6] </ref>, support irregular parallelism and some location independence, but require programmer management of concurrency, storage-management, and task granularity which limits scalability and portability. Data-parallel object-oriented languages, such as pC++ [28], provide little support for expressing task-level parallelism.
Reference: [24] <author> V. Karamcheti and A. A. Chien. </author> <title> A comparison of architectural support for messaging on the TMC CM-5 and the Cray T3D. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995. </year> <note> Available from http://www-csag.cs.uiuc.edu/ papers/cm5-t3d-messaging.ps. </note>
Reference-contexts: Various thread interaction schemas in the hybrid stack-heap execution model. 3.5 Fast Communication and Thread Scheduling To support fine-grained, distributed programs efficiently, the Concert implementation is built atop Fast Messages (FM) <ref> [24] </ref>, which utilizes novel implementation techniques such as receiver-initiated data transfer to support high-performance messaging in the face of irregular communication that is unsynchronized with ongoing computation (a consequence of our dynamic programming model).
Reference: [25] <author> V. Karamcheti and A. A. Chien. </author> <title> View caching: Efficient software shared memory for dynamic computations. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1997. </year>
Reference-contexts: regions and lift access regions above loops and conditionals, as shown in Figure 6, to create regions of optimized sequential code with the efficiency of a sequential uniprocessor implementation. 3.3 Locality Optimizations Since global pointer-based data structures are fundamental for many dynamic (e.g. data-dependent) computations, Concert supports two locality optimizations <ref> [25, 43] </ref> to efficiently implement such structures on modern architectures with deep memory hierarchies, such as NUMA machines, whether cache-coherent or not. When static coarse-grained aliasing information is available, we apply dynamic pointer alignment, a generalization of static loop tiling and communication optimizations. <p> At run time, the program concur-rency structure allows these iterations to be reordered dynamically, guided by runtime data access information, to maximize data reuse and hide communication latency. View caching <ref> [25] </ref> supports efficient runtime object caching in dynamic computations, relying on application knowledge of data access semantics to construct customized latency-tolerant coherence protocols that require reduced message traffic and synchronization.
Reference: [26] <author> V. Karamcheti, J. Plevyak, and A. A. Chien. </author> <title> Runtime mechanisms for efficient dynamic multithreading. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 37:2140, </volume> <year> 1996. </year>
Reference-contexts: Our hybrid stack-heap execution model <ref> [33, 26] </ref> provides a flexible runtime interface to the compiler, shown in Table 1, allowing it to generate code which optimistically executes a logical thread sequentially on its caller's stack, lazily creating a different thread only when the callee computation needs to suspend or be scheduled separately. <p> For example, robust communication is important at small numbers of processors when communication traffic is high, and load-balancing is essential for large numbers of processors. Space limitations prevent us from a detailed analysis for the other applications; the reader is referred elsewhere <ref> [44, 26] </ref> for additional details. 5 Related Work The Concert system is related to a wide variety of work on concurrent object-oriented languages that can be loosely classified as actor-based, task-parallel, and data-parallel.
Reference: [27] <author> W. Y. Kim and G. Agha. </author> <title> Efficient support for location transparency in concurrent object-oriented programming languages. </title> <booktitle> In Proceedings of the Supercomputing '95 Conference, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Runtime optimizations contributing to good parallel performance. Actor-based languages [1, 20, 42, 29] are most similar in terms of high-level programming support, but have focused less <ref> [39, 27] </ref> on efficient implementation. Task-parallel object-oriented languages, mostly based on C++ extensions [16, 23, 6], support irregular parallelism and some location independence, but require programmer management of concurrency, storage-management, and task granularity which limits scalability and portability.
Reference: [28] <author> J. Lee and D. Gannon. </author> <title> Object oriented parallel programming. </title> <booktitle> In Proceedings of the ACM/IEEE Conference on Supercomputing. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Task-parallel object-oriented languages, mostly based on C++ extensions [16, 23, 6], support irregular parallelism and some location independence, but require programmer management of concurrency, storage-management, and task granularity which limits scalability and portability. Data-parallel object-oriented languages, such as pC++ <ref> [28] </ref>, provide little support for expressing task-level parallelism. HPC++ [2] is similar, expressing concurrency primarily as parallel operations across homogenous collections. ICC++ expresses data parallelism as task-level concurrency, providing greater programming power, but making efficient implementation significantly more challenging.
Reference: [29] <author> S. Murer, J. A. Feldman, C.-C. Lim, and M.-M. Seidel. pSather: </author> <title> Layered extensions to an object-oriented language for efficient parallel computation. </title> <type> Technical Report TR-93-028, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA, </address> <month> June </month> <year> 1993 </year> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Runtime optimizations contributing to good parallel performance. Actor-based languages <ref> [1, 20, 42, 29] </ref> are most similar in terms of high-level programming support, but have focused less [39, 27] on efficient implementation.
Reference: [30] <author> ORB 2.0 RFP Submission. </author> <title> Technical Report Document 94.9.41, </title> <booktitle> The Object Management Group, </booktitle> <year> 1994. </year>
Reference-contexts: This trend is profoundly reshaping the mainstream programming world which focuses on sequential computing, producing a large scale movement toward object-oriented languages (e.g. C++ [13], Smalltalk [15] and Java [38]) and object-based programming techniques (e.g. CORBA <ref> [30] </ref>, DCOM, OLE, and a wealth of other object standards). Such techniques provide encapsulation which supports code reuse and modularity (separate design), enabling the construction of larger, more complex systems and speeding the development process.
Reference: [31] <author> J. Plevyak. </author> <title> Optimization of Object-Oriented and Concurrent Programs. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, Illinois, </institution> <year> 1996. </year>
Reference-contexts: Thus, the state of each object is sequentially consistent. There is no consistency guaranteed between objects, but this capability can, naturally, be used to build arbitrary multi-object synchronization structures. These semantics were chosen both to provide programming convenience and allow compiler optimization of locking overhead <ref> [9, 34, 31] </ref>. Concrete examples are provided in the next section. 2.4 Large Scale Concurrency: Collections Large scale concurrency often requires manipulation of large groups of objects, as well as the systematic elimination of any single points of serialization, to achieve good scalability [8]. <p> Beneath that, in cooperation with the runtime system, 2) locality management supports distributed data structures and 3) light-weight thread support enables fine-grained concur-rency. This all rests on 4) efficient runtime mechanisms for communication and thread scheduling. 3.1 Program Analysis The Concert compiler implements global program analysis <ref> [32, 31] </ref> to obtain a variety of information: types of variables to resolve dynamic disptach, relative locality of objects, and container objects for storage optimizations. The analysis is context sensitive and adapts, in a demand-driven manner, to program structure.
Reference: [32] <author> J. Plevyak and A. A. Chien. </author> <title> Precise concrete type inference of object-oriented programs. </title> <booktitle> In Proceedings of OOPSLA'94, Object-Oriented Programming Systems, Languages and Architectures, </booktitle> <pages> pages 324340, </pages> <year> 1994. </year>
Reference-contexts: Beneath that, in cooperation with the runtime system, 2) locality management supports distributed data structures and 3) light-weight thread support enables fine-grained concur-rency. This all rests on 4) efficient runtime mechanisms for communication and thread scheduling. 3.1 Program Analysis The Concert compiler implements global program analysis <ref> [32, 31] </ref> to obtain a variety of information: types of variables to resolve dynamic disptach, relative locality of objects, and container objects for storage optimizations. The analysis is context sensitive and adapts, in a demand-driven manner, to program structure.
Reference: [33] <author> J. Plevyak, V. Karamcheti, X. Zhang, and A. Chien. </author> <title> A hybrid execution model for fine-grained languages on distributed memory multicomputers. </title> <booktitle> In Proceedings of Supercomputing'95, </booktitle> <year> 1995. </year>
Reference-contexts: Our hybrid stack-heap execution model <ref> [33, 26] </ref> provides a flexible runtime interface to the compiler, shown in Table 1, allowing it to generate code which optimistically executes a logical thread sequentially on its caller's stack, lazily creating a different thread only when the callee computation needs to suspend or be scheduled separately.
Reference: [34] <author> J. Plevyak, X. Zhang, and A. A. Chien. </author> <title> Obtaining sequential efficiency in concurrent object-oriented programs. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 311321, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Thus, the state of each object is sequentially consistent. There is no consistency guaranteed between objects, but this capability can, naturally, be used to build arbitrary multi-object synchronization structures. These semantics were chosen both to provide programming convenience and allow compiler optimization of locking overhead <ref> [9, 34, 31] </ref>. Concrete examples are provided in the next section. 2.4 Large Scale Concurrency: Collections Large scale concurrency often requires manipulation of large groups of objects, as well as the systematic elimination of any single points of serialization, to achieve good scalability [8]. <p> This, in turn, causes two contexts to be created for print datum, as the targets now come from differing class contexts (effectively giving them differing types). This next pass of analysis results in the graph in Figure 3. 3.2 Static Optimizations The Concert system implements three interprocedural static optimizations <ref> [12, 34] </ref> to reduce object access overhead and enlarge thread granularity: object inlining, method inlining and access region expansion. First, we apply object inlining to inline allocate object within other objects. Inline allocation lowers object access costs because the inlined objects' consistency can be managed by the container object.
Reference: [35] <author> D. J. Scales and M. S. Lam. </author> <title> The design and evaluation of a shared object system for distributed memory machines. </title> <booktitle> In First Symposium on Operating Systems Design and Implementation, </booktitle> <year> 1994. </year>
Reference-contexts: The applications exhibit good speedups ranging from 8.5 on 16 nodes for Grobner to 54.8 on 64 nodes for the force phase of FMM. These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [4, 35, 36, 21, 37] </ref>.
Reference: [36] <author> J. P. Singh, A. Gupta, and M. Levoy. </author> <title> Parallel visualization algorithms: Performance and architectural implications. </title> <journal> IEEE Computer, </journal> <volume> 27(7):4556, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: The applications exhibit good speedups ranging from 8.5 on 16 nodes for Grobner to 54.8 on 64 nodes for the force phase of FMM. These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [4, 35, 36, 21, 37] </ref>. <p> These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes [4, 35, 36, 21, 37]. For example, the Ra-diosity speedup of 23 on 32 T3D processors compares well with the previously reported speedup of 26 on 32 processors of the DASH machine <ref> [36] </ref>, despite hardware support for cache-coherent shared memory and an order of magnitude faster communication (in terms of processor clocks) in the DASH which better facilitates scalable performance. The good parallel performance is the aggregate effect of several optimizations which eliminate the overheads of Concert's high-level features.
Reference: [37] <author> J. P. Singh, C. Holt, J. L. Hennessy, and A. Gupta. </author> <title> A parallel adaptive fast multipole method. </title> <booktitle> In Proceedings of Supercomputing Conference, </booktitle> <pages> pages 5465, </pages> <year> 1993. </year>
Reference-contexts: The applications exhibit good speedups ranging from 8.5 on 16 nodes for Grobner to 54.8 on 64 nodes for the force phase of FMM. These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [4, 35, 36, 21, 37] </ref>.
Reference: [38] <author> Sun Microsystems Computer Corporation. </author> <title> The Java Language Specification, </title> <month> March </month> <year> 1995. </year> <note> Available at http://java.sun.com/1.0alpha2/doc/java-whitepaper.ps. </note>
Reference-contexts: This trend is profoundly reshaping the mainstream programming world which focuses on sequential computing, producing a large scale movement toward object-oriented languages (e.g. C++ [13], Smalltalk [15] and Java <ref> [38] </ref>) and object-based programming techniques (e.g. CORBA [30], DCOM, OLE, and a wealth of other object standards). Such techniques provide encapsulation which supports code reuse and modularity (separate design), enabling the construction of larger, more complex systems and speeding the development process. <p> A global object namespace allows data to be accessed uniformly, factoring data and task placement from the program's functional specification. Implicit storage management frees the programmer from the details of memory management. Long a recognized benefit in sequential programs [22, 15] and recently further popularized by Java <ref> [38] </ref>, implicit storage management simplifies concurrent programs significantly, particularly those with complex distributed data structures the Concert project's primary focus. 2.2 Expressing Concurrency ICC++ declares concurrency by annotating standard blocks (i.e. compound statements) and loops with the conc keyword. Both constructs are illustrated in Figure 1.
Reference: [39] <author> K. Taura, S. Matsuoka, and A. Yonezawa. StackThreads: </author> <title> An abstract machine for scheduling fine-grain threads on stock CPUs. </title> <booktitle> In Joint Symposium on Parallel Processing, </booktitle> <year> 1994. </year>
Reference-contexts: Runtime optimizations contributing to good parallel performance. Actor-based languages [1, 20, 42, 29] are most similar in terms of high-level programming support, but have focused less <ref> [39, 27] </ref> on efficient implementation. Task-parallel object-oriented languages, mostly based on C++ extensions [16, 23, 6], support irregular parallelism and some location independence, but require programmer management of concurrency, storage-management, and task granularity which limits scalability and portability.
Reference: [40] <author> Thinking Machines Corporation. </author> <title> Getting Started in CM Fortran, </title> <year> 1990. </year>
Reference-contexts: With respect to parallel systems in general, a wide va riety of high-level approaches to portable programming are being actively pursued. Global address-space languages [14] minimally extend a low-level language with global pointers. While efficiently implementable, they require programmer control of distribution, concurrency, and task granularity. Data parallel approaches <ref> [40, 7, 18] </ref> express parallelism across arrays, collections, or program constructs such as loops in the context of a single control flow model. Such programs achieve efficiency by grouping and scheduling operations on colocated data elements. However, they cannot easily express task-level or irregular con-currency.
Reference: [41] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 2436, </pages> <year> 1995. </year>
Reference-contexts: PROGRAM INPUT Grobner pavelle5 [4] Grobner basis IC-Cedar Myoglobin Molecular dynamics Radiosity Room <ref> [41] </ref> Hierarchical radiosity Barnes 16K bodies Hierarchical N-body FMM 32K bodies Hierarchical N-body FMM are only for the force phases. The speedup numbers are comparable to the best reported for low-level programming approaches.
Reference: [42] <author> A. Yonezawa, E. Shibayama, T. Takada, and Y. Honda. </author> <title> Object-oriented concurrent programming modelling and programming in an object-oriented concurrent language ABCL/1. </title> <editor> In A. Yonezawa and M. Tokoro, editors, </editor> <booktitle> Object-Oriented Concurrent Programming, </booktitle> <pages> pages 5589. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Runtime optimizations contributing to good parallel performance. Actor-based languages <ref> [1, 20, 42, 29] </ref> are most similar in terms of high-level programming support, but have focused less [39, 27] on efficient implementation.
Reference: [43] <author> X. Zhang and A. A. Chien. </author> <title> Dynamic pointer alignment: Tiling and communication optimizations for parallel pointer-based computations. </title> <note> Submitted for publication, </note> <year> 1996. </year>
Reference-contexts: regions and lift access regions above loops and conditionals, as shown in Figure 6, to create regions of optimized sequential code with the efficiency of a sequential uniprocessor implementation. 3.3 Locality Optimizations Since global pointer-based data structures are fundamental for many dynamic (e.g. data-dependent) computations, Concert supports two locality optimizations <ref> [25, 43] </ref> to efficiently implement such structures on modern architectures with deep memory hierarchies, such as NUMA machines, whether cache-coherent or not. When static coarse-grained aliasing information is available, we apply dynamic pointer alignment, a generalization of static loop tiling and communication optimizations.
Reference: [44] <author> X. Zhang, V. Karamcheti, T. Ng, and A. Chien. </author> <title> Optimizing COOP languages: Study of a protein dynamics program. </title> <booktitle> In IPPS'96, </booktitle> <year> 1996. </year>
Reference-contexts: For example, robust communication is important at small numbers of processors when communication traffic is high, and load-balancing is essential for large numbers of processors. Space limitations prevent us from a detailed analysis for the other applications; the reader is referred elsewhere <ref> [44, 26] </ref> for additional details. 5 Related Work The Concert system is related to a wide variety of work on concurrent object-oriented languages that can be loosely classified as actor-based, task-parallel, and data-parallel.
References-found: 44

