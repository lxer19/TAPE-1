URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15347-s98/public/doc/stricker-hpca97.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15347-s98/public/doc/
Root-URL: 
Title: Global Address Space, Non-Uniform Bandwidth: A Memory System Performance Characterization of Parallel Systems  
Author: T. Stricker and T. Gross ; 
Address: Pittsburgh, PA 15213 CH-8092 Zurich  
Affiliation: San Antonio, TX.  1 School of Computer Science 2 Institut fur Computer Systeme Carnegie Mellon University ETH Zurich  Institut fur Computer  
Date: February 1-5, 1997,  
Note: appears in: Proceedings of the ACM conference on High Performance Computer Architecture (HPCA3),  This research was sponsored in part by the Advanced Research Projects Agency (ITO) monitoredby SPAWAR undercontract N00039-93-C-0152. T. Stricker's current address:  Systeme, ETH Zurich, Switzerland. Copyright  
Abstract: Many parallel systems offer a simple view of memory: all storage cells are addressed uniformly. Despite a uniform view of the memory, the machines differ significantly in their memory system performance (and may offer slightly different consistency models). Cached and local memory accesses are much faster than remote read accesses to data generated by another processor or remote write to data intentionally pushed to memories close to another processor. The bandwidth from/to cache and local memory can be an order of magnitude (or more) higher than the bandwidth to/from remote memory. The situation is further complicated by the heavy influence of the access pattern (i.e. the spatial locality of reference) on both the local and the remote memory system bandwidth. In these modern machines, a compiler for a parallel system is faced with a number of options to accomplish a data transfer most efficiently. The decision for the best option requires a cost benefit model, obtained in an empirically evaluation of the memory system performance. We evaluate three DEC Alpha based parallel systems, to demonstrate the practicality of this approach. The common DEC-Alpha processor architecture facilitates a direct comparison of memory system performance. These systems are the DEC 8400, the Cray T3D, and the Cray T3E. The three systems differ in their clock speed, their scalability and in the amount of coherency they provide. The DEC 8400 is a shared memory, symmetric multiprocessor based on a high speed bus offering sequential consistency; the Cray T3D and T3E are scalable multicomputers based on a scalable 3D torus interconnect and either do not cache remote accesses at all (T3E) or provide only partial memory consistency within a node (T3D) and therefore typically leave consistency to the application or compiler. Our performance characterization shows that although the clock rate of the DEC 8400 doubled compared to the Cray T3D, the DEC 8400 offers only modest improvements in the performance of remote memory operations over the Cray T3D. The local and remote memory system performance of the Cray T3E 1997 IEEE. Published in the Proceedings of the THird International Symposium on High Performance Computer Architecture, February 1-5, 1997 in San Antonio, Texas, USA. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966. matches the doubled clock speed of the processor.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Adams. </author> <title> Cray T3D system architecture overview. </title> <type> Technical report, </type> <institution> Cray Research Inc., </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: For readers not familiar with the three architectures we discuss the important parameters relevant to the memory and communication system interface in Section 3. In addition to the technical reference material of the vendors (DEC 8400 [8, 6, 9, 7], Cray T3D <ref> [8, 1, 3] </ref>, Cray T3E [12, 4]), other research groups evaluated some aspects of these machines [2, 10, 15]. An empirical study comparing the two Alpha processors based on standard benchmarks provides useful insights using performance metrics not related to the memory system [5]. <p> With its completely different read and write paths and the external read-ahead logic, this memory system supports streamed access to large amounts of data. DRAM accesses within the same DRAM page are accelerated; see the technical data sheets or <ref> [1] </ref> for details regarding different speeds and bandwidths. Remote accesses are performed to a network interface, which is also built from ECL gate arrays.
Reference: [2] <author> R. Arpaci, D. Culler, A. Krishnamurthy, S. Steve Steinberg, and K. Yelick. </author> <title> Empirical evaluation of the Cray-T3D. </title> <booktitle> In Proc. 22nd Intl. Symposium on Computer Architecture, </booktitle> <pages> pages 320-331, </pages> <address> Santa Marguerita di Ligure, </address> <month> June </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: In addition to the technical reference material of the vendors (DEC 8400 [8, 6, 9, 7], Cray T3D [8, 1, 3], Cray T3E [12, 4]), other research groups evaluated some aspects of these machines <ref> [2, 10, 15] </ref>. An empirical study comparing the two Alpha processors based on standard benchmarks provides useful insights using performance metrics not related to the memory system [5]. Our goal is to measure and compare the memory systems of these modern parallel systems.
Reference: [3] <author> R. Barriuso and Knies A. </author> <title> SHMEM user's guide for C. </title> <type> Technical report, </type> <institution> Cray Research Inc., </institution> <month> June 20 </month> <year> 1994. </year> <note> Revision 2.1. </note>
Reference-contexts: For readers not familiar with the three architectures we discuss the important parameters relevant to the memory and communication system interface in Section 3. In addition to the technical reference material of the vendors (DEC 8400 [8, 6, 9, 7], Cray T3D <ref> [8, 1, 3] </ref>, Cray T3E [12, 4]), other research groups evaluated some aspects of these machines [2, 10, 15]. An empirical study comparing the two Alpha processors based on standard benchmarks provides useful insights using performance metrics not related to the memory system [5]. <p> Transfers are realized with a customized primitive similar to shmem put on the T3D and with shmem iput on the T3E <ref> [3] </ref>. On the DEC 8400, we could chose a different sequence without explicit transpose, but without establishing locality of reference, performance is dismal. So to obtain performance on the DEC 8400, we must find a similar structure for the application, and we re-targeted the Fx compiler to generate such code.
Reference: [4] <author> F. Chism. </author> <title> Communication latency and bandwidth on the Cray T3E. </title> <booktitle> In Proc. 10th Intl. Parallel Processing Symposium, Slides, Vendor Presentation, </booktitle> <address> Honolulu, HI, </address> <month> April </month> <year> 1996. </year> <note> IEEE. </note>
Reference-contexts: For readers not familiar with the three architectures we discuss the important parameters relevant to the memory and communication system interface in Section 3. In addition to the technical reference material of the vendors (DEC 8400 [8, 6, 9, 7], Cray T3D [8, 1, 3], Cray T3E <ref> [12, 4] </ref>), other research groups evaluated some aspects of these machines [2, 10, 15]. An empirical study comparing the two Alpha processors based on standard benchmarks provides useful insights using performance metrics not related to the memory system [5].
Reference: [5] <author> Z. Cvetanovic and D. Bhandarkar. </author> <title> Performance characterization of the Alpha 21164 microprocessor using TP and SPEC work-loads. </title> <booktitle> In Proc. 2nd High Performance Computer Architecture Conference, </booktitle> <pages> pages 270-279, </pages> <address> San Jose, </address> <month> Jan </month> <year> 1996. </year> <note> IEEE. </note>
Reference-contexts: An empirical study comparing the two Alpha processors based on standard benchmarks provides useful insights using performance metrics not related to the memory system <ref> [5] </ref>. Our goal is to measure and compare the memory systems of these modern parallel systems. Since it is nowadays usual to use a compiler to map applications onto a parallel system, we are particularity interested in an investigation that pays attention to the access patterns encountered in compiled code.
Reference: [6] <institution> Digital Equipment Corp., Maynard MA. Alpha 21164 Microprocessor, </institution> <note> Hardware Reference Manual, 1995. EC-QAEQB-TE. </note>
Reference-contexts: For readers not familiar with the three architectures we discuss the important parameters relevant to the memory and communication system interface in Section 3. In addition to the technical reference material of the vendors (DEC 8400 <ref> [8, 6, 9, 7] </ref>, Cray T3D [8, 1, 3], Cray T3E [12, 4]), other research groups evaluated some aspects of these machines [2, 10, 15]. An empirical study comparing the two Alpha processors based on standard benchmarks provides useful insights using performance metrics not related to the memory system [5].
Reference: [7] <institution> Digital Equipment Corp., Maynard MA. </institution> <note> AlphaServer 8200 and AlphaServer 8400, Technical Summary, 1995. BC-N446-10. </note>
Reference-contexts: For readers not familiar with the three architectures we discuss the important parameters relevant to the memory and communication system interface in Section 3. In addition to the technical reference material of the vendors (DEC 8400 <ref> [8, 6, 9, 7] </ref>, Cray T3D [8, 1, 3], Cray T3E [12, 4]), other research groups evaluated some aspects of these machines [2, 10, 15]. An empirical study comparing the two Alpha processors based on standard benchmarks provides useful insights using performance metrics not related to the memory system [5]. <p> The write-back L3 cache comprises 4MB of fast SRAM (10ns) and is located on each processor board. According to specifications in <ref> [7] </ref> a 20 ns latency and a 915 MB/s bandwidth could be realized. The DRAM memory of a DEC 8400 is built from memory modules, which are two-way interleaved. With four memory modules, a maximal interleaving of 8 is possible. Like a workstation, the DEC 8400 supports virtual memory. <p> This bus is clocked at 75 MHz, a quarter of the clock frequency of the microprocessor, yielding a peak transfer-rate of 2.4 GByte/s across the system bus. This limit is reduced to a peak of 1.6 GByte/s under the best burst transfer protocol <ref> [9, 7] </ref>. A bus system puts limitations on scalability (fixed number of slots for processor and memory cards) but provides free broadcast, which significantly simplifies the implementation of globally coherent caches. 3.2 Cray T3D In the Cray T3D, the caches play a much smaller role than in the DEC 8400. <p> Scalability to a large number of processors was not a target for the DEC 8400 series of machines and did not constrain the design. According to <ref> [7] </ref> a DEC8400 is limited to 12 processors and/or 14 GBytes of memory.
Reference: [8] <editor> Richard Sites (editor). </editor> <title> Alpha Architecture Reference Manual. </title> <institution> Digital Equipment Corp., </institution> <address> Burlington MA, </address> <year> 1992. </year> <month> EY-L520E-DP. </month>
Reference-contexts: For readers not familiar with the three architectures we discuss the important parameters relevant to the memory and communication system interface in Section 3. In addition to the technical reference material of the vendors (DEC 8400 <ref> [8, 6, 9, 7] </ref>, Cray T3D [8, 1, 3], Cray T3E [12, 4]), other research groups evaluated some aspects of these machines [2, 10, 15]. An empirical study comparing the two Alpha processors based on standard benchmarks provides useful insights using performance metrics not related to the memory system [5]. <p> For readers not familiar with the three architectures we discuss the important parameters relevant to the memory and communication system interface in Section 3. In addition to the technical reference material of the vendors (DEC 8400 [8, 6, 9, 7], Cray T3D <ref> [8, 1, 3] </ref>, Cray T3E [12, 4]), other research groups evaluated some aspects of these machines [2, 10, 15]. An empirical study comparing the two Alpha processors based on standard benchmarks provides useful insights using performance metrics not related to the memory system [5].
Reference: [9] <author> D. Fenwick, D. Foley, W. Gist, S. VanDoren, and D. Wissell. </author> <title> The AlphaServer 8000 series: High-end server platform development. </title> <journal> Digital Technical Journal Vol., </journal> <volume> 7(43), </volume> <month> Spring </month> <year> 1995. </year>
Reference-contexts: For readers not familiar with the three architectures we discuss the important parameters relevant to the memory and communication system interface in Section 3. In addition to the technical reference material of the vendors (DEC 8400 <ref> [8, 6, 9, 7] </ref>, Cray T3D [8, 1, 3], Cray T3E [12, 4]), other research groups evaluated some aspects of these machines [2, 10, 15]. An empirical study comparing the two Alpha processors based on standard benchmarks provides useful insights using performance metrics not related to the memory system [5]. <p> This bus is clocked at 75 MHz, a quarter of the clock frequency of the microprocessor, yielding a peak transfer-rate of 2.4 GByte/s across the system bus. This limit is reduced to a peak of 1.6 GByte/s under the best burst transfer protocol <ref> [9, 7] </ref>. A bus system puts limitations on scalability (fixed number of slots for processor and memory cards) but provides free broadcast, which significantly simplifies the implementation of globally coherent caches. 3.2 Cray T3D In the Cray T3D, the caches play a much smaller role than in the DEC 8400.
Reference: [10] <author> V. Karamcheti and A. Chien. </author> <title> A comparison of architectural support for messaging on the TMC CM-5 and Cray T3D. </title> <booktitle> In Proc. 22nd Intl. Symposium on Computer Architecture, </booktitle> <pages> pages 298-307, </pages> <address> Santa Marguerita di Ligure, </address> <month> June </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: In addition to the technical reference material of the vendors (DEC 8400 [8, 6, 9, 7], Cray T3D [8, 1, 3], Cray T3E [12, 4]), other research groups evaluated some aspects of these machines <ref> [2, 10, 15] </ref>. An empirical study comparing the two Alpha processors based on standard benchmarks provides useful insights using performance metrics not related to the memory system [5]. Our goal is to measure and compare the memory systems of these modern parallel systems.
Reference: [11] <author> C. Koelbel, D. Loveman, G. Steele, and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: the practical importance of vectorizable memory-intensive workloads, and when adapting such workloads to the cache oriented memory systems of microprocessors, difficulties may arise [18]. 2.1 Parallel machines as compiler target This investigation is part of the Fx Fortran compiler project; Fx [17] is a dialect of High Performance Fortran (HPF) <ref> [11] </ref>, and the Fx compiler has been re-targeted to a number of parallel systems. For many applications, the key to getting good performance is to generate efficient communication code; communication code is any code that moves data from one memory zone to another.
Reference: [12] <author> S. Scott. </author> <title> Synchronization and communication in the Cray T3E multiprocessor. </title> <booktitle> In Proc. 7th. International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, MA, </address> <month> Oct </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: For readers not familiar with the three architectures we discuss the important parameters relevant to the memory and communication system interface in Section 3. In addition to the technical reference material of the vendors (DEC 8400 [8, 6, 9, 7], Cray T3D [8, 1, 3], Cray T3E <ref> [12, 4] </ref>), other research groups evaluated some aspects of these machines [2, 10, 15]. An empirical study comparing the two Alpha processors based on standard benchmarks provides useful insights using performance metrics not related to the memory system [5]. <p> There is no L3 cache, but the memory system includes support for memory streams. For a complete a complete discussion about the design of the streaming units and the Eregisters see <ref> [12] </ref>. Remote stores and remote loads are performed through a set of external E-registers located in the support circuitry around the DEC Alpha processor. For the moment, we rely on a first implementation of the shmem iput and shmem iget communication primitives provided by Cray Research for the Cray T3E.
Reference: [13] <author> J. Stichnoth and T. Gross. </author> <title> A communication backend for parallel language compilers. </title> <booktitle> In Proceedings of 8th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 224-236, </pages> <address> Columbus, Ohio, August 1995. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: So to obtain performance on the DEC 8400, we must find a similar structure for the application, and we re-targeted the Fx compiler to generate such code. The Fx compilers for the DEC 8400 and the Cray T3D are based on the Catacomb compiler back-end <ref> [13] </ref>, which generates highly optimized code for explicit data transfers. This compiler does not rely on any hardware-support for coherent shared memory and carefully separates synchronization operations from data transfers, according to the direct deposit model (see next section).
Reference: [14] <author> T. Stricker. </author> <title> Direct deposit A communication infrastructure for parallel and distributed programs. </title> <type> Ph.D. Thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, </institution> <note> TR CMU-CS-96-xxx. </note>
Reference-contexts: Catacomb provides a general way of generating communication code for all array assignment statements and array distributions, not just for transposes of two dimensional, block distributed data. 2.2 Direct deposit/fetch models The Fx parallelizing compiler uses the direct deposit model <ref> [14, 16, 17] </ref>, which captures the style of communication relying on remote load/stores as primitives to transfer data. Coherency is only established at explicit synchronization points. <p> Such copy transfers are common in transpose operations. A third Store Constant benchmark was written as a dual to the Load Sum benchmark to evaluate store performance <ref> [14] </ref>. The resulting graphs did not add enough insight to the picture to warrant the space in a short conference paper. The store benchmarks confirmed the specified, default write-back policies of the caches and the proper function of the write back queues.
Reference: [15] <author> T. Stricker and T. Gross. </author> <title> Optimizing memory system performance for communication in parallel computers. </title> <booktitle> In Proc. 22nd Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 308-319, </pages> <address> Portofino, Italy, </address> <month> June </month> <year> 1995. </year> <month> ACM/IEEE. </month>
Reference-contexts: In addition to the technical reference material of the vendors (DEC 8400 [8, 6, 9, 7], Cray T3D [8, 1, 3], Cray T3E [12, 4]), other research groups evaluated some aspects of these machines <ref> [2, 10, 15] </ref>. An empirical study comparing the two Alpha processors based on standard benchmarks provides useful insights using performance metrics not related to the memory system [5]. Our goal is to measure and compare the memory systems of these modern parallel systems. <p> To tune the performance of the Fx compiler, we first measure the basic performance for key operations of the "copy-transfer-model" <ref> [15] </ref> to obtain performance figures for local and remote transfers. These micro-benchmarks allow the compiler writer, the compiler or the runtime-system to pick the least expensive way to move data in the system. <p> The copy transfer model provides a bandwidth-oriented characterization of a memory system <ref> [15] </ref> that pays attention to memory access patterns. In this model each communication step is seen as a composition of basic copy transfers with known performance characteristics. <p> If a given platform allows more than one way to implement a communication step, the modeled bandwidth metric is used to determine the best way to implement this communication step. The model used in <ref> [15] </ref> is asymptotic and applies only to large memory-to-memory transfers. We extend the copy transfer model by a working set parameter to capture the temporal locality of computation and communication methods that can be blocked for caches. <p> In this section we select a few key working sets from our general memory characterizations in Figures 1 to 6 and discuss the memory system performance in more detail. The characterizations are according to the basic copy transfer model <ref> [15] </ref> (no working set parameter, full copy operation read and write) and focus on large copy transfers with no reuse of data and without temporal locality in the caches. 6.1 Copy transfers in local memory Figures 9 to 11 depict the measured throughput of a local memory copy with either strided
Reference: [16] <author> T. Stricker, J. Stichnoth, D. O'Hallaron, S. Hinrichs, and T. Gross. </author> <title> Decoupling synchronization and data transfer in message passing systems of parallel computers. </title> <booktitle> In Proc. Intl. Conf. on Supercomputing, </booktitle> <pages> pages 1-10, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: Catacomb provides a general way of generating communication code for all array assignment statements and array distributions, not just for transposes of two dimensional, block distributed data. 2.2 Direct deposit/fetch models The Fx parallelizing compiler uses the direct deposit model <ref> [14, 16, 17] </ref>, which captures the style of communication relying on remote load/stores as primitives to transfer data. Coherency is only established at explicit synchronization points. <p> On a T3D, the massively parallel performance of our compiler generated 2D-FFT written in Fx Fortran stays around 20 MFlop/s per processor and reaches a total performance of 8.75 GFlops when run on 512 processors. The code shows almost linear scalability from 16 to 512 nodes <ref> [16] </ref>. Based on our model of memory and communication system performance we expect to report similar scalability and a sustained aggregate performance for a 2D-FFT of about 20 GFlops, once we run the code on a full-size machine.
Reference: [17] <author> J. Subhlok, J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Programming task and data parallelism on a multicomputer. </title> <booktitle> In Proc. 4th ACM Symp. on Principles and Practice of Parallel Prog. (PPoPP), </booktitle> <pages> pages 13-22, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: the Los Alamos and Liv-ermore National Laboratories point to the practical importance of vectorizable memory-intensive workloads, and when adapting such workloads to the cache oriented memory systems of microprocessors, difficulties may arise [18]. 2.1 Parallel machines as compiler target This investigation is part of the Fx Fortran compiler project; Fx <ref> [17] </ref> is a dialect of High Performance Fortran (HPF) [11], and the Fx compiler has been re-targeted to a number of parallel systems. <p> Catacomb provides a general way of generating communication code for all array assignment statements and array distributions, not just for transposes of two dimensional, block distributed data. 2.2 Direct deposit/fetch models The Fx parallelizing compiler uses the direct deposit model <ref> [14, 16, 17] </ref>, which captures the style of communication relying on remote load/stores as primitives to transfer data. Coherency is only established at explicit synchronization points.
Reference: [18] <author> H. Wasserman. </author> <title> Benchmark tests on the Digital Equipment Corporation Alpha AXP 21164-based AlphaServer 8400. </title> <booktitle> In Proc. 1996 International Conference on Supercomputing, </booktitle> <pages> pages 333-340, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: Typical workloads for machines of this class at the Los Alamos and Liv-ermore National Laboratories point to the practical importance of vectorizable memory-intensive workloads, and when adapting such workloads to the cache oriented memory systems of microprocessors, difficulties may arise <ref> [18] </ref>. 2.1 Parallel machines as compiler target This investigation is part of the Fx Fortran compiler project; Fx [17] is a dialect of High Performance Fortran (HPF) [11], and the Fx compiler has been re-targeted to a number of parallel systems.
Reference: [19] <author> X. Zhang and X. Qin. </author> <title> Performance prediction and evaluation of parallel processing on a NUMA multiprocessor. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(10) </volume> <pages> 1059-68, </pages> <month> Oct </month> <year> 1991. </year> <month> 12 </month>
Reference-contexts: This model is known as the non-uniform memory access model (NUMA) and a large number of papers (e.g., <ref> [19] </ref>) analyze algorithm performance under that model. The NUMA model does not distinguish between local and remote memory and describes memory just as fast and slow memory in terms of access latency.
References-found: 19

