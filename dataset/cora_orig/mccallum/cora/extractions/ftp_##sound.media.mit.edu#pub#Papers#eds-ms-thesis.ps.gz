URL: ftp://sound.media.mit.edu/pub/Papers/eds-ms-thesis.ps.gz
Refering-URL: http://sound.media.mit.edu/papers.html
Root-URL: http://www.media.mit.edu
Title: Extracting Expressive Performance Information from Recorded Music  Architecture and Planning  
Author: by Eric David Scheirer B.S. cum laude Stephen A. Benton 
Degree: in partial fulfillment of the requirements for the degree of Master of Science in Media Arts and Sciences at the  Certified by Barry Vercoe Professor of Media Arts and Sciences Thesis Supervisor Accepted by  Chairman, Departmental Committee on Graduate Students  
Date: September 1995  August 11, 1995  
Note: (1993) Submitted to the Program in  c Massachusetts Institute of Technology 1995. All rights reserved. Author: Program in  Program in  
Affiliation: Computer Science B.S. Linguistics Cornell University  Media Arts and Sciences, School of  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Media Arts and Sciences, School of Architecture and Planning  Media Arts and Sciences  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Jeff Bilmes. </author> <title> Timing is of the essence: Perceptual and computational techniques for representing, learning, and reproducing expressive timing in percussive rhythm. </title> <type> Master's thesis, </type> <institution> MIT Media Laboratory, </institution> <year> 1993. </year>
Reference-contexts: He combined this so-called auditory transform with principal components analysis techniques, and was able to use the resulting system to detect onsets in performances on pitched tabla drums. Schloss and Bilmes Schloss [20] and Bilmes <ref> [1] </ref>, in 1985 and 1993 respectively, built systems which could transcribe multi-timbral percussive music for the purpose of analyzing its expressive content. Both were successful, although they had slightly different goals. <p> We can similarly recreate other sorts of analyses such as those found in [16] or <ref> [1] </ref> by treating the timing variables as random Gaussian variables rather than known values. 1 Depending on which question we want to answer, though, the answers may be less satisfactory for small timing details.
Reference: [2] <author> Edwin Fischer. J.s. bach: </author> <title> The well-tempered clavier. Compact Disc Recording, </title> <year> 1989. </year>
Reference: [3] <author> Glenn Gould. J.s. bach: The well-tempered clavier, book i. </author> <title> Compact Disc Recording, </title> <year> 1993. </year>
Reference: [4] <author> Stephen Handel. </author> <title> Listening. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Thus, if we are to be able to use an automated system for understanding timing relationships between melodies and harmony, it must be able to resolve differences at this level of accuracy or finer. 5 ms is generally taken as the threshold of perceptual difference (JND) for musical performance <ref> [4] </ref>; if we wish to be able to reconstruct performances identical to the original, the timing accuracy must be at this level or better. 1.2.1 Existing systems Musical transcription systems have been an area of research since the early days of computer music.
Reference: [5] <author> Michael Hawley. </author> <title> Structure out of Sound. </title> <type> PhD thesis, </type> <institution> MIT Media Laboratory, </institution> <year> 1993. </year>
Reference-contexts: What publications are available [7] suggest that their work is frequency-domain based, and can cope with a variety of musical situations, including the ambiguity of the human voice and several-voice polyphony. Hawley Hawley describes a system for frequency-domain multi-voice transcription of piano music in his PhD dissertation <ref> [5] </ref>. Although relatively few details are provided, it seems to be able to handle two or more simultaneous notes.
Reference: [6] <author> Keith Jarrett. J.s. bach: The well-tempered clavier, book i. </author> <title> Compact Disc Recording, </title> <year> 1988. </year>
Reference: [7] <author> Haruhiro Katayose and Seiji Inokuchi. </author> <title> The kansei music system. </title> <journal> Computer Music Journal, </journal> <volume> 13(4), </volume> <year> 1989. </year>
Reference-contexts: Inokuchi et al Seiji Inokuchi and his collaborators at Osaka University in Japan have been conducting research into transcription for many years. Unfortunately, many of the references for their work are not yet available in English. What publications are available <ref> [7] </ref> suggest that their work is frequency-domain based, and can cope with a variety of musical situations, including the ambiguity of the human voice and several-voice polyphony. Hawley Hawley describes a system for frequency-domain multi-voice transcription of piano music in his PhD dissertation [5].
Reference: [8] <author> Carol Krumhansl. </author> <title> Cognitive Foundations of Musical Pitch. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1991. </year>
Reference-contexts: It is clear that the human music-cognition system is working with representations of music on many different levels which guide and shape the perception of a particular musical performance. Work such as Krumhansl's tonal hierarchy <ref> [8] </ref> and Narmour's multi-layered grouping rules [13], [14] show evidence for certain low- and mid-level cognitive 10 representations for musical structure. Syntactic work such as Lerdahl and Jackendoffs' [9], while not as well-grounded experimentally, suggests a possible structure for higher levels of music cognition.
Reference: [9] <author> Fred Lerdahl and Ray Jackendoff. </author> <title> A Generative Theory of Tonal Music. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: Work such as Krumhansl's tonal hierarchy [8] and Narmour's multi-layered grouping rules [13], [14] show evidence for certain low- and mid-level cognitive 10 representations for musical structure. Syntactic work such as Lerdahl and Jackendoffs' <ref> [9] </ref>, while not as well-grounded experimentally, suggests a possible structure for higher levels of music cognition.
Reference: [10] <author> Robert Maher. </author> <title> An Approach for the Separation of Voices in Composite Musical Signals. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1989. </year>
Reference: [11] <author> Robert Maher. </author> <title> Evaluation of a method for separating digitized duet signals. </title> <journal> J. Audio Eng. Soc., </journal> <volume> 38(12), </volume> <year> 1990. </year> <month> 55 </month>
Reference-contexts: Bilmes's transcription system was part of a larger system for the analysis of expressive timing in percussive music. It modeled small deviations in timing around an overall tempo structure, and could extract multiple simultaneous or nearly-simultaneous onsets by different instruments. Maher Maher's system ([10], <ref> [11] </ref>) build on Moorer's work, attempting to ease some of the restrictions there. His system, also for duet transcription, does allow harmonically-related onsets to occur simultaneously.
Reference: [12] <author> James Moorer. </author> <title> On the Segmentation and Analysis of Continuous Musical Sound by Digital Computer. </title> <type> PhD thesis, </type> <institution> CCRMA - Stanford University, </institution> <year> 1975. </year>
Reference-contexts: This list should not be construed as complete; it is rather difficult to locate references to all of the myriad systems which have been constructed, and it seems that no recent and systematic review of the field exists. Moorer Moorer's 1975 dissertation <ref> [12] </ref> used a system based on a bank of sharply-tuned bandpass filters to transcribe works with no more than two independent voices.
Reference: [13] <author> Eugene Narmour. </author> <title> The Analysis and Cognition of Basic Melodic Structures. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1990. </year>
Reference-contexts: It is clear that the human music-cognition system is working with representations of music on many different levels which guide and shape the perception of a particular musical performance. Work such as Krumhansl's tonal hierarchy [8] and Narmour's multi-layered grouping rules <ref> [13] </ref>, [14] show evidence for certain low- and mid-level cognitive 10 representations for musical structure. Syntactic work such as Lerdahl and Jackendoffs' [9], while not as well-grounded experimentally, suggests a possible structure for higher levels of music cognition.
Reference: [14] <author> Eugene Narmour. </author> <title> The Analysis and Cognition of Melodic Complexity. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1993. </year>
Reference-contexts: It is clear that the human music-cognition system is working with representations of music on many different levels which guide and shape the perception of a particular musical performance. Work such as Krumhansl's tonal hierarchy [8] and Narmour's multi-layered grouping rules [13], <ref> [14] </ref> show evidence for certain low- and mid-level cognitive 10 representations for musical structure. Syntactic work such as Lerdahl and Jackendoffs' [9], while not as well-grounded experimentally, suggests a possible structure for higher levels of music cognition.
Reference: [15] <author> Alan Oppenheim and S. Hamid Nawab. </author> <title> Symbolic and Knowledge-Based Signal Processing. </title> <publisher> Prentice-Hall, Inc, </publisher> <year> 1992. </year>
Reference-contexts: This idea is similar to those presented in Oppenheim and Nawab's recent book <ref> [15] </ref> regarding symbolic signal processing. From this viewpoint, score-aided transcription can be viewed as a step in the direction of building musical systems with layers of significance other than a signal-processing network alone. <p> As one example of this, work is currently in progress on a blackboard system architecture (see, eg, <ref> [15] </ref>) for investigation of these issues.
Reference: [16] <author> Caroline Palmer. </author> <title> Timing in Skilled Music Performance. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <year> 1989. </year>
Reference-contexts: As part of the process of building music-understanding computer systems, we would like to study and analyze human expressive performance. Such analysis helps with the goal of building machines that can both understand and produce humanistic musical performances. Typically, research into expressive performance for example, that of Palmer <ref> [16] </ref> - uses sophisticated equipment such as the B osendorfer optical-recording piano to transcribe performances by expert pianists into symbolic form for analysis by the researcher. <p> As we shall see, once we are armed with this information, it is a significantly easier problem to extract accurate timings from the digital audio stream. Palmer <ref> [16] </ref> suggests certain levels of timing accuracy which can be understood as benchmarks for a system which is to extract note information at a level useful for understanding interpretation. <p> We can similarly recreate other sorts of analyses such as those found in <ref> [16] </ref> or [1] by treating the timing variables as random Gaussian variables rather than known values. 1 Depending on which question we want to answer, though, the answers may be less satisfactory for small timing details.
Reference: [17] <author> Athanasios Papoulis. </author> <title> Probability, Random Variables, and Stochastic Processes. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, third edition, </address> <year> 1991. </year>
Reference-contexts: which the current system could be improved, and conclude with some thoughts on the value of transcription systems. 4.1 Stochastic Analysis of Music Performance Part of the value of the sort of variance-of-error study conducted in the Results section is that we can treat extracted data as a stochastic estimator <ref> [17] </ref> for the actual performance, and make firm enough assumptions about the distribution of the estimation errors that we can obtain usable results. It is clear that some aspects of expressive music performance can be readily analyzed within the constraints of the variance in extraction discussed above. <p> We can calculate the probabilities that the notes were actually struck within the 5 ms window of perceptual simultaneity, or that the earlier or later was, in fact, struck first. To do this calculation, we build a Bayesian estimator of the time lag <ref> [17] </ref>, and use error functions; we find that the probability that the earlier extraction was actually struck first is 0.6643, and 1 It is arguable that they should have been treated this way in the cited work to begin with, since there is bound to be sensor noise coming into play.
Reference: [18] <author> Alan Ruttenberg. </author> <title> Optical reading of typeset music. </title> <type> Master's thesis, </type> <institution> MIT Media Laboratory, </institution> <year> 1991. </year>
Reference-contexts: The resulting score-file could be quantized moved to lie on rhythmic boundaries if the performance is rhythmically uneven. There are also systems based on optical character recognition techniques which can be used to scan and convert the notated score. Alan Ruttenberg's MS thesis <ref> [18] </ref> is an example 15 of such a system. 2.1.2 Extracted score-file information The specific kinds of syntactic information which are extracted from the score-file are those which have an influence on the attack- and release-finding algorithms described in the next section.
Reference: [19] <author> Andras Schiff. J.s. bach: The well-tempered clavier, book i. </author> <title> Compact Disc Recording, </title> <year> 1984. </year>
Reference: [20] <author> W. Andrew Schloss. </author> <title> On the Automatic Transcription of Percussive Music from Acoustical Signal to High-Level Analysis. </title> <type> PhD thesis, </type> <institution> CCRMA - Stanford University, </institution> <month> May </month> <year> 1985. </year>
Reference-contexts: He combined this so-called auditory transform with principal components analysis techniques, and was able to use the resulting system to detect onsets in performances on pitched tabla drums. Schloss and Bilmes Schloss <ref> [20] </ref> and Bilmes [1], in 1985 and 1993 respectively, built systems which could transcribe multi-timbral percussive music for the purpose of analyzing its expressive content. Both were successful, although they had slightly different goals.
Reference: [21] <author> John Stautner. </author> <title> The auditory transform. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <year> 1982. </year>
Reference-contexts: Within this framework, the system was a success at transcribing violin and guitar duets. Only rough timing accuracy was required, as the results were quantized to be similar to the original score. Stautner In his 1983 MS thesis, Stautner <ref> [21] </ref> used frequency-domain methods to attempt to model the human auditory system, basing his filter parameters on findings from research into the auditory physiology.
Reference: [22] <author> Barry Vercoe. </author> <title> The synthetic performer in the context of live performance. </title> <booktitle> In Proc. Int. Computer Music Conf., </booktitle> <year> 1984. </year> <month> 56 </month>
Reference-contexts: It is anticipated that any of these situations could be dealt with in the current architec 46 ture, although the tempo-follower would have to be made more robust in order to handle performance which are not well-modeled by linear tempo segments. This is generally a solvable problem, though see <ref> [22] </ref> for an example. 4.3 Evidence-Integration Systems The evidence integration aspects of the system are the most novel, and at the same time, the least satisfying.
References-found: 22

