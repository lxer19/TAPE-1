URL: http://www.cs.cornell.edu/Info/People/ronitt/PAP/games.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/ronitt/papers.html
Root-URL: http://www.cs.cornell.edu
Title: Efficient Algorithms for Learning to Play Repeated Games Against Computationally Bounded Adversaries  
Author: Yoav Freund Michael Kearns Yishay Mansour Dana Ron Ronitt Rubinfeld Robert E. Schapire 
Affiliation: AT&T Bell Laboratories  AT&T Bell Laboratories  Tel-Aviv University  M.I.T.  Cornell University  AT&T Bell Laboratories  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Thomas Dean, Dana Angluin, Kenneth Basye, Sean Engelson, Leslie Kaelbling, Evangelos Kokkevis, and Oded Maron. </author> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <journal> Machine Learning, </journal> <volume> 18(1) </volume> <pages> 81-108, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Where no confusion will result, we drop the superscript N indicating the current round. As a simple example, we might consider a contract adversary that plays 1 in the current round if and only if p 11 &lt; for some threshold 2 <ref> [0; 1] </ref>. This adversary is willing to let its opponent receive a payoff only if the opponent is not too wealthy already. In general, one might consider adversaries whose current play is a complicated function of not only the p ab but also of other statistics of the history. <p> To see that the present situation is considerably more favorable, we make the following definition: p 2 <ref> [0; 1] </ref> has rational complexity at most N if p is a rational number whose numerator and denominator are both at most N . <p> Again, N = O ((1=*) log (1=*)) suffices. 2 (Lemma 4.2) Armed with the analysis of algorithm L 1 , we can now return to the two-dimensional adversary f ff;fi (p 00 ; p 11 ). In this case, the optimal contract payoff is the largest value p 2 <ref> [0; 1] </ref> such that f ff;fi (1 p; p) = 1. In other words, an optimal contract strategy tries to maintain p 00 = 1 p and p 11 = p for p as large as possible; see Figure 1. <p> Our algorithm will still attempt to penny-match, but using a different strategy. The algorithm partitions the p 11 axis into disjoint subinter-vals. Initially, there is only the single subinterval <ref> [0; 1] </ref>. <p> More formally, a Probabilistic State Automaton M is a tuple (Q; q 0 ; fl; t ), where Q is a finite set of n states, q 0 2 Q is the designated starting state, fl : Q ! <ref> [0; 1] </ref>, is the state bias function, and t : Q fi f0; 1g ! Q is the transition function. The adversary PSA M is initially in its starting state q 0 . <p> This latter event can happen only n 1 times before h is a true homing sequence. To estimate the coin biases encountered on the last execution of ff, we use an idea of Dean et al. <ref> [1] </ref> in the related setting of learning DFA's with noisy outputs 4 . Assume we execute ff, m consecutive times, and we are interested in computing the sequence of state coin biases passed on the mth execution of ff. <p> More formally, a PTA M , M = (Q; q 0 ; fl; t ) is the same as a PSA except that now the transition function t is defined as follows: t : Q fi f0; 1g fi Q ! <ref> [0; 1] </ref>, where for every q 2 Q, and for every b 2 f0; 1g, P is the case when M is a PSA, M is initially in its starting state q 0 .
Reference: [2] <author> Lance Fortnow and Duke Whang. </author> <title> Optimality and domination in repeated games with bounded players. </title> <booktitle> In The 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 741-749, </pages> <year> 1994. </year>
Reference-contexts: Part of this research was conducted while visiting AT&T Bell Laboratories. Supported by ONR Young Investigator Award N00014-93-1-0590 and grant No. 92-00226 from the United States-Israel Binational Science Foundation. of Fortnow and Whang <ref> [2] </ref>. Here we examine the problem of learning to play various games optimally against resource-bounded adversaries, with an explicit emphasis on the computational efficiency of the learning algorithm. <p> For playing penny-matching, we may not need to build a detailed model of M as shown by Fortnow and Whang, it suffices to discover a penny-matching cycle of M <ref> [2] </ref>. For contract, while an exact model of M may still be unnecessary, we must do enough exploration to find any regions where M plays 1 frequently. <p> This continues a line of research investigated by Gilboa and Samet [3], and more recently by Fortnow and Whang <ref> [2] </ref> and Vovk [12]. Our main result is a polynomial-time algorithm for learning to play any game nearly optimally against any finite automaton with probabilistic states (defined below) and small cover time. <p> However, as in their original strategy for DFA's, it may take an exponential number of rounds (exponential in the number of states of the adversary PSA) to achieve payoff which is close to optimal. Fortnow and Whang <ref> [2] </ref> showed that in the case of DFA's, if the underlying game played has a certain property (held by contract but not by penny-matching), then there is also an exponential lower bound for the number of rounds needed before approaching optimality. <p> As before we let n = jQj. It is worth noting that every automaton which has both probabilistic transitions and probabilistic actions can be transformed into an automaton of approximately the same size which has only probabilistic transitions. Fortnow and Whang <ref> [2] </ref> show that there exist PTA's for which there is no optimal strategy, even for penny-matching (where a strategy g is optimal with respect to an adversary f if lim inf N!1 PAYOFF N f (g) is maximized).
Reference: [3] <author> Itzhak Gilboa and Dov Samet. </author> <title> Bounded versus unbounded rationality: The tyranny of the weak. </title> <journal> Games and Economic Behavior, </journal> <volume> 1(3) </volume> <pages> 213-221, </pages> <year> 1989. </year>
Reference-contexts: Some authors have examined the further problem of learning to play optimally against an adversary whose precise strategy is unknown, but is constrained to lie in some known class of strategies (for instance, see Gilboa and Samet <ref> [3] </ref>). It is this research that forms our starting point. <p> This continues a line of research investigated by Gilboa and Samet <ref> [3] </ref>, and more recently by Fortnow and Whang [2] and Vovk [12]. Our main result is a polynomial-time algorithm for learning to play any game nearly optimally against any finite automaton with probabilistic states (defined below) and small cover time. <p> As in the case of DFA's, for every PSA there exists an optimal strategy that is a repeated simple cycle <ref> [3] </ref>. Moreover, if the automaton is given, the optimal cycle can be found efficiently using dynamic programming. Gilboa and Samet [3] note that their algorithm for learning an optimal cycle strategy against DFA's can be easily adapted to PSA's. <p> As in the case of DFA's, for every PSA there exists an optimal strategy that is a repeated simple cycle <ref> [3] </ref>. Moreover, if the automaton is given, the optimal cycle can be found efficiently using dynamic programming. Gilboa and Samet [3] note that their algorithm for learning an optimal cycle strategy against DFA's can be easily adapted to PSA's.
Reference: [4] <author> Ehud Kalai. </author> <title> Bounded rationality and strategic complexity in repeated games. </title> <booktitle> Game Theory and Applications, </booktitle> <pages> pages 131-157, </pages> <year> 1990. </year>
Reference-contexts: The typical approach is to assume that the adversary's strategy is a member of some natural class of computationally bounded strategies most often, a class of finite automata. (For a survey on the area of bounded rationality, see the paper of Kalai <ref> [4] </ref>.) Many previous papers examine how various aspects of classical game theory change in this setting; a good example is the question of whether cooperation is a stable solution for prisoner's dilemma when both players are finite automata [6, 8].
Reference: [5] <author> N. Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: Similar arguments apply to learning to play contract. On the positive side, suppose that the recent history adversary f (~u; ~v) is drawn from a class A of boolean functions for which there is an absolute mistake-bounded algorithm <ref> [5] </ref> that is, a prediction algorithm A that makes at most m (`; size (f )) mistakes in predicting the values of any target function chosen from A on any sequence of 2`-bit inputs (history vectors), where m (`; size (f )) is a polynomial. <p> Note that f S is a 2-DNF formula over the variables u 1 ; : : : ; u ` ; v 1 ; : : : ; v ` . Since such formulae have an efficient absolute mistake-bounded algorithm <ref> [5] </ref>, our comments above imply that we can learn to play penny-matching nearly optimally against such adversaries in polynomial time. Let us now examine the more subtle problem of playing contract against this class. <p> If L 1 's play fails to match that of f , then the width of the version space has been at least halved. In the usual models of on-line prediction <ref> [5] </ref>, the sequence of inputs x i to f to be classified (that is, penny-matched) is arbitrary, and the halving algorithm may be forced to make N prediction mistakes in N trials due to the arbitrary precision of the x i (an unfavorable sequence would always arrange the next x i
Reference: [6] <author> Abraham Neyman. </author> <title> Bounded complexity justifies cooperation in the finitely repeated prisoners' dilemma. </title> <journal> Economics Letters, </journal> <volume> 19 </volume> <pages> 227-229, </pages> <year> 1985. </year>
Reference-contexts: survey on the area of bounded rationality, see the paper of Kalai [4].) Many previous papers examine how various aspects of classical game theory change in this setting; a good example is the question of whether cooperation is a stable solution for prisoner's dilemma when both players are finite automata <ref> [6, 8] </ref>. Some authors have examined the further problem of learning to play optimally against an adversary whose precise strategy is unknown, but is constrained to lie in some known class of strategies (for instance, see Gilboa and Samet [3]). It is this research that forms our starting point.
Reference: [7] <author> Christos H. Papadimitriou and John N. Tsitsiklis. </author> <title> The complexity of Markov decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3):441 - 450, </volume> <year> 1987. </year>
Reference-contexts: If randomized transitions are allowed, we show that even approximating the optimal strategy against a given automaton is PSPACE -complete. This improves the result of Papadimitriou and Tsitsiklis <ref> [7] </ref>. 2 Models and Definitions In this paper, games are played by two players. One will be called the adversary and the other the strategy learning algorithm. <p> We also prove that if the adversary automaton has probabilistic transitions (defined below) then even the problem of approximating the optimal payoff when the automaton is given to the algorithm is PSPACE -complete (improving on a result of Papadimitriou and Tsitsiklis <ref> [7] </ref>), which precludes a positive result for learning to play optimally against such machines. 5.1 Probabilistic State Automata A probabilistic state automaton (PSA) can be thought of as a generalization of a DFA in which each state contains a different biased coin (with the usual definition of a DFA being obtained <p> Fortnow and Whang [2] show that there exist PTA's for which there is no optimal strategy, even for penny-matching (where a strategy g is optimal with respect to an adversary f if lim inf N!1 PAYOFF N f (g) is maximized). Papadimitriou and Tsitsiklis <ref> [7] </ref> show that computing the exact payoff of the optimal strategy (given that it exists) is PSPACE -complete. Unfortunately, this result is not sufficient to dismiss the possibility of an efficient strategy learning algorithm, because such an algorithm must only discover an approximately optimal strategy. <p> In the following theorem we claim that even approximating the optimal payoff for N = poly (n) rounds within W (1= p n) is PSPACE -complete. As in the work of Papadimitriou and Tsitsiklis <ref> [7] </ref>, we assume the automaton is given as input to the algorithm. Clearly, if the automaton is not given (as in the learning setting), the problem of learning an approximately optimal strategy is at least as hard.
Reference: [8] <author> Christos H. Papadimitriou and Mihalis Yannakakis. </author> <title> On complexity as bounded rationality. </title> <booktitle> In Proceedings of the Twenty Sixth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 726-733, </pages> <year> 1994. </year>
Reference-contexts: survey on the area of bounded rationality, see the paper of Kalai [4].) Many previous papers examine how various aspects of classical game theory change in this setting; a good example is the question of whether cooperation is a stable solution for prisoner's dilemma when both players are finite automata <ref> [6, 8] </ref>. Some authors have examined the further problem of learning to play optimally against an adversary whose precise strategy is unknown, but is constrained to lie in some known class of strategies (for instance, see Gilboa and Samet [3]). It is this research that forms our starting point.
Reference: [9] <author> Ronald L. Rivest and Robert E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <journal> Information and Computation, </journal> <volume> 103(2) </volume> <pages> 299-347, </pages> <year> 1993. </year>
Reference-contexts: In fact, it is possible to show [10] that in the case of small cover time, the PSA learning problem reduces to the problem of orientation (details omitted), and thus we concentrate on this latter problem. A well-studied procedure for orientation when learning DFA's uses homing sequences <ref> [9] </ref>. A homing sequence for a DFA is an action sequence h such that regardless of the starting state, the sequence of state labels observed during the execution of h uniquely determines the state reached.
Reference: [10] <author> Dana Ron and Ronitt Rubinfeld. </author> <title> Exactly learning automata with small cover time. </title> <booktitle> In Proceedings of the Eighth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 98-109, </pages> <year> 1995. </year>
Reference-contexts: Our result improves on the work of Ron and Rubinfeld <ref> [10] </ref>, who examine the special case of state bias functions that take only two possible values and 1 for 0 1. Theorem 5.1 For any game G, there is an efficient strategy learning algorithm for playing G against the class of probabilistic state automata with polynomial cover time. <p> In fact, it is possible to show <ref> [10] </ref> that in the case of small cover time, the PSA learning problem reduces to the problem of orientation (details omitted), and thus we concentrate on this latter problem. A well-studied procedure for orientation when learning DFA's uses homing sequences [9].
Reference: [11] <author> Dana Ron and Ronitt Rubinfeld. </author> <title> Learning fallible finite state automata. </title> <journal> Machine Learning, </journal> <volume> 18 </volume> <pages> 149-185, </pages> <year> 1995. </year>
Reference-contexts: In order to simplify the presentation we start by making two assumptions; the first can be made without loss of generality, and the second can be removed using a technique due to Ron and Rubinfeld <ref> [11] </ref>. Assumption (1): M is irreducible in the sense that there is no smaller automaton equivalent to it.
Reference: [12] <author> V. G. Vovk. </author> <title> An optimal-control application of two paradigms of on-line learning. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 98-109, </pages> <year> 1994. </year>
Reference-contexts: This continues a line of research investigated by Gilboa and Samet [3], and more recently by Fortnow and Whang [2] and Vovk <ref> [12] </ref>. Our main result is a polynomial-time algorithm for learning to play any game nearly optimally against any finite automaton with probabilistic states (defined below) and small cover time.
References-found: 12

