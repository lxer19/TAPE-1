URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1191/CS-TR-93-1191.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1191/
Root-URL: http://www.cs.wisc.edu
Title: Smoothing Methods for Convex Inequalities and Linear Complementarity Problems  
Author: Chunhui Chen O. L. Mangasarian 
Keyword: Key Words: Smoothing, convex inequalities, linear complementarity. Abbreviated Title: Smoothing Methods in Mathematical Programming  
Date: November 1993  
Abstract: A smooth approximation p(x; ff) to the plus function: maxfx; 0g, is obtained by integrating the sigmoid function 1=(1 + e ffx ), commonly used in neural networks. By means of this approximation, linear and convex inequalities are converted into smooth, convex unconstrained minimization problems, the solution of which approximates the solution of the original problem to a high degree of accuracy for ff sufficiently large. In the special case when a Slater constraint qualification is satisfied, an exact solution can be obtained for finite ff. Speedup over MINOS 5.4 was as high as 515 times for linear inequalities of size 1000 fi 1000, and 580 times for convex inequalities with 400 variables. Linear complementarity problems are converted into a system of smooth nonlinear equations and are solved by a quadratically convergent Newton method. For monotone LCP's with as many as 400 variables, the proposed approach was as much as 85 times faster than Lemke's method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.W. Cottle, J.-S. Pang, and R.E. Stone. </author> <title> The Linear Complementarity Problem. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Let f (x) = 2 2 (21) We will show that under the assumption that M is a P 0 matrix, that is a matrix with nonnegative minors <ref> [1] </ref>, then all the stationary points of (21) are solutions of (20). First we will state a simple lemma for P 0 matrices. Lemma 4.2 Suppose M 2 R nfin is a P 0 matrix. <p> Therefore x i (M x) i = x i (d i x i ) = d i x 2 i , which is negative whenever x i 6= 0; i = 1; ; n. This contradicts Theorem 3.4.2 of <ref> [1] </ref>. Theorem 4.1 Consider LCP (M; q) with M 2 P 0 . Let x (ff) be a stationary point of min x2R n f (x), where f (x) is defined by (21). Then x (ff) is a solution of (20). <p> Note that the class of P 0 matrices contains the classes of P matrices, positive semi-definite matrices and row-sufficient matrices <ref> [1] </ref>. For this class of matrices, if f (x) defined by (21) has a stationary point, that point is also a solution of (20). Now we establish the existence of a solution to (20) for P 0 " R 0 matrices. <p> Now we establish the existence of a solution to (20) for P 0 " R 0 matrices. A matrix M is called an R 0 matrix if the only solution to LCP (M; 0) is the zero vector <ref> [1] </ref>. 11 Theorem 4.2 Consider LCP (M; q) with M 2 P 0 " R 0 . The system of nonlinear equations (20) always has a solution. Proof Let f (x) = 1 2 kxp (xM xq; ff)k 2 2 .
Reference: [2] <author> A. J. Hoffman. </author> <title> On approximate solutions of systems of linear inequalities. </title> <journal> Journal of Research of the National Bureau of Standards, </journal> <volume> 49 </volume> <pages> 263-265, </pages> <year> 1952. </year>
Reference-contexts: In the following, we will prove that a solution of (5) gives an approximate solution of (2). First we will state an error bound lemma for linear inequalities. Lemma 2.1 Error bound <ref> [2] </ref> [5] Suppose that the linear inequalities Ax b have a nonempty solution set X.
Reference: [3] <author> M. Kojima, S. Mizuno, and A. Yoshise. </author> <title> A polynomial-time algorithm for a class of linear complementarity problems. </title> <institution> Research Report B-193, Department of Information Sciences, Tokyo Institute of Technology, </institution> <address> Tokyo, Japan, </address> <year> 1987. </year>
Reference-contexts: In the following theorem, we assume that all the elements of matrix M and vector q are integers and n 2. Let L is the size of LCP (M; q) defined by <ref> [3] </ref> L = b i=1 j=1 i=n X log (jq i j) + log (n 2 )c + 1: Theorem 4.4 Suppose that LCP (M; q) is solvable. Let x (ff) be a solution of (20) with ff ff = p n2 L . <p> Hence x (ff)(M x (ff) + q) = ff 2 &lt; ff 2 2 2L p n2 L . By the purification procedure described in Appendix B <ref> [3] </ref>, x (ff) can be purified to a solution of LCP (M; q). 12 5 Numerical Results We now give a summary of our computational experience with the algorithms described in this paper. All the algorithms were run on a DECstation 3100. All problems were randomly generated.
Reference: [4] <author> K. Madsen and H.B. Nielsen. </author> <title> A finite smoothing algorithm for linear l 1 estimation. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 3(2) </volume> <pages> 223-235, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The inverse function p 1 is well defined for x 2 (0; 1). 8. p (x; ff) &gt; p (x; fi), for ff &lt; fi, x 2 R. Smoothing techniques have been used for l 1 -minimization problems <ref> [4] </ref> and in multi-commodity flows problem [11] using a linear quadratic smoothing function with encouraging numerical results. We now summarize our results. In Section 2 we treat linear inequalities by converting them to unconstrained differentiable minimization problems.
Reference: [5] <author> O.L. Mangasarian. </author> <title> A condition number for linear inequalities and linear programs. </title> <editor> In G. Bamberg and O. Opitz, editors, </editor> <booktitle> Proceedings of 6. Symposium uber Operations Research, </booktitle> <address> Augsburg, </address> <month> 7-9 September </month> <year> 1981, </year> <pages> pages 3-15, </pages> <address> Konigstein, 1981. Verlagsgruppe Athenaum/Hain/Scriptor/Hanstein. </address>
Reference-contexts: In the following, we will prove that a solution of (5) gives an approximate solution of (2). First we will state an error bound lemma for linear inequalities. Lemma 2.1 Error bound [2] <ref> [5] </ref> Suppose that the linear inequalities Ax b have a nonempty solution set X.
Reference: [6] <author> O.L. Mangasarian. </author> <title> A condition number for differentiable convex inequalities. </title> <journal> Mathematics of Operations Research, </journal> <volume> 10(2) </volume> <pages> 175-179, </pages> <year> 1985. </year>
Reference-contexts: (ff) and x 2 (ff) be solutions of (15) with f = f 1 and f = f 2 respectively. (i) Let X be bounded and let g satisfy Slater constraint qualification: g (^x) &lt; 0 or let g (x) be differentiable and satisfy the Slater and asymptotic constraint qualification <ref> [6] </ref>. <p> Then there exist x 1 (x 1 (ff)) and x 2 (x 2 (ff)), both in X, such that kx 1 (ff) x 1 (x 1 (ff))k 1 ff and log 2 p where C 1 and C 2 are constants dependent on g (x) <ref> [13, 6] </ref>. (ii) If the Slater constraint qualification is satisfied by g (x) 0, then there exists an ff &gt; 0 such that for any ff ff, x 1 (ff) and x 2 (ff) solve the convex inequalities (12) exactly.
Reference: [7] <author> O.L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <type> Technical Report 1129, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, </institution> <note> Decem-ber 1992. ORSA Journal on Computing 1993, 5(4), </note> <year> 1993. </year>
Reference-contexts: Note that (x) + = R x 1 (y)dy, where (x) is the step function: (x) = 1 if x &gt; 0 In the extensive neural network literature <ref> [7] </ref>, the step function is very effectively approximated by the sigmoid function s (x; ff) := 1 + e ffx ; ff &gt; 0 See Figures 1 and 3.
Reference: [8] <author> O.L. Mangasarian. </author> <title> Error bounds for inconsistent linear inequalities and programs. </title> <type> Technical Report 1166, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, </institution> <month> July </month> <year> 1993. </year> <note> Operations Research Letters, submitted. 15 </note>
Reference-contexts: In fact a multiple of value of f (x) bounds the distance of x to the set of minimizers of k (Ax b) + k 1 for the case when f = f 1 , see <ref> [8] </ref>.
Reference: [9] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.0 user's guide. </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1983. </year>
Reference-contexts: All the algorithms were run on a DECstation 3100. All problems were randomly generated. The smoothing algorithms were implemented in C. Lemke's method was written in FORTRAN. The CPU times for the smoothing algorithms and Lemke's method do not include the time to input data. The time of MINOS5.4 <ref> [9] </ref> is the execution time for subroutine M5SOLV and also does not include the input time. For linear and convex inequalities, we use the BFGS algorithm to solve the unconstrained minimization problem for variables up to 400 for linear inequalities and 150 for convex inequalities.
Reference: [10] <author> J. Nocedal. </author> <title> Theory of algorithms for unconstrained optimization. </title> <journal> Acta Numerica, </journal> <pages> pages 199-242, </pages> <year> 1992. </year>
Reference-contexts: For linear and convex inequalities, we use the BFGS algorithm to solve the unconstrained minimization problem for variables up to 400 for linear inequalities and 150 for convex inequalities. For larger problems, limited memory BFGS algorithm <ref> [10] </ref> was used. Starting with ff = 5, we increased ff by a factor of 1.05 to 1.2. The algorithm terminates when infeasibilities are less than 1.0e-7.
Reference: [11] <author> M. C. Pinar and S. A. Zenios. </author> <title> On smoothing exact penalty functions for convex constrained optimization. </title> <type> Technical report, </type> <institution> Decision Science Department, University of Pennsylvania, </institution> <address> Philadelphia, PA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: The inverse function p 1 is well defined for x 2 (0; 1). 8. p (x; ff) &gt; p (x; fi), for ff &lt; fi, x 2 R. Smoothing techniques have been used for l 1 -minimization problems [4] and in multi-commodity flows problem <ref> [11] </ref> using a linear quadratic smoothing function with encouraging numerical results. We now summarize our results. In Section 2 we treat linear inequalities by converting them to unconstrained differentiable minimization problems.
Reference: [12] <author> J. Ren. </author> <title> Computable error bounds in mathematical programming. </title> <type> Technical Report 1173, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: By Lemma 5.2.1 of <ref> [12] </ref>, there exists a x 1 (x 1 (ff)) 2 X 1 such that kx 1 (ff) x 1 (x 1 (ff))k 1 1 (A; b)(k (Ax 1 (ff) b) + k 1 k (Ax 1 (x 1 (ff)) b) + k 1 ) + 1 (A; b) ff Similarly, x <p> By Lemma 5.3.2 of <ref> [12] </ref> and some tedious computation, we get the desired conclusion. Remark 2.1 Suppose that the solution set of (2) is nonempty and bounded, then the level sets of f (x) are compact and f (x) is strongly convex on its level sets. <p> Let x (ff) be a solution of (20). Then there exists an x (x (ff)) which is a solution of LCP (M; q) such that kx (ff) x (x (ff))k 2 t (M; q) n log 2 ; where t (M; q) is a constant, see Theorem 2.2.1 <ref> [12] </ref>. The following theorem proves that if ff is sufficiently large, then a solution of (20) can be purified to a solution of LCP (M; q). In the following theorem, we assume that all the elements of matrix M and vector q are integers and n 2.
Reference: [13] <author> S.M. Robinson. </author> <title> Application of error bounds for convex programming in a linear space. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 13 </volume> <pages> 271-273, </pages> <year> 1975. </year>
Reference-contexts: Then there exist x 1 (x 1 (ff)) and x 2 (x 2 (ff)), both in X, such that kx 1 (ff) x 1 (x 1 (ff))k 1 ff and log 2 p where C 1 and C 2 are constants dependent on g (x) <ref> [13, 6] </ref>. (ii) If the Slater constraint qualification is satisfied by g (x) 0, then there exists an ff &gt; 0 such that for any ff ff, x 1 (ff) and x 2 (ff) solve the convex inequalities (12) exactly.
Reference: [14] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1970. </year> <month> 16 </month>
Reference-contexts: Let rc (g) denote the recession cone of a proper convex function g, that is rc (g) = fyj sup x2dom g (g (x + y) g (x)) 0g, where dom g is the domain of g <ref> [14] </ref>. Now we will state a condition under which (15) has a solution. Theorem 3.1 Let g : R n ! R m be continuous and convex and let f (x) be defined as in (13) or (14). The following are equivalent: 1.
References-found: 14

