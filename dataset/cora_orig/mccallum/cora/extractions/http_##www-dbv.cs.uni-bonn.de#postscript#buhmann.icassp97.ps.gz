URL: http://www-dbv.cs.uni-bonn.de/postscript/buhmann.icassp97.ps.gz
Refering-URL: 
Root-URL: 
Email: email:fjb,thg@cs.uni-bonn.de,  
Title: ROBUST VECTOR QUANTIZATION BY COMPETITIVE LEARNING  
Author: Joachim M. Buhmann Thomas Hofmann Rheinische Friedrich-Wilhelms-Universitat 
Web: http://www-dbv.cs.uni-bonn.de  
Address: Romerstrae 164, D-53117 Bonn, Germany  
Affiliation: Institut fur Informatik III,  
Abstract: Competitive neural networks can be used to efficiently quantize image and video data. We discuss a novel class of vector quantizers which perform noise robust data compression. The vector quantizers are trained to simultaneously compensate channel noise and code vector elimination noise. The training algorithm to estimate code vectors is derived by the maximum entropy principle in the spirit of deterministic annealing. We demonstrate the performance of noise robust codebooks with compression results for a teleconferencing system on the basis of a wavelet image representation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Antonini, M. Barlaud, P. Mathieu, and I. Daubechies. </author> <title> Image coding using wavelet transform. </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> 1(2) </volume> <pages> 205-220, </pages> <year> 1992. </year>
Reference-contexts: Due to the fact, that severe bandwidth limitations as well as noisy transmission channels are a typical problem especially for wireless teleconferencing, this is a realistic scenario for robust vector quantization. The wavelet transformation and the grouping scheme of wavelet coefficients into blocks is due to <ref> [1] </ref>, with a three-level biorthogonal wavelet transformation. Data vectors were generated by grouping neighboring wavelet coefficients in blocks of size 4fi4 and 2 fi 2 for each sub-band separately, c.f.
Reference: [2] <author> J. M. Buhmann and H. Kuhnel. </author> <title> Complexity optimized data clustering by competitive neural networks. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 75-88, </pages> <year> 1993. </year>
Reference-contexts: Robust vector quantization is formulated as an optimization problem, extending the source-channel coding approach presented in [9, 5]. An efficient optimization heuristic is derived within the deterministic annealing framework <ref> [13, 3, 2] </ref>. The on-line version of the proposed algorithm is a neural network technique, which belongs to the class of competitive learning methods. The rigorous derivation from an optimization principle guarantees an information-theoretic interpretation of the presented algorithm.
Reference: [3] <author> J. M. Buhmann and H. Kuhnel. </author> <title> Vector quantisa-tion with complexity costs. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39 </volume> <pages> 1133-1145, </pages> <year> 1993. </year>
Reference-contexts: Robust vector quantization is formulated as an optimization problem, extending the source-channel coding approach presented in [9, 5]. An efficient optimization heuristic is derived within the deterministic annealing framework <ref> [13, 3, 2] </ref>. The on-line version of the proposed algorithm is a neural network technique, which belongs to the class of competitive learning methods. The rigorous derivation from an optimization principle guarantees an information-theoretic interpretation of the presented algorithm. <p> This problem is also known as source-channel coding. We assume the noise characteristics of the channel to be known and denote by S ff- the probability of receiving index after sending ff through the channel. It has been noticed <ref> [9, 5, 11, 3] </ref> that source-channel coding may result in a topological ordering of prototypes, since the channel noise breaks the permutation symmetry of the prototype indices. A similar mechanism has also been applied in the self-organizing map [8]. <p> A systematic way to obtain on-line equations for squared Eu-clidean distortions is to approximate the difference y N+1 ff between the centroids for N and N + 1 data vectors <ref> [3] </ref>. The resulting equations are given by y N+1 ff + p N+1 ff ;(11) where p N+1 ff = p N ff + Pfd (N + 1) = ffg is a running average for prototype y ff , p 0 ff = 1.
Reference: [4] <author> Ch. Darken and J. Moody. </author> <title> Note on learning rate schedules for stochastic optimization. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <year> 1991. </year>
Reference-contexts: This dependency on the history of a neuron avoids the problem of defining an appropriate global learning rate. To accelerate the convergence rate it might be advantageous to include an additional learning gain, based on the `Search-Then-Converge' heuristic <ref> [4] </ref>. The modified update rules are obtained by replacing p N+1 ff by 1 + p N+1 the mobility of the neuron weights y ff is increased. 5. RESULTS We have tested the presented vector quantization algorithm on wavelet-transformed video sequences from a teleconferencing application.
Reference: [5] <author> M. Farvardin. </author> <title> A study of vector quantization for noisy channels. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 36 </volume> <pages> 799-809, </pages> <year> 1990. </year>
Reference-contexts: We present a novel approach to the design of optimal noise robust vector quantizers, which not only compactly encode the data, but also compensate channel noise and random code vector eliminations. Robust vector quantization is formulated as an optimization problem, extending the source-channel coding approach presented in <ref> [9, 5] </ref>. An efficient optimization heuristic is derived within the deterministic annealing framework [13, 3, 2]. The on-line version of the proposed algorithm is a neural network technique, which belongs to the class of competitive learning methods. <p> This problem is also known as source-channel coding. We assume the noise characteristics of the channel to be known and denote by S ff- the probability of receiving index after sending ff through the channel. It has been noticed <ref> [9, 5, 11, 3] </ref> that source-channel coding may result in a topological ordering of prototypes, since the channel noise breaks the permutation symmetry of the prototype indices. A similar mechanism has also been applied in the self-organizing map [8].
Reference: [6] <author> A. Gersho and R. M. Gray. </author> <title> Vector Quantization and Signal Processing. </title> <publisher> Kluwer Academic Publisher, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: 1. INTRODUCTION Vector quantization <ref> [6] </ref> deals with the problem of encoding an information source by means of a finite size codebook. If the code optimization is data driven, this is more specifically called adaptive vector quantization. <p> Starting from Eqs. (2,3) different update schemes for reaching a local minimum are possible [10], moreover there exists a large number of heuristics to incrementally split clusters and to deal with unused codewords, c.f. <ref> [6] </ref>. An important extension of the vector quantization problem is to consider a noisy transmission channel in the codebook design phase. This problem is also known as source-channel coding.
Reference: [7] <author> Th. Hofmann and J.M. Buhmann. </author> <title> An annealed neural gas network for robust vector quantization. </title> <booktitle> In ICANN`96, Lecture Notes in Computer Science 1112. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1996. </year>
Reference-contexts: A similar mechanism has also been applied in the self-organizing map [8]. A second fundamental extension of the basic vector quantization model deals with random eliminations of code-book vectors and is called robust vector quantization <ref> [7] </ref>. In this communication model the codebook design has to deal with the problem, that certain prototypes may not be available at encoding time t due to a temporary codebook reduction Y (t) Y.
Reference: [8] <author> T. Kohonen. </author> <title> Self-organizing formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43 </volume> <pages> 59-69, </pages> <year> 1982. </year>
Reference-contexts: The on-line version of the proposed algorithm is a neural network technique, which belongs to the class of competitive learning methods. The rigorous derivation from an optimization principle guarantees an information-theoretic interpretation of the presented algorithm. This also covers the self-organizing map <ref> [8] </ref> and the neural gas [12] as special cases. 2. ROBUST VECTOR QUANTIZATION Assume a sample set of data vectors X = fx i 2 IR d : 1 i N g is given. <p> It has been noticed [9, 5, 11, 3] that source-channel coding may result in a topological ordering of prototypes, since the channel noise breaks the permutation symmetry of the prototype indices. A similar mechanism has also been applied in the self-organizing map <ref> [8] </ref>. A second fundamental extension of the basic vector quantization model deals with random eliminations of code-book vectors and is called robust vector quantization [7]. <p> If neurons are activated, they get tuned to that specific stimuli, a relation which is directly represented in Eq. (11), since the direction of weight changes is always towards the new stimulus x N+1 . Compared to the on-line learning rules proposed by Kohonen et al. <ref> [8] </ref> and Martinetz et al. [12], the learning rate in Eq. (11) is different for every `neuron', since it depends inversely on the number of data points assigned so far. This dependency on the history of a neuron avoids the problem of defining an appropriate global learning rate.
Reference: [9] <author> H. Kumazawa, M. Kasahara, and T. Namekawa. </author> <title> A construction of vector quantizers for noisy channels. </title> <booktitle> Electronics and Engeneering in Japan, </booktitle> <address> 67B(4):39-47, </address> <year> 1984. </year>
Reference-contexts: We present a novel approach to the design of optimal noise robust vector quantizers, which not only compactly encode the data, but also compensate channel noise and random code vector eliminations. Robust vector quantization is formulated as an optimization problem, extending the source-channel coding approach presented in <ref> [9, 5] </ref>. An efficient optimization heuristic is derived within the deterministic annealing framework [13, 3, 2]. The on-line version of the proposed algorithm is a neural network technique, which belongs to the class of competitive learning methods. <p> This problem is also known as source-channel coding. We assume the noise characteristics of the channel to be known and denote by S ff- the probability of receiving index after sending ff through the channel. It has been noticed <ref> [9, 5, 11, 3] </ref> that source-channel coding may result in a topological ordering of prototypes, since the channel noise breaks the permutation symmetry of the prototype indices. A similar mechanism has also been applied in the self-organizing map [8].
Reference: [10] <author> Y. Linde, A. Buzo, and R. M. Gray. </author> <title> An algorithm for vector quantizer design. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 28 </volume> <pages> 84-95, </pages> <year> 1980. </year>
Reference-contexts: Squared Euclidean distortions imply the optimal choice of the codebook vectors as the center of mass of the associated data. Starting from Eqs. (2,3) different update schemes for reaching a local minimum are possible <ref> [10] </ref>, moreover there exists a large number of heuristics to incrementally split clusters and to deal with unused codewords, c.f. [6]. An important extension of the vector quantization problem is to consider a noisy transmission channel in the codebook design phase. This problem is also known as source-channel coding.
Reference: [11] <author> S.P. Luttrell. </author> <title> Hierarchical vector quantization. </title> <booktitle> IEE Proceedings, </booktitle> <volume> 136 </volume> <pages> 405-413, </pages> <year> 1989. </year>
Reference-contexts: This problem is also known as source-channel coding. We assume the noise characteristics of the channel to be known and denote by S ff- the probability of receiving index after sending ff through the channel. It has been noticed <ref> [9, 5, 11, 3] </ref> that source-channel coding may result in a topological ordering of prototypes, since the channel noise breaks the permutation symmetry of the prototype indices. A similar mechanism has also been applied in the self-organizing map [8].
Reference: [12] <author> T. Martinetz, S.G. Berkovich, and K.J. Schulten. </author> <title> Neural-gas network for vector quantization and its application to time-series prediction. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(4) </volume> <pages> 558-569, </pages> <year> 1993. </year>
Reference-contexts: The on-line version of the proposed algorithm is a neural network technique, which belongs to the class of competitive learning methods. The rigorous derivation from an optimization principle guarantees an information-theoretic interpretation of the presented algorithm. This also covers the self-organizing map [8] and the neural gas <ref> [12] </ref> as special cases. 2. ROBUST VECTOR QUANTIZATION Assume a sample set of data vectors X = fx i 2 IR d : 1 i N g is given. <p> In the simpler case without channel noise and with uniform elimination probabilities * ff = *, 8ff, the objec tive function is essentially equivalent to the neural gas model, introduced by Martinetz et al. <ref> [12] </ref>. 3. CODEBOOK DESIGN BY DETERMINISTIC ANNEALING In this section we apply the optimization framework called deterministic annealing (DA) for optimal code-book design. The core of all annealing methods is a computational temperature T , which controls the amplitude of noise, artificially introduced in the optimization process. <p> Compared to the on-line learning rules proposed by Kohonen et al. [8] and Martinetz et al. <ref> [12] </ref>, the learning rate in Eq. (11) is different for every `neuron', since it depends inversely on the number of data points assigned so far. This dependency on the history of a neuron avoids the problem of defining an appropriate global learning rate.
Reference: [13] <author> K. Rose, E. Gurewitz, and G. Fox. </author> <title> Vector quantization by deterministic annealing. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(4) </volume> <pages> 1249-1257, </pages> <year> 1992. </year>
Reference-contexts: Robust vector quantization is formulated as an optimization problem, extending the source-channel coding approach presented in [9, 5]. An efficient optimization heuristic is derived within the deterministic annealing framework <ref> [13, 3, 2] </ref>. The on-line version of the proposed algorithm is a neural network technique, which belongs to the class of competitive learning methods. The rigorous derivation from an optimization principle guarantees an information-theoretic interpretation of the presented algorithm. <p> In the high temperature regime F T is convex, while we recover the original cost function in the limit of T ! 0. The free energy for vector quantization is given by <ref> [13] </ref>, F T (Y) = T i=1 K X exp T If D is differentiable, the equations which result from minimizing the free energy are the generalized centroid conditions with Gibbs probabilities Pfe (i) = ffg = fi T D (x i ; y ff ) P K fi T D
References-found: 13

