URL: http://www.cs.columbia.edu/pdis_lab/papers/learning/ml95/m.ps.Z
Refering-URL: http://www.cs.columbia.edu/pdis_lab/
Root-URL: 
Email: pkc@cs.columbia.edu and sal@cs.columbia.edu  
Title: A Comparative Evaluation of Voting and Meta-learning on Partitioned Data  
Author: Philip K. Chan and Salvatore J. Stolfo 
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Abstract: Much of the research in inductive learning concentrates on problems with relatively small amounts of data. With the coming age of very large network computing, it is likely that orders of magnitude more data in databases will be available for various learning problems of real world importance. Some learning algorithms assume that the entire data set fits into main memory, which is not feasible for massive amounts of data. One approach to handling a large data set is to partition the data set into subsets, run the learning algorithm on each of the subsets, and combine the results. In this paper we evaluate different techniques for learning from partitioned data. Our meta-learning approach is empirically compared with techniques in the literature that aim to com bine multiple evidence to arrive at one prediction.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. & Kibler, D. </author> <year> (1989). </year> <title> Noise-tolerant instance-based learning algorithms. </title> <booktitle> Proc. </booktitle> <pages> IJCAI-89 (pp. 794-799). </pages>
Reference-contexts: For those incremental algorithms that do not require all examples to be resident in memory, like neural nets, many demand multiple passes over the data to achieve convergence, which usually consumes substantial processing time. Incremental IBL <ref> (Aha & Kibler, 1989) </ref> makes only one pass over the data and stores only a subset of the training examples; however, it does not bound the number of examples retained during training. Quinlan (1979) approached the problem of scaling with a windowing technique.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Bel-mont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: Experiments were run to compare the accuracy of the different techniques we presented so far. The next section discusses our findings. 4 Experiments and Results Two inductive learning algorithms were used in our experiments. ID3 (Quinlan, 1986) and CART <ref> (Breiman et al., 1984) </ref> were obtained from NASA Ames Research Center in the IND package (Buntine & Caruana, 1991). They are both decision tree learning algorithms that require all training examples to be resident in main memory. Two data sets were used in our studies.
Reference: <author> Buntine, W. & Caruana, R. </author> <year> (1991). </year> <title> Introduction to IND and Recursive Partitioning. </title> <institution> NASA Ames Research Center. </institution>
Reference-contexts: The next section discusses our findings. 4 Experiments and Results Two inductive learning algorithms were used in our experiments. ID3 (Quinlan, 1986) and CART (Breiman et al., 1984) were obtained from NASA Ames Research Center in the IND package <ref> (Buntine & Caruana, 1991) </ref>. They are both decision tree learning algorithms that require all training examples to be resident in main memory. Two data sets were used in our studies.
Reference: <author> Catlett, J. </author> <year> (1991). </year> <title> Megainduction: A test flight. </title> <booktitle> Proc. Eighth Intl. Work. Machine Learning (pp. </booktitle> <pages> 596-599). </pages>
Reference: <author> Chan, P. & Stolfo, S. </author> <year> (1993a). </year> <title> Experiments on multi-strategy learning by meta-learning. </title> <booktitle> Proc. Second Intl. Conf. Info. Know. Manag. </booktitle> <pages> (pp. 314-323). </pages>
Reference-contexts: Together with an arbitration rule, the learned arbiter resolves conflicts among the classifiers when necessary. Other arbiter schemes were investigated in (Chan & Stolfo, 1995). 3.2 Combiner The aim of the combiner strategy <ref> (Chan & Stolfo, 1993a) </ref> is to coalesce the predictions from the base classifiers by learning the relationship between these predictions and the correct prediction. <p> Return meta-level training instances as in class-combiner with the addition of the attribute vectors; i.e., T = f (class (x); C 1 (x); C 2 (x); ::::C k (x); attribute vector (x)) j x 2 Eg: (This scheme is de noted as class-attribute-combiner.) <ref> (Chan & Stolfo, 1993a) </ref>. Experiments were run to compare the accuracy of the different techniques we presented so far. The next section discusses our findings. 4 Experiments and Results Two inductive learning algorithms were used in our experiments.
Reference: <author> Chan, P. & Stolfo, S. </author> <year> (1993b). </year> <title> Meta-learning for multi-strategy and parallel learning. </title> <booktitle> Proc. Second Intl. Work. on Multistrategy Learning (pp. </booktitle> <pages> 150-165). </pages>
Reference-contexts: Common techniques such as voting and statistical schemes are evaluated. These familiar techniques are compared to our proposed meta-learning techniques, which were first presented in <ref> (Chan & Stolfo, 1993b) </ref>. The contributions of this paper are two fold. We systematically compare schemes reported in the literature to our proposed meta-learning techniques. <p> We now discuss our meta-learning techniques for combin ing classifications produced by multiple classifiers. These techniques differ from the simple voting and statistical methods we just discussed. 3 Meta-learning Techniques Rather than learning weights, our approach introduced in <ref> (Chan & Stolfo, 1993b) </ref> is to meta-learn a set of new classifiers (or meta-classifiers) whose training data are based on predictions of a set of base classifiers. Our techniques fall into two general categories: the arbiter and combiner schemes. We distinguish between base classifiers and arbiters/combiners as follows.
Reference: <author> Chan, P. & Stolfo, S. </author> <year> (1993c). </year> <title> Toward multistrategy parallel and distributed learning in sequence analysis. </title> <booktitle> Proc. First Intl. Conf. Intel. Sys. Mol. Biol. </booktitle> <pages> (pp. 65-73). </pages>
Reference-contexts: Learners that search M-of-N concepts and other counting-related decision rules might be useful in locating effective combining rules. We are also studying the use of multiple learning algorithms in generating base classifiers to improve the overall prediction accuracy <ref> (Chan & Stolfo, 1993c) </ref>. Moreover, the arbiter tree approach can also be applied to combiners to generate combiner trees (Chan & Stolfo, 1995). Acknowledgements We thank the anonymous reviewers for their comments.
Reference: <author> Chan, P. & Stolfo, S. </author> <year> (1993d). </year> <title> Toward parallel and distributed learning by meta-learning. </title> <booktitle> Working Notes AAAI Work. Know. Disc. </booktitle> <pages> Databases (pp. 227-240). </pages>
Reference-contexts: The arbiter/combiner is also a classifier, and hence other arbiters or combiners can be computed from the set of predictions of other arbiters/combiners. 3.1 Arbiter An arbiter <ref> (Chan & Stolfo, 1993d) </ref> is learned by some learning algorithm to arbitrate among predictions generated by different base classifiers. That is, its purpose is to provide an alternate and more educated prediction when the base classifiers present diverse predictions. <p> Our approach is to run the serial code on a number of data subsets in parallel and combine the results with meta-learning thus reducing and limiting the amount of data inspected by any one learning process <ref> (Chan & Stolfo, 1993d) </ref>. This approach has the advantage of using the same serial code without the time-consuming process of parallelizing it. Since the meta-learning framework for combining the results of learned concepts is independent of the learning algorithm, it can be used with different algorithms. <p> Furthermore, our techniques are also data and algorithm-independent, which enable any learning algorithm to train on large data sets (preliminary results from another data set and two other algorithms are reported in <ref> (Chan & Stolfo, 1993d) </ref>.) We are investigating meta-learners that are specialized in combining decisions. Learners that search M-of-N concepts and other counting-related decision rules might be useful in locating effective combining rules.
Reference: <author> Chan, P. & Stolfo, S. </author> <year> (1995). </year> <title> Learning arbiter and combiner trees from partitioned data for scaling machine learning. </title> <note> Proc. Intl. Conf. Knowledge Discovery and Data Mining. To appear. </note>
Reference-contexts: Once the training set is formed, an arbiter is generated by the same learning algorithm used to train the base classifiers. Together with an arbitration rule, the learned arbiter resolves conflicts among the classifiers when necessary. Other arbiter schemes were investigated in <ref> (Chan & Stolfo, 1995) </ref>. 3.2 Combiner The aim of the combiner strategy (Chan & Stolfo, 1993a) is to coalesce the predictions from the base classifiers by learning the relationship between these predictions and the correct prediction. <p> We are also studying the use of multiple learning algorithms in generating base classifiers to improve the overall prediction accuracy (Chan & Stolfo, 1993c). Moreover, the arbiter tree approach can also be applied to combiners to generate combiner trees <ref> (Chan & Stolfo, 1995) </ref>. Acknowledgements We thank the anonymous reviewers for their comments. This work has been partially supported by grants from NSF (IRI-94-13847 and CDA-90-24735), New York State Science and Technology Foundation, and Citicorp.
Reference: <author> Craven, M. & Shavlik, J. </author> <year> (1993). </year> <title> Learning to represent codons: A challenge problem for constructive induction. </title> <booktitle> Proc. </booktitle> <pages> IJCAI-93 (pp. 1319-1324). </pages>
Reference-contexts: There are three possible classes in this task. Each sequence has 60 nucleotides with 8 different values each (four base ones plus four combinations). The data set contains 3,190 training instances. The protein coding region (PCR) data set <ref> (Craven & Shavlik, 1993) </ref>, courtesy of Craven and Shavlik, contains DNA nucleotide sequences and their binary classifications (coding or non-coding). Each sequence has 15 nucleotides with four different values each. The PCR data set has 20,000 sequences.
Reference: <author> Littlestone, N. & Warmuth, M. </author> <year> (1989). </year> <title> The weighted majority algorithm. </title> <type> (Technical Report UCSC-CRL-89-16): </type> <institution> Univ. Cal., Santa Cruz. </institution>
Reference: <author> Quinlan, J. R. </author> <year> (1979). </year> <title> Induction over large data bases. </title> <type> (Technical Report STAN-CS-79-739): </type> <institution> Comp. Sci. Dept., Stanford Univ. </institution>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: Experiments were run to compare the accuracy of the different techniques we presented so far. The next section discusses our findings. 4 Experiments and Results Two inductive learning algorithms were used in our experiments. ID3 <ref> (Quinlan, 1986) </ref> and CART (Breiman et al., 1984) were obtained from NASA Ames Research Center in the IND package (Buntine & Caruana, 1991). They are both decision tree learning algorithms that require all training examples to be resident in main memory. Two data sets were used in our studies.
Reference: <author> Schapire, R. </author> <year> (1990). </year> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 197-226. </pages>
Reference: <author> Towell, G., Shavlik, J., & Noordewier, M. </author> <year> (1990). </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> Proc. </booktitle> <pages> AAAI-90 (pp. 861-866). </pages>
Reference-contexts: They are both decision tree learning algorithms that require all training examples to be resident in main memory. Two data sets were used in our studies. The DNA splice junction (SJ) data set <ref> (Towell et al., 1990) </ref>, courtesy of Towell, Shavlik and Noordewier, contains sequences of nucleotides and the type of splice junction, if any, at the Class Attribute vector Example Base classifiers' predictions class (x) attribute vector (x) x C 1 (x) C 2 (x) C 3 (x) table attrvec 1 x 1
Reference: <author> Utgoff, P. </author> <year> (1989). </year> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 161-186. </pages>
Reference-contexts: However, some incremental algorithms do require the storage of all examples for future examination during learning, for example, ID5 <ref> (Utgoff, 1989) </ref>. That is, these incremental learning algorithms still demand that all examples fit in the main memory, which is not plausible for massive amounts of data.
Reference: <author> Wah, B. </author> <year> (1993). </year> <title> High performance computing and communications for grand challenge applications: Computer vision, speech and natural language processing, </title> <journal> and artificial intelligence. IEEE Trans. Know. Data. Eng., </journal> <volume> 5(1), </volume> <pages> 138-154. </pages>
Reference-contexts: With the coming age of very large network computing, it is likely that orders of magnitude more data in databases will be available for various learning problems of real world importance. The Grand Challenges of HPCC <ref> (Wah, 1993) </ref> are perhaps the best examples. Financial institutions and market analysis firms are already dealing with overwhelming amounts of global information that in time will undoubtedly grow in size faster than improvements in machine resources. Some learning algorithms require all the data to be resident in main memory.
Reference: <author> Wirth, J. & Catlett, J. </author> <year> (1988). </year> <title> Experiments on the costs and benefits of windowing in ID3. </title> <booktitle> Proc. Fifth Intl. Conf. Machine Learning (pp. </booktitle> <pages> 87-99). </pages>
Reference: <author> Wolpert, D. </author> <year> (1992). </year> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> 241-259. </pages>
Reference: <author> Xu, L., Krzyzak, A., & Suen, C. </author> <year> (1992). </year> <title> Methods of combining multiple classifires and their applications to handwriting recognition. </title> <journal> IEEE Trans. Sys. Man. Cyb., </journal> <volume> 22, </volume> <pages> 418-435. </pages>
Reference: <author> Zhang, X., Mckenna, M., Mesirov, J., & Waltz, D. </author> <year> (1989). </year> <title> An Efficient Implementation of the Backpropagation Algorithm on the Connection Machine CM-2. </title> <type> (Technical Report RL89-1): </type> <institution> Thinking Machines Corp. </institution>
Reference: <author> Zhang, X., Mesirov, J., & Waltz, D. </author> <year> (1992). </year> <title> A hybrid system for protein secondary structure prediction. </title> <journal> J. Mol. Biol., </journal> <volume> 225, </volume> <pages> 1049-1063. </pages>
References-found: 22

