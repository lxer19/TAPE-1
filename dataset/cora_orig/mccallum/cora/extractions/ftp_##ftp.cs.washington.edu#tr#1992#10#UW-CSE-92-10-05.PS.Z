URL: ftp://ftp.cs.washington.edu/tr/1992/10/UW-CSE-92-10-05.PS.Z
Refering-URL: http://www.cs.washington.edu/homes/zahorjan/homepage/listof.htm
Root-URL: 
Title: Improving the Performance of Runtime Parallelization  
Author: Shun-Tak Leung and John Zahorjan 
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Technical Report 92-10-05 Revised February 1993 (This paper will appear in the Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming.) 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: This too is done in parallel. 4 Performance Comparison In this section, we compare the speeds of the various inspector algorithms and their performance impacts on the executor. We use successive over-relaxation as a concrete example. 4.1 Successive Over-Relaxation Successive over-relaxation (SOR) <ref> [1, 5] </ref> is a numerical technique that can be used to obtain the solution of a sparse linear system. Given a non-singular n fi n matrix A and an n-dimensional vector b, we want to solve the system of linear equations Ax = b for the unknown n-dimensional vector x.
Reference: [2] <author> Edward D. Lazowska, John Zahorjan, G. Scott Graham, and Kenneth C. Sevcik. </author> <title> Quantitative System Performance. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1984. </year>
Reference-contexts: In our experiments, the source loop was manually transformed into its corresponding inspector and executor because we did not have compiler support for this conversion. 4.2 Results We applied the SOR technique to solve the global balance equations <ref> [2] </ref> corresponding to a queueing network model with blocking [4]. The model itself consists of a number of service centers in series, each with a finite capacity queue. This sort of model is commonly used to evaluate the performance of communication networks built from switches with finite buffer space.
Reference: [3] <author> S. Leung and J. Zahorjan. </author> <title> Extending the domain and improving the execution performance of runtime parallelization. </title> <note> Technical Report , in preparation, </note> <institution> Department of Computer Science & Engineering, University of Washington, </institution> <month> Oc-tober </month> <year> 1992. </year>
Reference-contexts: The executor shown in Figure 3, first proposed by Saltz et al. [6, 8] and adopted for all work presented here, is sufficient to do this for the class of source loops under consideration. (Improvements to both executor efficiency and domain of applicability are developed in a companion paper <ref> [3] </ref>.) do w = 1 to depth pardo all i such that wf [i] = w if (g (i) &lt; i) then a1 = anew [g (i)] else a1 = aold [g (i)] endif ... anew [i] = F (a1, a2, ...) enddo enddo aold = anew As written in Figure <p> Because our focus in this paper is on improving the inspector phase, we have not attempted to address these executor inefficiencies here, but leave them for a companion paper <ref> [3] </ref>. Since we ran identical executor code with each inspector, any differences observed in their efficiencies must stem from differences in the schedules the inspectors produce. We make two observations in this regard. First, the doacross-local technique distinguishes itself with slightly lower efficiency than the others.
Reference: [4] <author> Raif O. Onvural. </author> <title> Survey of closed queueing networks with blocking. </title> <journal> ACM Computing Surveys, </journal> <volume> 22(2) </volume> <pages> 83-121, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In our experiments, the source loop was manually transformed into its corresponding inspector and executor because we did not have compiler support for this conversion. 4.2 Results We applied the SOR technique to solve the global balance equations [2] corresponding to a queueing network model with blocking <ref> [4] </ref>. The model itself consists of a number of service centers in series, each with a finite capacity queue. This sort of model is commonly used to evaluate the performance of communication networks built from switches with finite buffer space.
Reference: [5] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. </author> <title> Numerical Recipes: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: This too is done in parallel. 4 Performance Comparison In this section, we compare the speeds of the various inspector algorithms and their performance impacts on the executor. We use successive over-relaxation as a concrete example. 4.1 Successive Over-Relaxation Successive over-relaxation (SOR) <ref> [1, 5] </ref> is a numerical technique that can be used to obtain the solution of a sparse linear system. Given a non-singular n fi n matrix A and an n-dimensional vector b, we want to solve the system of linear equations Ax = b for the unknown n-dimensional vector x.
Reference: [6] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Multiprocessors and runtime compilation. </title> <booktitle> In Proc. International Workshop on Compilers for Parallel Computers, </booktitle> <address> Paris, </address> <year> 1990. </year>
Reference-contexts: For these programs, runtime parallelization is an attractive alternative. In this section, we introduce the basic mechanisms used to support runtime parallelization, as developed by Saltz and his colleagues <ref> [6, 7, 8] </ref>. We begin by presenting the basic scheme, and then examine a technique they have proposed for increasing its efficiency. Our improvements to these techniques are presented in subsequent sections. 1.1 The Basic Inspector/Executor Scheme of Saltz et al. We call the loop being parallelized the source loop. <p> These additional considerations are not shown for simplicity. do i = 1 to n enddo The basic idea of runtime parallelization, as defined by Saltz et al. <ref> [6, 7, 8] </ref>, is for the compiler to produce two pieces of customized code for each source loop . The first, called the inspector, calculates the values of the index functions used in each iteration of the loop, and from these values determines the inter iteration dependencies. <p> On the other hand, because the inspector disregards both anti- (write-after-read) and output (write after-write) dependencies, these must be enforced by the executor. The executor shown in Figure 3, first proposed by Saltz et al. <ref> [6, 8] </ref> and adopted for all work presented here, is sufficient to do this for the class of source loops under consideration. (Improvements to both executor efficiency and domain of applicability are developed in a companion paper [3].) do w = 1 to depth pardo all i such that wf [i] <p> How many times the executor will be run can also be taken into account, if it is known or can be reasonably estimated: the larger the estimated number of executor invocations, the greater the tendency to choose boostrapping over sectioning. 5 Conclusions Saltz et al. <ref> [6, 8, 7] </ref> have made important contributions to the area of runtime parallelization. Our work builds on theirs. The purpose of the work described here is to improve the efficiency of the inspector phase of runtime parallelization through its parallel execution.
Reference: [7] <author> J. Saltz and R. Mirchandaney. </author> <title> The preprocessed doacross loop. </title> <booktitle> In Proc. 1991 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: For these programs, runtime parallelization is an attractive alternative. In this section, we introduce the basic mechanisms used to support runtime parallelization, as developed by Saltz and his colleagues <ref> [6, 7, 8] </ref>. We begin by presenting the basic scheme, and then examine a technique they have proposed for increasing its efficiency. Our improvements to these techniques are presented in subsequent sections. 1.1 The Basic Inspector/Executor Scheme of Saltz et al. We call the loop being parallelized the source loop. <p> These additional considerations are not shown for simplicity. do i = 1 to n enddo The basic idea of runtime parallelization, as defined by Saltz et al. <ref> [6, 7, 8] </ref>, is for the compiler to produce two pieces of customized code for each source loop . The first, called the inspector, calculates the values of the index functions used in each iteration of the loop, and from these values determines the inter iteration dependencies. <p> Saltz et al. do this using an alternative general runtime loop parallelization procedure, which they call the preprocessed doacross <ref> [7] </ref>. We explain this technique briefly in the following subsection. 1.2 Doacross Parallelization Doacross parallelization [7] is a general runtime parallelization technique that is not in the class of inspector/executor approaches. <p> Saltz et al. do this using an alternative general runtime loop parallelization procedure, which they call the preprocessed doacross <ref> [7] </ref>. We explain this technique briefly in the following subsection. 1.2 Doacross Parallelization Doacross parallelization [7] is a general runtime parallelization technique that is not in the class of inspector/executor approaches. Doacross is based on the use of an auxiliary array that specifies whether or not each data array element has already been written in the current invocation of the source loop. <p> The disadvantages of doacross, though, are that it applies to only a restricted class of loop, and that it has potentially significantly suboptimal parallel efficiency. In fact, Saltz et al. find that while the doacross parallelization is preferable to sequential execution <ref> [7] </ref>, it is not a viable alternative to the inspector/executor approach for loops amenable to the latter technique [8]. Thus, we will not consider doacross parallelization further for general runtime parallelization. <p> How many times the executor will be run can also be taken into account, if it is known or can be reasonably estimated: the larger the estimated number of executor invocations, the greater the tendency to choose boostrapping over sectioning. 5 Conclusions Saltz et al. <ref> [6, 8, 7] </ref> have made important contributions to the area of runtime parallelization. Our work builds on theirs. The purpose of the work described here is to improve the efficiency of the inspector phase of runtime parallelization through its parallel execution.
Reference: [8] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Runtime parallelization and scheduling of loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(5) </volume> <pages> 603-612, </pages> <month> May </month> <year> 1991. </year> <month> 9 </month>
Reference-contexts: For these programs, runtime parallelization is an attractive alternative. In this section, we introduce the basic mechanisms used to support runtime parallelization, as developed by Saltz and his colleagues <ref> [6, 7, 8] </ref>. We begin by presenting the basic scheme, and then examine a technique they have proposed for increasing its efficiency. Our improvements to these techniques are presented in subsequent sections. 1.1 The Basic Inspector/Executor Scheme of Saltz et al. We call the loop being parallelized the source loop. <p> These additional considerations are not shown for simplicity. do i = 1 to n enddo The basic idea of runtime parallelization, as defined by Saltz et al. <ref> [6, 7, 8] </ref>, is for the compiler to produce two pieces of customized code for each source loop . The first, called the inspector, calculates the values of the index functions used in each iteration of the loop, and from these values determines the inter iteration dependencies. <p> On the other hand, because the inspector disregards both anti- (write-after-read) and output (write after-write) dependencies, these must be enforced by the executor. The executor shown in Figure 3, first proposed by Saltz et al. <ref> [6, 8] </ref> and adopted for all work presented here, is sufficient to do this for the class of source loops under consideration. (Improvements to both executor efficiency and domain of applicability are developed in a companion paper [3].) do w = 1 to depth pardo all i such that wf [i] <p> In fact, Saltz et al. find that while the doacross parallelization is preferable to sequential execution [7], it is not a viable alternative to the inspector/executor approach for loops amenable to the latter technique <ref> [8] </ref>. Thus, we will not consider doacross parallelization further for general runtime parallelization. However, there is one case in which doacross parallelization can be applied usefully: parallelizing the computation of wavefronts in the inspector. As shown in Figure 2, the inspector is sequential, and so is very expensive. In [8] this <p> technique <ref> [8] </ref>. Thus, we will not consider doacross parallelization further for general runtime parallelization. However, there is one case in which doacross parallelization can be applied usefully: parallelizing the computation of wavefronts in the inspector. As shown in Figure 2, the inspector is sequential, and so is very expensive. In [8] this expense is reduced by parallelizing the sequential inspector loop using the doacross technique. There are in fact two variants of the parallel inspector used in [8]. Each employs doacross to parallelize the computation of the wavefronts. <p> As shown in Figure 2, the inspector is sequential, and so is very expensive. In <ref> [8] </ref> this expense is reduced by parallelizing the sequential inspector loop using the doacross technique. There are in fact two variants of the parallel inspector used in [8]. Each employs doacross to parallelize the computation of the wavefronts. They differ, however, in how the iterations in each wavefront are assigned to processors for execution by the executor. The first method, doacross-global, appears to be inherently sequential [8], and so is expensive. <p> are in fact two variants of the parallel inspector used in <ref> [8] </ref>. Each employs doacross to parallelize the computation of the wavefronts. They differ, however, in how the iterations in each wavefront are assigned to processors for execution by the executor. The first method, doacross-global, appears to be inherently sequential [8], and so is expensive. However, it produces an iteration assignment that is as balanced as possible. In contrast, the second approach, doacross-local, is easily parallelized, but makes no guarantees about how balanced the resulting schedule will be. <p> In the worst case, doacross-local can produce a schedule that is sequential in a situation where doacross-global would produce a schedule with perfect speedup. 1.3 Paper Goals and Organization While the doacross technique does parallelize the inspector to some degree, performance may still be poor <ref> [8] </ref>. Saltz et al. reports an experiment in which, using sixteen processors, the inspector took 100 ms. while the executor required only 23 ms. In comparison, the source loop took 241 ms. when run sequentially, indicating that the efficiency of the inspector was less than 15%. <p> How many times the executor will be run can also be taken into account, if it is known or can be reasonably estimated: the larger the estimated number of executor invocations, the greater the tendency to choose boostrapping over sectioning. 5 Conclusions Saltz et al. <ref> [6, 8, 7] </ref> have made important contributions to the area of runtime parallelization. Our work builds on theirs. The purpose of the work described here is to improve the efficiency of the inspector phase of runtime parallelization through its parallel execution.
References-found: 8

