URL: http://www-cs-students.stanford.edu/~csilvers/papers/chi2-dmkd.ps
Refering-URL: http://www-cs-students.stanford.edu/~csilvers/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: csilvers@cs.stanford.edu  brin@cs.stanford.edu  motwani@cs.stanford.edu  
Title: Beyond Market Baskets: Generalizing Association Rules to Dependence Rules  
Author: CRAIG SILVERSTEIN SERGEY BRIN RAJEEV MOTWANI Editor: Usama Fayyad 
Keyword: data mining, market basket, association rules, dependence rules, closure properties, text mining  
Address: Stanford, CA 94305  
Affiliation: Department of Computer Science, Stanford University,  
Note: 130 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: One of the more well-studied problems in data mining is the search for association rules in market basket data. Association rules are intended to identify patterns of the type: A customer purchasing item A often also purchases item B. Motivated partly by the goal of generalizing beyond market basket data and partly by the goal of ironing out some problems in the definition of association rules, we develop the notion of dependence rules that identify statistical dependence in both the presence and absence of items in itemsets. We propose measuring significance of dependence via the chi-squared test for independence from classical statistics. This leads to a measure that is upward-closed in the itemset lattice, enabling us to reduce the mining problem to the search for a border between dependent and independent itemsets in the lattice. We develop pruning strategies based on the closure property and thereby devise an efficient algorithm for discovering dependence rules. We demonstrate our algorithm's effectiveness by testing it on census data, text data (wherein we seek term dependence), and synthetic data. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agrawal, R., Arning, A., Bollinger, T., Mehta, M., Shafer, J., and Srikant, R. </author> <year> 1996. </year> <title> The Quest Data Mining System. </title> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery in Databases and Data. </booktitle>
Reference-contexts: On the other hand, the text data is too big: we were forced to prune words with low support even before starting our mining algorithm. To get data that is the appropriate size for exploring the effectiveness of our algorithm, we turn to synthetic data from IBM's Quest group <ref> (Agrawal et al., 1996) </ref>. We generated market basket data with 99997 baskets and 871 items. We set the average basket size to be 20, and the average size of large itemsets to be 4.
Reference: <author> Agrawal, R., Imielinski, T., and Swami, A. </author> <year> 1993. </year> <title> Mining association rules between sets of items in large databases. </title> <booktitle> Proceedings of the ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pp. </pages> <year> 207216. </year>
Reference: <author> Agrawal, R., Imielinski, T., and Swami, A. </author> <year> 1993. </year> <title> Database mining: a performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, 5:914925. </journal>
Reference: <author> Agrawal, R., Mannila, H., Srikant, R., Toivonen, H., and Verkamo, A.I. </author> <year> 1996. </year> <title> Fast Discovery of Association Rules. </title> <editor> Fayyad et al. (Fayyad et al., </editor> <year> 1996), </year> <pages> pp. 307328. </pages>
Reference-contexts: On the other hand, the text data is too big: we were forced to prune words with low support even before starting our mining algorithm. To get data that is the appropriate size for exploring the effectiveness of our algorithm, we turn to synthetic data from IBM's Quest group <ref> (Agrawal et al., 1996) </ref>. We generated market basket data with 99997 baskets and 871 items. We set the average basket size to be 20, and the average size of large itemsets to be 4.
Reference: <author> Agrawal, R. and Srikant, R. </author> <year> 1994. </year> <title> Fast algorithms for mining association rules in large databases. </title> <booktitle> Proceedings of the 20th International Conference on Very Large Data Bases, </booktitle> <pages> pp. 487499. </pages>
Reference: <author> Agresti, A. </author> <year> 1992. </year> <title> A survey of exact inference for contingency tables. </title> <journal> Statistical Science, </journal> <volume> 7 </volume> <pages> 131-177. </pages>
Reference: <author> Dietzfelbinger, M., Karlin, A., Mehlhorn, K., Meyer, F. auf der Heide, Rohnert, H., and Tarjan, R. </author> <year> 1988. </year> <title> Dynamic perfect hashing: Upper and lower bounds. </title> <booktitle> Proceedings of the 18th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 524531. </pages>
Reference: <author> Ewald, R. </author> <year> 1994. </year> <title> Keynote address. </title> <booktitle> The 3rd International Conference on Information and Knowledge Management. </booktitle>
Reference-contexts: Appendix B The data sets used in this paper are accessible via the following URL: http://www.research.microsoft.com/datamine Notes 1. A classic, albeit apocryphal, example is the rule that people who buy diapers in the afternoon are particularly likely to buy beer at the same time <ref> (Ewald, 1994) </ref>. 2. The p value can be easily calculated via formulas, or obtained from widely available tables for the chi-squared distribution. 3. In reality we would mine this data rather than query for it.
Reference: <author> Fayyad, U.M., Piatetsky-Shapiro, G. Smyth, P., and Uthrusamy, R. </author> <year> 1996. </year> <title> Advances in Knowledge Discovery and Data Mining, </title> <publisher> AAAI Press. </publisher>
Reference: <author> Fredman, M., Koml os, J., and Szemer edi, E. </author> <year> 1984. </year> <title> Storing a sparse table with O(1) worst case access time. </title> <journal> Journal of the ACM, 31(3):538544. </journal>
Reference-contexts: Implementation Details and Analysis We now specify some of the key implementation details of our algorithm and obtain bounds on its running time. The most expensive part of the algorithm is Step 8. We propose an implementation based on perfect hash tables <ref> (Fredman et al., 1984, Dietzfelbinger et al., 1988) </ref>. In these hash tables, there are no collisions, and insertion, deletion, and lookup all take constant time. The space used is linear in the size of the data. Both notsig and cand are stored in hash tables.
Reference: <author> Fukuda, T., Morimoto, Y., Morishita, S., and Tokuyama, T. </author> <year> 1996. </year> <title> Mining Optimized Association Rules for Numeric Attributes. </title> <booktitle> Proceedings of the Fifteenth ACM Symposium on Principles of Database Systems. </booktitle>
Reference: <author> Fukuda, T., Morimoto, Y., Morishita, S., and Tokuyama, T. </author> <year> 1996. </year> <title> Mining optimized association rules for numeric data. </title> <booktitle> Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pp. 1324. </pages>
Reference: <author> Gunopulos, D., Mannila, H., and Saluja, S. </author> <year> 1997. </year> <title> Discovering all most specific sentences by randomized algorithms. </title> <booktitle> Proceedings of the 6th International Conference on Database Theory. </booktitle>
Reference-contexts: At the i th level of the lattice, all itemsets are of size i and are called i-itemsets. The level-wise algorithms start with all i-itemsets satisfying a given property, and use this knowledge to explore (i + 1)-itemsets. Another class of algorithms, random walk algorithms <ref> (Gunopulos et al., 1997) </ref>, generate a series of random walks, each of which explores the local structure of the border. A random walk is a walk up the itemset lattice. It starts with the empty itemset and adds items one at a time to form a larger itemset. <p> Since this property is not downward-closed, it would not be effective at pruning in a level-wise algorithm. A random walk algorithm, for instance <ref> (Gunopulos et al., 1997) </ref>, might be appropriate for this kind of pruning. 5.1. The Algorithm Combining the chi-squared dependence rule with pruning via CT-support, we obtain the algorithm in Figure 1. Definition 11.
Reference: <author> Han, J. and Fu, Y. </author> <year> 1995. </year> <title> Discovery of multiple-level association rules from large databases. </title> <booktitle> Proceedings of the 21st International Conference on Very Large Data Bases, </booktitle> <pages> pp. 420431. </pages>
Reference: <author> Houtsma, M. and Swami, A. </author> <year> 1995. </year> <title> Set-oriented mining of association rules. </title> <booktitle> Proceedings of the International Conference on Data Engineering, </booktitle> <pages> pp. 2534. </pages>
Reference: <author> Klemettinen, M., Mannila, H., Ronkainen, P., Toivonen, H., and Verkamo, A.I. </author> <year> 1994. </year> <title> Finding interesting rules from large sets of discovered association rules. </title> <booktitle> Proceedings of the 3rd International Conference on Information and Knowledge Management, </booktitle> <pages> pp. </pages> <address> 401407 Lancaster, H.O. </address> <year> 1969. </year> <title> The Chi-squared Distribution, </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address> <institution> de Laplace, P.S. 1878. Oeuvres compl etes de Laplace publi ees sous les auspices de l'Acad emie des Sciences par M.M. les secr etaires perp etuels, Gauthier-Villar, Paris, </institution> <year> 1878/1912. </year>
Reference: <author> Mannila, H., Toivonen, H., and Inkeri Verkamo, A. </author> <year> 1994. </year> <title> Efficient algorithms for discovering association rules. </title> <booktitle> Proceedings of the AAAI Workshop on Knowledge Discovery in Databases, </booktitle> <pages> pp. 144155. </pages> <editor> de Moivre, A. </editor> <volume> 1733. </volume> <editor> Approximatio ad summam terminorum binomii (a + b) n in seriem expansi, </editor> <title> Supplement to Miscellanea Analytica, </title> <publisher> London. </publisher>
Reference: <author> Moore, D.S. </author> <year> 1986. </year> <title> Tests of chi-squared type. Goodness-of-Fit Techniques, </title> <editor> R.B. D'Agostino and M.A. Stephens (Eds.), </editor> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <pages> pp. 6395. </pages>
Reference-contexts: In this case, the chi-squared test rests on the normal approximation to the binomial distribution (more precisely, to the hypergeometric distribution). This approximation breaks down when the expected values are small. As a rule of thumb, Moore <ref> (Moore, 1986) </ref> recommends the use of chi-squared test only if * all cells in the contingency table have expected value greater than 1; 15 * and, at least 80% of the cells in the contingency table have expected value greater than 5.
Reference: <author> Mosteller, F. and Wallace, D. </author> <year> 1964. </year> <title> Inference and Disputed Authorship: The Federalists, </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Consequently, we develop the notion of mining rules that identify dependencies (generalizing associations) taking into consideration both the presence and the absence of items as a basis for generating rules. To give a concrete example, Mosteller and Wallace <ref> (Mosteller and Wallace, 1964) </ref> studied dependencies in text data to determine the authorship of each essay in the Federalist Papers. This collection of essays was written in the late 1700s by John Jay, Alexander Hamilton, and James Madison, but the essays were all signed Publius.
Reference: <author> Park, J.S., Chen, M.S., and Yu, P.S. </author> <year> 1995. </year> <title> An effective hash based algorithm for mining association rules. </title> <booktitle> Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pp. 175186. </pages>
Reference-contexts: Theorem 5 The running time of Algorithm Dependence Rules for level i is O (njcandj minfn; 2 i g + ijnotsigj 2 ): 19 It is instructive to compare the algorithm in Figure 1 to the hash-based algorithm of Park, Chen, and Yu <ref> (Park et al., 1995) </ref> for the support-confidence framework. Their algorithm also uses hashing to construct a candidate set cand, which they then iterate over to verify the results. One difference is that verification is easier in their case, since they only need to test support. <p> One difference is that verification is easier in their case, since they only need to test support. We also need to test chi-squared values, a more expensive operation that makes careful construction of cand more important. Another difference is we use perfect hashing while Park, Chen, and Yu <ref> (Park et al., 1995) </ref> allow collisions. While collisions reduce the effectiveness of pruning, they do not affect the final result. The advantage of allowing collisions is that the hash table may be smaller. Hashing with collisions is necessary when the database is much larger than main memory.
Reference: <author> Pearson, K. </author> <year> 1900. </year> <title> On a criterion that a given system of deviations from the probable in the case of correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. </title> <journal> Philos. Mag., 5:157175. </journal>
Reference-contexts: Note that, by definition, the 2 random variable is asymptotically distributed as the square of a standard normal variable. Pearson <ref> (Pearson, 1900) </ref> extended the definition to the multinomial case, where X can take on any value in a set U .
Reference: <author> Piatetsky, G. and Frawley, W. </author> <year> 1991. </year> <title> Knowledge Discovery in Databases, </title> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Sloan Research Fellowship, an IBM Faculty Partnership Award, an ARO MURI Grant DAAH049610007, and NSF Young Investigator Award CCR9357849, with matching funds from IBM, Mitsubishi, Schlumberger Foundation, Shell Foundation, and Xerox Corporation. 2 While Piatetsky-Shapiro and Frawley <ref> (Piatetsky and Frawley, 1991) </ref> define an association problem as the general problem of finding recurring patterns in data, much of the recent work on mining of large-scale databases has concerned the important special case of finding association rules.
Reference: <author> Savasere, A., Omiecinski, E., and Navathe, S. </author> <year> 1995. </year> <title> An efficient algorithm for mining association rules in large databases. </title> <booktitle> Proceedings of the International Conference on Very Large Data Bases, </booktitle> <pages> pp. </pages> <note> 432444 Srikant, </note> <author> R. and Agrawal, R. </author> <year> 1995. </year> <title> Mining generalized association rules. </title> <booktitle> Proceedings of the 21st International Conference on Very Large Data Bases, </booktitle> <pages> pp. 407419. </pages>
Reference: <author> Toivonen, H. </author> <year> 1996. </year> <title> Sampling large databases for finding association rules. </title> <booktitle> Proceedings of the 22nd International Conference on Very Large Data Bases, </booktitle> <pages> pp. </pages> <month> 134145. </month> <title> 30 Craig Silverstein obtained an A.B. degree in Computer Science from Harvard University and is currently a Ph.D. candidate in Computer Science at Stanford University. He is a recipient of a National Defense Science and Engineering Graduate fellowship and an Achievement Awards for College Scientists fellowhip. His research interests include information retrieval on natural language queries and discovering causality via data mining. </title>

References-found: 24

