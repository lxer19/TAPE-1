URL: http://www.research.microsoft.com/users/jplatt/smo-nips.ps
Refering-URL: http://www.research.microsoft.com/users/jplatt/smo.html
Root-URL: http://www.research.microsoft.com
Email: jplatt@microsoft.com  
Title: Using Analytic QP and Sparseness to Speed Training of Support Vector Machines  
Author: John C. Platt 
Address: 1 Microsoft Way Redmond, WA 98052  
Affiliation: Microsoft Research  
Abstract: Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) problem. This paper proposes an algorithm for training SVMs: Sequential Minimal Optimization, or SMO. SMO breaks the large QP problem into a series of smallest possible QP problems which are analytically solvable. Thus, SMO does not require a numerical QP library. SMO's computation time is dominated by evaluation of the kernel, hence kernel optimizations substantially quicken SMO. For the MNIST database, SMO is 1.7 times as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be 1500 times faster than the PCG chunking algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. J. C. Burges. </author> <title> A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, </title> <type> 2(2), </type> <year> 1998. </year>
Reference-contexts: 1 INTRODUCTION In the last few years, there has been a surge of interest in Support Vector Machines (SVMs) <ref> [1] </ref>. SVMs have empirically been shown to give good generalization performance on a wide variety of problems. However, the use of SVMs is still limited to a small group of researchers. One possible reason is that training algorithms for SVMs are slow, especially for large problems. <p> output of the SVM, K is a kernel function which measures the similarity of a stored training example ~x i to the input ~x, y i 2 f1; +1g is the desired output of the classifier, b is a threshold, and ff i are weights which blend the different kernels <ref> [1] </ref>. For linear SVMs, the kernel function K is linear, hence equation (1) can be expressed as u = ~w ~x b (2) where ~w = P Training of an SVM consists of finding the ff i . <p> The QP sub-problem consists of every non-zero ff i from the previous sub-problem combined with the M worst examples that violate the KKT conditions (6), for some M <ref> [1] </ref>. At the last step, the entire set of non-zero ff i has been identified, hence the last step solves the entire QP problem. Chunking reduces the dimension of the matrix from the number of training examples to approximately the number of non-zero ff i . <p> Kernel evaluation time can be dramatically reduced in certain common situations, e.g., when a linear SVM is used, or when the input data is sparse (mostly zero). The result of kernel evaluations can also be cached in memory <ref> [1] </ref>. There are two components to SMO: an analytic method for solving for the two ff i , and a heuristic for choosing which multipliers to optimize. <p> The CPU time of all algorithms are measured on an unloaded 266 MHz Pentium II processor running Windows NT 4. The chunking algorithm uses the projected conjugate gradient algorithm as its QP solver, as suggested by Burges <ref> [1] </ref>. All algorithms use sparse dot product code and kernel caching, as appropriate [1, 2]. Both SMO and chunking share folded linear SVM code. The SMO algorithm is tested on three real-world data sets. The results of the experiments are shown in Tables 1 and 2. <p> The chunking algorithm uses the projected conjugate gradient algorithm as its QP solver, as suggested by Burges [1]. All algorithms use sparse dot product code and kernel caching, as appropriate <ref> [1, 2] </ref>. Both SMO and chunking share folded linear SVM code. The SMO algorithm is tested on three real-world data sets. The results of the experiments are shown in Tables 1 and 2. Further tests on artificial data sets can be found in [8, 7].
Reference: [2] <author> T. Joachims. </author> <title> Making large-scale SVM learning practical. </title> <editor> In B. Scholkopf, C. J. C. Burges, and A. J. Smola, editors, </editor> <booktitle> Advances in Kernel Methods Support Vector Learning, </booktitle> <pages> pages 169184. </pages> <publisher> MIT Press, </publisher> <year> 1998. </year>
Reference-contexts: However, Osuna et al. [6] suggest keeping a fixed size matrix for every sub-problem, deleting some examples and adding others which violate the KKT conditions. Using a fixed-size matrix allows SVMs to be trained on very large training sets. Joachims <ref> [2] </ref> suggests adding and subtracting examples according to heuristics for rapid convergence. <p> As the SMO algorithm progresses, ff i that are at the bounds are likely to stay at the bounds, while ff i that are not at the bounds will move as other examples are optimized. As a further optimization, SMO uses the shrinking heuristic proposed in <ref> [2] </ref>. After the pass through the entire training set, shrinking finds examples which fulfill the KKT conditions more than the worst example failed the KKT conditions. <p> The chunking algorithm uses the projected conjugate gradient algorithm as its QP solver, as suggested by Burges [1]. All algorithms use sparse dot product code and kernel caching, as appropriate <ref> [1, 2] </ref>. Both SMO and chunking share folded linear SVM code. The SMO algorithm is tested on three real-world data sets. The results of the experiments are shown in Tables 1 and 2. Further tests on artificial data sets can be found in [8, 7]. <p> However, SVM light can also potentially use linear SVM folding. In these experiments, SMO uses a very simple least-recently-used ker nel cache of Hessian rows, while SVM light uses a more complex kernel cache and modifies its heuristics to utilize the kernel effectively <ref> [2] </ref>. Therefore, SMO does not benefit from the kernel cache at the largest problem sizes, while SVM light speeds up by a factor of 2.5 . Utilizing sparseness to compute kernels yields a large advantage for SMO due to the lack of heavy numerical QP overhead.
Reference: [3] <author> L. Kaufman. </author> <title> Solving the quadratic programming problem arising in support vector classification. </title> <editor> In B. Sch olkopf, C. J. C. Burges, and A. J. Smola, editors, </editor> <booktitle> Advances in Kernel Methods Support Vector Learning, </booktitle> <pages> pages 147168. </pages> <publisher> MIT Press, </publisher> <year> 1998. </year>
Reference-contexts: Chunking reduces the dimension of the matrix from the number of training examples to approximately the number of non-zero ff i . If standard QP techniques are used, chunking cannot handle large-scale training problems, because even this reduced matrix cannot fit into memory. Kaufman <ref> [3] </ref> has described a QP algorithm that does not require the storage of the entire Hessian. The decomposition technique [6] is similar to chunking: decomposition breaks the large QP problem into smaller QP sub-problems. <p> Decomposition and SMO have the advantage, over standard PCG chunking, of ignoring the examples whose Lagrange multipliers are at C. This ad vantage is reflected in the scaling exponents for PCG chunking versus SMO and SVM light . PCG chunking can be altered to have a similar property <ref> [3] </ref>. Notice that PCG chunking uses the same sparse dot product code and linear SVM folding code as SMO. However, these optimizations do not speed up PCG chunking due to the overhead of numerically solving large QP sub-problems.
Reference: [4] <author> Y. LeCun. </author> <title> MNIST handwritten digit database. </title> <note> Available on the web at http:// www.research.att.com/ yann/ocr/mnist/. </note>
Reference-contexts: Each web page is represented as 300 sparse binary keywords attributes. The third test set is the MNIST database of handwritten digits, from AT&T Research Labs <ref> [4] </ref>. One classifier of MNIST, class 8, is trained. The inputs are 784-dimensional non-binary vectors and are stored as sparse vectors. A fifth-order polynomial kernel is used to match the AT&T accuracy results.
Reference: [5] <author> C. J. Merz and P. M. Murphy. </author> <title> UCI repository of machine learning databases, </title> <booktitle> 1998. </booktitle> <address> [http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, CA: </address> <institution> University of Cali-fornia, Department of Information and Computer Science. </institution>
Reference-contexts: The SMO algorithm is tested on three real-world data sets. The results of the experiments are shown in Tables 1 and 2. Further tests on artificial data sets can be found in [8, 7]. The first test set is the UCI Adult data set <ref> [5] </ref>. The SVM is given 14 attributes of a census form of a household and asked to predict whether that household has an income greater than $50,000. Out of the 14 attributes, eight are categorical and six are continuous.
Reference: [6] <author> E. Osuna, R. Freund, and F. Girosi. </author> <title> Improved training algorithm for support vector machines. </title> <booktitle> In Proc. IEEE Neural Networks in Signal Processing '97, </booktitle> <year> 1997. </year>
Reference-contexts: If standard QP techniques are used, chunking cannot handle large-scale training problems, because even this reduced matrix cannot fit into memory. Kaufman [3] has described a QP algorithm that does not require the storage of the entire Hessian. The decomposition technique <ref> [6] </ref> is similar to chunking: decomposition breaks the large QP problem into smaller QP sub-problems. However, Osuna et al. [6] suggest keeping a fixed size matrix for every sub-problem, deleting some examples and adding others which violate the KKT conditions. <p> Kaufman [3] has described a QP algorithm that does not require the storage of the entire Hessian. The decomposition technique <ref> [6] </ref> is similar to chunking: decomposition breaks the large QP problem into smaller QP sub-problems. However, Osuna et al. [6] suggest keeping a fixed size matrix for every sub-problem, deleting some examples and adding others which violate the KKT conditions. Using a fixed-size matrix allows SVMs to be trained on very large training sets. Joachims [2] suggests adding and subtracting examples according to heuristics for rapid convergence.
Reference: [7] <author> J. C. Platt. </author> <title> Fast training of SVMs using sequential minimal optimization. </title> <editor> In B. Scholkopf, C. J. C. Burges, and A. J. Smola, editors, </editor> <booktitle> Advances in Kernel Methods Support Vector Learning, </booktitle> <pages> pages 185208. </pages> <publisher> MIT Press, </publisher> <year> 1998. </year>
Reference-contexts: SMO decomposes the overall QP problem into fixed-size QP sub-problems, similar to the decomposition method <ref> [7] </ref>. Unlike previous methods, however, SMO chooses to solve the smallest possible optimization problem at each step. For the standard SVM, the smallest possible optimization problem involves two elements of ~ff because the ~ff must obey one linear equality constraint. <p> The result of kernel evaluations can also be cached in memory [1]. There are two components to SMO: an analytic method for solving for the two ff i , and a heuristic for choosing which multipliers to optimize. Pseudo-code for the SMO algorithm can be found in <ref> [8, 7] </ref>, along with the relationship to other optimization and machine learning algorithms. 2.1 SOLVING FOR TWO LAGRANGE MULTIPLIERS To solve for the two Lagrange multipliers ff 1 and ff 2 , SMO first computes the constraints on these multipliers and then solves for the constrained minimum. <p> Both SMO and chunking share folded linear SVM code. The SMO algorithm is tested on three real-world data sets. The results of the experiments are shown in Tables 1 and 2. Further tests on artificial data sets can be found in <ref> [8, 7] </ref>. The first test set is the UCI Adult data set [5]. The SVM is given 14 attributes of a census form of a household and asked to predict whether that household has an income greater than $50,000.
Reference: [8] <author> J. C. Platt. </author> <title> Sequential minimal optimization: A fast algorithm for training support vec-tor machines. </title> <type> Technical Report MSRTR9814, </type> <institution> Microsoft Research, </institution> <year> 1998. </year> <note> Available at http://www.research.microsoft.com/ jplatt/smo.html. </note>
Reference-contexts: The result of kernel evaluations can also be cached in memory [1]. There are two components to SMO: an analytic method for solving for the two ff i , and a heuristic for choosing which multipliers to optimize. Pseudo-code for the SMO algorithm can be found in <ref> [8, 7] </ref>, along with the relationship to other optimization and machine learning algorithms. 2.1 SOLVING FOR TWO LAGRANGE MULTIPLIERS To solve for the two Lagrange multipliers ff 1 and ff 2 , SMO first computes the constraints on these multipliers and then solves for the constrained minimum. <p> Both SMO and chunking share folded linear SVM code. The SMO algorithm is tested on three real-world data sets. The results of the experiments are shown in Tables 1 and 2. Further tests on artificial data sets can be found in <ref> [8, 7] </ref>. The first test set is the UCI Adult data set [5]. The SVM is given 14 attributes of a census form of a household and asked to predict whether that household has an income greater than $50,000.
Reference: [9] <author> V. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: The quadratic form in (3) involves a Hessian matrix of dimension equal to the number of training examples. This matrix cannot be fit into 128 Megabytes if there are more than 4000 training examples. Vapnik <ref> [9] </ref> describes a method to solve the SVM QP, which has since been known as chunking. Chunking relies on the fact that removing training examples with ff i = 0 does not change the solution.
References-found: 9

