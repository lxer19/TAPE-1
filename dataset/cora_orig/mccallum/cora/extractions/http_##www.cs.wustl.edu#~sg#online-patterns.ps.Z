URL: http://www.cs.wustl.edu/~sg/online-patterns.ps.Z
Refering-URL: http://www.cs.wustl.edu/~sg/
Root-URL: 
Email: sg@cs.wustl.edu  kwek@cs.wustl.edu  sds@cs.wustl.edu  
Title: Agnostic Learning of Geometric Patterns (Extended Abstract)  
Author: Sally A. Goldman Stephen S. Kwek Stephen D. Scott 
Address: St. Louis, MO 63130  St. Louis, MO 63130  St. Louis, MO 63130  
Affiliation: Dept. of Computer Science Washington University  Dept. of Computer Science Washington University  Dept. of Computer Science Washington University  
Date: 1997 1  
Note: In Proceedings of the Tenth Annual ACM Conference on Computational Learning Theory, to appear,  Supported in part by NSF NYI Grant CCR-9357707 with matching funds provided by Xerox PARC and WUTA.  
Abstract: Goldberg, Goldman, and Scott demonstrated how the problem of recognizing a landmark from a one-dimensional visual image can be mapped to that of learning a one-dimensional geometric pattern and gave a PAC algorithm to learn that class. We present an on-line agnostic learning algorithm for learning the class of one-dimensional geometric patterns. Since, when moving from the processed visual image to a one-dimensional pattern some key information is lost, we define a class of two-dimensional geometric patterns for which the important features from the visual image are incorporated in the geometric pattern, and show how to extend our agnostic learning algorithm to this class of two-dimensional patterns. Next, we extend our construction to obtain an efficient agnostic learning algorithm (that tolerate classification and attribute noise) for the class of geometric patterns of arbitrary (constant) dimension d. This mapping and learning algorithm is applicable to any data representable as a constant-dimensional array of values, e.g. sonar data, temporal difference information, or amplitudes of a waveform. To our knowledge, these classes of patterns are more complex than any class of geometric patterns previously studied. Also our results are easily adapted to learn the union of fixed-dimensional boxes from multiple-instance examples. Finally, our algorithms can be made to tolerate concept shift. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: Furthermore, our algorithm is easily adapted to use the rule that an example is positive if at least one of its points is inside some target box (or other variations). 2 AGNOSTIC LEARNING MODEL AND WINNOW In this paper we consider the on-line (or mistake-bound) learning model <ref> [1, 15] </ref> as applied to concept learning (i.e. the classification of each example is 1 or 0).
Reference: [2] <author> J. Aslam and S. Decatur. </author> <title> Specification and simulation of statistical query algorithms for efficiency and noise tolerance. </title> <booktitle> In Proc. 8th Annu. Conf. on Comput. Learning Theory, </booktitle> <pages> pages 437-446. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1995. </year>
Reference-contexts: By contrast, for the union of intervals the consistency problem is trivial. Goldman and Scott [7, 8] gave an efficient algorithm that uses the statistical query model <ref> [2, 12] </ref> to PAC-learn the class of continuous one-dimensional geometric 4 patterns under high noise rates (any rate &lt; 1=2 of classification noise).
Reference: [3] <author> P. Auer and M. K. Warmuth. </author> <title> Tracking the best disjunction. </title> <booktitle> In Proc. of the 36th Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 312-321. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1995. </year>
Reference-contexts: Then Winnow makes at most O (k (M opt + log N )) mistakes. Recently Auer and Warmuth <ref> [3] </ref> suggested a version of Winnow that tolerates concept shift (i.e. the target disjunction may change completely in time). When a weight is sufficiently small, they do not demote it any further. The mistake bound they obtain is linear in the number of shifts. <p> This will decrease the time complexity by a factor of O (m). However, the space complexity becomes O (m 2d ) instead of O (m) and the algorithm becomes more complicated. Moreover, to adapt the version of Winnow that tolerates concept shift <ref> [3] </ref>, we still have to examine all the counterexamples to update the weights in each trial. 5 EFFICIENT AGNOSTIC LEARNING OF TWO-DIMENSIONAL PATTERNS To motivate our definition of a two-dimensional geometric pattern, we now review some findings from the experimental work done by Goldman and Scott [7, 8].
Reference: [4] <author> T. Dietterich, R. Lathrop, and T. Lozano-Perez. </author> <title> Solving the multiple-instance problem with axis-parallel rectangles. </title> <journal> Artificial Intelligence, </journal> <note> 1996. To appear. </note>
Reference-contexts: Finally, our algorithms can be made to tolerate concept shift. There is also a relationship between this work and the task of learning from multiple-instance examples <ref> [4] </ref>. In the multiple-instance learning model, the target concept is a boolean function and each example is a collection of instances and the example (collection) is classified as positive iff at least one of its elements is mapped to positive by the target concept. <p> While our class is more restrictive, our algorithm is online, noise tolerant, agnostic, and can be modified to handle concept shift. 7 LEARNING FROM MULTIPLE-INSTANCE EXAMPLES Recently, motivated by drug discovery, Dietterich et al. <ref> [4] </ref> introduced the notion of learning from multiple-instance examples where the target concept is simply a boolean function, each example is a collection of instances, and the example (collection) is classified as positive iff at least one of its elements is mapped to 1 by the target concept.
Reference: [5] <author> P. Goldberg, S. A. Goldman, and S. D. Scott. </author> <title> PAC learning of one-dimensional patterns. </title> <journal> Machine Learning, </journal> <volume> 25(1) </volume> <pages> 51-70, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: The matching algorithm must determine if the robot is near L (i.e. in a small circle centered around L). Because the visual image may change significantly as small movements around L are made, the pattern matching approach encounters difficulties. Goldberg, Goldman and Scott <ref> [5, 7] </ref> proposed using a learning algorithm to construct an accurate hypothesis for performing landmark matching. They obtained their training data by converting the visual data into one-dimensional geometric patterns. <p> Each example (instance) is a configuration of up to n points from f1; : : : ; sg, where it is labeled according to whether or not it visually resembles the target pattern based on the Hausdorff metric (for example, see Gruber [9]). Goldberg, Goldman and Scott <ref> [5] </ref> gave an Occam-based PAC algorithm for learning the class of one-dimensional geometric patterns from the continuous domain. Following that work, Goldman and Scott [7] gave a statistical query algorithm (and hence a noise-tolerant PAC algorithm) for the same class.
Reference: [6] <author> P. Goldberg. </author> <title> PAC Learning Geometrical Figures. </title> <type> PhD thesis, </type> <institution> University of Edinburgh, </institution> <year> 1992. </year>
Reference-contexts: Also, Goldberg <ref> [6] </ref> proved that given a set S of examples labeled according to a concept in C k;n , it is NP-complete to find some one-dimensional geometric pattern (of any number of points) that correctly classifies all examples in S. <p> The continuous version using the L 2 norm was studied by Goldberg <ref> [6] </ref>, yielding a PAC algorithm.
Reference: [7] <author> S. A. Goldman and S. D. Scott. </author> <title> A theoretical and empirical study of a noise-tolerant algorithm to learn geometric patterns. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 191-199. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: The matching algorithm must determine if the robot is near L (i.e. in a small circle centered around L). Because the visual image may change significantly as small movements around L are made, the pattern matching approach encounters difficulties. Goldberg, Goldman and Scott <ref> [5, 7] </ref> proposed using a learning algorithm to construct an accurate hypothesis for performing landmark matching. They obtained their training data by converting the visual data into one-dimensional geometric patterns. <p> Goldberg, Goldman and Scott [5] gave an Occam-based PAC algorithm for learning the class of one-dimensional geometric patterns from the continuous domain. Following that work, Goldman and Scott <ref> [7] </ref> gave a statistical query algorithm (and hence a noise-tolerant PAC algorithm) for the same class. <p> By contrast, for the union of intervals the consistency problem is trivial. Goldman and Scott <ref> [7, 8] </ref> gave an efficient algorithm that uses the statistical query model [2, 12] to PAC-learn the class of continuous one-dimensional geometric 4 patterns under high noise rates (any rate &lt; 1=2 of classification noise). <p> that tolerates concept shift [3], we still have to examine all the counterexamples to update the weights in each trial. 5 EFFICIENT AGNOSTIC LEARNING OF TWO-DIMENSIONAL PATTERNS To motivate our definition of a two-dimensional geometric pattern, we now review some findings from the experimental work done by Goldman and Scott <ref> [7, 8] </ref>. A potential problem with their approach is that information is lost when mapping from signatures to patterns. We now briefly describe their mapping.
Reference: [8] <author> S. A. Goldman and S. D. Scott. </author> <title> A theoretical and empirical study of a noise-tolerant algorithm to learn geometric patterns. </title> <type> Technical report WUCS-97-20, </type> <institution> Washington University in St. Louis, </institution> <year> 1997. </year>
Reference-contexts: We then apply Winnow [15] to obtain our agnostic learning algorithm, and the virtual weight technique of Maass and Warmuth [18] to make our algorithm efficient. In some experimental work performed by Goldman and Scott <ref> [8] </ref> it was found that in moving from the processed one-dimensional visual image to a one-dimensional pattern, some key information was lost. We define a class of two-dimensional geometric patterns for which the important features from the visual image are incorporated in the two-dimensional pattern. <p> By contrast, for the union of intervals the consistency problem is trivial. Goldman and Scott <ref> [7, 8] </ref> gave an efficient algorithm that uses the statistical query model [2, 12] to PAC-learn the class of continuous one-dimensional geometric 4 patterns under high noise rates (any rate &lt; 1=2 of classification noise). <p> that tolerates concept shift [3], we still have to examine all the counterexamples to update the weights in each trial. 5 EFFICIENT AGNOSTIC LEARNING OF TWO-DIMENSIONAL PATTERNS To motivate our definition of a two-dimensional geometric pattern, we now review some findings from the experimental work done by Goldman and Scott <ref> [7, 8] </ref>. A potential problem with their approach is that information is lost when mapping from signatures to patterns. We now briefly describe their mapping.
Reference: [9] <author> P. Gruber. </author> <title> Approximation of Convex Bodies. </title> <publisher> Brikhauser Verlag, </publisher> <year> 1983. </year>
Reference-contexts: Each example (instance) is a configuration of up to n points from f1; : : : ; sg, where it is labeled according to whether or not it visually resembles the target pattern based on the Hausdorff metric (for example, see Gruber <ref> [9] </ref>). Goldberg, Goldman and Scott [5] gave an Occam-based PAC algorithm for learning the class of one-dimensional geometric patterns from the continuous domain. Following that work, Goldman and Scott [7] gave a statistical query algorithm (and hence a noise-tolerant PAC algorithm) for the same class.
Reference: [10] <author> D. Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Inform. Comput., </journal> <volume> 100(1) </volume> <pages> 78-150, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: The performance of the on-line learner is measured by the total loss (which is equivalent to the number of prediction mistakes made when using the discrete loss function) over all trials. Our on-line learning algorithms are agnostic <ref> [10, 13] </ref> in the sense that they make no assumptions whatsoever about the target concept to be learned. Instead, we compare their performance with the performance of the best hypothesis selected from a comparison or "touchstone" class.
Reference: [11] <author> J. Hong, X. Tan, B. Pinette, R. Weiss, and E. Rise-man. </author> <title> Image-based homing. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 22(1) </volume> <pages> 38-45, </pages> <year> 1992. </year>
Reference-contexts: and Scott of using learning versus pattern matching for the landmark matching problem can be applied to a wide range of data, the rest of their work was specific to the data from an imaging system that generates a one-dimensional array of light intensities (called a signature) taken at eye-level <ref> [11, 14, 19, 20] </ref>. The motivation for using one-dimensional data is to reduce the processing time. For some settings, such as an office environment, it seems feasible that the signature taken at eye-level is sufficient.
Reference: [12] <author> M. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proc. 25th Annu. ACM Sym-pos. Theory Comput., </booktitle> <pages> pages 392-401. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: By contrast, for the union of intervals the consistency problem is trivial. Goldman and Scott [7, 8] gave an efficient algorithm that uses the statistical query model <ref> [2, 12] </ref> to PAC-learn the class of continuous one-dimensional geometric 4 patterns under high noise rates (any rate &lt; 1=2 of classification noise).
Reference: [13] <author> M. Kearns, R. Schapire, and L. Sellie. </author> <title> Toward efficient agnostic learning. </title> <journal> Machine Learning, </journal> 17(2/3):115-142, 1994. 
Reference-contexts: The performance of the on-line learner is measured by the total loss (which is equivalent to the number of prediction mistakes made when using the discrete loss function) over all trials. Our on-line learning algorithms are agnostic <ref> [10, 13] </ref> in the sense that they make no assumptions whatsoever about the target concept to be learned. Instead, we compare their performance with the performance of the best hypothesis selected from a comparison or "touchstone" class.
Reference: [14] <author> T. Levitt and D.Lawton. </author> <title> Qualitative navigation for mobile robots. </title> <journal> Artificial Intelligence, </journal> <volume> 44(3) </volume> <pages> 305-360, </pages> <year> 1990. </year>
Reference-contexts: and Scott of using learning versus pattern matching for the landmark matching problem can be applied to a wide range of data, the rest of their work was specific to the data from an imaging system that generates a one-dimensional array of light intensities (called a signature) taken at eye-level <ref> [11, 14, 19, 20] </ref>. The motivation for using one-dimensional data is to reduce the processing time. For some settings, such as an office environment, it seems feasible that the signature taken at eye-level is sufficient.
Reference: [15] <author> N. Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: We then apply Winnow <ref> [15] </ref> to obtain our agnostic learning algorithm, and the virtual weight technique of Maass and Warmuth [18] to make our algorithm efficient. <p> Furthermore, our algorithm is easily adapted to use the rule that an example is positive if at least one of its points is inside some target box (or other variations). 2 AGNOSTIC LEARNING MODEL AND WINNOW In this paper we consider the on-line (or mistake-bound) learning model <ref> [1, 15] </ref> as applied to concept learning (i.e. the classification of each example is 1 or 0). <p> An important result in this model is Littlestone's on-line noise-tolerant algorithm Winnow for learning k-disjunctions of boolean attributes when there is a large number N of irrelevant attributes <ref> [15] </ref>. Winnow makes 3 predictions based on a linear threshold function N X w i x i where w i is the weight associated with the boolean attribute x i . If the prediction is wrong then the weights are updated as follows.
Reference: [16] <author> N. Littlestone. </author> <title> Redundant noisy attributes, attribute errors, and linear threshold learning using Winnow. </title> <booktitle> In Proc. 4th Annu. Workshop on Com-put. Learning Theory, </booktitle> <pages> pages 147-156, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: On a false positive prediction, for each literal x i that is 1, Winnow demotes the weight w i by dividing it by ff. Littlestone <ref> [16] </ref> showed that Winnow makes at most O (A + k log N ) mistakes on any sequence of trials where the target k-disjunction makes at most A attribute errors. <p> In the agnostic model, whenever the best hypothesis makes a prediction mistake, we only need to change at most k attributes of the example so that the classification is consistent. Thus we have the following interpretation of the mistake bound in the presence of attribute noise. Theorem 1 <ref> [16] </ref> Suppose in a sequence of trials for (on-line) learning an unknown boolean concept defined by k of N possible attributes, the best disjunction makes M opt mistakes (classification errors). Then Winnow makes at most O (k (M opt + log N )) mistakes.
Reference: [17] <author> P. Long and L. Tan. </author> <title> PAC learning axis-aligned rectangles with respect to product distributions from multiple-instance examples. </title> <booktitle> In Proceedings of the Ninth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 228-234. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1996. </year>
Reference-contexts: In the multiple-instance learning model, the target concept is a boolean function and each example is a collection of instances and the example (collection) is classified as positive iff at least one of its elements is mapped to positive by the target concept. Long and Tan <ref> [17] </ref> described an efficient PAC algorithm for learning a single axis-parallel box in Q d from multiple-instance examples under a product distribution where Q denotes the set of rationals and d need not be constant. <p> Their model is primarily motivated by the problem of predicting whether a molecule would bind at a particular site. They argued empirically that axis-parallel rectangles are good hypotheses for this and other similar learning problems. Subsequently, Long and Tan <ref> [17] </ref> described an efficient PAC algorithm for learning a single axis-parallel box in Q d from multiple-instance examples under a product distribution where Q denotes the set of rationals and d need not be constant.
Reference: [18] <author> W. Maass and M. Warmuth. </author> <title> Efficient learning with virtual threshold gates. </title> <booktitle> In Proc. 12th International Conference on Machine Learning, </booktitle> <pages> pages 378-386. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: We then apply Winnow [15] to obtain our agnostic learning algorithm, and the virtual weight technique of Maass and Warmuth <ref> [18] </ref> to make our algorithm efficient. In some experimental work performed by Goldman and Scott [8] it was found that in moving from the processed one-dimensional visual image to a one-dimensional pattern, some key information was lost. <p> We now use the virtual weight technique of Maass and Warmuth <ref> [18] </ref> to implicitly maintain the weights. The basic idea is to simulate Winnow by grouping concepts that "behave alike" into blocks. <p> Note that m depends polynomially on the mistake bound of our algorithm and n, the maximum number of points per example. We group the weights associated with the attributes from A by using a simple adaptation of Maass and War-muth's algorithm <ref> [18] </ref> for learning unions of boxes in fixed dimension. (The grouping of the weights for A comp is similar.) The difference is that our examples are configurations of n points instead of single points. Suppose we want to predict the classification of an example X.
Reference: [19] <author> B. Pinette. </author> <title> Image-Based Navigation Through Large-Scaled Environments. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Amherst, </institution> <year> 1993. </year>
Reference-contexts: and Scott of using learning versus pattern matching for the landmark matching problem can be applied to a wide range of data, the rest of their work was specific to the data from an imaging system that generates a one-dimensional array of light intensities (called a signature) taken at eye-level <ref> [11, 14, 19, 20] </ref>. The motivation for using one-dimensional data is to reduce the processing time. For some settings, such as an office environment, it seems feasible that the signature taken at eye-level is sufficient. <p> A potential problem with their approach is that information is lost when mapping from signatures to patterns. We now briefly describe their mapping. The signatures (which can be viewed as the raw data), consisted of s + 1 distinct light intensity values. (In the data from Pinette <ref> [19] </ref> that they used, s = 359.) Each signature is pre-processed by computing its first derivative and then normalizing it by dividing each of the s derivative values by the difference between the signature's maximum and minimum values.
Reference: [20] <author> H. Suzuki and S. Arimoto. </author> <title> Visual control of autonomous mobile robot based on self-organizing model for pattern learning. </title> <journal> Journal of Robotic Systems, </journal> <volume> 5(5) </volume> <pages> 453-470, </pages> <year> 1988. </year>
Reference-contexts: and Scott of using learning versus pattern matching for the landmark matching problem can be applied to a wide range of data, the rest of their work was specific to the data from an imaging system that generates a one-dimensional array of light intensities (called a signature) taken at eye-level <ref> [11, 14, 19, 20] </ref>. The motivation for using one-dimensional data is to reduce the processing time. For some settings, such as an office environment, it seems feasible that the signature taken at eye-level is sufficient.
References-found: 20

