URL: http://wwwipd.ira.uka.de/~prechelt/Biblio/neurnetw97.ps.gz
Refering-URL: 
Root-URL: 
Title: Investigation of the CasCor Family of Learning Algorithms  
Author: Lutz Prechelt 
Keyword: Section Computational Analysis. Keywords constructive learning, additive learning, cross validation, cascade correlation, empirical study.  
Note: To appear in "Neural Networks"  
Address: D-76128 Karlsruhe, Germany  
Affiliation: Fakultat fur Informatik Universitat Karlsruhe  
Email: (prechelt@ira.uka.de)  
Phone: +49/721/608-4068, Fax: +49/721/694092  
Abstract: Six learning algorithms are investigated and compared empirically. All of them are based on variants of the candidate training idea of the Cascade Correlation method. The comparison was performed using 42 different datasets from the Proben1 benchmark collection. The results indicate: (1) for these problems it is slightly better not to cascade the hidden units, (2) error minimization candidate training is better that covariance maximization for regression problems but may be a little worse for classification problems, (3) for most learning tasks, considering validation set errors during the selection of the best candidate will not lead to improved networks, but for a few tasks it will. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ash, T. </author> <year> (1989). </year> <title> Dynamic node creation in backpropagation networks. </title> <journal> Connection Science, </journal> <volume> 1(4) </volume> <pages> 365-375. </pages>
Reference-contexts: Among the earliest are Gallant's tower and inverted pyramid proposals based on simple perceptrons (Gallant, 1986) followed by several refinements, extensions, and re-inventions, e.g. (Baffes and Zelle, 1992, Frean, 1990, Mezar and Nadal, 1989, Simon, 1993). Several methods, e.g. <ref> (Ash, 1989, Wang et al., 1994) </ref>, propose addition of units in the hidden layers of standard MLPs during normal backpropagation training, but this approach severely disturbs the training process because of the interaction of hidden units.
Reference: <author> Baffes, P. T. and Zelle, J. M. </author> <year> (1992). </year> <title> Growing layers of perceptrons: Introducing the Extentron algorithm. </title> <booktitle> In Proc. Int. Joint Conf. on Neural Networks 1992, </booktitle> <volume> vol. 2, </volume> <pages> Baltimore, pp. 392-397. </pages>
Reference: <author> Cun, Y. L., Denker, J. S., and Solla, S. A. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D. S. (ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman Publishers, </publisher> <pages> pp. 598-605. </pages>
Reference: <author> Fahlman, S. E. and Lebiere, C. </author> <year> (1990). </year> <title> The Cascade-Correlation learning architecture. </title> <type> Technical Report CMU-CS-90-100, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: Although additive methods seemed to be quite interesting in terms of computation time and ease of use, only few of them have actually been used in real applications. The notable exception is the Cascade Correlation (CasCor) algorithm proposed by <ref> (Fahlman and Lebiere, 1990) </ref>, which has been used in many applications and is implemented in most larger NN simulator programs.
Reference: <author> Finnoff, W., Hergert, F., and Zimmermann, H. G. </author> <year> (1993). </year> <title> Improving model selection by non-convergent methods. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 771-783. </pages>
Reference-contexts: Regularization methods use a network with a large number of parameters, but limit the size of each parameter dimension by imposing additional constraints on each weight besides error minimization; examples are weight decay (Krogh and Hertz, 1991), soft weight sharing (Nowlan and Hinton, 1992), and others; see <ref> (Finnoff et al., 1993, Reed, 1993) </ref> for an overview. Although additive methods seemed to be quite interesting in terms of computation time and ease of use, only few of them have actually been used in real applications.
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> The Upstart algorithm: A method for constructing and training feed-forward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 198-209. </pages>
Reference: <author> Fritzke, B. </author> <year> (1994). </year> <title> Growing cell structures | a self-organizing network for unsupervised and supervised learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(9) </volume> <pages> 1441-1460. </pages>
Reference-contexts: A number of completely different approaches to additive learning are based on radial basis functions (RBFs) that cover the input space with regions instead of segmenting it with borders as MLPs do. One of the most sophisticated RBF methods is the supervised version of Growing Cell Structures <ref> (Fritzke, 1994) </ref> . It uses a front-end network based on an approximated k-dimensional Voronoi tessellation of the input space using k-dimensional simplices of units and unsupervised local learning and a back-end to re-interpret the front-end network units as RBFs and to compute outputs from them.
Reference: <author> Gallant, S. I. </author> <year> (1986). </year> <title> Three constructive algorithms for network learning. </title> <booktitle> In Proc. of the 8th Annual Conf. of the Cognitive Science Society, </booktitle> <pages> pp. 652-660. </pages>
Reference-contexts: Such data is presented in the study at hand. 2.3 Related work There are many other suggestions for additive learning besides the CasCor family. Among the earliest are Gallant's tower and inverted pyramid proposals based on simple perceptrons <ref> (Gallant, 1986) </ref> followed by several refinements, extensions, and re-inventions, e.g. (Baffes and Zelle, 1992, Frean, 1990, Mezar and Nadal, 1989, Simon, 1993).
Reference: <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58. </pages>
Reference-contexts: 1 Introduction In order to obtain good generalization performance when training a neural network, it must have the right size. Networks that are too small cannot represent the required function, while networks that are too large are prone to overfitting <ref> (Geman et al., 1992) </ref>. To limit the effective size of a network in order to avoid overfitting one can either use additive or subtractive methods or regularization.
Reference: <author> Hanson, S. J. </author> <year> (1990). </year> <title> Meiosis networks. </title> <editor> In Touretzky, D. S. (ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman Publishers, </publisher> <pages> pp. 533-541. </pages>
Reference: <author> Hassibi, B. and Stork, D. G. </author> <year> (1993). </year> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In Hanson, S. J., Cowan, J. D., Giles, C. L. (eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman Publishers, </publisher> <pages> pp. 164-171. </pages>
Reference: <author> Krogh, A. and Hertz, J. A. </author> <year> (1992). </year> <title> A simple weight decay can improve generalization. </title> <editor> In Moody, J. E., Hanson, S. J., Lippmann, R. P. (eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman Publishers, </publisher> <pages> pp. 950-957. </pages>
Reference: <author> Levin, A. U., Leen, T. K., and Moody, J. E. </author> <year> (1994). </year> <title> Fast pruning using principal components. </title>
Reference: <editor> In Cowan, J. D., Tesauro, G., Alspector, J. (eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman Publishers, </publisher> <pages> pp. 35-42. </pages>
Reference: <author> Littmann, E. and Ritter, H. </author> <year> (1992). </year> <title> Cascade network architectures. </title> <booktitle> In Proc. Int. Joint Conf. on Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> Baltimore, pp. 398-404. </pages>
Reference-contexts: This error minimization approach for candidate training is used in the as yet unpublished Cascade2 algorithm of 5 Fahlman 1 and, in a somewhat different form, the CasEr algorithm <ref> (Littmann and Ritter, 1992) </ref>. The form given below is similar to that of Cascade2 (except where noted). For direct error minimization we must create virtual output connections for the candidate units. <p> The same is true for CasEr (tanh activation function), with the additional restriction that the output weights are assumed to have constant value 1, i.e., are not trained. The CasEr article <ref> (Littmann and Ritter, 1992) </ref> evaluated the algorithm for classification prob lems only (plus a failed attempt to learn the Mackey-Glass time series) and concludes that CasCor works better than CasEr.
Reference: <author> Littmann, E. and Ritter, H. </author> <year> (1993). </year> <title> Generalization abilities of cascade network architectures. </title>
Reference-contexts: Not even constructive unit splitting with reasonable initializations for the new units works well (Hanson, 1989, Wynne-Jones, 1991). <ref> (Littmann and Ritter, 1993) </ref> propose direct cascading where local linear maps or different neural modules are cascaded and produce the output from the union of the original network inputs and the outputs of previous modules. This approach works well e.g. for predicting the Mackey-Glass time series.
Reference: <editor> In Hanson, S. J., Cowan, J. D., Giles, C. L. (eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman Publishers, </publisher> <pages> pp. 188-195. </pages>
Reference: <author> Mezard, M. and Nadal, J.-P. </author> <year> (1989). </year> <title> Learning in feedforward layered networks: The Tiling algorithm. </title> <journal> Journal of Physics A: Math. Gen., </journal> <volume> 22(12) </volume> <pages> 2191-2203. </pages>
Reference: <author> Nowlan, S. J. and Hinton, G. E. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4(4) </volume> <pages> 473-493. </pages> <note> 17 Prechelt, </note> <author> L. </author> <year> (1994). </year> <title> PROBEN1 | A set of benchmarks and benchmarking rules for neu-ral network training algorithms. </title> <type> Technical Report 21/94, </type> <institution> Fakultat fur Informatik, Univer-sitat Karlsruhe, Germany. </institution> <note> Anonymous FTP: /pub/papers/techreports/1994/1994-21.ps.gz on ftp.ira.uka.de. </note>
Reference-contexts: Regularization methods use a network with a large number of parameters, but limit the size of each parameter dimension by imposing additional constraints on each weight besides error minimization; examples are weight decay (Krogh and Hertz, 1991), soft weight sharing <ref> (Nowlan and Hinton, 1992) </ref>, and others; see (Finnoff et al., 1993, Reed, 1993) for an overview. Although additive methods seemed to be quite interesting in terms of computation time and ease of use, only few of them have actually been used in real applications.
Reference: <author> Reed, R. </author> <year> (1993). </year> <title> Pruning algorithms | a survey. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(5) </volume> <pages> 740-746. </pages>
Reference-contexts: Regularization methods use a network with a large number of parameters, but limit the size of each parameter dimension by imposing additional constraints on each weight besides error minimization; examples are weight decay (Krogh and Hertz, 1991), soft weight sharing (Nowlan and Hinton, 1992), and others; see <ref> (Finnoff et al., 1993, Reed, 1993) </ref> for an overview. Although additive methods seemed to be quite interesting in terms of computation time and ease of use, only few of them have actually been used in real applications.
Reference: <author> Riedmiller, M. and Braun, H. </author> <year> (1993). </year> <title> A direct adaptive method for faster backpropagation learning: The RPROP algorithm. </title> <booktitle> In Proc. of the IEEE Intl. Conf. on Neural Networks, </booktitle> <address> San Francisco, CA, </address> <pages> pp. 586-591. </pages>
Reference-contexts: They have several things in common (except where noted): (1) output units use the identity activation function, (2) hidden units use the x=(1 + jxj) activation function, (3) squared error is used as cost function, (4) termination criteria are as described above, (5) the RPROP learning rule <ref> (Riedmiller and Braun, 1993) </ref> is used for weight update, with the parameters + = 1:2, = 0:5, 0 2 [0:05 : : :0:2] randomly per weight, max = 50, min = 0, initial weights from [-0.1: : :0.1] randomly, and (6) nine units are used in each candidate pool.
Reference: <author> Simon, N. </author> <year> (1993). </year> <title> Constructive supervised learning algorithms for artificial neural networks. </title> <type> Master's thesis, </type> <institution> Delft University of Technology, Department of Electrical Engineering, Delft, Netherlands. </institution>
Reference: <author> Sjtgaard, S. </author> <year> (1991). </year> <title> A Conceptual Approach to Generalisation in Dynamic Neural Networks. </title> <type> PhD thesis, </type> <institution> Aarhus University, Aarhus, Danmark. </institution>
Reference-contexts: Although this power is in principle useful, it can be a disadvantage if such strong nonlinearity is not required to solve the problem and no sufficient number of training examples is available to control the power <ref> (Sjtgaard, 1991) </ref>. 2.2 Possible solutions 1. To remedy the first problem, one can change the learning rule and train directly for minimization of the output errors instead of for maximization of covariance. <p> However, for regression tasks the covariance learning rule is obviously ill-suited and should be replaced by direct error minimization. 1 All information about Cascade2 is from personal communication with Scott Fahlman, April 1994 and later. 6 2. For the second problem <ref> (Sjtgaard, 1991) </ref> shows that networks generated with CasCor can systematically have worse generalization than networks trained with the same method without cascading of the hidden units. He places all hidden units in one hidden layer.
Reference: <author> Wang, Z., Massimo, C. D., Tham, M. T., and Morris, A. J. </author> <year> (1994). </year> <title> A procedure for determining the topology of multilayer feedforward neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(2) </volume> <pages> 291-300. </pages>
Reference: <author> Wynne-Jones, M. </author> <year> (1991). </year> <title> Node splitting: A constructive algorithm for feed-forward neural networks. </title> <editor> In Moody, J. E., Hanson, S. J., Lippmann, R. P. (eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman Publishers, </publisher> <pages> pp. 1072-1079. </pages>
Reference: <author> Yeung, D.-Y. </author> <year> (1991). </year> <title> A neural network approach to constructive induction. </title> <editor> In Lawrence A. Birn-baum, G. C. C., editor, </editor> <booktitle> Machine Learning Proc. of the 8th Int. Workshop, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman Publishers, </publisher> <pages> pp. 228-232. </pages>
Reference-contexts: He places all hidden units in one hidden layer. Unfortunately, Sjtgaard uses but a single artificial learning problem with only two inputs and one output. <ref> (Yeung, 1991) </ref> had the same algorithm idea and concludes that there is almost no difference to CasCor for a number of learning problems. Fahlman says that the cascading is important for some problems and will not hurt for the others.
Reference: <author> Zollner, R., Schmitz, H. J., Wunsch, F., and Krey, U. </author> <year> (1992). </year> <title> Fast generating algorithm for a general three-layer perceptron. </title> <booktitle> Neural Networks, </booktitle> <volume> 5(5) </volume> <pages> 771-777. 18 </pages>
References-found: 27

