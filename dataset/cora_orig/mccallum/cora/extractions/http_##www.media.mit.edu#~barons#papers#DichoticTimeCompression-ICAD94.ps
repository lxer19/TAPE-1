URL: http://www.media.mit.edu/~barons/papers/DichoticTimeCompression-ICAD94.ps
Refering-URL: http://www.media.mit.edu/~barons/AronsAnnotatedBibliography.html
Root-URL: http://www.media.mit.edu
Email: barons@media.mit.edu  
Title: Efficient listening with two ears: Dichotic time compression and spatialization  
Author: Barry Arons 
Affiliation: MIT Media Laboratory  
Date: Nov. 79)  
Address: (Santa Fe, NM,  20 Ames Street Cambridge MA, 02139  
Note: To appear in the Proceedings of the1994 International Conference of Auditory Display  
Abstract: To increase the amount of information we can collect in a given amount of time, it is possible to employ signal processing techniques to speed up the rate at which recorded sounds are presented to the ears. Besides simply speeding up the playback, it is possible to auditorily display the signals in a way that allows us to process and interpret the signals more efficiently by exploiting the use of our two ears. This paper first reviews time compression techniques for increasing the amount of information that can be presented to a listener, with an emphasis on techniques that use two ears. The paper then describes a new technique that integrates these dichotic time compression techniques into a spatial audio display system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Foulke, E. </author> <title> The Perception of Time Compressed Speech. Ch. 4 in Perception of Language, </title> <note> edited by Kjeldergaard, </note> <author> P. M., D. L. Horton, and J. J. Jenkins, </author> <type> 79107. </type> <institution> Columbus, OH: Merrill, </institution> <year> 1971. </year>
Reference-contexts: Spontaneous, or conversational, speech can be time compressed by a factor of about two and still remain intelligible and comprehensible <ref> [1, 2, 3, 4] </ref>. 2 Time compression techniques rely on the temporal redundancy of speech as demonstrated by Miller and Lickliderthe intelligibility of speech recordings interrupted by periods of silence remains high if the number of interruptions per second and the portion of time the speech is on are properly selected
Reference: [2] <author> Foulke, W. and T. G. Sticht. </author> <title> Review of Research on the Intelligibility and Comprehension of Accelerated Speech. </title> <journal> Psychological Bulletin 72 (1969): </journal> <volume> 5062. </volume>
Reference-contexts: Spontaneous, or conversational, speech can be time compressed by a factor of about two and still remain intelligible and comprehensible <ref> [1, 2, 3, 4] </ref>. 2 Time compression techniques rely on the temporal redundancy of speech as demonstrated by Miller and Lickliderthe intelligibility of speech recordings interrupted by periods of silence remains high if the number of interruptions per second and the portion of time the speech is on are properly selected
Reference: [3] <author> Beasley, D. S. and J. E. Maki. </author> <title> Time and Frequency-Altered Speech. Ch. 12 in Contemporary Issues in Experimental Phonetics, edited by Lass, </title> <editor> N. J., </editor> <address> 419458. New York: </address> <publisher> Academic Press, </publisher> <year> 1976. </year>
Reference-contexts: Spontaneous, or conversational, speech can be time compressed by a factor of about two and still remain intelligible and comprehensible <ref> [1, 2, 3, 4] </ref>. 2 Time compression techniques rely on the temporal redundancy of speech as demonstrated by Miller and Lickliderthe intelligibility of speech recordings interrupted by periods of silence remains high if the number of interruptions per second and the portion of time the speech is on are properly selected
Reference: [4] <author> Arons, B. </author> <title> Techniques, Perception, </title> <booktitle> and Applications of Time-Compressed Speech. In Proceedings of 1992 Conference, </booktitle> <publisher> American Voice I/O Society, </publisher> <month> Sep. </month> <year> 1992, </year> <month> 169177. </month>
Reference-contexts: Spontaneous, or conversational, speech can be time compressed by a factor of about two and still remain intelligible and comprehensible <ref> [1, 2, 3, 4] </ref>. 2 Time compression techniques rely on the temporal redundancy of speech as demonstrated by Miller and Lickliderthe intelligibility of speech recordings interrupted by periods of silence remains high if the number of interruptions per second and the portion of time the speech is on are properly selected
Reference: [5] <author> Miller, G. A. and J. C. R. Licklider. </author> <title> The Intelligibility of Interrupted Speech. </title> <journal> Journal of the Acoustic Society of America 22 (1950): </journal> <volume> 167173. </volume>
Reference-contexts: 2, 3, 4]. 2 Time compression techniques rely on the temporal redundancy of speech as demonstrated by Miller and Lickliderthe intelligibility of speech recordings interrupted by periods of silence remains high if the number of interruptions per second and the portion of time the speech is on are properly selected <ref> [5] </ref>. A) Original signal 1 3 5 7 9 B) Sampling method (B) shows the signal time compressed by the sampling method with every other segment removed. The amount of time compression can be varied by changing the relative lengths of the retained and discarded segments.
Reference: [6] <author> Fairbanks, G., W. L. Everitt, and R. P. Jaeger. </author> <title> Method for Time or Frequency Compression-Expansion of Speech. </title> <journal> Transactions of the Institute of Radio Engineers, Professional Group on Audio AU-2 (1954): </journal> <volume> 712. </volume> <editor> Reprinted in G. Fairbanks, </editor> <title> Experimental Phonetics: Selected Articles, </title> <publisher> University of Illinois Press, </publisher> <year> 1966. </year>
Reference-contexts: The amount of time compression can be varied by changing the relative lengths of the retained and discarded segments. One of the simplest techniques to time compress a recording is the sampling, or Fairbanks, method <ref> [6] </ref>. This technique consists of removing short segments of the signal at regular time intervals (figure 1). For speech recordings these segments are usually longer than a pitch period (&gt;~10ms) and shorter than a phoneme (&lt;~100 ms), and are often 3060ms.
Reference: [7] <author> Roucos, S. and A. M. Wilgus. </author> <title> High Quality TimeScale Modification for Speech. </title> <booktitle> In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, IEEE, </booktitle> <year> 1985, </year> <month> 493496. </month>
Reference-contexts: 3 Decreasing the amplitude of the end of one segment while increasing the amplitude of the beginning of the next segment. 3 The synchronized overlap-and-add (SOLA) method of time compression further improves the quality of the speech by ensuring that the segments are optimally aligned before performing the cross fade <ref> [7, 8] </ref>. This is done by checking different amounts of overlap between the end of one speech segment and the beginning of the next to find where the signals are the most similar (i.e., by computing the cross correlation).
Reference: [8] <author> Hejna Jr, D. J. </author> <title> Real-Time TimeScale Modification of Speech via the Synchronized Overlap-Add Algorithm. </title> <type> Masters thesis, </type> <institution> MIT, 1990. Department of Electrical Engineering and Computer Science. </institution>
Reference-contexts: 3 Decreasing the amplitude of the end of one segment while increasing the amplitude of the beginning of the next segment. 3 The synchronized overlap-and-add (SOLA) method of time compression further improves the quality of the speech by ensuring that the segments are optimally aligned before performing the cross fade <ref> [7, 8] </ref>. This is done by checking different amounts of overlap between the end of one speech segment and the beginning of the next to find where the signals are the most similar (i.e., by computing the cross correlation).
Reference: [9] <author> Moore, B. C. J. </author> <title> An Introduction to the Psychology of Hearing. </title> <address> New York: </address> <publisher> Academic Press, </publisher> <address> 3d edition, </address> <year> 1989. </year>
Reference-contexts: it effectively removes entire pitch periods, and produces better sounding speech than the sampling method. 3 Integrating information between the ears There are a variety of psychoacoustical phenomena that illustrate the human ability to integrate information presented to both ears (e.g., localization, lateralization, binaural masking level differences, and binaural beatssee <ref> [9] </ref>). Speech signals are treated differently than tones or noise in higher levels of human auditory processing, and are grouped more cohesively than other sounds [10]. For example, the continuity of pitch helps control attention when speech signals are presented to both ears. <p> Moore says that while the most reliable cues used in the localization of sounds depends upon a comparison of the signals reaching the two ears, there are also phenomena of auditory space perception which result from monaural processing of the signals <ref> [9, p. 194] </ref>. Note however, that the spatialized DTCS technique described here is not strictly presenting two monaural channels, rather there are still a variety of rich interaural cues. The HRTF cues, including interau-ral intensity differences and monaural spectral cues, are all present.
Reference: [10] <author> Bregman, A. S. </author> <title> Auditory Scene Analysis: The Perceptual Organization of Sound. Cam - bridge, </title> <address> MA: </address> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Speech signals are treated differently than tones or noise in higher levels of human auditory processing, and are grouped more cohesively than other sounds <ref> [10] </ref>. For example, the continuity of pitch helps control attention when speech signals are presented to both ears. <p> Speech sounds in particular are very rich in familiar information. These common speech spectral cues make it easier for us to perceive these two channels as a single auditory stream <ref> [10] </ref>. This spatialized DTCS technique was informally found to produce an externalized virtual image.
Reference: [11] <author> Gray, J. A. and A. A. I. Wedderburn. </author> <title> Grouping Strategies with Simultaneous Stimuli. </title> <journal> Quarterly Journal of Experimental Psychology 12 (1960): </journal> <volume> 180184. </volume>
Reference-contexts: Gray and Wed-derburn showed that although there is a tendency to group signals according to the ear they are presented to, this can be overcome if there are strong cues that favor a different grouping <ref> [11, 12] </ref>.
Reference: [12] <author> Hede, A. J. </author> <title> Awareness of List Organization and the Gray and Wedderburn Effect. </title> <booktitle> Psychological Reports 43 (1978): </booktitle> <pages> 371374. </pages>
Reference-contexts: Gray and Wed-derburn showed that although there is a tendency to group signals according to the ear they are presented to, this can be overcome if there are strong cues that favor a different grouping <ref> [11, 12] </ref>.
Reference: [13] <author> Scott, R. J. </author> <title> Time Adjustment in Speech Synthesis. </title> <journal> Journal of the Acoustic Society of America 41 (1967): </journal> <volume> 6065. </volume>
Reference-contexts: Segments can be completely overlapped (left), or offset by half of a sampling period (right). This style of presentation of dichotic time compressed speech (hereafter DTCS) was first described by Scott in the mid 1960s <ref> [13] </ref>. Scott reports that subjects found the dichotic presentation to be more intelligible than a diotic presentation. The dichotic speech sounds a bit annoying at first, as most listeners switched attention between their ears, but this unusual sensation became less noticeable over time. <p> Scott says although there is a temporal mismatch of the two speech signals when presented dichotically, a fusing of information at both ears must take place to in 4 Diotic presentation is when the same signal is presented to both ears over headphones. 4 crease the intelligibility <ref> [13, p. 64] </ref>. Gerber showed that under a variety of different configurations intelligibility of time compressed speech was always better for dichotic presentation than with diotic presentation [14, 15].
Reference: [14] <author> Gerber, S. E. </author> <title> Limits of Speech Time Compression. In Time-Compressed Speech, edited by Duker, </title> <editor> S., </editor> <address> 456465. Metuchen, NJ: Scarecrow, </address> <year> 1974. </year>
Reference-contexts: Gerber showed that under a variety of different configurations intelligibility of time compressed speech was always better for dichotic presentation than with diotic presentation <ref> [14, 15] </ref>. <p> Unfortunately, this cross talk will degrade the DTCS signal, as Gerber notes if one listens to both signals with both ears, the intelligibility is poorer than if one listens to one signal with one ear and the other signal with the other ear <ref> [14, p. 459] </ref>. However, it is possible to create a virtual sound source where each ear only receives one channel of the DTCS signal. This can be achieved by placing two virtual sound sources at the same location, but only filtering each signal for one ear (figure 5A).
Reference: [15] <author> Gerber, S. E. and B. H. Wulfeck. </author> <title> The Limiting Effect of Discard Interval on Time-Compressed Speech. </title> <booktitle> Language and Speech 20 (1977): </booktitle> <pages> 108115. </pages>
Reference-contexts: Gerber showed that under a variety of different configurations intelligibility of time compressed speech was always better for dichotic presentation than with diotic presentation <ref> [14, 15] </ref>. <p> With a properly selected discard interval (the length of audio segment removed from the signal and played to the opposite ear), word intelligibility errors decreased 49% for a 2:1 time compression under dichotic conditions <ref> [15] </ref>. 5 It is also possible to create an analogous dichotic SOLA signal by processing speech through the SOLA algorithm a second time with an offset in the starting point.
Reference: [16] <author> Wenzel, E. M. </author> <title> Localization in Virtual Acoustic Displays. Presence 1 (1992): </title> <type> 80107. </type>
Reference-contexts: not contain all of the information contained in the original recording. 5 Presenting DTCS spatially In spatial audio display systems one or more channels of audio are presented to the ears based on the head related transfer function (HRTF) and the spatial location of the source relative to each ear <ref> [16, 17] </ref>.
Reference: [17] <author> Wenzel, E. M. </author> <title> Spatial Sound and Sonification. In Auditory Display: Sonification, Audification, and Auditory Interfaces, edited by Kramer, </title> <editor> G., </editor> <booktitle> 127150. Santa Fe Institute 8 Studies in the Sciences of Complexity in the Sciences of Complexity, </booktitle> <volume> Vol. </volume> <pages> XVII. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1994. </year>
Reference-contexts: not contain all of the information contained in the original recording. 5 Presenting DTCS spatially In spatial audio display systems one or more channels of audio are presented to the ears based on the head related transfer function (HRTF) and the spatial location of the source relative to each ear <ref> [16, 17] </ref>.
Reference: [18] <author> Arons, B. SpeechSkimmer: </author> <title> Interactively Skimming Recorded Speech. </title> <booktitle> In Proceedings of the ACM Symposium on User Interface Software and Technology (UIST), ACM SIGGRAPH and ACM SIGCHI, </booktitle> <publisher> ACM Press, </publisher> <month> Nov. </month> <year> 1993, </year> <month> 187196. </month>
Reference-contexts: It is useful to be able to present time compressed speech in a virtual acoustic display, such as in user interfaces that allow skimming or browsing of recorded audio material <ref> [18, 19] </ref>, or systems that attempt to present multiple streams of recorded speech simultaneously [20, 21]. Presenting speech that has been time compressed using the basic sampling or SOLA techniques in a spatial audio display system is straightforward, as it can be treated like any other audio source.
Reference: [19] <author> Arons, B. </author> <title> Interactively Skimming Recorded Speech. </title> <type> Ph.D. dissertation, </type> <institution> MIT, </institution> <year> 1994. </year>
Reference-contexts: It is useful to be able to present time compressed speech in a virtual acoustic display, such as in user interfaces that allow skimming or browsing of recorded audio material <ref> [18, 19] </ref>, or systems that attempt to present multiple streams of recorded speech simultaneously [20, 21]. Presenting speech that has been time compressed using the basic sampling or SOLA techniques in a spatial audio display system is straightforward, as it can be treated like any other audio source.
Reference: [20] <author> Arons, B. </author> <title> A Review of the Cocktail Party Effect. </title> <journal> Journal of the American Voice I/O Society 12 (1992): </journal> <volume> 3550. </volume>
Reference-contexts: It is useful to be able to present time compressed speech in a virtual acoustic display, such as in user interfaces that allow skimming or browsing of recorded audio material [18, 19], or systems that attempt to present multiple streams of recorded speech simultaneously <ref> [20, 21] </ref>. Presenting speech that has been time compressed using the basic sampling or SOLA techniques in a spatial audio display system is straightforward, as it can be treated like any other audio source.
Reference: [21] <author> Mullins, A. T. </author> <title> Audio Streamer: Browsing Concurrent Audio Streams. </title> <type> Masters thesis, </type> <institution> MIT, </institution> <year> 1995. </year>
Reference-contexts: It is useful to be able to present time compressed speech in a virtual acoustic display, such as in user interfaces that allow skimming or browsing of recorded audio material [18, 19], or systems that attempt to present multiple streams of recorded speech simultaneously <ref> [20, 21] </ref>. Presenting speech that has been time compressed using the basic sampling or SOLA techniques in a spatial audio display system is straightforward, as it can be treated like any other audio source.
Reference: [22] <institution> Crystal River Engineering. The Beachtron: Three-Dimensional Audio for PC-Compati - bles, </institution> <address> Groveland, CA. </address> <year> 1993. </year>
Reference-contexts: S1 B) DTCS ear 1 DTCS ear 2 Spatial Processor A L Spatial Processor B L Location (x, y, z) DTCS originating from a single virtual sound location, but each is only presented to a sin - gle ear. (B) System configuration for spatializing DTCS using two Beachtron boards <ref> [22] </ref>. Moore says that while the most reliable cues used in the localization of sounds depends upon a comparison of the signals reaching the two ears, there are also phenomena of auditory space perception which result from monaural processing of the signals [9, p. 194].
References-found: 22

