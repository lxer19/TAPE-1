URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3776/3776.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: CAUCHY-LIKE PRECONDITIONERS FOR 2-DIMENSIONAL ILL-POSED PROBLEMS  
Author: MISHA E. KILMER 
Keyword: Key words. Regularization, ill-posed problems, Toeplitz, Cauchy-like, preconditioner, conjugate gradient, normal equations, image processing, deblurring  
Date: March 28, 1997  
Note: AMS(MOS) subject classifications. 65R20, 45L10, 94A12  
Abstract: Ill-conditioned matrices with block Toeplitz, Toeplitz block (BTTB) structure arise from the discretization of certain ill-posed problems in signal and image processing. We use a preconditioned conjugate gradient algorithm to compute a regularized solution to this linear system given noisy data. Our preconditioner is a Cauchy-like block diagonal approximation to an orthogonal transformation of the BTTB matrix. We show the preconditioner has desirable properties when the kernel of the ill-posed problem is smooth: the largest singular values of the preconditioned matrix are clustered around one, the smallest singular values remain small, and the subspaces corresponding to the largest and smallest singular values, respectively, remain unmixed. For a system involving np variables, the preconditioned algorithm costs only O(np(lg n + lg p)) operations per iteration. We demonstrate the effectiveness of the preconditioner on three examples. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Chan, J. Nagy, and R. Plemmons, </author> <title> FFT-based preconditioners for Toeplitz-block least squares problems, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 30 (1993), </volume> <pages> pp. </pages> <month> 1740-1768. </month> <title> [2] , Circulant preconditioned Toeplitz least squares iterations, </title> <note> SIAM J. Matrix Anal. and Appl., 15 (1994), p. 80. </note>
Reference-contexts: Also, preconditioners for BTTB matrices which are block circulant (BC), circulant block (CB), or block circulant with circulant blocks (BCCB) have been found to be very efficient <ref> [4, 23, 1] </ref>. For example, if the preconditioner is determined to be block Toeplitz with circulant blocks (BTCB), applying the preconditioner can be reduced to solving p systems involving n fi n Toeplitz matrices [4].
Reference: [3] <author> R. Chan and M. Yeung, </author> <title> Circulant preconditioners for Toeplitz matrices with positive continuous generating functions, </title> <institution> Math. Comput., </institution> <year> (1992), </year> <pages> pp. 233-240. </pages> <note> Cited by [2]. </note>
Reference-contexts: Chan circulant approximation. Finally, let the BTTB matrix T be given as T = R T ; where R is a non-singular n fi n matrix. A lemma proved by R. Chan and M. Yeung <ref> [3] </ref> will be useful. Lemma 4.3 (R. Chan and M. Yeung). Let h 2 C 2 .
Reference: [4] <author> R. H. Chan and X.-Q. Jin, </author> <title> A family of block preconditioners for block systems, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 13 (1992), </volume> <pages> pp. 1218-1235. </pages>
Reference-contexts: methods like conjugate gradients can take advantage of the well-known fact that matrix-vector products involving BTTB matrices with n blocks of size p can be computed in O (np (lg p + lg n)) operations by embedding the matrix inside a 2pn fi 2pn block circulant matrix with circulant blocks <ref> [4] </ref>. Also, preconditioners for BTTB matrices which are block circulant (BC), circulant block (CB), or block circulant with circulant blocks (BCCB) have been found to be very efficient [4, 23, 1]. <p> Also, preconditioners for BTTB matrices which are block circulant (BC), circulant block (CB), or block circulant with circulant blocks (BCCB) have been found to be very efficient <ref> [4, 23, 1] </ref>. For example, if the preconditioner is determined to be block Toeplitz with circulant blocks (BTCB), applying the preconditioner can be reduced to solving p systems involving n fi n Toeplitz matrices [4]. <p> For example, if the preconditioner is determined to be block Toeplitz with circulant blocks (BTCB), applying the preconditioner can be reduced to solving p systems involving n fi n Toeplitz matrices <ref> [4] </ref>. However, for indefinite and/or ill-conditioned systems, the O (n lg 2 n) and O (n 2 ) factorization algorithms for Toeplitz matrices can be numerically unstable; these algorithms can require as many as O (n 3 ) operations in order to maintain stability [25, 12, 7]. <p> Finally, we define H to be the BTCB matrix H = 6 6 6 4 H 1 H 0 H 1 : : : H 2n . . . . . . . . . . . . 3 7 7 7 : It was shown in <ref> [4] </ref> that H is the closest BTCB matrix to T in the Frobenius norm. The goal is to develop a preconditioner from an appropriately transformed version of the matrix H. <p> Now since the H i are circulant, they can be diagonalized by the matrix F <ref> [4] </ref>; therefore, for each (i; j), F H ij F fl is diagonal. In x1 we assumed that the unknowns are ordered first in the increasing ff direction, then in order of increasing fi. <p> We now discuss two special cases for which H T has singular values clustered around zero. First we will need the following lemma from <ref> [4] </ref>. Lemma 4.1 (R. Chan and Q. Jin). Assume that the BTTB matrix T is sym metric. Let the entries of block T i be denoted t (i) (i) jjkj for 1 j; k p, 1 i n.
Reference: [5] <author> R. H. Chan and M. K. Ng, </author> <title> Conjugate gradient methods for Toeplitz systems, </title> <journal> SIAM Review, </journal> <volume> 38 (1996), </volume> <pages> pp. 427-482. </pages>
Reference-contexts: However, since H j is the T. Chan circulant approximant of T j , we have <ref> [5] </ref> l = s=n+1 s (1 n 2i p Since ~ t (l) (j) l we use the above substitution in Equations (17) and (18). <p> In this case the diagonals of block H i of the BTCB matrix H will be the T. Chan approximation to block T i given by the formula (see <ref> [5] </ref>, for instance) h l = p jk=l ( mod n) (i) Acknowledgments. I would like to thank Professor Dianne P. O'Leary for her helpful comments and guidance during the preparation of this paper. My thanks, also, to Dr. Jim Nagy for providing suggestions on numerical examples.
Reference: [6] <author> T. Chan, </author> <title> An optimal circulant preconditioner for Toeplitz systems, </title> <journal> SIAM J. Sci. Statist. Com-put., </journal> <volume> 9 (1988), </volume> <pages> pp. 766-771. </pages>
Reference-contexts: For each T i , let us define H i to be its T. Chan circulant approximation <ref> [6] </ref>, so that the diagonals of H i are given by h j = (nj)t (i) jn h n+j 0 &lt; j &lt; n 6 The matrix H i is the closest circulant matrix in the Frobenius norm to T i [6]. <p> Chan circulant approximation <ref> [6] </ref>, so that the diagonals of H i are given by h j = (nj)t (i) jn h n+j 0 &lt; j &lt; n 6 The matrix H i is the closest circulant matrix in the Frobenius norm to T i [6].
Reference: [7] <author> T. F. Chan and P. C. Hansen, </author> <title> A lookahead Levinson algorithm for general Toeplitz systems, </title> <booktitle> IEEE Proc. Signal Processing, 40 (1992), </booktitle> <pages> pp. 1079-1090. </pages> <note> Cited by [11]. </note>
Reference-contexts: However, for indefinite and/or ill-conditioned systems, the O (n lg 2 n) and O (n 2 ) factorization algorithms for Toeplitz matrices can be numerically unstable; these algorithms can require as many as O (n 3 ) operations in order to maintain stability <ref> [25, 12, 7] </ref>. To overcome this difficulty, we make use of the fact that Toeplitz matrices are related to Cauchy-like matrices by fast orthogonal transformations [17, 9, 11]. The particular Cauchy-like matrices discussed in x2 permit fast matrix-vector multiplication.
Reference: [8] <author> H. E. Fleming, </author> <title> Equivalence of regularization and truncated iteration in the solution of ill-posed problems, </title> <journal> Linear Algebra Appl., </journal> <volume> 130 (1990), </volume> <pages> pp. 133-150. </pages>
Reference-contexts: If the discrete Picard condition holds, then CGLS acts as an iterative regularization method with the iteration index taking the role of the regularization parameter <ref> [8, 13, 15] </ref>. The spread and clustering of the singular values govern the speed and convergence of the algorithm [26]. Preconditioning is therefore often applied in an effort to cluster the singular values and thus, to speed convergence.
Reference: [9] <author> I. Gohberg, T. Kailath, and V. Olshevsky, </author> <title> Fast Gaussian elimination with partial pivoting of matrices with displacement structure, </title> <journal> Math. Comput., </journal> <volume> 64 (1995), </volume> <pages> pp. 1557-1576. </pages>
Reference-contexts: To overcome this difficulty, we make use of the fact that Toeplitz matrices are related to Cauchy-like matrices by fast orthogonal transformations <ref> [17, 9, 11] </ref>. The particular Cauchy-like matrices discussed in x2 permit fast matrix-vector multiplication. An advantage of Cauchy-like matrices is that their inverses are also Cauchy-like, unlike Toeplitz matrices whose inverses are not generally Toeplitz. <p> Fortunately, certain properties of Cauchy-like matrices insure that LU factorizations of Cauchy-like matrices may be computed using only the matrices ; fi and the generators without ever forming the matrix C; see <ref> [9] </ref>, for example. One disadvantage of Toeplitz matrices is that permutations of Toeplitz matrices are not necessarily Toeplitz, so that incorporating pivoting into fast factorization schemes becomes difficult and expensive. However, because of (4), it is easy to show the following (see [17, 11], for example): Property 1. <p> However, because of (4), it is easy to show the following (see [17, 11], for example): Property 1. Row and column permutations of Cauchy-like matrices are Cauchy-like, as are leading principal submatrices. This property and the fact that Schur complements of Cauchy-like matrices are Cauchy-like <ref> [9] </ref> lead to fast algorithms for factoring Cauchy-like matrices which can pivot for stability [9, 11]. We use the algorithm developed by Gu [11] which performs a fast O (`n 2 ) variation of LU decomposition with modified complete pivoting. <p> Row and column permutations of Cauchy-like matrices are Cauchy-like, as are leading principal submatrices. This property and the fact that Schur complements of Cauchy-like matrices are Cauchy-like [9] lead to fast algorithms for factoring Cauchy-like matrices which can pivot for stability <ref> [9, 11] </ref>. We use the algorithm developed by Gu [11] which performs a fast O (`n 2 ) variation of LU decomposition with modified complete pivoting. <p> The third important property is that Toeplitz matrices also satisfy certain displacement equations <ref> [21, 9] </ref> which allow them to be transformed via fast Fourier transforms into Cauchy-like matrices [17, 9]: Property 3. <p> The third important property is that Toeplitz matrices also satisfy certain displacement equations [21, 9] which allow them to be transformed via fast Fourier transforms into Cauchy-like matrices <ref> [17, 9] </ref>: Property 3.
Reference: [10] <author> R. C. Gonzalez and R. E. Woods, </author> <title> Digital Image Processing, </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1992. </year>
Reference-contexts: In optics, for example, t is called the point spread function and describes the response of the system or measuring device to a single point of light at coordinates (ff; fi) <ref> [10] </ref>. Thus if the values ^ f (ff; fi) represent light intensities reflected from a three-dimensional object, the integral equation might be used to model the blurring of that object when its picture is taken using a camera with a warped lens.
Reference: [11] <author> M. Gu, </author> <title> Stable and efficient algorithms for structured systems of linear equations, </title> <type> Tech. Report LBL-37690, </type> <institution> Lawrence Berkeley Laboratory, </institution> <year> 1995. </year>
Reference-contexts: To overcome this difficulty, we make use of the fact that Toeplitz matrices are related to Cauchy-like matrices by fast orthogonal transformations <ref> [17, 9, 11] </ref>. The particular Cauchy-like matrices discussed in x2 permit fast matrix-vector multiplication. An advantage of Cauchy-like matrices is that their inverses are also Cauchy-like, unlike Toeplitz matrices whose inverses are not generally Toeplitz. <p> One disadvantage of Toeplitz matrices is that permutations of Toeplitz matrices are not necessarily Toeplitz, so that incorporating pivoting into fast factorization schemes becomes difficult and expensive. However, because of (4), it is easy to show the following (see <ref> [17, 11] </ref>, for example): Property 1. Row and column permutations of Cauchy-like matrices are Cauchy-like, as are leading principal submatrices. This property and the fact that Schur complements of Cauchy-like matrices are Cauchy-like [9] lead to fast algorithms for factoring Cauchy-like matrices which can pivot for stability [9, 11]. <p> Row and column permutations of Cauchy-like matrices are Cauchy-like, as are leading principal submatrices. This property and the fact that Schur complements of Cauchy-like matrices are Cauchy-like [9] lead to fast algorithms for factoring Cauchy-like matrices which can pivot for stability <ref> [9, 11] </ref>. We use the algorithm developed by Gu [11] which performs a fast O (`n 2 ) variation of LU decomposition with modified complete pivoting. <p> This property and the fact that Schur complements of Cauchy-like matrices are Cauchy-like [9] lead to fast algorithms for factoring Cauchy-like matrices which can pivot for stability [9, 11]. We use the algorithm developed by Gu <ref> [11] </ref> which performs a fast O (`n 2 ) variation of LU decomposition with modified complete pivoting. Recall that in complete pivoting, at every elimination step one chooses the largest element in the current submatrix as the pivot in order to reduce element growth. <p> This algorithm computes the pivoted LU factorization (C = P LU Q where P and Q are permutation matrices) <ref> [11, Alg. 2] </ref> using only the generators, which are easy to determine and to update (see x5), and Gu shows that the algorithm can be efficient and numerically stable. <p> 2i S 1 = diag (e i (2n1)i S 0 = diag (1; e i i and F is the normalized inverse discrete Fourier transform matrix defined by F = p n : We note that Toeplitz matrices are orthogonally related to Cauchy-like matrices through other fast transformations as well <ref> [11] </ref>. However, the particular relation in Property 3 can be exploited to determine a O (n lg n) stable algorithm for multiplication by the inverse of the Cauchy-like matrix [22]. <p> In fact, any pivoting strategy that yields this type of permutation will give a reasonable preconditioner for our scheme. We refer the interested reader to <ref> [11] </ref> for details of Gu's modified complete pivoting strategy. The key fact is that his algorithm makes its pivoting decision based on the size of the elements in the generator corresponding to the block that remains to be factored.
Reference: [12] <author> M. H. Gutknecht and M. Hochbruck, </author> <title> Look-ahead Levinson and Schur algorithms for non-Hermitian Toeplitz systems, </title> <type> Tech. Report IPS 93-11, </type> <institution> IPS Supercomputing, ETH-Zurich, </institution> <year> 1993. </year> <note> Cited by [11]. </note>
Reference-contexts: However, for indefinite and/or ill-conditioned systems, the O (n lg 2 n) and O (n 2 ) factorization algorithms for Toeplitz matrices can be numerically unstable; these algorithms can require as many as O (n 3 ) operations in order to maintain stability <ref> [25, 12, 7] </ref>. To overcome this difficulty, we make use of the fact that Toeplitz matrices are related to Cauchy-like matrices by fast orthogonal transformations [17, 9, 11]. The particular Cauchy-like matrices discussed in x2 permit fast matrix-vector multiplication.
Reference: [13] <author> M. Hanke, J. Nagy, and R. Plemmons, </author> <title> Preconditioned iterative regularization for ill-posed problems, Numerical Linear Algebra and Sci. </title> <journal> Computing, </journal> <note> (1993). </note> <author> L. Reichel, A. Ruttan and R.S. Varga, </author> <title> editors. </title>
Reference-contexts: If the discrete Picard condition holds, then CGLS acts as an iterative regularization method with the iteration index taking the role of the regularization parameter <ref> [8, 13, 15] </ref>. The spread and clustering of the singular values govern the speed and convergence of the algorithm [26]. Preconditioning is therefore often applied in an effort to cluster the singular values and thus, to speed convergence. <p> Let us compare our preconditioning scheme with the preconditioning method given in <ref> [13] </ref> for the BTTB matrices of discrete ill-posed problems. In [13] the precon-ditioner is determined by forming the T. Chan BCCB approximant to T , computing its eigenvalues via 2-D fast Fourier transforms, and then replacing all the eigenvalues below a tolerance with ones. <p> Let us compare our preconditioning scheme with the preconditioning method given in <ref> [13] </ref> for the BTTB matrices of discrete ill-posed problems. In [13] the precon-ditioner is determined by forming the T. Chan BCCB approximant to T , computing its eigenvalues via 2-D fast Fourier transforms, and then replacing all the eigenvalues below a tolerance with ones. <p> Therefore, our method is similar to their BCCB based preconditioner in that we also rely on a rank revealing factorization to determine the appropriate cutoff which is used to form the preconditioner. We choose our cutoff tolerance in a manner similar to that given in <ref> [13] </ref>. However, our preconditioner is formed from a BTCB approximant to T , which requires approximating T only on one level unlike the BCCB approximant which requires approximating T on two levels. <p> The most notable difference is that is we rely on a transformation to Cauchy-like matrices; therefore we may use a fast pivoted factorization scheme, rather than 2-D Fourier transforms, to generate the necessary rank revealing information. While the preconditioner in <ref> [13] </ref> requires O (pn (lg p + lg n)) operations to precompute, our preconditioner requires, in the worst case, O (pb w lg p+pn lg n+ P p i ) operations to precompute, where b w denotes the maximum block bandwidth of the matrix. <p> In the next section, we show that our preconditioner is just as effective as the one in <ref> [13] </ref> in clustering the large singular values around one. Further, we show that the small singular values remain small and that the upper and lower subspaces remain unmixed. 4. Properties of the preconditioner. <p> fl = 1 1 M ^ Q 3 ^ 3 : Since Q fl 1 has orthonormal columns, as does ^ Q 3 , it follows that kV fl ^ V 3 k 2 m fl ^ Nk+1 (maxf1; max i (1) We note that if the preconditioner developed in <ref> [13] </ref> for their right preconditioning scheme is applied to the left rather than the right, a similar result can be obtained. Next we show that ^ j j for j corresponding to the last N m fl singular values, and thus ^ Nk+1 is small. <p> We consider the BTTB matrix T with entries given by t ij = ce :1 ((ij) 2 +(kl) 2 ) ; 5 i j; k l 5 0 otherwise; where c is a normalization constant. This matrix is the one used in <ref> [13] </ref> and has a condition number of about 10 11 . Since this is a BT T B matrix, only the first row and first column of the 2n blocks T i ; n &lt; i &lt; n; need to be generated and stored. <p> This was the best preconditioner that could be determined using this selection method; after 13 iterations a relative error of 3:86 fi 10 1 was reached. For comparison, the dashed-dot line illustrates the optimal convergence behavior of right-preconditioned scheme in <ref> [13] </ref>, where the cutoff was determined to be 725 eigenvalues. This method achieves a minimum relative error value of 3:49 fi 10 1 in 9 iterations. 6.2. Example 2. <p> line shows convergence when m fl = 0; dashed line shows convergence for our preconditioner with m fl = 711 using the d-selection method; dotted line shows convergence when m fl = 583 using the Fourier coefficient selection method; dash-dotted line shows the convergence behavior for the preconditioning scheme in <ref> [13] </ref> with the cutoff at 725 eigenvalues. added. Next, we set g = T ^ f + e, where e is a normally distributed random vector, generated with the Matlab randn function, scaled so that the noise level was 10 2 . <p> Therefore it is easy to show that the cost of precomputing our preconditioner is reduced to O (m 2 1 + p lg p + n lg n + N ) operations for this special case. Likewise, the cost of precomputing the preconditioner in <ref> [13] </ref> reduces to O (p lg p + n lg n + N ) operations. 6.3. Example 3. In both Examples 1 and 2, the matrix T was symmetric and its block bandwidth was small relative to the number p of blocks. Since unsymmetry 21 Fig. 5. <p> line shows convergence when m fl = 0; dashed line shows convergence for our preconditioner with m fl = 122 using the d-selection method; dotted line shows convergence when m fl = 109 using the Fourier coefficient selection method; dash-dotted line shows the convergence behavior for the preconditioning scheme in <ref> [13] </ref> with the cutoff at 116 eigenvalues. 23 Fig. 9. Blurred, noisy image, Example 3. and larger bandwidth can be encountered in practice, we consider the non-symmetric matrix with a larger block bandwidth as follows. <p> In contrast, the precondi tioned iterative scheme in <ref> [13] </ref> could do no better than 1:24 fi 10 1 after 17 iterations (the preconditioner which achieved this value was constructed using a cutoff of 574 eigenvalues). 24 Fig. 10. Solid line shows 2-D Fourier coefficients of the noisy data sorted in order of decreasing magnitude. <p> line shows convergence when m fl = 0; dashed line shows convergence for our preconditioner with m fl = 576 using the d-selection method; dotted line shows convergence when m fl = 435 using the Fourier coefficient selection method; dash-dotted line shows the convergence behavior for the preconditioning scheme in <ref> [13] </ref> with the cutoff at 574 eigenvalues. 25 6.4. Results Summary. We conducted several other experiments comparing the effectiveness of our preconditioner with the effectiveness of the preconditioner found in [13]. <p> fl = 435 using the Fourier coefficient selection method; dash-dotted line shows the convergence behavior for the preconditioning scheme in <ref> [13] </ref> with the cutoff at 574 eigenvalues. 25 6.4. Results Summary. We conducted several other experiments comparing the effectiveness of our preconditioner with the effectiveness of the preconditioner found in [13]. The experiments, which we now summarize, were conducted using matrices of different sizes and structure, different original images, and various noise levels. <p> We also found this to be true when the matrix was blockwise unsymmetric. Consequently, our preconditioner can show significant improvement for these types of problems over the preconditioner in <ref> [13] </ref> when the cutoff, determined by the noise level, is large enough to include some mid-range singular values, as evidenced in Example 3. For larger noise levels, there was no consistent or significant advantage to using one precondition over the other. <p> Examples 1 and 2 show that when the block bandwidth is small relative to block size, the matrix is symmetric, and the dimension of the upper subspace is small relative to N , the optimal preconditioner in <ref> [13] </ref> can produce solutions with slightly smaller relative error in somewhat fewer iterations than our optimal preconditioner. It is important to remember that both preconditioners were sensitive to the choice of cutoff so finding the optimal preconditioner is difficult in practice. <p> Also, the cost to initialize our preconditioner in Examples 1 and 2 is of the same order of magnitude as the initialization cost of the preconditioner in <ref> [13] </ref>. In short, our preconditioner never performs much worse than the preconditioner in [13] and can perform much better in some cases. 7. Conclusions. We have developed an efficient algorithm for computing regularized solutions to discrete ill-posed problems involving BTTB matrices. <p> Also, the cost to initialize our preconditioner in Examples 1 and 2 is of the same order of magnitude as the initialization cost of the preconditioner in <ref> [13] </ref>. In short, our preconditioner never performs much worse than the preconditioner in [13] and can perform much better in some cases. 7. Conclusions. We have developed an efficient algorithm for computing regularized solutions to discrete ill-posed problems involving BTTB matrices. Our algorithm uses an orthogonal transform to transform the BTTB matrix and its BTCB approximant to Cauchy-like matrices whose blocks are Cauchy-like. <p> Our results indicate that the algorithm is both efficient and practical with the truncation parameters m i chosen using our second heuristic. Finally, the results indicate that our preconditioned method is competitive with the preconditioned method of <ref> [13] </ref> in terms of both the number of iterations required to reach a reasonable regularized solution and the amount of work per iteration. We note that this preconditioner can also be applied in situations where the matrix T is only block Toeplitz and the blocks are not necessarily Toeplitz.
Reference: [14] <author> P. C. Hansen, </author> <title> The discrete Picard condition for discrete ill-posed problems, </title> <journal> BIT, </journal> <volume> 30 (1990), </volume> <pages> pp. </pages> <month> 658-672. </month> <title> [15] , Rank Deficient and Discrete Ill-Posed Problems, </title> <type> PhD thesis, </type> <institution> Technical University of Denmark, </institution> <month> July, </month> <year> 1995. </year> <note> UNIC Report UNIC-95-07. </note>
Reference-contexts: The matrix T has been normalized so that its largest singular value is of order 1. 2. The uncontaminated data vector ^g satisfies the discrete Picard condition; i.e., the spectral coefficients of ^g decay in absolute value faster than the singular values <ref> [27, 14] </ref>. 3. The additive noise is zero-mean white Gaussian. In this case, the components of the error e are independent random variables normally distributed with mean zero and variance * 2 . 4. The noise level, kek 2 k^gk 2 , is strictly less than one.
Reference: [16] <author> P. C. Hansen and D. P. O'Leary, </author> <title> The use of the L-curve in the regularization of discrete ill-posed problems, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14 (1993), </volume> <pages> pp. 1487-1503. </pages>
Reference-contexts: Second, when to stop the CGLS iteration in order to get the best approximate solution is a well-studied but open question (for instance, see <ref> [16] </ref> and the references therein). We do not solve this problem, but we consider other algorithmic issues in the following subsections. 16 5.1. Determining the size of the K (1) ii .
Reference: [17] <author> G. Heinig, </author> <title> Inversion of generalized cauchy matrices and other classes of structured matrices, Linear Algebra in Signal Processing, </title> <journal> IMA Volumes in Math. and Appl., </journal> <volume> 69 (1994), </volume> <pages> pp. 95-114. </pages>
Reference-contexts: To overcome this difficulty, we make use of the fact that Toeplitz matrices are related to Cauchy-like matrices by fast orthogonal transformations <ref> [17, 9, 11] </ref>. The particular Cauchy-like matrices discussed in x2 permit fast matrix-vector multiplication. An advantage of Cauchy-like matrices is that their inverses are also Cauchy-like, unlike Toeplitz matrices whose inverses are not generally Toeplitz. <p> One disadvantage of Toeplitz matrices is that permutations of Toeplitz matrices are not necessarily Toeplitz, so that incorporating pivoting into fast factorization schemes becomes difficult and expensive. However, because of (4), it is easy to show the following (see <ref> [17, 11] </ref>, for example): Property 1. Row and column permutations of Cauchy-like matrices are Cauchy-like, as are leading principal submatrices. This property and the fact that Schur complements of Cauchy-like matrices are Cauchy-like [9] lead to fast algorithms for factoring Cauchy-like matrices which can pivot for stability [9, 11]. <p> For our purposes it was convenient to set D = diag (u 11 ; : : : ; u nn ) and U D 1 U to obtain the equivalent factorization C = P LDU Q. We also exploit the following property of Cauchy-like matrices <ref> [17] </ref>. Property 2. The inverse of a Cauchy-like matrix is Cauchy-like: C 1 = x T i ! j 1i;jn 3 The generators X and W can be determined from the relations [17] CX = A; W T C = B T :(6) Thus, given the LU factorization of C, solving <p> We also exploit the following property of Cauchy-like matrices <ref> [17] </ref>. Property 2. The inverse of a Cauchy-like matrix is Cauchy-like: C 1 = x T i ! j 1i;jn 3 The generators X and W can be determined from the relations [17] CX = A; W T C = B T :(6) Thus, given the LU factorization of C, solving for X and W requires only O (`n 2 ) operations and is stable when C is well-conditioned. <p> The third important property is that Toeplitz matrices also satisfy certain displacement equations [21, 9] which allow them to be transformed via fast Fourier transforms into Cauchy-like matrices <ref> [17, 9] </ref>: Property 3.
Reference: [18] <author> M. R. Hestenes and E. </author> <title> Stiefel, Methods of conjugate gradients for solving linear systems, </title> <institution> J. Res. Natl. Bur. Standards, </institution> <month> 49 </month> <year> (1952), </year> <pages> pp. 409-436. </pages>
Reference-contexts: An appropriate preconditioner will speed convergence to this approximate solution without adding components in the lower subspace. 3.1. Regularization by preconditioned conjugate gradients. The standard conjugate gradient (CG) method <ref> [18] </ref> is an iterative method for solving systems of linear equations for which the matrix is symmetric positive definite. If the matrix is not symmetric positive definite, one can use the CGLS algorithm [18], a variant of standard CG that solves the normal equations in factored form. <p> Regularization by preconditioned conjugate gradients. The standard conjugate gradient (CG) method <ref> [18] </ref> is an iterative method for solving systems of linear equations for which the matrix is symmetric positive definite. If the matrix is not symmetric positive definite, one can use the CGLS algorithm [18], a variant of standard CG that solves the normal equations in factored form. If the discrete Picard condition holds, then CGLS acts as an iterative regularization method with the iteration index taking the role of the regularization parameter [8, 13, 15].
Reference: [19] <author> R. A. Horn and C. R. Johnson, </author> <title> Topics in Matrix Analysis, </title> <publisher> Cambridge University Press, </publisher> <year> 1991. </year>
Reference-contexts: From Theorem 3.3.16 of <ref> [19] </ref> , Applying the same theorem to Equation (14) with 2 i + j N + 1 for N m fl + 1 j N we have i+j1 (I M 1 C) i (M 1 (K C)) + j (M 1 S) 1 (M 1 ) i (H T ) + <p> Lemma 4.4. Given * &gt; 0, let k fl , j fl , U , and V be defined as in Lemma 4.3 with h 2 C 2 . Then i (T H) k Rk 2 *; N nj fl + 1 i N: Proof: Using <ref> [19, lemmas 3.3.16 and 4.2.15] </ref>, i (T H) i ( R U ) + 1 ( R V ) i ( R U ) + k Rk 2 *; i = 1; : : :; N: However, since U has rank j fl , the rank of R U is N
Reference: [20] <author> A. K. Jain, </author> <title> An operator factorization method for restoration of blurred images, </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-26 (1977), </volume> <pages> pp. 1061-1071. </pages>
Reference-contexts: This method achieves a minimum relative error value of 3:49 fi 10 1 in 9 iterations. 6.2. Example 2. As a second image processing example, we consider the BTTB matrix T = T 0 T 0 where T 0 is the 32 fi 32 Toeplitz matrix with diagonals (see <ref> [20] </ref>) h k = &lt; c B L ) B L 0 jkj B w 0 otherwise where c is a normalization constant, B L = 2, and B w , the bandwidth of T 0 , is set to 5.
Reference: [21] <author> T. Kailath, S. Kung, and M. Morf, </author> <title> Displacement ranks of matrices and linear equations, </title> <journal> Journal of Math. Anal. and Appl., </journal> <volume> 78 (1979), </volume> <pages> pp. 395-407. </pages>
Reference-contexts: The third important property is that Toeplitz matrices also satisfy certain displacement equations <ref> [21, 9] </ref> which allow them to be transformed via fast Fourier transforms into Cauchy-like matrices [17, 9]: Property 3.
Reference: [22] <author> M. E. Kilmer and D. P. O'Leary, </author> <title> Pivoted Cauchy-like preconditioners for regularized solution of ill-posed problems, </title> <type> Tech. Report CS-TR-3682, </type> <institution> University of Maryland, College Park, </institution> <year> 1996. </year>
Reference-contexts: This preconditioner is the two-dimensional generalization of the preconditioner for Toeplitz matrices discussed in <ref> [22] </ref>. We begin with a discussion in x2 of Cauchy-like matrices and some of their important properties. We discuss the regularizing properties of conjugate gradients and our choice of preconditioner in x3. In x4 we show that our preconditioner has desirable properties. <p> However, the particular relation in Property 3 can be exploited to determine a O (n lg n) stable algorithm for multiplication by the inverse of the Cauchy-like matrix <ref> [22] </ref>. Property 3 implies that if T is a Toeplitz block matrix, it satisfies (I F )T (I S fl 4 where denotes the Kronecker tensor product and C is Cauchy-like. <p> Following the discussion in <ref> [22] </ref>, j~a k (l) j! k j j (l)T ~ b j j away from the corners and the diagonal of ~ K ll . <p> Applying the preconditioner. Since M 1 is block diagonal, to compute M 1 r requires the p computations K (1)1 ii r i where r i is the length m i subvector of r beginning at index ip + 1. Using Algorithm 2 of <ref> [22] </ref>, we compute each K (1)1 stably in O (n lg n) operations.
Reference: [23] <author> T. Ku and C.-C. J. Kuo, </author> <title> On the spectrum of a family of preconditioned block Toeplitz matrices, </title> <note> SIAM J. </note> <institution> Sci. Stat. Comput., </institution> <month> 13 (July, </month> <year> 1992), </year> <pages> pp. 948-966. </pages>
Reference-contexts: Also, preconditioners for BTTB matrices which are block circulant (BC), circulant block (CB), or block circulant with circulant blocks (BCCB) have been found to be very efficient <ref> [4, 23, 1] </ref>. For example, if the preconditioner is determined to be block Toeplitz with circulant blocks (BTCB), applying the preconditioner can be reduced to solving p systems involving n fi n Toeplitz matrices [4].
Reference: [24] <author> G. W. Stewart and J. Guang Sun, </author> <title> Matrix Perturbation Theory, </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: Proof: Since the matrices in (16) are all Hermitian, we may apply Corollary IV.4.9 and problem 4, page 211, of <ref> [24] </ref> to obtain N (E) + m fl +i (E M ) m fl +i ((M 1 C) fl (M 1 C)) m fl +1 (E M ) + i (E) N (E) + m fl +i (E C ) m fl +i (C fl C) m fl +1 (E C
Reference: [25] <author> D. R. Sweet, </author> <title> The use of pivoting to improve the numerical performance of Toeplitz matrix algorithms, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 14 (1993), </volume> <pages> pp. 468-493. </pages>
Reference-contexts: However, for indefinite and/or ill-conditioned systems, the O (n lg 2 n) and O (n 2 ) factorization algorithms for Toeplitz matrices can be numerically unstable; these algorithms can require as many as O (n 3 ) operations in order to maintain stability <ref> [25, 12, 7] </ref>. To overcome this difficulty, we make use of the fact that Toeplitz matrices are related to Cauchy-like matrices by fast orthogonal transformations [17, 9, 11]. The particular Cauchy-like matrices discussed in x2 permit fast matrix-vector multiplication.
Reference: [26] <author> A. van der Sluis and H. van der Vorst, </author> <title> The rate of convergence of conjugate gradients, </title> <journal> Numer. Math, </journal> <volume> 48 (1986), </volume> <pages> pp. 543-560. </pages>
Reference-contexts: If the discrete Picard condition holds, then CGLS acts as an iterative regularization method with the iteration index taking the role of the regularization parameter [8, 13, 15]. The spread and clustering of the singular values govern the speed and convergence of the algorithm <ref> [26] </ref>. Preconditioning is therefore often applied in an effort to cluster the singular values and thus, to speed convergence. According to (10), we desire that the preconditioner cluster only the large singular values for which j i j j i j.
Reference: [27] <author> J. M. Varah, </author> <title> Pitfalls in the numerical solution of linear ill-posed problems, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 4 (1983), </volume> <pages> pp. 164-176. 28 </pages>
Reference-contexts: The matrix T has been normalized so that its largest singular value is of order 1. 2. The uncontaminated data vector ^g satisfies the discrete Picard condition; i.e., the spectral coefficients of ^g decay in absolute value faster than the singular values <ref> [27, 14] </ref>. 3. The additive noise is zero-mean white Gaussian. In this case, the components of the error e are independent random variables normally distributed with mean zero and variance * 2 . 4. The noise level, kek 2 k^gk 2 , is strictly less than one.
References-found: 25

