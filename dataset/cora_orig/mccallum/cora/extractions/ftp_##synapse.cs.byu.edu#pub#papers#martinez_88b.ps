URL: ftp://synapse.cs.byu.edu/pub/papers/martinez_88b.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Title: Digital Neural Networks  
Author: Tony R. Martinez 
Address: Provo, Utah 84602  
Affiliation: Computer Science Dept., 230 TMCB Brigham Young University,  
Note: In Proceedings of the 1988 IEEE Systems Man and Cybernetics Conference, pp. 681-684, 1988.  To Appear in the Proceedings of the 1988 IEEE International Conference on System Man and Cybernetics  
Abstract: Demands for applications requiring massive parallelism in symbolic environments have given rebirth to research in models labeled as neura l networks. These models are made up of many simple nodes which are highly interconnected such that computation takes place as data flows amongst the nodes of the network. To present, most models have proposed nodes based on simple analog functions, where inputs are multiplied by weights and summed, the total then optionally being transformed by an arbitrary function at the node. Learning in these systems is accomplished by adjusting the weights on the input lines. This paper discusses the use of digital (boolean) nodes as a primitive building block in connectionist systems. Digital nodes naturally engender new paradigms and mechanisms for learning and processing in connectionist networks. The digital nodes are used as the basic building block of a class of models called ASOCS (Adaptive Self-Organizing Concurrent Systems). These models combine massive parallelism with the ability to adapt in a self-organizing fashion. Basic features of standard neural network learning algorithms and those proposed using digital nodes are compared and contrasted. The latter mechanisms can lead to vastly improved efficiency for many applications. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Chang, J. and J. J. Vidal, </author> <title> "Inferencing in Hardware," </title> <booktitle> Proceedings of the MCCUniversity Research Symposium, </booktitle> <address> Austin, TX, </address> <month> (July </month> <year> 1987). </year>
Reference-contexts: In fact, the availability of a maturing field of digital VLSI was one of the initial motivations for starting this work. Fabrication of initial ASOCS test chips is currently underway <ref> [1] </ref>. Conclusion This paper has sought to compare and contrast some of the critical features of learning mechanisms engendered for connectionist architectures by the use of weight summing nodes versus programmable boolean gates. The compared features include limits on learning, speed of learning, and generalization.
Reference: 2. <author> Hinton, G., Sejnowski, T, and D. Ackley, </author> <title> "Boltzmann Machines: Constraint Satisfaction Networks that Learn," </title> <type> Tech. Rep CMU-CS-84-119, </type> <address> CMU, Pittsburgh, PA, </address> <year> (1984). </year>
Reference-contexts: The same topology may or may not solve a given function depending on the initial setting of weights, order of training, or other parameters. Some seek to escape the local minima through simulated annealing or other stochastic mechanisms <ref> [2] </ref>, but convergence to a correct network remains a probabilistic endeavor.
Reference: 3. <author> Hopfield. J. J., and D. W. Tank, </author> <title> "'Neural' Computation of Decisions in Optimization Problems," </title> <journal> Biological Cybernetics, </journal> <volume> (52), </volume> <pages> pp. 141-152, </pages> <year> (1985). </year>
Reference: 4. <author> Martinez, T. R., </author> <title> "Adaptive Self-Organizing Logic Networks," </title> <type> Ph.D. Dissertation, Technical Report - CSD 860093, </type> <institution> University of California, </institution> <address> Los Angeles, CA (May 1986). </address> <month> 8 </month>
Reference: 5. <author> Martinez T. R., </author> <title> "Models of Parallel Adaptive Logic," </title> <booktitle> Proceedings of the 1987 IEEE Systems Man and Cybernetics Conference, </booktitle> <pages> pp. 290-296, </pages> <month> (October, </month> <year> 1987). </year>
Reference-contexts: Because of this, gradient seeking iterative algorithms which make small changes on weights are not natural. A class of models which have the DPLM as the atomic unit has been proposed, with the name of Adaptive Self-Organizing Concurrent Systems (ASOCS) <ref> [5] </ref>. Very briefly, an ASOCS model consists of an arbitrary network of 5 DPLM's, each with two inputs and one output which can connect to multiple nodes allowing both feedforward and feedback mechanisms. During execution the DPLM's process boolean input into boolean outputs.
Reference: 6. <author> Martinez, T. R. and J. J. Vidal, </author> <title> "Adaptive Parallel Logic Networks," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 5, </volume> <year> (1988). </year>
Reference: 7. <author> Nilsson, N., </author> <title> Learning Machines, </title> <publisher> McGraw-Hill, </publisher> <year> (1965). </year>
Reference-contexts: However, the number of linearly separable (LS) functions grows as LS (P,n) = 2 n (P-1-i)!i! for P &gt; n and 2 for P n , where n is the number of inputs and P is the number of patterns to be classified <ref> [7] </ref>. Note that for a general classifier P is the total possible patterns, which equals 2 n for the boolean case.
Reference: 8. <author> Rosenblatt, F. </author> <title> Principles of Neurodynamics, </title> <publisher> Spartan Books, </publisher> <address> Washington, D.C., </address> <year> (1962). </year>
Reference: 9. <editor> Rumelhart, D. and McClelland, J., </editor> <booktitle> Parallel Distributed Processing:, </booktitle> <volume> Vol. I, </volume> <pages> pp. 318-362, </pages> <publisher> MIT Press, </publisher> <year> (1986). </year>
Reference-contexts: Unlearning worsens if one attempts larger weight changes to remedy the first problem of gradual convergence. This all leads to extreme learning times. As an example, training of the simple 2 input XOR function is reported in <ref> [9] </ref> as taking 558 sweeps through the 4 input patterns. In ASOCS, an instance need only be presented to the system one time. Adaptation takes place and convergence is guaranteed. Also, the network continues to correctly discriminate all instances previously presented.
Reference: 10. <author> Verstraete, R. A., </author> <title> "Assignment of Functional Responsibility in Perceptrons," </title> <type> Ph.D. Dissertation, </type> <institution> Computer Science Department, University of California, </institution> <address> Los Angeles, CA, </address> <month> (June </month> <year> 1986). </year>
Reference-contexts: Thus, highly parallel implementations of these networks are possible, where highspeed parallel execution can take place in both the processing and adaptation phases. Digital Nodes and Connectionist Networks As an alternative to the weight summing units, programmable gates which do direct boolean functions on their inputs have been proposed <ref> [10] </ref>. The basic gate in this family of dynamic programmable logic modules (DPLM) is the two input single output implementation. DPLM x 1 x 2 The inputs and outputs are all boolean and the node can be set to any one of the 16 boolean functions of 2 inputs.
Reference: 11. <author> Widrow, B., </author> <title> "Generalization and Information Storage in Networks of Adaline 'Neurons'," </title> <booktitle> Proceedings of the Conference on Self-Organizing Systems, </booktitle> <pages> pp. 435-462, </pages> <year> (1962). </year>
References-found: 11

