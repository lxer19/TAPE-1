URL: http://www.cs.umt.edu/CS/FAC/OPITZ/papers/aaai97.ps
Refering-URL: http://www.cs.umt.edu/CS/FAC/OPITZ/papers/aaai97.html
Root-URL: 
Email: email: rmaclin@d.umn.edu  email: opitz@cs.umt.edu  
Title: An Empirical Evaluation of Bagging and Boosting  
Author: Richard Maclin David Opitz 
Address: Providence, Rhode Island,  Duluth, MN 55812  Missoula, MT 59812  
Affiliation: Artificial Intelligence,  Computer Science Department University of Minnesota-Duluth  Department of Computer Science University of Montana  
Note: Appears in The Fourteenth National Conference on  1997, AAAI Press.  
Abstract: An ensemble consists of a set of independently trained classifiers (such as neural networks or decision trees) whose predictions are combined when classifying novel instances. Previous research has shown that an ensemble as a whole is often more accurate than any of the single classifiers in the ensemble. Bagging (Breiman 1996a) and Boosting (Freund & Schapire 1996) are two relatively new but popular methods for producing ensembles. In this paper we evaluate these methods using both neural networks and decision trees as our classification algorithms. Our results clearly show two important facts. The first is that even though Bagging almost always produces a better classifier than any of its individual component classifiers and is relatively impervious to overfitting, it does not generalize any better than a baseline neural-network ensemble method. The second is that Boosting is a powerful technique that can usually produce better ensembles than Bagging; however, it is more susceptible to noise and can quickly overfit a data set. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Alpaydin, E. </author> <year> 1993. </year> <title> Multiple networks for function learning. </title> <booktitle> In Proceedings of the 1993 IEEE International Conference on Neural Networks, </booktitle> <volume> volume I, </volume> <pages> 27-32. </pages>
Reference: <author> Breiman, L. </author> <year> 1996a. </year> <title> Bagging predictors. </title> <booktitle> Machine Learning 24(2) </booktitle> <pages> 123-140. </pages>
Reference-contexts: The resulting classifier (hereafter referred to as an ensemble) is generally more accurate than any of the individual classifiers making up the ensemble. Two popular methods for creating ensembles are Bagging <ref> (Breiman 1996a) </ref> and Boosting (or Arcing) (Freund & Schapire 1996). These methods rely on "resampling" techniques to obtain different training sets for each of the classifiers. <p> In this paper we concentrate on two popular methods (Bagging and Boosting) that try to generate disagreement among the classifiers by altering the training set each classifier sees. Bagging Classifiers Bagging <ref> (Breiman 1996a) </ref> is a "bootstrap" (Efron & Tibshirani 1993) ensemble method that creates individuals for its ensemble by training each classifier on a random redistribution of the training set. <p> We will also compare Bagging and Boosting to other methods such as Opitz and Shavlik's (1996b) approach to creating an ensemble. This approach uses genetic search to find classifiers that are accurate and differ in their predictions. Conclusions This paper presents an empirical evaluation of Bagging <ref> (Breiman 1996a) </ref> and Boosting (Freund & Schapire 1996) for neural networks and decision trees. Our results demonstrate that a Bagging ensemble nearly always outperforms a single classifier. Our results also show that an Arcing ensemble can greatly outperform both a Bagging ensemble and a single classifier.
Reference: <author> Breiman, L. </author> <year> 1996b. </year> <title> Bias, variance, and arcing classifiers. </title> <type> Technical Report TR 460, </type> <institution> UC-Berkeley, Berkeley, </institution> <address> CA. </address>
Reference-contexts: ensemble that are better able to correctly predict examples for which the current ensemble performance is poor. (Note that in Bagging, the resampling of the training set is not dependent on the performance of the earlier classifiers.) In this work we examine two new and powerful forms of Boosting: Arcing <ref> (Breiman 1996b) </ref> and Ada-Boosting (Freund & Schapire 1996). Like Bagging, these methods choose a training set of size N for classifier K + 1 by probabilistically selecting (with replacement) examples from the original N training examples. <p> The probability p i for selecting example i to be part of classifier K +1's training set is defined as p i = 4 j=1 (1 + m j 4 ) Breiman chose the value of the power (4) empirically after trying several different values <ref> (Breiman 1996b) </ref>. Unlike Ada-Boosting, Arcing combines its classifiers by unweighted voting. Table 1: Summary of the data sets used in this paper.
Reference: <author> Breiman, L. </author> <year> 1996c. </year> <title> Stacked regressions. </title> <booktitle> Machine Learning 24(1) </booktitle> <pages> 49-64. </pages>
Reference: <author> Clemen, R. </author> <year> 1989. </year> <title> Combining forecasts: A review and annotated bibliography. </title> <journal> Journal of Forecasting 5 </journal> <pages> 559-583. </pages>
Reference: <author> Drucker, H., and Cortes, C. </author> <year> 1996. </year> <title> Boosting decision trees. </title> <editor> In Touretsky, D.; Mozer, M.; and Hasselmo, M., eds., </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> 479-485. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Drucker, H.; Cortes, C.; Jackel, L.; LeCun, Y.; and Vap-nik, V. </author> <year> 1994. </year> <title> Boosting and other machine learning algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> 53-61. </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Efron, B., and Tibshirani, R. </author> <year> 1993. </year> <title> An Introduction to the Bootstrap. </title> <address> New York: </address> <publisher> Chapman and Hall. </publisher>
Reference-contexts: In this paper we concentrate on two popular methods (Bagging and Boosting) that try to generate disagreement among the classifiers by altering the training set each classifier sees. Bagging Classifiers Bagging (Breiman 1996a) is a "bootstrap" <ref> (Efron & Tibshirani 1993) </ref> ensemble method that creates individuals for its ensemble by training each classifier on a random redistribution of the training set.
Reference: <author> Freund, Y., and Schapire, R. </author> <year> 1996. </year> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> 148-156. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The resulting classifier (hereafter referred to as an ensemble) is generally more accurate than any of the individual classifiers making up the ensemble. Two popular methods for creating ensembles are Bagging (Breiman 1996a) and Boosting (or Arcing) <ref> (Freund & Schapire 1996) </ref>. These methods rely on "resampling" techniques to obtain different training sets for each of the classifiers. <p> Each individual classifier in the ensemble is generated with a different random sampling of the training set. Boosting Classifiers Boosting <ref> (Freund & Schapire 1996) </ref> encompasses a family of methods. The focus of these methods is to produce a series of classifiers. The training set used for each member of the series is chosen based on the performance of the earlier classifier (s) in the series. <p> able to correctly predict examples for which the current ensemble performance is poor. (Note that in Bagging, the resampling of the training set is not dependent on the performance of the earlier classifiers.) In this work we examine two new and powerful forms of Boosting: Arcing (Breiman 1996b) and Ada-Boosting <ref> (Freund & Schapire 1996) </ref>. Like Bagging, these methods choose a training set of size N for classifier K + 1 by probabilistically selecting (with replacement) examples from the original N training examples. Unlike Bagging, however, the probability of selecting an example is not equal across the training set. <p> This approach uses genetic search to find classifiers that are accurate and differ in their predictions. Conclusions This paper presents an empirical evaluation of Bagging (Breiman 1996a) and Boosting <ref> (Freund & Schapire 1996) </ref> for neural networks and decision trees. Our results demonstrate that a Bagging ensemble nearly always outperforms a single classifier. Our results also show that an Arcing ensemble can greatly outperform both a Bagging ensemble and a single classifier.
Reference: <author> Hansen, L., and Salamon, P. </author> <year> 1990. </year> <title> Neural network ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 12 </journal> <pages> 993-1001. </pages>
Reference: <author> Krogh, A., and Vedelsby, J. </author> <year> 1995. </year> <title> Neural network ensembles, cross validation, and active learning. </title> <editor> In Tesauro, G.; Touretzky, D.; and Leen, T., eds., </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> 231-238. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Lincoln, W., and Skrzypek, J. </author> <year> 1989. </year> <title> Synergy of clustering multiple back propagation networks. </title> <editor> In Touretzky, D., ed., </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> 650-659. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Maclin, R., and Shavlik, J. </author> <year> 1995. </year> <title> Combining the predictions of multiple classifiers: Using competitive learning to initialize neural networks. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 524-530. </pages>
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1994. </year> <title> UCI repository of machine learning databases (machine-readable data repository). </title> <institution> University of California-Irvine, Department of Information and Computer Science. </institution>
Reference-contexts: 19 - 35 134 19 25 40 splice 3190 3 - 60 240 2 25 30 vehicle 846 4 18 - 18 4 10 40 Results To evaluate the performance of Bagging and Boosting we performed experiments on a number of data sets drawn from the UCI data set repository <ref> (Murphy & Aha 1994) </ref>. We report Bagging and Boosting error rates for each data set for both neural network and decision tree ensembles along with the error rate for simply using a single network or single decision tree.
Reference: <author> Opitz, D., and Shavlik, J. </author> <year> 1996a. </year> <title> Actively searching for an effective neural-network ensemble. </title> <booktitle> Connection Science </booktitle> 8(3/4):337-353. 
Reference: <author> Opitz, D., and Shavlik, J. </author> <year> 1996b. </year> <title> Generating accurate and diverse members of a neural-network ensemble. </title> <editor> In Touret-sky, D.; Mozer, M.; and Hasselmo, M., eds., </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> 535-541. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Perrone, M. </author> <year> 1993. </year> <title> Improving Regression Estimation: Averaging Methods for Variance Reduction with Extension to General Convex Measure Optimization. </title> <type> Ph.D. Dissertation, </type> <institution> Brown University, Providence, RI. </institution>
Reference: <author> Quinlan, J. R. </author> <year> 1996. </year> <title> Bagging, boosting, </title> <booktitle> and c4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 725-730. </pages> <publisher> AAAI/MIT Press. </publisher>
Reference: <author> Sollich, P., and Krogh, A. </author> <year> 1996. </year> <title> Learning with ensembles: How over-fitting can be useful. </title> <editor> In Touretsky, D.; Mozer, M.; and Hasselmo, M., eds., </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> 190-196. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: This argument seems especially pertinent to Ada-Boosting for two reasons. The first and most obvious reason is that its method for updating the probabilities may be over-emphasizing noisy examples. The second reason is that the classifiers are combined using weighted voting. Previous work <ref> (Sollich & Krogh 1996) </ref> has shown that optimizing the combining weights can lead to overfitting while an unweighted voting scheme is generally resilient to the problems of overfitting.
Reference: <author> Wolpert, D. </author> <year> 1992. </year> <title> Stacked generalization. </title> <booktitle> Neural Networks 5 </booktitle> <pages> 241-259. </pages>
Reference-contexts: To do this we would try to preserve the benefits of Boosting while preventing overfitting on noisy data sets. We also plan to compare Bagging and Boosting methods to other methods introduced more recently. In particular we intend to examine the use of Stacking <ref> (Wolpert 1992) </ref> as a method of training a combining function, so as to avoid the effect of having to weight classifiers. We will also compare Bagging and Boosting to other methods such as Opitz and Shavlik's (1996b) approach to creating an ensemble.
References-found: 20

