URL: ftp://ftp.cse.unsw.edu.au/pub/doc/papers/UNSW/9301.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/research/tr.html
Root-URL: http://www.cse.unsw.edu.au
Title: Limits on Team Identification of Languages  
Author: Sanjay Jain and Arun Sharma 
Affiliation: SCHOOL OF COMPUTER SCIENCE AND ENGINEERING THE UNIVERSITY OF NEW SOUTH WALES  
Note: Computational  
Abstract: SCS&E Report 9301 February, 1993 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. M. Barzdin. </author> <title> Two theorems on the limiting synthesis of functions. In Theory of Algorithms and Programs, Latvian State University, </title> <journal> Riga, </journal> <volume> 210 </volume> <pages> 82-88, </pages> <year> 1974. </year> <note> In Russian. </note>
Reference-contexts: The class Ex is a set theoretic summary of the capability of single machines to Ex-identify collections of functions. L. Blum and M. Blum [2] and Barzdin <ref> [1] </ref> showed that the class Ex is not closed under union. This result may be viewed as a fundamental limitation on building general purpose devices for learning functions, and, to an extent, justifies the use of heuristic methods in Artificial Intelligence.
Reference: [2] <author> L. Blum and M. Blum. </author> <title> Toward a mathematical theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 28 </volume> <pages> 125-155, </pages> <year> 1975. </year>
Reference-contexts: The class Ex is a set theoretic summary of the capability of single machines to Ex-identify collections of functions. L. Blum and M. Blum <ref> [2] </ref> and Barzdin [1] showed that the class Ex is not closed under union. This result may be viewed as a fundamental limitation on building general purpose devices for learning functions, and, to an extent, justifies the use of heuristic methods in Artificial Intelligence. <p> The next definition introduces Gold's criteria for successful identification of a function. Definition 11 <ref> [15, 2, 6] </ref> Let a 2 N [ fflg. (i) M Ex a -identifies f (written: f 2 Ex a (M)) () (9i j ' i = a f )[M (f )# = i]. (ii) Ex a = fS j (9M)[S Ex a (M)]g: Case and Smith [6] motivate anomalies (or, <p> The a = fl case was introduced by L. Blum and M. Blum <ref> [2] </ref> and the other a &gt; 0 cases were first considered by Case and Smith [6]. 4.3 Language Identification Definition 12 A text T for a language L is a mapping from N into (N [ f#g) such that L is the set of natural numbers in the range of T <p> Clearly, L 2 Team 2 3 TxtEx. Suppose by way of contradiction some machine M TxtEx fl - identifies L. Without loss of generality, assume that M is order independent <ref> [2] </ref>. Then, by the operator recursion theorem [4], there exists a 1-1 increasing, nowhere 0, recursive function p such that W p (i) 's can be described as follows. Enumerate h0; p (0)i and h1; p (1)i in both W p (0) and W p (1) .
Reference: [3] <author> M. Blum. </author> <title> A machine independent theory of the complexity of recursive functions. </title> <journal> Journal of the ACM, </journal> <volume> 14 </volume> <pages> 322-336, </pages> <year> 1967. </year>
Reference-contexts: W i denotes the domain of ' i . W i is, then, the r.e. set/language ( N) accepted by '-program i. We can (and do) also think of i as (coding) a (type 0 [16]) grammar for generating W i . denotes an arbitrary Blum complexity measure <ref> [3] </ref> for '. W i;n denotes the set fx n j i (x) ng. hi; ji stands for an arbitrary computable one to one encoding of all pairs of natural numbers onto N [27].
Reference: [4] <author> J. </author> <title> Case. Periodicity in generations of automata. </title> <journal> Mathematical Systems Theory, </journal> <volume> 8 </volume> <pages> 15-32, </pages> <year> 1974. </year>
Reference-contexts: Clearly, L 2 Team 2 3 TxtEx. Suppose by way of contradiction some machine M TxtEx fl - identifies L. Without loss of generality, assume that M is order independent [2]. Then, by the operator recursion theorem <ref> [4] </ref>, there exists a 1-1 increasing, nowhere 0, recursive function p such that W p (i) 's can be described as follows. Enumerate h0; p (0)i and h1; p (1)i in both W p (0) and W p (1) . <p> Proof. Suppose i; j; k; l; q; i 0 ; k 0 ; j 0 ; l 0 ; i 1 are given as above. Without loss of generality we assume i 0 = i. By a suitably padded version of the operator recursion theorem <ref> [4] </ref> there exists a recursive, 1-1, q 0 such that the sets W q 0 (S 1 ;S 2 ;S 3 ;S 4 ;S 0 2 ;M;x) , may be defined as follows in stages. <p> Proof. Suppose i; j; k; l; q; i 0 ; k 0 ; j 0 ; l 0 ; i 1 are given as above. Without loss of generality we assume i 0 = i. By a suitably padded version of the operator recursion theorem <ref> [4] </ref>, there exists a recursive, 1-1, q 0 such that the sets W q 0 (S 1 ;S 2 ;S 3 ;S 4 ;S 0 2 ;M;x) may be defined as follows. <p> Then by the implicit use of the operator recursion theorem <ref> [4] </ref>, there exists a 1-1, recursive, increasing p such that W p () may be described as follows. Recall that [x 1 : : x 2 ] denotes the set fx j x 1 x x 2 g.
Reference: [5] <author> J. Case and C. Lynes. </author> <title> Machine inductive inference and language identification. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 140 </volume> <pages> 107-115, </pages> <year> 1982. </year>
Reference-contexts: If M (T )#, then M (T ) is defined = the unique i such that ( 1 n)[M (T [n]) = i], otherwise we say that M (T ) diverges (written: M (T )"). Definition 14 <ref> [15, 5, 20] </ref> Let a 2 N [ fflg. (i) M TxtEx a identifies T , [M (T )# and W M (T ) = a content (T )]. (ii) M TxtEx a identifies L (written: L 2 TxtEx a (M)) () M TxtEx a -identifies each text for L. (iii) <p> behind why probabilistic identification is different from team identification for languages by showing that an analog of Pitt's connection holds for language identification if the learning agent is also presented with negative information. 31 Finally we note that results from [22] could be used to show that for TxtBc-identification (see <ref> [5] </ref> for definition), if i &gt; j=2, then Team i j TxtBc = TxtBc . Thus, team inference with respect to TxtBc-identification behaves differently from team inference with respect to TxtEx-identification. A study of probabilistic and team identification for TxtBc-identification on the lines of the present paper is open.
Reference: [6] <author> J. Case and C. Smith. </author> <title> Comparison of identification criteria for machine inductive inference. </title> <journal> Theoretical Computer Science, </journal> <volume> 25 </volume> <pages> 193-220, </pages> <year> 1983. </year>
Reference-contexts: The next definition introduces Gold's criteria for successful identification of a function. Definition 11 <ref> [15, 2, 6] </ref> Let a 2 N [ fflg. (i) M Ex a -identifies f (written: f 2 Ex a (M)) () (9i j ' i = a f )[M (f )# = i]. (ii) Ex a = fS j (9M)[S Ex a (M)]g: Case and Smith [6] motivate anomalies (or, <p> Definition 11 [15, 2, 6] Let a 2 N [ fflg. (i) M Ex a -identifies f (written: f 2 Ex a (M)) () (9i j ' i = a f )[M (f )# = i]. (ii) Ex a = fS j (9M)[S Ex a (M)]g: Case and Smith <ref> [6] </ref> motivate anomalies (or, mistakes) in the final programs in Definition 11 from the fact that physicists sometimes do employ explanations with anomalies. The a = fl case was introduced by L. Blum and M. <p> The a = fl case was introduced by L. Blum and M. Blum [2] and the other a &gt; 0 cases were first considered by Case and Smith <ref> [6] </ref>. 4.3 Language Identification Definition 12 A text T for a language L is a mapping from N into (N [ f#g) such that L is the set of natural numbers in the range of T .
Reference: [7] <author> R. P. Daley, B. Kalyanasundaram, and M. Velauthapillai. </author> <title> Breaking the probability 1/2 barrier in fin-type learning. </title> <booktitle> In Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 203-217. </pages> <editor> A. C. M. </editor> <publisher> Press, </publisher> <year> 1992. </year>
Reference-contexts: For other success ratios, the structure of team language identification is different from finite identification of functions by a team <ref> [9, 11, 10, 31, 17, 8, 7] </ref>. Acknowledgements We would like thank John Case for suggesting this investigation, providing helpful critical comments, and discussing various aspects of this work.
Reference: [8] <author> R. P. Daley, L. Pitt, M. Velauthapillai, and T. </author> <title> Will. Relations between probabilistic and team one-shot learners. </title> <editor> In L. Valiant and M. Warmuth, editors, </editor> <booktitle> Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 228-239. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1991. </year>
Reference-contexts: For other success ratios, the structure of team language identification is different from finite identification of functions by a team <ref> [9, 11, 10, 31, 17, 8, 7] </ref>. Acknowledgements We would like thank John Case for suggesting this investigation, providing helpful critical comments, and discussing various aspects of this work.
Reference: [9] <author> R. Freivalds. </author> <title> Functions computable in the limit by probabilistic machines. </title> <booktitle> Mathematical Foundations of Computer Science, </booktitle> <year> 1975. </year>
Reference-contexts: For other success ratios, the structure of team language identification is different from finite identification of functions by a team <ref> [9, 11, 10, 31, 17, 8, 7] </ref>. Acknowledgements We would like thank John Case for suggesting this investigation, providing helpful critical comments, and discussing various aspects of this work.
Reference: [10] <author> R. Freivalds. </author> <title> Finite identification of general recursive functions by probabilistic strategies. </title> <booktitle> In Proceedings of the Conference on Algebraic, Arithmetic and Categorical Methods in Computation Theory, </booktitle> <pages> pages 138-145. </pages> <address> Akedemie-Verlag, Berlin, </address> <year> 1979. </year> <month> 32 </month>
Reference-contexts: For other success ratios, the structure of team language identification is different from finite identification of functions by a team <ref> [9, 11, 10, 31, 17, 8, 7] </ref>. Acknowledgements We would like thank John Case for suggesting this investigation, providing helpful critical comments, and discussing various aspects of this work.
Reference: [11] <author> R. Freivalds. </author> <title> On the principle capabilities of probabilistic algorithms in inductive inference. </title> <journal> Semiotika Inform, </journal> <volume> 12 </volume> <pages> 137-140, </pages> <year> 1979. </year>
Reference-contexts: For other success ratios, the structure of team language identification is different from finite identification of functions by a team <ref> [9, 11, 10, 31, 17, 8, 7] </ref>. Acknowledgements We would like thank John Case for suggesting this investigation, providing helpful critical comments, and discussing various aspects of this work.
Reference: [12] <author> M. Fulk. </author> <title> A Study of Inductive Inference machines. </title> <type> PhD thesis, </type> <institution> SUNY at Buffalo, </institution> <year> 1985. </year>
Reference-contexts: It can be shown that an alternative formulation in which successful machines in the team are required to be successful on all texts for L is equivalent to our definition in the sense that both formulations yield the same collections of identifiable languages (the reader is directed to Fulk <ref> [12, 13] </ref> for arguments of such equivalences). Probabilistic language identification is the subject of next definition. Again, as was the case with probabilistic function identification, we delay the formal details of probability of identification in the following definition to Section 4.5.
Reference: [13] <author> M. Fulk. </author> <title> Prudence and other conditions on formal language learning. </title> <journal> Information and Computation, </journal> <volume> 85 </volume> <pages> 1-11, </pages> <year> 1990. </year>
Reference-contexts: It can be shown that an alternative formulation in which successful machines in the team are required to be successful on all texts for L is equivalent to our definition in the sense that both formulations yield the same collections of identifiable languages (the reader is directed to Fulk <ref> [12, 13] </ref> for arguments of such equivalences). Probabilistic language identification is the subject of next definition. Again, as was the case with probabilistic function identification, we delay the formal details of probability of identification in the following definition to Section 4.5.
Reference: [14] <author> Gill. </author> <title> Computational complexity of probabilistic turing machines. </title> <journal> SIAM Journal of Computing, </journal> <year> 1977. </year>
Reference-contexts: Probabilistic machines behave very much like computable machines except that every now and then they have the ability to base their actions on the outcome of a random event like a coin flip. (For a discussion of probabilistic Turing machines see Gill <ref> [14] </ref>.) The next definition informally describes probabilistic identification of functions; we delay the formal details of the probability of identification till Section 4.5. Below, P ranges over probabilistic machines.
Reference: [15] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: We say that M converges on f to i just in case, for all but finitely many n, M (f [n]) = i. The following definition is Gold's criterion for successful identification of functions by learning machines. Definition 1 <ref> [15] </ref> (a) M Ex-identifies f just in case M, fed the graph of f , converges to a program index for f . <p> We say that M converges on text T to i just in case for all but finitely many n, M (T [n]) = i. The following definition introduces Gold's criteria for successful identification of languages. Definition 4 <ref> [15] </ref> (a) M TxtEx-identifies a text T just in case M, fed T , converges to a grammar for content (T ). (b) M TxtEx-identifies an r.e. language L just in case M TxtEx-identifies each text for L. <p> SEG denotes the set of all finite initial segments, ff [n] j f 2 R ^ n 2 Ng. Note that f [n] [ f (n; x)g is a new finite initial segment of length n + 1 formed by extending f [n] suitably. Definition 7 <ref> [15] </ref> A function learning machine is an algorithmic device which computes a mapping from SEG into N. The output of a function learning machine M on initial segment f [n], denoted M (f [n]), is interpreted as the index of a program in our fixed acceptable programming system '. <p> The next definition introduces Gold's criteria for successful identification of a function. Definition 11 <ref> [15, 2, 6] </ref> Let a 2 N [ fflg. (i) M Ex a -identifies f (written: f 2 Ex a (M)) () (9i j ' i = a f )[M (f )# = i]. (ii) Ex a = fS j (9M)[S Ex a (M)]g: Case and Smith [6] motivate anomalies (or, <p> If M (T )#, then M (T ) is defined = the unique i such that ( 1 n)[M (T [n]) = i], otherwise we say that M (T ) diverges (written: M (T )"). Definition 14 <ref> [15, 5, 20] </ref> Let a 2 N [ fflg. (i) M TxtEx a identifies T , [M (T )# and W M (T ) = a content (T )]. (ii) M TxtEx a identifies L (written: L 2 TxtEx a (M)) () M TxtEx a -identifies each text for L. (iii) <p> Identification from texts is an abstraction of learning from positive data. Similarly, learning from both positive and negative data can be abstracted as identification from informants. The notion of informants, defined below, was first considered by Gold <ref> [15] </ref>. Definition 21 A text I is called an informant for a language L just in case content (I) = fhx; 1i j x 2 Lg [ fhx; 0i j x 62 Lg. The next definition formalizes identification from informants.
Reference: [16] <author> J. Hopcroft and J. Ullman. </author> <title> Introduction to Automata Theory Languages and Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1979. </year>
Reference-contexts: W i denotes the domain of ' i . W i is, then, the r.e. set/language ( N) accepted by '-program i. We can (and do) also think of i as (coding) a (type 0 <ref> [16] </ref>) grammar for generating W i . denotes an arbitrary Blum complexity measure [3] for '. W i;n denotes the set fx n j i (x) ng. hi; ji stands for an arbitrary computable one to one encoding of all pairs of natural numbers onto N [27].
Reference: [17] <author> S. Jain and A. Sharma. </author> <title> Finite learning by a team. </title> <editor> In M. Fulk and J. Case, editors, </editor> <booktitle> Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <address> Rochester, New York, </address> <pages> pages 163-177. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: We would also like to note that the structure of team language identification is similar to the structure of finite identification (identification without any mind changes) of functions by a team for success ratios 2=3 (see <ref> [17] </ref>). For other success ratios, the structure of team language identification is different from finite identification of functions by a team [9, 11, 10, 31, 17, 8, 7]. Acknowledgements We would like thank John Case for suggesting this investigation, providing helpful critical comments, and discussing various aspects of this work. <p> For other success ratios, the structure of team language identification is different from finite identification of functions by a team <ref> [9, 11, 10, 31, 17, 8, 7] </ref>. Acknowledgements We would like thank John Case for suggesting this investigation, providing helpful critical comments, and discussing various aspects of this work.
Reference: [18] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Aggregating inductive expertise. </title> <journal> Information and Control, </journal> <volume> 70 </volume> <pages> 69-95, </pages> <year> 1986. </year>
Reference-contexts: Team 1 n Ex-identification was investigated by Smith [29, 30] and Team m n Ex-identification was studied by Osherson, Stob, and Weinstein <ref> [18] </ref>. Pitt [21] noticed an interesting connection between 2 Team 1 n Ex-identification and function identification by a single probabilistic machine. <p> ; M 2 ; : : : ; M n g, is said to Team m n TxtEx-identify a text T just in case at least m members in the team TxtEx-identify T . 1 The general case of team function identification was also studied by Osherson, Stob, and Weinstein <ref> [18] </ref>. 3 (b) A team of n machines fM 1 ; M 2 ; : : : ; M n g is said to Team m n TxtEx-identify a language L just in case fM 1 ; M 2 ; : : : ; M n g Team m n TxtEx-identify <p> In the context of function identification, Osherson, Stob, and Weinstein <ref> [18] </ref> and Pitt and Smith [25] have shown that the collections of functions that can be identified by teams with success ratio greater than one-half (that is, a majority of members in the team are required to be successful) are the same as those collections of functions that can be identified <p> Theorem 1 <ref> [18, 25] </ref> (8j; k j j k &gt; 1 2 )(8a)[Team j Surprisingly, an analog of Theorem 1 for language identification holds for success ratio 2=3 as opposed to success ratio 1=2 for function identification. <p> Below, whenever we use a set as an argument to majority we assume the argument to be a multiset. 3 Corollary 1 also appears in Osherson, Stob, and Weinstein <ref> [18] </ref>, and may also be shown using an argument from Pitt [22] about probabilistic language learning. 12 Proof of Theorem 2. Let j; k, and a be as given in the hypothesis of the theorem.
Reference: [19] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems that Learn, An Introduction to Learning Theory for Cognitive and Computer Scientists. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: Also, set operations, [, ", ae, set difference, etc., on teams result in multiset of machines. Definition 15 introduces team identification of functions and Definition 16 introduces team identification of languages. Definition 15 <ref> [30, 19] </ref> Let a 2 N [ fflg and let m; n 2 N + . (a) Let f 2 R.
Reference: [20] <author> D. Osherson and S. Weinstein. </author> <title> Criteria of language learning. </title> <journal> Information and Control, </journal> <volume> 52 </volume> <pages> 123-138, </pages> <year> 1982. </year>
Reference-contexts: If M (T )#, then M (T ) is defined = the unique i such that ( 1 n)[M (T [n]) = i], otherwise we say that M (T ) diverges (written: M (T )"). Definition 14 <ref> [15, 5, 20] </ref> Let a 2 N [ fflg. (i) M TxtEx a identifies T , [M (T )# and W M (T ) = a content (T )]. (ii) M TxtEx a identifies L (written: L 2 TxtEx a (M)) () M TxtEx a -identifies each text for L. (iii)
Reference: [21] <author> L. Pitt. </author> <title> A characterization of probabilistic inference. </title> <booktitle> In Proceedings of the 25th Symposium on the Foundations of Computer Science, </booktitle> <year> 1984. </year>
Reference-contexts: A team is said to identify a language if each member of some nonempty subset of the team identifies the language. Identification of programs for functions from their graph is another extensively studied area in Learning Theory. For this related problem, L. Pitt <ref> [21, 23] </ref> established that team identification is essentially equivalent to identification by a single probabilistic machine. <p> Team 1 n Ex-identification was investigated by Smith [29, 30] and Team m n Ex-identification was studied by Osherson, Stob, and Weinstein [18]. Pitt <ref> [21] </ref> noticed an interesting connection between 2 Team 1 n Ex-identification and function identification by a single probabilistic machine. <p> Below, P ranges over probabilistic machines. Definition 3 <ref> [21, 23] </ref> Let p be such that 0 p 1. (a) P Prob p Ex-identifies f just in case P Ex-identifies f with probability at least p. <p> In this case we say that f 2 Prob p Ex (P). (b) Prob p Ex = fS j (9P)[S Prob p Ex (P)]g. Pitt <ref> [21, 23] </ref>showed that if 1=(n + 1) &lt; p 1=n, then Team 1 n Ex = Prob p Ex. <p> Probabilistic language identification is the subject of next definition. Again, as was the case with probabilistic function identification, we delay the formal details of probability of identification in the following definition to Section 4.5. Definition 6 <ref> [21, 23] </ref> Let 0 p 1. (a) P Prob p TxtEx-identifies L just in case for each text T for L, P TxtEx-identifies T with probability at least p.
Reference: [22] <author> L. Pitt. </author> <title> A characterization of probabilistic inference. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1984. </year>
Reference-contexts: The response of P to input oe not only depends upon oe but also on the outcomes of coin flips performed by P while processing oe. We make these notions precise below; we closely follow Pitt <ref> [22, 23] </ref>. Let N m denote the set f0; 1; 2; : : :; m 1g. <p> However, to be able to compute such a probability, it needs to be established that the set fO j P O Ex a -identifies f g is measurable. This is the subject of next lemma. 10 Lemma 1 <ref> [22, 23] </ref> Let P be a probabilistic machine and let f 2 R. Then fO j P O Ex a -identifies f g is measurable. The following definition, motivated by the above lemma, introduces the probability of function identification. Definition 17 [22, 23] Let f 2 R and P be a <p> is the subject of next lemma. 10 Lemma 1 <ref> [22, 23] </ref> Let P be a probabilistic machine and let f 2 R. Then fO j P O Ex a -identifies f g is measurable. The following definition, motivated by the above lemma, introduces the probability of function identification. Definition 17 [22, 23] Let f 2 R and P be a probabilistic machine equipped with a t-sided coin (t 2). Then, pr 1 t (P Ex a -identifies f ) = pr 1 t (fO j P O Ex a -identifies f g). <p> The next lemma says that we do not sacrifice any learning power by restricting our attention to the investigation of identification by probabilistic machine equipped with only a two-sided coin. Lemma 2 (Adopted from <ref> [22, 23] </ref>) Let t; t 0 &gt; 2. Let P be a probabilistic machine with a t-sided coin. <p> For this reason, we will refer to pr 1 t as simply pr in the sequel. Also, we are at liberty to use whatever value of the number of sides of a coin that is convenient for the presentation at hand. Definition 18 <ref> [22, 23] </ref> Let 0 p 1. (a) P Prob p Ex a identifies f (written: f 2 Prob p Ex a (P)) just in case pr (P Ex a -identifies f ) p. (b) Prob p Ex a = fS R j (9P)[S Prob p Ex a (P)]g. 4.5.2 Probabilistic Language <p> Then, the probability of P TxtEx a -identifying T is taken to be pr 1 t (fO j P O TxtEx a -identifies T g). The next lemma establishes that the set fO j P O TxtEx a -identifies T g is measurable. Lemma 3 <ref> [22] </ref> Let P be a probabilistic machine and let T be a text. Then fO j P O TxtEx a -identifies T g is measurable. The following definition, motivated by the above lemma, introduces probability of identification of a text. Definition 19 [22] Let T be a text and P be <p> Lemma 3 <ref> [22] </ref> Let P be a probabilistic machine and let T be a text. Then fO j P O TxtEx a -identifies T g is measurable. The following definition, motivated by the above lemma, introduces probability of identification of a text. Definition 19 [22] Let T be a text and P be a probabilistic machine equipped with a t-sided coin (t 2). Then, pr 1 t (P TxtEx a -identifies T ) = pr 1 t (fO j P O TxtEx a -identifies T g). <p> Then, pr 1 t (P TxtEx a -identifies T ) = pr 1 t (fO j P O TxtEx a -identifies T g). As in the case of function identification, there is no loss of generality in assuming a two sided coin. Lemma 4 (Adopted from <ref> [22, 23] </ref>) Let t; t 0 &gt; 2. Let P be a probabilistic machine with a t-sided coin. <p> For this reason, we will refer to pr 1 t as simply pr in the sequel. Definition 20 <ref> [22] </ref> Let 0 p 1. (a) P Prob p TxtEx a identifies L (written: L 2 Prob p TxtEx a (P)) just in case for each text T for L pr (P TxtEx a -identifies T ) p. (b) Prob p TxtEx a = fL E j (9P)[L Prob p TxtEx <p> Below, whenever we use a set as an argument to majority we assume the argument to be a multiset. 3 Corollary 1 also appears in Osherson, Stob, and Weinstein [18], and may also be shown using an argument from Pitt <ref> [22] </ref> about probabilistic language learning. 12 Proof of Theorem 2. Let j; k, and a be as given in the hypothesis of the theorem. <p> Theorem 9 <ref> [22, 25] </ref> (8j &gt; 0)[Team j 2j Ex = Team 1 2 Ex]. <p> As a consequence of this result, a direct analog of Pitt's connection <ref> [22] </ref> for function inference does not lift to language learning! Theorem 10 Team 2 4 TxtEx Team 1 2 TxtEx fl 6= ;. Corollary 5 Team j 2j+1 TxtEx Team 1 2 TxtEx fl 6= ;. Proof of Theorem 10. <p> An attempt was made to pinpoint the reason behind why probabilistic identification is different from team identification for languages by showing that an analog of Pitt's connection holds for language identification if the learning agent is also presented with negative information. 31 Finally we note that results from <ref> [22] </ref> could be used to show that for TxtBc-identification (see [5] for definition), if i &gt; j=2, then Team i j TxtBc = TxtBc . Thus, team inference with respect to TxtBc-identification behaves differently from team inference with respect to TxtEx-identification.
Reference: [23] <author> L. Pitt. </author> <title> Probabilistic inductive inference. </title> <journal> Journal of the ACM, </journal> <volume> 36 </volume> <pages> 383-433, </pages> <year> 1989. </year>
Reference-contexts: A team is said to identify a language if each member of some nonempty subset of the team identifies the language. Identification of programs for functions from their graph is another extensively studied area in Learning Theory. For this related problem, L. Pitt <ref> [21, 23] </ref> established that team identification is essentially equivalent to identification by a single probabilistic machine. <p> Below, P ranges over probabilistic machines. Definition 3 <ref> [21, 23] </ref> Let p be such that 0 p 1. (a) P Prob p Ex-identifies f just in case P Ex-identifies f with probability at least p. <p> In this case we say that f 2 Prob p Ex (P). (b) Prob p Ex = fS j (9P)[S Prob p Ex (P)]g. Pitt <ref> [21, 23] </ref>showed that if 1=(n + 1) &lt; p 1=n, then Team 1 n Ex = Prob p Ex. <p> Probabilistic language identification is the subject of next definition. Again, as was the case with probabilistic function identification, we delay the formal details of probability of identification in the following definition to Section 4.5. Definition 6 <ref> [21, 23] </ref> Let 0 p 1. (a) P Prob p TxtEx-identifies L just in case for each text T for L, P TxtEx-identifies T with probability at least p. <p> The response of P to input oe not only depends upon oe but also on the outcomes of coin flips performed by P while processing oe. We make these notions precise below; we closely follow Pitt <ref> [22, 23] </ref>. Let N m denote the set f0; 1; 2; : : :; m 1g. <p> However, to be able to compute such a probability, it needs to be established that the set fO j P O Ex a -identifies f g is measurable. This is the subject of next lemma. 10 Lemma 1 <ref> [22, 23] </ref> Let P be a probabilistic machine and let f 2 R. Then fO j P O Ex a -identifies f g is measurable. The following definition, motivated by the above lemma, introduces the probability of function identification. Definition 17 [22, 23] Let f 2 R and P be a <p> is the subject of next lemma. 10 Lemma 1 <ref> [22, 23] </ref> Let P be a probabilistic machine and let f 2 R. Then fO j P O Ex a -identifies f g is measurable. The following definition, motivated by the above lemma, introduces the probability of function identification. Definition 17 [22, 23] Let f 2 R and P be a probabilistic machine equipped with a t-sided coin (t 2). Then, pr 1 t (P Ex a -identifies f ) = pr 1 t (fO j P O Ex a -identifies f g). <p> The next lemma says that we do not sacrifice any learning power by restricting our attention to the investigation of identification by probabilistic machine equipped with only a two-sided coin. Lemma 2 (Adopted from <ref> [22, 23] </ref>) Let t; t 0 &gt; 2. Let P be a probabilistic machine with a t-sided coin. <p> For this reason, we will refer to pr 1 t as simply pr in the sequel. Also, we are at liberty to use whatever value of the number of sides of a coin that is convenient for the presentation at hand. Definition 18 <ref> [22, 23] </ref> Let 0 p 1. (a) P Prob p Ex a identifies f (written: f 2 Prob p Ex a (P)) just in case pr (P Ex a -identifies f ) p. (b) Prob p Ex a = fS R j (9P)[S Prob p Ex a (P)]g. 4.5.2 Probabilistic Language <p> Then, pr 1 t (P TxtEx a -identifies T ) = pr 1 t (fO j P O TxtEx a -identifies T g). As in the case of function identification, there is no loss of generality in assuming a two sided coin. Lemma 4 (Adopted from <ref> [22, 23] </ref>) Let t; t 0 &gt; 2. Let P be a probabilistic machine with a t-sided coin. <p> for all i j and k &gt; l, there exists a q such that PROP (q; i; j; k; l) (P ROP S (q; i; j; k; l)). 5.3 Team Language Identification with Success Ratio 1 2 In the context of functions, the following result immediately follows from Pitt's connection <ref> [23] </ref> between team function identification and probabilistic function identification. Theorem 9 [22, 25] (8j &gt; 0)[Team j 2j Ex = Team 1 2 Ex]. <p> Corollary 15 Team 3 7 TxtEx Team 1 3 TxtEx. Theorem 28 Team 1 3 TxtEx Team 3 7 TxtEx 6= ;. Proof. Follows from team function hierarchy of Smith [30], (8n 2 N + )[Team 1 n Ex ae Team 1 n+1 Ex], and Pitt's connection for functions <ref> [23] </ref>, (8p j 0 &lt; p 1)(8n)[1=(n + 1) &lt; p 1=n ) Team 1 n Ex = Prob p Ex]. Theorem 29 Team 2 5 TxtEx Team 1 3 TxtEx 6= ;. Proof. By Theorem 10 Team 2 4 TxtEx Team 1 2 TxtEx 6= ;.
Reference: [24] <author> L. Pitt and C. Smith. </author> <title> Probability and plurality for aggregations of learning machines. </title> <booktitle> In Proceedings of the 14th International Colloquium on Automata, Languages and Programming, </booktitle> <year> 1987. </year>
Reference-contexts: Using the above connection, Pitt and Smith <ref> [24, 25] </ref> studied the general case of Team m n Ex-identification 1 in which the criterion of success requires at least m out of n machines to be successful.
Reference: [25] <author> L. Pitt and C. Smith. </author> <title> Probability and plurality for aggregations of learning machines. </title> <journal> Information and Computation, </journal> <volume> 77 </volume> <pages> 77-92, </pages> <year> 1988. </year>
Reference-contexts: Using the above connection, Pitt and Smith <ref> [24, 25] </ref> studied the general case of Team m n Ex-identification 1 in which the criterion of success requires at least m out of n machines to be successful. <p> Below, we discuss some of these unexpected results. In the context of function identification, we have the following result immediately following from the results of Pitt and Smith <ref> [25] </ref>. <p> In the context of function identification, Osherson, Stob, and Weinstein [18] and Pitt and Smith <ref> [25] </ref> have shown that the collections of functions that can be identified by teams with success ratio greater than one-half (that is, a majority of members in the team are required to be successful) are the same as those collections of functions that can be identified by a single machine. <p> Theorem 1 <ref> [18, 25] </ref> (8j; k j j k &gt; 1 2 )(8a)[Team j Surprisingly, an analog of Theorem 1 for language identification holds for success ratio 2=3 as opposed to success ratio 1=2 for function identification. <p> Theorem 9 <ref> [22, 25] </ref> (8j &gt; 0)[Team j 2j Ex = Team 1 2 Ex].
Reference: [26] <author> H. Rogers. </author> <title> Godel numberings of partial recursive functions. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 23 </volume> <pages> 331-341, </pages> <year> 1958. </year>
Reference-contexts: E denotes the class of all recursively enumerable languages. L, with or without decorations, ranges over E. L, with or without decorations, ranges over subsets of E. ' denotes a standard acceptable programming system (also referred to as standard acceptable numbering) <ref> [26, 27] </ref>. ' i denotes the partial recursive function computed by the i th program in the standard acceptable programming system '. W i denotes the domain of ' i . W i is, then, the r.e. set/language ( N) accepted by '-program i.
Reference: [27] <author> H. Rogers. </author> <title> Theory of Recursive Functions and Effective Computability. </title> <publisher> McGraw Hill, </publisher> <address> New York, 1967. </address> <publisher> Reprinted by MIT Press, </publisher> <address> Cambridge, Massachusetts in 1987. </address>
Reference-contexts: In no way are these issues trivial; we simply don't have a formal handle on them at this stage. 3 Notation Recursion-theoretic concepts not explained below are treated in <ref> [27] </ref>. N denotes the set of natural numbers, f0; 1; 2; : : :g. <p> E denotes the class of all recursively enumerable languages. L, with or without decorations, ranges over E. L, with or without decorations, ranges over subsets of E. ' denotes a standard acceptable programming system (also referred to as standard acceptable numbering) <ref> [26, 27] </ref>. ' i denotes the partial recursive function computed by the i th program in the standard acceptable programming system '. W i denotes the domain of ' i . W i is, then, the r.e. set/language ( N) accepted by '-program i. <p> W i;n denotes the set fx n j i (x) ng. hi; ji stands for an arbitrary computable one to one encoding of all pairs of natural numbers onto N <ref> [27] </ref>. Corresponding projection functions are 1 and 2 . (8i; j 2 N) [ 1 (hi; ji) = i and 2 (hi; ji) = j and h 1 (x); 2 (x)i = x ].
Reference: [28] <author> H. Rogers. </author> <title> Theory of Recursive Functions and Effective Computability. </title> <publisher> McGraw Hill, </publisher> <address> New York, 1967. </address> <publisher> Reprinted, MIT Press 1987. </publisher>
Reference-contexts: We define grammar majority (A m ) as follows: W majority (A m ) = fx j for majority of g 2 A m ; x 2 W g g. Clearly, majority (A m ) can be defined using the s-m-n theorem <ref> [28] </ref>. Intuitively, majority (A m ) is a grammar for a language that consists of all such elements that are enumerated by a majority of grammars in A m .
Reference: [29] <author> C. Smith. </author> <title> The power of parallelism for automatic program systhesis. </title> <booktitle> In Proceedings of the 22nd Symposium on the Foundations of Computer Science, </booktitle> <year> 1981. </year> <month> 33 </month>
Reference-contexts: The idea of team identification for functions was first suggested by J. Case and extensively studied by Smith <ref> [29, 30] </ref>. The next definition describes team identification of functions. Recall that a team of machines is essentially a multiset of machines. <p> Team 1 n Ex-identification was investigated by Smith <ref> [29, 30] </ref> and Team m n Ex-identification was studied by Osherson, Stob, and Weinstein [18]. Pitt [21] noticed an interesting connection between 2 Team 1 n Ex-identification and function identification by a single probabilistic machine.
Reference: [30] <author> C. Smith. </author> <title> The power of pluralism for automatic program synthesis. </title> <journal> Journal of the ACM, </journal> <volume> 29 </volume> <pages> 1144-1165, </pages> <year> 1982. </year>
Reference-contexts: The idea of team identification for functions was first suggested by J. Case and extensively studied by Smith <ref> [29, 30] </ref>. The next definition describes team identification of functions. Recall that a team of machines is essentially a multiset of machines. <p> Team 1 n Ex-identification was investigated by Smith <ref> [29, 30] </ref> and Team m n Ex-identification was studied by Osherson, Stob, and Weinstein [18]. Pitt [21] noticed an interesting connection between 2 Team 1 n Ex-identification and function identification by a single probabilistic machine. <p> Also, set operations, [, ", ae, set difference, etc., on teams result in multiset of machines. Definition 15 introduces team identification of functions and Definition 16 introduces team identification of languages. Definition 15 <ref> [30, 19] </ref> Let a 2 N [ fflg and let m; n 2 N + . (a) Let f 2 R. <p> Corollary 15 Team 3 7 TxtEx Team 1 3 TxtEx. Theorem 28 Team 1 3 TxtEx Team 3 7 TxtEx 6= ;. Proof. Follows from team function hierarchy of Smith <ref> [30] </ref>, (8n 2 N + )[Team 1 n Ex ae Team 1 n+1 Ex], and Pitt's connection for functions [23], (8p j 0 &lt; p 1)(8n)[1=(n + 1) &lt; p 1=n ) Team 1 n Ex = Prob p Ex].
Reference: [31] <author> M. Velauthapillai. </author> <title> Inductive inference with bounded number of mind changes. </title> <booktitle> In Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 200-213, </pages> <year> 1989. </year> <month> 34 </month>
Reference-contexts: For other success ratios, the structure of team language identification is different from finite identification of functions by a team <ref> [9, 11, 10, 31, 17, 8, 7] </ref>. Acknowledgements We would like thank John Case for suggesting this investigation, providing helpful critical comments, and discussing various aspects of this work.
References-found: 31

