URL: ftp://ftp.cc.gatech.edu/pub/people/panesar/PADS-97.ps.Z
Refering-URL: http://www.cs.gatech.edu/grads/p/Kiran.Panesar/homepage.html
Root-URL: 
Title: Adaptive Flow Control in Time Warp shared memory multiprocessors that uses dynamically sized pools of
Author: Kiran S. Panesar Richard M. Fujimoto 
Note: for  
Address: Hillsboro OR 97124 Atlanta GA 30332  
Affiliation: Intel Corporation College of Computing 5200 NE Elam Young Parkway Georgia Institute of Technology  
Abstract: It is well known that Time Warp may suffer from poor performance due to excessive rollbacks caused by overly optimistic execution. Here we present a simple flow control mechanism using only local information and GVT that limits the number of uncommitted messages generated by a processor, thus throttling overly optimistic TW execution. The flow control scheme is analogous to traditional networking flow control mechanisms. A "window" of messages defines the maximum number of uncommitted messages allowed to be scheduled by a process. Committing messages is analogous to acknowledgments in networking flow control. The initial size of the window is calculated using a simple analytical model that estimates the instantaneous number of messages that a process will eventually commit. This window is expanded so that the process may progress up to the next commit point (generally the next fossil collection), and to accommodate optimistic execution. The expansions to the window are based on monitoring TW performance statistics so the window size automatically adapts to changing program behaviors. The flow control technique presented here is simple and fully automatic. No global knowledge or synchronization (other than GVT) is required. We also develop an implementation of the flow control scheme 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Ball and S. Hoyt. </author> <title> The adaptive Time-Warp con-currency control algorithm. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> volume 22, </volume> <pages> pages 174-177. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan-uary </month> <year> 1990. </year>
Reference-contexts: The blocking based protocols adapt the protocol to a point in between conservative and unconstrained optimistic processing. Instead of optimistically processing an event an LP is blocked. The blocking interval is based either on statistical estimates of the next event arrival <ref> [4, 1] </ref>, computing tradeoffs between rollback and blocking [5], global system state (NPSI [23]), or probabilistically [6]. The Local Adaptive Protocol [10] computes real time blocking windows based on previous LVT increments, but is prone to deadlock. Another class of adaptive schemes use memory management for optimism control.
Reference: [2] <author> S. Bellenot. </author> <title> Performance of a riskfree time warp op erating system. </title> <booktitle> In 7 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 155-158. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Broadly, there are two classes of optimism control schemes: non-adaptive and adaptive. System parameters, e.g. window sizes remain static in non-adaptive schemes, where as they are dynamic in the adaptive schemes. A number of non-adaptive schemes have been proposed <ref> [22, 24, 25, 19, 20, 2, 21, 17, 16] </ref>. These schemes typically leave the specification of parameters to the user, or rely on heuristics to statically set their parameters at the beginning of the simulation. The optimal parameter settings are in general dependent on the application.
Reference: [3] <author> S. R. Das and R. M. Fujimoto. </author> <title> An adaptive memory management protocol for Time Warp parallel simulation. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 201-210, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The Local Adaptive Protocol [10] computes real time blocking windows based on previous LVT increments, but is prone to deadlock. Another class of adaptive schemes use memory management for optimism control. Das's adaptive memory scheme <ref> [3] </ref> (AMM) based on Cancelback [12] rolls back LPs to reclaim event memory. The scheme adaptively allocates enough memory to balance cancelback and rollback overheads. AMM requires a global pool of memory. It also requires periodic global synchronization to implement cancelback, an expensive operation especially on distributed memory machines. <p> Increas ing window size therefore requires a larger working set in memory. This causes larger page management and TLB overheads in the operating system, leading to slower execution. Similar results were reported by Das et.al <ref> [3] </ref>. Other applications, e.g. PCS (section 7.1) also demonstrate similar behavior (figure 7). The question addressed by this paper is how to estimate a window size that is best for overall TW performance.
Reference: [4] <author> A. Ferscha. </author> <title> Probabilistic adaptive direct optimism control in Time Warp. </title> <booktitle> In Proceedings of the 9th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 120-129, </pages> <year> 1995. </year>
Reference-contexts: The blocking based protocols adapt the protocol to a point in between conservative and unconstrained optimistic processing. Instead of optimistically processing an event an LP is blocked. The blocking interval is based either on statistical estimates of the next event arrival <ref> [4, 1] </ref>, computing tradeoffs between rollback and blocking [5], global system state (NPSI [23]), or probabilistically [6]. The Local Adaptive Protocol [10] computes real time blocking windows based on previous LVT increments, but is prone to deadlock. Another class of adaptive schemes use memory management for optimism control. <p> The scheme adaptively allocates enough memory to balance cancelback and rollback overheads. AMM requires a global pool of memory. It also requires periodic global synchronization to implement cancelback, an expensive operation especially on distributed memory machines. The schemes surveyed here either require global knowledge (NPSI), relatively complex statistical models <ref> [4] </ref>, or global synchronization (AMM). In contrast, we have developed a simple, fully automatic flow control technique based on an analytic model that only utilizes local information and GVT. No global knowledge or synchronization is required other than the GVT computation, as flow control executes independently on each processor.
Reference: [5] <author> Alois Ferscha and Johannes Luthi. </author> <title> Estimating rollback overhead for optimism control in time warp. </title> <booktitle> In Proceedings of the IEEE Annual Simulation Symposium, </booktitle> <pages> pages 2-12, </pages> <year> 1995. </year>
Reference-contexts: The blocking based protocols adapt the protocol to a point in between conservative and unconstrained optimistic processing. Instead of optimistically processing an event an LP is blocked. The blocking interval is based either on statistical estimates of the next event arrival [4, 1], computing tradeoffs between rollback and blocking <ref> [5] </ref>, global system state (NPSI [23]), or probabilistically [6]. The Local Adaptive Protocol [10] computes real time blocking windows based on previous LVT increments, but is prone to deadlock. Another class of adaptive schemes use memory management for optimism control.
Reference: [6] <author> Alois Ferscha and SK Tripathi. </author> <title> Parallel and distributed simulation of discrete event systems. </title> <type> Technical Report CS-TR-3336, </type> <institution> Computer Science Department, University of Maryland College Park, </institution> <year> 1994. </year>
Reference-contexts: Instead of optimistically processing an event an LP is blocked. The blocking interval is based either on statistical estimates of the next event arrival [4, 1], computing tradeoffs between rollback and blocking [5], global system state (NPSI [23]), or probabilistically <ref> [6] </ref>. The Local Adaptive Protocol [10] computes real time blocking windows based on previous LVT increments, but is prone to deadlock. Another class of adaptive schemes use memory management for optimism control. Das's adaptive memory scheme [3] (AMM) based on Cancelback [12] rolls back LPs to reclaim event memory.
Reference: [7] <author> R. M. Fujimoto. </author> <title> Parallel discrete event simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Numerous variations of Time Warp have been proposed that attempt to reduce the amount of rolled back computation that may occur. Surveys of methods in this regard are described in <ref> [7, 18] </ref>. Broadly, there are two classes of optimism control schemes: non-adaptive and adaptive. System parameters, e.g. window sizes remain static in non-adaptive schemes, where as they are dynamic in the adaptive schemes. A number of non-adaptive schemes have been proposed [22, 24, 25, 19, 20, 2, 21, 17, 16].
Reference: [8] <author> R. M. Fujimoto. </author> <title> Performance of Time Warp under synthetic workloads. </title> <booktitle> In Proceedings of the SCS Mul-ticonference on Distributed Simulation, </booktitle> <volume> volume 22, </volume> <pages> pages 23-28. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> January </month> <year> 1990. </year>
Reference-contexts: As PCS is already a balanced workload, the two schemes demonstrate similar average rollback lengths, and percent of events rolled back (not plotted here). The gains for the flow control scheme are from smaller working sets in memory. 7.2 PHold PHold (Parallel Hold <ref> [8] </ref>) is a synthetic workload that models a closed queuing network with a fixed number of servers. Jobs circulate through the network, getting service at various servers in turn. The service time at each server is selected from one of many stochastic functions (exponential, uniform, deterministic etc.).
Reference: [9] <author> R. M. Fujimoto and K. Panesar. </author> <title> Buffer management in shared memory time warp systems. </title> <booktitle> In 9 th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 149-156, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Alternately, memory allocated for a communicating pair of processors can be used as the execution window. Our implementation uses the partitioned pools framework presented in <ref> [9] </ref> to control memory available to communicating pairs of processors. We present experimental evidence that the estimation models accurately predict window sizes, and quickly adapt to dynamic application characteristics. We also show that our mechanism for implementing the windows using partitioned memory pools [9] on a shared-memory multiprocessors has high performance <p> uses the partitioned pools framework presented in <ref> [9] </ref> to control memory available to communicating pairs of processors. We present experimental evidence that the estimation models accurately predict window sizes, and quickly adapt to dynamic application characteristics. We also show that our mechanism for implementing the windows using partitioned memory pools [9] on a shared-memory multiprocessors has high performance for the workloads that were tested. <p> Each message send requires memory to hold the header and data fields of the event. One implementation of the windowing algorithm is to control the amount of memory allocated to each processor pair for message sends. Our windowing mechanism is implemented using the partitioned pools framework presented in <ref> [9] </ref>. It was shown that allocating memory to a pair of communicating processors results in better performance than other memory management schemes (sender and receiver pools) due to improved spatial locality. The memory allocated to a pair of communicating processes is called a memory pool. <p> This happens when LPs consume all the memory available to them. In our implementation, sending a message changes the ownership of the memory associated with the message from the sender to the receiver (for a complete discussion of the memory management implementation see <ref> [9] </ref>). Memory is released and reassigned among processors during fossil collections. Consider the following scenario. LP B sends more messages to LP A than it receives from A. This causes A to accumulate more memory than B.
Reference: [10] <author> D. O. Hamnes and A. Tripathi. </author> <title> Evaluation of a local adaptive protocol for distributed discrete event simulation. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <pages> pages III:127-134, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Instead of optimistically processing an event an LP is blocked. The blocking interval is based either on statistical estimates of the next event arrival [4, 1], computing tradeoffs between rollback and blocking [5], global system state (NPSI [23]), or probabilistically [6]. The Local Adaptive Protocol <ref> [10] </ref> computes real time blocking windows based on previous LVT increments, but is prone to deadlock. Another class of adaptive schemes use memory management for optimism control. Das's adaptive memory scheme [3] (AMM) based on Cancelback [12] rolls back LPs to reclaim event memory.
Reference: [11] <author> D. R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: 1 Introduction 1.1 Time Warp Time Warp is a well known parallel discrete synchronization protocol that detects out-of-order executions of events as they occur, and recovers using a rollback mechanism <ref> [11] </ref>. It is well known that Time Warp may suffer from long rollbacks due to overly optimistic execution. Depending on the cost and frequency of rollback, the rollback overheads may dominate processing time.
Reference: [12] <author> D. R. Jefferson. </author> <title> Virtual time II: Storage management in distributed simulation. </title> <booktitle> In Proceedings of the Ninth Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 75-89, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: The Local Adaptive Protocol [10] computes real time blocking windows based on previous LVT increments, but is prone to deadlock. Another class of adaptive schemes use memory management for optimism control. Das's adaptive memory scheme [3] (AMM) based on Cancelback <ref> [12] </ref> rolls back LPs to reclaim event memory. The scheme adaptively allocates enough memory to balance cancelback and rollback overheads. AMM requires a global pool of memory. It also requires periodic global synchronization to implement cancelback, an expensive operation especially on distributed memory machines.
Reference: [13] <author> Peter J. B. King. </author> <title> Computer and Communications Systems Performance Modelling. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: The number of buffers required by an LP, i.e. the sequential window of a connection is analogous to the average number of servers in the queuing model. Since there is no restriction on the distribution of arrival rate or service times, the queue is a G=G=1 queue. From <ref> [13] </ref>, the average number of active servers W s is (2) where is the average arrival rate (the average rate of message sends), and 1= is the average service time (the average lifetime of a buffer).
Reference: [14] <author> Y-B. Lin and E. D. Lazowska. </author> <title> Determining the global virtual time in a distributed simulation. </title> <type> Technical Report 90-01-02, </type> <institution> Dept. of Computer Science, University of Washington, </institution> <address> Seattle, Washington, </address> <year> 1989. </year>
Reference-contexts: In figure 1 the best window size is 7 times the sequential window. The sequential window size is computed using a simple queuing model described later. As the simulation progresses, the latest value of GVT is required to commit events. There is a significant overhead associated with computing GVT <ref> [14, 27] </ref>. The computation involves taking a global minimum of all the unprocessed events in the system, and requires information from all processors. Therefore, GVT is computed periodically instead of continuously 1 . A process has uncommitted events that would have been committed if GVT was calculated more frequently.
Reference: [15] <author> B. D. Lubachevsky, A. Shwartz, and A. Weiss. </author> <title> Rollback sometimes works ... if filtered. </title> <booktitle> In 1989 Winter Simulation Conference Proceedings, </booktitle> <pages> pages 630-639, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: We also show that our mechanism for implementing the windows using partitioned memory pools [9] on a shared-memory multiprocessors has high performance for the workloads that were tested. The last section presents experiments on the Echo application from <ref> [15] </ref> illustrating that our flow control scheme effectively controls overly optimistic processes. 2 On Window Sizes An execution window size is defined as the maximum number of messages that an LP can have pending. <p> More dramatic gains are expected in throttling workloads that are created to highlight unthrot-tled TWs adverse rollback behavior. One such widely cited workload, "Echo" due to Lubachevsky <ref> [15] </ref>, exhibits rollbacks of increasing length as simulation time increases, and is a widely cited benchmark in TW stability studies. 8.1 Echo Workload Echo exhibits rollbacks of increasing length as simulated time increases. <p> The Echo workload described in the next paragraph is a slight variation of the original Echo simulation described in <ref> [15] </ref>. In Echo, there are 2 TW processes, A and B, that alternately rollback and compute forward. Initially, LP A makes progress while the second LP, B, waits. After a delay LP B sends a message to the LP A, rolling A back.
Reference: [16] <author> V. Madisetti, D. A. Hardaker, and R. M. Fujimoto. </author> <title> The MIMDIX operating system for parallel simulation. </title> <booktitle> Proceedings of the SCS Multiconference on Parallel and Distributed Simulation, </booktitle> <volume> 24(3) </volume> <pages> 65-74, </pages> <month> Jan-uary </month> <year> 1992. </year>
Reference-contexts: Broadly, there are two classes of optimism control schemes: non-adaptive and adaptive. System parameters, e.g. window sizes remain static in non-adaptive schemes, where as they are dynamic in the adaptive schemes. A number of non-adaptive schemes have been proposed <ref> [22, 24, 25, 19, 20, 2, 21, 17, 16] </ref>. These schemes typically leave the specification of parameters to the user, or rely on heuristics to statically set their parameters at the beginning of the simulation. The optimal parameter settings are in general dependent on the application.
Reference: [17] <author> V. Madisetti, J. Walrand, and D. Messerschmitt. Wolf: </author> <title> A rollback algorithm for optimistic distributed simulation systems. </title> <booktitle> In 1988 Winter Simulation Conference Proceedings, </booktitle> <pages> pages 296-305, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: Broadly, there are two classes of optimism control schemes: non-adaptive and adaptive. System parameters, e.g. window sizes remain static in non-adaptive schemes, where as they are dynamic in the adaptive schemes. A number of non-adaptive schemes have been proposed <ref> [22, 24, 25, 19, 20, 2, 21, 17, 16] </ref>. These schemes typically leave the specification of parameters to the user, or rely on heuristics to statically set their parameters at the beginning of the simulation. The optimal parameter settings are in general dependent on the application.
Reference: [18] <author> D. M. Nicol and R. M. Fujimoto. </author> <title> Parallel simulation today. </title> <journal> Annals of Operations Research, </journal> <volume> 53 </volume> <pages> 249-286, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Numerous variations of Time Warp have been proposed that attempt to reduce the amount of rolled back computation that may occur. Surveys of methods in this regard are described in <ref> [7, 18] </ref>. Broadly, there are two classes of optimism control schemes: non-adaptive and adaptive. System parameters, e.g. window sizes remain static in non-adaptive schemes, where as they are dynamic in the adaptive schemes. A number of non-adaptive schemes have been proposed [22, 24, 25, 19, 20, 2, 21, 17, 16].
Reference: [19] <author> Avinash C. Palaniswamy and Philip A. Wilsey. </author> <title> Scheduling time warp processes using adaptive control techniques. </title> <booktitle> In Winter Simulation Conference Proceedings, </booktitle> <pages> pages 731-738, </pages> <year> 1994. </year>
Reference-contexts: Broadly, there are two classes of optimism control schemes: non-adaptive and adaptive. System parameters, e.g. window sizes remain static in non-adaptive schemes, where as they are dynamic in the adaptive schemes. A number of non-adaptive schemes have been proposed <ref> [22, 24, 25, 19, 20, 2, 21, 17, 16] </ref>. These schemes typically leave the specification of parameters to the user, or rely on heuristics to statically set their parameters at the beginning of the simulation. The optimal parameter settings are in general dependent on the application.
Reference: [20] <author> H. Rajaei, R. Ayani, and L-E. Thorelli. </author> <title> The local time warp approach to parallel simulation. </title> <booktitle> In 7 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 119-126. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Broadly, there are two classes of optimism control schemes: non-adaptive and adaptive. System parameters, e.g. window sizes remain static in non-adaptive schemes, where as they are dynamic in the adaptive schemes. A number of non-adaptive schemes have been proposed <ref> [22, 24, 25, 19, 20, 2, 21, 17, 16] </ref>. These schemes typically leave the specification of parameters to the user, or rely on heuristics to statically set their parameters at the beginning of the simulation. The optimal parameter settings are in general dependent on the application.
Reference: [21] <author> P. L. Reiher, F. Wieland, and D. R. Jefferson. </author> <title> Limitation of optimism in the Time Warp Operating System. </title> <booktitle> In 1989 Winter Simulation Conference Proceedings, </booktitle> <pages> pages 765-770, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Broadly, there are two classes of optimism control schemes: non-adaptive and adaptive. System parameters, e.g. window sizes remain static in non-adaptive schemes, where as they are dynamic in the adaptive schemes. A number of non-adaptive schemes have been proposed <ref> [22, 24, 25, 19, 20, 2, 21, 17, 16] </ref>. These schemes typically leave the specification of parameters to the user, or rely on heuristics to statically set their parameters at the beginning of the simulation. The optimal parameter settings are in general dependent on the application.
Reference: [22] <author> L. M. Sokol, D. P. Briscoe, and A. P. Wieland. MTW: </author> <title> a strategy for scheduling discrete simulation events for concurrent execution. </title> <booktitle> In Proceedings of the SCS Mul-ticonference on Distributed Simulation, </booktitle> <volume> volume 19, </volume> <pages> pages 34-42. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: Broadly, there are two classes of optimism control schemes: non-adaptive and adaptive. System parameters, e.g. window sizes remain static in non-adaptive schemes, where as they are dynamic in the adaptive schemes. A number of non-adaptive schemes have been proposed <ref> [22, 24, 25, 19, 20, 2, 21, 17, 16] </ref>. These schemes typically leave the specification of parameters to the user, or rely on heuristics to statically set their parameters at the beginning of the simulation. The optimal parameter settings are in general dependent on the application.
Reference: [23] <author> S. Srinivasan and P. F. Reynolds. </author> <title> NPSI adaptive synchronization algorithms for PDES. </title> <booktitle> In 1995 Winter Simulation Proceedings, </booktitle> <pages> pages 658-665. </pages>
Reference-contexts: Instead of optimistically processing an event an LP is blocked. The blocking interval is based either on statistical estimates of the next event arrival [4, 1], computing tradeoffs between rollback and blocking [5], global system state (NPSI <ref> [23] </ref>), or probabilistically [6]. The Local Adaptive Protocol [10] computes real time blocking windows based on previous LVT increments, but is prone to deadlock. Another class of adaptive schemes use memory management for optimism control.
Reference: [24] <author> J. Steinman. </author> <title> Speedes:an approach to parallel simu lation. </title> <booktitle> In 6 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 24, </volume> <pages> pages 75-84. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: Broadly, there are two classes of optimism control schemes: non-adaptive and adaptive. System parameters, e.g. window sizes remain static in non-adaptive schemes, where as they are dynamic in the adaptive schemes. A number of non-adaptive schemes have been proposed <ref> [22, 24, 25, 19, 20, 2, 21, 17, 16] </ref>. These schemes typically leave the specification of parameters to the user, or rely on heuristics to statically set their parameters at the beginning of the simulation. The optimal parameter settings are in general dependent on the application.
Reference: [25] <author> J. S. Steinman. </author> <title> Breathing time warp. </title> <booktitle> In 7 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 109-118. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Broadly, there are two classes of optimism control schemes: non-adaptive and adaptive. System parameters, e.g. window sizes remain static in non-adaptive schemes, where as they are dynamic in the adaptive schemes. A number of non-adaptive schemes have been proposed <ref> [22, 24, 25, 19, 20, 2, 21, 17, 16] </ref>. These schemes typically leave the specification of parameters to the user, or rely on heuristics to statically set their parameters at the beginning of the simulation. The optimal parameter settings are in general dependent on the application.
Reference: [26] <author> Andrew S. Tanenbaum. </author> <title> Computer Networks. </title> <publisher> Prentice-Hall, </publisher> <address> 2 edition, </address> <year> 1990. </year>
Reference-contexts: to control execution, this approach based on monitoring scheduled events avoids cyclic dependencies between the control mechanism and the metrics on which it is based. 1.2 Flow Control in Networks In computer networks, a sender running on a fast or lightly loaded machine may swamp a slow receiver with packets <ref> [26] </ref>. This problem is solved by slowing down the sender by not allowing it to send packets until the receiver is ready to receive them. There are two concepts in flow control. First is the idea of a window, and the second is feedback. <p> The receiver moves the current window forward via acknowledgments, allowing the sender to progress. This mechanism improves the throughput of the network. Our approach to overly optimistic processes is analogous to traditional flow control mechanisms used in networks <ref> [26] </ref>. We estimate the number of scheduled uncommitted messages for each processor and use these estimates to define a moving window that defines an upper bound on the number of uncommit ted messages an LP can have scheduled at one time. Advancing GVT commits messages, therefore allowing more message sends.
Reference: [27] <author> Zhonge Xiao, John Cleary, Fabian Gomes, and Brian Unger. </author> <title> A fast asynchronous continuous gvt algorithm for shared memory multiprocessor architectures. </title> <booktitle> In 9 th Workshop on Parallel and Distributed Simulation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: In figure 1 the best window size is 7 times the sequential window. The sequential window size is computed using a simple queuing model described later. As the simulation progresses, the latest value of GVT is required to commit events. There is a significant overhead associated with computing GVT <ref> [14, 27] </ref>. The computation involves taking a global minimum of all the unprocessed events in the system, and requires information from all processors. Therefore, GVT is computed periodically instead of continuously 1 . A process has uncommitted events that would have been committed if GVT was calculated more frequently.
References-found: 27

