URL: http://www.cs.brown.edu/courses/cs295-8/etree.ps
Refering-URL: http://www.cs.brown.edu/courses/cs295-8/
Root-URL: http://www.cs.brown.edu/
Title: Elimination Trees and the Construction of Pools and Stacks  
Author: Nir Shavit Dan Touitou 
Date: February 28, 1996  
Affiliation: MIT and Tel-Aviv University  Tel-Aviv University  
Abstract: Shared pools and stacks are two coordination structures with a history of applications ranging from simple producer/consumer buffers to job-schedulers and procedure stacks. This paper introduces elimination trees, a novel form of diffracting trees that offer pool and stack implementations with superior response (on average constant) under high loads, while guaranteeing logarithmic time "deterministic" termination under sparse request patterns. 1 A preliminary version of this paper appeared in the proceedings of the 7th Annual Symposium on Parallel 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Aharonson and H. Attiya. </author> <title> Counting networks with arbitrary fan out. </title> <booktitle> In Proceedings of the 3 rd Symposium on Discrete Algorithms, </booktitle> <address> Orlando, Florida, </address> <month> January </month> <year> 1992. </year> <title> Also: </title> <type> Technical Report 679, </type> <institution> The Technion, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: k prism levels */ for i:=1 to k do place := random (1,size_i); him := register_to_memory_swap (Prism_i [place],mypid); if not_empty (him) then &lt;his_b,his_type&gt; := Location [him]; if his_b = b then if compare_and_swap (Location [mypid],&lt;b,mytype&gt;, &lt;0,EMPTY&gt;) then if my_type = his_type then if compare_and_swap (Location [him],&lt;b,his_type&gt;,&lt;0,DIFFRACTED&gt;) then 1. return b-&gt;OutputWire <ref> [1] </ref> else Location [mypid] := &lt;b,mytype&gt;; else if compare_and_swap (Location [him],&lt;b,his_type&gt;,&lt;0,ELIMINATED&gt;) then 2: return ELIMINATED; else Location [mypid] := &lt;b,mytype&gt;; else if Location [mypid]= &lt;0,DIFFRACTED&gt; return (b-&gt;OutputWire [0]) else return ELIMINATED repeat b-&gt;Spin times /* wait in hope of being collided with */ if Location [mypid] = &lt;0,DIFFRACTED&gt; then return b-&gt;OutputWire <p> Two independent research teams, Busch and Mavronicolas [6] and Aiello, Herlihy, Shavit, and Touitou [5], have recently extended our proofs to show that counting networks [4] in general, not only trees, work with anti-tokens (Busch and Mavronicolas [6] show this also for multi-balancers <ref> [1, 14] </ref>, that is, balancers with multiple inputs and output wires). In summary, elimination trees represent a new class of concurrent algorithms that we hope will prove an effective alternative to existing solutions for produce/consume coordination problems.
Reference: [2] <author> T.E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Since the formal introduction of the problem and its first solution by Manber [16], the literature has offered us a variety of possible pool implementations. On the one hand there are queue-lock based solutions such as of Anderson <ref> [2] </ref> and Mellor-Crummey and Scott [15], which offer good performance under sparse access patterns, but scale poorly since they offer little or no potential for parallelism in high load situations.
Reference: [3] <author> A. Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: Of course, the tree structure is needed since one could still have long sequences of enqueues only. We compared the performance of elimination trees to other known methods using the Proteus Parallel Hardware Simulator [8] in a shared memory architecture similar to the Alewife machine of Agarwal et al. <ref> [3] </ref>. We first compared under high loads a variety of methods that can be used to implement a stack-like pool and are known to perform well under sparse access patterns. <p> the value carried by T p . 2.5 Performance of the Elimination Tree Based Pool We evaluated the performance of our elimination tree based pool construction relative to other known methods by running a collection of benchmarks on a simulated 256 processor distributed-shared-memory machine similar to the MIT Alewife machine <ref> [3] </ref> of Agarwal et. al. The presented results hopefully exemplify the potential in using elimination trees, but in no way claim to be a comprehensive study of their performance. Our simulations were performed using Proteus a multiprocessor simulator developed by Brewer, Dellarocas, Colbrook and Weihl [8].
Reference: [4] <author> J. Aspnes, M.P. Herlihy, and N. Shavit. </author> <title> Counting Networks. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 41, No. </volume> <month> 5 (September </month> <year> 1994), </year> <pages> pp. 1020-1048. </pages>
Reference-contexts: In fact, our tree construction is a novel form of a counting network <ref> [4] </ref> based counter, that allows decrement (anti-token) operations in addition to standard increment (token) operations. <p> Our formal model follows that of Aspnes, Herlihy, and Shavit <ref> [4] </ref> I/O-automata of Lynch and Tuttle [20]. An elimination balancer is a routing element with one input wire x and two output wires y 0 and y 1 . Tokens and anti-tokens arrive on the balancer's input wire at arbitrary times, and are output on its output wires. <p> In preliminary tests we found that the most efficient pool implementations are attained when using shared counting to load balance and control access to a shared array (see Figure 5). We thus realized the centralized pool in the style of <ref> [4] </ref>, given in Figure 5, where the headcounter and tailcounter are implemented using two counters of the following type: MCS The MCS-queue-lock of [15], whose response time is linear in the number of concurrent requests. Each processor locks the shared counter, increments it, and then unlocks it. <p> Two independent research teams, Busch and Mavronicolas [6] and Aiello, Herlihy, Shavit, and Touitou [5], have recently extended our proofs to show that counting networks <ref> [4] </ref> in general, not only trees, work with anti-tokens (Busch and Mavronicolas [6] show this also for multi-balancers [1, 14], that is, balancers with multiple inputs and output wires).
Reference: [5] <author> W. Aiello, M. Herlihy, N. Shavit and D. Touitou. </author> <title> Inc/Dec Counting Networks. </title> <type> Manuscript, </type> <month> December </month> <year> 1995. </year>
Reference-contexts: We use gap elimination balancers to construct counting trees that allow both increments and decrements. It has recently been shown by two independent teams, Busch and Mavronicolas [6] and Aiello, Herlihy, Shavit, and Touitou <ref> [5] </ref> that the increment/decrement properties we describe hold for counting networks in general, not only for trees. <p> Two independent research teams, Busch and Mavronicolas [6] and Aiello, Herlihy, Shavit, and Touitou <ref> [5] </ref>, have recently extended our proofs to show that counting networks [4] in general, not only trees, work with anti-tokens (Busch and Mavronicolas [6] show this also for multi-balancers [1, 14], that is, balancers with multiple inputs and output wires).
Reference: [6] <author> C. Busch and M. Mavronicolas. </author> <title> The Strength of Counting Networks. </title> <booktitle> Proceedings of the 15th Annual ACM Symposium on Principles of Distributed Computing, to appear, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: We use gap elimination balancers to construct counting trees that allow both increments and decrements. It has recently been shown by two independent teams, Busch and Mavronicolas <ref> [6] </ref> and Aiello, Herlihy, Shavit, and Touitou [5] that the increment/decrement properties we describe hold for counting networks in general, not only for trees. <p> Two independent research teams, Busch and Mavronicolas <ref> [6] </ref> and Aiello, Herlihy, Shavit, and Touitou [5], have recently extended our proofs to show that counting networks [4] in general, not only trees, work with anti-tokens (Busch and Mavronicolas [6] show this also for multi-balancers [1, 14], that is, balancers with multiple inputs and output wires). <p> Two independent research teams, Busch and Mavronicolas <ref> [6] </ref> and Aiello, Herlihy, Shavit, and Touitou [5], have recently extended our proofs to show that counting networks [4] in general, not only trees, work with anti-tokens (Busch and Mavronicolas [6] show this also for multi-balancers [1, 14], that is, balancers with multiple inputs and output wires). In summary, elimination trees represent a new class of concurrent algorithms that we hope will prove an effective alternative to existing solutions for produce/consume coordination problems.
Reference: [7] <author> R.D. Blumofe, and C.E. Leiserson. </author> <title> Sheduling Multithreaded Computations by Work Stealing. </title> <booktitle> In Proceeding of the 35th Symposium on Foundations of Computer Science, </booktitle> <pages> pages 365-368, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Shared pools offer a potential solution to such coordination problems, with a history of applications ranging from simple producer/consumer buffers to job-schedulers <ref> [7] </ref> and procedure stacks [26]. A pool [16] (also called a pile [22], global pool [7] or a producer/consumer buffer) is a concurrent data-type which supports the abstract operations: enqueue (e) adds element e to the pool, and dequeue - deletes and returns some element e from the pool. <p> Shared pools offer a potential solution to such coordination problems, with a history of applications ranging from simple producer/consumer buffers to job-schedulers <ref> [7] </ref> and procedure stacks [26]. A pool [16] (also called a pile [22], global pool [7] or a producer/consumer buffer) is a concurrent data-type which supports the abstract operations: enqueue (e) adds element e to the pool, and dequeue - deletes and returns some element e from the pool. A stack is a pool with a last-in-first-out (LIFO) ordering on enqueue and dequeue operations. <p> Monien [21], and Blumofe and Leiserson <ref> [7] </ref>. <p> Our empirical results show that unlike diffracting trees, and in spite of the fact that elimination trees offer a "deterministic" guarantee 1 of coordination, 1 they scale like the "randomized" methods <ref> [7, 13, 21, 22] </ref>, providing improved response time as the load on them increases. <p> We found that elimination trees scale substantially better than all of these methods including queue-locks [15], Combining trees [10], and Diffracting Trees [24]. We then compared Elimination trees to the load-balanced local pools techniques <ref> [16, 13, 22, 21, 7] </ref> which cannot be used to implement a stack-like pool and theoretically provide only linear performance under sparse access patterns. <p> A pool [16](also called a pile [22], centralized "pool" <ref> [7] </ref> or a producer/consumer buffer) is a concurrent data-type which maintains a multiset of values by supporting the abstract operations: enqueue (e) adds element e to the multiset, and dequeue deletes and returns some element e 3 from the multiset. <p> A successful operation is one that is guaranteed to return an answer within finite (in our construction, bounded) time. Note that the randomized decentralized techniques of <ref> [7, 13, 21, 22] </ref> implement a weaker "probabilistic" pool definition, where condition P2 is replaced by a probabilistic guarantee that dequeue operations succeed. 2.1 Elimination Trees Our pool implementation is based on the abstract notion of an elimination tree, a special form of the diffracting tree data structures introduced by Shavit <p> Naturally if the local pool is empty the dequeuing process waits until the pool is filled and then access it. The elimination tree is thus a load-balanced coordination medium among a distributed collection of pools. It differs from elegant 5 randomized constructions of <ref> [7, 13, 21, 22] </ref> in its deterministic dequeue termination guarantee and in performance. While work in an individual balancer is relatively high, each enqueue or dequeue request passes at most log w balancers both under high and under low loads. <p> Monien [21] (this method is a refinement of RSU), and the job-stealing method of Blumofe and Leiserson <ref> [7] </ref>. We also did not compare to Manber's deterministic method [16] as Kotz and Ellis [13] have shown empirically that the randomized methods tend to give better overall performance. <p> LIFO-based scheduling will not only eliminate in many cases excessive task creation, but it will also prevent 20 processors from attempting to dequeue and execute a task which depends on the results of other tasks [26]. Blumofe and Leiserson <ref> [7] </ref> provide a scheduler based on a randomized distributed pool having stack-like behavior on the level of local pools. We present here a construction of a pool that globally behaves like a stack.
Reference: [8] <author> E.A. Brewer, C.N. Dellarocas, A. Colbrook and W.E. Weihl. Proteus: </author> <title> A High-Performance Parallel-Architecture Simulator. </title> <type> MIT Technical Report /MIT/LCS/TR-561, </type> <month> September </month> <year> 1991. </year>
Reference-contexts: Of course, the tree structure is needed since one could still have long sequences of enqueues only. We compared the performance of elimination trees to other known methods using the Proteus Parallel Hardware Simulator <ref> [8] </ref> in a shared memory architecture similar to the Alewife machine of Agarwal et al. [3]. We first compared under high loads a variety of methods that can be used to implement a stack-like pool and are known to perform well under sparse access patterns. <p> The presented results hopefully exemplify the potential in using elimination trees, but in no way claim to be a comprehensive study of their performance. Our simulations were performed using Proteus a multiprocessor simulator developed by Brewer, Dellarocas, Colbrook and Weihl <ref> [8] </ref>. Proteus simulates parallel code by multiplexing several parallel threads on a single CPU. Each thread runs on its own virtual CPU with accompanying local memory, cache and communications hardware, keeping track of how much time is spent using each component.
Reference: [9] <author> D. Chaiken. </author> <title> Cache Coherence Protocols for Large-Scale Multiprocessors. </title> <type> S.M. thesis, </type> <institution> Massachusetts Institute of Technology, Laboratory for Computer Science Technical Report MIT/LCS/TR-489, </institution> <month> September </month> <year> 1990. </year> <month> 27 </month>
Reference-contexts: The cost of switching or wiring in the Alewife architecture is 1 cycle/packet. Each processor has a cache with 2048 lines of 8 bytes. The cache coherence is provided using a using a version of Chaiken's directory-based cache-coherence protocol <ref> [9] </ref>. 2.5.1 The Produce-Consume Benchmark We begin by comparing under various loads deterministic pool constructions which are known to guarantee good enqueue/dequeue time when the load is low (sparse access patterns). These methods are also the ones that can be modified to provide stack-like pool behaviour.
Reference: [10] <author> J.R. Goodman, M.K. Vernon, and P.J. Woest. </author> <title> Efficient Synchronization Primitives for Large--Scale Cache-Coherent multiprocessors. </title> <booktitle> In Proceedings of the 3rd ASPLOS, </booktitle> <pages> pages 64-75. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1989. </year>
Reference-contexts: We first compared under high loads a variety of methods that can be used to implement a stack-like pool and are known to perform well under sparse access patterns. We found that elimination trees scale substantially better than all of these methods including queue-locks [15], Combining trees <ref> [10] </ref>, and Diffracting Trees [24]. We then compared Elimination trees to the load-balanced local pools techniques [16, 13, 22, 21, 7] which cannot be used to implement a stack-like pool and theoretically provide only linear performance under sparse access patterns. <p> The code was taken directly from the article, and implemented using atomic operations: register to memory swap and compare and swap operations. 14 CTree A Fetch&Inc using an optimal width software combining tree following the protocol of Goodman et al. <ref> [10] </ref>, modified according to [11]. The tree's response time is logarithmic in the maximal number of processors. Optimal width means that when n processors participate in the simulation, a tree of width n=2 will be used [11].
Reference: [11] <author> M. Herlihy, B.H. Lim and N. Shavit. </author> <title> Low Contention Load Balancing on Large Scale Multiprocessors. </title> <booktitle> Proceedings of the 3rd Annual ASM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> July 1992, San Diego, CA. </address> <note> Full version available as a DEC TR. </note>
Reference-contexts: The code was taken directly from the article, and implemented using atomic operations: register to memory swap and compare and swap operations. 14 CTree A Fetch&Inc using an optimal width software combining tree following the protocol of Goodman et al. [10], modified according to <ref> [11] </ref>. The tree's response time is logarithmic in the maximal number of processors. Optimal width means that when n processors participate in the simulation, a tree of width n=2 will be used [11]. <p> using an optimal width software combining tree following the protocol of Goodman et al. [10], modified according to <ref> [11] </ref>. The tree's response time is logarithmic in the maximal number of processors. Optimal width means that when n processors participate in the simulation, a tree of width n=2 will be used [11]. DTree A Diffracting Tree of width 32, using the optimized parameters of [24], whose response time is logarithmic in w = 32 which is smaller than the maximal number of processors. The prism sizes were 8,4,2,2 and 1 for levels 1; : : :; 5 respectively.
Reference: [12] <author> M. Herlihy and J.M. Wing. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <booktitle> In ACM Transaction on Programming Languages and Systems, </booktitle> <volume> 12(3), </volume> <pages> pages 463-492, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Theorem 3.5 In any sequential execution the stack-like pool provides a last-in-first-out order on enqueues and dequeues. 22 In Section 3.5 we present empirical evidence that suggests that even though the stack-like pool is not linearizable <ref> [12] </ref> to a sequential stack, it is linearizable in executions without severe timing anomalies, hence our use of the term "stack-like." 3.2 Implementing the Gap Elimination Balancer One can modify the pool elimination balancer construction from the former section so that it satisfies the gap step property. <p> As can be seen, though tokens are accessing a shared toggle bit instead of two separate ones, high elimination rates on the prisms allow the efficiency of the stack-like pool to fall from that of the Pool [32] only slightly. 24 3.5 Almost Linearizability Herlihy and Wing's Linearizability <ref> [12] </ref> is a consistency condition that specifies the allowable concurrent behaviours of an object by way of a mapping to a sequentially specified object whose behaviours are easy to state. <p> A stack-like pool implementation is linearizable <ref> [12] </ref> if it ensures that every execution does not contain a dequeue operation that is not linearizable . Our elimination tree based IncDecCounter [w] is easily shown not to be linearizable to a sequential counter with increments and decrements.
Reference: [13] <author> D. Kotz and C. S. Ellis. </author> <title> Evaluation of Concurrent Pools. </title> <booktitle> In Proceedings of the International Conference on Distributed Computing Systems, </booktitle> <pages> pages 378-385, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: On the other hand, there are a variety of that "load-balanced local pools" based algorithms like Manber's search tree structure [16] and the simple and effective randomized work-pile and job-stealing techniques as designed by Kotz and Ellis <ref> [13] </ref>, Rudolph, Slivkin-Allaluf, and Upfal [22], Luling and B. Monien [21], and Blumofe and Leiserson [7]. <p> Our empirical results show that unlike diffracting trees, and in spite of the fact that elimination trees offer a "deterministic" guarantee 1 of coordination, 1 they scale like the "randomized" methods <ref> [7, 13, 21, 22] </ref>, providing improved response time as the load on them increases. <p> We found that elimination trees scale substantially better than all of these methods including queue-locks [15], Combining trees [10], and Diffracting Trees [24]. We then compared Elimination trees to the load-balanced local pools techniques <ref> [16, 13, 22, 21, 7] </ref> which cannot be used to implement a stack-like pool and theoretically provide only linear performance under sparse access patterns. <p> A successful operation is one that is guaranteed to return an answer within finite (in our construction, bounded) time. Note that the randomized decentralized techniques of <ref> [7, 13, 21, 22] </ref> implement a weaker "probabilistic" pool definition, where condition P2 is replaced by a probabilistic guarantee that dequeue operations succeed. 2.1 Elimination Trees Our pool implementation is based on the abstract notion of an elimination tree, a special form of the diffracting tree data structures introduced by Shavit <p> Naturally if the local pool is empty the dequeuing process waits until the pool is filled and then access it. The elimination tree is thus a load-balanced coordination medium among a distributed collection of pools. It differs from elegant 5 randomized constructions of <ref> [7, 13, 21, 22] </ref> in its deterministic dequeue termination guarantee and in performance. While work in an individual balancer is relatively high, each enqueue or dequeue request passes at most log w balancers both under high and under low loads. <p> scale well at higher levels of concurrency. 2.5.3 Response Time Benchmark We compared elimination trees to the randomized method of Rudolph, Silvkin-Allalouf, and Upfal (RSU) [22], which we chose as a representative of the class of load-balanced local pools methods, which also include the randomized methods of Kotz and Ellis <ref> [13] </ref> (RSU is a refinement of this method), of Luling and B. Monien [21] (this method is a refinement of RSU), and the job-stealing method of Blumofe and Leiserson [7]. We also did not compare to Manber's deterministic method [16] as Kotz and Ellis [13] have shown empirically that the randomized <p> randomized methods of Kotz and Ellis <ref> [13] </ref> (RSU is a refinement of this method), of Luling and B. Monien [21] (this method is a refinement of RSU), and the job-stealing method of Blumofe and Leiserson [7]. We also did not compare to Manber's deterministic method [16] as Kotz and Ellis [13] have shown empirically that the randomized methods tend to give better overall performance. One should keep in mind that there are various situations in which any one 18 of these techniques outperforms all the others and vice versa.
Reference: [14] <author> E.W. Felten, A. LaMarca, R. Ladner. </author> <title> Building Counting Networks from Larger Balancers. </title> <institution> University of Washington T.R. #93-04-09. </institution>
Reference-contexts: Two independent research teams, Busch and Mavronicolas [6] and Aiello, Herlihy, Shavit, and Touitou [5], have recently extended our proofs to show that counting networks [4] in general, not only trees, work with anti-tokens (Busch and Mavronicolas [6] show this also for multi-balancers <ref> [1, 14] </ref>, that is, balancers with multiple inputs and output wires). In summary, elimination trees represent a new class of concurrent algorithms that we hope will prove an effective alternative to existing solutions for produce/consume coordination problems.
Reference: [15] <author> J.M. </author> <title> Mellor-Crummey and M.L. Scott Synchronization without Contention. </title> <booktitle> In Proceedings of the 4th International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Since the formal introduction of the problem and its first solution by Manber [16], the literature has offered us a variety of possible pool implementations. On the one hand there are queue-lock based solutions such as of Anderson [2] and Mellor-Crummey and Scott <ref> [15] </ref>, which offer good performance under sparse access patterns, but scale poorly since they offer little or no potential for parallelism in high load situations. <p> We first compared under high loads a variety of methods that can be used to implement a stack-like pool and are known to perform well under sparse access patterns. We found that elimination trees scale substantially better than all of these methods including queue-locks <ref> [15] </ref>, Combining trees [10], and Diffracting Trees [24]. We then compared Elimination trees to the load-balanced local pools techniques [16, 13, 22, 21, 7] which cannot be used to implement a stack-like pool and theoretically provide only linear performance under sparse access patterns. <p> Constructing a pool object from a Pool [w] tree is straightforward: each tree output wire is connected to a sequentially accessed "local" pool, a simple queue protected by a Mellor-Crummey and Scott MCS-queue-lock <ref> [15] </ref> will do. The MCS-queue-lock has the property of being "fair," and so every access request to the queue will be granted within a bounded number of operations. A process performs an enqueue operation by shepherding a token "carrying" the value the down the tree. <p> Each of the toggle bit locations is protected by an MCS-queue-lock <ref> [15] </ref>. A process shepherding a token or anti-token through the balancer decides on which wire to exit according to the value of the respective token or anti-token toggle bit, 0 to the left and 1 to the right, toggling the bit as it leaves. <p> if compare_and_swap (Location [mypid],&lt;b,my_type&gt;, &lt;0,EMPTY&gt;) then i:= b-&gt;Toggles [mytype]; b-&gt;Toggles [mytype] := Not (i); ReleaseLock (b-&gt;Locks [mytype]); 3: return b-&gt;OutputWire [i]; else ReleaseLock (b-&gt;Locks [mytype]); if Location [mypid]= &lt;0,DIFFRACTED&gt; return (b-&gt;OutputWire [0]) else return ELIMINATED Our implementation also uses standard AquireLock and ReleaseLock procedures to enter and exit the MCS-queue-lock <ref> [15] </ref>. 9 Initially, processor p announces the arrival of its token at node b, by writing b and its token type to Location [p]. <p> We thus realized the centralized pool in the style of [4], given in Figure 5, where the headcounter and tailcounter are implemented using two counters of the following type: MCS The MCS-queue-lock of <ref> [15] </ref>, whose response time is linear in the number of concurrent requests. Each processor locks the shared counter, increments it, and then unlocks it.
Reference: [16] <author> Udi Manber. </author> <title> On maintaining dynamic information in a concurrent environment SIAM J. </title> <booktitle> Computing 15(4), </booktitle> <pages> pages 1130-1142, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: Shared pools offer a potential solution to such coordination problems, with a history of applications ranging from simple producer/consumer buffers to job-schedulers [7] and procedure stacks [26]. A pool <ref> [16] </ref> (also called a pile [22], global pool [7] or a producer/consumer buffer) is a concurrent data-type which supports the abstract operations: enqueue (e) adds element e to the pool, and dequeue - deletes and returns some element e from the pool. <p> A stack is a pool with a last-in-first-out (LIFO) ordering on enqueue and dequeue operations. Since the formal introduction of the problem and its first solution by Manber <ref> [16] </ref>, the literature has offered us a variety of possible pool implementations. <p> On the other hand, there are a variety of that "load-balanced local pools" based algorithms like Manber's search tree structure <ref> [16] </ref> and the simple and effective randomized work-pile and job-stealing techniques as designed by Kotz and Ellis [13], Rudolph, Slivkin-Allaluf, and Upfal [22], Luling and B. Monien [21], and Blumofe and Leiserson [7]. <p> This linear behaviour under sparse access patterns holds also for Manber's tree based deterministic job-stealing method <ref> [16] </ref>. Shavit and Zemach's diffracting trees [24] have recently been proposed as a reasonable middle-of-the-road solution to the problem. <p> We found that elimination trees scale substantially better than all of these methods including queue-locks [15], Combining trees [10], and Diffracting Trees [24]. We then compared Elimination trees to the load-balanced local pools techniques <ref> [16, 13, 22, 21, 7] </ref> which cannot be used to implement a stack-like pool and theoretically provide only linear performance under sparse access patterns. <p> Monien [21] (this method is a refinement of RSU), and the job-stealing method of Blumofe and Leiserson [7]. We also did not compare to Manber's deterministic method <ref> [16] </ref> as Kotz and Ellis [13] have shown empirically that the randomized methods tend to give better overall performance. One should keep in mind that there are various situations in which any one 18 of these techniques outperforms all the others and vice versa.
Reference: [17] <author> G.H. Pfister and A. Norton. </author> <title> `Hot Spot' contention and combining in multistage interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(11):933-938, </volume> <month> November </month> <year> 1985. </year>
Reference-contexts: In fact, our tree construction is a novel form of a counting network [4] based counter, that allows decrement (anti-token) operations in addition to standard increment (token) operations. However, this simple approach is bound to fail since the toggle bit at root of the tree will be a hot-spot <ref> [17, 18] </ref> and a sequential bottleneck that is no better than a centralized stack implementation. The problem is overcome by placing a diffracting prism [24] structure in front of the toggle bit inside every balancer.
Reference: [18] <author> D. Gawlick. </author> <title> Processing 'hot spots' in high performance systems. </title> <booktitle> In Proceedings COMPCON'85, </booktitle> <year> 1985. </year>
Reference-contexts: In fact, our tree construction is a novel form of a counting network [4] based counter, that allows decrement (anti-token) operations in addition to standard increment (token) operations. However, this simple approach is bound to fail since the toggle bit at root of the tree will be a hot-spot <ref> [17, 18] </ref> and a sequential bottleneck that is no better than a centralized stack implementation. The problem is overcome by placing a diffracting prism [24] structure in front of the toggle bit inside every balancer.
Reference: [19] <author> J. Kubiatowicz. </author> <title> Personal communication (February 1995). </title>
Reference-contexts: This paper presents shared memory implementations of elimination trees, and uses them for constructing pools and and stack-like pools. There is clearly room for experimentation on real machines and networks. Given the hardware fetch-and-complement operation to be added to the Alewife machine's Sparcle chip's set of colored load/store operations <ref> [19] </ref>, one will be able to implement a shared memory elimination-tree in a wait-free manner, that is, without any locks. Our plan is to test such "hardware supported" elimination-tree performance.
Reference: [20] <author> N.A. Lynch and M.R. Tuttle. </author> <title> Hierarchical Correctness Proofs for Distributed Algorithms. </title> <booktitle> In Sixth ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1987, </year> <pages> pp. 137-151. </pages> <note> Full version available as MIT Technical Report MIT/LCS/TR-387. </note>
Reference-contexts: Our formal model follows that of Aspnes, Herlihy, and Shavit [4] I/O-automata of Lynch and Tuttle <ref> [20] </ref>. An elimination balancer is a routing element with one input wire x and two output wires y 0 and y 1 . Tokens and anti-tokens arrive on the balancer's input wire at arbitrary times, and are output on its output wires. Every token carries a value.
Reference: [21] <author> R. Luling, and B. Monien. </author> <title> A Dynamic Distributed Load Balancing Algorithm with Provable Good Performance. </title> <booktitle> In Proceedings of the 5rd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 164-173, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: On the other hand, there are a variety of that "load-balanced local pools" based algorithms like Manber's search tree structure [16] and the simple and effective randomized work-pile and job-stealing techniques as designed by Kotz and Ellis [13], Rudolph, Slivkin-Allaluf, and Upfal [22], Luling and B. Monien <ref> [21] </ref>, and Blumofe and Leiserson [7]. <p> Our empirical results show that unlike diffracting trees, and in spite of the fact that elimination trees offer a "deterministic" guarantee 1 of coordination, 1 they scale like the "randomized" methods <ref> [7, 13, 21, 22] </ref>, providing improved response time as the load on them increases. <p> We found that elimination trees scale substantially better than all of these methods including queue-locks [15], Combining trees [10], and Diffracting Trees [24]. We then compared Elimination trees to the load-balanced local pools techniques <ref> [16, 13, 22, 21, 7] </ref> which cannot be used to implement a stack-like pool and theoretically provide only linear performance under sparse access patterns. <p> A successful operation is one that is guaranteed to return an answer within finite (in our construction, bounded) time. Note that the randomized decentralized techniques of <ref> [7, 13, 21, 22] </ref> implement a weaker "probabilistic" pool definition, where condition P2 is replaced by a probabilistic guarantee that dequeue operations succeed. 2.1 Elimination Trees Our pool implementation is based on the abstract notion of an elimination tree, a special form of the diffracting tree data structures introduced by Shavit <p> Naturally if the local pool is empty the dequeuing process waits until the pool is filled and then access it. The elimination tree is thus a load-balanced coordination medium among a distributed collection of pools. It differs from elegant 5 randomized constructions of <ref> [7, 13, 21, 22] </ref> in its deterministic dequeue termination guarantee and in performance. While work in an individual balancer is relatively high, each enqueue or dequeue request passes at most log w balancers both under high and under low loads. <p> Monien <ref> [21] </ref> (this method is a refinement of RSU), and the job-stealing method of Blumofe and Leiserson [7]. We also did not compare to Manber's deterministic method [16] as Kotz and Ellis [13] have shown empirically that the randomized methods tend to give better overall performance.
Reference: [22] <author> L. Rudolph, M. Slivkin, and E. Upfal. </author> <title> A Simple Load Balancing Scheme for Task Allocation in Parallel Machines. </title> <booktitle> In Proceedings of the 3rd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 237-245, </pages> <month> July </month> <year> 1991. </year> <month> 28 </month>
Reference-contexts: Shared pools offer a potential solution to such coordination problems, with a history of applications ranging from simple producer/consumer buffers to job-schedulers [7] and procedure stacks [26]. A pool [16] (also called a pile <ref> [22] </ref>, global pool [7] or a producer/consumer buffer) is a concurrent data-type which supports the abstract operations: enqueue (e) adds element e to the pool, and dequeue - deletes and returns some element e from the pool. <p> On the other hand, there are a variety of that "load-balanced local pools" based algorithms like Manber's search tree structure [16] and the simple and effective randomized work-pile and job-stealing techniques as designed by Kotz and Ellis [13], Rudolph, Slivkin-Allaluf, and Upfal <ref> [22] </ref>, Luling and B. Monien [21], and Blumofe and Leiserson [7]. <p> Our empirical results show that unlike diffracting trees, and in spite of the fact that elimination trees offer a "deterministic" guarantee 1 of coordination, 1 they scale like the "randomized" methods <ref> [7, 13, 21, 22] </ref>, providing improved response time as the load on them increases. <p> We found that elimination trees scale substantially better than all of these methods including queue-locks [15], Combining trees [10], and Diffracting Trees [24]. We then compared Elimination trees to the load-balanced local pools techniques <ref> [16, 13, 22, 21, 7] </ref> which cannot be used to implement a stack-like pool and theoretically provide only linear performance under sparse access patterns. <p> We found that in many high load situations elimination trees are inferior to these methods (as explained in the sequel, we chose for the comparison a representative technique, the randomized technique of Rudolph, Slivkin, and Upfal <ref> [22] </ref>), especially for job distribution applications where a typical processor is the dequeuer of its latest enqueue (though in many cases not by much). However, our empirical evidence suggests that elimination trees provide up to a factor of 30 better response time than randomized methods under sparse loads. <p> A pool [16](also called a pile <ref> [22] </ref>, centralized "pool" [7] or a producer/consumer buffer) is a concurrent data-type which maintains a multiset of values by supporting the abstract operations: enqueue (e) adds element e to the multiset, and dequeue deletes and returns some element e 3 from the multiset. <p> A successful operation is one that is guaranteed to return an answer within finite (in our construction, bounded) time. Note that the randomized decentralized techniques of <ref> [7, 13, 21, 22] </ref> implement a weaker "probabilistic" pool definition, where condition P2 is replaced by a probabilistic guarantee that dequeue operations succeed. 2.1 Elimination Trees Our pool implementation is based on the abstract notion of an elimination tree, a special form of the diffracting tree data structures introduced by Shavit <p> Naturally if the local pool is empty the dequeuing process waits until the pool is filled and then access it. The elimination tree is thus a load-balanced coordination medium among a distributed collection of pools. It differs from elegant 5 randomized constructions of <ref> [7, 13, 21, 22] </ref> in its deterministic dequeue termination guarantee and in performance. While work in an individual balancer is relatively high, each enqueue or dequeue request passes at most log w balancers both under high and under low loads. <p> However, unlike our the multi-layered balancer constructions, they do not continue to scale well at higher levels of concurrency. 2.5.3 Response Time Benchmark We compared elimination trees to the randomized method of Rudolph, Silvkin-Allalouf, and Upfal (RSU) <ref> [22] </ref>, which we chose as a representative of the class of load-balanced local pools methods, which also include the randomized methods of Kotz and Ellis [13] (RSU is a refinement of this method), of Luling and B.
Reference: [23] <author> N. Shavit, and D. Touitou. </author> <title> Elimination Trees and the Construction of Pools and Stack. </title> <booktitle> In Proceedings of the 7th Annual Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 54-63, </pages> <month> July </month> <year> 1995. </year>
Reference: [24] <author> N. Shavit and A. Zemach. </author> <title> Diffracting Trees. </title> <booktitle> In Proceedings of the Annual Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: This linear behaviour under sparse access patterns holds also for Manber's tree based deterministic job-stealing method [16]. Shavit and Zemach's diffracting trees <ref> [24] </ref> have recently been proposed as a reasonable middle-of-the-road solution to the problem. <p> However, this simple approach is bound to fail since the toggle bit at root of the tree will be a hot-spot [17, 18] and a sequential bottleneck that is no better than a centralized stack implementation. The problem is overcome by placing a diffracting prism <ref> [24] </ref> structure in front of the toggle bit inside every balancer. <p> We found that elimination trees scale substantially better than all of these methods including queue-locks [15], Combining trees [10], and Diffracting Trees <ref> [24] </ref>. We then compared Elimination trees to the load-balanced local pools techniques [16, 13, 22, 21, 7] which cannot be used to implement a stack-like pool and theoretically provide only linear performance under sparse access patterns. <p> implement a weaker "probabilistic" pool definition, where condition P2 is replaced by a probabilistic guarantee that dequeue operations succeed. 2.1 Elimination Trees Our pool implementation is based on the abstract notion of an elimination tree, a special form of the diffracting tree data structures introduced by Shavit and Zemach in <ref> [24] </ref>. Our formal model follows that of Aspnes, Herlihy, and Shavit [4] I/O-automata of Lynch and Tuttle [20]. An elimination balancer is a routing element with one input wire x and two output wires y 0 and y 1 . <p> In that case no dequeue operation will never have to wait indefinitely at a leaf. This satisfies property P2. 2.2 Pool Elimination Balancers The scalable performance of our pool constructions depends on providing an efficient implementation of an elimination balancer. Diffracting balancers were introduced in <ref> [24] </ref>. Our shared memory construction of a diffracting elimination balancer, apart from providing a mechanism for token/anti-token elimination, also improves on the performance of the original diffracting balancer design. While a regular diffracting balancer [24] is constructed from a single prism array and a toggle bit, the elimination balancer we use <p> Diffracting balancers were introduced in <ref> [24] </ref>. Our shared memory construction of a diffracting elimination balancer, apart from providing a mechanism for token/anti-token elimination, also improves on the performance of the original diffracting balancer design. While a regular diffracting balancer [24] is constructed from a single prism array and a toggle bit, the elimination balancer we use in our pool construction (see lefthand side of Figure 2) has a sequence of prism arrays and two 6 toggle bits, one for tokens and one for anti-tokens 2 . <p> The reader can easily convince herself that this suffices to guarantee the pool-balancing property. However, if many tokens were to attempt to access the same toggle bit concurrently, the bit would quickly become a hot spot. The solution presented in <ref> [24] </ref> is to add a prism array in front of each toggle bit. Before accessing the bit, the process shepherding the token selects a location l in the prism uniformly at random, hoping to "collide" with another token which selected l. <p> If such a diffracting collision does not occur, the process toggles the bit as above and leaves accordingly. As proved in <ref> [24] </ref>, the combination of diffracted tokens and toggling tokens behaves exactly as if all tokens toggled the bit, because if any two diffracted tokens were to access the bit instead, after they both toggled it the bit state would anyhow return to its initial state. <p> We typically found the optimal performance was when the prism width at a balancer on a given level is the same as the width of the subtree below it (this conforms with recent projections based on steady-state analysis [25]). Moreover, unlike the single prism array of <ref> [24] </ref>, we found it more effective to pass a token through a series of prisms of decreasing size, thus 2 The two separate toggle locations are an artifact of the pool-balancing property. <p> The tree's response time is logarithmic in the maximal number of processors. Optimal width means that when n processors participate in the simulation, a tree of width n=2 will be used [11]. DTree A Diffracting Tree of width 32, using the optimized parameters of <ref> [24] </ref>, whose response time is logarithmic in w = 32 which is smaller than the maximal number of processors. The prism sizes were 8,4,2,2 and 1 for levels 1; : : :; 5 respectively. <p> However, as observed by Shavit and Zemach <ref> [24] </ref>, as the level of 16 concurrency increases, while the diffracting tree manages only to keep the average latency constant, the average latency in the elimination tree continues to decrease due to the increased numbers of successful eliminating collisions taking place on the top levels of tree. <p> It follows that the elimination and diffracting tree performance graphs converge, and at sufficiently high levels of concurrency remain far better than the combining tree. 2.5.2 Counting Benchmark Our new multi-layered prism approach is slightly more costly but scales better than the original single prism construction of Shavit and Zemach <ref> [24] </ref>, since it increases the likelihood of successful collisions. This conforms with the steady-state modeling of diffracting trees by Shavit, Upfal, and Zemach [25]. <p> Clearly, the gap step property implies the pool balancing property on the balancer's output wires. Claim 3.1 Every gap elimination balancer satisfies the pool balancing property. We design IncDecCounter [w] as a counting tree <ref> [24] </ref> (a special case of the structure with regular token routing balancers replaced by token/anti-token routing gap elimination balancers). <p> from gap elimination balancers has the gap step property on its output wires, that is, in any quiescent state: 0 (y i y i ) (y j y j ) 1 21 Proof: We use that fact that the layout of the IncDecCounter is identical to that of a counting--tree <ref> [24] </ref>, in order to show that if for some execution the IncDecCounter reaches a quiescent state which does not satisfies the gap step property, then there is an execution of the counting tree in which the step property is violated too. This is a contradiction to Theorem 5.5 of [24]. <p> counting--tree <ref> [24] </ref>, in order to show that if for some execution the IncDecCounter reaches a quiescent state which does not satisfies the gap step property, then there is an execution of the counting tree in which the step property is violated too. This is a contradiction to Theorem 5.5 of [24]. Let T g be an IncDecCounter constructed from gap balancers g, and let T b be the isomorphic counting tree which is the result of replacing every gap balancer g in IncDecCounter by a regular balancer b. <p> Note that for tightly synchronized executions (w = 0), our stack-like implementation 25 is linearizable to a stack at almost all levels of concurrency. 4 Conclusions and Further Research Our paper introduces the notion of "anti-tokens" to allow decrement operations on a counting-tree <ref> [24] </ref>.
Reference: [25] <author> N. Shavit, E. Upfal, and A. Zemach. </author> <title> A Steady-State Analysis of Diffracting Trees. </title> <type> Unpublised manuscript. </type> <institution> Tel-Aviv University. </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: We typically found the optimal performance was when the prism width at a balancer on a given level is the same as the width of the subtree below it (this conforms with recent projections based on steady-state analysis <ref> [25] </ref>). Moreover, unlike the single prism array of [24], we found it more effective to pass a token through a series of prisms of decreasing size, thus 2 The two separate toggle locations are an artifact of the pool-balancing property. <p> This conforms with the steady-state modeling of diffracting trees by Shavit, Upfal, and Zemach <ref> [25] </ref>.
Reference: [26] <author> K. Taura, S. Matsuoka, and A. Yonezawa. </author> <title> An Efficient Implementation Scheme of Concurrent Object-Oriented Languages on Stock Multicomputers. </title> <booktitle> In Proceedings of the 4th Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 218-228, </pages> <month> May </month> <year> 1993. </year> <month> 29 </month>
Reference-contexts: Shared pools offer a potential solution to such coordination problems, with a history of applications ranging from simple producer/consumer buffers to job-schedulers [7] and procedure stacks <ref> [26] </ref>. A pool [16] (also called a pile [22], global pool [7] or a producer/consumer buffer) is a concurrent data-type which supports the abstract operations: enqueue (e) adds element e to the pool, and dequeue - deletes and returns some element e from the pool. <p> LIFO-based scheduling will not only eliminate in many cases excessive task creation, but it will also prevent 20 processors from attempting to dequeue and execute a task which depends on the results of other tasks <ref> [26] </ref>. Blumofe and Leiserson [7] provide a scheduler based on a randomized distributed pool having stack-like behavior on the level of local pools. We present here a construction of a pool that globally behaves like a stack.
References-found: 26

