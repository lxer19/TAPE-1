URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/94.Multiprogrammed_Synchronization.ps.Z
Refering-URL: http://www.cs.rochester.edu/trs/systems-trs.html
Root-URL: 
Email: fbob,kthanasi,scottg@cs.rochester.edu  
Title: High Performance Synchronization Algorithms for Multiprogrammed Multiprocessors (Extended Abstract)  
Author: Robert W. Wisniewski, Leonidas Kontothanassis, and Michael L. Scott 
Date: June 1994  
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract: Scalable busy-wait synchronization algorithms are essential for achieving good parallel program performance on large scale multiprocessors. Such algorithms include mutual exclusion locks, reader-writer locks, and barrier synchronization. Unfortunately, scalable synchronization algorithms are particularly sensitive to the effects of multiprogramming: their performance degrades sharply when processors are shared among different applications, or even among processes of the same application. In this paper we describe the design and evaluation of scalable scheduler-conscious mutual exclusion locks, reader-writer locks, and barriers, and show that by sharing information across the kernel-application interface we can achieve good performance in the presence of multiprogramming.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <month> February </month> <year> 1992. </year> <booktitle> Originally presented at the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Performance degrades in the presence of multiprogramming under the following circumstances: * A process is preempted while holding a lock. This situation arises in both mutual exclusion and reader-writer locks when a process is preempted in its critical section. It has been addressed by several researchers <ref> [1, 3, 5, 11] </ref>. fl This work was supported in part by National Science Foundation grants numbers CCR-9319445 and CDA-8822724, by ONR contract number N00014-92-J-1801 (in conjunction with the ARPA Research in Information Science and Technology-High Performance Computing, Software Science and Technical program, ARPA Order no. 8930), and by ARPA research
Reference: [2] <author> T. S. Axelrod. </author> <title> Effects of Synchronization Barriers on Multiprocessor Performance. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 129-140, </pages> <year> 1986. </year>
Reference-contexts: It incorporates our scheduler-conscious counter-based barrier [7] into a two-level barrier scheme, and adjusts dynamically to the available number of processors in an application's partition. Two-level barriers employ a single counter on each processor, and a scalable barrier between processors. They were originally proposed by Axelrod <ref> [2] </ref> to minimize requirements for locks; Markatos et al. [10] first suggested their use to minimize overhead on multiprogrammed systems. The ability to perform well in the presence of multiprogramming is a combination of intelligent algorithms and extensions to the kernel interface.
Reference: [3] <author> D. L. Black. </author> <title> Scheduling Support for Concurrency and Parallelism in the Mach Operating System. </title> <journal> Computer, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Performance degrades in the presence of multiprogramming under the following circumstances: * A process is preempted while holding a lock. This situation arises in both mutual exclusion and reader-writer locks when a process is preempted in its critical section. It has been addressed by several researchers <ref> [1, 3, 5, 11] </ref>. fl This work was supported in part by National Science Foundation grants numbers CCR-9319445 and CDA-8822724, by ONR contract number N00014-92-J-1801 (in conjunction with the ARPA Research in Information Science and Technology-High Performance Computing, Software Science and Technical program, ARPA Order no. 8930), and by ARPA research
Reference: [4] <author> M. Crovella, P. Das, C. Dubnicki, T. LeBlanc, and E. </author> <title> Markatos. </title> <booktitle> Multiprogramming on Mul tiprocessors. In Proceedings of the Third IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 590-597, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: The first is a mutual-exclusion ticket lock that uses handshaking to detect 1 Our barrier algorithm assumes that processors are partitioned among applications (i.e. that each processor is dedicated to a particular application), as suggested by several recent studies <ref> [4, 9, 15, 13] </ref>.
Reference: [5] <author> J. Edler, J. Lipkis, and E. Schonberg. </author> <title> Process Management for Highly Parallel UNIX Sys tems. </title> <booktitle> In Proceedings of the USENIX Workshop on Unix and Supercomputers, </booktitle> <address> Pittsburgh, PA, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: Performance degrades in the presence of multiprogramming under the following circumstances: * A process is preempted while holding a lock. This situation arises in both mutual exclusion and reader-writer locks when a process is preempted in its critical section. It has been addressed by several researchers <ref> [1, 3, 5, 11] </ref>. fl This work was supported in part by National Science Foundation grants numbers CCR-9319445 and CDA-8822724, by ONR contract number N00014-92-J-1801 (in conjunction with the ARPA Research in Information Science and Technology-High Performance Computing, Software Science and Technical program, ARPA Order no. 8930), and by ARPA research <p> It also maintains a generation count for the application's partition. The kernel increments this count each time it changes the allocation of processes to processors. Extensions (1) and (2) are based in part on ideas introduced in Symunix <ref> [5] </ref>, and described in our work on queued locks [14]. Extension (3) is a generalization of the interface described in our work on small-scale scheduler-conscious barriers [7]. None of these extensions requires the kernel to maintain information that it does not already have available in its internal data structures.
Reference: [6] <author> A. R. Karlin, K. Li, M. S. Manasse, and S. Owicki. </author> <title> Empirical Studies of Competitive Spinning for a Shared-Memory Multiprocessor. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 41-55, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: This situation arises with locks, but is satisfactorily addressed by techniques that choose dynamically between spinning and yielding, based on observed lengths of critical sections <ref> [6] </ref>. A more severe version of the problem occurs with barriers, where the decision between spinning and yielding needs to be based not on critical section lengths, but on whether there are preempted processes that have not yet reached the barrier [7].
Reference: [7] <author> L. Kontothanassis and R. Wisniewski. </author> <title> Using Scheduler Information to Achieve Optimal Barrier Synchronization Performance. </title> <booktitle> In Proceedings of the Fourth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: A more severe version of the problem occurs with barriers, where the decision between spinning and yielding needs to be based not on critical section lengths, but on whether there are preempted processes that have not yet reached the barrier <ref> [7] </ref>. Scalable barriers exacerbate the problem by requiring portions of the barrier code in different processes to be interleaved in a deterministic order|an order that may conflict with the scheduling policy on a multiprogrammed processor, and that may require an unreasonable number of context switches [10]. <p> Placing limits on the use of this mechanism allows the kernel to maintain control of the processor. In previous work we have shown how kernel/application shared information can be used to build centralized barriers for small-scale machines <ref> [7] </ref>, and queued mutual-exclusion locks [14] for scalable machines. This paper extends our previous work by providing scheduler-conscious algorithms for: 1. A mutual-exclusion ticket lock. The ticket lock has lower overhead than a queued lock in the absence of contention, and scales almost as well when equipped with proportional backoff. <p> The second is a fair reader-writer lock based on the scheduler-oblivious code of Krieger, Stumm, and Unrau [8]. The third is a scheduler-conscious tree barrier. It incorporates our scheduler-conscious counter-based barrier <ref> [7] </ref> into a two-level barrier scheme, and adjusts dynamically to the available number of processors in an application's partition. Two-level barriers employ a single counter on each processor, and a scalable barrier between processors. <p> Extensions (1) and (2) are based in part on ideas introduced in Symunix [5], and described in our work on queued locks [14]. Extension (3) is a generalization of the interface described in our work on small-scale scheduler-conscious barriers <ref> [7] </ref>. None of these extensions requires the kernel to maintain information that it does not already have available in its internal data structures. <p> Processes running on the same processor use a centralized barrier and the scheduler information algorithm described in <ref> [7] </ref> to coordinate among themselves, spinning or yielding as appropriate. The last process arriving at the barrier on a given processor becomes the representative of that processor. Representative processes participate in a tree barrier as described in [12]. 4 The partition generation count allows us to handle repartitioning. <p> a standard tree barrier in which processes always spin (until preempted by the kernel scheduler). (2) a centralized barrier in which processes always spin (until preempted by the kernel scheduler), (3) a centralized barrier in which processes always yield the processor, rather than spin, (4) our earlier scheduler-conscious centralized barrier <ref> [7] </ref>, and (5) our scheduler-conscious two-level barrier.
Reference: [8] <author> O. Krieger, M. Stumm, and R. Unrau. </author> <title> A Fair Fast Scalable Reader-Writer Lock. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The second is a fair reader-writer lock based on the scheduler-oblivious code of Krieger, Stumm, and Unrau <ref> [8] </ref>. The third is a scheduler-conscious tree barrier. It incorporates our scheduler-conscious counter-based barrier [7] into a two-level barrier scheme, and adjusts dynamically to the available number of processors in an application's partition. Two-level barriers employ a single counter on each processor, and a scalable barrier between processors.
Reference: [9] <author> S. T. Leutenegger and M. K. Vernon. </author> <title> Performance of Multiprogrammed Multiprocessor Scheduling Algorithms. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Boulder, CO, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: The first is a mutual-exclusion ticket lock that uses handshaking to detect 1 Our barrier algorithm assumes that processors are partitioned among applications (i.e. that each processor is dedicated to a particular application), as suggested by several recent studies <ref> [4, 9, 15, 13] </ref>.
Reference: [10] <author> E. Markatos, M. Crovella, P. Das, C. Dubnicki, and T. LeBlanc. </author> <title> The Effects of Multiprogram ming on Barrier Synchronization. </title> <booktitle> In Proceedings of the Third IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 662-669, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Scalable barriers exacerbate the problem by requiring portions of the barrier code in different processes to be interleaved in a deterministic order|an order that may conflict with the scheduling policy on a multiprogrammed processor, and that may require an unreasonable number of context switches <ref> [10] </ref>. We have developed a simple and coherent set of mechanisms to handle synchronization difficulties arising from multiprogramming. The key idea is to share information across the application-kernel interface in order to eliminate the sources of overhead mentioned above. <p> Two-level barriers employ a single counter on each processor, and a scalable barrier between processors. They were originally proposed by Axelrod [2] to minimize requirements for locks; Markatos et al. <ref> [10] </ref> first suggested their use to minimize overhead on multiprogrammed systems. The ability to perform well in the presence of multiprogramming is a combination of intelligent algorithms and extensions to the kernel interface. We have extended the kernel interface in three ways: 1.
Reference: [11] <author> B. D. Marsh, M. L. Scott, T. J. LeBlanc, and E. P. Markatos. </author> <title> First-Class User-Level Threads. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 110-121, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Performance degrades in the presence of multiprogramming under the following circumstances: * A process is preempted while holding a lock. This situation arises in both mutual exclusion and reader-writer locks when a process is preempted in its critical section. It has been addressed by several researchers <ref> [1, 3, 5, 11] </ref>. fl This work was supported in part by National Science Foundation grants numbers CCR-9319445 and CDA-8822724, by ONR contract number N00014-92-J-1801 (in conjunction with the ARPA Research in Information Science and Technology-High Performance Computing, Software Science and Technical program, ARPA Order no. 8930), and by ARPA research
Reference: [12] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The last process arriving at the barrier on a given processor becomes the representative of that processor. Representative processes participate in a tree barrier as described in <ref> [12] </ref>. 4 The partition generation count allows us to handle repartitioning. We shadow this generation count with a count that belongs to the barrier. The process at the root of the inter-processor barrier tree checks the barrier generation count against the partition generation count. <p> The first of these outperforms the others in the absence of multiprogramming <ref> [12] </ref>, but performs terribly with multiprogramming, because a process can spin away the bulk of a scheduling quantum waiting for a peer to reach the barrier, when that peer has been preempted. The scheduler-conscious algorithms outperform the scheduler-oblivious ones by a wide margin.
Reference: [13] <author> A. Tucker and A. Gupta. </author> <title> Process Control and Scheduling Issues for Multiprogrammed Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: The first is a mutual-exclusion ticket lock that uses handshaking to detect 1 Our barrier algorithm assumes that processors are partitioned among applications (i.e. that each processor is dedicated to a particular application), as suggested by several recent studies <ref> [4, 9, 15, 13] </ref>.
Reference: [14] <author> R. W. Wisniewski, L. Kontothanassis, and M. L. Scott. </author> <title> Scalable Spin Locks for Multipro grammed Systems. </title> <booktitle> In Proceedings of the Eigth International Parallel Processing Symposium, </booktitle> <pages> pages 583-589, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year> <note> Earlier but expanded version available as TR 454, </note> <institution> Computer Science Department, University of Rochester, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: This situation can arise in locks that enforce a predetermined ordering, either for the sake of fairness or to minimize contention on large-scale machines <ref> [14] </ref>. * A process spins waiting for its peers when some of them are preempted. This situation arises with locks, but is satisfactorily addressed by techniques that choose dynamically between spinning and yielding, based on observed lengths of critical sections [6]. <p> Placing limits on the use of this mechanism allows the kernel to maintain control of the processor. In previous work we have shown how kernel/application shared information can be used to build centralized barriers for small-scale machines [7], and queued mutual-exclusion locks <ref> [14] </ref> for scalable machines. This paper extends our previous work by providing scheduler-conscious algorithms for: 1. A mutual-exclusion ticket lock. The ticket lock has lower overhead than a queued lock in the absence of contention, and scales almost as well when equipped with proportional backoff. <p> It also maintains a generation count for the application's partition. The kernel increments this count each time it changes the allocation of processes to processors. Extensions (1) and (2) are based in part on ideas introduced in Symunix [5], and described in our work on queued locks <ref> [14] </ref>. Extension (3) is a generalization of the interface described in our work on small-scale scheduler-conscious barriers [7]. None of these extensions requires the kernel to maintain information that it does not already have available in its internal data structures.
Reference: [15] <author> J. Zahorjan and C. McCann. </author> <title> Processor Scheduling in Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 214-225, </pages> <address> Boulder, CO, </address> <month> May </month> <year> 1990. </year> <month> 8 </month>
Reference-contexts: The first is a mutual-exclusion ticket lock that uses handshaking to detect 1 Our barrier algorithm assumes that processors are partitioned among applications (i.e. that each processor is dedicated to a particular application), as suggested by several recent studies <ref> [4, 9, 15, 13] </ref>.
References-found: 15

