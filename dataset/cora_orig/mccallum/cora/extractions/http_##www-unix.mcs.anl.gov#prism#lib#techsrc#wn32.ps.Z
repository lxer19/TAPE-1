URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn32.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: Spectral Division Methods for Block Generalized Schur Decompositions  
Author: Xiaobai Sun Enrique S. Quintana-Orti 
Date: August, 1996  
Abstract: This paper describes from a new perspective the inverse-free spectral division methods for block generalized Schur decompositions and presents a more efficient, accurate and stable algorithm. Even in the case that only a right deflating subspace of a matrix pencil is of interest, as in many engineering application problems, the new algorithm can be used, with a low extra cost, to obtain posterior estimates on the backward accuracy of a computed orthonormal basis for the deflating subspace. The idea behind the new algorithm can be straightforwardly applied to non-inverse-free versions of spectral division algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. F. Arnold and A. J. Laub. </author> <title> Generalized eigenproblem algorithms and software for 17 algebraic Riccati equations. </title> <journal> Proc. IEEE, </journal> <volume> 72 </volume> <pages> 1746-1754, </pages> <year> 1984. </year>
Reference-contexts: We conducted two sets of experiments. The matrix pencils (A; B) in the first set are 100 fi 100 random matrices with entries independent and normally distributed within <ref> [1; 1] </ref>. We test the algorithms for both the case of division along the imaginary axis and the case along the unit circle, cf. Section 7. Because of using the same stopping criterion, all the three algorithms require about the same number of iterations to compute the right deflating subspace. <p> The second set of experiments consists of matrix pencils specially designed with known eigenvalue structures. We present our results on three examples used in <ref> [1, 3] </ref>.
Reference: [2] <author> L. Auslander and A. Taso. </author> <title> On parallelizable eigensolvers. </title> <journal> Adv. Appl. Math., </journal> <volume> 13 </volume> <pages> 253-261, </pages> <year> 1992. </year>
Reference-contexts: The idea of the so called spectral division methods, or deflating subspace methods, is to decompose the matrix pencil into two of smaller size by first separating two complementary deflating subspaces <ref> [2, 18, 4, 5] </ref>. When necessary, the same idea can be applied to one or two of the smaller matrix pencils, recursively.
Reference: [3] <author> Z. Bai, J. W. Demmel, , and M. Gu. </author> <title> Inverse free parallel spectral divide and conquer algorithms for nonsymmetric eigenproblems. </title> <type> Technical Report RR 94-01, </type> <institution> Department of Mathematics, University of Kentucky, </institution> <year> 1994. </year> <note> To appear in Numer. Math. </note>
Reference-contexts: For k = 0; 1; : : : Q 21 Q 22 B k ! 0 , (QR factorization) A k+1 = Q 21 A k , The drawbacks of Malyshev's algorithm are circumvented by Bai, Demmel and Gu <ref> [3] </ref>. Specifically, they propose an effective inverse-free stopping criterion using the Cholesky fac tors R k of successive matrix pencils (cf. Figure 1): kR k R k1 k 1 10 n * kR k k 1 ; (4) where * is the machine precision. <p> Moreover, the two-sided 3 Malyshev iteration approach takes twice as many iterations as the one-sequence Newton iteration. An approach to compute the left spectral projectors without the left iteration is mentioned in <ref> [3] </ref> but it introduces the inverse of the converged Cholesky factor, which is possibly ill-conditioned. In this paper we present a stable inverse-free block generalized Schur decomposition method based on one-sided subspace iteration and an accurate subspace extraction scheme. This method costs no more than the less stable one in [3] <p> <ref> [3] </ref> but it introduces the inverse of the converged Cholesky factor, which is possibly ill-conditioned. In this paper we present a stable inverse-free block generalized Schur decomposition method based on one-sided subspace iteration and an accurate subspace extraction scheme. This method costs no more than the less stable one in [3] and reduces the arithmetic cost by half compared to its counterparts with two-sided iterations. <p> It is 9 then easy to see that (kA k k; kB k k) converges as k ! 1, and all convergent subsequences of (A k ; B k ) must converge to the same limit, by the continuity of the 2-norm. According to Malyshev [18] and Bai-Demmel-Gu <ref> [3] </ref>, the smallest singular value of (A t 1 ; B t 1 ) t is bounded away from 0. More precisely, it is shown in [3] that oe min ( A 1 ! where d (A;B) is defined as the distance of (A; B) to matrix pencils singular with respect <p> According to Malyshev [18] and Bai-Demmel-Gu <ref> [3] </ref>, the smallest singular value of (A t 1 ; B t 1 ) t is bounded away from 0. More precisely, it is shown in [3] that oe min ( A 1 ! where d (A;B) is defined as the distance of (A; B) to matrix pencils singular with respect to the unit circle: d (A;B) = inff kEk + kF k : det ((A+E) e i (B +F )) = 0 for some g: (14) <p> Our proof of this result is an improvement on the one given in <ref> [3] </ref> in that none of A k and B k is assumed nonsingular. 6 Accurate extraction of deflating subspaces There are two factors affecting the accuracy of the computed deflating subspaces: first, how well the cs-ab iteration preserve the deflating subspaces of the initial matrix pencil (A 0 ; B 0 <p> For perturbation analysis and error analysis of subspace preserving, see [18] and <ref> [3] </ref>. We shall be concerned in this section with subspace extraction approaches. <p> With the appropriate transformation on (A; B), the cs-ab iteration algorithms can divide the spectrum in various ways. Though such concept is applied in [18], the technique presented by Bai, Demmel and Gu <ref> [3] </ref> for the class of Mobius transformations on matrix pencils is extremely simple. The general theory behind their technique can be found in an early work by Stewart and J-G Sun [26]: Theorem 3 Let M be a nonsingular matrix. <p> Algorithm mm-inv uses two-sided Malyshev's iterations and involves inverse operations in subspace extraction; algorithm mm-gqr uses two-sided Malyshev's iteration and the generalized QR factorization scheme in subspace extraction as suggested in <ref> [3] </ref>; algorithm m-abqr uses only one-sided Malyshev's iteration and our subspace extraction scheme, which treats A 1 and B 1 equally without referring to (A 1 + B 1 ) 1 , cf. Section 4 and Section 6. <p> The second set of experiments consists of matrix pencils specially designed with known eigenvalue structures. We present our results on three examples used in <ref> [1, 3] </ref>. <p> There are no noticeable differences between the two options. Following the notation in <ref> [3] </ref> we use (A) to denote the gap between the eigenvalues and the imaginary axis: (A) = min jRe ( j )j The bigger the gap (A) is, the faster (A k ; B k ) converges, cf. Section 5. <p> k ) instead of computing or resorting to approximate spectral projectors (A k + B k ) 1 A k or (A k + B k ) 1 B k , which involve inverse operations on possibly poorly-conditioned matrices; 2) we have removed the possible inconsistency with two-sided iteration methods <ref> [3] </ref> due to two independent iterations on (A 0 ; B 0 ) and (A h 0 ; B h 0 ); 3) we have eliminated the assumption that either A k or B k is nonsingular in the convergence proof of (A k ; B k ); 4) we provide
Reference: [4] <author> Z. Bai and J. W. Demmel. </author> <title> Design of a parallel nonsymmetric eigenroutine toolbox, part I. </title> <type> Technical Report RR 92-09, </type> <institution> Department of Mathematics, University of Kentucky, </institution> <year> 1992. </year>
Reference-contexts: The idea of the so called spectral division methods, or deflating subspace methods, is to decompose the matrix pencil into two of smaller size by first separating two complementary deflating subspaces <ref> [2, 18, 4, 5] </ref>. When necessary, the same idea can be applied to one or two of the smaller matrix pencils, recursively.
Reference: [5] <author> Z. Bai and J. W. Demmel. </author> <title> Design of a parallel nonsymmetric eigenroutine toolbox, part II. </title> <type> Technical report, </type> <institution> Department of Mathematics, University of Kentucky, </institution> <year> 1996. </year>
Reference-contexts: The idea of the so called spectral division methods, or deflating subspace methods, is to decompose the matrix pencil into two of smaller size by first separating two complementary deflating subspaces <ref> [2, 18, 4, 5] </ref>. When necessary, the same idea can be applied to one or two of the smaller matrix pencils, recursively.
Reference: [6] <author> Z. Bai, J. W. Demmel, J. Dongarra, A. Petitet, and H. Robinson. </author> <title> The spectral decomposition of nonsymmetric matrices on distributed memory multiprocessors. </title> <type> Technical Report CS-95-273, </type> <institution> Computer Science Dept., University of Tennessee at Knoxville, </institution> <year> 1995. </year> <note> to appear in SIAM J. Sci. Comp. </note>
Reference: [7] <author> C. H. Bischof and G. Quintana-Orti. </author> <title> Computing rank-revealing QR factorizations of dense matrices. </title> <type> Technical Report MCS-P559-0196, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1996. </year> <note> to appear in ACM Trans. Math. Soft. </note>
Reference: [8] <author> R. Byers, C. He, and V. Mehrmann. </author> <title> The matrix sign function and the computation of invariant subspaces. </title> <type> Technical Report SPC 94-25, </type> <institution> Faculty of Mathematics, Technische Universitt Chemnitz-Zwickau, </institution> <year> 1994. </year> <note> to appear in SIAM J. Mat. Anal. Appl. </note>
Reference: [9] <author> T. Chan. </author> <title> Rank-revealing QR factorizations. </title> <journal> Lin. Alg. Appl., </journal> 88/89:67-82, 1987. 
Reference: [10] <author> J. Gardiner and A. J. Laub. </author> <title> A generalisation fo the matrix sign function solution for algebraic Riccati equations. </title> <journal> Int. J. Control, </journal> <volume> 44 </volume> <pages> 823-832, </pages> <year> 1986. </year>
Reference-contexts: Section 3). The Newton iteration for computing the matrix sign function is first introduced by Roberts [22], and its version for matrix pencils, presented in <ref> [10] </ref>, is defined as: A k+1 = (A k + BA 1 The pencil sequence (A k ; B) of (2) has the following property. <p> We shall describe the pioneering works of [3,18] in other aspects in later sections when related topics are discussed. In <ref> [10, 18, 22] </ref>, the solution of the algebraic Riccati equation [17] is concerned and in such case only the right deflating subspace is of interest. The idea of the deflating subspace methods is generalized in [3,5] to computing the block Schur decomposition of matrix pencils as shown in (1).
Reference: [11] <author> S. K. </author> <title> Godunov. The problem of dichotomy of the spectrum of a matrix. </title> <journal> Siberian Math. J., </journal> <volume> 27(5) </volume> <pages> 649-660, </pages> <year> 1986. </year>
Reference-contexts: Notice that there is no inverse involved in the iteration. We also note in passing that a singular pencil can be detected at the normalization step. The theoretical foundation of Malyshev's iteration is related to previous works of Godunov and Bulgakov, among others (see <ref> [11, 15, 16] </ref> and references therein). While the Newton iteration divides the spectrum along the imaginary axis, Malyshev's iteration divides the spectrum along the unit circle.
Reference: [12] <author> G. Golub and C. Van Loan. </author> <title> Matrix computations. </title> <publisher> The John Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction In this paper we discuss and present modifications on spectral division algorithms for computing block Schur decompositions <ref> [12, 26] </ref> of a matrix pencil (A; B), where A and B are square matrices of order n.
Reference: [13] <author> P. Hong and C. T. Pan. </author> <title> The rank revealing QR and SVD. </title> <journal> Math. Comp., </journal> <volume> 58 </volume> <pages> 213-232, </pages> <year> 1992. </year>
Reference: [14] <author> C. Kenney and A. J. Laub. </author> <title> Rational iteration methods for the matrix sign function. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 21 </volume> <pages> 487-494, </pages> <year> 1991. </year> <month> 18 </month>
Reference-contexts: Due to the inverse operation at each step, the Newton iteration is sensitive to perturbations and rounding errors in numerical computation when matrix A is poorly conditioned. There are inverse free iteration schemes for the matrix sign function, such as Newton-Schulz iteration <ref> [14] </ref>, at the expense of loosing global convergence. For more details on numerical computation and high performance implementation of the Newton iteration, see, for example, [4,5,6,8,19]. Malyshev presented an alternative scheme, see Figure 1, for the spectral division [18]. Notice that there is no inverse involved in the iteration. <p> We thus have A k+1 = A 2 0 and B k+1 = B 2 0 . This is in fact the Cayley power method <ref> [14] </ref> for the matrix sign function with B 0 = A B and A 0 = A + B.
Reference: [15] <author> V.N. Kublanovskaya. </author> <title> An approach to solving the spectal problem of a b. </title> <editor> In B. Kagstrom and A. Ruhe, editors, </editor> <booktitle> Matrix Pencils, </booktitle> <pages> pages 17-29, </pages> <year> 1983. </year>
Reference-contexts: Notice that there is no inverse involved in the iteration. We also note in passing that a singular pencil can be detected at the normalization step. The theoretical foundation of Malyshev's iteration is related to previous works of Godunov and Bulgakov, among others (see <ref> [11, 15, 16] </ref> and references therein). While the Newton iteration divides the spectrum along the imaginary axis, Malyshev's iteration divides the spectrum along the unit circle. <p> J. Dongarra for bringing our attention to the open problems with spectral division methods; to C. H. Bischof for his discussions with us on using the CS decomposition theory; to L. Elsner for his suggestion to investigate in links between the inverse free algorithms and the AB algorithms <ref> [15] </ref>, one of which turns out to be the cs-ab equivalence rule; to D. O'Leary for introducing to us some of the references and related work; and to G. W. Stewart for his comments and suggestions through our work presented in this paper.
Reference: [16] <author> V.N. Kublanovskaya. </author> <title> AB algorithm and its modifications for the spectral problem of linear pencils of matrices. </title> <journal> Numer. Math., </journal> <volume> 43 </volume> <pages> 329-342, </pages> <year> 1984. </year>
Reference-contexts: Notice that there is no inverse involved in the iteration. We also note in passing that a singular pencil can be detected at the normalization step. The theoretical foundation of Malyshev's iteration is related to previous works of Godunov and Bulgakov, among others (see <ref> [11, 15, 16] </ref> and references therein). While the Newton iteration divides the spectrum along the imaginary axis, Malyshev's iteration divides the spectrum along the unit circle.
Reference: [17] <author> H. Kwakernaak and R. Sivan. </author> <title> Linear optimal control systems. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: We shall describe the pioneering works of [3,18] in other aspects in later sections when related topics are discussed. In [10, 18, 22], the solution of the algebraic Riccati equation <ref> [17] </ref> is concerned and in such case only the right deflating subspace is of interest. The idea of the deflating subspace methods is generalized in [3,5] to computing the block Schur decomposition of matrix pencils as shown in (1).
Reference: [18] <author> A. N. Malyshev. </author> <title> Parallel algorithm for solvoing some spectral problems of linear algebra. </title> <journal> Lin. Alg. Appl., </journal> 188/189:489-520, 1993. 
Reference-contexts: The idea of the so called spectral division methods, or deflating subspace methods, is to decompose the matrix pencil into two of smaller size by first separating two complementary deflating subspaces <ref> [2, 18, 4, 5] </ref>. When necessary, the same idea can be applied to one or two of the smaller matrix pencils, recursively. <p> For more details on numerical computation and high performance implementation of the Newton iteration, see, for example, [4,5,6,8,19]. Malyshev presented an alternative scheme, see Figure 1, for the spectral division <ref> [18] </ref>. Notice that there is no inverse involved in the iteration. We also note in passing that a singular pencil can be detected at the normalization step. <p> We shall describe the pioneering works of [3,18] in other aspects in later sections when related topics are discussed. In <ref> [10, 18, 22] </ref>, the solution of the algebraic Riccati equation [17] is concerned and in such case only the right deflating subspace is of interest. The idea of the deflating subspace methods is generalized in [3,5] to computing the block Schur decomposition of matrix pencils as shown in (1). <p> Within the framework of cs-ab iterations, Malyshev iteration is overflow free and guarantees global convergence with the assumptions that (A 0 ; B 0 ) is regular with respect to the unit circle <ref> [18] </ref>, i.e., det (A B) 6= 0 with jj = 1, and that (A 0 ; B 0 ) has eigenvalues on both sides of the unit circle. <p> It is 9 then easy to see that (kA k k; kB k k) converges as k ! 1, and all convergent subsequences of (A k ; B k ) must converge to the same limit, by the continuity of the 2-norm. According to Malyshev <ref> [18] </ref> and Bai-Demmel-Gu [3], the smallest singular value of (A t 1 ; B t 1 ) t is bounded away from 0. <p> For perturbation analysis and error analysis of subspace preserving, see <ref> [18] </ref> and [3]. We shall be concerned in this section with subspace extraction approaches. <p> With the appropriate transformation on (A; B), the cs-ab iteration algorithms can divide the spectrum in various ways. Though such concept is applied in <ref> [18] </ref>, the technique presented by Bai, Demmel and Gu [3] for the class of Mobius transformations on matrix pencils is extremely simple. The general theory behind their technique can be found in an early work by Stewart and J-G Sun [26]: Theorem 3 Let M be a nonsingular matrix.
Reference: [19] <author> P. Pandey, C. Kenney, and A. J. Laub. </author> <title> A parallel algorithm for the matrix sign function. </title> <journal> Int. J. of High Speed Computing, </journal> <volume> 2 </volume> <pages> 181-191, </pages> <year> 1990. </year>
Reference: [20] <author> G. Quintana, X. Sun, and C. H. Bischof. </author> <title> A BLAS-3 version of the QR factorization with column pivoting. </title> <type> Technical Report MCS-P551-1295, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1996. </year> <note> to appear in SIAM J. Sci. Comp. </note>
Reference: [21] <author> G. Quintana-Orti and E. S. Quintana-Orti. </author> <title> Parallel algorithms for computing rank-revealing QR factorizations. </title> <booktitle> In High performance computing and networking, </booktitle> <year> 1996. </year> <note> to appear. </note>
Reference: [22] <author> J. Roberts. </author> <title> Linear model reduction and solution of algebraic Riccati equations by the use of the sign function. </title> <journal> Int. J. Control, </journal> <volume> 32 </volume> <pages> 677-687, </pages> <year> 1980. </year>
Reference-contexts: Section 3). The Newton iteration for computing the matrix sign function is first introduced by Roberts <ref> [22] </ref>, and its version for matrix pencils, presented in [10], is defined as: A k+1 = (A k + BA 1 The pencil sequence (A k ; B) of (2) has the following property. <p> We shall describe the pioneering works of [3,18] in other aspects in later sections when related topics are discussed. In <ref> [10, 18, 22] </ref>, the solution of the algebraic Riccati equation [17] is concerned and in such case only the right deflating subspace is of interest. The idea of the deflating subspace methods is generalized in [3,5] to computing the block Schur decomposition of matrix pencils as shown in (1).
Reference: [23] <author> G. W. Stewart. </author> <title> On the sensitivity of the eigenvalue problem. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 9 </volume> <pages> 669-686, </pages> <year> 1972. </year>
Reference-contexts: Section 5. To estimate the sensitivity of subspaces to perturbations in matrix pencils, we compute a lower bound of the quantity dif introduced by Stewart <ref> [23] </ref>: dif [(A 11 ; B 11 ); (A 22 ; B 22 )] = inf maxfkP k F ;kQk F g maxfkQA 11 + A 22 P k F ; kQB 11 + B 22 P k F g: The lower bound we compute is the smallest singular value of <p> A 22 P k F ; kQB 11 + B 22 P k F g: The lower bound we compute is the smallest singular value of the matrix operator T that maps (Q; P ) to (QA 11 + A 22 P; QB 11 + B 22 P ), cf. <ref> [23] </ref>. Example 1.
Reference: [24] <author> G. W. Stewart. </author> <title> Perturbation theory for the generalized eigenvalue problem. </title> <editor> In G. Golub C. de Boor, editor, </editor> <booktitle> Recent advances in numerical analysis, </booktitle> <pages> pages 193-206. </pages> <publisher> Academic Press, </publisher> <year> 1978. </year>
Reference-contexts: There are cases where it is necessary to write the eigenvalue problem in the cross-product form fiAx = ffBx as suggested by Stewart <ref> [24] </ref>; in other cases we use for convenience the traditional, asymmetrical definition Ax = Bx. 3 Malyshev-like iterations We introduce in this section basic algebraic properties of the Malyshev iteration and Malyshev-like iterations.
Reference: [25] <author> G. W. Stewart. </author> <title> An updating algorithm for subspace tracking. </title> <journal> IEEE Trans. Signal Processing, </journal> <volume> 40 </volume> <pages> 1535-1541, </pages> <year> 1992. </year>
Reference-contexts: To extract a deflating subspace from, say, A 1 +B we need to separate the range and nullspace of A 1 +B. Many effective numerical algorithms for such task have been further developed in recent years, such as rank-revealing QR factorizations (RR-QRF) [7,9,13,20,21], and rank-revealing URV <ref> [25] </ref>. Due to the inverse operation at each step, the Newton iteration is sensitive to perturbations and rounding errors in numerical computation when matrix A is poorly conditioned.
Reference: [26] <author> G. W. Stewart and J. G. Sun. </author> <title> Matrix perturbation theory. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction In this paper we discuss and present modifications on spectral division algorithms for computing block Schur decompositions <ref> [12, 26] </ref> of a matrix pencil (A; B), where A and B are square matrices of order n. <p> permutation matrix (or a unitary matrix) in a rank-revealing decomposition. 2 Block Schur decompositions Our stable scheme to obtain both right and left deflating subspaces with one-sided iteration is based on the very idea of a direct construction proof for the generalized Schur decomposition of matrix pencils, see, for example, <ref> [26] </ref>. 4 Theorem 1 (Generalized Schur Decomposition - Moler and Stewart, 1973) If A and B are in IC nfin , then there exist unitary matrices Q and Z such that Q h AZ = S and Q h BZ = T are upper triangular. <p> By Weierstrass's canonical form <ref> [26] </ref>, or, the spectral resolution theorem by Stewart and J-G Sun [26], we may let X in be a basis matrix for the deflating subspace corresponding to the eigenvalues inside the unit circle fl fjj &lt; 1g, i.e., A X in = BX in fl. <p> By Weierstrass's canonical form <ref> [26] </ref>, or, the spectral resolution theorem by Stewart and J-G Sun [26], we may let X in be a basis matrix for the deflating subspace corresponding to the eigenvalues inside the unit circle fl fjj &lt; 1g, i.e., A X in = BX in fl. <p> Though such concept is applied in [18], the technique presented by Bai, Demmel and Gu [3] for the class of Mobius transformations on matrix pencils is extremely simple. The general theory behind their technique can be found in an early work by Stewart and J-G Sun <ref> [26] </ref>: Theorem 3 Let M be a nonsingular matrix. Given the matrix pencil (A; B), set (C; D) = (A; B)(M I 2 ). <p> The theory on decoupling a matrix pencil into two of smaller size with one known or computed deflating subspace is long established, see, for example, Stewart and J-G. Sun <ref> [26] </ref>. The same technique is used by Van Dooren [27] for computing reducing subspaces of singular pencils.
Reference: [27] <author> P. Van Dooren. </author> <title> Reducing subspaces: Definitions, properties and algorithms. </title> <editor> In B. Kagstrom and A. Ruhe, editors, </editor> <booktitle> Matrix pencils, </booktitle> <pages> pages 17-29, </pages> <year> 1983. </year> <month> 19 </month>
Reference-contexts: The theory on decoupling a matrix pencil into two of smaller size with one known or computed deflating subspace is long established, see, for example, Stewart and J-G. Sun [26]. The same technique is used by Van Dooren <ref> [27] </ref> for computing reducing subspaces of singular pencils.
References-found: 27

