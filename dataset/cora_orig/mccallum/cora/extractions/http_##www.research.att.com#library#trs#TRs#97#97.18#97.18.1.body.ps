URL: http://www.research.att.com/library/trs/TRs/97/97.18/97.18.1.body.ps
Refering-URL: http://www.research.att.com/library/trs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Speech Recognition by Composition of Weighted Finite Automata  
Author: Fernando C. N. Pereira Michael D. Riley 
Date: April 20, 1996  
Address: 600 Mountain Ave., Murray Hill, NJ 07974  
Affiliation: AT&T Labs Research  
Abstract: We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Lalit R. Bahl, Fred Jelinek, and Robert Mercer. </author> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Trans. PAMI, </journal> <volume> 5(2) </volume> <pages> 179-190, </pages> <month> March </month> <year> 1983. </year>
Reference-contexts: The highest-probability path through that automaton estimates the most likely word sequence for the given utterance. The finite-state model of speech recognition that we have just described is hardly novel. In fact, it is equivalent to that presented in <ref> [1] </ref>, in the sense that it generates the same weighted language. However, the transduction cascade approach presented here allows one to view the computations in new ways. <p> In a traditional integrated-search recognizer, a single large transducer is built in advance by R = A ./ D ./ M , and used in recognition to compute argmax w 2 (O ./ R)(w) for each observation sequence O <ref> [1] </ref>. This approach is not practical if the size of R exceeds available memory, as is typically the case for large-vocabulary speech recognition with n-gram language models for n &gt; 2. <p> This replacement of a symbol from one alphabet (for example, a word) by the automaton that represents its substituted language from a over a finer-grained alphabet (for example, phones) is the usual stage-combination operation for speech rec-ognizers <ref> [1] </ref>. However, it has been shown that context-dependent phone models, which model a phone in the context of its adjacent phones, provide substantial improvements in recognition accuracy [10]. Further, the pronunciation of a word will be affected by its neighboring words, inducing context dependencies across word boundaries.
Reference: [2] <author> Jean Berstel. </author> <title> Transductions and Context-Free Languages. Number 38 in Leitfaden der angewandten Mathematik and Mechanik LAMM. </title> <address> Teub-ner Studienbucher, Stuttgart, Germany, </address> <year> 1979. </year>
Reference-contexts: Finally, finite-state minimization techniques can be used to reduce the size of cascade levels and thus improve recognition efficiency [12]. Weighted languages and transductions are generalizations of the standard notions of language and transduction in formal language theory <ref> [2, 6] </ref>. A weighted language is a mapping from strings over an alphabet to weights, while a weighted transduction is a mapping from pairs of strings over two alphabets to weights. <p> The notion of weighted rational transduction arises from the combination of two ideas in automata theory: rational transductions, used in many aspects of formal language theory <ref> [2] </ref>, and weighted languages and automata, developed in pattern recognition [4, 15] and algebraic automata theory [3, 5, 8].
Reference: [3] <author> Jean Berstel and Christophe Reutenauer. </author> <title> Rational Series and Their Languages. </title> <booktitle> Number 12 in EATCS Monographs on Theoretical Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1988. </year>
Reference-contexts: The notion of weighted rational transduction arises from the combination of two ideas in automata theory: rational transductions, used in many aspects of formal language theory [2], and weighted languages and automata, developed in pattern recognition [4, 15] and algebraic automata theory <ref> [3, 5, 8] </ref>. Ordinary (unweighted) rational transductions have been successfully applied by researchers at Xerox PARC [7] and at the University of Paris 7 [13, 14, 20, 21], among others, to several problems in language processing, including morphological analysis, dictionary compression and syntactic analysis. <p> Both of these weight structures are special cases of commutative semirings, which are the basis of the general theory of weighted languages, transductions and automata <ref> [3, 5, 8] </ref>.
Reference: [4] <author> Taylor R. Booth and Richard A. Thompson. </author> <title> Applying probability measures to abstract languages. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-22(5):442-450, </volume> <month> May </month> <year> 1973. </year>
Reference-contexts: The notion of weighted rational transduction arises from the combination of two ideas in automata theory: rational transductions, used in many aspects of formal language theory [2], and weighted languages and automata, developed in pattern recognition <ref> [4, 15] </ref> and algebraic automata theory [3, 5, 8]. Ordinary (unweighted) rational transductions have been successfully applied by researchers at Xerox PARC [7] and at the University of Paris 7 [13, 14, 20, 21], among others, to several problems in language processing, including morphological analysis, dictionary compression and syntactic analysis. <p> More generally, a closed semiring is one in which collecting over infinite sets is well defined. Finally, some particular cases arising in the discussion below can be shown to be well defined for the plus-times semiring under certain mild conditions on the weights assigned to strings or automata transitions <ref> [4, 8] </ref>. 2.2 Weighted Transductions and Languages In the transduction cascade (1), each stage corresponds to a mapping from input-output pairs (r; s) to probabilities P (sjr).
Reference: [5] <author> Samuel Eilenberg. </author> <title> Automata, Languages, and Machines, volume A. </title> <publisher> Academic Press, </publisher> <address> San Diego, California, </address> <year> 1974. </year>
Reference-contexts: The notion of weighted rational transduction arises from the combination of two ideas in automata theory: rational transductions, used in many aspects of formal language theory [2], and weighted languages and automata, developed in pattern recognition [4, 15] and algebraic automata theory <ref> [3, 5, 8] </ref>. Ordinary (unweighted) rational transductions have been successfully applied by researchers at Xerox PARC [7] and at the University of Paris 7 [13, 14, 20, 21], among others, to several problems in language processing, including morphological analysis, dictionary compression and syntactic analysis. <p> Both of these weight structures are special cases of commutative semirings, which are the basis of the general theory of weighted languages, transductions and automata <ref> [3, 5, 8] </ref>. <p> Generalized to the weighted case and to transductions, it states that weighted rational languages and transductions are exactly those that can be represented by weighted finite automata <ref> [5, 8] </ref>. Furthermore, all the operations on languages and transductions we have discussed have finite-automata counterparts, which we have implemented. Any cascade representable in terms of those operations can thus be implemented directly as an appropriate combination of the programs implementing each of the operations.
Reference: [6] <author> Michael A. Harrison. </author> <title> Introduction to Formal Language Theory. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachussets, </address> <year> 1978. </year>
Reference-contexts: Finally, finite-state minimization techniques can be used to reduce the size of cascade levels and thus improve recognition efficiency [12]. Weighted languages and transductions are generalizations of the standard notions of language and transduction in formal language theory <ref> [2, 6] </ref>. A weighted language is a mapping from strings over an alphabet to weights, while a weighted transduction is a mapping from pairs of strings over two alphabets to weights. <p> In addition to composition, weighted trans-ductions (and languages, given the identification of languages with trans-ductions presented earlier) can be constructed from simpler ones using the operations shown in Table 1, which generalize in a straightforward way the regular operations well-known from traditional automata theory <ref> [6] </ref>. In fact, the rational languages and transductions are exactly those that can be built from singletons by applications of scaling, sum, concatenation and closure. <p> rational transduction ( P w D w ) fl gives the probabilities for realizations of word sequences as phone sequences if we leave aside cross-word context dependencies, which will be discussed in Section 3. 2.3 Weighted Automata Kleene's theorem states that regular languages are exactly those representable by finite-state acceptors <ref> [6] </ref>. Generalized to the weighted case and to transductions, it states that weighted rational languages and transductions are exactly those that can be represented by weighted finite automata [5, 8]. Furthermore, all the operations on languages and transductions we have discussed have finite-automata counterparts, which we have implemented.
Reference: [7] <author> Ronald M. Kaplan and Martin Kay. </author> <title> Regular models of phonological rule systems. </title> <journal> Computational Linguistics, </journal> <volume> 3(20) </volume> <pages> 331-378, </pages> <year> 1994. </year>
Reference-contexts: Ordinary (unweighted) rational transductions have been successfully applied by researchers at Xerox PARC <ref> [7] </ref> and at the University of Paris 7 [13, 14, 20, 21], among others, to several problems in language processing, including morphological analysis, dictionary compression and syntactic analysis. HMMs and probabilistic finite-state language models can be shown to be equivalent to WFSAs.
Reference: [8] <editor> Werner Kuich and Arto Salomaa. Semirings, </editor> <booktitle> Automata, Languages. Number 5 in EATCS Monographs on Theoretical Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1986. </year> <month> 19 </month>
Reference-contexts: The notion of weighted rational transduction arises from the combination of two ideas in automata theory: rational transductions, used in many aspects of formal language theory [2], and weighted languages and automata, developed in pattern recognition [4, 15] and algebraic automata theory <ref> [3, 5, 8] </ref>. Ordinary (unweighted) rational transductions have been successfully applied by researchers at Xerox PARC [7] and at the University of Paris 7 [13, 14, 20, 21], among others, to several problems in language processing, including morphological analysis, dictionary compression and syntactic analysis. <p> HMMs and probabilistic finite-state language models can be shown to be equivalent to WFSAs. In algebraic automata theory, rational series and rational transductions <ref> [8] </ref> are the algebraic counterparts of WF-SAs and WFSTs and give the correct generalizations to the weighted case of the standard algebraic operations on formal languages and transductions, such as union, concatenation, intersection, restriction and composition. We believe our work is the first application of these generalizations to speech processing. <p> Both of these weight structures are special cases of commutative semirings, which are the basis of the general theory of weighted languages, transductions and automata <ref> [3, 5, 8] </ref>. <p> More generally, a closed semiring is one in which collecting over infinite sets is well defined. Finally, some particular cases arising in the discussion below can be shown to be well defined for the plus-times semiring under certain mild conditions on the weights assigned to strings or automata transitions <ref> [4, 8] </ref>. 2.2 Weighted Transductions and Languages In the transduction cascade (1), each stage corresponds to a mapping from input-output pairs (r; s) to probabilities P (sjr). <p> Generalized to the weighted case and to transductions, it states that weighted rational languages and transductions are exactly those that can be represented by weighted finite automata <ref> [5, 8] </ref>. Furthermore, all the operations on languages and transductions we have discussed have finite-automata counterparts, which we have implemented. Any cascade representable in terms of those operations can thus be implemented directly as an appropriate combination of the programs implementing each of the operations. <p> by [[A]] = L A (i A ) . (8) The appropriate generalization of Kleene's theorem to weighted acceptors and transducers states that under suitable conditions guaranteeing that the inner sum in (7) is defined, weighted rational languages and transductions are exactly those defined by weighted automata as outlined here <ref> [8] </ref>. Weighted acceptors and transducers are thus faithful implementations of rational languages and transductions, and all the operations on these described above have corresponding implementations in terms of algorithms on automata.
Reference: [9] <author> Bernard Lang. </author> <title> A generative view of ill-formed input processing. In ATR Symposium on Basic Research for Telephone Interpretation, </title> <address> Ky-oto, Japan, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: In this paper we will be concerned with the weighted rational case, although some of the theory can be profitably extended more general language classes closed under intersection with regular languages and composition with rational transductions <ref> [9, 23] </ref>. The notion of weighted rational transduction arises from the combination of two ideas in automata theory: rational transductions, used in many aspects of formal language theory [2], and weighted languages and automata, developed in pattern recognition [4, 15] and algebraic automata theory [3, 5, 8].
Reference: [10] <author> Kai-Fu Lee. </author> <title> Context dependent phonetic hidden Markov models for continuous speech recognition. </title> <journal> IEEE Trans. ASSP, </journal> <volume> 38(4) </volume> <pages> 599-609, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: However, it has been shown that context-dependent phone models, which model a phone in the context of its adjacent phones, provide substantial improvements in recognition accuracy <ref> [10] </ref>. Further, the pronunciation of a word will be affected by its neighboring words, inducing context dependencies across word boundaries. We could include context-dependent models, such as triphone models, in our presentation by expanding our `atomic models' in A to one for every phone in a distinct triphonic context.
Reference: [11] <author> Andrej Ljolje and Michael D. Riley. </author> <title> Optimal speech recognition using phone recognition and lexical access. </title> <booktitle> In Proceedings of ICSLP, </booktitle> <pages> pages 313-316, </pages> <address> Banff, Canada, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The whole lattice typically too big, so the computation includes a pruning mechanism that generates only those states and transitions that appear in high-probability paths. This lattice is in turn transduced into a word lattice (word recognition), again possibly with pruning, which is then composed with the language model <ref> [11, 18] </ref>. The best approach depends on the specific task, which determines the size of intermediate results. By having a general package to manipulate weighted automata, we have been able to experiment with various alternatives. So far, our presentation has used context-independent phone models.
Reference: [12] <author> Mehryar Mohri. </author> <title> On the use of sequential transducers in natural language processing. This volume. </title>
Reference-contexts: Finally, finite-state minimization techniques can be used to reduce the size of cascade levels and thus improve recognition efficiency <ref> [12] </ref>. Weighted languages and transductions are generalizations of the standard notions of language and transduction in formal language theory [2, 6]. A weighted language is a mapping from strings over an alphabet to weights, while a weighted transduction is a mapping from pairs of strings over two alphabets to weights. <p> More complex forms of context dependency such as those based on classification trees over a bounded neighborhood of the target phone can too be compiled into appropriate transducers and interposed in the recognition cascade without changing any aspect of the recognition algorithm. Transducer determinization and minimization techniques <ref> [12] </ref> can be used to make context-dependency transducers as compact as possible. 4 Implementation The transducer operations described in this paper, together with a variety of support functions, have been implemented in C. <p> With Emerald Chung, we have been refining the connection between a time-synchronous Viterbi decoder and lazy composition to improve time and space efficiency. With Mehryar Mohri, we have been developing improved composition filters, as well as exploring on-the-fly and local determinization techniques for transducers and weighted automata <ref> [12] </ref> to decrease the impact of nondeterminism on the size (and thus the time required to create) composed automata.
Reference: [13] <author> Mehryar Mohri. </author> <title> Compact representations by finite-state transducers. </title> <booktitle> In 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> San Francisco, California, 1994. New Mexico State University, Las Cruces, New Mexico, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Ordinary (unweighted) rational transductions have been successfully applied by researchers at Xerox PARC [7] and at the University of Paris 7 <ref> [13, 14, 20, 21] </ref>, among others, to several problems in language processing, including morphological analysis, dictionary compression and syntactic analysis. HMMs and probabilistic finite-state language models can be shown to be equivalent to WFSAs.
Reference: [14] <author> Mehryar Mohri. </author> <title> Syntactic analysis by local grammars and automata: an efficient algorithm. </title> <booktitle> In Proceedings of the International Conference on Computational Lexicography (COMPLEX 94), </booktitle> <address> Budapest, Hungary, </address> <year> 1994. </year> <institution> Linguistic Institute, Hungarian Academy of Sciences. </institution>
Reference-contexts: Ordinary (unweighted) rational transductions have been successfully applied by researchers at Xerox PARC [7] and at the University of Paris 7 <ref> [13, 14, 20, 21] </ref>, among others, to several problems in language processing, including morphological analysis, dictionary compression and syntactic analysis. HMMs and probabilistic finite-state language models can be shown to be equivalent to WFSAs.
Reference: [15] <author> A. Paz. </author> <title> Introduction to Probabilistic Automata. </title> <publisher> Academic, </publisher> <year> 1971. </year>
Reference-contexts: The notion of weighted rational transduction arises from the combination of two ideas in automata theory: rational transductions, used in many aspects of formal language theory [2], and weighted languages and automata, developed in pattern recognition <ref> [4, 15] </ref> and algebraic automata theory [3, 5, 8]. Ordinary (unweighted) rational transductions have been successfully applied by researchers at Xerox PARC [7] and at the University of Paris 7 [13, 14, 20, 21], among others, to several problems in language processing, including morphological analysis, dictionary compression and syntactic analysis.
Reference: [16] <author> Fernando C. N. Pereira, Michael Riley, and Richard W. Sproat. </author> <title> Weighted rational transductions and their application to human language processing. </title> <booktitle> In Human Language Technology Workshop, </booktitle> <pages> pages 262-267, </pages> <address> San Francisco, California, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We would also like to thank Raffaele Giancarlo, Isabelle Guyon, Carsten Lund and Yoram Singer as well as the editors of this volume for many helpful comments. Portions of this paper are adapted from a paper presented at the 1994 ARPA Human Language Technology Workshop <ref> [16] </ref>.
Reference: [17] <author> Giuseppe Riccardi, Enrico Bocchieri, and Roberto Pieraccini. </author> <title> Nondeterministic stochastic language models for speech recognition. </title> <booktitle> In Proceedings IEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 237-240. </pages> <publisher> IEEE, </publisher> <year> 1995. </year>
Reference-contexts: The ARPA ATIS task, for example, uses a context transducer with 40,386 transitions, a dictionary with 4,816 transitions and a class-based variable-length n-gram language model <ref> [17] </ref> with 359,532 transitions. The composition of these three automata would have around 6 fi 10 6 transitions.
Reference: [18] <author> Michael Riley, Andrej Ljolje, Donald Hindle, and Fernando C. N. Pereira. </author> <title> The AT&T 60,000 word speech-to-text system. </title> <editor> In J. M. Pardo, E. Enrquez, J. Ortega, J. Ferreiros, J. Macas, and F.J.Valverde, editors, Eurospeech'95: </editor> <booktitle> ESCA 4th European Conference on Speech Communication and Technology, </booktitle> <volume> volume 1, </volume> <pages> pages 207-210, </pages> <address> Madrid, 20 Spain, </address> <month> September </month> <year> 1995. </year> <booktitle> European Speech Communication Association (ESCA). </booktitle>
Reference-contexts: The whole lattice typically too big, so the computation includes a pruning mechanism that generates only those states and transitions that appear in high-probability paths. This lattice is in turn transduced into a word lattice (word recognition), again possibly with pruning, which is then composed with the language model <ref> [11, 18] </ref>. The best approach depends on the specific task, which determines the size of intermediate results. By having a general package to manipulate weighted automata, we have been able to experiment with various alternatives. So far, our presentation has used context-independent phone models. <p> has been carefully designed to allow for efficient transition matching while hiding the details of transition indexing and sorting. 5 Applications We have used our implementation in a variety of speech recognition and language processing tasks, including continuous speech recognition in the 60,000-word ARPA North American Business News (NAB) task <ref> [18] </ref> and the 2,000-word ARPA ATIS task, isolated word recognition for directory lookup tasks, and segmentation of Chinese text into words [22]. The NAB task is by far the largest one we have attempted so far. In our 1994 experiments [18], we used a 60,000-word vocabulary, and several very large automata, <p> in the 60,000-word ARPA North American Business News (NAB) task <ref> [18] </ref> and the 2,000-word ARPA ATIS task, isolated word recognition for directory lookup tasks, and segmentation of Chinese text into words [22]. The NAB task is by far the largest one we have attempted so far. In our 1994 experiments [18], we used a 60,000-word vocabulary, and several very large automata, including a phone-to-syllable transducer with 5 fi 10 5 transitions, a syllable-to-word (dictionary) transducer with 10 5 transitions and a language model (5-gram) with 3:4fi10 7 transitions.
Reference: [19] <author> Michael Riley, Fernando Pereira, and Emerald Chung. </author> <title> Lazy transducer composition: a flexible method for on-the-fly expansion of context-dependent grammar network. </title> <booktitle> IEEE Automatic Speech Recognition Workshop, </booktitle> <address> Snowbird, Utah, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Instead, we have developed a lazy implementation of composition, in which the states and arcs of the composed automaton are created by pairing states and arcs in the composition arguments only as they are required by some other operation, such as search, on the composed automaton <ref> [19] </ref>. The use of an abstract datatype for automata facilitates this, since functions operating on automata do not need to distinguish between concrete and lazy automata. <p> The composition of these three automata would have around 6 fi 10 6 transitions. However, for a typical sentence only around 5% of those transitions are actually visited <ref> [19] </ref>. 6 Further Work We have been investigating a variety of improvements, extensions and applications of the present work. With Emerald Chung, we have been refining the connection between a time-synchronous Viterbi decoder and lazy composition to improve time and space efficiency.
Reference: [20] <author> Emmanuel Roche. </author> <title> Analyse Syntaxique Transformationelle du Fran~cais par Transducteurs et Lexique-Grammaire. </title> <type> PhD thesis, </type> <institution> Universite Paris 7, </institution> <year> 1993. </year>
Reference-contexts: Ordinary (unweighted) rational transductions have been successfully applied by researchers at Xerox PARC [7] and at the University of Paris 7 <ref> [13, 14, 20, 21] </ref>, among others, to several problems in language processing, including morphological analysis, dictionary compression and syntactic analysis. HMMs and probabilistic finite-state language models can be shown to be equivalent to WFSAs.
Reference: [21] <editor> Max Silberztein. Dictionnaires electroniques et analise automatique de textes: le systeme INTEX. </editor> <publisher> Masson, </publisher> <address> Paris, France, </address> <year> 1993. </year>
Reference-contexts: Ordinary (unweighted) rational transductions have been successfully applied by researchers at Xerox PARC [7] and at the University of Paris 7 <ref> [13, 14, 20, 21] </ref>, among others, to several problems in language processing, including morphological analysis, dictionary compression and syntactic analysis. HMMs and probabilistic finite-state language models can be shown to be equivalent to WFSAs.
Reference: [22] <author> Richard Sproat, Chilin Shih, Wiliam Gale, and Nancy Chang. </author> <title> A stochastic finite-state word-segmentation algorithm for Chinese. </title> <booktitle> In 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 66-73, </pages> <address> San Francisco, California, 1994. New Mexico State University, Las Cruces, New Mexico, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We believe our work is the first application of these generalizations to speech processing. While we concentrate here on speech recognition applications, the same framework and tools have also been applied to other language processing tasks such as the segmentation of Chinese text into words <ref> [22] </ref>. We explain how a standard HMM-based recognizer can be naturally viewed as equivalent to a cascade of weighted transductions, and how the approach requires no modification to accommodate context dependencies that cross higher-level unit boundaries, for instance cross-word context-dependent models. <p> We have used our implementation in a variety of speech recognition and language processing tasks, including continuous speech recognition in the 60,000-word ARPA North American Business News (NAB) task [18] and the 2,000-word ARPA ATIS task, isolated word recognition for directory lookup tasks, and segmentation of Chinese text into words <ref> [22] </ref>. The NAB task is by far the largest one we have attempted so far.
Reference: [23] <author> Ray Teitelbaum. </author> <title> Context-free error analysis by evaluation of algebraic power series. </title> <booktitle> In Proc. Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 196-199, </pages> <address> Austin, Texas, </address> <year> 1973. </year>
Reference-contexts: In this paper we will be concerned with the weighted rational case, although some of the theory can be profitably extended more general language classes closed under intersection with regular languages and composition with rational transductions <ref> [9, 23] </ref>. The notion of weighted rational transduction arises from the combination of two ideas in automata theory: rational transductions, used in many aspects of formal language theory [2], and weighted languages and automata, developed in pattern recognition [4, 15] and algebraic automata theory [3, 5, 8].
References-found: 23

