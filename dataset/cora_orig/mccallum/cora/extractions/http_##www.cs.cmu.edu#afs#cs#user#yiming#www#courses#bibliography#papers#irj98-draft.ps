URL: http://www.cs.cmu.edu/afs/cs/user/yiming/www/courses/bibliography/papers/irj98-draft.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/yiming/www/courses/bibliography/papers/
Root-URL: 
Email: yiming@cs.cmu.edu  
Title: An Evaluation of Statistical Approaches to Text Categorization  
Author: Yiming Yang 
Web: http://www.cs.cmu.edu/~yiming  
Address: Pittsburgh, PA 15213-3702, USA  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Apte, F. Damerau, and S. Weiss. </author> <title> Towards language independent automated learning of text categorization models. </title> <booktitle> In Proceedings of the 17th Annual ACM/SIGIR conference, </booktitle> <year> 1994. </year>
Reference-contexts: A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers [19, 10, 12], decision trees [5, 10, 12], inductive rule learning algorithms <ref> [1, 3, 13] </ref>, neural networks [21, 14] and on-line learning approaches [3, 9]. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization. However, without a unified methodology in empirical evaluations, objective comparisons of different methods are difficult. <p> Finally, we summarize our conclusions in Section 6. 2 Classifiers Evaluated on Reuters 2.1 Classifiers We consider the text categorization systems whose results on the various versions of the Reuters corpus have been published in the literature <ref> [6, 10, 1, 21, 13, 3, 27, 14] </ref> 1 . In addition to these results, we present new results of three systems. <p> Evaluation results of NaiveBayes on Reuters were reported by Lewis & Ringuette [10] and Moulinier [12], respectively. 4. Inductive rule learning in Disjunctive Normal Form (DNF) was tested in the WASP-1, RIPPER and CHARADE systems <ref> [1, 13, 3] </ref>. DNF rules are of equal power to DTrees in machine learning theory [11]. Empirical results for the comparison between DNF and DTree approaches, however, are rarely available in text categorization, except in an indirect comparison by Apte et al.[1] 5. <p> Reuters version 3 was constructed by Apte et al. for their evaluation of the SWAP-1 by removing all of the unlabeled documents from the training and test sets and restricting the categories to have a training-set frequency of at least two <ref> [1] </ref>. A formatted version of this collection was prepared by Y. Yang and colleagues, and is currently available at Carnegie Mellon University's web site through http://moscow.mt.cs.cmu.edu:8081/reuters 21450/apte. <p> This is counter-intuitive because, given that classifier performance scores should improve when the unlabeled documents are removed from the test set; EXPERTS should continue to be the best performer or nearly the best on 20 Reuters 3. Apte et. al. <ref> [1] </ref> compared the results of SWAP-1 on Reuters version 3 to the results of NaiveBayes and DTree by Lewis on Reuters version 2 [10], and concluded from its significantly better performance that rule-learning methods were superior to decision trees for text categorization.
Reference: [2] <author> Timothy A.H. Bell and Alisair Moffat. </author> <title> The design of a high performance information filtering system. </title> <booktitle> In Proceedings of the 19th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'96), </booktitle> <pages> pages 12-20, </pages> <year> 1996. </year>
Reference-contexts: In more general terms, the scaling problem in kNN can be reduced to the scaling problem in on 23 line document ranking, for which a number of techniques have been studied in the literature, including partial indexing and ranking <ref> [15, 2] </ref>, document clustering [8], dimensionality reduction [27] and parallel computing [4]. 6 Conclusions The following conclusions are reached from this study: 1. Comparative evaluation across methods and experiments is important for understanding the state-of-the-art in text categorization.
Reference: [3] <author> William W. Cohen and Yoram Singer. </author> <title> Context-sensitive learning metods for text categorization. </title> <booktitle> In SIGIR '96: Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1996. </year> <pages> 307-315. </pages>
Reference-contexts: A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers [19, 10, 12], decision trees [5, 10, 12], inductive rule learning algorithms <ref> [1, 3, 13] </ref>, neural networks [21, 14] and on-line learning approaches [3, 9]. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization. However, without a unified methodology in empirical evaluations, objective comparisons of different methods are difficult. <p> number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers [19, 10, 12], decision trees [5, 10, 12], inductive rule learning algorithms [1, 3, 13], neural networks [21, 14] and on-line learning approaches <ref> [3, 9] </ref>. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization. However, without a unified methodology in empirical evaluations, objective comparisons of different methods are difficult. <p> Finally, we summarize our conclusions in Section 6. 2 Classifiers Evaluated on Reuters 2.1 Classifiers We consider the text categorization systems whose results on the various versions of the Reuters corpus have been published in the literature <ref> [6, 10, 1, 21, 13, 3, 27, 14] </ref> 1 . In addition to these results, we present new results of three systems. <p> Evaluation results of NaiveBayes on Reuters were reported by Lewis & Ringuette [10] and Moulinier [12], respectively. 4. Inductive rule learning in Disjunctive Normal Form (DNF) was tested in the WASP-1, RIPPER and CHARADE systems <ref> [1, 13, 3] </ref>. DNF rules are of equal power to DTrees in machine learning theory [11]. Empirical results for the comparison between DNF and DTree approaches, however, are rarely available in text categorization, except in an indirect comparison by Apte et al.[1] 5. <p> By summing up these positively and negatively weighted vectors, the prototype vector of this category is obtained. This method is easy to implement and efficient in computation, and has been used as a baseline in several evaluations <ref> [9, 3] </ref>. A potential weakness of this method is the assumption of one centroid per category, and consequently, Rocchio does not perform well when the documents belonging to a category naturally form separate clusters. 7. LLSF stands for Linear Least Squares Fit, a mapping approach developed by Yang [25]. <p> The matrix defines a mapping from an arbitrary document to a vector of weighted categories. By sorting these category weights, a ranked list of categories is obtained for the input document. 8. Sleeping Experts (EXPERTS) are on-line learning algorithms newly applied to text categorization <ref> [3] </ref>. On-line learning aims to reduce the computation complexity of the training phase for large appli cations. EXPERTS updates the weights of n-gram phrases incrementally. 9. The kNN text categorization method stands for k-nearest neighbor classification. <p> In this table, Reuters versions 2 and 3 have the densest columns. Several claims were made in published evaluations using the results on these collections. Cohen concluded that EXPERTS was the best performer ever on the Reuters version 2 collection <ref> [3] </ref>, and Table 6 agrees with this. <p> However, since BEP is a lower bound on F 1 , SWAP-1 may yet be a better performer than DTree. In any case, the claim by Apte et al. about the advantage of rule-learning algorithms over non-rule learning algorithms is not convincing. Similarly, the claim by Cohen & Yoram <ref> [3] </ref> about the advantage of the context-sensitive classifiers (RIPPER and EXPERTS) over linear classifiers is not necessarily supported by the empirical results here.
Reference: [4] <author> R.H. Creecy, B.M. Masand, S.J. Smith, and D.L. Waltz. </author> <title> Trading mips and memory for knowledge engineering: classifying census returns on the connection machine. </title> <journal> Comm. ACM, </journal> <volume> 35 </volume> <pages> 48-63, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Text categorization (TC) is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers <ref> [4, 22] </ref>, Bayesian probabilistic classifiers [19, 10, 12], decision trees [5, 10, 12], inductive rule learning algorithms [1, 3, 13], neural networks [21, 14] and on-line learning approaches [3, 9]. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization. <p> In more general terms, the scaling problem in kNN can be reduced to the scaling problem in on 23 line document ranking, for which a number of techniques have been studied in the literature, including partial indexing and ranking [15, 2], document clustering [8], dimensionality reduction [27] and parallel computing <ref> [4] </ref>. 6 Conclusions The following conclusions are reached from this study: 1. Comparative evaluation across methods and experiments is important for understanding the state-of-the-art in text categorization.
Reference: [5] <author> N. Fuhr, S. Hartmanna, G. Lustig, M. Schwantner, and K. Tzeras. </author> <title> Air/x a rule-based multistage indexing systems for large subject fields. In 606-623, editor, </title> <booktitle> Proceedings of RIAO'91, </booktitle> <year> 1991. </year>
Reference-contexts: 1 Introduction Text categorization (TC) is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models <ref> [5, 26] </ref>, nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers [19, 10, 12], decision trees [5, 10, 12], inductive rule learning algorithms [1, 3, 13], neural networks [21, 14] and on-line learning approaches [3, 9]. <p> 1 Introduction Text categorization (TC) is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers [19, 10, 12], decision trees <ref> [5, 10, 12] </ref>, inductive rule learning algorithms [1, 3, 13], neural networks [21, 14] and on-line learning approaches [3, 9]. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization.
Reference: [6] <author> P.J. Hayes and S. P. Weinstein. Construe/tis: </author> <title> a system for content-based indexing of a database of new stories. </title> <booktitle> In Second Annual Conference on Innovative Applications of Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: Finally, we summarize our conclusions in Section 6. 2 Classifiers Evaluated on Reuters 2.1 Classifiers We consider the text categorization systems whose results on the various versions of the Reuters corpus have been published in the literature <ref> [6, 10, 1, 21, 13, 3, 27, 14] </ref> 1 . In addition to these results, we present new results of three systems. <p> CONSTRUE is an expert system developed at the Carnegie Group, and the earliest system eval-uated on the Reuters corpus <ref> [6] </ref>. Impressive results (about 90% in both recall and precision, on average) were reported on a small subset (3%) of this corpus. <p> The Reuters corpus consists of over 20,00 Reuters newswire stories in the period between 1987 to 1991. The original corpus (Reuters-22173) was provided by the Carnegie Group, Inc. (CGI) and used to evaluate their CONSTRUE system in 1990 <ref> [6] </ref>. Several versions have been derived from this corpus since them by varying the documents in the corpus, the division between the training and test set, and the categories used for evaluation. Table 1 summarizes these versions. 6 Reuters version 2 (also called Reuters-22450), prepared by D.
Reference: [7] <author> W. Hersh, C. Buckley, T.J. Leone, and D. Hickman. Ohsumed: </author> <title> an interactive retrieval evaluation and new large text collection for research. </title> <booktitle> In 17th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'94), </booktitle> <pages> pages 192-201, </pages> <year> 1994. </year>
Reference-contexts: This problem is further illustrated by the OHSUMED collection <ref> [7] </ref>, which is another corpus commonly used in text categorization research. OHSUMED contains 233,445 documents indexed using 14,321 unique categories; there are about 13 categories per document on average. For this collection, the trivial rejector will have a global average error rate of 0.1% and an accuracy of 99.9%.
Reference: [8] <author> Makato Iwayama and Takenobu Tokunaga. </author> <title> Cluster-based text categorization: a comparison of category search strategies. </title> <booktitle> In Proceedings of the 18th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'95), </booktitle> <pages> pages 273-281, </pages> <year> 1995. </year> <month> 25 </month>
Reference-contexts: In more general terms, the scaling problem in kNN can be reduced to the scaling problem in on 23 line document ranking, for which a number of techniques have been studied in the literature, including partial indexing and ranking [15, 2], document clustering <ref> [8] </ref>, dimensionality reduction [27] and parallel computing [4]. 6 Conclusions The following conclusions are reached from this study: 1. Comparative evaluation across methods and experiments is important for understanding the state-of-the-art in text categorization.
Reference: [9] <author> David D. Lewis, Robert E. Schapire, James P. Callan, and Ron Papka. </author> <title> Training algorithms for linear text classifiers. </title> <booktitle> In SIGIR '96: Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1996. </year> <pages> 298-306. </pages>
Reference-contexts: number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers [19, 10, 12], decision trees [5, 10, 12], inductive rule learning algorithms [1, 3, 13], neural networks [21, 14] and on-line learning approaches <ref> [3, 9] </ref>. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization. However, without a unified methodology in empirical evaluations, objective comparisons of different methods are difficult. <p> By summing up these positively and negatively weighted vectors, the prototype vector of this category is obtained. This method is easy to implement and efficient in computation, and has been used as a baseline in several evaluations <ref> [9, 3] </ref>. A potential weakness of this method is the assumption of one centroid per category, and consequently, Rocchio does not perform well when the documents belonging to a category naturally form separate clusters. 7. LLSF stands for Linear Least Squares Fit, a mapping approach developed by Yang [25].
Reference: [10] <author> D.D. Lewis and M. Ringuette. </author> <title> Comparison of two learning algorithms for text categorization. </title> <booktitle> In Proceedings of the Third Annual Symposium on Document Analysis and Information Retrieval (SDAIR'94), </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction Text categorization (TC) is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers <ref> [19, 10, 12] </ref>, decision trees [5, 10, 12], inductive rule learning algorithms [1, 3, 13], neural networks [21, 14] and on-line learning approaches [3, 9]. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization. <p> 1 Introduction Text categorization (TC) is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers [19, 10, 12], decision trees <ref> [5, 10, 12] </ref>, inductive rule learning algorithms [1, 3, 13], neural networks [21, 14] and on-line learning approaches [3, 9]. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization. <p> Finally, we summarize our conclusions in Section 6. 2 Classifiers Evaluated on Reuters 2.1 Classifiers We consider the text categorization systems whose results on the various versions of the Reuters corpus have been published in the literature <ref> [6, 10, 1, 21, 13, 3, 27, 14] </ref> 1 . In addition to these results, we present new results of three systems. <p> The simplicity of this assumption makes the computation of the NaiveBayes classifier far more efficient than the exponential complexity of non-naive Bayes or DTree approaches because it does not use word combinations as predictors. Evaluation results of NaiveBayes on Reuters were reported by Lewis & Ringuette <ref> [10] </ref> and Moulinier [12], respectively. 4. Inductive rule learning in Disjunctive Normal Form (DNF) was tested in the WASP-1, RIPPER and CHARADE systems [1, 13, 3]. DNF rules are of equal power to DTrees in machine learning theory [11]. <p> Several versions have been derived from this corpus since them by varying the documents in the corpus, the division between the training and test set, and the categories used for evaluation. Table 1 summarizes these versions. 6 Reuters version 2 (also called Reuters-22450), prepared by D. Lewis <ref> [10] </ref>, contains all of the docu-ments in the original corpus (version 1) except the 723 test documents. The documents are split into two chronologically contiguous chunks; the early one is used for training, and the later one for testing. A subset of 113 categories were chosen for evaluation. <p> If the recall and precision of a classifier can be tuned to have an equal value, then this value is called the break-even point (BEP) of the system <ref> [10] </ref>. BEP has been commonly used in text categorization evaluations. If the recall and precision values cannot be made exactly equal, the average of the nearest recall and precision values is used as the interpolated BEP. <p> As a result, the optimization of the trade-off, or the optimal value of F 1 , is difficult to obtain. Pcut is an abbreviation of proportional assignment, a method which has been used in previous text categorization research <ref> [10, 21] </ref>. Before any binary decision is made, the classifier produces confidence scores for all the n fi m document-category decision pairs, where n is the number of test documents and m is the number of training-set categories. <p> Apte et. al. [1] compared the results of SWAP-1 on Reuters version 3 to the results of NaiveBayes and DTree by Lewis on Reuters version 2 <ref> [10] </ref>, and concluded from its significantly better performance that rule-learning methods were superior to decision trees for text categorization. However, to see the perils of this conclusion, kNN has a performance of 0.69 on version 2, but a performance of 0.85 on version 3.
Reference: [11] <author> Tom Mitchell. </author> <title> Machine Learning. </title> <address> McCraw Hill, </address> <year> 1996. </year>
Reference-contexts: Adapting CONSTRUE to other application domains would be costly and labor-intensive. 2. Decision tree (DTree) is a well-known machine learning approach to automatic induction of classification trees based on training data <ref> [16, 11] </ref>. Applied to text categorization, DTree algorithms are used to select informative words based on an information gain criterion, and predict categories of each document according to the occurrence of word combinations in the document. <p> Evaluation result of DTree algorithms on the Reuters text categorization collection were reported by Lewis & Ringuette (using the IND package)[10] and Moulinier (using C4.5)[12], respectively. 3. The Naive Bayes probabilistic classifies (NaiveBayes) is another commonly-used learning approach to text categorization <ref> [11] </ref>. The basic idea is to use the conditional probabilities of categories given a word to estimate the probabilities of categories given a document. The naive part of such a model is the assumption of word independence. <p> Inductive rule learning in Disjunctive Normal Form (DNF) was tested in the WASP-1, RIPPER and CHARADE systems [1, 13, 3]. DNF rules are of equal power to DTrees in machine learning theory <ref> [11] </ref>. Empirical results for the comparison between DNF and DTree approaches, however, are rarely available in text categorization, except in an indirect comparison by Apte et al.[1] 5.
Reference: [12] <author> I. Moulinier. </author> <title> Is learning bias an issue on the text categorization problem? In Technical report, </title> <institution> LAFORIA-LIP6, Universite Paris VI, </institution> <note> page (to appear), </note> <year> 1997. </year>
Reference-contexts: 1 Introduction Text categorization (TC) is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers <ref> [19, 10, 12] </ref>, decision trees [5, 10, 12], inductive rule learning algorithms [1, 3, 13], neural networks [21, 14] and on-line learning approaches [3, 9]. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization. <p> 1 Introduction Text categorization (TC) is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers [19, 10, 12], decision trees <ref> [5, 10, 12] </ref>, inductive rule learning algorithms [1, 3, 13], neural networks [21, 14] and on-line learning approaches [3, 9]. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization. <p> The simplicity of this assumption makes the computation of the NaiveBayes classifier far more efficient than the exponential complexity of non-naive Bayes or DTree approaches because it does not use word combinations as predictors. Evaluation results of NaiveBayes on Reuters were reported by Lewis & Ringuette [10] and Moulinier <ref> [12] </ref>, respectively. 4. Inductive rule learning in Disjunctive Normal Form (DNF) was tested in the WASP-1, RIPPER and CHARADE systems [1, 13, 3]. DNF rules are of equal power to DTrees in machine learning theory [11].
Reference: [13] <author> I. Moulinier, G. Raskinis, and J. Ganascia. </author> <title> Text categorization: a symbolic approach. </title> <booktitle> In Proceedings of the Fifth Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <year> 1996. </year>
Reference-contexts: A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers [19, 10, 12], decision trees [5, 10, 12], inductive rule learning algorithms <ref> [1, 3, 13] </ref>, neural networks [21, 14] and on-line learning approaches [3, 9]. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization. However, without a unified methodology in empirical evaluations, objective comparisons of different methods are difficult. <p> Finally, we summarize our conclusions in Section 6. 2 Classifiers Evaluated on Reuters 2.1 Classifiers We consider the text categorization systems whose results on the various versions of the Reuters corpus have been published in the literature <ref> [6, 10, 1, 21, 13, 3, 27, 14] </ref> 1 . In addition to these results, we present new results of three systems. <p> Evaluation results of NaiveBayes on Reuters were reported by Lewis & Ringuette [10] and Moulinier [12], respectively. 4. Inductive rule learning in Disjunctive Normal Form (DNF) was tested in the WASP-1, RIPPER and CHARADE systems <ref> [1, 13, 3] </ref>. DNF rules are of equal power to DTrees in machine learning theory [11]. Empirical results for the comparison between DNF and DTree approaches, however, are rarely available in text categorization, except in an indirect comparison by Apte et al.[1] 5.
Reference: [14] <author> H.T. Ng, W.B. Goh, and K.L. </author> <title> Low. Feature selection, perceptron learning, and a usability case study for text categorization. </title> <booktitle> In 20th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'97), </booktitle> <pages> pages 67-73, </pages> <year> 1997. </year>
Reference-contexts: A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers [19, 10, 12], decision trees [5, 10, 12], inductive rule learning algorithms [1, 3, 13], neural networks <ref> [21, 14] </ref> and on-line learning approaches [3, 9]. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization. However, without a unified methodology in empirical evaluations, objective comparisons of different methods are difficult. <p> Finally, we summarize our conclusions in Section 6. 2 Classifiers Evaluated on Reuters 2.1 Classifiers We consider the text categorization systems whose results on the various versions of the Reuters corpus have been published in the literature <ref> [6, 10, 1, 21, 13, 3, 27, 14] </ref> 1 . In addition to these results, we present new results of three systems. <p> Empirical results for the comparison between DNF and DTree approaches, however, are rarely available in text categorization, except in an indirect comparison by Apte et al.[1] 5. The neural network (NNet) approaches to text categorization were evaluated on Reuters by Wiener et al. [21] and Ng. et al. <ref> [14] </ref>, respectively. For convenience, the former system (developed at Xerox PARC) is referred to as NNet.PARC in this paper; the latter system is named CLASSI.
Reference: [15] <author> Michael Persin. </author> <title> Document filtering for fast ranking. </title> <booktitle> In Proceedings of the 17th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'94), </booktitle> <pages> pages 341-348, </pages> <year> 1994. </year>
Reference-contexts: In more general terms, the scaling problem in kNN can be reduced to the scaling problem in on 23 line document ranking, for which a number of techniques have been studied in the literature, including partial indexing and ranking <ref> [15, 2] </ref>, document clustering [8], dimensionality reduction [27] and parallel computing [4]. 6 Conclusions The following conclusions are reached from this study: 1. Comparative evaluation across methods and experiments is important for understanding the state-of-the-art in text categorization.
Reference: [16] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Adapting CONSTRUE to other application domains would be costly and labor-intensive. 2. Decision tree (DTree) is a well-known machine learning approach to automatic induction of classification trees based on training data <ref> [16, 11] </ref>. Applied to text categorization, DTree algorithms are used to select informative words based on an information gain criterion, and predict categories of each document according to the occurrence of word combinations in the document.
Reference: [17] <author> G. Salton. </author> <title> Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Pennsylvania, </address> <year> 1989. </year>
Reference-contexts: (CGI) 182 21,450 723 (80%) Version 2 (Lewis) 113 14,704 6,746 (42%) Version 2.2 (Yang) 113 7,789 3,309 (100%) Version 3 (Apte) 93 7,789 3,309 (100%) Version 4 (PARC) 93 9,610 3,662 (100%) documents and category names (each name is treated as a bag of words), and the SMART system <ref> [17] </ref> is used as the search engine. These classifiers can be divided into two types: independent binary classifiers or m-ary (m &gt; 2) classifiers. Given a document, an independent binary classifier makes a YES/NO decision for each category, independently from its decisions on other categories. <p> The LLSF method was only evaluated on three of the five versions because its computation is more costly than those in kNN and WORD. 4.1 Preprocessing and Feature Selection The bench-marking retrieval system, SMART <ref> [17] </ref>, is used as a unified preprocessor for the kNN, LLSF and WORD systems. It is used for removing stop words, stemming, and term weighting, where a term is a word after stemming. A phrasing option is available in SMART but has not been used in these experiments.
Reference: [18] <author> G. Salton and M. J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill Computer Science Series. McGraw-Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: For the global evaluation of a classifier on a collection of test documents, we adapt the procedure for the conventional interpolated 11-point average precision <ref> [18] </ref>, as described below: 1. For each document, compute the recall and precision at each position in the ranked list where a correct category is found. 2.
Reference: [19] <author> K. Tzeras and S. Hartman. </author> <title> Automatic indexing based on bayesian inference networks. </title> <booktitle> In Proc 16th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'93), </booktitle> <pages> pages 22-34, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Text categorization (TC) is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers <ref> [19, 10, 12] </ref>, decision trees [5, 10, 12], inductive rule learning algorithms [1, 3, 13], neural networks [21, 14] and on-line learning approaches [3, 9]. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization.
Reference: [20] <author> C.J. van Rijsbergen. </author> <title> Information Retrieval. </title> <publisher> Butterworths, </publisher> <address> London, </address> <year> 1979. </year>
Reference-contexts: A problem with the interpolation is that when the nearest recall and precision values are far apart, the BEP may not reflect the true behavior of the system. The F 1 measure, defined by C.J. van Rijsbergen <ref> [20] </ref>, is another common choice for a single-numbered performance measure: F 1 = 2rp=(r + p): It balances recall and precision in a way that gives them equal weight.
Reference: [21] <author> E. Wiener, J.O. Pedersen, </author> <title> and A.S. Weigend. A neural network approach to topic spotting. </title> <booktitle> In Proceedings of the Fourth Annual Symposium on Document Analysis and Information Retrieval (SDAIR'95), </booktitle> <year> 1995. </year> <month> 26 </month>
Reference-contexts: A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers [19, 10, 12], decision trees [5, 10, 12], inductive rule learning algorithms [1, 3, 13], neural networks <ref> [21, 14] </ref> and on-line learning approaches [3, 9]. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization. However, without a unified methodology in empirical evaluations, objective comparisons of different methods are difficult. <p> Finally, we summarize our conclusions in Section 6. 2 Classifiers Evaluated on Reuters 2.1 Classifiers We consider the text categorization systems whose results on the various versions of the Reuters corpus have been published in the literature <ref> [6, 10, 1, 21, 13, 3, 27, 14] </ref> 1 . In addition to these results, we present new results of three systems. <p> Empirical results for the comparison between DNF and DTree approaches, however, are rarely available in text categorization, except in an indirect comparison by Apte et al.[1] 5. The neural network (NNet) approaches to text categorization were evaluated on Reuters by Wiener et al. <ref> [21] </ref> and Ng. et al. [14], respectively. For convenience, the former system (developed at Xerox PARC) is referred to as NNet.PARC in this paper; the latter system is named CLASSI. <p> A formatted version of this collection was prepared by Y. Yang and colleagues, and is currently available at Carnegie Mellon University's web site through http://moscow.mt.cs.cmu.edu:8081/reuters 21450/apte. Reuters version 4 was constructed by the research group at Xerox PARC, and was used for their evaluation of their neural network approaches <ref> [21] </ref>. This version was drawn from Reuters version 1 by eliminating the unlabeled documents and some rare categories. Instead of taking continuous chunks of documents for training and testing, it slices the collection into many small chunks that do not overlap temporally. <p> As a result, the optimization of the trade-off, or the optimal value of F 1 , is difficult to obtain. Pcut is an abbreviation of proportional assignment, a method which has been used in previous text categorization research <ref> [10, 21] </ref>. Before any binary decision is made, the classifier produces confidence scores for all the n fi m document-category decision pairs, where n is the number of test documents and m is the number of training-set categories.
Reference: [22] <author> Y. Yang. </author> <title> Expert network: Effective and efficient learning from human decisions in text catego-rization and retrieval. </title> <booktitle> In 17th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'94), </booktitle> <pages> pages 13-22, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Text categorization (TC) is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 26], nearest neighbor classifiers <ref> [4, 22] </ref>, Bayesian probabilistic classifiers [19, 10, 12], decision trees [5, 10, 12], inductive rule learning algorithms [1, 3, 13], neural networks [21, 14] and on-line learning approaches [3, 9]. With more and more methods available, cross-method evaluation becomes increasingly important to identify the state-of-the-art in text categorization. <p> The last two, k and p, are classifier-specific parameters. Thorough investigations on suitable choices of these parameter values were reported in previous papers where the main observations were that the performance of kNN is relatively stable for a large range of k values <ref> [22] </ref>, and that satisfactory performance of LLSF depends on whether p is sufficiently large [23]. 15 Given the large number of possible combinations of parameter values, exhaustive testing of all the combinations is neither practical nor necessary. We take a greedy-search strategy for parameter tuning.
Reference: [23] <author> Y. Yang. </author> <title> Noise reduction in a statistical approach to text categorization. </title> <booktitle> In Proceedings of the 18th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'95), </booktitle> <pages> pages 256-263, </pages> <year> 1995. </year>
Reference-contexts: Thorough investigations on suitable choices of these parameter values were reported in previous papers where the main observations were that the performance of kNN is relatively stable for a large range of k values [22], and that satisfactory performance of LLSF depends on whether p is sufficiently large <ref> [23] </ref>. 15 Given the large number of possible combinations of parameter values, exhaustive testing of all the combinations is neither practical nor necessary. We take a greedy-search strategy for parameter tuning. <p> LLSF is an eager learning method, and has a off-line training phase and an on-line testing phase. The training phase has a quadratic time complexity, O (pn 0 ) where p is the number of principal components (singular vectors) used for computing an approximated LLSF solution <ref> [23] </ref>, and n 0 = maxfm; ng is the larger number between n, the number of training documents, and m, the number of unique terms in the training documents. This quadratic complexity is the computational bottleneck for scaling this method to large applications.
Reference: [24] <author> Y. Yang. </author> <title> An evaluation of statistical approach to text categorization. </title> <note> In Technical Report CMU-CS-97-127, </note> <institution> Computer Science Department, Carnegie Mellon University, </institution> <year> 1997. </year>
Reference-contexts: In the further scaling test of kNN on OHSUMED, we observed 11.5 CPU minutes for the indexing of 183,229 training documents, and an on-line response of 1.0 CPU second per test document on average, with a micro-average F 1 value of .49 <ref> [24] </ref>. This is the only text categorization method which has been examined on the full domain of OHSUMED.
Reference: [25] <author> Y. Yang and C.G. Chute. </author> <title> A linear least squares fit mapping method for information retrieval from natural language texts. </title> <booktitle> In Proceedings of the 14th International Conference on Computational Linguistics (COLING 92), </booktitle> <pages> pages 447-453, </pages> <year> 1992. </year>
Reference-contexts: A potential weakness of this method is the assumption of one centroid per category, and consequently, Rocchio does not perform well when the documents belonging to a category naturally form separate clusters. 7. LLSF stands for Linear Least Squares Fit, a mapping approach developed by Yang <ref> [25] </ref>. A multivari-ate regression model is automatically learned from a training set of documents and their categories.
Reference: [26] <author> Y. Yang and C.G. Chute. </author> <title> An example-based mapping method for text categorization and retrieval. </title> <journal> ACM Transaction on Information Systems (TOIS), </journal> <pages> pages 253-277, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Text categorization (TC) is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models <ref> [5, 26] </ref>, nearest neighbor classifiers [4, 22], Bayesian probabilistic classifiers [19, 10, 12], decision trees [5, 10, 12], inductive rule learning algorithms [1, 3, 13], neural networks [21, 14] and on-line learning approaches [3, 9].
Reference: [27] <author> Y. Yang and J.P. Pedersen. </author> <title> Feature selection in statistical learning of text categorization. </title> <booktitle> In The Fourteenth International Conference on Machine Learning, </booktitle> <pages> pages 412-420, </pages> <year> 1997. </year> <month> 27 </month>
Reference-contexts: Finally, we summarize our conclusions in Section 6. 2 Classifiers Evaluated on Reuters 2.1 Classifiers We consider the text categorization systems whose results on the various versions of the Reuters corpus have been published in the literature <ref> [6, 10, 1, 21, 13, 3, 27, 14] </ref> 1 . In addition to these results, we present new results of three systems. <p> Five feature selection criteria were tested with kNN and LLSF, including information gain, mutual exclusion, a 2 statistic, document frequency and term strength; a thorough evaluation of these feature selection methods was reported elsewhere <ref> [27] </ref>. The 2 statistic was found most effective, and information 13 gain and document frequency yielded similar results. On the Reuters collection version 3, for example, the 2 -based feature selection reduced the training set vocabulary from 24,858 unique words to 2,485. <p> In more general terms, the scaling problem in kNN can be reduced to the scaling problem in on 23 line document ranking, for which a number of techniques have been studied in the literature, including partial indexing and ranking [15, 2], document clustering [8], dimensionality reduction <ref> [27] </ref> and parallel computing [4]. 6 Conclusions The following conclusions are reached from this study: 1. Comparative evaluation across methods and experiments is important for understanding the state-of-the-art in text categorization.
References-found: 27

