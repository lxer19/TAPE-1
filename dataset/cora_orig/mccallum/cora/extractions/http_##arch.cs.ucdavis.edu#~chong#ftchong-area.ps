URL: http://arch.cs.ucdavis.edu/~chong/ftchong-area.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C.html
Root-URL: http://www.cs.ucdavis.edu
Email: ftchong@ai.mit.edu  
Title: Critique of flash, typhoon, and shrimp  
Author: Frederic T. Chong 
Date: September 7, 1994  
Note: A  
Abstract: Warning: This paper was written for an MIT area exam and the material is intentionally inflam- matory. Suggestions are welcome. 
Abstract-found: 1
Intro-found: 1
Reference: [BBLS91] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report RNR-91-002, </type> <institution> Numerical Aerodynamic Simulation (NAS) Systems Division, NASA Ames Research Center, Moffett Field, </institution> <address> CA 94035, </address> <year> 1991. </year>
Reference-contexts: Both studies show that software caching can increase performance when the working sets of applications will not fit in the hardware cache of a conventional protocol. They examined applications from the NAS <ref> [BBLS91] </ref> and SPLASH [SWG92] benchmark suites, as well as Cholesky factorization [RG93] and EM3D (described in Section 6). The typhoon study compared their software caching protocol, Stache, to conventional DIR N N B hardware cache protocols.
Reference: [BK94] <author> Eric A. Brewer and Bradley C. Kuszmaul. </author> <title> How to get good performance from the CM-5 data network. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year> <note> To Appear. </note>
Reference-contexts: A further source of performance is the interleaving of communication with computation in the message-passing code. In contrast, the delayed-update code performs all the communication in each iteration at once. Sending too much data at once fills up the buffering in the network and causes congestion. Brewer and Kuszmaul <ref> [BK94] </ref> have shown that the CM-5 network can buffer no more than about 10 messages per processor and that performance degrades severely if each processor has more than about 5 messages in the network. Such limits apply to any network.
Reference: [BLA + 94] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, and Edward W. Felton. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: 1 Introduction I critique three papers which describe flash [KOH + 94], typhoon [RLW94], and shrimp <ref> [BLA + 94] </ref>, three recent multiprocessor designs. Although I focus on the scalability and performance of these systems, 1 I also examine protection and usability issues central to these systems. In examining these four issues, I focus on communications requirements.
Reference: [CDG + 93] <author> David E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel programming in split-c. </title> <booktitle> In Supercomputing `93, </booktitle> <pages> pages 262-273. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: A message handler is code which is executed upon receiving a message. A message may specify the handler to be executed and may contain arguments for the handler. Message handlers have been shown to efficiently support a variety of communication and computation models [D + 92] [vCGS92] <ref> [CDG + 93] </ref>. flash and typhoon use handlers to support shared-memory coherence operations. Both systems have been influenced by the work of Henry and Joerg [HJ92]. Henry and Joerg dramatically increased handler performance by adding fast dispatch hardware to a commercial processor. <p> The study of applications is reassuring, but the comparisons are limited to shared-memory implementations. The next section discusses alternative implementations. 6 Irregular Applications The typhoon paper presents a study of EM3D, an irregular electromagnetics application originally used by Culler and others <ref> [CDG + 93] </ref>. EM3D models electromagnetic wave propagation through three-dimensional objects. The computation involves an irregular bipartite graph. The graph consists of E nodes, representing electric field intensities, and H nodes, representing magnetic field intensities. The typhoon study of EM3D makes three questionable conclusions.
Reference: [CGSG93] <author> Rohit Chandra, Kourosh Gharachorloo, Vijayaraghavan Soundararajan, and Anoop Gupta. </author> <title> Performance evaluation of hybrid hardware and software distributed shared memory protocols. </title> <type> Technical Report CSL-TR-93-597, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> Decem-ber </month> <year> 1993. </year> <title> 8 Better examples might be found in N-body applications [SHG92], but it is unclear that a worthwhile user-level shared-memory protocol exists for such applications. </title> <type> 13 </type>
Reference-contexts: In other words, only data mappings are discussed, but these imply computation mappings. Based on EM3D, typhoon claims high performance on irregular applications without preprocessing costs. There are, however, actually two aspects to preprocessing, dependency preprocessing 5 The flash data is given in a separate technical report <ref> [CGSG93] </ref>. 8 and data remapping. In dependency preprocessing, irregular dependencies may be examined so that data may be sent, requiring one message, rather than fetched, requiring two messages. In data remapping, data layout may be optimized to minimize communication resulting from non-local dependencies.
Reference: [CSBS94] <author> Frederic T. Chong, Shamik D. Sharma, Eric A. Brewer, and Joel Saltz. </author> <title> Multiprocessor runtime support for fine-grained, irregular DAGs. </title> <note> Submitted for publication, </note> <year> 1994. </year>
Reference-contexts: The messaging unit can receive messages from the network and write the received data to memory. None of the papers, however, quantify the impact of this reduced overhead on applications. We studied the overhead in sparse triangular solves on the CM-5 <ref> [CSBS94] </ref>. Having no separate messaging unit, the CM-5 [Thi93a] requires the compute processor to perform all the mechanics of sending and receiving a message. Our study found that such overhead accounted for 15 to 40 percent of execution time. <p> On an architecture with more efficient communication, we found that preprocessing improved sparse triangular solve performance up to a factor of 1.5 on the Thinking Machines CM5 <ref> [CSBS94] </ref>. Note that caching strategies are not a substitute for data remapping. Cache strategies can move data to the processor which uses it. The data, however, is used wherever the relevant computation is mapped.
Reference: [D + 92] <author> William J. Dally et al. </author> <title> The message-driven processor: A multicomputer processing node with efficient mechanisms. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 23-39, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: A message handler is code which is executed upon receiving a message. A message may specify the handler to be executed and may contain arguments for the handler. Message handlers have been shown to efficiently support a variety of communication and computation models <ref> [D + 92] </ref> [vCGS92] [CDG + 93]. flash and typhoon use handlers to support shared-memory coherence operations. Both systems have been influenced by the work of Henry and Joerg [HJ92]. Henry and Joerg dramatically increased handler performance by adding fast dispatch hardware to a commercial processor.
Reference: [DMS + 92] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives. </title> <type> Technical Report 92-12, </type> <institution> ICASE, Hampton, Virginia, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: When compared to oblivious distributions, the benefits of data remapping can be substantial. Oblivious distributions are the best that can be done at compile-time with no dependency information, and generally involve block or cyclic mappings. On irregular mesh computations similar to EM3D, preprocessing performed by the PARTI <ref> [DMS + 92] </ref> software libraries resulted in factors of 2.5 to 3 in performance gains on the Intel iPSC/860 and Delta [Dun92] multiprocessors.
Reference: [Dun92] <author> Thomas H. Dunigan. </author> <title> Performance of the Intel touchstone delta mesh. </title> <type> Technical Report ORNL/TM-11983, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, Tennessee 37831, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: On irregular mesh computations similar to EM3D, preprocessing performed by the PARTI [DMS + 92] software libraries resulted in factors of 2.5 to 3 in performance gains on the Intel iPSC/860 and Delta <ref> [Dun92] </ref> multiprocessors. On an architecture with more efficient communication, we found that preprocessing improved sparse triangular solve performance up to a factor of 1.5 on the Thinking Machines CM5 [CSBS94]. Note that caching strategies are not a substitute for data remapping.
Reference: [FSSHT91] <author> Anja Feldmann, Jiri Sgall, and Shang-Hua-Teng. </author> <title> Dynamic scheduling on parallel machines. </title> <booktitle> In IEEE 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 111-120, </pages> <year> 1991. </year>
Reference-contexts: Multiprogramming implies that different users and processes are executing simultaneously on different nodes of a multiprocessor. Neither paper motivates the need for multiprogramming, but some motivation can be found in the modeling community [ZB91] [MEB88] [LV90] [ZM90], the operating systems community [TG89], and the theory community <ref> [FSSHT91] </ref> [SWW91]. None of these studies is conclusive. As a whole, however, they suggest that there are better alternatives to strict gang scheduling.
Reference: [HJ92] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A tightly-coupled processor-network interface. </title> <booktitle> In Proceedings of the fifth international conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <address> Boston, Massachusetts, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Message handlers have been shown to efficiently support a variety of communication and computation models [D + 92] [vCGS92] [CDG + 93]. flash and typhoon use handlers to support shared-memory coherence operations. Both systems have been influenced by the work of Henry and Joerg <ref> [HJ92] </ref>. Henry and Joerg dramatically increased handler performance by adding fast dispatch hardware to a commercial processor.
Reference: [Kaa92] <author> M. F. Kaashoek. </author> <title> Group Communication in Distributed Systems. </title> <type> PhD thesis, </type> <institution> Vrije University, </institution> <address> Amsterdam, </address> <year> 1992. </year>
Reference-contexts: However, as described later, map should be extended to operate on groups of processors rather than just pairs. Motivation for group communication is given by Kaashoek <ref> [Kaa92] </ref>. The shrimp paper presents the map operation with the following syntax: map (send_buffer, destination, receive_buffer) The types of the arguments are unclear. send buffer and receive buffer must include both starting address and length information.
Reference: [KJA + 93] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Unlike the custom magic chip in flash, the NP in typhoon is based upon a sparc integer processor. This decision will be examined in Section 3. 2 Such integration was first proposed by another ongoing project, the MIT Alewife multiprocessor <ref> [KJA + 93] </ref>. 2 2.3 shrimp While flash and typhoon are designed to support coherent shared-memory, the Princeton shrimp multiprocessor adopts a simpler model of memory-to-memory communication.
Reference: [KOH + 94] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Ghara-chorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta adn Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: 1 Introduction I critique three papers which describe flash <ref> [KOH + 94] </ref>, typhoon [RLW94], and shrimp [BLA + 94], three recent multiprocessor designs. Although I focus on the scalability and performance of these systems, 1 I also examine protection and usability issues central to these systems. In examining these four issues, I focus on communications requirements.
Reference: [Lei93] <author> Charles Leiserson. </author> <title> Talk given at Thinking Machines Corporation, </title> <address> Cambridge, Massachusetts., </address> <month> August </month> <year> 1993. </year>
Reference-contexts: This would be very expensive, especially given that it requires n kernel crossings per processor. All-to-all mapping could be established for all processors, in one kernel call per processor, with a global map function: 4 Note that the protection mechanisms discussed only ensure availability, not fairness. Leiserson's Virtual Networks <ref> [Lei93] </ref> are a proposal for ensuring a fair distribution of network resources. 5 global_map (char* send_buffer_address, int send_buffer_length, char* receive_buffer_address, int receive_buffer_length) Where send buffer address is a pointer to n buffers of length send buffer length and n is the number of processors on the machine.
Reference: [LV90] <author> Scott T. Leutenegger and Mary K. Vernon. </author> <title> The performance of multiprogrammed multiprocessor scheduling policies. </title> <booktitle> In Proceedings of the 1990 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 226-236, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Multiprogramming implies that different users and processes are executing simultaneously on different nodes of a multiprocessor. Neither paper motivates the need for multiprogramming, but some motivation can be found in the modeling community [ZB91] [MEB88] <ref> [LV90] </ref> [ZM90], the operating systems community [TG89], and the theory community [FSSHT91] [SWW91]. None of these studies is conclusive. As a whole, however, they suggest that there are better alternatives to strict gang scheduling.
Reference: [MEB88] <author> Shikharesh Majumdar, Derek L. Eager, and Richard B. Bunt. </author> <title> Scheduling in multiprogrammed parallel systems. </title> <booktitle> In Proceedings of the 1988 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 104-112, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Multiprogramming implies that different users and processes are executing simultaneously on different nodes of a multiprocessor. Neither paper motivates the need for multiprogramming, but some motivation can be found in the modeling community [ZB91] <ref> [MEB88] </ref> [LV90] [ZM90], the operating systems community [TG89], and the theory community [FSSHT91] [SWW91]. None of these studies is conclusive. As a whole, however, they suggest that there are better alternatives to strict gang scheduling.
Reference: [MKAK94] <author> Kenneth Mackenzie, John Kubiatowicz, Anant Agarwal, and Frans Kaashoek. FUGU: </author> <title> Implementing protection and virtual memory in a multimodel, multiuser multiprocessor. </title> <note> Submitted for publication, </note> <year> 1994. </year>
Reference-contexts: Care must be taken to make sure the message consumption timeouts can keep up with the rate a user might be injecting messages into the network. The fugu <ref> [MKAK94] </ref> system, however, suggests that message consumption need only allow the network to make enough progress to deschedule any process that is clogging the network. For example, an operating system on processor 0 may recognize that a particular process on processor 1 is sending too many messages to node 0.
Reference: [PBGB93] <author> Gregory M. Papadopoulos, G. Andy Boughton, Rober Greiner, and Michael J. Beckerle. </author> <title> *T: Integrated building blocks for parallel computing. </title> <booktitle> In Supercomputing `93. IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: They miss the important alternative of unrestricted user access with timeouts that ensure network resources are available to the next user. The *T <ref> [PBGB93] </ref> system presents this alternative. Additionally, the map operation provided by shrimp, for establishing protected communication between pairs of processors, is inadequate for irregular or cooperative applications. I propose extensions to map which provide scalable support for efficient group communication. <p> All three systems miss an important alternative: the protocol processor can run user code as long as there are timeout mechanisms that ensure message consumption and availability of the protocol processor. If a user handler has not consumed a message in a specified amount of time, the *T multiprocessor <ref> [PBGB93] </ref> interrupts the handler, receives the next message, and either invokes the new handler or queues the message for later handling. The errant handler is restarted at a later time. *T does not have a separate protocol processor.
Reference: [RG93] <author> Edward Rothberg and Anoop Gupta. </author> <title> An efficient block-oriented approach to parallel sparse Cholesky factorization. </title> <booktitle> In Supercomputing `93, </booktitle> <pages> pages 503-512. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: Both studies show that software caching can increase performance when the working sets of applications will not fit in the hardware cache of a conventional protocol. They examined applications from the NAS [BBLS91] and SPLASH [SWG92] benchmark suites, as well as Cholesky factorization <ref> [RG93] </ref> and EM3D (described in Section 6). The typhoon study compared their software caching protocol, Stache, to conventional DIR N N B hardware cache protocols. They found that Stache performed up to 25 percent better for data sets that do not fit in the hardware cache.
Reference: [RLW94] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: 1 Introduction I critique three papers which describe flash [KOH + 94], typhoon <ref> [RLW94] </ref>, and shrimp [BLA + 94], three recent multiprocessor designs. Although I focus on the scalability and performance of these systems, 1 I also examine protection and usability issues central to these systems. In examining these four issues, I focus on communications requirements.
Reference: [SCMB90] <author> Joel H. Saltz, Kay Crowley, Ravi Mirchandaney, and Harry Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 303-312, </pages> <year> 1990. </year>
Reference-contexts: Third, they claim that a user-level, hybrid protocol is easier to write than a message-passing program. The next three section deny each of these three claims. 6.1 Run-Time Preprocessing Run-time preprocessing can result in good message-passing implementations of irregular applications <ref> [SCMB90] </ref>. In such applications, dependencies are generally not known at compile-time. Instead, dependencies are computed at run-time before the real computation begins. This run-time preprocessing step can also optimize the mapping of data and computation onto processors. Many applications have dependencies that do not change for long periods of computation.
Reference: [SHG92] <author> Jaswinder Pal Singh, John L. Hennessey, and Anoop Gupta. </author> <title> Implications of hierarchical n-body methods for multiprocessor architecture. </title> <type> Technical Report CSL-TR-92-506, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1992. </year>
Reference: [SWG92] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> Splash: Stanford parallel applications for shared-memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <year> 1992. </year>
Reference-contexts: Both studies show that software caching can increase performance when the working sets of applications will not fit in the hardware cache of a conventional protocol. They examined applications from the NAS [BBLS91] and SPLASH <ref> [SWG92] </ref> benchmark suites, as well as Cholesky factorization [RG93] and EM3D (described in Section 6). The typhoon study compared their software caching protocol, Stache, to conventional DIR N N B hardware cache protocols.
Reference: [SWW91] <author> D. B. Schmoys, J. Wein, and D. P. Williamson. </author> <title> Scheduling parallel machines on-line. </title> <booktitle> In IEEE 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 131-140, </pages> <year> 1991. </year> <month> 14 </month>
Reference-contexts: Multiprogramming implies that different users and processes are executing simultaneously on different nodes of a multiprocessor. Neither paper motivates the need for multiprogramming, but some motivation can be found in the modeling community [ZB91] [MEB88] [LV90] [ZM90], the operating systems community [TG89], and the theory community [FSSHT91] <ref> [SWW91] </ref>. None of these studies is conclusive. As a whole, however, they suggest that there are better alternatives to strict gang scheduling.
Reference: [TG89] <author> Andrew Tucker and Anoop Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Multiprogramming implies that different users and processes are executing simultaneously on different nodes of a multiprocessor. Neither paper motivates the need for multiprogramming, but some motivation can be found in the modeling community [ZB91] [MEB88] [LV90] [ZM90], the operating systems community <ref> [TG89] </ref>, and the theory community [FSSHT91] [SWW91]. None of these studies is conclusive. As a whole, however, they suggest that there are better alternatives to strict gang scheduling.
Reference: [Thi93a] <institution> Thinking Machines Corporation, Cambridge, MA. </institution> <type> CM-5 Technical Summary, </type> <month> November </month> <year> 1993. </year>
Reference-contexts: The messaging unit can receive messages from the network and write the received data to memory. None of the papers, however, quantify the impact of this reduced overhead on applications. We studied the overhead in sparse triangular solves on the CM-5 [CSBS94]. Having no separate messaging unit, the CM-5 <ref> [Thi93a] </ref> requires the compute processor to perform all the mechanics of sending and receiving a message. Our study found that such overhead accounted for 15 to 40 percent of execution time.
Reference: [Thi93b] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CMMD Reference Manual (Version 3.0), </note> <month> May </month> <year> 1993. </year>
Reference-contexts: Given limited network buffering, sending short messages interleaved with computation is better than sending long messages of aggregated data. 7 The CMAML functions are low level communications functions used by the CMMD message-passing library <ref> [Thi93b] </ref> on the CM-5. 11 typedef struct h node f double value; int edge count; double flweights; double flfle node values; int fle node procs; struct h node flnext; g h node t; int rec count, rec limit; void compute handler (double flenode value, double value) f rec count++; fl (enode
Reference: [vCGS92] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Eric Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <address> Queensland, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: A message handler is code which is executed upon receiving a message. A message may specify the handler to be executed and may contain arguments for the handler. Message handlers have been shown to efficiently support a variety of communication and computation models [D + 92] <ref> [vCGS92] </ref> [CDG + 93]. flash and typhoon use handlers to support shared-memory coherence operations. Both systems have been influenced by the work of Henry and Joerg [HJ92]. Henry and Joerg dramatically increased handler performance by adding fast dispatch hardware to a commercial processor.
Reference: [Wil92] <author> John Wilkes. </author> <title> Hamlyn an interface for sender-based communications. </title> <type> Technical Report HPL-OSR-92-13, </type> <institution> Hewlett-Packard Laboratories, Operating Systems Research Department, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: The primary contribution of shrimp is low-overhead, protected communication through the separation of protection and data transfer operations. Such separation was first proposed by Wilkes for the Hamlyn interface <ref> [Wil92] </ref>. Protection is checked using a map operation, which uses a kernel call to map a send buffer on one processor to a receive buffer on another processor.
Reference: [ZB91] <author> Songnian Zhou and Timothy Brecht. </author> <title> Processor pool-based scheduling for large-scale NUMA multiprocessors. </title> <booktitle> In Proceedings of the 1991 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 133-142, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Multiprogramming implies that different users and processes are executing simultaneously on different nodes of a multiprocessor. Neither paper motivates the need for multiprogramming, but some motivation can be found in the modeling community <ref> [ZB91] </ref> [MEB88] [LV90] [ZM90], the operating systems community [TG89], and the theory community [FSSHT91] [SWW91]. None of these studies is conclusive. As a whole, however, they suggest that there are better alternatives to strict gang scheduling.
Reference: [ZM90] <author> John Zahorjan and Cathy McCann. </author> <title> Processor scheduling in shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1990 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 214-224, </pages> <month> May </month> <year> 1990. </year> <month> 15 </month>
Reference-contexts: Multiprogramming implies that different users and processes are executing simultaneously on different nodes of a multiprocessor. Neither paper motivates the need for multiprogramming, but some motivation can be found in the modeling community [ZB91] [MEB88] [LV90] <ref> [ZM90] </ref>, the operating systems community [TG89], and the theory community [FSSHT91] [SWW91]. None of these studies is conclusive. As a whole, however, they suggest that there are better alternatives to strict gang scheduling.
References-found: 32

