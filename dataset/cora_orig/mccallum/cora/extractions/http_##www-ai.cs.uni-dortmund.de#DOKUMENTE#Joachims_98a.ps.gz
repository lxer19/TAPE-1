URL: http://www-ai.cs.uni-dortmund.de/DOKUMENTE/Joachims_98a.ps.gz
Refering-URL: http://www-ai.informatik.uni-dortmund.de/PERSONAL/joachims.html
Root-URL: 
Title: Text Categorization with Support Vector Machines: Learning with Many Relevant Features  
Author: Thorsten Joachims 
Address: Informatik LS8, Baroper Str. 301 44221 Dortmund, Germany  
Affiliation: Universitat Dortmund  
Abstract: This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore, they are fully automatic, eliminating the need for manual parameter tuning.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> C. Cortes and V. Vapnik. </author> <title> Support-vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 273-297, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Since building text classifiers by hand is difficult and time-consuming, it is advantageous to learn classifiers from examples. In this paper I will explore and identify the benefits of Support Vector Machines (SVMs) for text categorization. SVMs are a new learning method introduced by V. Vapnik et al. [9] <ref> [1] </ref>. They are well-founded in terms of computational learning theory and very open to theoretical understanding and analysis. After reviewing the standard feature vector representation of text, I will identify the particular properties of text in this representation in section 4.
Reference: 2. <author> T. Joachims. </author> <title> A probabilistic analysis of the rocchio algorithm with tfidf for text categorization. </title> <booktitle> In International Conference on Machine Learning (ICML), </booktitle> <year> 1997. </year>
Reference-contexts: Unfortunately, in text categorization there are only very few irrelevant features. Figure 1 shows the results of an experiment on the Reuters "acq" category (see section 5). All features are ranked according to their (binary) information gain. Then a naive Bayes classifier <ref> [2] </ref> is trained using only those features ranked 1-200, 201-500, 501-1000, 1001-2000, 2001-4000, 4001-9962. The results in figure 1 show that even features ranked lowest still contain considerable information and are somewhat relevant. A classifier using only those "worst" features has a performance much better than random. <p> Each method represents a different machine learning approach: density estimation using a naive Bayes classifier <ref> [2] </ref>, the Rocchio algorithm [7] as the most popular learning method from information retrieval, a distance weighted k-nearest neighbor classifier [5][10], and the C4.5 decision tree/rule learner [6]. SVM training is carried out with the SVM light 2 package. The SVM light package will be described in a forthcoming paper.
Reference: 3. <author> T. Joachims. </author> <title> Text categorization with support vector machines: Learning with many relevant features. </title> <type> Technical Report 23, </type> <institution> Universitat Dortmund, LS VIII, </institution> <year> 1997. </year>
Reference-contexts: A document belongs to a category if it is indexed with at least one indexing term from that category. After preprocessing, the training corpus contains 15561 distinct terms. Results: Figure 2 shows the results on the Reuters corpus. The Precision/Recall-Breakeven Point (see e. g. <ref> [3] </ref>) is used as a measure of performance and mi-croaveraging [10][3] is applied to get a single performance value over all binary classification tasks. <p> Comparing training time, SVMs are roughly comparable to C4.5, but they are more expensive than naive Bayes, Rocchio, and k-NN. Nevertheless, current research is likely to improve efficiency of SVM-type quadratic programming problems. SVMs are faster than k-NN at classification time. More details can found in <ref> [3] </ref>. 6 Conclusions This paper introduces support vector machines for text categorization. It provides both theoretical and empirical evidence that SVMs are very well suited for text categorization.
Reference: 4. <author> J. Kivinen, M. Warmuth, and P. Auer. </author> <title> The perceptron algorithm vs. winnow: Linear vs. logarithmic mistake bounds when few input variables are relevant. </title> <booktitle> In Conference on Computational Learning Theory, </booktitle> <year> 1995. </year>
Reference-contexts: Document vectors are sparse: For each document, the corresponding docu-ment vector contains only few entries which are not zero. Kivinen et al. <ref> [4] </ref> give both theoretical and empirical evidence for the mistake bound model that "additive" algorithms, which have a similar inductive bias like SVMs, are well suited for problems with dense concepts and sparse instances.
Reference: 5. <author> T. Mitchell. </author> <title> Machine Learning. </title> <publisher> McGraw-Hill, </publisher> <year> 1997. </year>
Reference: 6. <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Each method represents a different machine learning approach: density estimation using a naive Bayes classifier [2], the Rocchio algorithm [7] as the most popular learning method from information retrieval, a distance weighted k-nearest neighbor classifier [5][10], and the C4.5 decision tree/rule learner <ref> [6] </ref>. SVM training is carried out with the SVM light 2 package. The SVM light package will be described in a forthcoming paper. Test Collections: The empirical evaluation is done on two test collection. The first one is the "ModApte" split of the Reuters-21578 dataset compiled by David Lewis.
Reference: 7. <author> J. Rocchio. </author> <title> Relevance feedback in information retrieval. </title> <editor> In G. Salton, editor, </editor> <booktitle> The SMART Retrieval System: Experiments in Automatic Document Processing, </booktitle> <pages> pages 313-323. </pages> <publisher> Prentice-Hall Inc., </publisher> <year> 1971. </year>
Reference-contexts: Each method represents a different machine learning approach: density estimation using a naive Bayes classifier [2], the Rocchio algorithm <ref> [7] </ref> as the most popular learning method from information retrieval, a distance weighted k-nearest neighbor classifier [5][10], and the C4.5 decision tree/rule learner [6]. SVM training is carried out with the SVM light 2 package. The SVM light package will be described in a forthcoming paper.
Reference: 8. <author> G. Salton and C. Buckley. </author> <title> Term weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24(5) </volume> <pages> 513-523, </pages> <year> 1988. </year>
Reference-contexts: Following the recommendation of [11], the information gain criterion will be used in this paper to select a subset of features. Finally, from IR it is known that scaling the dimensions of the feature vector with their inverse document frequency (IDF) <ref> [8] </ref> improves performance. Here the "tfc" variant is used. To abstract from different document lengths, each document feature vector is normalized to unit length. 3 Support Vector Machines Support vector machines are based on the Structural Risk Minimization principle [9] from computational learning theory.
Reference: 9. <author> Vladimir N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: Since building text classifiers by hand is difficult and time-consuming, it is advantageous to learn classifiers from examples. In this paper I will explore and identify the benefits of Support Vector Machines (SVMs) for text categorization. SVMs are a new learning method introduced by V. Vapnik et al. <ref> [9] </ref> [1]. They are well-founded in terms of computational learning theory and very open to theoretical understanding and analysis. After reviewing the standard feature vector representation of text, I will identify the particular properties of text in this representation in section 4. <p> Here the "tfc" variant is used. To abstract from different document lengths, each document feature vector is normalized to unit length. 3 Support Vector Machines Support vector machines are based on the Structural Risk Minimization principle <ref> [9] </ref> from computational learning theory. The idea of structural risk minimization is to find a hypothesis h for which we can guarantee the lowest true error. The true error of h is the probability that h will make an error on an unseen and randomly selected test example. <p> An upper bound can be used to connect the true error of a hypothesis h with the error of h on the training set and the complexity of H (measured by VC-Dimension), the hypothesis space containing h <ref> [9] </ref>. Support vector machines find the hypothesis h which (approximately) minimizes this bound on the true error by effectively and efficiently controlling the VC-Dimension of H. SVMs are very universal learners. In their basic form, SVMs learn linear threshold function. <p> The same margin argument also suggest a heuristic for selecting good parameter settings for the learner (like the kernel width in an RBF network) <ref> [9] </ref>. The best parameter setting is the one which produces the hypothesis with the lowest VC-Dimension.
Reference: 10. <author> Y. Yang. </author> <title> An evaluation of statistical approaches to text categorization. </title> <type> Technical Report CMU-CS-97-127, </type> <institution> Carnegie Mellon University, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: Naive Bayes performs best using all features. for the k-NN classifier were tried. The results for the parameters with the best performance on the test set are reported. On the Reuters data the k-NN classifier performs best among the conventional methods (see figure 2). This replicates the findings of <ref> [10] </ref>. Compared to the conventional methods all SVMs perform better independent of the choice of parameters. Even for complex hypotheses spaces, like polynomials of degree 5, no overfitting occurs despite using all 9962 features.
Reference: 11. <author> Y. Yang and J. Pedersen. </author> <title> A comparative study on feature selection in text categorization. </title> <booktitle> In International Conference on Machine Learning (ICML), </booktitle> <year> 1997. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: This representation scheme leads to very high-dimensional feature spaces containing 10000 dimensions and more. Many have noted the need for feature selection to make the use of conventional learning methods possible, to improve generalization accuracy, and to avoid "overfitting". Following the recommendation of <ref> [11] </ref>, the information gain criterion will be used in this paper to select a subset of features. Finally, from IR it is known that scaling the dimensions of the feature vector with their inverse document frequency (IDF) [8] improves performance. Here the "tfc" variant is used.
References-found: 11

