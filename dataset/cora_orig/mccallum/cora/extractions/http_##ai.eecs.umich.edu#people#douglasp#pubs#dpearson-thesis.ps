URL: http://ai.eecs.umich.edu/people/douglasp/pubs/dpearson-thesis.ps
Refering-URL: http://ai.eecs.umich.edu/people/douglasp/pubs/thesis.html
Root-URL: 
Title: LEARNING PROCEDURAL PLANNING KNOWLEDGE IN COMPLEX ENVIRONMENTS  
Author: by Douglas John Pearson Professor John H. Holland Professor Keki B. Irani 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Science and Engineering) in The  Doctoral Committee: Associate Professor John E. Laird, Chair  
Note: Associate Professor Colleen M. Seifert This research was supported under contract N66001-95-C-6013 from the Advanced Systems Technology Office of the Advanced Research Projects Agency and the Naval Command and Ocean Surveillance Center, RDT&E division; and by a University of Michigan Rackham Predoctoral Fel lowship.  
Date: 1996  
Affiliation: University of Michigan  
Abstract-found: 0
Intro-found: 1
Reference: <institution> 152 BIBLIOGRAPHY </institution>
Reference: [Aasman, 1995] <author> Jannes Aasman. </author> <title> Modelling Driver Behavior in Soar. </title> <type> PhD thesis, </type> <institution> Rijksuniversiteit Groningen, </institution> <year> 1995. </year>
Reference: [Asker, 1994] <author> Lars Asker. </author> <title> Improving accuracy of incorrect domain theories. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 19-27, </pages> <year> 1994. </year>
Reference-contexts: These systems are only intended to be representative examples of deliberate, declarative learners. There are a great number of other theory revision systems, each embodying different strengths and properties. For example, FORTE [Richards and Mooney, 1991] extends EITHER to first order logic as does GENTRE <ref> [Asker, 1994] </ref>. DUCTOR [Cain, 1991] is similar to EITHER, but including additional abduction components, and is in turn similar to OCCAM [Pazzani, 1988; Pazzani, 1991]. CLIPS-R [Murphy and Pazzani, 1994] even applies theory revision to production rules.
Reference: [Baffes and Mooney, 1993] <author> Paul T. Baffes and Raymond J. Mooney. </author> <title> Symbolic revision of theories with m-of-n rules. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1135-1140, </pages> <year> 1993. </year>
Reference-contexts: Finally, the learning is assumed to be correct and is generally irreversible. Therefore a domain with processes that evolve would be unlearnable (E10). EITHER/NEITHER, FOIL/FOCL and TRAIL EITHER [Ourston and Mooney, 1990] and the more recent NEITHER <ref> [Baffes and Mooney, 1993] </ref>, correct errors in Horn-clause propositional logic domain theories. EITHER corrects errors in the theory as a whole. The theory's preconditions are antecedents of the Horn clauses that lead to an example being classified as belonging to the theory.
Reference: [Benson and Nilsson, 1996] <author> Scott Benson and Nils J. Nilsson. </author> <title> Reacting, planning, and learning in an autonomous agent. </title> <editor> In Stephen Muggleton, Donald Michie, and Koichi Furukawa, editors, </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 14. </volume> <publisher> Oxford University Press, </publisher> <year> 1996. </year>
Reference-contexts: The classification rules FOIL learns can be seen as operator preconditions, represented in a semi-extensional disjunctive normal form. As the algorithm is non-incremental and the representation is built up one literal at a time, learning time is proportional to the size of the representation. TRAIL TRAIL <ref> [Benson, 1995; Benson and Nilsson, 1996] </ref> learns planning knowledge in the form of teleo-operators (or TOPS). Operators consist of a precondition (or preimage) which can be disjunctive, an action, a single intended effect (or postcondition literal) and a set of probabilistic side effects. <p> This monitoring usually assumes actions produce discrete, deterministic effects, limiting the scope of actions that can be modeled. TRAIL <ref> [Benson, 1995; Benson and Nilsson, 1996] </ref> is an exception to this, explicitly monitoring one primary effect of an operator but also probabilistically modeling a set of possible side-effects of the action. 49 These detection methods classify failures as the inability to accurately predict changes in the environment. <p> Thus, learning would either conclude that braking produces zero deceleration (the final state) or a range of conflicting decelerations (from the sequence of states). 11.2.4 TRAIL TRAIL <ref> [Benson, 1995; Benson and Nilsson, 1996] </ref> learns planning knowledge in the form of teleo-operators (or TOPS). Operators consist of a possibly disjunctive, precondition (or preimage), an action, a single intended effect (or postcondition literal) and a set of probabilistic side effects.
Reference: [Benson, 1995] <author> Scott Benson. </author> <title> Inductive learning of reactive action models. </title> <booktitle> In Proceedings of the Twelth International Conference on Machine Learning, </booktitle> <pages> pages 47-54, </pages> <year> 1995. </year>
Reference-contexts: The classification rules FOIL learns can be seen as operator preconditions, represented in a semi-extensional disjunctive normal form. As the algorithm is non-incremental and the representation is built up one literal at a time, learning time is proportional to the size of the representation. TRAIL TRAIL <ref> [Benson, 1995; Benson and Nilsson, 1996] </ref> learns planning knowledge in the form of teleo-operators (or TOPS). Operators consist of a precondition (or preimage) which can be disjunctive, an action, a single intended effect (or postcondition literal) and a set of probabilistic side effects. <p> This is the approach used in EXPO [Gil, 1992; Gil, 1994], LIVE [Shen and Simon, 1989; Shen, 1994], OBSERVER [Wang, 1995; Wang, 1996] and TRAIL <ref> [Benson, 1995] </ref> (although TRAIL records a sequence of states to allow for actions with duration). <p> This monitoring usually assumes actions produce discrete, deterministic effects, limiting the scope of actions that can be modeled. TRAIL <ref> [Benson, 1995; Benson and Nilsson, 1996] </ref> is an exception to this, explicitly monitoring one primary effect of an operator but also probabilistically modeling a set of possible side-effects of the action. 49 These detection methods classify failures as the inability to accurately predict changes in the environment. <p> Thus, learning would either conclude that braking produces zero deceleration (the final state) or a range of conflicting decelerations (from the sequence of states). 11.2.4 TRAIL TRAIL <ref> [Benson, 1995; Benson and Nilsson, 1996] </ref> learns planning knowledge in the form of teleo-operators (or TOPS). Operators consist of a possibly disjunctive, precondition (or preimage), an action, a single intended effect (or postcondition literal) and a set of probabilistic side effects.
Reference: [Booker et al., 1989] <author> L. B. Booker, D. E. Goldberg, and J. H. Holland. </author> <title> Classifier systems and genetic algorithms. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 235-282, </pages> <year> 1989. </year>
Reference-contexts: Eventually, the agent may drive more slowly, but this will never have been explicitly located as the cause of the failure. These learners typically learn a compact, intentional representation. Classifier Systems Classifiers <ref> [Holland, 1986; Booker et al., 1989] </ref>, represent domain knowledge as rules. The rules are typically fixed length binary strings which restricts their expressiveness and the agent relies on reactive execution rather than planning to select actions. <p> Errors in a specific rule's knowledge are not identified and corrected by classifier systems. Instead, rules that are generally successful are used to create new offspring through the use of a genetic algorithm, with the offspring replacing other low strength (and hopefully incorrect) rules. It can be shown <ref> [Booker et al., 1989] </ref> that by doing this the more useful sections of a rule, or building blocks, will be preserved in future rule populations, while less useful parts are lost. The genetic algorithm requires declarative access to the rule base to produce the offspring generation.
Reference: [Cain, 1991] <author> Timothy Cain. </author> <title> The ductor: A theory revision system for propositional domains. </title> <booktitle> In Proceedings of the Eighth International Conference on Machine Learning, </booktitle> <pages> pages 485-489, </pages> <year> 1991. </year>
Reference-contexts: These systems are only intended to be representative examples of deliberate, declarative learners. There are a great number of other theory revision systems, each embodying different strengths and properties. For example, FORTE [Richards and Mooney, 1991] extends EITHER to first order logic as does GENTRE [Asker, 1994]. DUCTOR <ref> [Cain, 1991] </ref> is similar to EITHER, but including additional abduction components, and is in turn similar to OCCAM [Pazzani, 1988; Pazzani, 1991]. CLIPS-R [Murphy and Pazzani, 1994] even applies theory revision to production rules.
Reference: [Carbonell and Gil, 1987] <author> Jaime G. Carbonell and Yolanda Gil. </author> <title> Learning by experimentation. </title> <booktitle> In Proceedings of the International Workshop on Machine Learning, </booktitle> <pages> pages 256-265, </pages> <year> 1987. </year>
Reference-contexts: To detect this failure each of the operator's actions are verified as having been achieved in the external world. If none of the actions have been achieved, this may suggest that the operator failed to apply at all, perhaps because there was an unknown precondition of the operator <ref> [Carbonell and Gil, 1987] </ref>. This monitoring usually assumes actions produce discrete, deterministic effects, limiting the scope of actions that can be modeled.
Reference: [Carbonell et al., 1991] <author> Jaime G. Carbonell, Craig A. Knoblock, and Steven Minton. </author> <title> Prodigy: An integrated architecture for planning and learning. </title> <editor> In Kurt VanLehn, editor, </editor> <booktitle> Architectures for Intelligence. </booktitle> <publisher> Lawrence Erlbaum, </publisher> <year> 1991. </year>
Reference-contexts: The STRIPS-like planning representation allows these systems to reason in large state and goal spaces (E1and E2) and (for EXPO and OBSERVER) to use a full planner from the PRODIGY architecture <ref> [Minton et al., 1989; Carbonell et al., 1991] </ref>. Experiments or explorations are deliberately selected for their ability to guide learning, thereby re-using the internal planning as a source of guidance during learning (E11).
Reference: [Clearwater et al., 1989] <author> Scott H. Clearwater, Tze-Pin Cheng, Haym Hirsh, and Bruce G. Buchanan. </author> <title> Incremental batch learning. </title> <booktitle> In Proceedings of the International Workshop on Machine Learning, </booktitle> <pages> pages 366-370, </pages> <year> 1989. </year>
Reference-contexts: Thus IMPROV and EXPO define two interesting, k-incremental learners along the spectrum from pure incremental learners to pure non-incremental learners (Figure 9.8). learners K-incremental learning is related to incremental batch learners, such as RL <ref> [Clearwater et al., 1989] </ref>. However, those learners train on a set of randomly selected instances and therefore are passive learners. IMPROV and EXPO are actively creating instances as they act in the world.
Reference: [Covrigaru, 1992] <author> Arie Covrigaru. </author> <title> Emergence of meta-level control in multi-tasking autonomous agents. </title> <type> PhD thesis, </type> <institution> University of Michigan, Department of Electrical Engineering and Computer Science, </institution> <year> 1992. </year>
Reference-contexts: This representation can be extended to cover multiple actions that occur simultaneously (e.g. turning and braking at the same time) by representing the combined actions as a single, compound operator (e.g. Turn-and-Brake) <ref> [Covrigaru, 1992] </ref>. either as direct, operator implementation knowledge (in the top of the figure) or as a pair of more primitive operators (in the lower part of the figure). Notice that the knowledge about whether a particular effect should occur moves into the operator preconditions for the primitive operators.
Reference: [DeJong and Mooney, 1986] <author> Gerald F. DeJong and Raymond J. Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: In the subgoal, operators are again selected and applied in an effort to decide how to resolve the original impasse. Results from the processing are cached by Soar's only learning mechanism: chunking [Laird et al., 1986], a form of Explanation-Based Learning <ref> [Mitchell et al., 1986; DeJong and Mooney, 1986] </ref>. It is important to realize that chunking only analyzes the trace of working memory elements created and tested during problem solving, so learning costs are proportional to execution costs.
Reference: [Doorenbos, 1993] <author> Robert B. Doorenbos. </author> <title> Matching 100,000 learned rules. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 290-296, </pages> <year> 1993. </year> <month> 153 </month>
Reference-contexts: The match algorithm has also been tested on cases of matching over 100,000 rules <ref> [Doorenbos, 1993] </ref> and the counter-intuitive result has been shown that, for some systems, there is no increase in match cost as the number of rules increases. 5. Efficient Calculation of Operator Effects Operator effects are encoded as production rules. <p> As the old rules are not discarded this means the rule base will grow in size over the life of the agent. This could slow down the agent's execution performance. However, our hope (born out by the empirical evidence of matching 100,000 rules <ref> [Doorenbos, 1993] </ref> without increased cost) is that improved match algorithms will ensure no long term slow down as more rules are learned. <p> This speed up is offset by an increase in match cost as rules are added, however empirical evidence indicates that for the latest version of Soar's matcher, the match time increases either very slowly or not at all as more rules are added <ref> [Doorenbos, 1993] </ref>. In our experience with IMPROV, no slow down is apparent as new rules are learned, but we have not explored learning especially large numbers of rules. 72 CHAPTER 9 Correcting Operator Preconditions Having detected an error, the agent is faced with two, related problems: 1. <p> An alternative is selected when the agent considers choosing the same operator in service of 1 Soar's highly optimized match algorithm has time complexity that's proportional to the size of working memory, but under certain conditions is independent of the number of rules being matched <ref> [Doorenbos, 1993] </ref>. 83 the same goal. This leads to a high degree of exploration, as the agent will always try an alternative action when faced with a goal it has earlier failed to achieve. 3. <p> In this research project we have not directly examined this trade-off. However, on certain problems, the Soar matcher has been shown to maintain a constant speed as up to 100,000 new rules are learned <ref> [Doorenbos, 1993] </ref>. This result suggests that knowledge compilation in Soar may be beneficial. However, as this remains a matter for debate, IMPROV can either be run in a mode where knowledge is compiled, or where full problem decomposition occurs.
Reference: [Fikes and Nilsson, 1971] <author> R. E. Fikes and N. J. Nilsson. </author> <title> STRIPS: A new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 189-208, </pages> <year> 1971. </year>
Reference-contexts: They are not generally applicable to the more challenging environments of Chapter 2. 22 EXPO, LIVE and OBSERVER EXPO [Gil, 1991; Gil, 1993], LIVE [Shen and Simon, 1989] and OBSERVER [Wang, 1995; Wang, 1996] share a similar STRIPS-like <ref> [Fikes and Nilsson, 1971] </ref> representation. Operators are declaratively represented as structures with lists of preconditions and effects. The preconditions are represented in disjunctive normal form. The effects are limited to a single, state-to-state transition, although they can include conditional effects.
Reference: [Gil, 1991] <author> Yolanda Gil. </author> <title> A domain-independent framework for effective experimentation in planning. </title> <booktitle> In Proceedings of the International Machine Learning Workshop, </booktitle> <pages> pages 13-17, </pages> <year> 1991. </year>
Reference-contexts: They are not generally applicable to the more challenging environments of Chapter 2. 22 EXPO, LIVE and OBSERVER EXPO <ref> [Gil, 1991; Gil, 1993] </ref>, LIVE [Shen and Simon, 1989] and OBSERVER [Wang, 1995; Wang, 1996] share a similar STRIPS-like [Fikes and Nilsson, 1971] representation. Operators are declaratively represented as structures with lists of preconditions and effects. The preconditions are represented in disjunctive normal form. <p> Operators are declaratively represented as structures with lists of preconditions and effects. The preconditions are represented in disjunctive normal form. The effects are limited to a single, state-to-state transition, although they can include conditional effects. EXPO <ref> [Gil, 1991; Gil, 1993] </ref> learns corrections to its domain knowledge by designing experiments to refine initially overgeneral operator preconditions. Errors are detected by explicit monitoring of operator preconditions and actions, before and after an operator is executed. <p> IMPROV delays learning until a correct plan has been found. If training is delayed even further, until a unique cause of the failure has been identified, this results in a system that is actively experimenting (such as the induction that happens in EXPO <ref> [Gil, 1991; Gil, 1993] </ref>-see Chapter 3 for a discussion of related approaches to learning precondition knowledge).
Reference: [Gil, 1992] <author> Yolanda Gil. </author> <title> Acquiring Domain Knowledge for Planning by Experimentation. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Also, they learn slowly as a result of their weak methods. In contrast, the second category (shown in Figure 1.1 (b)) consists of symbolic theory revision systems (e.g. EITHER [Ourston and Mooney, 1990], EXPO <ref> [Gil, 1992] </ref>, OCCAM [Paz-zani, 1988]). These systems learn declarative planning knowledge through stronger methods that explicitly reason to identify and correct errors in the agent's domain knowledge. <p> The standard approach to making this comparison is by explicitly comparing a declarative model of the operator to the states before and/or after the operator is applied. This is the approach used in EXPO <ref> [Gil, 1992; Gil, 1994] </ref>, LIVE [Shen and Simon, 1989; Shen, 1994], OBSERVER [Wang, 1995; Wang, 1996] and TRAIL [Benson, 1995] (although TRAIL records a sequence of states to allow for actions with duration). <p> We also described how most existing systems that build plans and then detect errors in their execution (e.g. CHEF [Hammond, 1989; Hammond et al., 1990], EXPO <ref> [Gil, 1992] </ref> and Phoenix [Howe and Cohen, 1991]) typically rely on an explicit comparison of the sensed environment to the agent's operator knowledge at each step of a plan. This class of systems typically rely on three distinct components during plan execution: 1. <p> In the remainder of this section we will examine some of the existing approaches to learning operator effects and discuss the range of operator effects they can model. 11.2.1 LIVE, EXPO and OBSERVER LIVE [Shen, 1989; Shen, 1994], EXPO <ref> [Gil, 1992; Gil, 1994] </ref> and OBSERVER [Wang, 1995; Wang, 1996] are three, related approaches to learning planning knowledge to model the effect of actions.
Reference: [Gil, 1993] <author> Yolanda Gil. </author> <title> Efficient domain-independent experimentation. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pages 128-134, </pages> <year> 1993. </year>
Reference-contexts: They are not generally applicable to the more challenging environments of Chapter 2. 22 EXPO, LIVE and OBSERVER EXPO <ref> [Gil, 1991; Gil, 1993] </ref>, LIVE [Shen and Simon, 1989] and OBSERVER [Wang, 1995; Wang, 1996] share a similar STRIPS-like [Fikes and Nilsson, 1971] representation. Operators are declaratively represented as structures with lists of preconditions and effects. The preconditions are represented in disjunctive normal form. <p> Operators are declaratively represented as structures with lists of preconditions and effects. The preconditions are represented in disjunctive normal form. The effects are limited to a single, state-to-state transition, although they can include conditional effects. EXPO <ref> [Gil, 1991; Gil, 1993] </ref> learns corrections to its domain knowledge by designing experiments to refine initially overgeneral operator preconditions. Errors are detected by explicit monitoring of operator preconditions and actions, before and after an operator is executed. <p> IMPROV delays learning until a correct plan has been found. If training is delayed even further, until a unique cause of the failure has been identified, this results in a system that is actively experimenting (such as the induction that happens in EXPO <ref> [Gil, 1991; Gil, 1993] </ref>-see Chapter 3 for a discussion of related approaches to learning precondition knowledge).
Reference: [Gil, 1994] <author> Yolanda Gil. </author> <title> Learning by experimentation: Incremental refinement of incomplete planning domains. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pages 87-95, </pages> <year> 1994. </year>
Reference-contexts: The standard approach to making this comparison is by explicitly comparing a declarative model of the operator to the states before and/or after the operator is applied. This is the approach used in EXPO <ref> [Gil, 1992; Gil, 1994] </ref>, LIVE [Shen and Simon, 1989; Shen, 1994], OBSERVER [Wang, 1995; Wang, 1996] and TRAIL [Benson, 1995] (although TRAIL records a sequence of states to allow for actions with duration). <p> In the remainder of this section we will examine some of the existing approaches to learning operator effects and discuss the range of operator effects they can model. 11.2.1 LIVE, EXPO and OBSERVER LIVE [Shen, 1989; Shen, 1994], EXPO <ref> [Gil, 1992; Gil, 1994] </ref> and OBSERVER [Wang, 1995; Wang, 1996] are three, related approaches to learning planning knowledge to model the effect of actions.
Reference: [Gratch and DeJong, 1992] <author> Jonathan Gratch and Gerald DeJong. Composer: </author> <title> A probabilistic solution to the utility problem in speed-up learning. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 235-240, </pages> <year> 1992. </year>
Reference-contexts: This is an example of the utility problem [Minton, 1990]. The extra rules may not be used sufficiently often to produce an overall performance improvement. Considerable research has been directed at examining this problem (e.g. <ref> [Gratch and DeJong, 1992; Wray et al., 1996; Minton, 1996] </ref>). In this research project we have not directly examined this trade-off. However, on certain problems, the Soar matcher has been shown to maintain a constant speed as up to 100,000 new rules are learned [Doorenbos, 1993].
Reference: [Gupta, 1987] <author> Ajay Gupta. </author> <title> Explanation-based failure recovery. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 606-610, </pages> <year> 1987. </year>
Reference: [Hall, 1986] <author> R.J. Hall. </author> <title> Learning by failing to explain. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 568-572, </pages> <address> Philadelphia, PA, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: To detect this type of failure, the agent needs additional knowledge to signal that it should be possible to build a plan for the current goal. For example, a number of systems attempt to construct explanatory parses of training examples (e.g. <ref> [VanLehn, 1987; Hall, 1986; Hall, 1988] </ref>). The inability to build an explanation is equivalent to an incomplete plan failure. Rajamoney and DeJong [1988] also examined cases where the agent knows that only one plan should be formed, but multiple plans are found.
Reference: [Hall, 1988] <author> Robert J. Hall. </author> <title> Learning by failing to explain: Using partial explanations to learn in incomplete or intractable domains. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 45-78, </pages> <year> 1988. </year>
Reference-contexts: To detect this type of failure, the agent needs additional knowledge to signal that it should be possible to build a plan for the current goal. For example, a number of systems attempt to construct explanatory parses of training examples (e.g. <ref> [VanLehn, 1987; Hall, 1986; Hall, 1988] </ref>). The inability to build an explanation is equivalent to an incomplete plan failure. Rajamoney and DeJong [1988] also examined cases where the agent knows that only one plan should be formed, but multiple plans are found.
Reference: [Hammond et al., 1990] <author> Kristian Hammond, Timothy Converse, and Charles Martin. </author> <title> Integrating planning and acting in a case-based framework. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 292-297, </pages> <address> Boston, Mass., 1990. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: We also described how most existing systems that build plans and then detect errors in their execution (e.g. CHEF <ref> [Hammond, 1989; Hammond et al., 1990] </ref>, EXPO [Gil, 1992] and Phoenix [Howe and Cohen, 1991]) typically rely on an explicit comparison of the sensed environment to the agent's operator knowledge at each step of a plan. This class of systems typically rely on three distinct components during plan execution: 1.
Reference: [Hammond, 1986] <author> Kristian J. Hammond. </author> <title> Learning to anticipate and avoid planning problems through the explanation of failures. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 556-560, </pages> <year> 1986. </year>
Reference-contexts: These systems generally assume complete and perfect sensing as the theory must cover all training instances (although this restriction can be relaxed to improve noise tolerance, as happens in TRAIL) (E8). CHEF CHEF <ref> [Hammond, 1989; Hammond, 1986] </ref> uses case-based reasoning to repair plans and recover from failures. CHEF records a library of plans and a library of patches that can be used to convert an existing plan into one applicable to a new goal.
Reference: [Hammond, 1989] <editor> Kristian Hammond. Chef. In C. K. Riesbeck and R. C. Schank, editors, </editor> <title> Inside case-based reasoning. </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1989. </year>
Reference-contexts: These systems generally assume complete and perfect sensing as the theory must cover all training instances (although this restriction can be relaxed to improve noise tolerance, as happens in TRAIL) (E8). CHEF CHEF <ref> [Hammond, 1989; Hammond, 1986] </ref> uses case-based reasoning to repair plans and recover from failures. CHEF records a library of plans and a library of patches that can be used to convert an existing plan into one applicable to a new goal. <p> We also described how most existing systems that build plans and then detect errors in their execution (e.g. CHEF <ref> [Hammond, 1989; Hammond et al., 1990] </ref>, EXPO [Gil, 1992] and Phoenix [Howe and Cohen, 1991]) typically rely on an explicit comparison of the sensed environment to the agent's operator knowledge at each step of a plan. This class of systems typically rely on three distinct components during plan execution: 1.
Reference: [Hard et al., 1990] <author> D. Hard, S. Anderson, and P. Cohen. </author> <title> Envelopes as a vehicle for improving the efficiency of plan execution. </title> <booktitle> In Proceedings of DARPA Workshop on Innovative Approaches to Planning, Scheduling and Contol, </booktitle> <pages> pages 71-76, </pages> <year> 1990. </year>
Reference: [Holland, 1986] <author> John H. Holland. </author> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach, Volume II. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: Figure 1.1 (a) shows the main components of agents in the first class, reinforcement learners (e.g. Q-Learning [Watkins and Dayan, 1992], Classifiers <ref> [Holland, 1986] </ref>, Backpropogation [Rumel-hart et al., 1986]). These systems use weak inductive learning methods to directly modify an agent's execution knowledge. The execution knowledge is generally represented procedurally (e.g. in a neural net). <p> Eventually, the agent may drive more slowly, but this will never have been explicitly located as the cause of the failure. These learners typically learn a compact, intentional representation. Classifier Systems Classifiers <ref> [Holland, 1986; Booker et al., 1989] </ref>, represent domain knowledge as rules. The rules are typically fixed length binary strings which restricts their expressiveness and the agent relies on reactive execution rather than planning to select actions.
Reference: [Howe and Cohen, 1991] <author> Adele E. Howe and Paul R. Cohen. </author> <title> Failure recovery: A model and experiments. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 801-808, </pages> <month> July </month> <year> 1991. </year> <month> 154 </month>
Reference-contexts: We also described how most existing systems that build plans and then detect errors in their execution (e.g. CHEF [Hammond, 1989; Hammond et al., 1990], EXPO [Gil, 1992] and Phoenix <ref> [Howe and Cohen, 1991] </ref>) typically rely on an explicit comparison of the sensed environment to the agent's operator knowledge at each step of a plan. This class of systems typically rely on three distinct components during plan execution: 1.
Reference: [Huffman and Laird, 1993] <author> Scott B. Huffman and John E. Laird. </author> <title> Learning procedures from inter-active natural language instructions. </title> <editor> In P. Utgoff, editor, </editor> <booktitle> Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <year> 1993. </year>
Reference-contexts: The agent was then required to re-learn the correct effects of its actions. 93 Many forms of feedback [E11] In the driving domain we experimented with adding additional knowledge to guide IMPROV's learning. In additional work, IMPROV was integrated with a system for taking human instructions, Instructo-Soar <ref> [Huffman and Laird, 1993; Huffman, 1994] </ref>, and demonstrated on some simple tasks in a modified robotic domain. This demonstrates IMPROV's ability to receive guidance either from an external source (an instructor) or from an internal source (additional knowledge) [Pearson and Huffman, 1995].
Reference: [Huffman and Laird, 1994] <author> S. B. Huffman and J. E. Laird. </author> <title> Learning from highly flexible tutorial instruction. </title> <booktitle> In Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-94), </booktitle> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: This uniformity of representation and processing facilitates the inclusion of additional knowledge in guiding the induction, in the same manner that the rest of IMPROV can be improved through additional knowledge. For example, IMPROV (and SCA2) has been successfully integrated with Instructo-Soar <ref> [Huffman and Laird, 1994; Huffman, 1994] </ref>, a system for learning from instructions [Pearson and Huffman, 1995]. This integration was greatly simplified as IMPROV, SCA2 and Instructo-Soar shared the same representation and processing model. In Section 10.5 we evaluate the benefits of this additional knowledge in improving learning.
Reference: [Huffman et al., 1993] <author> Scott B. Huffman, Douglas J. Pearson, and John E. Laird. </author> <title> Correcting imperfect domain theories: A knowledge-level analysis. </title> <editor> In Susan Chipman and Alan L. Meyrowitz, editors, </editor> <title> Foundations of Knowledge Acquisition: Cognitive Models of Complex Learning. </title> <publisher> Kluwer Academic, </publisher> <year> 1993. </year> <note> Also available as technical report number CSE-TR-114-91, </note> <institution> University of Michi-gan Department of Electrical Engineering and Computer Science, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: The important point from the figure is that each class of knowledge level error can give rise to a range of performance failures. The range of performance failures is described briefly here, summarizing our more detailed analysis in <ref> [Huffman et al., 1993] </ref>. Planning Failures Execution Failures Incomplete Impossible Prec. Effects Explicit State Unmet Unmet Detection Overgen. Preconds. * * * Overspec.
Reference: [Huffman, 1994] <author> S. B. Huffman. </author> <title> Instructable autonomous agents. </title> <type> PhD thesis, </type> <institution> University of Michi-gan, Dept. of Electrical Engineering and Computer Science, </institution> <year> 1994. </year>
Reference-contexts: This uniformity of representation and processing facilitates the inclusion of additional knowledge in guiding the induction, in the same manner that the rest of IMPROV can be improved through additional knowledge. For example, IMPROV (and SCA2) has been successfully integrated with Instructo-Soar <ref> [Huffman and Laird, 1994; Huffman, 1994] </ref>, a system for learning from instructions [Pearson and Huffman, 1995]. This integration was greatly simplified as IMPROV, SCA2 and Instructo-Soar shared the same representation and processing model. In Section 10.5 we evaluate the benefits of this additional knowledge in improving learning. <p> The agent was then required to re-learn the correct effects of its actions. 93 Many forms of feedback [E11] In the driving domain we experimented with adding additional knowledge to guide IMPROV's learning. In additional work, IMPROV was integrated with a system for taking human instructions, Instructo-Soar <ref> [Huffman and Laird, 1993; Huffman, 1994] </ref>, and demonstrated on some simple tasks in a modified robotic domain. This demonstrates IMPROV's ability to receive guidance either from an external source (an instructor) or from an internal source (additional knowledge) [Pearson and Huffman, 1995]. <p> For example, a system for learning knowledge from natural language instruction, Instructo-Soar <ref> [Huffman, 1994] </ref>, was successfully integrated with IMPROV in about two weeks; producing an agent that could recover from incorrect instruc tions or use instruction to guide recovery [Pearson and Huffman, 1995]. 4. Presenting K-Incremental learning IMPROV collects sets of training instances prior to learning.
Reference: [Laird and Newell, 1983] <author> J.E. Laird and A. Newell. </author> <title> A universal weak method: Summary of results. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 771-773, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: The learning can be made stronger by adding additional knowledge sources to guide the agent. This is analogous to making a weak search method stronger by adding additional knowledge <ref> [Laird and Newell, 1983] </ref>. This is a more flexible alternative to building a collection of domain specific methods and then selecting among them or integrating the results of 98 their reasoning.
Reference: [Laird et al., 1986] <author> John E. Laird, Paul S. Rosenbloom, and Allen Newell. </author> <title> Chunking in soar: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 11-46, </pages> <year> 1986. </year>
Reference-contexts: In the subgoal, operators are again selected and applied in an effort to decide how to resolve the original impasse. Results from the processing are cached by Soar's only learning mechanism: chunking <ref> [Laird et al., 1986] </ref>, a form of Explanation-Based Learning [Mitchell et al., 1986; DeJong and Mooney, 1986]. It is important to realize that chunking only analyzes the trace of working memory elements created and tested during problem solving, so learning costs are proportional to execution costs. <p> Instead, at each step during execution, IMPROV builds an explanation of why the next operator is being chosen. This explanation leads, through Soar's Explanation-Based Learning method, chunking <ref> [Laird et al., 1986] </ref>, to a new rule that matches the precondition of the operator being chosen, along with a test for the current goal instance. This process is shown in Figure 6.7.
Reference: [Laird et al., 1987] <author> John E. Laird, Allen Newell, and Paul S. Rosenbloom. </author> <title> Soar: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33(1) </volume> <pages> 1-64, </pages> <year> 1987. </year>
Reference-contexts: IMPROV exists as both a theoretical system for the deliberate learning of procedural planning knowledge and as a specific implementation of this theory within a particular cognitive architecture, Soar <ref> [Laird et al., 1987] </ref>. In presenting a theoretical or functional description, as well as a specific implementation, the intention is to help identify the contributions to other learning systems. For example, a Soar agent's knowledge is encoded as production rules. <p> the agent has this level of access, it does not mean it will be unable to function in complex environments, but the system will require careful design or it will fail. 3.2.2 Planning Knowledge Implementation We have mentioned that the current implementation of IMPROV is built within the Soar architecture <ref> [Laird et al., 1987] </ref> 1 . In Soar, all activity is cast in terms of applying operators to move through a state space represented by attribute-value pairs. During problem solving, Soar repeatedly selects and applies operators.
Reference: [Laird et al., 1995] <author> J. E. Laird, W. L. Johnson, R. M. Jones, F. Koss, J. F. Lehman, P. E. Nielsen, P. S. Rosenbloom, R. Rubinoff, K. B. Schamb, M. Tambe, J. Van Dyke, M. van Lent, and R. E. Wray. </author> <title> Simulated intelligent forces for air: </title> <booktitle> The Soar/IFOR project 1995. In Proceedings of the Fifth Conference on Computer Generated Forces and Behavioral Representation, </booktitle> <pages> pages 27-36, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: The efficiency of this representation has been demonstrated for complex, real-time domains including the control of simulated aircraft [Pearson et al., 1993a; Pearson et al., 1993b] and tactical air combat <ref> [Laird et al., 1995; Tambe et al., 1995] </ref>. The match algorithm has also been tested on cases of matching over 100,000 rules [Doorenbos, 1993] and the counter-intuitive result has been shown that, for some systems, there is no increase in match cost as the number of rules increases. 5.
Reference: [Laird, 1988] <author> John E. Laird. </author> <title> Recovery from incorrect knowledge in Soar. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 618-623, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: As we want to apply IMPROV to tasks that are time-critical (E6) we'd like to avoid this slow-down. Instead, our approach is to incrementally add rules that work together with the agent's existing knowledge to change the incorrect operator selection or implementation <ref> [Laird, 1988] </ref>. This method will be described in detail in Section 9.1.4. By adding just a few new rules, each correction can be made in constant time. As the old rules are not discarded this means the rule base will grow in size over the life of the agent. <p> As IMPROV is restricted to procedural access to its knowledge, it cannot modify the knowledge directly. Instead, IMPROV corrects the operator knowledge by learning additional rules that correct the decision about which operator to select. This is a general approach for modifying knowledge in Soar agents <ref> [Laird, 1988] </ref>. In IMPROV, a rule is learned to reject the original (incorrect) operator and another rule is learned which suggests the new (correct) 87 operator.
Reference: [Maclin and Shavlik, 1996] <author> R. Maclin and J. W. Shavlik. </author> <title> Creating advice-taking reinforcement learners. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 251-282, </pages> <year> 1996. </year>
Reference-contexts: Also, they are generally only able to learn from a single source of knowledge-reinforcement (E11). Recently there has been some work on using other knowledge sources, such as instruction <ref> [Maclin and Shavlik, 1996] </ref>, in neural network learning, but incorporating other knowledge sources remains a challenging problem. 3.1.4 Summary of Existing Approaches To summarize, we can classify existing research in terms of its knowledge representation, the access to that knowledge and whether the knowledge is directly reasoned about.
Reference: [Mahadevan and Connell, 1991] <author> Sridhar Mahadevan and Jonathan Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 768-773, </pages> <year> 1991. </year>
Reference-contexts: This means the methods are either not used on planning tasks or require a complete and correct model for the effects of each action. An interesting exception is the work of Mahadevan on learning action models for a robotic domain <ref> [Mahadevan, 1992; Mahadevan and Connell, 1991] </ref>. Q-learning [Watkins, 1989; Watkins and Dayan, 1992] is used to learn the expected reward of taking each possible action in any given state. As we discussed in Section 4.1 this large representation of states and actions is only appropriate for relatively small state spaces.
Reference: [Mahadevan, 1992] <author> Sridhar Mahadevan. </author> <title> Enhancing transfer in reinforcement learning by building stochastic models of robot actions. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> pages 290-299, </pages> <year> 1992. </year>
Reference-contexts: This means the methods are either not used on planning tasks or require a complete and correct model for the effects of each action. An interesting exception is the work of Mahadevan on learning action models for a robotic domain <ref> [Mahadevan, 1992; Mahadevan and Connell, 1991] </ref>. Q-learning [Watkins, 1989; Watkins and Dayan, 1992] is used to learn the expected reward of taking each possible action in any given state. As we discussed in Section 4.1 this large representation of states and actions is only appropriate for relatively small state spaces.
Reference: [McCloskey and Cohen, 1989] <author> Michael McCloskey and Neal J. Cohen. </author> <title> Catastrophic interference in connectionist networks: The sequential learning problem. </title> <journal> The Psychology of Learning and Motivation, </journal> <volume> 24 </volume> <pages> 109-165, </pages> <year> 1989. </year>
Reference-contexts: Many reinforcement learners attempt to maximize "reinforcement" without explicit structures to represent goals, leading to problems of task interference, where the trained system loses skill on one task as it learns another <ref> [McCloskey and Cohen, 1989; Ratcliff, 1990] </ref>. E3. Actions may have a complex structure Actions may have duration, taking time to produce changes in the world and possibly producing a series of sequential or transitory effects. <p> Thus, learning time is proportional to the size of the network. As in classifiers, the size of the representation is fixed, ensuring that learning does not slow down over time, but leading to problems with over-training and task interference <ref> [McCloskey and Cohen, 1989; Ratcliff, 1990] </ref>. 3.1.3 Implicit Learning of Extensional Representations Q-Learning Q-learning [Watkins, 1989; Watkins and Dayan, 1992] also implicitly corrects the agent's domain knowledge by improving the accuracy of the predicted reward for taking a particular action.
Reference: [Miller, 1991] <author> Craig M. Miller. </author> <title> A constraint-motivated model of concept formation. </title> <booktitle> In The Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 827-831, </pages> <year> 1991. </year> <month> 155 </month>
Reference-contexts: The category being learned is which operator is the correct one to use for the current state and goal. In this chapter we will discuss environmental properties that constrain the inductive learner and then present a learning method, SCA2, that is an extension of the symbolic category learner, SCA <ref> [Miller, 1991; Miller, 1993] </ref>. Readers who are not interested in the details of the inductive process can proceed to the next chapter. The environmental properties we first considered in Chapter 2 constrain the design of the inductive learner.
Reference: [Miller, 1993] <author> Craig M. Miller. </author> <title> A model of concept acquisition in the context of a unified theory of cognition. </title> <type> PhD thesis, </type> <institution> The University of Michigan, Dept. of Computer Science and Electrical Engineering, </institution> <year> 1993. </year>
Reference-contexts: The category being learned is which operator is the correct one to use for the current state and goal. In this chapter we will discuss environmental properties that constrain the inductive learner and then present a learning method, SCA2, that is an extension of the symbolic category learner, SCA <ref> [Miller, 1991; Miller, 1993] </ref>. Readers who are not interested in the details of the inductive process can proceed to the next chapter. The environmental properties we first considered in Chapter 2 constrain the design of the inductive learner. <p> Third, its representation must be expressive enough to describe the large sets of disjunctive conditions required by complex tasks (E4). Finally, the learner should take advantage of additional knowledge sources or feedback when they are available (E11). 8.1 SCA2 Functional Description IMPROV uses SCA2 (an extension of SCA <ref> [Miller, 1993] </ref>) for inductive learning. SCA is an incremental, symbolic category learner that represents classification knowledge as production rules. SCA is a computational model that accurately models many human behaviors exhibited during category learning.
Reference: [Minton et al., 1989] <author> Steven Minton, Jaime G. Carbonell, Craig A. Knoblock, Daniel R. Kuokka, Oren Etzioni, and Yolanda Gil. </author> <title> Explanation-based learning: A problem-solving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 63-118, </pages> <year> 1989. </year>
Reference-contexts: The STRIPS-like planning representation allows these systems to reason in large state and goal spaces (E1and E2) and (for EXPO and OBSERVER) to use a full planner from the PRODIGY architecture <ref> [Minton et al., 1989; Carbonell et al., 1991] </ref>. Experiments or explorations are deliberately selected for their ability to guide learning, thereby re-using the internal planning as a source of guidance during learning (E11).
Reference: [Minton, 1990] <author> Steven N. Minton. </author> <title> Quantatative results concerning the utility of explanation-based learning. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 363-392, </pages> <year> 1990. </year>
Reference-contexts: There is the potential for the additional rules to slow down the rule matcher by a greater amount than is saved during planning through compiled macro-operators. This is an example of the utility problem <ref> [Minton, 1990] </ref>. The extra rules may not be used sufficiently often to produce an overall performance improvement. Considerable research has been directed at examining this problem (e.g. [Gratch and DeJong, 1992; Wray et al., 1996; Minton, 1996]). In this research project we have not directly examined this trade-off.
Reference: [Minton, 1996] <author> Steven Minton. </author> <title> Is there any need for domain-dependent control information? a reply. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 855-862, </pages> <year> 1996. </year>
Reference-contexts: This is an example of the utility problem [Minton, 1990]. The extra rules may not be used sufficiently often to produce an overall performance improvement. Considerable research has been directed at examining this problem (e.g. <ref> [Gratch and DeJong, 1992; Wray et al., 1996; Minton, 1996] </ref>). In this research project we have not directly examined this trade-off. However, on certain problems, the Soar matcher has been shown to maintain a constant speed as up to 100,000 new rules are learned [Doorenbos, 1993].
Reference: [Mitchell et al., 1986] <author> Tom M. Mitchell, R. M. Keller, and S. T. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <year> 1986. </year>
Reference-contexts: In the subgoal, operators are again selected and applied in an effort to decide how to resolve the original impasse. Results from the processing are cached by Soar's only learning mechanism: chunking [Laird et al., 1986], a form of Explanation-Based Learning <ref> [Mitchell et al., 1986; DeJong and Mooney, 1986] </ref>. It is important to realize that chunking only analyzes the trace of working memory elements created and tested during problem solving, so learning costs are proportional to execution costs. <p> Traditional approaches to classifying errors have been based on performance failures, such as failing to achieve a desired effect in the world <ref> [Mitchell et al., 1986; Rajamoney and DeJong, 1987] </ref>. The scope of the learning system is defined in terms of the classes of performance failures that it can recover from. In this section we will propose an alternative method for classifying errors based on the agent's knowledge.
Reference: [Mitchell, 1982] <author> Tom M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18(2) </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: The set of operator preconditions is represented as two sets, one a most specific version and the other a most general version, consist with the observed training instances. These sets are refined in a manner similar to that of version spaces <ref> [Mitchell, 1982] </ref> and relies on operator preconditions being a single, conjunctive set.
Reference: [Muggleton and Feng, 1992] <author> Stephen Muggleton and Cao Feng. </author> <title> Efficient induction of logic programs. </title> <editor> In Stephen Muggleton, editor, </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: TRAIL requires that the preconditions of the operator remain true for the duration of the operator. This means that each state, along the course of a successful operator application, is a positive instance of the operator precondition. The ILP module <ref> [Muggleton and Feng, 1992] </ref> is trained on each of these instances. This learning is similar to IMPROV's training on a set of instances, but in IMPROV's case the instances are collected across multiple training episodes.
Reference: [Murphy and Pazzani, 1994] <author> Patrick M. Murphy and Michael J. Pazzani. </author> <title> Revision of production system rule-bases. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pages 199-207, </pages> <year> 1994. </year>
Reference-contexts: For example, FORTE [Richards and Mooney, 1991] extends EITHER to first order logic as does GENTRE [Asker, 1994]. DUCTOR [Cain, 1991] is similar to EITHER, but including additional abduction components, and is in turn similar to OCCAM [Pazzani, 1988; Pazzani, 1991]. CLIPS-R <ref> [Murphy and Pazzani, 1994] </ref> even applies theory revision to production rules.
Reference: [Oates and Cohen, 1996] <author> Tim Oates and Paul R. Cohen. </author> <title> Searching for planning operators with context-dependent and probabilistic effects. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 863-868, </pages> <year> 1996. </year>
Reference-contexts: However, in Mahadevan's system it is assumed that complex effects, such as the braking example, can be decomposed into a series of primitive actions, each producing a single effect. 11.2.3 MSDD Multi-Stream Dependency Detection (MSDD) <ref> [Oates and Cohen, 1996] </ref> is a learning method that has been applied to learning planning operator preconditions and effects.
Reference: [Ourston and Mooney, 1990] <author> Dirk Ourston and Raymond J. Mooney. </author> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 815-820, </pages> <year> 1990. </year>
Reference-contexts: As a result they are usually only applied to domains with small state and goal spaces. Also, they learn slowly as a result of their weak methods. In contrast, the second category (shown in Figure 1.1 (b)) consists of symbolic theory revision systems (e.g. EITHER <ref> [Ourston and Mooney, 1990] </ref>, EXPO [Gil, 1992], OCCAM [Paz-zani, 1988]). These systems learn declarative planning knowledge through stronger methods that explicitly reason to identify and correct errors in the agent's domain knowledge. <p> Finally, the learning is assumed to be correct and is generally irreversible. Therefore a domain with processes that evolve would be unlearnable (E10). EITHER/NEITHER, FOIL/FOCL and TRAIL EITHER <ref> [Ourston and Mooney, 1990] </ref> and the more recent NEITHER [Baffes and Mooney, 1993], correct errors in Horn-clause propositional logic domain theories. EITHER corrects errors in the theory as a whole.
Reference: [Pazzani and Kibler, 1992] <author> Michael Pazzani and Dennis Kibler. </author> <title> The role of prior knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 57-94, </pages> <year> 1992. </year>
Reference-contexts: A non-incremental, greedy algorithm iteratively adds the best repair to the new theory until all examples are covered. FOIL/FOCL FOIL [Quinlan, 1990] learns horn clause classification rules, or first-order logic programs, and is therefore an example of inductive logic programming. FOIL has been extended to FOCL <ref> [Pazzani et al., 1991; Pazzani and Kibler, 1992] </ref>, which also includes an explanation-based learning component. FOIL uses a greedy algorithm, based on adding literals with maximum information gain, to cover a 24 set of positive training instances, while avoiding covering negative instances.
Reference: [Pazzani et al., 1991] <author> Michael J. Pazzani, Clifford A. Brunk, and Glenn Silverstein. </author> <title> A knowledge-intensive approach to learning relational concepts. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 432-436, </pages> <year> 1991. </year>
Reference-contexts: A non-incremental, greedy algorithm iteratively adds the best repair to the new theory until all examples are covered. FOIL/FOCL FOIL [Quinlan, 1990] learns horn clause classification rules, or first-order logic programs, and is therefore an example of inductive logic programming. FOIL has been extended to FOCL <ref> [Pazzani et al., 1991; Pazzani and Kibler, 1992] </ref>, which also includes an explanation-based learning component. FOIL uses a greedy algorithm, based on adding literals with maximum information gain, to cover a 24 set of positive training instances, while avoiding covering negative instances.
Reference: [Pazzani, 1988] <author> Michael J. Pazzani. </author> <title> Integrated learning with incorrect and incomplete theories. </title> <booktitle> In Proceedings of the International Machine Learning Conference, </booktitle> <pages> pages 291-297, </pages> <year> 1988. </year>
Reference-contexts: For example, FORTE [Richards and Mooney, 1991] extends EITHER to first order logic as does GENTRE [Asker, 1994]. DUCTOR [Cain, 1991] is similar to EITHER, but including additional abduction components, and is in turn similar to OCCAM <ref> [Pazzani, 1988; Pazzani, 1991] </ref>. CLIPS-R [Murphy and Pazzani, 1994] even applies theory revision to production rules.
Reference: [Pazzani, 1991] <author> Michael Pazzani. </author> <title> Learning to predict and explain: An integration of similarity-based, theory driven, and explanation-based learning. </title> <journal> Journal of the Learning Sciences, </journal> <volume> 1(2) </volume> <pages> 153-199, </pages> <year> 1991. </year>
Reference-contexts: For example, FORTE [Richards and Mooney, 1991] extends EITHER to first order logic as does GENTRE [Asker, 1994]. DUCTOR [Cain, 1991] is similar to EITHER, but including additional abduction components, and is in turn similar to OCCAM <ref> [Pazzani, 1988; Pazzani, 1991] </ref>. CLIPS-R [Murphy and Pazzani, 1994] even applies theory revision to production rules.
Reference: [Pearson and Huffman, 1995] <author> Douglas J. Pearson and Scott B. Huffman. </author> <title> Combining learn-ing from instruction with recovery from incorrect knowledge. In Machine Learning Conference Workshop on Agents that learn from other agents., </title> <note> 1995. Available from http://ai.eecs.umich.edu/people/douglasp/homepage.html. </note>
Reference-contexts: This weak, domain independent error detection method can easily be augmented with additional domain specific knowledge. In evaluating IMPROV we compare the weak approach to one augmented by instruction (an external agent signaling errors <ref> [Pearson and Huffman, 1995] </ref>) and by an additional theory of failure states (knowledge of states that will lead inevitably to failure). <p> For example, IMPROV (and SCA2) has been successfully integrated with Instructo-Soar [Huffman and Laird, 1994; Huffman, 1994], a system for learning from instructions <ref> [Pearson and Huffman, 1995] </ref>. This integration was greatly simplified as IMPROV, SCA2 and Instructo-Soar shared the same representation and processing model. In Section 10.5 we evaluate the benefits of this additional knowledge in improving learning. SCA2's learning is limited to symbolic concept descriptions. <p> This demonstrates IMPROV's ability to receive guidance either from an external source (an instructor) or from an internal source (additional knowledge) <ref> [Pearson and Huffman, 1995] </ref>. The experiments were not designed to compare IMPROV's learning rate or accuracy to that of other systems, but rather to evaluate the scope of IMPROV's learning and demonstrate its ability to learn in environments with a wide range of challenging properties. <p> For example, a system for learning knowledge from natural language instruction, Instructo-Soar [Huffman, 1994], was successfully integrated with IMPROV in about two weeks; producing an agent that could recover from incorrect instruc tions or use instruction to guide recovery <ref> [Pearson and Huffman, 1995] </ref>. 4. Presenting K-Incremental learning IMPROV collects sets of training instances prior to learning. This approach provides the learner with access to more information than is normally available to incremental learners, while still exhibiting incremental learning performance.
Reference: [Pearson and Laird, 1996] <author> Douglas J. Pearson and John E. Laird. </author> <title> Toward incremental knowledge correction for agents in complex environments. </title> <editor> In Stephen Muggleton, Donald Michie, and Koichi Furukawa, editors, </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 15. </volume> <publisher> Oxford University Press, </publisher> <year> 1996. </year> <note> In press. </note>
Reference-contexts: This research explores learning procedural planning knowledge through deliberate reasoning about the correctness of an agent's knowledge, as shown in Figure 1.2. The system, IMPROV <ref> [Pearson and Laird, 1996] </ref>, uses an expressive knowledge representation so that it can learn complex actions that produce conditional or sequential effects over time.
Reference: [Pearson et al., 1993a] <author> Douglas J. Pearson, Scott B. Huffman, Mark B. Willis, John E. Laird, and Randolph M. Jones. </author> <title> Intelligent multi-level control in a highly reactive domain. </title> <editor> In F. C. A. Groen, S. Hirose, and C. E. Thorpe, editors, </editor> <booktitle> Proceedings of the Third International Conference on Intelligent Autonomous Systems, Burke, </booktitle> <address> VA, 1993. </address> <publisher> IOS Press. </publisher>
Reference-contexts: The efficiency of this representation has been demonstrated for complex, real-time domains including the control of simulated aircraft <ref> [Pearson et al., 1993a; Pearson et al., 1993b] </ref> and tactical air combat [Laird et al., 1995; Tambe et al., 1995].
Reference: [Pearson et al., 1993b] <author> Douglas J. Pearson, Scott B. Huffman, Mark B. Willis, John E. Laird, and Randolph M. Jones. </author> <title> A symbolic solution to intelligent real-time control. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 11 </volume> <pages> 279-291, </pages> <year> 1993. </year>
Reference-contexts: The efficiency of this representation has been demonstrated for complex, real-time domains including the control of simulated aircraft <ref> [Pearson et al., 1993a; Pearson et al., 1993b] </ref> and tactical air combat [Laird et al., 1995; Tambe et al., 1995].
Reference: [Pearson, 1995] <author> Douglas J. Pearson. </author> <title> Active learning in correcting domain theories: Help or hindrance. </title> <booktitle> In AAAI Symposium on Active Learning, </booktitle> <year> 1995. </year> <note> Available from http://ai.eecs.umich.edu/people/douglasp/homepage.html. </note>
Reference-contexts: This allows a passive learner to more easily recover from any early, incorrect learning. The important point is that in agent-based learning, it is more important to assign credit correctly than it is to assign credit quickly <ref> [Pearson, 1995] </ref>. 9.1.4 Changing the domain knowledge In systems where the operator knowledge is represented declaratively, the learner can directly modify the operator knowledge to make a correction. As IMPROV is restricted to procedural access to its knowledge, it cannot modify the knowledge directly.
Reference: [Quinlan, 1986] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Miller [1993] showed that when ID3's <ref> [Quinlan, 1986] </ref> information gain metric was used to select the feature, prediction accuracy was comparable to that of ID3. However, this metric is only one of a number of methods that can be used to select the feature to focus on during learning.
Reference: [Quinlan, 1990] <author> J. R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: The ranking is based on the number of examples covered, the size of the repair and the number of new, incorrectly classified examples it creates. A non-incremental, greedy algorithm iteratively adds the best repair to the new theory until all examples are covered. FOIL/FOCL FOIL <ref> [Quinlan, 1990] </ref> learns horn clause classification rules, or first-order logic programs, and is therefore an example of inductive logic programming. FOIL has been extended to FOCL [Pazzani et al., 1991; Pazzani and Kibler, 1992], which also includes an explanation-based learning component.
Reference: [Rajamoney and DeJong, 1987] <author> Shankar Rajamoney and Gerald DeJong. </author> <title> The classification, detection and handling of imperfect theory problems. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 205-207, </pages> <year> 1987. </year>
Reference-contexts: Traditional approaches to classifying errors have been based on performance failures, such as failing to achieve a desired effect in the world <ref> [Mitchell et al., 1986; Rajamoney and DeJong, 1987] </ref>. The scope of the learning system is defined in terms of the classes of performance failures that it can recover from. In this section we will propose an alternative method for classifying errors based on the agent's knowledge.
Reference: [Ratcliff, 1990] <author> Roger Ratcliff. </author> <title> Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions. </title> <journal> Psychological Review, </journal> <volume> 97(2) </volume> <pages> 285-308, </pages> <year> 1990. </year>
Reference-contexts: Many reinforcement learners attempt to maximize "reinforcement" without explicit structures to represent goals, leading to problems of task interference, where the trained system loses skill on one task as it learns another <ref> [McCloskey and Cohen, 1989; Ratcliff, 1990] </ref>. E3. Actions may have a complex structure Actions may have duration, taking time to produce changes in the world and possibly producing a series of sequential or transitory effects. <p> Thus, learning time is proportional to the size of the network. As in classifiers, the size of the representation is fixed, ensuring that learning does not slow down over time, but leading to problems with over-training and task interference <ref> [McCloskey and Cohen, 1989; Ratcliff, 1990] </ref>. 3.1.3 Implicit Learning of Extensional Representations Q-Learning Q-learning [Watkins, 1989; Watkins and Dayan, 1992] also implicitly corrects the agent's domain knowledge by improving the accuracy of the predicted reward for taking a particular action.
Reference: [Reece and Tate, 1994] <author> Glen A. Reece and Austin Tate. </author> <title> Synthesizing protection monitors from causal structure. </title> <editor> In Kristian Hammond, editor, </editor> <booktitle> Proceedings of the Second International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> pages 146-151, </pages> <year> 1994. </year>
Reference: [Richards and Mooney, 1991] <author> Bradley L. Richards and Raymond J. Mooney. </author> <title> First-order theory revision. </title> <booktitle> In Proceedings of the Eighth International Conference on Machine Learning, </booktitle> <pages> pages 447-451, </pages> <year> 1991. </year>
Reference-contexts: These systems are only intended to be representative examples of deliberate, declarative learners. There are a great number of other theory revision systems, each embodying different strengths and properties. For example, FORTE <ref> [Richards and Mooney, 1991] </ref> extends EITHER to first order logic as does GENTRE [Asker, 1994]. DUCTOR [Cain, 1991] is similar to EITHER, but including additional abduction components, and is in turn similar to OCCAM [Pazzani, 1988; Pazzani, 1991]. CLIPS-R [Murphy and Pazzani, 1994] even applies theory revision to production rules.
Reference: [Rumelhart et al., 1986] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propogation. </title> <booktitle> In Parallel Distributed Processing, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: This ensures that learning time does not increase as the agent learns more. However, it does mean that classifiers suffer from over-training and task interference as previously valuable rules are discarded to make room for currently useful rules. Backpropogation in Neural Networks Neural networks trained using backpropogation <ref> [Rumelhart et al., 1986] </ref>, or related temporal difference methods [Samuel, 1959; Sutton, 1988; Tesauro, 1992], implicitly correct domain knowledge by adjusting the network to more accurately model the training information or environment. Domain knowledge is represented as a network, intentionally encoding a complex mathematical function.
Reference: [Samuel, 1959] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal on Research and Development, </journal> <volume> 3 </volume> <pages> 210-229, </pages> <year> 1959. </year>
Reference-contexts: However, it does mean that classifiers suffer from over-training and task interference as previously valuable rules are discarded to make room for currently useful rules. Backpropogation in Neural Networks Neural networks trained using backpropogation [Rumelhart et al., 1986], or related temporal difference methods <ref> [Samuel, 1959; Sutton, 1988; Tesauro, 1992] </ref>, implicitly correct domain knowledge by adjusting the network to more accurately model the training information or environment. Domain knowledge is represented as a network, intentionally encoding a complex mathematical function. <p> Therefore, this policy encodes precondition knowledge. These systems typically do not learn a model of the effects of taking an action. If they perform internal simulations, they usually assume a complete and correct model for the effects of taking an action (e.g Samuel's checker player <ref> [Samuel, 1959] </ref>) and limit learning to determining the correct conditions for when to take that action.
Reference: [Sanborn and Hendler, 1988] <author> J. Sanborn and J. Hendler. </author> <title> A model of reaction for planning in dynamic environments. </title> <journal> Artificial Intelligence in Engineering, </journal> <volume> 3(2) </volume> <pages> 95-102, </pages> <year> 1988. </year> <month> 157 </month>
Reference: [Shen and Simon, 1989] <author> Wei-Min Shen and Herbert A. Simon. </author> <title> Rule creation and rule learning through environmental exploration. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 675-680, </pages> <address> Detroit, Michigan, </address> <year> 1989. </year>
Reference-contexts: They are not generally applicable to the more challenging environments of Chapter 2. 22 EXPO, LIVE and OBSERVER EXPO [Gil, 1991; Gil, 1993], LIVE <ref> [Shen and Simon, 1989] </ref> and OBSERVER [Wang, 1995; Wang, 1996] share a similar STRIPS-like [Fikes and Nilsson, 1971] representation. Operators are declaratively represented as structures with lists of preconditions and effects. The preconditions are represented in disjunctive normal form. <p> The standard approach to making this comparison is by explicitly comparing a declarative model of the operator to the states before and/or after the operator is applied. This is the approach used in EXPO [Gil, 1992; Gil, 1994], LIVE <ref> [Shen and Simon, 1989; Shen, 1994] </ref>, OBSERVER [Wang, 1995; Wang, 1996] and TRAIL [Benson, 1995] (although TRAIL records a sequence of states to allow for actions with duration).
Reference: [Shen, 1989] <author> W. Shen. </author> <title> Learning from the Environment Based on Percepts and Actions. </title> <type> PhD thesis, </type> <institution> Cagnegie-Mellon University, </institution> <year> 1989. </year>
Reference-contexts: Once the cause of the failure has been determined, EXPO changes the operator preconditions accordingly. LIVE <ref> [Shen, 1989; Shen, 1994] </ref> and the more recent OBSERVER [Wang, 1995; Wang, 1996], learn domain knowledge by executing actions in the environment and observing the results. LIVE and OBSERVER rely on the assumption that changes in the environment are due to deterministic actions of the agent. <p> In the remainder of this section we will examine some of the existing approaches to learning operator effects and discuss the range of operator effects they can model. 11.2.1 LIVE, EXPO and OBSERVER LIVE <ref> [Shen, 1989; Shen, 1994] </ref>, EXPO [Gil, 1992; Gil, 1994] and OBSERVER [Wang, 1995; Wang, 1996] are three, related approaches to learning planning knowledge to model the effect of actions.
Reference: [Shen, 1994] <author> Wei-Min Shen. </author> <title> Autonomous Learning from the Environment. </title> <publisher> Computer Science Press, W. H. Freeman and Company, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Once the cause of the failure has been determined, EXPO changes the operator preconditions accordingly. LIVE <ref> [Shen, 1989; Shen, 1994] </ref> and the more recent OBSERVER [Wang, 1995; Wang, 1996], learn domain knowledge by executing actions in the environment and observing the results. LIVE and OBSERVER rely on the assumption that changes in the environment are due to deterministic actions of the agent. <p> The standard approach to making this comparison is by explicitly comparing a declarative model of the operator to the states before and/or after the operator is applied. This is the approach used in EXPO [Gil, 1992; Gil, 1994], LIVE <ref> [Shen and Simon, 1989; Shen, 1994] </ref>, OBSERVER [Wang, 1995; Wang, 1996] and TRAIL [Benson, 1995] (although TRAIL records a sequence of states to allow for actions with duration). <p> In the remainder of this section we will examine some of the existing approaches to learning operator effects and discuss the range of operator effects they can model. 11.2.1 LIVE, EXPO and OBSERVER LIVE <ref> [Shen, 1989; Shen, 1994] </ref>, EXPO [Gil, 1992; Gil, 1994] and OBSERVER [Wang, 1995; Wang, 1996] are three, related approaches to learning planning knowledge to model the effect of actions.
Reference: [Sutton, 1988] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: However, it does mean that classifiers suffer from over-training and task interference as previously valuable rules are discarded to make room for currently useful rules. Backpropogation in Neural Networks Neural networks trained using backpropogation [Rumelhart et al., 1986], or related temporal difference methods <ref> [Samuel, 1959; Sutton, 1988; Tesauro, 1992] </ref>, implicitly correct domain knowledge by adjusting the network to more accurately model the training information or environment. Domain knowledge is represented as a network, intentionally encoding a complex mathematical function. <p> This method can be generalized to situations where feedback (or reward) is delayed. The predicted reward for a state should match the reward when that state is reached, allowing the feedback from the final state to be passed back to earlier states <ref> [Sutton, 1988; Watkins and Dayan, 1992] </ref>. To summarize, detecting planning failures requires the agent to have knowledge about the task, separate from knowledge about how to do the task. This knowledge is either that creating a plan should be possible or that specific states are impossible.
Reference: [Sutton, 1990] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <year> 1990. </year>
Reference-contexts: Together the Q-values for a particular state effectively represent the preconditions for the different actions that the agent could take. An example of a system that uses Q-learning for agent-based learning is Dyna-Q <ref> [Sutton, 1990] </ref>. Neural networks, Classifiers and Q-learning make few assumptions about their environment and do not model the effects of the agent's actions in the world. This makes them applicable to domains with limited sensing, exogenous events and processes that change over time (E8, E9, E10).
Reference: [Tambe et al., 1995] <author> M. Tambe, W. L. Johnson, R. M. Jones, F. Koss, J. E. Laird, P. E. Rosen-bloom, and K. B. Schwamb. </author> <title> Intelligent agents for interactive simulation environments. </title> <journal> AI Magazine, </journal> <year> 1995. </year>
Reference-contexts: The efficiency of this representation has been demonstrated for complex, real-time domains including the control of simulated aircraft [Pearson et al., 1993a; Pearson et al., 1993b] and tactical air combat <ref> [Laird et al., 1995; Tambe et al., 1995] </ref>. The match algorithm has also been tested on cases of matching over 100,000 rules [Doorenbos, 1993] and the counter-intuitive result has been shown that, for some systems, there is no increase in match cost as the number of rules increases. 5.
Reference: [Tesauro, 1992] <author> Gerald Tesauro. </author> <title> Temporal difference learning of backgammon strategy. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 451-457, </pages> <year> 1992. </year>
Reference-contexts: However, it does mean that classifiers suffer from over-training and task interference as previously valuable rules are discarded to make room for currently useful rules. Backpropogation in Neural Networks Neural networks trained using backpropogation [Rumelhart et al., 1986], or related temporal difference methods <ref> [Samuel, 1959; Sutton, 1988; Tesauro, 1992] </ref>, implicitly correct domain knowledge by adjusting the network to more accurately model the training information or environment. Domain knowledge is represented as a network, intentionally encoding a complex mathematical function.
Reference: [VanLehn, 1987] <author> Kurt VanLehn. </author> <title> Learning one subprocedure per lesson. </title> <journal> Artificial Intelligence, </journal> <volume> 31(1) </volume> <pages> 1-40, </pages> <year> 1987. </year>
Reference-contexts: To detect this type of failure, the agent needs additional knowledge to signal that it should be possible to build a plan for the current goal. For example, a number of systems attempt to construct explanatory parses of training examples (e.g. <ref> [VanLehn, 1987; Hall, 1986; Hall, 1988] </ref>). The inability to build an explanation is equivalent to an incomplete plan failure. Rajamoney and DeJong [1988] also examined cases where the agent knows that only one plan should be formed, but multiple plans are found.
Reference: [Wang, 1995] <author> Xuemei Wang. </author> <title> Learning by observation and practice: An incremental approach for planning operator acquisition. </title> <booktitle> In Proceedings of the Twelth International Conference on Machine Learning, </booktitle> <pages> pages 549-557, </pages> <year> 1995. </year>
Reference-contexts: They are not generally applicable to the more challenging environments of Chapter 2. 22 EXPO, LIVE and OBSERVER EXPO [Gil, 1991; Gil, 1993], LIVE [Shen and Simon, 1989] and OBSERVER <ref> [Wang, 1995; Wang, 1996] </ref> share a similar STRIPS-like [Fikes and Nilsson, 1971] representation. Operators are declaratively represented as structures with lists of preconditions and effects. The preconditions are represented in disjunctive normal form. The effects are limited to a single, state-to-state transition, although they can include conditional effects. <p> Once the cause of the failure has been determined, EXPO changes the operator preconditions accordingly. LIVE [Shen, 1989; Shen, 1994] and the more recent OBSERVER <ref> [Wang, 1995; Wang, 1996] </ref>, learn domain knowledge by executing actions in the environment and observing the results. LIVE and OBSERVER rely on the assumption that changes in the environment are due to deterministic actions of the agent. <p> The standard approach to making this comparison is by explicitly comparing a declarative model of the operator to the states before and/or after the operator is applied. This is the approach used in EXPO [Gil, 1992; Gil, 1994], LIVE [Shen and Simon, 1989; Shen, 1994], OBSERVER <ref> [Wang, 1995; Wang, 1996] </ref> and TRAIL [Benson, 1995] (although TRAIL records a sequence of states to allow for actions with duration). <p> In the remainder of this section we will examine some of the existing approaches to learning operator effects and discuss the range of operator effects they can model. 11.2.1 LIVE, EXPO and OBSERVER LIVE [Shen, 1989; Shen, 1994], EXPO [Gil, 1992; Gil, 1994] and OBSERVER <ref> [Wang, 1995; Wang, 1996] </ref> are three, related approaches to learning planning knowledge to model the effect of actions.
Reference: [Wang, 1996] <author> Xuemei Wang. </author> <title> Learning Planning Operators by Observation and Practice. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1996. </year>
Reference-contexts: They are not generally applicable to the more challenging environments of Chapter 2. 22 EXPO, LIVE and OBSERVER EXPO [Gil, 1991; Gil, 1993], LIVE [Shen and Simon, 1989] and OBSERVER <ref> [Wang, 1995; Wang, 1996] </ref> share a similar STRIPS-like [Fikes and Nilsson, 1971] representation. Operators are declaratively represented as structures with lists of preconditions and effects. The preconditions are represented in disjunctive normal form. The effects are limited to a single, state-to-state transition, although they can include conditional effects. <p> Once the cause of the failure has been determined, EXPO changes the operator preconditions accordingly. LIVE [Shen, 1989; Shen, 1994] and the more recent OBSERVER <ref> [Wang, 1995; Wang, 1996] </ref>, learn domain knowledge by executing actions in the environment and observing the results. LIVE and OBSERVER rely on the assumption that changes in the environment are due to deterministic actions of the agent. <p> The standard approach to making this comparison is by explicitly comparing a declarative model of the operator to the states before and/or after the operator is applied. This is the approach used in EXPO [Gil, 1992; Gil, 1994], LIVE [Shen and Simon, 1989; Shen, 1994], OBSERVER <ref> [Wang, 1995; Wang, 1996] </ref> and TRAIL [Benson, 1995] (although TRAIL records a sequence of states to allow for actions with duration). <p> In the remainder of this section we will examine some of the existing approaches to learning operator effects and discuss the range of operator effects they can model. 11.2.1 LIVE, EXPO and OBSERVER LIVE [Shen, 1989; Shen, 1994], EXPO [Gil, 1992; Gil, 1994] and OBSERVER <ref> [Wang, 1995; Wang, 1996] </ref> are three, related approaches to learning planning knowledge to model the effect of actions.
Reference: [Watkins and Dayan, 1992] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Technical note: </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: Figure 1.1 (a) shows the main components of agents in the first class, reinforcement learners (e.g. Q-Learning <ref> [Watkins and Dayan, 1992] </ref>, Classifiers [Holland, 1986], Backpropogation [Rumel-hart et al., 1986]). These systems use weak inductive learning methods to directly modify an agent's execution knowledge. The execution knowledge is generally represented procedurally (e.g. in a neural net). <p> As in classifiers, the size of the representation is fixed, ensuring that learning does not slow down over time, but leading to problems with over-training and task interference [McCloskey and Cohen, 1989; Ratcliff, 1990]. 3.1.3 Implicit Learning of Extensional Representations Q-Learning Q-learning <ref> [Watkins, 1989; Watkins and Dayan, 1992] </ref> also implicitly corrects the agent's domain knowledge by improving the accuracy of the predicted reward for taking a particular action. Knowledge is represented by a set of Q-values, one for each state and action pair. <p> This method can be generalized to situations where feedback (or reward) is delayed. The predicted reward for a state should match the reward when that state is reached, allowing the feedback from the final state to be passed back to earlier states <ref> [Sutton, 1988; Watkins and Dayan, 1992] </ref>. To summarize, detecting planning failures requires the agent to have knowledge about the task, separate from knowledge about how to do the task. This knowledge is either that creating a plan should be possible or that specific states are impossible. <p> This means the methods are either not used on planning tasks or require a complete and correct model for the effects of each action. An interesting exception is the work of Mahadevan on learning action models for a robotic domain [Mahadevan, 1992; Mahadevan and Connell, 1991]. Q-learning <ref> [Watkins, 1989; Watkins and Dayan, 1992] </ref> is used to learn the expected reward of taking each possible action in any given state. As we discussed in Section 4.1 this large representation of states and actions is only appropriate for relatively small state spaces.
Reference: [Watkins, 1989] <author> Christopher J. C. H. Watkins. </author> <title> Learning from delayed rewards. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <address> England, </address> <year> 1989. </year>
Reference-contexts: As in classifiers, the size of the representation is fixed, ensuring that learning does not slow down over time, but leading to problems with over-training and task interference [McCloskey and Cohen, 1989; Ratcliff, 1990]. 3.1.3 Implicit Learning of Extensional Representations Q-Learning Q-learning <ref> [Watkins, 1989; Watkins and Dayan, 1992] </ref> also implicitly corrects the agent's domain knowledge by improving the accuracy of the predicted reward for taking a particular action. Knowledge is represented by a set of Q-values, one for each state and action pair. <p> This means the methods are either not used on planning tasks or require a complete and correct model for the effects of each action. An interesting exception is the work of Mahadevan on learning action models for a robotic domain [Mahadevan, 1992; Mahadevan and Connell, 1991]. Q-learning <ref> [Watkins, 1989; Watkins and Dayan, 1992] </ref> is used to learn the expected reward of taking each possible action in any given state. As we discussed in Section 4.1 this large representation of states and actions is only appropriate for relatively small state spaces.
Reference: [Wray et al., 1996] <author> Robert Wray, John Laird, and Randolph Jones. </author> <title> Compilation of non-contemporaneous constraints. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 771-778, </pages> <address> Portland, OR, </address> <month> August </month> <year> 1996. </year> <month> 158 </month>
Reference-contexts: This is an example of the utility problem [Minton, 1990]. The extra rules may not be used sufficiently often to produce an overall performance improvement. Considerable research has been directed at examining this problem (e.g. <ref> [Gratch and DeJong, 1992; Wray et al., 1996; Minton, 1996] </ref>). In this research project we have not directly examined this trade-off. However, on certain problems, the Soar matcher has been shown to maintain a constant speed as up to 100,000 new rules are learned [Doorenbos, 1993].
References-found: 84

