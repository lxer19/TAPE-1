URL: http://www.cs.ucsd.edu/~calder/pfdc/papers/pfdc-hsu.ps.Z
Refering-URL: http://www.cs.ucsd.edu/~calder/pfdc/program.html
Root-URL: http://www.cs.ucsd.edu
Title: IPERF A Framework for Automatic Construction of Performance Prediction Models  
Author: Chung-Hsing Hsu and Ulrich Kremer 
Affiliation: Department of Computer Science Rutgers University  
Abstract: Performance prediction models at the source code level are crucial in optimizing compilers, programming environments, and performance debugging tools. For each performance prediction task, minimal-cost models are needed that deliver the required accuracy. Current techniques to derive the desired models are ad hoc. This paper discusses a new framework for automatic construction of cost-effective performance prediction models for different target systems at the program source level. A target system consists of a target optimizing compiler, a target operating system, and a target architecture with a multi-level memory hierarchy. Preliminary results for a small computation kernel on a set of twelve target systems indicate the effectiveness of the proposed framework. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> National Compiler Infrastructure (NCI) project. </institution> <note> Overview available online at http://www-suif.stanford.edu/suif/nci/index.html., Co-funded by NSF/DARPA, </note> <year> 1998. </year>
Reference-contexts: Hsu and Kremer 9 experimentation for larger program kernels and whole programs, we are planning to implement a fully automatic IPERF prototype system based on the SUIF compiler infrastructure that is currently being extended as part of the NSF/DARPA National Compiler Infrastructure Project <ref> [1] </ref>. SUIF contains front-ends for different languages such as Fortran, C, C++, and Java. The set of target machines for the proposed system will span across several current and future architectures. We are currently investigating different model lattice construction and search strategies for compositional and numerical models.
Reference: [2] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 112-125, </pages> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Examples include finding the best loop order of a loop nest to take advantage of the target machines memory hierarchy [8, 7, 20, 15], or determining an efficient computation and data mapping onto a multiprocessor system <ref> [10, 2, 5, 9, 13] </ref>. Nearly all published models are given and are different in terms of their cost and accuracy. It is hard to compare their efficiency and effectiveness across different optimizations. The IPERF framework provides a platform to evaluate them automatically.
Reference: [3] <author> E. A. Brewer. </author> <title> High-level optimization via automated statistical modeling. </title> <editor> In Jeanne Ferrante and David Padua, editors, </editor> <booktitle> 5th ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <pages> pages 80-91, </pages> <address> New York, NY, USA, </address> <month> August </month> <year> 1995. </year> <note> ACM Press. </note>
Reference-contexts: Brewer's auto-calibration toolkit constructs from a list of user provided models an additive model through linear regression over performance profiles. The cost of the model is reduced by repeatedly eliminating statistically insignificant terms in the additive model <ref> [4, 3] </ref>. The toolkit has no explicit cost metric and relies on the user to provide the models. Crovella and LeBlanc developed the lost cycles toolkit that constructs models for a fixed set of performance factors individually through linear regression, and then sum them up [14].
Reference: [4] <author> Eric Allen Brewer. </author> <title> Portable High-Performancei Supercomputing: High-Level Platform-Dependent Optimizations. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: Brewer's auto-calibration toolkit constructs from a list of user provided models an additive model through linear regression over performance profiles. The cost of the model is reduced by repeatedly eliminating statistically insignificant terms in the additive model <ref> [4, 3] </ref>. The toolkit has no explicit cost metric and relies on the user to provide the models. Crovella and LeBlanc developed the lost cycles toolkit that constructs models for a fixed set of performance factors individually through linear regression, and then sum them up [14].
Reference: [5] <author> S. Chatterjee, J.R. Gilbert, R. Schreiber, and S-H. Teng. </author> <title> Optimal evaluation of array expressions on massively parallel machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17(1) </volume> <pages> 123-156, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Examples include finding the best loop order of a loop nest to take advantage of the target machines memory hierarchy [8, 7, 20, 15], or determining an efficient computation and data mapping onto a multiprocessor system <ref> [10, 2, 5, 9, 13] </ref>. Nearly all published models are given and are different in terms of their cost and accuracy. It is hard to compare their efficiency and effectiveness across different optimizations. The IPERF framework provides a platform to evaluate them automatically.
Reference: [6] <author> Joel Emer and Nickolas Gloy. </author> <title> A language for describing predictors and its application to automatic synthesis. </title> <booktitle> In 24th International Symposium on Computer Architecture, </booktitle> <pages> pages 304-314, </pages> <year> 1997. </year>
Reference-contexts: More recently, Emer and Gloy have investigated generic algorithms to generate new models for the purpose of branch prediction <ref> [6] </ref>. Instead of working with a predefined model space, their method may generate new performance factors. The paper is organized as follows. Section 2 presents the overall structure of the proposed framework.
Reference: [7] <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On estimating and enhancing cache effectiveness. </title> <booktitle> In 1991 Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 328-343, </pages> <year> 1991. </year>
Reference-contexts: Performance prediction in the context of a specific optimization has been extensively discussed in the literature. Examples include finding the best loop order of a loop nest to take advantage of the target machines memory hierarchy <ref> [8, 7, 20, 15] </ref>, or determining an efficient computation and data mapping onto a multiprocessor system [10, 2, 5, 9, 13]. Nearly all published models are given and are different in terms of their cost and accuracy. It is hard to compare their efficiency and effectiveness across different optimizations. <p> Hsu and Kremer 7 Models definition remarks T ALU c 0 + c 1 fl n + c 2 fl n 2 n : loop bound, assumes unit cost per iteration T TLB 1 n 2 n n 2 P otherwise E : #TLB entries, P : page size <ref> [11, 7] </ref> L2 : model for L 2 cache T L b &gt; &gt; &lt; n 2 D a D a + 1 ss [n + (ss 1) fl (a + 1)fl otherwise (n a fl D)] D = minfd &gt; 0 j ffi nfld l mod (S) 0g blocking cache
Reference: [8] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 587-616, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Performance prediction in the context of a specific optimization has been extensively discussed in the literature. Examples include finding the best loop order of a loop nest to take advantage of the target machines memory hierarchy <ref> [8, 7, 20, 15] </ref>, or determining an efficient computation and data mapping onto a multiprocessor system [10, 2, 5, 9, 13]. Nearly all published models are given and are different in terms of their cost and accuracy. It is hard to compare their efficiency and effectiveness across different optimizations.
Reference: [9] <author> J. Garcia, E. Ayguade, and J. Labarta. </author> <title> Dynamic data distribution with control flow analysis. </title> <booktitle> In Proceedings of Supercomputing '96, </booktitle> <address> Pitts-burgh, PA, </address> <month> November </month> <year> 1996. </year> , <note> available at URL http://www.supercomp.org/sc96/proceedings. </note>
Reference-contexts: Examples include finding the best loop order of a loop nest to take advantage of the target machines memory hierarchy [8, 7, 20, 15], or determining an efficient computation and data mapping onto a multiprocessor system <ref> [10, 2, 5, 9, 13] </ref>. Nearly all published models are given and are different in terms of their cost and accuracy. It is hard to compare their efficiency and effectiveness across different optimizations. The IPERF framework provides a platform to evaluate them automatically.
Reference: [10] <author> M. Gupta and P. Banerjee. </author> <title> Compile-time estimation of communication costs on multicomputers. </title> <booktitle> In Proceedings of the 6th International Parallel Processing Symposium, </booktitle> <address> Beverly Hills, CA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: Examples include finding the best loop order of a loop nest to take advantage of the target machines memory hierarchy [8, 7, 20, 15], or determining an efficient computation and data mapping onto a multiprocessor system <ref> [10, 2, 5, 9, 13] </ref>. Nearly all published models are given and are different in terms of their cost and accuracy. It is hard to compare their efficiency and effectiveness across different optimizations. The IPERF framework provides a platform to evaluate them automatically.
Reference: [11] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, California, </address> <note> second edition, </note> <year> 1996. </year>
Reference-contexts: Hsu and Kremer 7 Models definition remarks T ALU c 0 + c 1 fl n + c 2 fl n 2 n : loop bound, assumes unit cost per iteration T TLB 1 n 2 n n 2 P otherwise E : #TLB entries, P : page size <ref> [11, 7] </ref> L2 : model for L 2 cache T L b &gt; &gt; &lt; n 2 D a D a + 1 ss [n + (ss 1) fl (a + 1)fl otherwise (n a fl D)] D = minfd &gt; 0 j ffi nfld l mod (S) 0g blocking cache
Reference: [12] <author> C-H. Hsu and U. Kremer. </author> <title> A framework for qualitative performance prediction. </title> <type> Technical Report LCSR-TR98-363, </type> <institution> Department of Computer Science, Rutgers University, </institution> <month> July </month> <year> 1998. </year>
Reference-contexts: In all cases, the operating system was Solaris 2.5.1. The resulting measured execution times are shown in Figure 6. behavior for most of the target systems. The exact description of the method used to obtain the measured performance numbers can be found in <ref> [12] </ref>. The IPERF prototype does not consider the characteristics of the target compiler in any of its performance models. However, the target machines characteristics as listed in Figure 7 are considered. <p> The constants c i represent the coefficients in a linear combination of the different components of the models. The actual values of the coefficients are determined during the fitting process. A detailed discussion of these models can be found in <ref> [12] </ref>. Model Search and Evaluation The IPERF prototype system does not contain any automatic, "intelligent" strategy to search the performance model lattice. In the experiments described below, a reasonable subset of performance factors and models was selected by hand.
Reference: [13] <author> K. Kennedy and U. Kremer. </author> <title> Automatic data layout for distributed memory machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <note> 1998. To appear. </note>
Reference-contexts: Examples include finding the best loop order of a loop nest to take advantage of the target machines memory hierarchy [8, 7, 20, 15], or determining an efficient computation and data mapping onto a multiprocessor system <ref> [10, 2, 5, 9, 13] </ref>. Nearly all published models are given and are different in terms of their cost and accuracy. It is hard to compare their efficiency and effectiveness across different optimizations. The IPERF framework provides a platform to evaluate them automatically. <p> However, Saavedra-Barrera's framework does consider the optimization passes implicitly, by reducing the cost of each term. The IPERF framework proposes explicit models for compilation passes. The effectiveness of explicit models for compilation passes has been illustrated by our previous work on performance prediction for automatic data layout <ref> [13] </ref>, and by other researchers, such as Ko-Yang Wang in the context of instruction scheduling for superscalar architectures [19]. More recently, Emer and Gloy have investigated generic algorithms to generate new models for the purpose of branch prediction [6].
Reference: [14] <author> Wagner Meira, Jr. </author> <title> Understanding Parallel Program Performance Using Cause-Effect Analysis. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Rochester, </institution> <month> August </month> <year> 1997. </year> <month> TR-663. </month>
Reference-contexts: The toolkit has no explicit cost metric and relies on the user to provide the models. Crovella and LeBlanc developed the lost cycles toolkit that constructs models for a fixed set of performance factors individually through linear regression, and then sum them up <ref> [14] </ref>. The effectiveness of the toolkit depends on the independence and completeness of these performance factors. Note that Brewer's toolkit and Crovella and LeBlanc's toolkit include the automatic generation to profiling codes, while IPERF relies on the user to provide the performance profile of a good quality.
Reference: [15] <author> K. S. M c Kinley, S. Carr, and C.-W. Tseng. </author> <title> Improving data locality with loop transformations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 18(4) </volume> <pages> 424-453, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: Performance prediction in the context of a specific optimization has been extensively discussed in the literature. Examples include finding the best loop order of a loop nest to take advantage of the target machines memory hierarchy <ref> [8, 7, 20, 15] </ref>, or determining an efficient computation and data mapping onto a multiprocessor system [10, 2, 5, 9, 13]. Nearly all published models are given and are different in terms of their cost and accuracy. It is hard to compare their efficiency and effectiveness across different optimizations.
Reference: [16] <author> William H. Press, Saul A. Teukolsky, William T. Vet-terling, and Brian P. Flannery. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, 2 edition, </address> <year> 1992. </year>
Reference-contexts: In the IPERF prototype system, two fitting methods are available, each appropriate for a different error criteria, namely ordinary least-square fitting (OLS) and weighted least-square fitting (WLS) <ref> [16] </ref>. OLS is better suited for expressing constraints on absolute prediction errors, while WLS deals better with the relative prediction errors constraints.
Reference: [17] <author> Rafael H. Saavedra-Barrera. </author> <title> CPU Performance Evaluation and Execution Time Prediction Using Narrow Spectrum Benchmarking. </title> <type> PhD thesis, </type> <institution> U.C. Berkeley, </institution> <month> February </month> <year> 1992. </year> <month> UCB/CSD-92-684. </month>
Reference-contexts: Saavedra-Barrera developed a micro-benchmarking framework that constructs the additive model from a fixed set of more than hundred terms, each of which is derived by linear regression of an appropriate set of micro-benchmarks <ref> [17] </ref>. The portability of this framework is limited. However, Saavedra-Barrera's framework does consider the optimization passes implicitly, by reducing the cost of each term. The IPERF framework proposes explicit models for compilation passes.
Reference: [18] <author> O. Temam, C. Fricker, and W. Jalby. </author> <title> Cache interference phenomena. </title> <booktitle> In Proceedings of the Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 261-271, </pages> <address> New York, NY, USA, May 1994. </address> <publisher> ACM Press. </publisher>
Reference-contexts: a D a + 1 ss [n + (ss 1) fl (a + 1)fl otherwise (n a fl D)] D = minfd &gt; 0 j ffi nfld l mod (S) 0g blocking cache model d : reuse distance in iterations a : associativity, S : #sets, ss : subblock size <ref> [18, 21] </ref> T L nb n fl n D L1 &gt; a L1 and n 0 otherwise D Li : parameter D for L i nonblocking cache model a Li : associativity of L i Summary The IPERF prototype system was able to identify different performance models for different accuracy/cost trade-offs.
Reference: [19] <author> K-Y. Wang. </author> <title> Precise compile-time performance prediction for superscalar-based computers. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <address> Or-lando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: The IPERF framework proposes explicit models for compilation passes. The effectiveness of explicit models for compilation passes has been illustrated by our previous work on performance prediction for automatic data layout [13], and by other researchers, such as Ko-Yang Wang in the context of instruction scheduling for superscalar architectures <ref> [19] </ref>. More recently, Emer and Gloy have investigated generic algorithms to generate new models for the purpose of branch prediction [6]. Instead of working with a predefined model space, their method may generate new performance factors. The paper is organized as follows.
Reference: [20] <author> M. E. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Performance prediction in the context of a specific optimization has been extensively discussed in the literature. Examples include finding the best loop order of a loop nest to take advantage of the target machines memory hierarchy <ref> [8, 7, 20, 15] </ref>, or determining an efficient computation and data mapping onto a multiprocessor system [10, 2, 5, 9, 13]. Nearly all published models are given and are different in terms of their cost and accuracy. It is hard to compare their efficiency and effectiveness across different optimizations.
Reference: [21] <author> Michael J. Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley Co., </publisher> <year> 1996. </year> <note> Hsu and Kremer 10 </note>
Reference-contexts: a D a + 1 ss [n + (ss 1) fl (a + 1)fl otherwise (n a fl D)] D = minfd &gt; 0 j ffi nfld l mod (S) 0g blocking cache model d : reuse distance in iterations a : associativity, S : #sets, ss : subblock size <ref> [18, 21] </ref> T L nb n fl n D L1 &gt; a L1 and n 0 otherwise D Li : parameter D for L i nonblocking cache model a Li : associativity of L i Summary The IPERF prototype system was able to identify different performance models for different accuracy/cost trade-offs.
References-found: 21

