URL: http://www.cs.toronto.edu/~greiner/PAPERS/distrib.ps
Refering-URL: http://www.cs.toronto.edu/~greiner/PAPERS/
Root-URL: 
Title: A Distributed Plan Verifier  
Author: Yan Xiao Russell Greiner 
Abstract: This paper describes the architecture of an efficient plan verifier that can first detect faults in a planner's plans and use these observed errors to identify possible problems in the planner's knowledge base and suggest appropriate corrections.
Abstract-found: 1
Intro-found: 1
Reference: [All83] <author> James Allen. </author> <title> Recognizing intentions from natural language utterances. </title> <editor> In M. Brady and R.C. Berwick, editors, </editor> <booktitle> Computational Models of Discourse, </booktitle> <pages> page 114. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Mass, </address> <year> 1983. </year>
Reference-contexts: We will see that this allows the critics to have narrower range of knowledge, yet still be useful, provided it has superior knowledge in its own sub-domain. Once incorporated with plan recognition system <ref> [All83, KA86, SSG78] </ref>, the PV system can also be developed into a practical and comprehensive warning system (which was the original objective of this research), capable of detecting possible problems of some ascribed plan and of giving both warnings and helpful information. 3 Simple (Blocks World) Example We start with the
Reference: [GN87] <author> Michael R. Genesereth and Nils J. Nilsson. </author> <booktitle> Logical Foundations of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, CA, </address> <year> 1987. </year>
Reference-contexts: can also be developed into a practical and comprehensive warning system (which was the original objective of this research), capable of detecting possible problems of some ascribed plan and of giving both warnings and helpful information. 3 Simple (Blocks World) Example We start with the classic AI approach to planning <ref> [GN87] </ref>: Given a goal , a planning agent P generates a plan (i.e., a sequence of actions) that it thinks will achieve the goal . (That is, we insist that P be "rational" [New81, p8].) As an example, consider the simple block world situation shown in Figure 1, and imagine the
Reference: [HR85] <author> Barbara Hayes-Roth. </author> <title> A blackboard architecture for control. </title> <journal> Artificial Intelligence: An International Journal, </journal> <volume> 26(3) </volume> <pages> 251-321, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Section 6 discusses the benefits of this "distributed plan verifier" approach. 2 Related Work Our approach shares many similarities with several other systems. First, many systems exploit the synergy of multiple agents working together to achieve a single goal. These agents are called "knowledge sources" (KSs) in BlackBoard systems <ref> [HR85] </ref>. Our use of these agents is slightly different, as (1) each KS is assumed to be an expert in its own domain; by contrast, we assume that P and PV share a great deal of information, but PV knows more of that domain, more accurately.
Reference: [KA86] <author> H.A. Kautz and J.F. Allen. </author> <title> Generalized plan recognition. </title> <booktitle> In AAAI-86, </booktitle> <pages> pages 32-37, </pages> <year> 1986. </year>
Reference-contexts: We will see that this allows the critics to have narrower range of knowledge, yet still be useful, provided it has superior knowledge in its own sub-domain. Once incorporated with plan recognition system <ref> [All83, KA86, SSG78] </ref>, the PV system can also be developed into a practical and comprehensive warning system (which was the original objective of this research), capable of detecting possible problems of some ascribed plan and of giving both warnings and helpful information. 3 Simple (Blocks World) Example We start with the
Reference: [LS83] <author> C.P. Langlotz and E.H. Shortliffe. </author> <title> Adapting a consultation system to critique user plans. </title> <journal> Int. J. Man-Machine Studies, </journal> <volume> 19 </volume> <pages> 479-496, </pages> <year> 1983. </year>
Reference-contexts: We use plan verification as a way to critique plans. In many systems, the critic must itself be able to plan <ref> [LS83, Mil83] </ref>. This is not required in our work; instead, our critics simply check the plan by analyzing parts of the planner's proof and knowledge base that it used to generate the plan.
Reference: [Mal90] <author> Raphael Malyankar. </author> <title> Coorperative Route Planning by Multiple Agents. </title> <type> Master's Thesis, </type> <institution> University of New Hampshire, </institution> <year> 1990. </year>
Reference-contexts: Another difference is that LEAP's underlying task is to convert a sub-optimal plan into better one; by contrast, our P-and-PV-system is trying to obtain any acceptable plan. Our work is also related to cooperative planning. As one example, Malyankar <ref> [Mal90] </ref> considers how multiple agents, having different views of the common world, can cooperatively plan a route. In that work, each agent predicts potential conflicts between its planned route with the others', and uses these predictions to determine what the other agents need to know.
Reference: [Mil83] <author> P.L. Miller. Attending: </author> <title> Critiquing a physician's management. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Inteligence, </journal> <volume> PAMI-5:449-461, </volume> <year> 1983. </year>
Reference-contexts: We use plan verification as a way to critique plans. In many systems, the critic must itself be able to plan <ref> [LS83, Mil83] </ref>. This is not required in our work; instead, our critics simply check the plan by analyzing parts of the planner's proof and knowledge base that it used to generate the plan.
Reference: [MMS85] <author> Thomas M. Mitchell, Sridhar Mahadevan, and Louis I. Steinberg. </author> <title> LEAP: A learning apprentice for VLSI design. </title> <booktitle> In IJCAI-85, </booktitle> <pages> pages 573-80, </pages> <address> Los Angeles, </address> <month> August </month> <year> 1985. </year>
Reference-contexts: This leads to a second difference: (2) while a blackboard system's only objective is to work on the task at hand, one of PV's goals is to "enlighten" P. This ties our Plan Verification approach in with several learning systems. One example of this genre is the LEAP system <ref> [MMS85] </ref>. Here again one part of a system verifies the plan of other, and uses this information to enlighten the "assistant". It differs in that LEAP tries to educate itself, while PV tries to educate P.
Reference: [New81] <author> Alan Newell. </author> <title> The knowledge level. </title> <journal> AI Magazine, </journal> <volume> 2(2) </volume> <pages> 1-20, </pages> <month> Summer </month> <year> 1981. </year>
Reference-contexts: and helpful information. 3 Simple (Blocks World) Example We start with the classic AI approach to planning [GN87]: Given a goal , a planning agent P generates a plan (i.e., a sequence of actions) that it thinks will achieve the goal . (That is, we insist that P be "rational" <ref> [New81, p8] </ref>.) As an example, consider the simple block world situation shown in Figure 1, and imagine the goal of our planner, P, is to build a tower with block a on top of block b. | i.e., a b hand (Real) Initial World b c d Goal State a b
Reference: [Sac77] <author> E.D. Sacerdoti. </author> <title> A Structure for Plans and Behaviour. </title> <publisher> Elsevier North-Holland, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: In particular, this overall system may be unable to detect "global problems" with a plan, as each verifier only sees that the plan is "locally correct". (One such plan involves painting the ladder before painting the ceiling c.f., <ref> [Sac77] </ref>.) This work does not deal with such interactions. 6 Conclusions In general, a planner must (correctly) know every fact relevant to any goal. This places quite a burden on both the planner itself, and to its author.
Reference: [Sim69] <author> Herbert Simon. </author> <booktitle> Science Of the Artificial. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1969. </year>
Reference-contexts: Of course, the overall system may require several iterations | after some PV i vetos P's plan, P will produce a correct plan. Surely this makes many assumptions about the nature of the domain. In particular, this assumes that the different parts are (at worst) semi-decomposable <ref> [Sim69] </ref>. More research is required to determine, empirically or analytically, which domain qualify.
Reference: [SSG78] <author> C.F. Schmidt, N.S. Sridharan, and J.L. Goodson. </author> <title> The plan recoginition problem. </title> <journal> Artificial Intelligence: An International Journal, </journal> <volume> 11 </volume> <pages> 45-83, </pages> <year> 1978. </year>
Reference-contexts: We will see that this allows the critics to have narrower range of knowledge, yet still be useful, provided it has superior knowledge in its own sub-domain. Once incorporated with plan recognition system <ref> [All83, KA86, SSG78] </ref>, the PV system can also be developed into a practical and comprehensive warning system (which was the original objective of this research), capable of detecting possible problems of some ascribed plan and of giving both warnings and helpful information. 3 Simple (Blocks World) Example We start with the
References-found: 12

