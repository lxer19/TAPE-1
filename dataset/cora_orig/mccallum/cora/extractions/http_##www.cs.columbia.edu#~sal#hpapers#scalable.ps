URL: http://www.cs.columbia.edu/~sal/hpapers/scalable.ps
Refering-URL: http://www.cs.columbia.edu:80/~sal/recent-papers.html
Root-URL: 
Title: Scalability of Hierarchical Meta-Learning on Partitioned Data  
Author: Philip K. Chan Salvatore J. Stolfo 
Keyword: speedup, scalability, arbiter and combiner trees, meta-learning, parallel/distributed processing, inductive learning  
Note: This work was partially funded by grants from NSF (IRI-96-32225 CDA-96-25374), ARPA (F30602 96-1-0311), and NYSSTF (423115-445).  
Address: Melbourne, FL 32901  New York, NY 10027  
Affiliation: Computer Science Florida Institute of Technology  Department of Computer Science Columbia University  
Email: pkc@cs.fit.edu  sal@cs.columbia.edu  
Phone: FAX: (407) 984-8461  (212) 939-7080  
Date: May 8, 1997  
Abstract: In this paper we study the issue of how to scale machine learning algorithms, that typically are designed to deal with main-memory based datasets, to efficiently learn models from large distributed databases. We have explored an approach called meta-learning that is related to the traditional approaches of data reduction commonly employed in distributed database query processing systems. We explore the scalability of learning arbiter and combiner trees from partitioned data. Arbiter and combiner trees integrate classifiers trained in parallel from small disjoint subsets. Previous work demonstrated the efficacy of these meta-learning architectures in terms of accuracy of the computed meta-classifiers. Here we discuss the computational performance of constructing arbiter and combiner trees in terms of speedup and scalability as a function of database size and number of partitions. The performance of serial learning algorithms is evaluated. We then analyze the performance of the algorithms used to construct combiner and arbiter trees in parallel. Our empirical results validate these analyses and indicate that the techniques can effectively scale up to large datasets with millions of records using cheap commodity hardware. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Boswell. </author> <note> Manual for CN2 version 6.1. Turing Institure, 1990. Int. Doc. IND: TI/MLT/4.0T/RAB/1.2. </note>
Reference-contexts: We obtained ID3 [21] and CART [2] as part of the IND package [3] from NASA Ames Research Center and was implemented in C. CN2 [11] implemented in C was obtained from Dr. Clark <ref> [1] </ref>.
Reference: [2] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: Section 5 summarizes our findings and describes future directions. 2 Serial Evaluation of Learning Algorithms In this study we evaluate five learning algorithms: ID3 [21], CART <ref> [2] </ref>, BAYES [13], CN2 [11], and WPEBLS [12] in a serial environment. ID3 [21] and CART [2] compute decision trees. CN2 [11] is a rule learning algorithm. WPEBLS is the weighted version of PEBLS [12], which is a nearest-neighbor learning algorithm. <p> Section 5 summarizes our findings and describes future directions. 2 Serial Evaluation of Learning Algorithms In this study we evaluate five learning algorithms: ID3 [21], CART <ref> [2] </ref>, BAYES [13], CN2 [11], and WPEBLS [12] in a serial environment. ID3 [21] and CART [2] compute decision trees. CN2 [11] is a rule learning algorithm. WPEBLS is the weighted version of PEBLS [12], which is a nearest-neighbor learning algorithm. BAYES [13] is a naive Bayesian learning algorithm that is based on computing conditional probabilities as described in [11]. <p> Since at each level O (a) attributes are evaluated with O (n) examples, the time spent at each level is O (an). Therefore, the time complexity of ID3 is O (a 2 n) in the worst case. In CART <ref> [2, 3] </ref> the values of each attribute at each node are grouped into two disjoint subsets. Like ID3, The height of the tree is bounded by the number of attributes, O (a). <p> We obtained ID3 [21] and CART <ref> [2] </ref> as part of the IND package [3] from NASA Ames Research Center and was implemented in C. CN2 [11] implemented in C was obtained from Dr. Clark [1].
Reference: [3] <author> W. Buntine and R. Caruana. </author> <title> Introduction to IND and Recursive Partitioning. </title> <institution> NASA Ames Research Center, </institution> <year> 1991. </year> <month> 28 </month>
Reference-contexts: Since at each level O (a) attributes are evaluated with O (n) examples, the time spent at each level is O (an). Therefore, the time complexity of ID3 is O (a 2 n) in the worst case. In CART <ref> [2, 3] </ref> the values of each attribute at each node are grouped into two disjoint subsets. Like ID3, The height of the tree is bounded by the number of attributes, O (a). <p> We obtained ID3 [21] and CART [2] as part of the IND package <ref> [3] </ref> from NASA Ames Research Center and was implemented in C. CN2 [11] implemented in C was obtained from Dr. Clark [1].
Reference: [4] <author> J. Catlett. </author> <title> Megainduction: A test flight. </title> <booktitle> In Proc. Eighth Intl. Work. Machine Learning, </booktitle> <pages> pages 596-599, </pages> <year> 1991. </year>
Reference-contexts: Crossovers among ID3, CART, and BAYES occur between 100,000 and 1 million examples. With 5 million examples, CART was faster than ID3 and BAYES was the slowest. ID3 completed processing 5 million records in about 2,800 seconds (47 minutes), which is much less than Catlett's <ref> [4] </ref> projection of several months for ID3 to process 1 million records. The huge gap merits some explanation. First, the projection was made five years ago, state-of-the-art processor speed has much improved since then.
Reference: [5] <author> P. Chan. </author> <title> A critical review of CN2: A polythetic classifier system. </title> <type> Technical Report CS-88-09 (Master's paper), </type> <institution> Department of Computer Science, Vanderbilt University, Nashville, TN, </institution> <year> 1988. </year>
Reference-contexts: The weight vector is incrementally updated and takes O (n 2 ) time. The time complexity for WPEBLS is therefore O (anv 2 + n 2 ) in the worst case. 3 The time complexity of CN2 <ref> [11, 5] </ref> is a function of how many complexes (candidate antecedents (or LHS's) of a rule) are evaluated. Since CN2 performs a general-to-specific beam search on the complexes, a fixed number of complexes is retained at each specialization step.
Reference: [6] <author> P. Chan. </author> <title> An Extensible Meta-Learning Approach for Scalable and Accurate Inductive Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, NY, </address> <year> 1996. </year>
Reference-contexts: amounts of data are involved. 2.2 Empirical time performance We performed two sets of experiments: the first set used the splice junction data (courtesy of Towell, Shavlik, and Noordewier [25]) with training set size up to 100,000 examples, the second set used artificial data based on a Boolean concept 1 <ref> [6] </ref> with set size up to 10 million when all algorithms exceeded the main memory. We obtained ID3 [21] and CART [2] as part of the IND package [3] from NASA Ames Research Center and was implemented in C. CN2 [11] implemented in C was obtained from Dr. Clark [1]. <p> We next describe hierarchical meta-learning on partitioned data. 3 Hierarchical Meta-learning Hierarchical meta-learning is designed to speed up the training process by partitioning a large data set into smaller subsets, learning from the subsets in parallel, and integrating the learned models <ref> [6] </ref>. Since much information is lost when models are learned from smaller subsets, prediction accuracy of hierarchical meta-learning in an important concern.
Reference: [7] <author> P. Chan and S. Stolfo. </author> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Work. Multistrategy Learning, </booktitle> <pages> pages 150-165, </pages> <year> 1993. </year>
Reference-contexts: The arbiter/combiner is also a classifier, and hence other arbiters or combiners can be computed from the set of predictions of other arbiters/combiners in a hierarchical fashion. 10 3.1.1 Arbiter An arbiter <ref> [7] </ref> is learned by some learning algorithm to arbitrate among predictions generated by different base classifiers. This arbiter, together with an arbitration rule, decides a final classification outcome based upon the base predictions. <p> Once the training set is formed, an arbiter is generated by the same learning algorithm used to train the base classifiers. Together with an arbitration rule, the learned arbiter resolves conflicts among the classifiers when necessary. 3.1.2 Combiner In the combiner <ref> [7] </ref> strategy, the predictions of the learned base classifiers on the training set form the basis of the meta-learner's training set. A composition rule, which varies in different schemes, determines the content of training examples for the meta-learner.
Reference: [8] <author> P. Chan and S. Stolfo. </author> <title> Toward scalable and parallel learning: A case study in splice junction prediction. </title> <type> Technical Report CUCS-032-94, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, NY, </address> <year> 1994. </year> <booktitle> (Presented at the ML94 Workshop on Machine Learning and Molecular Biology). </booktitle>
Reference-contexts: Splice junctions In a set of experiments we measured the CPU training time of ID3, CART, BAYES, and WPEBLS with the number of training examples varying from 10 to 100,000 in the splice junction domain (examples were randomly selected and duplicated from the original data set, which has 3,190 examples) <ref> [8] </ref>. Thus, the training sets contain many duplicate examples. The results in CPU time on Sun IPXs are plotted in Figure 1. We observe that WPEBLS performed comparatively worse than the other three algorithms when more training examples were presented.
Reference: [9] <author> P. Chan and S. Stolfo. </author> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> In Proc. Twelfth Intl. Conf. Machine Learning, </booktitle> <pages> pages 90-98, </pages> <year> 1995. </year>
Reference-contexts: Others have studied approaches based upon direct parallelization of a learning algorithm run on a multiprocessor. However, a different parallelization port is necessary for a distinct combination of learning algorithm and parallel architecture. A review of such approaches has appeared elsewhere <ref> [9] </ref>. <p> This implies that the accuracy of each base classifier will likely degrade since a small sample of the database is processed. Thus, we also seek to boost the accuracy of the final computed model by combining the collective knowledge of the constituent base classifiers.. In a previous study <ref> [9] </ref> we demonstrated that our meta-learning techniques that aim to combine multiple models outperform the voting-based and statistical techniques in terms of prediction accuracy. In another study [10] hierarchical meta-learning techniques were developed to integrate classifiers learned from partitioned data in a parallel environment.
Reference: [10] <author> P. Chan and S. Stolfo. </author> <title> Learning arbiter and combiner trees from partitioned data for scaling machine learning. </title> <booktitle> In Proc. Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 39-44, </pages> <year> 1995. </year>
Reference-contexts: In a previous study [9] we demonstrated that our meta-learning techniques that aim to combine multiple models outperform the voting-based and statistical techniques in terms of prediction accuracy. In another study <ref> [10] </ref> hierarchical meta-learning techniques were developed to integrate classifiers learned from partitioned data in a parallel environment. Empirical results indicate that the accuracy performance of hierarchical meta-learning is comparable to (or sometimes exceeds) the global classifier computed by a machine learning algorithm applied to the entire data set. <p> Since much information is lost when models are learned from smaller subsets, prediction accuracy of hierarchical meta-learning in an important concern. Our results in <ref> [10] </ref> indicate that the accuracy performance of hierarchical meta-learning on partitioned data 9 is comparable to (and sometimes exceeds) that of serial learning from the whole data set. <p> The combiner tree approach, however, always classifies at most the size of the meta-level training set. Therefore, combiner trees can be generated more efficiently than arbiter tress in certain cases. An evaluation of the prediction accuracy for hierarchical meta-learning is discussed in <ref> [10] </ref>.
Reference: [11] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-285, </pages> <year> 1989. </year>
Reference-contexts: Section 5 summarizes our findings and describes future directions. 2 Serial Evaluation of Learning Algorithms In this study we evaluate five learning algorithms: ID3 [21], CART [2], BAYES [13], CN2 <ref> [11] </ref>, and WPEBLS [12] in a serial environment. ID3 [21] and CART [2] compute decision trees. CN2 [11] is a rule learning algorithm. WPEBLS is the weighted version of PEBLS [12], which is a nearest-neighbor learning algorithm. <p> Section 5 summarizes our findings and describes future directions. 2 Serial Evaluation of Learning Algorithms In this study we evaluate five learning algorithms: ID3 [21], CART [2], BAYES [13], CN2 <ref> [11] </ref>, and WPEBLS [12] in a serial environment. ID3 [21] and CART [2] compute decision trees. CN2 [11] is a rule learning algorithm. WPEBLS is the weighted version of PEBLS [12], which is a nearest-neighbor learning algorithm. BAYES [13] is a naive Bayesian learning algorithm that is based on computing conditional probabilities as described in [11]. <p> ID3 [21] and CART [2] compute decision trees. CN2 <ref> [11] </ref> is a rule learning algorithm. WPEBLS is the weighted version of PEBLS [12], which is a nearest-neighbor learning algorithm. BAYES [13] is a naive Bayesian learning algorithm that is based on computing conditional probabilities as described in [11]. <p> The total time complexity for CART is therefore O ((av + an)a) or O (a 2 v + a 2 n) in the worst case. BAYES <ref> [11] </ref> calculates the conditional probabilities for each attribute value given a class and the probabilities for each class. O (av) conditional probabilities are calculated and each takes O (n) time, hence O (avn) time is needed. The class probabilities can be calculated in O (n) time. <p> The weight vector is incrementally updated and takes O (n 2 ) time. The time complexity for WPEBLS is therefore O (anv 2 + n 2 ) in the worst case. 3 The time complexity of CN2 <ref> [11, 5] </ref> is a function of how many complexes (candidate antecedents (or LHS's) of a rule) are evaluated. Since CN2 performs a general-to-specific beam search on the complexes, a fixed number of complexes is retained at each specialization step. <p> We obtained ID3 [21] and CART [2] as part of the IND package [3] from NASA Ames Research Center and was implemented in C. CN2 <ref> [11] </ref> implemented in C was obtained from Dr. Clark [1]. WPEBLS [12] and BAYES [13] (as described in [11]) were reimplemented in C++. 1 The expression is (x 0 &lt; `N 0 ^ x 1 &lt; 500 ^ x 2 &lt; `N 0 ) _ (x 1 &gt; 500 ^ x <p> We obtained ID3 [21] and CART [2] as part of the IND package [3] from NASA Ames Research Center and was implemented in C. CN2 <ref> [11] </ref> implemented in C was obtained from Dr. Clark [1]. WPEBLS [12] and BAYES [13] (as described in [11]) were reimplemented in C++. 1 The expression is (x 0 &lt; `N 0 ^ x 1 &lt; 500 ^ x 2 &lt; `N 0 ) _ (x 1 &gt; 500 ^ x 2 &gt; `N 0 ^ x 3 &gt; 500) _ (x 4 &lt; `N 0 ^ x 5
Reference: [12] <author> S. Cost and S. Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference-contexts: Section 5 summarizes our findings and describes future directions. 2 Serial Evaluation of Learning Algorithms In this study we evaluate five learning algorithms: ID3 [21], CART [2], BAYES [13], CN2 [11], and WPEBLS <ref> [12] </ref> in a serial environment. ID3 [21] and CART [2] compute decision trees. CN2 [11] is a rule learning algorithm. WPEBLS is the weighted version of PEBLS [12], which is a nearest-neighbor learning algorithm. <p> Serial Evaluation of Learning Algorithms In this study we evaluate five learning algorithms: ID3 [21], CART [2], BAYES [13], CN2 [11], and WPEBLS <ref> [12] </ref> in a serial environment. ID3 [21] and CART [2] compute decision trees. CN2 [11] is a rule learning algorithm. WPEBLS is the weighted version of PEBLS [12], which is a nearest-neighbor learning algorithm. BAYES [13] is a naive Bayesian learning algorithm that is based on computing conditional probabilities as described in [11]. <p> O (av) conditional probabilities are calculated and each takes O (n) time, hence O (avn) time is needed. The class probabilities can be calculated in O (n) time. Therefore, the time complexity of BAYES is O (avn + n) or O (avn). WPEBLS <ref> [12] </ref> calculates a set of value distance matrices (VDMs) and a vector of weights for the exemplars. Each attribute has a VDM of size v by v, which takes O (nv 2 ) to calculate. For a attributes, O (anv 2 ) time is needed for a VDMs. <p> We obtained ID3 [21] and CART [2] as part of the IND package [3] from NASA Ames Research Center and was implemented in C. CN2 [11] implemented in C was obtained from Dr. Clark [1]. WPEBLS <ref> [12] </ref> and BAYES [13] (as described in [11]) were reimplemented in C++. 1 The expression is (x 0 &lt; `N 0 ^ x 1 &lt; 500 ^ x 2 &lt; `N 0 ) _ (x 1 &gt; 500 ^ x 2 &gt; `N 0 ^ x 3 &gt; 500) _ (x
Reference: [13] <author> R. Duda and P. Hart. </author> <title> Pattern classification and scene analysis. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1973. </year>
Reference-contexts: Section 5 summarizes our findings and describes future directions. 2 Serial Evaluation of Learning Algorithms In this study we evaluate five learning algorithms: ID3 [21], CART [2], BAYES <ref> [13] </ref>, CN2 [11], and WPEBLS [12] in a serial environment. ID3 [21] and CART [2] compute decision trees. CN2 [11] is a rule learning algorithm. WPEBLS is the weighted version of PEBLS [12], which is a nearest-neighbor learning algorithm. BAYES [13] is a naive Bayesian learning algorithm that is based on <p> we evaluate five learning algorithms: ID3 [21], CART [2], BAYES <ref> [13] </ref>, CN2 [11], and WPEBLS [12] in a serial environment. ID3 [21] and CART [2] compute decision trees. CN2 [11] is a rule learning algorithm. WPEBLS is the weighted version of PEBLS [12], which is a nearest-neighbor learning algorithm. BAYES [13] is a naive Bayesian learning algorithm that is based on computing conditional probabilities as described in [11]. <p> We obtained ID3 [21] and CART [2] as part of the IND package [3] from NASA Ames Research Center and was implemented in C. CN2 [11] implemented in C was obtained from Dr. Clark [1]. WPEBLS [12] and BAYES <ref> [13] </ref> (as described in [11]) were reimplemented in C++. 1 The expression is (x 0 &lt; `N 0 ^ x 1 &lt; 500 ^ x 2 &lt; `N 0 ) _ (x 1 &gt; 500 ^ x 2 &gt; `N 0 ^ x 3 &gt; 500) _ (x 4 &lt; `N
Reference: [14] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM 3 user's guide and reference manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <year> 1993. </year>
Reference-contexts: Next, we discuss our parallel implementation and empirical experiments on very large data sets. 21 nodes. 4.5 Parallel implementation The hierarchical meta-learning strategies were implemented on a parallel and distributed platform based on the message-passing model. To satisfy our goal of portability, we chose PVM (Parallel Virtual Machine) <ref> [14] </ref> to provide message passing support|PVM supports a common interface for message passing among machines of diverse architectures. The computing platform we used consists of eight HP 9000/735 workstations on a dedicated FDDI (Fiber Distribution Data Interface) network. tree with 8 leaf nodes.
Reference: [15] <author> C. Grammes. GNUFIT v1.2, </author> <year> 1993. </year> <note> (Available at ftp://ftp.dartmouth.edu/pub/gnuplot/gnufit12.tar.gz). </note>
Reference-contexts: The curve approximations were computed using GNUFIT <ref> [15] </ref> with the Marquardt-Levenberg algorithm [22, 19], a nonlinear least squares fit. To approximate the training speed with polynomial equations, we inspect how closely the three polynomials fit the data points.
Reference: [16] <author> J. Gustafson. </author> <title> Reevaluating Amdahl's law. </title> <journal> Comm. ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <year> 1988. </year>
Reference-contexts: That is, Speedup = T S : For this metric, T S is usually the time for the fastest serial algorithm, which could be the parallel algorithm running serially. * Scaled speedup <ref> [16, 18] </ref> provides a metric for scalability. It measures the speedup of a parallel system when the problem size increases linearly with the number of processors. Scaled speedup = T S (W fi p) ; (1) where T S and T P are expressed as functions of problem size.
Reference: [17] <author> V. Kumar, A. Grama, A. Gupta, and G. Karypis. </author> <title> Introduction to parallel computing: Design and analysis of algorithms. </title> <address> Benjamin-Cummings, Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: We define our terms before we evaluate the approach in a parallel and distributed setting. * T S = serial execution time. * T P = parallel execution time. * p = number of processors. * n = input size (number of training examples). * W = problem size (work) <ref> [17] </ref>, which measures the total number of computational units needed for serial execution. That is, T S = W fi t u , where t u is the time spent for a unit of computation. Hence, T S / W .
Reference: [18] <author> V. Kumar and A. Gupta. </author> <title> Analyzing scalability of parallel algorithms and architectures. </title> <journal> J. Parallel & Distributed Computing, </journal> <volume> 22 </volume> <pages> 379-391, </pages> <year> 1994. </year> <month> 29 </month>
Reference-contexts: That is, Speedup = T S : For this metric, T S is usually the time for the fastest serial algorithm, which could be the parallel algorithm running serially. * Scaled speedup <ref> [16, 18] </ref> provides a metric for scalability. It measures the speedup of a parallel system when the problem size increases linearly with the number of processors. Scaled speedup = T S (W fi p) ; (1) where T S and T P are expressed as functions of problem size. <p> Scaled speedup = T S (W fi p) ; (1) where T S and T P are expressed as functions of problem size. Parallel systems exhibiting linear or near-linear scaled speedup (with respect to p, the number of processors) is considered scalable. Other scalability metrics can be found in <ref> [18] </ref>. * Speed is characterized by the algorithm's time complexity. 4.2 Speedup analysis For pedagogical reasons, we focus our analysis on arbiter trees; similar results can be obtained for combiner trees.
Reference: [19] <author> W. Press, B. Flannery, S. Teukolsky, and W. Vetterling. </author> <title> Numerical recipies in C: </title> <booktitle> The art of scientific computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1988. </year>
Reference-contexts: The curve approximations were computed using GNUFIT [15] with the Marquardt-Levenberg algorithm <ref> [22, 19] </ref>, a nonlinear least squares fit. To approximate the training speed with polynomial equations, we inspect how closely the three polynomials fit the data points.
Reference: [20] <author> F. Provost and D. Hennessy. </author> <title> Scaling up: Distributed machine learning with cooperation. </title> <booktitle> In Proc. AAAI-96. </booktitle> <publisher> AAAI Press, </publisher> <year> 1996. </year> <pages> 74-79. </pages>
Reference-contexts: A review of such approaches has appeared elsewhere [9]. An alternative approach we study here is to apply data reduction techniques common in distributed query processing where a cluster of networked (commodity) computers can be profitably employed to learn from large databases. (Provost <ref> [20] </ref> has studied the same approach for a distributed rule learning system.) This means one may partition the data into a number of smaller disjoint training subsets, apply some learning algorithm on each subset (perhaps all in parallel), followed by a phase that combines the separately learned models or classifiers in
Reference: [21] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: We then analyze in Section 4 the speedup and scalability that can be achieved by utilizing hierarchical meta-learning in a parallel and distributed environment. Section 5 summarizes our findings and describes future directions. 2 Serial Evaluation of Learning Algorithms In this study we evaluate five learning algorithms: ID3 <ref> [21] </ref>, CART [2], BAYES [13], CN2 [11], and WPEBLS [12] in a serial environment. ID3 [21] and CART [2] compute decision trees. CN2 [11] is a rule learning algorithm. WPEBLS is the weighted version of PEBLS [12], which is a nearest-neighbor learning algorithm. <p> Section 5 summarizes our findings and describes future directions. 2 Serial Evaluation of Learning Algorithms In this study we evaluate five learning algorithms: ID3 <ref> [21] </ref>, CART [2], BAYES [13], CN2 [11], and WPEBLS [12] in a serial environment. ID3 [21] and CART [2] compute decision trees. CN2 [11] is a rule learning algorithm. WPEBLS is the weighted version of PEBLS [12], which is a nearest-neighbor learning algorithm. BAYES [13] is a naive Bayesian learning algorithm that is based on computing conditional probabilities as described in [11]. <p> Let * a = the number of attributes, * v = the largest number of distinct values for an attribute (i.e., its domain size), and * n = the number of training examples. The time complexity of ID3 <ref> [21] </ref> is a function of the number of levels in the decision tree it forms. The height of the tree is bounded by the number of attributes, O (a). <p> We obtained ID3 <ref> [21] </ref> and CART [2] as part of the IND package [3] from NASA Ames Research Center and was implemented in C. CN2 [11] implemented in C was obtained from Dr. Clark [1].
Reference: [22] <author> A. Ralston and P. Rabinowitz. </author> <title> A first course in numerical analysis. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1978. </year>
Reference-contexts: The curve approximations were computed using GNUFIT [15] with the Marquardt-Levenberg algorithm <ref> [22, 19] </ref>, a nonlinear least squares fit. To approximate the training speed with polynomial equations, we inspect how closely the three polynomials fit the data points.
Reference: [23] <author> S. Stolfo, A. Prodromidis, S. Tselepis, W. Lee, W. Fan, and P. Chan. </author> <title> JAM: Java agents for meta-learning over distributed databases. </title> <booktitle> In Proc. 3rd Intl. Conf. Know. Disc. Data Mining, </booktitle> <year> 1997. </year> <note> To Appear. </note>
Reference-contexts: Furthermore, if we relax our portability goal, using customized platform-dependent message-passing routines rather than portable ones reduces communication overhead among processors and improves overall time performance. To increase the accessibility of learning and meta-learning systems on computer networks, we designed the JAM (Java Agents for Meta-learning) architecture <ref> [23] </ref>. JAM is a portable meta-learning infrastructure built upon Java technology currently under development at Columbia University (in collaboration with Florida Tech).
Reference: [24] <author> X. Sun and L. Ni. </author> <title> Scalable problems and memory-bounded speedup. </title> <journal> J. Parallel & Distributed Comp., </journal> <volume> 19 </volume> <pages> 27-37, </pages> <year> 1993. </year>
Reference-contexts: Again, we note that the fastest serial case should be used for speedup analysis; the second analysis is presented for completeness. An alternate scalability metric is memory-bound scaled speedup <ref> [24] </ref>, which measures the increase in possible problem size with increasing number of processors, each with limited 19 available memory.
Reference: [25] <author> G. Towell, J. Shavlik, and M. Noordewier. </author> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proc. AAAI-90, </booktitle> <pages> pages 861-866, </pages> <year> 1990. </year>
Reference-contexts: That is, in addition to n, v could be a significant factor in time performance when large amounts of data are involved. 2.2 Empirical time performance We performed two sets of experiments: the first set used the splice junction data (courtesy of Towell, Shavlik, and Noordewier <ref> [25] </ref>) with training set size up to 100,000 examples, the second set used artificial data based on a Boolean concept 1 [6] with set size up to 10 million when all algorithms exceeded the main memory.
Reference: [26] <author> D. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259, </pages> <year> 1992. </year> <month> 30 </month>
Reference-contexts: Return meta-level training instances with the correct classification and the predictions; i.e., T = f (class (x); C 1 (x); C 2 (x); :::C k (x)) j x 2 Eg: This scheme was also used by Wolpert <ref> [26] </ref>. (For further reference, this scheme is denoted as class-combiner.) 2.
References-found: 26

