URL: ftp://ftp.cse.ogi.edu/pub/esr/reports/centr-div-control.ps.gz
Refering-URL: http://www.cse.ogi.edu/~calton/publication.html
Root-URL: http://www.cse.ogi.edu
Title: Divergence Control Algorithms for Epsilon Serializability  
Author: Kun-Lung Wu and Philip S. Yu Calton Pu 
Keyword: Index terms: epsilon serializability, divergence control, serializability, concurrency control, transaction processing.  
Date: August 1, 1994  
Address: P.O. Box 704 Yorktown Heights, NY 10598  New York, NY 10027  
Affiliation: IBM T.J. Watson Research Center  Department of Computer Science Columbia University  
Abstract: This paper presents divergence control methods for epsilon serializability (ESR) in centralized databases. ESR alleviates the strictness of serializability (SR) in transaction processing by allowing for limited inconsistency. The bounded inconsistency is automatically maintained by divergence control (DC) methods in a way similar to SR is maintained by concurrency control (CC) mechanisms. However, DC for ESR allows more concurrency than CC for SR. In this paper, we design DC methods in such a way that read-only transactions need not be serializable with other update transactions, but update transactions must be serializable among themselves. ESR is both feasible and widely applicable. We first demonstrate the feasibility of ESR by showing the design of three representative DC methods: two-phase locking, timestamp ordering and optimistic approaches. DC methods are designed by systematically enhancing CC algorithms in two stages: extension and relaxation. In the extension stage, a CC algorithm is analyzed to locate the places where it identifies non-SR conflicts of database operations. In the relaxation stage, the non-SR conflicts are relaxed to allow for controlled inconsistency. We then demonstrate the applicability of ESR by presenting the design of DC methods using other most known inconsistency specifications, such as absolute value, age and total number of non-serializably read data items. In addition, we present a performance study using an optimistic divergence control algorithm as an example to show that a substantial improvement in concurrency can be achieved in ESR by allowing for a small amount of inconsistency. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman, </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: 1 Introduction Serializability <ref> [1, 2] </ref> is the standard notion of correctness in transaction processing. Serializability (SR) maintains database consistency: when a database only runs serializable transactions, then (if the database starts out in a consistent state) the database is guaranteed to remain in a consistent state. However, serializability has its limitations. <p> A transaction is a sequence of operations that take a database from a consistent state to another. Transactions may be updates that contain at least one Write or queries that are read-only. Our terminology follows the standard model of conflict-based serializability <ref> [1] </ref>. Two operations are said to conflict if at least one of them is a Write, so we have read-write (R/W) and write-write (W/W) conflicts. Each pair of conflicting operations establishes a dependency. A history, or log, is a sequence of operations such as Reads and Writes. <p> Intuitively, in the standard model a log is shown to be an SRlog by rearranging its operations according to certain constraints imposed by R/W and W/W dependencies. A more formal way to specify concurrency control uses serialization graph (SG), where each arc represents the precede relation <ref> [1] </ref>. Transaction T 1 precedes T 2 when one of T 1 's operations precedes and conflicts (R/W or W/W) with T 2 's operations. Since the Serializability Theorem [1] says that a log H is SR iff its serialization graph SG (H) is acyclic, an acyclic SG implies an SRlog. <p> A more formal way to specify concurrency control uses serialization graph (SG), where each arc represents the precede relation <ref> [1] </ref>. Transaction T 1 precedes T 2 when one of T 1 's operations precedes and conflicts (R/W or W/W) with T 2 's operations. Since the Serializability Theorem [1] says that a log H is SR iff its serialization graph SG (H) is acyclic, an acyclic SG implies an SRlog. 2.2 Epsilon-Transaction (ET) An epsilon-transaction, denoted by ET, is an extension of standard transaction (denoted by T) by including a specification of an amount of permitted inconsistency. <p> A large number of CC algorithms have been proposed for ensuring serializability in transaction processing <ref> [12, 1] </ref>. These CC algorithms can be classified into two major categories: two-phase locking and non-locking CC [1]. Most existing database systems generally use two-phase locking. Among the non-locking CC approaches, timestamp ordering and optimistic concurrency control are the most often discussed. <p> A large number of CC algorithms have been proposed for ensuring serializability in transaction processing [12, 1]. These CC algorithms can be classified into two major categories: two-phase locking and non-locking CC <ref> [1] </ref>. Most existing database systems generally use two-phase locking. Among the non-locking CC approaches, timestamp ordering and optimistic concurrency control are the most often discussed. These three CC algorithms represent the most basic CC methods, from which other integrated CC algorithms can be designed [12, 1]. <p> Most existing database systems generally use two-phase locking. Among the non-locking CC approaches, timestamp ordering and optimistic concurrency control are the most often discussed. These three CC algorithms represent the most basic CC methods, from which other integrated CC algorithms can be designed <ref> [12, 1] </ref>. <p> However, a transaction cannot be committed until a certification, or validation, is performed to ensure SR. If any non-serializable execution is detected during the certification, the transaction is aborted and then restarted. Various approaches have been proposed to implement the certification procedure in OCC <ref> [1, 14, 15] </ref>. We only discuss an OCC method that uses strong locks and weak locks [14]. The strong-lock type supersedes the weak-lock type, and weak locks are always compatible with other weak locks. Each transaction asynchronously requests a weak lock at the time a data item is accessed.
Reference: [2] <author> C. H. Papadimitriou, </author> <title> "Serializability of concurrent updates," </title> <journal> Journal of the ACM, </journal> <volume> vol. 26, no. 4, </volume> <pages> pp. 631-653, </pages> <month> Oct. </month> <year> 1979. </year>
Reference-contexts: 1 Introduction Serializability <ref> [1, 2] </ref> is the standard notion of correctness in transaction processing. Serializability (SR) maintains database consistency: when a database only runs serializable transactions, then (if the database starts out in a consistent state) the database is guaranteed to remain in a consistent state. However, serializability has its limitations.
Reference: [3] <author> P. Franaszek and J. T. Robinson, </author> <title> "Limitations of concurrency in transaction processing," </title> <journal> ACM Trans. on Database Systems, </journal> <volume> vol. 10, no. 1, </volume> <pages> pp. 1-28, </pages> <month> Mar. </month> <year> 1985. </year>
Reference-contexts: However, serializability has its limitations. For example, SR requires that concurrent transactions, including read-only ones, be scheduled in a serializable order. When the number of concurrent transactions increases, the effective level of concurrency tends to decrease, forcing many transactions to wait or abort <ref> [3, 4, 5] </ref>. For read-only transactions that can tolerate some amount of inconsistency, the strictness of serializability may impose unnecessary limitations on the system throughput. To alleviate the limitations of SR, the notion of epsilon serializability (ESR) has been proposed [6, 7, 8, 9].
Reference: [4] <author> R. Agrawal, M. J. Carey, and M. Livny, </author> <title> "Concurrency control performance modeling: Alternatives and implications," </title> <journal> ACM Trans. on Database Systems, </journal> <volume> vol. 12, no. 4, </volume> <pages> pp. 609-654, </pages> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: However, serializability has its limitations. For example, SR requires that concurrent transactions, including read-only ones, be scheduled in a serializable order. When the number of concurrent transactions increases, the effective level of concurrency tends to decrease, forcing many transactions to wait or abort <ref> [3, 4, 5] </ref>. For read-only transactions that can tolerate some amount of inconsistency, the strictness of serializability may impose unnecessary limitations on the system throughput. To alleviate the limitations of SR, the notion of epsilon serializability (ESR) has been proposed [6, 7, 8, 9].
Reference: [5] <author> P. S. Yu, D. M. Dias, and S. S. Lavenberg, </author> <title> "On the analytical modeling of database concurrency control," </title> <journal> Journal of the ACM. </journal> <note> to appear, Also available as Tech. Rep. RC 15386, </note> <institution> IBM T. J. Watson Research Center, </institution> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: However, serializability has its limitations. For example, SR requires that concurrent transactions, including read-only ones, be scheduled in a serializable order. When the number of concurrent transactions increases, the effective level of concurrency tends to decrease, forcing many transactions to wait or abort <ref> [3, 4, 5] </ref>. For read-only transactions that can tolerate some amount of inconsistency, the strictness of serializability may impose unnecessary limitations on the system throughput. To alleviate the limitations of SR, the notion of epsilon serializability (ESR) has been proposed [6, 7, 8, 9]. <p> As presented in Section 3.3, the total number of non-SR conflicts is used as the inconsistency specification. The performance study uses an analytical model similar to those used in <ref> [5, 14, 15] </ref>, whose accuracy has been extensively validated by simulation. In the following analyses, we calculate the abort probability of a query ET for different contention environments and different * values. We assume ImpLimit = * for a query ET and ExpLimit &gt; 0 for an update ET. <p> Update ETs follow Poisson arrivals at rate ; each update ET accesses N exclusive locks (granules); each query ET accesses N q locks; and the database contains L total granules. Following the model in <ref> [5, 14, 15] </ref>, a query ET has N q + 2 states. State 0 represents the setup phase, including program fetch and message processing. After state 0, a query ET progresses to states 1, 2, : : :, N q .
Reference: [6] <author> C. Pu and A. Leff, "Epsilon-serializability," </author> <type> Tech. Rep. </type> <institution> CUCS-054-90, Department of Computer Science, Columbia University, </institution> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: For read-only transactions that can tolerate some amount of inconsistency, the strictness of serializability may impose unnecessary limitations on the system throughput. To alleviate the limitations of SR, the notion of epsilon serializability (ESR) has been proposed <ref> [6, 7, 8, 9] </ref>. Instead of weakening database consistency, ESR stretches it. ESR allows for limited inconsistency in read-only transaction processing while maintaining database consistency. Such ESR properties are automatically preserved by divergence control (DC) methods in a way similar to SR by concurrency control (CC) mechanisms.
Reference: [7] <author> C. Pu and A. Leff, </author> <title> "Replica control in distributed systems: An asynchronous approach," </title> <booktitle> in Proc. of ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> pp. 377-386, </pages> <year> 1991. </year>
Reference-contexts: For read-only transactions that can tolerate some amount of inconsistency, the strictness of serializability may impose unnecessary limitations on the system throughput. To alleviate the limitations of SR, the notion of epsilon serializability (ESR) has been proposed <ref> [6, 7, 8, 9] </ref>. Instead of weakening database consistency, ESR stretches it. ESR allows for limited inconsistency in read-only transaction processing while maintaining database consistency. Such ESR properties are automatically preserved by divergence control (DC) methods in a way similar to SR by concurrency control (CC) mechanisms. <p> ESR can model such trade-offs in both examples between the degrees of consistency and concurrency. Examples of ESR applications in distributed databases and replicated databases can be found in <ref> [7, 9] </ref>. ESR is both feasible and widely applicable. In this paper, we first demonstrate the feasibility of ESR by showing the design of representative DC methods. Although DC allows for bounded inconsistency, it prevents update transactions from causing inconsistency in the database. <p> WLCA is a particular algorithm for replicated distributed databases, while ESR is implemented by many DC methods for both centralized and distributed databases <ref> [7, 9] </ref>. Du and Elmagarmid [18] have introduced quasi-serializability (QSR) as a correctness criterion for federated databases. QSR may be seen as a generalization of read-only weak consistency, since QSR also maintains locally consistent views, but does not guarantee global SR.
Reference: [8] <author> C. Pu, </author> <title> "Generalized transaction processing with epsilon-serializability," </title> <booktitle> in Proc. of 4th Int. Workshop on High Performance Transaction Systems, </booktitle> <year> 1991. </year>
Reference-contexts: For read-only transactions that can tolerate some amount of inconsistency, the strictness of serializability may impose unnecessary limitations on the system throughput. To alleviate the limitations of SR, the notion of epsilon serializability (ESR) has been proposed <ref> [6, 7, 8, 9] </ref>. Instead of weakening database consistency, ESR stretches it. ESR allows for limited inconsistency in read-only transaction processing while maintaining database consistency. Such ESR properties are automatically preserved by divergence control (DC) methods in a way similar to SR by concurrency control (CC) mechanisms.
Reference: [9] <author> C. Pu and A. Leff, </author> <title> "Autonomous transaction execution with epsilon-serializability," </title> <booktitle> in Proc. of 2nd Int. Workshop on Research Issues on Data Engineering: Transaction and Query Processing, </booktitle> <pages> pp. 2-11, </pages> <year> 1992. </year>
Reference-contexts: For read-only transactions that can tolerate some amount of inconsistency, the strictness of serializability may impose unnecessary limitations on the system throughput. To alleviate the limitations of SR, the notion of epsilon serializability (ESR) has been proposed <ref> [6, 7, 8, 9] </ref>. Instead of weakening database consistency, ESR stretches it. ESR allows for limited inconsistency in read-only transaction processing while maintaining database consistency. Such ESR properties are automatically preserved by divergence control (DC) methods in a way similar to SR by concurrency control (CC) mechanisms. <p> ESR can model such trade-offs in both examples between the degrees of consistency and concurrency. Examples of ESR applications in distributed databases and replicated databases can be found in <ref> [7, 9] </ref>. ESR is both feasible and widely applicable. In this paper, we first demonstrate the feasibility of ESR by showing the design of representative DC methods. Although DC allows for bounded inconsistency, it prevents update transactions from causing inconsistency in the database. <p> WLCA is a particular algorithm for replicated distributed databases, while ESR is implemented by many DC methods for both centralized and distributed databases <ref> [7, 9] </ref>. Du and Elmagarmid [18] have introduced quasi-serializability (QSR) as a correctness criterion for federated databases. QSR may be seen as a generalization of read-only weak consistency, since QSR also maintains locally consistent views, but does not guarantee global SR.
Reference: [10] <author> K. Ramamrithan and C. Pu, </author> <title> "A formal characterization of epsilon-serializability," </title> <type> Tech. Rep. </type> <institution> CUCS-044-91, Department of Computer Science, Columbia University, </institution> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: Thus the sub-history formed by U ET 1 and Q ET 2 is not serializable. Assuming the non-SR interleavings are within the specified limits, log (1) qualifies as an ESRlog. Instead of ESRlogs, a more formal characterization of ESR has been introduced <ref> [10] </ref> using the ACTA framework [11]. <p> The above fuzziness bound is calculated for the fuzziness imported by a query ET. In a more recent paper <ref> [10] </ref>, we analyze the fuzziness bound in the results of the query ET in more detail. As an example, consider a query ET obtaining an approximate statistics on a group of accounts in a banking services system. <p> Are they valid only for a particular inconsistency specification? How many databases and applications fit that kind of inconsistency specification? In the paper giving a formal characterization of ESR <ref> [10] </ref>, we have sought to define inconsistency specifications more broadly. Each object in the database contains a value. A database state is the set of all object values. A database state space is the set of all possible database states.
Reference: [11] <author> P. K. Chrysanthis and K. Ramamritham, </author> <title> "ACTA: The Saga continues," in Transaction Models for Advanced Applications (A. </title> <editor> Elmagarmid, ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Thus the sub-history formed by U ET 1 and Q ET 2 is not serializable. Assuming the non-SR interleavings are within the specified limits, log (1) qualifies as an ESRlog. Instead of ESRlogs, a more formal characterization of ESR has been introduced [10] using the ACTA framework <ref> [11] </ref>. However, since the concept of transaction execution log is more intuitive to most readers, we use in this paper the less formal ESRlogs to characterize ESR. 2.3 Divergence Control (DC) By analogy to CC algorithms, which support SR efficiently, we want to design effective DC methods that guarantee ESR.
Reference: [12] <author> P. A. Bernstein and N. Goodman, </author> <title> "Concurrency control in distributed database systems," </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 13, no. 2, </volume> <pages> pp. 185-222, </pages> <month> June </month> <year> 1981. </year> <month> 23 </month>
Reference-contexts: A large number of CC algorithms have been proposed for ensuring serializability in transaction processing <ref> [12, 1] </ref>. These CC algorithms can be classified into two major categories: two-phase locking and non-locking CC [1]. Most existing database systems generally use two-phase locking. Among the non-locking CC approaches, timestamp ordering and optimistic concurrency control are the most often discussed. <p> Most existing database systems generally use two-phase locking. Among the non-locking CC approaches, timestamp ordering and optimistic concurrency control are the most often discussed. These three CC algorithms represent the most basic CC methods, from which other integrated CC algorithms can be designed <ref> [12, 1] </ref>. <p> For simplicity, we only discuss basic timestamp ordering algorithm <ref> [12] </ref>. In basic timestamp ordering CC, associated with each data item x, there are R-ts (x) and W-ts (x), representing the largest timestamp of any Read and Write operations, respectively, that have been processed against x. <p> identify the exact query ETs whose Import Accumulators need to be incremented, the maintenance of the accessed data items and the search itself when a conflict occurs may be costly, especially when a query ET may access a large number of data items. 1 If Thomas write rule is used <ref> [12] </ref>, the W (x) can simply be ignored and the corresponding transaction need not be aborted. 2 This can be done by simply maintaining a separate R ? ts (x) to record the largest timestamp of a Read operation by an update ET.
Reference: [13] <author> H. T. Kung and J. T. Robinson, </author> <title> "On optimistic methods for concurrency control," </title> <journal> ACM Trans. on Database Systems, </journal> <volume> vol. 6, no. 2, </volume> <pages> pp. 213-226, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: Since the non-SR conflicting requests are accepted only after checking the relevant Import Accumulators and Export Accumu, the log generated by the TODC method is an ESRlog. 2 3.3 Optimistic Divergence Control Summary of optimistic CC for SR: Optimistic concurrency control (OCC) <ref> [13] </ref>, unlike two-phase locking or timestamp ordering approaches, allows the Read or Write operations of any transaction to proceed immediately when the requests arrive. However, a transaction cannot be committed until a certification, or validation, is performed to ensure SR.
Reference: [14] <author> P. S. Yu and D. M. Dias, </author> <title> "Impact of large memory on the performance of optimistic concurrency control schemes," </title> <booktitle> in Proc. of PARBASE-90 Int. Conf. on Database, Parallel Architectures and Their Applications, </booktitle> <pages> pp. 86-90, </pages> <year> 1990. </year>
Reference-contexts: However, a transaction cannot be committed until a certification, or validation, is performed to ensure SR. If any non-serializable execution is detected during the certification, the transaction is aborted and then restarted. Various approaches have been proposed to implement the certification procedure in OCC <ref> [1, 14, 15] </ref>. We only discuss an OCC method that uses strong locks and weak locks [14]. The strong-lock type supersedes the weak-lock type, and weak locks are always compatible with other weak locks. Each transaction asynchronously requests a weak lock at the time a data item is accessed. <p> If any non-serializable execution is detected during the certification, the transaction is aborted and then restarted. Various approaches have been proposed to implement the certification procedure in OCC [1, 14, 15]. We only discuss an OCC method that uses strong locks and weak locks <ref> [14] </ref>. The strong-lock type supersedes the weak-lock type, and weak locks are always compatible with other weak locks. Each transaction asynchronously requests a weak lock at the time a data item is accessed. <p> As presented in Section 3.3, the total number of non-SR conflicts is used as the inconsistency specification. The performance study uses an analytical model similar to those used in <ref> [5, 14, 15] </ref>, whose accuracy has been extensively validated by simulation. In the following analyses, we calculate the abort probability of a query ET for different contention environments and different * values. We assume ImpLimit = * for a query ET and ExpLimit &gt; 0 for an update ET. <p> Update ETs follow Poisson arrivals at rate ; each update ET accesses N exclusive locks (granules); each query ET accesses N q locks; and the database contains L total granules. Following the model in <ref> [5, 14, 15] </ref>, a query ET has N q + 2 states. State 0 represents the setup phase, including program fetch and message processing. After state 0, a query ET progresses to states 1, 2, : : :, N q .
Reference: [15] <author> P. S. Yu, H.-U. Heiss, and D. M. Dias, </author> <title> "Modelling and analysis of a time-stamp history based certification protocol for concurrency control," </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> vol. 3, no. 4, </volume> <pages> pp. 525-537, </pages> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: However, a transaction cannot be committed until a certification, or validation, is performed to ensure SR. If any non-serializable execution is detected during the certification, the transaction is aborted and then restarted. Various approaches have been proposed to implement the certification procedure in OCC <ref> [1, 14, 15] </ref>. We only discuss an OCC method that uses strong locks and weak locks [14]. The strong-lock type supersedes the weak-lock type, and weak locks are always compatible with other weak locks. Each transaction asynchronously requests a weak lock at the time a data item is accessed. <p> As presented in Section 3.3, the total number of non-SR conflicts is used as the inconsistency specification. The performance study uses an analytical model similar to those used in <ref> [5, 14, 15] </ref>, whose accuracy has been extensively validated by simulation. In the following analyses, we calculate the abort probability of a query ET for different contention environments and different * values. We assume ImpLimit = * for a query ET and ExpLimit &gt; 0 for an update ET. <p> Update ETs follow Poisson arrivals at rate ; each update ET accesses N exclusive locks (granules); each query ET accesses N q locks; and the database contains L total granules. Following the model in <ref> [5, 14, 15] </ref>, a query ET has N q + 2 states. State 0 represents the setup phase, including program fetch and message processing. After state 0, a query ET progresses to states 1, 2, : : :, N q .
Reference: [16] <author> J. N. Gray, R. A. Lorie, G. R. Putzolu, and I. L. Traiger, </author> <title> "Granularity of locks and degrees of consistency in a shared data base," </title> <booktitle> in Proc. of IFIP TC-2 Working Conference on Modelling in Data Base Management Systems (G. </booktitle> <editor> M. Nijssen, </editor> <publisher> ed.), </publisher> <pages> pp. 1-29, </pages> <publisher> North-Holland, </publisher> <year> 1976. </year>
Reference-contexts: The investigation of correctness criteria weaker than SR (typically allowing some particular kind of inconsistency) is not new. In this section, since the DC methods in this paper are application independent, we omit the semantics-dependent models and focus on semantics-independent work. Historically the earliest, Gray et al. <ref> [16] </ref> have defined 4 degrees of consistency, numbered from 0 to 3. Degree 0 only protects a transaction's uncommitted updates from being overwritten. Degree 1 offers the same protection for recoverable transactions, but a transaction may still read uncommitted values.
Reference: [17] <author> H. Garcia-Molina and G. Wiederhold, </author> <title> "Read-only transactions in a distributed database," </title> <journal> ACM Trans. on Database Systems, </journal> <volume> vol. 7, no. 2, </volume> <pages> pp. 209-234, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: However, DC methods assure the update ETs to be degree 3 to prevent from introducing any inconsistency into the database. Since ESR also applies to DC methods other than 2PLDC, it is more general than Gray's degrees of consistency, which are specific to locking. Garcia-Molina and Wiederhold <ref> [17] </ref> have introduced the weak consistency class of read-only transactions, defined as "each local query has a consistent view of the data, but the union of all schedules may be inconsistent." Their Wait-for List Centralized Algorithm (WLCA) gives each local query a consistent view of the data, but the union of
Reference: [18] <author> W. Du and A. K. Elmagarmid, </author> <title> "Quasi serializability: a correctness criterion for global con-currency control in interbase," </title> <booktitle> in Proc. of Very Large Data Bases, </booktitle> <pages> pp. 347-355, </pages> <year> 1989. </year>
Reference-contexts: WLCA is a particular algorithm for replicated distributed databases, while ESR is implemented by many DC methods for both centralized and distributed databases [7, 9]. Du and Elmagarmid <ref> [18] </ref> have introduced quasi-serializability (QSR) as a correctness criterion for federated databases. QSR may be seen as a generalization of read-only weak consistency, since QSR also maintains locally consistent views, but does not guarantee global SR. QSR allows both updates and queries that may be globally inconsistent, but locally consistent.
Reference: [19] <author> H. F. Korth and G. D. Speegle, </author> <title> "Formal model of correctness without serializability," </title> <booktitle> in Proc. of ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> pp. 379-386, </pages> <year> 1988. </year>
Reference-contexts: QSR allows both updates and queries that may be globally inconsistent, but locally consistent. In contrast, ESR allows bounded local inconsistency for query ETs. Korth and Speegle <ref> [19] </ref> have introduced a formal model that allows (nested) transactions to specify pre-conditions and post-conditions to be satisfied before and after the transaction execution, respectively. These input and output conditions are predicates defined over the states of data objects. Their model includes multiple versions, nested transactions, and the consistency predicates.
Reference: [20] <author> A. Sheth and M. Rusinkiewicz, </author> <title> "Management of interdependent data: Specifying dependency and consistency requirements," </title> <booktitle> in Proc. of Workshop on Management of Replicated Data, </booktitle> <pages> pp. 133-136, </pages> <year> 1990. </year>
Reference-contexts: Their consistency specification is on data object states, while the ESR in this paper allows users to specify the amount of inconsistency tolerated by the ETs. 6.2 Specification of Inconsistency Sheth et al. have proposed a taxonomy for interdependent data management <ref> [20, 21, 22] </ref>. They separate data consistency criteria into temporal and spatial dimensions. The temporal consistency has two kinds. Eventual consistency refers to the temporal constraints specified by identity connections introduced by Wiederhold and Qian [23], omitted here since they specifically define relationships between copies in replicated databases.
Reference: [21] <author> A. Sheth and P. Krishnamurthy, </author> <title> "Redundant data management in Bellcore and BCC databases," </title> <type> Tech. Rep. </type> <institution> TM-STS-015011/1, Bell Communications Research, </institution> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: Their consistency specification is on data object states, while the ESR in this paper allows users to specify the amount of inconsistency tolerated by the ETs. 6.2 Specification of Inconsistency Sheth et al. have proposed a taxonomy for interdependent data management <ref> [20, 21, 22] </ref>. They separate data consistency criteria into temporal and spatial dimensions. The temporal consistency has two kinds. Eventual consistency refers to the temporal constraints specified by identity connections introduced by Wiederhold and Qian [23], omitted here since they specifically define relationships between copies in replicated databases.
Reference: [22] <author> A. Sheth, Y. Leu, and A. K. Elmagarmid, </author> <title> "Maintaining consistency of interdependent data in multidatabases systems," </title> <type> Tech. Rep. </type> <institution> TM-STS-019409/1, Bell Communications Research, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: Their consistency specification is on data object states, while the ESR in this paper allows users to specify the amount of inconsistency tolerated by the ETs. 6.2 Specification of Inconsistency Sheth et al. have proposed a taxonomy for interdependent data management <ref> [20, 21, 22] </ref>. They separate data consistency criteria into temporal and spatial dimensions. The temporal consistency has two kinds. Eventual consistency refers to the temporal constraints specified by identity connections introduced by Wiederhold and Qian [23], omitted here since they specifically define relationships between copies in replicated databases.
Reference: [23] <author> G. Wiederhold and X. Qian, </author> <title> "Modeling asynchrony in distributed databases," </title> <booktitle> in Proc. of Int. Conf. on Data Engineering, </booktitle> <pages> pp. 246-250, </pages> <year> 1987. </year>
Reference-contexts: They separate data consistency criteria into temporal and spatial dimensions. The temporal consistency has two kinds. Eventual consistency refers to the temporal constraints specified by identity connections introduced by Wiederhold and Qian <ref> [23] </ref>, omitted here since they specifically define relationships between copies in replicated databases. Lagging consistency refers to asynchronously updated copies, in the same style of quasi-copies [24]. The spatial consistency criteria are divided into three cases.
Reference: [24] <author> R. Alonso, D. Barbara, and H. Garcia-Molina, </author> <title> "Data caching issues in an information retrieval systems," </title> <journal> ACM Trans. on Database Systems, </journal> <volume> vol. 15, no. 3, </volume> <pages> pp. 359-384, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: The temporal consistency has two kinds. Eventual consistency refers to the temporal constraints specified by identity connections introduced by Wiederhold and Qian [23], omitted here since they specifically define relationships between copies in replicated databases. Lagging consistency refers to asynchronously updated copies, in the same style of quasi-copies <ref> [24] </ref>. The spatial consistency criteria are divided into three cases. Inconsistency is controlled by limiting either (1) the number of data items changed asynchronously, (2) the data value changed asynchronously, or (3) the number of allowed asynchronous operations. <p> These are similar to the specification methods described in this paper, in particular Section 4. DC methods are clearly feasible implementations for these inconsistency specifications. Another specification approach is Controlled Inconsistency proposed by Barbara and Garcia 21 Molina [25], which generalizes their work on quasi-copies <ref> [24] </ref>. Controlled Inconsistency specifies arithmetic consistency constraints, similar to the data value limit in interdependent data management. They constrain updates to be safe operations, defined by some semantic correctness criteria appropriate to the application (enforced by transaction groups).
Reference: [25] <author> D. Barbara and H. Garcia-Molina, </author> <title> "The case for controlled inconsistency in replicated data," </title> <booktitle> in Proc. of Workshop on Management of Replicated Data, </booktitle> <pages> pp. 35-42, </pages> <year> 1990. </year> <month> 24 </month>
Reference-contexts: These are similar to the specification methods described in this paper, in particular Section 4. DC methods are clearly feasible implementations for these inconsistency specifications. Another specification approach is Controlled Inconsistency proposed by Barbara and Garcia 21 Molina <ref> [25] </ref>, which generalizes their work on quasi-copies [24]. Controlled Inconsistency specifies arithmetic consistency constraints, similar to the data value limit in interdependent data management. They constrain updates to be safe operations, defined by some semantic correctness criteria appropriate to the application (enforced by transaction groups).
References-found: 25

