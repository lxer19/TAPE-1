URL: ftp://ftp.cs.umass.edu/pub/lesser/whitehair-idp-upc-doc1.ps
Refering-URL: http://dis.cs.umass.edu/research/idp.html
Root-URL: 
Title: A Framework for the Analysis of Sophisticated Control in Interpretation Systems  
Author: Robert C. Whitehair Victor R. Lesser 
Note: This work was supported by the Office of Naval Research contract N00014-92-J-1450. The content does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred.  
Date: Number 93-53  
Pubnum: Technical Report  
Abstract: This paper introduces a framework for the analysis of sophisticated search control architectures in AI systems. The framework is based on two formalisms, the Interpretation Decision Problem (IDP), which models the characteristics and problem structure of a domain, and the UPC formalism, which provides a general model of control and problem solving. Using these models, the problem structures of disparate domains and the problem solving architectures constructed to exploit these structures can be viewed from a unified perspective where control and problem solving actions can be considered a single class of problem solving activity. Models built from this unified perspective offer advantages for describing, predicting and explaining the behavior of interpretation systems and for generalizing a specific problem solving architecture to other domains. Use of the IDP and UPC formalisms also supports the synthesis of new, more flexible problem solving architectures. Examples based on formalizing uncertainty and subproblem interaction are used to illustrate the power of the IDP/UPC framework. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Hans Berliner. </author> <title> The B* tree search algorithm: A best-first proof procedure. </title> <journal> Artificial Intelligence, </journal> <volume> 12 </volume> <pages> 23-40, </pages> <year> 1979. </year>
Reference-contexts: Figure 13 summarizes these topics and illustrates their relationships to each other relative to monotone and non-monotone domains, which are also defined and discussed in the following subsections. 3.1 Complex and Restricted Problem Domains Early work on the search paradigm was restricted to constrained domains such as game playing <ref> [1, 35] </ref>, and theorem proving [32]. The heuristic knowledge used in the search process was relatively limited. <p> For the most part, the pruning strategy formed the basis for algorithms such as AO fl , B fl , SSS fl , Alpha-Beta, and their generalizations <ref> [1, 25, 34, 38] </ref>. <p> In research projects involving restricted domains, the emphasis was on developing algorithms that enabled the problem 27 solver to prune paths based on intermediate problem solving results <ref> [1, 25, 34, 38] </ref>. Previously, Pearl [34] has shown that statistical properties of a problem solving technique, such as expected cost, can be determined from an analysis of the structure of a graphical representation of the search paths explored by the problem solver. <p> The total number of solutions generated for an arbitrary set I can be enormous. In general, the role of sophisticated control is to limit the size of I by implicitly enumerating as much of the search space as possible. This objective is illustrated in Fig. 21, from Berliner <ref> [1] </ref>. Figure 21.a represents the set I generated by grammar G without the use of sophisticated control techniques. In this example, problem solving is essentially exhaustive every possible derivation tree is generated and compared. Figure 21.b shows the effects of sophisticated control.
Reference: [2] <author> Norman Carver and Victor Lesser. </author> <title> The Evolution of Blackboard Control. </title> <journal> Expert Systems with Applications, </journal> <volume> 7(1), </volume> <year> 1991. </year> <note> Special issue on The Blackboard Paradigm and Its Applications. </note>
Reference-contexts: Such techniques include the focus of control mechanisms introduced in the Hearsay-II speech understanding system [12, 20] and the Distributed Vehicle Monitoring Testbed (DVMT) <ref> [2, 6, 8] </ref>, sophisticated control techniques such as goal processing [4, 5, 26, 27], and abstracting and approximating computational domain theories [9, 28].
Reference: [3] <author> Norman Carver and Victor R. Lesser. </author> <title> Planning for the control of an interpretation system. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 23(6), </volume> <year> 1993. </year> <note> Special Issue on Scheduling, Planning, and Control. </note>
Reference-contexts: Non-local control will also be referred to as sophisticated control. As a consequence of sophisticated control, the problem solving process takes on a recursive quality where the control issue becomes a search problem in itself <ref> [3] </ref>. The control component becomes a knowledge based mechanism that reasons and searches for the best operator. The control mechanism must reason about which states to evaluate, when to evaluate them, and which evaluation architectures to use.
Reference: [4] <author> Daniel D. Corkill and Victor R. Lesser. </author> <title> A goal-directed Hearsay-II architecture: Unifying data-directed and goal-directed control. </title> <type> Technical Report 81-15, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, Massachusetts 01003, </address> <month> June </month> <year> 1981. </year>
Reference-contexts: Intuitively, interpretation problems are tasks where a stream of input data is analyzed and an explanation is postulated as to what domain events occurred to generate the signal data the problem solver is attempting to interpret the signal data and determine what caused it <ref> [4, 12] </ref>. Given the success of some sophisticated interpretation problem solving techniques, there has been an obvious desire to understand the underlying principles and domain properties in order to generalize the techniques to other domains. <p> Such techniques include the focus of control mechanisms introduced in the Hearsay-II speech understanding system [12, 20] and the Distributed Vehicle Monitoring Testbed (DVMT) [2, 6, 8], sophisticated control techniques such as goal processing <ref> [4, 5, 26, 27] </ref>, and abstracting and approximating computational domain theories [9, 28]. This paper presents a formal framework for investigating and analyzing the relationship between the performance of search-based interpretation problem solving systems and the inherent properties, or structure, of problem domains in which they are applied. <p> These implicit enumeration strategies are often referred to as control or meta-reasoning strategies. (In this work, they will be referred to as meta-level, or abstract, operators.) The implementation of a control or meta-reasoning strategy will be referred to as a control architecture <ref> [4, 19, 18] </ref>. The analysis framework is intended to be used to analyze sophisticated control architectures (sophisticated control will be fully defined in Section 3.2) in complex domains (complex domains are defined in the following subsection). <p> Further problem solving actions are then applied to the goals and the original search states are considered to be connected and are no longer used to initiate problem solving activity. In numerous studies, goal processing has been shown to be an effective means for countering local redundancy and uncertainty <ref> [4, 5, 6] </ref>. In addition, goal processing has been shown to be effective in top-down processing algorithms where goals are used to constrain the actions of data-directed operators [4, 5, 6]. Intuitively, goals can be thought of as set descriptors. <p> In numerous studies, goal processing has been shown to be an effective means for countering local redundancy and uncertainty <ref> [4, 5, 6] </ref>. In addition, goal processing has been shown to be effective in top-down processing algorithms where goals are used to constrain the actions of data-directed operators [4, 5, 6]. Intuitively, goals can be thought of as set descriptors. <p> It may be possible to avoid some of the costs associated with these redundant activities by using a single goal operator. Furthermore, this form of goal processing allows a problem solver to reason about the goals themselves. Corkill and Lesser discuss the advantages of this capability in <ref> [4, 5, 6] </ref>, and this work is further extended in [26, 27, 9] 16 In these figures, and in the following text, the operator superscript notation represents the i th application of an operator. 80 Clearly, there are cases where this form of goal processing is not advantageous. <p> Specifically, it is becoming apparent that approximate processing [28, 9] and goal processing <ref> [4, 5, 26, 27] </ref> are forms of processing that can be generalized to many real-world domains. This general strategy is one in which a problem solver uses approximate processing operators to gain a comprehensive view of the data it must interpret, then uses this perspective to guide subsequent problem solving.
Reference: [5] <author> Daniel D. Corkill, Victor R. Lesser, and Eva Hudlick a. </author> <title> Unifying data-directed and goal-directed control: An example and experiments. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 143-147, </pages> <address> Pittsburgh, Pennsylvania, </address> <month> August </month> <year> 1982. </year>
Reference-contexts: Such techniques include the focus of control mechanisms introduced in the Hearsay-II speech understanding system [12, 20] and the Distributed Vehicle Monitoring Testbed (DVMT) [2, 6, 8], sophisticated control techniques such as goal processing <ref> [4, 5, 26, 27] </ref>, and abstracting and approximating computational domain theories [9, 28]. This paper presents a formal framework for investigating and analyzing the relationship between the performance of search-based interpretation problem solving systems and the inherent properties, or structure, of problem domains in which they are applied. <p> The IDP/UPC framework is particularly effective for analyzing problem solving systems, such as the extended Hearsay-II [12] blackboard model introduced by Lesser and Corkill, that integrate both top-down and bottom-up processing in a hierarchy of abstraction spaces <ref> [5, 6] </ref>. To our knowledge, no other analysis framework provides a perspective where different approaches to control can be analyzed as part of a unified domain theory. <p> Further problem solving actions are then applied to the goals and the original search states are considered to be connected and are no longer used to initiate problem solving activity. In numerous studies, goal processing has been shown to be an effective means for countering local redundancy and uncertainty <ref> [4, 5, 6] </ref>. In addition, goal processing has been shown to be effective in top-down processing algorithms where goals are used to constrain the actions of data-directed operators [4, 5, 6]. Intuitively, goals can be thought of as set descriptors. <p> In numerous studies, goal processing has been shown to be an effective means for countering local redundancy and uncertainty <ref> [4, 5, 6] </ref>. In addition, goal processing has been shown to be effective in top-down processing algorithms where goals are used to constrain the actions of data-directed operators [4, 5, 6]. Intuitively, goals can be thought of as set descriptors. <p> It may be possible to avoid some of the costs associated with these redundant activities by using a single goal operator. Furthermore, this form of goal processing allows a problem solver to reason about the goals themselves. Corkill and Lesser discuss the advantages of this capability in <ref> [4, 5, 6] </ref>, and this work is further extended in [26, 27, 9] 16 In these figures, and in the following text, the operator superscript notation represents the i th application of an operator. 80 Clearly, there are cases where this form of goal processing is not advantageous. <p> Specifically, it is becoming apparent that approximate processing [28, 9] and goal processing <ref> [4, 5, 26, 27] </ref> are forms of processing that can be generalized to many real-world domains. This general strategy is one in which a problem solver uses approximate processing operators to gain a comprehensive view of the data it must interpret, then uses this perspective to guide subsequent problem solving.
Reference: [6] <author> Daniel David Corkill. </author> <title> A Framework for Organizational Self-Design in Distributed Problem Solving Networks. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <month> February </month> <year> 1983. </year> <note> (Also published as Technical Report 82-33, </note> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, Massachusetts 01003, </address> <month> December </month> <year> 1982.). </year>
Reference-contexts: Such techniques include the focus of control mechanisms introduced in the Hearsay-II speech understanding system [12, 20] and the Distributed Vehicle Monitoring Testbed (DVMT) <ref> [2, 6, 8] </ref>, sophisticated control techniques such as goal processing [4, 5, 26, 27], and abstracting and approximating computational domain theories [9, 28]. <p> Given these rules, there is a great deal of ambiguity in this grammar. This grammar is based loosely on the vehicle tracking domain of the Distributed Vehicle Monitoring Testbed (DVMT) <ref> [6] </ref>. Figure 7 shows the same grammar modified to include a class of meta-level operators referred to as goal operators presented in [27]. This grammar is analyzed at length in Section 7. <p> The IDP/UPC framework is particularly effective for analyzing problem solving systems, such as the extended Hearsay-II [12] blackboard model introduced by Lesser and Corkill, that integrate both top-down and bottom-up processing in a hierarchy of abstraction spaces <ref> [5, 6] </ref>. To our knowledge, no other analysis framework provides a perspective where different approaches to control can be analyzed as part of a unified domain theory. <p> Quantitative analysis methods based on the IDP are presented in Section 6. Section 7 presents an example showing how the IDP model can be used to define problem structures in the Distributed Vehicle Monitoring Testbed (DVMT) <ref> [6] </ref>. The UPC formalism is introduced in Section 9. Section 10 presents experimental verification of the IDP/UPC framework. <p> Opportunistic search enables problem solving to proceed bottom-up from low-level data, top-down from general expectations and abstractions, or either top-down or bottom-up from intermediate results. The advantages of opportunistic processing have been demonstrated in a variety of projects including <ref> [12, 6, 10] </ref> The interpretation search spaces presented in this paper are all convergent search spaces. The general structures that define these spaces are defined in Section 4.2. <p> This technique has been used extensively in Operations Research, where it is referred to as Branch-and-Bound [33, 34, 25]. Top-down and bottom-up architectures have been effectively combined in a number of systems such as blackboard systems <ref> [12, 6] </ref>. In these systems, the top-down strategies, such as goal processing techniques, can be considered meta-control architectures because they use abstract or approximate states. <p> The grammar used in the example of redundancy will not be used in any other examples.) Redundancy is also shown in Fig. 35. This example depicts a stylized version of redundant processing that can occur in certain interpretation tasks such as the Distributed Vehicle Monitoring Testbed (DVMT) <ref> [6] </ref>. The problem solving strategy embodied in this example offers alternative paths for interpreting tracks of data, in this case the input uvwxyz. In Fig. 35.b, a track interpretation is formed incrementally by extending a partial interpretation one time unit. <p> For many interpretation domains, such as the DVMT <ref> [6] </ref> and Hearsay-II [12], monotonicity does not hold. A simple example illustrates how this might occur. Figure 41 shows two interpretations of signal data in a speech understanding domain. <p> Finally, the credibility assigned to terminal symbols is a random number generated with an expected value and variance equal to the credibility and variance passed to the feature list of the terminal symbol. 6.3 An Example of Using Feature Lists in a Grammar the domain in the DVMT <ref> [6] </ref>. This grammar has been used to generate problem instances such as the one shown in Fig. 48. The grammar generates a sequence of signal data, each with a time and location. <p> The analysis contained in this section will focus on abstractions used in sophisticated control mechanisms that have been implemented in the Distributed Vehicle Monitoring Testbed (DVMT) <ref> [6] </ref> and that have been implemented to exploit problem structures of non-monotonic domains. <p> Further problem solving actions are then applied to the goals and the original search states are considered to be connected and are no longer used to initiate problem solving activity. In numerous studies, goal processing has been shown to be an effective means for countering local redundancy and uncertainty <ref> [4, 5, 6] </ref>. In addition, goal processing has been shown to be effective in top-down processing algorithms where goals are used to constrain the actions of data-directed operators [4, 5, 6]. Intuitively, goals can be thought of as set descriptors. <p> In numerous studies, goal processing has been shown to be an effective means for countering local redundancy and uncertainty <ref> [4, 5, 6] </ref>. In addition, goal processing has been shown to be effective in top-down processing algorithms where goals are used to constrain the actions of data-directed operators [4, 5, 6]. Intuitively, goals can be thought of as set descriptors. <p> It may be possible to avoid some of the costs associated with these redundant activities by using a single goal operator. Furthermore, this form of goal processing allows a problem solver to reason about the goals themselves. Corkill and Lesser discuss the advantages of this capability in <ref> [4, 5, 6] </ref>, and this work is further extended in [26, 27, 9] 16 In these figures, and in the following text, the operator superscript notation represents the i th application of an operator. 80 Clearly, there are cases where this form of goal processing is not advantageous. <p> 1 6 ) + cost (op 2 6 ) + cost (op 3 6 ) + cost (op 4 6 ). (For the sake of simplicity, the cost of merging identical states will be ignored.) Analysis of these costs will use models similar to the cost models from the DVMT <ref> [6] </ref>. In the DVMT, the cost of an operator application can be approximated by a constant factor plus a function of the number of inputs and the number of outputs. <p> Though these figures are approximations, they are representative of the costs of DVMT search operators. The input component, n, of Equation 7.2 reflects the cost of retrieving data from the blackboard and the combinatorial nature of the reasoning processes used by DVMT operators <ref> [9, 6] </ref>. The output component, m, reflects the cost of writing data to the blackboard. In this simple example, goal processing has clear performance advantages. However, it is still unclear as to when, in general, goal processing is effective and when it is detrimental to performance. <p> This work was specific to one aspect of goal processing in the DVMT domain, and left open questions regarding the general properties of domains where goal processing is useful. More specifically, this analysis did not consider the potential benefits of the subgoaling mechanisms described in <ref> [6] </ref>. In the example presented in this section, the principle difference in cost can be attributed to the fact that without goal processing, op 6 had to be applied four times to connect the low-level states. <p> In the DVMT, operators that extend goals are pruned under certain conditions. Furthermore, such pruning is done often enough that it is advantageous to use goal processing as described in <ref> [6] </ref>. 8 A Basis for Analysis An Optimal Objective Strateg y As discussed in Section 2, the analysis paradigms supported by the IDP/UPC framework all involve the use of four elements: a problem's structure and a problem solver's objective strategy, control architecture, and performance level (or behavior). <p> This extension was intended to approximate a grammar representative of a vehicle tracking domain such as the domain associated with the DVMT <ref> [6] </ref> For each of the SNTs of G 0 , M, O, A, B, N, we added nonterminals of the form A1, A2, A3, A4, A5 where the symbol could be any one of the SNTs and the number indicates the number of time-locations in a vehicle track.
Reference: [7] <author> Randall Davis. </author> <title> Meta-rules: Reasoning about control. </title> <journal> Artificial Intelligence, </journal> <volume> 15 </volume> <pages> 179-222, </pages> <year> 1980. </year>
Reference-contexts: 1 Introduction Many research projects have shown that sophisticated 1 control and problem solving techniques, such as meta-level processing, can be applied in complex problem solving domains to improve system performance <ref> [7, 16, 17, 21, 37, 39] </ref>. However, for many of these techniques, there are open issues regarding why the technique works, how to use a particular technique properly in its original domain, and how to generalize the technique to other domains.
Reference: [8] <author> Keith Decker, Marty Humphrey, and Victor Lesser. </author> <title> Experimenting with control in the DVMT. </title> <type> Technical Report 89-00, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, Massachusetts 01003, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: Such techniques include the focus of control mechanisms introduced in the Hearsay-II speech understanding system [12, 20] and the Distributed Vehicle Monitoring Testbed (DVMT) <ref> [2, 6, 8] </ref>, sophisticated control techniques such as goal processing [4, 5, 26, 27], and abstracting and approximating computational domain theories [9, 28]. <p> For example, in the DVMT, an analysis indicated that goal processing is not always an effective tool [26, 27]. Subsequent work exploited this observation and resulted in significant performance improvements <ref> [8] </ref>. This work was specific to one aspect of goal processing in the DVMT domain, and left open questions regarding the general properties of domains where goal processing is useful. More specifically, this analysis did not consider the potential benefits of the subgoaling mechanisms described in [6].
Reference: [9] <author> Keith S. Decker, Victor R. Lesser, and Robert C. Whitehair. </author> <title> Extending a blackboard architecture for approximate processing. </title> <journal> The Journal of Real-Time Systems, </journal> 2(1/2):47-79, 1990. Also COINS TR-89-115. 
Reference-contexts: Such techniques include the focus of control mechanisms introduced in the Hearsay-II speech understanding system [12, 20] and the Distributed Vehicle Monitoring Testbed (DVMT) [2, 6, 8], sophisticated control techniques such as goal processing [4, 5, 26, 27], and abstracting and approximating computational domain theories <ref> [9, 28] </ref>. This paper presents a formal framework for investigating and analyzing the relationship between the performance of search-based interpretation problem solving systems and the inherent properties, or structure, of problem domains in which they are applied. <p> In the IDP/UPC framework, interacting subproblem structures will form the basis for designing, implementing, and analyzing meta-level operations. In particular, relationships between states will be explicitly represented with abstract or approximate states. Recent studies 55 <ref> [26, 27, 9] </ref> have demonstrated that knowledge of a problem's structure can be derived by reasoning about interacting subproblems and that this knowledge can be used to more effectively control problem solving activities. <p> For example, some interacting subproblem structures can be used to dynamically implement hierarchical problem solving strategies, or to implement efficient pruning techniques such as the inhibition of redundant processing <ref> [10, 26, 9] </ref>. In general, control architectures such as the hierarchical problem solving strategies used in [12] and [11, 10, 27] can be modeled by transforming an IDP problem representation to include abstractions and approximations used in control actions in the form of meta-operators. <p> Furthermore, this form of goal processing allows a problem solver to reason about the goals themselves. Corkill and Lesser discuss the advantages of this capability in [4, 5, 6], and this work is further extended in <ref> [26, 27, 9] </ref> 16 In these figures, and in the following text, the operator superscript notation represents the i th application of an operator. 80 Clearly, there are cases where this form of goal processing is not advantageous. <p> Though these figures are approximations, they are representative of the costs of DVMT search operators. The input component, n, of Equation 7.2 reflects the cost of retrieving data from the blackboard and the combinatorial nature of the reasoning processes used by DVMT operators <ref> [9, 6] </ref>. The output component, m, reflects the cost of writing data to the blackboard. In this simple example, goal processing has clear performance advantages. However, it is still unclear as to when, in general, goal processing is effective and when it is detrimental to performance. <p> Section 11.2 presents a formal definition of projection space extensions to UPC models. 11.1 Background The effort to formally incorporate notions of projected or abstracted search spaces into the traditional model is based on Approximate Processing concepts described in <ref> [12, 28, 29, 9] </ref> and on goal processing concepts described in [27]. Approximate processing is based on exploiting the structure of a search space to form abstractions of the space with well understood effects. <p> The results of searching the abstract space can be mapped back to the original space and used to enhance problem solving in that space. Experiments 114 with approximate processing <ref> [29, 9] </ref> showed that significant efficiencies can be gained with careful exploitation of approximate processing mechanisms. <p> In particular, through experimentation and analysis, we are finding many commonalities between disparate domains that seem to indicate that certain control architectures can be used to form a basis for a very powerful and general form of problem solving. Specifically, it is becoming apparent that approximate processing <ref> [28, 9] </ref> and goal processing [4, 5, 26, 27] are forms of processing that can be generalized to many real-world domains. <p> This method is applicable for domains that admit satisficing solutions [36]. In particular, approximate processing can be used when an acceptable solution can sacrifice precision, certainty or completeness <ref> [28, 29, 9] </ref>. 144
Reference: [10] <author> Edmund H. Durfee. </author> <title> A Unified Approach to Dynamic Coordination: Planning Actions and Interactions in a Distributed Problem Solving Network. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <month> September </month> <year> 1987. </year> <note> (Also published as Technical Report 87-84, </note> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA, September,1987.). </address>
Reference-contexts: Opportunistic search enables problem solving to proceed bottom-up from low-level data, top-down from general expectations and abstractions, or either top-down or bottom-up from intermediate results. The advantages of opportunistic processing have been demonstrated in a variety of projects including <ref> [12, 6, 10] </ref> The interpretation search spaces presented in this paper are all convergent search spaces. The general structures that define these spaces are defined in Section 4.2. <p> For example, some interacting subproblem structures can be used to dynamically implement hierarchical problem solving strategies, or to implement efficient pruning techniques such as the inhibition of redundant processing <ref> [10, 26, 9] </ref>. In general, control architectures such as the hierarchical problem solving strategies used in [12] and [11, 10, 27] can be modeled by transforming an IDP problem representation to include abstractions and approximations used in control actions in the form of meta-operators. <p> For example, some interacting subproblem structures can be used to dynamically implement hierarchical problem solving strategies, or to implement efficient pruning techniques such as the inhibition of redundant processing [10, 26, 9]. In general, control architectures such as the hierarchical problem solving strategies used in [12] and <ref> [11, 10, 27] </ref> can be modeled by transforming an IDP problem representation to include abstractions and approximations used in control actions in the form of meta-operators.
Reference: [11] <author> Edmund H. Durfee and Victor R. Lesser. </author> <title> Incremental planning to control a blackboard-based problem solver. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 58-64, </pages> <address> Philadelphia, Pennsylvania, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: For example, some interacting subproblem structures can be used to dynamically implement hierarchical problem solving strategies, or to implement efficient pruning techniques such as the inhibition of redundant processing [10, 26, 9]. In general, control architectures such as the hierarchical problem solving strategies used in [12] and <ref> [11, 10, 27] </ref> can be modeled by transforming an IDP problem representation to include abstractions and approximations used in control actions in the form of meta-operators.
Reference: [12] <author> Lee D. Erman, Frederick Hayes-Roth, Victor R. Lesser, and D. Raj Reddy. </author> <title> The Hearsay-II speech-understanding system: Integrating knowledge to resolve uncertainty. </title> <journal> Computing Surveys, </journal> <volume> 12(2) </volume> <pages> 213-253, </pages> <month> June </month> <year> 1980. </year> <month> 145 </month>
Reference-contexts: Intuitively, interpretation problems are tasks where a stream of input data is analyzed and an explanation is postulated as to what domain events occurred to generate the signal data the problem solver is attempting to interpret the signal data and determine what caused it <ref> [4, 12] </ref>. Given the success of some sophisticated interpretation problem solving techniques, there has been an obvious desire to understand the underlying principles and domain properties in order to generalize the techniques to other domains. <p> Given the success of some sophisticated interpretation problem solving techniques, there has been an obvious desire to understand the underlying principles and domain properties in order to generalize the techniques to other domains. Such techniques include the focus of control mechanisms introduced in the Hearsay-II speech understanding system <ref> [12, 20] </ref> and the Distributed Vehicle Monitoring Testbed (DVMT) [2, 6, 8], sophisticated control techniques such as goal processing [4, 5, 26, 27], and abstracting and approximating computational domain theories [9, 28]. <p> The IDP/UPC framework is particularly effective for analyzing problem solving systems, such as the extended Hearsay-II <ref> [12] </ref> blackboard model introduced by Lesser and Corkill, that integrate both top-down and bottom-up processing in a hierarchy of abstraction spaces [5, 6]. To our knowledge, no other analysis framework provides a perspective where different approaches to control can be analyzed as part of a unified domain theory. <p> Convergent search spaces are associated with a set of properties often referred to as opportunistic. Opportunistic processing was first introduced in work on the Hearsay-II project <ref> [12] </ref>. The object of opportunistic processing is to enable a system to be guided by the most recently discovered results and not by a requirement to satisfy specific subgoals. <p> Opportunistic search enables problem solving to proceed bottom-up from low-level data, top-down from general expectations and abstractions, or either top-down or bottom-up from intermediate results. The advantages of opportunistic processing have been demonstrated in a variety of projects including <ref> [12, 6, 10] </ref> The interpretation search spaces presented in this paper are all convergent search spaces. The general structures that define these spaces are defined in Section 4.2. <p> This technique has been used extensively in Operations Research, where it is referred to as Branch-and-Bound [33, 34, 25]. Top-down and bottom-up architectures have been effectively combined in a number of systems such as blackboard systems <ref> [12, 6] </ref>. In these systems, the top-down strategies, such as goal processing techniques, can be considered meta-control architectures because they use abstract or approximate states. <p> For example, some interacting subproblem structures can be used to dynamically implement hierarchical problem solving strategies, or to implement efficient pruning techniques such as the inhibition of redundant processing [10, 26, 9]. In general, control architectures such as the hierarchical problem solving strategies used in <ref> [12] </ref> and [11, 10, 27] can be modeled by transforming an IDP problem representation to include abstractions and approximations used in control actions in the form of meta-operators. <p> For many interpretation domains, such as the DVMT [6] and Hearsay-II <ref> [12] </ref>, monotonicity does not hold. A simple example illustrates how this might occur. Figure 41 shows two interpretations of signal data in a speech understanding domain. In this example, the partial interpretation tickled me with the weather is rated higher than the partial interpretation tickled me with a feather. <p> In many interpretation domains, such as speech recognition, components with high individual credibilities are often inconsistent with some or all of the other components of a partial interpretation <ref> [12] </ref>. For example, a particular word may be the best interpretation for a particular time-slice of data, but that word may not be consistent with the meaning (i.e., the semantics) of a partial interpretation the problem solver is trying to extend. <p> Section 11.2 presents a formal definition of projection space extensions to UPC models. 11.1 Background The effort to formally incorporate notions of projected or abstracted search spaces into the traditional model is based on Approximate Processing concepts described in <ref> [12, 28, 29, 9] </ref> and on goal processing concepts described in [27]. Approximate processing is based on exploiting the structure of a search space to form abstractions of the space with well understood effects.
Reference: [13] <author> Mark S. Fox. </author> <title> Constraint-directed Search: A Case Study of Job-Shop Scheduling. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1983. </year> <note> (Also published as Technical Report CMU-CS-83-161, </note> <institution> Computer Science Department, Carnegie-Mellon University, Pittsburgh, Pennsylvania 15213.). </institution>
Reference-contexts: Such a plan would have well defined tasks that correspond to base space operators as well as mechanisms for verifying that the plan was working. Constraint Directed Search A hybrid approach involves treating the projection space solution as a constraint network, such as the constraint networks defined by Fox <ref> [13] </ref>. Selective refinement 143 is used to map portions of the projection space solution back to the base search space, or sophisticated control based search can be used to achieve a similar result.
Reference: [14] <author> King Sun Fu. </author> <title> Syntactic Pattern Recognition and Applications. </title> <publisher> PH, </publisher> <year> 1982. </year>
Reference-contexts: In addition, the use of formal grammars and the associated graph structures as a basis for analyzing interpretation problems and for constructing problem solving systems is extensive. One of the more extensive uses of formal grammars is presented by Fu in <ref> [14] </ref>. Fu uses a representation he formalizes as a stochastic grammar that is virtually identical to component structure of the IDP/UPC framework. However, Fu's work differs from this work in several important ways. Most importantly, Fu's emphasis is on parsing.
Reference: [15] <author> Gerald Gazdar, Geoffrey K. Pullum, and Ivan A. Sag. </author> <title> Auxiliaries and related phenomena in a restrictive theory of grammar. </title> <booktitle> Language, </booktitle> <volume> 58(3) </volume> <pages> 591-638, </pages> <year> 1982. </year>
Reference-contexts: In particular, it is necessary to understand how the simulator generates problem instances and characteristics of the problem instances, such as credibility. approach is based on the notion of the feature list convention developed by Gazdar, et al. <ref> [15] </ref>. As shown in the figure, each symbol of the grammar has a feature list. In addition, each production rule is associated with a set of functions which take a feature list as an input. <p> The conditional probabilities and expected cost components of the UPC vectors are computed a priori. The domain characteristics that change from run to run are represented with the feature list convention <ref> [15] </ref>. Using this testbed, we have conducted two sets of verification/validation experiments using the grammars shown in Figures 64 and 65.
Reference: [16] <author> Michael R. Genesereth. </author> <title> An overview of meta-level architecture. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 119-124, </pages> <address> Washington, D.C., </address> <month> August </month> <year> 1983. </year>
Reference-contexts: 1 Introduction Many research projects have shown that sophisticated 1 control and problem solving techniques, such as meta-level processing, can be applied in complex problem solving domains to improve system performance <ref> [7, 16, 17, 21, 37, 39] </ref>. However, for many of these techniques, there are open issues regarding why the technique works, how to use a particular technique properly in its original domain, and how to generalize the technique to other domains.
Reference: [17] <author> Michael R. Genesereth and David E. Smith. </author> <title> Meta-level architecture. </title> <type> Technical report, </type> <institution> Computer Science Department, Stanford University, Stanford, </institution> <address> California 94305, </address> <month> December </month> <year> 1982. </year>
Reference-contexts: 1 Introduction Many research projects have shown that sophisticated 1 control and problem solving techniques, such as meta-level processing, can be applied in complex problem solving domains to improve system performance <ref> [7, 16, 17, 21, 37, 39] </ref>. However, for many of these techniques, there are open issues regarding why the technique works, how to use a particular technique properly in its original domain, and how to generalize the technique to other domains.
Reference: [18] <author> Barbara Hayes-Roth. </author> <title> A blackboard architecture for control. </title> <journal> Artificial Intelligence, </journal> <volume> 26(3) </volume> <pages> 251-321, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: These implicit enumeration strategies are often referred to as control or meta-reasoning strategies. (In this work, they will be referred to as meta-level, or abstract, operators.) The implementation of a control or meta-reasoning strategy will be referred to as a control architecture <ref> [4, 19, 18] </ref>. The analysis framework is intended to be used to analyze sophisticated control architectures (sophisticated control will be fully defined in Section 3.2) in complex domains (complex domains are defined in the following subsection).
Reference: [19] <author> Barbara Hayes-Roth and Frederick Hayes-Roth. </author> <title> A cognitive model of planning. </title> <journal> Cognitive Science, </journal> <volume> 3(4) </volume> <pages> 275-310, </pages> <month> October-December </month> <year> 1979. </year>
Reference-contexts: These implicit enumeration strategies are often referred to as control or meta-reasoning strategies. (In this work, they will be referred to as meta-level, or abstract, operators.) The implementation of a control or meta-reasoning strategy will be referred to as a control architecture <ref> [4, 19, 18] </ref>. The analysis framework is intended to be used to analyze sophisticated control architectures (sophisticated control will be fully defined in Section 3.2) in complex domains (complex domains are defined in the following subsection).
Reference: [20] <author> Frederick Hayes-Roth and Victor R. Lesser. </author> <title> Focus of attention in the Hearsay-II system. </title> <booktitle> In Proceedings of the Fifth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 27-35, </pages> <address> Tiblisi, Georgia, USSR, </address> <month> August </month> <year> 1977. </year>
Reference-contexts: Given the success of some sophisticated interpretation problem solving techniques, there has been an obvious desire to understand the underlying principles and domain properties in order to generalize the techniques to other domains. Such techniques include the focus of control mechanisms introduced in the Hearsay-II speech understanding system <ref> [12, 20] </ref> and the Distributed Vehicle Monitoring Testbed (DVMT) [2, 6, 8], sophisticated control techniques such as goal processing [4, 5, 26, 27], and abstracting and approximating computational domain theories [9, 28].
Reference: [21] <author> Eva Hudlick a and Victor R. Lesser. </author> <title> Meta-level control through fault detection and diagnosis. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 153-161, </pages> <address> Austin, Texas, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: 1 Introduction Many research projects have shown that sophisticated 1 control and problem solving techniques, such as meta-level processing, can be applied in complex problem solving domains to improve system performance <ref> [7, 16, 17, 21, 37, 39] </ref>. However, for many of these techniques, there are open issues regarding why the technique works, how to use a particular technique properly in its original domain, and how to generalize the technique to other domains.
Reference: [22] <author> Norman F. Carver III. </author> <title> Sophisticated Control for Interpretation: Planning to Resolve Sources of Uncertainty. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Interpretation is a form of constructive problem solving based on abductive inferencing <ref> [22] </ref>. Interpretation is similar to a closely related form of problem solving, classification, and to a more distant form of problem solving, parsing. <p> Another issue that we are addressing is associated with the analysis tools we define in this paper and their applicability to complex real world problems. We are addressing this issue by implementing an IDP grammar for the RESUN problem solving system <ref> [22] </ref>. We will use this implementation to 137 test the existing analysis tools and to derive new ones. One of the important problems is the use of infinite grammars needed to characterize real-world problem domains.
Reference: [23] <author> Craig A. Knoblock. </author> <title> Automatically Generating Abstractions for Problem Solving. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1991. </year> <note> (Also published as Technical Report CMU-CS-91-120, </note> <institution> School of Computer Science, Carnegie Mellon University.). </institution>
Reference-contexts: Such domains are characteristic of real-world domains such as signal interpretation, robotic audition, image processing, and natural language processing. 7.1 Goal Processing Goal processing is a form of hierarchical problem solving <ref> [23] </ref> that has been incorporated in the DVMT to enable a problem solver to reason about courses of action in ways that are independent of the means for instantiating the actions. After new search states are generated, partial constraints are applied to the states to generate meta-states, or goals.
Reference: [24] <author> Craig A. Knoblock. </author> <title> Search reduction in hierarchical problem solving. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 686-691, </pages> <address> San Diego, California, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Other researchers have reported analyses of the structures of abstract and approximate search spaces. For example, Knoblock discusses the maximum potential reduction in problem solving costs that can be achieved with the use of abstract and approximate search spaces <ref> [24] </ref>. However, his analysis does not explain how these reductions might be achieved and his techniques do not address the issues presented in Section 1. <p> Experiments 114 with approximate processing [29, 9] showed that significant efficiencies can be gained with careful exploitation of approximate processing mechanisms. This paradigm can be viewed as a form of hierarchical problem solving, such as that discussed by Newell [31], Minsky [30], and Knoblock <ref> [24] </ref>. 11.2 Formalizing Projection Spaces The extended UPC formalism is intended to explicitly represent characteristics of search spaces that are used implicitly in control architectures.
Reference: [25] <editor> Vipin Kumar and Laveen N. Kanal. </editor> <title> The CDP: A unifying formulation for heuristic search, dynamic programming, and branch-and-bound. </title> <editor> In L. Kanal and V. Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence, Symbolic computation, chapter 1, </booktitle> <pages> pages 1-27. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: For the most part, the pruning strategy formed the basis for algorithms such as AO fl , B fl , SSS fl , Alpha-Beta, and their generalizations <ref> [1, 25, 34, 38] </ref>. <p> In research projects involving restricted domains, the emphasis was on developing algorithms that enabled the problem 27 solver to prune paths based on intermediate problem solving results <ref> [1, 25, 34, 38] </ref>. Previously, Pearl [34] has shown that statistical properties of a problem solving technique, such as expected cost, can be determined from an analysis of the structure of a graphical representation of the search paths explored by the problem solver. <p> Extended studies and analyses of the work on search techniques for restricted domains have resulted in the identification of a taxonomy of search procedures by Kanal and Kumar <ref> [25] </ref>. Each of the procedural categories specified by Kanal and Kumar is defined in terms of the fundamental structure of the search spaces to which the procedures included in the category can be successfully applied. <p> Kanal and Kumar's taxonomy links heuristic search techniques with problem solving techniques that are more closely associated with operations research (OR). This work has shown that basic AI search techniques can be classified as belonging to a subcategory of either branch-and-bound procedures or dynamic programming procedures <ref> [25] </ref>. More recently, AI problem solving has ventured into complex domains such as speech recognition, natural language processing, vision processing, pattern recognition, etc., that differ from the earlier, more restricted domains in two important ways. <p> These structures do not exist in the more complex domains and most of the search 28 and pruning techniques that were developed in earlier work, such as AO fl , B fl , SSS fl , Alpha-Beta, and their generalizations, are not applicable <ref> [25] </ref>. There are a variety of reasons for this. The most significant is that complex problem formulations do not exhibit monotone properties and structures. <p> Kanal and Kumar's taxonomy only classifies problems that are, minimally, monotone. As discussed by Kanal and Kumar <ref> [25] </ref>, in order for AO fl , alpha-beta, B fl , SSS fl and their generalizations to be applicable to a given domain, the problem formulation for that domain must exhibit monotonic properties. <p> This approach is similar to that used in the Composite Decision Process (CDP) model of Kanal and Kumar <ref> [25] </ref>. In these situations, interpretations take the form of derivation trees of X and the constructive search operators used in interpretation problems are viewed as production rules of G. G, therefore, defines how interpretations are decomposed. <p> These computations can be stored and retrieved at run-time to reduce the cost of computing UPC values dynamically. 42 4.4 Interpretation Problem Solving and Formal Problem Solving Paradigms As discussed by Kanal and Kumar in <ref> [25] </ref>, formal problem solving paradigms that are based on search techniques can be divided into two general categories, either top-down or bottom-up. The IDP/UPC framework is consistent with this and supports the analysis of control architectures from either a top-down or a bottom-up perspective. <p> Using bottom-up control architectures, rules of the grammar are inversely applied and more comprehensive derivation trees are generated and represented as new states. Thus, larger problems are solved starting with their smaller components. This approach is used in many search procedures and is the basis for dynamic programming <ref> [33, 34, 25] </ref>. The examples in Section 4.1 are representative of bottom-up problem solving in interpretation problems. Using top-down control architectures, some representation of the total set of interpretations is repeatedly partitioned and pruned. <p> After each partition, all members of the partition are deleted for which it can be shown that, even after elimination, the most credible interpretation is an element of one of the remaining partitions. This technique has been used extensively in Operations Research, where it is referred to as Branch-and-Bound <ref> [33, 34, 25] </ref>. Top-down and bottom-up architectures have been effectively combined in a number of systems such as blackboard systems [12, 6]. In these systems, the top-down strategies, such as goal processing techniques, can be considered meta-control architectures because they use abstract or approximate states. <p> was kept intentionally simple to illustrate the representation used in the IDP formalism, subsequent examples in this and future papers will be based on very similar analysis. 5.5 Non-Monotonicity and Bounding Functions Monotonicity is a property of problem structures that has been used to construct many important search control architectures <ref> [25] </ref>. Though there is no guarantee that a monotone problem can be solved efficiently, most existing efficient control architectures such as AO fl , alpha-beta, B fl , SSS fl and their generalizations are only applicable in monotone domains. <p> As a consequence, the resulting extension may have an arbitrarily low-rated credibility depending on how ridiculous it is. 5.5.2 Bounding Functions In certain monotone domains, problem solving is simplified by the fact that optimal subproblem solutions are guaranteed to be components of the optimal solution <ref> [25] </ref>. In non-monotone interpretation problems, this guarantee obviously does not hold. However, based on the problem structure defined by a domain's characteristic grammar, G, and evaluation function f , certain bounding functions can be defined. <p> Finally, in developing the IDP/UPC framework, we have developed a better understanding of sophisticated problem solving and its relationship to other forms of problem solving. The IDP/UPC framework was designed intentionally as an extension to the framework introduced by Kanal and Kumar <ref> [25] </ref> and used to develop a taxonomy of search-based problem solving techniques.
Reference: [26] <author> V. R. Lesser, D. D. Corkill, R. C. Whitehair, and J. A. Hernandez. </author> <title> Focus of control through goal relationships. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Such techniques include the focus of control mechanisms introduced in the Hearsay-II speech understanding system [12, 20] and the Distributed Vehicle Monitoring Testbed (DVMT) [2, 6, 8], sophisticated control techniques such as goal processing <ref> [4, 5, 26, 27] </ref>, and abstracting and approximating computational domain theories [9, 28]. This paper presents a formal framework for investigating and analyzing the relationship between the performance of search-based interpretation problem solving systems and the inherent properties, or structure, of problem domains in which they are applied. <p> Redundancy provides a problem solver with flexibility in choosing problem-solving activities but also allows results to be rederived using alternative paths, possibly without recognizing the redundancy until the last step. Various studies have shown that the cost associated with redundancy can be substantial <ref> [26] </ref>. Figure 34 shows an example of redundancy. (Note that this grammar is not the same as the grammar that is being used to illustrate other phenomena. The grammar used in the example of redundancy will not be used in any other examples.) Redundancy is also shown in Fig. 35. <p> In the IDP/UPC framework, interacting subproblem structures will form the basis for designing, implementing, and analyzing meta-level operations. In particular, relationships between states will be explicitly represented with abstract or approximate states. Recent studies 55 <ref> [26, 27, 9] </ref> have demonstrated that knowledge of a problem's structure can be derived by reasoning about interacting subproblems and that this knowledge can be used to more effectively control problem solving activities. <p> For example, some interacting subproblem structures can be used to dynamically implement hierarchical problem solving strategies, or to implement efficient pruning techniques such as the inhibition of redundant processing <ref> [10, 26, 9] </ref>. In general, control architectures such as the hierarchical problem solving strategies used in [12] and [11, 10, 27] can be modeled by transforming an IDP problem representation to include abstractions and approximations used in control actions in the form of meta-operators. <p> Definition 5.15 Subproblem Relationships and Interactions If a function, r, is defined in terms of the Component Sets and/or Result Sets of two or more subproblems, we say that r defines a relationship (or interaction) between the subproblems. In <ref> [26] </ref> a number of relationships were defined and their use in controlling problem solving was discussed. <p> Furthermore, this form of goal processing allows a problem solver to reason about the goals themselves. Corkill and Lesser discuss the advantages of this capability in [4, 5, 6], and this work is further extended in <ref> [26, 27, 9] </ref> 16 In these figures, and in the following text, the operator superscript notation represents the i th application of an operator. 80 Clearly, there are cases where this form of goal processing is not advantageous. <p> In this simple example, goal processing has clear performance advantages. However, it is still unclear as to when, in general, goal processing is effective and when it is detrimental to performance. For example, in the DVMT, an analysis indicated that goal processing is not always an effective tool <ref> [26, 27] </ref>. Subsequent work exploited this observation and resulted in significant performance improvements [8]. This work was specific to one aspect of goal processing in the DVMT domain, and left open questions regarding the general properties of domains where goal processing is useful. <p> Specifically, it is becoming apparent that approximate processing [28, 9] and goal processing <ref> [4, 5, 26, 27] </ref> are forms of processing that can be generalized to many real-world domains. This general strategy is one in which a problem solver uses approximate processing operators to gain a comprehensive view of the data it must interpret, then uses this perspective to guide subsequent problem solving.
Reference: [27] <author> V. R. Lesser, R. C. Whitehair, D. D. Corkill, and J. A. Hernandez. </author> <title> Goal relationships and their use in a blackboard architecture. </title> <editor> In V. Jagannathan, Rajendra Dodhiawala, and Lawrence Baum, editors, </editor> <booktitle> Blackboard Architectures and Applications, </booktitle> <pages> pages 9-26. </pages> <publisher> Academic Press, Inc., </publisher> <year> 1989. </year> <month> 146 </month>
Reference-contexts: Such techniques include the focus of control mechanisms introduced in the Hearsay-II speech understanding system [12, 20] and the Distributed Vehicle Monitoring Testbed (DVMT) [2, 6, 8], sophisticated control techniques such as goal processing <ref> [4, 5, 26, 27] </ref>, and abstracting and approximating computational domain theories [9, 28]. This paper presents a formal framework for investigating and analyzing the relationship between the performance of search-based interpretation problem solving systems and the inherent properties, or structure, of problem domains in which they are applied. <p> In contrast, sophisticated control architectures evaluate problem solving actions by selectively applying a process that examines a search path's relationship with other, possibly interacting, search paths <ref> [27] </ref>. (Relationships will typically be based on the distribution of domain events and will be statistical in nature. <p> This grammar is based loosely on the vehicle tracking domain of the Distributed Vehicle Monitoring Testbed (DVMT) [6]. Figure 7 shows the same grammar modified to include a class of meta-level operators referred to as goal operators presented in <ref> [27] </ref>. This grammar is analyzed at length in Section 7. <p> In the case of cooperating paths, potential solutions to a subproblem impose constraints on sibling subproblems. These ideas are very important in the IDP/UPC framework and a more extensive and formal description is discussed in Section 5 and in previous work of mine in <ref> [27] </ref>. In choosing a path for extension in an interrelated search space, a sophisticated control algorithm seeks to optimize the amount of constraint that is generated by the extension both locally, for the path being extended, and more globally, for other paths related to the path being extended. <p> In the IDP/UPC framework, interacting subproblem structures will form the basis for designing, implementing, and analyzing meta-level operations. In particular, relationships between states will be explicitly represented with abstract or approximate states. Recent studies 55 <ref> [26, 27, 9] </ref> have demonstrated that knowledge of a problem's structure can be derived by reasoning about interacting subproblems and that this knowledge can be used to more effectively control problem solving activities. <p> For example, some interacting subproblem structures can be used to dynamically implement hierarchical problem solving strategies, or to implement efficient pruning techniques such as the inhibition of redundant processing [10, 26, 9]. In general, control architectures such as the hierarchical problem solving strategies used in [12] and <ref> [11, 10, 27] </ref> can be modeled by transforming an IDP problem representation to include abstractions and approximations used in control actions in the form of meta-operators. <p> Though the majority of discussion related to interacting subproblems is contained in later sections and other publications <ref> [27] </ref>, their definitions and representations are included here for consistency. 5.4.1 Defining Interacting Subproblems Formal definitions of interacting subproblems will be based on the following definitions of component set and result set. <p> Furthermore, this form of goal processing allows a problem solver to reason about the goals themselves. Corkill and Lesser discuss the advantages of this capability in [4, 5, 6], and this work is further extended in <ref> [26, 27, 9] </ref> 16 In these figures, and in the following text, the operator superscript notation represents the i th application of an operator. 80 Clearly, there are cases where this form of goal processing is not advantageous. <p> In this simple example, goal processing has clear performance advantages. However, it is still unclear as to when, in general, goal processing is effective and when it is detrimental to performance. For example, in the DVMT, an analysis indicated that goal processing is not always an effective tool <ref> [26, 27] </ref>. Subsequent work exploited this observation and resulted in significant performance improvements [8]. This work was specific to one aspect of goal processing in the DVMT domain, and left open questions regarding the general properties of domains where goal processing is useful. <p> Section 11.2 presents a formal definition of projection space extensions to UPC models. 11.1 Background The effort to formally incorporate notions of projected or abstracted search spaces into the traditional model is based on Approximate Processing concepts described in [12, 28, 29, 9] and on goal processing concepts described in <ref> [27] </ref>. Approximate processing is based on exploiting the structure of a search space to form abstractions of the space with well understood effects. A problem solver capable of exploiting approximate processing has access to simplified operators that it can use to search the abstracted version of a given search space. <p> Specifically, it is becoming apparent that approximate processing [28, 9] and goal processing <ref> [4, 5, 26, 27] </ref> are forms of processing that can be generalized to many real-world domains. This general strategy is one in which a problem solver uses approximate processing operators to gain a comprehensive view of the data it must interpret, then uses this perspective to guide subsequent problem solving.
Reference: [28] <author> Victor R. Lesser and Jasmina Pavlin. </author> <title> Performing approximate processing to address real-time constraints. </title> <type> COINS Technical Report 87-126, </type> <institution> University of Massachusetts, </institution> <year> 1988. </year>
Reference-contexts: Such techniques include the focus of control mechanisms introduced in the Hearsay-II speech understanding system [12, 20] and the Distributed Vehicle Monitoring Testbed (DVMT) [2, 6, 8], sophisticated control techniques such as goal processing [4, 5, 26, 27], and abstracting and approximating computational domain theories <ref> [9, 28] </ref>. This paper presents a formal framework for investigating and analyzing the relationship between the performance of search-based interpretation problem solving systems and the inherent properties, or structure, of problem domains in which they are applied. <p> Section 11.2 presents a formal definition of projection space extensions to UPC models. 11.1 Background The effort to formally incorporate notions of projected or abstracted search spaces into the traditional model is based on Approximate Processing concepts described in <ref> [12, 28, 29, 9] </ref> and on goal processing concepts described in [27]. Approximate processing is based on exploiting the structure of a search space to form abstractions of the space with well understood effects. <p> In particular, through experimentation and analysis, we are finding many commonalities between disparate domains that seem to indicate that certain control architectures can be used to form a basis for a very powerful and general form of problem solving. Specifically, it is becoming apparent that approximate processing <ref> [28, 9] </ref> and goal processing [4, 5, 26, 27] are forms of processing that can be generalized to many real-world domains. <p> This method is applicable for domains that admit satisficing solutions [36]. In particular, approximate processing can be used when an acceptable solution can sacrifice precision, certainty or completeness <ref> [28, 29, 9] </ref>. 144
Reference: [29] <author> Victor R. Lesser, Jasmina Pavlin, and Edmund Durfee. </author> <title> Approximate processing in real-time problem solving. </title> <journal> AI Magazine, </journal> <volume> 9(1) </volume> <pages> 49-61, </pages> <month> Spring </month> <year> 1988. </year>
Reference-contexts: Section 11.2 presents a formal definition of projection space extensions to UPC models. 11.1 Background The effort to formally incorporate notions of projected or abstracted search spaces into the traditional model is based on Approximate Processing concepts described in <ref> [12, 28, 29, 9] </ref> and on goal processing concepts described in [27]. Approximate processing is based on exploiting the structure of a search space to form abstractions of the space with well understood effects. <p> The results of searching the abstract space can be mapped back to the original space and used to enhance problem solving in that space. Experiments 114 with approximate processing <ref> [29, 9] </ref> showed that significant efficiencies can be gained with careful exploitation of approximate processing mechanisms. <p> This method is applicable for domains that admit satisficing solutions [36]. In particular, approximate processing can be used when an acceptable solution can sacrifice precision, certainty or completeness <ref> [28, 29, 9] </ref>. 144
Reference: [30] <author> Marvin Minsky. </author> <title> Steps towards artificial intelligence. </title> <editor> In E. A. Fiegenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought, </booktitle> <pages> pages 406-450. </pages> <publisher> McGraw-Hill, </publisher> <year> 1963. </year>
Reference-contexts: Experiments 114 with approximate processing [29, 9] showed that significant efficiencies can be gained with careful exploitation of approximate processing mechanisms. This paradigm can be viewed as a form of hierarchical problem solving, such as that discussed by Newell [31], Minsky <ref> [30] </ref>, and Knoblock [24]. 11.2 Formalizing Projection Spaces The extended UPC formalism is intended to explicitly represent characteristics of search spaces that are used implicitly in control architectures.
Reference: [31] <author> A. Newell, J. C. Shaw, and H. a. Simon. </author> <title> The process of creative thinking. </title> <booktitle> In Contemporary Approaches to Creative Thinking, </booktitle> <pages> pages 63-119. </pages> <publisher> Atherton Press, </publisher> <address> New York, </address> <year> 1962. </year>
Reference-contexts: Experiments 114 with approximate processing [29, 9] showed that significant efficiencies can be gained with careful exploitation of approximate processing mechanisms. This paradigm can be viewed as a form of hierarchical problem solving, such as that discussed by Newell <ref> [31] </ref>, Minsky [30], and Knoblock [24]. 11.2 Formalizing Projection Spaces The extended UPC formalism is intended to explicitly represent characteristics of search spaces that are used implicitly in control architectures.
Reference: [32] <author> A. Newell, J. C. Shaw, and H. a. Simon. </author> <title> Empirical explorations with the logic theory machine: A case history of heuristics. </title> <editor> In E. A. Fiegenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought, </booktitle> <pages> pages 109-133. </pages> <publisher> McGraw-Hill, </publisher> <year> 1963. </year>
Reference-contexts: topics and illustrates their relationships to each other relative to monotone and non-monotone domains, which are also defined and discussed in the following subsections. 3.1 Complex and Restricted Problem Domains Early work on the search paradigm was restricted to constrained domains such as game playing [1, 35], and theorem proving <ref> [32] </ref>. The heuristic knowledge used in the search process was relatively limited. This can be expressed formally by saying that, for a restricted problem domain such as game playing or logic, cost (operator i ) = O (1), i.e., the cost of applying an operator is a constant value.
Reference: [33] <author> Christos H. Papadimitriou and Kenneth Steiglitz. </author> <title> Combinatorial Optimization: Algorithms and Complexity. </title> <publisher> Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: The objective strategy of a problem solver can be thought of as being analogous to the objective function of problem solving strategies such as the simplex algorithm <ref> [33] </ref>. In a typical analysis situation, the object of consideration will be the control architecture the algorithm (s) used by a problem solver to choose its next problem solving action. Control architectures will be expressed in terms of the problem structure. <p> Using bottom-up control architectures, rules of the grammar are inversely applied and more comprehensive derivation trees are generated and represented as new states. Thus, larger problems are solved starting with their smaller components. This approach is used in many search procedures and is the basis for dynamic programming <ref> [33, 34, 25] </ref>. The examples in Section 4.1 are representative of bottom-up problem solving in interpretation problems. Using top-down control architectures, some representation of the total set of interpretations is repeatedly partitioned and pruned. <p> After each partition, all members of the partition are deleted for which it can be shown that, even after elimination, the most credible interpretation is an element of one of the remaining partitions. This technique has been used extensively in Operations Research, where it is referred to as Branch-and-Bound <ref> [33, 34, 25] </ref>. Top-down and bottom-up architectures have been effectively combined in a number of systems such as blackboard systems [12, 6]. In these systems, the top-down strategies, such as goal processing techniques, can be considered meta-control architectures because they use abstract or approximate states.
Reference: [34] <author> Judea Pearl. </author> <title> Heuristics: Intelligent Search Strategies for Computer Problem Solving. </title> <publisher> Addison-Wesley, </publisher> <address> first edition, </address> <year> 1984. </year>
Reference-contexts: Viewing interpretation problems as discrete optimization problems sets the analysis framework apart from previous analysis techniques that are used to analyze problem solving in domains where the objective is to find the shortest, or lowest-cost search path, the highest-rated solution, or a solution path to a winning position <ref> [34] </ref>. The analysis framework formalizes the representation of problem structures and control architectures in such a way that the following issues can be addressed; 1. Is it possible to formally define sophisticated control architectures and complex problem solving domains? 2. <p> For the most part, the pruning strategy formed the basis for algorithms such as AO fl , B fl , SSS fl , Alpha-Beta, and their generalizations <ref> [1, 25, 34, 38] </ref>. <p> In research projects involving restricted domains, the emphasis was on developing algorithms that enabled the problem 27 solver to prune paths based on intermediate problem solving results <ref> [1, 25, 34, 38] </ref>. Previously, Pearl [34] has shown that statistical properties of a problem solving technique, such as expected cost, can be determined from an analysis of the structure of a graphical representation of the search paths explored by the problem solver. <p> In research projects involving restricted domains, the emphasis was on developing algorithms that enabled the problem 27 solver to prune paths based on intermediate problem solving results [1, 25, 34, 38]. Previously, Pearl <ref> [34] </ref> has shown that statistical properties of a problem solving technique, such as expected cost, can be determined from an analysis of the structure of a graphical representation of the search paths explored by the problem solver. <p> In particular, Pearl examines the effects of pruning operators and heuristics for ordering the application of operators in various game playing domains and in other restricted domains. In contrast to the analysis techniques, the analysis techniques described by Pearl in <ref> [34] </ref> do not take into consideration dynamic subproblem interactions or the long-term effects of an action. Extended studies and analyses of the work on search techniques for restricted domains have resulted in the identification of a taxonomy of search procedures by Kanal and Kumar [25]. <p> Using bottom-up control architectures, rules of the grammar are inversely applied and more comprehensive derivation trees are generated and represented as new states. Thus, larger problems are solved starting with their smaller components. This approach is used in many search procedures and is the basis for dynamic programming <ref> [33, 34, 25] </ref>. The examples in Section 4.1 are representative of bottom-up problem solving in interpretation problems. Using top-down control architectures, some representation of the total set of interpretations is repeatedly partitioned and pruned. <p> After each partition, all members of the partition are deleted for which it can be shown that, even after elimination, the most credible interpretation is an element of one of the remaining partitions. This technique has been used extensively in Operations Research, where it is referred to as Branch-and-Bound <ref> [33, 34, 25] </ref>. Top-down and bottom-up architectures have been effectively combined in a number of systems such as blackboard systems [12, 6]. In these systems, the top-down strategies, such as goal processing techniques, can be considered meta-control architectures because they use abstract or approximate states.
Reference: [35] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <editor> In E. A. Fiegenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought, </booktitle> <pages> pages 71-105. </pages> <publisher> McGraw-Hill, </publisher> <year> 1963. </year>
Reference-contexts: Figure 13 summarizes these topics and illustrates their relationships to each other relative to monotone and non-monotone domains, which are also defined and discussed in the following subsections. 3.1 Complex and Restricted Problem Domains Early work on the search paradigm was restricted to constrained domains such as game playing <ref> [1, 35] </ref>, and theorem proving [32]. The heuristic knowledge used in the search process was relatively limited.
Reference: [36] <author> Herbert A. Simon. </author> <booktitle> The Sciences of the Artificial. </booktitle> <publisher> MIT Press, </publisher> <year> 1969. </year>
Reference-contexts: For example, for a speech understanding system, there are implied requirements that the system respond to spoken input in a timely fashion. Situations where this strategy is appropriate are sometimes described as satisficing problems <ref> [36] </ref>. For interpretation tasks, satisficing strategies are based on the assumption that a good interpretation, i.e., one that is within some * of the optimal (or correct) interpretation, has a semantic interpretation that is very similar to that of the correct interpretation. <p> This process can continue until a solution is determined. Approximate Processing (Satisficing) Another direct method for using the results of projection space search is to return them as the actual result of problem solving. This method is applicable for domains that admit satisficing solutions <ref> [36] </ref>. In particular, approximate processing can be used when an acceptable solution can sacrifice precision, certainty or completeness [28, 29, 9]. 144
Reference: [37] <author> Mark Stefik. </author> <title> Planning and meta-planning (MOLGEN: Part 2). </title> <journal> Artificial Intelligence, </journal> <volume> 16 </volume> <pages> 141-170, </pages> <year> 1981. </year>
Reference-contexts: 1 Introduction Many research projects have shown that sophisticated 1 control and problem solving techniques, such as meta-level processing, can be applied in complex problem solving domains to improve system performance <ref> [7, 16, 17, 21, 37, 39] </ref>. However, for many of these techniques, there are open issues regarding why the technique works, how to use a particular technique properly in its original domain, and how to generalize the technique to other domains.
Reference: [38] <author> G. C. Stockman. </author> <title> A minimax algorithm better than Alpha-Beta? Artificial Intelligence, </title> <booktitle> 12 </booktitle> <pages> 179-196, </pages> <year> 1979. </year>
Reference-contexts: For the most part, the pruning strategy formed the basis for algorithms such as AO fl , B fl , SSS fl , Alpha-Beta, and their generalizations <ref> [1, 25, 34, 38] </ref>. <p> In research projects involving restricted domains, the emphasis was on developing algorithms that enabled the problem 27 solver to prune paths based on intermediate problem solving results <ref> [1, 25, 34, 38] </ref>. Previously, Pearl [34] has shown that statistical properties of a problem solving technique, such as expected cost, can be determined from an analysis of the structure of a graphical representation of the search paths explored by the problem solver.
Reference: [39] <author> Robert Wilensky. Meta-planning: </author> <title> Representing and using knowledge about planning in problem solving and natural language understanding. </title> <journal> Cognitive Science, </journal> <volume> 5(3) </volume> <pages> 197-233, </pages> <month> July-September </month> <year> 1981. </year>
Reference-contexts: 1 Introduction Many research projects have shown that sophisticated 1 control and problem solving techniques, such as meta-level processing, can be applied in complex problem solving domains to improve system performance <ref> [7, 16, 17, 21, 37, 39] </ref>. However, for many of these techniques, there are open issues regarding why the technique works, how to use a particular technique properly in its original domain, and how to generalize the technique to other domains.
References-found: 39

