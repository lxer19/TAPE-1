URL: http://iacoma.cs.uiuc.edu/iacoma-papers/osplace.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Title: Optimizing Instruction Cache Performance for Operating System Intensive Workloads 1  
Author: Josep Torrellas, Chun Xia, and Russell Daigle 
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: High instruction cache hit rates are key to high performance. One known technique to improve the hit rate of caches is to use an optimizing compiler to minimize cache interference via an improved layout of the code. This technique, however, has been applied to application code only, even though there is evidence that the operating system often uses the cache heavily and with less uniform patterns than applications. Therefore, it is unknown how well existing optimizations perform for systems code and whether better optimizations can be found. We address this problem in this paper. This paper characterizes in detail the locality patterns of the operating system code and shows that there is substantial locality. Unfortunately, caches are not able to extract much of it: rarely-executed special-case code disrupts spatial locality, loops with few iterations that call routines make loop locality hard to exploit, and plenty of loop-less code hampers temporal locality. As a result, interference within popular execution paths dominates instruction cache misses. Based on our observations, we propose an algorithm to expose these localities and reduce interference. For a range of cache sizes, associativities, lines sizes, and other organizations we show that we reduce total instruction miss rates by 31-86% (up to 2.9 absolute points). Using a simple model this corresponds to execution time reductions in the order of 12-26%. In addition, our optimized operating system combines well with optimized or unoptimized applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, J. Hennessy, and M. Horowitz. </author> <title> Cache Performance of Operating System and Multiprogramming Workloads. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(4) </volume> <pages> 393-431, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: Indeed, there is some evidence that backs this claim. Clark [9] reported a lower performance of the VAX-11/780 cache when operating system activity was taken into account. Similarly, Agarwal et al <ref> [1] </ref> pointed out the many 1 This work was supported in part by the National Science Foundation under grants NSF Young Investigator Award MIP 94-57436, RIA MIP 93-08098, MIP 93-07910, and MIP 89-20891; NASA Contract No.
Reference: [2] <author> T. Anderson, H. Levy, B. Bershad, and E. Lazowska. </author> <title> The Interaction of Architecture and Operating System Design. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Similarly, Chen and Bershad [6] report that systems code has lower locality than application code. They also point out the self-interference in the cache. Other researchers like Ousterhout [14] and Anderson et al <ref> [2] </ref> also indicate the different nature of the operating system activity. Finally, Nagle et al [13] point out that instruction cache performance is becoming increasingly important in new-generation operating systems.
Reference: [3] <author> J. B. Andrews. </author> <title> A Hardware Tracing Facility for a Multiprocessing Supercomputer. </title> <type> Technical Report 1009, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: We use a hardware performance monitor that gathers uninterrupted reference traces of application and operating system in real time without introducing perturbation. The performance monitor <ref> [3] </ref> has one probe connected to each of the four processors. The probes collect all the references issued by the processors except those that hit in the per-processor 16 Kbyte first level instruction cache. Each probe has a trace buffer that stores over one million references.
Reference: [4] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: We tried to choose four system intensive workloads that involve a variety of system activity. TRFD 4 is a mix of 4 copies of a hand parallelized version of the TRFD Perfect Club code <ref> [4] </ref>. Each program is run with 4 processes. The code is predominately composed of matrix multiplies and data interchanges. It is highly parallel 1 This relatively large increase is in part the result of instrumenting a non-RISC assembler code: the Alliant processors use Motorola 60820 assembler code. yet synchronization intensive.
Reference: [5] <author> P. P. Chang and W. W. Hwu. </author> <title> Trace Selection for Compiling Large C Application Programs to Microcode. </title> <booktitle> In Proceedings of the 21st Annual Workshop on Microprogramming and Microarchitectures, </booktitle> <pages> pages 21-29, </pages> <month> Novem-ber </month> <year> 1988. </year>
Reference-contexts: McFarling's technique [11] uses a profile of the conditional, loop, and routine structure of the program. With this information, he places the basic blocks so that callers of routines, loops, and conditionals do not interfere with the callee routines or their descendants. Hwu and Chang's technique <ref> [5, 7] </ref> is based on identifying groups of basic blocks within a routine that tend to execute in sequence. These basic blocks are then placed in contiguous cache locations. Furthermore, routines are placed such that frequent callee routines follow immediately after their callers. <p> Note that we often end up placing some of the basic blocks of a callee routine surrounded by basic blocks of the caller. This is one of the main differences between an algorithm proposed by Chang and Hwu <ref> [5] </ref> and ours. Once we have created the sequences out of the four seeds, we catenate them and place them in the cache contiguously. With this placement, we expose much spatial locality and, consequently, reduce self-interference misses. We will describe some details of the algorithm in Section 4.
Reference: [6] <author> J. B. Chen and B. N. Bershad. </author> <title> The Impact of Operating System Structure on Memory System Performance. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 120-133, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: They also show that these self-interference misses are concentrated in ranges of addresses and attribute them to the relative lack of loops in the code. Similarly, Chen and Bershad <ref> [6] </ref> report that systems code has lower locality than application code. They also point out the self-interference in the cache. Other researchers like Ousterhout [14] and Anderson et al [2] also indicate the different nature of the operating system activity.
Reference: [7] <author> W. Y. Chen, P. P. Chang, T. M. Conte, and W. W. Hwu. </author> <title> The Effect of Code Expanding Optimizations on Instruction Cache Design. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(9) </volume> <pages> 1045-1057, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Improving the performance of caches has been addressed by many researchers. It has been shown that it is feasible to reduce the misses in applications via careful page placement [10] or via improved code layout in the cache <ref> [7, 11, 12] </ref>. The former technique is not relevant because the operating system text is usually not paged. The latter technique is based on repositioning or replicating code, usually to reduce cache conflicts. McFarling's technique [11] uses a profile of the conditional, loop, and routine structure of the program. <p> McFarling's technique [11] uses a profile of the conditional, loop, and routine structure of the program. With this information, he places the basic blocks so that callers of routines, loops, and conditionals do not interfere with the callee routines or their descendants. Hwu and Chang's technique <ref> [5, 7] </ref> is based on identifying groups of basic blocks within a routine that tend to execute in sequence. These basic blocks are then placed in contiguous cache locations. Furthermore, routines are placed such that frequent callee routines follow immediately after their callers. <p> These basic blocks are then placed in contiguous cache locations. Furthermore, routines are placed such that frequent callee routines follow immediately after their callers. The resulting spatial locality and low interference saves many misses. They also investigate function inlining <ref> [7] </ref> but find it largely ineffective because code expansion increases cache conflicts. Finally, Mendlson et al [12] perform code replication based on static information to eliminate conflicts. In all cases, the results are good. <p> The algorithm is further described in [15]. 5 Evaluation In this section we summarize the performance impact of our optimization in a variety of situations. We examine different levels of optimization: Base refers to the original unopti-mized layout; C-H refers to the layout generated by Chang-Hwu's <ref> [7] </ref> algorithm; OptS refers to our layout with Self-ConfFree area, sequences, and no loop optimization; OptL is OptS plus the loop optimization; finally, OptA is OptS plus the layout of the application optimized with sequences and loops.
Reference: [8] <author> D. Cheriton, A. Gupta, P. Boyle, and H. Goosen. </author> <title> The VMP Multiprocessor: Initial Experience, Refinements and Performance Evaluation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 410-421, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: In all cases, the total cache size is 8 Kbytes and the line size 32 bytes. An alternative to the previous scheme is to provide a very small cache dedicated to the important sections of the operating system only. Similar approaches have been suggested in the literature <ref> [8] </ref>. We have set up a 1 Kbyte such cache (about the size of SelfConfFree) where the most important parts of the sequences are saved. An additional 7 Kbyte cache has been made available to the application and rest of the operating system.
Reference: [9] <author> D. Clark. </author> <title> Cache Performance in the VAX-11/780. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(1) </volume> <pages> 24-37, </pages> <month> Febru-ary </month> <year> 1983. </year>
Reference-contexts: Given that the operating system code has a complex functionality, a large size, and interrupt-driven transfers of control among its procedures, caches may be less effective in intercepting the accesses in these workloads. Indeed, there is some evidence that backs this claim. Clark <ref> [9] </ref> reported a lower performance of the VAX-11/780 cache when operating system activity was taken into account.
Reference: [10] <author> R. Kessler and M. Hill. </author> <title> Page Placement Algorithms for Large Real-Indexed Caches. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(4) </volume> <pages> 338-359, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Improving the performance of caches has been addressed by many researchers. It has been shown that it is feasible to reduce the misses in applications via careful page placement <ref> [10] </ref> or via improved code layout in the cache [7, 11, 12]. The former technique is not relevant because the operating system text is usually not paged. The latter technique is based on repositioning or replicating code, usually to reduce cache conflicts.
Reference: [11] <author> S. McFarling. </author> <title> Program Optimization for Instruction Caches. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183-191, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Improving the performance of caches has been addressed by many researchers. It has been shown that it is feasible to reduce the misses in applications via careful page placement [10] or via improved code layout in the cache <ref> [7, 11, 12] </ref>. The former technique is not relevant because the operating system text is usually not paged. The latter technique is based on repositioning or replicating code, usually to reduce cache conflicts. McFarling's technique [11] uses a profile of the conditional, loop, and routine structure of the program. <p> The former technique is not relevant because the operating system text is usually not paged. The latter technique is based on repositioning or replicating code, usually to reduce cache conflicts. McFarling's technique <ref> [11] </ref> uses a profile of the conditional, loop, and routine structure of the program. With this information, he places the basic blocks so that callers of routines, loops, and conditionals do not interfere with the callee routines or their descendants.
Reference: [12] <author> A. Mendlson, S. Pinter, and R. Shtokhamer. </author> <title> Compile Time Intruction Cache Optimizations. </title> <booktitle> In Computer Architecture News, </booktitle> <pages> pages 44-51, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Improving the performance of caches has been addressed by many researchers. It has been shown that it is feasible to reduce the misses in applications via careful page placement [10] or via improved code layout in the cache <ref> [7, 11, 12] </ref>. The former technique is not relevant because the operating system text is usually not paged. The latter technique is based on repositioning or replicating code, usually to reduce cache conflicts. McFarling's technique [11] uses a profile of the conditional, loop, and routine structure of the program. <p> Furthermore, routines are placed such that frequent callee routines follow immediately after their callers. The resulting spatial locality and low interference saves many misses. They also investigate function inlining [7] but find it largely ineffective because code expansion increases cache conflicts. Finally, Mendlson et al <ref> [12] </ref> perform code replication based on static information to eliminate conflicts. In all cases, the results are good. However, given the complexity of the operating system, it is not known whether similar methods can be successfully applied to it.
Reference: [13] <author> D. Nagle, R. Uhlig, T. Mudge, and S. Sechrest. </author> <title> Optimal Allocation of On-chip Memory for Multiple-API Operating Systems. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 358-369, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Similarly, Chen and Bershad [6] report that systems code has lower locality than application code. They also point out the self-interference in the cache. Other researchers like Ousterhout [14] and Anderson et al [2] also indicate the different nature of the operating system activity. Finally, Nagle et al <ref> [13] </ref> point out that instruction cache performance is becoming increasingly important in new-generation operating systems. Clearly, given the practical importance of achieving high hit rates in small instruction caches, the caching behavior of systems code needs to be understood better and improved.
Reference: [14] <author> J. Ousterhout. </author> <booktitle> Why Aren't Operating Systems Getting Faster as Fast as Hardware? In Proceedings Summer 1990 USENIX Conference, </booktitle> <pages> pages 247-256, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Similarly, Chen and Bershad [6] report that systems code has lower locality than application code. They also point out the self-interference in the cache. Other researchers like Ousterhout <ref> [14] </ref> and Anderson et al [2] also indicate the different nature of the operating system activity. Finally, Nagle et al [13] point out that instruction cache performance is becoming increasingly important in new-generation operating systems.
Reference: [15] <author> J. Torrellas, R Daigle, and C. Xia. </author> <title> Optimizing Instruction Cache Performance for Operating System Work-loads. </title> <type> Technical Report 1387, </type> <institution> Center for Supercomputing Research and Development, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Once the buffers have been emptied, processors are restarted via another hardware interrupt. With this approach, we can trace an unbounded continuous stretch of the workload. Furthermore, this is done with negligible perturbation because the processors are stopped in hardware. More details are presented in <ref> [15] </ref>. 2.2 Software Setup The multiprocessor operating system used in our experiments is a slightly modified version of Alliant's Concentrix 3.0. Concentrix is symmetric and is based on Unix BSD 4.2. All processors share all operating system data structures. <p> These escapes will be encoded to tell the trace buffer what virtual-to-physical page mapping has occurred. Hence, when analyzing the address trace, we can reconstruct the virtual addresses of the application <ref> [15] </ref>. Using this approach, we first insert escape sequences at the entry and exit of each routine. With this information, we gather statistics such as the most frequently executed routines, and the common paths through the operating system and application code. <p> The code is therefore placed in the cache in segments of decreasing frequency of execution. This effect minimizes the impact of self-interference, since popular sequences will be placed close to other equally popular ones, and therefore cannot conflict with them. In <ref> [15] </ref>, we show how the set of values for Exec-Thresh and BranchThresh is selected. Basically, the first set is chosen based on gaps in the curves that plot the distribution of execution frequencies for basic blocks and outgoing edge probabilities for basic blocks respectively. <p> The chosen values are ExecThresh = 1.4% and BranchThresh = 40%. Then, successive iterations use values for these two parameters that decrease one order of magnitude every time <ref> [15] </ref>. This is repeated until all operating system code is selected. We do not simply place all these sequences consecutively. <p> These gaps are filled with non-executed code. This way, while all sequences exploit spatial locality, the most popular SelfConfFree bytes in the sequences will additionally exploit temporal locality. After some experiments <ref> [15] </ref>, we chose the SelfConfFree area to be about 1 Kbyte. Filling the gaps with non-executed code is easy, since the gaps are small, and there is abundant non-executed code. ory. Addresses increase from bottom to top within a cache-sized chunk and then from left to right. <p> Overall, the code increases in size because of the extra branches that we have to add when we do basic block motion. However, the increase in dynamic size is as low as 2.0% on average and, therefore, the impact is very small. The algorithm is further described in <ref> [15] </ref>. 5 Evaluation In this section we summarize the performance impact of our optimization in a variety of situations. <p> In this case, we place the routine in the logical cache of one of the loops and place non-executed code in the corresponding addresses of the other loop's logical cache. To reduce the likelihood of this problem, we prune out rarely-called routines. More details are given in <ref> [15] </ref>. The number of misses with this algorithm is shown in the Call (for callees) bars of Figure 12. Focusing on the operating system misses, we see that they increase by 20-100% over OptA.
Reference: [16] <author> J. Torrellas, A. Gupta, and J. Hennessy. </author> <title> Characterizing the Caching and Synchronization Performance of a Multiprocessor Operating System. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 162-174, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: NAG 1 613; and grant 1-1-28028 from the Univ. of Illinois Research Board. 2 Currently with Tandem Computers Inc. cache misses caused by the operating system. Torrellas et al <ref> [16] </ref> reported that the operating system code causes a large fraction of the cache misses and, in addition, suffers considerable self-interference in the cache. They also show that these self-interference misses are concentrated in ranges of addresses and attribute them to the relative lack of loops in the code. <p> To do this with little perturbation, we execute single machine instructions that cause data reads to specified addresses. The performance monitor can capture the addresses read from and interpret them according to an agreed-upon protocol. This methodology was suggested in <ref> [16] </ref>. To distinguish these escape accesses from real accesses, we do as follows. One first type of escapes, used for operating system instrumentation, access odd addresses in the operating system code segment. We can easily distinguish these escapes from instruction reads since the latter are aligned on even address boundaries.
References-found: 16

