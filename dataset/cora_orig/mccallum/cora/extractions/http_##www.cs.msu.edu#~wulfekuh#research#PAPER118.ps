URL: http://www.cs.msu.edu/~wulfekuh/research/PAPER118.ps
Refering-URL: http://www.cs.msu.edu/~wulfekuh/research.html
Root-URL: http://www.cs.msu.edu
Email: wulfekuh@cps.msu.edu, punch@cps.msu.edu  
Title: Finding Salient Features for Personal Web Page Categories  
Author: Marilyn R. Wulfekuhler and William F. Punch Wells Hall, 
Web: http://isl.cps.msu.edu/GA  
Address: E. Lansing MI 48824  
Affiliation: Michigan State University,  
Note: Genetic Algorithms Research and Applications Group, the GARAGe A714  
Abstract: We examine techniques that "discover" features in sets of pre-categorized documents, such that similar documents can be found on the World Wide Web. First, we examine techniques which will classify training examples with high accuracy, then explain why this is not necessarily useful. We then describe a method for extracting word clusters from the raw document features. Results show that the clustering technique is successful in discovering word grouops which can be used to find similar information on the World Wide Web.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Lycos, </author> <note> http://www.lycos.com. </note>
Reference-contexts: Thirdly, the Web is dynamic, with the potential for both content and location to change at any time. Current web searching is based on traditional information retrieval techniques and is typically based on boolean operations of keywords. Major web search services such as Lycos <ref> [1] </ref>, Alta Vista [2], WebCrawler [3], and Open Text [4] employ software robots (also called spiders) which traverse the web and create an index based on the full text of the documents they find.
Reference: [2] <institution> Alta Vista, </institution> <note> http://altavista.digital.com. </note>
Reference-contexts: Thirdly, the Web is dynamic, with the potential for both content and location to change at any time. Current web searching is based on traditional information retrieval techniques and is typically based on boolean operations of keywords. Major web search services such as Lycos [1], Alta Vista <ref> [2] </ref>, WebCrawler [3], and Open Text [4] employ software robots (also called spiders) which traverse the web and create an index based on the full text of the documents they find.
Reference: [3] <author> WebCrawler, </author> <note> http://www.webcrawler.com. </note>
Reference-contexts: Thirdly, the Web is dynamic, with the potential for both content and location to change at any time. Current web searching is based on traditional information retrieval techniques and is typically based on boolean operations of keywords. Major web search services such as Lycos [1], Alta Vista [2], WebCrawler <ref> [3] </ref>, and Open Text [4] employ software robots (also called spiders) which traverse the web and create an index based on the full text of the documents they find.
Reference: [4] <institution> Open Text, </institution> <note> http://index.opentext.net. </note>
Reference-contexts: Current web searching is based on traditional information retrieval techniques and is typically based on boolean operations of keywords. Major web search services such as Lycos [1], Alta Vista [2], WebCrawler [3], and Open Text <ref> [4] </ref> employ software robots (also called spiders) which traverse the web and create an index based on the full text of the documents they find.
Reference: [5] <author> P. A. Devijver and J. Kittler, </author> <title> Pattern Recognition: A Statistical Approach. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice-Hall, Inc., </publisher> <year> 1982. </year>
Reference-contexts: There is no a priori knowledge of patterns that belong to certain groups, or even how many groups are appropriate. Refer to basic pattern recognition and clustering texts such as <ref> [5, 6, 7] </ref> for further information. <p> The above classifier using all 5190 features misclassified 50 out of 85 documents, for an accuracy of 41:18%. We found we can improve the classification accuracy through feature selection techniques. Some effective conventional methods for feature selection are sequential forward selection <ref> [5, 11] </ref>, sequential floating feature selection [12], and genetic algorithm search [13, 14]. Sequential forward selection achieved 17/85 errors, or 80% accuracy by selecting 13 features. These features were engin, action, david, contempl, affirm, architectur, ave, osha, abund, rehabilit, notic, commerc, transact.
Reference: [6] <author> A. K. Jain and R. C. Dubes, </author> <title> Algorithms for Clustering Data. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: There is no a priori knowledge of patterns that belong to certain groups, or even how many groups are appropriate. Refer to basic pattern recognition and clustering texts such as <ref> [5, 6, 7] </ref> for further information.
Reference: [7] <author> M. R. Anderberg, </author> <title> Cluster Analysis for Applications. </title> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1973. </year> <month> 9 </month>
Reference-contexts: There is no a priori knowledge of patterns that belong to certain groups, or even how many groups are appropriate. Refer to basic pattern recognition and clustering texts such as <ref> [5, 6, 7] </ref> for further information.
Reference: [8] <author> A. K. Jain and B. Chandrasekaran, </author> <title> "Dimensionality and sample size considerations in pattern recogni-tion practice," </title> <booktitle> in Handbook of Statistics, </booktitle> <volume> Vol. 2 (P. </volume> <editor> R. Krishnaiah and L. N. Kanal, </editor> <booktitle> Eds.), </booktitle> <pages> pp. 835-855, </pages> <address> Amsterdam: </address> <publisher> North-Holland Publishing Company, </publisher> <year> 1982. </year>
Reference-contexts: 23 government 15 labor 13 legal 34 total 85 Table 1: Sample data set categories The conventional rule of thumb for statistical pattern recognition techniques is that you need five to ten times as many training samples as features for each class in order to estimate the class probability distributions <ref> [8] </ref>. For our 4 class problem and 5190 features, this means we would need one to two hundred thousand training documents.
Reference: [9] <author> C. Fox, </author> <title> "Lexical analysis and stoplists," in Information Retrieval Data Structures and Algorithms (W. </title> <editor> B. Frakes and R. Baeza-Yates, </editor> <booktitle> Eds.), </booktitle> <pages> pp. 102-130, </pages> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: We then parse each document for words, defined as a contiguous string of letters, ignoring HTML tags, digits, and punctuation, as well as common English words (such as the, of, and, etc) from a pre-defined stop list. Our stop list is the one given by Fox <ref> [9] </ref>. We then reduce each remaining word to its word stem, so that words like computer and computing both become the same term, comput. This is a standard information retrieval technique, and we use the algorithm from Frakes [10].
Reference: [10] <author> W. B. Frakes, </author> <title> "Stemming algorithms," in Information Retrieval Data Structures and Algorithms (W. </title> <editor> B. Frakes and R. Baeza-Yates, </editor> <booktitle> Eds.), </booktitle> <pages> pp. 131-160, </pages> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: Our stop list is the one given by Fox [9]. We then reduce each remaining word to its word stem, so that words like computer and computing both become the same term, comput. This is a standard information retrieval technique, and we use the algorithm from Frakes <ref> [10] </ref>. All unique word stems from the entire training set of documents form the global feature space. Stemming the words has the obvious advantage of reducing the feature space, resulting in 5190 distinct stemmed features vs. 7633 distinct non-stemmed features in our 85 documents.
Reference: [11] <author> A. Whitney, </author> <title> "A direct method of nonparametric measurement selection," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 20, </volume> <pages> pp. 1100-1103, </pages> <year> 1971. </year>
Reference-contexts: The above classifier using all 5190 features misclassified 50 out of 85 documents, for an accuracy of 41:18%. We found we can improve the classification accuracy through feature selection techniques. Some effective conventional methods for feature selection are sequential forward selection <ref> [5, 11] </ref>, sequential floating feature selection [12], and genetic algorithm search [13, 14]. Sequential forward selection achieved 17/85 errors, or 80% accuracy by selecting 13 features. These features were engin, action, david, contempl, affirm, architectur, ave, osha, abund, rehabilit, notic, commerc, transact.
Reference: [12] <author> F. J. Ferri, P. Pudil, M. Hatef, and J. Kittler, </author> <title> "Comparative study of techniques for large-scale feature selection," in Pattern Recognition in Practice IV, Mutliple Paradigms, Comparative Studies and Hybrid Systems (E. </title> <editor> S. Gelsema and L. S. Kanal, </editor> <booktitle> Eds.), </booktitle> <pages> pp. 403-413, </pages> <address> Amsterdam: </address> <publisher> Elsevier, </publisher> <year> 1994. </year>
Reference-contexts: The above classifier using all 5190 features misclassified 50 out of 85 documents, for an accuracy of 41:18%. We found we can improve the classification accuracy through feature selection techniques. Some effective conventional methods for feature selection are sequential forward selection [5, 11], sequential floating feature selection <ref> [12] </ref>, and genetic algorithm search [13, 14]. Sequential forward selection achieved 17/85 errors, or 80% accuracy by selecting 13 features. These features were engin, action, david, contempl, affirm, architectur, ave, osha, abund, rehabilit, notic, commerc, transact.
Reference: [13] <author> W. F. Punch, E. D. Goodman, M. Pei, L. Chia-Shun, P. Hovland, and R. Enbody, </author> <title> "Further research on feature selection and classification using genetic algorithms," </title> <booktitle> in International Conference on Genetic Algorithms 93, </booktitle> <address> (Champaign, IL), </address> <year> 1993. </year>
Reference-contexts: We found we can improve the classification accuracy through feature selection techniques. Some effective conventional methods for feature selection are sequential forward selection [5, 11], sequential floating feature selection [12], and genetic algorithm search <ref> [13, 14] </ref>. Sequential forward selection achieved 17/85 errors, or 80% accuracy by selecting 13 features. These features were engin, action, david, contempl, affirm, architectur, ave, osha, abund, rehabilit, notic, commerc, transact.
Reference: [14] <author> W. Siedlecki and J. Sklansky, </author> <title> "A note on genetic algorithms for large scale feature selection," </title> <journal> Pattern Recognition Letters, </journal> <volume> vol. 10, </volume> <pages> pp. 335-347, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: We found we can improve the classification accuracy through feature selection techniques. Some effective conventional methods for feature selection are sequential forward selection [5, 11], sequential floating feature selection [12], and genetic algorithm search <ref> [13, 14] </ref>. Sequential forward selection achieved 17/85 errors, or 80% accuracy by selecting 13 features. These features were engin, action, david, contempl, affirm, architectur, ave, osha, abund, rehabilit, notic, commerc, transact.
Reference: [15] <author> J. A. Hartigan and M. A. Wong, </author> <title> "A k-means clustering algorithm," </title> <journal> Applied Statistics, </journal> <volume> vol. 28, </volume> <pages> pp. 100-108, </pages> <year> 1979. </year>
Reference-contexts: This causes the column vectors to be similar (near each other in the document space) and thus the terms are put into the same group. To group the features, we used Hartigan's K-means partitional clustering algorithm <ref> [15] </ref> as implemented by S-PLUS 2 , where K points are chosen randomly as the means of K groups in the 85 dimensional space. Each of the 5190 points is then assigned to the group whose mean is closest in Euclidean distance.
References-found: 15

