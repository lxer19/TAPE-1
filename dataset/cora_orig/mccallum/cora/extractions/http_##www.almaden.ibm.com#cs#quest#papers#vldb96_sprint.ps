URL: http://www.almaden.ibm.com/cs/quest/papers/vldb96_sprint.ps
Refering-URL: http://www.almaden.ibm.com/cs/quest/publications.html
Root-URL: 
Title: SPRINT: A Scalable Parallel Classifier for Data Mining  
Author: John Shafer Rakesh Agrawal Manish Mehta 
Address: 650 Harry Road, San Jose, CA 95120  
Affiliation: IBM Almaden Research Center  
Abstract: Classification is an important data mining problem. Although classification is a well-studied problem, most of the current classification algorithms require that all or a portion of the the entire dataset remain permanently in memory. This limits their suitability for mining over large databases. We present a new decision-tree-based classification algorithm, called SPRINT that removes all of the memory restrictions, and is fast and scalable. The algorithm has also been designed to be easily parallelized, allowing many processors to work together to build a single consistent model. This parallelization, also presented here, exhibits excellent scalability as well. The combination of these characteristics makes the proposed algorithm an ideal tool for data min ing.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rakesh Agrawal, Sakti Ghosh, Tomasz Imielinski, Bala Iyer, and Arun Swami. </author> <title> An interval classifier for database mining applications. </title> <booktitle> In Proc. of the VLDB Conference, </booktitle> <pages> pages 560-573, </pages> <address> Vancou-ver, British Columbia, Canada, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Decision trees can be constructed relatively fast compared to other methods. Another advantage is that decision tree models are simple and easy to understand [20]. Moreover, trees can be easily converted into SQL statements that can be used to access databases efficiently <ref> [1] </ref>. Finally, decision tree classifiers obtain similar and sometimes better accuracy when compared with other classification methods [16]. We have therefore focused on building a scalable and parallelizable decision-tree classifier. <p> Incremental learning methods, where the data is classified in batches, have also been studied [18][25]. However, the cumulative cost of classifying data incrementally can sometimes exceed the cost of classifying the entire training set once. In <ref> [1] </ref>, a classifier built with database considerations, the size of the training set was overlooked. Instead, the focus was on building a classifier that could use database indices to improve the retrieval efficiency while classifying test data.
Reference: [2] <author> Rakesh Agrawal, Tomasz Imielinski, and Arun Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(6) </volume> <pages> 914-925, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Classification has been identified as an important problem in the emerging field of data mining <ref> [2] </ref>. While classification is a well-studied problem (see [24] [16] for excellent overviews), only recently has there been focus on algorithms that can handle large databases. The intuition is that by classifying larger datasets, we fl Also, Department of Computer Science, University of Wis-consin, Madison. <p> Due to the lack of a classification benchmark containing large datasets, we use the synthetic database proposed in <ref> [2] </ref> for all of our experiments. Each record in this synthetic database consists of nine attributes four of which are shown in Table 1. Ten classification functions were also proposed in [2] to produce databases with distributions with varying complexities. <p> Due to the lack of a classification benchmark containing large datasets, we use the synthetic database proposed in <ref> [2] </ref> for all of our experiments. Each record in this synthetic database consists of nine attributes four of which are shown in Table 1. Ten classification functions were also proposed in [2] to produce databases with distributions with varying complexities. In this paper, we present results for two of these function. Function 2 results in fairly small decision trees, while function 7 produces very large trees.
Reference: [3] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: The algorithms presented there also require processor communication to evaluate any given split point, limiting the number of possible partitioning schemes the algorithms can efficiently consider for each leaf. The Darwin toolkit from Thinking Machines also contained a parallel implementation of the decision-tree classifier CART <ref> [3] </ref>; however, details of this parallelization are not available in published literature. The recently proposed SLIQ classification algorithm [15] addressed several issues in building a fast scalable classifier. SLIQ gracefully handles disk-resident data that is too large to fit in memory. <p> We conclude with a summary in Section 5. An expanded version of this paper is available in [22]. 2 Serial Algorithm A decision tree classifier is built in two phases <ref> [3] </ref> [20]: a growth phase and a prune phase. In the growth phase, the tree is built by recursively partitioning the 1 SPRINT stands for Scalable PaRallelizable INndution of decision Trees. <p> How to find split points that define node tests. 2. Having chosen a split point, how to partition the data. The well-known CART <ref> [3] </ref> and C4.5 [20] classifiers, for example, grow trees depth-first and repeatedly sort the data at every node of the tree to arrive at the best splits for numeric attributes. <p> The value of a split point depends upon how well it separates the classes. Several splitting indices have been proposed in the past to evaluate the goodness of the split. We use the gini index, originally proposed in <ref> [3] </ref>, based on our experience with SLIQ. For a data set S containing examples from n classes, gini (S) is defined as gini (S) = 1 P j where p j is the relative frequency of class j in S. <p> Since SPRINT uses SLIQ 's pruning method, the final trees obtained using the two algorithms are also identical. Thus, the accuracy and tree size characteristics of SPRINT are identical to SLIQ. A detailed comparison of SLIQ's accuracy, execution time, and tree size with those of CART <ref> [3] </ref> and C4 (a predecessor of C4.5 [20]) is available in [15]. This performance evaluation shows that compared to other classifiers, SLIQ achieves comparable or better classification accuracy, but produces small decision trees and has small execution times.
Reference: [4] <author> Jason Catlett. </author> <title> Megainduction: Machine Learning on Very Large Databases. </title> <type> PhD thesis, </type> <institution> University of Sydney, </institution> <year> 1991. </year>
Reference-contexts: To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 22nd VLDB Conference Mumbai (Bombay), India, 1996 will be able to improve the accuracy of the classification model. This hypothesis has been studied and confirmed in <ref> [4] </ref>, [5], and [6]. In classification, we are given a set of example records, called a training set, where each record consists of several fields or attributes. Attributes are either continuous, coming from an ordered domain, or categorical, coming from an unordered domain.
Reference: [5] <author> Philip K. Chan and Salvatore J. Stolfo. </author> <title> Experiments on multistrategy learning by meta-learning. </title> <booktitle> In Proc. Second Intl. Conference on Info. and Knowledge Mgmt., </booktitle> <pages> pages 314-323, </pages> <year> 1993. </year>
Reference-contexts: To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 22nd VLDB Conference Mumbai (Bombay), India, 1996 will be able to improve the accuracy of the classification model. This hypothesis has been studied and confirmed in [4], <ref> [5] </ref>, and [6]. In classification, we are given a set of example records, called a training set, where each record consists of several fields or attributes. Attributes are either continuous, coming from an ordered domain, or categorical, coming from an unordered domain. <p> The first method used data sampling at each node of the decision tree, and the second discretized continuous attributes. However, Catlett only considered datasets that could fit in memory; the largest training data had only 32,000 examples. Chan and Stolfo <ref> [5] </ref> [6] considered partitioning the data into subsets that fit in memory and then developing a classifier on each subset in parallel. The output of multiple classifiers is combined using various algorithms to reach the final classification.
Reference: [6] <author> Philip K. Chan and Salvatore J. Stolfo. </author> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Workshop on Multistrategy Learning, </booktitle> <pages> pages 150-165, </pages> <year> 1993. </year>
Reference-contexts: To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 22nd VLDB Conference Mumbai (Bombay), India, 1996 will be able to improve the accuracy of the classification model. This hypothesis has been studied and confirmed in [4], [5], and <ref> [6] </ref>. In classification, we are given a set of example records, called a training set, where each record consists of several fields or attributes. Attributes are either continuous, coming from an ordered domain, or categorical, coming from an unordered domain. <p> The first method used data sampling at each node of the decision tree, and the second discretized continuous attributes. However, Catlett only considered datasets that could fit in memory; the largest training data had only 32,000 examples. Chan and Stolfo [5] <ref> [6] </ref> considered partitioning the data into subsets that fit in memory and then developing a classifier on each subset in parallel. The output of multiple classifiers is combined using various algorithms to reach the final classification.
Reference: [7] <author> D. J. DeWitt, S. Ghandeharizadeh, D. A. Schnei-der, A. Bricker, H.-I. Hsiao, and R. Rasmussen. </author> <title> The Gamma database machine project. </title> <journal> In IEEE Transactions on Knowledge and Data Engineering, </journal> <pages> pages 44-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: For comparison, we also discuss two parallelizations of SLIQ. These algorithms all assume a shared-nothing parallel environment where each of N processors has private memory and disks. The processors are connected by a communication network and can communicate only by passing messages. Examples of such parallel machines include GAMMA <ref> [7] </ref>, Teradata [23], and IBM's SP2 [12]. 3.1 Data Placement and Workload Balancing Recall that the main data structures used in SPRINT are the attribute lists and the class histograms.
Reference: [8] <author> D. J. DeWitt, J. F. Naughton, and D. A. Schnei-der. </author> <title> Parallel sorting on a shared-nothing architecture using probabilistic splitting. </title> <booktitle> In Proc. of the 1st Int'l Conf. on Parallel and Distributed Information Systems, </booktitle> <pages> pages 280-291, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Lists for categorical attributes are therefore evenly partitioned and require no further processing. However, continuous attribute lists must now be sorted and reparti-tioned into contiguous sorted sections. For this, we use the parallel sorting algorithm given in <ref> [8] </ref>. The result of this sorting operation is that each processor gets a fairly equal-sized sorted sections of each attribute list. of the lists for a 2-processor configuration. 3.2 Finding split points Finding split points in parallel SPRINT is very similar to the serial algorithm.
Reference: [9] <author> D. J. Fifield. </author> <title> Distributed tree construction from large data-sets. </title> <type> Bachelor's Honours Thesis, </type> <institution> Aus-tralian National University, </institution> <year> 1992. </year>
Reference-contexts: In [1], a classifier built with database considerations, the size of the training set was overlooked. Instead, the focus was on building a classifier that could use database indices to improve the retrieval efficiency while classifying test data. Work by Fifield in <ref> [9] </ref> examined parallelizing the decision-tree classifier ID3 [19] serial classifier. Like ID3, this work assumes that the entire dataset can fit in real memory and does not address issues such as disk I/O.
Reference: [10] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: threshold (about 1.5 million records for our system configuration), SLIQ starts thrashing, whereas SPRINT continues to exhibit a nearly linear scaleup. 4.3 Parallel Performance To examine how well the SPRINT algorithm performs in parallel environments, we implemented its paral-lelization on an IBM SP2 [12], using the standard MPI communication primitives <ref> [10] </ref>. The use of MPI allows our implementation to be completely portable to other Page 8 shared-nothing parallel architectures, including workstation clusters. Experiments were conducted on a 16-node IBM SP2 Model 9076.
Reference: [11] <author> D. E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: Applications of classification arise in diverse fields, such as retail target marketing, customer retention, fraud detection and medical diagnosis [16]. Several classification models have been proposed over the years, e.g. neural networks [14], statistical models like linear/quadratic discriminants [13], decision trees [3][20] and genetic models <ref> [11] </ref>. Among these models, decision trees are particularly suited for data mining [2][15]. Decision trees can be constructed relatively fast compared to other methods. Another advantage is that decision tree models are simple and easy to understand [20].
Reference: [12] <institution> Int'l Business Machines. Scalable POWERparallel Systems, </institution> <address> GA23-2475-02 edition, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: We present in this paper a decision-tree-based classification algorithm, called SPRINT 1 , that removes all of the memory restrictions, and is fast and scalable. The algorithm has also been designed to be easily par-allelized. Measurements of this parallel implementation on a shared-nothing IBM POWERparallel System SP2 <ref> [12] </ref>, also presented here, show that SPRINT has excellent scaleup, speedup and sizeup properties. The combination of these characteristics makes SPRINT an ideal tool for data mining. <p> These algorithms all assume a shared-nothing parallel environment where each of N processors has private memory and disks. The processors are connected by a communication network and can communicate only by passing messages. Examples of such parallel machines include GAMMA [7], Teradata [23], and IBM's SP2 <ref> [12] </ref>. 3.1 Data Placement and Workload Balancing Recall that the main data structures used in SPRINT are the attribute lists and the class histograms. SPRINT achieves uniform data placement and workload balancing by distributing the attribute lists evenly over N processors of a shared-nothing machine. <p> soon as we cross an input size threshold (about 1.5 million records for our system configuration), SLIQ starts thrashing, whereas SPRINT continues to exhibit a nearly linear scaleup. 4.3 Parallel Performance To examine how well the SPRINT algorithm performs in parallel environments, we implemented its paral-lelization on an IBM SP2 <ref> [12] </ref>, using the standard MPI communication primitives [10]. The use of MPI allows our implementation to be completely portable to other Page 8 shared-nothing parallel architectures, including workstation clusters. Experiments were conducted on a 16-node IBM SP2 Model 9076. <p> Attached to each node is a 100MB disk on which we stored our datasets. The processors all run AIX level 4.1 and communicate with each other through the High-Performance-Switch with HPS-tb2 adaptors. See <ref> [12] </ref> for SP2 hardware details. Due to the available disk space being smaller than the available memory, we are prevented from running any experiments where attribute lists are forced to disk. This results in I/O costs, which scale linearly in SPRINT, becoming a smaller fraction of the overall execution time.
Reference: [13] <author> M. James. </author> <title> Classificaton Algorithms. </title> <publisher> Wiley, </publisher> <year> 1985. </year>
Reference-contexts: Applications of classification arise in diverse fields, such as retail target marketing, customer retention, fraud detection and medical diagnosis [16]. Several classification models have been proposed over the years, e.g. neural networks [14], statistical models like linear/quadratic discriminants <ref> [13] </ref>, decision trees [3][20] and genetic models [11]. Among these models, decision trees are particularly suited for data mining [2][15]. Decision trees can be constructed relatively fast compared to other methods. Another advantage is that decision tree models are simple and easy to understand [20].
Reference: [14] <author> R. Lippmann. </author> <title> An introduction to computing with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 4(22), </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: Once a model is built, it can be used to determine the class of future unclassified records. Applications of classification arise in diverse fields, such as retail target marketing, customer retention, fraud detection and medical diagnosis [16]. Several classification models have been proposed over the years, e.g. neural networks <ref> [14] </ref>, statistical models like linear/quadratic discriminants [13], decision trees [3][20] and genetic models [11]. Among these models, decision trees are particularly suited for data mining [2][15]. Decision trees can be constructed relatively fast compared to other methods.
Reference: [15] <author> Manish Mehta, Rakesh Agrawal, and Jorma Ris-sanen. SLIQ: </author> <title> A fast scalable classifier for data mining. </title> <booktitle> In Proc. of the Fifth Int'l Conference on Extending Database Technology (EDBT), </booktitle> <address> Avi-gnon, France, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: The Darwin toolkit from Thinking Machines also contained a parallel implementation of the decision-tree classifier CART [3]; however, details of this parallelization are not available in published literature. The recently proposed SLIQ classification algorithm <ref> [15] </ref> addressed several issues in building a fast scalable classifier. SLIQ gracefully handles disk-resident data that is too large to fit in memory. It does not use small memory-sized datasets obtained via sampling or partitioning, but builds a single decision tree using the entire training set. <p> SLIQ, on the other hand, replaces this repeated sorting with one-time sort by using separate lists for each attribute (see <ref> [15] </ref> for details). SLIQ uses a data structure called a class list which must remain memory resident at all times. <p> As stated earlier, these histograms are used to initialize the C above histograms when evaluating continuous split-points in the next pass. 2.4 Comparison with SLIQ The technique of creating separate attribute lists from the original data was first proposed by the SLIQ algorithm <ref> [15] </ref>. In SLIQ, an entry in an attribute list consists only of an attribute value and a rid; the class labels are kept in a separate data-structure called a class list which is indexed by rid. <p> Thus, the accuracy and tree size characteristics of SPRINT are identical to SLIQ. A detailed comparison of SLIQ's accuracy, execution time, and tree size with those of CART [3] and C4 (a predecessor of C4.5 [20]) is available in <ref> [15] </ref>. This performance evaluation shows that compared to other classifiers, SLIQ achieves comparable or better classification accuracy, but produces small decision trees and has small execution times. <p> We only compare our algorithm with SLIQ because is has been shown in <ref> [15] </ref> that SLIQ in most cases outperforms other popular decision-tree classifiers. For the disk-resident datasets which we will be exploring here, SLIQ is the only other viable algorithm. Experiments were conducted on an IBM RS/6000 250 workstation running AIX level 3.2.5.
Reference: [16] <author> D. Michie, D. J. Spiegelhalter, and C. C. Taylor. </author> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction Classification has been identified as an important problem in the emerging field of data mining [2]. While classification is a well-studied problem (see [24] <ref> [16] </ref> for excellent overviews), only recently has there been focus on algorithms that can handle large databases. The intuition is that by classifying larger datasets, we fl Also, Department of Computer Science, University of Wis-consin, Madison. <p> Once a model is built, it can be used to determine the class of future unclassified records. Applications of classification arise in diverse fields, such as retail target marketing, customer retention, fraud detection and medical diagnosis <ref> [16] </ref>. Several classification models have been proposed over the years, e.g. neural networks [14], statistical models like linear/quadratic discriminants [13], decision trees [3][20] and genetic models [11]. Among these models, decision trees are particularly suited for data mining [2][15]. Decision trees can be constructed relatively fast compared to other methods. <p> Another advantage is that decision tree models are simple and easy to understand [20]. Moreover, trees can be easily converted into SQL statements that can be used to access databases efficiently [1]. Finally, decision tree classifiers obtain similar and sometimes better accuracy when compared with other classification methods <ref> [16] </ref>. We have therefore focused on building a scalable and parallelizable decision-tree classifier. A decision tree is a class discriminator that recursively partitions the training set until each partition consists entirely or dominantly of examples from one class. <p> We, therefore, focus only on the classification time metric in our performance evaluation in this paper. 4.1 Datasets An often used benchmark in classification is STATLOG <ref> [16] </ref>; however, its largest dataset contains only 57,000 training examples. Due to the lack of a classification benchmark containing large datasets, we use the synthetic database proposed in [2] for all of our experiments.
Reference: [17] <institution> NASA Ames Research Center. </institution> <note> Introduction to IND Version 2.1, GA23-2475-02 edition, </note> <year> 1992. </year>
Reference-contexts: Once we are finished with the scan, we consider all subsets of the attribute values as possible split points and compute the corresponding gini index. If the cardinality of an attribute is above certain threshold, the greedy algorithm initially proposed for IND <ref> [17] </ref> is instead used for subsetting. The important point is that the infor mation required for computing the gini index for any subset splitting is available in the count matrix. The memory allocated for a count matrix is re claimed after the splits for the corresponding attribute have been evaluated.
Reference: [18] <author> J. R. Quinlan. </author> <title> Induction over large databases. </title> <type> Technical Report STAN-CS-739, </type> <institution> Stanfard University, </institution> <year> 1979. </year>
Reference: [19] <author> J. Ross Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: In [1], a classifier built with database considerations, the size of the training set was overlooked. Instead, the focus was on building a classifier that could use database indices to improve the retrieval efficiency while classifying test data. Work by Fifield in [9] examined parallelizing the decision-tree classifier ID3 <ref> [19] </ref> serial classifier. Like ID3, this work assumes that the entire dataset can fit in real memory and does not address issues such as disk I/O.
Reference: [20] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufman, </publisher> <year> 1993. </year>
Reference-contexts: [14], statistical models like linear/quadratic discriminants [13], decision trees [3]<ref> [20] </ref> and genetic models [11]. Among these models, decision trees are particularly suited for data mining [2][15]. Decision trees can be constructed relatively fast compared to other methods. Another advantage is that decision tree models are simple and easy to understand [20]. Moreover, trees can be easily converted into SQL statements that can be used to access databases efficiently [1]. Finally, decision tree classifiers obtain similar and sometimes better accuracy when compared with other classification methods [16]. We have therefore focused on building a scalable and parallelizable decision-tree classifier. <p> We conclude with a summary in Section 5. An expanded version of this paper is available in [22]. 2 Serial Algorithm A decision tree classifier is built in two phases [3] <ref> [20] </ref>: a growth phase and a prune phase. In the growth phase, the tree is built by recursively partitioning the 1 SPRINT stands for Scalable PaRallelizable INndution of decision Trees. <p> How to find split points that define node tests. 2. Having chosen a split point, how to partition the data. The well-known CART [3] and C4.5 <ref> [20] </ref> classifiers, for example, grow trees depth-first and repeatedly sort the data at every node of the tree to arrive at the best splits for numeric attributes. SLIQ, on the other hand, replaces this repeated sorting with one-time sort by using separate lists for each attribute (see [15] for details). <p> Thus, the accuracy and tree size characteristics of SPRINT are identical to SLIQ. A detailed comparison of SLIQ's accuracy, execution time, and tree size with those of CART [3] and C4 (a predecessor of C4.5 <ref> [20] </ref>) is available in [15]. This performance evaluation shows that compared to other classifiers, SLIQ achieves comparable or better classification accuracy, but produces small decision trees and has small execution times.
Reference: [21] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publ. Co., </publisher> <year> 1989. </year>
Reference-contexts: We therefore focus only on the tree-growth phase. For pruning, we use the algorithm used in SLIQ, which is based on the Minimum Description Length principle <ref> [21] </ref>.
Reference: [22] <author> John C. Shafer, Rakesh Agrawal, and Manish Mehta. SPRINT: </author> <title> A scalable parallel classifier for data mining. </title> <type> Research report, </type> <institution> IBM Almaden Research Center, </institution> <address> San Jose, California, </address> <year> 1996. </year> <note> Available from http://www.almaden.ibm.com/cs/quest. </note>
Reference-contexts: In Section 4, we give a performance evaluation of the serial and parallel algorithms using measurements from their implementation on SP2. We conclude with a summary in Section 5. An expanded version of this paper is available in <ref> [22] </ref>. 2 Serial Algorithm A decision tree classifier is built in two phases [3] [20]: a growth phase and a prune phase. In the growth phase, the tree is built by recursively partitioning the 1 SPRINT stands for Scalable PaRallelizable INndution of decision Trees. <p> During this splitting operation, we also build class 2 Because file-creation is usually an expensive operation, we have a solution that does not require the creation of new files for each new attribute list. The details of this optimization can be found in <ref> [22] </ref>. histograms for each new leaf. As stated earlier, these histograms are used to initialize the C above histograms when evaluating continuous split-points in the next pass. 2.4 Comparison with SLIQ The technique of creating separate attribute lists from the original data was first proposed by the SLIQ algorithm [15].
Reference: [23] <author> Teradata Corp. </author> <title> DBC/1012 Data Base Computer System Manual, </title> <note> C10-0001-02 release 2.0 edition, </note> <month> November </month> <year> 1985. </year>
Reference-contexts: These algorithms all assume a shared-nothing parallel environment where each of N processors has private memory and disks. The processors are connected by a communication network and can communicate only by passing messages. Examples of such parallel machines include GAMMA [7], Teradata <ref> [23] </ref>, and IBM's SP2 [12]. 3.1 Data Placement and Workload Balancing Recall that the main data structures used in SPRINT are the attribute lists and the class histograms. SPRINT achieves uniform data placement and workload balancing by distributing the attribute lists evenly over N processors of a shared-nothing machine.
Reference: [24] <author> Sholom M. Weiss and Casimir A. </author> <title> Kulikowski. Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems. </title> <publisher> Morgan Kaufman, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction Classification has been identified as an important problem in the emerging field of data mining [2]. While classification is a well-studied problem (see <ref> [24] </ref> [16] for excellent overviews), only recently has there been focus on algorithms that can handle large databases. The intuition is that by classifying larger datasets, we fl Also, Department of Computer Science, University of Wis-consin, Madison.
Reference: [25] <author> J. Wirth and J. Catlett. </author> <title> Experiments on the costs and benefits of windowing in ID3. </title> <booktitle> In 5th Int'l Conference on Machine Learning, </booktitle> <year> 1988. </year> <pages> Page 12 </pages>
References-found: 25

