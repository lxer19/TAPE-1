URL: http://www.cs.waikato.ac.nz/ml/publications/1996/KaiMing-WP96.ps
Refering-URL: http://www.cs.waikato.ac.nz/cs/Pub/Staff/kaiming.html
Root-URL: 
Email: kaiming@cs.waikato.ac.nz  btlow@se.cuhk.hk  
Title: Theory Combination: an alternative to Data Combination  
Author: Kai Ming Ting Boon Toh Low 
Keyword: theory combination, data combination, empirical evaluation, learning curve, near-asymptotic performance.  
Address: Hamilton, New Zealand  Hong Kong, Shatin, Hong Kong  
Affiliation: University of Waikato,  Chinese University of  
Abstract: The approach of combining theories learned from multiple batches of data provide an alternative to the common practice of learning one theory from all the available data (i.e., the data combination approach). This paper empirically examines the base-line behaviour of the theory combination approach in classification tasks. We find that theory combination can lead to better performance even if the disjoint batches of data are drawn randomly from a larger sample, and relate the relative performance of the two approaches to the learning curve of the classifier used. The practical implication of our results is that one should consider using theory combination rather than data combination, especially when multiple batches of data for the same task are readily available. Another interesting result is that we empirically show that the near-asymptotic performance of a single theory, in some classification task, can be significantly improved by combining multiple theories (of the same algorithm) if the constituent theories are substantially different and there is some regularity in the theories to be exploited by the combination method used. Comparisons with known theoretical results are also provided. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D.W., D. </author> <title> Kibler & M.K. Albert (1991), Instance-Based Learning Algorithms, </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> pp. 37-66. </pages>
Reference-contexts: An "oracle" combination method is also used for comparison. It always makes the correct prediction from its constituent classifiers, if one exists. 5 4. Experiments and Results Two inductive learning algorithms, IB1* and NB* (Ting, 1994; 1996a), are used in our experiments. IB1* is a variant of IB1 <ref> (Aha, Kibler & Albert, 1991) </ref> that incorporates the modified value-difference metric (Cost & Salzberg, 1993) and NB* is an implementation of the Naive Bayes (Cestnik, 1990) algorithm. Both algorithms include a method (Fayyad & Irani, 1993) for discretising continuous-valued attributes in the preprocessing.
Reference: <author> Ali, K.M. & M.J. </author> <title> Pazzani (1996), Error Reduction through Learning Multiple Descriptions, </title> <journal> Machine Learning, </journal> <volume> Vol. 24, No. 3, </volume> <pages> pp. 173-206. </pages>
Reference-contexts: This last note also applies when one or more classes are supported by only few instances in a dataset. Though our investigation is limited to one type of combination method, we believe that the results in this paper are applicable to other reasonable combination methods <ref> (e.g., Ali & Pazzani, 1996) </ref> judging from the performance of the "oracle" combination method. In all datasets, the oracle performs significantly better than data combination, sometimes with huge margins. This shows there is plenty of room for any reasonable combination method to gain advantage.
Reference: <author> Baxt, W.G. </author> <year> (1992), </year> <title> Improving the Accuracy of an Artificial Neural Network using Multiple Differently Trained Networks, </title> <journal> Neural Computation, </journal> <volume> Vol. 4, No. 5, </volume> <pages> pp. 772-780, </pages> <publisher> The MIT Press. </publisher>
Reference-contexts: The additional data can be used as an extra task to better support learning for the main task. A more subtle situation is that the data is pre-sorted into different groups according to some criterion when the data is collected <ref> (e.g., Baxt, 1992) </ref>. This is an interesting scenario which we intend to explore in the near future. When more than two batches are available, one possible method is to stack up the combinations in a binary tree structure as have been reported by Chan and Stolfo (1996).
Reference: <author> Breiman, L. </author> <year> (1996a), </year> <title> Bagging Predictors, </title> <journal> Machine Learning, </journal> <volume> Vol. 24, No. 2, </volume> <pages> pp. 123-140. </pages>
Reference-contexts: Related Work We focus our review on how multiple models are generated, with just a minor note on how they are combined since that is not our emphasis. Some work on multiple models employs sampling methods to generate the models, e.g., bagging <ref> (Breiman, 1996a) </ref> and boosting (Freund & Schapire, 1996; Quinlan, 1996; Breiman, 1996b). Each sample dataset has either the same data size as the available dataset or a high percentage of the total instances.
Reference: <author> Breiman, L. </author> <year> (1996b), </year> <title> Bias, Variance, and Arcing Classifiers, </title> <type> Technical Report 460, </type> <institution> Department of Statistics, University of California, Berkeley, </institution> <address> CA. </address>
Reference: <author> Breiman, L., J.H. Friedman, R.A. Olshen & C.J. </author> <title> Stone (1984), Classification And Regression Trees, </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: Ting and Cameron-Jones' (1994) empirical result shows that by storing only one instance per class for IB1*, which is the perfect bias in this domain, achieves the best result (i.e., 25% error rate). A similar result is obtained by computing the Bayes rules <ref> (Breiman et al, 1984) </ref>. Thus, the performance of the oracle below the 25% error rate mark is due to random guesses for any classifiers and no other combination method can do better. For the IB1* settings we are using here, the contribution of random guesses could be much higher.
Reference: <author> Brodley, C.E. </author> <year> (1993), </year> <title> Addressing the Selective Superiority Problem: Automatic Algorithm/Model Class Selection, </title> <booktitle> in Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. 17-24. </pages>
Reference-contexts: Some methods provide guidance as to how to partition the description space. Some use information gain criterion (Utgoff, 1989), user-provided information (Tcheng, Lambert & Rendell, 1989), or hand-crafted rules <ref> (Brodley, 1993) </ref> to guide the recursive partitioning process in a tree structure; and others (Ting, 1994; Wettschereck, 1994) employ a confidence measure provided from one particular learned theory, during classification, to decide which one of the two different theories shall be used for final prediction.
Reference: <author> Buntine, W. </author> <year> (1991), </year> <title> Classifiers: A Theoretical and Empirical Study, </title> <booktitle> in Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 638-644, </pages> <publisher> Morgan-Kaufmann. </publisher>
Reference: <author> Catlett, J. </author> <year> (1991), </year> <title> Megainduction: machine learning on very large databases. </title> <type> Doctoral dissertation, </type> <institution> Basser Department of Computer Science, University of Sydney, Australia. </institution>
Reference: <author> Caruana, R. </author> <year> (1996), </year> <title> Algorithms and Applications for Multitask Learning, </title> <booktitle> in Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. 87-95, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Batches of data coming from seemingly similar but different sources, thus possibly different tasks, are not covered in our investigation in this paper. We do not think a theory combination method is a good choice in such a situation. The multitask learning method <ref> (Caruana, 1996) </ref> could be a better choice. The additional data can be used as an extra task to better support learning for the main task. A more subtle situation is that the data is pre-sorted into different groups according to some criterion when the data is collected (e.g., Baxt, 1992).
Reference: <author> Cestnik, B. </author> <year> (1990), </year> <title> Estimating Probabilities: A Crucial Task in Machine Learning, </title> <booktitle> in Proceedings of the European Conference on Artificial Intelligence, </booktitle> <pages> pp. 147-149. </pages>
Reference-contexts: Experiments and Results Two inductive learning algorithms, IB1* and NB* (Ting, 1994; 1996a), are used in our experiments. IB1* is a variant of IB1 (Aha, Kibler & Albert, 1991) that incorporates the modified value-difference metric (Cost & Salzberg, 1993) and NB* is an implementation of the Naive Bayes <ref> (Cestnik, 1990) </ref> algorithm. Both algorithms include a method (Fayyad & Irani, 1993) for discretising continuous-valued attributes in the preprocessing. This preprocessing improved the performance of the two algorithms in most of the continuous-valued attribute domains studied by Ting (1994).
Reference: <author> Chan, P.K. & S.J. </author> <title> Stolfo (1995), A Comparative Evaluation of Voting and Meta-learning on Partitioned Data, </title> <booktitle> in Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. 90-98, </pages> <publisher> Morgan Kaufmann. </publisher> <address> 19 Chan, P.K. & S.J. </address> <month> Stolfo </month> <year> (1996), </year> <title> On the Accuracy of Meta-learning for Scalable Data Mining, </title> <journal> in Journal of Intelligent System, </journal> <note> to appear. </note>
Reference: <author> Cost, S & S. </author> <title> Salzberg (1993), A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features, </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> pp. 57-78. </pages>
Reference-contexts: Experiments and Results Two inductive learning algorithms, IB1* and NB* (Ting, 1994; 1996a), are used in our experiments. IB1* is a variant of IB1 (Aha, Kibler & Albert, 1991) that incorporates the modified value-difference metric <ref> (Cost & Salzberg, 1993) </ref> and NB* is an implementation of the Naive Bayes (Cestnik, 1990) algorithm. Both algorithms include a method (Fayyad & Irani, 1993) for discretising continuous-valued attributes in the preprocessing.
Reference: <author> Craven, M.W. & J.W. </author> <title> Shavlik (1993), Learning to Represent Codons: A Challenge Problem for Constructive Induction, </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1319-1324. </pages>
Reference: <author> Fayyad, </author> <title> U.M. & K.B. Irani (1993), Multi-Interval Discretization of Continuous-Valued Attributes for Classification Learning, </title> <booktitle> in Proceedings of 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1022-1027. </pages>
Reference-contexts: IB1* is a variant of IB1 (Aha, Kibler & Albert, 1991) that incorporates the modified value-difference metric (Cost & Salzberg, 1993) and NB* is an implementation of the Naive Bayes (Cestnik, 1990) algorithm. Both algorithms include a method <ref> (Fayyad & Irani, 1993) </ref> for discretising continuous-valued attributes in the preprocessing. This preprocessing improved the performance of the two algorithms in most of the continuous-valued attribute domains studied by Ting (1994).
Reference: <author> Fayyad, U.M., N. Weir & S. </author> <title> Djorgovski (1993), SKICAT: A Machine Learning System for Automated Cataloging of Large Scale Sky Surveys, </title> <booktitle> in Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. 112-119, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: IB1* is a variant of IB1 (Aha, Kibler & Albert, 1991) that incorporates the modified value-difference metric (Cost & Salzberg, 1993) and NB* is an implementation of the Naive Bayes (Cestnik, 1990) algorithm. Both algorithms include a method <ref> (Fayyad & Irani, 1993) </ref> for discretising continuous-valued attributes in the preprocessing. This preprocessing improved the performance of the two algorithms in most of the continuous-valued attribute domains studied by Ting (1994).
Reference: <author> Freund, Y. & R.E. </author> <title> Schapire (1996), Experiments with a New Boosting Algorithm, </title> <booktitle> in Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. 148-156, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hansen, L.K. & P. </author> <month> Salamon </month> <year> (1990), </year> <title> Neural Network Ensembles, </title> <journal> in IEEE Transactions of Pattern Analysis and Machine Intelligence, </journal> <volume> 12, </volume> <pages> pp. 993-1001. </pages>
Reference: <author> Ho, </author> <title> T.K., J.J. Hull & S.N. Srihari (1994), Decision Combination in Multiple Classifier Systems, </title> <journal> in IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 16, No. 1, </volume> <pages> pp. 66-75. </pages>
Reference: <author> Jacobs, R.A., M.I. Jordan, S.J. Nowlan & G.E. </author> <title> Hinton (1991), Adaptive Mixtures of Local Experts, </title> <booktitle> in Neural Computation 3, </booktitle> <pages> pp. 79-87. </pages>
Reference: <author> Kearns, M. & H.S. </author> <title> Seung (1995), Learning from a Population of Hypotheses, </title> <journal> Machine Learning, </journal> <volume> 18, </volume> <pages> pp. 255-276, </pages> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Even for IB1* in the protein coding dataset (in Figure 11 (a)), when the complete learning curve is not observed, this phenomenon still occurs. This empirical result seems to be stronger, in terms of the expected performance of theory combination, than a theoretical result <ref> (Kearns & Seung, 1995) </ref> based on the same working assumption. This theoretical work seeks "... the possibility of somehow combining the independent hypotheses in a way that considerably outperforms any single hypothesis" (Kearns & Seung, 1995). The `single hypothesis' refers to any of the theory combination's constituent theories. <p> result seems to be stronger, in terms of the expected performance of theory combination, than a theoretical result <ref> (Kearns & Seung, 1995) </ref> based on the same working assumption. This theoretical work seeks "... the possibility of somehow combining the independent hypotheses in a way that considerably outperforms any single hypothesis" (Kearns & Seung, 1995). The `single hypothesis' refers to any of the theory combination's constituent theories.
Reference: <author> Kohavi, R. & D.H. </author> <title> Wolpert (1996), Bias Plus Variance Decomposition for Zero-One Loss Functions, </title> <booktitle> in Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. 275-283, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Meir's result states that theory combination outperforms data combination for small training data size, and theory combination can be worse for intermediate sample sizes. This result is based on the analysis of bias/variance decomposition which are quite different in classification tasks <ref> (Kohavi & Wolpert, 1996) </ref>. It also assumes that the data batches are independent for tractability; but our empirical result has no such assumption.
Reference: <author> Kononenko, I. & M. </author> <title> Kovacic (1992), Learning as Optimization: Stochastic Generation of Multiple Knowledge, </title> <booktitle> in Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. 257-262, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Cooper, 1993) by using different initial random weight configurations or/and orders of training data; multiple decision trees (Kwok & Carter, 1990; Buntine, 1991; Oliver & Hand, 1995) by selecting tests with information gains close to the maximum, generating option trees, or pruning a tree in different ways; and multiple rules <ref> (Kononenko & Kovacic, 1992) </ref> by stochastic search guided by heuristics. These works re-order the rank of the classes by (weighted) averaging the outputs of multiple neural networks, or class probabilities of multiple trees, or by using Naive Bayesian combination of different rules.
Reference: <author> Krogh, A. & J. </author> <month> Vedelsby </month> <year> (1995), </year> <title> Neural Network Ensembles, Cross Validation, and Active Learning, </title> <booktitle> in Advances in Neural Information Processing Systems 7, </booktitle> <editor> G. Tesauro, </editor> <publisher> D.S. </publisher>
Reference: <editor> Touretsky & T.K. Leen (Editors), </editor> <booktitle> pp. </booktitle> <pages> 231-238, </pages> <publisher> MIT Press. </publisher>
Reference: <author> Kwok, S. & C. </author> <title> Carter (1990), Multiple Decision Trees, </title> <booktitle> Uncertainty in Artificial Intelligence 4, </booktitle> <editor> R. Shachter, T. Levitt, L. Kanal and J. Lemmer (Editors), </editor> <booktitle> pp. </booktitle> <pages> 327-335, </pages> <publisher> North-Holland. 20 Meir, R. </publisher> <year> (1994), </year> <title> Bias, Variance and the Combination of Estimators: The Case of Linear Least Squares, </title> <type> Technical Report No. 922, </type> <institution> Department of Electrical Engineering, Technion, Haifa, Israel. </institution>
Reference: <author> Merz, C.J. </author> <year> (1995), </year> <title> Dynamic Learning Bias Selection, </title> <booktitle> in Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL: </address> <publisher> Unpublished, </publisher> <pages> pp. 386-395. </pages>
Reference: <author> Merz, C.J. & Murphy, P.M. </author> <year> (1996), </year> <note> UCI Repository of machine learning databases [http:// www.ics.uci.edu/ mlearn/MLRepository.html]. </note> <institution> Irvine, CA: University of California, Department of Information and Computer Science. </institution>
Reference-contexts: We use the nearest neighbour for making prediction in IB1* and the default settings are as used in IB1 1 in all experiments. No parameter settings are required for NB*. Our studies employ two artificial domains and four real-world datasets obtained from the UCI repository of machine learning databases <ref> (Merz & Murphy, 1996) </ref>. The two noisy artificial domains are the waveform and LED24 domains introduced by Breiman, Friedman, Olshen and Stone (1984). Each instance of the waveform domains contains twenty-one relevant and nineteen irrelevant continuous-valued attributes. There are three uniformly distributed classes in this domain.
Reference: <author> Oliver, J.J. </author> & <title> D.J. Hand (1995), On Pruning and Averaging Decision Trees, </title> <booktitle> in Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. 430-437. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Perrone, </author> <title> M.P. & L.N. Cooper (1993), When Networks Disagree: Ensemble Methods for Hybrid Neural Networks, in Artificial Neural Networks for Speech and Vision, R.J. Mammone (Editor), </title> <publisher> Chapman-Hall. </publisher>
Reference: <author> Provost, F.J. & D.N. </author> <title> Hennessy (1996), Scaling Up: Distributed Machine Learning with Cooperation, </title> <booktitle> in Proceedings of the Thirteen National Conference on Artificial Intelligence, </booktitle> <pages> pp. 74-79, </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Quinlan, J.R. </author> <year> (1996), </year> <title> Boosting, </title> <booktitle> Bagging, and C4.5, in Proceedings of the 13th National Conference on Artificial Intelligence, </booktitle> <pages> pp. 725-730, </pages> <publisher> AAAI Press. </publisher>
Reference: <author> Quinlan, J.R., P.J. Compton, K.A. Horn & L. </author> <title> Lazarus (1987), Inductive Knowledge Acquisition: A Case Study, in Applications of Expert Systems, </title> <editor> J.R. Quinlan (Editor). </editor> <publisher> Turing Institute Press with Addison Wesley. </publisher>
Reference: <author> Schaffer, C. </author> <year> (1993), </year> <title> Selecting a Classification Method by Cross-validation. </title> <booktitle> Preliminary Papers of the Fourth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pp. 15-25. </pages>
Reference-contexts: Nevertheless, more evidence is required to show that it is generally applicable. On the other aspect, there is no restriction that one must use a single learning algorithm for all batches of data. One may apply a model selection technique <ref> (Schaffer, 1993) </ref> to choose one among several learning algorithms for each batch of data, and then perform the combination. However, this incurs multiple folds of computational requirement.
Reference: <author> Schapire, R.E. </author> <year> (1990), </year> <title> The Strength of Weak Learnability, </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> pp. 197-227, </pages> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: & Salamon, 1990; Perrone & Cooper, 1993; Oliver & Hand, 1995; Chan & Stolfo, 1995; Breiman, 1996a,1996b; Freund & Schapire, 1996; Ali & Pazzani, 1996) only show the possibility of improving the performance using multiple models in some datasets, without considering the (near-)asymptotic performance; despite the theoretical result of boosting <ref> (Schapire, 1990) </ref> shows that this is possible (Schapire, 1996). In regression settings, Meir (1994) mathematically analyses the effect of (linearly) combining several least squares linear estimators on the expected performance, under the same working assumption.
Reference: <author> Schapire, R.E. </author> <year> (1996), </year> <title> private communication. </title>
Reference-contexts: Oliver & Hand, 1995; Chan & Stolfo, 1995; Breiman, 1996a,1996b; Freund & Schapire, 1996; Ali & Pazzani, 1996) only show the possibility of improving the performance using multiple models in some datasets, without considering the (near-)asymptotic performance; despite the theoretical result of boosting (Schapire, 1990) shows that this is possible <ref> (Schapire, 1996) </ref>. In regression settings, Meir (1994) mathematically analyses the effect of (linearly) combining several least squares linear estimators on the expected performance, under the same working assumption.
Reference: <author> Sejnowski, T.J. & C.R. </author> <title> Rosenberg (1987), Parallel networks that learn to pronounce English text, </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> pp. 145-168. </pages>
Reference-contexts: It consists of 3163 case data and diagnoses for one of the many thyroid disorders: euthyroidism. Eighteen binary attributes and seven continuous-valued attributes are used in this dataset. The task is to predict whether a patient suffers euthyroid or not. The goal of the NETtalk task <ref> (Sejnowski & Rosenberg, 1987) </ref> is to learn to pronounce English words by studying a dictionary of correct pronunciations. In this task, each letter to be pronounced is presented to the classifier together with the three preceding and three succeeding letters in the word.
Reference: <author> Sollich, P. & A. </author> <title> Krogh (1996), Learning with ensembles: How overfitting can be useful, </title> <booktitle> in Advances in Neural Information Processing Systems 8, </booktitle> <editor> D.S. Touretzky, M.C. Mozer & M.E. </editor> <publisher> Hasselmo (Editors) , MIT Press. </publisher>
Reference: <author> Tcheng, D., B. Lambert, C-Y. Lu & L. </author> <title> Rendell (1989), Building Robust Learning Systems by Combining Induction and Optimization, </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 806-812, </pages> <publisher> Morgan Kaufmann. </publisher> <address> 21 Ting, K.M. </address> <year> (1994), </year> <title> Discretization of Continuous-Valued Attributes and Instance-Based Learning, </title> <type> Technical Report No.491, </type> <institution> Basser Department of Computer Science, University of Sydney. </institution>
Reference-contexts: Some methods provide guidance as to how to partition the description space. Some use information gain criterion (Utgoff, 1989), user-provided information <ref> (Tcheng, Lambert & Rendell, 1989) </ref>, or hand-crafted rules (Brodley, 1993) to guide the recursive partitioning process in a tree structure; and others (Ting, 1994; Wettschereck, 1994) employ a confidence measure provided from one particular learned theory, during classification, to decide which one of the two different theories shall be used for
Reference: <author> Ting, K.M. </author> & <title> R.M. Cameron-Jones (1994), Exploring a Framework for Instance Based Learning and Naive Bayesian Classifiers, </title> <booktitle> in Proceedings of the Seventh Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 100-107, </pages> <publisher> World Scientific. </publisher>
Reference-contexts: For the IB1* settings we are using here, the contribution of random guesses could be much higher. NB* also has the right bias in this domain because it is equivalent to an one instance per class instance-based learner <ref> (Ting and Cameron-Jones, 1994) </ref>. It approaches the best result in the near-asymptotic region for all methods shown in Figure 5 (b). This explains why theory combination can not outperform data combination in this region.
Reference: <author> Ting, K.M. </author> <year> (1996a), </year> <note> Discretisation in Lazy Learning Algorithms, to appear in the special issue of Lazy Learning in Artificial Intelligence Review Journal. </note>
Reference: <author> Ting, K.M. </author> <year> (1996b), </year> <title> The Characterisation of Predictive Accuracy and Decision Combination, </title> <booktitle> in Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. 498-506, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is also true when this combination is being stacked up to form a tree, i.e., a binary tree is better than higher order trees (Chan & Stolfo; 1995). We employ a recent theory combination method <ref> (Ting, 1996b) </ref> to conduct our investigation. This is based on the characterisation and estimation of predictive accuracy for each prediction of a classifier. Each classifier is trained independently and uses a cross-validation method to perform an estimation of predictive accuracy. <p> Ronny Meir provides some pointer to the related work in NIPS. This paper is prepared with support from the Direct Grant from the Chinese University of Hong Kong and Department of Computer Science, University of Waikato. 17 Appendix The Method of Theory Combination We use the composite learner framework <ref> (Ting, 1996b) </ref> as the method for theory combination in our experiments. Ting uses the term the characterisation of predictive accuracy to mean the use of a measure in an induced theory as an indicator for its predictive accuracy.
Reference: <author> Towell, G., J. Shavlik & M. </author> <month> Noordewier </month> <year> (1990), </year> <title> Refinement of Approximate Domain Theories by Knowledge-Based Artificial Neural Networks, </title> <booktitle> in Proceedings of the Eighth National Conference on Artificial Intelligence. </booktitle>
Reference: <author> Utgoff, P.E. </author> <year> (1989), </year> <title> Perceptron Trees: A case study in hybrid concept representations, </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> pp. 337-391. </pages>
Reference-contexts: Some methods provide guidance as to how to partition the description space. Some use information gain criterion <ref> (Utgoff, 1989) </ref>, user-provided information (Tcheng, Lambert & Rendell, 1989), or hand-crafted rules (Brodley, 1993) to guide the recursive partitioning process in a tree structure; and others (Ting, 1994; Wettschereck, 1994) employ a confidence measure provided from one particular learned theory, during classification, to decide which one of the two different theories
Reference: <author> Wettschereck, D. </author> <year> (1994), </year> <title> A Hybrid Nearest-Neighbor and Nearest-Hyperrectangle Algorithm, </title> <booktitle> in Proceedings of the Seventh European Conference on Machine Learning, LNAI-784, </booktitle> <pages> pp. 323-335, </pages> <publisher> Springer Verlag. </publisher>
Reference: <author> Wolpert, D.H. </author> <year> (1992), </year> <title> Stacked Generalization, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 5, </volume> <pages> pp. 241-259, </pages> <publisher> Pergamon Press. </publisher> <pages> 22 </pages>
Reference-contexts: During classification, the network trained using the low risk group is used if its output is below certain threshold, otherwise the other network is used instead. This method may only be applicable when the information about the sorting criterion is available. Stacked generalisation <ref> (Wolpert, 1992) </ref> is a general method of combining multiple models learned from the entire dataset. The models can either be induced from the same or different learning algorithms (Merz, 1995; Ho, Hull & Srihari, 1994).
References-found: 46

