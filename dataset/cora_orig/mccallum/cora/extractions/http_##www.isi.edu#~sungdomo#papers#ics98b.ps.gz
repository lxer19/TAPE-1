URL: http://www.isi.edu/~sungdomo/papers/ics98b.ps.gz
Refering-URL: http://www.isi.edu/~sungdomo/publications.html
Root-URL: http://www.isi.edu
Email: fbso,sungdomo,mhallg@isi.edu  
Title: Measuring the Effectiveness of Automatic Parallelization in SUIF  
Author: Byoungro So, Sungdo Moon, and Mary W. Hall 
Address: Marina del Rey, CA 90292  
Affiliation: USC Information Sciences Institute  
Abstract: This paper presents both an experiment and a system for inserting run-time dependence and privatization testing. The goal of the experiment is to measure empirically the remaining opportunities for exploiting loop-level parallelism that are missed by state-of-the-art parallelizing compiler technology. We perform run-time testing of data accessed within all candidate loops not parallelized by the compiler to identify which of these loops could safely execute in parallel for the given program input. This system extends the Lazy Privatizing Doall (LPD) test to simultaneously instrument multiple loops in a nest. Using the results of interprocedural array data-flow analysis, we avoid unnecessary instrumentation of arrays with compile-time provable dependences or loops nested inside outer parallelized loops. We have implemented the system in the Stan-ford SUIF compiler and have measured programs in three benchmark suites. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. P. Amarasinghe. </author> <title> Parallelizing Compiler Techniques Based on Linear Inequalities. </title> <type> PhD thesis, </type> <institution> Dept. of Electrical Engineering, Stanford University, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: We omit discussion of how these data-flow values are calculated, and refer the reader to previous publications <ref> [1, 10] </ref>. At a particular program region corresponding to loop L, the portions of arrays described by each set in the 4-tuple are parameterized by loop index variable i (where, for clarity of presentation, i is assumed to be normalized to start at 1 and step by 1).
Reference: [2] <author> W. Blume, R. Doallo, R. Eigenmann, J. Grout, J. Hoeflinger, T. Lawrence, J. Lee, D. Padua, Y. Paek, B. Pottenger, L. Rauchw-erger, and P. Tu. </author> <title> Parallel programming with Polaris. </title> <journal> IEEE Computer, </journal> <volume> 29(12):7882, </volume> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Parallelizing compilers are becoming increasingly successful at exploiting coarse-grain parallelism in scientific computations, as evidenced by recent experimental results from both the Polaris system at University of Illinois and from the Stanford SUIF compiler <ref> [2, 9] </ref>. While these results are impressive overall, some of the programs presented achieve little or no speedup when executed in parallel. This observation raises again questions that have been previously addressed by experiments in the early 90's [3, 20].
Reference: [3] <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the Perfect Benchmark programs. </title> <journal> IEEE Transaction on Parallel and Distributed Systems, </journal> <volume> 3(6):643656, </volume> <month> November </month> <year> 1992. </year>
Reference-contexts: While these results are impressive overall, some of the programs presented achieve little or no speedup when executed in parallel. This observation raises again questions that have been previously addressed by experiments in the early 90's <ref> [3, 20] </ref>. <p> Section 6 presents the experimental results. 2 Related Work Potential Parallelism Experiments. A number of experiments in the early 90s performed hand parallelization of benchmark programs to identify opportunities to improve the effectiveness of parallelizing compilers <ref> [3, 14, 20] </ref>. Two of these experiments compared hand-parallelized programs to compiler-parallelized versions, pointing to the large gap between inherent parallelism in the programs and what commercial compilers of the time were able to exploit [3, 20]. <p> Two of these experiments compared hand-parallelized programs to compiler-parallelized versions, pointing to the large gap between inherent parallelism in the programs and what commercial compilers of the time were able to exploit <ref> [3, 20] </ref>. Blume and Eigenmann performed such a comparison for 13 programs from the Perfect benchmark suite [3]. They cited the need for compilers to incorporate array privatization and interprocedural analysis, among other things, to exploit a coarser granularity of parallelism. <p> Two of these experiments compared hand-parallelized programs to compiler-parallelized versions, pointing to the large gap between inherent parallelism in the programs and what commercial compilers of the time were able to exploit [3, 20]. Blume and Eigenmann performed such a comparison for 13 programs from the Perfect benchmark suite <ref> [3] </ref>. They cited the need for compilers to incorporate array privatization and interprocedural analysis, among other things, to exploit a coarser granularity of parallelism. These early studies focused developers of commercial and research compiler-writers to investigate incorporating these techniques, and now they are beginning to make their way into practice.
Reference: [4] <author> K. D. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKin-ley, J. M. Mellor-Crummey, L. Torczon, and S. Warren. </author> <title> The ParaS-cope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 81(2):244263, </address> <month> Feburary </month> <year> 1993. </year>
Reference-contexts: Incorporation of other optimizations in the literature is complementary to what we have done. We also extend the LPD test to evaluate multiple loops in a nest simultaneously. Parallel Programming Tools. A number of parallel programming tools, including ParaScope <ref> [4] </ref> and PIPS [11], allow a user to examine the data dependences in loops in their program and make assertions about whether the dependences should be ignored by the compiler.
Reference: [5] <author> K. D. Cooper, M. W. Hall, and K. Kennedy. </author> <title> A methodology for procedure cloning. </title> <booktitle> Computer Languages, </booktitle> <address> 19(2):105117, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: To behave correctly in the presence of different calling contexts, analysis can employ procedure cloning to replicate the procedure body and tailor it to the set of instrumented variables and OuterPar for a particular calling context <ref> [5, 15] </ref>. Alternatively, the procedure body can examine flags passed to it as parameters at run time to decide whether instrumentation is required for the current calling context.
Reference: [6] <author> G. Goff. </author> <title> Practical techniques to augment dependence analysis in the presence of symbolic terms. </title> <type> Technical Report TR92194, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: For the loops in this category, it is straightforward to derive breaking conditions by extracting constraints on dependences directly from the dependence and privatization tests <ref> [6, 18] </ref>. * C+B: Identifies loops that require both of the previously described techniques. In some cases, derivation of breaking conditions is not as simple as extracting them directly from the dependence test.
Reference: [7] <author> J. Gu, Z. Li, and G. Lee. </author> <title> Symbolic array dataflow analysis for array privatization and program parallelization. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, California, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: While not in common practice, a few techniques refine their array data-flow analysis results in this way <ref> [7, 16, 22] </ref>. * BC: Identifies certain loops whose safe parallelization depends on values of variables not known at compile time. 6 Program Total NL CF BC C+B OD CI IE DD apsi 32 1 21 0 9 1 0 0 0 mgrid 1 0 0 0 0 0 0 1
Reference: [8] <author> M. W. Hall, S. P. Amarasinghe, B. R. Murphy, S.-W. Liao, and M. S. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, California, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: tracing facility that traces accesses to dependent arrays to perform this run-time testing, rather than pruning the instrumentation arrays using the analysis approach presented here. 3 Background on Parallelization Analysis The system described in this paper augments an existing automatic parallelization system that is part of the Stanford SUIF compiler <ref> [8, 9, 10] </ref>. The system parallelizes loops whose iterations can be executed in parallel, partitioning the iterations to execute on different processors. <p> In the NAS benchmark suite, only buk and fftpde failed to achieve a speedup. The compiler was less successful in parallelizing the PERFECT benchmark suite, obtaining a speedup on only 5 of the 12 programs in the experiment <ref> [8] </ref>. Of the six PERFECT programs presented here, only adm and trak failed to obtain a speedup.
Reference: [9] <author> M. W. Hall, J. M. Anderson, S. P. Amarasinghe, B. R. Murphy, S.- W. Liao, E. Bugnion, and M. S. Lam. </author> <title> Maximizing multiprocessor performance with the SUIF compiler. </title> <journal> IEEE Computer, </journal> <volume> 29(12):8489, </volume> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Parallelizing compilers are becoming increasingly successful at exploiting coarse-grain parallelism in scientific computations, as evidenced by recent experimental results from both the Polaris system at University of Illinois and from the Stanford SUIF compiler <ref> [2, 9] </ref>. While these results are impressive overall, some of the programs presented achieve little or no speedup when executed in parallel. This observation raises again questions that have been previously addressed by experiments in the early 90's [3, 20]. <p> tracing facility that traces accesses to dependent arrays to perform this run-time testing, rather than pruning the instrumentation arrays using the analysis approach presented here. 3 Background on Parallelization Analysis The system described in this paper augments an existing automatic parallelization system that is part of the Stanford SUIF compiler <ref> [8, 9, 10] </ref>. The system parallelizes loops whose iterations can be executed in parallel, partitioning the iterations to execute on different processors. <p> In a previous publication, SUIF achieved a speedup on seven of the SPEC95FP programs; of these seven, su2cor achieved a speedup of only 4 on 8 processors of a Digital Alphaserver 8400 <ref> [9] </ref>. The remaining six obtained a speedup of more than 6. The programs apsi, wave5, and fpppp were the only three not to obtain a speedup. In the NAS benchmark suite, only buk and fftpde failed to achieve a speedup.
Reference: [10] <author> M. W. Hall, B. R. Murphy, S. P. Amarasinghe, S.-W. Liao, and M. S. Lam. </author> <title> Interprocedural analysis for parallelization. </title> <booktitle> In Proceedings of the 8th International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 6180, </pages> <address> Columbus, Ohio, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: tracing facility that traces accesses to dependent arrays to perform this run-time testing, rather than pruning the instrumentation arrays using the analysis approach presented here. 3 Background on Parallelization Analysis The system described in this paper augments an existing automatic parallelization system that is part of the Stanford SUIF compiler <ref> [8, 9, 10] </ref>. The system parallelizes loops whose iterations can be executed in parallel, partitioning the iterations to execute on different processors. <p> Further discussion of reductions is omitted in this paper for clarity of presentation.) The compiler uses an interprocedural array data-flow analysis to determine which loops access independent memory locations, or for which privatization eliminates remaining dependences <ref> [10] </ref>. The analysis computes data-flow values for each program region, where a region is either a basic block, a loop body, a loop, a procedure call, or a procedure body. <p> We omit discussion of how these data-flow values are calculated, and refer the reader to previous publications <ref> [1, 10] </ref>. At a particular program region corresponding to loop L, the portions of arrays described by each set in the 4-tuple are parameterized by loop index variable i (where, for clarity of presentation, i is assumed to be normalized to start at 1 and step by 1). <p> A subsequent transformation phase, described in the next section, actually inserts the instrumentation code. The instrumentation analysis is fully inter-procedural, and it is implemented in the interprocedural framework that is part of the SUIF system <ref> [10] </ref>. The analysis phase uses the results of array data-flow analysis to significantly limit the amount of work performed by instrumentation. Unlike previous work, the analysis distinguishes between dependences that the compiler can prove and ones it has assumed because of conservative approximations. <p> The interprocedural analysis applies the same basic interproce-dural analysis framework used for array data-flow analysis, as discussed in Section 3 <ref> [10] </ref>. In this case, the only interesting program regions are loops, procedure bodies and procedure calls. The interprocedural algorithm is defined in Figure 1.
Reference: [11] <author> F. Irigoin, P. Jouvelot, and R. Triolet. </author> <title> Semantical interprocedural par-allelization: An overview of the PIPS project. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <pages> pages 244 251, </pages> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Incorporation of other optimizations in the literature is complementary to what we have done. We also extend the LPD test to evaluate multiple loops in a nest simultaneously. Parallel Programming Tools. A number of parallel programming tools, including ParaScope [4] and PIPS <ref> [11] </ref>, allow a user to examine the data dependences in loops in their program and make assertions about whether the dependences should be ignored by the compiler.
Reference: [12] <author> T. Lawrence. </author> <title> Implementation of run time techniques in the Polaris Fortran restructurer. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1996. </year>
Reference-contexts: We augment the Lazy Privatizing Doall (LPD) test to instrument and test whether any of the candidate unparallelized loops in the program can be safely parallelized at run time <ref> [12, 19] </ref>. Our system extends the LPD test to instrument multiple loops in a nest, and perform instrumentation across procedure boundaries. We are thus able to locate all the loops in the program whose iterations can be safely executed in parallel for a particular program input. <p> Rauchwerger and Padua present the LPD test we use here. Later work on the LPD test and a description of an implementation in Polaris are described by Lawrence <ref> [12] </ref>. Rauchwerger and Padua also describe an extended version called the LRPD test that also locates array reductions [19]. We ignore array reductions beyond the ones already parallelized by SUIF. <p> To correctly privatize such arrays requires that the compiler initialize the upwards exposed locations in the private copy of the array prior to executing the loop. The LPD test has a more restricted privatization criterion, where an exposed read cannot be written at all in the loop <ref> [12, 19] </ref>. 4 Instrumentation Analysis The instrumentation system uses the results of array data-flow analysis and dependence and privatization tests to decide which loops and which variables in each loop should be instrumented. An initial instrumentation analysis phase, described in this section, designates loops and arrays for instrumentation. <p> For this purpose, we extend the Lazy Privatizing DOALL test defined by Rauchwerger and Padua and subsequently refined by Lawrence <ref> [12, 19] </ref>. The most significant difference in the test we have developed is the extension of LPD to simultaneously test for independence and privatization across multiple loops in a nest, which we will describe after initially presenting the LPD test within a single loop. <p> Reducing Space Requirements. As compared to previous work, our approach can be implemented such that it reduces the space requirements for shadow arrays. Our formulation uses a single boolean O to test for output dependences, replacing a full integer shadow array used in previous work <ref> [12, 19] </ref>. The shadow array is not necessary because we are using a time stamp of the iteration of the write access for the write shadow array S w rather than the boolean used by Rauchwerger and Padua [19]. <p> To further reduce the space requirements, we can employ a space-saving technique suggested by Lawrence <ref> [12] </ref>. <p> Using signs on read and write shadow arrays reduces the total space requirements to two integer shadow arrays plus a scalar boolean for output dependence rather than the three boolean and one integer shadow arrays used by Rauchw-erger and Padua [19] or the four integer shadow arrays used by Lawrence <ref> [12] </ref>. 5.2 Instrumenting Multiple Loops in a Nest Suppose loop L k is nested inside loop L j and for both loops, array A must be instrumented (i.e., A 2 Instr (L k ) ^ A 2 Instr (L j )).
Reference: [13] <author> S.-W. Liao, J. Robert P. Bosch, A. Ghuloum, and M. S. Lam. </author> <title> SUIF Explorer: A programming assistant for parallel machines. </title> <booktitle> In Proceedings of the 2nd SUIF Compiler Workshop, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: Recently, the SUIF Explorer has been developed to provide a programmer interface to export the compiler's knowledge about why it failed to parallelize a loop, and allow the user to make higher-level assertions such as whether arrays are privatizable or can be transformed to parallel reductions <ref> [13] </ref>. The SUIF Explorer incorporates run-time parallelization testing to pinpoint loops that are potentially parallelizable. Such a tool could be useful to users in developing and debugging parallel programs. For example, the !HPF$INDEPENDENT directive in High Performance Fortran allows users to assert to the compiler that a loop is parallelizable.
Reference: [14] <author> K. S. McKinley. </author> <title> Automatic and Interactive Parallelization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: Section 6 presents the experimental results. 2 Related Work Potential Parallelism Experiments. A number of experiments in the early 90s performed hand parallelization of benchmark programs to identify opportunities to improve the effectiveness of parallelizing compilers <ref> [3, 14, 20] </ref>. Two of these experiments compared hand-parallelized programs to compiler-parallelized versions, pointing to the large gap between inherent parallelism in the programs and what commercial compilers of the time were able to exploit [3, 20].
Reference: [15] <author> J. Mellor-Crummey. </author> <title> Compile-time support for efficient data race detection in shared-memory parallel programs. </title> <booktitle> In Proceedings of ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 129139, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: To behave correctly in the presence of different calling contexts, analysis can employ procedure cloning to replicate the procedure body and tailor it to the set of instrumented variables and OuterPar for a particular calling context <ref> [5, 15] </ref>. Alternatively, the procedure body can examine flags passed to it as parameters at run time to decide whether instrumentation is required for the current calling context.
Reference: [16] <author> S. Moon, M. W. Hall, and B. R. Murphy. </author> <title> Predicated array data-flow analysis for run-time parallelization. </title> <booktitle> In Proceedings of the 1998 ACM International Conference on Supercomputing, pages 204211, </booktitle> <address> Mel-bourne, Australia, </address> <month> July </month> <year> 1998. </year>
Reference-contexts: While not in common practice, a few techniques refine their array data-flow analysis results in this way <ref> [7, 16, 22] </ref>. * BC: Identifies certain loops whose safe parallelization depends on values of variables not known at compile time. 6 Program Total NL CF BC C+B OD CI IE DD apsi 32 1 21 0 9 1 0 0 0 mgrid 1 0 0 0 0 0 0 1 <p> In other work, we have developed an approach called predicated array data-flow analysis to combine the two techniques and provide a more general mecha nism for deriving and exploiting breaking conditions <ref> [16] </ref>. * OD: Identifies loops with only output dependences, all of which have control flow tests based on loop-invariant array values to determine whether the write occurs on each iteration.
Reference: [17] <author> R. Ponnusamy, J. Saltz, A. Choudhary, Y.-S. Hwang, and G. Fox. </author> <title> Run-time support and compilation methods for user-specified irregular data distributions. </title> <journal> IEEE Transaction on Parallel and Distributed Systems, </journal> <volume> 6(8):815831, </volume> <month> August </month> <year> 1995. </year>
Reference-contexts: Run-Time Parallelization Techniques. The literature describes run-time parallelization techniques based on an inspector/executor model, where an inspector tests subscript expressions in the loop at run time and an executor executes the loop in parallel if the inspector determines it is safe <ref> [17, 19] </ref>. Rauchwerger and Padua present the LPD test we use here. Later work on the LPD test and a description of an implementation in Polaris are described by Lawrence [12]. Rauchwerger and Padua also describe an extended version called the LRPD test that also locates array reductions [19]. <p> Ponnusamy et al. discuss avoiding re-computation of the inspector on subsequent executions of a loop when values of variables accessed by the inspector have not changed <ref> [17] </ref>. Our system does not use these techniques, but it does introduce new analyses to avoid unnecessary instrumentation by examining the results of interprocedural array data-flow analysis. We do not instrument compile-time provably dependent arrays or loops nested inside already parallelized loops. <p> Such dependences can be parallelized with a simple inspector/executor that only determines control flow paths taken through the loop (a control inspector). * IE: Identifies loops that can probably only be parallelized with an inspector/executor model <ref> [17, 19] </ref>.
Reference: [18] <author> W. Pugh and D. Wonnacott. </author> <title> Eliminating false data dependences using the Omega test. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 140 151, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: For the loops in this category, it is straightforward to derive breaking conditions by extracting constraints on dependences directly from the dependence and privatization tests <ref> [6, 18] </ref>. * C+B: Identifies loops that require both of the previously described techniques. In some cases, derivation of breaking conditions is not as simple as extracting them directly from the dependence test.
Reference: [19] <author> L. Rauchwerger and D. Padua. </author> <title> The LRPD test: Speculative run-time parallelization of loops with privatization and reduction paralleliza-tion. </title> <booktitle> In Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 218232, </pages> <address> La Jolla, California, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: We augment the Lazy Privatizing Doall (LPD) test to instrument and test whether any of the candidate unparallelized loops in the program can be safely parallelized at run time <ref> [12, 19] </ref>. Our system extends the LPD test to instrument multiple loops in a nest, and perform instrumentation across procedure boundaries. We are thus able to locate all the loops in the program whose iterations can be safely executed in parallel for a particular program input. <p> Run-Time Parallelization Techniques. The literature describes run-time parallelization techniques based on an inspector/executor model, where an inspector tests subscript expressions in the loop at run time and an executor executes the loop in parallel if the inspector determines it is safe <ref> [17, 19] </ref>. Rauchwerger and Padua present the LPD test we use here. Later work on the LPD test and a description of an implementation in Polaris are described by Lawrence [12]. Rauchwerger and Padua also describe an extended version called the LRPD test that also locates array reductions [19]. <p> Rauchwerger and Padua present the LPD test we use here. Later work on the LPD test and a description of an implementation in Polaris are described by Lawrence [12]. Rauchwerger and Padua also describe an extended version called the LRPD test that also locates array reductions <ref> [19] </ref>. We ignore array reductions beyond the ones already parallelized by SUIF. <p> Rauchwerger and Padua suggest executing the inspector in parallel, and even executing the entire loop in parallel speculatively and restoring the original state if it turns out not to be parallelizable <ref> [19] </ref>. Ponnusamy et al. discuss avoiding re-computation of the inspector on subsequent executions of a loop when values of variables accessed by the inspector have not changed [17]. <p> To correctly privatize such arrays requires that the compiler initialize the upwards exposed locations in the private copy of the array prior to executing the loop. The LPD test has a more restricted privatization criterion, where an exposed read cannot be written at all in the loop <ref> [12, 19] </ref>. 4 Instrumentation Analysis The instrumentation system uses the results of array data-flow analysis and dependence and privatization tests to decide which loops and which variables in each loop should be instrumented. An initial instrumentation analysis phase, described in this section, designates loops and arrays for instrumentation. <p> For this purpose, we extend the Lazy Privatizing DOALL test defined by Rauchwerger and Padua and subsequently refined by Lawrence <ref> [12, 19] </ref>. The most significant difference in the test we have developed is the extension of LPD to simultaneously test for independence and privatization across multiple loops in a nest, which we will describe after initially presenting the LPD test within a single loop. <p> Reducing Space Requirements. As compared to previous work, our approach can be implemented such that it reduces the space requirements for shadow arrays. Our formulation uses a single boolean O to test for output dependences, replacing a full integer shadow array used in previous work <ref> [12, 19] </ref>. The shadow array is not necessary because we are using a time stamp of the iteration of the write access for the write shadow array S w rather than the boolean used by Rauchwerger and Padua [19]. <p> The shadow array is not necessary because we are using a time stamp of the iteration of the write access for the write shadow array S w rather than the boolean used by Rauchwerger and Padua <ref> [19] </ref>. To further reduce the space requirements, we can employ a space-saving technique suggested by Lawrence [12]. <p> Using signs on read and write shadow arrays reduces the total space requirements to two integer shadow arrays plus a scalar boolean for output dependence rather than the three boolean and one integer shadow arrays used by Rauchw-erger and Padua <ref> [19] </ref> or the four integer shadow arrays used by Lawrence [12]. 5.2 Instrumenting Multiple Loops in a Nest Suppose loop L k is nested inside loop L j and for both loops, array A must be instrumented (i.e., A 2 Instr (L k ) ^ A 2 Instr (L j )). <p> Such dependences can be parallelized with a simple inspector/executor that only determines control flow paths taken through the loop (a control inspector). * IE: Identifies loops that can probably only be parallelized with an inspector/executor model <ref> [17, 19] </ref>. <p> The only approach we know that could parallelize such loops is a speculative inspector/executor model, where the loop is parallelized speculatively, and the inspector is run concurrently with executing the loop <ref> [19] </ref>. The thirteen programs contain a total of 122 additional paral-lelizable loops found by the LPD test. (Note that this number contains only loops that were executed at run time and that were not nested inside SUIF-proven parallel loops.
Reference: [20] <author> J. P. Singh and J. L. Hennessy. </author> <title> An empirical investigation of the effectiveness and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: While these results are impressive overall, some of the programs presented achieve little or no speedup when executed in parallel. This observation raises again questions that have been previously addressed by experiments in the early 90's <ref> [3, 20] </ref>. <p> Section 6 presents the experimental results. 2 Related Work Potential Parallelism Experiments. A number of experiments in the early 90s performed hand parallelization of benchmark programs to identify opportunities to improve the effectiveness of parallelizing compilers <ref> [3, 14, 20] </ref>. Two of these experiments compared hand-parallelized programs to compiler-parallelized versions, pointing to the large gap between inherent parallelism in the programs and what commercial compilers of the time were able to exploit [3, 20]. <p> Two of these experiments compared hand-parallelized programs to compiler-parallelized versions, pointing to the large gap between inherent parallelism in the programs and what commercial compilers of the time were able to exploit <ref> [3, 20] </ref>. Blume and Eigenmann performed such a comparison for 13 programs from the Perfect benchmark suite [3]. They cited the need for compilers to incorporate array privatization and interprocedural analysis, among other things, to exploit a coarser granularity of parallelism.
Reference: [21] <author> P. Tu. </author> <title> Automatic Array Privatization and Demand-driven Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illi-nois at Urbana-Champaign, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Arc2d has two loops that could be parallelized taking control flow into account, and one loop that we categorize as requiring an inspector/executor because subscript expressions access index arrays; however, a special-case array element symbolic analysis could parallelize this loop, as is done in Polaris <ref> [21] </ref>. If we look at the eight columns, two refer to loops that could be parallelized statically, NL and CF, making up 58 of the total loops (and this number is only so large because apsi and adm are the same program).
Reference: [22] <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proceedings of the 6th International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 500521, </pages> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year> <month> 8 </month>
Reference-contexts: While not in common practice, a few techniques refine their array data-flow analysis results in this way <ref> [7, 16, 22] </ref>. * BC: Identifies certain loops whose safe parallelization depends on values of variables not known at compile time. 6 Program Total NL CF BC C+B OD CI IE DD apsi 32 1 21 0 9 1 0 0 0 mgrid 1 0 0 0 0 0 0 1
References-found: 22

