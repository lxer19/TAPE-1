URL: http://polaris.cs.uiuc.edu/reports/1252.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Abstract-found: 0
Intro-found: 1
Reference: [Aboe91] <author> M. Aboelaze, </author> <title> ``Multi-level hypercube network,'' </title> <booktitle> 1991 Int. Parallel Processing Symp., </booktitle> <address> Anaheim CA, </address> <month> April 30 May 2, </month> <pages> pp. 475-480. </pages>
Reference-contexts: In addition, hierarchical interconnects have been an active area of research over the past 20 years. For example, there have been hierarchies based on shared buses [WuLi81], crossbar switches [AgMa85], meshes [Carl85], hypercubes [DaEa90] [DaEa91] <ref> [Aboe91] </ref>, and combinations of Omega networks and composite cube networks [Padm91a]. In Section 4.2 we attempt to develop a taxonomy of hierarchical systems that will provide the architectural context for our performance studies of hierarchical systems in Chapter 5. <p> SGC/EA avoids this problem, and provides a more balanced organization. Other researchers have studied systems similar to SGC/IP. [Carl85] is SGC/IP with global and local meshes. <ref> [Aboe91] </ref> is essentially a generalization of SGC/IP to multiple levels of hierarchy, for hypercubes. 67 4.2.2. Partitioned global channels (PGC) An alternative to shared global channels is a partitioned global channels (PGC) approach.
Reference: [AbPa89] <author> S. Abraham and K. Padmanabhan, </author> <title> ``Performance of the direct binary n-cube network for multiprocessors,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 38, no. 7, </volume> <month> July </month> <year> 1989, </year> <pages> pp. 1000-1011. </pages>
Reference-contexts: Shared global channels (SGC) Consider the standard model of a point-to-point architecture in Figure 4.1 (see also <ref> [AbPa89] </ref>): we model each node with fanout B as a processor connected to a (B+1)x (B+1) crossbar. The SGC approach simply translates the switch organization of flat systems to clustered systems. Hence, each cluster or supernode should have associated with it a shared global crossbar switch of fanout B. <p> Routing strategy does not affect latency without contention or maximum network 78 capacity. However, at low to medium traffic, hypercubes and tori that use LR routing have lower queueing delays than the same systems that use random routing. As explained in <ref> [AbPa89] </ref>, this is because of the lower probability of multiple simultaneous message arrivals at high-dimensional channels. [AbPa90] presented an analysis of the torus with random routing; an approach to studying LR routing in tori will be presented in Section 5.5.2. <p> Let P y,x be the probability that a message received on a y link will continue on an x link. Let P x,x be the probability that a message received on an x link will continue on an x link. We follow the method used in <ref> [AbPa89] </ref>, and count the number of source-destination (S-D) pairs that use specific links. Consider an incoming y link to processor i, and the messages that terminate at i. <p> Modeling hypercubes with locality We will use left-to-right (LR) routing in the FH. With LR routing, the bits of a message header are scanned from the highest order bit to lowest order bit. The random routing analysis of <ref> [AbPa89] </ref> assumed that a processor cannot send messages to itself. The LR routing analysis of [AbPa89] made the implicit inconsistent assumption that a processor can do this. <p> With LR routing, the bits of a message header are scanned from the highest order bit to lowest order bit. The random routing analysis of <ref> [AbPa89] </ref> assumed that a processor cannot send messages to itself. The LR routing analysis of [AbPa89] made the implicit inconsistent assumption that a processor can do this. While this inconsistency is almost negligible for uniform traffic, the effects of the inconsistency could be appreciable if we consider locality of traffic, for medium to large localities.
Reference: [AbPa90] <author> S. Abraham and K. Padmanabhan, </author> <title> ``Constraint based evaluation of multicom-puter networks,'' </title> <booktitle> 1990 Int. Conf. on Parallel Processing, Aug. 1990, </booktitle> <volume> vol. I, </volume> <pages> pp. 521-525. </pages>
Reference-contexts: Limited wiring costs force the construction of narrow channels, and messages have to be broken into many nibbles and transmitted over many cycles. We follow the approach taken by Dally [Dall90] and Abraham and Padmanabhan <ref> [AbPa90] </ref>, comparing networks based on wiring costs. We will apply these methods to re-examine the well-known concept of hierarchical systems. Our work will yield new perspectives on hierarchical systems and introduce new structures to help overcome the problems of packaging large-scale systems. 4 1.2. On methodology 1.2.1. <p> Bisection width [Thom79] is the minimum number of wires crossing a bisection of the network graph, over all possible bisections; it has often been used as an estimate of wiring area. In [Dall90], k-ary n-cubes with constant bisection widths were compared. In <ref> [AbPa90] </ref>, both switch pinout and bisection width were used as cost factors in analytical studies of k-ary n-cubes. In both studies, different network topologies were constrained to have the same wiring costs or pinouts by varying the widths of their channels. <p> of [Dall90] (published in preliminary form before 1987, hence the discrepancy in dates) were the basis for [RaJo87], which compared the capability of tori, hypercubes and cube-connected cycles to emulate butterflies and spanning trees. ([RaJo87] assumed simple layouts and made direct estimates of network area, instead of using bisection width.) <ref> [AbPa90] </ref> is a study in greater depth of (bidirectional) k-ary n-cubes with wiring constraints. Both switch pinout and bisection width were examined as cost constraints. Channel delays were assumed to be constant. Detailed analysis was presented for cut-through routing, 73 which is similar to wormhole routing with infinite buffers. <p> They have half the number of channels as their bidirectional counterparts. Hence, FMSNs have twice the channel width of FTs, and CMSNs twice that of CTs. 5.3. Modeling strategy We will adapt the analysis of <ref> [AbPa90] </ref> to our architectures; this is a standard slotted-time queueing model for studying single-packet messages in point-to-point networks, adapted for multiple-packet messages. We will consider both the message switching and partial cut-through queueing disciplines in clustered systems. <p> Partial cut-through is similar to wormhole routing in [Dall90]. It allows a message with enough routing information to ``cut-through'' to the next switch once the outgoing channel is free, even if part of the message has not arrived <ref> [AbPa90] </ref>. In our analytical models, we will ignore the effect of wire length and time-of-flight delays. The technologies used in Thinking Machine's CM-5 or IBM's Vulcan, for example, allow several bits to be pipelined through a wire. <p> However, at low to medium traffic, hypercubes and tori that use LR routing have lower queueing delays than the same systems that use random routing. As explained in [AbPa89], this is because of the lower probability of multiple simultaneous message arrivals at high-dimensional channels. <ref> [AbPa90] </ref> presented an analysis of the torus with random routing; an approach to studying LR routing in tori will be presented in Section 5.5.2. As in [HsYe91], we assume random routing in the clustered systems for convenience of analysis, but LR routing in flat systems. <p> The systems based on MSNs all use random routing, because the principle of LR routing is not clearly defined in those networks. The GSE has a more restricted routing scheme [HsYZ87] (see Section 5.3.3 for details). The assumption of infinite buffering in <ref> [AbPa90] </ref> is unrealistic. However, we expect network operation to be in regions of moderate channel utilization (&lt;80%), where the message delay curve is relatively flat and few buffers are necessary. <p> Message latency in the absence of contention is easy to compute, but it is an inadequate predictor of performance. Realistic finite-buffer analysis tends to be computationally intensive, and infinite-buffer analysis is a reasonable compromise (the infinite-buffer assumption is also made in <ref> [AbPa90] </ref> and [Agar91]). Our results may be considered an upper bound on performance. We do not expect the relative performance of our systems to change if only a small number of buffers are available per switch (as in [Dall90]). 79 5.3.1. <p> Let m be the traffic rate (in nibbles per cycle) on a global channel. For the flat systems, m is also the traffic rate on a local channel. In <ref> [AbPa90] </ref>, P t is defined to be the probability of termination, that is, the probability that a message received on a global channel will terminate at that node. <p> For a flat system, let each processor be attached to a B+1xB+1 switch (B ports are for network channels, and one is for the processor). Under uniform traffic, all channels have the same traffic rate. (This is true for all the networks under consideration, regardless of routing strategy.) In <ref> [AbPa90] </ref>, the equation mBP t =m g t m was derived for systems with random routing. (Intuitively, this says that in equilibrium the total rate of messages terminating at a processor from all 93 of its input channels is equal to the rate of message generation.) Since we will be looking <p> Similarly, at dimension i, m/2 i terminates at the processor. Summing up all these termination rates, i=0 n 1 -1 m hhh =2m N 1 -1 hhhhh =m g t m . If we evaluate the equation from <ref> [AbPa90] </ref> for the FH, m g t m =mBP t =mn 1 n 1 N 1 2 (N 1 -1) hhhhhhhh . Hence, m g,sat for both routing strategies are identical, and we can apply the same equation in both cases. <p> We examine hierarchical organization as a strategy for overcoming some packaging constraints. We develop a taxonomy for classifying most hierarchical organizations that have been proposed, and choose an approach, the SGC/EA, that we feel to be most promising in terms of architectural flexibility and performance. The methods of <ref> [AbPa90] </ref> for studying networks with wiring constraints are applied to a variety of clustered systems, in addition to many non-hierarchical networks. We also study briefly an alternative hierarchical organization, SGC/IP.
Reference: [Abra90] <author> S. Abraham, </author> <title> Issues in the Architecture of Direct Interconnection Schemes for Multiprocessors, </title> <type> PhD Thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: On a higher level, the graph-theoretic approach ignores the fact that the switches and channels of a network are ultimately media for the transmission of messages. The nature of these messages has a great impact on performance and must be accounted for in the analysis. For 2 example, Abraham <ref> [Abra90] </ref> showed that if relatively simple and natural assumptions are made about message traffic, networks that have similar static (graph-theoretic) characteristics may turn out to have very different message routing performance. This is also exemplified in our research involving a generalized shuffle-exchange network. <p> Destinations outside the sphere are requested less frequently. Generally, each processor has its own sphere, or small groups of processors may share a sphere of locality. There are numerous ways to define the sphere of locality; it can be based on physical network distances <ref> [Abra90] </ref>, network partitions [DaEa90], [HsYe91], or combinations of the two. A message traffic model that has attracted much concern is the ``hot-spot'' model studied in [PfNo85], in which all (or a large number of) processors will attempt to send messages to a single destination with relatively high probability. <p> The equations we have derived are for message switching. For partial cut-through, we estimate the time saved for a message at each switch to be its service time t m . This is a simpler estimate than the one used in <ref> [Abra90] </ref>, and has been used widely in the literature (see, for example, [KrSn83] and [Agar91]). <p> The correction factor for partial cut-through is fairly accurate, and the inaccuracies in the total delay under cut-through mostly come from the message switching analysis, with such well-known problems as assuming independent arrivals, among other things <ref> [Abra90] </ref>. The analysis for random routing works very well. It was observed in [Abra90] that the LR analysis for the FH is less accurate, and we see this in our results for LR routing in the FT as well. 6.1. <p> The correction factor for partial cut-through is fairly accurate, and the inaccuracies in the total delay under cut-through mostly come from the message switching analysis, with such well-known problems as assuming independent arrivals, among other things <ref> [Abra90] </ref>. The analysis for random routing works very well. It was observed in [Abra90] that the LR analysis for the FH is less accurate, and we see this in our results for LR routing in the FT as well. 6.1.
Reference: [Agar91] <author> A. Agarwal, </author> <title> ``Limits on interconnection network performance,'' </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 2, No. 4, </volume> <month> October </month> <year> 1991, </year> <pages> pp. 398-412. </pages>
Reference-contexts: This simple mathematical model has been used to drive most queueing-theory-based studies of interconnection networks. Based on observations of locality in message traffic, researchers have proposed extensions to the uniform traffic model. Some prefer a ``sphere of locality'' model (for example, [ReFu87] or <ref> [Agar91] </ref>), in which a processor is more likely to send messages to a small number of destinations within its sphere of locality. Destinations outside the sphere are requested less frequently. Generally, each processor has its own sphere, or small groups of processors may share a sphere of locality. <p> Depending on implementations, either may be more appropriate. In subsequent sections, it is assumed that a cluster is implemented on a pin-limited package which is referred to as a ``board''; it may actually be a multi-chip module, etc. Other constraints have been used in the literature. For example, <ref> [Agar91] </ref> examined fixed channel width, bisection width, and node pinout, and took into account both switch delay and wire delay. In Section 4.3, we will survey various studies on the effect of wiring constraints on network architecture, emphasizing the work relating to actual performance studies of networks with packaging constraints. <p> The authors found that, under the constant pinout constraint, high-dimension networks such as hypercubes are superior to low-dimension networks, because of their higher maximum capacity. Under the constant bisection-width constraint, their results are very similar to [Dall90]. A subsequent study <ref> [Agar91] </ref> took into account both switch delay and wire delay. For the constant bisection-width constraint, Agarwal disputed the finding in [Dall90] that 2-dimensional tori are optimal. He concluded instead that when switch delays are considered, the best network has a moderately higher dimension. 4.4. <p> Message latency in the absence of contention is easy to compute, but it is an inadequate predictor of performance. Realistic finite-buffer analysis tends to be computationally intensive, and infinite-buffer analysis is a reasonable compromise (the infinite-buffer assumption is also made in [AbPa90] and <ref> [Agar91] </ref>). Our results may be considered an upper bound on performance. We do not expect the relative performance of our systems to change if only a small number of buffers are available per switch (as in [Dall90]). 79 5.3.1. <p> For partial cut-through, we estimate the time saved for a message at each switch to be its service time t m . This is a simpler estimate than the one used in [Abra90], and has been used widely in the literature (see, for example, [KrSn83] and <ref> [Agar91] </ref>). Hence, for partial cut-through, T global,cut =T global - N 1 -1 N 1 hhhhh 2 n 1 hhh t m , and T local,cut =T local - N 0 -1 N 0 hhhhh 2 T cut =aT local,cut +(1-a)T global,cut .
Reference: [AgCh89] <author> A. Agarwal and M. Cherian, </author> <title> ``Adaptive backoff synchronization techniques,'' </title> <booktitle> 16th Int. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1989, </year> <pages> pp. 396-406. </pages>
Reference-contexts: Backoff schemes for barriers Some work has been done on the use of ``backoff'' schemes in barriers. (See, for example, <ref> [AgCh89] </ref>, [Jaya87], and [Jaya88].) In the 2-counter barrier, a backoff scheme would require processors entering the barrier to wait a certain amount of time before polling the counter B that controls the exit phase. <p> A short backoff time is not very useful, while an overlong backoff time results in unnecessarily long completion times which would be disastrous in time-critical applications. For example, <ref> [AgCh89] </ref> observed that backoff is effective at reducing network traffic at the cost of increasing processor idle time, and there are situations in which some form of combining is desirable for barrier synchronizations. <p> Because of these difficulties, we will limit our study of backoff schemes in Chapter 3 to an idealized situation: processors will start polling B only after all processors have entered the barrier. Hence, only one access to B is necessary. <ref> [AgCh89] </ref> makes the distinction between backing off on the barrier variable (A) and the barrier flag (B); in our 20 ideal case, this will not be necessary, because processors only make a single access to each counter. 21 CHAPTER 3 HARDWARE SUPPORT FOR HOT-SPOT ACCESSES 3.1.
Reference: [AgMa85] <author> D. Agrawal and I. Mahgoub, </author> <title> ``Performance analysis of cluster-based supersys-tems,'' </title> <booktitle> 1st Int. Conf. on Supercomputer Systems, IEEE Comp. </booktitle> <publisher> Soc. Press, </publisher> <year> 1985, </year> <pages> pp. 593-602. </pages>
Reference-contexts: Each cluster bus provides connection to a directory; non-local accesses go through the directory to the global mesh network. In addition, hierarchical interconnects have been an active area of research over the past 20 years. For example, there have been hierarchies based on shared buses [WuLi81], crossbar switches <ref> [AgMa85] </ref>, meshes [Carl85], hypercubes [DaEa90] [DaEa91] [Aboe91], and combinations of Omega networks and composite cube networks [Padm91a]. In Section 4.2 we attempt to develop a taxonomy of hierarchical systems that will provide the architectural context for our performance studies of hierarchical systems in Chapter 5. <p> The global network is a multistage network based on the shuffle, with multiplexor- and demultiplexor-switches. The parallel processor component of the recently-announced Cray S-MP also takes this approach: processors within a cluster access a global crossbar through a single cluster bus. <ref> [AgMa85] </ref> is another example of SGC/EA. Processors within a cluster access a global crossbar via a single cluster bus. The original formulation was for a shared-memory architecture, and processors can access memories in the same cluster through a local crossbar. [MaEl92] generalizes the structure of [AgMa85] to multiple levels of hierarchy. <p> through a single cluster bus. <ref> [AgMa85] </ref> is another example of SGC/EA. Processors within a cluster access a global crossbar via a single cluster bus. The original formulation was for a shared-memory architecture, and processors can access memories in the same cluster through a local crossbar. [MaEl92] generalizes the structure of [AgMa85] to multiple levels of hierarchy. The Extended Hypercube of [KuPa92a] is a variation of SGC/EA: each cluster is a hypercube, with all nodes within a cluster connected to a single network controller, which is similar to the global switch in [ChHo91] and [HsYe91].
Reference: [AnBP92] <author> J.B. Andrews, C.J. Beckmann, and D.K. Poulsen, </author> <title> ``Broadcasting/forwarding networks for fast synchronization,'' </title> <booktitle> Jour. of Parallel and Distributed Computing, </booktitle> <month> June </month> <year> 1992, </year> <note> to appear. </note>
Reference-contexts: However, the conservative approach seems reasonable for our purposes. We will focus on hardware for combining hot-spot accesses, in particular combining networks. Multicast hardware for networks has also been proposed for speeding up synchronization accesses; a good overview of such networks is <ref> [AnBP92] </ref>. Some tree-structured hardware has also been proposed for combining hot-spot accesses. Special hardware of this type tends to be too rigid and difficult to use in an environment with process migration or multiprogramming, or when only some of the processors are involved in the synchronization.
Reference: [ArJo89] <author> N.S. Arenstorf and H.F. Jordan, </author> <title> ``Comparing barrier algorithms,'' </title> <booktitle> Parallel Computing 12, </booktitle> <month> Nov. </month> <year> 1989, </year> <pages> pp. 157-170. </pages>
Reference-contexts: In this chapter, we will examine some software techniques for removing hot-spot accesses for barriers. This will form a basis for the simulation studies in the next chapter. 2.2. Software barriers A good survey of barrier algorithms is found in <ref> [ArJo89] </ref>. Most barriers can be divided into two phases. In the entry phase, processors ``check in'' to the barrier by updating a data structure. This data structure can be a single counter, resulting in a hot spot, or a tree of synchronization variables. <p> The exit phase is generally implemented in three ways: (1) exiting processors check a single variable, 11 which becomes a hot spot, (2) exiting processors are notified by a broadcast mechanism, or (3) exiting processors are notified through a tree of synchronization variables. <ref> [ArJo89] </ref> studies the performance of combinations of these mechanisms in a bus-based environment. We will focus on a few of these implementations, for systems with multistage networks. <p> In the following subsections, we will divide our choices into two major barrier types, those with and without hot spots. [HeFM88] also compares the performance of several software barriers; the barriers in the paper are minor variants of the structures studied in <ref> [ArJo89] </ref>. [MeSc91] also presented efficient algorithms for synchronization. However, they assumed a multiprocessor configuration in which each processor is able to access one of the memory modules without going through the network; the performance improvements are from clever use of the direct access capability. <p> A and B are hot spots; that is, all the processors will try to access the memory modules containing them, and intense congestion in the network will be the result [KuPf86]. A variation is a counter+broadcast barrier (similar algorithms can be found in <ref> [ArJo89] </ref>). The entry phase is identical to that of the 2-counter barrier. In the exit phase, the last processor to enter sets individual flags (stored in shared memory) for all the other processors. <p> The entry phase still produces hot-spot traffic. (Note that this version of the algorithm cannot be used repeatedly. However, a trivial modification involving the addition of a local variable will allow repeated utilization. See <ref> [ArJo89] </ref> for details.) A lower bound for finishing time for the counter+broadcast barrier is also 2*(2logN+N+2), assuming all processors enter the barrier at the same cycle; the entry phase is identical to the 2-counter barrier, and in the exit phase, the last processor to enter takes 1 cycle to inject the <p> The tree barrier avoids hot spots by replacing each hot spot with a tree of synchronization variables. The version used in our experiments is a static-scheduled version of the software synchronization tree of [YeTL87] and [TaYe90], and is similar in implementation to the barrier in <ref> [ArJo89] </ref> with tree-structured entry and exit phases. The butterfly barrier [Broo86] combines 14 entry and exit phases into a structure similar to an FFT butterfly. We also implement a static scheduled version of the butterfly barrier for our experiments. <p> The writes performed by processor 0 to the flags of 2 and 4 are not critical, and issuing them early has little effect on performance. Notice that this algorithm for 2-way software tree combining was adapted from <ref> [ArJo89] </ref> and is not easily generalizable to k-way combining, where k&gt;2 processors synchronize on the same variable in each stage. Some adaptations had to be made; in fact, the access pattern changes slightly. There is the additional complication in the cases where logk does not divide logN. <p> The fairly straightforward technique of forced misrouting is sufficient for avoiding deadlocks in the network. The various types of software barriers appear to perform similarly in a multistage network environment as in a bus-oriented environment (see <ref> [ArJo89] </ref>). The schemes with hot spots are very sensitive to system size and processor arrival times. The schemes with software combining degrade gracefully with system size and are less sensitive to variable processor arrival times. 54 For small systems, parallel queue accesses are reasonable without combining hardware.
Reference: [AsJM89] <author> A. Asthana, H. Jagadish, and B. Mathews, </author> <title> ``Impact of advanced VLSI packaging on the design of a large parallel computer,'' </title> <booktitle> 1989 Int. Conf. on Parallel Processing, Aug. 1989, </booktitle> <volume> vol. I, </volume> <pages> pp. 323-327. </pages>
Reference-contexts: Large multiprocessor systems may contain hundreds or even thousands of powerful processors and memories. Using today's VLSI technologies, such systems require innovative packaging approaches that may include multi-chip modules and multi-layer boards, and may possibly span over several chassis and cabinets. [BaWh85] and <ref> [AsJM89] </ref>, among others, have described projects in which the system architecture of large computers has been influenced heavily by packaging problems.
Reference: [BaWh85] <author> D. Balderes and M. White, </author> <title> ``Package effects on CPU performance of large commercial processors,'' </title> <booktitle> 35th Electronic Components Conference, </booktitle> <month> May </month> <year> 1985, </year> <pages> pp. 351-355. </pages>
Reference-contexts: Large multiprocessor systems may contain hundreds or even thousands of powerful processors and memories. Using today's VLSI technologies, such systems require innovative packaging approaches that may include multi-chip modules and multi-layer boards, and may possibly span over several chassis and cabinets. <ref> [BaWh85] </ref> and [AsJM89], among others, have described projects in which the system architecture of large computers has been influenced heavily by packaging problems.
Reference: [Baud87] <author> J. Baudrillard, </author> <title> The Ecstasy of Communication, </title> <publisher> Semiotext(e) Books, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: In environments where fast hot-spot access is necessary, it appears that a combining SSE network is a worthwhile investment. 55 CHAPTER 4 HIERARCHICAL SYSTEMS AND WIRING CONSTRAINTS Today there is a pornography of information and com munication, a pornography of circuits and networks... Jean Baudrillard <ref> [Baud87] </ref> 4.1. Introduction Packaging problems have become important constraints in the design of large multiprocessor systems. Much research, such as [PeKr87] (quoted in Chapter 1), has pointed to the need for an integrated approach to system design that incorporates the impact of packaging technology.
Reference: [BePo90] <author> C. Beckmann and C. Polychronopoulos, </author> <title> ``Fast barrier synchronization hardware,'' </title> <booktitle> Supercomputing 1990, </booktitle> <month> November </month> <year> 1990, </year> <pages> pp. 180-189. </pages>
Reference-contexts: The concern is with the finishing time. We use barriers as an extreme environment to tax the capabilities of combining networks. Fast barriers are crucial for the efficient execution of many parallel algorithms and compiler optimizations (see, for example, [Poly88]). Cheap barrier hardware has been proposed, for example <ref> [BePo90] </ref> and [HwSh91], but combining networks can handle many other hot-spot accesses, not just barriers. We also use several well-defined software barrier schemes for performance comparisons. <p> Lipovski and Vaughan's fetch-and-op tree [LiVa88], for example, is only capable of combining simultaneous accesses to a single hot spot. It is more suitable for SIMD machines. Other hardware schemes have been 25 proposed for barrier synchronization, such as <ref> [BePo90] </ref> and [HwSh91], but they are not flexible enough to handle traffic to memory hot spots. Feedback has been proposed by Scott and Sohi [ScSo89] for avoiding tree saturation, but it does not improve the latency of hot-spot accesses and is not a substitute for combining.
Reference: [BGWW91] <author> E. Brooks III, B. Gorda, K. Warren, and T. </author> <title> Welcome, ``Split-join and message passing programming models on the BBN TC2000,'' </title> <booktitle> 1991 Int. Conf. on Parallel Processing, Aug. 1991, </booktitle> <volume> vol. II, </volume> <pages> pp. 54-59. </pages>
Reference: [Bian89] <author> R. Bianchini, </author> <title> ``Ultracomputer packaging and prototypes,'' Ultracomputer Note No. </title> <type> 152, </type> <month> Jan. </month> <year> 1989. </year> <month> 164 </month>
Reference-contexts: It generally assumes that all networks have constant-width channels, since channels are modeled by the edges of the graph. VLSI-based systems tend to be wire-limited [Dall90]. Experience with the construction of large multiprocessor systems such as Cedar [GKLS83], Ultracomputer <ref> [Bian89] </ref> and J-Machine [Dall89] indicates that system wiring is a significant portion of system cost and may even be a major limiting constraint. The graph-theoretic approach oversimplifies or overlooks entirely difficulties of implementation and packaging, including wiring costs and limited package pinouts. <p> Hence, only global messages travel through all the stages, and local messages have a shorter distance. (Each processor also has its own connection to the global network.) 4.3. Modeling networks with wiring constraints The problems of wiring a large Omega network are well-known. For example, <ref> [Bian89] </ref> describes in detail advanced packaging techniques for the construction of a network for large Ultracomputer systems. Much work has been done on the performance analysis of multistage networks.
Reference: [Bork88] <author> S. Borkar et al., </author> <title> ``iWARP: an integrated solution to high-speed parallel computing,'' </title> <booktitle> Supercomputing '88, </booktitle> <month> Nov. </month> <year> 1988, </year> <pages> pp. 330-339. </pages>
Reference-contexts: In left-to-right (LR) routing, a message first takes all its hops along the highest-order dimension, and then along each successive lower-order dimension. LR routing is similar to e-cube routing in the hypercube [Dall90] and the routing scheme in iWARP <ref> [Bork88] </ref>. Routing strategy does not affect latency without contention or maximum network 78 capacity. However, at low to medium traffic, hypercubes and tori that use LR routing have lower queueing delays than the same systems that use random routing.
Reference: [Broo86] <author> E.D. Brooks III, </author> <title> ``The butterfly barrier,'' </title> <booktitle> Int. Jour. of Parallel Programming, </booktitle> <volume> vol. 15, no. 4, </volume> <year> 1986, </year> <pages> pp. 295-307. </pages>
Reference-contexts: The version used in our experiments is a static-scheduled version of the software synchronization tree of [YeTL87] and [TaYe90], and is similar in implementation to the barrier in [ArJo89] with tree-structured entry and exit phases. The butterfly barrier <ref> [Broo86] </ref> combines 14 entry and exit phases into a structure similar to an FFT butterfly. We also implement a static scheduled version of the butterfly barrier for our experiments.
Reference: [Carl85] <author> D. Carlson, </author> <title> ``The mesh with a global mesh: a flexible, high-speed organization for parallel computation,'' </title> <booktitle> 1st Int. Conf. on Supercomputer Systems, IEEE Comp. </booktitle> <publisher> Soc. Press, </publisher> <year> 1985, </year> <pages> pp. 618-627. </pages>
Reference-contexts: In addition, hierarchical interconnects have been an active area of research over the past 20 years. For example, there have been hierarchies based on shared buses [WuLi81], crossbar switches [AgMa85], meshes <ref> [Carl85] </ref>, hypercubes [DaEa90] [DaEa91] [Aboe91], and combinations of Omega networks and composite cube networks [Padm91a]. In Section 4.2 we attempt to develop a taxonomy of hierarchical systems that will provide the architectural context for our performance studies of hierarchical systems in Chapter 5. <p> If we assume that all channels have the same bandwidth, these channels become overutilized and saturate at low loads, while the channels further away from the interface processor are underutilized. SGC/EA avoids this problem, and provides a more balanced organization. Other researchers have studied systems similar to SGC/IP. <ref> [Carl85] </ref> is SGC/IP with global and local meshes. [Aboe91] is essentially a generalization of SGC/IP to multiple levels of hierarchy, for hypercubes. 67 4.2.2. Partitioned global channels (PGC) An alternative to shared global channels is a partitioned global channels (PGC) approach.
Reference: [ChAB91] <author> C. Chen, D.P. Agrawal, and J.R. Burke, </author> <title> ``A class of hierarchical networks for VLSI/WSI based multicomputers,'' </title> <booktitle> 4th CSI/IEEE Int. Symp. on VLSI Design, </booktitle> <month> Jan. </month> <year> 1991, </year> <pages> pp. 267-272. </pages>
Reference-contexts: The Hierarchical Cubic Networks (HCN) of [GhDe89] consist of a local hypercube with 2 n nodes, and 2 m clusters in a completely connected global network. If m&lt;n, channels are replicated between pairs of clusters. The DBCube of <ref> [ChAB91] </ref> also has clusters of hypercubes, globally connected by a DeBruijn network. As in the HCN, channel replication is used if the number of nodes per cluster is greater than the cluster fanout. The WK-recursive topologies of [VeSa88] are based on complete interconnections.
Reference: [ChAg90] <author> T. Chung and D. Agrawal, </author> <title> ``On network characterization of and optimal broadcasting in the Manhattan Street network,'' </title> <booktitle> Proc. INFOCOM '90, </booktitle> <pages> pp. 465-472. </pages>
Reference-contexts: for MSN hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 116 If we count the extra distance incurred in the MSN for ``correcting'' to the proper direction on a route, we find that the average message distance is N 1 -1 N 1 dd N 1 /2+N 1 -4 hhhhhhhhhhhhhh . (This agrees with the result of <ref> [ChAg90] </ref>, derived independently using a different method.) This result implies that the MSN actually incurs very minor overhead for medium to large system sizes. However, for small systems, the inferiority of the average message distance of the MSN when compared to that of the torus becomes noticeable.
Reference: [ChHo91] <author> S. Chowdhury and M. Holliday, </author> <title> ``Stability and performance of alternative two-level interconnection networks,'' </title> <booktitle> 1991 Int. Conf. on Parallel Processing, Aug. 1991, </booktitle> <volume> vol. I, </volume> <pages> pp. 720-721. </pages>
Reference-contexts: Many compared the number of links required in the different systems, and some scaled their performance numbers based on link costs. Most studies have limited the discussion to traffic patterns with high locality. However, only <ref> [ChHo91] </ref> and our work (first published in [HsYe91], and expanded in [HsYe92a] and [HsYe92b]) have incorporated wiring constraints directly into the architectural parameters that affect system performance, that 59 is, channel width and message length. <p> We defined a good example of SGC/EA in [HsYe91]: each cluster has a global switch providing hypercube connections to other clusters, and local processors access the global switch via a system of buses. A similar structure was studied in <ref> [ChHo91] </ref>. SGC/EA systems based on tori 65 and generalized shuffle-exchange networks were analyzed in [HsYe92a] and [HsYe92b]. Several systems are based on SGC/EA. Dash [LLGW92] has a single cluster bus and a global mesh. <p> The Extended Hypercube of [KuPa92a] is a variation of SGC/EA: each cluster is a hypercube, with all nodes within a cluster connected to a single network controller, which is similar to the global switch in <ref> [ChHo91] </ref> and [HsYe91]. The network controllers are connected to form a global hypercube network. This appears to be overkill in terms of hardware complexity; we will show that if enough ports are provided to the global switch, a local network connecting the nodes within the same cluster is not necessary.
Reference: [ChSh87] <author> W. Chen and J. Sheu, </author> <title> ``Fault-tolerant two-level multistage interconnection networks,'' </title> <booktitle> 7th Int. Conf. on Dist. Comp. Sys., </booktitle> <month> Sept. </month> <year> 1987, </year> <pages> pp. 120-127. </pages>
Reference-contexts: The Extended Hypercube with Folded Connections of [KuPa92b] is the identical organization but with folded hypercubes instead of the full hypercubes. <ref> [ChSh87] </ref> proposes a structure in which processors within a cluster share a local crossbar. The local crossbar provides connections to several global crossbars and allows access to other clusters.
Reference: [Cyph89] <author> R. Cypher, </author> <title> ``Theoretical aspects of VLSI pin limitations,'' </title> <type> IBM Research Report RJ 7115 (67362), </type> <institution> IBM Almaden Research Center, </institution> <year> 1989. </year>
Reference-contexts: Finally, [FrDh86] is a case study of building a 72 2048x2048 Omega network with a 4-bit data path, with performance estimates. For point-to-point networks, studies incorporating wiring constraints tend to be of more recent vintage. Work of a more theoretical slant includes <ref> [Cyph89] </ref>, in which theoretical bounds for chip pinout requirements for hypercubes, meshes, and shuffle-exchange networks were derived. [ReFu87] made some simplifying assumptions about network topology and examined the queueing performance of large, unbounded, symmetric networks with pin-limited switches.
Reference: [DaEa90] <author> S. Dandamudi and D. Eager, </author> <title> ``Hierarchical interconnection networks for multi-computer systems,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-39, no. 6, </volume> <month> June </month> <year> 1990, </year> <pages> pp. 786-797. </pages>
Reference-contexts: Destinations outside the sphere are requested less frequently. Generally, each processor has its own sphere, or small groups of processors may share a sphere of locality. There are numerous ways to define the sphere of locality; it can be based on physical network distances [Abra90], network partitions <ref> [DaEa90] </ref>, [HsYe91], or combinations of the two. A message traffic model that has attracted much concern is the ``hot-spot'' model studied in [PfNo85], in which all (or a large number of) processors will attempt to send messages to a single destination with relatively high probability. <p> In addition, hierarchical interconnects have been an active area of research over the past 20 years. For example, there have been hierarchies based on shared buses [WuLi81], crossbar switches [AgMa85], meshes [Carl85], hypercubes <ref> [DaEa90] </ref> [DaEa91] [Aboe91], and combinations of Omega networks and composite cube networks [Padm91a]. In Section 4.2 we attempt to develop a taxonomy of hierarchical systems that will provide the architectural context for our performance studies of hierarchical systems in Chapter 5. <p> Hence, the interface processor has a large fanout, but all other processors have small fanouts. SGC/IP was defined in a topology-independent context in <ref> [DaEa90] </ref>. The authors studied two-level networks with local hypercubes and various global topologies, and extended their analysis in [DaEa91]. A major problem was observed in [DaEa90], for traffic that does not have an extremely high degree of locality. <p> Hence, the interface processor has a large fanout, but all other processors have small fanouts. SGC/IP was defined in a topology-independent context in <ref> [DaEa90] </ref>. The authors studied two-level networks with local hypercubes and various global topologies, and extended their analysis in [DaEa91]. A major problem was observed in [DaEa90], for traffic that does not have an extremely high degree of locality. Note that the local channels attached to the interface processor are carrying most of the global traffic for the cluster. <p> In this chapter, we will study the performance of SGC/IP hierarchical networks based on local and global hypercubes. 7.1. SGC/IP approach In Chapter 4, we described the SGC/IP approach to clustering proposed in <ref> [DaEa90] </ref>. The cluster comprises an internal network, for example a hypercube. One of the processors is chosen to be the interface processor (IP), which has extra links connecting to the IPs in other clusters. The analysis in [DaEa90] used a traditional approach, assuming channels of equal bandwidth. <p> approach In Chapter 4, we described the SGC/IP approach to clustering proposed in <ref> [DaEa90] </ref>. The cluster comprises an internal network, for example a hypercube. One of the processors is chosen to be the interface processor (IP), which has extra links connecting to the IPs in other clusters. The analysis in [DaEa90] used a traditional approach, assuming channels of equal bandwidth. Wiring costs were accounted for by factoring them into the performance numbers obtained from the analysis. We will apply the new approach of equating wiring costs by incorporating wiring constraints directly into architectural assumptions. <p> The global channels (assuming a CH) saturate at t m,g BP t hhhh N 0 (1-a) 1 hhhhhhhh , where t m,g is the number of nibbles in a message transmitted on a global channel, and BP t is approximately equal to 2. A major problem encountered in <ref> [DaEa90] </ref> is the saturation of the local channels at the IP, which become the bottlenecks. Using our formulation, for the IP local channels to handle the maximum bandwidth allowable on the global channels, X2 t m,g t m,l hhhh . <p> Queueing analysis of SGC/IP In this section, we will study the performance of SGC/IP networks with local and global hypercubes. Our exposition will be based on the work of <ref> [DaEa90] </ref>, which was a continuous-time analysis without consideration of multiple-nibble messages and wiring constraints. In this system, each of N 0 -1 processors are attached to a (n 0 +1) x (n 0 +1) switch, while the IP is attached to a (n 1 +1) x (n 1 +1) switch. <p> The traffic rate on a local channel from local messages is m local =a n 0 P t,l m g t m,l hhhhhh , where t m,l is the number of nibbles in a message routing on a local channel. <ref> [DaEa90] </ref> defines a j-level link (or channel) as a local link that connects a node at a distance j from the IP to a node at a distance j-1 from the IP. [DaEa90] does not distinguish between queueing delays on such a j-level channel from the j-level channel that is in <p> hhhhhh , where t m,l is the number of nibbles in a message routing on a local channel. <ref> [DaEa90] </ref> defines a j-level link (or channel) as a local link that connects a node at a distance j from the IP to a node at a distance j-1 from the IP. [DaEa90] does not distinguish between queueing delays on such a j-level channel from the j-level channel that is in the opposite direction. Although from symmetry forward and reverse channels have the same traffic rates, they do not necessarily have the same arrival distributions (and hence, queueing delays). <p> Although from symmetry forward and reverse channels have the same traffic rates, they do not necessarily have the same arrival distributions (and hence, queueing delays). We will make the distinction: we call the j-level channel as defined by <ref> [DaEa90] </ref> the forward j-level channel, and the channel in the reverse direction the reverse j-level channel. Hence, a reverse j-level channel routes from a node that is j hops from the IP to a node that is j+1 hops from the IP. <p> Hence, a reverse j-level channel routes from a node that is j hops from the IP to a node that is j+1 hops from the IP. Glo bal messages always move toward the IP on forward channels and away from the IP on reverse channels. As in <ref> [DaEa90] </ref>, let p j be the probability that a global message uses a j-level channel. Hence, p j = (n 0 -j+1) J n 0 J N 1 S I L k M O hhhhhhhhhhhhhhh . <p> Consider the delay of local messages. Let the average queueing delay on a forward j-level channel be s for,j , that on a reverse j-level channel be s rev,j , as computed by Equation 5.1. From <ref> [DaEa90] </ref>, the average delay at a channel of a local message is s local = n 0 2 n 0 -1 1 hhhhhhh j=1 n 0 I L j-1 M O J s for,j +s rev,j hhhhhhhhh M O .
Reference: [DaEa91] <author> S. Dandamudi and D. Eager, </author> <title> ``On hypercube-based hierarchical interconnection network design,'' </title> <booktitle> Jour. of Parallel and Distributed Computing 12, </booktitle> <month> July </month> <year> 1991, </year> <pages> pp. 283-289. </pages>
Reference-contexts: In addition, hierarchical interconnects have been an active area of research over the past 20 years. For example, there have been hierarchies based on shared buses [WuLi81], crossbar switches [AgMa85], meshes [Carl85], hypercubes [DaEa90] <ref> [DaEa91] </ref> [Aboe91], and combinations of Omega networks and composite cube networks [Padm91a]. In Section 4.2 we attempt to develop a taxonomy of hierarchical systems that will provide the architectural context for our performance studies of hierarchical systems in Chapter 5. <p> Hence, the interface processor has a large fanout, but all other processors have small fanouts. SGC/IP was defined in a topology-independent context in [DaEa90]. The authors studied two-level networks with local hypercubes and various global topologies, and extended their analysis in <ref> [DaEa91] </ref>. A major problem was observed in [DaEa90], for traffic that does not have an extremely high degree of locality. Note that the local channels attached to the interface processor are carrying most of the global traffic for the cluster.
Reference: [Dall89] <author> W. Dally, </author> <title> ``The J-Machine: system support for actors,'' Actors: Knowledge-based Concurrent Computing, </title> <editor> Hewitt and Agha eds., </editor> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: It generally assumes that all networks have constant-width channels, since channels are modeled by the edges of the graph. VLSI-based systems tend to be wire-limited [Dall90]. Experience with the construction of large multiprocessor systems such as Cedar [GKLS83], Ultracomputer [Bian89] and J-Machine <ref> [Dall89] </ref> indicates that system wiring is a significant portion of system cost and may even be a major limiting constraint. The graph-theoretic approach oversimplifies or overlooks entirely difficulties of implementation and packaging, including wiring costs and limited package pinouts.
Reference: [Dall90] <author> W. Dally, </author> <title> ``Performance analysis of k-ary n-cube interconnection networks,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-39, no. 6, </volume> <month> June </month> <year> 1990, </year> <pages> pp. 775-785. </pages>
Reference-contexts: On a lower level, the graph-theoretic approach ignores the physical problems of network construction. It generally assumes that all networks have constant-width channels, since channels are modeled by the edges of the graph. VLSI-based systems tend to be wire-limited <ref> [Dall90] </ref>. Experience with the construction of large multiprocessor systems such as Cedar [GKLS83], Ultracomputer [Bian89] and J-Machine [Dall89] indicates that system wiring is a significant portion of system cost and may even be a major limiting constraint. <p> We should no longer make architectural decisions based solely on traditional analysis that does not address packaging concerns. Limited wiring costs force the construction of narrow channels, and messages have to be broken into many nibbles and transmitted over many cycles. We follow the approach taken by Dally <ref> [Dall90] </ref> and Abraham and Padmanabhan [AbPa90], comparing networks based on wiring costs. We will apply these methods to re-examine the well-known concept of hierarchical systems. Our work will yield new perspectives on hierarchical systems and introduce new structures to help overcome the problems of packaging large-scale systems. 4 1.2. <p> Bisection width [Thom79] is the minimum number of wires crossing a bisection of the network graph, over all possible bisections; it has often been used as an estimate of wiring area. In <ref> [Dall90] </ref>, k-ary n-cubes with constant bisection widths were compared. In [AbPa90], both switch pinout and bisection width were used as cost factors in analytical studies of k-ary n-cubes. <p> Their results are not easily applied to conventional topologies such as hypercubes. <ref> [Dall90] </ref> reports some of the earliest work to use bisection width as the wiring cost constraint. The study compared unidirectional k-ary n-cubes, varying their channel widths to keep their bisection widths constant. <p> Dally found that all k-ary n-cubes have the same maximum capacity, but small-dimensional networks are superior because they have low latencies (by latency, we refer to the average delay of a message in an unloaded network). Hence, tori have the best performance under these criteria. The results of <ref> [Dall90] </ref> (published in preliminary form before 1987, hence the discrepancy in dates) were the basis for [RaJo87], which compared the capability of tori, hypercubes and cube-connected cycles to emulate butterflies and spanning trees. ([RaJo87] assumed simple layouts and made direct estimates of network area, instead of using bisection width.) [AbPa90] is <p> The authors found that, under the constant pinout constraint, high-dimension networks such as hypercubes are superior to low-dimension networks, because of their higher maximum capacity. Under the constant bisection-width constraint, their results are very similar to <ref> [Dall90] </ref>. A subsequent study [Agar91] took into account both switch delay and wire delay. For the constant bisection-width constraint, Agarwal disputed the finding in [Dall90] that 2-dimensional tori are optimal. He concluded instead that when switch delays are considered, the best network has a moderately higher dimension. 4.4. <p> Under the constant bisection-width constraint, their results are very similar to <ref> [Dall90] </ref>. A subsequent study [Agar91] took into account both switch delay and wire delay. For the constant bisection-width constraint, Agarwal disputed the finding in [Dall90] that 2-dimensional tori are optimal. He concluded instead that when switch delays are considered, the best network has a moderately higher dimension. 4.4. Conclusions In this chapter, we surveyed some past work on hierarchical organizations and the analysis of networks with wiring constraints. <p> We will consider both the message switching and partial cut-through queueing disciplines in clustered systems. In message switching for multiple-nibble messages, an entire message has to arrive at the output port of a switch before it can be forwarded. Partial cut-through is similar to wormhole routing in <ref> [Dall90] </ref>. It allows a message with enough routing information to ``cut-through'' to the next switch once the outgoing channel is free, even if part of the message has not arrived [AbPa90]. In our analytical models, we will ignore the effect of wire length and time-of-flight delays. <p> In left-to-right (LR) routing, a message first takes all its hops along the highest-order dimension, and then along each successive lower-order dimension. LR routing is similar to e-cube routing in the hypercube <ref> [Dall90] </ref> and the routing scheme in iWARP [Bork88]. Routing strategy does not affect latency without contention or maximum network 78 capacity. However, at low to medium traffic, hypercubes and tori that use LR routing have lower queueing delays than the same systems that use random routing. <p> Our results may be considered an upper bound on performance. We do not expect the relative performance of our systems to change if only a small number of buffers are available per switch (as in <ref> [Dall90] </ref>). 79 5.3.1. The SGC/EA cluster/global interface In a clustered system, each board or cluster of N 0 processors is connected to the global switch by a system of k (&lt;N 0 ) buses. We emphasize that, with certain topologies, it is necessary to use k&gt;1 buses. <p> Figure 5.2 shows some example configurations of FTs and CTs. Figure 5.3 shows examples of FHs and CHs. Now consider the bisection widths of hypercubes, tori, and MSNs. We can adapt the terms for bisection width from <ref> [Dall90] </ref>, making appropriate adjustments for bidirectional channels. For a hypercube with N 1 processors (and channel width C), the bisection width is CN 1 . For bidirectional tori, it is 4C dd N 1 ; for MSNs, it is 2C dd N 1 . <p> Hence, in a carefully designed system where all these parameters are taken into account, channel efficiency may not be a factor. 5.4.2. Saturation of networks with constant bisection widths As in <ref> [Dall90] </ref>, we will use bisection width as a measure of wiring area and compare flat and clustered systems with constant bisection width.
Reference: [DaPS87] <author> F. Darema-Rogers, G.F. Pfister and K. </author> <title> So, ``Memory access patterns of parallel scientific programs,'' </title> <booktitle> Proc. 1987 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <volume> vol. </volume> <pages> 15, </pages> <address> no.1, Banff, Alberta, Canada, </address> <month> May </month> <year> 1987, </year> <pages> pp. 46-57. </pages>
Reference-contexts: The validation of the uniform traffic model has not been adequately addressed, but that is beyond the scope of this work. 1.2.2. Address traces Another approach attempts to emulate memory traffic during actual machine operation by using address traces from small kernels or parallel programs (see, for example, <ref> [DaPS87] </ref> or [TuVe88]). The use of address traces avoids many of the abstractions of mathematical models. Successive messages from the same processor are no longer independent, and neighboring processors may send messages that are dependent on each other.
Reference: [DGKL86] <author> S. Dickey, A. Gottlieb, R. Kenner, and Y. Liu, </author> <title> ``Designing VLSI network nodes to reduce memory traffic in a shared memory parallel computer,'' Ultracomputer Note No. </title> <type> 125, </type> <institution> Courant Institute, </institution> <address> New York University, </address> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: It ``decombines'' with P2; that is, P2 is removed from the wait buffer, and returns to a queue in the switch. P1 and P2 eventually return to their respective processors. Typical designs for combining switches for (bidirectional) Omega networks can be found in <ref> [DGKL86] </ref>. Consider an SSE network using 2x2 switches with combining/decombining hardware, as shown in Figure 3.2. A 4-slot queue is associated with each input port. Each switch contains a wait buffer for storing combined packets. <p> However, hot-spot accesses do put tight restrictions on the granularity of computations (and hence, the performance of an application). In our experiments, we have assumed that all the networks have the same cycle times. Although the addition of combining hardware would slow down the network cycle time, <ref> [DGKL86] </ref> estimates that the increase is only 10 to 20%. Hence, the relative performance of the systems we measured should not change much. In conclusion, it appears that the SSE network can give improved performance for hot-spot accesses at a reasonable cost.
Reference: [DiKu89] <author> D. Dias and M. Kumar, </author> <title> ``Preventing congestion in multistage networks in the presence of hotspots,'' </title> <booktitle> Int. Conf. on Parallel Processing, Aug. 1989, </booktitle> <volume> vol. I, </volume> <pages> pp. 9-13. </pages>
Reference-contexts: Feedback has been proposed by Scott and Sohi [ScSo89] for avoiding tree saturation, but it does not improve the latency of hot-spot accesses and is not a substitute for combining. Similarly, the intelligent allocation of hardware switch buffers (for example, <ref> [DiKu89] </ref> and [Tzen91]) relieves congestion, but does not address the problem of high-latency hot-spot accesses. A different approach is taken in [Gupt89]: compiler analysis is used to reorder code so the processor can be doing useful work while waiting for a barrier to complete.
Reference: [Fran81] <author> M.A. Franklin, </author> <title> ``VLSI performance comparison of banyan and crossbar communications networks,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-30, no. 4, </volume> <month> April </month> <year> 1981, </year> <pages> pp. 283-291. 165 </pages>
Reference-contexts: Performance trends are similar to those of [KrSn83], except the optimal region for 2x2 switches is greatly expanded, because of the restriction of finite buffers. More detailed treatments can be found in the work of Franklin and his associates. In <ref> [Fran81] </ref>, single-chip implementations of crossbars and banyans were studied. The channel width and switch size were kept constant, and the chip area, switch delay, and area-delay product were compared. Performance was based on a circuit-switched model analyzed in [Pate81].
Reference: [FrDh86] <author> M.A. Franklin and S. Dhar, </author> <title> ``Interconnection networks: physical design and performance analysis,'' </title> <booktitle> Jour. of Parallel and Distributed Computing 3, </booktitle> <year> 1986, </year> <pages> pp. 352-372. </pages>
Reference-contexts: A wide data path was achieved by combining, in parallel, several crossbar chips with narrower data paths. Optimal configurations of subnetwork size and data path slice were found based on chip count, delay, and chip count-delay product. Finally, <ref> [FrDh86] </ref> is a case study of building a 72 2048x2048 Omega network with a 4-bit data path, with performance estimates. For point-to-point networks, studies incorporating wiring constraints tend to be of more recent vintage.
Reference: [FrWT82] <author> M.A. Franklin, D. Wann, and W. Thomas, </author> <title> ``Pin Limitations and Partitioning of VLSI Interconnection Networks,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-31, no. 11, </volume> <month> Nov. </month> <year> 1982, </year> <pages> pp. 1109-1116. </pages>
Reference-contexts: The channel width and switch size were kept constant, and the chip area, switch delay, and area-delay product were compared. Performance was based on a circuit-switched model analyzed in [Pate81]. It was found that the difference between crossbars and banyans was very small, based on these implementational constraints. In <ref> [FrWT82] </ref>, the authors studied the problems involved in building a large crossbar network or banyan network from smaller single-chip crossbar switches. The system data path width and chip pinouts were kept constant across all configurations. <p> It can be constructed by dividing the data path into slices, and having a single crossbar chip handle each slice. One slice of the data path determines the setting of the switch and broadcasts the state information to all the other slices. This approach avoids the problems discussed in <ref> [FrWT82] </ref>, such as the necessity for replicating routing information in each slice and ensuring that the different slices of a switch are set to the same switching state. 75 The family of k-ary n-cubes will be represented by the hypercube and the (bidirectional) torus.
Reference: [FuSw92] <editor> W.K. Fuchs and E.E. Swartzlander, </editor> <title> ``Wafer-scale integration: architectures and algorithms,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 25 no. 4, </volume> <month> April </month> <year> 1992, </year> <pages> pp. 6-8. </pages>
Reference-contexts: The burgeoning field of wafer-scale integration (see, for example, <ref> [FuSw92] </ref> and the papers that follow in the journal) is another example of the impact of packaging technology on system architecture.
Reference: [GeJS82] <author> E.F. Gehringer, A.K. Jones, and Z.Z. Segall, </author> <title> ``The Cm* testbed,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 15, no. 10, </volume> <month> Oct. </month> <year> 1982, </year> <month> pp.38-50. </month>
Reference-contexts: Hence, more cost-effective interconnect schemes, such as hypercubes, meshes, tori, or shuffle-exchange networks have to be used. 58 Several hierarchical multiprocessor systems have been constructed. Cm* <ref> [GeJS82] </ref> was one of the earliest. Processors within a cluster are connected via a bus to a module called the Kmap. The Kmap also provides access to other clusters through several intercluster buses. <p> A similar structure was studied in [ChHo91]. SGC/EA systems based on tori 65 and generalized shuffle-exchange networks were analyzed in [HsYe92a] and [HsYe92b]. Several systems are based on SGC/EA. Dash [LLGW92] has a single cluster bus and a global mesh. Cm* <ref> [GeJS82] </ref> uses a local bus to connect processors within the same cluster, and several global buses to provide global communications. In the Cenju system [NTKM91], processors within a cluster access a global network through a cluster bus.
Reference: [GGKM83] <author> A. Gottlieb, R. Grishman, C. Kruskal, K. McAuliffe, L. Rudolph and M. Snir, </author> <title> ``The NYU Ultracomputer --- designing an MIMD shared memory parallel computer,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-32, no. 2, </volume> <year> 1983, </year> <pages> pp. 175-189. </pages>
Reference-contexts: A non-combining Omega network [Lawr75] handled non-hot-spot requests, and a multistage network with special hardware combined simultaneous hot-spot requests. (The combining network was not built for the RP3 prototype.) Combining networks were originally proposed by the NYU Ultracomputer project <ref> [GGKM83] </ref>, for combining fetch-and-op instructions. As an illustration of how combining may work with fetch-and-op instructions, consider processor A performing fetch&+(X,1) and processor B fetch&+(X,2), where X is a shared variable initialized to 0. Suppose the two messages combine in the network, and A's request continues to shared memory.
Reference: [GhDe89] <author> K. Ghose and K.R. Desai, </author> <title> ``The HCN: a versatile interconnection network based on cubes,'' </title> <booktitle> Supercomputing '89, </booktitle> <pages> pp. 426-435. </pages>
Reference-contexts: To avoid the problems discussed earlier, restrictions are made on the number of processors within a cluster and the cluster fanout. The Hierarchical Cubic Networks (HCN) of <ref> [GhDe89] </ref> consist of a local hypercube with 2 n nodes, and 2 m clusters in a completely connected global network. If m&lt;n, channels are replicated between pairs of clusters. The DBCube of [ChAB91] also has clusters of hypercubes, globally connected by a DeBruijn network.
Reference: [GiMo91] <author> W. Giloi and S. Montenegro, </author> <title> ``Choosing the interconnect of distributed memory systems by cost and blocking behavior,'' </title> <booktitle> 1991 Int. Parallel Processing Symp., </booktitle> <address> Anaheim CA, </address> <month> April 30 May 2, </month> <pages> pp. 438-444. </pages>
Reference-contexts: Since the number of clusters is restricted to be equal to the number of processors in a cluster, one processor in each cluster has a free global channel that can be used for building the next level of the hierarchy. TICNET <ref> [GiMo91] </ref> is essentially PGC with local and global crossbars. The processors within a cluster are connected via a local crossbar. Each processor has its own channel to a global crossbar, which connects up corresponding processors in each of the other clusters. <p> next step up that will make a difference in P t is B=N-1 (and not B=N, since it is not necessary for a cluster to have a channel routing to itself.) In Chapter 4, we mentioned systems with a small number of clusters connected with global crossbars, for example, TICNET <ref> [GiMo91] </ref>.
Reference: [GJMW89] <author> K. Gallivan, W. Jalby, A. Malony, and H. Wijshoff, </author> <title> ``Performance prediction of loop constructs on multiprocessor hierarchical-memory systems,'' </title> <booktitle> 1989 Int. Conf. on Supercomputing, </booktitle> <address> Crete, Greece, </address> <month> June </month> <year> 1989, </year> <pages> pp. 433-442. </pages>
Reference: [GKLS83] <author> D. Gajski, D. Kuck, D. Lawrie, and A. Sameh, </author> <title> ``CEDAR -- a large scale multiprocessor", </title> <booktitle> 1983 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1983, </year> <pages> pp. 524-529. </pages>
Reference-contexts: It generally assumes that all networks have constant-width channels, since channels are modeled by the edges of the graph. VLSI-based systems tend to be wire-limited [Dall90]. Experience with the construction of large multiprocessor systems such as Cedar <ref> [GKLS83] </ref>, Ultracomputer [Bian89] and J-Machine [Dall89] indicates that system wiring is a significant portion of system cost and may even be a major limiting constraint. The graph-theoretic approach oversimplifies or overlooks entirely difficulties of implementation and packaging, including wiring costs and limited package pinouts.
Reference: [GoLR83] <author> A. Gottlieb, B. Lubachevsky, and L. Rudolph, </author> <title> ``Basic techniques for the efficient coordination of very large numbers of cooperating sequential processors,'' </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> vol. 5, no. 2, </volume> <month> April </month> <year> 1983, </year> <pages> pp. 164-189. </pages>
Reference-contexts: Parallel queue accesses are sufficiently different from barriers and will test different aspects of the hardware. 48 We first implemented parallel queue inserts and deletes similar to the algorithms outlined in [Rudo81] and <ref> [GoLR83] </ref>. (For details see Appendix A.) Inserts and deletes are symmetrical, and each insert/delete involves six potential hot-spot accesses. We simulated only the accesses to semaphores and counters that ensure correct queue accesses, and have left out the actual data movements to and from the queue locations. <p> Using this data, we can study the interactions between architectural clustering and techniques for hierarchical program restructuring and latency reduction. 160 APPENDIX A PARALLEL QUEUE ALGORITHMS For all systems measured, the queue algorithms were ``hardcoded'' as finite-state machines. The first queue algorithm we implemented was based on [Rudo81] and <ref> [GoLR83] </ref>. The queue is finite-length and is implemented as a circular array of cells. Each processor gets a Bakery algo rithm ``ticket'' for the requested queue location. Qu and ql keep upper and lower bounds on the number of elements in the queue.
Reference: [Gupt89] <author> R. Gupta, </author> <title> ``The fuzzy barrier: a mechanism for high speed synchronization of processors,'' </title> <booktitle> ASPLOS 1989, </booktitle> <pages> 54-63. </pages>
Reference-contexts: Similarly, the intelligent allocation of hardware switch buffers (for example, [DiKu89] and [Tzen91]) relieves congestion, but does not address the problem of high-latency hot-spot accesses. A different approach is taken in <ref> [Gupt89] </ref>: compiler analysis is used to reorder code so the processor can be doing useful work while waiting for a barrier to complete. This latency-hiding technique can be used in combination with hardware combining to improve barrier performance. Investigation of similar techniques is beyond the scope of this work.
Reference: [HaMS86] <author> J.P. Hayes, T.N. Mudge, and Q.F. Stout, </author> <title> ``Architecture of a hypercube supercomputer", </title> <booktitle> 1986 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1986, </year> <pages> pp. 653-660. </pages>
Reference-contexts: Our primary concern is to keep system wiring cost constant and perhaps pay some penalty on local wiring costs for improvement in performance. For example, consider an NCUBE-like system <ref> [HaMS86] </ref>: a 1024-processor traditional flat hypercube (FH) has a 64-processor subcube and 512 pins per board. Each processor has a 10x10 switch, of which 4 each of inputs and outputs go off-board. Hence, there are 64*4*2=512 channels, and they can only be one bit wide.
Reference: [HaSm77] <author> J.A. Harris and D.R. Smith, </author> <title> ``Hierarchical multiprocessor organizations,'' </title> <booktitle> 4th Ann. Symp. on Comp. Arch., </booktitle> <year> 1977, </year> <pages> pp. 41-48. </pages>
Reference-contexts: Odds and ends Some structures do not fit easily into our taxonomy. There have been systems proposed where the processors are physically in different hierarchies, not just the interconnections. For example, the system of <ref> [HaSm77] </ref> is a tree of buses, with processors at all the nodes of the tree. [IrNa88] introduced clusters consisting of processors connected to local memory modules through a cluster network (which can be a crossbar). The cluster network also provides connections to neighboring clusters.
Reference: [HeFM88] <author> D. Hensgen, R. Finkel, and U. Manber, </author> <title> ``Two algorithms for barrier synchronization,'' </title> <booktitle> Int. Jour. of Parallel Programming, </booktitle> <volume> vol. 17, no. 1, </volume> <year> 1988, </year> <pages> pp. 1-17. </pages>
Reference-contexts: We will focus on a few of these implementations, for systems with multistage networks. In the following subsections, we will divide our choices into two major barrier types, those with and without hot spots. <ref> [HeFM88] </ref> also compares the performance of several software barriers; the barriers in the paper are minor variants of the structures studied in [ArJo89]. [MeSc91] also presented efficient algorithms for synchronization. <p> In a bus-based system, the extra traffic generated by the butterfly could swamp the bus and result in severe performance degradation. Tree barriers are more suitable than butterfly barriers for bus-based systems; this is the conclusion reached by studies such as <ref> [HeFM88] </ref> and does not necessarily apply to systems with multistage networks. As might be expected, the hot-spot schemes perform better when starting times are spread out. Requests from the processors are distributed over time, and the initial hot-spot contention is 40 less intense.
Reference: [HsYe91] <author> W. Hsu and P. Yew, </author> <title> ``The performance of hierarchical systems with wiring constraints,'' </title> <booktitle> 1991 Int. Conf. on Parallel Processing, Aug. 1991, </booktitle> <volume> vol. I, </volume> <pages> pp. 9-16. </pages>
Reference-contexts: Destinations outside the sphere are requested less frequently. Generally, each processor has its own sphere, or small groups of processors may share a sphere of locality. There are numerous ways to define the sphere of locality; it can be based on physical network distances [Abra90], network partitions [DaEa90], <ref> [HsYe91] </ref>, or combinations of the two. A message traffic model that has attracted much concern is the ``hot-spot'' model studied in [PfNo85], in which all (or a large number of) processors will attempt to send messages to a single destination with relatively high probability. <p> Many compared the number of links required in the different systems, and some scaled their performance numbers based on link costs. Most studies have limited the discussion to traffic patterns with high locality. However, only [ChHo91] and our work (first published in <ref> [HsYe91] </ref>, and expanded in [HsYe92a] and [HsYe92b]) have incorporated wiring constraints directly into the architectural parameters that affect system performance, that 59 is, channel width and message length. Our work represents a more detailed assessment of the performance of hierarchical networks, under more realistic constraints. <p> For the more distant global connections, where it is unrealistic to use buses, point-to-point networks will be used. We defined a good example of SGC/EA in <ref> [HsYe91] </ref>: each cluster has a global switch providing hypercube connections to other clusters, and local processors access the global switch via a system of buses. A similar structure was studied in [ChHo91]. SGC/EA systems based on tori 65 and generalized shuffle-exchange networks were analyzed in [HsYe92a] and [HsYe92b]. <p> The Extended Hypercube of [KuPa92a] is a variation of SGC/EA: each cluster is a hypercube, with all nodes within a cluster connected to a single network controller, which is similar to the global switch in [ChHo91] and <ref> [HsYe91] </ref>. The network controllers are connected to form a global hypercube network. This appears to be overkill in terms of hardware complexity; we will show that if enough ports are provided to the global switch, a local network connecting the nodes within the same cluster is not necessary. <p> As explained in [AbPa89], this is because of the lower probability of multiple simultaneous message arrivals at high-dimensional channels. [AbPa90] presented an analysis of the torus with random routing; an approach to studying LR routing in tori will be presented in Section 5.5.2. As in <ref> [HsYe91] </ref>, we assume random routing in the clustered systems for convenience of analysis, but LR routing in flat systems. Hence, our performance numbers for clustered systems are conservative estimates. <p> These guidelines ignore channel efficiency effects. If messages are short and transmissions are not efficient, the number of local buses prescribed above may be barely insufficient to handle saturation traffic. As observed in <ref> [HsYe91] </ref>, if local traffic is heavy, processors are able to issue requests past the rate of m g,sat without saturating the global channels. <p> Data for the MSN have also been omitted; trends are similar to the uniform traffic environments. The performance of FHs and CHs under local traffic has been discussed in detail in <ref> [HsYe91] </ref>. In summary, the saturation load of an FH does not change if locality of traffic increases. Although the average message distance of messages is shorter, the channels of the FH that handle both global and local traffic are over-utilized, whereas the channels that handle only global traffic are under-utilized.
Reference: [HsYe92a] <author> W. Hsu and P. Yew, </author> <title> ``The impact of wiring constraints on hierarchical network performance,'' </title> <booktitle> 1992 Int. Parallel Processing Symp., </booktitle> <month> March </month> <year> 1992, </year> <pages> pp. 580-588. </pages>
Reference-contexts: Many compared the number of links required in the different systems, and some scaled their performance numbers based on link costs. Most studies have limited the discussion to traffic patterns with high locality. However, only [ChHo91] and our work (first published in [HsYe91], and expanded in <ref> [HsYe92a] </ref> and [HsYe92b]) have incorporated wiring constraints directly into the architectural parameters that affect system performance, that 59 is, channel width and message length. Our work represents a more detailed assessment of the performance of hierarchical networks, under more realistic constraints. <p> A similar structure was studied in [ChHo91]. SGC/EA systems based on tori 65 and generalized shuffle-exchange networks were analyzed in <ref> [HsYe92a] </ref> and [HsYe92b]. Several systems are based on SGC/EA. Dash [LLGW92] has a single cluster bus and a global mesh. Cm* [GeJS82] uses a local bus to connect processors within the same cluster, and several global buses to provide global communications. <p> Hence, the system wiring cost of a flat k-ary n-cube and its clustered analogue are always identical. Next consider torus-like structures. (As in <ref> [HsYe92a] </ref>, we will study networks with wraparound channels; hence, in an N-processor system, processor 0 is connected to 1, dd N-1, dd N, and N- dd N.) Consider a 1024-processor bidirectional flat torus (FT) with a 64-processor subtorus and 512 pins per board.
Reference: [HsYe92b] <author> W. Hsu and P. Yew, </author> <title> ``Performance evaluation of wire-limited hierarchical networks,'' </title> <journal> submitted to IEEE Trans. on Parallel and Distributed Systems. </journal> <volume> 166 </volume>
Reference-contexts: Many compared the number of links required in the different systems, and some scaled their performance numbers based on link costs. Most studies have limited the discussion to traffic patterns with high locality. However, only [ChHo91] and our work (first published in [HsYe91], and expanded in [HsYe92a] and <ref> [HsYe92b] </ref>) have incorporated wiring constraints directly into the architectural parameters that affect system performance, that 59 is, channel width and message length. Our work represents a more detailed assessment of the performance of hierarchical networks, under more realistic constraints. <p> A similar structure was studied in [ChHo91]. SGC/EA systems based on tori 65 and generalized shuffle-exchange networks were analyzed in [HsYe92a] and <ref> [HsYe92b] </ref>. Several systems are based on SGC/EA. Dash [LLGW92] has a single cluster bus and a global mesh. Cm* [GeJS82] uses a local bus to connect processors within the same cluster, and several global buses to provide global communications.
Reference: [HsYZ87] <author> W. Hsu, P. Yew, and C. Zhu, </author> <title> ``An enhancement scheme for hypercube interconnection networks,'' </title> <booktitle> 1987 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1987, </year> <pages> pp. 820-823. </pages>
Reference-contexts: For 2 example, Abraham [Abra90] showed that if relatively simple and natural assumptions are made about message traffic, networks that have similar static (graph-theoretic) characteristics may turn out to have very different message routing performance. This is also exemplified in our research involving a generalized shuffle-exchange network. In <ref> [HsYZ87] </ref>, we found that performance trade-offs were more complicated than the static graph-theoretic measure of diameter would indicate; system size actually turned out to be an important factor in relative performance. On a lower level, the graph-theoretic approach ignores the physical problems of network construction. <p> Since we are concerned with pinout constraints, we will also study a network based on the graph of [ImIt81], which has an optimal diameter for a given channel fanout. Its performance (without wiring constraint considerations) was analyzed in <ref> [HsYZ87] </ref>. It is actually a generalization of the shuffle-exchange graph [Ston71], and we shall refer to this network as a generalized shuffle-exchange (GSE) network. We will examine these topologies in more detail and construct two-level hierarchical networks from them using the SGC/EA approach. <p> Hence, our performance numbers for clustered systems are conservative estimates. The systems based on MSNs all use random routing, because the principle of LR routing is not clearly defined in those networks. The GSE has a more restricted routing scheme <ref> [HsYZ87] </ref> (see Section 5.3.3 for details). The assumption of infinite buffering in [AbPa90] is unrealistic. However, we expect network operation to be in regions of moderate channel utilization (&lt;80%), where the message delay curve is relatively flat and few buffers are necessary.
Reference: [HwGh87] <author> K. Hwang and J. Ghosh, </author> <title> ``Hypernet architectures for parallel processing,'' </title> <booktitle> 1987 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1987, </year> <pages> pp. 810-819. </pages>
Reference-contexts: Both global and local networks are hypercubes. If the cluster fanout is smaller than the number of nodes within a cluster, then the nodes within each cluster with the lower addresses have direct access to the global channels. Hypernets are another generalization of the CCC concept. <ref> [HwGh87] </ref> focuses on hypernets constructed from hypercubes, but they can be based on many different topologies. To avoid the problems discussed earlier, restrictions are made on the number of processors within a cluster and the cluster fanout.
Reference: [HwSh91] <author> K. Hwang and S. Shang, </author> <title> ``Wired-NOR barrier synchronization for designing large shared-memory multiprocessors,'' </title> <booktitle> 1991 Int. Conf. on Parallel Processing, Aug. 1991, </booktitle> <volume> vol. I, </volume> <pages> 171-175. </pages>
Reference-contexts: We use barriers as an extreme environment to tax the capabilities of combining networks. Fast barriers are crucial for the efficient execution of many parallel algorithms and compiler optimizations (see, for example, [Poly88]). Cheap barrier hardware has been proposed, for example [BePo90] and <ref> [HwSh91] </ref>, but combining networks can handle many other hot-spot accesses, not just barriers. We also use several well-defined software barrier schemes for performance comparisons. In parallel queues, processors issue continuous streams of accesses to several hot spots, and the interesting measure is the average access time to the parallel queue. <p> Lipovski and Vaughan's fetch-and-op tree [LiVa88], for example, is only capable of combining simultaneous accesses to a single hot spot. It is more suitable for SIMD machines. Other hardware schemes have been 25 proposed for barrier synchronization, such as [BePo90] and <ref> [HwSh91] </ref>, but they are not flexible enough to handle traffic to memory hot spots. Feedback has been proposed by Scott and Sohi [ScSo89] for avoiding tree saturation, but it does not improve the latency of hot-spot accesses and is not a substitute for combining.
Reference: [ImIt81] <author> M. Imase and M. Itoh, </author> <title> ``Design to minimize diameter on building-block network,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-30, no. 6, </volume> <month> June </month> <year> 1981, </year> <pages> pp. 439-442. </pages>
Reference-contexts: MSNs can have channels that are twice as wide as analogous tori, for the same pinout and bisection width. Since we are concerned with pinout constraints, we will also study a network based on the graph of <ref> [ImIt81] </ref>, which has an optimal diameter for a given channel fanout. Its performance (without wiring constraint considerations) was analyzed in [HsYZ87]. It is actually a generalization of the shuffle-exchange graph [Ston71], and we shall refer to this network as a generalized shuffle-exchange (GSE) network.
Reference: [IrNa88] <author> K. Irani and A. Naji, </author> <title> ``Performance study of a clustered shared-memory multiprocessor,'' </title> <booktitle> 1988 ACM Int. Conf. on Supercomputing, </booktitle> <month> July </month> <year> 1988, </year> <pages> pp. 304-314. </pages>
Reference-contexts: Odds and ends Some structures do not fit easily into our taxonomy. There have been systems proposed where the processors are physically in different hierarchies, not just the interconnections. For example, the system of [HaSm77] is a tree of buses, with processors at all the nodes of the tree. <ref> [IrNa88] </ref> introduced clusters consisting of processors connected to local memory modules through a cluster network (which can be a crossbar). The cluster network also provides connections to neighboring clusters. In addition, a processor can access any memory through a system interconnection network (of some chosen topology).
Reference: [Jaya87] <author> D. Jayasimha, </author> <title> ``Parallel access to synchronization variables,'' </title> <booktitle> 1987 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1987, </year> <pages> pp. 97-100. </pages>
Reference-contexts: Backoff schemes for barriers Some work has been done on the use of ``backoff'' schemes in barriers. (See, for example, [AgCh89], <ref> [Jaya87] </ref>, and [Jaya88].) In the 2-counter barrier, a backoff scheme would require processors entering the barrier to wait a certain amount of time before polling the counter B that controls the exit phase.
Reference: [Jaya88] <author> D. Jayasimha, </author> <title> ``Distributed synchronizers,'' </title> <booktitle> 1988 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1988, </year> <pages> pp. 23-27. </pages>
Reference-contexts: Backoff schemes for barriers Some work has been done on the use of ``backoff'' schemes in barriers. (See, for example, [AgCh89], [Jaya87], and <ref> [Jaya88] </ref>.) In the 2-counter barrier, a backoff scheme would require processors entering the barrier to wait a certain amount of time before polling the counter B that controls the exit phase.
Reference: [Jaya92] <author> D. Jayasimha, </author> <title> ``Partially shared variables and hierarchical shared memory multiprocessor architectures,'' </title> <booktitle> 1992 IEEE Int. Phoenix Conf. on Computers and Communications, </booktitle> <month> April </month> <year> 1992, </year> <note> to appear. </note>
Reference: [Kell79] <author> R.M. Keller et al., </author> <title> ``A loosely-coupled applicative multiprocessing system,'' </title> <booktitle> 1979 Nat. Computer Conf., AFIPS, </booktitle> <volume> vol. 48, </volume> <month> June </month> <year> 1979, </year> <pages> pp. 613-622. </pages>
Reference-contexts: The local crossbar provides connections to several global crossbars and allows access to other clusters. This is an inefficient version of SGC/EA, since each cluster has a fanout equal to the 66 number of processors per cluster, and the global crossbars incur additional hardware cost. <ref> [Kell79] </ref> describes an SGC/EA structure comprising a binary tree of buses with processors at the leaf nodes only. This was analyzed in [WuLi81]; a similar structure with non-binary fanouts was studied in [WiMu88].
Reference: [Koni91] <author> J. Konicek et al., </author> <title> ``The organization of the Cedar system,'' </title> <booktitle> 1991 Int. Conf. on Parallel Processing, Aug. 1991, </booktitle> <volume> vol. I, </volume> <pages> pp. 49-56. </pages>
Reference-contexts: The Kmap also provides access to other clusters through several intercluster buses. The MIDAS project [Mapl85] proposed a system with processors, memories and I/O devices organized as a pyramid, with a hierarchy of crossbar switches providing the interconnections. The Cedar architecture <ref> [Koni91] </ref> has processors within a cluster sharing local storage through a crossbar, with an Omega network providing connections to global shared memory. Cenju [NTKM91], developed primarily for circuit simulation, has a global multistage shuffle-based network. Processors within a cluster access the global network through a cluster bus.
Reference: [KrSn83] <author> C.P. Kruskal and M. Snir, </author> <title> ``The performance of multistage interconnection networks for multiprocessors,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-32, No. 12, </volume> <month> Dec. </month> <year> 1983, </year> <pages> pp. 1091-1098. </pages>
Reference-contexts: For example, [Bian89] describes in detail advanced packaging techniques for the construction of a network for large Ultracomputer systems. Much work has been done on the performance analysis of multistage networks. Some of this research has addressed pin-limitations in switch implementations. <ref> [KrSn83] </ref> introduced some detailed analysis for estimating delays in Omega networks with infinite buffering. The authors applied this analysis to compare Omega networks using switches 71 of different sizes. They assumed that a switch is implemented on a single chip, and the chip is pin-limited. <p> They found that networks with bigger switches can handle heavier traffic intensities and mapped out regions of architectural parameters where specific switch sizes are superior. [SzFa90] presented analysis of Omega networks with finite buffers and advanced queue designs, and compared networks of comparable costs using the criteria of <ref> [KrSn83] </ref>. It was assumed that a 2x2 switch is k/2 times as fast as a kxk switch. Performance trends are similar to those of [KrSn83], except the optimal region for 2x2 switches is greatly expanded, because of the restriction of finite buffers. <p> sizes are superior. [SzFa90] presented analysis of Omega networks with finite buffers and advanced queue designs, and compared networks of comparable costs using the criteria of <ref> [KrSn83] </ref>. It was assumed that a 2x2 switch is k/2 times as fast as a kxk switch. Performance trends are similar to those of [KrSn83], except the optimal region for 2x2 switches is greatly expanded, because of the restriction of finite buffers. More detailed treatments can be found in the work of Franklin and his associates. In [Fran81], single-chip implementations of crossbars and banyans were studied. <p> For N=B-1, the CGSE will have better saturation performance than the FH. However, the fanout of the global crossbar becomes too large for fast implementations. 5.5. Queueing analysis In this subsection we will discuss the queueing analysis we use to arrive at our performance numbers. In <ref> [KrSn83] </ref>, using the Pollaczek-Khinchin mean-value formula, the authors derived the expression for the average time spent by a message in a queue: (5.1) s d = 1 hh + V hhhhhhhhh where E is the mean of the arrivals per cycle at the queue, and V is the variance of the <p> For partial cut-through, we estimate the time saved for a message at each switch to be its service time t m . This is a simpler estimate than the one used in [Abra90], and has been used widely in the literature (see, for example, <ref> [KrSn83] </ref> and [Agar91]). Hence, for partial cut-through, T global,cut =T global - N 1 -1 N 1 hhhhh 2 n 1 hhh t m , and T local,cut =T local - N 0 -1 N 0 hhhhh 2 T cut =aT local,cut +(1-a)T global,cut .
Reference: [KuPa92a] <author> J. Kumar and L. Patnaik, </author> <title> ``Extended hypercube: a hierarchical interconnection network of hypercubes,'' </title> <journal> IEEE Trans. on Par. and Dist. Systems, </journal> <volume> vol. 3, no. 1, </volume> <month> Jan. </month> <year> 1992, </year> <pages> pp. 45-57. </pages>
Reference-contexts: Processors within a cluster access a global crossbar via a single cluster bus. The original formulation was for a shared-memory architecture, and processors can access memories in the same cluster through a local crossbar. [MaEl92] generalizes the structure of [AgMa85] to multiple levels of hierarchy. The Extended Hypercube of <ref> [KuPa92a] </ref> is a variation of SGC/EA: each cluster is a hypercube, with all nodes within a cluster connected to a single network controller, which is similar to the global switch in [ChHo91] and [HsYe91]. The network controllers are connected to form a global hypercube network.
Reference: [KuPa92b] <author> J. Kumar and L. Patnaik, </author> <title> ``Hierarchical network of hypercubes with folded connections,'' </title> <booktitle> Proc. Parallel Systems Fair, 1992 Int. Parallel Processing Symp., </booktitle> <month> March </month> <year> 1992, </year> <pages> pp. 33-37. </pages>
Reference-contexts: This appears to be overkill in terms of hardware complexity; we will show that if enough ports are provided to the global switch, a local network connecting the nodes within the same cluster is not necessary. The Extended Hypercube with Folded Connections of <ref> [KuPa92b] </ref> is the identical organization but with folded hypercubes instead of the full hypercubes. [ChSh87] proposes a structure in which processors within a cluster share a local crossbar. The local crossbar provides connections to several global crossbars and allows access to other clusters.
Reference: [KuPf86] <author> M. Kumar and G. Pfister, </author> <title> ``The onset of hot spot contention,'' </title> <booktitle> 1986 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1986, </year> <pages> pp. 28-34. </pages>
Reference-contexts: A and B are hot spots; that is, all the processors will try to access the memory modules containing them, and intense congestion in the network will be the result <ref> [KuPf86] </ref>. A variation is a counter+broadcast barrier (similar algorithms can be found in [ArJo89]). The entry phase is identical to that of the 2-counter barrier. In the exit phase, the last processor to enter sets individual flags (stored in shared memory) for all the other processors.
Reference: [LaDh88] <author> S. Lakshmivarahan and S.K. Dhall, </author> <title> ``A new hierarchy of hypercube interconnection schemes for parallel computers,'' </title> <booktitle> The Jour. of Supercomputing, </booktitle> <volume> 2, </volume> <year> 1988, </year> <pages> pp. 81-108. </pages>
Reference-contexts: Notice this does not exclude traditional topologies such as hypercubes and meshes, which are generally not considered to be hierarchical, or recursive structures such as those of <ref> [LaDh88] </ref>. However, we will focus on topologies that cannot be reduced to a single-level organization. 61 Let N 1 be the system size, and N 1 =2 n 1 .
Reference: [Lawr75] <author> D. Lawrie, </author> <title> ``Access and alignment of data in an array processor", </title> <journal> IEEE Trans. on Computers, </journal> <month> December </month> <year> 1975, </year> <pages> pp. 1145-1155. </pages>
Reference-contexts: The process repeats O (N) times before each processor can successfully access the lock once. A more sophisticated and general solution was proposed by the IBM RP3 group [PfNo85]. A non-combining Omega network <ref> [Lawr75] </ref> handled non-hot-spot requests, and a multistage network with special hardware combined simultaneous hot-spot requests. (The combining network was not built for the RP3 prototype.) Combining networks were originally proposed by the NYU Ultracomputer project [GGKM83], for combining fetch-and-op instructions.
Reference: [Lee90] <author> C. Lee, </author> <title> ``Barrier synchronization over multistage interconnection networks,'' </title> <booktitle> 2nd IEEE Symp. on Parallel and Distributed Processing, </booktitle> <month> Dec. </month> <year> 1990, </year> <pages> pp. 130-133. 167 </pages>
Reference-contexts: We also perform simulation-based benchmarking studies based on barriers and queues. We prefer simulation over analysis for this work; in these environments, analysis is too cumbersome without oversimplifying the model. For example, <ref> [Lee90] </ref> studied the performance of barriers in multistage networks based on the analysis of unbuffered networks from [Pate81]. This type of analysis is useful for regular memory traffic, but inappropriate for synchronization traffic.
Reference: [Leis85] <author> C. Leiserson, </author> <title> ``Fat trees: universal networks for hardware-efficient supercomputing,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-34, no. 10, </volume> <month> Oct. </month> <year> 1985, </year> <pages> pp. 892-901. </pages>
Reference-contexts: A possible alternative is to use variable-width channels depending on the distance from the IP, that is, the channel width becomes a function of the level j. This is similar to the idea behind Fat Trees <ref> [Leis85] </ref>.
Reference: [LeKK86] <author> G. Lee, C. Kruskal and D. Kuck, </author> <title> ``The effectiveness of combining in shared memory parallel computers in the presence of hot spots,'' </title> <booktitle> 1986 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1986, </year> <pages> pp. 35-41. </pages>
Reference-contexts: Closed traffic is more realistic in most synchronization and OS operations, although most previous work on hot-spot traffic (such as [PfNo85] and <ref> [LeKK86] </ref>) has focused on open traffic. 10 Performance studies of combining networks have mostly focused on abstract probabilistic models. We have chosen a benchmarking approach in an effort to approximate real environments more closely. We examine two types of significantly different applications which involve hot-spot accesses: barriers and queues. <p> Comparing different versions of the SSE network We first ran experiments comparing the Stationary scheme and the Perambulating scheme. All processors started to synchronize at the same time. We also studied the effects of different wait buffer sizes, since earlier work (for example <ref> [LeKK86] </ref>) indicates that limited wait buffer space is an important factor affecting the performance.
Reference: [Levi87] <author> S.P. Levitan, </author> <title> ``Measuring communications structures in parallel architectures and algorithms,'' in The Characteristics of Parallel Algorithms, </title> <editor> ed. L.H. Jamieson, D. Gannon and R.J. Douglass, </editor> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1987. </year>
Reference-contexts: The use of programs in an intermediate language requires much less storage space than address traces and provides more flexibility. A variant of this approach involves estimating lower- and upper- bound performance of simple parallel algorithms on machines based on specific topologies (see, for example, <ref> [Levi87] </ref>). We consider this method too high-level for our purposes. 1.2.3. Queueing analysis vs. simulation studies The message traffic generated from uniform traffic-based models are often used to drive queueing analysis or simulation studies.
Reference: [Liu88] <author> Y. Liu, </author> <title> ``Delta network performance and hot spot traffic,'' Ultracomputer Note No. </title> <type> 132, </type> <institution> Courant Institute, </institution> <address> New York University, </address> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: Other researchers have presented analysis for general hot-spot traffic, but this type of analysis does not give information for estimating the performance of barriers and queues. For example, <ref> [Liu88] </ref> addressed the analysis of general hot-spot traffic in unbuffered networks and presented some performance bounds for buffered networks. We emphasize that there are two components to the problems resulting from hot-spot accesses: the long latency of the hot-spot accesses themselves, and the hot-spot congestion that slows down non-hot-spot accesses.
Reference: [LiVa88] <author> G. Lipovski and P. Vaughan, </author> <title> ``A Fetch-and-op implementation for parallel computers,'' </title> <booktitle> 15th Int. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1988, </year> <pages> pp. 384-392. </pages>
Reference-contexts: Special hardware of this type tends to be too rigid and difficult to use in an environment with process migration or multiprogramming, or when only some of the processors are involved in the synchronization. Lipovski and Vaughan's fetch-and-op tree <ref> [LiVa88] </ref>, for example, is only capable of combining simultaneous accesses to a single hot spot. It is more suitable for SIMD machines. Other hardware schemes have been 25 proposed for barrier synchronization, such as [BePo90] and [HwSh91], but they are not flexible enough to handle traffic to memory hot spots.
Reference: [LLGW92] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam, </author> <title> ``The Stanford Dash multiprocessor,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 25, no. 3, </volume> <month> Mar. </month> <year> 1992, </year> <pages> pp. 63-79. </pages>
Reference-contexts: Cenju [NTKM91], developed primarily for circuit simulation, has a global multistage shuffle-based network. Processors within a cluster access the global network through a cluster bus. In Dash <ref> [LLGW92] </ref>, processors in a cluster share memory through a bus. Each cluster bus provides connection to a directory; non-local accesses go through the directory to the global mesh network. In addition, hierarchical interconnects have been an active area of research over the past 20 years. <p> A similar structure was studied in [ChHo91]. SGC/EA systems based on tori 65 and generalized shuffle-exchange networks were analyzed in [HsYe92a] and [HsYe92b]. Several systems are based on SGC/EA. Dash <ref> [LLGW92] </ref> has a single cluster bus and a global mesh. Cm* [GeJS82] uses a local bus to connect processors within the same cluster, and several global buses to provide global communications. In the Cenju system [NTKM91], processors within a cluster access a global network through a cluster bus.
Reference: [MaBa92] <author> Q. Malluhi and M. Bayoumi, </author> <title> ``Properties and performance of the hierarchical hypercube,'' </title> <booktitle> 1992 Int. Parallel Processing Symp., </booktitle> <month> March </month> <year> 1992, </year> <pages> pp. 47-50. </pages>
Reference-contexts: If there are B global channels per cluster, and B+j processors per cluster, then only B of those processors (similar to interface processors) will be directly connected to global channels. 69 The hierarchical hypercube of <ref> [MaBa92] </ref> is based on the same idea as the CCC. Both global and local networks are hypercubes. If the cluster fanout is smaller than the number of nodes within a cluster, then the nodes within each cluster with the lower addresses have direct access to the global channels.
Reference: [MaEl92] <editor> I.O. Mahgoub and A.K. Elmagarmid, </editor> <title> ``Performance analysis of a generalized class of m-level hierarchical multiprocessor systems,'' </title> <journal> IEEE Trans. on Par. and Dist. Systems, </journal> <volume> vol. 3, no. 2, </volume> <month> Mar. </month> <year> 1992, </year> <pages> pp. 129-138. </pages>
Reference-contexts: Processors within a cluster access a global crossbar via a single cluster bus. The original formulation was for a shared-memory architecture, and processors can access memories in the same cluster through a local crossbar. <ref> [MaEl92] </ref> generalizes the structure of [AgMa85] to multiple levels of hierarchy. The Extended Hypercube of [KuPa92a] is a variation of SGC/EA: each cluster is a hypercube, with all nodes within a cluster connected to a single network controller, which is similar to the global switch in [ChHo91] and [HsYe91].
Reference: [Mapl85] <author> C. Maples, </author> <title> ``Pyramids, </title> <booktitle> crossbars and thousands of processors,'' 1985 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1985, </year> <pages> pp. 682-688. </pages>
Reference-contexts: Cm* [GeJS82] was one of the earliest. Processors within a cluster are connected via a bus to a module called the Kmap. The Kmap also provides access to other clusters through several intercluster buses. The MIDAS project <ref> [Mapl85] </ref> proposed a system with processors, memories and I/O devices organized as a pyramid, with a hierarchy of crossbar switches providing the interconnections. The Cedar architecture [Koni91] has processors within a cluster sharing local storage through a crossbar, with an Omega network providing connections to global shared memory.
Reference: [Maxe87] <author> N. Maxemchuk, </author> <title> ``Routing in the Manhattan Street network,'' </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. COM-35, No. 5, </volume> <month> May </month> <year> 1987, </year> <pages> pp. 503-512. </pages>
Reference-contexts: When pins and wires are limited, networks with unidirectional channels become very attractive, because they have half the number of channels per node as their bidirectional counterparts. Maxemchuk's MaNhattan Street Network (MSN) <ref> [Maxe87] </ref> is a unidirectional torus with channels that alternate in direction. For example, nodes in the even/odd numbered rows and columns would have channels routing to increasing/decreasing node numbers, with the appropriate moduli.
Reference: [MeSc80] <author> P. Merlin and P. Schweitzer, </author> <title> ``Deadlock avoidance in store-and-forward networks --- I: store-and-forward deadlock,'' </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. COM-28, No.3, </volume> <year> 1980, </year> <pages> pp. 345-354. </pages>
Reference-contexts: Deadlock handling in the SSE A basic problem with recirculating networks with finite-sized buffers is the possibility of deadlock. Classical deadlock avoidance schemes (see, for example, <ref> [MeSc80] </ref>) usually involve a fair amount of buffer space or a large number of virtual network channels. They are too involved to be implemented in hardware or firmware. In addition, buffer management schemes are usually designed to avoid packet collision.
Reference: [MeSc91] <author> J. Mellor-Crummey and M. Scott, </author> <title> ``Algorithms for scalable synchronization on shared-memory multiprocessors,'' </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> Vol. 9 No. 1, </volume> <month> Feb. </month> <year> 1991, </year> <pages> pp. 21-65. </pages>
Reference-contexts: In the following subsections, we will divide our choices into two major barrier types, those with and without hot spots. [HeFM88] also compares the performance of several software barriers; the barriers in the paper are minor variants of the structures studied in [ArJo89]. <ref> [MeSc91] </ref> also presented efficient algorithms for synchronization. However, they assumed a multiprocessor configuration in which each processor is able to access one of the memory modules without going through the network; the performance improvements are from clever use of the direct access capability. <p> However, this idea does not alleviate the congestion experienced by other processors. The use of an alternate ``quick path'' is the basis behind many of the algorithms in <ref> [MeSc91] </ref> (in their case, the quick path is the connection between a processor and its local memory). In Figure 3.6, we plot graphs of performance bounds for the various barriers. For the software barriers, we used the bounds derived in Chapter 2.
Reference: [NTKM91] <author> T. Nakata, N. Tanabe, N. Kajihara, S. Matsushita, H. Onozuka, Y. Asano, and N. Koike, ``Cenju: </author> <title> a multiprocessor system with a distributed shared memory scheme for modular circuit simulation,'' </title> <booktitle> Int. Symp. on Shared Memory Multiprocessing, </booktitle> <address> Tokyu, Japan, </address> <month> April </month> <year> 1991, </year> <pages> pp. 82-90. </pages>
Reference-contexts: The Cedar architecture [Koni91] has processors within a cluster sharing local storage through a crossbar, with an Omega network providing connections to global shared memory. Cenju <ref> [NTKM91] </ref>, developed primarily for circuit simulation, has a global multistage shuffle-based network. Processors within a cluster access the global network through a cluster bus. In Dash [LLGW92], processors in a cluster share memory through a bus. <p> Several systems are based on SGC/EA. Dash [LLGW92] has a single cluster bus and a global mesh. Cm* [GeJS82] uses a local bus to connect processors within the same cluster, and several global buses to provide global communications. In the Cenju system <ref> [NTKM91] </ref>, processors within a cluster access a global network through a cluster bus. The global network is a multistage network based on the shuffle, with multiplexor- and demultiplexor-switches.
Reference: [Padm91a] <author> K. Padmanabhan, </author> <title> ``Effective architectures for data access in a shared memory hierarchy,'' </title> <booktitle> Jour. of Parallel and Distributed Computing 11, </booktitle> <year> 1991, </year> <pages> pp. 314-327. </pages>
Reference-contexts: In addition, hierarchical interconnects have been an active area of research over the past 20 years. For example, there have been hierarchies based on shared buses [WuLi81], crossbar switches [AgMa85], meshes [Carl85], hypercubes [DaEa90] [DaEa91] [Aboe91], and combinations of Omega networks and composite cube networks <ref> [Padm91a] </ref>. In Section 4.2 we attempt to develop a taxonomy of hierarchical systems that will provide the architectural context for our performance studies of hierarchical systems in Chapter 5. Most of the previous research on hierarchical systems has used fairly traditional models of hardware cost comparisons. <p> The cluster network also provides connections to neighboring clusters. In addition, a processor can access any memory through a system interconnection network (of some chosen topology). The number of ports per cluster to the system interconnection network can be chosen by the system architect. <ref> [Padm91a] </ref> examined several hierarchical organizations based on indirect binary n-cube (IBNC) networks. One is a system with both local and global IBNC networks; each processor has its own connection to the global network.
Reference: [Padm91b] <author> K. Padmanabhan, </author> <title> ``On the trade-off between node degree and communication channel width in shuffle-exchange networks,'' </title> <booktitle> 3rd IEEE Symposium on Parallel and Distributed Processing, </booktitle> <address> Dallas TX, </address> <month> Dec. </month> <year> 1991, </year> <pages> pp. 120-127. </pages>
Reference-contexts: B can be chosen by the system architect based on the desired performance. For the purposes of this paper, we have restricted B to powers of 2. <ref> [Padm91b] </ref> presented an analysis of flat GSEs with non-power-of-2 fanouts. A flat GSE (FGSE) with fanout B would simply be a GSE partitioned across N boards, with N 0 processors per board.
Reference: [Pate81] <author> J.H. Patel, </author> <title> ``Performance of processor-memory interconnections for multiprocessors,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-30, No. 10, </volume> <month> Oct. </month> <year> 1981, </year> <pages> pp. 771-780. 168 </pages>
Reference-contexts: We also perform simulation-based benchmarking studies based on barriers and queues. We prefer simulation over analysis for this work; in these environments, analysis is too cumbersome without oversimplifying the model. For example, [Lee90] studied the performance of barriers in multistage networks based on the analysis of unbuffered networks from <ref> [Pate81] </ref>. This type of analysis is useful for regular memory traffic, but inappropriate for synchronization traffic. Requests competing for the same resources have to be dropped and resubmitted; it is possible for ``lagging'' synchronization requests to be dropped repeatedly because of contention, resulting in livelock. <p> In [Fran81], single-chip implementations of crossbars and banyans were studied. The channel width and switch size were kept constant, and the chip area, switch delay, and area-delay product were compared. Performance was based on a circuit-switched model analyzed in <ref> [Pate81] </ref>. It was found that the difference between crossbars and banyans was very small, based on these implementational constraints. In [FrWT82], the authors studied the problems involved in building a large crossbar network or banyan network from smaller single-chip crossbar switches.
Reference: [PeKr87] <author> W. Pence and P. Krusius, </author> <title> ``The fundamental limits for electronic packaging and systems,'' </title> <journal> IEEE Trans. on Components, Hybrids, and Manufacturing Technology, </journal> <volume> Vol. CHMT-10, No. 2, </volume> <month> June </month> <year> 1987, </year> <pages> pp. 176-183. </pages>
Reference-contexts: We also introduce a cost-effective option that will address some of the problems of non-uniform memory access. The second part of this dissertation attempts to incorporate into network analysis the lower-level constraints of constructing and packaging a large network. These concerns are summarized well in <ref> [PeKr87] </ref>: ``many critical packaging problems are not independent, and ... different system levels cannot be effectively designed independently from each other.'' As packaging technology has improved, wiring costs and pinouts have become important constraints in the construction of large multiprocessor networks. <p> Jean Baudrillard [Baud87] 4.1. Introduction Packaging problems have become important constraints in the design of large multiprocessor systems. Much research, such as <ref> [PeKr87] </ref> (quoted in Chapter 1), has pointed to the need for an integrated approach to system design that incorporates the impact of packaging technology. Large multiprocessor systems may contain hundreds or even thousands of powerful processors and memories.
Reference: [PfNo85] <author> G. Pfister and V. Norton, </author> <title> ``Hot spot contention and combining in multistage interconnection networks,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-34, No.10, </volume> <year> 1985, </year> <pages> pp. 943-948. </pages>
Reference-contexts: In the first part of this dissertation, we will focus on synchronization and other non-uniform memory accesses in shared memory machines. These accesses are very different from traditional models of memory access, in which processors are assumed to request all memories with equal probabil 3 ity. Earlier work <ref> [PfNo85] </ref> has shown that this type of access can lead to severe network congestion and performance degradation. Several solutions have been proposed, including hardware enhancements for combining conflicting requests and software algorithms for congestion control. However, the software algorithms are slow, and hardware enhancements are expensive. <p> There are numerous ways to define the sphere of locality; it can be based on physical network distances [Abra90], network partitions [DaEa90], [HsYe91], or combinations of the two. A message traffic model that has attracted much concern is the ``hot-spot'' model studied in <ref> [PfNo85] </ref>, in which all (or a large number of) processors will attempt to send messages to a single destination with relatively high probability. This type of access pattern is typical of synchronization and many other operating system functions. It may lead to extreme network congestion. <p> In Chapter 7, we look at the performance of alternative approaches for hierarchical designs. Conclusions are drawn in Chapter 8. 9 CHAPTER 2 HOT-SPOT ACCESSES AND SOFTWARE TECHNIQUES 2.1. Background In <ref> [PfNo85] </ref>, Pfister and Norton identified hot spots, a problematic type of multiprocessor memory access, in which a large number of processors try to access one or a small number of memory modules. <p> Hot-spot accesses are extremely detrimental to system performance. Even memory traffic not directed at the hot spots can be seriously obstructed. This is the ``tree saturation'' effect described in <ref> [PfNo85] </ref>. Many hardware- and software-based techniques have been proposed to avoid hot-spot accesses. The focus of our work in Chapters 2 and 3 is to evaluate the effectiveness of some of these techniques, especially combining networks (which will be described in detail in Chapter 3). <p> Closed traffic is more realistic in most synchronization and OS operations, although most previous work on hot-spot traffic (such as <ref> [PfNo85] </ref> and [LeKK86]) has focused on open traffic. 10 Performance studies of combining networks have mostly focused on abstract probabilistic models. We have chosen a benchmarking approach in an effort to approximate real environments more closely. <p> The process repeats O (N) times before each processor can successfully access the lock once. A more sophisticated and general solution was proposed by the IBM RP3 group <ref> [PfNo85] </ref>. A non-combining Omega network [Lawr75] handled non-hot-spot requests, and a multistage network with special hardware combined simultaneous hot-spot requests. (The combining network was not built for the RP3 prototype.) Combining networks were originally proposed by the NYU Ultracomputer project [GGKM83], for combining fetch-and-op instructions. <p> This latency-hiding technique can be used in combination with hardware combining to improve barrier performance. Investigation of similar techniques is beyond the scope of this work. Although multistage combining networks are fast and scalable, they are quite expensive <ref> [PfNo85] </ref>. We propose using a single-stage shuffle-exchange combining network as a compromise between an expensive multistage combining network and a sophisticated synchronization bus. We can use such a network just for handling hot-spot traffic. <p> However, it performs poorly when small wait buffers are used because when wait buffers close to the hot memory fill up, no more combining can take place. The network starts to exhibit behavior analogous to the ``tree-saturation'' observed in <ref> [PfNo85] </ref>. In this case, it is not regular memory traffic that becomes impeded by hot-spot requests 1 , but rather the return packets that use the same network.
Reference: [PiSm88] <author> F. Pittelli and D. Smitley, </author> <title> ``Analysis of a 3D toroidal network for a shared memory architecture,'' </title> <booktitle> Supercomputing 1988, </booktitle> <month> November </month> <year> 1988, </year> <pages> pp. 42-47. </pages>
Reference-contexts: The routing hardware will then forcibly misroute one of the packets from the queues to any output port that would accept a packet. We call this forced misrouting. Similar misrouting schemes have been proposed for different purposes (for example, <ref> [PiSm88] </ref>). The routing hardware makes necessary changes to the routing information in the packet so that it can arrive at the correct destination. Packets that are on the first trip from their source processors to the hot memory are never misrouted.
Reference: [Poly88] <author> C. Polychronopoulos, </author> <title> ``Compiler optimizations for enhancing parallelism and their impact on architecture design,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. 37, No. 8, </volume> <year> 1988, </year> <pages> pp. 991-1004. </pages>
Reference-contexts: The concern is with the finishing time. We use barriers as an extreme environment to tax the capabilities of combining networks. Fast barriers are crucial for the efficient execution of many parallel algorithms and compiler optimizations (see, for example, <ref> [Poly88] </ref>). Cheap barrier hardware has been proposed, for example [BePo90] and [HwSh91], but combining networks can handle many other hot-spot accesses, not just barriers. We also use several well-defined software barrier schemes for performance comparisons.
Reference: [PrVu81] <author> F. Preparata and J. Vuillemin, </author> <title> ``The cube-connected cycles: a versatile communication network for parallel computation,'' </title> <journal> Communications of the ACM, </journal> <volume> Vol. 24, No. 5, </volume> <year> 1981, </year> <pages> pp. 300-309. </pages>
Reference-contexts: In both cases, the distribution of channels among nodes could still be problematic, if restrictions are not made on the number of nodes within a cluster and the cluster fanout. The cube-connected-cycles (CCC) network <ref> [PrVu81] </ref> is a classical example of PGC. The global network is a hypercube, and each supernode of the global network is a cycle. Processors within a cluster are connected as a ring, and some subset of these processors have direct access to global channels.
Reference: [RaJo87] <author> A.G. Ranade and S.L. Johnson, </author> <title> ``The communication efficiency of meshes, boolean cubes and cube connected cycles for wafer scale integration,'' </title> <booktitle> 1987 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1987, </year> <pages> pp. 479-482. </pages>
Reference-contexts: Hence, tori have the best performance under these criteria. The results of [Dall90] (published in preliminary form before 1987, hence the discrepancy in dates) were the basis for <ref> [RaJo87] </ref>, which compared the capability of tori, hypercubes and cube-connected cycles to emulate butterflies and spanning trees. ([RaJo87] assumed simple layouts and made direct estimates of network area, instead of using bisection width.) [AbPa90] is a study in greater depth of (bidirectional) k-ary n-cubes with wiring constraints.
Reference: [ReFu87] <author> D. Reed and R. Fujimoto, </author> <title> Multicomputer networks: message-based parallel processing, </title> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1987. </year>
Reference-contexts: This simple mathematical model has been used to drive most queueing-theory-based studies of interconnection networks. Based on observations of locality in message traffic, researchers have proposed extensions to the uniform traffic model. Some prefer a ``sphere of locality'' model (for example, <ref> [ReFu87] </ref> or [Agar91]), in which a processor is more likely to send messages to a small number of destinations within its sphere of locality. Destinations outside the sphere are requested less frequently. Generally, each processor has its own sphere, or small groups of processors may share a sphere of locality. <p> For point-to-point networks, studies incorporating wiring constraints tend to be of more recent vintage. Work of a more theoretical slant includes [Cyph89], in which theoretical bounds for chip pinout requirements for hypercubes, meshes, and shuffle-exchange networks were derived. <ref> [ReFu87] </ref> made some simplifying assumptions about network topology and examined the queueing performance of large, unbounded, symmetric networks with pin-limited switches. Their results are not easily applied to conventional topologies such as hypercubes. [Dall90] reports some of the earliest work to use bisection width as the wiring cost constraint.
Reference: [Rudo81] <author> L. Rudolph, </author> <title> Software structures for ultraparallel computing, </title> <type> PhD dissertation, </type> <institution> Dept. of Computer Science, </institution> <address> New York University, New York, </address> <year> 1981. </year>
Reference-contexts: Parallel queue accesses are sufficiently different from barriers and will test different aspects of the hardware. 48 We first implemented parallel queue inserts and deletes similar to the algorithms outlined in <ref> [Rudo81] </ref> and [GoLR83]. (For details see Appendix A.) Inserts and deletes are symmetrical, and each insert/delete involves six potential hot-spot accesses. We simulated only the accesses to semaphores and counters that ensure correct queue accesses, and have left out the actual data movements to and from the queue locations. <p> We did not implement delete because its performance is highly dependent on implementation details of the data structures. The relative performance of the hqueue-like algorithm on the three networks is similar to the relative performance of the queue algorithm from <ref> [Rudo81] </ref> (see Figure 3.13). For parallel queues, we did not consider the use of software combining. In parallel queues, accesses are not as well-structured as in barriers. It is not realistic to simplify the algorithms in [TaYe90] by using static scheduling. <p> Using this data, we can study the interactions between architectural clustering and techniques for hierarchical program restructuring and latency reduction. 160 APPENDIX A PARALLEL QUEUE ALGORITHMS For all systems measured, the queue algorithms were ``hardcoded'' as finite-state machines. The first queue algorithm we implemented was based on <ref> [Rudo81] </ref> and [GoLR83]. The queue is finite-length and is implemented as a circular array of cells. Each processor gets a Bakery algo rithm ``ticket'' for the requested queue location. Qu and ql keep upper and lower bounds on the number of elements in the queue. <p> The Ultracomputer project uses simpler instructions and relies on a test-op-retest instruction sequence (for example, see <ref> [Rudo81] </ref> for details). 163
Reference: [ScSo89] <author> S. Scott and G. Sohi, </author> <title> ``The use of feedback in multiprocessors and its application to tree saturation control,'' </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 1, No. 4, </volume> <year> 1990, </year> <pages> pp. 384-398. </pages>
Reference-contexts: It is more suitable for SIMD machines. Other hardware schemes have been 25 proposed for barrier synchronization, such as [BePo90] and [HwSh91], but they are not flexible enough to handle traffic to memory hot spots. Feedback has been proposed by Scott and Sohi <ref> [ScSo89] </ref> for avoiding tree saturation, but it does not improve the latency of hot-spot accesses and is not a substitute for combining. Similarly, the intelligent allocation of hardware switch buffers (for example, [DiKu89] and [Tzen91]) relieves congestion, but does not address the problem of high-latency hot-spot accesses.
Reference: [Ston71] <author> H. Stone, </author> <title> ``Parallel processing with the perfect shuffle,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. 20, </volume> <pages> No.2, </pages> <year> 1971, </year> <pages> pp. 153-161. </pages>
Reference-contexts: Unidentified hot spots may also result from poor programming practices or bugs, and multiprogramming introduces other potential difficulties, but the treatment of such problems is beyond the scope of this dissertation. The single-stage shuffle-exchange (SSE) network is constructed from 2x2 crossbar switches <ref> [Ston71] </ref>. Figure 3.1 shows an SSE network with 8 ports used as a synchronization network for a multiprocessor system. For the purposes of this chapter, we assume a shared-memory dance-hall 26 architecture. A bidirectional Omega network provides processors with access to shared memory modules. <p> Since we are concerned with pinout constraints, we will also study a network based on the graph of [ImIt81], which has an optimal diameter for a given channel fanout. Its performance (without wiring constraint considerations) was analyzed in [HsYZ87]. It is actually a generalization of the shuffle-exchange graph <ref> [Ston71] </ref>, and we shall refer to this network as a generalized shuffle-exchange (GSE) network. We will examine these topologies in more detail and construct two-level hierarchical networks from them using the SGC/EA approach.
Reference: [SzFa90] <author> T. Szymanski and C. Fang, </author> <title> ``Design and analysis of buffered crossbars and banyans with cut-through switching,'' </title> <booktitle> Supercomputing 90, </booktitle> <pages> pp. 264-273. </pages>
Reference-contexts: They assumed that switch cycle time is independent of switch fanout, and compared networks with the same number of switches. They found that networks with bigger switches can handle heavier traffic intensities and mapped out regions of architectural parameters where specific switch sizes are superior. <ref> [SzFa90] </ref> presented analysis of Omega networks with finite buffers and advanced queue designs, and compared networks of comparable costs using the criteria of [KrSn83]. It was assumed that a 2x2 switch is k/2 times as fast as a kxk switch.
Reference: [TaYe90] <author> P. Tang and P. Yew, </author> <title> ``Software combining algorithms for distributing hot-spot addressing,'' </title> <booktitle> Jour. of Parallel and Distributed Computing 10, </booktitle> <month> Oct. </month> <year> 1990, </year> <pages> pp. 130-139. </pages>
Reference-contexts: Hence, their results are not applicable to dance-hall shared-memory machines. 2.2.1. Barrier synchronization with hot spots One of the simplest barrier implementations is a 2-counter barrier described in <ref> [TaYe90] </ref>. There are two shared counters: A controls the entry phase and is initialized to N (the number of processors), and B controls the exit phase and is initialized to zero. <p> at the memory for other processors, one cycle memory delay, logN network latency back to the processor, and one cycle for retrieval by the last processor to exit. (The argument for the exit phase is similar.) Two counters are necessary to ensure correctness if the routine is used repeatedly (see <ref> [TaYe90] </ref>). A and B are hot spots; that is, all the processors will try to access the memory modules containing them, and intense congestion in the network will be the result [KuPf86]. A variation is a counter+broadcast barrier (similar algorithms can be found in [ArJo89]). <p> We also examine two hot-spot-free barriers. The tree barrier avoids hot spots by replacing each hot spot with a tree of synchronization variables. The version used in our experiments is a static-scheduled version of the software synchronization tree of [YeTL87] and <ref> [TaYe90] </ref>, and is similar in implementation to the barrier in [ArJo89] with tree-structured entry and exit phases. The butterfly barrier [Broo86] combines 14 entry and exit phases into a structure similar to an FFT butterfly. We also implement a static scheduled version of the butterfly barrier for our experiments. <p> It is possible to combine more complicated fetch&ops, such as conditional fetch&ops similar to Cedar synchronization primitives <ref> [TaYe90] </ref>. Some simple conditional fetch&ops are used in the barrier algorithms described in Chapter 2, and in the parallel queue algorithms in Appen 23 dix A. They are atomic, and most take this general form (where X is a shared variable): if X meets condition, fetch&+(X,constant). <p> It is difficult to obtain sets of random barrier starting times that are representative of most parallel applications. Experiment 1 is a ``worst case'' scenario; experiment 2 gives the performance of barrier synchronization in a different environment. The barrier algorithm used is the standard 2-counter barrier described in <ref> [TaYe90] </ref>. The finishing times are measured for various system sizes. The finishing times of the 2-counter barrier algorithm in the SSE vary depending on the placement of the two hot spots. This is because the SSE network is not symmetrical with respect to the nodes. <p> Greater software overhead can be expected in realistic environments. In addition, software trees may not be appropriate for hot-spot accesses that are not as well-behaved as barriers. The combining algorithms become extremely complicated (see <ref> [TaYe90] </ref>) and accesses are slow. 3.4. Parallel queues Next we compare the performance of a combining SSE, a combining Omega network, and a non-combining Omega network using parallel queue accesses. <p> For parallel queues, we did not consider the use of software combining. In parallel queues, accesses are not as well-structured as in barriers. It is not realistic to simplify the algorithms in <ref> [TaYe90] </ref> by using static scheduling. The dynamic-scheduled versions of software-combining algorithms are much slower than hardware combining. 3.5. Discussion and conclusions There are many advantages to using the SSE for request combining. An SSE is relatively cheaper than a multistage combining network. <p> 5: wait for response; goto 6 6: fetch&+(ql,1); goto 7 7: wait for response; end 162 In all the queue algorithms (as in the barriers), conditional fetch-and-op operations (lines 0 in INSERT, 12 in DELETE, and 2 in H_INSERT) are implemented as single instructions, similar to Cedar synchronization primitives (see <ref> [TaYe90] </ref> for details). The Ultracomputer project uses simpler instructions and relies on a test-op-retest instruction sequence (for example, see [Rudo81] for details). 163
Reference: [Thom79] <author> C.D. Thompson, </author> <title> ``A complexity of VLSI,'' </title> <institution> Dept. of Computer Science, Carnegie-Mellon Univ., </institution> <type> Tech. Rep. </type> <institution> CMU-CS-80-140, </institution> <month> Aug. </month> <year> 1980. </year>
Reference-contexts: Channel degree or fanout is the number of channels at a node. The bisection of a graph is a partition of the nodes of the graph into two equal halves. Bisection width <ref> [Thom79] </ref> is the minimum number of wires crossing a bisection of the network graph, over all possible bisections; it has often been used as an estimate of wiring area. In [Dall90], k-ary n-cubes with constant bisection widths were compared.
Reference: [TuVe88] <author> S. Turner and A. Veidenbaum, </author> <title> ``Performance of a shared memory system for vector multiprocessors,'' </title> <booktitle> 1988 Int. Conf. on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988, </year> <pages> pp. 315-325. </pages>
Reference-contexts: Address traces Another approach attempts to emulate memory traffic during actual machine operation by using address traces from small kernels or parallel programs (see, for example, [DaPS87] or <ref> [TuVe88] </ref>). The use of address traces avoids many of the abstractions of mathematical models. Successive messages from the same processor are no longer independent, and neighboring processors may send messages that are dependent on each other.
Reference: [Tzen91] <author> N. Tzeng, </author> <title> ``An approach to the performance improvement of multistage interconnection networks with nonuniform traffic spots,'' </title> <booktitle> 1991 Int. Conf. on Parallel Processing, Aug. 1991, </booktitle> <volume> Vol. I, </volume> <pages> pp. 542-545. </pages>
Reference-contexts: Feedback has been proposed by Scott and Sohi [ScSo89] for avoiding tree saturation, but it does not improve the latency of hot-spot accesses and is not a substitute for combining. Similarly, the intelligent allocation of hardware switch buffers (for example, [DiKu89] and <ref> [Tzen91] </ref>) relieves congestion, but does not address the problem of high-latency hot-spot accesses. A different approach is taken in [Gupt89]: compiler analysis is used to reorder code so the processor can be doing useful work while waiting for a barrier to complete.
Reference: [VeSa88] <author> G.D. Vecchia and C. Sanges, </author> <title> ``A recursively scalable network VLSI implementation,'' </title> <booktitle> Future Generation Computer Systems 4 (1988), </booktitle> <pages> pp. 235-243. 169 </pages>
Reference-contexts: If m&lt;n, channels are replicated between pairs of clusters. The DBCube of [ChAB91] also has clusters of hypercubes, globally connected by a DeBruijn network. As in the HCN, channel replication is used if the number of nodes per cluster is greater than the cluster fanout. The WK-recursive topologies of <ref> [VeSa88] </ref> are based on complete interconnections. A processor has local channels to all other processors in the same cluster. Each processor also has a single global channel which is connected to a processor in a different cluster.
Reference: [WiMu88] <author> D.C. Winsor and T.N. Mudge, </author> <title> ``Analysis of bus hierarchies for multiprocessors,'' </title> <booktitle> 15th Ann. Symp. on Comp. Arch., </booktitle> <year> 1988, </year> <pages> pp. 100-107. </pages>
Reference-contexts: This was analyzed in [WuLi81]; a similar structure with non-binary fanouts was studied in <ref> [WiMu88] </ref>. Instead of SGC/EA, a variant of SGC has been proposed, in which a single interface processor (IP) directly accesses the global switch in each cluster. All other processors are connected to the interface processor via a local network. We call this SGC/IP.
Reference: [Wood89] <author> D. Wood, </author> <title> ``Parallel queues and pools, an evaluation,'' Ultracomputer Note #150, </title> <institution> Courant Institute, </institution> <address> New York University, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: We also implemented an alternative parallel queue algorithm similar to the ``hqueue'' described in <ref> [Wood89] </ref>. (For details see Appendix A.) This algorithm uses semaphores to protect queue locations, instead of Baker's ticket scheme. Insert is simpler, but there is extra overhead in delete. We compared the performance of inserts (Experiment 1) on the three networks. <p> next access */ c [myloc] &lt;- c [myloc]+1 mod qsize /* implemented as one instruction */ 21: wait for response, goto 22 22: qu &lt;- qu-1, goto 23 /* atomic read-modify-write */ 23: wait for response, end We also implemented an alternative queue insertion algorithm based on hqueue, described in <ref> [Wood89] </ref>. Hqueue has infinite length and is implemented as a circular array of linked lists. The circular array is protected by an array of semaphores, lock [] (initialized to 1).
Reference: [WuLi81] <author> S. Wu and M. Liu, </author> <title> ``A cluster structure as an interconnection network for large multiprocessor systems,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-30, No. 4, </volume> <month> Apr. </month> <year> 1981, </year> <pages> pp. 254-264. </pages>
Reference-contexts: Each cluster bus provides connection to a directory; non-local accesses go through the directory to the global mesh network. In addition, hierarchical interconnects have been an active area of research over the past 20 years. For example, there have been hierarchies based on shared buses <ref> [WuLi81] </ref>, crossbar switches [AgMa85], meshes [Carl85], hypercubes [DaEa90] [DaEa91] [Aboe91], and combinations of Omega networks and composite cube networks [Padm91a]. In Section 4.2 we attempt to develop a taxonomy of hierarchical systems that will provide the architectural context for our performance studies of hierarchical systems in Chapter 5. <p> This was analyzed in <ref> [WuLi81] </ref>; a similar structure with non-binary fanouts was studied in [WiMu88]. Instead of SGC/EA, a variant of SGC has been proposed, in which a single interface processor (IP) directly accesses the global switch in each cluster. All other processors are connected to the interface processor via a local network.
Reference: [YePL82] <author> P. Yew, D. Padua, and D. Lawrie, </author> <title> ``Stochastic properties of a multiple-layer single-stage shuffle-exchange network in a message switching environment,'' </title> <journal> Journal of Digital Systems, </journal> <volume> Vol. VI, No. 4, </volume> <month> Winter </month> <year> 1982, </year> <pages> pp. 387-410. </pages>
Reference-contexts: A unidirectional SSE has fewer stages than an Omega network. It is also attractive from the packaging perspective. In a large network, the cables between network stages are a major part of the system cost. The single stage of the SSE can avoid these problems. Special routing algorithms <ref> [YePL82] </ref> can be used to reduce average message distance. Combined packets can be evenly distributed through the network, and small wait buffers are sufficient for good performance. <p> If a match is found, unnecessary hops can be avoided. This is very similar to the routing on a hypercube and is called the fast-finishing algorithm <ref> [YePL82] </ref>. 29 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 3.2.1. Placement of combined packets In an SSE, a packet P1 traveling from source S to destination D may not go through the same switches when it returns from destination D to source S. <p> On each succeeding step, b=logB=3 bits of the address are changed (and the address is shifted) until the destination is reached. Fast-finishing algorithms <ref> [YePL82] </ref> exist for the SE, in which a message takes the minimum length path to reach its destination. Analogous algorithms can be constructed for the GSE. However, we will not consider fast-finishing algorithms for the GSE, because they complicate performance analysis.

References-found: 101

