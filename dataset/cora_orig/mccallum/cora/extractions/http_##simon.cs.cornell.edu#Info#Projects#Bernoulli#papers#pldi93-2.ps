URL: http://simon.cs.cornell.edu/Info/Projects/Bernoulli/papers/pldi93-2.ps
Refering-URL: 
Root-URL: 
Email: huff@cs.cornell.edu  
Phone: (607) 254-8830  
Title: Lifetime-Sensitive Modulo Scheduling  
Author: Richard A. Huff 
Address: Ithaca, NY 14853  
Affiliation: Department of Computer Science Cornell University  
Abstract: This paper shows how to software pipeline a loop for minimal register pressure without sacrificing the loop's minimum execution time. This novel bidirectional slack-scheduling method has been implemented in a FORTRAN compiler and tested on many scientific benchmarks. The empirical resultswhen measured against an absolute lower bound on execution time, and against a novel schedule-independent absolute lower bound on register pressureindicate near-optimal performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. R. Allen, K. Kennedy, C. Porterfield, and J. Warren. </author> <title> Conversion of control dependence to data dependence. </title> <booktitle> In Proceedings of the Tenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 177-189, </pages> <month> Jan. </month> <year> 1983. </year>
Reference-contexts: This optimization generalizes to if-conversion <ref> [1, 13] </ref>, which transforms unstructured acyclic code into branch-free predicated code. Since modulo scheduling is applicable only to loops with branch-free bodies, if-conversion allows more loops to be modulo scheduled. Finally, predicated execution greatly simplifies code generation after modulo scheduling a loop; see [19] for details.
Reference: [2] <author> G. R. Beck, D. W. L. Yen, and T. L. Anderson. </author> <title> The Cydra-5 mini-supercomputer: Architecture and implementation. </title> <journal> Journal of Supercomputing, </journal> <volume> 7(1/2), </volume> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: The scheduler's execution time is analyzed in Section 6. Performance measurements are shown in Section 7. Finally, Section 8 offers some comparisons with related work. 2 Target Machine The target machine is a hypothetical VLIW processor similar to Cydrome's Cydra 5 <ref> [20, 2] </ref>, including architectural support for overlapping loops without using code duplication [5]. Nevertheless, the scheduling techniques shown in this paper can be directly applied to conventional RISC machines [14, 23], albeit at the expense of code expansion [19]. 2.1 Functional Units Functional-unit latencies are given in Table 1.
Reference: [3] <author> D. G. Bradlee, S. J. Eggers, and R. R. Henry. </author> <title> Integrating register allocation and instruction scheduling for RISCs. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 122-131, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Although the first approach can avoid excessive pressure at some cost in execution time <ref> [8, 3] </ref>, neither approach schedules for minimal register pressure. In general, unidirectional strategies (top-down or bottom-up) unnecessarily stretch operand lifetimes; for example, by scheduling loads too early or stores too late. This paper presents a novel bidirectional strategy that simultaneously schedules some operations late and other operations early. <p> The slack scheduling framework handles these timing constraints more naturally than prior methods. Prior efforts at lifetime-sensitive scheduling have been in the context of straight-line code for conventional RISC processors <ref> [8, 3] </ref>. This work has advocated Integrated Prepass Scheduling (IPS) within a list-scheduling framework. IPS switches between a heuristic for avoiding pipeline interlock and a heuristic for reducing register pressure, based on how close the partial schedule is to a register pressure limit.
Reference: [4] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Proceedingsof the Sixteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 25-35, </pages> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: The heuristic's primary goal is to minimize each value's lifetime, in the hope that this will minimize the overall peak register pressure. 5.1 Computing MinLT The compiler eases the scheduler's task by putting the loop body into static single assignment (SSA) form <ref> [4] </ref>, thereby giving each value a unique defining operation and a precise set of flow dependencies. In that setting, a value's lifetime is merely the length of its longest flow dependence in the final schedule.
Reference: [5] <author> J. C. Dehnert, P. Y.-T. Hsu, and J. P. Bratt. </author> <title> Overlapped loop support in the Cydra 5. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 26-38, </pages> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: Performance measurements are shown in Section 7. Finally, Section 8 offers some comparisons with related work. 2 Target Machine The target machine is a hypothetical VLIW processor similar to Cydrome's Cydra 5 [20, 2], including architectural support for overlapping loops without using code duplication <ref> [5] </ref>. Nevertheless, the scheduling techniques shown in this paper can be directly applied to conventional RISC machines [14, 23], albeit at the expense of code expansion [19]. 2.1 Functional Units Functional-unit latencies are given in Table 1. The compiler assumes the responsibility for honoring these latencies, scheduling no-ops wherever necessary. <p> The compiler assumes the responsibility for honoring these latencies, scheduling no-ops wherever necessary. All functional units are fully pipelined; except for the divider, which is not pipelined at all. The brtop conditional branch conveniently combines several loop-management duties into one instruction <ref> [5] </ref>. The load latency of 13 cycles was chosen to represent the cost of bypassing a first-level cache and hitting a large off-chip second-level cachea reasonable choice for a compiler that does not perform any loop-tiling transformations [25].
Reference: [6] <author> J. C. Dehnert and R. A. Towle. </author> <title> Compiling for the Cydra 5. </title> <journal> Journal of Supercomputing, </journal> <volume> 7(1/2), </volume> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: turn, packing it full of operations before considering the next cycle [7, 24, 12]. * For software pipelining loops with recurrences, where a cycle-by-cycle approach is inherently inadequate, compilers have considered each operation in turn, always placing an operation as early as possible in the partial schedule constructed thus far <ref> [9, 6] </ref>. Although the first approach can avoid excessive pressure at some cost in execution time [8, 3], neither approach schedules for minimal register pressure. In general, unidirectional strategies (top-down or bottom-up) unnecessarily stretch operand lifetimes; for example, by scheduling loads too early or stores too late. <p> This paper presents a novel bidirectional strategy that simultaneously schedules some operations late and other operations early. The resulting slack-scheduling framework modulo schedules a loop for minimal register pressure 1 with an increased likelihood of achieving a minimal II. This framework has been integrated into Cydrome's FORTRAN77 compiler <ref> [6] </ref> and tested on all eligible DO loops in the Lawrence Livermore Loops, the SPEC89 FORTRAN benchmarks, and the Perfect Club codesa total of 1,525 loops. <p> For example, consider the loop shown in y (i)) that are used two iterations later. Dataflow analysis can detect that the values may be passed from iteration to iteration within registers rather than main memory. Performing this load/store elimination <ref> [15, 6] </ref> leaves the resulting registers live for more than II cycles, as will be illustrated shortly. The compiler must ensure that the successive outputs of an operation can be kept in distinct registers. <p> Even in the absence of recurrence circuits, the dynamic-priority scheme is an important refinement to the critical-path method [10]. Cydrome's scheduler has a similar backtracking operation-driven framework, with very different heuristics <ref> [6] </ref>. In particular, it does not employ a dynamic priority scheme; instead, it relies on a static priority that favors those operations whose initial slack is minimal. Thus the scheduler cannot detect when a recurrence circuit becomes fixed.
Reference: [7] <author> P. B. Gibbons and S. S. Muchnick. </author> <title> Efficient instruction scheduling for a pipelined architecture. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 11-16, </pages> <year> 1986. </year>
Reference-contexts: Consequently, most schedulers consider each cycle in turn, packing it full of operations before considering the next cycle <ref> [7, 24, 12] </ref>. * For software pipelining loops with recurrences, where a cycle-by-cycle approach is inherently inadequate, compilers have considered each operation in turn, always placing an operation as early as possible in the partial schedule constructed thus far [9, 6].
Reference: [8] <author> J. R. Goodman and W.-C. Hsu. </author> <title> Code scheduling and register allocation in large basic blocks. </title> <booktitle> In Proceedings of the 1988 International Conferenceon Supercomputing, </booktitle> <pages> pages 442-452, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Although the first approach can avoid excessive pressure at some cost in execution time <ref> [8, 3] </ref>, neither approach schedules for minimal register pressure. In general, unidirectional strategies (top-down or bottom-up) unnecessarily stretch operand lifetimes; for example, by scheduling loads too early or stores too late. This paper presents a novel bidirectional strategy that simultaneously schedules some operations late and other operations early. <p> The slack scheduling framework handles these timing constraints more naturally than prior methods. Prior efforts at lifetime-sensitive scheduling have been in the context of straight-line code for conventional RISC processors <ref> [8, 3] </ref>. This work has advocated Integrated Prepass Scheduling (IPS) within a list-scheduling framework. IPS switches between a heuristic for avoiding pipeline interlock and a heuristic for reducing register pressure, based on how close the partial schedule is to a register pressure limit.
Reference: [9] <author> M. S. Lam. </author> <title> A Systolic Array Optimizing Compiler. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: turn, packing it full of operations before considering the next cycle [7, 24, 12]. * For software pipelining loops with recurrences, where a cycle-by-cycle approach is inherently inadequate, compilers have considered each operation in turn, always placing an operation as early as possible in the partial schedule constructed thus far <ref> [9, 6] </ref>. Although the first approach can avoid excessive pressure at some cost in execution time [8, 3], neither approach schedules for minimal register pressure. In general, unidirectional strategies (top-down or bottom-up) unnecessarily stretch operand lifetimes; for example, by scheduling loads too early or stores too late. <p> Finally, predicated execution greatly simplifies code generation after modulo scheduling a loop; see [19] for details. To address the problem of modulo scheduling loops-with-branches on machines without predicated execution, two extensions to modulo scheduling have been developed; namely, hierarchical reduction <ref> [9] </ref>, and enhanced modulo scheduling [23]. <p> The compiler must ensure that the successive outputs of an operation can be kept in distinct registers. In the absence of hardware support, the loop may be unrolled and the duplicate register specifiers renamed appropriately <ref> [9] </ref>. However, this modulo variable expansion technique can result in a large amount of code expansion [18]. A rotating register file can solve this problem without duplicating code. Consider saving the series of values generated by an operation in its own infinite pushdown stack. <p> Thus the scheduler cannot detect when a recurrence circuit becomes fixed. To be safe, the scheduler places all operations that are on recurrence circuits, before placing any other operations. In order to dispense with backtracking altogether, the Warp compiler special-cases recurrence circuits within a list-scheduling framework <ref> [9] </ref>. In essence, the compiler fixes the relative timing of the operations on a recurrence circuit before scheduling the overall loop body. By thus reducing each recurrence circuit to a complex pseudo-operation, only acyclic dependencies remain, which are easily dealt with. <p> The empirical results in <ref> [9] </ref> and Section 7 support this intuition. The hardware timing constraints that arise when locally compacting microcode are similar to recurrence constraints. Prior work in this area refers to an operation's Estart and Lstart bounds as its absolute timing, while the MinDist relation encodes the operations' extended timings [24].
Reference: [10] <author> D. Landskov, S. Davidson, B. Shriver, and P. W. Mallet. </author> <title> Local microcode compaction techniques. </title> <journal> ACM Computing Surveys, </journal> <pages> pages 261-294, </pages> <month> Sept. </month> <year> 1980. </year>
Reference-contexts: introduction, consult [14]. Prior scheduling research has focussed on achieving minimal execution time, without regard for whether the heuristics unnecessarily inflate register pressure. * Ever since the studies of the late 1970's <ref> [10, 16] </ref>, most people have advocated using list scheduling, in either a top-down or bottom-up fashion. <p> The dynamic-priority scheme can detect this transition because the scheduler maintains precise Estart and Lstart bounds for all operations at all times. Even in the absence of recurrence circuits, the dynamic-priority scheme is an important refinement to the critical-path method <ref> [10] </ref>. Cydrome's scheduler has a similar backtracking operation-driven framework, with very different heuristics [6]. In particular, it does not employ a dynamic priority scheme; instead, it relies on a static priority that favors those operations whose initial slack is minimal.
Reference: [11] <author> E. L. Lawler. </author> <title> Combinatorial Optimization: Networks and Matroids. </title> <publisher> Saunders College Publishing, </publisher> <year> 1976. </year>
Reference-contexts: In any case, RecMII can be computed in O (V fi E fi log V ) time by indirectly finding a circuit with the minimum cost-to-time ratio, where a dependence arc is viewed as having a cost of latency, and a time of ! <ref> [11] </ref>. If R is the minimum cost-to-time ratio, then RecMII = dRe. The ResMII and RecMII lower bounds are defined using the ceiling function because it does not make sense to talk about a non-integral II.
Reference: [12] <author> P. G. Lowney, S. M. Freudenberger, T. J. Karzes, W. D. Licht-enstein, R. P. Nix, J. S. O'Donnell, and J. C. Ruttenberg. </author> <title> The Multiflow trace scheduling compiler. </title> <journal> Journal of Supercomputing, </journal> <volume> 7(1/2), </volume> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Consequently, most schedulers consider each cycle in turn, packing it full of operations before considering the next cycle <ref> [7, 24, 12] </ref>. * For software pipelining loops with recurrences, where a cycle-by-cycle approach is inherently inadequate, compilers have considered each operation in turn, always placing an operation as early as possible in the partial schedule constructed thus far [9, 6].
Reference: [13] <author> J. C. H. Park and M. S. Schlansker. </author> <title> On predicated execution. </title> <type> Technical Report HPL-91-58, </type> <institution> Hewlett-Packard Laboratories, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: This optimization generalizes to if-conversion <ref> [1, 13] </ref>, which transforms unstructured acyclic code into branch-free predicated code. Since modulo scheduling is applicable only to loops with branch-free bodies, if-conversion allows more loops to be modulo scheduled. Finally, predicated execution greatly simplifies code generation after modulo scheduling a loop; see [19] for details.
Reference: [14] <author> S. Ramakrishnan. </author> <title> Software pipelining in PA-RISC compilers. </title> <journal> Hewlett-Packard Journal, </journal> <pages> pages 39-45, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: introduction, consult <ref> [14] </ref>. Prior scheduling research has focussed on achieving minimal execution time, without regard for whether the heuristics unnecessarily inflate register pressure. * Ever since the studies of the late 1970's [10, 16], most people have advocated using list scheduling, in either a top-down or bottom-up fashion. <p> Nevertheless, the scheduling techniques shown in this paper can be directly applied to conventional RISC machines <ref> [14, 23] </ref>, albeit at the expense of code expansion [19]. 2.1 Functional Units Functional-unit latencies are given in Table 1. The compiler assumes the responsibility for honoring these latencies, scheduling no-ops wherever necessary. All functional units are fully pipelined; except for the divider, which is not pipelined at all.
Reference: [15] <author> B. R. Rau. </author> <title> Data flow and dependence analysis for instruction level parallelism. </title> <booktitle> In Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 236-250, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: For example, consider the loop shown in y (i)) that are used two iterations later. Dataflow analysis can detect that the values may be passed from iteration to iteration within registers rather than main memory. Performing this load/store elimination <ref> [15, 6] </ref> leaves the resulting registers live for more than II cycles, as will be illustrated shortly. The compiler must ensure that the successive outputs of an operation can be kept in distinct registers. <p> Old values can be read out of anywhere in the stack, and new values can be pushed on top, but a value cannot be modified once it has been pushed onto the stack; that is, the stack enforces a dynamic single assignment discipline <ref> [15] </ref>. Since each value pushed onto the stack has the same lifetime, only a constant portion of the stack is live at any given moment.
Reference: [16] <author> B. R. Rau and J. A. Fisher. </author> <title> Instruction-level parallel processing: History, overview and perspective. </title> <journal> Journal of Supercomputing, </journal> <volume> 7(1/2), </volume> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: introduction, consult [14]. Prior scheduling research has focussed on achieving minimal execution time, without regard for whether the heuristics unnecessarily inflate register pressure. * Ever since the studies of the late 1970's <ref> [10, 16] </ref>, most people have advocated using list scheduling, in either a top-down or bottom-up fashion.
Reference: [17] <author> B. R. Rau and C. D. Glaeser. </author> <title> Some scheduling techniques and an easily schedulable horizontal architecture for high performance scientific computing. </title> <booktitle> In Proceedings of the 14th Annual Microprogramming Workshop, </booktitle> <pages> pages 183-197, </pages> <month> Oct. </month> <year> 1981. </year>
Reference: [18] <author> B. R. Rau, M. Lee, P. Tirumalai, and M. S. Schlansker. </author> <title> Register allocation for software pipelined loops. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 283-299, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: In the absence of hardware support, the loop may be unrolled and the duplicate register specifiers renamed appropriately [9]. However, this modulo variable expansion technique can result in a large amount of code expansion <ref> [18] </ref>. A rotating register file can solve this problem without duplicating code. Consider saving the series of values generated by an operation in its own infinite pushdown stack. <p> In any case, the LiveVector's maximum, MaxLive, is the desired lower bound. Allocating registers for a modulo-scheduled loop is beyond the scope of this paper. For an extensive discussion of the problem, including heuristic solutions and empirical results, consult <ref> [18] </ref>. One of the most remarkable results reported in that paper is the ability of their allocation strategies to almost always achieve the MaxLive lower bound on a schedule's register pressure 4 . Due to that result, this paper approximates a schedule's register pressure with its MaxLive lower bound.
Reference: [19] <author> B. R. Rau, M. S. Schlansker, and P. Tirumalai. </author> <title> Code generation schemas for modulo scheduled loops. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 158-169, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Nevertheless, the scheduling techniques shown in this paper can be directly applied to conventional RISC machines [14, 23], albeit at the expense of code expansion <ref> [19] </ref>. 2.1 Functional Units Functional-unit latencies are given in Table 1. The compiler assumes the responsibility for honoring these latencies, scheduling no-ops wherever necessary. All functional units are fully pipelined; except for the divider, which is not pipelined at all. <p> This optimization generalizes to if-conversion [1, 13], which transforms unstructured acyclic code into branch-free predicated code. Since modulo scheduling is applicable only to loops with branch-free bodies, if-conversion allows more loops to be modulo scheduled. Finally, predicated execution greatly simplifies code generation after modulo scheduling a loop; see <ref> [19] </ref> for details. To address the problem of modulo scheduling loops-with-branches on machines without predicated execution, two extensions to modulo scheduling have been developed; namely, hierarchical reduction [9], and enhanced modulo scheduling [23].
Reference: [20] <author> B. R. Rau, D. W. L. Yen, W. Yen, and R. A. Towle. </author> <title> The Cydra 5 departmental supercomputer: Design philosophies, decisions, and trade-offs. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 12-35, </pages> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: The scheduler's execution time is analyzed in Section 6. Performance measurements are shown in Section 7. Finally, Section 8 offers some comparisons with related work. 2 Target Machine The target machine is a hypothetical VLIW processor similar to Cydrome's Cydra 5 <ref> [20, 2] </ref>, including architectural support for overlapping loops without using code duplication [5]. Nevertheless, the scheduling techniques shown in this paper can be directly applied to conventional RISC machines [14, 23], albeit at the expense of code expansion [19]. 2.1 Functional Units Functional-unit latencies are given in Table 1.
Reference: [21] <author> J. C. Tiernan. </author> <title> An efficient search algorithm to find the elementary circuits of a graph. </title> <journal> Communications of the ACM, </journal> <pages> pages 722-726, </pages> <month> Dec. </month> <year> 1970. </year>
Reference-contexts: Nevertheless, even conservative lower bounds on ! can lessen scheduling constraints significantly. Although a graph can contain exponentially many elementary recurrence circuits, most loop bodies have very few. So the compiler computes RecMII by simply scanning each circuit <ref> [21] </ref>. In any case, RecMII can be computed in O (V fi E fi log V ) time by indirectly finding a circuit with the minimum cost-to-time ratio, where a dependence arc is viewed as having a cost of latency, and a time of ! [11].
Reference: [22] <author> P. Tirumalai, M. Lee, and M. S. Schlansker. </author> <title> Parallelization of loops with exits on pipelined architectures. </title> <booktitle> In IEEE Proceedings of Supercomputing '90, </booktitle> <pages> pages 200-212, </pages> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: The compiler is capable of modulo scheduling arbitrary DO loops with unstructured bodies, so long as the loop bodies are acyclic, have no assigned or computed gotos, and make no procedure calls. Even loops with early exits can be modulo scheduled <ref> [22] </ref>, although that experimental feature was not employed for these experiments. Nevertheless, the compiler does not attempt to modulo schedule loops with less than 5 iterations, or more than 30 basic blocks before if-conversion, or which have ResMII &gt; 500, as the benefits would be minimal.
Reference: [23] <author> N. J. Warter, J. W. Bockhaus, G. E. Haab, and K. Subrama-nian. </author> <title> Enhanced modulo scheduling for loops with conditional branches. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 170-179, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Nevertheless, the scheduling techniques shown in this paper can be directly applied to conventional RISC machines <ref> [14, 23] </ref>, albeit at the expense of code expansion [19]. 2.1 Functional Units Functional-unit latencies are given in Table 1. The compiler assumes the responsibility for honoring these latencies, scheduling no-ops wherever necessary. All functional units are fully pipelined; except for the divider, which is not pipelined at all. <p> Finally, predicated execution greatly simplifies code generation after modulo scheduling a loop; see [19] for details. To address the problem of modulo scheduling loops-with-branches on machines without predicated execution, two extensions to modulo scheduling have been developed; namely, hierarchical reduction [9], and enhanced modulo scheduling <ref> [23] </ref>.
Reference: [24] <author> P. Wijaya and V. H. Allan. </author> <title> Incremental foresighted local compaction. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 163-171, </pages> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: Consequently, most schedulers consider each cycle in turn, packing it full of operations before considering the next cycle <ref> [7, 24, 12] </ref>. * For software pipelining loops with recurrences, where a cycle-by-cycle approach is inherently inadequate, compilers have considered each operation in turn, always placing an operation as early as possible in the partial schedule constructed thus far [9, 6]. <p> The hardware timing constraints that arise when locally compacting microcode are similar to recurrence constraints. Prior work in this area refers to an operation's Estart and Lstart bounds as its absolute timing, while the MinDist relation encodes the operations' extended timings <ref> [24] </ref>. The slack scheduling framework handles these timing constraints more naturally than prior methods. Prior efforts at lifetime-sensitive scheduling have been in the context of straight-line code for conventional RISC processors [8, 3]. This work has advocated Integrated Prepass Scheduling (IPS) within a list-scheduling framework.
Reference: [25] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The brtop conditional branch conveniently combines several loop-management duties into one instruction [5]. The load latency of 13 cycles was chosen to represent the cost of bypassing a first-level cache and hitting a large off-chip second-level cachea reasonable choice for a compiler that does not perform any loop-tiling transformations <ref> [25] </ref>. A memory latency register specifies the load latency that the compiler has chosen to schedule for. The hardware honors this load latency by freezing instruction issue whenever a load does not complete in time. 2.2 Predicated Execution Every operation has a 1-bit predicate input.
References-found: 25

