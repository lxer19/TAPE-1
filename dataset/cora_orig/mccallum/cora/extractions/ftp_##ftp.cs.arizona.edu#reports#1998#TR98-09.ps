URL: ftp://ftp.cs.arizona.edu/reports/1998/TR98-09.ps
Refering-URL: http://www.cs.arizona.edu/research/reports.html
Root-URL: http://www.cs.arizona.edu
Email: fjag,bkmoon,rtsg@cs.arizona.edu brucelee@us.ibm.com jmrodrigue@west.raytheon.com  
Title: Parallel Algorithms for Computing Temporal Aggregates  
Author: Jose Alvin G. Gendrano Bruce C. Huang Jim M. Rodrigue Bongki Moon Richard T. Snodgrass 
Note: September,  This work was sponsored in part by National Science Foundation grants CDA-9500991 and IRI-9632569.  
Date: 98-9  1998  
Address: 9000 S. Rita Road 1151 East Hermans Road Tucson, AZ 85721 Tucson, AZ 85744 Tucson, AZ 85706  Tucson, AZ 85721  
Affiliation: Dept. of Computer Science IBM Storage Systems Division Raytheon Missile Systems Co. University of Arizona  Department of Computer Science The University of Arizona  
Pubnum: Technical Report  
Abstract: The ability to model the temporal dimension is essential to many applications. Furthermore, the rate of increase in database size and response time requirements has out-paced advancements in processor and mass storage technology, leading to the need for parallel temporal database management systems. In this paper, we introduce a variety of parallel temporal aggregation algorithms for a shared-nothing architecture based on the sequential Aggregation Tree algorithm. Via an empirical study, we found that the number of processing nodes, the partitioning of the data, the placement of results, and the degree of data reduction effected by the aggregation impacted the performance of the algorithms in different ways. We designed the Time Division Merge algorithm to produce distributed result placement, as differentiated from the centralized result strategies used by the other proposed algorithms. For centralized results and high data reduction, we found that the Pairwise Merge algorithm was preferred regardless of the number of processing nodes, but for low data reduction it was only preferred up to 32 nodes, while a variant of Time Division Merge was best for larger configurations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bitton, H. Boral, D. DeWitt, and W. K. Wilkinson. </author> <title> Parallel algorithms for the execution of relational database operations. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(3) </volume> <pages> 324-353, </pages> <month> September </month> <year> 1983. </year>
Reference-contexts: Early research on developing parallel algorithms focused on the framework of general-purpose multiprocessor machines. Bitton et al. proposed two parallel algorithms for processing (conventional) aggregate functions <ref> [1] </ref>. The Subqueries with a Parallel Merge algorithm computes partial aggregates on each partition and combines the partial results in a parallel merge stage to obtain a final result. Another algorithm, Project by list, exploits the ability of the parallel system architecture to broadcast tuples to multiple processors.
Reference: [2] <institution> Ohio Supercomputer Center. </institution> <note> LAM/MPI parallel computing. http://www.osc.edu/lam.html, 1998. </note>
Reference-contexts: Each machine was booted with version 2.0.30 of the Linux kernel. For message passing between the Pentium nodes, we used the LAM implementation of the MPI communication standard <ref> [2] </ref>. With the LAM implementation, we observed an average communication latency of 790 microseconds and an average transfer rate of about 5 Mbytes/second. 4.2 Experimental Parameters In order to help precisely define the parameters for each set of tests, we describe an experiment classification scheme. <p> We then expected TDM+C to outperform PM as more nodes are added, but we were suprised to realize that PM was still performing better than TDM+C up to perhaps 50 nodes. To find out what was going on behind the scenes, we used the LAM XMPI package <ref> [2] </ref> to visually track the progression of messages within the various TDM+C and PM runs.
Reference: [3] <author> Surajit Chaudhuri and Umeshwar Dayal. </author> <title> An overview of data warehousing and OLAP technology. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 26(1), </volume> <month> March </month> <year> 1997. </year>
Reference-contexts: The appeal of parallel processing technology becomes even stronger as the size of many data-intensive applications become large, as can be seen in OLAP systems and data warehousing environments <ref> [3, 4] </ref>. Although there exist efficient algorithms for computing temporal aggregates [9, 10, 12], to the best of our knowledge, no parallel algorithm has been developed for computing temporal aggregates. In this paper, we present several new parallel algorithms for the computation of temporal aggregates on a shared-nothing architecture [11].
Reference: [4] <author> Anindya Datta, Bongki Moon, and Helen Thomas. </author> <title> A case for parallelism in data warehousing and OLAP. </title> <booktitle> In International Workshop on Data Warehouse Design and OLAP Technology (DWDOT98), </booktitle> <address> Vienna, Austria, </address> <month> August </month> <year> 1998. </year>
Reference-contexts: The appeal of parallel processing technology becomes even stronger as the size of many data-intensive applications become large, as can be seen in OLAP systems and data warehousing environments <ref> [3, 4] </ref>. Although there exist efficient algorithms for computing temporal aggregates [9, 10, 12], to the best of our knowledge, no parallel algorithm has been developed for computing temporal aggregates. In this paper, we present several new parallel algorithms for the computation of temporal aggregates on a shared-nothing architecture [11].
Reference: [5] <author> David DeWitt and Jim Gray. </author> <title> Parallel database systems: The future of high performance database systems. </title> <journal> Communications of the ACM, </journal> <volume> 35(6) </volume> <pages> 85-98, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: To see the effects of data partitioning on the performance of the temporal algorithms, the synthetic tables were partitioned horizontally either by SSN or by StartDate. The SSN and StartDate partitioning schemes were attempts to model range partitioning based on temporal and non-temporal attributes in a parallel database system <ref> [5] </ref>. The tuple size was fixed at 41 bytes/tuple. This parameter is given, for figuring out the actual size of the dataset, in bytes.
Reference: [6] <author> David J. DeWitt, Shahram Ghandeharizadeh, Donovan A. Schneider, Allan Bricker, Hui-I Hsiao, and Rick Rasmussen. </author> <title> The Gamma database machine project. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 44-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Another algorithm, Project by list, exploits the ability of the parallel system architecture to broadcast tuples to multiple processors. The more recent Gamma database machine project <ref> [6] </ref> implemented similar scalar aggregates and aggregate functions on a shared-nothing architecture. The parallel temporal aggregation algorithms proposed in this paper are based on the (sequential) Aggregation Tree algorithm (SEQ) designed by Kline [9].
Reference: [7] <author> R. Epstein. </author> <title> Techniques for processing of aggregates in relational database systems. </title> <type> Technical Report UCB/ERL M7918, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <month> February </month> <year> 1979. </year>
Reference-contexts: Section 4 presents empirical results obtained from the experiments performed on a shared-nothing Pentium cluster. Finally, section 5 concludes the paper and gives an outlook to future work. 2 Background and Related Work There are two types of non-temporal aggregates in relational database systems, scalar aggregates and aggregate functions <ref> [7] </ref>. Scalar aggregates are operations such as average, min, max, count, and sum that produce a single value over an entire relation, while aggregate functions first partition a relation based on some attribute value and then compute scalar aggregates independently on the individual partitions. <p> Simple algorithms for evaluating scalar aggregates and aggregate functions were discussed by Epstein <ref> [7] </ref>. A different approach employing program transformation methods to systematically generate efficient iterative programs for aggregate queries has also been suggested [8]. Tumas extended Epstein's algorithms to apply to temporal aggregates [12]; these were further extended by Kline [9].
Reference: [8] <author> J.C. Freytag and N. Goodman. </author> <title> Translating aggregate queries into iterative programs. </title> <booktitle> In Proceedings of the 12th VLDB Conference, </booktitle> <pages> pages 138-146, </pages> <address> Kyoto, Japan, </address> <year> 1986. </year>
Reference-contexts: Simple algorithms for evaluating scalar aggregates and aggregate functions were discussed by Epstein [7]. A different approach employing program transformation methods to systematically generate efficient iterative programs for aggregate queries has also been suggested <ref> [8] </ref>. Tumas extended Epstein's algorithms to apply to temporal aggregates [12]; these were further extended by Kline [9]. While the resulting algorithms were quite effective in a uniprocessor environment, all suffer from poor scale-up performance, which identifies the need to develop parallel algorithms to compute temporal aggregates.
Reference: [9] <author> Nick Kline and Richard T. Snodgrass. </author> <title> Computing temporal aggregates. </title> <booktitle> In the 11th Inter. Conference on Data Engineering, </booktitle> <pages> pages 222-231, </pages> <address> Taipei, Taiwan, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: The appeal of parallel processing technology becomes even stronger as the size of many data-intensive applications become large, as can be seen in OLAP systems and data warehousing environments [3, 4]. Although there exist efficient algorithms for computing temporal aggregates <ref> [9, 10, 12] </ref>, to the best of our knowledge, no parallel algorithm has been developed for computing temporal aggregates. In this paper, we present several new parallel algorithms for the computation of temporal aggregates on a shared-nothing architecture [11]. <p> In this paper, we present several new parallel algorithms for the computation of temporal aggregates on a shared-nothing architecture [11]. Specifically, we focus on the Aggregation Tree algorithm <ref> [9] </ref> and propose several approaches to parallelize that algorithm. The performance of the parallel algorithms relative to various data set and operational characteristics is our main interest. The rest of this paper is organized as follows. <p> A different approach employing program transformation methods to systematically generate efficient iterative programs for aggregate queries has also been suggested [8]. Tumas extended Epstein's algorithms to apply to temporal aggregates [12]; these were further extended by Kline <ref> [9] </ref>. While the resulting algorithms were quite effective in a uniprocessor environment, all suffer from poor scale-up performance, which identifies the need to develop parallel algorithms to compute temporal aggregates. Early research on developing parallel algorithms focused on the framework of general-purpose multiprocessor machines. <p> The more recent Gamma database machine project [6] implemented similar scalar aggregates and aggregate functions on a shared-nothing architecture. The parallel temporal aggregation algorithms proposed in this paper are based on the (sequential) Aggregation Tree algorithm (SEQ) designed by Kline <ref> [9] </ref>. The aggregation tree is a binary tree that tracks the number of tuples whose timestamp periods contain an indicated time span. Each node of the tree contains a start time, an end time, and a count. <p> Each node of the tree contains a start time, an end time, and a count. When an aggregation tree is initialized, it begins with a single node containing &lt; 0; 1; 0 &gt; (see the initial tree in Figure 1). In the following example <ref> [9] </ref>, there are 4 tuples to be inserted into an empty aggregation tree (see Table 1 (a)). The start time value 18 of the first entry to be inserted splits the initial tree, resulting in the updated aggregation tree shown in Figure 1. <p> Here we are concerned only with determining a division of the timeline into p contiguous periods, each with approximately the same number of leaves. There are three main differences between our Modified Aggregation Tree algorithm used in this portion TDM+C and the original Aggregation Tree <ref> [9] </ref>, used in step 2 of Figure 5. First, the "count" field of this aggregation tree node is incremented by the count value of the local partition being inserted, instead of just by 1. Second, a parent node cannot have a count value greater than 0. <p> The SM coordinator uses a merge-sort variant in compiling and constructing the final results. 5 In SAT, all the periods are sent to the coordinator which builds a single, but large, aggregation tree. 6 The aggregation tree algorithm performs at its worst case when the dataset is sorted by time <ref> [9] </ref>. 14 With 100% reduction, PM and TDM+C catch up to TDM. Aside from constructing smaller aggregation trees, a high degree of data reduction decreases the number of aggregation tree leaves exchanged between nodes.
Reference: [10] <author> R. T. Snodgrass, S. Gomez, and E. McKenzie. </author> <title> Aggregates in the temporal query language TQuel. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5 </volume> <pages> 826-842, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: The appeal of parallel processing technology becomes even stronger as the size of many data-intensive applications become large, as can be seen in OLAP systems and data warehousing environments [3, 4]. Although there exist efficient algorithms for computing temporal aggregates <ref> [9, 10, 12] </ref>, to the best of our knowledge, no parallel algorithm has been developed for computing temporal aggregates. In this paper, we present several new parallel algorithms for the computation of temporal aggregates on a shared-nothing architecture [11].
Reference: [11] <author> Michael Stonebraker. </author> <title> The case for shared nothing. </title> <journal> A Quarterly bulletin of the IEEE Computer Society Technical Committee on Database Engineering, </journal> <volume> 9(1) </volume> <pages> 4-9, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: Although there exist efficient algorithms for computing temporal aggregates [9, 10, 12], to the best of our knowledge, no parallel algorithm has been developed for computing temporal aggregates. In this paper, we present several new parallel algorithms for the computation of temporal aggregates on a shared-nothing architecture <ref> [11] </ref>. Specifically, we focus on the Aggregation Tree algorithm [9] and propose several approaches to parallelize that algorithm. The performance of the parallel algorithms relative to various data set and operational characteristics is our main interest. The rest of this paper is organized as follows.
Reference: [12] <author> P. A. Tuma. </author> <title> Implementing historical aggregates in TempIS, 1992. </title> <type> Master's Thesis. </type>
Reference-contexts: The appeal of parallel processing technology becomes even stronger as the size of many data-intensive applications become large, as can be seen in OLAP systems and data warehousing environments [3, 4]. Although there exist efficient algorithms for computing temporal aggregates <ref> [9, 10, 12] </ref>, to the best of our knowledge, no parallel algorithm has been developed for computing temporal aggregates. In this paper, we present several new parallel algorithms for the computation of temporal aggregates on a shared-nothing architecture [11]. <p> Simple algorithms for evaluating scalar aggregates and aggregate functions were discussed by Epstein [7]. A different approach employing program transformation methods to systematically generate efficient iterative programs for aggregate queries has also been suggested [8]. Tumas extended Epstein's algorithms to apply to temporal aggregates <ref> [12] </ref>; these were further extended by Kline [9]. While the resulting algorithms were quite effective in a uniprocessor environment, all suffer from poor scale-up performance, which identifies the need to develop parallel algorithms to compute temporal aggregates.
Reference: [13] <author> Christopher B. Walton, Alfred G. Dale, and Roy M. Jenevein. </author> <title> A taxonomy and performance model of data skew effects in parallel joins. </title> <booktitle> In Proceedings of the 17th VLDB Conference, </booktitle> <pages> pages 537-548, </pages> <address> Barcelona, Spain, </address> <month> September </month> <year> 1991. </year> <month> 19 </month>
Reference-contexts: Impact of skew. We expect PM to outperform TDM+C in queries with heavy tuple placement skew and/or selection skew <ref> [13] </ref>. Tuple placement skew occurs when the number of tuples are not evenly distributed physically amongst all the worker nodes before a query is initiated. Selection skew happens when some nodes return more candidate tuples than other nodes.
References-found: 13

