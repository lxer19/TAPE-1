URL: http://www.cs.berkeley.edu/~alanm/CP/druschel.ieeenet.93.ps
Refering-URL: http://www.cs.berkeley.edu/~alanm/CP/bib.html
Root-URL: 
Title: Network Subsystem Design: A Case for an Integrated Data Path  
Author: Peter Druschel Mark B. Abbott Michael A. Pagels Larry L. Peterson 
Address: Tucson, AZ 85721  
Affiliation: Department of Computer Science University of Arizona  
Abstract: This paper argues that the CPU/memory data path is a potential throughput bottleneck in workstations connected to high-speed networks, and considers the implications for the design of the I/O subsystem.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. B. Abbott and L. L. Peterson. </author> <title> Increasing network throughput by integrating protocol layers. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <note> 1993. To appear. </note>
Reference-contexts: These repeated transfers across the CPU/memory data path frequently dominate the time required to process a message, and thus the bandwidth. Such data transfers can be minimized using a technique called Integrated Layer Processing (ILP) <ref> [5, 1] </ref>. ILP is a technique for implementing communication protocols that avoids accessing memory for data manipulations. The data manipulation steps from different protocols are combined into a pipeline. <p> In this way, a combined series of data manipulations only transfers data from memory to the CPU and back once, instead of potentially transferring the data once per distinct layer. A detailed performance study demonstrates that integration can have a significant impact on performance <ref> [1] </ref>.
Reference: [2] <author> M. Accetta, R. Baron, W. Bolosky, D. Golub, R. Rashid, A. Tevanian, and M. Young. </author> <title> Mach: A new kernel foundation for Unix development. </title> <booktitle> In Proceedings of the USENIX Summer '86 Conference, </booktitle> <month> July </month> <year> 1986. </year>
Reference-contexts: The Network Systems Research Group at the University of Arizona is studying issues related to OS support for high-speed networking. Our work is experimental, and is being done in the context of the x-kernel [10] integrated into the Mach operating system <ref> [2] </ref>. The resulting system runs on DecStation 5000/200 and HP 9000/720 workstations connected by ATM and FDDI networks. In way of overview, Mach provides a microkernel-based OS framework, and the x-kernel serves as the network subsystem.
Reference: [3] <author> M. L. Bailey, M. A. Pagels, and L. L. Peterson. </author> <title> The x-chip: An experiment in hardware multiplexing. </title> <booktitle> In IEEE Workshop on the Architecture and Implementation of High Performance Communication Subsystems, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: More elaborate adapters can be programmed by the host CPU to automatically recognize network packets by matching their headers, and place them into appropriate memory locations using DMA <ref> [11, 3] </ref>. 4.2 Cross-Domain Transfers Protection necessitates the transfer of data between protection domains (address spaces). In the simplest case, an I/O data stream is handled by a single application process running on top of a conventional monolithic kernel. In this case, each data unit must cross the user/kernel boundary. <p> An exception are computations whose performance is bound by the characteristics of the device, e.g., network link speed. Performing such computations in the adapter seems appropriate, since they cannot take advantage of increased processing speed <ref> [3] </ref>. With hardware streaming, no specific techniques are required for data transfer in the I/O subsystem.
Reference: [4] <author> D. R. Cheriton. </author> <title> The V distributed system. </title> <journal> Commun. ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> Mar. </month> <year> 1988. </year>
Reference-contexts: Software data copying as a means of transferring data across domain boundaries exacerbates the memory bottleneck problem. A number of techniques exist that rely on the virtual memory system to provide copy-free cross-domain data transfer. Virtual page remapping <ref> [4, 14] </ref> unmaps the pages containing data units from the sending domain and maps them into the receiving domain.
Reference: [5] <author> D. D. Clark and D. L. Tennenhouse. </author> <title> Architectural Considerations for a New Generation of Protocols. </title> <booktitle> In SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 200-208, </pages> <address> Philadelphia, PA, </address> <month> Sept. </month> <year> 1990. </year> <note> ACM. </note>
Reference-contexts: These repeated transfers across the CPU/memory data path frequently dominate the time required to process a message, and thus the bandwidth. Such data transfers can be minimized using a technique called Integrated Layer Processing (ILP) <ref> [5, 1] </ref>. ILP is a technique for implementing communication protocols that avoids accessing memory for data manipulations. The data manipulation steps from different protocols are combined into a pipeline.
Reference: [6] <author> R. Comerford and G. F. Watson. </author> <title> Memory catches up. </title> <journal> IEEE Spectrum, </journal> <volume> 29(10) </volume> <pages> 34-57, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Another approach is to reduce transfer latencies, but DRAM access times are considered to be near their technological limit. Several recently announced components integrate some form of a cache with a dynamic RAM to reduce the average access latency <ref> [6] </ref>. These integrated second level caches use large cache lines and are connected to the DRAM using wide data paths. As for any cache, the hit rate of these components depends on locality of reference. As we shall discuss in Section 3, accesses to I/O data exhibit poor locality.
Reference: [7] <author> P. Druschel and L. L. Peterson. </author> <title> High-performance cross-domain data transfer. </title> <type> Technical Report TR 93-5, </type> <institution> Department of Computer Science, University of Arizona, Tucson, Ariz., </institution> <month> Mar. </month> <year> 1993. </year> <month> 20 </month>
Reference-contexts: Measurements we performed on a DecStation 5000/200 suggest that page remapping is not fast enough to sustain the bandwidth of a high-speed network adapter <ref> [7] </ref>. Another complication arises from the fact that both techniques work at the granularity of the VM page size. A mismatch between data unit size and VM page size implies that a portions of the last page overlapped by the data unit will remain unused. <p> However, its use may compromise protection and security between the sharing protection domains. Since sharing is statica 10 particular page is always accessible in the same set of domainsa priori knowledge of all the recipients of a data unit is required. A hybrid technique, called dynamic page sharing (fbufs) <ref> [7] </ref>, combines page remapping with shared virtual memory, and exploits locality in network traffic to overcome some of the shortcomings of either technique. 4.3 Data Manipulations Data manipulations are computations that inspect and possibly modify every word of data in a network packet.
Reference: [8] <author> R. Fitzgerald and R. F. Rashid. </author> <title> The integration of virtual memory management and interprocess communication in Accent. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(2) </volume> <pages> 147-177, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: A number of techniques exist that rely on the virtual memory system to provide copy-free cross-domain data transfer. Virtual page remapping [4, 14] unmaps the pages containing data units from the sending domain and maps them into the receiving domain. Virtual copying (copy on write) <ref> [8] </ref> shares the transferred pages among the sending and receiving domain, and delays copying until one of the sharing domains attempts to write the shared data unit. Shared virtual memory [13] employs buffers that are statically shared among two or more domains to avoid data transfers.
Reference: [9] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Palo Alto, California, </address> <year> 1990. </year>
Reference-contexts: Hennessy and Patterson report, for example, that processor performance has improved at a rate of 50-100% per year since 1985, while memory performance has improved at a rate of only 7% per year <ref> [9] </ref>. Moreover, we expect this trend to continue because cost considerations will prevent the use of dramatically faster main memory and interconnect technology in this class of machine.
Reference: [10] <author> N. C. Hutchinson and L. L. Peterson. </author> <title> The x-Kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: The Network Systems Research Group at the University of Arizona is studying issues related to OS support for high-speed networking. Our work is experimental, and is being done in the context of the x-kernel <ref> [10] </ref> integrated into the Mach operating system [2]. The resulting system runs on DecStation 5000/200 and HP 9000/720 workstations connected by ATM and FDDI networks. In way of overview, Mach provides a microkernel-based OS framework, and the x-kernel serves as the network subsystem.
Reference: [11] <author> H. Kanakia and D. R. Cheriton. </author> <title> The VMP Network Adapter Board (NAB): High-Performance Network Communication for Multiprocessors. </title> <booktitle> In SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 175-187, </pages> <address> Stanford, CA, </address> <month> Aug. </month> <year> 1988. </year> <note> ACM. </note>
Reference-contexts: More elaborate adapters can be programmed by the host CPU to automatically recognize network packets by matching their headers, and place them into appropriate memory locations using DMA <ref> [11, 3] </ref>. 4.2 Cross-Domain Transfers Protection necessitates the transfer of data between protection domains (address spaces). In the simplest case, an I/O data stream is handled by a single application process running on top of a conventional monolithic kernel. In this case, each data unit must cross the user/kernel boundary.
Reference: [12] <author> M. A. Pagels, P. Druschel, and L. L. Peterson. </author> <title> Cache and TLB effectiveness in the processing of network data. </title> <type> Technical Report TR 93-4, </type> <institution> Department of Computer Science, University of Arizona, Tucson, Ariz., </institution> <year> 1993. </year>
Reference-contexts: With checksumming enabled, cache residency increases as there are fewer opportunities for a context switch to occur which flushes the cached packet data loaded by the checksum code. Details of the modifications made to the Mach Unix Server to collect these data can be found in <ref> [12] </ref>. Experimental data was collected in two settings. In the first, the receiving processor was lightly loaded by running only normal system programs and the experimental receiving program.
Reference: [13] <author> M. D. Schroeder and M. Burrows. </author> <title> Performance of firefly RPC. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 1-17, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: Virtual copying (copy on write) [8] shares the transferred pages among the sending and receiving domain, and delays copying until one of the sharing domains attempts to write the shared data unit. Shared virtual memory <ref> [13] </ref> employs buffers that are statically shared among two or more domains to avoid data transfers. Virtual page remapping has move rather than copy semantics, which limits its utility to situations where the sender needs no further access to the transferred data.
Reference: [14] <author> S.-Y. Tzou and D. P. Anderson. </author> <title> The performance of message-passing using restricted virtual memory remapping. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 21 </volume> <pages> 251-267, </pages> <month> Mar. </month> <year> 1991. </year> <month> 21 </month>
Reference-contexts: Software data copying as a means of transferring data across domain boundaries exacerbates the memory bottleneck problem. A number of techniques exist that rely on the virtual memory system to provide copy-free cross-domain data transfer. Virtual page remapping <ref> [4, 14] </ref> unmaps the pages containing data units from the sending domain and maps them into the receiving domain.
References-found: 14

