URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P647.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts97.htm
Root-URL: http://www.mcs.anl.gov
Email: fdisz,olson,stevensg@mcs.anl.gov  
Title: Performance Model of the Argonne Voyager Multimedia Server  
Author: Terrence Disz, Robert Olson, and Rick Stevens 
Address: Argonne, IL 60439  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Abstract: The Argonne Voyager Multimedia Server is being developed in the Futures Lab of the Mathematics and Computer Science Division at Argonne National Laboratory. As a network-based service for recording and playing multimedia streams, it is important that the Voyager system be capable of sustaining certain minimal levels of performance in order for it to be a viable system. In this article, we examine the performance characteristics of the server. As we examine the architecture of the system, we try to determine where bottlenecks lie, show actual vs potential performance, and recommend areas for improvement through custom architectures and system tuning. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Terrence L. Disz, Remy Evard, Mark W. Hender-son, William Nickless, Robert Olson, Michael E. Papka, and Rick Stevens, </author> <title> "Designing the future of collaborative science: Argonne's futures laboratory," </title> <journal> IEEE Parallel and Distributed Technology Systems and Applications, </journal> <volume> vol. 3, no. 2, </volume> <pages> pp. 14-21, </pages> <month> Summer </month> <year> 1995. </year>
Reference-contexts: 1 Introduction The Argonne Computing and Communications Infrastructure Futures Laboratory (Futures Lab) <ref> [1] </ref> was created to explore, develop, and prototype next-generation computing and communications infrastructure systems. An important goal of the Futures Lab project is to understand how to incorporate advanced display and media server systems into scientific computing environments.
Reference: [2] <author> D. McDysan and D. Spohn, </author> <title> ATM: Theory and Application, </title> <publisher> McGraw Hill, </publisher> <year> 1995. </year>
Reference-contexts: Most universities and research institutions have at least T1 capability, and we are seeing increased use MM Clientsfl (encode/decode)fl Networkfl Web Server Interfacefl Data Filters/Schedulingfl Reader/Writerfl Multimedia FIlefl Systemfl MM Serverfl of the Internet Protocol over high-speed ATM (Asynchronous Transfer Mode) networks <ref> [2] </ref> [3].
Reference: [3] <author> M. Laubach, </author> <title> "IP over ATM and the construction of high-speed subnet backbones," </title> <journal> ConneXions, </journal> <volume> vol. 8, no. 7, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: Most universities and research institutions have at least T1 capability, and we are seeing increased use MM Clientsfl (encode/decode)fl Networkfl Web Server Interfacefl Data Filters/Schedulingfl Reader/Writerfl Multimedia FIlefl Systemfl MM Serverfl of the Internet Protocol over high-speed ATM (Asynchronous Transfer Mode) networks [2] <ref> [3] </ref>.
Reference: [4] <author> D. Wiltzus, L. Berc, and S. Devadhar, "BAGNet: </author> <title> Experiences with an ATM metropolitan-area network," </title> <journal> ConneXions-The Interoperability Report, </journal> <volume> vol. 10, no. 3, </volume> <month> March </month> <year> 1996. </year>
Reference-contexts: The evolution of networks such as the Metropolitan Research and Education Network (MREN), a high-performance ATM network; vBNS (very high speed Backbone Network Service), a national high-performance network devoted to meritorious research projects; and the Bay Area Gigabit Network (BAGNet) <ref> [4] </ref> are providing everyday access to high-speed networks for researchers and educators, making practical the transmission of multimedia streams. The evolution of the MBONE and regular use of these networks for teleseminars, as showcased by BAGNet, gives us a glimpse of the future of network-based multimedia conferencing.
Reference: [5] <author> R. Haskin and F. Schmuck, </author> <title> "The Tiger Shark file system," </title> <booktitle> in Proceedings of the IEEE Computer Conference, 1996. IEEE, </booktitle> <month> March </month> <year> 1996. </year>
Reference-contexts: multimedia filesystem, on the other hand, is designed to support the demands of real-time storage and playback of continuous-time data streams. 3 The Voyager Implementation The Voyager server is implemented on an IBM SP2 using a suite of commonly available software tools: * The IBM Tiger Shark multimedia file system <ref> [5] </ref> * The Perl language [6] [7] * The Nexus run-time communications package [8] * The ACE communications toolkit [9] [10] * The LBNL multimedia tools Vic and Vat [11] * Standard Realtime Transport Protocol (RTP) [12] 3.1 Voyager Hardware The current Voyager system is implemented on an IBM 9076 SP2 <p> Non-ATM nodes connect to the Internet via ethernet to an RS/6000 970 with an ATM connection to the same Cisco router. A SPARCstation 20 serves as the Voyager Web and database server. 3.2 Server Software Voyager relies on the IBM Tiger Shark filesystem <ref> [5] </ref>, now part of the IBM Multimedia Server product, to provide reliable access to the 72 GB of fast/wide SCSI disk that is striped across several nodes. The Tiger Shark filesystem is present on the eight thin nodes.
Reference: [6] <author> Randall Schwartz, </author> <title> Learning Perl, </title> <publisher> O'Reilly and Associates, </publisher> <year> 1993. </year>
Reference-contexts: hand, is designed to support the demands of real-time storage and playback of continuous-time data streams. 3 The Voyager Implementation The Voyager server is implemented on an IBM SP2 using a suite of commonly available software tools: * The IBM Tiger Shark multimedia file system [5] * The Perl language <ref> [6] </ref> [7] * The Nexus run-time communications package [8] * The ACE communications toolkit [9] [10] * The LBNL multimedia tools Vic and Vat [11] * Standard Realtime Transport Protocol (RTP) [12] 3.1 Voyager Hardware The current Voyager system is implemented on an IBM 9076 SP2 [13].
Reference: [7] <author> Larry Wall, Tom Christiansen, and Randall Schwartz, </author> <title> Programming Perl, </title> <publisher> O'Reilly and Associates, </publisher> <year> 1996. </year>
Reference-contexts: is designed to support the demands of real-time storage and playback of continuous-time data streams. 3 The Voyager Implementation The Voyager server is implemented on an IBM SP2 using a suite of commonly available software tools: * The IBM Tiger Shark multimedia file system [5] * The Perl language [6] <ref> [7] </ref> * The Nexus run-time communications package [8] * The ACE communications toolkit [9] [10] * The LBNL multimedia tools Vic and Vat [11] * Standard Realtime Transport Protocol (RTP) [12] 3.1 Voyager Hardware The current Voyager system is implemented on an IBM 9076 SP2 [13].
Reference: [8] <author> I. Foster, C. Kesselman, and S. Tuecke, </author> <title> "The Nexus approach to integrating multithreading and communication," </title> <journal> JPDC, </journal> <volume> vol. 37, </volume> <pages> pp. "70-82", </pages> <year> 1996. </year>
Reference-contexts: real-time storage and playback of continuous-time data streams. 3 The Voyager Implementation The Voyager server is implemented on an IBM SP2 using a suite of commonly available software tools: * The IBM Tiger Shark multimedia file system [5] * The Perl language [6] [7] * The Nexus run-time communications package <ref> [8] </ref> * The ACE communications toolkit [9] [10] * The LBNL multimedia tools Vic and Vat [11] * Standard Realtime Transport Protocol (RTP) [12] 3.1 Voyager Hardware The current Voyager system is implemented on an IBM 9076 SP2 [13]. This is a twelve-node machine.
Reference: [9] <author> Douglas C. Schmidt, </author> <title> "The adaptive communication environment an object-oriented network programming toolkit for developing communication software," </title> <booktitle> in Proceedings of Sun Users Group Conference, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: data streams. 3 The Voyager Implementation The Voyager server is implemented on an IBM SP2 using a suite of commonly available software tools: * The IBM Tiger Shark multimedia file system [5] * The Perl language [6] [7] * The Nexus run-time communications package [8] * The ACE communications toolkit <ref> [9] </ref> [10] * The LBNL multimedia tools Vic and Vat [11] * Standard Realtime Transport Protocol (RTP) [12] 3.1 Voyager Hardware The current Voyager system is implemented on an IBM 9076 SP2 [13]. This is a twelve-node machine. <p> We also probe the performance observed when running both the ATM network and the Tiger Shark file system. The benchmarks use two basic application programs: a simple stream source and a flexible event-driven stream sink. Each is implemented in C++ and uses an ACE Reactor <ref> [9, 10] </ref> object to handle the de-multiplexing of multiple streams and the invocation of timer callbacks. The stream source is invoked with a desired bandwidth, block size, and target host and UDP port.
Reference: [10] <author> Douglas C. Schmidt, </author> <title> Pattern Languages of Program Design, chapter Reactor: An object behavioral pattern for concurrent event demultiplexing and event handler dispatching, </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: streams. 3 The Voyager Implementation The Voyager server is implemented on an IBM SP2 using a suite of commonly available software tools: * The IBM Tiger Shark multimedia file system [5] * The Perl language [6] [7] * The Nexus run-time communications package [8] * The ACE communications toolkit [9] <ref> [10] </ref> * The LBNL multimedia tools Vic and Vat [11] * Standard Realtime Transport Protocol (RTP) [12] 3.1 Voyager Hardware The current Voyager system is implemented on an IBM 9076 SP2 [13]. This is a twelve-node machine. <p> We also probe the performance observed when running both the ATM network and the Tiger Shark file system. The benchmarks use two basic application programs: a simple stream source and a flexible event-driven stream sink. Each is implemented in C++ and uses an ACE Reactor <ref> [9, 10] </ref> object to handle the de-multiplexing of multiple streams and the invocation of timer callbacks. The stream source is invoked with a desired bandwidth, block size, and target host and UDP port.
Reference: [11] <author> S. McCanne and V. Jacobsen, </author> <title> "Vic: A flexible framework for packet video," </title> <booktitle> in ACM Multimedia 95. ACM, </booktitle> <month> November </month> <year> 1995, </year> <pages> pp. 511-522. </pages>
Reference-contexts: implemented on an IBM SP2 using a suite of commonly available software tools: * The IBM Tiger Shark multimedia file system [5] * The Perl language [6] [7] * The Nexus run-time communications package [8] * The ACE communications toolkit [9] [10] * The LBNL multimedia tools Vic and Vat <ref> [11] </ref> * Standard Realtime Transport Protocol (RTP) [12] 3.1 Voyager Hardware The current Voyager system is implemented on an IBM 9076 SP2 [13]. This is a twelve-node machine. <p> For playback, no specific hardware is required. 3.4 Client Software A client needs the following software to view and record media sessions in Voyager: * A Web browser that supports forms * RTP-compliant video and audio clients We use the Vic <ref> [11] </ref> video client and the Vat audio clients from LBNL on the workstation platforms. Ports of these tools are also available for Microsoft Windows.
Reference: [12] <author> H. Schulzrinne, S. Casner, R. Frederick, and V. Jacobsen, "RTP: </author> <title> A transport protocol for real-time applications," </title> <month> January </month> <year> 1996, </year> <title> Network Working Group, </title> <type> RFC 1889. </type>
Reference-contexts: suite of commonly available software tools: * The IBM Tiger Shark multimedia file system [5] * The Perl language [6] [7] * The Nexus run-time communications package [8] * The ACE communications toolkit [9] [10] * The LBNL multimedia tools Vic and Vat [11] * Standard Realtime Transport Protocol (RTP) <ref> [12] </ref> 3.1 Voyager Hardware The current Voyager system is implemented on an IBM 9076 SP2 [13]. This is a twelve-node machine. <p> The session daemons are instantiated by a set of CGI programs on the Web server that participate in a distributed nPerl server control application. The media streams are transported by using RTP, the Realtime Transport Protocol as specified in RFC 1889 <ref> [12] </ref>, and RFC 1890 [14]. Video is encoded by using either Motion JPEG [15] or h.261 [16].
Reference: [13] <author> T. Agerwala, J. L. Martin, J. H. Mirza, D. C. Sadler, D. M. Dias, and M. Snir, </author> <title> "SP2 systems architecture," </title> <journal> IBM Systems Journal, </journal> <volume> vol. 34, no. 2, </volume> <year> 1995. </year>
Reference-contexts: * The Perl language [6] [7] * The Nexus run-time communications package [8] * The ACE communications toolkit [9] [10] * The LBNL multimedia tools Vic and Vat [11] * Standard Realtime Transport Protocol (RTP) [12] 3.1 Voyager Hardware The current Voyager system is implemented on an IBM 9076 SP2 <ref> [13] </ref>. This is a twelve-node machine.
Reference: [14] <author> H. Schulzrinne, </author> <title> "RTP profile for audio and video conferences with minimal control," </title> <month> January </month> <year> 1996, </year> <title> Network Working Group, </title> <type> RFC 1890. </type>
Reference-contexts: The session daemons are instantiated by a set of CGI programs on the Web server that participate in a distributed nPerl server control application. The media streams are transported by using RTP, the Realtime Transport Protocol as specified in RFC 1889 [12], and RFC 1890 <ref> [14] </ref>. Video is encoded by using either Motion JPEG [15] or h.261 [16]. Audio typically is encoded by using PCM. 3.3 Client Hardware The hardware that we have used at ANL includes * RS/6000 41T workstations, with the IBM Ultime-dia video and audio adapters and Turboways OC3 ATM adapters.
Reference: [15] <author> L. Berc, W. Fenner, R. Frederick, and S. Mc-Canne, </author> <title> "RTP payload format for JPEG-compressed video," </title> <month> October </month> <year> 1996, </year> <title> Network Working Group, RFC 2035. </title>
Reference-contexts: The media streams are transported by using RTP, the Realtime Transport Protocol as specified in RFC 1889 [12], and RFC 1890 [14]. Video is encoded by using either Motion JPEG <ref> [15] </ref> or h.261 [16]. Audio typically is encoded by using PCM. 3.3 Client Hardware The hardware that we have used at ANL includes * RS/6000 41T workstations, with the IBM Ultime-dia video and audio adapters and Turboways OC3 ATM adapters.
Reference: [16] <author> T. Turletti and C. Huitema, </author> <title> "RTP payload format for H.261 video streams," </title> <month> October </month> <year> 1996, </year> <title> Network Working Group, RFC 2032. </title>
Reference-contexts: The media streams are transported by using RTP, the Realtime Transport Protocol as specified in RFC 1889 [12], and RFC 1890 [14]. Video is encoded by using either Motion JPEG [15] or h.261 <ref> [16] </ref>. Audio typically is encoded by using PCM. 3.3 Client Hardware The hardware that we have used at ANL includes * RS/6000 41T workstations, with the IBM Ultime-dia video and audio adapters and Turboways OC3 ATM adapters.
Reference: [17] <author> Inc. </author> <note> Precept Software, Precept IP/TV Viewer Users Manual, Precept Software, Inc., initial release edition, </note> <month> June </month> <year> 1996, </year> <title> Part Number 201. </title>
Reference-contexts: Ports of these tools are also available for Microsoft Windows. We have also used the RTP tools from Precept <ref> [17] </ref>. 4 Theoretical Voyager Performance Limitations In this section we discuss the performance limitations of the Voyager system that are dictated by the architecture of the system we are using. Figure 2 is a detailed schematic our SP system.
Reference: [18] <author> Ronald Mraz, Douglas Freimuth, Edward Now-icki, and Gabriel Silberman, </author> <title> "Using commodity networks for distributed computing research," </title> <type> Tech. Rep., </type> <institution> IBM T.J. Watson Research Center, </institution> <year> 1995. </year>
Reference-contexts: Zero-copy ATM adapter technology is one solution to this problem. Researchers at IBM have built a prototype ATM adapter called Cheetah which uses DMA to transfer data between the ATM network and main memory <ref> [18, 19] </ref>. The system CPU is only involved in the setup of packet transfers. We have demonstrated the use of this adapter in a Voyager client machine, where it proved to work very efficiently. Such technology would prove very useful in the server itself.
Reference: [19] <author> Lucas Womack, Ronald Mraz, and Abraham Mendelson, </author> <title> "A study of virtual memory MTU reassembly (VMMR) within the PowerPC architecture," </title> <booktitle> in Proceedings of the Fifth International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS '97). IEEE MASCOTS97, </booktitle> <month> Jan-uary </month> <year> 1997. </year>
Reference-contexts: Zero-copy ATM adapter technology is one solution to this problem. Researchers at IBM have built a prototype ATM adapter called Cheetah which uses DMA to transfer data between the ATM network and main memory <ref> [18, 19] </ref>. The system CPU is only involved in the setup of packet transfers. We have demonstrated the use of this adapter in a Voyager client machine, where it proved to work very efficiently. Such technology would prove very useful in the server itself.
References-found: 19

