URL: http://www.isi.edu/soar/gratch/ML93.ps
Refering-URL: http://www.isi.edu/soar/gratch/home.html
Root-URL: 
Email: -gratch, dejong-@cs.uiuc.edu  chien@aig.jpl.nasa.gov  
Title: Learning Search Control Knowledge for Deep Space Network Scheduling  
Author: Jonathan Gratch Steve Chien and Gerald DeJong 
Date: June 1993, pp. 135-142.  
Address: Amherst, MA,  405 N. Mathews Av., Urbana, IL 61801  4800 Oak Grove Drive, Pasadena, CA 91109-8099  
Affiliation: Learning,  *Beckman Institute University of Illinois  Jet Propulsion Laboratory California Institute of Technology  
Note: Appears in the Proceedings of the Tenth International Conference on Machine  
Abstract: While the general class of most scheduling problems is NP-hard in worst-case complexity, in practice, for specific distributions of problems and constraints, domain-specific solutions have been shown to perform in much better than exponential time. Unfortunately, constructing such techniques is a knowledge-intensive and time-consuming process that requires a deep understanding of the domain and the scheduler. The goal of our work is to develop techniques to allow for automated learning of an effective domain-specific search strategy given a general problem solver with a flexible control architecture. In this approach, a learning system searches a space of possible control strategies, using statistics to evaluate performance over the expected problem distribution. We discuss an application of the approach to scheduling satellite communications. Using problem distributions based on actual mission requirements, our approach identified strategies that both decrease the amount of CPU time required to produce schedules, and increase the percentage of problems that are solvable within computational resource limitations.
Abstract-found: 1
Intro-found: 1
Reference: [Bell92] <author> C. E. Bell, </author> <title> Scheduling Deep Space Network Data Transmissions: A Lagrangian Relaxation Approach, </title> <type> Internal JPL report, </type> <month> July </month> <year> 1992. </year>
Reference-contexts: Two important aspects of this evaluation are that the task and problem distribution are based on a real-world situation, and the scheduling approach was developed independently of our learning work <ref> [Bell92] </ref>. The implementation includes a novel approach for improving learning efficiency. <p> Two time periods conflict if they use the same antenna and overlap in temporal extent. A valid schedule specifies a non-conflicting subset of all possible time periods where each project's requirements are satisfied. 3.1 THE SCHEDULER SYSTEM LR-26 is a heuristic approach to the scheduling problem <ref> [Bell92] </ref> developed at the Jet Propulsion Laboratory. It provides a good platform for learning as it can be modified easi ly to incorporate alternative heuristic strategies. Further--more, it uses an expert crafted control strategy that provide a challenging base-line to judge learned knowledge.
Reference: [Buntine89] <author> W. Buntine, </author> <title> A Critique of the Valiant Model, </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI, </address> <month> August </month> <year> 1989, </year> <pages> pp. 837-842. </pages>
Reference-contexts: Weak assumptions are a way of ensuring wide applicability. Unfortunately, the resulting bounds prove too large for most practical applications (see <ref> [Buntine89, Gratch92] </ref>). COMPOSER embodies stronger statistical assumptions suggested by the Central Limit Theorem [Hogg78 pp. 192-195]. These stronger assumptions drastically reduce the number of examples required to make statistical decisions, but they limit the applicability of the approach.
Reference: [Dechter92] <author> R. Dechter, </author> <booktitle> Constraint Networks,in Encyclopedia of Artificial Intelligence, </booktitle> <editor> Stuart C Shapiro (ed.), </editor> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1992. </year>
Reference-contexts: This suggests that heuristics can be crafted to exploit this regularity to improve performance. Many heuristics have been suggested to improve scheduling efficiency. Often these heuristics are stated as general principles (e.g. first instantiate variables that maximally constraint the rest of the search space <ref> [Dechter92] </ref>) and there may be many ways to realize them in a particular scheduler and domain. Furthermore, there are almost certainly interactions between methods used at different control points that makes it difficult to construct a good overall strategy. <p> Examples of alternative ordering methods include preferring high conflict variables and preferring low conflict variables 2 . These heuristics can be viewed as variable ordering heuristics from the CSP literature <ref> [Dechter92] </ref>. COMPOSER requires this search to be structured for hill-climbing search. A simple way to organize the search through this strategy space would be to treat all control points a equal and consider all single method changes to a given control strategy. <p> Selecting high conflict variable has the benefit of rapidly forcing many time-periods to be in or out of the schedule (e.g. reducing the number of steps to solution). Selecting low conflict intervals advocates adding those time-periods which cost little, and thus may satisfy constraints without causing conflicts <ref> [Dechter92] </ref>. moving onto the next level. <p> One interpretation of the constraint and child methods is that search should proceed by first identifying a highly constrained constraint, and then choose the least constrained way of satisfying it. It is interesting that this is consistent with the general recommendations in the CSP literature <ref> [Dechter92] </ref>. 5 DISCUSSION This paper evaluates COMPOSER on a real-world domain. COMPOSER is grounded in a mathematical framework. While the framework embodies statistical assumptions, there is a theoretical support for these (the Central Limit Theorem) and they enjoy wide acceptance in the statistical community.
Reference: [Fisher81] <author> M. Fisher, </author> <title> The Lagrangian Relaxation Method for Solving Integer Programming Problems, </title> <booktitle> Management Science 27, 1 (1981), </booktitle> <pages> pp. 1-18. </pages>
Reference-contexts: Integer programming is NP-hard, and the size of our scheduling problems makes the conventional approach impractical: a typical problem has approximately 650 variables and 1300 constraints. LR-26 embodies a heuristic approach called lagrangian relaxation <ref> [Fisher81] </ref>. Lagrangian relaxation requires identifying a set of constraints that, if removed, make the problem computationally easy. These constraints are relaxed, meaning they no longer act as constraints but instead modify the objective function.
Reference: [Gratch91] <author> J. Gratch and G. DeJong, </author> <title> A Hybrid Approach to Guaranteed Effective Control Strategies, </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <address> Evanston, IL, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: The implementation includes a novel approach for improving learning efficiency. The performance of the system, along with previous results in artificial planning domains <ref> [Gratch91, Gratch92] </ref>, demonstrates COMPOSER's flexibility and its potential to identify beneficial knowledge in practical learning problems. 2 COMPOSER COMPOSER is a statistical approach to improving the expected utility of problem solving. The overall approach is one of generate and test hill-climbing.
Reference: [Gratch92] <author> J. Gratch and G. DeJong, COMPOSER: </author> <title> A Probabilistic Solution to the Utility Problem in Speed-up Learning, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992, </year> <pages> pp. 235-240. </pages>
Reference-contexts: The implementation includes a novel approach for improving learning efficiency. The performance of the system, along with previous results in artificial planning domains <ref> [Gratch91, Gratch92] </ref>, demonstrates COMPOSER's flexibility and its potential to identify beneficial knowledge in practical learning problems. 2 COMPOSER COMPOSER is a statistical approach to improving the expected utility of problem solving. The overall approach is one of generate and test hill-climbing. <p> A transformation is adopted if it increases the expected utility of solving problems over that distribution. The generator then constructs a set of transformations to this new strategy and so on. For a complete description of the method see <ref> [Gratch92] </ref>. The algorithm is summarized in the Appendix. COMPOSER's solution is applicable in cases where the following conditions apply. 1. The control strategy space can be structured to facilitate hill-climbing search. In general, the space of such strategies is so large as to make exhaustive search intractable. <p> Weak assumptions are a way of ensuring wide applicability. Unfortunately, the resulting bounds prove too large for most practical applications (see <ref> [Buntine89, Gratch92] </ref>). COMPOSER embodies stronger statistical assumptions suggested by the Central Limit Theorem [Hogg78 pp. 192-195]. These stronger assumptions drastically reduce the number of examples required to make statistical decisions, but they limit the applicability of the approach. <p> In previous evaluations these stronger assumptions have proved reasonable: COMPOSER's assumptions produced sample complexities two to three order of magnitudes less than the weak assumptions adopted by Greiner and Jurisica, without compromising the statistical error <ref> [Gratch92] </ref>. However these promising results have the drawback that they are based on artificial domains and problem distributions. <p> A simple way to organize the search through this strategy space would be to treat all control points a equal and consider all single method changes to a given control strategy. This was the strategy in our PRODIGY implementation of COMPOSER <ref> [Gratch92] </ref>. Here, we used our knowledge of the scheduler to take advantage of interactions (or lack thereof) between control points to help structure the search. The intent of this organization is to reduce the branching factor in the control strategy search and improve the expected utility of locally optimal solutions. <p> This allows us to generate the m incremental utility values. There are several issues that lead us to this particular solution. In particular we found that other more efficient proposals for gathering statistics (see <ref> [Gratch92, Greiner92] </ref>) were not appropriate to this problem. We elaborate on this issue in Section 5. 4 EXPERIMENT AND RESULTS DSN-COMPOSER should, with high probability, improve the expected utility of the scheduler over the distribution of problems. This can be seen as two basic claims that can be tested empirically. <p> Admittedly, these statistical approaches have not seen wide use within machine learning systems, so there is some reason to be cautious about their applicability. However, the current scheduling results and previous demonstrations in artificial planning domains <ref> [Gratch92] </ref> pro vide growing support for the effectiveness and generality of the COMPOSER framework. An important aspect of statistical frameworks like COMPOSER is their flexibility. In our research we have applied the technique to scheduling and planning tasks, in both cases improving the average time to produce solutions. <p> While this is not bad from a complexity standpoint, it is a pragmatic concern. There have been a few proposals to reduce the expense in gathering statistics. In <ref> [Gratch92] </ref> we exploited properties of the transformations to gather statistics from a single solution attempt. That system relied on so-called rejection rules [Minton88] that only avoid backtracking. The same technique could not be applied to preference rules that suggest novel search directions.
Reference: [Gratch93] <author> J. Gratch and S. Chien, </author> <title> Learning Search Control Knowledge for the Deep Space Network Scheduling Problem: Extended Report and Guide to Software, </title> <type> Technical Report UIUCDCS-R-93-1789, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: For comparative purposes we include a secondary set of experiments that incorporate these unsolved problems. For a complete description of how training examples are generated, see <ref> [Gratch93] </ref>. 3.3 EXPECTED UTILITY In the DSN application a chief concern is with the computational efficiency of the scheduler. There is a strong need that the scheduler return quickly on average.
Reference: [Greiner89] <author> R. Greiner and J. Likuski, </author> <title> Incorporating Redundant Learned Rules: A Preliminary Formal Analysis of EBL, </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI, </address> <month> August </month> <year> 1989, </year> <pages> pp. 744-749. </pages>
Reference-contexts: In the best case we possess a detailed cost model of the problem solver that efficiently derives the ramification of proposed modifications without actually solving the problem (e.g. <ref> [Greiner89, Subramanian90] </ref>). In the worst case we can resort to brute-force simulation: solve the problem with and without the proposed modification and observe the difference in utility between the two solution attempts.
Reference: [Greiner92] <author> R. Greiner and I. Jurisica, </author> <title> A Statistical Approach to Solving the EBL Utility Problem, </title> <booktitle> Proceedings of the Na tional Conference on Artificial Intelligence, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992, </year> <pages> pp. 241-248. </pages>
Reference-contexts: There is sufficient regularity in the domain such that the cost of learning a good strategy can be amortized over the gains in solving many problems. COMPOSER can be seen as one of a class of statistical approaches to improving the expected utility of problem solving (see also <ref> [Greiner92, Laird92, Subramanian92] </ref>. The principle drawback of these techniques is that they find only local maxima, they may require many examples, and examples can be expensive to process. Furthermore, their statistical properties rest upon assumptions that may not hold in practice. <p> Furthermore, their statistical properties rest upon assumptions that may not hold in practice. Greiner and Jurisica demonstrate that, under very weak assumptions, the number of examples, or sample complexity, can be bounded by a polynomial function of the allowable statistical error <ref> [Greiner92] </ref>. Weak assumptions are a way of ensuring wide applicability. Unfortunately, the resulting bounds prove too large for most practical applications (see [Buntine89, Gratch92]). COMPOSER embodies stronger statistical assumptions suggested by the Central Limit Theorem [Hogg78 pp. 192-195]. <p> This allows us to generate the m incremental utility values. There are several issues that lead us to this particular solution. In particular we found that other more efficient proposals for gathering statistics (see <ref> [Gratch92, Greiner92] </ref>) were not appropriate to this problem. We elaborate on this issue in Section 5. 4 EXPERIMENT AND RESULTS DSN-COMPOSER should, with high probability, improve the expected utility of the scheduler over the distribution of problems. This can be seen as two basic claims that can be tested empirically. <p> In [Gratch92] we exploited properties of the transformations to gather statistics from a single solution attempt. That system relied on so-called rejection rules [Minton88] that only avoid backtracking. The same technique could not be applied to preference rules that suggest novel search directions. Greiner and Jurisica <ref> [Greiner92] </ref> propose one method for evaluating preference rules from a single solution attempt by maintaining upper and lower bounds on the utility of the novel search paths.
Reference: [Held70] <author> M. Held and R. M. Karp, </author> <title> The Traveling Salesman Problem and Minimum Spanning Trees, </title> <note> Operations Research 18, </note> <year> (1970), </year> <pages> pp. 1138-1162. </pages>
Reference-contexts: 1 INTRODUCTION General problem solving tasks like planning and scheduling are inherently complex. Nevertheless, in many practical situations these complex problems have reasonable solutions (e.g. traveling salesman problem <ref> [Held70] </ref>). Often we can take advantage of the structure of a domain or the distribution of problems to formulate effective solutions to complex problems. Unfortunately, a system designer must devote considerable expense to the performance aspects of an algorithm.
Reference: [Hogg78] <author> R. V. Hogg and A. T. Craig, </author> <title> Introduction to Mathematical Statistics, </title> <publisher> Macmillan Publishing Co., Inc., </publisher> <address> London, </address> <year> 1978. </year>
Reference: [Laird92] <author> P. Laird, </author> <title> Dynamic Optimization, </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <address> Aberdeen, Scotland, </address> <month> July </month> <year> 1992, </year> <pages> pp. 263-272. </pages>
Reference-contexts: There is sufficient regularity in the domain such that the cost of learning a good strategy can be amortized over the gains in solving many problems. COMPOSER can be seen as one of a class of statistical approaches to improving the expected utility of problem solving (see also <ref> [Greiner92, Laird92, Subramanian92] </ref>. The principle drawback of these techniques is that they find only local maxima, they may require many examples, and examples can be expensive to process. Furthermore, their statistical properties rest upon assumptions that may not hold in practice.
Reference: [Mackworth92] <author> A. K. </author> <title> Mackworth, </title> <booktitle> Constraint Satisfaction,in Encyclopedia of Artificial Intelligence, </booktitle> <editor> Stuart C Shapiro (ed.), </editor> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1992. </year>
Reference-contexts: This is a methodology for finding an assignment to integer variables that maximizes the value of an objective function, subject to a set of linear constraints. The objective function characterizes the value of the solution. Many constraint satisfaction problems (CSP) are easily cast as integer programming problems <ref> [Mackworth92] </ref>.
Reference: [Minton88] <author> S. </author> <title> Minton,in Learning Search Control Knowledge: An Explanation-Based Approach, </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: Utility might be defined as a function of the time to construct a plan. Our proposed solution to this learning problem, sometimes called the utility problem <ref> [Minton88] </ref>, is embodied in the COMPOSER system. COMPOSER can be characterized as a hill-climbing search in the space of possible strategies. The learning system alternately conjectures changes to the current control strategy and statistically evaluates them to determine how well they enhance expected utility. 1. <p> There have been a few proposals to reduce the expense in gathering statistics. In [Gratch92] we exploited properties of the transformations to gather statistics from a single solution attempt. That system relied on so-called rejection rules <ref> [Minton88] </ref> that only avoid backtracking. The same technique could not be applied to preference rules that suggest novel search directions. Greiner and Jurisica [Greiner92] propose one method for evaluating preference rules from a single solution attempt by maintaining upper and lower bounds on the utility of the novel search paths.
Reference: [Subramanian90] <author> D. Subramanian and R. Feldman, </author> <title> The Utility of EBL in Recursive Domain Theories, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA, </address> <month> August </month> <year> 1990, </year> <pages> pp. 942-949. </pages>
Reference-contexts: In the best case we possess a detailed cost model of the problem solver that efficiently derives the ramification of proposed modifications without actually solving the problem (e.g. <ref> [Greiner89, Subramanian90] </ref>). In the worst case we can resort to brute-force simulation: solve the problem with and without the proposed modification and observe the difference in utility between the two solution attempts.
Reference: [Subramanian92] <author> D. Subramanian and S. Hunter, </author> <title> Measuring Utility and the Design of Provably Good EBL Algorithms, </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <address> Aberdeen, Scotland, </address> <month> July </month> <year> 1992, </year> <pages> pp. 426-435. </pages>
Reference-contexts: There is sufficient regularity in the domain such that the cost of learning a good strategy can be amortized over the gains in solving many problems. COMPOSER can be seen as one of a class of statistical approaches to improving the expected utility of problem solving (see also <ref> [Greiner92, Laird92, Subramanian92] </ref>. The principle drawback of these techniques is that they find only local maxima, they may require many examples, and examples can be expensive to process. Furthermore, their statistical properties rest upon assumptions that may not hold in practice.
Reference: [Taha82] <author> H. A. Taha, </author> <title> Operations Research, an Introduction, </title> <publisher> Macmillan Publishing Co., INC., </publisher> <year> 1982. </year>
Reference-contexts: This system must make many repeated calls to the scheduler to compare variants of each weekly schedule. For this reason, LR-26 must provided solutions in a timely fashion. Scheduling is formulated as a 0-1 integer programming problem <ref> [Taha82] </ref>. This is a methodology for finding an assignment to integer variables that maximizes the value of an objective function, subject to a set of linear constraints. The objective function characterizes the value of the solution. Many constraint satisfaction problems (CSP) are easily cast as integer programming problems [Mackworth92].
References-found: 17

