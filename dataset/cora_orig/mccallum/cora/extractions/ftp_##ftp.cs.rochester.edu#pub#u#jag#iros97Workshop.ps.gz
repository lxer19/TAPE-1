URL: ftp://ftp.cs.rochester.edu/pub/u/jag/iros97Workshop.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/jag/publications.html
Root-URL: 
Title: Image Based Visual Simulation and Tele-Assisted Robot Control  
Author: Martin Jagersand 
Web: http://www.cs.rochester.edu/u/jag  
Address: Rochester, Rochester, NY 14627  
Affiliation: Department of Computer Science, University of  
Abstract: In tele-assistance a robot operates partly guided by a human and partly autonomously. Tele-assistance thus fills in the gap between full autonomy, and direct control by a human tele-operator. Instead of directly controlling the motor actions of a robot, in tele-assistance the human interacts with the robot using a deictic high level language. Pook and Ballard have implemented such a system using a hand sign language, sensed by an exoskeleton[26]. In this work we present a method in which the robot is instructed through visual pointing. The human operator instructs the robot by pointing out (currently using a mouse and images displayed on a computer screen) desired manipulations. The tele-assistance system transforms the coarse task descriptions supplied by the human into a visual (camera) space task and trajectory plan. The manipulation is carried out using visual servo-ing on an uncalibrated hand-eye system. For this we have developed an integrated model estimation, control and visual simulation system capable of transforming these visual space plans into motor action and providing image based synthethic visual feedback to the human operator in order to compensate for the delayed response of the real robotic system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> On-line m-peg movies of the experiments are availible. </institution> <note> To see [www-video- ] references use the menus in: http://www.cs.rochester.edu/u/jag/SimAct/videos.html http://www.cs.rochester.edu/u/jag/PercAct/robotvideos.html </note>
Reference-contexts: Right: Performing the planned insertion. Video 1 <ref> [1] </ref> The alignment serves two purposes. It aligns the piece in 6 D, obtaining the correct initial pose for the 6 DOF fine manipulation. Also during this phase the bootstrapped 6 DOF Jacobian is updated to an accurate estimate, allowing high precision moves in the later fine manipulation stage. <p> These images are taken from the controller's right camera. White lines show controller visual trajectory plans for carrying out manipulation. Video 2 and Video 3 <ref> [1] </ref>. Visual Programming. In tele-assisted teaching the operator is specifying the visual alignments necessary to start a servoing behavior or virtual tool interactively during manipulation. The visual programming interface is similar, except that all the visual specifications are done off-line before execution. <p> In the simple example above the resulting robot program is invariant to robot position and calibration, but not camera calibration. Camera invariance can be achieved by defining the visual goals relative to some fiducial points in the images. manipulation. Video 4 <ref> [1] </ref> 5 Discussion and Future Work The main result in this paper is that visual-motor model estimation, visual servoing and visual simulation can be integrated into one vision based tele-assistance system. Tele-assistance is a technique that is useful for solving real world manipulation tasks.
Reference: [2] <author> Weiss L. E. Sanderson A. C. Neumann C. P. </author> <title> Dynamic Sensor-Based Control of Robots with Visual Feedback J. of Robotics and Aut. </title> <publisher> v. </publisher> <month> RA-3 </month> <year> 1987 </year>
Reference-contexts: We use a two level approach. The motor commands (ffi k in eq. 7 above) are used to generate a desired joint space trajectory, where the low level joint control is implemented using RCCL. We achieve smooth visual ser-voing movement, rather than a look and move (in Weiss terminology <ref> [2] </ref>), by queuing the next trajectory segment before the current is finished, see Fig. 5. Note that ^ J k in eq. 7 is updated by eq. 6 in each step.
Reference: [3] <author> Feddema J. T. Lee G. C. S. </author> <title> Adaptive Image Feature Prediction and Control for Visual Tracking with a Hand-Eye Coordinated Camera IEEE Tr. </title> <journal> on Systems, Man and Cyber., </journal> <volume> v 20, no 5 1990 </volume>
Reference-contexts: We exploit this for prediction and outlier removal in tracking. 2.1 Recursive On-line Jacobian Estimation In most visual servoing work a Jacobian has been either (1) derived analytically, (2) derived partially analytically and partially estimated (eg. <ref> [3, 7] </ref>), or (3) determined experimentally by physically executing a set of orthogonal calibration movements e i (eg. [9]) and approximating the Jacobian with finite differences: ^ J (x; d) = (f (x+d 1 e 1 )f (x); : : : ; f (x+d n e n )f (x))D 1 where <p> Feddema points out that standard frame rate (30 Hz) is too low for direct joint motor feedback, and suggests that this can be overcome by also using joint feedback <ref> [3] </ref>. We use a two level approach. The motor commands (ffi k in eq. 7 above) are used to generate a desired joint space trajectory, where the low level joint control is implemented using RCCL.
Reference: [4] <author> Curwen R. Blake A. </author> <title> Dynamic Contours: Real time active splines In Active Vision ed. Blake, </title> <publisher> Yuille MIT Press 1992. </publisher>
Reference-contexts: We use real time visual feature trackers of three different kinds. Deformable contours snakes <ref> [4] </ref> are used to track surface discontinuities. A locally developed template matching tracker tracks multiple local features from surface markings or corners. For reliability in repeated experiments, or to deal with smooth featureless surfaces, we use special purpose trackers, which track attached surface markings or small lights.
Reference: [5] <author> Espiau B. Chaumette F. Rives P. </author> <title> A New Approach to Visual Servoing in Robotics IEEE Tr. </title> <booktitle> on Robotics and Automation p 313-326 v 8 no 3, </booktitle> <year> 1992. </year>
Reference: [6] <author> Wijesoma S. W. Wolfe D. F. H. Richards R. J. </author> <title> Eye-to-Hand Coordination for vision guided Robot Control Applications Int. </title> <journal> J. of Robotics Research, </journal> <volume> v 12 No 1 1993 </volume>
Reference: [7] <author> Papanikolopoulos N. P. Khosla P. K. </author> <title> Adaptive Robotic Visual Tracking: </title> <journal> Theory and Experiments IEEE Tr. on Aut. </journal> <note> Control Vol 38 no 3 1993 </note>
Reference-contexts: We exploit this for prediction and outlier removal in tracking. 2.1 Recursive On-line Jacobian Estimation In most visual servoing work a Jacobian has been either (1) derived analytically, (2) derived partially analytically and partially estimated (eg. <ref> [3, 7] </ref>), or (3) determined experimentally by physically executing a set of orthogonal calibration movements e i (eg. [9]) and approximating the Jacobian with finite differences: ^ J (x; d) = (f (x+d 1 e 1 )f (x); : : : ; f (x+d n e n )f (x))D 1 where
Reference: [8] <author> Harris M. </author> <title> Vision Guided Part Alignment with Degraded Data DAI TR #615, </title> <address> Edinburgh 1993 </address>
Reference-contexts: Several flavors of visual servoing exist, (e.g. [2, 3, 5, 6, 7, 10, 12, 14, 15, 18, 19, 20] 2 ). Visual models suitable for specifying simple visual alignments have also been studied <ref> [21, 11, 9, 8] </ref>. Learning by Watching.
Reference: [9] <author> Hollinghurst N. Cipolla R. </author> <title> Uncalibrated Stereo Hand-Eye Coordination Brit. </title> <booktitle> Machine Vision Conf 1993 </booktitle>
Reference-contexts: Several flavors of visual servoing exist, (e.g. [2, 3, 5, 6, 7, 10, 12, 14, 15, 18, 19, 20] 2 ). Visual models suitable for specifying simple visual alignments have also been studied <ref> [21, 11, 9, 8] </ref>. Learning by Watching. <p> prediction and outlier removal in tracking. 2.1 Recursive On-line Jacobian Estimation In most visual servoing work a Jacobian has been either (1) derived analytically, (2) derived partially analytically and partially estimated (eg. [3, 7]), or (3) determined experimentally by physically executing a set of orthogonal calibration movements e i (eg. <ref> [9] </ref>) and approximating the Jacobian with finite differences: ^ J (x; d) = (f (x+d 1 e 1 )f (x); : : : ; f (x+d n e n )f (x))D 1 where D = diag (d); d 2 &lt; n .
Reference: [10] <author> Jagersand M. Nelson R. </author> <title> Adaptive Differential Visual Feedback for uncalibrated hand-eye coordination and motor control TR# 579, </title> <type> U. </type> <institution> of Rochester 1994. </institution>
Reference-contexts: Visual servoing, when supplemented with on-line visual model estimation, fits into the active vision paradigm. 1 Terminology according to Weiss'[2] 2 For a review of this work we direct the reader to <ref> [10] </ref> or [18]. 1.2 An Integrated Approach to Uncalibrated Hand-Eye Coordination Our approach integrates three main functions into the hand-eye manipulation system. The three functions are model learning, movement and visual simulation. These functions run as concurrent processes. <p> The central model acquisition and application modules remain the same, while the manipulators and low level control, as well as the camera configuration and visual front-end are exchangeable. We use exactly the same algorithm for regular 3 and 6 DOF control on the PUMA arms <ref> [10] </ref>, as for the different and much more complicated Utah/MIT 16 DOF hand control [14], and the manipulation of non-rigid materials by two arms in parallel. <p> In previous work we have had good results with an asymmetric rank 1 correction formula <ref> [10, 30] </ref>: ^ J k+1 = ^ J k + (y measured ^ J k x)x T x T x For a set of orthogonal movements about a point fx k ; k = 1 : : : ng eq. 6 is identical to the finite difference update (eq. 5) in <p> The general principle is for each step in a manipulation task to use the simplest possible (lowest DOF) servoing movement. From our experimental evaluation in <ref> [10, 15] </ref>, we know that the smaller and simpler the manipulation model is, the more robust model estimation and servoing is achieved. Also controlling fewer DOF's require fewer visual measurements, and a smaller task description. The three main classes of servoing movements are transportation, alignment and fine manipulation (Fig. 9). <p> The manipulations are defined in visual space, independent of the manipulator, and transformed into motor space using the estimated visual-motor models (Section 2.1). For instance, a visually servoed 6 DOF fine manipulation can be done, using the same movement specification and visual servoing tool, both with a PUMA arm <ref> [10] </ref> and, by finger movements of a grasped object, with a Utah/MIT hand [14]. 4.4 Visual Space Deictic Teaching A remote tele-operator can describe tasks in several ways: Tele-assistance.
Reference: [11] <author> Kutulakos K. Jagersand M. </author> <title> Exploring objects by purposive viewpoint control and invariant-based hand-eye coordination Workshop on vision for robots In conjunction with IROS 1995. </title>
Reference-contexts: Several flavors of visual servoing exist, (e.g. [2, 3, 5, 6, 7, 10, 12, 14, 15, 18, 19, 20] 2 ). Visual models suitable for specifying simple visual alignments have also been studied <ref> [21, 11, 9, 8] </ref>. Learning by Watching.
Reference: [12] <author> Jagersand M. Nelson R. </author> <title> Visual Space Task Specification, </title> <booktitle> Planning and Control In Proc. IEEE Int. Symp. on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: However visual servoing alone is not sufficient to solve whole complex manipulation tasks. Two principal remaining challenges are the creation of useful manipulation sequences from combinations of simpler servoing movements and how to program or teach such a robot hand-eye system. In <ref> [12] </ref> we argue that in an uncalibrated setting it is advantageous to sequence different types of servoing movements with different manipulation models to solve nontrivial manipulation tasks. <p> After this the operator is queried for which piece to pick up next, and where to put it (Fig. 14 bottom left). After using the tele-assisted system on the puzzle task, as well as some others (installing a lightbulb <ref> [12] </ref> and robot golf: kicking a tennisball) we found that it is significantly easier to use than a telemanipulation system where the operator controls joint motions directly. The interaction with the operator is relatively sparse and on a higher level.
Reference: [13] <author> R. Nelson, M. Jagersand, O. </author> <title> Fuentes Virtual Tools: A Framework for Simplifying Sensory-Motor Control in Complex Robotic Systems TR 576 University of Rochester, short version in Workshop on vision for robots IROS 1995. </title>
Reference-contexts: For instance by sequencing a transport, alignment and guarded movement (pick) with another transport, alignment, and fine manipulation movement (place) we have a basic pick-and-place tool (Fig. 12). For a detailed treatment on the requirements to specify virtual tools see <ref> [13] </ref>. To use a virtual tool it needs to be instantiated with visual goals. For the pick-and-place tool this is the pick up and put down points. A property of our vision based approach is that a particular virtual tool can be mapped onto several manipulators.
Reference: [14] <author> Jagersand M. Fuentes O. Nelson R. </author> <title> Acquiring Visual-Motor Models for Preciion Manipulation with Robot Hands In Proc. </title> <booktitle> of ECCV, </booktitle> <year> 1996. </year>
Reference-contexts: We use exactly the same algorithm for regular 3 and 6 DOF control on the PUMA arms [10], as for the different and much more complicated Utah/MIT 16 DOF hand control <ref> [14] </ref>, and the manipulation of non-rigid materials by two arms in parallel. <p> For instance, a visually servoed 6 DOF fine manipulation can be done, using the same movement specification and visual servoing tool, both with a PUMA arm [10] and, by finger movements of a grasped object, with a Utah/MIT hand <ref> [14] </ref>. 4.4 Visual Space Deictic Teaching A remote tele-operator can describe tasks in several ways: Tele-assistance.
Reference: [15] <author> Jagersand M. Nelson R. Fuentes O. </author> <title> Experimental Evaluation of Uncalibrated Visual Servoing for Precision Manipulation In Proc. </title> <booktitle> of Int. Conf. on Robotics and Automation 1997. </booktitle>
Reference-contexts: The general principle is for each step in a manipulation task to use the simplest possible (lowest DOF) servoing movement. From our experimental evaluation in <ref> [10, 15] </ref>, we know that the smaller and simpler the manipulation model is, the more robust model estimation and servoing is achieved. Also controlling fewer DOF's require fewer visual measurements, and a smaller task description. The three main classes of servoing movements are transportation, alignment and fine manipulation (Fig. 9).
Reference: [16] <author> Jagersand M. </author> <title> Model Free View Synthesis of an Articulated Agent. </title> <booktitle> In Proc. of Computer Vision and Pattern Recognition 1997, </booktitle> <institution> also TR 595 U of Rochester. </institution>
Reference-contexts: For appearance based measures we use a linear subspace of the intensity images. Let U be a transformation matrix (We use a PCA based U , see <ref> [16] </ref>) and I a reference (mean) image. <p> Here we show how to use it to animate robot motion based on models estimated while controlling a real robot. The method also works without explicit control of the agent, and in <ref> [16] </ref> we have shown how to animate articulated human arm movements. We describe an off-line and on-line case.
Reference: [17] <author> Jagersand M. </author> <title> On-line Estimation of Visual-Motor Models for Robot Control and Visual Simulation PhD thesis, </title> <institution> U of Rochester 1997. </institution>
Reference-contexts: Instead we use a much reduced space of either appearance or feature vectors y = (y 1 : : : y m ) T to describe the visual state during manipulation. 3 More details on the visual-motor model estimation and control can be found in <ref> [17] </ref> In case of feature based measures the vector y consists of the image coordinate locations of tracked points or geometrical constructs based on tracking. We use real time visual feature trackers of three different kinds. Deformable contours snakes [4] are used to track surface discontinuities.
Reference: [18] <author> Corke P. I. </author> <title> High-Performance Visual Closed-Loop Robot Control PhD thesis U of Melbourne 1994. </title>
Reference-contexts: Visual servoing, when supplemented with on-line visual model estimation, fits into the active vision paradigm. 1 Terminology according to Weiss'[2] 2 For a review of this work we direct the reader to [10] or <ref> [18] </ref>. 1.2 An Integrated Approach to Uncalibrated Hand-Eye Coordination Our approach integrates three main functions into the hand-eye manipulation system. The three functions are model learning, movement and visual simulation. These functions run as concurrent processes. <p> At any time k we estimate a first order model, see Section 2.1, f (x) f (x k ) + J (x k )(x x k ). The model is valid around the current system configuration x k , and is described by the image <ref> [18] </ref> or visual-motor Jacobian defined as: (J j;i )(x k ) = @x i The image Jacobian not only relates visual changes to motor changes, as is exploited in visual feedback control, but also highly constrains the possible visual changes to the subspace y k+1 &lt; m of y k+1 =
Reference: [19] <author> Hosoda K. Asada M. </author> <title> Versatile Visual Servoing without Knowledge of True Jacobian Proc. </title> <booktitle> IROS 1994. </booktitle>
Reference: [20] <author> P. K. Allen, B. H. YoshimiA. </author> <title> Timcenko Hand-Eye Coordination for Robotics Tracking and Grasping In K. Hashimoto, editor, </title> <booktitle> Visual Servoing World Scientific 1994. </booktitle>
Reference: [21] <author> Hager G. Grunwald G. Toyama K. </author> <title> Feature-Based Visual Servoing and its Application to Telerobotics In V. Graefe, editor, Intelligent Robotic Systems Elsevier 1995. </title>
Reference-contexts: Several flavors of visual servoing exist, (e.g. [2, 3, 5, 6, 7, 10, 12, 14, 15, 18, 19, 20] 2 ). Visual models suitable for specifying simple visual alignments have also been studied <ref> [21, 11, 9, 8] </ref>. Learning by Watching. <p> Another use of synthesis and simulation is to let the operator perform a critical manipulation sequence completely in simulation, but with synthesized visual feedback. In previous work such systems have provided simulated visual feedback using CAD models and conventional computer graphics (eg. <ref> [21] </ref>). We present a view synthesis and computer animation method based on the same type of learned visual-motor models that are used in the visual servoing control. Here we show how to use it to animate robot motion based on models estimated while controlling a real robot.
Reference: [22] <author> Brooks R. </author> <title> Intelligence Without Reason AI memo nr 1293 MIT 1991. </title>
Reference-contexts: In this work we present a user interface in which the robot is instructed through visual pointing. Reactive Robotic Systems. The work on small mobile robots, and the subsumption architecture, pioneered by Brooks <ref> [22] </ref>, shows that agents can interact with a complex and changing environment without complete models. Instead of going through a fixed and inflexible sequence of actions, each action is triggered by a precondition perception that is sensitive to the environment.
Reference: [23] <author> Ikeuchi K. Suehiro T. </author> <title> Towards an Assembly Plan from Observation In Proc. </title> <booktitle> of Robotics and Automation 1992. </booktitle>
Reference-contexts: Visual models suitable for specifying simple visual alignments have also been studied [21, 11, 9, 8]. Learning by Watching. Model-based work in vision-based teaching of a robot, as pioneered by Kuniyoshi, (Seeing, Understanding and Doing Human Task [24]) and Ikeuchi (Assembly Plan From Observation <ref> [23] </ref>), offers an attractive way of teaching a robot by having it observe a human carrying out a task. We seek similar natural user interfaces, but in a model free framework, and without the symbolic AI based understanding part.
Reference: [24] <author> Kuniyoshi Y. Inaba M. Inoue H. </author> <title> Seeing, </title> <booktitle> Understanding and Doing Human Task In Proc. of Robotics and Automation 1992. </booktitle>
Reference-contexts: Visual models suitable for specifying simple visual alignments have also been studied [21, 11, 9, 8]. Learning by Watching. Model-based work in vision-based teaching of a robot, as pioneered by Kuniyoshi, (Seeing, Understanding and Doing Human Task <ref> [24] </ref>) and Ikeuchi (Assembly Plan From Observation [23]), offers an attractive way of teaching a robot by having it observe a human carrying out a task. We seek similar natural user interfaces, but in a model free framework, and without the symbolic AI based understanding part.
Reference: [25] <author> Ballard D. </author> <note> Animate Vision Journal of Artificial Intelligence nr 48 p57-86 1990. </note>
Reference-contexts: Active Vision. In an animate <ref> [25] </ref>, active, or behavioral vision system, the acquisition of visual information is not an independent, open-loop process, but instead depends on the active agent's interaction with the world.
Reference: [26] <author> Pook P., Ballard D. H., Teleassistance: </author> <booktitle> Contextual guidance for autonomous manipulation Proc. of AAAI, </booktitle> <month> Aug </month> <year> 1994. </year>
Reference-contexts: In tele-assistance, instead of directly controlling the motor actions of a robot, the human interacts with the robot using a deictic, high-level language. Pook and Ballard have implemented such a system using a hand sign language sensed by an exoskeleton <ref> [26] </ref>. In this work we present a user interface in which the robot is instructed through visual pointing. Reactive Robotic Systems. The work on small mobile robots, and the subsumption architecture, pioneered by Brooks [22], shows that agents can interact with a complex and changing environment without complete models. <p> In both a tele-operated and a tele-assisted system a human operator is involved in controlling the manipulation. It has been shown that delays in the system significantly degrade the tele-operators performance <ref> [26] </ref>. One use of our method is to generate synthesized, immediate visual feedback to the operator to compensate for system delays. Another use is to allow a robot operator to simulate a manioulation task off-line before executing it on a real manipulator.
Reference: [27] <author> Jacobsen S. Iversen E. Knutti D. Johnson R. Biggers K. </author> <booktitle> Design of the Utah/MIT Dextrous Hand Proc. of Robotics and Automation 1986 </booktitle>
Reference-contexts: The problem of making robot manipulators solve human like hand-eye tasks is not due to lack of suitable manipulators. Robot arms, and more recently robot hands have been built, inspired by and with freedoms similar to the human arm and hand <ref> [27] </ref>. Instead the problem lies mainly in the lack of a suitable programming and control methodology for these advanced robot arm and hand manipulators. In this paper we set out to develop such a methodology; one that will allow robot manipulators to solve everyday human tasks in hand-eye coordination.
Reference: [28] <author> Garcia, </author> <title> Zangwill Pathways to solutions, fixed points, and equilibria, </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: We use two ideas from optimization: (1) A trust region method [30] estimates the current model validity online, and controller response is restricted to be inside this region of trust. (2) A homotopy or path following method <ref> [28] </ref> is used to divide a potentially non-convex problem into several smaller convex problems by creating several subgoals along trajectories planned in visual space. The trust region method adjusts a parameter ff so that the controller never moves out of the validity region of the current Jacobian estimate.
Reference: [29] <author> Fletcher R. </author> <title> Practical Methods of Optimization Chichester, second ed. </title> <year> 1987 </year>
Reference-contexts: We want to update the Jacobian in such a way as to satisfy the most recent observation (secant condition): y measured = ^ J k+1 x The above condition is under-determined, and a family of Broyden updating formulas can be defined <ref> [29] </ref>.
Reference: [30] <author> Dahlquist G. Bjorck A. </author> <title> Numerical Methods Second Ed, </title> <publisher> Prentice Hall, </publisher> <month> 199x, </month> <type> preprint. </type>
Reference-contexts: In previous work we have had good results with an asymmetric rank 1 correction formula <ref> [10, 30] </ref>: ^ J k+1 = ^ J k + (y measured ^ J k x)x T x T x For a set of orthogonal movements about a point fx k ; k = 1 : : : ng eq. 6 is identical to the finite difference update (eq. 5) in <p> While previous work in visual servoing has mostly used proportional control, typically with slight modifications to account for some dynamics, we have found that a more sophisticated approach is beneficial. We use two ideas from optimization: (1) A trust region method <ref> [30] </ref> estimates the current model validity online, and controller response is restricted to be inside this region of trust. (2) A homotopy or path following method [28] is used to divide a potentially non-convex problem into several smaller convex problems by creating several subgoals along trajectories planned in visual space.
References-found: 30

