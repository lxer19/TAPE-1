URL: http://www.isi.edu/~sungdomo/papers/lcr98.ps.gz
Refering-URL: http://www.isi.edu/~sungdomo/publications.html
Root-URL: http://www.isi.edu
Email: fsungdomo,bso,mhallg@isi.edu  brm@cs.stanford.edu  
Phone: 2  
Title: A Case for Combining Compile-Time and Run-Time Parallelization  
Author: Sungdo Moon Byoungro So Mary W. Hall and Brian Murphy 
Address: Marina del Rey, CA 90292,  Stanford, CA 94305,  
Affiliation: 1 USC Information Sciences Institute  Dept. of Computer Science, Stanford University  
Abstract: This paper demonstrates that significant improvements to automatic parallelization technology require that existing systems be extended in two ways: (1) they must combine high-quality compile-time analysis with low-cost run-time testing; and, (2) they must take control flow into account during analysis. We support this claim with the results of an experiment that measures the safety of parallelization at run time for loops left unparallelized by the Stanford SUIF compiler's automatic parallelization system. We present results of measurements on programs from two benchmark suites - Specfp95 and Nas sample benchmarks which identify inherently parallel loops in these programs that are missed by the compiler. We characterize remaining parallelization opportunities, and find that most of the loops require run-time testing, analysis of control flow, or some combination of the two. We present a new compile-time analysis technique that can be used to parallelize most of these remaining parallel loops. This technique is designed to not only improve the results of compile-time parallelization, but also to produce low-cost, directed run-time tests that allow the system to defer binding of parallelization until run-time when safety cannot be proven statically. We call this approach predicated array data-flow analysis. We augment array data-flow analysis, which the compiler uses to identify independent and privatizable arrays, by associating with each array data-flow value a predicate. Predicated array data-flow analysis allows the compiler to derive "optimistic" data-flow values guarded by predicates; these predicates can be used to derive a run-time test guaranteeing the safety of parallelization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ammons, G., and Larus, J. R. </author> <title> Improving data-flow analysis with path profiles. </title> <booktitle> In Proceedings of the ACM SIGPLAN '98 Conference on Programming Language Design and Implementation (Montreal, </booktitle> <address> Canada, </address> <month> June </month> <year> 1998), </year> <pages> pp. 72-84. </pages>
Reference-contexts: Neither of the latter two prior works were designed with predicates as one of the data-flow analysis frameworks, and none of the three techniques derives run-time tests. Recently, additional approaches that, in some way, exploit control-flow information in data-flow analysis have been proposed <ref> [1, 5, 20] </ref>. Ammons and Larus's approach improves the precision of data-flow analysis along frequently taken control flow paths, called hot paths, by using profile information. Bodik et al. describe a demand-driven interprocedural correlation analysis that eliminates some branches by path specialization.
Reference: [2] <author> Blume, W., Doallo, R., Eigenmann, R., Grout, J., Hoeflinger, J., Lawrence, T., Lee, J., Padua, D., Paek, Y., Pottenger, B., Rauchw-erger, L., and Tu, P. </author> <title> Parallel programming with Polaris. </title> <booktitle> IEEE Computer 29, </booktitle> <month> 12 (December </month> <year> 1996), </year> <pages> 78-82. </pages>
Reference-contexts: 1 Introduction Parallelizing compilers are becoming increasingly successful at exploiting coarse-grain parallelism in scientific computations, as evidenced by recent experimental results from both the Polaris system at University of Illinois and from the Stan-ford SUIF compiler <ref> [2, 11] </ref>. While these results are impressive overall, some of ? This work has been supported by DARPA Contract DABT63-95-C-0118, a fellowship from AT&T Bell Laboratories, the Air Force Materiel Command and DARPA contract F30602-95-C-0098.
Reference: [3] <author> Blume, W., and Eigenmann, R. </author> <title> Performance analysis of parallelizing compilers on the Perfect Benchmark programs. </title> <booktitle> IEEE Transaction on Parallel and Distributed Systems 3, </booktitle> <month> 6 (November </month> <year> 1992), </year> <pages> 643-656. </pages>
Reference-contexts: This observation raises again questions that have been previously addressed by experiments in the early 90s <ref> [3, 21] </ref>: is the compiler exploiting all of the inherent parallelism in a set of programs, and if not, can we identify the techniques needed to exploit remaining parallelism opportunities? These earlier experiments motivated researchers and developers of paralleliz-ing compilers to begin incorporating techniques for locating coarse-grain parallelism, such as array <p> Parallelizing these loops improves the speedup from no speedup with the base compiler to a speedup of 1.2 with predicated array data-flow analysis. 5 Related Work A number of experiments in the early 90s performed hand parallelization of benchmark programs to identify opportunities to improve the effectiveness of parallelizing compilers <ref> [3, 21] </ref>. These experiments compared hand-parallelized programs to compiler-parallelized versions, pointing to the large gap between inherent parallelism in the programs and what commercial compilers of the time were able to exploit. Blume and Eigenmann performed such a comparison for 13 programs from the Perfect benchmark suite [3]. <p> These experiments compared hand-parallelized programs to compiler-parallelized versions, pointing to the large gap between inherent parallelism in the programs and what commercial compilers of the time were able to exploit. Blume and Eigenmann performed such a comparison for 13 programs from the Perfect benchmark suite <ref> [3] </ref>. They cited the need for compilers to incorporate array privatization and interprocedural analysis, among other things, to exploit a coarser granularity of parallelism. These early studies focused developers of commercial and research compilers to investigate incorporating these techniques, and now they are beginning to make their way into practice.
Reference: [4] <author> Blume, W. J. </author> <title> Symbolic Analysis Techniques for Effective Automatic Paralleliza-tion. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Further, now that these previously missing techniques are now performed automatically by the compiler, a new experiment can identify the next set of missing analysis techniques. Analysis techniques exploiting predicates have been developed for specific data-flow problems including constant propagation [26], type analysis [23], symbolic analysis for parallelization <ref> [10, 4] </ref>, and the array data-flow analysis described above [9, 24]. <p> Related to these array analysis techniques are approaches to enhance scalar symbolic analysis for par-allelization. Haghighat describes an algebra on control flow predicates [10] while Blume presents a method for combining control flow predicates with ranges of scalar variables <ref> [4] </ref>.
Reference: [5] <author> Bod ik, R., Gupta, R., and Soffa, M. L. </author> <title> Interprocedural conditional branch elimination. </title> <booktitle> In Proceedings of the ACM SIGPLAN '97 Conference on Programming Language Design and Implementation (Las Vegas, </booktitle> <address> Nevada, </address> <month> June </month> <year> 1997), </year> <pages> pp. 146-158. </pages>
Reference-contexts: Neither of the latter two prior works were designed with predicates as one of the data-flow analysis frameworks, and none of the three techniques derives run-time tests. Recently, additional approaches that, in some way, exploit control-flow information in data-flow analysis have been proposed <ref> [1, 5, 20] </ref>. Ammons and Larus's approach improves the precision of data-flow analysis along frequently taken control flow paths, called hot paths, by using profile information. Bodik et al. describe a demand-driven interprocedural correlation analysis that eliminates some branches by path specialization.
Reference: [6] <author> Cousot, P., and Cousot, R. </author> <title> Systematic design of program anaysis frameworks. </title> <booktitle> In Conference Record of the Sixth Annual ACM Symposium on Principles of Programming Languages (San Antonio, </booktitle> <address> Texas, </address> <month> January </month> <year> 1979), </year> <pages> pp. 269-282. </pages>
Reference-contexts: Cousot and Cousot describe a theoretical construction of a reduced cardinal power of two data-flow frameworks, in which a data-flow analysis is performed on the lattice of functions between the two original data-flow lattices, and this technique has been refined by Nielson <ref> [6, 16] </ref>. Neither of the latter two prior works were designed with predicates as one of the data-flow analysis frameworks, and none of the three techniques derives run-time tests. Recently, additional approaches that, in some way, exploit control-flow information in data-flow analysis have been proposed [1, 5, 20].
Reference: [7] <author> Goff, G. </author> <title> Practical techniques to augment dependence analysis in the presence of symbolic terms. </title> <type> Tech. Rep. </type> <institution> TR92-194, Dept. of Computer Science, Rice University, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: The notion of breaking conditions has been suggested by Goff and by Pugh and Wonnacott <ref> [7, 17] </ref>. We discuss how such conditions can be derived in much broader ways, and present how to use these conditions both to improve compile-time analysis or as the basis for run-time tests. The remainder of the paper is organized into three sections, related work and a conclusion. <p> For the loops in this category, it is straightforward to derive breaking conditions by extracting constraints on dependences directly from the dependence and privatization tests <ref> [7, 17] </ref>. - C+B: Identifies loops that require both of the previously described techniques. <p> In some cases, it is safe to parallelize a loop only for certain input data. A few researchers have considered how to derive breaking conditions on data dependences, conditions that would guarantee a data dependence does not hold at run time <ref> [7, 17] </ref>. In the example in Figure 2 (a), the loop is parallelizable under the run-time condition (m c or m = 0).
Reference: [8] <author> Gu, J., Li, Z., and Lee, G. </author> <title> Symbolic array dataflow analysis for array priva-tization and program parallelization. </title> <booktitle> In Proceedings of Supercomputing '95 (San Diego, </booktitle> <address> California, </address> <month> December </month> <year> 1995). </year>
Reference-contexts: A few existing techniques incorporate predicates, most notably guarded array data-flow analysis by Gu, Li, and Lee <ref> [8] </ref>. Our approach goes beyond previous work in several ways, but the most fundamental difference is the application of these predicates to derive run-time tests used to guard safe execution of parallelized versions of loops that the compiler cannot parallelize with static analysis alone. <p> While not in common practice, a few techniques refine their array data-flow analysis re sults in this way <ref> [8, 25] </ref>. - BC: Identifies certain loops whose safe parallelization depends on values of variables not known at compile time. <p> An MOFP solution could discover that the predicates for the definition and reference of help are equivalent; thus, none of array help is upwards exposed. Previous work by Gu, Li and Lee, and by Tu and Padua, produce a similar such solution <ref> [8, 25] </ref>. Predicate Embedding. In examples such as in Figure 1 (b), most compilers would assume that the element help [0] is upwards exposed because the loop assigns to only help [1 : d] but it possibly references help [j 1] and j ranges from 1 to d.
Reference: [9] <author> Gu, J., Li, Z., and Lee, G. </author> <title> Experience with efficient array data-flow analysis for array privatization. </title> <booktitle> In Proceedings of the Sixth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (Las Vegas, </booktitle> <address> Nevada, </address> <month> June </month> <year> 1997), </year> <pages> pp. 157-167. </pages>
Reference-contexts: Analysis techniques exploiting predicates have been developed for specific data-flow problems including constant propagation [26], type analysis [23], symbolic analysis for parallelization [10, 4], and the array data-flow analysis described above <ref> [9, 24] </ref>.
Reference: [10] <author> Haghighat, M. R. </author> <title> Symbolic Analysis for Parallelizing Compilers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Further, now that these previously missing techniques are now performed automatically by the compiler, a new experiment can identify the next set of missing analysis techniques. Analysis techniques exploiting predicates have been developed for specific data-flow problems including constant propagation [26], type analysis [23], symbolic analysis for parallelization <ref> [10, 4] </ref>, and the array data-flow analysis described above [9, 24]. <p> Related to these array analysis techniques are approaches to enhance scalar symbolic analysis for par-allelization. Haghighat describes an algebra on control flow predicates <ref> [10] </ref> while Blume presents a method for combining control flow predicates with ranges of scalar variables [4].
Reference: [11] <author> Hall, M. W., Anderson, J. M., Amarasinghe, S. P., Murphy, B. R., Liao, S.-W., Bugnion, E., and Lam, M. S. </author> <title> Maximizing multiprocessor performance with the SUIF compiler. </title> <booktitle> IEEE Computer 29, </booktitle> <month> 12 (December </month> <year> 1996), </year> <pages> 84-89. </pages>
Reference-contexts: 1 Introduction Parallelizing compilers are becoming increasingly successful at exploiting coarse-grain parallelism in scientific computations, as evidenced by recent experimental results from both the Polaris system at University of Illinois and from the Stan-ford SUIF compiler <ref> [2, 11] </ref>. While these results are impressive overall, some of ? This work has been supported by DARPA Contract DABT63-95-C-0118, a fellowship from AT&T Bell Laboratories, the Air Force Materiel Command and DARPA contract F30602-95-C-0098. <p> In a previous publication, SUIF achieved a speedup on seven of the Specfp95 programs; of these seven, su2cor achieved a speedup of only 4 on 8 processors of a Digital Alphaserver 8400 <ref> [11] </ref>. The remaining six obtained a speedup of more than 6. The programs apsi, wave5 and fpppp were the only three not to obtain a speedup. In the Nas benchmark suite, only buk and fftpde failed to achieve a speedup.
Reference: [12] <author> Hall, M. W., Murphy, B. R., Amarasinghe, S. P., Liao, S.-W., and Lam, M. S. </author> <title> Interprocedural analysis for parallelization. </title> <booktitle> In Proceedings of the 8th International Workshop on Languages and Compilers for Parallel Computing (Colum-bus, </booktitle> <address> Ohio, </address> <month> August </month> <year> 1995), </year> <pages> pp. 61-80. </pages>
Reference-contexts: If there are no upwards exposed read regions, privatization is safe. Otherwise, privatization is only possible if these upwards exposed read regions are not written by any other iteration. Predicated array data-flow analysis extends SUIF's existing array data-flow analysis implementation <ref> [12] </ref>. This analysis computes a four-tuple at each program region, h Read, Exposed, Write, MustWrite i, which are the set of array sections that may be read, may be upwards exposed, may be written and are always written, respectively.
Reference: [13] <author> Holley, L. H., and Rosen, B. K. </author> <title> Qualified data flow problems. </title> <booktitle> In Conference Record of the Seventh Annual ACM Symposium on Principles of Programming Languages (Las Vegas, </booktitle> <address> Nevada, </address> <month> January </month> <year> 1980), </year> <pages> pp. 68-82. </pages>
Reference-contexts: There are some similarities between our approach and much earlier work on data-flow analysis frameworks. Holley and Rosen describe a construction of qualified data-flow problems, akin to the MOFP solution, but with only a fixed, finite, disjoint set of predicates <ref> [13] </ref>. Cousot and Cousot describe a theoretical construction of a reduced cardinal power of two data-flow frameworks, in which a data-flow analysis is performed on the lattice of functions between the two original data-flow lattices, and this technique has been refined by Nielson [6, 16].
Reference: [14] <author> Irigoin, F. </author> <title> Interprocedural analyses for programming environments. </title> <booktitle> In Proceedings of the NSF-CNRS Workshop on Environment and Tools for Parallel Scientific Programming (September 1992). </booktitle>
Reference-contexts: By folding predicates into data-flow values, which we call predicate embedding, we can produce more precise data-flow values such as is achieved in the PIPS system by incorporating constraints derived from control-flow tests <ref> [14] </ref>. By deriving predi cates from operations on the data-flow values, which we call predicate extraction, we can obtain breaking conditions on dependences and for privatization, such that if the conditions hold, the loop can be parallelized. <p> The PIPS system includes such linear constraints in its data dependence and array data-flow analysis <ref> [14] </ref>. Predicated data-flow analysis that incorporates a predicate embedding operator can also derive this result and parallelize the loop at compile time. 3.3 Deriving Low-Cost Run-Time Parallelization Tests Breaking Conditions on Data Dependences. In some cases, it is safe to parallelize a loop only for certain input data.
Reference: [15] <author> Moon, S., Hall, M. W., and Murphy, B. R. </author> <title> Predicated array data-flow analysis for run-time parallelization. </title> <booktitle> In Proceedings of the 1998 ACM International Conference on Supercomputing (Melbourne, </booktitle> <address> Australia, </address> <month> July </month> <year> 1998), </year> <pages> pp. 204-211. </pages>
Reference-contexts: A more complete treatment is found elsewhere <ref> [15] </ref>. <p> While space considerations preclude a formal description of predicated array data-flow analysis, we touch on what modifications to an existing array data-flow analysis are required to realize this solution. The technique is described in more detail elsewhere <ref> [15] </ref>. 1. Analysis augments array data-flow values with predicates. Array data-flow analysis in the SUIF compiler already maintains, for a particular array, a set of regions of the array (instead of a single region that conservatively approximates all of the accesses within a loop).
Reference: [16] <author> Nielson, F. </author> <title> Expected forms of data flow analysis. In Programs as Data Objects, </title> <editor> H. Ganzinger and N. D. Jones, Eds., </editor> <volume> vol. </volume> <booktitle> 217 of Lecture Notes on Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1986, </year> <pages> pp. 172-191. </pages>
Reference-contexts: Cousot and Cousot describe a theoretical construction of a reduced cardinal power of two data-flow frameworks, in which a data-flow analysis is performed on the lattice of functions between the two original data-flow lattices, and this technique has been refined by Nielson <ref> [6, 16] </ref>. Neither of the latter two prior works were designed with predicates as one of the data-flow analysis frameworks, and none of the three techniques derives run-time tests. Recently, additional approaches that, in some way, exploit control-flow information in data-flow analysis have been proposed [1, 5, 20].
Reference: [17] <author> Pugh, W., and Wonnacott, D. </author> <title> Eliminating false data dependences using the Omega test. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation (San Francisco, </booktitle> <address> California, </address> <month> June </month> <year> 1992), </year> <pages> pp. 140-151. </pages>
Reference-contexts: The notion of breaking conditions has been suggested by Goff and by Pugh and Wonnacott <ref> [7, 17] </ref>. We discuss how such conditions can be derived in much broader ways, and present how to use these conditions both to improve compile-time analysis or as the basis for run-time tests. The remainder of the paper is organized into three sections, related work and a conclusion. <p> For the loops in this category, it is straightforward to derive breaking conditions by extracting constraints on dependences directly from the dependence and privatization tests <ref> [7, 17] </ref>. - C+B: Identifies loops that require both of the previously described techniques. <p> In some cases, it is safe to parallelize a loop only for certain input data. A few researchers have considered how to derive breaking conditions on data dependences, conditions that would guarantee a data dependence does not hold at run time <ref> [7, 17] </ref>. In the example in Figure 2 (a), the loop is parallelizable under the run-time condition (m c or m = 0). <p> Pugh and Wonnacott present an integer programming solution to derive such linear constraints on the scalar variables used in array accesses directly from the constraints on the data dependence problem <ref> [17] </ref>. Predicated analysis that incorporates a predicate extraction operator can also derive this predicate and use it to guard execution of a conditionally parallelized loop at run time.
Reference: [18] <author> Rauchwerger, L., and Padua, D. </author> <title> The LRPD test: Speculative run-time paral-lelization of loops with privatization and reduction parallelization. </title> <booktitle> In Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation (La Jolla, </booktitle> <address> California, </address> <month> June </month> <year> 1995), </year> <pages> pp. 218-232. </pages>
Reference-contexts: This paper empirically evaluates the remaining parallelism opportunities using an automatic run-time parallelization testing system. We augment the Lazy Privatizing Doall (LPD) test to instrument and test whether any of the candidate unparallelized loops in the program can be safely par-allelized at run time <ref> [18] </ref>. The LPD test determines whether remaining loops contain data dependences (different iterations access the same memory location, where at least one of the accesses is a write), and further, whether such dependences can be safely eliminated with privatization (whereby each processor accesses a private copy of the data). <p> Run-time parallelization techniques that use an inspector/executor model to test all access expressions and decide if parallelization is safe can be applied to these same loops, but such techniques can potentially introduce too much space and time overhead to make them profitable <ref> [18, 19] </ref>. The run-time tests introduced by predicated analysis are, by comparison, much simpler. Predicated data-flow analysis unifies in a single analysis technique several different approaches that combine predicates with array data-flow values. <p> Such dependences can be parallelized with a simple inspector/executor that only determines control flow paths taken through the loop (a control inspector). - IE: Identifies loops that can probably only be parallelized with an inspector/executor model <ref> [18] </ref>. <p> The only approach we know that could parallelize such loops is a speculative inspector/executor model, where the loop is parallelized speculatively, and the inspector is run concurrently with executing the loop <ref> [18] </ref>. The eight programs contain a total of 55 additional parallelizable loops found by the LPD test. (Note that this number contains only loops that were executed at run time.) The two programs apsi and su2cor have the most loops, nineteen. <p> Some previous work in run-time parallelization uses specialized techniques not based on data-flow analysis. An inspector/executor technique inspects array accesses at run time immediately prior to execution of the loop <ref> [18, 19] </ref>. The inspector decides whether to execute a parallel or sequential version of the loop. Predicated data-flow analysis instead derives run-time tests based on values of scalar variables that can be tested prior to loop execution.
Reference: [19] <author> Saltz, J. H., Mirchandaney, R., and Crowley, K. </author> <title> Run-time parallelization and scheduling of loops. </title> <booktitle> IEEE Transaction on Computers 40, </booktitle> <month> 5 (May </month> <year> 1991), </year> <pages> 603-612. </pages>
Reference-contexts: Run-time parallelization techniques that use an inspector/executor model to test all access expressions and decide if parallelization is safe can be applied to these same loops, but such techniques can potentially introduce too much space and time overhead to make them profitable <ref> [18, 19] </ref>. The run-time tests introduced by predicated analysis are, by comparison, much simpler. Predicated data-flow analysis unifies in a single analysis technique several different approaches that combine predicates with array data-flow values. <p> Some previous work in run-time parallelization uses specialized techniques not based on data-flow analysis. An inspector/executor technique inspects array accesses at run time immediately prior to execution of the loop <ref> [18, 19] </ref>. The inspector decides whether to execute a parallel or sequential version of the loop. Predicated data-flow analysis instead derives run-time tests based on values of scalar variables that can be tested prior to loop execution.
Reference: [20] <author> Sharma, S. D., Acharya, A., and Saltz, J. </author> <title> Defeered data-flow analysis: Algorithms, proofs and applications. </title> <type> Tech. Rep. </type> <institution> UMD-CS-TR-3845, Dept. of Computer Science, University of Maryland, </institution> <month> November </month> <year> 1997. </year>
Reference-contexts: Neither of the latter two prior works were designed with predicates as one of the data-flow analysis frameworks, and none of the three techniques derives run-time tests. Recently, additional approaches that, in some way, exploit control-flow information in data-flow analysis have been proposed <ref> [1, 5, 20] </ref>. Ammons and Larus's approach improves the precision of data-flow analysis along frequently taken control flow paths, called hot paths, by using profile information. Bodik et al. describe a demand-driven interprocedural correlation analysis that eliminates some branches by path specialization.
Reference: [21] <author> Singh, J. P., and Hennessy, J. L. </author> <title> An empirical investigation of the effectiveness and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing (April 1991). </booktitle>
Reference-contexts: This observation raises again questions that have been previously addressed by experiments in the early 90s <ref> [3, 21] </ref>: is the compiler exploiting all of the inherent parallelism in a set of programs, and if not, can we identify the techniques needed to exploit remaining parallelism opportunities? These earlier experiments motivated researchers and developers of paralleliz-ing compilers to begin incorporating techniques for locating coarse-grain parallelism, such as array <p> Parallelizing these loops improves the speedup from no speedup with the base compiler to a speedup of 1.2 with predicated array data-flow analysis. 5 Related Work A number of experiments in the early 90s performed hand parallelization of benchmark programs to identify opportunities to improve the effectiveness of parallelizing compilers <ref> [3, 21] </ref>. These experiments compared hand-parallelized programs to compiler-parallelized versions, pointing to the large gap between inherent parallelism in the programs and what commercial compilers of the time were able to exploit. Blume and Eigenmann performed such a comparison for 13 programs from the Perfect benchmark suite [3].
Reference: [22] <author> So, B., Moon, S., and Hall, M. W. </author> <title> Measuring the effectiveness of automatic parallelization in SUIF. </title> <booktitle> In Proceedings of the 1998 ACM International Conference on Supercomputing (Melbourne, </booktitle> <address> Australia, </address> <month> July </month> <year> 1998), </year> <pages> pp. 212-219. </pages>
Reference-contexts: We are thus able to locate all the loops in the program whose iterations can be safely executed in parallel for a particular program input, possibly requiring array privatization to create private copies of an array for each processor. The run-time parallelization system <ref> [22] </ref> augments the existing automatic parallelization system that is part of the Stanford SUIF compiler. We have evaluated the system on the Specfp95 and Nas sample benchmark suites. We used reference inputs for Specfp95 and the small inputs for Nas.
Reference: [23] <author> Strom, R. E., and Yellin, D. M. </author> <title> Extending typestate checking using conditional liveness analysis. </title> <journal> IEEE Transaction on Software Engineering 19, </journal> <month> 5 (May </month> <year> 1993), </year> <pages> 478-485. </pages>
Reference-contexts: Further, now that these previously missing techniques are now performed automatically by the compiler, a new experiment can identify the next set of missing analysis techniques. Analysis techniques exploiting predicates have been developed for specific data-flow problems including constant propagation [26], type analysis <ref> [23] </ref>, symbolic analysis for parallelization [10, 4], and the array data-flow analysis described above [9, 24].
Reference: [24] <author> Tu, P. </author> <title> Automatic Array Privatization and Demand-driven Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Analysis techniques exploiting predicates have been developed for specific data-flow problems including constant propagation [26], type analysis [23], symbolic analysis for parallelization [10, 4], and the array data-flow analysis described above <ref> [9, 24] </ref>. <p> Tu and Padua present a limited sparse approach on a gated SSA graph that is demand based, only examining predicates if they might assist in loop bounds or subscript values for parallelization, a technique that appears to be no more powerful than that of Gu, Li and Lee <ref> [24] </ref>. Related to these array analysis techniques are approaches to enhance scalar symbolic analysis for par-allelization. Haghighat describes an algebra on control flow predicates [10] while Blume presents a method for combining control flow predicates with ranges of scalar variables [4].
Reference: [25] <author> Tu, P., and Padua, D. </author> <title> Automatic array privatization. </title> <booktitle> In Proceedings of the 6th International Workshop on Languages and Compilers for Parallel Computing (Portland, </booktitle> <address> Oregon, </address> <month> August </month> <year> 1993), </year> <pages> pp. 500-521. </pages>
Reference-contexts: While not in common practice, a few techniques refine their array data-flow analysis re sults in this way <ref> [8, 25] </ref>. - BC: Identifies certain loops whose safe parallelization depends on values of variables not known at compile time. <p> An MOFP solution could discover that the predicates for the definition and reference of help are equivalent; thus, none of array help is upwards exposed. Previous work by Gu, Li and Lee, and by Tu and Padua, produce a similar such solution <ref> [8, 25] </ref>. Predicate Embedding. In examples such as in Figure 1 (b), most compilers would assume that the element help [0] is upwards exposed because the loop assigns to only help [1 : d] but it possibly references help [j 1] and j ranges from 1 to d.
Reference: [26] <author> Wegman, M. N., and Zadeck, F. K. </author> <title> Constant propagation with conditional branches. </title> <booktitle> ACM Transaction on Programming Languages and Systems 13, </booktitle> <month> 2 (April </month> <year> 1991), </year> <pages> 180-210. </pages>
Reference-contexts: Further, now that these previously missing techniques are now performed automatically by the compiler, a new experiment can identify the next set of missing analysis techniques. Analysis techniques exploiting predicates have been developed for specific data-flow problems including constant propagation <ref> [26] </ref>, type analysis [23], symbolic analysis for parallelization [10, 4], and the array data-flow analysis described above [9, 24].
References-found: 26

