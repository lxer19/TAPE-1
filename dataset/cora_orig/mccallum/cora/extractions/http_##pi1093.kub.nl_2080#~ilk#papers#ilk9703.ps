URL: http://pi1093.kub.nl:2080/~ilk/papers/ilk9703.ps
Refering-URL: http://ilk.kub.nl/~ilk/papers/abstracts.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: zavrel,walter,veenstra@kub.nl  
Title: Resolving PP attachment Ambiguities with Memory-Based Learning  
Author: Jakub Zavrel, Walter Daelemans, Jorn Veenstra 
Address: PO Box 90153, 5000 LE Tilburg, The Netherlands  
Affiliation: Computational Linguistics, Tilburg University  
Note: in: Proc. of Conference on Computational Natural Language Learning, pp. 136-144, ed. Mark Elison, Madrid 1997 (CoNLL97)ILK-03  
Abstract: In this paper we describe the application of Memory-Based Learning to the problem of Prepositional Phrase attachment disambiguation. We compare Memory-Based Learning, which stores examples in memory and generalizes by using intelligent similarity metrics, with a number of recently proposed statistical methods that are well suited to large numbers of features. We evaluate our methods on a common benchmark dataset and show that our method compares favorably to previous methods, and is well-suited to incorporating various unconventional representations of word patterns such as value difference metrics and Lexical Space.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., D. Kibler, and M. Albert. </author> <year> 1991. </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66. </pages>
Reference-contexts: The k-NN algorithm with this metric, and equal weighting for all features is called ib1 <ref> (Aha, Kibler, and Albert, 1991) </ref>. Usually k is set to 1. (X; Y ) = i=1 where: ffi (x i ; y i ) = 0 if x i = y i ; else 1 (2) This metric simply counts the number of (mis)matching feature values in both patterns.
Reference: <author> Brill, E. and P. Resnik. </author> <year> 1994. </year> <title> A rule-based approach to prepositional phrase attachment disambiguation. </title> <booktitle> In Proc. of 15th annual conference on Computational Linguistics. </booktitle>
Reference-contexts: This process is much more laborious than the automatic computation of IG-weights on the training set. The other methods for which results have been reported on this dataset include decision trees, Maximum Entropy (Ratnaparkhi, Reynar, and Roukos, 1994), and Error-Driven Transformation-Based Learning <ref> (Brill and Resnik, 1994) </ref>, 3 which 3 The results of Brill's method on the present benchmark were reconstructed by Collins and Brooks (1995). were clearly outperformed by both IB1 and IB1-IG, even though e.g. Brill & Resnik used more elaborate feature sets (words and WordNet classes).
Reference: <author> Cardie, Claire. </author> <year> 1996. </year> <title> Automatic feature set selection for case-based learning of linguistic knowledge. </title> <booktitle> In Proc. of Conference on Empirical Methods in NLP. </booktitle> <institution> University of Pennsylvania. </institution>
Reference-contexts: If no information about the importance of features is available, this is a reasonable choice. But if we have information about feature relevance, we can add linguistic bias to weight or select different features <ref> (Cardie, 1996) </ref>. An alternative, more empiricist, approach is to look at the behavior of features in the set of examples used for training. We can compute statistics about the relevance of features by looking at which features are good predictors of the class labels.
Reference: <author> Collins, M.J and J. Brooks. </author> <year> 1995. </year> <title> Prepositional phrase attachment through a backed-off model. </title> <booktitle> In Proc. of Third Workshop on Very Large Corpora, </booktitle> <address> Cambridge. </address>
Reference: <author> Cost, S. and S. Salzberg. </author> <year> 1993. </year> <title> A weighted nearest neighbour algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 57-78. </pages>
Reference: <author> Daelemans, W. </author> <year> 1995. </year> <title> Memory-based lexical acquisition and processing. </title> <editor> In P. Steffens, editor, </editor> <booktitle> Machine Translation and the Lexicon, volume 898 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, Berlin, </publisher> <pages> pages 85-98. </pages>
Reference: <author> Daelemans, W. </author> <year> 1996. </year> <title> Abstraction considered harmful: Lazy learning of language processing. </title> <booktitle> In Proc. of 6th Belgian-Dutch Conference on Machine Learning, </booktitle> <pages> pages 3-12. </pages> <month> Benelearn. </month>
Reference-contexts: Because language-processing tasks typically can only be described as a complex interaction of regularities, subregularities and (families of) exceptions, storing all empirical data as potentially useful in analogical extrapolation works better than extracting the main regularities and forgetting the individual examples <ref> (Daelemans, 1996) </ref>. 2.1 Analogy from Nearest Neighbors The techniques used are variants and extensions of the classic k-nearest neighbor (k-NN) classifier algorithm. The instances of a task are stored in a table, together with the associated "correct" output.
Reference: <author> Daelemans, Walter, Peter Berck, and Steven Gillis. </author> <year> 1996. </year> <title> Unsupervised discovery of phonological categories through supervised learning of morphological rules. </title> <booktitle> In Proc. of 16th Int. Conf. on Computational Linguistics, </booktitle> <pages> pages 95-100. </pages> <institution> Center for Sprogteknologi. </institution>
Reference-contexts: Because language-processing tasks typically can only be described as a complex interaction of regularities, subregularities and (families of) exceptions, storing all empirical data as potentially useful in analogical extrapolation works better than extracting the main regularities and forgetting the individual examples <ref> (Daelemans, 1996) </ref>. 2.1 Analogy from Nearest Neighbors The techniques used are variants and extensions of the classic k-nearest neighbor (k-NN) classifier algorithm. The instances of a task are stored in a table, together with the associated "correct" output.
Reference: <author> Daelemans, Walter and Antal van den Bosch. </author> <year> 1992. </year> <title> Generalisation performance of backpropagation learning on a syllabification task. </title> <booktitle> In Proc. of TWLT3: Connectionism and NLP, </booktitle> <pages> pages 27-37. </pages> <institution> Twente University. </institution>
Reference: <author> Dudani, S.A. </author> <year> 1976. </year> <title> The distance-weighted k-nearest neighbor rule. </title> <journal> In IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> volume SMC-6, </volume> <pages> pages 325-327. </pages>
Reference: <author> Franz, A. </author> <year> 1996. </year> <title> Learning pp attachment from corpus statistics. </title> <editor> In S. Wermter, E. Riloff, and G. Scheler, editors, </editor> <title> Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing, </title> <booktitle> volume 1040 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <pages> pages 188-202. </pages>
Reference: <author> Frazier, L. </author> <year> 1979. </year> <title> On Comprehending Sentences: Syntactic Parsing Strategies. </title> <type> Ph.d thesis, </type> <institution> University of Connecticut. </institution>
Reference-contexts: Psycholinguistic theories have resulted in disambiguation strategies which use syntactic information only, i.e. structural properties of the parse tree are used to choose between different attachment sites. Two principles based on syntactic information are Minimal Attachment (MA) and Late Closure (LC) <ref> (Frazier, 1979) </ref>. MA tries to construct the parse tree that has the fewest nodes, whereas LC tries to attach new constituents as low in the parse tree as possible. These strategies always choose the same attachment regardless of the lexical content of the sentence.
Reference: <author> Hindle, D. and M. Rooth. </author> <year> 1993. </year> <title> Structural ambiguity and lexical relations. </title> <journal> Computational Linguistics, </journal> <volume> 19 </volume> <pages> 103-120. </pages>
Reference: <author> Hughes, J. </author> <year> 1994. </year> <title> Automatically Acquiring a Classification of Words. </title> <type> Ph.d thesis, </type> <institution> School of Computer Studies, The University of Leeds. </institution>
Reference: <author> Marcus, M., B. Santorini, and M.A. Marcinkiewicz. </author> <year> 1993. </year> <title> Building a large annotated corpus of english: The penn treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 313-330. </pages>
Reference-contexts: The experiments in this section are conducted on a simplified version of the "full" PP-attachment problem, i.e. the attachment of a PP in the sequence: VP NP PP. The data consist of four-tuples of words, extracted from the Wall Street Journal Treebank <ref> (Marcus, Santorini, and Marcinkiewicz, 1993) </ref> by a group at IBM (Ratnaparkhi, Reynar, and Roukos, 1994). 2 They took all sentences that contained the pattern VP NP PP and extracted the head words from the constituents, yielding a V N1 P N2 pattern.
Reference: <author> Quinlan, J.R. </author> <year> 1993. </year> <title> c4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ratnaparkhi, A., J. Reynar, and S. Roukos. </author> <year> 1994. </year> <title> A maximum entropy model for prepositional phrase attachment. </title> <booktitle> In Workshop on Human Language Technology, </booktitle> <address> Plainsboro, NJ, March. ARPA. </address>
Reference-contexts: The data consist of four-tuples of words, extracted from the Wall Street Journal Treebank (Marcus, Santorini, and Marcinkiewicz, 1993) by a group at IBM <ref> (Ratnaparkhi, Reynar, and Roukos, 1994) </ref>. 2 They took all sentences that contained the pattern VP NP PP and extracted the head words from the constituents, yielding a V N1 P N2 pattern. <p> This process is much more laborious than the automatic computation of IG-weights on the training set. The other methods for which results have been reported on this dataset include decision trees, Maximum Entropy <ref> (Ratnaparkhi, Reynar, and Roukos, 1994) </ref>, and Error-Driven Transformation-Based Learning (Brill and Resnik, 1994), 3 which 3 The results of Brill's method on the present benchmark were reconstructed by Collins and Brooks (1995). were clearly outperformed by both IB1 and IB1-IG, even though e.g.
Reference: <author> Schutze, H. </author> <year> 1994. </year> <title> Distributional part-of-speech tagging. </title> <booktitle> In Proc. of 7th Conference of the Eu-ropean Chapter of the Association for Computational Linguistics, </booktitle> <address> Dublin, Ireland. </address>
Reference: <author> Shepard, R.N. </author> <year> 1987. </year> <title> Toward a universal law of generalization for psychological science. </title> <journal> Science, </journal> <volume> 237 </volume> <pages> 1317-1228. </pages>
Reference: <author> Stanfill, C. and D. Waltz. </author> <year> 1986. </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228, </pages> <month> December. </month>
Reference: <author> Wettschereck, D. </author> <year> 1994. </year> <title> A study of distance-based machine learning algorithms. </title> <type> Ph.d thesis, </type> <institution> Oregon State University. </institution>
Reference-contexts: Dudani further proposed the inverse distance weight (Equation 7), which has recently become popular in the MBL literature <ref> (Wettschereck, 1994) </ref>. In Equation 7, a small constant is usually added to the denominator to avoid division by zero. w j = d j of k, the number of nearest neighbors.
Reference: <author> Zavrel, J. and W. Daelemans. </author> <year> 1997. </year> <title> Memory-based learning: Using similarity for smoothing. </title> <booktitle> In Proc. of 35th annual meeting of the ACL, </booktitle> <address> Madrid. </address>
Reference: <author> Zavrel, J. and J. Veenstra. </author> <year> 1995. </year> <title> The language environment and syntactic word class acquisition. </title> <editor> In F. Wijnen and C. Koster, editors, </editor> <booktitle> Proc. of Groningen Assembly on Language Acquisition (GALA95), </booktitle> <address> Groningen. </address>
References-found: 23

