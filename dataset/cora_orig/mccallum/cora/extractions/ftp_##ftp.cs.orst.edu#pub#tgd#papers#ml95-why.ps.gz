URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/ml95-why.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Email: keb@comeng.chungnam.ac.kr  tgd@cs.orst.edu  
Title: Error-Correcting Output Coding Corrects Bias and Variance  
Author: Eun Bae Kong Thomas G. Dietterich 
Address: Taejon, 305-764, South Korea  Corvallis, OR 97331-3202  
Affiliation: Department of Computer Engineering Chungnam National University  Department of Computer Science Oregon State University  
Abstract: Previous research has shown that a technique called error-correcting output coding (ECOC) can dramatically improve the classification accuracy of supervised learning algorithms that learn to classify data points into one of k 2 classes. This paper presents an investigation of why the ECOC technique works, particularly when employed with decision-tree learning algorithms. It shows that the ECOC method| like any form of voting or committee|can reduce the variance of the learning algorithm. Furthermore|unlike methods that simply combine multiple runs of the same learning algorithm|ECOC can correct for errors caused by the bias of the learning algorithm. Experiments show that this bias correction ability relies on the non-local be havior of C4.5.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bakiri, G. </author> <year> (1991). </year> <title> Converting English text to speech: A machine learning approach. </title> <type> Tech. rep. </type> <institution> 91-30-2, Oregon State University, Corvallis, </institution> <address> OR. </address>
Reference: <author> Bates, J. M., & Granger, C. W. J. </author> <year> (1969). </year> <title> The combination of forecasts. Op. </title> <journal> Res. Qtly., </journal> <volume> 20, </volume> <pages> 319-325. </pages>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth. </publisher>
Reference-contexts: We can view this vector of feature values as a point in an n-dimensional feature space. Some learning algorithms, such as C4.5 (Quinlan, 1993a) and CART <ref> (Breiman, Friedman, Olshen, & Stone, 1984) </ref>, can solve such k-way classification problems directly. However, many learning algorithms are designed to solve binary (2-class) classification problems.
Reference: <author> Breiman, L. </author> <year> (1994). </year> <title> Bagging predictors. </title> <type> Tech. rep. 421, </type> <institution> Dept. of Statistics, Univ. of Cal., </institution> <address> Berke-ley, CA. </address>
Reference-contexts: Third, we discuss the conditions under which voting can reduce error and why the ECOC approach is a kind of voting. Fourth, we show that the ECOC method reduces both bias and variance while other methods such as bootstrap aggregating <ref> (Breiman, 1994) </ref> can only reduce variance. Finally, we report experiments that show that the differences in C4.5's bias errors across the various L problems depend on the non-local behavior of C4.5.
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proc. 10th Intl. Conf. Mach. Learn., </booktitle> <pages> pp. </pages> <address> 17-24 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Clemen, R. T. </author> <year> (1989). </year> <title> Combining forcasts: A review and annotated bibliography. </title> <journal> Int. J. of Forecasting, </journal> <volume> 5, </volume> <pages> 559-583. </pages>
Reference: <author> Dietterich, T. G., & Bakiri, G. </author> <year> (1991). </year> <title> Error-correcting output codes: A general method for improving multiclass inductive learning programs. </title> <booktitle> In Proc. AAAI-91, </booktitle> <pages> pp. 572-577. </pages> <publisher> AAAI Press/MIT Press. </publisher>
Reference: <author> Dietterich, T. G., & Bakiri, G. </author> <year> (1995). </year> <title> Solving mul-ticlass learning problems via error-correcting output codes. </title> <journal> J. of Art. Int. Res., </journal> <volume> 2, </volume> <pages> 263-286. </pages>
Reference: <author> Efron, B. </author> <year> (1978). </year> <title> Regression and ANOVA with zero-one data: Measures of residual variation. </title> <journal> J. Am. Stat. As., </journal> <volume> 73 (361), </volume> <pages> 113-121. </pages>
Reference-contexts: Extending notions of bias and variance to classification problems is not straightforward, and many alternative approaches are possible <ref> (Efron, 1978) </ref>. Our approach, which we have not previously seen, is based on idealized voting. Let A be a classifica tion learning algorithm, and let ^ f S i be the hypothesis produced by A when trained on training set S i .
Reference: <author> Hampshire II, J. B., & Waibel, A. H. </author> <year> (1990). </year> <title> A novel objective function for improved phoneme recognition using time-delay neural networks. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 1 (2), </volume> <pages> 216-228. </pages>
Reference: <author> Hansen, L., & Salamon, P. </author> <year> (1990). </year> <title> Neural network ensembles. </title> <journal> IEEE PAMI, </journal> <volume> 12, </volume> <pages> 993-1001. </pages>
Reference: <author> LeBlanc, M., & Tibshirani, R. </author> <year> (1993). </year> <title> Combining estimates in regression and classification. </title> <type> Tech. rep., </type> <institution> Dept. of Statistics, Univ. of Toronto. </institution>
Reference: <author> Makridakis, S., & Winkler, R. L. </author> <year> (1983). </year> <title> Averages of forecasts: Some empirical results. </title> <journal> Management Sci., </journal> <volume> 29 (9), </volume> <pages> 987-996. </pages>
Reference: <author> Meir, R. </author> <year> (1994). </year> <title> Bias, variance, and the combination of estimators; The case of linear least squares. </title> <type> Tech. rep., </type> <institution> Technion, Israel. </institution>
Reference: <author> Murphy, P., & Aha, D. </author> <year> (1994). </year> <title> UCI repository of machine learning databases [machine-readable data repository]. </title> <institution> Univ. of Cal., Irvine. </institution>
Reference-contexts: Dietterich & Bakiri (1991, 1995) have shown that the methods relative to the direct multiclass method using C4.5. Asterisk indicates difference is significant at the 0.05 level or better. The glass, vowel, soybean, audiology (standardized encoding), ISOLET, letter recognition, and NETtalk data sets are all from the Irvine repository <ref> (Murphy & Aha, 1994) </ref>. The POS task is to predict the part-of-speech of unknown words (Claire Cardie, personal communication) from their context. error-correcting output coding technique works very well with the decision-tree algorithm C4.5. Figure 1 compares the performance of C4.5 in eight domains.
Reference: <author> Nilsson, N. J. </author> <year> (1965). </year> <title> Learning Machines. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference-contexts: By contrast, the standard approach to converting a k-way classification problem into a set of binary classification problems is to define one function f i for each class, such that f i (x) = 1 if f (x) = c i and zero otherwise <ref> (see Nilsson, 1965) </ref>. We call this the one-per-class (OPC) method. During learning, a set of hypotheses, f ^ f 1 ; : : : ; ^ f k g, is learned.
Reference: <author> Perrone, M. P. </author> <year> (1993). </year> <title> Improving regression estimation: Averaging methods for variance reduction with extensions to general convex measure optimization. </title> <type> Ph.D. thesis, </type> <institution> Brown Univ. </institution>
Reference: <author> Perrone, M. P. </author> <year> (1994). </year> <title> Putting it all together: Methods for combining neural networks. </title> <editor> In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 6, </volume> <pages> 1188-1189. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Perrone, M. P., & Cooper, L. N. </author> <year> (1993). </year> <title> When networks disagree: Ensemble methods for hybrid neural networks. </title> <editor> In Mammone, R. J. (Ed.), </editor> <title> Neural networks for speech and image processing. </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1993a). </year> <title> C4.5: Programs for Empirical Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Fran-cisco, CA. </address>
Reference-contexts: It works by converting the k-class supervised learning problem into a large number L of two-class supervised learning problems. Any learning algorithm that can handle two-class problems, such as the decision tree algorithm C4.5 <ref> (Quinlan, 1993a) </ref>, can then be applied to learn each of these L problems. To classify a new (test) example, each of the L learned decision trees is evaluated. Then the ECOC method tells how to combine the results of these L evaluations to predict the class of the test example. <p> We will refer to the feature values describing example x i as a 1 (x i ); : : : ; a n (x i ). We can view this vector of feature values as a point in an n-dimensional feature space. Some learning algorithms, such as C4.5 <ref> (Quinlan, 1993a) </ref> and CART (Breiman, Friedman, Olshen, & Stone, 1984), can solve such k-way classification problems directly. However, many learning algorithms are designed to solve binary (2-class) classification problems.
Reference: <author> Quinlan, J. R. </author> <year> (1993b). </year> <title> Combining instance-based and model-based learning. </title> <editor> In Utgoff, P. E. (Ed.), </editor> <booktitle> Proc. Int. Conf. on Mach. Learn. </booktitle> <address> San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schapire, R. E. </author> <year> (1990). </year> <title> The strength of weak learn-ability. </title> <journal> Mach. Learn., </journal> <volume> 5 (2), </volume> <pages> 197-227. </pages>
Reference: <author> Wettschereck, D., & Dietterich, T. G. </author> <year> (1992). </year> <title> Improving the performance of radial basis function networks by learning center locations. </title> <editor> In Moody, J. E., Hanson, S. J., & Lippmann, R. P. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 4, </volume> <pages> 1133-1140. </pages> <publisher> Mor-gan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Zhang, X., Mesirov, J. P., & Waltz, D. L. </author> <year> (1992). </year> <title> Hybrid system for protein secondary structure prediction. </title> <journal> J. of Mol. Biol., </journal> <volume> 225. </volume>
References-found: 24

