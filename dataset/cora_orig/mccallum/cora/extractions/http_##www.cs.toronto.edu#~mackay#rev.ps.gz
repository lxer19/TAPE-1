URL: http://www.cs.toronto.edu/~mackay/rev.ps.gz
Refering-URL: http://131.111.48.24/mackay/README.html
Root-URL: 
Email: http://www.cs.utoronto.ca/~frey  http://wol.ra.phy.cam.ac.uk/mackay  
Title: A Revolution: Belief Propagation in Graphs With Cycles  
Author: Brendan J. Frey David J. C. MacKay 
Keyword: networks with cycles.  
Date: Dec. 1997.  
Note: In Advances in Neural Information Processing Systems 10, MIT Press: Cambridge, MA, 1998. Presented at the Neural Information Processing Systems Conference, Denver Colorado,  
Address: Cambridge University  
Affiliation: Department of Computer Science University of Toronto  Department of Physics, Cavendish Laboratory  
Abstract: Until recently, artificial intelligence researchers have frowned upon the application of probability propagation in Bayesian belief networks that have cycles. The probability propagation algorithm is only exact in networks that are cycle-free. However, it has recently been discovered that the two best error-correcting decoding algorithms are actually performing probability propagation in belief 
Abstract-found: 1
Intro-found: 1
Reference: <author> C. Berrou and A. </author> <month> Glavieux </month> <year> 1996. </year> <title> Near optimum error correcting coding and decoding: </title> <journal> Turbo-codes. IEEE Transactions on Communications 44, </journal> <pages> 1261-1271. </pages>
Reference-contexts: when decoded as described above. (See MacKay and Neal (1996) for details.) It is impressively close to Shannon's limit | significantly closer than the "Concatenated Code" (described in Lin and Costello (1983)) which was considered the best practical code until recently. 4 Another leap: Turbocodes The codeword for a turbocode <ref> (Berrou et al. 1996) </ref> consists of the original information vector, plus two sets of bits used to protect the information. Each of these two sets is produced by feeding the information bits into a linear feedback shift register (LFSR), which is a type of finite state machine.
Reference: <author> G. F. </author> <title> Cooper 1990. The computational complexity of probabilistic inference using Bayesian belief networks. </title> <booktitle> Artificial Intelligence 42, </booktitle> <pages> 393-405. </pages>
Reference-contexts: as described above, using a fixed number (18) of iterations. (See Frey (1998) for details.) Its performance is significantly closer to Shannon's limit than the performances of both the low-density parity-check code and the textbook standard "Concatenated Code". 5 Open questions We are certainly not claiming that the NP-hard problem <ref> (Cooper 1990) </ref> of probabilistic inference in general Bayesian networks can be solved in polynomial time by probability propagation. However, the results presented in this paper do show that there are practical problems which can be solved using approximate inference in graphs with cycles.
Reference: <author> B. J. </author> <title> Frey 1998. Bayesian Networks for Pattern Classification, Data Compression and Channel Coding, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <note> See http://www.cs.utoronto.ca/~frey. </note>
Reference: <author> R. G. </author> <title> Gallager 1963. Low-Density Parity-Check Codes, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> S. Lin and D. J. Costello, Jr. </author> <year> 1983. </year> <title> Error Control Coding: Fundamentals and Applications, </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> S. L. Lauritzen and D. J. </author> <month> Spiegelhalter </month> <year> 1988. </year> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society B 50, </journal> <pages> 157-224. </pages>
Reference: <author> D. J. C. MacKay and R. M. </author> <title> Neal 1995. Good codes based on very sparse matrices. In Cryptography and Coding. </title> <booktitle> 5th IMA Conference, number 1025 in Lecture Notes in Computer Science, </booktitle> <pages> 100-111, </pages> <publisher> Springer, </publisher> <address> Berlin Germany. </address>
Reference-contexts: Compared to other approximate inference techniques such as variational methods, probability propagation in graphs with cycles is unprincipled. How well do more principled decoders work? In <ref> (MacKay and Neal 1995) </ref>, a variational decoder that maximized a lower bound on Q K k=1 P (u k jy) was presented for low-density parity-check codes. However, it was found that the performance of the variational decoder was not as good as the performance of the probability propagation decoder.
Reference: <author> D. J. C. MacKay and R. M. </author> <title> Neal 1996. Near Shannon limit performance of low density parity check codes. </title> <journal> Electronics Letters 32, </journal> <pages> 1645-1646. </pages> <note> Due to editing errors, reprinted in E lectronics Letters 33, 457-458. </note>
Reference: <author> J. </author> <title> Pearl 1988. Probabilistic Reasoning in Intelligent Systems, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: However, for the more powerful codes discussed below, exact computations are intractable. Instead, one way the decoder can approximate the probabilities P (u k jy) is by applying the probability propagation algorithm <ref> (Pearl 1988) </ref> to the Bayesian network.
Reference: <author> C. E. </author> <title> Shannon 1948. A mathematical theory of communication. </title> <journal> Bell System Technical Journal 27, </journal> <pages> 379-423, 623-656. </pages>
Reference: <author> P. Smyth, D. Heckerman, and M. I. </author> <title> Jordan 1997. Probabilistic independence networks for hidden Markov probability models. </title> <booktitle> Neural Computation 9, </booktitle> <pages> 227-270. </pages>
Reference-contexts: Next, messages are passed from the information variables to the first constituent chain, s 1 . Messages are passed forward and then backward along this chain, in the manner of the forward-backward algorithm <ref> (Smyth et al. 1997) </ref>. After messages are passed from the first chain to the second chain s 2 , the second chain is processed using the forward-backward algorithm. To complete the iteration, messages are passed from s 2 to the information bits.
References-found: 11

