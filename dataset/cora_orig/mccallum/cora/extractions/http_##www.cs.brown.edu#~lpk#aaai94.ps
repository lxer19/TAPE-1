URL: http://www.cs.brown.edu/~lpk/aaai94.ps
Refering-URL: 
Root-URL: 
Email: farc,lpk,mllg@cs.brown.edu  
Title: Acting Optimally in Partially Observable Stochastic Domains  
Author: Anthony R. Cassandra Leslie Pack Kaelbling and Michael L. Littman 
Address: Providence, RI 02912  
Affiliation: Department of Computer Science Brown University  
Abstract: In this paper, we describe the partially observable Markov decision process (pomdp) approach to finding optimal or near-optimal control strategies for partially observable stochastic environments, given a complete model of the environment. The pomdp approach was originally developed in the operations research community and provides a formal basis for planning problems that have been of interest to the AI community. We found the existing algorithms for computing optimal control strategies to be highly computationally inefficient and have developed a new algorithm that is empirically more efficient. We sketch this algorithm and present preliminary results on several small problems that illustrate important properties of the pomdp approach. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Astrom, K. J. </author> <year> 1965. </year> <title> Optimal control of markov decision processes with incomplete state estimation. </title> <journal> J. Math. Anal. Appl. </journal> <volume> 10 </volume> <pages> 174-205. </pages>
Reference-contexts: Because of the way the state estimation module is constructed, it is not possible for the agent to purposely delude itself into believing that it is in a good state when it is not. The belief mdp is Markov <ref> (Astrom 1965) </ref>, that is, having information about previous belief states cannot improve the choice of action. Most importantly, if an agent adopts the optimal policy for the belief mdp, the resulting behavior will be optimal for the partially observable process.
Reference: <author> Barto, A. G.; Bradtke, S. J.; and Singh, S. P. </author> <year> 1991. </year> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report 91-57, </type> <institution> Department of Computer and Information Science, University of Massachusetts, Amherst, Massachusetts. </institution>
Reference-contexts: We will solve larger examples including tracking and surveillance problems. In addition, we hope to extend this work in a number of directions such as applying stochastic dynamic programming <ref> (Barto, Bradtke, & Singh 1991) </ref> and function approximation to derive an optimal value function, rather than solving for it analytically. We expect that good approximate policies may be found more quickly this way.
Reference: <author> Bellman, R. </author> <year> 1957. </year> <title> Dynamic Programming. </title> <publisher> Princeton, </publisher> <address> New Jersey: </address> <publisher> Princeton University Press. </publisher>
Reference: <author> Cassandra, A. R.; Kaelbling, L. P.; and Littman, M. L. </author> <year> 1994. </year> <title> Algorithms for partially observable markov decision processes. </title> <type> Technical Report Forthcoming, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island. </address>
Reference-contexts: The algorithmic challenge, then, is to find a b for which ^ V t (b) 6= V fl t (b) or to prove that no such b exists (i.e., that the approximation is perfect). The Witness algorithm <ref> (Cassandra, Kaelbling, & Littman 1994) </ref> defines a linear program that returns a single point that is a "witness" to the fact that ^ V t 6= V fl t . <p> In practice, this has resulted in vastly improved running times and the ability to run much larger example problems than existing pomdp algorithms. Details of the algorithm are outlined in a technical report <ref> (Cassandra, Kaelbling, & Littman 1994) </ref>. Representing Policies When value iteration converges, we are left with a set of vectors, V final , that constitutes an approximation to the optimal value function, V fl .
Reference: <author> Cheng, H.-T. </author> <year> 1988. </year> <title> Algorithms for Partially Observable Markov Decision Processes. </title> <type> Ph.D. Dissertation, </type> <institution> University of British Columbia, British Columbia, Canada. </institution>
Reference-contexts: Detailed algorithms have been developed for this problem (Smallwood & Sondik 1973; Monahan 1982; Cheng 1988) but are extremely inefficient. We describe a new algorithm, inspired by Cheng's linear support algorithm <ref> (Cheng 1988) </ref>, which both in theory and in practice seems to be more efficient than the others.
Reference: <author> Chrisman, L. </author> <year> 1992. </year> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 183-188. </pages> <address> San Jose, California: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: That is, act as if the uncertainty will be present for one action step, but that the environment will be completely observable thereafter. This approach, similar to one used by Chris-man <ref> (Chrisman 1992) </ref>, leads to policies that do not take actions to gain information and will therefore be suboptimal in many environments. The key to finding truly optimal policies in the partially observable case is to cast the problem as a completely observable continuous-space mdp. <p> Within the AI community, several of the issues addressed here have also been examined by researchers working on reinforcement learning. White head and Ballard (Whitehead & Ballard 1991) solve problems of partial observability through access to extra perceptual data. Chrisman <ref> (Chrisman 1992) </ref> and McCallum (McCallum 1993) describe algorithms for inducing a pomdp from interactions with the environment and use relatively simple approximations to the resulting optimal value function.
Reference: <author> Dean, T.; Kaelbling, L. P.; Kirman, J.; and Nichol-son, A. </author> <year> 1993. </year> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: We expect that good approximate policies may be found more quickly this way. Another aim is to integrate the pomdp framework with methods such as those used by Dean et al. <ref> (Dean et al. 1993) </ref> for finding approximately optimal policies quickly by considering only small regions of the search space. Acknowledgments Thanks to Lonnie Chrisman, Tom Dean and (indirectly) Ross Schachter for introducing us to pomdps.
Reference: <author> Howard, R. A. </author> <year> 1960. </year> <title> Dynamic Programming and Markov Processes. </title> <address> Cambridge, Massachusetts: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Cassandra's work was supported in part by National Science Foundation Award IRI-9257592. y Leslie Kaelbling's work was supported in part by a National Science Foundation National Young Investigator Award IRI-9257592 and in part by ONR Contract N00014-91-4052, ARPA Order 8225. z Michael Littman's work was supported by Bellcore. processes (mdps) <ref> (Howard 1960) </ref>. <p> The remaining difficulty is that belief space is continuous; the established algorithms for finding optimal policies in mdps work only in finite state spaces. In the following sections, we discuss the method of value iteration for finding optimal policies. Value Iteration Value iteration <ref> (Howard 1960) </ref> was developed for finding optimal policies for mdps. Since we have formulated the partially observable problem as an mdp over belief states, we can find optimal policies for pomdps in an analogous manner. The agent moves through the world according to its policy, collecting reward.
Reference: <author> Lovejoy, W. S. </author> <year> 1991. </year> <title> A survey of algorithmic methods for partially observed markov decision processes. </title> <journal> Annals of Operations Research 28(1) </journal> <pages> 47-65. </pages>
Reference-contexts: As the reliability of listening is made worse, strategies that listen more are found. Related Work There is an extensive discussion of pomdps in the operations research literature. Surveys by Monahan (Mon-ahan 1982) and Lovejoy <ref> (Lovejoy 1991) </ref> are good starting points. Within the AI community, several of the issues addressed here have also been examined by researchers working on reinforcement learning. White head and Ballard (Whitehead & Ballard 1991) solve problems of partial observability through access to extra perceptual data.
Reference: <author> McCallum, R. A. </author> <year> 1993. </year> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning. </booktitle> <address> Amherst, Massachusetts: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Within the AI community, several of the issues addressed here have also been examined by researchers working on reinforcement learning. White head and Ballard (Whitehead & Ballard 1991) solve problems of partial observability through access to extra perceptual data. Chrisman (Chrisman 1992) and McCallum <ref> (McCallum 1993) </ref> describe algorithms for inducing a pomdp from interactions with the environment and use relatively simple approximations to the resulting optimal value function.
Reference: <author> Monahan, G. E. </author> <year> 1982. </year> <title> A survey of partially observable markov decision processes: Theory, models, and algorithms. </title> <booktitle> Management Science 28(1) </booktitle> <pages> 1-16. </pages>
Reference: <author> Moore, R. C. </author> <year> 1985. </year> <title> A formal theory of knowledge and action. </title> <editor> In Hobbs, J. R., and Moore, R. C., eds., </editor> <title> Formal Theories of the Commonsense World. </title> <address> Norwood, New Jersey: </address> <publisher> Ablex Publishing Company. </publisher>
Reference-contexts: This problem has been addressed in the artificial intelligence (AI) community using formalisms of epis-temic logic and by incorporating knowledge preconditions and effects into their planners <ref> (Moore 1985) </ref>. These solutions are applicable to fairly high-level problems in which the environment is assumed to be completely deterministic, an assumption that often fails in low-level control problems.
Reference: <author> Papadimitriou, C. H., and Tsitsiklis, J. N. </author> <year> 1987. </year> <title> The complexity of markov decision processes. </title> <journal> Mathematics of Operations Research 12(3) </journal> <pages> 441-450. </pages>
Reference: <author> Smallwood, R. D., and Sondik, E. J. </author> <year> 1973. </year> <title> The optimal control of partially observable markov processes over a finite horizon. </title> <journal> Operations Research 21 </journal> <pages> 1071-1088. </pages>
Reference-contexts: Each of these vectors defines a region of the belief space such that a belief state is in a vector's region if its dot product with the vector is maximum. Thus, the vectors define a partition of belief space. It can be shown <ref> (Smallwood & Sondik 1973) </ref> that all state vectors that share a partition also share an optimal action, so a policy can be specified by a set of pairs, hff fl ; a fl i, where (b) = a fl if ff fl b ff b for all ff 2 V final
Reference: <author> Sondik, E. J. </author> <year> 1971. </year> <title> The Optimal Control of Partially Observable Markov Processes. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University, Stanford, California. </institution>
Reference-contexts: First of all, any finite horizon value function is piecewise linear and convex (Sondik 1971; Smallwood & Sondik 1973). In addition, for the infinite horizon, the value function can be approximated arbitrarily closely by a convex piecewise-linear function <ref> (Sondik 1971) </ref>. A representation that makes use of these properties was introduced by Sondik (Sondik 1971). Let V t be a set of jSj-dimensional vectors of real numbers. <p> In addition, for the infinite horizon, the value function can be approximated arbitrarily closely by a convex piecewise-linear function <ref> (Sondik 1971) </ref>. A representation that makes use of these properties was introduced by Sondik (Sondik 1971). Let V t be a set of jSj-dimensional vectors of real numbers. The optimal t-horizon value function can be written as: V t (b) = max b ff ; for some set V t .
Reference: <author> Sutton, R. S. </author> <year> 1990. </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning. </booktitle> <address> Austin, Texas: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Tan, M. </author> <year> 1991. </year> <title> Cost-sensitive reinforcement learning for adaptive classification and control. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: Chrisman (Chrisman 1992) and McCallum (McCallum 1993) describe algorithms for inducing a pomdp from interactions with the environment and use relatively simple approximations to the resulting optimal value function. Other relevant work in the AI community includes the work of Tan <ref> (Tan 1991) </ref> on inducing decision trees for performing low-cost identification of objects by selecting appropriate sensory tests. Future Work The results presented in this paper are preliminary. We intend, in the short term, to extend our algorithm to perform policy iteration, which is likely to be more efficient.
Reference: <author> Watkins, C. J. C. H., and Dayan, P. </author> <year> 1992. </year> <title> Q-learning. </title> <booktitle> Machine Learning 8(3) </booktitle> <pages> 279-292. </pages>
Reference-contexts: Constructing Optimal Policies Constructing an optimal policy can be quite difficult. Even specifying a policy at every point in the uncountable state space is challenging. One simple method is to find the optimal state-action value function, Q fl CO , for the completely observable mdp hS; A; T; Ri <ref> (Watkins & Dayan 1992) </ref>; then, given belief state b as input, generate action argmax a2A P CO (s; a). That is, act as if the uncertainty will be present for one action step, but that the environment will be completely observable thereafter. <p> Any piecewise-linear convex function can be expressed this way, but the particular vectors in V t can also be viewed as the values as sociated with different choices in the optimal policy, analogous to Watkins' Q-values <ref> (Watkins & Dayan 1992) </ref>; see (Cassandra, Kaelbling, & Littman 1994; Sondik 1971). The Witness Algorithm The task at each step in the value iteration algorithm is to find the set V t that represents V fl t given V t1 .
Reference: <author> Whitehead, S. D., and Ballard, D. H. </author> <year> 1991. </year> <title> Learning to perceive and act by trial and error. </title> <booktitle> Machine Learning 7(1) </booktitle> <pages> 45-83. </pages>
Reference-contexts: Surveys by Monahan (Mon-ahan 1982) and Lovejoy (Lovejoy 1991) are good starting points. Within the AI community, several of the issues addressed here have also been examined by researchers working on reinforcement learning. White head and Ballard <ref> (Whitehead & Ballard 1991) </ref> solve problems of partial observability through access to extra perceptual data. Chrisman (Chrisman 1992) and McCallum (McCallum 1993) describe algorithms for inducing a pomdp from interactions with the environment and use relatively simple approximations to the resulting optimal value function.
References-found: 19

