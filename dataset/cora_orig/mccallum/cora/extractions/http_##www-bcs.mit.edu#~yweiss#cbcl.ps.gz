URL: http://www-bcs.mit.edu/~yweiss/cbcl.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00390.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: yweiss@psyche.mit.edu  
Title: Belief Propagation and Revision in Networks with Loops  
Author: Yair Weiss 
Note: This publication can be retrieved by anonymous ftp to publications.ai.mit.edu. Copyright c Massachusetts Institute of Technology, 1997  
Address: E10-120, Cambridge, MA 02139, USA  
Date: 1616 November, 1997  155  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES  Dept. of Brain and Cognitive Sciences MIT  
Pubnum: A.I. Memo No.  C.B.C.L. Paper No.  
Abstract: Local belief propagation rules of the sort proposed by Pearl (1988) are guaranteed to converge to the optimal beliefs for singly connected networks. Recently, a number of researchers have empirically demonstrated good performance of these same algorithms on networks with loops, but a theoretical understanding of this performance has yet to be achieved. Here we lay a foundation for an understanding of belief propagation in networks with loops. For networks with a single loop, we derive an analytical relationship between the steady state beliefs in the loopy network and the true posterior probability. Using this relationship we show a category of networks for which the MAP estimate obtained by belief update and by belief revision can be proven to be optimal (although the beliefs will be incorrect). We show how nodes can use local information in the messages they receive in order to correct the steady state beliefs. Furthermore we prove that for all networks with a single loop, the MAP estimate obtained by belief revision at convergence is guaranteed to give the globally optimal sequence of states. The result is independent of the length of the cycle and the size of the state space. For networks with multiple loops, we introduce the concept of a "balanced network" and show simulation results comparing belief revision and update in such networks. We show that the Turbo code structure is balanced and present simulations on a toy Turbo code problem indicating the decoding obtained by belief revision at convergence is significantly more likely to be correct. This report describes research done at the Center for Biological and Computational Learning and the Department of Brain and Cognitive Sciences of the Massachusetts Institute of Technology. Support for the Center is provided in part by a grant from the National Science Foundation under contract ASC-9217041. YW was also supported by NEI R01 EY11005 to E. H. Adelson 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Benedetto, G. Montorsi, D. Divsalar, and F. Pol-lara. </author> <title> Soft-output decoding algorithms in iterative decoding of turbo codes. </title> <type> Technical Report 42-124, </type> <institution> JPL TDA, </institution> <year> 1996. </year>
Reference: [2] <author> C. Berrou, A. Glavieux, and P. Thitimajshima. </author> <title> Near shannon limit error-correcting coding and decoding: Turbo codes. </title> <booktitle> In Proc. IEEE International Communications Conference '93, </booktitle> <year> 1993. </year> <month> 13 </month>
Reference-contexts: Despite this fact, several groups [5, 8, 14] have recently reported excellent experimental results in inference in networks with loops by using Pearl's algorithm. Perhaps the most dramatic instance of this performance is in an error correcting code scheme known as "Turbo Codes" <ref> [2] </ref>. These codes have been described as "the most exciting and potentially important development in coding theory in many years" [9] and have recently been shown [4, 7] to utilize an algorithm equivalent to belief propagation in a network with loops.
Reference: [3] <author> D. P. Bertsekas. </author> <title> Dynamic Programming: Determin--istic and Stochastic Models. </title> <publisher> Prentice Hall, </publisher> <year> 1987. </year>
Reference-contexts: The belief revision rules described here are equivalent to those described by Pearl (1988) except for the normalization. In Hidden Markov Models, belief revision is equivalent to the Viterbi update rules [11], and is a special case of concurrent dynamic programming <ref> [3] </ref>. As can be seen from the previous discussion, update procedures of the type described by Pearl have been analyzed in many areas of optimization and applied mathematics. However, to the best of our knowledge, in all these contexts the network is assumed to be singly connected.
Reference: [4] <author> B.J. Frey and F.R. Kschischang. </author> <title> Probability propagation and iterative decoding. </title> <booktitle> In Proc. 34th Aller-ton Conference on Communications, Control and Computing, </booktitle> <year> 1996. </year>
Reference-contexts: Perhaps the most dramatic instance of this performance is in an error correcting code scheme known as "Turbo Codes" [2]. These codes have been described as "the most exciting and potentially important development in coding theory in many years" [9] and have recently been shown <ref> [4, 7] </ref> to utilize an algorithm equivalent to belief propagation in a network with loops. Although there is widespread agreement in the coding community that these codes "represent a genuine, and perhaps historic, breakthrough" [9] a theoretical understanding of their performance has yet to be achieved.
Reference: [5] <author> Brendan J. Frey. </author> <title> Bayesian Networks for Pattern Classification, Data Compression and Channel Coding. </title> <publisher> MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: P (A; B; CjE) = (A; B)(B; C)(C; A) (5) Pearl showed that his algorithm is not guaranteed to work for such a network and suggested various other ways to cope with loops [10]. Despite this fact, several groups <ref> [5, 8, 14] </ref> have recently reported excellent experimental results in inference in networks with loops by using Pearl's algorithm. Perhaps the most dramatic instance of this performance is in an error correcting code scheme known as "Turbo Codes" [2].
Reference: [6] <author> Arthur Gelb, </author> <title> editor. Applied Optimal Estimation. </title> <publisher> MIT Press, </publisher> <year> 1974. </year>
Reference-contexts: As pointed out by Pearl, the normalization step does not influence the final beliefs but ensures the stability of the message passing scheme. Special cases of these update rules are also functionally equivalent to the Baum Welch reestimation procedure in HMMs [11] and optimal smoothing <ref> [6] </ref>. Belief update will give the probability of every node being in a particular state given all the evidence. If, however, we set each node equal to the state that maximizes its belief function, the global sequence obtained will not necessarily maximize the posterior probability.
Reference: [7] <author> D. J. C. MacKay, R.J. McEliece, and J.F. Cheng. </author> <title> Turbo decoding as as an instance of pearl's `belief propagation' algorithm. </title> <journal> IEEE Journal on Selected Areas in Communication, </journal> <note> page in press, </note> <year> 1997. </year>
Reference-contexts: Perhaps the most dramatic instance of this performance is in an error correcting code scheme known as "Turbo Codes" [2]. These codes have been described as "the most exciting and potentially important development in coding theory in many years" [9] and have recently been shown <ref> [4, 7] </ref> to utilize an algorithm equivalent to belief propagation in a network with loops. Although there is widespread agreement in the coding community that these codes "represent a genuine, and perhaps historic, breakthrough" [9] a theoretical understanding of their performance has yet to be achieved.
Reference: [8] <author> D.J.C. Mackay and Radford M. Neal. </author> <title> Good error-correcting codes based on very sparse matrices. </title> <booktitle> In Cryptography and Coding - LNCS 1025. </booktitle> <year> 1995. </year>
Reference-contexts: P (A; B; CjE) = (A; B)(B; C)(C; A) (5) Pearl showed that his algorithm is not guaranteed to work for such a network and suggested various other ways to cope with loops [10]. Despite this fact, several groups <ref> [5, 8, 14] </ref> have recently reported excellent experimental results in inference in networks with loops by using Pearl's algorithm. Perhaps the most dramatic instance of this performance is in an error correcting code scheme known as "Turbo Codes" [2]. <p> The exact construction method of the unwrapped network is given in the appendix, but the basic idea is to replicate the evidence nodes as shown in the examples in figure 2. As we show in the appendix (see also <ref> [14, 8] </ref>), for any number of iterations of belief propagation in the loopy network, there exists an unwrapped network such that the messages received by a node in the unwrapped network are equivalent to those that would be received by a corresponding node in the loopy network.
Reference: [9] <author> R.J. McEliece, E. Rodemich, and J.F. Cheng. </author> <title> The turbo decision algorithm. </title> <booktitle> In Proc. 33rd Allerton Conference on Communications, Control and Computing, </booktitle> <year> 1995. </year>
Reference-contexts: Perhaps the most dramatic instance of this performance is in an error correcting code scheme known as "Turbo Codes" [2]. These codes have been described as "the most exciting and potentially important development in coding theory in many years" <ref> [9] </ref> and have recently been shown [4, 7] to utilize an algorithm equivalent to belief propagation in a network with loops. Although there is widespread agreement in the coding community that these codes "represent a genuine, and perhaps historic, breakthrough" [9] a theoretical understanding of their performance has yet to be <p> and potentially important development in coding theory in many years" <ref> [9] </ref> and have recently been shown [4, 7] to utilize an algorithm equivalent to belief propagation in a network with loops. Although there is widespread agreement in the coding community that these codes "represent a genuine, and perhaps historic, breakthrough" [9] a theoretical understanding of their performance has yet to be achieved. Here we lay a foundation for an understanding of belief propagation in networks with loops.
Reference: [10] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Furthermore, it does not exploit the decomposition of the probability function. The decomposition suggests that the problem should lend itself to parallel distributed processing at the nodes of the network. In 1986 J. Pearl <ref> [10] </ref> described local message passing schemes for inferring the states of the hidden nodes in these types of networks. The algorithm consists of simple local updates that can be executed in parallel and are guaranteed to converge to the correct answers. <p> P (A; B; CjE) = (A; B)(B; C)(C; A) (5) Pearl showed that his algorithm is not guaranteed to work for such a network and suggested various other ways to cope with loops <ref> [10] </ref>. Despite this fact, several groups [5, 8, 14] have recently reported excellent experimental results in inference in networks with loops by using Pearl's algorithm. Perhaps the most dramatic instance of this performance is in an error correcting code scheme known as "Turbo Codes" [2]. <p> The ordering of the nodes and links is somewhat arbitrary: for a Markov network such a representation can always be found but is not unique (e.g. <ref> [10] </ref>).
Reference: [11] <author> L.R. Rabiner. </author> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <journal> Proc. IEEE, </journal> <volume> 77(2) </volume> <pages> 257-286, </pages> <year> 1989. </year>
Reference-contexts: As pointed out by Pearl, the normalization step does not influence the final beliefs but ensures the stability of the message passing scheme. Special cases of these update rules are also functionally equivalent to the Baum Welch reestimation procedure in HMMs <ref> [11] </ref> and optimal smoothing [6]. Belief update will give the probability of every node being in a particular state given all the evidence. If, however, we set each node equal to the state that maximizes its belief function, the global sequence obtained will not necessarily maximize the posterior probability. <p> The belief revision rules described here are equivalent to those described by Pearl (1988) except for the normalization. In Hidden Markov Models, belief revision is equivalent to the Viterbi update rules <ref> [11] </ref>, and is a special case of concurrent dynamic programming [3]. As can be seen from the previous discussion, update procedures of the type described by Pearl have been analyzed in many areas of optimization and applied mathematics.
Reference: [12] <author> P. Smyth, D. Heckerman, and M. I. Jordan. </author> <title> Probabilistic independence networks for hidden markov probability models. </title> <booktitle> Neural Computation, </booktitle> <year> 1997. </year>
Reference-contexts: Since the publication of Pearl's algorithm, a number of variants of it have been published (see <ref> [12] </ref> for a review). The updates we describe here are functionally equivalent to those proposed by Pearl, but are easier to generalize to loopy networks. Informally, the message passing scheme proceeds as follows. Every node sends a probability vector to each of its neighbors.
Reference: [13] <author> A.J. </author> <title> Viterbi, A.M. Viterbi, and N.T. Sinhushayana. Interleaved concatenated codes: new perspectives on approaching the Shannon limit. </title> <booktitle> Proc. </booktitle> <institution> Natl Acad. Sci. USA, </institution> <month> 94 </month> <pages> 9525-9531, </pages> <year> 1997. </year>
Reference-contexts: Which of the two error rates should be used is, of course, application dependent. Traditionally block error rates are used for block codes and bit error rates are used for convolutional codes <ref> [13] </ref>. Here we are mostly interested in evaluating the error introduced by the suboptimal decoding algorithm. Thus we compare belief revision and belief update based on how often they agree with their corresponding optimal algorithms.

References-found: 13

