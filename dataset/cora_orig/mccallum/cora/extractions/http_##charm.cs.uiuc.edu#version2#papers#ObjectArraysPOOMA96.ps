URL: http://charm.cs.uiuc.edu/version2/papers/ObjectArraysPOOMA96.ps
Refering-URL: http://charm.cs.uiuc.edu/version2/papers/ObjectArraysPOOMA96.html
Root-URL: http://www.cs.uiuc.edu
Email: E-mail: fsanjeev,kaleg@cs.uiuc.edu  
Title: A Parallel Array Abstraction for Data-Driven Objects  
Author: Sanjeev Krishnan and Laxmikant V. Kale 
Address: Urbana, IL 61801  
Affiliation: Dept of Computer Science, University of Illinois,  
Abstract: We describe design and implementation of an abstraction for parallel arrays of data-driven objects. The arrays may be multi-dimensional, and the number of elements in an array is independent of the number of processors. The elements are mapped to processors by a user-controllable mapping function. The mapping may be changed during the parallel computation, which facilitates load balancing, and communication optimization, for example. Asynchronous method invocation is supported, with multicast, broadcast, and dimension-wide broadcast. The abstraction is illustrated using examples in fluid dynamics and molecular simulations.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <journal> Intl. Journal of Supercomputer Applications, </journal> <volume> 5(3), </volume> <year> 1996. </year>
Reference-contexts: NAS SP benchmark: 3 The NAS Scalar Pentadiagonal (SP) benchmark <ref> [1] </ref> is a Computational Fluid Dynamics program which uses an Alternating Direction Implicit (ADI) method to solve a system of partial differential equations. The computational space is a large three-dimensional cube consisting of an array of grid points, and the computation involves several iterations.
Reference: [2] <author> J. Board, L. V. Kale, K. Schulten, R. Skeel, and T. Schlick. </author> <title> Modeling biomolecules: Larger scales, longer durations. </title> <journal> IEEE Computational Science and Engineering, </journal> <volume> 1(4), </volume> <year> 1994. </year>
Reference-contexts: This overlap gives significant performance advantages over the traditional loosely-synchronous (separate phases of computation and communication) implementations. Parallel molecular dynamics: As another example, consider the parallel molecular dynamics simulation program NAMD <ref> [2] </ref>. The program consists of a three-dimensional computational space which is divided into cubical "cells" or "patches". The patches are distributed over processors by a sophisticated algorithm which maintains load balance as well as reduces communication. Patches need to exchange data (such as atomic parameters and forces) with each other.
Reference: [3] <author> F. Bodin, P. Beckman, D. Gannon, and S. Narayana, S. </author> <title> an d Yang. Distributed pC++: Basic Ideas for an Object Parallel Langua ge. </title> <journal> Scientific Programming, </journal> <volume> 2(3), </volume> <year> 1993. </year>
Reference-contexts: Some parallel object-oriented languages provide constructs similar to parallel arrays. Concurrent Aggregates [5] allows the programmer to create an aggregate of objects which has a common global name. Other objects interact with this aggregate as a whole, instead of individual objects. Collections in pC++ <ref> [3] </ref> provides parallel arrays with an HPF-like loosely-synchronous object-parallel model. Branched chares [8] are a related construct provided in Charm++, which are essentially parallel arrays with exactly one element per processor.
Reference: [4] <author> J. Bruno and P. Capello. </author> <title> Implementing the Beam and Warming method on the hypercube. </title> <booktitle> In Proceedings of the 3rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: Thus there are a total of three transpose operations needed per iteration. The advantage of this method is that computations within the sweep are hence completely local to a processor. However, the transpose operation between sweeps can result in significant overhead. * The multi-partition method <ref> [9, 4] </ref> : the array is divided into cubes, and each cube is assigned to a processor such that no two cubes with the same X, Y or Z coordinates are assigned to the same processor.
Reference: [5] <author> A. Chien. </author> <title> Concurrent Aggregates. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Some parallel object-oriented languages provide constructs similar to parallel arrays. Concurrent Aggregates <ref> [5] </ref> allows the programmer to create an aggregate of objects which has a common global name. Other objects interact with this aggregate as a whole, instead of individual objects. Collections in pC++ [3] provides parallel arrays with an HPF-like loosely-synchronous object-parallel model.
Reference: [6] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification (Draft), </title> <address> 1.0 edition, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: An extreme case of object-based parallelism occurs when almost identical operations are applied to all array elements in a synchronous | lock-step | manner; this case is well supported by data parallel languages such as HPF <ref> [6] </ref>. However, many applications require going beyond such lock-step mode. The progression through the life-cycle for each element object may be different, depending on the needs of the application, although all the elements object of an array may share the same structure of data and sets of methods. <p> The dynamic remapping facility provided with parallel arrays is particularly useful for moving objects in order to balance load periodically. 4 Previous work Parallel arrays have been used in various forms in several parallel programming systems. In data-parallel languages such as HPF <ref> [6] </ref> programmers have a shared-memory model in which arrays in the program are distributed over processors using compiler directives, and computations are assigned to processors using the "owner-computes" rule.
Reference: [7] <author> L. Kale, M. Bhandarkar, N. Jagathesan, S. Krishnan, and J. Yelon. </author> <title> Converse: An Interoperable Framework for Parallel Programming. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: It has been implemented as an extension to the Charm++ [8] language. Charm++ supports message driven objects, and provides several useful features for parallel programming. In addition to enhancing the utility of Charm++ , the new abstraction can be used directly in C++ based parallel programs, using the Converse <ref> [7] </ref> interoperability framework. 2 Description A parallel array is a group of objects (the array elements) with a common global name (id), which are organized in a multidimensional, distributed array, with each array element identified by its coordinates. <p> Note: the pack and unpack functions are virtual functions defined in the base class array. 5. Forward messages directed to the object from the old processor to the new processor. 2.5 Implementation The parallel array library is implemented on top of the Converse interoperable run-time framework <ref> [7] </ref>. The library can thus be used in conjunction with modules written in other programming systems such as PVM and MPI. Although the parallel array concepts we developed were implemented in the context of the Charm++ parallel object-oriented language, the essential features are language-independent.
Reference: [8] <author> L. Kale and S. Krishnan. </author> <title> Charm++ : A portable concurrent object oriented system based on C++. </title> <booktitle> In Proceedings of the Conference on Object Oriented Programmi ng Systems, Languages and Applications, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: This paper describes a parallel array-object abstraction that supports such applications. It supports asynchronous method invocation via message driven execution, which leads to efficient programs that can adaptively tolerate communication latencies. It has been implemented as an extension to the Charm++ <ref> [8] </ref> language. Charm++ supports message driven objects, and provides several useful features for parallel programming. <p> Concurrent Aggregates [5] allows the programmer to create an aggregate of objects which has a common global name. Other objects interact with this aggregate as a whole, instead of individual objects. Collections in pC++ [3] provides parallel arrays with an HPF-like loosely-synchronous object-parallel model. Branched chares <ref> [8] </ref> are a related construct provided in Charm++, which are essentially parallel arrays with exactly one element per processor.
Reference: [9] <author> N. H. Naik, V. K. Naik, and M. Nicoules. </author> <title> Parallelization of a class of implicit finite difference schemes in computational fluid dynamics. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 5(1), </volume> <year> 1993. </year> <month> 5 </month>
Reference-contexts: Thus there are a total of three transpose operations needed per iteration. The advantage of this method is that computations within the sweep are hence completely local to a processor. However, the transpose operation between sweeps can result in significant overhead. * The multi-partition method <ref> [9, 4] </ref> : the array is divided into cubes, and each cube is assigned to a processor such that no two cubes with the same X, Y or Z coordinates are assigned to the same processor.
References-found: 9

