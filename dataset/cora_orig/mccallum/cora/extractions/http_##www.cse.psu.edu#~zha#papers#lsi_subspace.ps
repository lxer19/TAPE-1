URL: http://www.cse.psu.edu/~zha/papers/lsi_subspace.ps
Refering-URL: http://www.cse.psu.edu/~zha/papers.html
Root-URL: http://www.cse.psu.edu
Title: A SUBSPACE-BASED MODEL FOR INFORMATION RETRIEVAL WITH APPLICATIONS IN LATENT SEMANTIC INDEXING  
Author: HONGYUAN ZHA 
Abstract: A theoretical foundation for latent semantic indexing (LSI) is proposed by adapting a model first used in array signal processing to the context of information retrieval using the concept of subspaces. It is shown that this subspace-based model coupled with minimal description length (MDL) principle leads to a statistical test to determine the dimensions of the latent-concept subspaces in LSI. The effect of weighting on the choice of the optimal dimensions of latent-concept subspaces is illustrated. It is also shown that the model imposes a so-called low-rank-plus-shift structure that is approximately satisfied by the cross-product of the term-document matrices. This structure can be exploited to give a more accurate updating scheme for LSI and to correct some of the misconception about the achievable retrieval accuracy in LSI updating. It is further demonstrated that based on the low-rank-plus-shift structure a divide-and-conquer method can be devised to compute the partial singular value decomposition (SVD) of a large sparse term-document matrix. Possible extensions of the model to improve retrieval accuracy are also pointed out. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> B.T. Bartell, G.W. Cottrell, and R.K. Belew. </editor> <title> Latent Semantic Indexing is an optimal special case of multidimensional scaling. </title> <booktitle> Proc. </booktitle> <editor> SIGIR-92, N. Belkin, editor, </editor> <address> New York, 1992. </address> <publisher> ACM Press. </publisher>
Reference-contexts: There are also text collections for which LSI is not significantly better than the much simpler original vector-space model. However, very little theoretical results have been derived for LSI except some explanation of its effectiveness based on multidimensional scaling and multiple regression <ref> [1, 2, 19] </ref>. The purpose of this paper is to place LSI on a y 307 Pond Laboratory, Department of Computer Science and Engineering, The Pennsylvania State University, University Park, PA 16802-6103. E-mail: zha@cse.psu.edu. Fax: (814) 865-3176.
Reference: [2] <editor> B.T. Bartell, G.W. Cottrell, and R.K. Belew. </editor> <title> Representing Documents Using an Explicit Model of Their Similarities. </title> <journal> Journal of American Society for Information Science, </journal> <volume> 46 </volume> <pages> 254-271, </pages> <year> 1995. </year>
Reference-contexts: There are also text collections for which LSI is not significantly better than the much simpler original vector-space model. However, very little theoretical results have been derived for LSI except some explanation of its effectiveness based on multidimensional scaling and multiple regression <ref> [1, 2, 19] </ref>. The purpose of this paper is to place LSI on a y 307 Pond Laboratory, Department of Computer Science and Engineering, The Pennsylvania State University, University Park, PA 16802-6103. E-mail: zha@cse.psu.edu. Fax: (814) 865-3176.
Reference: [3] <author> M.W. Berry, S.T. Dumais and G.W. O'Brien. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37 </volume> <pages> 573-595, </pages> <year> 1995. </year>
Reference-contexts: Corresponding to each of the k reduced dimensions is associated a latent-concept which may not have any explicit semantic content yet helps to discriminate documents <ref> [3, 5] </ref>. The effectiveness of LSI measured by, for example, increased average precision, has been demonstrated for several text collections [5, 7, 11, 13, 14]. There are also text collections for which LSI is not significantly better than the much simpler original vector-space model. <p> We now turn to the discussion of updating problems in LSI: In rapidly changing environments such as the World Wide Web, the document collection is frequently updated with new documents and terms constantly being added, and there is a need to find the latent-concept subspaces for the updated document collections <ref> [3] </ref>. Let A 2 R mfin be the original term-document matrix, and A k = P k k Q T k be the best rank-k approximation of A. There are three types of updating problems in LSI [3]. <p> is a need to find the latent-concept subspaces for the updated document collections <ref> [3] </ref>. Let A 2 R mfin be the original term-document matrix, and A k = P k k Q T k be the best rank-k approximation of A. There are three types of updating problems in LSI [3]. Here we concentrate only on updating documents:[18] Let D 2 R mfip be the p new documents. <p> Then the best rank-k approximation of B is given by B k j ([P k ; ^ P k ]U k ) ^ k Q k 0 T In <ref> [3, 16] </ref>, only [ k ; P T k D] instead of ^ B in (3.7) is used to construct the SVD of B. The R matrix in ^ B is completely discarded. <p> In Table 1, k = 100, p is the number of new documents added, Meth 1 is the updating algorithm discussed above and Meth 2 is that used in <ref> [3, 16] </ref>. Row 3 and row 4 of the table gives the average precisions in percentage. As is expected Meth 1 performs much better than Meth 2 for those seven combinations of p and s. <p> Row 5 of Table 1 gives the computed average precisions for k = 100 for our updating algorithm. Since the algorithms in <ref> [3, 16] </ref> always discard the R matrix in (3.7) therefore it makes no difference to the updated low-rank approximation whether it is computed with all the new documents all at once or incrementally with each subgroup at a time.
Reference: [4] <institution> Cornell SMART System, ftp://ftp.cs.cornell.edu/pub/smart. </institution>
Reference-contexts: The rest of the paper is organized as follows: In Section 2, we introduce the subspace-based model and discuss how to use MDL to determine the optimal dimension of the latent-concept subspace. We also evaluate the performance of our method using the Medline text collection <ref> [4] </ref>; In Section 3 we explore the low-rank-plus-shift structure of the term-document matrix and show how the structure can be used to develop novel algorithms for various computational problems in LSI. <p> MDL function values (left) and average precision (right) for the MEDLINE collection where ff is a parameter that can be adjusted for the purpose of renormalization. Example. We now use the MEDLINE text collection <ref> [4] </ref> to illustrate the accuracy of using our model and MLD for estimating the dimensions of the latent-concept subspaces. For MEDLINE the term-document matrix is 3681 fi 1033 and the number of queries is 30, where we used the stemming option supplied by the SMART system [4]. <p> the MEDLINE text collection <ref> [4] </ref> to illustrate the accuracy of using our model and MLD for estimating the dimensions of the latent-concept subspaces. For MEDLINE the term-document matrix is 3681 fi 1033 and the number of queries is 30, where we used the stemming option supplied by the SMART system [4]. Since we are also interested in effects of weighting on the latent-concept subspace dimension, we used two different weighting schemes len.lex and lxn.bpx for the MEDLINE text collection to generate the weighted term-document matrices [6, 17].
Reference: [5] <author> S. Deerwester, S.T. Dumais, T.K. Landauer, G.W. Furnas and R.A. Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the Society for Information Science, </journal> <volume> 41 </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference-contexts: 1. Introduction. Latent semantic indexing (LSI) is a concept-based automatic indexing method that aims at overcoming the fundamental synonymy and polysemy problems which plague traditional lexical-matching indexing schemes <ref> [5] </ref>. 1 LSI is an extension of the vector-space model for information retrieval [17]. <p> Corresponding to each of the k reduced dimensions is associated a latent-concept which may not have any explicit semantic content yet helps to discriminate documents <ref> [3, 5] </ref>. The effectiveness of LSI measured by, for example, increased average precision, has been demonstrated for several text collections [5, 7, 11, 13, 14]. There are also text collections for which LSI is not significantly better than the much simpler original vector-space model. <p> Corresponding to each of the k reduced dimensions is associated a latent-concept which may not have any explicit semantic content yet helps to discriminate documents [3, 5]. The effectiveness of LSI measured by, for example, increased average precision, has been demonstrated for several text collections <ref> [5, 7, 11, 13, 14] </ref>. There are also text collections for which LSI is not significantly better than the much simpler original vector-space model. However, very little theoretical results have been derived for LSI except some explanation of its effectiveness based on multidimensional scaling and multiple regression [1, 2, 19]. <p> A subspace-based model and MDL principle. The essence of LSI is to describe terms and documents as linear combinations of so-called latent concepts or variables <ref> [5] </ref>. However, these linear relations are often used as an ad hoc interpretation of the effects of SVD that are used to compute a reduced-dimension representation in Equation (1.1). <p> In particular no applicable method has been devised to determine the dimension k: a range of k from 100 to 500 or more have been suggested based on empirical evidences <ref> [5, 6, 13] </ref>. However, the optimal k is collection-dependent and there is a need for a systematic way for its determination. This certainly has important ramification for the computation of LSI. <p> Once k is determined, the latent-concept subspace can be determined by computing a partial SVD of the term-document matrix A as is done in <ref> [5] </ref>, i.e., let the SVD of A be k + U ? k (V ? Then spanfCg = spanfV k g. We should notice that only the latent-concept subspace is uniquely determined from the model but not each of the individual latent concepts. <p> Notice that for the average precisions only results for dimensions up to 550 are given. From the plot we can observe the well-known fact that for the MEDLINE text collection precisions go up with the increase of k up to a point, and then precisions actually go down <ref> [5] </ref>. The following table lists where maximum or minimum is achieved for both lxn.bpx and len.lex. lxn.bpx len.lex Precision (max) 148 56 The dimension estimate for lxn.bpx are certainly better than that for len.lex, but both of them seem to be within reasonable range of the optimal k.
Reference: [6] <author> S. Dumais. </author> <title> Improving the retrieval of information from external sources. Behavior Research Methods, </title> <journal> Instruments and Computers, </journal> <volume> 23 </volume> <pages> 229-236, </pages> <year> 1991. </year> <title> 3 we use S ? T to denote S T T = 0. </title>
Reference-contexts: In particular no applicable method has been devised to determine the dimension k: a range of k from 100 to 500 or more have been suggested based on empirical evidences <ref> [5, 6, 13] </ref>. However, the optimal k is collection-dependent and there is a need for a systematic way for its determination. This certainly has important ramification for the computation of LSI. <p> Since we are also interested in effects of weighting on the latent-concept subspace dimension, we used two different weighting schemes len.lex and lxn.bpx for the MEDLINE text collection to generate the weighted term-document matrices <ref> [6, 17] </ref>. As a measure of retrieval accuracy, we used 11-point average precision, a standard measure for comparing information retrieval systems [9].
References-found: 6

