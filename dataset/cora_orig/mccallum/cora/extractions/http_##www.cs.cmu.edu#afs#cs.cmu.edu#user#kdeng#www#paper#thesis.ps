URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/kdeng/www/paper/thesis.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/kdeng/www/research.html
Root-URL: 
Title: Memory-based Time Series Recognition  A New Methodology and Real World Applications  
Author: Kan Deng Advised by: Andrew W. Moore 
Note: A Robotics Ph.D. Thesis Proposal  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> Rabiner, </author> <title> L.R., A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, </title> <booktitle> in Proceedings of the IEEE, </booktitle> <volume> Vol. 77, No. 2, </volume> <month> Feb. </month> <year> 1989. </year>
Reference: [2] <author> Hannaford, B., and Lee, P., </author> <title> Hidden Markov Model Analysis of Force/Torque Information in Telemanipulation, </title> <journal> in the International Journal of Robotics Research., </journal> <volume> Vol. 10, No. 5, </volume> <month> October </month> <year> 1991. </year>
Reference: [3] <author> Nechyba, M.C., and Xu, Y., </author> <title> Human Control Strategy: Abstraction, Verification and Replication, </title> <note> to appear in IEEE Control Systems Magazine, </note> <month> April, </month> <year> 1997. </year>
Reference-contexts: Since it is clearly unsafe to ask someone to driver while drunk, we use a dynamic driving simulator (see Figure 4) instead <ref> [3] </ref> to collect data. The user has independent control over steering (horizontal mouse position), the brake (left mouse button) and the accelerator (right mouse button). For a particular driver, the simulator records the distance from the road median at 5Hz (the simulator itself runs at 50Hz).
Reference: [4] <author> Pook, P. K., and Ballard, D.H., </author> <title> Recognizing Teleoperated Manipulations, </title> <journal> in IEEE Control Systems Magazine, </journal> <year> 1993. </year>
Reference: [5] <author> Elman, J.L, </author> <title> Finding Structure in Time, </title> <booktitle> in Cognitive Science 14, </booktitle> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference: [6] <author> Seawell, </author> <title> T.I., and Kalman, B.L., Time Variability While Training a Parallel Neural Net Network, </title> <type> TR WUCS-95-27, </type> <institution> Dept. Computer Science, Washington University, </institution> <month> August </month> <year> 1995. </year>
Reference: [7] <author> Nikovski, D.N., </author> <title> Adaptive Computation Techniques for Time Series Analysis, A Master Degree Thesis, </title> <institution> Dept. Computer Science, Southern Illinois University. </institution> <month> July </month> <year> 1995. </year>
Reference: [8] <author> Vandewalle, J.,and De Moor B., </author> <title> On the use of the singular value decomposition in identification and signal processing. </title> <booktitle> in the Proceeding of the workshop of NATO Advanced Study Institute on Numerical Linear Algebra, Digital Signal Processing and Parallel Algorithms, </booktitle> <address> Leuven, Belgium, </address> <month> Aug. </month> <year> 1988. </year>
Reference: [9] <author> Dellaert, F., Polzin, T., and Waibel, A., </author> <title> Recognizing Emotion in Speech. </title> <booktitle> in ICSLP96 Conference Proceedings, </booktitle> <year> 1996. </year> <note> 34 References </note>
Reference: [10] <author> Aha, D.W., and Kibler, D., </author> <year> 1989. </year> <title> Noise-tolerant Instance-based Learning Algorithms. </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 794-799), </pages> <address> Detroit, MI. </address> <note> Published by Mor-gan Kaufmann, </note> <year> 1989. </year>
Reference: [11] <author> Atkeson, C.G., Moore, A.W., and Schaal, S., </author> <year> 1997. </year> <title> Locally Weighted Learning, </title> <note> to appear in AI Review, </note> <year> 1997. </year>
Reference-contexts: Once the weights of the data points in memory are ready, we can estimate by maximum likelihood esti mation method. The locally weighted version of likelihood is defined by equation 8, (Eq 8) 1. There are many possible weighting functions, referring to <ref> [11] </ref>. x w i dist i q,( ) 2 2 dist i q,( ) K b q ( ) P y i ( ) i 1= = Locally Weighted Logistic Regression for Classification 19 Locally Weighted Logistic Regression To find which maximizes equation 8, we can use Newton-Ralphson method [30].
Reference: [12] <author> Bishop, </author> <title> C.M., 1995. Neural Networks for Pattern Recognition. </title> <publisher> Published by Oxford University Press Inc., </publisher> <address> New York, </address> <year> 1995. </year>
Reference: [13] <author> Quinlan, J.R., </author> <year> 1993. </year> <title> C4.5 Programs for Machine Learning. </title> <publisher> Published by Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference: [14] <author> Jordan, M.I., and Jacobs, R.A., </author> <year> 1993. </year> <title> Hierarchical Mixtures of Experts and the EM Algorithm. MIT technical report. </title> <journal> AI. </journal> <volume> Memo No. 1440, C.B.C.L. Memo No. 83, </volume> <year> 1993. </year>
Reference: [15] <author> James, M., </author> <year> 1985. </year> <title> Classification Algorithms. </title> <publisher> Published by John Wiley & Sons, inc. </publisher> <year> 1985. </year>
Reference: [16] <author> Gilks, W.R., Clayton, D.G., Speigelhalter, D.J., Best, N.G., McNeil, A.J., Sharples, L.D., and Kirby, A.J., </author> <year> 1993. </year> <title> Modeling Complexity: Applications of Gibbs Sampling in Medicine. </title> <journal> in Journal of the Royal Statistical Society B., </journal> <volume> 55 </volume> <pages> 39-102, </pages> <year> 1993. </year>
Reference: [17] <author> Makhoul, J., Roucos, S., and Gish, H., </author> <year> 1985. </year> <title> Vector Quantization in Speech Coding. </title> <booktitle> in Proceedings of the IEEE, </booktitle> <volume> Vol. 73, No. 11, </volume> <pages> (pp. 1551-1588). </pages> <month> Nov. </month> <year> 1985. </year>
Reference: [18] <author> Moore, A.W., Hill, D.J., and Johnson, M., </author> <year> 1994. </year> <title> An Empirical Investigation of Brute Force to Choose Features, Smoothers and Function Approximators. </title> <booktitle> in Computational Learning Theory and Natural Learning Systems. </booktitle> <volume> Vol. 3. </volume> <publisher> Published by MIT Press, </publisher> <year> 1994. </year>
Reference: [19] <author> Moore, A.W., Schneider, J., et al. Auton Project, </author> <note> http://www.cs.cmu.edu/~AUTON. </note>
Reference: [20] <author> Grosse., E., LOESS: </author> <title> Multi-variate Smoothing by Moving Least Squares. </title> <editor> in Schumaker, L.L., Chul, C.K., and Ward, J.D., editors, </editor> <title> Approximation Theory VI. </title> <publisher> Academic Press, </publisher> <year> 1989. </year>
Reference-contexts: The first common solution is editing, in which only a small subset of all the data points are used. The drawback is that some fine details may be lost. Another solution is caching, in which a multi-variate spline model is built when the data is first loaded. <ref> [20] </ref> does this with a kd-tree structure and multi-linear interpolation within tree leaves. Unfortunately, continuous interpolation above two dimensions is very expensive in computation and memory. [21] also uses a caching method but ignores continuity by storing separate discontinuous linear maps in the leaves. <p> In that case, avoiding the zero-weight data points is not much help. To overcome this problem, we use caching method combining with kd-tree structure <ref> [20, 21] </ref>, following the paradigm of our earlier paradigm [32, 23]. Q * * * * space closer to the query.
Reference: [21] <author> Quinlan, J.R., </author> <title> Combining Instance-Based and Model-Based Learning. </title> <booktitle> in Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <year> 1993. </year>
Reference-contexts: Another solution is caching, in which a multi-variate spline model is built when the data is first loaded. [20] does this with a kd-tree structure and multi-linear interpolation within tree leaves. Unfortunately, continuous interpolation above two dimensions is very expensive in computation and memory. <ref> [21] </ref> also uses a caching method but ignores continuity by storing separate discontinuous linear maps in the leaves. Another downside to caching solutions is that they only record the fitted surface, and may neglect the fine details in the raw datasets. <p> In that case, avoiding the zero-weight data points is not much help. To overcome this problem, we use caching method combining with kd-tree structure <ref> [20, 21] </ref>, following the paradigm of our earlier paradigm [32, 23]. Q * * * * space closer to the query.
Reference: [22] <author> Friedman, J.H., Bentley, J.L., and Finkel, R.A., </author> <title> An Algorithm for Finding Best matches in Logarithmic Expected Time. </title> <journal> in ACM Trans. on Mathematical Software, </journal> <volume> 3(3) </volume> <pages> 209-226, </pages> <month> September, </month> <year> 1977. </year>
Reference-contexts: Another downside to caching solutions is that they only record the fitted surface, and may neglect the fine details in the raw datasets. A third solution uses a technique called range-searching with kd-trees <ref> [22] </ref>. It is possible to arrange data in such a way that given a query point and a distance, all data points within the given distance of the query are returned without needing to search the entire memory. <p> But the difficulties are: 1. How to find those neighboring data points given a query? 2. How to select the neighboring subset so as to save the cost greatly without losing too much fine details? To solve the first question, we can resort to kd-tree <ref> [22] </ref>. It is possible to arrange data points in the way of kd-tree, which partitions the data points hierarchically into subsets, illustrated by figure 12.
Reference: [23] <author> Deng, K., Moore, A.W., </author> <title> Multiresolution Instance-based Learning, </title> <booktitle> in IJCAI Proceedings, Vol.2, </booktitle> <year> 1995. </year>
Reference-contexts: In this thesis, we will combine kd-tree data structure with caching technique to re-organize the memory and information retrieval mechanism, and show how this method speeds up locally weighted logistic regression dramatically. In fact, we have already sucessfully introduced this method for other memory-based algorithms <ref> [23, 32] </ref>. Following diagram illustrates the structure of the entire system and our contributions. <p> In that case, avoiding the zero-weight data points is not much help. To overcome this problem, we use caching method combining with kd-tree structure [20, 21], following the paradigm of our earlier paradigm <ref> [32, 23] </ref>. Q * * * * space closer to the query.
Reference: [24] <author> Johnston, J. </author> <title> (John) Econometric Methods. by New York: </title> <publisher> McGraw-Hill. </publisher> <year> 1972 </year>
Reference-contexts: Third, no model is perfect. By appealing to the Central Limit Theorem, it is often reasonable to treat the noise as Gaussian with mean zero <ref> [24] </ref>. Furthermore, we assume the noise x t at certain time instant t is independent from noise x s at any other time instant s. This kind of noise is often called white noise. 1. In this thesis proposal, we only consider univariate time series.
Reference: [25] <author> Brockwell, P.J, Davis, R.A. </author> <title> Time Series: Theory and Methods, second edition. </title> <publisher> by Springer-Verlag New York, Inc. </publisher> <year> 1991. </year>
Reference-contexts: If we consider the exogenous variables, ARMA model can be extended to ARMAX, defined by equation 2, (Eq 2) Neither ARMA (p,q) model nor ARMAX (p,q,r) is a panacea for all kinds of time series, both of them assume the time series is stationary and invertible, referring to <ref> [25] </ref>. Although in a long term, human driving performance varies a lot, in a short term, we can treat it as stationary. <p> Therefore, it is reasonable to represent a time series by its corresponding ARMA model parameters and . The estimation of and is often done by Maximum Likelihood method <ref> [25] </ref>. While we collect the time series samples, we evaluate the physical and/or psychological properties of each of them. For the car performance series, we can label each series as drunken or sober depending on the drivers sobriety condition.
Reference: [26] <author> Akaike, H. </author> <title> Canonical correlation analysis of time series and the use of an information criterion, in System Identification: Advances and Case Studies, </title> <editor> R.K.Mehra and D.G.Lainiotis, Eds., </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: Selecting several typical time series samples from this domain. 2. For each possible combination of p and q (usually both p and q are no bigger than 10), calculating the value of AIC, which is a selection criteria proposed by <ref> [26] </ref>, based on the time series samples selected. 3. The best p and q should correspond to the minimum AIC value [27].
Reference: [27] <author> Choi, B. S. </author> <title> ARMA Model Identification, </title> <publisher> by Springer-Verlag New York, Inc. </publisher> <year> 1992. </year> <note> References 35 </note>
Reference-contexts: The best p and q should correspond to the minimum AIC value <ref> [27] </ref>. After we have selected p and q values, we can reconfirm the validation of ARMA (p,q) with specified p and q for a certain domain, using a testing method called Portmanteau, referring to [28].
Reference: [28] <institution> Statistical Sciences, </institution> <note> S-plus Guide to Statistical & Mathematical Analysis, Version 3.3, </note> <institution> Seattle: StatSci, a division of MathSoft, Inc. </institution> <year> 1995 </year>
Reference-contexts: The best p and q should correspond to the minimum AIC value [27]. After we have selected p and q values, we can reconfirm the validation of ARMA (p,q) with specified p and q for a certain domain, using a testing method called Portmanteau, referring to <ref> [28] </ref>. After we have selected p and q, and have reconfirmed that ARMA (p,q) is the adequate model for this domain, we collect numerous time series samples from this domain. For each of them, we estimate the parameter vectors and involved in the ARMA (p,q) model.
Reference: [29] <author> Reza, </author> <title> F.M., An Introduction to Information Theory, one of AcGraw-Hill Electrical and Electronic Engineering Series, </title> <publisher> by John Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1961. </year>
Reference-contexts: We measure the difference between two ARMA models by Kullback-Liebler information distance <ref> [29] </ref>. Now, lets come back to the memory, which contains the ARMA model parameter vectors of a number of time series samples.
Reference: [30] <author> Press. W.H., Teukolsky, S.A., Vetterling, W.T. and Flannery, B.P. </author> <year> 1994. </year> <title> Numerical Recipes in C, the Art of Scientific Computing, </title> <note> second edition. Published by the Press Syndicate of the University of Cambridge, </note> <year> 1994. </year>
Reference-contexts: to [11]. x w i dist i q,( ) 2 2 dist i q,( ) K b q ( ) P y i ( ) i 1= = Locally Weighted Logistic Regression for Classification 19 Locally Weighted Logistic Regression To find which maximizes equation 8, we can use Newton-Ralphson method <ref> [30] </ref>. It is a recursive process ing, which starts from a random assignment of (usually we assign the initial to be a zero vector). Based on our practice, Newton-Ralphson recursive process for locally weighted logistic regression converges in no more than 10 iterations. 3.
Reference: [31] <author> Moore, A.W., Schneider, J., et al. Auton Project, </author> <note> http://www.cs.cmu.edu/~AUTON. </note>
Reference-contexts: For each data set, we try six different classifiers, including nearest neighbor method, k-nearest neighbors, decision tree C4.5, feed-forward perceptron, a locally weighted linear regression package called GMBL2 <ref> [31] </ref> and our locally weighted logistic regression method. Each data set is in fact a matrix. Each row is a data point, which consists of input and output two parts. The output of these four data sets are all boolean. The dimensionalities of the inputs vary from 6-d to 34-d.
Reference: [32] <author> Moore, A.W., Schneider, J., Deng, K., </author> <title> Efficient Locally Weighted Polynomial Regression Predictions, </title> <note> accepted by ICML-97. </note>
Reference-contexts: In this thesis, we will combine kd-tree data structure with caching technique to re-organize the memory and information retrieval mechanism, and show how this method speeds up locally weighted logistic regression dramatically. In fact, we have already sucessfully introduced this method for other memory-based algorithms <ref> [23, 32] </ref>. Following diagram illustrates the structure of the entire system and our contributions. <p> In that case, avoiding the zero-weight data points is not much help. To overcome this problem, we use caching method combining with kd-tree structure [20, 21], following the paradigm of our earlier paradigm <ref> [32, 23] </ref>. Q * * * * space closer to the query.
Reference: [33] <author> Pomerleau, D., RALPH: </author> <title> Rapidly Adapting Lateral Position Handler in 1995 IEEE Symposium on Intelligent Vehicle, </title> <address> Detroit, Michigan, U.S.A. </address> <year> 1995. </year>
Reference-contexts: Time series module, which does the estimation of the ARMA model and recognize the time series based on the classification of the ARMA parameters. 3. Signal module, which gives the driver a proper signal, such as a blinking lamp, and/or a microphone. <ref> [33] </ref> has done some very impressive work in autonomous vehicle. We can use their navigation system to collect the datasets. Our job will be the time series module. The overall system is illustrated in figure 14. 2.
References-found: 33

