URL: http://ana.lcs.mit.edu/nrg/papers/week3/ieee-network395.ps
Refering-URL: http://ana.lcs.mit.edu/nrg/previous.html
Root-URL: 
Title: Abstract Congestion control ensures that network resources are divided fairly and efficiently among competing connections.
Note: Page 1 of 11  Although congestion control has been studied by  The  that is finally standardized or widely implemented.  
Abstract: The proposed credit-based mechanism provides ow control tailored to ATM networks. Simulation, analysis and experiments on switching hardware have shown that for a wide variety of traffic patterns, credit control is fair, uses links efficiently, minimizes delay, and guarantees no cell loss due to congestion. The credit system is especially well suited to data traffic that is bursty, unpredictable, and has little tolerance for delay. Other approaches to ow control, including rate-based ow control, may require less expensive hardware and may be effective for steady traffic, but do not handle bursty traffic well. While no one can predict what kind of traffic will dominate future networks, recent evidence suggests that it will be bursty. Thus a major challenge for network research will be to find congestion control mechanisms that blend the hardware simplicity of rate-based ow control with credit-based ow controls ability to handle bursts. This research, in turn, will depend on more experience with real applications and high-level protocols running over ATM. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ATM Forum, </author> <title> ATM User-Network Interface Specification, Version 3.0, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1993. </year>
Reference: [2] <author> Blackwell, et al. </author> <title> An Experimental Flow Controlled Multicast ATM Switch, </title> <booktitle> Proceedings of First Annual Conference on Telecommunications R&D in Massachusetts, </booktitle> <volume> Vol. 6, </volume> <pages> pp. 33-38, </pages> <month> October 25, </month> <year> 1994. </year>
Reference-contexts: In the second credit control phase, the sender maintains a non- negative credit balance, Crd_Bal, to ensure no overow of the allocated buffer in the receiver. An experimental OC-12 (622-Mbps) ATM switch <ref> [2] </ref> with credit ow control has been developed by BNR and Harvard. An ATM network testbed involving multiple Page 3 of 11 copies of this switch and a variety of ATM host adapter cards is operational.
Reference: [3] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H. T. Kung, M. Lam, M. Levine, M. Wire, C. Peterson, J. Susman, J. Sutton, J. Urbanski and J. Webb, </author> <title> Integrating Systolic and Memory Communication in iWarp, </title> <booktitle> Conference Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <address> Seattle, Washington, </address> <month> June </month> <year> 1990, </year> <pages> pp. 70-81. </pages> <note> Page 11 of 11 </note>
Reference-contexts: In contrast, in CUP the sender does not retransmit lost data cells, the receiver does not reorder received cells, and data cells do not carry sequence numbers. It can be shown [10] that CUP produces the same buffer management results as the well-known incremental credit updating methods (see, e.g., <ref> [3, 6] </ref>). In these other methods, instead of sending Fwd_Cnt values upstream the receiver sends incremental credit values to be added to Crd_Bal at the sender. 4.2. Static vs. adaptive credit control We call a credit-based ow control static or adaptive, if the buffer allocation is static or adaptive, respectively.
Reference: [4] <author> G. Irlam, </author> <title> Unix File Size Survey - 1993, </title> <note> Usenet comp.arch.storage, &lt;URL: http://www.base.com/gordoni/ ufs93.html&gt;, last updated September 1994. </note>
Reference-contexts: For instance, TCP sessions often transfer no more than a few dozen KBytes [13] and the required transmission time on a link of OC-3 rate, 155Mbps, is only a few milliseconds. (An extensive Unix file size survey <ref> [4] </ref> has shown that the average file length is only around 22Kbytes and most files are smaller than 2Kbytes.) This situation will only get worse with increasing network capacity, and with the increasing differences in bandwidth available in different parts of the network.
Reference: [5] <author> W. E. Leland, M. S. Taqqu, W. Willinger and D. V. </author> <title> Wilson On the Self-Similar Nature of Ethernet Traffic, </title> <booktitle> Proc. ACM SIGCOMM '93 Symposium on Communications Architectures, Protocols and Applications, </booktitle> <month> September </month> <year> 1993, </year> <pages> pp. 183193. </pages>
Reference-contexts: Rate-based ow control works well with smooth traffic. Bursty traffic lacks any of the predictability of smooth traffic, as observed in some computer communications traffic <ref> [5] </ref>. Some kinds of bursts stem from users and applications. A Mosaic user clicking on a link, for instance, wants to see a page or image as soon as possible. The network cannot predict when the clicks will occur.
Reference: [6] <author> M. G. H. Katevenis, </author> <title> Fast Switching and Fair Control of Congested Flow in Broadband Networks, </title> <journal> IEEE J. on Selected Areas in Commun., </journal> <volume> vol. SAC-5, no. 8, </volume> <pages> pp. 13151326, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: In contrast, in CUP the sender does not retransmit lost data cells, the receiver does not reorder received cells, and data cells do not carry sequence numbers. It can be shown [10] that CUP produces the same buffer management results as the well-known incremental credit updating methods (see, e.g., <ref> [3, 6] </ref>). In these other methods, instead of sending Fwd_Cnt values upstream the receiver sends incremental credit values to be added to Crd_Bal at the sender. 4.2. Static vs. adaptive credit control We call a credit-based ow control static or adaptive, if the buffer allocation is static or adaptive, respectively.
Reference: [7] <author> V. Jacobson, </author> <title> Congestion Avoidance and Control, </title> <booktitle> Proc. SIGCOMM 88 Symposium on Communications Architectures and Protocols, </booktitle> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: A good deal of traffic over future ATM networks may well use existing transport protocols such as TCP/IP. TCP has its own window-based ow control mechanism <ref> [7] </ref> which interprets lost or delayed packets as evidence of congestion. Studies by Fang and Chen of Sandia National Laboratories show that TCP connections over credit get close to fair shares of bandwidth and achieve full link utilization.
Reference: [8] <author> H. T. Kung, T. Blackwell and A. Chapman, </author> <title> Credit-Based Flow Control for ATM Networks: Credit Update Protocol, Adaptive Credit Allocation, and Statistical Multiplexing, </title> <booktitle> Proc. ACM SIGCOMM '94 Symposium on Communications Architectures, Protocols and Applications, </booktitle> <month> August 31September 2, </month> <year> 1994, </year> <pages> pp. 101-114. </pages>
Reference-contexts: Next, we overview the sender- and receiver-oriented adaptive approaches. Finally, we describe in some detail a receiver-oriented adaptive buffer allocation scheme. 4.1. Credit Update Protocol The Credit Update Protocol (CUP) <ref> [8] </ref> is an efficient and robust protocol for implementing credit control over a link. (A link can be a physical link connecting two adjacent nodes, or a virtual circuit connecting two remote nodes.) As depicted by Figure 2, for each ow-controlled VC the sender keeps a running total Tx_Cnt of all <p> Thus Crd_Bal computed by the sender using Equation (1) is the proper new credit balance, in the sense that as long as the sender transmits no more than Crd_Bal cells, it will not overrun the VCs allocated buffer in the receiver. See <ref> [8] </ref> for a scheme of using credit_check cells periodically sent from the sender to the receiver, to recover from possible loss of data or credit cells. The frequency at which the receiver sends credit records for a VC depends on the VCs progress. <p> The freed up buffer space will automatically be assigned to other VCs which have data to forward and are not congested downstream. Adaptive buffer allocation can be implemented at the sender or receiver node. As depicted by Figure 3, in a sender-oriented adaptive scheme <ref> [8, 11] </ref> the sender adaptively allocates a shared input-buffer at the receiver among a number of VCs from the sender that share the same buffer pool. The sender can allocate buffer for the VCs based on their measured, relative bandwidth usage on the output port p [8]. <p> The sender can allocate buffer for the VCs based on their measured, relative bandwidth usage on the output port p <ref> [8] </ref>. Receiver-oriented adaptation [9] is depicted by Figure 4. The receiver adaptively allocates a shared output-buffer among a number of VCs from one or more senders that share the same buffer pool. <p> If the VC does increase its bandwidth usage, then as described in Section 4.3 the adaptation scheme will notice the increased usage and will subsequently increase the buffer allocation for the VC <ref> [8, 9] </ref>. The receiver-oriented adaptive buffer allocation scheme in [9] uses M given by Equation (5). Analysis and simulation results have shown that with this choice of M the adaptive scheme gives excellent performance in utilization, fairness, and ramp-up [9]. 5.2.
Reference: [9] <author> H. T. Kung and K. Chang, </author> <title> Receiver-Oriented Adaptive Buffer Allocation in Credit-Based Flow Control for ATM Networks, </title> <booktitle> Proc. INFOCOM 95, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: The sender can allocate buffer for the VCs based on their measured, relative bandwidth usage on the output port p [8]. Receiver-oriented adaptation <ref> [9] </ref> is depicted by Figure 4. The receiver adaptively allocates a shared output-buffer among a number of VCs from one or more senders that share the same buffer pool. The receiver can allocate buffer for the VCs based on their measured, relative bandwidth usage on the output port q [9]. <p> adaptation <ref> [9] </ref> is depicted by Figure 4. The receiver adaptively allocates a shared output-buffer among a number of VCs from one or more senders that share the same buffer pool. The receiver can allocate buffer for the VCs based on their measured, relative bandwidth usage on the output port q [9]. Receiver-oriented adaptation is suited for the case where a common buffer pool in a receiver is shared by VCs from multiple upstream nodes. <p> Since only the receiver needs to use N2 values, it can conveniently change them locally, as described in Section 4.4. 4.4. Receiver-oriented adaptive buffer allocation We describe the underlying idea of the receiver-oriented adaptive buffer allocation algorithm in <ref> [9] </ref>. In referring to the size, in cells, of the common buffer pool in the receiver. For each allocation interval, which is set to be at least RTT, the receiver will compute a new allocation and an N2 value for each VC according its relative bandwidth usage. <p> For the purpose of presenting the basic adaptive idea here, it is without loss of generality that in this section oor and ceiling notations for certain quantities are ignored, such as those in the right- hand sides of the above two equations. See <ref> [9] </ref> for precise definitions and analysis of all quantities. It is easy to see that the adaptive formula of Equation (3) will not introduce cell loss. <p> Simulation results in <ref> [9] </ref> show that this overhead is generally below a few percent and sometimes below 1 percent.) On the other hand, an inactive VC could be given an N2 value as small as one. <p> If the VC does increase its bandwidth usage, then as described in Section 4.3 the adaptation scheme will notice the increased usage and will subsequently increase the buffer allocation for the VC <ref> [8, 9] </ref>. The receiver-oriented adaptive buffer allocation scheme in [9] uses M given by Equation (5). Analysis and simulation results have shown that with this choice of M the adaptive scheme gives excellent performance in utilization, fairness, and ramp-up [9]. 5.2. <p> If the VC does increase its bandwidth usage, then as described in Section 4.3 the adaptation scheme will notice the increased usage and will subsequently increase the buffer allocation for the VC [8, 9]. The receiver-oriented adaptive buffer allocation scheme in <ref> [9] </ref> uses M given by Equation (5). Analysis and simulation results have shown that with this choice of M the adaptive scheme gives excellent performance in utilization, fairness, and ramp-up [9]. 5.2. <p> The receiver-oriented adaptive buffer allocation scheme in <ref> [9] </ref> uses M given by Equation (5). Analysis and simulation results have shown that with this choice of M the adaptive scheme gives excellent performance in utilization, fairness, and ramp-up [9]. 5.2. Link-by-link ow control to increase quality of control Link-by-link ow control has shorter and more predictable control loop delay than end-to-end ow control. This implies smaller memory requirements for switching nodes and higher performance in utilization, transmission time, fairness, etc. <p> Here we present a brief summary of these simulation results, with a focus on work done at Harvard. For details the reader should contact responsible researchers and look into the references. A thorough simulation study of credit ow control <ref> [9] </ref> shows that it is almost perfectly fair not just with steady traffic in the GFC configuration, but also with highly bursty traffic over links of widely varying bandwidths and propagation delays. <p> A study by Su, Golmie, Chang, and Benmo- hamed of NIST confirms the fairness and high link utilization of credit over GFC. Simulation results in <ref> [9] </ref> show that the same high performance can actually be achieved with a large number of VCs, e.g., 500 active VCs, sharing the same output link. To evaluate the effectiveness of the adaptive buffer algorithm during severe congestion, the simulation suite in [9] includes a case where the bandwidth of a <p> Simulation results in <ref> [9] </ref> show that the same high performance can actually be achieved with a large number of VCs, e.g., 500 active VCs, sharing the same output link. To evaluate the effectiveness of the adaptive buffer algorithm during severe congestion, the simulation suite in [9] includes a case where the bandwidth of a bottleneck link is suddenly reduced 100-fold. The simulation results show that during the bandwidth reduction period, not only do new VCs still ramp up quickly (modulo to link RTT), but also they ramp up in a fair manner.
Reference: [10] <author> H. T. Kung and A. Chapman, </author> <title> The FCVC (Flow-Controlled Virtual Channels) Proposal for ATM Networks, Version 2.0, </title> <booktitle> 1993. A summary appears in Proc. 1993 International Conf. on Network Protocols, </booktitle> <address> San Francisco, California, </address> <month> October </month> <year> 1922, 1993, </year> <pages> pp. 116-127. </pages> <note> (Postscript files of this and other related papers by the authors and their colleagues are avail-able via anonymous FTP from virtual.harvard.edu:/pub/htk/ atm.) </note>
Reference-contexts: To implement this, each data packet carries a sequence number. In contrast, in CUP the sender does not retransmit lost data cells, the receiver does not reorder received cells, and data cells do not carry sequence numbers. It can be shown <ref> [10] </ref> that CUP produces the same buffer management results as the well-known incremental credit updating methods (see, e.g., [3, 6]). In these other methods, instead of sending Fwd_Cnt values upstream the receiver sends incremental credit values to be added to Crd_Bal at the sender. 4.2.
Reference: [11] <author> C. Ozveren, R. Simcoe, and G. Varghese, </author> <title> Reliable and Efficient Hop-by-Hop Flow Control, </title> <booktitle> Proc. ACM SIGCOMM '94 Symposium on Communications Architectures, Protocols and Applications, </booktitle> <month> August 31-September 2, </month> <year> 1994, </year> <pages> pp. 89-100. </pages>
Reference-contexts: The freed up buffer space will automatically be assigned to other VCs which have data to forward and are not congested downstream. Adaptive buffer allocation can be implemented at the sender or receiver node. As depicted by Figure 3, in a sender-oriented adaptive scheme <ref> [8, 11] </ref> the sender adaptively allocates a shared input-buffer at the receiver among a number of VCs from the sender that share the same buffer pool. The sender can allocate buffer for the VCs based on their measured, relative bandwidth usage on the output port p [8].
Reference: [12] <author> Larry Roberts, </author> <title> Enhanced PRCA (Proportional Rate-Control Algorithm), </title> <address> ATM-Forum/94-0735R1, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: When traffic changes, optimal rates to be assigned to the affected VCs must change accordingly. For this reason, adaptive rate setting is necessary for bursty traffic, and has been subject to intensive research for many years. The Enhanced Proportional Rate-Control Algorithm (EPRCA) <ref> [12] </ref> is one of the latest schemes considered at the ATM Forum, and represents the kind of adaptive rate setting schemes this article assumes. Rate adaptation can not be so precise that the newly derived rates will be just right with respect to current load, for at least two reasons.
Reference: [13] <author> A. Schmidt and R. Campbell, </author> <title> Internet Protocol Traffic Analysis with Applications for ATM Switch Design, </title> <journal> ACM SIGCOMM Computer Communication Review, </journal> <volume> Vol. 23, No. 2, </volume> <pages> pp. 39-52, </pages> <month> April, </month> <year> 1993. </year>
Reference-contexts: However, the network load may change faster than the control system can react. On a very fast network, transfers may also take too little time to achieve a steady state. For instance, TCP sessions often transfer no more than a few dozen KBytes <ref> [13] </ref> and the required transmission time on a link of OC-3 rate, 155Mbps, is only a few milliseconds. (An extensive Unix file size survey [4] has shown that the average file length is only around 22Kbytes and most files are smaller than 2Kbytes.) This situation will only get worse with increasing
Reference: [14] <author> R. J. Simcoe, </author> <title> Configurations for Fairness and Other Test, </title> <address> ATM_Forum/ 94-0557, </address> <year> 1994. </year>
Reference-contexts: However, setting rates perfectly or near optimally is a complicated matter. Consider, for example, the configuration of Figure 6, known at the ATM Forum as Generic Fairness Configuration (GFC) <ref> [14] </ref>. All traffic sources are assumed to be persistently greedy, and can transmit at the peak link rate when the bandwidth is available.
References-found: 14

