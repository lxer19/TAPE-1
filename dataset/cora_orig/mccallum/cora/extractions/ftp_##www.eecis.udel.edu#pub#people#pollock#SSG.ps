URL: ftp://www.eecis.udel.edu/pub/people/pollock/SSG.ps
Refering-URL: http://www.eecis.udel.edu:80/~norris/
Root-URL: http://www.cis.udel.edu
Email: norris@udel.edu pollock@udel.edu  
Title: A Scheduler-Sensitive Global Register Allocator  
Author: Cindy Norris Lori. L. Pollock 
Address: 19716  
Affiliation: Department of Computer and Information Sciences University of Delaware Newark, DE,  
Abstract: Compile-time reordering of machine-level instructions has been very successful at achieving large increases in performance of programs on machines offering fine-grained parallelism. However, because of the interdependences between instruction scheduling and register allocation, it is not clear which of these two phases of the compiler should run first to generate the most efficient final code. In this paper, we describe our investigation into slight modifications to key phases of a successful global register allocator to create a scheduler-sensitive register allocator, which is then followed by an "off-the-shelf" instruction sched-uler. Our experimental studies reveal that this approach achieves speedups comparable and increasingly better than previous cooperative approaches with an increasing number of available registers without the complexities of the previous approaches. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. G. Bradlee. </author> <title> Retargetable Instruction Scheduling for Pipelined Processors. </title> <type> PhD thesis, </type> <institution> University of Washing-ton, </institution> <year> 1992. </year>
Reference-contexts: The registers are allocated using a round robin approach, because experimental results indicate round robin provides a better allocation in the context of code scheduling and global register allocation than a first fit approach <ref> [1] </ref>. The register allocators work with code written in iloc, a low-level intermediate code developed at Rice University for the development of optimizing compilers [3]. Iloc code is generated assuming an infinite number of virtual registers. <p> The top portion of Figure 5 depicts the organization of the phases of our implementation of Bradlee's version of Goodman and Hsu's Integrated Prepass Scheduler (IPS) [2] <ref> [1] </ref> [11].
Reference: [2] <author> D. G. Bradlee, S. J. Eggers, and R. R. Henry. </author> <title> Integrating register allocation and instruction scheduling for RISCs. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 122-131, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Only recently have there been strategies developed to introduce some communication of requirements between the scheduler and the allocator such that the two phases can cooperate to generate better code [11] <ref> [2] </ref> [15]. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation. <p> On highly pipelined machine models, the integrated prepass scheduling approach slightly outperformed the dag-driven register allocation approach. Bradlee, Eggers, and Henry <ref> [2] </ref> developed a slightly modified version of integrated prepass scheduling (which they called IPS) in which they calculate the register limit in a different way, replace the local allocator with a global allocator, and invoke the scheduler again after allocation in order to better schedule spill code. <p> give great performance, we appropriately renamed bwm to BMW for the comparison against other cooperative approaches. 5 Comparison with Other Approaches We implemented and compared the speedups over Postpass of our BMW strategy, Goodman and Hsu's Integrated Prepass Scheduler (IPS) algorithm with the improvements suggested by Bradlee, Henry and Eggers <ref> [2] </ref>, and our Enhanced Integrated Prepass Scheduling. The top portion of Figure 5 depicts the organization of the phases of our implementation of Bradlee's version of Goodman and Hsu's Integrated Prepass Scheduler (IPS) [2] [1] [11]. <p> Goodman and Hsu's Integrated Prepass Scheduler (IPS) algorithm with the improvements suggested by Bradlee, Henry and Eggers <ref> [2] </ref>, and our Enhanced Integrated Prepass Scheduling. The top portion of Figure 5 depicts the organization of the phases of our implementation of Bradlee's version of Goodman and Hsu's Integrated Prepass Scheduler (IPS) [2] [1] [11]. We chose to use IPS for comparison to SSG rather than the RASE method of Bradlee, Henry and Eggers [2] for several reasons: (1) Bradlee, Henry and Eggers reported little speedup in the RASE method over the IPS method except in the case of very large basic blocks. <p> The top portion of Figure 5 depicts the organization of the phases of our implementation of Bradlee's version of Goodman and Hsu's Integrated Prepass Scheduler (IPS) <ref> [2] </ref> [1] [11]. We chose to use IPS for comparison to SSG rather than the RASE method of Bradlee, Henry and Eggers [2] for several reasons: (1) Bradlee, Henry and Eggers reported little speedup in the RASE method over the IPS method except in the case of very large basic blocks. (2) The IPS method is simpler to implement than RASE due to the simultaneous local register allocation and scheduling phase in RASE.
Reference: [3] <author> P. Briggs, K. D. Cooper, and L. Torczon. </author> <title> R n programming environment newsletter #44. </title> <institution> Department of Computer Science, Rice University, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: The register allocators work with code written in iloc, a low-level intermediate code developed at Rice University for the development of optimizing compilers <ref> [3] </ref>. Iloc code is generated assuming an infinite number of virtual registers.
Reference: [4] <author> P. Briggs, K. D. Cooper, and L. Torczon. Rematerializa-tion. </author> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: Due to the interdependences between instruction scheduling and register allocation, which allocates registers to values so as to minimize the number of mem fl This work was partially supported by NSF under grant CCR-9300212. ory accesses at run-time [7] [6] [5] [16] <ref> [4] </ref> [12], the compiler writer faces the problem of determining which phase should run first to generate the most efficient final code. <p> Based on its demonstrated success, we selected the optimistic allocator (OA) developed by Briggs, Cooper, and Torczon <ref> [4] </ref> as our base allocator. if (P) if (P) else else v = 5 v2 = 5 .. = v3 ... 3.1 The Base Register Allocator Like most global register allocators, OA is based on a graph coloring allocation method, namely, the method developed by Chaitin [6]. <p> If all nodes are colored during the execution of the SELECT stage, the process terminates. 3.2 A Dag Driven BUILD The BUILD stage of SSG is significantly different from the BUILD employed by Briggs, Cooper, and Torczon <ref> [4] </ref> in OA. In their BUILD, the interferences are determined assuming that the given code order represents the final order of the instructions to be executed. In the context of local code scheduling, the interference graph should reflect whether two values interfere given any legitimate code reordering. <p> In order to do this comparison, the optimistic allocator of Briggs, Cooper, and Torczon <ref> [4] </ref> was implemented to perform the conventional register allocation. The registers are allocated using a round robin approach, because experimental results indicate round robin provides a better allocation in the context of code scheduling and global register allocation than a first fit approach [1].
Reference: [5] <author> D. Callahan and B. Koblenz. </author> <title> Register allocation via hierarchical graph coloring. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 192-203, </pages> <address> Toronto, CANADA, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Due to the interdependences between instruction scheduling and register allocation, which allocates registers to values so as to minimize the number of mem fl This work was partially supported by NSF under grant CCR-9300212. ory accesses at run-time [7] [6] <ref> [5] </ref> [16] [4] [12], the compiler writer faces the problem of determining which phase should run first to generate the most efficient final code.
Reference: [6] <author> G. Chaitin, M. Auslander, A. K. Chandra, J. Cocke, M. E. Hopkins, and P. W. Markstein. </author> <title> Register allocation via coloring. </title> <journal> Computer Languages, </journal> <volume> 6 </volume> <pages> 47-57, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: Due to the interdependences between instruction scheduling and register allocation, which allocates registers to values so as to minimize the number of mem fl This work was partially supported by NSF under grant CCR-9300212. ory accesses at run-time [7] <ref> [6] </ref> [5] [16] [4] [12], the compiler writer faces the problem of determining which phase should run first to generate the most efficient final code. <p> Briggs, Cooper, and Torczon [4] as our base allocator. if (P) if (P) else else v = 5 v2 = 5 .. = v3 ... 3.1 The Base Register Allocator Like most global register allocators, OA is based on a graph coloring allocation method, namely, the method developed by Chaitin <ref> [6] </ref>. An interference graph is constructed in which the nodes represent candidates for physical registers, and edges connect nodes that must be assigned different physical registers because their values will coexist during program execution.
Reference: [7] <author> F. Chow and J. Hennessy. </author> <title> Register allocation by priority-based coloring. </title> <booktitle> In Proceedings of the SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <month> June </month> <year> 1984. </year>
Reference-contexts: Due to the interdependences between instruction scheduling and register allocation, which allocates registers to values so as to minimize the number of mem fl This work was partially supported by NSF under grant CCR-9300212. ory accesses at run-time <ref> [7] </ref> [6] [5] [16] [4] [12], the compiler writer faces the problem of determining which phase should run first to generate the most efficient final code.
Reference: [8] <author> R. Cytron, J. Ferrante, B. Rosen, and M. Wegman. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> Octo-ber </month> <year> 1991. </year>
Reference-contexts: The input to the RENUMBER phase consists of intermediate code generated using an unlimited number of virtual registers. In the first step of the RENUMBER stage, the Static Single Assignment (SSA) graph <ref> [8] </ref> is computed. This representation transforms the code so that each use corresponds to a single definition. To achieve this, virtual registers are renumbered and special functions called phi functions are inserted into the code.
Reference: [9] <author> Freudenberger and Ruttenberg. </author> <title> Phase ordering of register allocation and instruction scheduling. </title> <booktitle> Proceeding of International Workshop on Code Generation Concepts and Techniques, </booktitle> <year> 1992. </year>
Reference: [10] <author> P. B. Gibbons and S. S. Muchnick. </author> <title> Efficient instruction scheduling for a pipelined architecture. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: In order to exploit the available instruction level parallelism in programs and utilize the full potential of the node processors, the compiler includes an instruction scheduling phase which rearranges the code sequence to hide latencies and reduce possible run-time delays [13] [18] <ref> [10] </ref> [17]. <p> by the COALESCE stage if they do not add an antidependence. 4 Evaluation of SSG Strategies 4.1 Experimental Framework The performance of SSG has been compared to the performance of the Postpass scheduling approach where a conventional register allocator is followed by a scheduler based on the Gibbons and Muchnick <ref> [10] </ref> basic block scheduling algorithm. In order to do this comparison, the optimistic allocator of Briggs, Cooper, and Torczon [4] was implemented to perform the conventional register allocation.
Reference: [11] <author> J. R. Goodman and W. Hsu. </author> <title> Code scheduling and register allocation in large basic blocks. </title> <booktitle> In Supercomputing '88 Proceedings, </booktitle> <pages> pages 442-452, </pages> <address> Orlando, Florida, </address> <month> November </month> <year> 1988. </year>
Reference-contexts: Only recently have there been strategies developed to introduce some communication of requirements between the scheduler and the allocator such that the two phases can cooperate to generate better code <ref> [11] </ref> [2] [15]. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation. <p> The discussion of the experimental results is split into two sections, one focusing on comparing the relative performance of our different scheduler-sensitive register allocation strategies and one focusing on comparisons of our "best" strategy with the other cooperative approaches. 2 Previous Cooperative Strategies Goodman and Hsu <ref> [11] </ref> have developed two separate cooperative strategies. <p> The top portion of Figure 5 depicts the organization of the phases of our implementation of Bradlee's version of Goodman and Hsu's Integrated Prepass Scheduler (IPS) [2] [1] <ref> [11] </ref>.
Reference: [12] <author> L. J. Hendren, G. R. Gao, E. R. Altman, and C. Mukerji. </author> <title> A register allocation framework based on hierarchical cyclic interval graphs. </title> <booktitle> In International Workshop on Compiler Construction, </booktitle> <address> Paderdorn, GERMANY, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Due to the interdependences between instruction scheduling and register allocation, which allocates registers to values so as to minimize the number of mem fl This work was partially supported by NSF under grant CCR-9300212. ory accesses at run-time [7] [6] [5] [16] [4] <ref> [12] </ref>, the compiler writer faces the problem of determining which phase should run first to generate the most efficient final code.
Reference: [13] <author> J. L. Hennessy and T. Gross. </author> <title> Postpass code optimization of pipeline constraints. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 422-448, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: In order to exploit the available instruction level parallelism in programs and utilize the full potential of the node processors, the compiler includes an instruction scheduling phase which rearranges the code sequence to hide latencies and reduce possible run-time delays <ref> [13] </ref> [18] [10] [17].
Reference: [14] <author> F. H. McMahon. </author> <title> FORTRAN CPU Performance Analysis. </title> <institution> Lawrence Livermore Laboratories, </institution> <year> 1972. </year>
Reference-contexts: The hypothetical machine contains a hardware interlock mechanism, and thus no-ops do not need to be inserted in order to enforce interlocks. The speedup measurements for SSG were achieved by translating each of the first 12 Livermore Loops <ref> [14] </ref> into iloc intermediate code and performing classical local and global optimizations. The performance of SSG was evaluated by calculating the speedup of SSG followed by scheduling against OA (conventional) register allocation followed by scheduling.
Reference: [15] <author> S. S. Pinter. </author> <title> Register allocation with instruction scheduling: a new approach. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Only recently have there been strategies developed to introduce some communication of requirements between the scheduler and the allocator such that the two phases can cooperate to generate better code [11] [2] <ref> [15] </ref>. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation. <p> In addition, the final scheduler is complicated by simultaneously performing local register allocation with instruction scheduling. Since submission of this paper, Pinter <ref> [15] </ref> has published an article describing a cooperative approach based on building a parallel interference graph and some heuristics for trading off between scheduling and register spilling.
Reference: [16] <author> T. A. Proebsting and C. N. Fischer. </author> <title> Probabilistic register allocation. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 300-310, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Due to the interdependences between instruction scheduling and register allocation, which allocates registers to values so as to minimize the number of mem fl This work was partially supported by NSF under grant CCR-9300212. ory accesses at run-time [7] [6] [5] <ref> [16] </ref> [4] [12], the compiler writer faces the problem of determining which phase should run first to generate the most efficient final code.
Reference: [17] <author> S. Weiss and J. E. Smith. </author> <title> A study of scalar compilation techniques for pipelined supercomputers. </title> <booktitle> In Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1987. </year>
Reference-contexts: In order to exploit the available instruction level parallelism in programs and utilize the full potential of the node processors, the compiler includes an instruction scheduling phase which rearranges the code sequence to hide latencies and reduce possible run-time delays [13] [18] [10] <ref> [17] </ref>.
Reference: [18] <author> H. Young. </author> <title> Evaluation of a decoupled computer architecture and the design of a vector extension. </title> <note> Computer Sciences Technical Report 603, 21(4), </note> <month> July </month> <year> 1985. </year>
Reference-contexts: In order to exploit the available instruction level parallelism in programs and utilize the full potential of the node processors, the compiler includes an instruction scheduling phase which rearranges the code sequence to hide latencies and reduce possible run-time delays [13] <ref> [18] </ref> [10] [17].
References-found: 18

