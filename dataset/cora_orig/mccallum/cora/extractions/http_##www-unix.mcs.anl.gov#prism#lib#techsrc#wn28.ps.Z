URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn28.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: Design of a Parallel Nonsymmetric Eigenroutine Toolbox, Part II  
Author: Zhaojun Bai and James Demmel 
Date: January 18, 1996  
Abstract: In Part I of this report [2], we proposed to build a toolbox for solving the dense nonsymmetric eigenvalue problem on distributed memory parallel computers. Part I described basic building blocks, and their uses in different combinations, for extracting the eigenvalues and/or their corresponding invariant subspaces of a nonsymmetric matrix. However, the numerical accuracy and stability of some of our proposed building blocks, such as the matrix sign function, are poorly understood. Part II of this report focusses on the numerical analysis of these building blocks. We discuss perturbation theory of the matrix sign function, the conditioning of its computation, the numerical stability of the overall algorithm, and iterative refinement schemes. Numerical examples are also presented. An extension of the matrix sign function based algorithm to compute left and right deflating subspaces for the generalized eigenvalue problem of a regular matrix pencil is also described. fl Department of Mathematics, University of Kentucky, Lexington, KY 40506. The author was supported in part by ARPA grant DM28E04120 and P-95006 via a subcontract from Argonne National Laboratory and by an NSF grant ASC-9313958 and in part by an DOE grant DE-FG03-94ER25219 via subcontracts from University of California at Berkeley. y Computer Science Division and Mathematics Department, University of California, Berkeley, CA 94720. The author was funded in part by the Advanced Research Projects Agency (DOD) contract DAAH04-95-1-0077 through University of Tennessee subcontract ORA7453.02, ARPA contract DAAL03-91-C-0047 through University of Tennessee subcontract ORA4466.02, NSF contracts ASC-9313958 and ASC-9404748, DOE contracts DE-FG03-94ER25219, DE-FG03-94ER25206, DOE contract No. W-31-109-Eng-38 through subcontract No. 951322401 with Argonne National Laboratory, and NSF Infrastructure Grant Nos. CDA-8722788 and CDA-9401156. The information presented here does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <note> LAPACK Users' Guide (second edition). SIAM, Philadelphia, </note> <year> 1995. </year>
Reference-contexts: We discussed how these tools can be used in different combinations on different problems and architectures for extracting all or part of eigenvalues of a nonsymmetric matrix and/or their corresponding invariant subspaces. Rather than using "black box" eigenroutines such as provided by EISPACK [28, 19] and LAPACK <ref> [1] </ref>, we expect the toolbox approach to allow us more flexibility in developing efficient problem-oriented eigenproblem solvers on high performance machines, especially on parallel distributed memory machines. <p> The computed Schur form (quasi-triangular matrix) T and Schur vectors Q by the QR algorithm satisfy Q T (A + E) Q = T ; where E is of the order of ukAk. Numerical software for the QR algorithm is available in EISPACK [28] and LAPACK <ref> [1] </ref>. Although nonconvergent examples have been found, they are quite rare in practice [5, 14]. We note that the eigenvalues on the (block)-diagonal of T may appear in any order.
Reference: [2] <author> Z. Bai and J. Demmel. </author> <title> Design of a parallel nonsymmetric eigenroutine toolbox, Part I. </title> <editor> In R. F. Sincovec et al, editor, </editor> <booktitle> Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing. </booktitle> <publisher> SIAM, </publisher> <year> 1993. </year> <note> Long version available as Computer Science Report CSD-92-718, </note> <institution> University of California, Berkeley, </institution> <year> 1992. </year>
Reference-contexts: 1 Introduction As stated at the beginning of Part I of this report <ref> [2] </ref>: "It is a challenge to design a parallel algorithm for the nonsymmetric eigenproblem that uses coarse grain parallelism effectively, scales for larger problems on larger machines, does not waste time dealing with the parts of the spectrum in which the user is not interested, and deals with highly nonnormal matrices <p> By computing the matrix sign function of a Mobius transformation of A, the spectrum can be divided along arbitrary lines and circles, rather than just along the imaginary axis. See Part I of this report <ref> [2] </ref> and the references therein for more details. Unfortunately, in finite precision arithmetic, the ill-conditioning of a matrix A k with respect to inversion, together with rounding errors, may destroy the convergence of the Newton iteration (1.2), or cause convergence to the wrong answer.
Reference: [3] <author> Z. Bai, J. Demmel, J. Dongarra, A. Petitet, H. Robinson, and K. Stanley. </author> <title> The spectral decomposition of nonsymmetric matrices on distributed memory parallel computers. </title> <institution> Computer Science Department Techical Report CS-95-273, University of Tennessee at Knoxville, </institution> <year> 1995. </year> <note> To appear in SIAM J. Sci. Comp. </note>
Reference-contexts: Performance evaluation of the matrix sign function based algorithm on parallel distributed memory machines, such as the Intel Delta and CM-5, is reported in <ref> [3] </ref>.
Reference: [4] <author> Z. Bai, J. Demmel, and M. Gu. </author> <title> Inverse free parallel spectral divide and conquer algorithms for nonsymmetric eigenproblems. </title> <institution> Department of Mathematics RR 94-01, University of Kentucky, </institution> <year> 1994. </year> <note> To appear in Numer. Math. </note>
Reference-contexts: However, it avoids the matrix inverse. Numerically, this is a more promising approach. The new approach is based on the work of Bulgakov, Godunov and Malyshev [9, 24, 25]. In <ref> [4] </ref>, we have improved their results in several important ways, and made it a truly practical and inverse free highly parallel algorithm for both the standard and generalized spectral decomposition problems. In brief, the difference between the matrix sign function and inverse free methods is as follows. <p> The matrix sign function method is significantly faster than inverse free method when it converges, but there are some very difficult problems where the inverse free algorithm gives a more accurate answer than the matrix sign function algorithm. The interested reader may see the report <ref> [4] </ref> for details.
Reference: [5] <author> S. Batterson. </author> <title> Convergence of the shifted QR algorithm on 3 by 3 normal matrices. </title> <journal> Num. Math., </journal> <volume> 58 </volume> <pages> 341-352, </pages> <year> 1990. </year>
Reference-contexts: Numerical software for the QR algorithm is available in EISPACK [28] and LAPACK [1]. Although nonconvergent examples have been found, they are quite rare in practice <ref> [5, 14] </ref>. We note that the eigenvalues on the (block)-diagonal of T may appear in any order. Therefore, if an application requires an invariant subspace corresponding to the eigenvalues in a specific region in complex plane, a second step of reordering eigenvalues on the diagonal of T is necessary.
Reference: [6] <author> A. Bojanczyk and P. Van Dooren. </author> <type> personal communication, </type> <year> 1993. </year>
Reference-contexts: Therefore, if an application requires an invariant subspace corresponding to the eigenvalues in a specific region in complex plane, a second step of reordering eigenvalues on the diagonal of T is necessary. A guaranteed stable implementation of this reordering is described in <ref> [6] </ref>. The matrix sign function based algorithm can be regarded as an algorithm to combine these two steps into one.
Reference: [7] <author> S. Boyd and V. Balakrishnan. </author> <title> A regularity result for the singular values of a transfer matrix and a quadratically convergent algorithm for computing its L 1 -norm. </title> <journal> Systems Control Letters, </journal> <volume> 15 </volume> <pages> 1-7, </pages> <year> 1990. </year>
Reference-contexts: It is natural to take ! 2 = 1=d 2 A as the condition number of the matrix sign function. Algorithms for computing d A and related problems can be found in <ref> [12, 8, 7, 10] </ref>. 4. The bound (2.7) is similar to the bound of the norm of the Frechet derivative of the matrix sign function of A at X given by Roberts [27]: kF (sign (A); X)k l i on where l is the length of the closed contour .
Reference: [8] <author> S. Boyd, V. Balakrishnan, and P. Kabamba. </author> <title> A bisection method for computing the H 1 norm of a transfer matrix and related problems. Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> 2(3) </volume> <pages> 207-219, </pages> <year> 1989. </year>
Reference-contexts: It is natural to take ! 2 = 1=d 2 A as the condition number of the matrix sign function. Algorithms for computing d A and related problems can be found in <ref> [12, 8, 7, 10] </ref>. 4. The bound (2.7) is similar to the bound of the norm of the Frechet derivative of the matrix sign function of A at X given by Roberts [27]: kF (sign (A); X)k l i on where l is the length of the closed contour .
Reference: [9] <author> A. Ya. Bulgakov and S. K. </author> <title> Godunov. Circular dichotomy of the spectrum of a matrix. </title> <journal> Siberian Math. J., </journal> <volume> 29(5) </volume> <pages> 734-744, </pages> <year> 1988. </year> <month> 23 </month>
Reference-contexts: However, it avoids the matrix inverse. Numerically, this is a more promising approach. The new approach is based on the work of Bulgakov, Godunov and Malyshev <ref> [9, 24, 25] </ref>. In [4], we have improved their results in several important ways, and made it a truly practical and inverse free highly parallel algorithm for both the standard and generalized spectral decomposition problems.
Reference: [10] <author> R. Byers. </author> <title> A bisection method for measuring the distance of a stable matrix to the unstable matrices. </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> 9(5) </volume> <pages> 875-881, </pages> <year> 1988. </year>
Reference-contexts: It is natural to take ! 2 = 1=d 2 A as the condition number of the matrix sign function. Algorithms for computing d A and related problems can be found in <ref> [12, 8, 7, 10] </ref>. 4. The bound (2.7) is similar to the bound of the norm of the Frechet derivative of the matrix sign function of A at X given by Roberts [27]: kF (sign (A); X)k l i on where l is the length of the closed contour .
Reference: [11] <author> R. Byers, C. He, and V. Mehrmann. </author> <title> The matrix sign function method and the computation of invariant subspaces. </title> <type> Technical Report Preprint SPC 94-25, </type> <institution> Fakultat fur Mathematik, TU Chemnitz-Zwickau, Germany, </institution> <year> 1994. </year>
Reference-contexts: Recently, an asymptotic perturbation bound of sign (A) was given by Byers, He and Mehrmann <ref> [11] </ref>. <p> We note that the error bound (4.23) is essentially the same as the error bound given by Byers, He and Mehrmann <ref> [11] </ref>, although we use a different approach. In [11], it is assumed that kEk &lt; O (u)kSk in (4.19). Therefore, O ( p u) term in (4.23) is replaced by O (u). We consider the O (u)kSk accuracy for the computed matrix sign function to be an unrealistic assumption. <p> We note that the error bound (4.23) is essentially the same as the error bound given by Byers, He and Mehrmann <ref> [11] </ref>, although we use a different approach. In [11], it is assumed that kEk &lt; O (u)kSk in (4.19). Therefore, O ( p u) term in (4.23) is replaced by O (u). We consider the O (u)kSk accuracy for the computed matrix sign function to be an unrealistic assumption. <p> It is interestingn to ask whether error bound (4.22) or (4.23) is sharper, i.e., which one of the quantities d A and ffi = sep (A 11 ; A 22 ) is larger. In <ref> [11] </ref>, an example of a 2 by 2 matrix is given to show that the quantity ffi is larger than the quantity d A . However, we can also devise simple examples to show that d A can be larger than ffi = sep (A 11 ; A 22 ).
Reference: [12] <author> R. Byers and N. K. Nichols. </author> <title> On the stability radius of a generalized state-space system. </title> <journal> Lin. Alg. Appl., </journal> <volume> 188-189:113-134, </volume> <year> 1993. </year>
Reference-contexts: It is natural to take ! 2 = 1=d 2 A as the condition number of the matrix sign function. Algorithms for computing d A and related problems can be found in <ref> [12, 8, 7, 10] </ref>. 4. The bound (2.7) is similar to the bound of the norm of the Frechet derivative of the matrix sign function of A at X given by Roberts [27]: kF (sign (A); X)k l i on where l is the length of the closed contour .
Reference: [13] <author> F. Chatelin. </author> <title> Simultaneous Newton's iteration for the eigenproblem. </title> <journal> Comput. </journal> <volume> Suppl., 5 </volume> <pages> 67-74, </pages> <year> 1984. </year>
Reference-contexts: If higher accuracy is desired, we may use iterative refinement techniques to improve the accuracy of computed invariant subspace. There are two classes of methods, the first one is due to Stewart [29], Dongarra, Moler and Wilkinson [18], and Chatelin <ref> [13] </ref>. Even though these methods all apparently solve different equations, as shown by Demmel [17], after changing variables, they all solve the same Riccati equation in the inner loop.
Reference: [14] <author> D. Day. </author> <title> How the QRalgorithm fails to converge and how to fix it. </title> <note> in preparation. </note>
Reference-contexts: Numerical software for the QR algorithm is available in EISPACK [28] and LAPACK [1]. Although nonconvergent examples have been found, they are quite rare in practice <ref> [5, 14] </ref>. We note that the eigenvalues on the (block)-diagonal of T may appear in any order. Therefore, if an application requires an invariant subspace corresponding to the eigenvalues in a specific region in complex plane, a second step of reordering eigenvalues on the diagonal of T is necessary.
Reference: [15] <author> J. Demmel. </author> <title> The condition number of equivalence transformations that block diagonal-ize matrix pencils. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 20(3) </volume> <pages> 599-610, </pages> <year> 1983. </year>
Reference-contexts: This is a straightforward calculation, with the minimum being obtained for i = 0 and oe j = kRk. 2. The condition number of a semi-simple eigenvalue is equal to the secant of the acute angle between its left and right eigenvectors <ref> [22, 15] </ref>. Using the above reduction to 2 by 2 subproblems (this unitary transformation of coordinates does not changes angles between vectors), this is again a straightforward calculation. 3.
Reference: [16] <author> J. Demmel. </author> <title> On condition numbers and the distance to the nearest ill-posed problem. </title> <journal> Num. Math., </journal> <volume> 51(3) </volume> <pages> 251-289, </pages> <year> 1987. </year>
Reference-contexts: In theory, in the worst case, such a deflation procedure may be also needed for the intermediate matrices in the Newton iteration. We now look more closely at the situation of near convergence of the Newton iteration, and relate the error to the distance to the nearest ill-posed problem <ref> [16] </ref>. As before, the ill-posed problems are those matrices with pure-imaginary eigenvalues. Without loss of generality, let us assume A is of the form A = A 11 A 12 ! where (A 11 ) 2 C + and (A 22 ) 2 C . <p> This is 14 reminiscent of the difference between the quantities ffi = sep (A 11 ; A 22 ) and sep (A 11 ; A 22 ) <ref> [16] </ref>. In practice, we will use the a posteriori bound kE 21 k=kAk anyway, since if we block upper-triangularize Q H A Q by setting the (2; 1) block to zero, kE 21 k=kAk is precisely the backward error we introduce. <p> Table 3 lists the sep (A 11 ; A 22 ), the number of iterative refinement steps and the backward accuracy of improved invariant subspace. As shown in the convergence analysis for the iterative solvers (6.26) and (6.27) of the Ric-cati equation by Stewart [29] and Demmel <ref> [16] </ref>, if we let = (kA 12 k F kE 21 k F )=sep 2 (A 11 ; A 22 ), then under the assumptions k &lt; 1=4 and k &lt; 1=12, the iterations (6.26) and (6.27) converge, respectively.
Reference: [17] <author> J. Demmel. </author> <title> Three methods for refining estimates of invariant subspaces. </title> <journal> Computing, </journal> <volume> 38 </volume> <pages> 43-57, </pages> <year> 1987. </year>
Reference-contexts: There are two classes of methods, the first one is due to Stewart [29], Dongarra, Moler and Wilkinson [18], and Chatelin [13]. Even though these methods all apparently solve different equations, as shown by Demmel <ref> [17] </ref>, after changing variables, they all solve the same Riccati equation in the inner loop.
Reference: [18] <author> J. Dongarra, C. Moler, and J. H. Wilkinson. </author> <title> Improving the accuracy of computed eigenvalues and eigenvectors. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 20 </volume> <pages> 46-58, </pages> <year> 1984. </year>
Reference-contexts: If higher accuracy is desired, we may use iterative refinement techniques to improve the accuracy of computed invariant subspace. There are two classes of methods, the first one is due to Stewart [29], Dongarra, Moler and Wilkinson <ref> [18] </ref>, and Chatelin [13]. Even though these methods all apparently solve different equations, as shown by Demmel [17], after changing variables, they all solve the same Riccati equation in the inner loop.
Reference: [19] <author> B. S. Garbow, J. M. Boyle, J. J. Dongarra, and C. B. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide Extension, </title> <booktitle> volume 51 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1977. </year>
Reference-contexts: We discussed how these tools can be used in different combinations on different problems and architectures for extracting all or part of eigenvalues of a nonsymmetric matrix and/or their corresponding invariant subspaces. Rather than using "black box" eigenroutines such as provided by EISPACK <ref> [28, 19] </ref> and LAPACK [1], we expect the toolbox approach to allow us more flexibility in developing efficient problem-oriented eigenproblem solvers on high performance machines, especially on parallel distributed memory machines.
Reference: [20] <author> J. Gardiner and A. Laub. </author> <title> A generalization of the matrix-sign function solution for algebraic Riccati equations. </title> <journal> Int. J. Control, </journal> <volume> 44 </volume> <pages> 823-832, </pages> <year> 1986. </year>
Reference-contexts: A matrix pencil A B is regular if A B is square and det (A B) is not identically zero. In <ref> [20] </ref>, Gardiner and Laub have considered an extension of the Newton iteration for computing the matrix sign function to a matrix pencil for solving generalized algebraic Riccati equations. Here we discuss another possible approach, which includes the computation of both left and right deflating subspaces.
Reference: [21] <author> N. J. Higham. </author> <title> The matrix sign decomposition and its relation to the polar decomposition. </title> <journal> Lin. Alg. Appl., </journal> 212/213:3-20, 1994. 
Reference-contexts: It is desirable to have a perturbation analysis of the matrix sign function related to the distance from A to the nearest ill-posed problem. We will denote this distance by d A . Perturbation theory and condition number estimation of the matrix sign function are discussed in <ref> [23, 21, 26] </ref>. However, none of the existing error bounds explicitly reveals the relationship between the sensitivity of the matrix sign function and the distance to the nearest ill-posed problem. In this section, we will derive a new perturbation bound which explicitly reveals such relationship.
Reference: [22] <author> T. Kato. </author> <title> Perturbation Theory for Linear Operators. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, 2 edition, </address> <year> 1980. </year>
Reference-contexts: We only prove the bound (2.7). The bound (2.5) can be proved by using a similar technique to that illustrated below. Following Roberts [27] (or Kato <ref> [22] </ref>), the matrix sign function can also be defined using Cauchy integral representation: sign (A) = 2 sign + (A) I (2.8) where sign + (A) = 1 Z (iI A) 1 di; is any simple closed curve with positive direction enclosing + (A). sign + (A) is the spectral projector <p> This is a straightforward calculation, with the minimum being obtained for i = 0 and oe j = kRk. 2. The condition number of a semi-simple eigenvalue is equal to the secant of the acute angle between its left and right eigenvectors <ref> [22, 15] </ref>. Using the above reduction to 2 by 2 subproblems (this unitary transformation of coordinates does not changes angles between vectors), this is again a straightforward calculation. 3.
Reference: [23] <author> C. Kenney and A. Laub. </author> <title> Polar decomposition and matrix sign function condition estimates. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 12 </volume> <pages> 488-504, </pages> <year> 1991. </year>
Reference-contexts: It is desirable to have a perturbation analysis of the matrix sign function related to the distance from A to the nearest ill-posed problem. We will denote this distance by d A . Perturbation theory and condition number estimation of the matrix sign function are discussed in <ref> [23, 21, 26] </ref>. However, none of the existing error bounds explicitly reveals the relationship between the sensitivity of the matrix sign function and the distance to the nearest ill-posed problem. In this section, we will derive a new perturbation bound which explicitly reveals such relationship.
Reference: [24] <author> A. N. Malyshev. </author> <title> Guaranteed accuracy in spectral problems of linear algebra, I,II. </title> <institution> Siberian Adv. in Math., 2(1,2):144-197,153-204, </institution> <year> 1992. </year>
Reference-contexts: However, it avoids the matrix inverse. Numerically, this is a more promising approach. The new approach is based on the work of Bulgakov, Godunov and Malyshev <ref> [9, 24, 25] </ref>. In [4], we have improved their results in several important ways, and made it a truly practical and inverse free highly parallel algorithm for both the standard and generalized spectral decomposition problems.
Reference: [25] <author> A. N. Malyshev. </author> <title> Parallel algorithm for solving some spectral problems of linear algebra. </title> <journal> Lin. Alg. Appl., </journal> <volume> 188,189:489-520, </volume> <year> 1993. </year>
Reference-contexts: However, it avoids the matrix inverse. Numerically, this is a more promising approach. The new approach is based on the work of Bulgakov, Godunov and Malyshev <ref> [9, 24, 25] </ref>. In [4], we have improved their results in several important ways, and made it a truly practical and inverse free highly parallel algorithm for both the standard and generalized spectral decomposition problems.
Reference: [26] <author> R. Mathias. </author> <title> Condition estimation for the matrix sign function via the Schur decomposition. </title> <editor> In J. G. Lewis, editor, </editor> <booktitle> Proceedings of the Fifth SIAM Conference on Applied Linear Algebra. </booktitle> <publisher> SIAM, </publisher> <year> 1994. </year> <month> 24 </month>
Reference-contexts: It is desirable to have a perturbation analysis of the matrix sign function related to the distance from A to the nearest ill-posed problem. We will denote this distance by d A . Perturbation theory and condition number estimation of the matrix sign function are discussed in <ref> [23, 21, 26] </ref>. However, none of the existing error bounds explicitly reveals the relationship between the sensitivity of the matrix sign function and the distance to the nearest ill-posed problem. In this section, we will derive a new perturbation bound which explicitly reveals such relationship.
Reference: [27] <author> J. Roberts. </author> <title> Linear model reduction and solution of the algebraic Riccati equation. </title> <journal> Inter. J. Control, </journal> <volume> 32 </volume> <pages> 677-687, </pages> <year> 1980. </year>
Reference-contexts: In this report, we will address these issues. Let us first restate some of basic definitions and ideas in Part I to establish notation. The matrix sign function of a matrix A is defined as follows <ref> [27] </ref>: Let A = X diag (J + ; J )X 1 be the Jordan canonical form of a matrix A 2 C nfin , where the eigenvalues of J + lie in the open right half plane (C + ) and those of J lie in the open left half <p> We only prove the bound (2.7). The bound (2.5) can be proved by using a similar technique to that illustrated below. Following Roberts <ref> [27] </ref> (or Kato [22]), the matrix sign function can also be defined using Cauchy integral representation: sign (A) = 2 sign + (A) I (2.8) where sign + (A) = 1 Z (iI A) 1 di; is any simple closed curve with positive direction enclosing + (A). sign + (A) is <p> Algorithms for computing d A and related problems can be found in [12, 8, 7, 10]. 4. The bound (2.7) is similar to the bound of the norm of the Frechet derivative of the matrix sign function of A at X given by Roberts <ref> [27] </ref>: kF (sign (A); X)k l i on where l is the length of the closed contour . Recently, an asymptotic perturbation bound of sign (A) was given by Byers, He and Mehrmann [11].
Reference: [28] <author> B. T. Smith, J. M. Boyle, J. J. Dongarra, B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide, </title> <booktitle> volume 6 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1976. </year>
Reference-contexts: We discussed how these tools can be used in different combinations on different problems and architectures for extracting all or part of eigenvalues of a nonsymmetric matrix and/or their corresponding invariant subspaces. Rather than using "black box" eigenroutines such as provided by EISPACK <ref> [28, 19] </ref> and LAPACK [1], we expect the toolbox approach to allow us more flexibility in developing efficient problem-oriented eigenproblem solvers on high performance machines, especially on parallel distributed memory machines. <p> The computed Schur form (quasi-triangular matrix) T and Schur vectors Q by the QR algorithm satisfy Q T (A + E) Q = T ; where E is of the order of ukAk. Numerical software for the QR algorithm is available in EISPACK <ref> [28] </ref> and LAPACK [1]. Although nonconvergent examples have been found, they are quite rare in practice [5, 14]. We note that the eigenvalues on the (block)-diagonal of T may appear in any order.
Reference: [29] <author> G. W. Stewart. </author> <title> Error and perturbation bounds for subspaces associated with certain eigenvalue problems. </title> <journal> SIAM Review, </journal> <volume> 15(4) </volume> <pages> 727-764, </pages> <year> 1973. </year>
Reference-contexts: (A)k 4 kA 12 k 2 kffiAk; (2.9) where A is assumed to have the form of (1.1), kffiAk is sufficiently small, and ffi = sep (A 11 ; A 22 ) = oe min (I A 11 A T the separation of the matrices A 11 and A 22 <ref> [29] </ref>. is the Kronecker product. Comparing the bounds (2.7) and (2.9), we note that first the bound (2.7) is a global bound and (2.9) is an asymptotic bound. Second, the assumption (2.6) for the bound (2.7) has a simple geometric interpretation (see Remark 2 above). <p> This is in turn a simple calculation. We note that for the solution R of the Sylvester equation (3.12), we have kRk sep (A 11 ; A 22 ) where the equality is attainable <ref> [29] </ref>. From Lemma 1, we see that the conditioning of the matrix sign function computation is closely related to the norm of the projection P , therefore the norm of R, which in turn is closely related to the quantity sep (A 11 ; A 22 ). <p> Following Stewart <ref> [29] </ref>, it means that it is harder to separate the invariant subspaces corresponding to the matrices A 11 and A 22 . The following theorem discusses the conditioning of the eigenvalues of sign (A), and the distance from sign (A) to the nearest ill-posed problem. <p> If higher accuracy is desired, we may use iterative refinement techniques to improve the accuracy of computed invariant subspace. There are two classes of methods, the first one is due to Stewart <ref> [29] </ref>, Dongarra, Moler and Wilkinson [18], and Chatelin [13]. Even though these methods all apparently solve different equations, as shown by Demmel [17], after changing variables, they all solve the same Riccati equation in the inner loop. <p> Table 3 lists the sep (A 11 ; A 22 ), the number of iterative refinement steps and the backward accuracy of improved invariant subspace. As shown in the convergence analysis for the iterative solvers (6.26) and (6.27) of the Ric-cati equation by Stewart <ref> [29] </ref> and Demmel [16], if we let = (kA 12 k F kE 21 k F )=sep 2 (A 11 ; A 22 ), then under the assumptions k &lt; 1=4 and k &lt; 1=12, the iterations (6.26) and (6.27) converge, respectively.
Reference: [30] <author> G. W. Stewart. </author> <title> A Jacobi-like algorithm for computing the Schur decomposition of a non-Hermitian matrix. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 6 </volume> <pages> 853-864, </pages> <year> 1985. </year>
Reference-contexts: The second class of methods can be described as a Jacobi type technique to compute the Schur decomposition of a nonhermitian matrix <ref> [30] </ref>, although originally, it is not motivated for refining estimates of an invariant subspace. <p> The method we describe here is due to Stewart <ref> [30] </ref>, which was proposed to compute the Schur decomposition of a nonhermitian matrix in parallel. <p> The scheme is ultimately quadratic convergent. Therefore, in general, we can expect to reduce the * 2 entries to the order of * 1 entries in only about one to two sweeps. The method is fine grain and suitable for parallel implementation <ref> [30, 33] </ref>.
Reference: [31] <author> G. W. Stewart and J.-G. Sun. </author> <title> Matrix Perturbation Theory. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: In practice, of course, this is a question of rank determination. &gt;From the well-known perturbation theory of the singular value decomposition (see <ref> [31, page 260] </ref>, for example), the space spanned by the corresponding singular vectors is perturbed by at most O (j)=gap S , where gap S is the shortest distance from any singular value in S to any singular value not in S: gap S j min oe 62 S In order
Reference: [32] <author> L. N. Trefethen. </author> <title> Pseudospectra of matrices. </title> <booktitle> In 1991 Dundee Numerical Analysis Conference Proceedings, </booktitle> <address> Dundee, Scotland, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: The desired bound (2.7) follows from the bounds on kI 1 k and kI 2 k and the identity sign (A + ffiA) sign (A) = 2 (sign + (A + ffiA) sign + (A)): A few remarks are in order: 1. In the language of pseudospectra <ref> [32] </ref>, the condition (2.6) means that the kffiAk pseudospectra of A do not cross the pure imaginary axis. 2. <p> Theorem 2 Let A and R be as in Lemma 1. Then 1. Let ffiS have the property that S + ffiS has a pure imaginary eigenvalue. Then ffiS may be chosen with kffiSk = 1=kSk but no smaller. In the language of <ref> [32] </ref>, the *- pseudospectrum of S excludes the imaginary axis for * &lt; 1=kSk, and intersects it for * 1=kSk. 2. The condition number of the eigenvalues of S is kP k.
Reference: [33] <author> R. van de Geijn. </author> <title> A novel storage scheme for parallel jacobi methods. </title> <institution> Computer Science Dept. TR-88-26, University of Texas, Austin, TX, </institution> <month> July </month> <year> 1988. </year>
Reference-contexts: The scheme is ultimately quadratic convergent. Therefore, in general, we can expect to reduce the * 2 entries to the order of * 1 entries in only about one to two sweeps. The method is fine grain and suitable for parallel implementation <ref> [30, 33] </ref>.
References-found: 33

