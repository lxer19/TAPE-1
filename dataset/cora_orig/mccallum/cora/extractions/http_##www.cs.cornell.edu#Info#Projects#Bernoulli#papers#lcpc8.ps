URL: http://www.cs.cornell.edu/Info/Projects/Bernoulli/papers/lcpc8.ps
Refering-URL: http://www.cs.cornell.edu/Info/Projects/Bernoulli/
Root-URL: http://www.cs.cornell.edu
Title: Automatic Parallelization of the Conjugate Gradient Algorithm  
Author: Vladimir Kotlyar, Keshav Pingali, and Paul Stodghill 
Address: Ithaca, New York 14853  
Affiliation: Department of Computer Science Cornell University  Argonne.  
Abstract: The conjugate gradient (CG) method is a popular Krylov space method for solving systems of linear equations of the form Ax = b, where A is a symmetric positive-definite matrix. This method can be applied regardless of whether A is dense or sparse. In this paper, we show how restructuring compiler technology can be applied to transform a sequential, dense matrix CG program into a parallel, sparse matrix CG program. On the IBM SP-2, the performance of our compiled code is comparable to that of handwritten code from the PETSc library at 
Abstract-found: 1
Intro-found: 1
Reference: [AL93] <author> Jennifer M. Anderson and Monica S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 112 - 125, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Therefore, eliminating constraints will result in communication in the final program. The problem of determining which constraints to remove to produce the best alignment solution is NP-hard. In practice, heuristics are used to select the constraints to remove <ref> [CGS93, Fea92, AL93] </ref>. 5.2 Alignments for CG in Handwritten Code There are two alignments that are used for conjugate gradient. The one with the highest degree of parallelism is an alignment with a 2D template space.
Reference: [Bas95] <author> Achim Basermann. </author> <title> Parallel sparse matrix computations in iterative solvers on distributed memory machines. </title> <booktitle> In Proceedings of the SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, </address> <month> February </month> <year> 1995. </year> <note> SIAM Press. </note>
Reference-contexts: The transformed code is better suited for parallelization. It has been demonstrated in <ref> [Bas95] </ref> that an effective and scalable means of parallelization is to distribute sets of rows of A and the corresponding elements of x and y to processors. Once this is done, the scaling loop can be executed in parallel, without communication. <p> However, this alignment is most appropriate when either A is dense or when the non-zeros are uniformly distributed in A. These assumptions are not met in sparse matrices arising in practice from finite element problems and the like, and a 1D alignment turns out to be superior <ref> [Bas95] </ref>.
Reference: [BBLS91] <author> D.H. Bailey, J.T. Barton, T.A. Lasinski, and H.D. Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report RNR-91-02, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> January </month> <year> 1991. </year>
Reference: [BKK + 94] <author> David Bau, Induprakas Kodukula, Vladimir Kotlyar, Keshav Pingali, and Paul Stodghil. </author> <title> Solving alignment using simple linear algebra. </title> <editor> In K. Pingali, U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. Seventh International Workshop., </booktitle> <publisher> LNCS. LNCS, Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: However, this trivial solution exhibits no parallelism. We are interested only in solutions that exhibit some degree of parallelism, or non-trivial solutions. We have developed an algorithm for solving such systems of constraints and producing a solution with the largest degree of parallelism <ref> [BKK + 94] </ref>. If a non-trivial solution is found, we are done. Otherwise, we have an overconstrained system and we must drop one or more constraints to obtain a non-trivial solution. Each removed constraint represents a set of references that will be misaligned.
Reference: [BKW94] <author> Aart J.C. Bik, Peter M.W. Knijnenburg, and Harry A.G. Wijshoff. </author> <title> Reshaping access patterns for generating sparse codes. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Languages and Compiler for Parallel Computing, </booktitle> <address> Ithaca, New York, </address> <month> August 8-10, </month> <year> 1994. </year> <note> Springer-Verlag. LNCS #892. </note>
Reference-contexts: The Sparse Compiler has a fairly general annotation language for describing the sparsity of the data structures, and the compiler's job is to introduce sparsity guards into the code, to optimize these guards, and to select and generate an appropriate compressed data layout. In <ref> [BKW94] </ref>, loop transformations are done to try to align array references so that access to the selected compressed data layout is more efficient.
Reference: [BW93a] <author> Aart Bik and Harry Wijshoff. </author> <title> Advanced compiler optimizations for sparse computations. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 430-439, </pages> <month> Novem-ber </month> <year> 1993. </year>
Reference-contexts: However, they give up the ability to analyze and restructure codes at compile-time. Wijshoff and co-workers have studied the problem of sparsification in the context of sequential and uniform memory access computers <ref> [BW93a] </ref>. In contrast, we deal with distributed memory parallel machines in which locality of data is the driving concern. How well do we actually do? The performance of our compiled code on the IBM SP-2 is comparable to that of handwritten code in the PETSc library from Argonne [SG94].
Reference: [BW93b] <author> Aart Bik and Harry Wijshoff. </author> <title> Compilation techniques for sparse matrix computations. </title> <booktitle> In Proceedings of the International Conference on Supercomputing. </booktitle> <publisher> ACM Press, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: These issues require more work. 8 Related work Our ideas on parallel, sparse compiling have been influenced most heavily by the work by Bik, et al. on the Sparse Compiler and Saltz, et al. on PARTI/Chaos. The Sparse Compiler is introduced in <ref> [BW93b] </ref>. The input source code is sequential, dense code, as in our case, and the output is sequential, sparse code.
Reference: [CGS93] <author> Siddartha Chatterjee, John Gilbert, and Robert Schreiber. </author> <title> The alignment-distribution graph. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. Sixth International Workshop., number 768 in LNCS. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Therefore, eliminating constraints will result in communication in the final program. The problem of determining which constraints to remove to produce the best alignment solution is NP-hard. In practice, heuristics are used to select the constraints to remove <ref> [CGS93, Fea92, AL93] </ref>. 5.2 Alignments for CG in Handwritten Code There are two alignments that are used for conjugate gradient. The one with the highest degree of parallelism is an alignment with a 2D template space.
Reference: [DPSM91] <author> Raja Das, Ravi Ponnusamy, Joel Saltz, and Dimitri Mavriplis. </author> <title> Distributed memory compiler methods for irregular problems data copy reuse and runtime partitioning. </title> <type> ICASE Report 91-73, </type> <institution> Institute for Computer Applications in Science and Engineering, NASA Langley Research Center, Hamp-ton, Virginia, </institution> <month> September </month> <year> 1991. </year>
Reference: [DUSH94] <author> Raja Das, Mustafa Uysal, Joel Saltz, and Yuan-Shin Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 462-479, </pages> <month> September </month> <year> 1994. </year> <note> Also available as University of Maryland Technical Report CS-TR-3163 and UMIACS-TR-93-109. </note>
Reference: [Fea92] <author> Paul Feautrier. </author> <title> Toward automatic distribution. </title> <type> Technical Report 92.95, </type> <institution> IBP/MASI, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Therefore, eliminating constraints will result in communication in the final program. The problem of determining which constraints to remove to produce the best alignment solution is NP-hard. In practice, heuristics are used to select the constraints to remove <ref> [CGS93, Fea92, AL93] </ref>. 5.2 Alignments for CG in Handwritten Code There are two alignments that are used for conjugate gradient. The one with the highest degree of parallelism is an alignment with a 2D template space.
Reference: [KW94] <author> P.M.W. Knijnenburg and H.A.G. Wijshoff. </author> <title> On improving data locality in sparse matrix computations. </title> <type> Technical Report 94-15, </type> <institution> Department of Computer Science, Leiden University, </institution> <year> 1994. </year>
Reference: [LRT79] <author> R. J. Lipton, D. J. Rose, and R. E. Tarjan. </author> <title> Generalized nested dissection. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16 </volume> <pages> 346-358, </pages> <year> 1979. </year>
Reference-contexts: But, in unstructured sparse problems, these regular distributions do not do as good a job as irregular distributions. In unstructured sparse problems, the best distributions are usually found by explicitly constructing a graph representing the communication, and using node partitioning techniques like recursive spectral bisection <ref> [MTTV93, LRT79, PSL90] </ref> to compute the distribution function. However, the irregularity of the resulting distribution creates some problems for the compiler. The first is that, because the resulting distribution is irregular, it is unlikely to have a nice linear closed form, like the block/cyclic distributions do.
Reference: [Mac94] <editor> PDEase Programmer's Manual. </editor> <publisher> Macsyma Inc., </publisher> <address> 20 Academy Street, Arling-ton, MA 02174., </address> <year> 1994. </year>
Reference-contexts: From our perspective, this algorithm has three attractions. First, it is very popular; for example, it is the primary solver in Macsyma's PDEase finite element package <ref> [Mac94] </ref>. Therefore, this problem is of real interest. Second, matrix computations in CG do not cause fill. Finally, there are many handwritten versions of this code available for study. The CG algorithm can be applied regardless of whether A is dense or sparse.
Reference: [MTTV93] <author> G. L. Miller, S.-H. Teng, W. Thurston, and S. A. Vavasis. </author> <title> Automatic mesh partitioning. </title> <editor> In A. George, J. Gilbert, and J. Liu, editors, </editor> <title> Graph Theory and Sparse Matrix Computation, volume 56 of IMA Volumes in Mathematics and its Applications. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: But, in unstructured sparse problems, these regular distributions do not do as good a job as irregular distributions. In unstructured sparse problems, the best distributions are usually found by explicitly constructing a graph representing the communication, and using node partitioning techniques like recursive spectral bisection <ref> [MTTV93, LRT79, PSL90] </ref> to compute the distribution function. However, the irregularity of the resulting distribution creates some problems for the compiler. The first is that, because the resulting distribution is irregular, it is unlikely to have a nice linear closed form, like the block/cyclic distributions do.
Reference: [PSC + 93] <author> Ravi Ponnusamy, Joel Saltz, Alok Choudhary, Yuan-Shin Hwang, and Geof-frey Fox. </author> <title> Runtime support and compilation methods for user-specified data distributions. </title> <institution> Technical Report CS-TR-3194 and UMIACS-TR-93-135, University of Maryland, </institution> <month> November </month> <year> 1993. </year> <note> To appear in IEEE Transactions on Parallel and Distributed Systems. </note>
Reference-contexts: Some examples (not meant to be exhaustive) of this approach are found in work that has been done on the Rice and Syracuse Fortran D compilers in combination with the PARTI and Chaos runtime systems ([DUSH94], <ref> [PSC + 93] </ref>, [WDS + 95], [SPM + 94]). In this work, the programmer gives the compiler code, similar to that shown in Figure 3 or Figure 4, augmented with alignment and distribution information.
Reference: [PSL90] <author> A. Pothen, H. D. Simon, and K.-P. Liou. </author> <title> Partitioning sparse matrices with eigenvectors of graphs. </title> <journal> SIAM Journal of Matrix Analysis and Applications, </journal> <volume> 11 </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference-contexts: But, in unstructured sparse problems, these regular distributions do not do as good a job as irregular distributions. In unstructured sparse problems, the best distributions are usually found by explicitly constructing a graph representing the communication, and using node partitioning techniques like recursive spectral bisection <ref> [MTTV93, LRT79, PSL90] </ref> to compute the distribution function. However, the irregularity of the resulting distribution creates some problems for the compiler. The first is that, because the resulting distribution is irregular, it is unlikely to have a nice linear closed form, like the block/cyclic distributions do.
Reference: [Pug91] <author> William Pugh. </author> <title> The Omega Test: A fast and practical integer programming algorithm for dependence analysis. </title> <booktitle> In Proceedings Supercomputing '91, </booktitle> <pages> pages 4-13, </pages> <address> Albequerque, New Mexico, </address> <month> November 18-22, </month> <year> 1991. </year>
Reference: [RP89] <author> Anne Rogers and Keshav Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 69-80, </pages> <address> Portland, Oregon, </address> <month> June 21-23, </month> <year> 1989. </year> <note> Published as ACM SIGPLAN Notices 24(7). </note>
Reference: [Saa89] <author> Youcef Saad. </author> <title> Kyrlov subspace methods on supercomputers. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 10(6) </volume> <pages> 1200-1232, </pages> <month> November </month> <year> 1989. </year>
Reference: [SG94] <author> B. Smith and W. Gropp. </author> <title> Portable, parallel, reusable krylov space codes. </title> <booktitle> In Colorado Conference on Iterative Methods, </booktitle> <address> Colorado, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: In contrast, we deal with distributed memory parallel machines in which locality of data is the driving concern. How well do we actually do? The performance of our compiled code on the IBM SP-2 is comparable to that of handwritten code in the PETSc library from Argonne <ref> [SG94] </ref>.
Reference: [SHM + 94] <author> John N. Shadid, Scott A. Hutchinson, Harry K. Moffat, Gary L. Hennigan, Bruce Hendrickson, and Robert W. Leland. </author> <title> A 65+ gflops/s unstructured finite element simulation of chemically reacting flows on the intel paragon. </title> <booktitle> In Supercomputing '94, </booktitle> <pages> pages 673-679, </pages> <address> Washington, D.C., </address> <month> November 14-18, </month> <year> 1994. </year>
Reference-contexts: In most applications, these matrices are very large and very sparse. For example, one of the Gordon Bell prizes in 1994 was won by a team that implemented a chemical vapor deposition (CVD) code in which the matrices had roughly one million rows and five hundred non-zeros per row <ref> [SHM + 94] </ref>. Since there is also a lot of parallelism in these computations, it is natural to use parallel computers in this context. Unfortunately, parallelizing sparse matrix codes is much more challenging than parallelizing dense matrix codes. One problem is keeping the computation to communication ratio high.
Reference: [SPM + 94] <author> Shamik D. Sharma, Ravi Ponnusamy, Bongki Moon, Yuan-Shin Hwang, Raja Das, and Joel Saltz. </author> <title> Run-time and compile-time support for adaptive irregular problems. </title> <booktitle> In Supercomputing '94, </booktitle> <address> Washington, D.C., </address> <month> November 14-18, </month> <year> 1994. </year>
Reference-contexts: Some examples (not meant to be exhaustive) of this approach are found in work that has been done on the Rice and Syracuse Fortran D compilers in combination with the PARTI and Chaos runtime systems ([DUSH94], [PSC + 93], [WDS + 95], <ref> [SPM + 94] </ref>). In this work, the programmer gives the compiler code, similar to that shown in Figure 3 or Figure 4, augmented with alignment and distribution information.
Reference: [SW90] <author> Youcef Saad and Harry A.G. Wijshoff. </author> <title> Spark: A benchmark package for sparse computations. </title> <booktitle> In Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <pages> pages 239-253, </pages> <address> Somewhere, Sometime, </address> <year> 1990. </year>
Reference: [WDS + 95] <author> Janet Wu, Raja Das, Joel Saltz, Harry Berryman, and Seema Hiranandani. </author> <title> Distributed memory compiler design for sparse problems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(6), </volume> <year> 1995. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: How does our work compare to other efforts in this area? The `inspector-executor' approach of Saltz and co-workers generates parallel sparse matrix code from sequential sparse matrix programs <ref> [WDS + 95] </ref>. They do not have our problem of `sparsification' | that is, converting dense matrix code into sparse matrix code. However, they give up the ability to analyze and restructure codes at compile-time. <p> Some examples (not meant to be exhaustive) of this approach are found in work that has been done on the Rice and Syracuse Fortran D compilers in combination with the PARTI and Chaos runtime systems ([DUSH94], [PSC + 93], <ref> [WDS + 95] </ref>, [SPM + 94]). In this work, the programmer gives the compiler code, similar to that shown in Figure 3 or Figure 4, augmented with alignment and distribution information.
References-found: 25

