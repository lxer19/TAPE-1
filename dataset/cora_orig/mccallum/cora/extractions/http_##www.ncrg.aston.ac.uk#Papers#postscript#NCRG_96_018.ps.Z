URL: http://www.ncrg.aston.ac.uk/Papers/postscript/NCRG_96_018.ps.Z
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Phone: 2  
Title: Adaptive Back-Propagation in On-Line Learning of Multilayer Networks  
Author: Ansgar H. L. West ; and David Saad 
Address: Edinburgh EH9 3JZ, U.K.  Birmingham B4 7ET, U.K.  
Affiliation: 1 Department of Physics, University of Edinburgh  Neural Computing Research Group, University of Aston  
Abstract: An adaptive back-propagation algorithm is studied and compared with gradient descent (standard back-propagation) for on-line learning in two-layer neural networks with an arbitrary number of hidden units. Within a statistical mechanics framework, both numerical studies and a rigorous analysis show that the adaptive back-propagation method results in faster training by breaking the symmetry between hidden units more efficiently and by providing faster convergence to optimal generalization than gradient descent.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Cybenko, </author> <title> Math. </title> <booktitle> Control Signals and Systems 2, </booktitle> <month> 303 </month> <year> (1989). </year>
Reference-contexts: 1 INTRODUCTION Multilayer feedforward perceptrons (MLPs) are widely used in classification and regression applications due to their ability to learn a range of complicated maps <ref> [1] </ref> from examples. When learning a map f 0 from N -dimensional inputs ~ to scalars the parameters fW g of the student network are adjusted according to some training algorithm so that the map defined by these parameters f W approximates the teacher f 0 as close as possible.
Reference: [2] <author> D. Saad and S. A. </author> <title> Solla, </title> <journal> Phys. Rev. </journal> <volume> E 52, </volume> <month> 4225 </month> <year> (1995). </year>
Reference-contexts: We measure the efficiency of these training algorithms by how fast (or whether at all) they converge to an "acceptable" generalization error. This research has been motivated by recent work <ref> [2] </ref> investigating an on-line learning scenario of a general two-layer student network trained by gradient descent on a task defined by a teacher network of similar architecture. <p> To investigate possible improvements we introduce an adaptive back-propagation algorithm, which improves the ability of the student to distinguish between hidden nodes of the teacher. We compare its efficiency with that of gradient descent in training two-layer networks following the framework of <ref> [2] </ref>. In this paper we present numerical studies and a rigorous analysis of both the breaking of the symmetric phase and the convergence to optimal performance. <p> The whole adaptive back-propagation training algorithm is therefore: A i (; fi; W ; ~ ; ) = N f ffi i ~ (3) with ffi as in Eq. (2). To compare the adaptive back-propagation algorithm with normal gradient descent, we follow the statistical mechanics calculation in <ref> [2] </ref>. Here we will only outline the main ideas and present the results of the calculation. As we are interested in the typical behaviour of our training algorithm we average over all possible instances of the examples ~. <p> The generalization error * g , measuring the typical performance, can be expressed in these variables only <ref> [2] </ref>. The order parameters Q ij and R in are the new dynamical variables, which are self-averaging with respect to the randomness in the training data in the thermodynamic limit (N ! 1). <p> n ~ dQ ij = e ffi i x j + e ffi j x i ~ D E : (4) All the integrals in Eqs. (4) and the generalization error can be calculated explicitly if we choose g (x) = erf (x= p 2) as the sigmoidal activation function <ref> [2] </ref>. The exact form of the resulting dynamical equations for adaptive back-propagation is similar to the equations in [2] and will be presented elsewhere [4]. They can easily be integrated numerically for any number of K student and M teacher hidden units. <p> E : (4) All the integrals in Eqs. (4) and the generalization error can be calculated explicitly if we choose g (x) = erf (x= p 2) as the sigmoidal activation function <ref> [2] </ref>. The exact form of the resulting dynamical equations for adaptive back-propagation is similar to the equations in [2] and will be presented elsewhere [4]. They can easily be integrated numerically for any number of K student and M teacher hidden units. <p> The fixed point of the truncated equations of motion Q fl = C fl = 2K 1 r K 1 K (2K 1) is independent of fi and thus identical to the one obtained in <ref> [2] </ref>. However, the symmetric solution is an unstable fixed point of the dynamics and the small perturbations introduced by the generically nonsymmetric initial conditions will eventually drive the student towards specialization. <p> are quite stable as the maximal learning rates, for which the learning process diverges, are about 30% higher than the optimal rates. 4 SUMMARY AND DISCUSSION This research has been motivated by the dominance of the suboptimal symmetric phase in on-line learning of two-layer feedforward networks trained by gradient descent <ref> [2] </ref>. This trapping is emphasized for inappropriate small learning rates but exists in all training scenarios, effecting the learning process considerably.
Reference: [3] <author> M. Biehl and H. Schwarze, J. </author> <note> Phys. A 28, 643 (1995). </note>
Reference-contexts: We find that adaptive back-propagation can significantly reduce training time in both regimes by breaking the symmetry between hidden units more efficiently and by providing faster exponential convergence to zero generalization error. 2 DERIVATION OF THE DYNAMICAL EQUATIONS The student network we consider is a soft committee machine <ref> [3] </ref>, consisting of K hidden units which are connected to N -dimensional inputs ~ by their weight vectors W = fW i g (i = 1; : : : ; K).
Reference: [4] <author> A. West and D. Saad, </author> <note> in preparation (1995). </note>
Reference-contexts: The exact form of the resulting dynamical equations for adaptive back-propagation is similar to the equations in [2] and will be presented elsewhere <ref> [4] </ref>. They can easily be integrated numerically for any number of K student and M teacher hidden units. For the remainder of the paper, we will however focus on the realizable case (K = M ) and uncorrelated isotropic teachers of unit length T nm = ffi nm . <p> We also find that the optimal learning rate opt , which exhibits the shortest symmetric phase, is significantly lower in this regime than during convergence <ref> [4] </ref>. During convergence, independent of which algorithm is used, the time constant for decay to zero generalization error scales with K, due to the necessary rescaling of the learning rate by 1=K as the typical quadratic deviation between teacher and student output increases proportional to K.
References-found: 4

