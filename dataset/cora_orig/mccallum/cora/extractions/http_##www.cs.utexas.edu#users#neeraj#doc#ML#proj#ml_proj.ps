URL: http://www.cs.utexas.edu/users/neeraj/doc/ML/proj/ml_proj.ps
Refering-URL: http://www.cs.utexas.edu/users/neeraj/professional.html
Root-URL: 
Email: neeraj@cs.utexas.edu  
Title: CS395T:Machine Learning  Using Hidden Neural Nodes as Features for C4.5  under the guidance of  
Author: Neeraj Garg Dr. Ray Mooney 
Date: December 17, 1997  
Pubnum: Project Report  
Abstract: Decision Tree algorithms lack the capacity to invent new features. Hidden units in feed forward neural network trained on the same input space, represent significant novel features learnt by the neural network. We use features represented by hidden units in addition to original features while constructing a decision tree. We expected smaller trees and better generalization. Experiments done over a number of domains indicate that these features are not always useful. As this system can represent much more complex hypotheses, the problem of over-fitting restricts the performance. The trees learnt are almost always shorter, though the decisions at nodes are now much more complex.
Abstract-found: 1
Intro-found: 1
Reference: [Breiman et al., 1984] <author> Breiman, L., Friedman, J., Olshen, R., and Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks/Cole. </publisher>
Reference-contexts: In [Brodley and Utgoff, 1992] Brodley and Utgoff tried to train a linear machine at every node. Their result was similar to ours that smaller trees are obtained if the bias of the domain matches the bias of the algorithm. CART <ref> [Breiman et al., 1984] </ref> also have complex decision nodes.
Reference: [Brodley and Utgoff, 1992] <author> Brodley, C. E. and Utgoff, P. E. </author> <year> (1992). </year> <title> Multivariate versus univariate decision trees. </title> <type> Technical report, </type> <institution> Department of Computer Sciences University of Massachussetts. </institution>
Reference-contexts: C4.5 [Quinlan, 1992] is a widely used algorithm with very good performance. However, it can use only the given features and lacks the ability to invent new features. As pointed out in <ref> [Brodley and Utgoff, 1992] </ref> in space of continuous features it can draw only those hyperplanes which are parallel to the axis. For example, in a two dimensional input space, it will discriminate on the basis of lines that are parallel to the x-axis or the y-axis. <p> The pitfall is, that trying our system out is costly in terms of execution time due to involvement of back propagation. 6 Related Work There have been various attempts at using more complex decision nodes. In <ref> [Brodley and Utgoff, 1992] </ref> Brodley and Utgoff tried to train a linear machine at every node. Their result was similar to ours that smaller trees are obtained if the bias of the domain matches the bias of the algorithm. CART [Breiman et al., 1984] also have complex decision nodes.
Reference: [Mitchell, 1997] <author> Mitchell, T. </author> <year> (1997). </year> <title> Machine Learning. </title> <publisher> McGraw Hill. </publisher>
Reference-contexts: For example, in a two dimensional input space, it will discriminate on the basis of lines that are parallel to the x-axis or the y-axis. As shown in Mitchell <ref> [Mitchell, 1997] </ref> points out that absence of complex features can cause exponential growth in the tree size. Another way to visualize the problem is to see that we cannot sum evidence.
Reference: [Quinlan, 1992] <author> Quinlan, R. J. </author> <year> (1992). </year> <title> C4.5; Programs for Machine Learning. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction 1.1 Need for Complex Features Decision Trees has been used widely as fast classifiers in a number of domains. C4.5 <ref> [Quinlan, 1992] </ref> is a widely used algorithm with very good performance. However, it can use only the given features and lacks the ability to invent new features.
Reference: [Utgoff, 1989] <author> Utgoff, P. </author> <year> (1989). </year> <title> Perceptron trees: A case study in hybrid concept representation. Connection Science. Set Curves Set Curves Set Curves Set Curves Curves Curves </title>
Reference-contexts: Their result was similar to ours that smaller trees are obtained if the bias of the domain matches the bias of the algorithm. CART [Breiman et al., 1984] also have complex decision nodes. Perceptron trees <ref> [Utgoff, 1989] </ref> are also very similar to our work. 7 Issues effecting Performance In this section, some practical issues which could have helped in improving the performance of our system, but could not be explored due to lack of time are listed. 1.
References-found: 5

