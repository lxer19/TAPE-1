URL: http://www-white.media.mit.edu:80/vismod/publications/techdir/TR-375.ps.Z
Refering-URL: http://www-white.media.mit.edu/~testarne/asl/index.html
Root-URL: http://www.media.mit.edu
Email: thad,sandy@media.mit.edu  
Title: Sign Language Recognition from Video Using Hidden Markov Models  
Author: Thad Starner and Alex Pentland 
Address: 20 Ames Street, Cambridge MA 02139  
Affiliation: Room E15-383, The Media Laboratory Massachusetts Institute of Technology  
Note: Real-Time American  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 375 An earlier version appeared ISCV '95 Abstract Hidden Markov models (HMM's) have been used prominently and successfully in speech recognition and, more recently, in handwriting recognition. Consequently, they seem ideal for visual recognition of complex, structured hand gestures such as are found in sign language. We describe two experiments that demonstrate a real-time HMM-based system for recognizing sentence level American Sign Language (ASL) without explicitly modeling the fingers. The first experiment tracks hands wearing colored gloves and attains a word accuracy of 99%. The second experiment tracks hands without gloves and attains a word accuracy of 92%. Both experiments have a 40 word lexicon.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Baum. </author> <title> An inequality and associated maximization technique in statistical estimation of probabilistic functions of Markov processes. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8, </pages> <year> 1972. </year>
Reference-contexts: Recently, Wilson and Bo-bick [21] explore incorporating multiple representations in HMM frameworks. 4 Hidden Markov Modeling While a substantial body of literature exists on HMM technology <ref> [1, 8, 13, 23] </ref>, this section briefly outlines a traditional discussion of the algorithms. After outlining the fundamental theory in training and testing a discrete HMM, this result is then generalized to the continuous density case used in the experiments. For broader discussion of the topic, [8, 17] are recommended. <p> While the technique described only handles a single observation sequence, it is easy to extend to a set of observation sequences. A more formal discussion can be found in <ref> [1, 8, 23] </ref>. While the estimation and evaluation processes described above are sufficient for the development of an HMM system, the Viterbi algorithm provides a quick means of evaluating a set of HMM's in practice as well as providing a solution for the decoding problem.
Reference: [2] <author> C. Charayaphan and A. Marble. </author> <title> Image processing system for interpreting motion in American Sign Language. </title> <journal> Journal of Biomedical Engineering, </journal> <volume> 14 </volume> <pages> 419-425, </pages> <year> 1992. </year>
Reference-contexts: However, these systems have generally concentrated on isolated signs and small training and test sets. Tamura and Kawasaki demonstrate an early image processing system which recognizes 20 Japanese signs based on matching cheremes [20]. Charayaphan and Marble <ref> [2] </ref> demonstrate a feature set that distinguishes between the 31 isolated ASL signs in their training set (which also acts as the test set). More recently, Cui and Weng [3] have shown an image-based system with 96% accuracy on 28 isolated gestures.
Reference: [3] <author> Y. Cui and J. Weng. </author> <title> Learning-based hand sign recognition. Intl. Work. Auto. Face Gest. </title> <booktitle> Recog. (IWAFGR) '95 Proceedings, p. </booktitle> <pages> 201-206, </pages> <year> 1995 </year>
Reference-contexts: Charayaphan and Marble [2] demonstrate a feature set that distinguishes between the 31 isolated ASL signs in their training set (which also acts as the test set). More recently, Cui and Weng <ref> [3] </ref> have shown an image-based system with 96% accuracy on 28 isolated gestures. Takahashi and Kishino in [19] discuss a user dependent Dataglove-based system that recognizes 34 of the 46 Japanese kana alphabet gestures using a joint angle and hand orientation coding technique.
Reference: [4] <author> T. Darrell and A. Pentland. Space-time gestures. CVPR, p. </author> <month> 335-340, </month> <year> 1993. </year>
Reference-contexts: This experiment is significant because it uses a 25x25 pixel quantized subsampled camera image as a feature vector. Even with such low-level information, the model can learn the set of motions and recognize them with respectable accuracy. Darrell and Pentland <ref> [4] </ref> dynamic time warping, a technique similar to HMM's, to match the interpolated responses of several learned image templates.
Reference: [5] <author> B. Dorner. </author> <title> Hand shape identification and tracking for sign language interpretation. </title> <booktitle> IJCAI Workshop on Looking at People, </booktitle> <year> 1993. </year>
Reference-contexts: To date, most work on sign language recognition has employed expensive wired "datagloves" which the user must wear [19]. In addition, these systems have mostly concentrated on finger signing, in which the user spells each word with finger signs corresponding to the letters of the alphabet <ref> [5] </ref>. However, most signing does not involve finger spelling but instead, gestures which represent whole words, allowing signed conversations to proceed at about the pace of spoken conversation. <p> being used and only one mixture (Gaussian density) is being assumed, the algorithms above can proceed normally, incorporating these changes for the continuous density case. 5 Tracking Hands in Video Previous systems have shown that, given some constraints, relatively detailed models of the hands can be recovered from video images <ref> [5, 14] </ref>. However, many of these constraints conflict with recognizing ASL in a natural context, either by requiring simple, unchanging backgrounds (unlike clothing); not allowing occlusion; requiring carefully labelled gloves; or being difficult to run in real time.
Reference: [6] <author> I. Essa, T. Darrell, and A. Pentland. </author> <title> Tracking facial motion. </title> <booktitle> IEEE Workshop on Nonrigid and articulated Motion, </booktitle> <address> Austin TX, </address> <month> Nov. 94. </month>
Reference-contexts: For the purposes of this experiment, this aspect of ASL will be ignored. Furthermore, in ASL the eyebrows are raised for a question, relaxed for a statement, and furrowed for a directive. While we have also built systems that track facial features <ref> [6] </ref>, this source of information will not be used to aid recognition in the task addressed here. While the scope of this work is not to create a user independent, full lexicon system for recognizing ASL, the system should be extensible toward this goal.
Reference: [7] <author> B. Horn. </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <address> NY, </address> <year> 1986. </year>
Reference-contexts: dx dy c = I 0 0 2 0 0 (x 0 and y 0 are the x and y coordinates normalized to the centroid) The axis of least inertia is then determined by the major axis of the bounding ellipse, which corresponds to the primary eigenvector of the matrix <ref> [7] </ref>. Note that this leaves a 180 degree ambiguity in the angle of the ellipses. To address this problem, the angles were only allowed to range from -90 to +90 degrees. When tracking skin tones, the above analysis helps to model situations of hand ambiguity implicitly.
Reference: [8] <author> X. Huang, Y. Ariki, and M. Jack. </author> <title> Hidden Markov Models for Speech Recognition. </title> <publisher> Edinburgh Univ. Press, Edinburgh, </publisher> <year> 1990. </year>
Reference-contexts: Explicit segmentation on the word level is not necessary for either training or recognition [18]. Language and context models can be applied on several different levels, and much related development of this technology has already been done by the speech recognition community <ref> [8] </ref>. Consequently, sign language recognition seems an ideal machine vision application of HMM technology, offering the benefits of problem scalability, well defined meanings, a pre-determined language model, a large base of users, and immediate applications for a recognizer. <p> Recently, Wilson and Bo-bick [21] explore incorporating multiple representations in HMM frameworks. 4 Hidden Markov Modeling While a substantial body of literature exists on HMM technology <ref> [1, 8, 13, 23] </ref>, this section briefly outlines a traditional discussion of the algorithms. After outlining the fundamental theory in training and testing a discrete HMM, this result is then generalized to the continuous density case used in the experiments. For broader discussion of the topic, [8, 17] are recommended. <p> After outlining the fundamental theory in training and testing a discrete HMM, this result is then generalized to the continuous density case used in the experiments. For broader discussion of the topic, <ref> [8, 17] </ref> are recommended. A time domain process demonstrates a Markov property if the conditional probability density of the current event, given all present and past events, depends only on the jth most recent events. <p> There are three key problems in HMM use: evaluation, estimation, and decoding. The evaluation problem is that given an observation sequence and a model, what is the probability that the observed sequence was generated by the model (P r (Oj)) (notational style from <ref> [8] </ref>)? If this can be evaluated for all competing models for an observation sequence, then the model with the highest probability can be chosen for recognition. P r (Oj) can be calculated several ways. <p> While the technique described only handles a single observation sequence, it is easy to extend to a set of observation sequences. A more formal discussion can be found in <ref> [1, 8, 23] </ref>. While the estimation and evaluation processes described above are sufficient for the development of an HMM system, the Viterbi algorithm provides a quick means of evaluating a set of HMM's in practice as well as providing a solution for the decoding problem. <p> While this is not strictly proper, the values are approximately equal in contiguous iterations <ref> [8] </ref> and seem not to make an empirical difference [23].
Reference: [9] <author> T. Humphries, C. Padden, and T. O'Rourke. </author> <title> A Basic Course in American Sign Language. </title> <editor> T. J. </editor> <publisher> Publ., Inc., </publisher> <address> Silver Spring, MD, </address> <year> 1980. </year>
Reference-contexts: Table 1 shows the words chosen for each class. Six personal pronouns, nine verbs, twenty nouns, and five adjectives are included making a total lexicon of forty words. The words were chosen by paging through Humphries et al. <ref> [9] </ref> and selecting those which would generate coherent sentences when chosen randomly for each part of speech. 1 Table 1: ASL Test Lexicon part of speech vocabulary pronoun I, you, he, we, you (pl), they verb want, like, lose, dontwant, dontlike, love, pack, hit, loan noun box, car, book, table, paper,
Reference: [10] <author> B. Juang. </author> <title> Maximum likelihood estimation for mixture multivariate observations of Markov chains. </title> <journal> AT&T Tech. J., </journal> <volume> 64 </volume> <pages> 1235-1249, </pages> <year> 1985. </year>
Reference-contexts: So far the discussion has assumed some method of quantization of feature vectors into classes. However, instead of using vector quantization, the actual probability densities for the features may be used. Baum-Welch, Viterbi, and the forward-backward algorithms can be modified to handle a variety of characteristic densities <ref> [10] </ref>. In this context, however, the densities will be assumed to be Gaussian.
Reference: [11] <author> K. Murakami and H. </author> <title> Taguchi. Gesture recognition using recurrent neural networks. </title> <booktitle> CHI '91 Conference Proceedings, p. </booktitle> <pages> 237-241, </pages> <year> 1991. </year>
Reference-contexts: The test user makes each of the 46 gestures 10 times to provide data for principle component and cluster analysis. A separate test set is created from five iterations of the alphabet by the user, with each gesture well separated in time. Murakami and Taguchi <ref> [11] </ref> describe a similar Dataglove system using recurrent neural networks. However, in this experiment a 42 static-pose finger alphabet is used, and the system achieves up to 98% recognition for trainers of the system and 77% for users not in the training set.
Reference: [12] <author> H. Poizner, U. Bellugi, and V. Lutes-Driscoll. </author> <title> Perception of American Sign Language in dynamic point-light displays. </title> <editor> J. Exp. Pyschol.: </editor> <booktitle> Human Perform., </booktitle> <volume> 7 </volume> <pages> 430-440, </pages> <year> 1981. </year>
Reference-contexts: The hand tracking stage of the system does not attempt a fine description of hand shape; studies have shown that such detailed information may not be necessary for humans to interpret sign language <ref> [12, 16] </ref>. Instead, the tracking process produces only a coarse description of hand shape, orientation, and trajectory. The hands are tracked by their color: in the first experiment via solidly colored gloves and in the second, via their natural skin tone.
Reference: [13] <author> L. Rabiner and B. Juang. </author> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine, </journal> <volume> p. </volume> <pages> 4-16, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Recently, Wilson and Bo-bick [21] explore incorporating multiple representations in HMM frameworks. 4 Hidden Markov Modeling While a substantial body of literature exists on HMM technology <ref> [1, 8, 13, 23] </ref>, this section briefly outlines a traditional discussion of the algorithms. After outlining the fundamental theory in training and testing a discrete HMM, this result is then generalized to the continuous density case used in the experiments. For broader discussion of the topic, [8, 17] are recommended. <p> Note that since Viterbi only guarantees the maximum of P r (O; Sj) over all state sequences S (as a result of the first order Markov assumption) instead of the sum over all possible state sequences, the resultant scores are only an approximation. However, <ref> [13] </ref> shows that this is often sufficient. So far the discussion has assumed some method of quantization of feature vectors into classes. However, instead of using vector quantization, the actual probability densities for the features may be used.
Reference: [14] <author> J. Rehg and T. Kanade. DigitEyes: </author> <title> vision-based human hand tracking. </title> <institution> School of Computer Science Technical Report CMU-CS-93-220, Carnegie Mellon Univ., </institution> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: being used and only one mixture (Gaussian density) is being assumed, the algorithms above can proceed normally, incorporating these changes for the continuous density case. 5 Tracking Hands in Video Previous systems have shown that, given some constraints, relatively detailed models of the hands can be recovered from video images <ref> [5, 14] </ref>. However, many of these constraints conflict with recognizing ASL in a natural context, either by requiring simple, unchanging backgrounds (unlike clothing); not allowing occlusion; requiring carefully labelled gloves; or being difficult to run in real time.
Reference: [15] <author> J. Schlenzig, E. Hunter, and R. Jain. </author> <title> Recursive identification of gesture inputers using hidden Markov models. </title> <booktitle> Proc. Second Ann. Conf. on Appl. of Comp. Vision, p. </booktitle> <pages> 187-194, </pages> <year> 1994. </year>
Reference-contexts: Even with such low-level information, the model can learn the set of motions and recognize them with respectable accuracy. Darrell and Pentland [4] dynamic time warping, a technique similar to HMM's, to match the interpolated responses of several learned image templates. Schlenzig et al. <ref> [15] </ref> use hidden Markov models to recognize "hello," "good-bye," and "rotate." While Baum-Welch re-estimation was not implemented, this study shows the continuous gesture recognition capabilities of HMM's by recognizing gesture sequences.
Reference: [16] <author> G. Sperling, M. Landy, Y. Cohen, and M. Pavel. </author> <title> Intelligible encoding of ASL image sequences at extremely low information rates. </title> <journal> Comp. Vision, Graphics, and Image Proc., </journal> <volume> 31 </volume> <pages> 335-391, </pages> <year> 1985. </year>
Reference-contexts: The hand tracking stage of the system does not attempt a fine description of hand shape; studies have shown that such detailed information may not be necessary for humans to interpret sign language <ref> [12, 16] </ref>. Instead, the tracking process produces only a coarse description of hand shape, orientation, and trajectory. The hands are tracked by their color: in the first experiment via solidly colored gloves and in the second, via their natural skin tone. <p> ASL uses approximately 6000 gestures for common words and finger spelling for communication of obscure words or proper nouns. Conversants in ASL may describe a person, place, or thing and then point to a place in space to store that object temporarily for later reference <ref> [16] </ref>. For the purposes of this experiment, this aspect of ASL will be ignored. Furthermore, in ASL the eyebrows are raised for a question, relaxed for a statement, and furrowed for a directive.
Reference: [17] <author> T. Starner. </author> <title> Visual Recognition of American Sign Language Using Hidden Markov Models. </title> <type> Master's thesis, </type> <institution> MIT Media Laboratory, </institution> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: After outlining the fundamental theory in training and testing a discrete HMM, this result is then generalized to the continuous density case used in the experiments. For broader discussion of the topic, <ref> [8, 17] </ref> are recommended. A time domain process demonstrates a Markov property if the conditional probability density of the current event, given all present and past events, depends only on the jth most recent events.
Reference: [18] <author> T. Starner, J. Makhoul, R. Schwartz, and G. Chou. </author> <title> On-line cursive handwriting recognition using speech recognition methods. </title> <booktitle> ICASSP, </booktitle> <address> V-125, </address> <year> 1994. </year>
Reference-contexts: Hidden Markov models have intrinsic properties which make them very attractive for sign language recognition. Explicit segmentation on the word level is not necessary for either training or recognition <ref> [18] </ref>. Language and context models can be applied on several different levels, and much related development of this technology has already been done by the speech recognition community [8]. <p> Processing proceeds normally except for simple rules to handle hand and face ambiguity described in the next section. 6 Feature Extraction and Hand Ambiguity Previous experience has shown that it is often best to start simply and evolve a feature set as testing progresses <ref> [18] </ref>. Since finger spelling is not allowed and there are few ambiguities in the test vocabulary based on individual finger motion, a relatively coarse tracking system may be used. <p> However, a practical solution to this problem is the use of context training and a statistical grammar instead of the rule-based grammar. Using context modeling as described before may significantly improve recognition accuracy in a more general implementation as shown by the speech and handwriting recognition communities <ref> [18] </ref>. While a rule-based grammar explicitly constrains the word order, statistical context modeling would have a similar effect while generalizing to allow different sentence structures. In the speech community, such modeling occurs at the "triphone" level, where groups of three phonemes are recognized as one unit.
Reference: [19] <author> T. Takahashi and F. Kishino. </author> <title> Hand gesture coding based on experiments using a hand gesture interface device. </title> <journal> SIGCHI Bul., </journal> <volume> 23(2) </volume> <pages> 67-73, </pages> <year> 1991. </year>
Reference-contexts: In sign language, each gesture already has assigned meaning, and strong rules of context and grammar may be applied to make recognition tractable. To date, most work on sign language recognition has employed expensive wired "datagloves" which the user must wear <ref> [19] </ref>. In addition, these systems have mostly concentrated on finger signing, in which the user spells each word with finger signs corresponding to the letters of the alphabet [5]. <p> Charayaphan and Marble [2] demonstrate a feature set that distinguishes between the 31 isolated ASL signs in their training set (which also acts as the test set). More recently, Cui and Weng [3] have shown an image-based system with 96% accuracy on 28 isolated gestures. Takahashi and Kishino in <ref> [19] </ref> discuss a user dependent Dataglove-based system that recognizes 34 of the 46 Japanese kana alphabet gestures using a joint angle and hand orientation coding technique. The test user makes each of the 46 gestures 10 times to provide data for principle component and cluster analysis.
Reference: [20] <author> S. Tamura and S. Kawasaki. </author> <title> Recognition of sign language motion images. </title> <journal> Pattern Recognition, </journal> <volume> 21 </volume> <pages> 343-353, </pages> <year> 1988. </year>
Reference-contexts: However, these systems have generally concentrated on isolated signs and small training and test sets. Tamura and Kawasaki demonstrate an early image processing system which recognizes 20 Japanese signs based on matching cheremes <ref> [20] </ref>. Charayaphan and Marble [2] demonstrate a feature set that distinguishes between the 31 isolated ASL signs in their training set (which also acts as the test set). More recently, Cui and Weng [3] have shown an image-based system with 96% accuracy on 28 isolated gestures.
Reference: [21] <author> A. Wilson and A. Bobick. </author> <title> Learning visual behavior for gesture analysis. </title> <booktitle> Proc. IEEE Int'l. Symp. on Comp. Vis., </booktitle> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: Schlenzig et al. [15] use hidden Markov models to recognize "hello," "good-bye," and "rotate." While Baum-Welch re-estimation was not implemented, this study shows the continuous gesture recognition capabilities of HMM's by recognizing gesture sequences. Recently, Wilson and Bo-bick <ref> [21] </ref> explore incorporating multiple representations in HMM frameworks. 4 Hidden Markov Modeling While a substantial body of literature exists on HMM technology [1, 8, 13, 23], this section briefly outlines a traditional discussion of the algorithms.
Reference: [22] <author> J. Yamato, J. Ohya, and K. Ishii. </author> <title> Recognizing human action in time-sequential images using hidden Markov models. </title> <booktitle> Proc. 1992 ICCV, p. </booktitle> <pages> 379-385. </pages> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: An early effort by Yamato et al. <ref> [22] </ref> uses discrete HMM's to recognize image sequences of six different tennis strokes among three subjects. This experiment is significant because it uses a 25x25 pixel quantized subsampled camera image as a feature vector.
Reference: [23] <author> S. Young. </author> <title> HTK: Hidden Markov Model Toolkit V1.5. </title> <institution> Cambridge Univ. Eng. Dept. Speech Group and Entropic Research Lab. Inc., </institution> <address> Washington DC, </address> <month> Dec. </month> <year> 1993. </year> <month> 7 </month>
Reference-contexts: Recently, Wilson and Bo-bick [21] explore incorporating multiple representations in HMM frameworks. 4 Hidden Markov Modeling While a substantial body of literature exists on HMM technology <ref> [1, 8, 13, 23] </ref>, this section briefly outlines a traditional discussion of the algorithms. After outlining the fundamental theory in training and testing a discrete HMM, this result is then generalized to the continuous density case used in the experiments. For broader discussion of the topic, [8, 17] are recommended. <p> While the technique described only handles a single observation sequence, it is easy to extend to a set of observation sequences. A more formal discussion can be found in <ref> [1, 8, 23] </ref>. While the estimation and evaluation processes described above are sufficient for the development of an HMM system, the Viterbi algorithm provides a quick means of evaluating a set of HMM's in practice as well as providing a solution for the decoding problem. <p> While this is not strictly proper, the values are approximately equal in contiguous iterations [8] and seem not to make an empirical difference <ref> [23] </ref>. <p> While initial training of the models might rely on manual segmentation or, in this case, evenly dividing the evidence among the models, embedded training trains the models in situ and allows model bound aries to shift through a probabilistic entry into the initial states of each model <ref> [23] </ref>. Generally, a sign can be affected by both the sign in front of it and the sign behind it. For phonemes in speech, this is called "co-articulation." While this can confuse systems trying to recognize isolated signs, the context information can be used to aid recognition.
References-found: 23

