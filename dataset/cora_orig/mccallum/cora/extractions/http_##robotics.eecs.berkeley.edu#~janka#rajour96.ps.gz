URL: http://robotics.eecs.berkeley.edu/~janka/rajour96.ps.gz
Refering-URL: http://robotics.eecs.berkeley.edu/~janka/des_publ.html
Root-URL: 
Title: Visually guided navigation  
Author: Jana Kosecka 
Address: 333 Cory Hall 98, UC Berkeley, Berkeley, CA 94720-1772  
Affiliation: Robotics laboratory, Department of Electrical Engineering and Computer Science,  
Abstract: Rich sensory information, robust control strategies and proper representation of the environment are crucial for successful navigation of the mobile robot. We propose a model of the environment which is suitable for global navigation using visual sensing. At the lowest level of interaction of the robot with the environment we employ visual servoing techniques which facilitate robust local navigation by means of relative positioning. Further we demonstrate how to use these local control strategies in a global setting. The environment is represented in terms of place graph, where the nodes correspond to places and arcs have associated servoing strategies for moving from one place to another. The global navigation is expressed as a sequence of relative positioning tasks obtainable from the search of a place graph. The proposed model facilitates generation of motion plans which can be executed in robust manner thanks to the presence of the sensory information in the feedback loop. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Allen, A. Timcenko, B. Yoshimi, and P. Michelman. </author> <title> Automated tracking and grasping of a moving object with a robotic hand-eye system. </title> <booktitle> In Proceedings of the Visual Servoing Workshop, IEEE Intetnational Conference on Robotics and Automation, </booktitle> <year> 1994. </year>
Reference: [2] <author> N. Ayache and O. Faugeras. </author> <title> Building, registrating and fusing noisy visual maps. </title> <journal> The International Journal of Robotics Research, </journal> <volume> 7(6):45 - 65, </volume> <month> December </month> <year> 1988. </year>
Reference: [3] <author> N. Ayache and O. Faugeras. </author> <title> Maintaning representations of the environment of a mobile robot. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 5(6):804 - 819, </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: The Kalman filter has been also used for fusion of binocular and trinocular images between different views by Ayache and Faugeras [2,3]. The main and appealing objective was to achieve a representation of the environment which would capture both the geometry and the uncertainty. In <ref> [3] </ref> authors developed an approach for combining the uncertainty arising from the imaging process with the uncertainty arising from the measure of displacement of the mobile base. Processing was based on points, lines and planes, which were chosen as the primitive parts of the environment model.
Reference: [4] <author> R. Basri and E. Rivlin. </author> <title> Localization and positioning using combinations of model views. </title> <booktitle> In Proceedings of the ARPA Image Understanding Workshop, </booktitle> <pages> pages 377 - 396, </pages> <year> 1993. </year>
Reference: [5] <author> J. J. Craig. </author> <title> Introduction to Robotics: Mechanics and control. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: For a detailed derivation of equation (5) see <ref> [5] </ref>. Matrix C M R corresponds to the relative rotation of F M with respect to the F C . In general holds that C M R = ( M C R) T .
Reference: [6] <author> J. L. Crowley. </author> <title> World modeling and position estimation for a mobile robot using ultrasonic ranging. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 674-680, </pages> <month> May </month> <year> 1989. </year>
Reference: [7] <author> Ch. Engels and G. Schoner. </author> <title> Dynamic fields endow behavior-based robos with representations. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> (14):55-77, 1995. 
Reference-contexts: The geometrical structure of the environment is either expressed explicitly [30] or merely enters the problem in the form which is convenient for control purposes <ref> [7] </ref>. Both the potential field methods and dynamic systems approach concentrates mainly 2 on the control issues and assumes near to ideal sensing.
Reference: [8] <author> S. Engelson. </author> <title> Passive Navigation and Visual Place Recognition. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1993. </year> <month> 18 </month>
Reference: [9] <author> B. Espiau, F. Chaumette, and P. Rives. </author> <title> A new approach to visual servoing in robotics. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 8(3):313 - 326, </volume> <month> June </month> <year> 1992. </year>
Reference-contexts: Visual servoing approaches [11] provide suitable platform for taking into account sensing, incorporating the geometric assumptions about the environment and deriving stable control strategies for achieving the goal. Either using geometric primitives alone <ref> [9] </ref> or relating the parameters of the optical flow model, with the motion of the mobile base [33] various local navigational tasks can be accomplished in a robust way. The issues pertaining to global navigation have so far not been addressed within this paradigm. <p> The form of interaction matrix for point features is: L T L T (3) which corresponds to the well known optical flow equation written in matrix form. Derivation of the interaction matrix for line features can be found in [19] and <ref> [9] </ref>. The desired velocity screw is related to the error measured in the image plane and represents commands to the actuators needed for accomplishment of the task. Choosing points and lines as elementary features we derive the appropriate control laws for the nonholonomic mobile base with the pan platform.
Reference: [10] <author> R. Grupen and J. Connoly. </author> <title> Harmonic functions for navigation. </title> <type> Technical report, </type> <institution> University of Massachusets, Amherst, </institution> <year> 1992. </year>
Reference: [11] <author> G. Hager and S. Hutchinson, </author> <title> editors. Visual Servoing Achievements, Applications and Open Problems, </title> <booktitle> Visual Servoing Workshop, IEEE International Conference on Robotics and Automation. </booktitle> <publisher> IEEE Press, </publisher> <year> 1994. </year>
Reference-contexts: These approaches often resulted in a successful working systems but failed to be generalizable due to the weak link between the relational model of the environment and its intrinsic geometrical structure. Visual servoing approaches <ref> [11] </ref> provide suitable platform for taking into account sensing, incorporating the geometric assumptions about the environment and deriving stable control strategies for achieving the goal.
Reference: [12] <author> Greg D. Hager. </author> <title> A modular system for robust positioning using feedback from stereo vision. </title> <type> Technical Report YALEU/DCS/RR-1074, </type> <institution> Yale University, </institution> <month> May </month> <year> 1995. </year>
Reference: [13] <author> K. Hashimoto. </author> <title> Visual Servoing. </title> <publisher> World Scientific, </publisher> <year> 1993. </year>
Reference-contexts: Various techniques have been successfully applied in hand-eye coordination problems, manipulation [12,1,14] and more recently in mobile robotics [29,33]. A more detailed overview of different approaches can be found in <ref> [13] </ref>. Our approach follows the theory developed in [9,32] and is referred to as task function approach. The overall philosophy of this approach is to formalize a servoing task as regulating a particular error function to zero.
Reference: [14] <author> N. J. Hollinghurst and R. Cippolla. </author> <title> Uncalibrated stereo hand-eye coordination. </title> <journal> Image and Vision Computing, </journal> <volume> 12(3) </volume> <pages> 187-192, </pages> <year> 1994. </year>
Reference: [15] <author> I. Horswill. </author> <title> Specialization of Perceptual Processes. </title> <type> PhD thesis, </type> <institution> Massachusets Institute of Technology, </institution> <year> 1994. </year>
Reference: [16] <author> D. P. Huttenlocher and W. J. Rucklidge. </author> <title> A multi-resolution technique for comparing images using Hausdorff distance. </title> <type> Technical Report TR92-1321, </type> <institution> Cornell University, </institution> <year> 1992. </year>
Reference: [17] <author> I. Kamon and E. Rivlin. </author> <title> Sensory based motion planning with global proofs. </title> <booktitle> In Proceedings of IROS-95. </booktitle> <address> Pittsburg, USA, </address> <year> 1995. </year>
Reference: [18] <author> O. Khatib. </author> <title> Real-time obstacle avoidance for manipulators and mobile robots. </title> <journal> International Journal of Robotics Research, </journal> <volume> 5(1) </volume> <pages> 90-98, </pages> <year> 1986. </year>
Reference: [19] <author> J. Kosecka. </author> <title> A Framework for Modeling and Verifying Visually Guided Agents: Design, Analysis and Experiments. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <year> 1996. </year>
Reference-contexts: The form of interaction matrix for point features is: L T L T (3) which corresponds to the well known optical flow equation written in matrix form. Derivation of the interaction matrix for line features can be found in <ref> [19] </ref> and [9]. The desired velocity screw is related to the error measured in the image plane and represents commands to the actuators needed for accomplishment of the task. <p> By estimating the pose of the camera, the velocity control is governed by the control law based on the simple gradient based scheme which was introduced in <ref> [19] </ref>. Another approach which would bypass the computation of the pose is to use a different representation of the pose of the mobile base with respect to the doorway. <p> In the current experimental setup the initialization of the servoing strategies as well as recognition of landmarks is done by user. What follows is an example 16 of the door servoing task expressed in task specification language introduced in <ref> [19] </ref>. The search for and initialization of doorway features can be written in a following way: InitServo (door i ) := Search (~s) k ServoC where we utilize the degree of freedom of the pan platform.
Reference: [20] <editor> J. Kosecka, R. Bajcsy, and M. Mintz. </editor> <title> Control of visually guided behaviors. </title> <editor> In Christopher Brown and Demetri Terzoupoulos, editors, </editor> <booktitle> Real-Time Computer Vision. </booktitle> <publisher> Cambridge Press, </publisher> <year> 1994. </year>
Reference-contexts: The combination of visual servoing and obstacle detection was tested earlier <ref> [20] </ref> in the context of simpler visual features.
Reference: [21] <author> B. Kuipers and Y. Byun. </author> <title> A robot exploration and mapping strategy based on semantic hierarchy of spatial representations. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 8, </volume> <year> 1991. </year>
Reference: [22] <author> J. C. Latombe. </author> <title> Robot Motion Planning. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: The approaches vary in the type of sensing used as well as the amount of a priori information about the environment. In the motion planning literature <ref> [22] </ref> the authors typically assume that the model of the environment is known and that the odometric readings provide an accurate estimate of the mobile robot's position in the global coordinate frame.
Reference: [23] <author> A. Lazanas and J. C. Latombe. </author> <title> Landmark-based robot navigation. </title> <journal> Algorithmica, </journal> <volume> 13 </volume> <pages> 472-501, </pages> <year> 1995. </year>
Reference-contexts: Landmarks are defined as a set of naturally occurring features, which are visually distinctive and recognizable <ref> [23] </ref>. We assume that the set of landmarks and their relative relationships is determined ahead of time by the designer (i.e., the landmarks and their features coordinates in the global coordinate system are known). In the case the relationship between landmarks is not known it can be established by exploration. <p> The inner horopter places a restriction on the viewing angle of the two points 5 In this case the notion of a place is the same as the notion of a visibility region of a landmark <ref> [23] </ref>. 6 In the context of stereo vision horopter is a locus of points which can be seen with stereo pair with a particular vergence angle.
Reference: [24] <author> J. Leonard. </author> <title> Directed Sonar Sensing for Mobile Robot Navigation. </title> <type> PhD thesis, </type> <institution> University of Oxford, Department of Engineering Science, </institution> <year> 1990. </year> <month> 19 </month>
Reference: [25] <author> V. Lumelsky and T. Skewis. </author> <title> Incorporating range sensing in the robot navigation. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 5(20) </volume> <pages> 1058-1068, </pages> <year> 1990. </year>
Reference: [26] <author> R. Mandelbaum. </author> <title> Sensor Processing for Mobile Robot Localization, Exploration and Navigation. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <year> 1995. </year>
Reference: [27] <author> M. Mataric. </author> <title> Environment learing using a distributed representation. </title> <booktitle> Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <year> 1990. </year>
Reference: [28] <author> J.-M. Odobez. </author> <title> A visually oriented representation of planar relative position. </title> <type> Technical report, </type> <institution> Grasp Laboratory, University of Pennsylvania, </institution> <year> 1995. </year> <note> (submitted to) ICPR-95. </note>
Reference-contexts: The pose can be expressed in terms of visual angle ff and depth ratio fi between the two points as proposed by <ref> [28] </ref>. The desired linear velocity can then be computed by differentiating this relationship. The angular velocity of the camera sensor frame ! C around ~z M is derived using a visual servoing paradigm. <p> The delimitation of the visibility region of a doorway by horopters is incorporated for the purpose of conveniently employing the visual servoing strategy described in <ref> [28] </ref>. By using this strategy for two points the mobile base can be positioned relative to the doorway as described in Section 2.3. We term the second type of the region associated with a landmark the approachability region.
Reference: [29] <author> R. Pissard-Gibollet and P. Rives. </author> <title> Applying visual servoing techniques to control a mobile hand-eye system. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <address> Nagoya, Japan, </address> <year> 1995. </year>
Reference: [30] <author> E. Rimon and D. E. Koditschek. </author> <title> Exact robot navigation using artificial potential functions. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 8(5) </volume> <pages> 501-519, </pages> <year> 1993. </year>
Reference-contexts: This problem was partially elevated by methods embedded in the theory of dynamical systems which emphasize the control issues, while still pertaining the notion of the explicit model of the environment. The geometrical structure of the environment is either expressed explicitly <ref> [30] </ref> or merely enters the problem in the form which is convenient for control purposes [7]. Both the potential field methods and dynamic systems approach concentrates mainly 2 on the control issues and assumes near to ideal sensing.
Reference: [31] <author> P. Rives and R. Pissard-Gibollet. </author> <title> Reactive mobile robots based on visual servoing approach. </title> <booktitle> In Artificial Intelligence Planning and Simulation (AIS), </booktitle> <address> Juillet, Perth, Australia, </address> <year> 1992. </year>
Reference-contexts: The matrix C which allows us to take into account more measurements then actual number of task constraints. It can be shown that a simple gradient based scheme <ref> [31] </ref> is sufficient to ensure exponential convergence to zero of the output. The control law chosen has the following form: T d = e (r; t) where is positive gain factor.
Reference: [32] <author> C. Samson, M. Le Borgne, and B. Espiau. </author> <title> Robot Control The Task Function Approach. </title> <booktitle> Oxford Engineering Science Series. </booktitle> <publisher> Claderon Press, </publisher> <year> 1991. </year>
Reference: [33] <author> J. Santos-Victor and G. </author> <title> Sandini. Docking behaviors via active perception. </title> <booktitle> In Proceedings of the 3rd International Symposium on Intelligent Robotic Systems, Pisa, Italy, </booktitle> <pages> pages 303 - 314, </pages> <year> 1995. </year>
Reference-contexts: Either using geometric primitives alone [9] or relating the parameters of the optical flow model, with the motion of the mobile base <ref> [33] </ref> various local navigational tasks can be accomplished in a robust way. The issues pertaining to global navigation have so far not been addressed within this paradigm.
Reference: [34] <author> K. Sutherland. </author> <title> Landmark selection for accurate navigation. </title> <booktitle> In Proceedings of the DARPA Image Understanding Workshop, </booktitle> <year> 1993. </year>
Reference: [35] <author> C. J. Taylor and D. J. Kriegman. </author> <title> Exploration strategies for mobile robots. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <address> Atlanta, Georgia, </address> <year> 1993. </year>
Reference-contexts: In such a scenario robot needs the capability of systematically exploring the environment and searching for the object, while building a model of the environment <ref> [35] </ref>. In order to be able to accomplish such a task, the capabilities of the robot involved in delivery tasks must include: capability of recognizing a set of distinctive landmarks, localizing itself with respect to a landmark and following boundaries of obstacles.
Reference: [36] <author> C.J. Taylor and D. J. Kriegman. </author> <title> Vision-based motion planning and exploration algorithms for mobile robots. </title> <editor> In K. Goldberg, editor, </editor> <booktitle> Proceedings of the Workshop on the Algorithmic Foundations of Robotics, </booktitle> <year> 1994. </year>
Reference-contexts: Position of the mobile robot in the global coordinate system is often not necessary. The type of maps which can be typically acquired by this means are relational maps, with local coordinate system attached to the individual landmarks <ref> [36] </ref>. Such an exploration strategy however may be quite time consuming and in some cases ineffective. When some a priori model of the environment is already available, the location of the object to be delivered may be described qualitatively and there is no need for a thorough exploration. <p> In the case the relationship between landmarks is not known it can be established by exploration. An exploration strategy for discovering relationships between a given set of recognizable landmarks and building a relational model of the environment was proposed in <ref> [36] </ref>. Since in our case the landmark is composed from various features, we need to take into account the limited field of view of the camera, which restricts the points from which all features can be seen simultaneously. <p> We are currently looking at some means how to obtain this partitioning automatically from 2-D polygonal model of the world or by exploration 7 . Two landmarks are in the neighborhood of each other if their associated places overlap. The regions where places overlap are referred to as gateways <ref> [36] </ref>. For the successful navigation between the places we have to make sure that the setpoints of the elementary servoing strategies are chosen in such a manner that they will bring the camera frame (or the mobile base frame for that matter) to a gateway region of two landmarks.
Reference: [37] <author> E. Yeh and D. J. Kriegman. </author> <title> Toward selecting and recognizing natural landmarks. </title> <booktitle> In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <year> 1995. </year> <month> 20 </month>
Reference-contexts: Ideally it should be possible to acquire the set of reference views, 17 which would correspond to the distinctive places in the environment auto-matically though an exploration process. An attempt to address this difficult problem in a probabilistic framework was made by <ref> [37] </ref>. The approach outlined above proposed the servoing strategy as only means of navigation and the environment model was partitioned in such manner that at every point in the free space some servoing strategy could be applied. This type of approach can be well combined with some open-loop motion planning.
References-found: 37

