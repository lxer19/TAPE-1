URL: http://www.cs.rochester.edu/u/mccallum/mccallum-nips96.ps.gz
Refering-URL: http://www.cs.rochester.edu/stats/oldmonths/1998.05/docs-name.html
Root-URL: 
Email: mccallum@cs.rochester.edu  
Title: Category: Control, Navigation and Planning. Key words: Reinforcement learning, Exploration, Hidden state. Prefer oral presentation.
Author: Andrew Kachites McCallum 
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract: This paper presents Fringe Exploration, a technique for efficient exploration in partially observable domains. The key idea, (applicable to many exploration techniques), is to keep statistics in the space of possible short-term memories, instead of in the agent's current state space. Experimental results in a partially observable maze and in a difficult driving task with visual routines show dramatic performance improvements.
Abstract-found: 1
Intro-found: 1
Reference: [ Barto et al., 1995 ] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72(1) </volume> <pages> 81-138, </pages> <year> 1995. </year>
Reference-contexts: 1 The Problem Efficient exploration is of fundamental importance for autonomous systems that learn to act. In recent years, a variety of exploration techniques have been proposed for reinforcement learning (RL). Many researchers use undirected techniquesapproaches closely related to random walks, e.g. <ref> [ Mozer and Bachrach, 1989; Barto et al., 1995 ] </ref> . These include the common random action with probability e and Boltzmann distribution based on utility and decreasing temperature.
Reference: [ Chrisman, 1992 ] <author> Lonnie Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 183-188, </pages> <year> 1992. </year>
Reference-contexts: State identification techniques use memory of past percepts and actions to distinguish states that are aliased by these non-Markovian dependencies. Several reinforcement learning algorithms augment their state representations on-line by adding memory; examples include techniques based on partially observable Markov decision processes <ref> [ Chrisman, 1992; McCallum, 1993 ] </ref> , recurrent neural networks [ Lin and Mitchell, 1992 ] and suffix trees [ McCallum, 1995 ] . The problem is that directed exploration techniques all hinge on the assumption that the state of the world is observable. <p> No previous research addresses the combination of directed exploration with hidden state in reinforcement learning; all previous hidden state work has used undirected exploration. Efficient exploration in non-Markovian domains has long been understood as a special difficulty, (Chrisman refers to it as the `Exacerbated Exploration Problem' <ref> [ Chrisman, 1992 ] </ref> ), but little is known about it.
Reference: [ Kaelbling, 1990 ] <author> Leslie P. Kaelbling. </author> <title> Learning in Embedded Systems. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1990. </year>
Reference-contexts: Statistics used include action counts, e.g. [ Sato et al., 1988; Thrun, 1992 ] , action recency [ Sutton, 1990 ] , and confidence intervals on utility data <ref> [ Kaelbling, 1990 ] </ref> . It has been shown both analytically and empirically that directed exploration reduces worst-case learning time from exponential to only a low-degree polynomial [ Thrun, 1992; Koenig and Simmons, 1993 ] . Directed exploration is clearly superior. <p> To adapt this to Fringe Exploration, as with the two techniques above, we want to use the Q-value available in the leaf, but determine our confidence in that Q-value according to the action-frequency data in the fringe. Thus, we use the same formulation described in <ref> [ Kaelbling, 1990 ] </ref> , except the counts passed to the Student-T function come from the fringe instead of the leaf. 4 Experimental Results USM with Fringe Exploration has been applied to two task domains.
Reference: [ Koenig and Simmons, 1993 ] <author> S. Koenig and R. G. Simmons. </author> <title> Complexity analysis of real-time reinforcement learning. </title> <booktitle> In Proceeding of the Eleventh National Conference on Artificial Intelligence AAAI-93, </booktitle> <pages> pages 99-105, </pages> <address> Menlo Park, CA, 1993. </address> <publisher> AAAI, AAAI Press/The MIT Press. </publisher>
Reference-contexts: It has been shown both analytically and empirically that directed exploration reduces worst-case learning time from exponential to only a low-degree polynomial <ref> [ Thrun, 1992; Koenig and Simmons, 1993 ] </ref> . Directed exploration is clearly superior. A second important issue in reinforcement learning is hidden state.
Reference: [ Lin and Mitchell, 1992 ] <author> Long-Ji Lin and Tom M. Mitchell. </author> <title> Reinforcement learning with hidden states. </title> <booktitle> In Proceedings of the Second International Conference on Simulation of Adaptive Behavior: From Animals to Animats, </booktitle> <pages> pages 271-280, </pages> <year> 1992. </year>
Reference-contexts: Several reinforcement learning algorithms augment their state representations on-line by adding memory; examples include techniques based on partially observable Markov decision processes [ Chrisman, 1992; McCallum, 1993 ] , recurrent neural networks <ref> [ Lin and Mitchell, 1992 ] </ref> and suffix trees [ McCallum, 1995 ] . The problem is that directed exploration techniques all hinge on the assumption that the state of the world is observable. That is, they all depend on associating unique exploration statistics with each world state.
Reference: [ McCallum, 1993 ] <author> R. Andrew McCallum. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In The Proceedings of the Tenth International Machine Learning Conference. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1993. </year>
Reference-contexts: State identification techniques use memory of past percepts and actions to distinguish states that are aliased by these non-Markovian dependencies. Several reinforcement learning algorithms augment their state representations on-line by adding memory; examples include techniques based on partially observable Markov decision processes <ref> [ Chrisman, 1992; McCallum, 1993 ] </ref> , recurrent neural networks [ Lin and Mitchell, 1992 ] and suffix trees [ McCallum, 1995 ] . The problem is that directed exploration techniques all hinge on the assumption that the state of the world is observable.
Reference: [ McCallum, 1995 ] <author> R. Andrew McCallum. </author> <title> Instance-based utile distinctions for reinforcement learning. </title> <booktitle> In The Proceedings of the Twelfth International Machine Learning Conference. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1995. </year>
Reference-contexts: Several reinforcement learning algorithms augment their state representations on-line by adding memory; examples include techniques based on partially observable Markov decision processes [ Chrisman, 1992; McCallum, 1993 ] , recurrent neural networks [ Lin and Mitchell, 1992 ] and suffix trees <ref> [ McCallum, 1995 ] </ref> . The problem is that directed exploration techniques all hinge on the assumption that the state of the world is observable. That is, they all depend on associating unique exploration statistics with each world state. This assumption is broken by hidden state. <p> in this paper, it can be applied to many different exploration techniques. 3 Fringe Exploration with USM Fringe Exploration has been implemented in conjunction with Utile Suffix Memory (USM), a reinforcement learning algorithm that learns to use variable amounts of short-term memory in order to solve tasks with hidden state <ref> [ McCallum, 1995 ] </ref> . <p> The layers of hypothesis distinctions are called the fringe. The depth of the fringe is a configurable parameter of USM. Limited space prevents including more precise details of USM; a full description can be found in <ref> [ McCallum, 1995 ] </ref> . To implement Fringe Exploration in USM, we keep exploration statistics associated with the tree nodes at the bottom of the fringe, instead of the normal practice of keeping them in the agent's internal states (which would be the official leaves of the tree). <p> First, in order to compare several different exploration strategies, (each applied traditionally and with Fringe Exploration), we gather results from the same local-perception maze task used in <ref> [ McCallum, 1995 ] </ref> . Second, in order to show that the approach scales well to larger, more difficult tasks, we provide results in a complex highway driving task based on visual routines. Maze. The maze is pictured in the top left of figure 1.
Reference: [ McCallum, 1996 ] <author> Andrew Kachites McCallum. </author> <title> Learning to use selective attention and short-term memory. </title> <booktitle> In From Animals to Animats:Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Since the agent can only see one truck (one lane) at a time, there is much hidden state. Limited space precludes a full description of the task; see <ref> [ McCallum, 1996 ] </ref> for more details. Undirected exploration is compared with directed Fringe Exploration, examining performance after 30,000 steps of training; (there is no need to gather results with traditional directed exploration since it is known to perform so badly).
Reference: [ Mozer and Bachrach, 1989 ] <author> M. C. Mozer and J. R. Bachrach. </author> <title> Discovering the structure of a reactive environment by exploration. </title> <type> Technical Report CU-CS-451-89, </type> <institution> Dept. of Computer Science, University of Colorado, Boulder, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: 1 The Problem Efficient exploration is of fundamental importance for autonomous systems that learn to act. In recent years, a variety of exploration techniques have been proposed for reinforcement learning (RL). Many researchers use undirected techniquesapproaches closely related to random walks, e.g. <ref> [ Mozer and Bachrach, 1989; Barto et al., 1995 ] </ref> . These include the common random action with probability e and Boltzmann distribution based on utility and decreasing temperature.
Reference: [ Ron et al., 1994 ] <author> Dana Ron, Yoram Singer, and Naftali Tishby. </author> <title> Learning probabilistic automata with variable memory length. </title> <booktitle> In Proceedings Computational Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: This section describes how Fringe Exploration was added to USM. USM organizes its short-term memory in a Suffix Tree <ref> [ Ron et al., 1994 ] </ref> . The leaves of the tree are the internal states of the agent. The leaves may have different depths in different parts of the tree. Deeper branches of the tree correspond to distinctions based on observations and actions further back in time.
Reference: [ Sato et al., 1988 ] <author> Mitsuo Sato, Kenichi Abe, and Hiroshi Takeda. </author> <title> Learning control of finite markov chains with an explicit trade-off between estimation and control. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 18(5), </volume> <month> September </month> <year> 1988. </year>
Reference-contexts: To improve learning speed, several researchers have proposed directed exploration techniquesapproaches that use statistics from the learning experience to more efficiently guide the search. Statistics used include action counts, e.g. <ref> [ Sato et al., 1988; Thrun, 1992 ] </ref> , action recency [ Sutton, 1990 ] , and confidence intervals on utility data [ Kaelbling, 1990 ] . <p> Counter-based Exploration uses counts of the number of times an action was taken from a particular state, and tries to choose actions that have been chosen less frequently <ref> [ Sato et al., 1988; Thrun, 1992 ] </ref> .
Reference: [ Sutton, 1990 ] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: To improve learning speed, several researchers have proposed directed exploration techniquesapproaches that use statistics from the learning experience to more efficiently guide the search. Statistics used include action counts, e.g. [ Sato et al., 1988; Thrun, 1992 ] , action recency <ref> [ Sutton, 1990 ] </ref> , and confidence intervals on utility data [ Kaelbling, 1990 ] . It has been shown both analytically and empirically that directed exploration reduces worst-case learning time from exponential to only a low-degree polynomial [ Thrun, 1992; Koenig and Simmons, 1993 ] . <p> Traditionally, the s's would be agent-internal states; with Fringe Exploration, the s's are fringe nodes. The agent deterministically chooses the action with the highest `Eval'. Recency-based Exploration uses counts of the number of time steps that have passed since an action was taken from a particular state <ref> [ Sutton, 1990 ] </ref> . This recency information is combined with the Q-value according to the following formula, (where (s; a) is the number of steps since action a was last executed from state s).
Reference: [ Thrun, 1992 ] <author> Sebastian B. Thrun. </author> <title> The role of exploration in learning control. In Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: To improve learning speed, several researchers have proposed directed exploration techniquesapproaches that use statistics from the learning experience to more efficiently guide the search. Statistics used include action counts, e.g. <ref> [ Sato et al., 1988; Thrun, 1992 ] </ref> , action recency [ Sutton, 1990 ] , and confidence intervals on utility data [ Kaelbling, 1990 ] . <p> It has been shown both analytically and empirically that directed exploration reduces worst-case learning time from exponential to only a low-degree polynomial <ref> [ Thrun, 1992; Koenig and Simmons, 1993 ] </ref> . Directed exploration is clearly superior. A second important issue in reinforcement learning is hidden state. <p> Counter-based Exploration uses counts of the number of times an action was taken from a particular state, and tries to choose actions that have been chosen less frequently <ref> [ Sato et al., 1988; Thrun, 1992 ] </ref> .
Reference: [ Ullman, 1984 ] <author> Shimon Ullman. </author> <title> Visual routines. </title> <journal> Cognition, </journal> <volume> 18 </volume> <pages> 97-159, </pages> <year> 1984. </year> <title> (Also in: Visual Cognition, </title> <editor> S. Pinker ed., </editor> <year> 1985). </year>
Reference-contexts: The agent gets punished severely for scraping into the back of a slower truck; and gets punished mildly while a tail-gating faster truck honks for it to get out of the way. The agent's perception is based on a directable fovea, manipulated with visual routines <ref> [ Ullman, 1984 ] </ref> . The list of features in the perception vector, as well as sample values, and a bird's eye view of the environment are shown in figure 2.
Reference: [ Whitehead, 1991 ] <author> S. D. Whitehead. </author> <title> Complexity and cooperation in Q-learning. In L.A. </title> <editor> Birnbaum and G.C. Collins, editors, </editor> <booktitle> Proceedings of the Eighth International Workshopon Machine Learning, </booktitle> <pages> pages 363-367, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These include the common random action with probability e and Boltzmann distribution based on utility and decreasing temperature. While easy to implement, these approaches are often unbearably inefficient; Whitehead has proved that undirected exploration can cause the learning time to scale exponentially with the size of the state space <ref> [ Whitehead, 1991 ] </ref> . To improve learning speed, several researchers have proposed directed exploration techniquesapproaches that use statistics from the learning experience to more efficiently guide the search.
References-found: 15

