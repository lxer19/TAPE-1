URL: ftp://ftp.cse.ucsc.edu/pub/ml/annals.ps.Z
Refering-URL: http://www.cse.ucsc.edu/~haussler/pubs.html
Root-URL: http://www.cse.ucsc.edu
Title: Mutual Information, Metric Entropy, and Cumulative Relative Entropy Risk  
Author: David Haussler Manfred Opper 
Keyword: Abbreviated title: Mutual Information and Risk  Key words and phrases: mutual information, Hellinger distance, relative entropy, metric entropy, minimax risk, Bayes risk, density estimation, Kullback-Leibler distance.  
Web: 62B10, 62C20, 94A29.  
Note: Submitted to Annals of Statistics  AMS 1991 subject classifications: Primary 62G07; secondary  Supported by NSF grant IRI-9123692. Email addresses: haussler@cse.ucsc.edu Supported by Heisenberg fellowship of DFG. Email addresses: opper@physik.uni-wuerzburg.de  
Date: December 29, 1996  
Affiliation: UC Santa Cruz  Universitat Wurzburg  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Barron and L. Gyorfi and E. van der Meulen. </author> <title> Distribution estimation consistent in total variation and in two types of information divergence. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38 </volume> <pages> 1437-1454, </pages> <year> 1992. </year>
Reference-contexts: All logarithms are natural logarithms unless otherwise specified. We assume throughout that 0 log 0 = 0 log x 0 = 0, where x is any nonnegative finite number. We will also employ functions taking values in the extended reals <ref> [1; +1] </ref>, and and in particular use the extended log function obtained by defining log 0 = 1 and log 1 = 1. Expectations over extended real-valued functions are defined whenever they do not take both the value +1 with positive probability and the value 1 with positive probability. <p> Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. [6, 10, 51, 56, 54]. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in <ref> [1, 40] </ref>. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. [22, 37]). <p> A related boundedness condition is used in the investigation of consistent density estimation with respect to relative entropy risk in <ref> [1] </ref>. <p> It can be shown that for any fi and , f fi; () is a nondecreasing function on <ref> [0; 1) taking values in [0; 1] </ref>, and if f fi; () is finite for any &gt; 0, then lim f fi; () = f ; (0): To verify this last property, note that lim !0 R Bayes 1;; 1+ = 1. <p> It can be shown that for any fi and , f fi; () is a nondecreasing function on [0; 1) taking values in <ref> [0; 1] </ref>, and if f fi; () is finite for any &gt; 0, then lim f fi; () = f ; (0): To verify this last property, note that lim !0 R Bayes 1;; 1+ = 1. <p> ) and use the fact that for Gaussian processes and c &gt; 0 fi Z d ( ~ )e c R 1 2 1 X " c k # Here k , k = 1; 2; : : : ; 1 are the eigenvalues of the process on the interval <ref> [0; 1] </ref>. <p> A classical example is the following. Let fi be the Lipschitz class F p;ffi (C; L) of densities on Y = <ref> [0; 1] </ref> satisfying sup y2 [0;1] jdP (y)j C and having derivatives dP (k) (y) of order k p with the Lipschitz condition on the p- th derivative jdP (p) (p) (y 0 )j Ljy y 0 j ffi for y; y 0 2 [0; 1]. <p> L) of densities on Y = <ref> [0; 1] </ref> satisfying sup y2 [0;1] jdP (y)j C and having derivatives dP (k) (y) of order k p with the Lipschitz condition on the p- th derivative jdP (p) (p) (y 0 )j Ljy y 0 j ffi for y; y 0 2 [0; 1]. Since the 31 functions in F p;ffi (C 1 ; L) are uniformly bounded, they have an integrable envelope function, and hence R minimax 1; 1+ &lt; 1 for all &gt; 0.
Reference: [2] <author> S. Amari. </author> <title> Differential geometry of curved exponential families-curvatures and information loss. </title> <journal> Annals of Statistics, </journal> <volume> 10 </volume> <pages> 357-385, </pages> <year> 1982. </year>
Reference-contexts: Amari has developed an extensive theory that relates the risk when fl is the true state of Nature to certain differential-geometric properties of the parameter space fi in the neighborhood of fl involving Fisher information and related quantities <ref> [2, 3] </ref> (see also [57, 39, 55]). Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. [6, 10, 51, 56, 54].
Reference: [3] <author> S. Amari and N. Murata. </author> <title> Statistical theory of learning curves under entropic loss. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 140-153, </pages> <year> 1993. </year>
Reference-contexts: Amari has developed an extensive theory that relates the risk when fl is the true state of Nature to certain differential-geometric properties of the parameter space fi in the neighborhood of fl involving Fisher information and related quantities <ref> [2, 3] </ref> (see also [57, 39, 55]). Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. [6, 10, 51, 56, 54].
Reference: [4] <author> A. Barron. </author> <title> The strong ergodic theorem for densities: generalized Shannon-McMillan-Breiman theorem. </title> <journal> The Annals of Probability, </journal> <volume> 13 </volume> <pages> 1292-1303, </pages> <year> 1985. </year>
Reference-contexts: More general results, including the above corollary, follow from results in Pinsker's book [46] (see also <ref> [4] </ref>). Applying Theorem (1) and taking the supremum over in Corollary (2), it follows that if fi is finite then for all n, R minimax n log jfij and lim n!1 R minimax n = log jfij.
Reference: [5] <author> A. Barron. In T. M. Cover and B. Gopinath, </author> <title> editors, Open Problems in Communication and Computation, chapter 3.20. Are Bayes rules consistent in information?, </title> <address> pages 85-91. </address> <year> 1987. </year>
Reference-contexts: Since maximin minimax always, we have sup R Bayes 1;; 1+ R minimax 1; 1+ , and from this we obtain the result stated in the Theorem. 2 The method used in obtaining the upper bound in the above result is a familiar one (see e.g. <ref> [5, 32] </ref>).
Reference: [6] <author> A. Barron. </author> <title> The exponential convergence of posterior probabilities with implications for Bayes estimators of density functions. </title> <type> Technical Report 7, </type> <institution> Dept. of Statistics, U. Ill. Urbana-Champaign, </institution> <year> 1987. </year>
Reference-contexts: Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. <ref> [6, 10, 51, 56, 54] </ref>. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 40]. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. [22, 37]). <p> fl log fi d ( ~ )e n (1ff)I ff (P fljjP ~ ) fl R n;P Bayes = D KL (P n log fi d ( ~ )e nI 1 (P fljjQ ~ ) + fl: 10 The upper bound of part (1.) is similar to results given in <ref> [6] </ref>, and is mentioned there for the case P = Q. To the best of our knowledge, the lower bound, and the results in part (2.), are new. The proof is given in a series of lemmas and calculations.
Reference: [7] <author> A. Barron, B. Clarke, and D. Haussler. </author> <title> Information bounds for the risk of bayesian predictions and the redundancy of universal codes. </title> <booktitle> In Proc. International Symposium on Information Theory. </booktitle>
Reference-contexts: These results were extended to the Bayes and minimax risk in [18] (see also <ref> [7] </ref>). Related lower bounds, which are often quoted, were obtained by Rissanen [50], based on 7 certain asymptotic normality assumptions.
Reference: [8] <author> A. Barron, B. Clarke, and D. Haussler. </author> <title> Information bounds for the risk of bayesian predictions and the redundancy of universal codes. </title> <booktitle> In Proc. International Symposium on Information Theory, </booktitle> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: This provides a beautiful connection between information theory and statistics. This connection also extends to other fields, as is discussed in <ref> [17, 8] </ref>. In data compression, the cumulative relative entropy risk is the redundancy, which is the expected excess code length for the best adaptive coding method, as compared to the best coding method that has prior knowledge of the true distribution [17, 40, 43].
Reference: [9] <author> A. Barron and T. </author> <title> Cover. A bound on the financial value of information. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> 34 </volume> <pages> 1097-1100, </pages> <year> 1988. </year>
Reference-contexts: The minimax risk is called "information" channel capacity [20], p. 184. In mathematical finance and gambling theory, the cumulative relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution <ref> [9, 17] </ref>. In computational learning theory, this risk is the average additional loss suffered by an adaptive algorithm that predicts each observation before it arrives, based on the previous observations, as compared to an algorithm that makes predictions knowing the true distribution [32, 33].
Reference: [10] <author> A. Barron and Y. Yang. </author> <title> Information theoretic lower bounds on convergence rates of nonparametric estimators, 1995. </title> <type> unpublished manuscript. </type>
Reference-contexts: Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. <ref> [6, 10, 51, 56, 54] </ref>. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 40]. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. [22, 37]). <p> While this work is too extensive to summarize here, we do note that some authors have also taken the general approach that we take here in using notions of metric entropy (defined below), and specifically using the Hellinger distance in obtaining these bounds (e.g. <ref> [41, 11, 12, 30, 53, 13, 10] </ref>). The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [54] (see Corollary 1, p. 360) and Barron and Yang [10]. <p> The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [54] (see Corollary 1, p. 360) and Barron and Yang <ref> [10] </ref>. This work is somewhat complementary to ours, in that it treats instantaneous risk, whereas we focus on cumulative risk. <p> Different assumptions, and different methods (using Fano's inequality) are used to obtain related general results in <ref> [10] </ref>. In this paper we describe a new approach, employing the Hellinger metric and certain Laplace integrals, to bounding both the Bayes and minimax risks for the cumulative relative entropy loss. For most fi the bounds are fairly tight. We show this for fi that satisfy some general conditions. <p> The method for obtaining the lower bound by choosing a discrete prior on a well-separated set of is also similar in many respects to standard lower bound methods, such as those that use Fano's inequality or Assouad's lemma (see e.g. <ref> [12, 10, 56] </ref>), but the method is particularly clean in the present framework, giving a fairly good match to the upper bound. <p> Furthermore, since the functions in F p;ffi (C 1 ; L) are uniformly bounded, all L q distances (q 1) are equivalent. As shown by Barron and Yang <ref> [10] </ref>, a further restriction to uniformly lower bounded densities also makes the Hellinger distance equivalent to the L q distances and does not change the metric entropy asymptot ically.
Reference: [11] <author> L. Birge. </author> <title> Approximation dans les espaces metriques et theorie de l'estimation. </title> <journal> Zeitschrift fuer Wahrscheinlichkeitstheorie und Verwandte Gebiete, </journal> <volume> 65 </volume> <pages> 181-237, </pages> <year> 1983. </year>
Reference-contexts: While this work is too extensive to summarize here, we do note that some authors have also taken the general approach that we take here in using notions of metric entropy (defined below), and specifically using the Hellinger distance in obtaining these bounds (e.g. <ref> [41, 11, 12, 30, 53, 13, 10] </ref>). The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [54] (see Corollary 1, p. 360) and Barron and Yang [10]. <p> This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including Le Cam [41], Birge <ref> [11, 12] </ref>, Hasminskii and Ibragimov [30], and van de Geer [53]. 4.1 Basic bounds Our main theorem gives bounds on I (fi fl ; Y n ) and D KL (P n fl jjM n; ) in terms of the logarithms of two Laplace transforms of the I divergence, one at <p> Since I ff (P jjQ) D ff (P; Q), the lower bounds follow directly from the lower bounds of Theorem 2. For the upper bounds, we will need the following lemma, which is a simple extension of Lemma 4.4 of <ref> [11] </ref>. Lemma 4 For any distributions P and Q on Y and any 0 &lt; ff &lt; 1, D KL (P jjQ) sup b ff dQ (y) !! Proof. <p> This general approach was developed by Le Cam and Birge in the context of other loss functions <ref> [41, 11, 12] </ref>. We illustrate it now by applying the above lemma to establish a simple relationship between the metric dimension of (fi; h) and the asymptotic growth rate of the minimax risk R minimax n .
Reference: [12] <author> L. Birge. </author> <title> On estimating a density using Hellinger distance and some other strange facts. Probability theory and related fields, </title> <booktitle> 71 </booktitle> <pages> 271-291, </pages> <year> 1986. </year>
Reference-contexts: While this work is too extensive to summarize here, we do note that some authors have also taken the general approach that we take here in using notions of metric entropy (defined below), and specifically using the Hellinger distance in obtaining these bounds (e.g. <ref> [41, 11, 12, 30, 53, 13, 10] </ref>). The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [54] (see Corollary 1, p. 360) and Barron and Yang [10]. <p> This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including Le Cam [41], Birge <ref> [11, 12] </ref>, Hasminskii and Ibragimov [30], and van de Geer [53]. 4.1 Basic bounds Our main theorem gives bounds on I (fi fl ; Y n ) and D KL (P n fl jjM n; ) in terms of the logarithms of two Laplace transforms of the I divergence, one at <p> The method for obtaining the lower bound by choosing a discrete prior on a well-separated set of is also similar in many respects to standard lower bound methods, such as those that use Fano's inequality or Assouad's lemma (see e.g. <ref> [12, 10, 56] </ref>), but the method is particularly clean in the present framework, giving a fairly good match to the upper bound. <p> This general approach was developed by Le Cam and Birge in the context of other loss functions <ref> [41, 11, 12] </ref>. We illustrate it now by applying the above lemma to establish a simple relationship between the metric dimension of (fi; h) and the asymptotic growth rate of the minimax risk R minimax n .
Reference: [13] <author> L. Birge and P. Massart. </author> <title> Rates of convergence for minimum contrast estimators. Probability Theory and Related Fields, </title> <booktitle> 97 </booktitle> <pages> 113-150, </pages> <year> 1993. </year>
Reference-contexts: While this work is too extensive to summarize here, we do note that some authors have also taken the general approach that we take here in using notions of metric entropy (defined below), and specifically using the Hellinger distance in obtaining these bounds (e.g. <ref> [41, 11, 12, 30, 53, 13, 10] </ref>). The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [54] (see Corollary 1, p. 360) and Barron and Yang [10]. <p> These are the the packing and covering numbers, and the associated metric entropy, introduced by Kolmogorov and Tikhomirov in [38] and commonly used in the theory of empirical processes (see e.g. <ref> [24, 47, 29, 13] </ref>). For the following definitions, let (S; ) be any complete separable metric space. Definition 1 (Metric entropy, also called Kolmogorov *-entropy [38]) A partition of S is a collection f i g of Borel subsets of S that are pairwise disjoint and whose union is S.
Reference: [14] <author> L. L. </author> <title> Cam. An extension of Wald's theory of statistical decision functions. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 26 </volume> <pages> 69-81, </pages> <year> 1955. </year> <month> 37 </month>
Reference-contexts: This result can be obtained with limited effort from the general results in an early paper of Le Cam <ref> [14] </ref>. Special cases of the result were derived by Gallager [27] and Davisson and Leon-Garcia [21], and the general result is given in [31].
Reference: [15] <author> R. H. Cameron and W. T. Martin. </author> <title> Transformation of wiener integrals under translations. </title> <journal> Ann. Math., </journal> <volume> 45 </volume> <pages> 386-396, </pages> <year> 1944. </year>
Reference-contexts: In this case, it is easy to calculate the I-divergences explicitly for all ff. Let P be the measure corresponding to the random process Y (x) and let the dominating measure - be the Wiener measure. Then, from the Cameron-Martin formula <ref> [15] </ref>, the Radon-Nikodym derivative is found to be dP = exp [ 0 1 Z 1 2 (x)dx ]: (10) Inserting this into the definition of the I-divergences, we obtain I ff (P fl jjP ) = 2 2 0 2 For the case where the prior over the space of
Reference: [16] <author> B. Clarke. </author> <title> Asymptotic cumulative risk and Bayes risk under entropy loss with applications. </title> <type> PhD thesis, </type> <institution> Dept. of Statistics, University of Ill., </institution> <year> 1989. </year>
Reference-contexts: In this case they were even able to estimate the lower order additive terms in this approximation, which involve the Fisher information and the entropy of the prior. Further related results were given by Efroimovich [25] and Clarke <ref> [16] </ref>.
Reference: [17] <author> B. Clarke and A. Barron. </author> <title> Information-theoretic asymptotics of Bayes methods. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 36(3) </volume> <pages> 453-471, </pages> <year> 1990. </year>
Reference-contexts: For cumulative relative entropy loss, the Bayes risk has a fundamental information theoretic interpretation: it is the mutual information between a random variable representing the choice of the parameter fl of the true distribution, and the random variable given by the n observations <ref> [36, 25, 17] </ref>. This provides a beautiful connection between information theory and statistics. This connection also extends to other fields, as is discussed in [17, 8]. <p> This provides a beautiful connection between information theory and statistics. This connection also extends to other fields, as is discussed in <ref> [17, 8] </ref>. In data compression, the cumulative relative entropy risk is the redundancy, which is the expected excess code length for the best adaptive coding method, as compared to the best coding method that has prior knowledge of the true distribution [17, 40, 43]. <p> In data compression, the cumulative relative entropy risk is the redundancy, which is the expected excess code length for the best adaptive coding method, as compared to the best coding method that has prior knowledge of the true distribution <ref> [17, 40, 43] </ref>. The minimax risk is called "information" channel capacity [20], p. 184. In mathematical finance and gambling theory, the cumulative relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution [9, 17]. <p> The minimax risk is called "information" channel capacity [20], p. 184. In mathematical finance and gambling theory, the cumulative relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution <ref> [9, 17] </ref>. In computational learning theory, this risk is the average additional loss suffered by an adaptive algorithm that predicts each observation before it arrives, based on the previous observations, as compared to an algorithm that makes predictions knowing the true distribution [32, 33]. <p> The bounds are also fairly tight. However, in smooth parametric cases, our general bounds are too crude to give the precise estimates of the low order additive constants that were obtained by Clarke and Barron <ref> [17, 18] </ref>. The paper is organized as follows. In sections 2 and 3 we give precise definitions of the risks that we evaluate, and discuss the conditions required for our bounds to hold. Here we also compare our bounds to those obtained previously by other authors. <p> Further related results were given by Efroimovich [25] and Clarke [16]. Clarke and Barron gave a detailed analysis, with applications, of the risk of the Bayes strategy as a function of the true state of Nature <ref> [17] </ref>, discussing the relation of the Bayes risk to the notion of redundancy in information theory, and giving applications to hypothesis testing and portfolio selection theory. These results were extended to the Bayes and minimax risk in [18] (see also [7]). <p> In this case, for large n, both bounds differ by a constant approximately equal to D log 4 2 for small fl. In this classical case, Clarke and Barron <ref> [17] </ref> have determined the exact answer to within o (1), and it is R n;P Bayes ( fl ) = D log 2 1 log detJ ( fl ) D + o (1): Thus our simpler methods do not give the best known additive constants in the bounds for this classical <p> As pointed out by Clarke and Barron <ref> [17] </ref>, the scaling ~ D 2 log n of the Bayes risk for the smooth parametric families is strongly related to the asymptotic normality of the properly normalized posterior distribution.
Reference: [18] <author> B. Clarke and A. Barron. </author> <title> Jefferys' prior is asymptotically least favorable under entropy risk. </title> <journal> J. Statistical Planning and Inference, </journal> <volume> 41 </volume> <pages> 37-60, </pages> <year> 1994. </year>
Reference-contexts: The bounds are also fairly tight. However, in smooth parametric cases, our general bounds are too crude to give the precise estimates of the low order additive constants that were obtained by Clarke and Barron <ref> [17, 18] </ref>. The paper is organized as follows. In sections 2 and 3 we give precise definitions of the risks that we evaluate, and discuss the conditions required for our bounds to hold. Here we also compare our bounds to those obtained previously by other authors. <p> These results were extended to the Bayes and minimax risk in <ref> [18] </ref> (see also [7]). Related lower bounds, which are often quoted, were obtained by Rissanen [50], based on 7 certain asymptotic normality assumptions. <p> 2 with ff = 1=2 and Fatou's lemma lim inf I (fi fl ; Y n ) lim inf i X ( j )e 2 D 2 i n!1 X ( j )e 2 D 2 = i = H (fi fl ): This result generalizes the similar result in <ref> [18] </ref> (Corollary 1) by removing the additional conditions assumed there. More general results, including the above corollary, follow from results in Pinsker's book [46] (see also [4]).
Reference: [19] <author> G. F. Clements. </author> <title> Entropy of several sets of real-valued functions. </title> <journal> Pacific J. Math., </journal> <volume> 13 </volume> <pages> 1085-1095, </pages> <year> 1963. </year>
Reference-contexts: As shown by Barron and Yang [10], a further restriction to uniformly lower bounded densities also makes the Hellinger distance equivalent to the L q distances and does not change the metric entropy asymptot ically. By a result of Clements <ref> [19] </ref>, the metric entropy of fi under L 1 distance is given by K * (fi; L 1 ) * p+ffi : Hence K * (fi; h) * 1 The asymptotic growth rate of the minimax risk R minimax n for the above example, and many others, can be determined using
Reference: [20] <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: The minimax risk is called "information" channel capacity <ref> [20] </ref>, p. 184. In mathematical finance and gambling theory, the cumulative relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution [9, 17]. <p> Then R n; ^ P ( fl ) = t=1 Y t1 fl (y t1 ) Y dP fl (y t ) = D KL (P n by the chain rule for relative entropy (see e.g. <ref> [20] </ref>, p. 23). Of course the statistician seeks a strategy that minimizes risk. One approach is to assume that Nature is a strategic adversary, and hence selects the worst case fl for any particular strategy of the statistician. <p> n; log d ^ P It follows that the Bayes risk for relative entropy loss is given by R Bayes n; = fi fl jjM n; ) = I (fi fl ; Y n ); the mutual information between the parameter fi fl and the observations Y n . (See <ref> [20] </ref>, p. 18, for general definition and discussion of the mutual information.) It turns out that there is a simple, universal relationship between the Bayes risk R Bayes n; and the minimax risk R minimax n . <p> The last equality follows from the fact that the KL divergence is additive over the 12 product of independent distributions (see e.g. <ref> [20] </ref>, p. 23). <p> Note that this quantity is nonnegative. When H (fi) is finite it is easily verified that I (fi fl ; Y n ) = H (fi fl ) H (fi fl jY n ) (see e.g. <ref> [20] </ref>, p. 20), and thus lim sup n!1 I (fi fl ; Y n ) H (fi fl ) in this case as well.
Reference: [21] <author> L. Davisson and A. Leon-Garcia. </author> <title> A source matching approach to finding minimax codes. </title> <journal> IEEE transactions on information theory, </journal> <volume> IT-26:166-174, </volume> <year> 1980. </year>
Reference-contexts: This result can be obtained with limited effort from the general results in an early paper of Le Cam [14]. Special cases of the result were derived by Gallager [27] and Davisson and Leon-Garcia <ref> [21] </ref>, and the general result is given in [31]. Theorem 1 [31] R minimax n = sup R Bayes n; ; where the supremum is taken over all (Borel) probability measures on the parameter space fi.
Reference: [22] <author> L. Devroye and L. Gyorfi. </author> <title> Nonparametric density estimation, the L 1 view. </title> <publisher> Wiley, </publisher> <year> 1985. </year>
Reference-contexts: Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 40]. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. <ref> [22, 37] </ref>).
Reference: [23] <author> P. Diaconis and D. Freedman. </author> <title> On the consistency of Bayes estimates. </title> <journal> Ann. Statist., </journal> <volume> 14 </volume> <pages> 1-26, </pages> <year> 1986. </year>
Reference-contexts: All prior distributions on fi used in this paper are assumed to be Borel distributions of this type, and suprema over priors are also assumed to be only with respect to Borel distributions of this type. Further discussion of these issues can be found in the appendix of <ref> [23] </ref>. Finally, for integer or real-valued functions f and g, we say f ~ g if lim n!1 f (n) f g if lim inf n!1 g (n) &gt; 0 and lim sup n!1 g (n) &lt; 1. All logarithms are natural logarithms unless otherwise specified.
Reference: [24] <author> R. M. Dudley. </author> <title> A course on empirical processes. </title> <booktitle> Lecture Notes in Mathematics, </booktitle> <volume> 1097 </volume> <pages> 2-142, </pages> <year> 1984. </year>
Reference-contexts: These are the the packing and covering numbers, and the associated metric entropy, introduced by Kolmogorov and Tikhomirov in [38] and commonly used in the theory of empirical processes (see e.g. <ref> [24, 47, 29, 13] </ref>). For the following definitions, let (S; ) be any complete separable metric space. Definition 1 (Metric entropy, also called Kolmogorov *-entropy [38]) A partition of S is a collection f i g of Borel subsets of S that are pairwise disjoint and whose union is S.
Reference: [25] <author> S. Y. Efroimovich. </author> <title> Information contained in a sequence of observations. Problems in Information Transmission, </title> <booktitle> 15 </booktitle> <pages> 178-189, </pages> <year> 1980. </year>
Reference-contexts: For cumulative relative entropy loss, the Bayes risk has a fundamental information theoretic interpretation: it is the mutual information between a random variable representing the choice of the parameter fl of the true distribution, and the random variable given by the n observations <ref> [36, 25, 17] </ref>. This provides a beautiful connection between information theory and statistics. This connection also extends to other fields, as is discussed in [17, 8]. <p> In this case they were even able to estimate the lower order additive terms in this approximation, which involve the Fisher information and the entropy of the prior. Further related results were given by Efroimovich <ref> [25] </ref> and Clarke [16].
Reference: [26] <author> M. Feder, Y. Freund, and Y. Mansour. </author> <title> Optimal universal learning and prediction of probabilistic concepts. </title> <booktitle> In Proc. of IEEE Information Theory Conference, </booktitle> <pages> page 233. </pages> <publisher> IEEE, </publisher> <year> 1995. </year>
Reference-contexts: In the future we hope to further explore the applications of these results to specific estimation problems, such as the "concept learning" or "pattern classification" problems examined in current machine learning and neural network research. Some initial results along these lines can be found in [45, 34] (see also <ref> [26, 42] </ref>). There are also several other directions for further research one might pursue.
Reference: [27] <author> R. Gallager. </author> <title> Source coding with side information and universal coding. </title> <type> Technical Report LIDS-P-937, </type> <institution> MIT Laboratory for Information and Decision Systems, </institution> <year> 1979. </year>
Reference-contexts: This result can be obtained with limited effort from the general results in an early paper of Le Cam [14]. Special cases of the result were derived by Gallager <ref> [27] </ref> and Davisson and Leon-Garcia [21], and the general result is given in [31]. Theorem 1 [31] R minimax n = sup R Bayes n; ; where the supremum is taken over all (Borel) probability measures on the parameter space fi.
Reference: [28] <author> J. Ghosh, S. Ghosal, and T. Samanta. </author> <title> Statistical decision theory and related topics v. </title> <editor> In S. Gupta and J. O. Berger, editors, </editor> <title> Stability and Convergence of the Posterior in Non-Regular Problems. </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: It is interesting to look at nonregular families of probabil ities, for which the posterior fails to converge to a nontrivial limit. (For conditions that are necessary for convergence, see <ref> [28] </ref>). As an example for such nonsmooth densities, we study the following simple family on R dP (y) = e (y) I fy&gt;g ; 2 R: (8) Obviously, D KL (P fl jjP ) = 1, whenever &gt; fl and the Fisher information does not exist for any .
Reference: [29] <author> E. Gine and J. Zinn. </author> <title> Some limit theorems for empirical processes. </title> <journal> Annals of Probability, </journal> <volume> 12 </volume> <pages> 929-989, </pages> <year> 1984. </year>
Reference-contexts: These are the the packing and covering numbers, and the associated metric entropy, introduced by Kolmogorov and Tikhomirov in [38] and commonly used in the theory of empirical processes (see e.g. <ref> [24, 47, 29, 13] </ref>). For the following definitions, let (S; ) be any complete separable metric space. Definition 1 (Metric entropy, also called Kolmogorov *-entropy [38]) A partition of S is a collection f i g of Borel subsets of S that are pairwise disjoint and whose union is S.
Reference: [30] <author> R. Hasminskii and I. Ibragimov. </author> <title> On density estimation in the view of Kolmogorov's ideas in approximation theory. </title> <journal> Annals of statistics, </journal> <volume> 18 </volume> <pages> 999-1010, </pages> <year> 1990. </year>
Reference-contexts: While this work is too extensive to summarize here, we do note that some authors have also taken the general approach that we take here in using notions of metric entropy (defined below), and specifically using the Hellinger distance in obtaining these bounds (e.g. <ref> [41, 11, 12, 30, 53, 13, 10] </ref>). The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [54] (see Corollary 1, p. 360) and Barron and Yang [10]. <p> This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including Le Cam [41], Birge [11, 12], Hasminskii and Ibragimov <ref> [30] </ref>, and van de Geer [53]. 4.1 Basic bounds Our main theorem gives bounds on I (fi fl ; Y n ) and D KL (P n fl jjM n; ) in terms of the logarithms of two Laplace transforms of the I divergence, one at the value ff = 1
Reference: [31] <author> D. Haussler. </author> <title> A general minimax result for relative entropy. </title> <journal> IEEE Trans. Info Th., </journal> <note> 1997. to appear. 38 </note>
Reference-contexts: This result can be obtained with limited effort from the general results in an early paper of Le Cam [14]. Special cases of the result were derived by Gallager [27] and Davisson and Leon-Garcia [21], and the general result is given in <ref> [31] </ref>. Theorem 1 [31] R minimax n = sup R Bayes n; ; where the supremum is taken over all (Borel) probability measures on the parameter space fi. <p> This result can be obtained with limited effort from the general results in an early paper of Le Cam [14]. Special cases of the result were derived by Gallager [27] and Davisson and Leon-Garcia [21], and the general result is given in <ref> [31] </ref>. Theorem 1 [31] R minimax n = sup R Bayes n; ; where the supremum is taken over all (Borel) probability measures on the parameter space fi.
Reference: [32] <author> D. Haussler and A. Barron. </author> <title> How well do Bayes methods work for on-line prediction of f+1; 1g values? In Proceedings of the Third NEC Symposium on Computation and Cognition. </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: In computational learning theory, this risk is the average additional loss suffered by an adaptive algorithm that predicts each observation before it arrives, based on the previous observations, as compared to an algorithm that makes predictions knowing the true distribution <ref> [32, 33] </ref>. <p> Since maximin minimax always, we have sup R Bayes 1;; 1+ R minimax 1; 1+ , and from this we obtain the result stated in the Theorem. 2 The method used in obtaining the upper bound in the above result is a familiar one (see e.g. <ref> [5, 32] </ref>).
Reference: [33] <author> D. Haussler, M. Kearns, and R. E. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <journal> Machine Learning, </journal> <volume> 14(1) </volume> <pages> 83-113, </pages> <year> 1994. </year>
Reference-contexts: In computational learning theory, this risk is the average additional loss suffered by an adaptive algorithm that predicts each observation before it arrives, based on the previous observations, as compared to an algorithm that makes predictions knowing the true distribution <ref> [32, 33] </ref>.
Reference: [34] <author> D. Haussler and M. Opper. </author> <title> General bounds on the mutual information between a parameter and n conditionally independent observations. </title> <booktitle> In Proceedings of the Seventh Annual ACM Workshop on Computational Learning Theory, </booktitle> <year> 1995. </year>
Reference-contexts: Hence, the corollary shows exponential convergence. Finally, let us note that Theorem 2 and Corollary 1 can also be used to characterize the mutual information between fi fl and Y n (Bayes risk) in the general case when fi is uncountably infinite but finite dimensional. This was demonstrated in <ref> [34] </ref>. <p> In the future we hope to further explore the applications of these results to specific estimation problems, such as the "concept learning" or "pattern classification" problems examined in current machine learning and neural network research. Some initial results along these lines can be found in <ref> [45, 34] </ref> (see also [26, 42]). There are also several other directions for further research one might pursue. <p> distribution in fi but is "close to" a distribution fi, and giving a more complete characterization of the mutual information I (fi fl ; Y n ) in terms of the metric entropy properties of fi for the infinite dimensional case, as was done for the finite dimensional case in <ref> [34] </ref>. 34 9 Appendix Here we give the proof of Lemma 5. Lemma 9 Assume 0 &lt; ff &lt; 1 and &gt; 0. Let P , R and U be any distributions on Y . Let c = dP 1+ dU .
Reference: [35] <author> D. Haussler and M. Opper. </author> <title> Mutual information, metric entropy, and risk in estimation of probability distributions. </title> <type> Technical Report UCSC-CRL-96-27, </type> <institution> Univ. of Calif. Computer Research Lab, </institution> <address> Santa Cruz, CA, </address> <year> 1996. </year>
Reference-contexts: It remains open to get a useful characterization of these quantities for the cases where our assumptions do not hold, and to get more precise bounds when they do. In <ref> [35] </ref> we also show how general bounds on instantaneous risk in estimating a distribution for various other loss functions can be derived in a very simple manner from the bounds on cumulative relative entropy risk.
Reference: [36] <author> I. Ibragimov and R. Hasminskii. </author> <title> On the information in a sample about a parameter. </title> <booktitle> In Second Int. Symp. on Information Theory, </booktitle> <pages> pages 295-309, </pages> <year> 1972. </year>
Reference-contexts: For cumulative relative entropy loss, the Bayes risk has a fundamental information theoretic interpretation: it is the mutual information between a random variable representing the choice of the parameter fl of the true distribution, and the random variable given by the n observations <ref> [36, 25, 17] </ref>. This provides a beautiful connection between information theory and statistics. This connection also extends to other fields, as is discussed in [17, 8]. <p> Hasminskii showed that I (fi fl ; Y n ) ~ (D=2) log n when Y is the real line and the conditional distributions P are a smooth family of densities indexed by a real-valued parameter vector in a compact set fi of dimension D, and certain other conditions apply <ref> [36] </ref>. In this case they were even able to estimate the lower order additive terms in this approximation, which involve the Fisher information and the entropy of the prior. Further related results were given by Efroimovich [25] and Clarke [16].
Reference: [37] <author> A. J. Izenman. </author> <title> Recent developments in nonparametric density estimation. </title> <journal> JASA, </journal> <volume> 86(413) </volume> <pages> 205-224, </pages> <year> 1991. </year>
Reference-contexts: Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 40]. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. <ref> [22, 37] </ref>).
Reference: [38] <author> A. N. Kolmogorov and V. M. Tihomirov. </author> <title> *-entropy and *-capacity of sets in functional spaces. </title> <journal> Amer. Math. Soc. Translations (Ser. </journal> <volume> 2), 17 </volume> <pages> 277-364, </pages> <year> 1961. </year>
Reference-contexts: Under this assumption, (fi; h) is a metric space. We show how bounds on the minimax risk can be obtained by looking at properties of this metric space. These are the the packing and covering numbers, and the associated metric entropy, introduced by Kolmogorov and Tikhomirov in <ref> [38] </ref> and commonly used in the theory of empirical processes (see e.g. [24, 47, 29, 13]). For the following definitions, let (S; ) be any complete separable metric space. Definition 1 (Metric entropy, also called Kolmogorov *-entropy [38]) A partition of S is a collection f i g of Borel subsets <p> covering numbers, and the associated metric entropy, introduced by Kolmogorov and Tikhomirov in <ref> [38] </ref> and commonly used in the theory of empirical processes (see e.g. [24, 47, 29, 13]). For the following definitions, let (S; ) be any complete separable metric space. Definition 1 (Metric entropy, also called Kolmogorov *-entropy [38]) A partition of S is a collection f i g of Borel subsets of S that are pairwise disjoint and whose union is S. The diameter of a set A S is given by diam (A) = sup x;y2A (x; y). <p> By M * (S; ) we denote the cardinality of the largest finite *-separated subset of S, or 1 if arbitrarily large such sets exist. The following lemma is easily verified <ref> [38] </ref>. <p> Kolmogorov and Tikhomirov also introduced an abstract notion of the dimension of a metric space in their seminal paper <ref> [38] </ref>. In the following, the metric is omitted from the notation, being understood from the context. Definition 3 The upper and lower metric dimensions [38] of S are defined by dim (S) = lim sup K * (S) * dim (S) = lim inf K * (S) * respectively. <p> Kolmogorov and Tikhomirov also introduced an abstract notion of the dimension of a metric space in their seminal paper <ref> [38] </ref>. In the following, the metric is omitted from the notation, being understood from the context. Definition 3 The upper and lower metric dimensions [38] of S are defined by dim (S) = lim sup K * (S) * dim (S) = lim inf K * (S) * respectively. When dim (S) = dim (S), then this value is denoted dim (S) and called the metric dimension of S.
Reference: [39] <author> F. Komaki. </author> <title> On asymptotic properties of predictive distributions. </title> <type> Technical Report METR 94-21, </type> <institution> U. Yokyo, Math. and Eng. Physics, </institution> <year> 1994. </year>
Reference-contexts: Amari has developed an extensive theory that relates the risk when fl is the true state of Nature to certain differential-geometric properties of the parameter space fi in the neighborhood of fl involving Fisher information and related quantities [2, 3] (see also <ref> [57, 39, 55] </ref>). Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. [6, 10, 51, 56, 54]. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 40].
Reference: [40] <author> L. Gyorfi and I. Pali and E. van der Meulen. </author> <title> There is no universal source code for an infinite alphabet. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 40 </volume> <pages> 267-271, </pages> <year> 1994. </year>
Reference-contexts: In data compression, the cumulative relative entropy risk is the redundancy, which is the expected excess code length for the best adaptive coding method, as compared to the best coding method that has prior knowledge of the true distribution <ref> [17, 40, 43] </ref>. The minimax risk is called "information" channel capacity [20], p. 184. In mathematical finance and gambling theory, the cumulative relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution [9, 17]. <p> Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. [6, 10, 51, 56, 54]. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in <ref> [1, 40] </ref>. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. [22, 37]).
Reference: [41] <author> L. LeCam. </author> <title> Asymptotic methods in statistical decision theory. </title> <publisher> Springer, </publisher> <year> 1986. </year>
Reference-contexts: None of our results depend on the choice of the dominating measure -, hence for any distribution Q, the Radon-Nikodym derivative dQ d- will be abbreviated simply as dQ, following the convention in Le Cam's text <ref> [41] </ref>. Furthermore, all integrals in the results below are assumed, without specific notation, to be taken with respect to the measure -, unless otherwise indicated. <p> While this work is too extensive to summarize here, we do note that some authors have also taken the general approach that we take here in using notions of metric entropy (defined below), and specifically using the Hellinger distance in obtaining these bounds (e.g. <ref> [41, 11, 12, 30, 53, 13, 10] </ref>). The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [54] (see Corollary 1, p. 360) and Barron and Yang [10]. <p> This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including Le Cam <ref> [41] </ref>, Birge [11, 12], Hasminskii and Ibragimov [30], and van de Geer [53]. 4.1 Basic bounds Our main theorem gives bounds on I (fi fl ; Y n ) and D KL (P n fl jjM n; ) in terms of the logarithms of two Laplace transforms of the I divergence, <p> This general approach was developed by Le Cam and Birge in the context of other loss functions <ref> [41, 11, 12] </ref>. We illustrate it now by applying the above lemma to establish a simple relationship between the metric dimension of (fi; h) and the asymptotic growth rate of the minimax risk R minimax n .
Reference: [42] <author> R. Meir and N. Merhav. </author> <title> On the stochastic complexity of learning realizable and unrealizable rules. </title> <booktitle> Machine Learning, </booktitle> <address> 19(3):241-, </address> <year> 1995. </year>
Reference-contexts: In the future we hope to further explore the applications of these results to specific estimation problems, such as the "concept learning" or "pattern classification" problems examined in current machine learning and neural network research. Some initial results along these lines can be found in [45, 34] (see also <ref> [26, 42] </ref>). There are also several other directions for further research one might pursue.
Reference: [43] <author> N. Merhav and M. Feder. </author> <title> A strong version of the redundancy-capacity theorem of universal coding. </title> <journal> IEEE Trans. Info Th., </journal> <volume> 41(3):714-, </volume> <year> 1995. </year>
Reference-contexts: In data compression, the cumulative relative entropy risk is the redundancy, which is the expected excess code length for the best adaptive coding method, as compared to the best coding method that has prior knowledge of the true distribution <ref> [17, 40, 43] </ref>. The minimax risk is called "information" channel capacity [20], p. 184. In mathematical finance and gambling theory, the cumulative relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution [9, 17]. <p> Thus both this strengthened upper bound and the given lower bound hold on a set of measure 1 e fl in this case. Finally, we note that part (2) is related to part (1) in the same way that the strong redundancy-capacity theorem of universal coding in <ref> [43] </ref> is related to the usual theorems concerning average redundancy. It is possible to state a variant of Theorem 2 using the the D ff distances. Here we also make use of a particular choice for the family of distributions Q that appear in Theorem 2.
Reference: [44] <author> M. Opper and D. Haussler. </author> <title> Calculation of the learning curve of Bayes optimal classification algorithm for learning a perceptron with noise. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 75-87, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Finally, in statistical mechanics, the Bayes risk can be related to the free energy <ref> [44, 45] </ref>. In this paper, we provide upper and lower bounds on the Bayes risk for cumulative relative entropy loss in the form of Laplace integrals of the Hellinger distance between pairs of distributions in fP : 2 fig.
Reference: [45] <author> M. Opper and D. Haussler. </author> <title> Bounds for predictive errors in the statistical mechanics of in supervised learning. </title> <journal> Physical Review Letters, </journal> <volume> 75(20) </volume> <pages> 3772-3775, </pages> <year> 1995. </year>
Reference-contexts: Finally, in statistical mechanics, the Bayes risk can be related to the free energy <ref> [44, 45] </ref>. In this paper, we provide upper and lower bounds on the Bayes risk for cumulative relative entropy loss in the form of Laplace integrals of the Hellinger distance between pairs of distributions in fP : 2 fig. <p> In the future we hope to further explore the applications of these results to specific estimation problems, such as the "concept learning" or "pattern classification" problems examined in current machine learning and neural network research. Some initial results along these lines can be found in <ref> [45, 34] </ref> (see also [26, 42]). There are also several other directions for further research one might pursue.
Reference: [46] <author> M. S. Pinsker. </author> <title> Information and Information Stability of Random Variables and Processes (Transl.). </title> <type> Holden Day, </type> <year> 1964. </year> <month> 39 </month>
Reference-contexts: More general results, including the above corollary, follow from results in Pinsker's book <ref> [46] </ref> (see also [4]). Applying Theorem (1) and taking the supremum over in Corollary (2), it follows that if fi is finite then for all n, R minimax n log jfij and lim n!1 R minimax n = log jfij.
Reference: [47] <author> D. Pollard. </author> <title> Empirical Processes: Theory and Applications, </title> <booktitle> volume 2 of NSF-CBMS Regional Conference Series in Probability and Statistics. </booktitle> <institution> Institute of Math. Stat. and Am. Stat. Assoc., </institution> <year> 1990. </year>
Reference-contexts: These are the the packing and covering numbers, and the associated metric entropy, introduced by Kolmogorov and Tikhomirov in [38] and commonly used in the theory of empirical processes (see e.g. <ref> [24, 47, 29, 13] </ref>). For the following definitions, let (S; ) be any complete separable metric space. Definition 1 (Metric entropy, also called Kolmogorov *-entropy [38]) A partition of S is a collection f i g of Borel subsets of S that are pairwise disjoint and whose union is S.
Reference: [48] <author> A. Renyi. </author> <title> On measures of entropy and information. </title> <editor> In L. C. et.al., editor, </editor> <title> Fourth Berkeley Sym. </title> <journal> on Math., Stat. and Prob., </journal> <pages> pages 547-561. </pages> <year> 1960. </year>
Reference-contexts: However, in some sense it is not much stronger than the relative entropy boundedness condition, as it can also be shown that a simple function of the ff-affinity, the I-divergence defined below, approaches the relative entropy as ff approaches 1 <ref> [48] </ref>. Furthermore, it can be shown that the ff-affinity boundedness condition is weaker than the integrable envelop condition, R sup fl 2fi dP fl &lt; 1, used in the minimax analysis of relative entropy risk in [54]. <p> In obtaining these bounds, we use several notions of "distance" between probability distributions based on the ff-affinities. One such family of distances are the I-divergences introduced by Renyi <ref> [48] </ref>.
Reference: [49] <author> A. Renyi. </author> <title> On the amount of information concerning an unknown parameter in a sequence of observations. </title> <journal> Publ. Math. Inst. Hungar. Acad. Sci., </journal> <volume> 9 </volume> <pages> 617-625, </pages> <year> 1964. </year>
Reference-contexts: It also follows that if fi is infinite, then lim n!1 R minimax n = 1. In the case that fi is finite, results of Renyi <ref> [49] </ref> show further that the difference I (fi fl ; Y n ) H (fi fl ) converges to zero exponentially fast in n. We also obtain this result as follows.
Reference: [50] <author> J. Rissanen. </author> <title> Stochastic complexity and modeling. </title> <journal> The Annals of Statistics, </journal> <volume> 14(3) </volume> <pages> 1080-1100, </pages> <year> 1986. </year>
Reference-contexts: These results were extended to the Bayes and minimax risk in [18] (see also [7]). Related lower bounds, which are often quoted, were obtained by Rissanen <ref> [50] </ref>, based on 7 certain asymptotic normality assumptions.
Reference: [51] <author> J. Rissanen, T. Speed, and B. Yu. </author> <title> Density estimation by stochastic complexity. </title> <journal> IEEE Trans. Info. Th., </journal> <volume> 38 </volume> <pages> 315-323, </pages> <year> 1992. </year>
Reference-contexts: Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. <ref> [6, 10, 51, 56, 54] </ref>. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 40]. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. [22, 37]).
Reference: [52] <author> K. Symanzik. </author> <title> Proof and refinements of an inequality of Feynman. </title> <journal> J.Math. Phys., </journal> <volume> 6:1155-, </volume> <year> 1965. </year>
Reference-contexts: Hence both bounds hold on the complement of the union of these two sets, which has -measure at least 1 2e fl . We begin with the upper bounds. This requires the following lemma which has been previously utilized in the framework of Statistical Physics <ref> [52] </ref>. Lemma 1 Let P = P (w) be a measure on a set W and Q = Q (v) be a measure on a set V .
Reference: [53] <author> S. van deGeer. </author> <title> Hellinger-consistency of certain nonparametric maximum likelihood estimators. </title> <journal> Annals of Statistics, </journal> <volume> 21 </volume> <pages> 14-44, </pages> <year> 1993. </year>
Reference-contexts: While this work is too extensive to summarize here, we do note that some authors have also taken the general approach that we take here in using notions of metric entropy (defined below), and specifically using the Hellinger distance in obtaining these bounds (e.g. <ref> [41, 11, 12, 30, 53, 13, 10] </ref>). The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [54] (see Corollary 1, p. 360) and Barron and Yang [10]. <p> This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including Le Cam [41], Birge [11, 12], Hasminskii and Ibragimov [30], and van de Geer <ref> [53] </ref>. 4.1 Basic bounds Our main theorem gives bounds on I (fi fl ; Y n ) and D KL (P n fl jjM n; ) in terms of the logarithms of two Laplace transforms of the I divergence, one at the value ff = 1 (the relative entropy) and the
Reference: [54] <author> W. Wong and X. Shen. </author> <title> Probability inequalities for likelihood ratios and convergence rates for sieve MLE's. </title> <journal> Annals of Statistics, </journal> <volume> 23(2) </volume> <pages> 339-362, </pages> <year> 1995. </year>
Reference-contexts: Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. <ref> [6, 10, 51, 56, 54] </ref>. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 40]. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. [22, 37]). <p> The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen <ref> [54] </ref> (see Corollary 1, p. 360) and Barron and Yang [10]. This work is somewhat complementary to ours, in that it treats instantaneous risk, whereas we focus on cumulative risk. <p> Furthermore, it can be shown that the ff-affinity boundedness condition is weaker than the integrable envelop condition, R sup fl 2fi dP fl &lt; 1, used in the minimax analysis of relative entropy risk in <ref> [54] </ref>. Further discussion of the relationship between these conditions is given at the end of section 4.2. Note that there we reparamertize by setting ff = 1 + , &gt; 0, to avoid confusion with another usage of ff. <p> ff (P fl ; P ~ ) + n since 1 = log n ff R 1;U ; 1+ ( fl ) + o (1) Since R Bayes 1;; 1+ , the result then follows from Theorem 2. 2 3 We recently noticed that a related result is given in <ref> [54] </ref>, Theorem 5, although no explicit relationship with the ff affinities is given in the latter result. 18 Note: no attempt has been made to optimize the constants in this theorem. Now let 4 Z sup dP : We call sup 2fi dP the envelop function for fi.
Reference: [55] <author> K. Yamanishi. </author> <title> A loss bound model for on-line stochastic prediction algorithms. </title> <journal> Information and Computation, </journal> <volume> 119 </volume> <pages> 39-54, </pages> <year> 1995. </year>
Reference-contexts: Amari has developed an extensive theory that relates the risk when fl is the true state of Nature to certain differential-geometric properties of the parameter space fi in the neighborhood of fl involving Fisher information and related quantities [2, 3] (see also <ref> [57, 39, 55] </ref>). Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. [6, 10, 51, 56, 54]. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 40].
Reference: [56] <author> B. Yu. </author> <title> Lower bounds on expected redundancy for nonparametric classes. </title> <journal> IEEE Trans. Info. Th., </journal> <volume> 42(1), </volume> <year> 1996. </year>
Reference-contexts: Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. <ref> [6, 10, 51, 56, 54] </ref>. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 40]. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. [22, 37]). <p> The method for obtaining the lower bound by choosing a discrete prior on a well-separated set of is also similar in many respects to standard lower bound methods, such as those that use Fano's inequality or Assouad's lemma (see e.g. <ref> [12, 10, 56] </ref>), but the method is particularly clean in the present framework, giving a fairly good match to the upper bound.
Reference: [57] <author> H. Zhu and R. Rohwer. </author> <title> Information geometric measurements of generalization. </title> <type> Technical Report NCRG 4350, </type> <institution> Aston U., England, Neural computing research group, </institution> <year> 1995. </year> <month> 40 </month>
Reference-contexts: Amari has developed an extensive theory that relates the risk when fl is the true state of Nature to certain differential-geometric properties of the parameter space fi in the neighborhood of fl involving Fisher information and related quantities [2, 3] (see also <ref> [57, 39, 55] </ref>). Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. [6, 10, 51, 56, 54]. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 40]. <p> Jensen's inequality, it can be verified that when R Bayes 1;; 1+ &lt; 1, the minimizing ^ P , i.e. the Bayes strategy, is the distribution U = U defined by dU = fi d ( fl )dP 1+ 1 C ; where C ; = Y fi fl 1+ <ref> [57] </ref>.
References-found: 57

