URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/94.SUPERCOMPUTING.performance_prediction.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/leblanc/students.html
Root-URL: 
Email: fcrovella,leblancg@cs.rochester.edu  
Phone: (716) 275-5671  
Title: Parallel Performance Prediction Using Lost Cycles Analysis  
Author: Mark E. Crovella and Thomas J. LeBlanc 
Note: This research was supported under NSF CISE Institutional Infrastructure Program Grant No. CDA-8822724, and ONR Contract No. N00014-92-J-1801 (in conjunction with the DARPA HPCC program, ARPA Order No. 8930). Mark Crovella is supported by an ARPA Research Assistantship in High Performance Computing administered by the  
Address: Rochester, New York 14627  
Affiliation: Department of Computer Science University of Rochester  Institute for Advanced Computer Studies, University of Maryland.  
Abstract: Most performance debugging and tuning of parallel programs is based on the "measure-modify" approach, which is heavily dependent on detailed measurements of programs during execution. This approach is extremely time-consuming and does not lend itself to predicting performance under varying conditions. Analytic modeling and scalability analysis provide predictive power, but are not widely used in practice, due primarily to their emphasis on asymptotic behavior and the difficulty of developing accurate models that work for real-world programs. In this paper we describe a set of tools for performance tuning of parallel programs that bridges this gap between measurement and modeling. Our approach is based on lost cycles analysis, which involves measurement and modeling of all sources of overhead in a parallel program. We first describe a tool for measuring overheads in parallel programs that we have incorporated into the runtime environment for Fortran programs on the Kendall Square KSR1. We then describe a tool that fits these overhead measurements to analytic forms. We illustrate the use of these tools by analyzing the performance tradeoffs among parallel implementations of 2D FFT. These examples show how our tools enable programmers to develop accurate performance models of parallel applications without requiring extensive performance modeling expertise. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas E. Anderson and Edward D. Lazowska. Quartz: </author> <title> A tool for tuning parallel program performance. </title> <booktitle> In ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 115-125, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: A number of researchers in the parallel performance evaluation and tuning community have focused on measurement of multiple parallel overheads. In particular the PEM system has developed a taxonomy of parallel overheads similar to ours [5], and Quartz and MemSpy together can measure the overhead categories we use <ref> [1; 20] </ref>. However, since these (and similar tool sets) are not oriented toward performance prediction of alternative implementations, the specific overhead categories they use are not always amenable to easy analysis. In addition, the completeness criterion has not been emphasized in most previous overhead measurement work [5; 22; 28].
Reference: [2] <author> A. C. Atkinson and A. N. Donev. </author> <title> Optimum Experimental Design. </title> <booktitle> Oxford Statistical Science Series. </booktitle> <publisher> Oxford Science Publications, </publisher> <year> 1992. </year>
Reference-contexts: In addition, overhead models for many factors are nonlinear. For these two reasons, the user must use factorial or reduced-factorial experimental designs, and must measure more than two values for each factor <ref> [2; 4] </ref>. In practice, these criteria are met if the user samples the "edges" of the parameter space, for 3 or more points on each edge.
Reference: [3] <author> Thomas Ball and James R. Larus. </author> <title> Optimally profiling and tracing programs. </title> <booktitle> In Conference Record of the Nineteenth POPL, </booktitle> <address> Albuquerque, NM, </address> <month> 19-22 January </month> <year> 1992. </year>
Reference-contexts: Measuring and debugging program performance without gathering large amounts of data is an important capability in its own right, and is the subject of much current effort <ref> [3; 21; 23] </ref>.
Reference: [4] <author> George E. P. Box, William G. Hunter, and J. Stu-art Hunter. </author> <title> Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model 9 Building. Wiley Series in Probability and Math--ematical Statistics. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1978. </year>
Reference-contexts: In addition, overhead models for many factors are nonlinear. For these two reasons, the user must use factorial or reduced-factorial experimental designs, and must measure more than two values for each factor <ref> [2; 4] </ref>. In practice, these criteria are met if the user samples the "edges" of the parameter space, for 3 or more points on each edge.
Reference: [5] <author> Helmar Burkhart and Roland Millen. </author> <title> Performance measurement tools in a multiprocessor environment. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(5) </volume> <pages> 725-737, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: A number of researchers in the parallel performance evaluation and tuning community have focused on measurement of multiple parallel overheads. In particular the PEM system has developed a taxonomy of parallel overheads similar to ours <ref> [5] </ref>, and Quartz and MemSpy together can measure the overhead categories we use [1; 20]. However, since these (and similar tool sets) are not oriented toward performance prediction of alternative implementations, the specific overhead categories they use are not always amenable to easy analysis. <p> However, since these (and similar tool sets) are not oriented toward performance prediction of alternative implementations, the specific overhead categories they use are not always amenable to easy analysis. In addition, the completeness criterion has not been emphasized in most previous overhead measurement work <ref> [5; 22; 28] </ref>. A common method of modeling of overheads in parallel programs is scalability analysis [18]. Scala bility analysis develops analytic, asymptotic models of computation and selected overhead categories as a function of the size of the problem n and the number of processors p.
Reference: [6] <author> David Callahan, Ken Kennedy, and Allan Porter-field. </author> <title> Analyzing and visualizing performance of memory hierarchies. </title> <booktitle> In Performance Instrumentation and Visualization, </booktitle> <pages> pages 1-26. </pages> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: this, we only needed to measure a small number of data points in each of the 2 environmental dimensions, and compare the resulting lost cycles models. 5 Related Work The majority of the tools and metrics devised for performance evaluation and tuning reflect their orientation on the measure-modify paradigm (e.g., <ref> [6; 14; 15] </ref>). Such tools are very useful in application fine-tuning, but usually do not provide completeness (i.e., they don't measure all sources of overhead in the execution). As a result, they are limited to cases in which the principal overheads are known in advance.
Reference: [7] <author> Mark J. Clement and Michael J. Quinn. </author> <title> Analytical performance prediction on multicomputers. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 886-894, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Our approach, while not providing as much accuracy as is possible using the template approach, does not restrict the programmer to any set of pre-constructed templates. Our work bears similar goals to <ref> [7] </ref>, which takes a static approach to performance prediction, rather than the dynamic approach we use. Finally, the notion of selecting alternative implementations based on the environment is present in the ISSOS system [26]. The emphasis in that work is on selecting alternative implementations dynamically based on ongoing system monitoring.
Reference: [8] <author> Mark E. Crovella and Thomas J. LeBlanc. </author> <title> Performance debugging using parallel performance predicates. </title> <booktitle> In Proceedings of the 3rd ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 140-150, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Each of these extensions appears to be straightforward within the existing framework however. 3 Tools for Lost Cycles Anal ysis 3.1 Measuring Lost Cycles: pp In previous work <ref> [8] </ref> we showed that basing measurement on logical expressions that recognize lost cycles is a particularly useful approach. We call these expressions performance predicates. The use of performance predicates to specify categories of lost cycles makes program instrumentation straightforward, and allows predicate profiles to be constructed based on user demands.
Reference: [9] <author> Mark E. Crovella and Thomas J. LeBlanc. </author> <title> The search for lost cycles: A new approach to parallel program performance evaluation. </title> <type> Technical Report 479, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Additional case studies showing the use of lost cycles analysis on less regularly-structured applications are presented in <ref> [9] </ref>. The serial implementation of 2D FFT consists of a number of iterations which consist of 1D FFTs on columns of the input matrix, followed by 1D FFTs on the rows of the matrix.
Reference: [10] <author> Lawrence A. Crowl, Mark Crovella, Thomas J. LeBlanc, and Michael L. Scott. </author> <title> The advantages of multiple parallelizations in combinatorial search. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 110-123, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: In fact, the best parallelization for a given application can depend on the size of the input dataset, the structure of the input dataset, the specific problem definition, the number of processors used, and the particular machine used <ref> [10] </ref>. Exploring each of these environmental factors fully requires the predictive power of modeling; it is simply impractical to measure the effects of all these factors after each modification of the application.
Reference: [11] <author> H. A. David. </author> <title> Order Statistics. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1981. </year>
Reference-contexts: Load imbalance can arise in two ways: variation in the running time of each loop iteration, and unequal numbers of loop iterations handled by different processors. If variation in running time of iterations is random, the time taken by the longest iteration can be modeled using order statistics (e.g., <ref> [11] </ref>) and predicted to grow proportionally to p Communication loss can be difficult to model, but for this simple application is proportional to p. Finally, resource contention can be expected to grow linearly once the number of processors passes a threshold value.
Reference: [12] <institution> Digital Equipment Corporation. DECChip 21064-AA RISC microprocessor preliminary data sheet. Digital Equipment Corporation, Maynard, </institution> <address> MA, </address> <year> 1992. </year>
Reference-contexts: Although the performance monitoring hardware on the KSR is rather unique, something comparable may be required for other cache-coherent architectures. On simpler architectures, such as a message-passing system, the performance monitoring capabilities of the DEC Alpha <ref> [12] </ref> should be sufficient to gather the same information. pp is currently installed for use by the user community at the Cornell Theory Center on their 128-node KSR1. Example output from the current version of pp is shown in Figure 1.
Reference: [13] <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support for shared-memory parallel computing. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11 </volume> <pages> 1-32, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Performance tuning over a range of machines, data, or processors is important because the best parallelization of an application is often not fixed. Many researchers have noted that the best paral-lelization for a given application can vary depending on the input, machine, or problem definition <ref> [13; 25; 27] </ref>. In fact, the best parallelization for a given application can depend on the size of the input dataset, the structure of the input dataset, the specific problem definition, the number of processors used, and the particular machine used [10].
Reference: [14] <author> Aaron J. Goldberg and John L. Hennessy. </author> <title> Mtool: An integrated system for performance debugging shared memory multiprocessor applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 28-40, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: this, we only needed to measure a small number of data points in each of the 2 environmental dimensions, and compare the resulting lost cycles models. 5 Related Work The majority of the tools and metrics devised for performance evaluation and tuning reflect their orientation on the measure-modify paradigm (e.g., <ref> [6; 14; 15] </ref>). Such tools are very useful in application fine-tuning, but usually do not provide completeness (i.e., they don't measure all sources of overhead in the execution). As a result, they are limited to cases in which the principal overheads are known in advance.
Reference: [15] <author> Michael T. Heath and Jennifer A. Etheridge. </author> <title> Visualizing the performance of parallel programs. </title> <journal> IEEE Software, </journal> <volume> 8(5) </volume> <pages> 29-39, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: this, we only needed to measure a small number of data points in each of the 2 environmental dimensions, and compare the resulting lost cycles models. 5 Related Work The majority of the tools and metrics devised for performance evaluation and tuning reflect their orientation on the measure-modify paradigm (e.g., <ref> [6; 14; 15] </ref>). Such tools are very useful in application fine-tuning, but usually do not provide completeness (i.e., they don't measure all sources of overhead in the execution). As a result, they are limited to cases in which the principal overheads are known in advance.
Reference: [16] <author> Raj Jain. </author> <title> The Art of Computer Systems Performance Analysis. </title> <publisher> Wiley and Sons, Inc., </publisher> <year> 1991. </year>
Reference-contexts: In some cases, models will be additive; in other cases they will be multiplicative. The data collected in step 1 can be used to distinguish these cases by using standard allocation of variation techniques for factorial designs <ref> [16] </ref>. Allocation of variation uses least-squares fits to assess the main (additive) and interaction (multiplicative) terms of a model.
Reference: [17] <author> Kendall Square Research. </author> <title> KSR1 principles of operation. Kendall Square Research, </title> <type> 170 Tracer Lane, </type> <address> Waltham MA, </address> <month> 15 October </month> <year> 1991. </year>
Reference-contexts: Additional calls to our library routines are inserted at the start and end of parallel loops, parallel tasks, 3 and synchronization operations. The inserted li-brary calls are quite simple and could easily be added by a source-to-source preprocessor. The KSR1 <ref> [17] </ref> is a two-level ring architecture in which all memory is managed as a cache, which is organized in two levels on each node. Thus, inter-node communication occurs only as the result of misses in the secondary cache.
Reference: [18] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing scalability of parallel algorithms and architectures. </title> <type> Technical report, </type> <institution> TR-91-18, Computer Science Department, University of Minnesota, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: In addition, the completeness criterion has not been emphasized in most previous overhead measurement work [5; 22; 28]. A common method of modeling of overheads in parallel programs is scalability analysis <ref> [18] </ref>. Scala bility analysis develops analytic, asymptotic models of computation and selected overhead categories as a function of the size of the problem n and the number of processors p.
Reference: [19] <author> Ted Lehr, Zary Segall, Dalibor Vrsalovic, Eddie Caplan, Alan Chung, and Charles Fineman. </author> <title> Visualizing performance debugging. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 38-51, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Traditional performance debugging and tuning of parallel programs is based on detailed measurement of program executions. Programmers typically implement a program, measure its performance in detail, modify the program, and iterate. This "measure-modify" approach to performance tuning <ref> [19] </ref> results in detailed knowledge about a series of executions of a parallel program. However, programmers would often like to know about other, potential, executions of the program: with varying inputs, on a different number of processors, or on a different machine.
Reference: [20] <author> Margaret Martonosi, Anoop Gupta, and Thomas Anderson. MemSpy: </author> <title> Analyzing memory system bottlenecks in programs. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 1-12, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: A number of researchers in the parallel performance evaluation and tuning community have focused on measurement of multiple parallel overheads. In particular the PEM system has developed a taxonomy of parallel overheads similar to ours [5], and Quartz and MemSpy together can measure the overhead categories we use <ref> [1; 20] </ref>. However, since these (and similar tool sets) are not oriented toward performance prediction of alternative implementations, the specific overhead categories they use are not always amenable to easy analysis. In addition, the completeness criterion has not been emphasized in most previous overhead measurement work [5; 22; 28].
Reference: [21] <author> Barton P. Miller and Jong-Deok Choi. </author> <title> A mechanism for efficient debugging of parallel programs. </title> <booktitle> In Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 135-144, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Measuring and debugging program performance without gathering large amounts of data is an important capability in its own right, and is the subject of much current effort <ref> [3; 21; 23] </ref>.
Reference: [22] <author> Peter Mtller-Nielsen and Jtrgen Staunstrup. Problem-heap: </author> <title> A paradigm for multiprocessor algorithms. </title> <journal> Parallel Computing, </journal> <volume> 4 </volume> <pages> 64-74, </pages> <year> 1987. </year>
Reference-contexts: However, since these (and similar tool sets) are not oriented toward performance prediction of alternative implementations, the specific overhead categories they use are not always amenable to easy analysis. In addition, the completeness criterion has not been emphasized in most previous overhead measurement work <ref> [5; 22; 28] </ref>. A common method of modeling of overheads in parallel programs is scalability analysis [18]. Scala bility analysis develops analytic, asymptotic models of computation and selected overhead categories as a function of the size of the problem n and the number of processors p.
Reference: [23] <author> Robert H. B. Netzer and Barton P. Miller. </author> <title> Optimal tracing and replay for debugging message-passing parallel programs. </title> <booktitle> In Proceedings Supercomputing '92, </booktitle> <pages> pages 502-511, </pages> <address> Minn., MN, </address> <month> November </month> <year> 1992. </year> <note> IEEE. </note>
Reference-contexts: Measuring and debugging program performance without gathering large amounts of data is an important capability in its own right, and is the subject of much current effort <ref> [3; 21; 23] </ref>.
Reference: [24] <author> G. R. Nudd, E. Papaefstathiou, Y. Papay, T. J. Atherton, C. T. Clarke, D. J. Kerbyson, A. F. Stratton, R. Ziani, and M. J. Zemerly. </author> <title> A layered approach to the characterisation of parallel systems for performance prediction. </title> <booktitle> In Performance Evaluation of Parallel Systems (PEPS) '93, </booktitle> <pages> pages 26-34, </pages> <address> U. Warwick, U.K., </address> <month> 29-30 November </month> <year> 1993. </year>
Reference-contexts: In addition, they cannot be used directly for performance prediction or for a comparison of alternative implementations in general because of their reliance on asymptotic analysis, and on constants which must be determined experimentally. Another method of performance prediction is based on the notion of parallel program templates <ref> [24; 29] </ref>. These methods require the programmer to explicitly select a program template, which defines the mapping from application-level constructs onto the parallel hardware. Templates free the programmer from developing complex performance models, since by selecting a template for the application, the programmer also implicitly selects a performance model.
Reference: [25] <author> V. Nageshwara Rao and Vipin Kumar. </author> <title> Parallel depth-first search. </title> <journal> International Journal of Parallel Processing, </journal> <volume> 16(6), </volume> <year> 1989. </year>
Reference-contexts: Performance tuning over a range of machines, data, or processors is important because the best parallelization of an application is often not fixed. Many researchers have noted that the best paral-lelization for a given application can vary depending on the input, machine, or problem definition <ref> [13; 25; 27] </ref>. In fact, the best parallelization for a given application can depend on the size of the input dataset, the structure of the input dataset, the specific problem definition, the number of processors used, and the particular machine used [10].
Reference: [26] <author> Karsten Schwan, Rajiv Amnath, Sridhar Vasude-van, and David Ogle. </author> <title> A language and system for the construction and tuning of parallel programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(4) </volume> <pages> 455-471, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: Our work bears similar goals to [7], which takes a static approach to performance prediction, rather than the dynamic approach we use. Finally, the notion of selecting alternative implementations based on the environment is present in the ISSOS system <ref> [26] </ref>. The emphasis in that work is on selecting alternative implementations dynamically based on ongoing system monitoring.
Reference: [27] <author> J. Subhlok, J. M. Stichnoth, D. R. O'Hallaron, and T. Gross. </author> <title> Programming task and data parallelism on a multicomputer. </title> <booktitle> In Proceedings of the Fourth PPOPP, </booktitle> <address> San Diego, CA, </address> <month> 20-22 May </month> <year> 1993. </year>
Reference-contexts: Performance tuning over a range of machines, data, or processors is important because the best parallelization of an application is often not fixed. Many researchers have noted that the best paral-lelization for a given application can vary depending on the input, machine, or problem definition <ref> [13; 25; 27] </ref>. In fact, the best parallelization for a given application can depend on the size of the input dataset, the structure of the input dataset, the specific problem definition, the number of processors used, and the particular machine used [10]. <p> Data Parallel 2D FFT A comparison of the task parallel and data parallel implementations of 2D FFT on the iWarp was presented in <ref> [27] </ref>. On that machine, the authors discovered that as data set sizes are varied past a certain threshold, the choice of which implementation is best changes. For small data sets (n 128) the parallel tasking implementation outperformed the pure data parallel implementation.
Reference: [28] <author> Thin-Fong Tsuei and Mary K. Vernon. </author> <title> Diagnosing parallel program speedup limitations using resource contention models. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, pages I-185 - I-189. </booktitle> <publisher> The Pennsylvania State University Press, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Communication loss is measured as a product of the number of cache misses and the ideal time to perform the cache line transfers. Resource contention (contention for the ring interconnect and for remote memories) is measured as in <ref> [28] </ref> | that is, the ideal time to perform the communication operations is compared to the actual elapsed time. Since the KSR1 hardware monitors both the number of cache lines transferred and the elapsed time waiting for cache lines, this calculation is straightforward. <p> However, since these (and similar tool sets) are not oriented toward performance prediction of alternative implementations, the specific overhead categories they use are not always amenable to easy analysis. In addition, the completeness criterion has not been emphasized in most previous overhead measurement work <ref> [5; 22; 28] </ref>. A common method of modeling of overheads in parallel programs is scalability analysis [18]. Scala bility analysis develops analytic, asymptotic models of computation and selected overhead categories as a function of the size of the problem n and the number of processors p.
Reference: [29] <author> Dalibor Vrsalovic, Daniel P. Siewiorek, Zary Z. Se-gal, and Edward F. Gehringer. </author> <title> Performance prediction and calibration for a class of multiprocessor systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(11) </volume> <pages> 1353-1365, </pages> <month> November </month> <year> 1988. </year> <month> 10 </month>
Reference-contexts: In addition, they cannot be used directly for performance prediction or for a comparison of alternative implementations in general because of their reliance on asymptotic analysis, and on constants which must be determined experimentally. Another method of performance prediction is based on the notion of parallel program templates <ref> [24; 29] </ref>. These methods require the programmer to explicitly select a program template, which defines the mapping from application-level constructs onto the parallel hardware. Templates free the programmer from developing complex performance models, since by selecting a template for the application, the programmer also implicitly selects a performance model.
References-found: 29

