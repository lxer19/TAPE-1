URL: http://www.cri.ensmp.fr/doc/A-264.ps.Z
Refering-URL: http://www.cri.ensmp.fr/rapports.html
Root-URL: 
Email: (coelho@cri.ensmp.fr)  
Phone: voice: (+33 1) 64 69 48 52, fax: (+33 1) 64 69 47 09  
Title: Compilation of I/O Communications for HPF  
Author: Fabien Coelho 
Address: 35, rue Saint-Honore, 77305 Fontainebleau cedex, France  
Affiliation: Centre de Recherche en Informatique, Ecole des mines de Paris,  
Web: A-264-CRI)  
Note: (in Frontiers'95 also report  
Abstract: The MIMD Distributed Memory architecture is the choice architecture for massively parallel machines. It insures scalability, but at the expense of programming ease. New languages such as HPF were introduced to solve this problem: the user advises the compiler about data distribution and parallel computations through directives. This paper focuses on the compilation of I/O communications for HPF. Data must be efficiently collected to or updated from I/O nodes with vectorized messages, for any possible mapping. The problem is solved using standard polyhedron scanning techniques. The code generation issues to handle the different cases are addressed. Then the method is improved and extended to parallel I/Os. This work suggests new HPF directives for parallel I/Os. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <title> Compilers Prin ciples, Techniques, and Tools. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: The simplification scheme used in the implementation may keep unnecessary variables in some cases. Second, the generated code would benefit from many standard optimizations such as strength reduction, invariant code motion, dag detection and constant propagation <ref> [1] </ref>, which are performed by any classical compiler at no cost for hpfc.
Reference: [2] <author> S. P. Amarasinghe and M. S. Lam. </author> <title> Communication Op timization and Code Generation for Distributed Memory Machines. </title> <booktitle> In ACM SIGPLAN International Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year> <month> 7 </month>
Reference-contexts: It is in the spirit of the client-server paradigm, and a database-like protocol insures the coherency of the concurrent accesses. Moreover polyhedron scanning techniques have proven to be efficient and realistic methods for compilers to deal with general code transformations [39, 27] as well as distributed code generation <ref> [2, 29, 4] </ref>. In [2], a dataflow analysis is used to determine the communication sets. These sets are presented in a linear framework, which includes more parametrization and overlaps. The data mapping onto the processors is a superset of the hpf mapping. <p> Moreover polyhedron scanning techniques have proven to be efficient and realistic methods for compilers to deal with general code transformations [39, 27] as well as distributed code generation [2, 29, 4]. In <ref> [2] </ref>, a dataflow analysis is used to determine the communication sets. These sets are presented in a linear framework, which includes more parametrization and overlaps. The data mapping onto the processors is a superset of the hpf mapping.
Reference: [3] <author> C. Ancourt. </author> <title> Generation automatique de codes de trans fert pour multiprocesseurs a memoires locales. </title> <type> PhD thesis, </type> <institution> Universite Paris VI, </institution> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: Moreover other addressing schemes [33] may be translated into a linear framework. 1.3 Resolution The integer solutions to the previous set of equations must be enumerated to generate the communications. Any polyhedron scanning technique <ref> [19, 3, 5, 39, 17, 27, 13, 30, 28] </ref> can be used. The key issue is the control of the enumeration order to generate tight bounds and to reduce the control overhead. Here the word polyhedron denotes a set of constraints that defines a subspace of integer points.
Reference: [4] <author> C. Ancourt, F. Coelho, F. Irigoin, and R. Keryell. </author> <title> A Lin ear Algebra Framework for Static HPF Code Distribution. </title> <booktitle> In Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, </address> <month> Dec. </month> <year> 1993. </year> <note> Also available as TR EMP A/250/CRI. </note>
Reference-contexts: declarations, the hpf directives and the local addressing scheme (Figure 4: each node allocates a 9 fi 15 array A 0 to store its local part of distributed array A, and the displayed formulae allow to switch from global to local addresses) are translated into linear constraints, as suggested in <ref> [4] </ref>. Together with the region, it gives a fully linear description of the communication problem, that is the enumeration of the elements to be sent and received. Figure 5 shows the linear constraints derived for the running example. <p> are ordered by dimension first, then the cycle before the array variables: : : However the generated code may still be improved. 5 Pio ( io ) 1 io 2 0 ffi io 1 First, Hermite transformations could have been systematically applied to minimize the polyhedron dimension, as suggested in <ref> [4] </ref>. The simplification scheme used in the implementation may keep unnecessary variables in some cases. Second, the generated code would benefit from many standard optimizations such as strength reduction, invariant code motion, dag detection and constant propagation [1], which are performed by any classical compiler at no cost for hpfc. <p> However the record oriented file organization defined by the Fortran standard, which may also be mapped onto i/o nodes, is not directly addressed by this work, but such mappings may also be translated into linear constraints and compiled <ref> [4] </ref>. 3.3 Related work Other teams investigate the distributed memory multicomputers i/o issues, both on the language and runtime support point of view. Most works focus on the development of runtime libraries to handle parallel i/o in a convenient way for users. <p> It is in the spirit of the client-server paradigm, and a database-like protocol insures the coherency of the concurrent accesses. Moreover polyhedron scanning techniques have proven to be efficient and realistic methods for compilers to deal with general code transformations [39, 27] as well as distributed code generation <ref> [2, 29, 4] </ref>. In [2], a dataflow analysis is used to determine the communication sets. These sets are presented in a linear framework, which includes more parametrization and overlaps. The data mapping onto the processors is a superset of the hpf mapping. <p> The mapping semantics is a subset of the hpf mapping semantics. The Pandore local memory allocation is based on a page-like technique, managed by the compiler [30]. Both teh local addressing scheme and the cyclic distributions are integrated in <ref> [4] </ref> to compile hpf. Moreover equalities are used to improve the scanning loop nests, and temporary allocation issues are discussed. Conclusion Experiments were performed on a network of workstations and on a CM5 with the PVM3-based generated code for the host-node model. <p> From Fortran 77 code and static hpf mapping directives, it generates portable PVM 3-based code. It implements several optimizations such as message vectorization and overlap analysis on top of a runtime resolution compilation. Future work includes experiments, the implementation of advanced optimizations <ref> [4] </ref> and tests on real world codes. Acknowledgements I am thankfull to Corinne Ancourt, Beatrice Creusillet, Fran~cois Irigoin, Pierre Jouvelot, Kelita Le N enaon, Alexis Platonoff and Xavier Redon for their comments and suggestions.
Reference: [5] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with DO loops. </title> <booktitle> In Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Moreover other addressing schemes [33] may be translated into a linear framework. 1.3 Resolution The integer solutions to the previous set of equations must be enumerated to generate the communications. Any polyhedron scanning technique <ref> [19, 3, 5, 39, 17, 27, 13, 30, 28] </ref> can be used. The key issue is the control of the enumeration order to generate tight bounds and to reduce the control overhead. Here the word polyhedron denotes a set of constraints that defines a subspace of integer points. <p> One loop bound level must only depend on the outer levels, hence the triangular structure. The technique used in the implementation is algorithm row echelon <ref> [5] </ref>. It is a two stage algorithm that takes as input a polyhedron and a list of variables, and generates a scannable polyhedron on these variables, the others being considered as parameters.
Reference: [6] <author> B. Apvrille. </author> <title> Calcul de regions de tableaux exactes. </title> <booktitle> In Ren contres Francophones du Parallelisme, </booktitle> <pages> pages 65-68, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: )-MUST-READ-fff 1 + ff 2 1 + m; 3 ff 1 ; 3 ff 2 g ff 0 ff 0 1 ; ff 0 each array accessed within this statement, a set of linear equalities and inequalities describing the region of accessed elements can be automatically derived from the program <ref> [36, 37, 6, 7] </ref>. A region is given an action and an approximation. The action is READ if the elements are read and WRITE if written. The approximation is MUST if all the elements are actually accessed and MAY if only a subset of these elements will probably be accessed.
Reference: [7] <author> B. Apvrille-Creusillet. </author> <title> Regions exactes et privatisation de tableaux. </title> <type> Master's thesis, </type> <institution> Universite Paris VI, </institution> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: )-MUST-READ-fff 1 + ff 2 1 + m; 3 ff 1 ; 3 ff 2 g ff 0 ff 0 1 ; ff 0 each array accessed within this statement, a set of linear equalities and inequalities describing the region of accessed elements can be automatically derived from the program <ref> [36, 37, 6, 7] </ref>. A region is given an action and an approximation. The action is READ if the elements are read and WRITE if written. The approximation is MUST if all the elements are actually accessed and MAY if only a subset of these elements will probably be accessed.
Reference: [8] <author> F. Bodin, L. Kervella, and T. Priol. Fortran-S: </author> <title> A Fortran Interface for Shared Virtual Memory Architectures. </title> <booktitle> In Supercomputing, </booktitle> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Non portable codes are produced at great expense. This problem must be solved to enlarge the potential market for these machines. The SVM (Shared Virtual Memory) approach <ref> [31, 8] </ref> puts the burden on the hardware and operating system, which have to simulate a shared memory. The hpf Forum chose to put it on the language and compiler technology [22], following early academic and commercial experiments [25, 14, 38, 40, 11, 12].
Reference: [9] <author> R. Bordawekar, J. M. del Rosario, and A. Choudhary. </author> <title> De sign and Evaluation of Primitives for Parallel I/O. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 452-461, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: The suggested solutions focus more on general issues than on specific techniques to handle the required communications efficiently. They are designed for the spmd programming model, and the allowed data distribution semantics is reduced with respect to the extended data mapping available in hpf. In <ref> [9] </ref>, a two-phase access strategy is advocated to handle parallel i/o efficiently. One phase performs the i/o, and the other redistribute the data as expected by the application.
Reference: [10] <author> R. R. Bordawekar, A. N. Choudhary, and J. M. del Rosario. </author> <title> An experimental performance evaluation of touchstone delta concurrent file system. </title> <booktitle> In ACM International Conference on Supercomputing, </booktitle> <pages> pages 367-376, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The user is given a way to advise the compiler about data distributions and parallel computations, through a set of directives added to Fortran 90. The Forum did not address parallel i/o since no concensus was found. However MPP machines have parallel i/o capabilities <ref> [10] </ref> that have to be used efficiently to run real applications [35, 20, 32]. For instance, the TMC's CM5 or the Intel's Paragon have so called i/o nodes attached to their fast network. They are used in parallel by applications requiring high i/o throughput.
Reference: [11] <author> T. Brandes. </author> <title> Efficient data parallel programming without explicit message passing for distributed memory multiprocessors. </title> <type> Internal Report AHR-92 4, </type> <institution> High Performance Computing Center, German National Research Institute for Computer Science, </institution> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: The SVM (Shared Virtual Memory) approach [31, 8] puts the burden on the hardware and operating system, which have to simulate a shared memory. The hpf Forum chose to put it on the language and compiler technology [22], following early academic and commercial experiments <ref> [25, 14, 38, 40, 11, 12] </ref>. The user is given a way to advise the compiler about data distributions and parallel computations, through a set of directives added to Fortran 90. The Forum did not address parallel i/o since no concensus was found.
Reference: [12] <author> T. Brandes. </author> <title> Evaluation of high performance fortran on some real applications. In High-Performance Computing and Networking, </title> <publisher> Springer-Verlag LNCS 797, </publisher> <pages> pages 417-422, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: The SVM (Shared Virtual Memory) approach [31, 8] puts the burden on the hardware and operating system, which have to simulate a shared memory. The hpf Forum chose to put it on the language and compiler technology [22], following early academic and commercial experiments <ref> [25, 14, 38, 40, 11, 12] </ref>. The user is given a way to advise the compiler about data distributions and parallel computations, through a set of directives added to Fortran 90. The Forum did not address parallel i/o since no concensus was found.
Reference: [13] <author> Z. Chamski. </author> <title> Fast and efficient generation of loop bounds. </title> <note> Research Report 2095, INRIA, </note> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: Moreover other addressing schemes [33] may be translated into a linear framework. 1.3 Resolution The integer solutions to the previous set of equations must be enumerated to generate the communications. Any polyhedron scanning technique <ref> [19, 3, 5, 39, 17, 27, 13, 30, 28] </ref> can be used. The key issue is the control of the enumeration order to generate tight bounds and to reduce the control overhead. Here the word polyhedron denotes a set of constraints that defines a subspace of integer points.
Reference: [14] <author> M. Chen and J. Cowie. </author> <title> Prototyping Fortran-90 compilers for Massively Parallel Machines. </title> <journal> ACM SIGPLAN Notices, </journal> <pages> pages 94-105, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The SVM (Shared Virtual Memory) approach [31, 8] puts the burden on the hardware and operating system, which have to simulate a shared memory. The hpf Forum chose to put it on the language and compiler technology [22], following early academic and commercial experiments <ref> [25, 14, 38, 40, 11, 12] </ref>. The user is given a way to advise the compiler about data distributions and parallel computations, through a set of directives added to Fortran 90. The Forum did not address parallel i/o since no concensus was found.
Reference: [15] <author> F. Coelho. </author> <title> Etude de la Compilation du high perfor mance fortran. </title> <type> Master's thesis, </type> <institution> Universite Paris VI, </institution> <month> Sept. </month> <year> 1993. </year> <institution> Rapport de DEA Systemes Informatiques. TR EMP E/178/CRI. </institution>
Reference-contexts: Thus the enumeration order is the same. This technique is implemented in hpfc, a prototype hpf compiler <ref> [16, 15] </ref>. An excerpt of the automatically generated code for the running example is shown in on the nodes. The spmd code is parameterized by the processor identity ( 1 ; 2 ) which is instanciated differently on each node.
Reference: [16] <author> F. Coelho. </author> <title> Experiments with HPF compilation for a net work of workstations. In High-Performance Computing and Networking, </title> <publisher> Springer-Verlag LNCS 797, </publisher> <pages> pages 423-428, </pages> <month> Apr. </month> <year> 1994. </year> <note> Also available as TR EMP A/257/CRI. </note>
Reference-contexts: Thus the enumeration order is the same. This technique is implemented in hpfc, a prototype hpf compiler <ref> [16, 15] </ref>. An excerpt of the automatically generated code for the running example is shown in on the nodes. The spmd code is parameterized by the processor identity ( 1 ; 2 ) which is instanciated differently on each node.
Reference: [17] <author> J.-F. Collard, P. Feautrier, and T. Risset. </author> <title> Construction of DO loops from Systems of Affine Constraints. LIP RR93 15, </title> <address> ENS-Lyon, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Moreover other addressing schemes [33] may be translated into a linear framework. 1.3 Resolution The integer solutions to the previous set of equations must be enumerated to generate the communications. Any polyhedron scanning technique <ref> [19, 3, 5, 39, 17, 27, 13, 30, 28] </ref> can be used. The key issue is the control of the enumeration order to generate tight bounds and to reduce the control overhead. Here the word polyhedron denotes a set of constraints that defines a subspace of integer points.
Reference: [18] <author> P. F. Corbett, D. G. Feitelson, J.-P. Prost, and S. John son Baylor. </author> <title> Parallel Access to Files in the Vesta File System. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 472-481, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: This kind of arrangement is representative of i/o capabilities of some real world machines. However, there is no need for i/o nodes to be physically mapped as depicted. This regular mapping may only be a virtual one <ref> [18] </ref> which allows to attach groups of processors to i/o nodes. the link between the i/o nodes and their attached processors. io ranges over the i/o nodes. An auxiliary dummy variable (ffi io ) is used to scan the corresponding processors. <p> This whatever the hpf mapping, as part of a dataparallel program to be compiled to a mimd architecture. In [23], the PETSc/Chameleon package is presented. It emphasizes portability and parallel i/o abstraction. <ref> [18] </ref> suggests to decluster explicitely the files, thus defining logical partitions to be dealt with separately. Their approach is investigated in the context of the Vesta file system. An interface is provided to help the user to perform parallel i/o. In [34], the pious system is described.
Reference: [19] <author> P. Feautrier. </author> <title> Parametric integer programming. </title> <journal> RAIRO Recherche Operationnelle, </journal> <volume> 22 </volume> <pages> 243-268, </pages> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: Moreover other addressing schemes [33] may be translated into a linear framework. 1.3 Resolution The integer solutions to the previous set of equations must be enumerated to generate the communications. Any polyhedron scanning technique <ref> [19, 3, 5, 39, 17, 27, 13, 30, 28] </ref> can be used. The key issue is the control of the enumeration order to generate tight bounds and to reduce the control overhead. Here the word polyhedron denotes a set of constraints that defines a subspace of integer points.
Reference: [20] <author> S. A. Fineberg. </author> <title> Implementing the NHT-1 Application I/O Benchmark. </title> <journal> ACM SIGARCH Computer Architecture Newsletter, </journal> <volume> 21(5) </volume> <pages> 23-30, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: The Forum did not address parallel i/o since no concensus was found. However MPP machines have parallel i/o capabilities [10] that have to be used efficiently to run real applications <ref> [35, 20, 32] </ref>. For instance, the TMC's CM5 or the Intel's Paragon have so called i/o nodes attached to their fast network. They are used in parallel by applications requiring high i/o throughput. For networks of workstations, files are usually centralized on a server and accessed through NFS.
Reference: [21] <author> H. P. F. Forum. </author> <title> High Performance Fortran Journal of Development. </title> <institution> Rice University, Houston, Texas, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: A possible syntax is shown in Figure 15. It advises the compiler about the distribution of the i/os onto the i/o nodes of the machine for data-parallel i/o. It is in the spirit of the hint approach that was investigated by the hpf Forum <ref> [21] </ref>, that is to give some information to the compiler.
Reference: [22] <author> H. P. F. Forum. </author> <title> High Performance Fortran Language Spec ification. </title> <institution> Rice University, Houston, Texas, </institution> <month> May </month> <year> 1993. </year> <note> Version 1.0. </note>
Reference-contexts: The SVM (Shared Virtual Memory) approach [31, 8] puts the burden on the hardware and operating system, which have to simulate a shared memory. The hpf Forum chose to put it on the language and compiler technology <ref> [22] </ref>, following early academic and commercial experiments [25, 14, 38, 40, 11, 12]. The user is given a way to advise the compiler about data distributions and parallel computations, through a set of directives added to Fortran 90. The Forum did not address parallel i/o since no concensus was found.
Reference: [23] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Application Driven Parallel I/O. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 462-471, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: The technique presented in this paper would help the compilation of the communications involved by such a redistribution (between the i/o nodes and the computation nodes). This whatever the hpf mapping, as part of a dataparallel program to be compiled to a mimd architecture. In <ref> [23] </ref>, the PETSc/Chameleon package is presented. It emphasizes portability and parallel i/o abstraction. [18] suggests to decluster explicitely the files, thus defining logical partitions to be dealt with separately. Their approach is investigated in the context of the Vesta file system.
Reference: [24] <author> A. Geist, A. Beguelin, J. Dongarra, J. Weicheng, R. Manchek, and V. Sunderam. </author> <title> PVM 3 User's Guide and Reference Manual. </title> <institution> Oak Ridge National Laboratory, Oak Ridge, Tennessee, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: They are used in parallel by applications requiring high i/o throughput. For networks of workstations, files are usually centralized on a server and accessed through NFS. Such a system can be seen as a parallel machine using PVM-like libraries <ref> [24] </ref>. These systems are often programmed with a host-node model. To allow nodes to access disk data, one solution is to provide the machine with parallel i/o which ensure i/o operation coherency when many nodes share the same data.
Reference: [25] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compil ing Fortran D for MIMD Distributed-Memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: The SVM (Shared Virtual Memory) approach [31, 8] puts the burden on the hardware and operating system, which have to simulate a shared memory. The hpf Forum chose to put it on the language and compiler technology [22], following early academic and commercial experiments <ref> [25, 14, 38, 40, 11, 12] </ref>. The user is given a way to advise the compiler about data distributions and parallel computations, through a set of directives added to Fortran 90. The Forum did not address parallel i/o since no concensus was found.
Reference: [26] <author> F. Irigoin, P. Jouvelot, and R. Triolet. </author> <title> Semantical in terprocedural parallelization: An overview of the PIPS project. </title> <booktitle> In ACM International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: We have presented a technique based on polyhedron scanning methods to compile parallel i/o communications for distributed memory multicomputers. This technique for the host-node architecture is implemented within hpfc, a prototype hpf compiler developed at CRI. This compiler is part of the PIPS automatic parallelizer <ref> [26] </ref>. From Fortran 77 code and static hpf mapping directives, it generates portable PVM 3-based code. It implements several optimizations such as message vectorization and overlap analysis on top of a runtime resolution compilation. Future work includes experiments, the implementation of advanced optimizations [4] and tests on real world codes.
Reference: [27] <author> W. Kelly and W. Pugh. </author> <title> A framework for unifying re ordering transformations. </title> <type> UMIACS-TR-93 134, </type> <institution> Institute for Advanced Computer Studies, University of Maryland, </institution> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: Moreover other addressing schemes [33] may be translated into a linear framework. 1.3 Resolution The integer solutions to the previous set of equations must be enumerated to generate the communications. Any polyhedron scanning technique <ref> [19, 3, 5, 39, 17, 27, 13, 30, 28] </ref> can be used. The key issue is the control of the enumeration order to generate tight bounds and to reduce the control overhead. Here the word polyhedron denotes a set of constraints that defines a subspace of integer points. <p> In [34], the pious system is described. It is in the spirit of the client-server paradigm, and a database-like protocol insures the coherency of the concurrent accesses. Moreover polyhedron scanning techniques have proven to be efficient and realistic methods for compilers to deal with general code transformations <ref> [39, 27] </ref> as well as distributed code generation [2, 29, 4]. In [2], a dataflow analysis is used to determine the communication sets. These sets are presented in a linear framework, which includes more parametrization and overlaps. The data mapping onto the processors is a superset of the hpf mapping.
Reference: [28] <author> M. Le Fur. </author> <title> Scanning Parametrized Polyhedron using Fourier-Motzkin Elimination. </title> <note> Publication interne 858, IRISA, </note> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Moreover other addressing schemes [33] may be translated into a linear framework. 1.3 Resolution The integer solutions to the previous set of equations must be enumerated to generate the communications. Any polyhedron scanning technique <ref> [19, 3, 5, 39, 17, 27, 13, 30, 28] </ref> can be used. The key issue is the control of the enumeration order to generate tight bounds and to reduce the control overhead. Here the word polyhedron denotes a set of constraints that defines a subspace of integer points.
Reference: [29] <author> M. Le Fur, J.-L. Pazat, and F. Andre. </author> <title> Commutative loop nests distribution. </title> <booktitle> In Workshop on Compilers for Parallel Computers, Delft, </booktitle> <pages> pages 345-350, </pages> <month> Dec. </month> <year> 1993. </year> <note> extended version in IRISA TR 757, Sept. 93. </note>
Reference-contexts: It is in the spirit of the client-server paradigm, and a database-like protocol insures the coherency of the concurrent accesses. Moreover polyhedron scanning techniques have proven to be efficient and realistic methods for compilers to deal with general code transformations [39, 27] as well as distributed code generation <ref> [2, 29, 4] </ref>. In [2], a dataflow analysis is used to determine the communication sets. These sets are presented in a linear framework, which includes more parametrization and overlaps. The data mapping onto the processors is a superset of the hpf mapping. <p> These sets are presented in a linear framework, which includes more parametrization and overlaps. The data mapping onto the processors is a superset of the hpf mapping. The local memory allocation scheme is very simplistic (no address translations) and cyclic distributions are handled through processor virtualization. <ref> [29] </ref> presents similar techniques in the context of the Pandore project, for commutative loop nests. The mapping semantics is a subset of the hpf mapping semantics. The Pandore local memory allocation is based on a page-like technique, managed by the compiler [30].
Reference: [30] <author> H. Le Verge, V. Van Dongen, and D. K. Wilde. </author> <title> Loop nest synthesis unsing the polyhedral library. </title> <note> Publication interne 830, IRISA, </note> <month> May </month> <year> 1994. </year>
Reference-contexts: Moreover other addressing schemes [33] may be translated into a linear framework. 1.3 Resolution The integer solutions to the previous set of equations must be enumerated to generate the communications. Any polyhedron scanning technique <ref> [19, 3, 5, 39, 17, 27, 13, 30, 28] </ref> can be used. The key issue is the control of the enumeration order to generate tight bounds and to reduce the control overhead. Here the word polyhedron denotes a set of constraints that defines a subspace of integer points. <p> The mapping semantics is a subset of the hpf mapping semantics. The Pandore local memory allocation is based on a page-like technique, managed by the compiler <ref> [30] </ref>. Both teh local addressing scheme and the cyclic distributions are integrated in [4] to compile hpf. Moreover equalities are used to improve the scanning loop nests, and temporary allocation issues are discussed.
Reference: [31] <author> K. Li. </author> <title> Shared Virtual Memory on Loosely Coupled Multi processors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> Sept. </month> <year> 1986. </year>
Reference-contexts: Non portable codes are produced at great expense. This problem must be solved to enlarge the potential market for these machines. The SVM (Shared Virtual Memory) approach <ref> [31, 8] </ref> puts the burden on the hardware and operating system, which have to simulate a shared memory. The hpf Forum chose to put it on the language and compiler technology [22], following early academic and commercial experiments [25, 14, 38, 40, 11, 12].
Reference: [32] <author> Z. Lin and S. Zhou. </author> <title> Parallelizing I/O Intensive Appli cations for a Workstation Cluster: a Case Study. </title> <journal> ACM SIGARCH Computer Architecture Newsletter, </journal> <volume> 21(5) </volume> <pages> 15-22, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: The Forum did not address parallel i/o since no concensus was found. However MPP machines have parallel i/o capabilities [10] that have to be used efficiently to run real applications <ref> [35, 20, 32] </ref>. For instance, the TMC's CM5 or the Intel's Paragon have so called i/o nodes attached to their fast network. They are used in parallel by applications requiring high i/o throughput. For networks of workstations, files are usually centralized on a server and accessed through NFS.
Reference: [33] <author> Y. Maheo and J.-L. Pazat. </author> <title> Distributed array management for HPF compilers. </title> <note> Publication interne 787, IRISA, </note> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: It enables the direct enumeration of the elements to exchange without expensive address computations at runtime. However what is really needed is the ability to switch from global to local addresses, so any other addressing scheme would fit into this technique. Moreover other addressing schemes <ref> [33] </ref> may be translated into a linear framework. 1.3 Resolution The integer solutions to the previous set of equations must be enumerated to generate the communications. Any polyhedron scanning technique [19, 3, 5, 39, 17, 27, 13, 30, 28] can be used.
Reference: [34] <author> S. A. Moyer and V. S. Sunderam. </author> <title> PIOUS: A Scalable Parallel I/O System for Distributed Computing Environments. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <pages> pages 71-78, </pages> <year> 1994. </year>
Reference-contexts: It emphasizes portability and parallel i/o abstraction. [18] suggests to decluster explicitely the files, thus defining logical partitions to be dealt with separately. Their approach is investigated in the context of the Vesta file system. An interface is provided to help the user to perform parallel i/o. In <ref> [34] </ref>, the pious system is described. It is in the spirit of the client-server paradigm, and a database-like protocol insures the coherency of the concurrent accesses.
Reference: [35] <author> B. K. Pasquale and G. C. Polyzos. </author> <title> A Static Analysis of I/O Characteristics of Scientific Applications in a Production Workload. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 388-397, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: The Forum did not address parallel i/o since no concensus was found. However MPP machines have parallel i/o capabilities [10] that have to be used efficiently to run real applications <ref> [35, 20, 32] </ref>. For instance, the TMC's CM5 or the Intel's Paragon have so called i/o nodes attached to their fast network. They are used in parallel by applications requiring high i/o throughput. For networks of workstations, files are usually centralized on a server and accessed through NFS.
Reference: [36] <author> R. Triolet. </author> <title> Contribution a la parallelisation automatique de programmes Fortran comportant des appels de procedures. </title> <type> PhD thesis, </type> <institution> Universite Paris VI, </institution> <year> 1984. </year>
Reference-contexts: )-MUST-READ-fff 1 + ff 2 1 + m; 3 ff 1 ; 3 ff 2 g ff 0 ff 0 1 ; ff 0 each array accessed within this statement, a set of linear equalities and inequalities describing the region of accessed elements can be automatically derived from the program <ref> [36, 37, 6, 7] </ref>. A region is given an action and an approximation. The action is READ if the elements are read and WRITE if written. The approximation is MUST if all the elements are actually accessed and MAY if only a subset of these elements will probably be accessed.
Reference: [37] <author> R. Triolet, P. Feautrier, and F. Irigoin. </author> <title> Direct paralleliza tion of call statements. </title> <booktitle> In Proceedings of the ACM Symposium on Compiler Construction, </booktitle> <year> 1986. </year>
Reference-contexts: )-MUST-READ-fff 1 + ff 2 1 + m; 3 ff 1 ; 3 ff 2 g ff 0 ff 0 1 ; ff 0 each array accessed within this statement, a set of linear equalities and inequalities describing the region of accessed elements can be automatically derived from the program <ref> [36, 37, 6, 7] </ref>. A region is given an action and an approximation. The action is READ if the elements are read and WRITE if written. The approximation is MUST if all the elements are actually accessed and MAY if only a subset of these elements will probably be accessed.
Reference: [38] <author> C.-W. Tseng. </author> <title> An Optimising Fortran D Compiler for MIMD Distributed Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, Texas, </institution> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: The SVM (Shared Virtual Memory) approach [31, 8] puts the burden on the hardware and operating system, which have to simulate a shared memory. The hpf Forum chose to put it on the language and compiler technology [22], following early academic and commercial experiments <ref> [25, 14, 38, 40, 11, 12] </ref>. The user is given a way to advise the compiler about data distributions and parallel computations, through a set of directives added to Fortran 90. The Forum did not address parallel i/o since no concensus was found.
Reference: [39] <author> M. J. Wolf and M. S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Moreover other addressing schemes [33] may be translated into a linear framework. 1.3 Resolution The integer solutions to the previous set of equations must be enumerated to generate the communications. Any polyhedron scanning technique <ref> [19, 3, 5, 39, 17, 27, 13, 30, 28] </ref> can be used. The key issue is the control of the enumeration order to generate tight bounds and to reduce the control overhead. Here the word polyhedron denotes a set of constraints that defines a subspace of integer points. <p> In [34], the pious system is described. It is in the spirit of the client-server paradigm, and a database-like protocol insures the coherency of the concurrent accesses. Moreover polyhedron scanning techniques have proven to be efficient and realistic methods for compilers to deal with general code transformations <ref> [39, 27] </ref> as well as distributed code generation [2, 29, 4]. In [2], a dataflow analysis is used to determine the communication sets. These sets are presented in a linear framework, which includes more parametrization and overlaps. The data mapping onto the processors is a superset of the hpf mapping.
Reference: [40] <author> H. Zima and B. M. Chapman. </author> <title> Compiling for distributed memory systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <month> Feb. </month> <year> 1993. </year> <month> 8 </month>
Reference-contexts: The SVM (Shared Virtual Memory) approach [31, 8] puts the burden on the hardware and operating system, which have to simulate a shared memory. The hpf Forum chose to put it on the language and compiler technology [22], following early academic and commercial experiments <ref> [25, 14, 38, 40, 11, 12] </ref>. The user is given a way to advise the compiler about data distributions and parallel computations, through a set of directives added to Fortran 90. The Forum did not address parallel i/o since no concensus was found.
References-found: 40

