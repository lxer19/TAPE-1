URL: http://www.cs.iastate.edu/~balakris/papers/simtec.ps
Refering-URL: http://www.cs.iastate.edu/~balakris/publications.html
Root-URL: http://www.cs.iastate.edu
Title: AN EMPIRICAL COMPARISON OF FLAT-SPOT ELIMINATION TECHNIQUES IN BACK-PROPAGATION NETWORKS  
Author: Rajesh Parekh, Karthik Balakrishnan, Vasant Honavar 
Address: Ames, Iowa 50011  
Affiliation: Department of Computer Science Iowa State University.  
Abstract: Back-Propagation (BP)[Rumelhart et al, 1986] is a popular algorithm employed for training multilayer connectionist learning systems with nonlinear activation function (sigmoid). However, BP is plagued by excruciatingly slow convergence for many applications, and this drawback has been partly attributed to the Flat-Spots problem. Literature defines flat-spots as regions where the derivative of the sigmoid activation function approaches zero, and in these regions the weight changes become negligible, despite the presence of considerable classification error. Thus learning slows down dramatically. Several researchers have addressed this problem posed by flat-spots [Fahlman, 1988, Balakrishnan & Honavar, 1992]. In this paper we present a new way of dealing with the flat-spots in the output layer. This new method uses a Perceptron-like weight-modification strategy to complement BP in the output layer. We also report an empirical evaluation of the comparative performances of these techniques on some data-sets that have been used extensively in bench-marking inductive learning algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: <editor> Touretzky, G.E. Hinton, and T.J. Sejnowski (editors), </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <address> San Mateo, </address> <publisher> CA : Mor-gan Kaufmann Publishers, </publisher> <pages> pp 38-51, </pages> <year> 1988. </year> <editor> [Nilsson, 1965] Nilsson, Nils J. </editor> <title> Learning Machines Foundations of trainable pattern classification systems. </title> <address> New York, </address> <publisher> McGraw-Hill. </publisher> <year> 1965. </year> <note> [Rosenblatt, 1958] Rosenblatt, </note> <author> F. </author> <title> The Perceptron : a probabilistic model for information storage and organization in the brain. </title> <journal> Psychological Review 65, </journal> <pages> pp 386-408, </pages> <year> 1958. </year> <editor> [Rumelhart et al, 1986] Rumelhart, D.E., Hinton, G.E., and Williams, </editor> <title> R.J. Learning Internal Representations by Error Propagation. </title> <booktitle> In Parallel Distributed Processing, </booktitle> <volume> Vol. I, </volume> <editor> Rumelhart D.E., & McClelland, J.L. (editors), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
References-found: 1

