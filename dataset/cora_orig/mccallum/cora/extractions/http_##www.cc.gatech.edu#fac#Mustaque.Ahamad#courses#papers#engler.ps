URL: http://www.cc.gatech.edu/fac/Mustaque.Ahamad/courses/papers/engler.ps
Refering-URL: http://www.cs.gatech.edu/computing/classes/cs6420_97_fall/reading.html
Root-URL: 
Title: Efficient Support for Fine-Grain Parallelism  
Author: Dawson R. Engler Gregory R. Andrews David K. Lowenthal 
Note: TR 93-13a  
Date: Filaments:  
Affiliation: DEPARTMENT OF COMPUTER SCIENCE  
Abstract-found: 0
Intro-found: 1
Reference: [Ande88] <author> Anderson, T. E., Lazowska, E. D., and Levy, H. M. </author> <title> The performance implications of thread management alternatives for shared memory multiprocessors. </title> <journal> IEEE Transactions on Computers 38, </journal> <month> 12 (Dec. </month> <year> 1989), </year> <pages> 1631-1644. </pages>
Reference-contexts: Queue Structures Local ready queues are used for each of the three types of threads (RTC, iterative, and fork/join). This both reduces contention <ref> [Ande88] </ref> and allows scheduling to be locality driven [Mark92]. Three different queues are used, because each is managed somewhat differently, as discussed in Sections 4.3-4.5. This also allows using multiple types of threads in an application. <p> Several researchers have proposed ways to reduce the inefficiencies of standard thread packages. Anderson et al. <ref> [Ande88] </ref> discusses the gain from using local ready queues, and [Ande91] shows how to do user-level scheduling [Ande91]. Schoenberg and Hummel [Humm91] explain how to avoid allocating a stack per thread and context switching in nested parallel for loops, which are a form of run-to-completion threads.
Reference: [Ande91] <author> Anderson, T. E., Bershad, B. N., Lazowska, E. D., and Levy, H. M. </author> <title> Scheduler activations: effective kernel support for the user-level management of parallelism. </title> <booktitle> Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991, </year> <pages> 95-109. </pages>
Reference-contexts: Several researchers have proposed ways to reduce the inefficiencies of standard thread packages. Anderson et al. [Ande88] discusses the gain from using local ready queues, and <ref> [Ande91] </ref> shows how to do user-level scheduling [Ande91]. Schoenberg and Hummel [Humm91] explain how to avoid allocating a stack per thread and context switching in nested parallel for loops, which are a form of run-to-completion threads. <p> Several researchers have proposed ways to reduce the inefficiencies of standard thread packages. Anderson et al. [Ande88] discusses the gain from using local ready queues, and <ref> [Ande91] </ref> shows how to do user-level scheduling [Ande91]. Schoenberg and Hummel [Humm91] explain how to avoid allocating a stack per thread and context switching in nested parallel for loops, which are a form of run-to-completion threads. Markatos et al. [Mark92] presents a thorough study of the tradeoffs between load balancing and locality in shared memory machines.
Reference: [Andr91] <author> Andrews, G. R. </author> <title> Concurrent Programming: </title> <booktitle> Principles and Practice. </booktitle> <publisher> Benjamin/Cummings Publishing Co., </publisher> <address> Redwood City, CA, </address> <year> 1991. </year>
Reference-contexts: The first coarse-grain program statically divides the interval into equal size segments, one per server (processor). Each server process then executes a sequential adaptive quadrature algorithm on its segment. The second coarse-grain program uses the dynamic bag of tasks paradigm <ref> [Carr86, Andr91] </ref>. In particular, there is one central bag of tasks, and each task in the bag specifies one subinterval. Initially, the bag contains the entire interval over which to integrate.
Reference: [Baas88] <author> Baase, Sara. </author> <title> Computer Algorithms: Introduction to Design and Analysis. </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: The results are summarized below; they are discussed in detail in [Lowe93]. Convolution is a method frequently used in engineering and related fields to solve problems such as polynomial multiplication <ref> [Baas88] </ref>. The Mandelbrot set is, of course, a special set of complex numbers that occurs in fractals [Dewd85]. Both adjoint convolution and Mandelbrot can result in load-imbalanced programs, and neither has data locality. The Filaments programs for both create one run-to-completion thread per point.
Reference: [Bers88] <author> Bershad, B. N., Lazowska, E. D., and Levy, H. M. </author> <title> PRESTO: a system for object-oriented parallel programming. </title> <journal> SoftwarePractice and Experience 18, </journal> <month> 8 (August </month> <year> 1988), </year> <pages> 713-732. </pages>
Reference-contexts: Related Work There is a wealth of related research on threads packages, some of which support fine-grain parallelism. The first step towards efficient parallelism was lightweight thread packages such as Threads [Doep87], Presto <ref> [Bers88] </ref>, mSystem [Buhr90], C Threads [Coop90], mC++ [Buhr92], and Sun Lightweight Processes [Stei92]. We will call these standard packages to distinguish them from threads packages that do not have a stack for each thread.
Reference: [Buhr90] <author> Buhr, Peter A. and Stroobosscher, R. A. </author> <title> The uSystem: providing light-weight concurrency on shared memory multiprocessor computers running UNIX. </title> <journal> SoftwarePractice and Experience 20, </journal> <month> 9 (Sept. </month> <year> 1990), </year> <pages> 929-964. </pages>
Reference-contexts: An easy way to get around the last two problems is simply to install Filaments on top of a widely supported thread package (e.g., uSystem <ref> [Buhr90] </ref>) and to use its threads as the Filaments servers. Unfortunately, the only ways we see to solve the code size problem are to revert back to contexts or to resort to runtime code generation. - 16 - 5. <p> Related Work There is a wealth of related research on threads packages, some of which support fine-grain parallelism. The first step towards efficient parallelism was lightweight thread packages such as Threads [Doep87], Presto [Bers88], mSystem <ref> [Buhr90] </ref>, C Threads [Coop90], mC++ [Buhr92], and Sun Lightweight Processes [Stei92]. We will call these standard packages to distinguish them from threads packages that do not have a stack for each thread.
Reference: [Buhr92] <author> Buhr, Peter A., Ditchfield, Glen, Stroobosscher, R. A., and Younger, B. M. </author> <title> uC++: concurrency in the object oriented language C++. </title> <journal> SoftwarePractice and Experience 22, </journal> <month> 2 (Feb. </month> <year> 1992), </year> <pages> 137-172. </pages>
Reference-contexts: Related Work There is a wealth of related research on threads packages, some of which support fine-grain parallelism. The first step towards efficient parallelism was lightweight thread packages such as Threads [Doep87], Presto [Bers88], mSystem [Buhr90], C Threads [Coop90], mC++ <ref> [Buhr92] </ref>, and Sun Lightweight Processes [Stei92]. We will call these standard packages to distinguish them from threads packages that do not have a stack for each thread.
Reference: [Carr86] <author> Carriero, N., Gelernter, D. and Leichter, J. </author> <title> Distributed data structures in Linda. </title> <booktitle> Thirteenth ACM Symp. on Princ. of Programming Languages, </booktitle> <month> January, </month> <year> 1986, </year> <pages> 236-242. </pages>
Reference-contexts: The first coarse-grain program statically divides the interval into equal size segments, one per server (processor). Each server process then executes a sequential adaptive quadrature algorithm on its segment. The second coarse-grain program uses the dynamic bag of tasks paradigm <ref> [Carr86, Andr91] </ref>. In particular, there is one central bag of tasks, and each task in the bag specifies one subinterval. Initially, the bag contains the entire interval over which to integrate.
Reference: [Coop90] <author> Cooper, Eric C. and Draves, Richard P. </author> <title> C Threads. Internal note of the Mach research project, </title> <month> September 11, </month> <year> 1990. </year>
Reference-contexts: Related Work There is a wealth of related research on threads packages, some of which support fine-grain parallelism. The first step towards efficient parallelism was lightweight thread packages such as Threads [Doep87], Presto [Bers88], mSystem [Buhr90], C Threads <ref> [Coop90] </ref>, mC++ [Buhr92], and Sun Lightweight Processes [Stei92]. We will call these standard packages to distinguish them from threads packages that do not have a stack for each thread.
Reference: [Cull93] <author> Culler, David E., et al. </author> <title> TAMa compiler controlled threaded abstract machine. </title> <journal> Journal of Parallel and Distributed Computing 18, </journal> <month> 347-370 </month> <year> (1993). </year>
Reference-contexts: This paper describes a threads package, Filaments, that employs a unique combination of techniques to implement fine-grain parallelism directly, efficiently, and portably on shared-memory multiprocessors. In particular, Filaments synthesizes and extends previous work such as WorkCrews [Vand88], Chores, the Uniform System [Thom88], and TAM <ref> [Cull93] </ref>. The most important technique is stateless threads; i.e., threads do not have a private stack. Thread descriptors are also small, so hundreds of thousands of threads can be supported without exhausting memory. <p> WorkCrews [Vand88] supports fork/join parallelism on small-scale, shared-memory multiprocessors. The package is implemented in Modula2+. WorkCrews introduced the concepts of pruning and of ordering queues to favor larger threads. Filaments has borrowed these - 17 - ideas in its implementation of fork/join threads. TAM <ref> [Cull93] </ref> is a compiler-controlled threaded abstract machine. It evolved from graph-based execution models for dataflow languages and provides a bridge between such models and the control flow models typically employed by standard multiprocessors.
Reference: [Dewd85] <author> Dewdney, A. K. </author> <title> Computer recreations. </title> <publisher> Scientific American (August, </publisher> <year> 1985), </year> <pages> 16-24. </pages>
Reference-contexts: The results are summarized below; they are discussed in detail in [Lowe93]. Convolution is a method frequently used in engineering and related fields to solve problems such as polynomial multiplication [Baas88]. The Mandelbrot set is, of course, a special set of complex numbers that occurs in fractals <ref> [Dewd85] </ref>. Both adjoint convolution and Mandelbrot can result in load-imbalanced programs, and neither has data locality. The Filaments programs for both create one run-to-completion thread per point. These threads do a variable amount of work, so to balance load, we simply schedule each thread on a random server.
Reference: [Doep87] <author> Doeppner, Thomas W. </author> <title> Threads: a system for the support of concurrent programming. </title> <type> TR CS-87-11, </type> <institution> Dept. of Computer Science, Brown University, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: Related Work There is a wealth of related research on threads packages, some of which support fine-grain parallelism. The first step towards efficient parallelism was lightweight thread packages such as Threads <ref> [Doep87] </ref>, Presto [Bers88], mSystem [Buhr90], C Threads [Coop90], mC++ [Buhr92], and Sun Lightweight Processes [Stei92]. We will call these standard packages to distinguish them from threads packages that do not have a stack for each thread.
Reference: [Eage93] <author> Eager, Derek L., and Zahorjan, John. Chores: </author> <title> enhanced run-time support for shared-memory parallel computing. </title> <journal> ACM Trans. on Computer Systems 11, </journal> <month> 1 (Feb. </month> <year> 1993), </year> <pages> 1-32. </pages>
Reference-contexts: For example, Lin and Snyder [Lin90] found a fine-grain implementation of Jacobi iteration using the Sequent Symmetry's parallel programming library to be 8 to 23 times slower than a coarse-grain one, depending on the problem size and number of processors. More recent work, such as Chores <ref> [Eage93] </ref>, has been able to get efficient performance by automatically clustering fine-grain tasks into larger units, but executing fine-grain tasks independently is still 10 to 20 times slower, again for Jacobi iteration on a Sequent Symmetry. <p> The resulting program was around 10% slower than the coarse-grain one. (Of course, no user would ever want to eliminate common subexpressions manually. It just shows that thread packages are not inherently a lot less efficient when executing fine-grain programs, as has been previously claimed, e.g., in <ref> [Lin90, Eage93] </ref>.) For fine-grain programs in which threads are statically assigned to servers, a good compiler might be able to eliminate common subexpressions. In any event, constants can always be saved across thread executions. <p> TAM defines an abstract machine of self-scheduling parallel threads, which is used as an intermediate language that is mapped to existing processors, whereas Filaments defines a portable systems-call library, which is used to specify parallelism in a traditional, imperative way. Chores <ref> [Eage93] </ref> is also similar both to Filaments and to the Uniform System. Chores runs on top of Presto on a Sequent Symmetry. It uses a central ready queue, but servers take jobs in chunks. This amortizes the lock overhead of the central ready queue.
Reference: [Engl94] <author> Engler, Dawson R. </author> <title> The implementation of efficient thread-based parallelism. </title> <note> In preparation. </note>
Reference-contexts: The relative effects of the various implementation techniques used by Filaments are described in detail in <ref> [Engl94] </ref>. The remainder of the paper is organized as follows. The next section describes the Filaments package and how it is used. Section 3 gives performance results for a variety of fine- and coarse-grain programs run on two contemporary multiprocessors: a 4-processor Silicon Graphics Iris and a 14-processor Sequent Symmetry. <p> In the case of programs like Fibonacci, this can give a factor of 2 or more improvement in performance. The specifics of this are discussed in <ref> [Engl94] </ref>. Fork/join programs tend to employ a divide-and-conquer strategy. Consequently, initial threads generate larger tasks, which get broken down into smaller and smaller ones [Vand88]. Fork/join programs also need to do load balancing.
Reference: [Free93] <author> Freeh, Vincent W. </author> <title> A comparison of implicit and explicit parallel programming. </title> <type> TR 93-30, </type> <institution> Dept. of Computer Science, The Univ. of Arizona, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: It is also written entirely in C, without any machine dependent context-switching code. The Filaments package has been used as a runtime library for parallel programs written in C (see Sections 3 and 4) and as the runtime system for a modified compiler for the functional language Sisal <ref> [Free93] </ref>. The relative effects of the various implementation techniques used by Filaments are described in detail in [Engl94]. The remainder of the paper is organized as follows. The next section describes the Filaments package and how it is used.
Reference: [Free94] <author> Freeh, Vincent W., and Lowenthal, David K. </author> <title> Distributed Filaments: efficient fine-grain parallelism on a cluster of workstations. </title> <note> In preparation. - 19 </note> - 
Reference: [Humm91] <author> Hummel, S. F., and Schonberg, E. </author> <title> Low-overhead scheduling of nested parallelism. </title> <journal> IBM Journal of Research and Development 35, </journal> <volume> 5 (September/November 1991), </volume> <pages> 743-765. </pages>
Reference-contexts: Several researchers have proposed ways to reduce the inefficiencies of standard thread packages. Anderson et al. [Ande88] discusses the gain from using local ready queues, and [Ande91] shows how to do user-level scheduling [Ande91]. Schoenberg and Hummel <ref> [Humm91] </ref> explain how to avoid allocating a stack per thread and context switching in nested parallel for loops, which are a form of run-to-completion threads. Markatos et al. [Mark92] presents a thorough study of the tradeoffs between load balancing and locality in shared memory machines.
Reference: [Kepp93] <author> Keppel, David. </author> <title> Tools and techniques for building fast portable threads packages. </title> <type> TR 93-05-06, </type> <institution> Dept. of Computer Science, University of Washington, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Schoenberg and Hummel [Humm91] explain how to avoid allocating a stack per thread and context switching in nested parallel for loops, which are a form of run-to-completion threads. Markatos et al. [Mark92] presents a thorough study of the tradeoffs between load balancing and locality in shared memory machines. Keppel <ref> [Kepp93] </ref> describes a portable threads package that supports efficient barrier synchronization and non-preemptive threads. Threads packages that support finer-grain parallelism include the Uniform System, WorkCrews, TAM, and Chores.
Reference: [Lin90] <author> Lin, Calvin, and Snyder, Lawrence. </author> <title> A comparison of programming models for shared-memory multiprocessors. </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <address> St. Charles, Ill., </address> <year> 1990, </year> <editor> p. </editor> <volume> II: </volume> <pages> 163-170 </pages>
Reference-contexts: Although fine-grain parallelism has attractive attributes as a programming model and a target for code generation, conventional wisdom is that a coarse-grain program will execute much more efficiently than a fine-grain one due to the overhead of process creation, context switching, and synchronization. For example, Lin and Snyder <ref> [Lin90] </ref> found a fine-grain implementation of Jacobi iteration using the Sequent Symmetry's parallel programming library to be 8 to 23 times slower than a coarse-grain one, depending on the problem size and number of processors. <p> The resulting program was around 10% slower than the coarse-grain one. (Of course, no user would ever want to eliminate common subexpressions manually. It just shows that thread packages are not inherently a lot less efficient when executing fine-grain programs, as has been previously claimed, e.g., in <ref> [Lin90, Eage93] </ref>.) For fine-grain programs in which threads are statically assigned to servers, a good compiler might be able to eliminate common subexpressions. In any event, constants can always be saved across thread executions. <p> The Uniform System also employs task generators (a related collection of tasks)and hence has essentially a coarse-grain programming model whereas Filaments directly supports a fine-grain model. Because of these differences, the Uniform System cannot efficiently support thread-per-point decompositions for problems like Jacobi, as Lin <ref> [Lin90] </ref> and others have noted. WorkCrews [Vand88] supports fork/join parallelism on small-scale, shared-memory multiprocessors. The package is implemented in Modula2+. WorkCrews introduced the concepts of pruning and of ordering queues to favor larger threads. Filaments has borrowed these - 17 - ideas in its implementation of fork/join threads.
Reference: [Lowe93] <author> Lowenthal, David K., and Engler, Dawson R. </author> <title> Performance experiments for the Filaments package. </title> <type> TR 93-26, </type> <institution> Dept. of Computer Science, The University of Arizona, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: Section 3.4 summarizes the performance of the other seven applications (for complete results, see <ref> [Lowe93] </ref>). The performance tables give execution times and speedups. The execution times are the averages of three test runs, as reported by getrusage, rounded to the nearest hundredth of a second. These times were normally very consistent, although occasionally the tests were run again when anomalies occurred. <p> This made the third program even slower than the other two coarse-grain programs. 3.4. Other Applications We also tested seven other applications: convolution, Mandelbrot set calculation, fast Fourier transform (FFT), Gaussian elimination, multigrid, Fibonacci numbers, and quicksort. The results are summarized below; they are discussed in detail in <ref> [Lowe93] </ref>. Convolution is a method frequently used in engineering and related fields to solve problems such as polynomial multiplication [Baas88]. The Mandelbrot set is, of course, a special set of complex numbers that occurs in fractals [Dewd85].
Reference: [Mark92] <author> Markatos, E. P. and T. J. LeBlanc. </author> <title> Load balancing vs. locality management in shared-memory multiprocessors. </title> <booktitle> Proc. 1992 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1992, </year> <note> p. I:258-267. </note>
Reference-contexts: Queue Structures Local ready queues are used for each of the three types of threads (RTC, iterative, and fork/join). This both reduces contention [Ande88] and allows scheduling to be locality driven <ref> [Mark92] </ref>. Three different queues are used, because each is managed somewhat differently, as discussed in Sections 4.3-4.5. This also allows using multiple types of threads in an application. Access to the RTC and iterative queues is non-locking in order to avoid locking overhead. <p> Schoenberg and Hummel [Humm91] explain how to avoid allocating a stack per thread and context switching in nested parallel for loops, which are a form of run-to-completion threads. Markatos et al. <ref> [Mark92] </ref> presents a thorough study of the tradeoffs between load balancing and locality in shared memory machines. Keppel [Kepp93] describes a portable threads package that supports efficient barrier synchronization and non-preemptive threads. Threads packages that support finer-grain parallelism include the Uniform System, WorkCrews, TAM, and Chores.
Reference: [Pres91] <author> Press, William H., and Teukolsky, Saul A. </author> <title> Multigrid methods for boundary value problems. </title> <booktitle> Computers in Physics, Sept./October 1991, </booktitle> <pages> 514-519. </pages>
Reference-contexts: This is due to the decreasing amount of work as the computation proceeds. Multigrid is a popular method for approximating solutions to partial differential equations <ref> [Pres91] </ref>. Multigrid methods are based on what is known as a coarse grid correction, which works as follows. First, a relaxation technique is used for a few iterations, then the results are restricted to a coarser grid, which has 1/4 the number of points of the original grid.
Reference: [Stei92] <author> Stein, D. and Shah, D. </author> <title> Implementing lightweight threads. </title> <booktitle> USENIX 1992, </booktitle> <month> June 8-12. </month>
Reference-contexts: Related Work There is a wealth of related research on threads packages, some of which support fine-grain parallelism. The first step towards efficient parallelism was lightweight thread packages such as Threads [Doep87], Presto [Bers88], mSystem [Buhr90], C Threads [Coop90], mC++ [Buhr92], and Sun Lightweight Processes <ref> [Stei92] </ref>. We will call these standard packages to distinguish them from threads packages that do not have a stack for each thread. The goal of standard packages is to provide the user with a natural thread abstraction, and many of the usual concurrent programming primitives; different packages provide different primitives.
Reference: [Thom88] <author> Thomas, Robert H. and Crowther, </author> <title> Will. The Uniform System: an approach to runtime support for large scale shared memory parallel processors. </title> <booktitle> Proceedings of the 1988 Conference on Parallel Processing, p. </booktitle> <pages> 245-254, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: This paper describes a threads package, Filaments, that employs a unique combination of techniques to implement fine-grain parallelism directly, efficiently, and portably on shared-memory multiprocessors. In particular, Filaments synthesizes and extends previous work such as WorkCrews [Vand88], Chores, the Uniform System <ref> [Thom88] </ref>, and TAM [Cull93]. The most important technique is stateless threads; i.e., threads do not have a private stack. Thread descriptors are also small, so hundreds of thousands of threads can be supported without exhausting memory. <p> Keppel [Kepp93] describes a portable threads package that supports efficient barrier synchronization and non-preemptive threads. Threads packages that support finer-grain parallelism include the Uniform System, WorkCrews, TAM, and Chores. The Uniform System <ref> [Thom88] </ref>, built for the BBN Butterfly, has several things in common with Filaments: There are no private stacks per thread, no context switches, and threads are not preemptable. The Uniform System's synchronous mode supports a simple form of barrier threads, and their finalization code is equivalent to our sequential code.
Reference: [Vand88] <author> Vandevoorde, M. and Roberts, E. WorkCrews: </author> <title> an abstraction for controlling parallelism. </title> <journal> Int. Journal of Parallel Programming 17, </journal> <month> 4 (Aug. </month> <year> 1988), </year> <pages> 347-366. </pages>
Reference-contexts: This paper describes a threads package, Filaments, that employs a unique combination of techniques to implement fine-grain parallelism directly, efficiently, and portably on shared-memory multiprocessors. In particular, Filaments synthesizes and extends previous work such as WorkCrews <ref> [Vand88] </ref>, Chores, the Uniform System [Thom88], and TAM [Cull93]. The most important technique is stateless threads; i.e., threads do not have a private stack. Thread descriptors are also small, so hundreds of thousands of threads can be supported without exhausting memory. <p> The specifics of this are discussed in [Engl94]. Fork/join programs tend to employ a divide-and-conquer strategy. Consequently, initial threads generate larger tasks, which get broken down into smaller and smaller ones <ref> [Vand88] </ref>. Fork/join programs also need to do load balancing. In Filaments, newly forked threads are put on the tail of a server's local queue, a server removes work from the front of its local queue, and other servers steal from the front of another server's queue. <p> This works well because the - 15 - largest units of work tend to be at the front of server queues. A very effective optimization is pruning <ref> [Vand88] </ref>. The concept is simple: If we have enough work, there is no need to create more. <p> Because of these differences, the Uniform System cannot efficiently support thread-per-point decompositions for problems like Jacobi, as Lin [Lin90] and others have noted. WorkCrews <ref> [Vand88] </ref> supports fork/join parallelism on small-scale, shared-memory multiprocessors. The package is implemented in Modula2+. WorkCrews introduced the concepts of pruning and of ordering queues to favor larger threads. Filaments has borrowed these - 17 - ideas in its implementation of fork/join threads. TAM [Cull93] is a compiler-controlled threaded abstract machine.
Reference: [Weic84] <author> Weicker, R. P. Dhrystone: </author> <title> a synthetic systems programming benchmark. </title> <journal> Comm. ACM 27, </journal> <month> 10 (October </month> <year> 1984), </year> <pages> 1013-1030. - 20 </pages> - 
Reference-contexts: The first two are of type unsigned long; the last is a void pointer. These arguments typically consist of two array indices and a pointer to shared data. Three arguments are sufficient for most applications (e.g., Weicheck found that, on average, procedures require 2.1 parameters <ref> [Weic84] </ref>). However, we also support variable-argument threads. When such a thread is created, the user provides a format string specifying the argument types as well as the arguments themselves. Supported argument types are integer, pointer, and double.
References-found: 26

