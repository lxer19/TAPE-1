URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/ml92-coverage.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Email: almualh@cs.orst.edu tgd@cs.orst.edu  
Title: On Learning More Concepts  
Author: Hussein Almuallim Thomas G. Dietterich 
Address: Corvallis, OR 97331  
Affiliation: Department of Computer Science Oregon State University  
Abstract: The coverage of a learning algorithm is the number of concepts that can be learned by that algorithm from samples of a given size. This paper asks whether good learning algorithms can be designed by maximizing their coverage. The paper extends a previous upper bound on the coverage of any Boolean concept learning algorithm and describes two algorithms|Multi-Balls and Large-Ball|whose coverage approaches this upper bound. Experimental measurement of the coverage of the ID3 and FRINGE algorithms shows that their coverage is far below this bound. Further analysis of Large-Ball shows that although it learns many concepts, these do not seem to be very interesting concepts. Hence, coverage maximization alone does not appear to yield practically-useful learning algorithms. The paper concludes with a definition of coverage within a bias, which suggests a way that coverage maximization could be applied to strengthen weak preference biases.
Abstract-found: 1
Intro-found: 1
Reference: [Almuallim and Dietterich 91] <author> Almuallim, H. and Di-etterich, T. G. </author> <title> Learning With Many Irrelevant Features. </title> <booktitle> Proceedings of the 9th National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> 547-552, </pages> <year> 1991. </year>
Reference-contexts: In short, the bias of Large-Ball is unlikely to be appropriate in real-world learning situations. This argument shows that coverage analysis alone is not sufficient to find a practically-useful inductive bias. This suggests that we combine coverage analysis with other methods for choosing inductive bias. For example, in <ref> [Almuallim and Dietterich 91] </ref>, we described learning situations in which the MIN-FEATURES bias|the bias that prefers consistent concepts definable over fewer features|is appropriate.
Reference: [Almuallim 91] <author> Almuallim, H. </author> <title> Exploiting Symmetry Properties in the Evaluation of Inductive Learning Algorithms: An Empirical Domain-Independent Comparative Study. </title> <type> Technical Report, </type> <institution> 1991-30-09, Dept. of Computer Science, Oregon State University, Corvallis, </institution> <address> OR 97331-3202. </address>
Reference-contexts: However, the great cost of this method limited Dietterich's experiments to concepts defined over only 3 features. In a separate paper <ref> [Almuallim 91] </ref>, we reported some techniques to reduce the computational costs involved in such experiments by resorting to statistical approximation and by exploiting the symmetry properties of learning algorithms with respect to permutation and negation of features. <p> With these techniques, we can carry out coverage evaluation experiments on the space of concepts defined on up to 5 Boolean features. Due to space limitations, we only give the results of these experiments here. Please see <ref> [Almuallim 91] </ref> or [Almuallim 92] for more details. In these experiments, three learning algorithms were considered: ID3 [Quinlan 86], FRINGE [Pagallo and Haussler 90] and MDT, which is an exhaustive algorithm that finds a decision tree with fewest nodes consistent with the training sample.
Reference: [Almuallim 92] <author> Almuallim, H. </author> <title> Concept Coverage and Its Application to Two Learning Tasks. </title> <type> Ph.D. Thesis. </type> <institution> Department of Computer Science, Oregon State University, Corvallis, Oregon. Forthcoming, </institution> <year> 1992. </year>
Reference-contexts: With these techniques, we can carry out coverage evaluation experiments on the space of concepts defined on up to 5 Boolean features. Due to space limitations, we only give the results of these experiments here. Please see [Almuallim 91] or <ref> [Almuallim 92] </ref> for more details. In these experiments, three learning algorithms were considered: ID3 [Quinlan 86], FRINGE [Pagallo and Haussler 90] and MDT, which is an exhaustive algorithm that finds a decision tree with fewest nodes consistent with the training sample.
Reference: [Blumer et.al. 87] <author> Blumer, A.; Ehrenfeucht, A.; Haus--sler, D.; and Warmuth, M. </author> <title> Learnability and the Vapnik-Chervonenkis Dimension, </title> <type> Technical Report UCSC-CRL-87-20, </type> <institution> Department of Computer and Information Sciences, University of California, Santa Cruz, </institution> <month> Nov. </month> <year> 1987. </year> <note> Also in Journal of ACM, </note> <month> 36(4) </month> <pages> 929-965, </pages> <year> 1990. </year>
Reference-contexts: Nevertheless, it is a well-known fact that learning larger classes of concepts necessarily requires a larger number of training examples <ref> [Blumer et.al. 87] </ref>. Such trade-off between the size of the class of concepts being learned and the required number of training examples dictates how far one can go in attempting to learn larger and larger classes of concepts. <p> A learning algorithm is a mapping from the space of samples to the space of concepts. The output of the algorithm is called an hypothesis. An hypothesis is consistent if it has no disagreement with the training sample. We adopt PAC learning <ref> [Blumer et.al. 87] </ref> as the criterion for successful learning, but we restrict this to learning under the uniform distribution only.
Reference: [Chernoff 52] <author> Chernoff, H. </author> <title> A Measure of Asymptotic Efficiency for Tests of Hypothesis Based on the Sums of Observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23, </volume> <pages> pp. 493-509, </pages> <year> 1952. </year>
Reference: [COLT 88] <institution> Proceedings of the 1988 Workshop on Computational Learning Theory. </institution> <note> Haussler, </note> <editor> D. and Pitt, L., Editors. </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988. </year>
Reference-contexts: 1 INTRODUCTION Research in computational learning theory (e.g., [Valiant 84], [Natarajan 87], <ref> [COLT 88] </ref>-[COLT 91]) has provided many insights into the capabilities and limitations of inductive learning from examples. However, an important shortcoming of most work in this area is that it focuses on learning concepts drawn from prespecified classes of concepts (e.g., linearly separable functions, k-DNF formulae). <p> instance, mentions this goal most explicitly by saying: "One goal of research in machine learning is to identify the largest possible class of con cepts that are learnable from examples." This goal is also declared (although less explicitly) in many papers in the related literature (e.g., [Valiant 84], [Natarajan 87], <ref> [COLT 88] </ref>-[COLT 91]). Nevertheless, it is a well-known fact that learning larger classes of concepts necessarily requires a larger number of training examples [Blumer et.al. 87].
Reference: [COLT 89] <institution> Proceedings of the Second Annual Workshop on Computational Learning Theory. </institution> <note> Rivest, </note> <editor> R., Haussler, D. and Warmuth, M.K., Editors. </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1989. </year>
Reference: [COLT 90] <institution> Proceedings of the Third Annual Workshop on Computational Learning Theory. Fulk, M.A. and Case, </institution> <note> J., </note> <editor> Editors. </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference: [COLT 91] <institution> Proceedings of the Fourth Annual Workshop on Computational Learning Theory. </institution> <note> Valiant, </note> <editor> L.G. and Warmuth, M., Editors. </editor> <publisher> Morgan Kauf-mann Publishers, </publisher> <year> 1991. </year>
Reference: [Dietterich 89] <author> Dietterich, T. G. </author> <title> Limitations on inductive learning. </title> <booktitle> In Proceedings of the Sixth International Conference on Machine Learning, </booktitle> <pages> 124-128. </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: In fact, many of the concept classes studied in computational learning theory have never been supported by any practical justification. Due to these difficulties, the learning algorithms and sample complexity bounds developed in computational learning theory have rarely been of practical value. Recently, an alternative theoretical framework was introduced <ref> [Dietterich 89] </ref>. <p> Can we design a learning algorithm that attains this optimal coverage? 3. What is the coverage of existing learning algo rithms? This paper contributes to answering each of these questions. First, we generalize the upper bound on coverage given in <ref> [Dietterich 89] </ref>. Next, we present two learning algorithms and determine their coverage analytically. The coverage of the first algorithm, Multi-Balls, is shown to be quite close to the upper bound. The coverage of the second algorithm, Large-Ball, turns out to be even better than Multi-Balls in many situations. <p> An example of a concept c is a pair hX; c (X)i where c (X) = 1 if X 2 c and 0 otherwise. The example is called positive in the first case, and negative in the second. As in <ref> [Dietterich 89] </ref>, we assume the uniform distribution over U n . However, all our results can be easily extended to the distributions where the probability is 0 on a subset of U n and uniform on the rest. <p> Except in our experimental work, we assume that examples in a sample are drawn independently (i.e., with replacement), and thus, a sample of size m does not necessarily contain m distinct examples. Note that this is different from <ref> [Dietterich 89] </ref>, where sampling is done without replacement. Assuming that m t 2 n , however, this difference is not significant. The disagreement between a training sample and a concept is the number of examples in the sample that are incorrectly classified by the concept. <p> An upper bound of this type has been proven for the case where the training sample is drawn randomly without replacement <ref> [Dietterich 89] </ref>. In the following, we generalize Dietterich's result and show that the same upper bound also holds for the case where sampling is done with replacement. In addition, we provide a closed form expression for this bound. <p> Proof (sketch): The proof follows the one given in <ref> [Dietterich 89] </ref> except that it uses a probabilistic counting argument instead of a discrete counting argument in order to handle sampling with replacement. 2 It can be shown that for 0 &lt; * &lt; 1 4 and m &lt; 1 above quantity is further bounded above by 2 (1* log 2 <p> Before considering this point further, let us measure the coverage of some popular learning algorithms. 6 COVERAGE OF CURRENT LEARNING ALGORITHMS A straightforward method to measure the coverage of a learning algorithm (as done in <ref> [Dietterich 89] </ref>) is to run it on every possible training sample. However, the great cost of this method limited Dietterich's experiments to concepts defined over only 3 features.
Reference: [Hoeffding 63] <author> Hoeffding, W. </author> <title> Probability Inequalities for Sums of Bounded Random Variables. </title> <journal> Journal of The American Statistical Association, </journal> <volume> 58, 13-3-, </volume> <year> 1963. </year>
Reference: [Holte 91] <author> Holte, R. C. </author> <note> Machine Learning as Error-Correction. Unpublished note, </note> <year> 1991. </year>
Reference-contexts: Acknowledgements Hussein Almuallim was supported by a scholarship from the University of Petroleum and Minerals, Saudi Arabia. The authors gratefully acknowledge the support of the NSF under grant number IRI-86-57316. Thanks to Rob Holte for useful discussions and for providing <ref> [Holte 91] </ref>, and to Prasad Tadepalli for comments on an earlier draft of the paper.
Reference: [Natarajan 87] <author> Natarajan, B.K. </author> <title> On learning Boolean Functions. </title> <booktitle> In Proceedings of the 19th Annual ACM Symposium on Theory of Computing 296-304. </booktitle> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: 1 INTRODUCTION Research in computational learning theory (e.g., [Valiant 84], <ref> [Natarajan 87] </ref>, [COLT 88]-[COLT 91]) has provided many insights into the capabilities and limitations of inductive learning from examples. However, an important shortcoming of most work in this area is that it focuses on learning concepts drawn from prespecified classes of concepts (e.g., linearly separable functions, k-DNF formulae). <p> 87], for instance, mentions this goal most explicitly by saying: "One goal of research in machine learning is to identify the largest possible class of con cepts that are learnable from examples." This goal is also declared (although less explicitly) in many papers in the related literature (e.g., [Valiant 84], <ref> [Natarajan 87] </ref>, [COLT 88]-[COLT 91]). Nevertheless, it is a well-known fact that learning larger classes of concepts necessarily requires a larger number of training examples [Blumer et.al. 87].
Reference: [Pagallo and Haussler 90] <author> Pagallo, G.; and Haussler, D. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5(1) </volume> <pages> 71-100, </pages> <year> 1990. </year>
Reference-contexts: Third, we considerably improve upon Di-etterich's limited experiments for estimating the coverage of existing learning algorithms. We find that the coverage of Large-Ball exceeds the coverage of ID3 [Quinlan 86] and FRINGE <ref> [Pagallo and Haussler 90] </ref> by more than an order of magnitude in most cases. These results are very thought-provoking, because, upon careful analysis, it becomes clear that the Large-Ball algorithm is rather trivial and uninteresting. <p> Due to space limitations, we only give the results of these experiments here. Please see [Almuallim 91] or [Almuallim 92] for more details. In these experiments, three learning algorithms were considered: ID3 [Quinlan 86], FRINGE <ref> [Pagallo and Haussler 90] </ref> and MDT, which is an exhaustive algorithm that finds a decision tree with fewest nodes consistent with the training sample. The coverage of these algorithms was measured for n = 5, * = ffi = 0:1 and m = 8; 10; 12; 14; and 16.
Reference: [Peterson and Weldon 72] <author> W. W. Peterson and E. J. Weldon. </author> <title> Error Correcting Codes, </title> <publisher> The MIT Press. </publisher> <address> p.86, </address> <year> 1972. </year>
Reference-contexts: A proof of this result in addition to a method of constructing the l-bit vectors, can be found in <ref> [Peterson and Weldon 72] </ref>. Using Lemmas 1 and 2, one can search for the appropriate value for d that leads to learning k different *-balls, for k as large as possible.
Reference: [Quinlan 86] <author> Quinlan, J. R. </author> <title> Induction of Decision Trees, </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: The coverage of the second algorithm, Large-Ball, turns out to be even better than Multi-Balls in many situations. Third, we considerably improve upon Di-etterich's limited experiments for estimating the coverage of existing learning algorithms. We find that the coverage of Large-Ball exceeds the coverage of ID3 <ref> [Quinlan 86] </ref> and FRINGE [Pagallo and Haussler 90] by more than an order of magnitude in most cases. These results are very thought-provoking, because, upon careful analysis, it becomes clear that the Large-Ball algorithm is rather trivial and uninteresting. <p> Due to space limitations, we only give the results of these experiments here. Please see [Almuallim 91] or [Almuallim 92] for more details. In these experiments, three learning algorithms were considered: ID3 <ref> [Quinlan 86] </ref>, FRINGE [Pagallo and Haussler 90] and MDT, which is an exhaustive algorithm that finds a decision tree with fewest nodes consistent with the training sample.
Reference: [Rivest 87] <author> Rivest, R. </author> <title> Learning Decision Lists. </title> <journal> Machine Learning, </journal> <volume> 2(3) </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: Of course, no such guarantees are given if the target concept is not in C. This naturally means that one should seek algorithms that learn concept classes that are as large (i.e., less restricted) as possible. Rivest <ref> [Rivest 87] </ref>, for instance, mentions this goal most explicitly by saying: "One goal of research in machine learning is to identify the largest possible class of con cepts that are learnable from examples." This goal is also declared (although less explicitly) in many papers in the related literature (e.g., [Valiant 84],
Reference: [Ross 88] <author> Ross, S. </author> <title> A First Course in Probability. </title> <publisher> Macmillan Publishing Company, </publisher> <address> New York. </address> <publisher> 3rd edition, </publisher> <pages> pp 111, </pages> <year> 1988. </year>
Reference-contexts: The theorem follows by showing that the probability of drawing such a sample is just the left-hand side of the inequality of the theorem. This is a variation of the Coupon collecting problem (e.g., <ref> [Ross 88] </ref> p. 111). 2 It can be shown that a sample of size 1 ln ffi is sufficient to make fi at least 1, and that in general, the value of fi is at least *m ln 1 n ln 2 + ln 2 ln 1 ffi provided that *
Reference: [Valiant 84] <author> L. G. Valiant. </author> <title> A Theory of the Learnable. </title> <journal> Communications of ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: 1 INTRODUCTION Research in computational learning theory (e.g., <ref> [Valiant 84] </ref>, [Natarajan 87], [COLT 88]-[COLT 91]) has provided many insights into the capabilities and limitations of inductive learning from examples. However, an important shortcoming of most work in this area is that it focuses on learning concepts drawn from prespecified classes of concepts (e.g., linearly separable functions, k-DNF formulae). <p> Rivest [Rivest 87], for instance, mentions this goal most explicitly by saying: "One goal of research in machine learning is to identify the largest possible class of con cepts that are learnable from examples." This goal is also declared (although less explicitly) in many papers in the related literature (e.g., <ref> [Valiant 84] </ref>, [Natarajan 87], [COLT 88]-[COLT 91]). Nevertheless, it is a well-known fact that learning larger classes of concepts necessarily requires a larger number of training examples [Blumer et.al. 87].
References-found: 19

