URL: http://www.daimi.aau.dk/~rmunk/PaA5a.ps.gz
Refering-URL: http://www.daimi.aau.dk/~rmunk/publications.html
Root-URL: http://www.daimi.aau.dk
Email: A.Basermann@kfa-juelich.de  Per.Christian.Hansen@uni-c.dk  rmunk@daimi.aau.dk  
Phone: 304,  
Title: Parallel Iterative Methods for Nonsymmetric Large-Scale Problems  
Author: Achim Basermann Martin Bucker Peter Weidner M.Buecker@kfa-juelich.de P.Weidner@kfa-juelich.de Per Christian Hansen Rasmus Munk Larsen 
Date: April 24, 1995  
Address: D-52425 Julich, Germany  Building  DK-2800 Lyngby, Denmark  DK-8000 Arhus C, Denmark  
Affiliation: Zentralinstitut fur Angewandte Mathematik Research Centre Julich GmbH (KFA)  UNI*C Danish Computing Center for Research and Education  Technical University of Denmark  Computer Science Department Institute for Mathematical Sciences, University of Arhus  
Note: APPARC PaA5a Deliverable ESPRIT BRA III Contract 6634  In addition to the support from the EEC ESPRIT Basic Research Action Programme, Project 6634 (APPARC), Achim Basermann was supported by the Graduiertenkolleg `In-formatik und Technik', RWTH Aachen, Germany, and Rasmus M. Larsen was supported by the Faculty of Natural Sciences, University of Arhus.  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. Barrett, M. Berry, T. Chan, J. Demmel, J. Donato, J. Dongarra, V. Eijkhout, R. Pozo, C. Romine & H. van der Vorst, </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993. </year>
Reference-contexts: Other investigations regarding error bounds for both QMR and TFQMR can be found in [16]. In practical applications, iterative solvers for nonsymmetric systems are usually applied with efficient preconditioning [34]. Suitable preconditioners for QMR and TFQMR are described in <ref> [1] </ref>, [19], and [37]. Different methods to precondition QMR or TFQMR are not discussed in this report. Nevertheless, the principle of a possible preconditioning is indicated in Algorithms 3.1-3.3 for the QMR method. <p> Hence the QMR method merely minimizes the right factor of the residual from (3.1). Algorithms 3.1-3.3 illustrate the QMR algorithm with the preconditioner M = M 1 M 2 <ref> [1] </ref> [4]. <p> In the literature, many variants of storage schemes can be found <ref> [1] </ref> [14] [28] [30] [33] [36]. 9 Algorithm 3.5. <p> j ff i1 # 2 ff i1 x (j) = x (j1) + j j d (j) w (2i+1) ae i y (2i+1) = w (2i+1) + fi i y (2i) until x (2i1) or x (2i) converge 10 In this report, we apply the CRS format (compressed row storage) <ref> [1] </ref>. This format is often used in FE programs and is suited for matrices with regular as well as irregular structure. The principle of the scheme is illustrated in Fig. 3.1 for matrix (3.4). <p> If the matrix A is stored row-wise in the CRS format then the matrix A T is given column-wise in the CCS format (Compressed Column Storage) <ref> [1] </ref>. Hence the operation y = As can be computed row-wise with the block ordering from Fig. 3.3, and the column-wise method can be applied for the operation z = A T t (see x3.3.2). Thus the same data structure can be used for both operations.
Reference: [2] <author> A. Basermann, P. Weidner, P. C. Hansen, Tz. Ostromsky & Z. Zlatev, </author> <title> Reordering of sparse matrices for parallel processing, </title> <booktitle> APPARC PaA3a Deliverable, ESPRIT BRA III Contract # 6634. </booktitle>
Reference-contexts: 1 Introduction This report summarizes our work on parallel iterative algorithms for large-scale problems. The work focuses on methods for nonsymmetric matrices, which are generally considered more difficult to treat than symmetric matrices. Thus, the work continues and generalizes the results already presented in Work Packages PaA3a <ref> [2] </ref> and PaA3b [3] for symmetric matrices. 1.1 The Need for Parallel Iterative Methods As the scientific and engineering problems get larger, direct methods that compute a decomposition of the matrix become too costly on today's computer systems. Hence, the need for iterative methods for linear systems of equations increases. <p> Compared with Deliverables PaA3a <ref> [2] </ref> and PaA3b [3], the data distribution model presented here is generalized. In addition to the communication schemes in [2] and [3], a scheme for the column-wise matrix-vector multiplication is described. <p> Compared with Deliverables PaA3a <ref> [2] </ref> and PaA3b [3], the data distribution model presented here is generalized. In addition to the communication schemes in [2] and [3], a scheme for the column-wise matrix-vector multiplication is described.
Reference: [3] <author> A. Basermann, C. Schelthoff & P. Weidner, </author> <title> Optimized kernels for the solution of partial differential equations on distributed memory machines, </title> <booktitle> APPARC PaA3b Deliverable, ESPRIT BRA III Contract # 6634. </booktitle>
Reference-contexts: 1 Introduction This report summarizes our work on parallel iterative algorithms for large-scale problems. The work focuses on methods for nonsymmetric matrices, which are generally considered more difficult to treat than symmetric matrices. Thus, the work continues and generalizes the results already presented in Work Packages PaA3a [2] and PaA3b <ref> [3] </ref> for symmetric matrices. 1.1 The Need for Parallel Iterative Methods As the scientific and engineering problems get larger, direct methods that compute a decomposition of the matrix become too costly on today's computer systems. Hence, the need for iterative methods for linear systems of equations increases. <p> Compared with Deliverables PaA3a [2] and PaA3b <ref> [3] </ref>, the data distribution model presented here is generalized. In addition to the communication schemes in [2] and [3], a scheme for the column-wise matrix-vector multiplication is described. <p> Compared with Deliverables PaA3a [2] and PaA3b <ref> [3] </ref>, the data distribution model presented here is generalized. In addition to the communication schemes in [2] and [3], a scheme for the column-wise matrix-vector multiplication is described.
Reference: [4] <author> M. Bucker, </author> <title> Parallelisierung der QMR-Methode zur Losung linearer Gleichungssysteme, </title> <institution> Bericht Jul-2955, Forschungszentrum Julich GmbH, ZAM, Julich, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Two modern solvers for systems of linear equations with sparse nonsingular nonsymmetric coefficient matrices are the methods QMR and TFQMR, which were developed by Freund and Nachtigal [17] [19]. Both algorithms are subject to current research <ref> [4] </ref> [18] [31]. The use of these methods is attractive for many real world applications; an example is the FE model from environmental science mentioned in x2.1. <p> Compared with other common iterative solvers for nonsymmetric systems, e.g. BiCG (BiConjugate Gradient) [29], BiCGSTAB (BiConjugate Gradient STABilized) [40] | an improved variant of BiCG | and CGS (Conjugate Gradient Squared) [38], QMR and TFQMR show a markedly better and more regular convergence behavior <ref> [4] </ref> [17] [19]. In [19], Freund and Nachtigal present error bounds for the QMR method and prove that the convergence rate of QMR is approximately the same as for GMRES (Generalized Minimal Residual) [35]. Other investigations regarding error bounds for both QMR and TFQMR can be found in [16]. <p> Hence the QMR method merely minimizes the right factor of the residual from (3.1). Algorithms 3.1-3.3 illustrate the QMR algorithm with the preconditioner M = M 1 M 2 [1] <ref> [4] </ref>. <p> (j) = W (H z e (j+1) ) for a z 2 IR : (3.3) The matrix W (j+1) 2 IR nfi (j+1) , the matrix H (j) 2 IR (j+1)fij of rank j and the first unit vector e (j+1) 2 IR j+1 are deduced from the CGS algorithm <ref> [4] </ref> [17]. As for QMR, only the right factor of the TFQMR residuals from (3.3) is minimized: kH (j) z (j) e (j+1) k 2 = min kH (j) z e (j+1) k 2 : Algorithms 3.4 and 3.5 illustrate the TFQMR method [4]. <p> j+1 are deduced from the CGS algorithm <ref> [4] </ref> [17]. As for QMR, only the right factor of the TFQMR residuals from (3.3) is minimized: kH (j) z (j) e (j+1) k 2 = min kH (j) z e (j+1) k 2 : Algorithms 3.4 and 3.5 illustrate the TFQMR method [4]. While Algorithm 3.4 shows the initialization Algorithm 3.5 displays the iteration of TFQMR. The vector z (j) introduced above is implicitly used to compute the iterates x (j) (see Algorithm 3.5). Algorithm 3.4. <p> Therefore no iteration numbers are given. The QMR method even achieved convergence for higher precision of the residual where TFQMR only converged for the matrix WATT1. This indicates a better convergence and error behavior of QMR compared with TFQMR (see also <ref> [4] </ref>). 3.4.2 Performance On the PARAGON, the tests were performed using the PARAGON FORTRAN compiler, version 4.5 [24] [25], and the PARAGON OSF/1 operating system, release 1.2 [26]. The programs were compiled with the optimization switches -O4 -Knoieee.
Reference: [5] <author> J. Christensen-Dalsgaard, P. C. Hansen & M. J. Thompson, </author> <title> GSVD analysis of helio-seismic inversions, Mon. Not. </title> <editor> R. Astr. </editor> <publisher> Soc. </publisher> <month> 264 </month> <year> (1993), </year> <pages> 541-465. </pages>
Reference-contexts: Now g denotes frequency splittings which are related to the amplitudes of the observed frequencies, f is the internal rotation that we seek to compute, and K is the mathematical solar model. See, e.g., <ref> [5] </ref> and [21, x2.1] for more details. <p> This iterative algorithm has several important applications in the treatment of inverse problems. For example, it can be used to compute approximations to the largest singular values and vectors of the matrix A, and these quantities are important analysis tools; see <ref> [5] </ref>.
Reference: [6] <author> A. T. Chronopoulos, </author> <title> s-step iterative methods for (non)symmetric (in)definite linear systems, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 28 (1991), </volume> <pages> 1776-1789. </pages>
Reference-contexts: In addition, QMR seems to be more stable than TFQMR in our case studies. Future research is scheduled to save further synchronization points in both the QMR and the TFQMR iteration by applying similar methods as described in <ref> [6] </ref>, [27], and [39]. Moreover, global synchronization can be reduced by suitable preconditioning. This can additionally result in more stable methods. Preconditioning is subject to further investigations in Deliverable PaA6b.
Reference: [7] <institution> Convex Computing Corporation, Convex Exemplar Programming Guide, </institution> <month> March </month> <year> 1994, </year> <month> DSW-067. </month>
Reference-contexts: Table 4.1 shows the latency for each layer in the memory hierarchy. These are the key figures when designing parallel applications for the SPP. The algorithm is implemented in ANSI C using the optimised BLAS-1 kernels in the CONVEX MLIB library, and the CPS subroutine library 1 <ref> [7] </ref>, [8]. The CPS library provides the basic primitives for thread management, synchronization and mutual exclusion, and the latter is implemented using either cache- or memory based semaphores. The most important operations available from the CPS library are summarized below.
Reference: [8] <author> Convex Computing Corporation, </author> <title> cps parallel compiler support library, Manual pages, </title> <month> March </month> <year> 1994. </year>
Reference-contexts: Table 4.1 shows the latency for each layer in the memory hierarchy. These are the key figures when designing parallel applications for the SPP. The algorithm is implemented in ANSI C using the optimised BLAS-1 kernels in the CONVEX MLIB library, and the CPS subroutine library 1 [7], <ref> [8] </ref>. The CPS library provides the basic primitives for thread management, synchronization and mutual exclusion, and the latter is implemented using either cache- or memory based semaphores. The most important operations available from the CPS library are summarized below.
Reference: [9] <institution> Convex Computing Corporation, Exemplar Architecture, </institution> <month> November </month> <year> 1993, </year> <month> DHW-014. </month>
Reference-contexts: Notice that this cache coherence control does not eliminate the problems which arise if two processors simultaneously try to a store a value to shared variable hence, instructions to create critical regions and synchronize processors are still needed. For further details see <ref> [9] </ref>. The latency associated with loading and storing to memory varies significantly depending on where in the memory hierarchy the associated cache line is currently situated (hence the term Non-Uniform Memory Access). Table 4.1 shows the latency for each layer in the memory hierarchy.
Reference: [10] <author> J. K. Cullum & R. A. Willoughby, </author> <title> Lanczos Algorithms for Large Symmetric Eigenvalue Computations. Vol. I Theory, </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1985. </year>
Reference-contexts: As a result, the matrix k will contain multiple singular values that approximate an isolated singular value of A. However, there are well-known techniques to deal with these results of finite-precision arithmetic, and we use the techniques described by Cullum & Willoughby <ref> [10] </ref>. The Lanczos bidiagonalization algorithm can also be used to compute approximate least squares solutions by projecting A onto the two subspaces spanned by the columns of U k and V k . <p> The algorithm we use is based on the symmetric Lanczos algorithm for computing the eigenvalues and eigenvectors of a symmetric matrix [20, Chapter 9]. It is mathematically equivalent to the bidiagonalization algorithm, and it has the advantage that the tests for spurious singular values proposed by Cullum & Willoughby <ref> [10, Chapter 5] </ref> and used in their Fortran package can be applied directly to the SVD computation. The symmetric Lanczos algorithm takes the following form: Algorithm 4.1. <p> The treatment of the spurious eigenvalues of T 2k is a more delicate problem. We use the identification test suggested by Cullum & Willoughby <ref> [10] </ref> based on a comparison of the eigenvalues of T 2k and those of the (2k 1) fi (2k 1) tridiagonal matrix ^ T 2k obtained from T 2k by deleting the first row and the first column. <p> If an eigenvalue of T 2k is repeated in ^ T 2k then it is considered as a spurious eigenvalue. This test is easy to implement, and we refer to <ref> [10] </ref> for details. The complete algorithm, including the test for spurious eigenvalues, takes the following form. Algorithm 4.2. <p> : T fi 2k = kv (k) k 2 fi 2k fi 2k+1 = ku (k+1) k 2 fi 2k+1 if k mod k check = 0 then Compute positive eigenvalues of T 2k : (k) (k) 2 ; : : : and detect spurious spurious using the strategy from <ref> [10, x4.5] </ref> Compute error estimates of isolated non-spurious eigenvalues e (k) (k) Mark eigenvalues (k) i which are either numerically multiple or non-spurious with e (k) i &lt; ukAk 1 as converged (u = machine precision) ffi = min f i j i is a converged eigenvalue of T 2k g
Reference: [11] <author> J. J. Dongarra & H. A. van der Vorst, </author> <title> Performance of various computers using standard techniques for solving sparse linear equations, </title> <booktitle> SuperComputer 51, </booktitle> <month> IX-5 </month> <year> (1992). </year>
Reference-contexts: In fact, the PCG algorithm is now suggested as a benchmark procedure for sparse linear equations <ref> [11] </ref>. Much less work has been done regarding iterative methods for nonsymmetric problems.
Reference: [12] <author> I. S. Duff, R. G. Grimes & J. G. Lewis, </author> <title> Sparse matrix test problems, </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 15(1), </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference-contexts: The maximum transfer rate is 200 Megabyte/second per channel in both directions. 3.4.1 Numerical Test Cases The tests we present here were carried out with two nonsingular nonsymmetric matrices from the Harwell-Boeing Sparse Matrix Collection <ref> [12] </ref> [13]. The matrix ORSREG1 stems from a three-dimensional discretization problem of oil reservoir simulation; the matrix WATT1 originates in a model from petroleum engineering. Since the Harwell-Boeing Sparse Matrix Collection does not contain large nonsymmetric matrices we use the large symmetric matrices ISR and ICG3D for performance tests.
Reference: [13] <author> I. S. Duff, R. G. Grimes & J. G. Lewis, </author> <title> User's guide for the Harwell-Boeing sparse matrix collection, release I, </title> <type> Technical Report TR/PA/92/86, </type> <institution> CERFACS, </institution> <address> Toulouse Cedex, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The maximum transfer rate is 200 Megabyte/second per channel in both directions. 3.4.1 Numerical Test Cases The tests we present here were carried out with two nonsingular nonsymmetric matrices from the Harwell-Boeing Sparse Matrix Collection [12] <ref> [13] </ref>. The matrix ORSREG1 stems from a three-dimensional discretization problem of oil reservoir simulation; the matrix WATT1 originates in a model from petroleum engineering. Since the Harwell-Boeing Sparse Matrix Collection does not contain large nonsymmetric matrices we use the large symmetric matrices ISR and ICG3D for performance tests.
Reference: [14] <author> P. Fernandes & P. Girdinio, </author> <title> A new storage scheme for an efficient implementation of the sparse matrix-vector product, </title> <booktitle> Parallel Computing, 12 (1989), </booktitle> <pages> 327-333. </pages>
Reference-contexts: In the literature, many variants of storage schemes can be found [1] <ref> [14] </ref> [28] [30] [33] [36]. 9 Algorithm 3.5.
Reference: [15] <author> R. D. Fierro, G. H. Golub, P. C. Hansen & D. P. O'Leary, </author> <title> Regularization by truncated total least squares, </title> <note> Report UNIC-93-14, December 1993 (20 pages); SIAM J. Sci. Comput., to appear. </note>
Reference-contexts: The regularizing effect of this approach is studied in [22]. Other regularized solutions can be obtained by replacing the least squares problem with other problems, e.g., total least squares <ref> [15] </ref>. In this deliverable, we focus on the Lanczos bidiagonalization algorithm for computing the largest singular values and corresponding vectors of A. We note that the bidiago-nalization procedure completely dominates the computing time.
Reference: [16] <author> R. W. Freund, </author> <title> Quasi-Kernel Polynomials and Convergence Results for Quasi-Minimal Residual Iterations, </title> <editor> in D. Braess and L. L. Shumaker, Editors, </editor> <booktitle> Numerical Methods of Approximation Theory, </booktitle> <volume> volume 9, </volume> <pages> pages 77-95, </pages> <publisher> Birkhauser, </publisher> <address> Basel, </address> <year> 1992. </year>
Reference-contexts: In [19], Freund and Nachtigal present error bounds for the QMR method and prove that the convergence rate of QMR is approximately the same as for GMRES (Generalized Minimal Residual) [35]. Other investigations regarding error bounds for both QMR and TFQMR can be found in <ref> [16] </ref>. In practical applications, iterative solvers for nonsymmetric systems are usually applied with efficient preconditioning [34]. Suitable preconditioners for QMR and TFQMR are described in [1], [19], and [37]. Different methods to precondition QMR or TFQMR are not discussed in this report.
Reference: [17] <author> R. W. Freund, </author> <title> A transpose-free quasi-minimal residual algorithm for non-Hermitian linear systems, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14(2) (1992), </volume> <pages> 470-482. 39 </pages>
Reference-contexts: In many cases, the use of the FE method results in largely unstructured systems of equations. Two modern solvers for systems of linear equations with sparse nonsingular nonsymmetric coefficient matrices are the methods QMR and TFQMR, which were developed by Freund and Nachtigal <ref> [17] </ref> [19]. Both algorithms are subject to current research [4] [18] [31]. The use of these methods is attractive for many real world applications; an example is the FE model from environmental science mentioned in x2.1. <p> Compared with other common iterative solvers for nonsymmetric systems, e.g. BiCG (BiConjugate Gradient) [29], BiCGSTAB (BiConjugate Gradient STABilized) [40] | an improved variant of BiCG | and CGS (Conjugate Gradient Squared) [38], QMR and TFQMR show a markedly better and more regular convergence behavior [4] <ref> [17] </ref> [19]. In [19], Freund and Nachtigal present error bounds for the QMR method and prove that the convergence rate of QMR is approximately the same as for GMRES (Generalized Minimal Residual) [35]. Other investigations regarding error bounds for both QMR and TFQMR can be found in [16]. <p> The operations with the preconditioner are not considered here; the analysis above regarding the number of operations per iteration holds for the trivial preconditioner M = I. 3.1.2 The TFQMR Method As well as QMR, the TFQMR algorithm is an iterative polynomial based method developed by Freund <ref> [17] </ref>. Opposite to QMR, the TFQMR iteration is transpose-free, i.e., matrix vector multiplications with the transposed coefficient matrix are not required. TFQMR is based on the CGS algorithm [38]. <p> While CGS merely applies linear combinations of these search directions for computing the current iterate, TFQMR determines particular iterates for either search direction <ref> [17] </ref>. <p> = W (H z e (j+1) ) for a z 2 IR : (3.3) The matrix W (j+1) 2 IR nfi (j+1) , the matrix H (j) 2 IR (j+1)fij of rank j and the first unit vector e (j+1) 2 IR j+1 are deduced from the CGS algorithm [4] <ref> [17] </ref>. As for QMR, only the right factor of the TFQMR residuals from (3.3) is minimized: kH (j) z (j) e (j+1) k 2 = min kH (j) z e (j+1) k 2 : Algorithms 3.4 and 3.5 illustrate the TFQMR method [4].
Reference: [18] <author> R. W. Freund, </author> <title> Block quasi-minimal residual iterations for non-Hermitian linear sys-tems, </title> <editor> in T. Manteuffel and S. McCormick, Editors, </editor> <booktitle> Proceedings of the Colorado Conference on Iterative Methods, volume 2, </booktitle> <institution> University of Colorado, Breckenridge, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Two modern solvers for systems of linear equations with sparse nonsingular nonsymmetric coefficient matrices are the methods QMR and TFQMR, which were developed by Freund and Nachtigal [17] [19]. Both algorithms are subject to current research [4] <ref> [18] </ref> [31]. The use of these methods is attractive for many real world applications; an example is the FE model from environmental science mentioned in x2.1.
Reference: [19] <author> R. W. Freund & N. M. Nachtigal, </author> <title> QMR: A quasi-minimal residual method for non-Hermitian linear systems, </title> <journal> Numer. Math., </journal> <volume> 60 (1991), </volume> <pages> 315-339. </pages>
Reference-contexts: In many cases, the use of the FE method results in largely unstructured systems of equations. Two modern solvers for systems of linear equations with sparse nonsingular nonsymmetric coefficient matrices are the methods QMR and TFQMR, which were developed by Freund and Nachtigal [17] <ref> [19] </ref>. Both algorithms are subject to current research [4] [18] [31]. The use of these methods is attractive for many real world applications; an example is the FE model from environmental science mentioned in x2.1. <p> Compared with other common iterative solvers for nonsymmetric systems, e.g. BiCG (BiConjugate Gradient) [29], BiCGSTAB (BiConjugate Gradient STABilized) [40] | an improved variant of BiCG | and CGS (Conjugate Gradient Squared) [38], QMR and TFQMR show a markedly better and more regular convergence behavior [4] [17] <ref> [19] </ref>. In [19], Freund and Nachtigal present error bounds for the QMR method and prove that the convergence rate of QMR is approximately the same as for GMRES (Generalized Minimal Residual) [35]. Other investigations regarding error bounds for both QMR and TFQMR can be found in [16]. <p> Compared with other common iterative solvers for nonsymmetric systems, e.g. BiCG (BiConjugate Gradient) [29], BiCGSTAB (BiConjugate Gradient STABilized) [40] | an improved variant of BiCG | and CGS (Conjugate Gradient Squared) [38], QMR and TFQMR show a markedly better and more regular convergence behavior [4] [17] <ref> [19] </ref>. In [19], Freund and Nachtigal present error bounds for the QMR method and prove that the convergence rate of QMR is approximately the same as for GMRES (Generalized Minimal Residual) [35]. Other investigations regarding error bounds for both QMR and TFQMR can be found in [16]. <p> Other investigations regarding error bounds for both QMR and TFQMR can be found in [16]. In practical applications, iterative solvers for nonsymmetric systems are usually applied with efficient preconditioning [34]. Suitable preconditioners for QMR and TFQMR are described in [1], <ref> [19] </ref>, and [37]. Different methods to precondition QMR or TFQMR are not discussed in this report. Nevertheless, the principle of a possible preconditioning is indicated in Algorithms 3.1-3.3 for the QMR method. <p> Nevertheless, the principle of a possible preconditioning is indicated in Algorithms 3.1-3.3 for the QMR method. Preconditioning techniques are subject to future research in Work Package PaA6b. 5 3.1.1 The QMR Method The QMR algorithm is an iterative polynomial based method <ref> [19] </ref>. It is deduced from the classical nonsymmetric Lanczos algorithm [20], which is used to generate basis vectors of the Krylov subspaces. The Lanczos method is combined with the quasi-minimal residual principle. <p> Lanczos algorithm without look-ahead generates the matrix Q (i) 2 IR nfii with Q (i) = q (1) q (2) q (i) fl whose columns are the Lanczos vectors q (j) , j = 1; : : : ; i, and the upper Hessenberg matrix H (i) 2 IR (i+1)fii <ref> [19] </ref>.
Reference: [20] <author> G. H. Golub & C. F. Van Loan, </author> <title> Matrix Computations, 2. </title> <editor> Ed., </editor> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: Preconditioning techniques are subject to future research in Work Package PaA6b. 5 3.1.1 The QMR Method The QMR algorithm is an iterative polynomial based method [19]. It is deduced from the classical nonsymmetric Lanczos algorithm <ref> [20] </ref>, which is used to generate basis vectors of the Krylov subspaces. The Lanczos method is combined with the quasi-minimal residual principle. <p> : :; v k ); we obtain a (k + 1) fi k bidiagonal matrix B k satisfying A V k = U k B k : (4.1) Now let the singular value decomposition (SVD) of B k be denoted by B k = P k k Q T see <ref> [20] </ref> for more details about the SVD. Then the large singular values of B k , given by k , are approximations to the large singular values of A. <p> The algorithm we use is based on the symmetric Lanczos algorithm for computing the eigenvalues and eigenvectors of a symmetric matrix <ref> [20, Chapter 9] </ref>. It is mathematically equivalent to the bidiagonalization algorithm, and it has the advantage that the tests for spurious singular values proposed by Cullum & Willoughby [10, Chapter 5] and used in their Fortran package can be applied directly to the SVD computation.
Reference: [21] <author> M. Hanke & P. C. Hansen, </author> <title> Regularization methods for large-scale problems, </title> <institution> Surv. Math. Ind. </institution> <month> 3 </month> <year> (1993), </year> <pages> 253-315. </pages>
Reference-contexts: In these applications, the kernel K is the mathematical model for the de-blurring of the image, and if the blurring is invariant then the discretization of (2.1) leads to very structured matrices, such as block Toeplitz matrices with Toeplitz blocks. See, e.g., <ref> [21, x2.3] </ref> for more details. Another application, which UNI*C has been involved with for some time, is helioseis-mology. Here, the aim is to determine the internal rotation of the sun from observations of the oscillations of the surface of the sun. <p> Now g denotes frequency splittings which are related to the amplitudes of the observed frequencies, f is the internal rotation that we seek to compute, and K is the mathematical solar model. See, e.g., [5] and <ref> [21, x2.1] </ref> for more details. <p> Alternatively, one can limit the size of x as measured by a norm or seminorm of x. We refer to <ref> [21] </ref> for a general treatment of various regularization methods for large-scale problems. 4 3 Quasi-Minimal Residual Methods For the analysis and solution of discretized ordinary or partial differential equations it is necessary to solve systems of equations with coefficient matrices of different sparsity patterns that depend on the discretization method. <p> Moreover, regularized solutions to Ax = b can be constructed from the approximate singular values and vectors, as discussed by Hanke & Hansen in <ref> [21, x7] </ref>. 4.1 Lanczos Bidiagonalization and LSQR The Lanczos bidiagonalization algorithm with starting vector u 1 = b=kbk 2 produces two sets of orthogonal Lanczos vectors fu i g and fv i g such that, after k steps and with U k = (u 1 ; u 2 ; : : <p> This choice ensure that u (1) has the desired SVD components, due to the fact that b satisfies a so-called "discrete Picard condition" <ref> [21] </ref>. 4.3 The Convex SPP Exemplar The Lanczos bidiagonalization has been implemented on the Convex SPP Exemplar installation in Arhus. Since this computer is based on a relatively new type of architecture, we shall in this section give an overview of the SPP architecture.
Reference: [22] <author> P. C. Hansen, </author> <title> Experience with regularizing CG iterations, </title> <note> Report UNIC-94-02, May 1994 (19 pages); submitted to BIT. </note>
Reference-contexts: This is, in fact, the LSQR algorithm by Paige & Saunders [32], and it is mathematically equivalent to applying the CG algorithm to the system A T T b. The regularizing effect of this approach is studied in <ref> [22] </ref>. Other regularized solutions can be obtained by replacing the least squares problem with other problems, e.g., total least squares [15]. In this deliverable, we focus on the Lanczos bidiagonalization algorithm for computing the largest singular values and corresponding vectors of A.
Reference: [23] <institution> Institut fur Statik und Dynamik der Luft- und Raumfahrtkonstruktionen der Univer-sitat Stuttgart, SMART, Benutzerhandbucher, </institution> <month> ISD-Berichte, </month> <pages> 1976-1992. </pages>
Reference-contexts: The matrix ISR comes from a FE model of structural mechanics in which stresses in materials induced by thermal expansion are calculated; the FE program SMART is applied in this model <ref> [23] </ref>. The matrix ICG3D stems from the water flow model of the environmental science application described in x2.1. The transport model results in nonsymmetric systems of linear equations, but test cases from simulation runs were not available for these investigations.
Reference: [24] <institution> Intel Supercomputer Systems Division, Beaverton, Oregon, </institution> <note> Paragon Fortran compiler user's guide, </note> <month> March </month> <year> 1994, </year> <title> Order Number: </title> <type> 312491-002. </type>
Reference-contexts: This indicates a better convergence and error behavior of QMR compared with TFQMR (see also [4]). 3.4.2 Performance On the PARAGON, the tests were performed using the PARAGON FORTRAN compiler, version 4.5 <ref> [24] </ref> [25], and the PARAGON OSF/1 operating system, release 1.2 [26]. The programs were compiled with the optimization switches -O4 -Knoieee. On the left, Fig. 3.5 illustrates execution times per iteration on 64 processors for the row-wise and the column-wise matrix-vector multiplication described in x3.3.2 (matrices ICG3D and ISR).
Reference: [25] <institution> Intel Supercomputer Systems Division, Beaverton, Oregon, </institution> <note> Paragon Fortran compiler, release 4.5, software product release notes, </note> <month> March </month> <year> 1994, </year> <title> Order Number: </title> <type> 313012-001. </type>
Reference-contexts: This indicates a better convergence and error behavior of QMR compared with TFQMR (see also [4]). 3.4.2 Performance On the PARAGON, the tests were performed using the PARAGON FORTRAN compiler, version 4.5 [24] <ref> [25] </ref>, and the PARAGON OSF/1 operating system, release 1.2 [26]. The programs were compiled with the optimization switches -O4 -Knoieee. On the left, Fig. 3.5 illustrates execution times per iteration on 64 processors for the row-wise and the column-wise matrix-vector multiplication described in x3.3.2 (matrices ICG3D and ISR).
Reference: [26] <institution> Intel Supercomputer Systems Division, Beaverton, Oregon, </institution> <note> Paragon user's guide, </note> <month> June </month> <year> 1994, </year> <title> Order Number: </title> <type> 312489-003. </type>
Reference-contexts: This indicates a better convergence and error behavior of QMR compared with TFQMR (see also [4]). 3.4.2 Performance On the PARAGON, the tests were performed using the PARAGON FORTRAN compiler, version 4.5 [24] [25], and the PARAGON OSF/1 operating system, release 1.2 <ref> [26] </ref>. The programs were compiled with the optimization switches -O4 -Knoieee. On the left, Fig. 3.5 illustrates execution times per iteration on 64 processors for the row-wise and the column-wise matrix-vector multiplication described in x3.3.2 (matrices ICG3D and ISR).
Reference: [27] <author> S. K. Kim & A. T. Chronopoulos, </author> <title> An efficient nonsymmetric Lanczos on parallel vector computers, </title> <journal> J. Comput. Appl. Math., </journal> <volume> 42 (1992), </volume> <pages> 357-374. </pages>
Reference-contexts: In addition, QMR seems to be more stable than TFQMR in our case studies. Future research is scheduled to save further synchronization points in both the QMR and the TFQMR iteration by applying similar methods as described in [6], <ref> [27] </ref>, and [39]. Moreover, global synchronization can be reduced by suitable preconditioning. This can additionally result in more stable methods. Preconditioning is subject to further investigations in Deliverable PaA6b. The parallel implementation of the Lanczos bidiagonalization algorithm also performs satisfactorily, especially when the implementation takes into account the cache misses.
Reference: [28] <author> C. P. Kruskal, L. Rudolph & M. Snir, </author> <title> Techniques for parallel manipulation of sparse matrices, </title> <booktitle> Theoretical Computer Science, 64 (1989), </booktitle> <pages> 135-157. </pages>
Reference-contexts: In the literature, many variants of storage schemes can be found [1] [14] <ref> [28] </ref> [30] [33] [36]. 9 Algorithm 3.5.
Reference: [29] <author> C. </author> <title> Lanczos, Solutions of systems of linear equations by minimized iterations, </title> <journal> Journal of Research of the National Bureau of Standards, </journal> <volume> 49, </volume> <year> (1952), </year> <pages> 33-53. </pages>
Reference-contexts: Instead of minimizing the residual norm in each iteration, only a factor of the residual is minimized. This results in a minimization problem that is much easier to solve and update from step to step. Compared with other common iterative solvers for nonsymmetric systems, e.g. BiCG (BiConjugate Gradient) <ref> [29] </ref>, BiCGSTAB (BiConjugate Gradient STABilized) [40] | an improved variant of BiCG | and CGS (Conjugate Gradient Squared) [38], QMR and TFQMR show a markedly better and more regular convergence behavior [4] [17] [19]. <p> The data distribution and the communication schemes we present here do not require any knowledge about a specific discretization mesh; the schemes are determined automatically by the analysis of the indices of the non-zero matrix elements. 3.3.3 Coupling In each iteration of the methods QMR (see x3.1.1) and BiCG <ref> [29] </ref>, two matrix-vector multiplications of the form y = As und z = A T t are performed that are independent of each other.
Reference: [30] <author> O. A. McBryan & E. F. Van de Velde, </author> <title> Matrix and vector operations on hypercube parallel processors, </title> <booktitle> Parallel Computing, 5 (1987), </booktitle> <pages> 117-125. </pages>
Reference-contexts: In the literature, many variants of storage schemes can be found [1] [14] [28] <ref> [30] </ref> [33] [36]. 9 Algorithm 3.5.
Reference: [31] <author> N. M. Nachtigal, </author> <title> A look-ahead variant of TFQMR, </title> <editor> in T. Manteuffel and S. McCormick, Editors, </editor> <booktitle> Proceedings of the Colorado Conference on Iterative Methods, volume 2, </booktitle> <institution> University of Colorado, Breckenridge, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Two modern solvers for systems of linear equations with sparse nonsingular nonsymmetric coefficient matrices are the methods QMR and TFQMR, which were developed by Freund and Nachtigal [17] [19]. Both algorithms are subject to current research [4] [18] <ref> [31] </ref>. The use of these methods is attractive for many real world applications; an example is the FE model from environmental science mentioned in x2.1.
Reference: [32] <author> C. C. Paige & M. A. Saunders, </author> <title> LSQR: an algorithm for sparse linear equations and sparse least squares, </title> <journal> ACM Trans. Math. </journal> <volume> Software 8 (1982), </volume> <pages> 43-71. </pages>
Reference-contexts: The iteration vector is therefore x (k) = V k B y k U T k b, where B y k is the pseudoinverse of B k . This is, in fact, the LSQR algorithm by Paige & Saunders <ref> [32] </ref>, and it is mathematically equivalent to applying the CG algorithm to the system A T T b. The regularizing effect of this approach is studied in [22]. Other regularized solutions can be obtained by replacing the least squares problem with other problems, e.g., total least squares [15].
Reference: [33] <author> S. Pissanetsky, </author> <title> Sparse Matrix Technology, </title> <publisher> Academic Press, </publisher> <address> London Orlando, </address> <year> 1984. </year>
Reference-contexts: In the literature, many variants of storage schemes can be found [1] [14] [28] [30] <ref> [33] </ref> [36]. 9 Algorithm 3.5.
Reference: [34] <author> Y. Saad, </author> <title> Preconditioning techniques for indefinite and nonsymmetric linear systems, </title> <journal> J. Comput. Appl. Math., </journal> <volume> 24 (1988), </volume> <pages> 89-105. </pages>
Reference-contexts: Other investigations regarding error bounds for both QMR and TFQMR can be found in [16]. In practical applications, iterative solvers for nonsymmetric systems are usually applied with efficient preconditioning <ref> [34] </ref>. Suitable preconditioners for QMR and TFQMR are described in [1], [19], and [37]. Different methods to precondition QMR or TFQMR are not discussed in this report. Nevertheless, the principle of a possible preconditioning is indicated in Algorithms 3.1-3.3 for the QMR method.
Reference: [35] <author> Y. Saad & M. H. Schulz, </author> <title> GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 7 (1986), </volume> <pages> 856-869. 40 </pages>
Reference-contexts: In [19], Freund and Nachtigal present error bounds for the QMR method and prove that the convergence rate of QMR is approximately the same as for GMRES (Generalized Minimal Residual) <ref> [35] </ref>. Other investigations regarding error bounds for both QMR and TFQMR can be found in [16]. In practical applications, iterative solvers for nonsymmetric systems are usually applied with efficient preconditioning [34]. Suitable preconditioners for QMR and TFQMR are described in [1], [19], and [37].
Reference: [36] <editor> U. Schendel, Sparse-Matrizen, R. </editor> <publisher> Oldenbourg Verlag, </publisher> <address> Munchen Wien, 1. Auflage, </address> <year> 1977. </year>
Reference-contexts: In the literature, many variants of storage schemes can be found [1] [14] [28] [30] [33] <ref> [36] </ref>. 9 Algorithm 3.5.
Reference: [37] <author> J. N. Shadid & R. S. Tuminaro, </author> <title> A comparison of preconditioned nonsymmetric Krylov methods on a large-scale MIMD machine, </title> <journal> SIAM J. Sci _ Comput., </journal> <volume> 15(2) (1994), </volume> <pages> 440-459. </pages>
Reference-contexts: Other investigations regarding error bounds for both QMR and TFQMR can be found in [16]. In practical applications, iterative solvers for nonsymmetric systems are usually applied with efficient preconditioning [34]. Suitable preconditioners for QMR and TFQMR are described in [1], [19], and <ref> [37] </ref>. Different methods to precondition QMR or TFQMR are not discussed in this report. Nevertheless, the principle of a possible preconditioning is indicated in Algorithms 3.1-3.3 for the QMR method.
Reference: [38] <author> P. Sonneveld, </author> <title> CGS, a fast Lanczos-type solver for nonsymmetric linear systems, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 10(1) (1989), </volume> <pages> 36-52. </pages>
Reference-contexts: This results in a minimization problem that is much easier to solve and update from step to step. Compared with other common iterative solvers for nonsymmetric systems, e.g. BiCG (BiConjugate Gradient) [29], BiCGSTAB (BiConjugate Gradient STABilized) [40] | an improved variant of BiCG | and CGS (Conjugate Gradient Squared) <ref> [38] </ref>, QMR and TFQMR show a markedly better and more regular convergence behavior [4] [17] [19]. In [19], Freund and Nachtigal present error bounds for the QMR method and prove that the convergence rate of QMR is approximately the same as for GMRES (Generalized Minimal Residual) [35]. <p> Opposite to QMR, the TFQMR iteration is transpose-free, i.e., matrix vector multiplications with the transposed coefficient matrix are not required. TFQMR is based on the CGS algorithm <ref> [38] </ref>. The CGS iterate is given by x (i) = x (i1) + ffi i1 (t (i1) + h (i) ) Algorithm 3.1.
Reference: [39] <author> C. D. Swanson & A. T. Chronopoulos, </author> <title> Parallel iterative s-step methods for unsymmetric linear systems, </title> <institution> University of Minnesota Supercomputer Institute Research Report UMSI 94/105, Minneapolis, Minnesota, </institution> <year> 1994. </year>
Reference-contexts: In addition, QMR seems to be more stable than TFQMR in our case studies. Future research is scheduled to save further synchronization points in both the QMR and the TFQMR iteration by applying similar methods as described in [6], [27], and <ref> [39] </ref>. Moreover, global synchronization can be reduced by suitable preconditioning. This can additionally result in more stable methods. Preconditioning is subject to further investigations in Deliverable PaA6b. The parallel implementation of the Lanczos bidiagonalization algorithm also performs satisfactorily, especially when the implementation takes into account the cache misses.
Reference: [40] <author> H. A. van der Vorst, </author> <title> Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems, </title> <journal> Siam J. Sci. Stat. Comput., </journal> <volume> 13(2) (1992), </volume> <pages> 631-644. </pages>
Reference-contexts: This results in a minimization problem that is much easier to solve and update from step to step. Compared with other common iterative solvers for nonsymmetric systems, e.g. BiCG (BiConjugate Gradient) [29], BiCGSTAB (BiConjugate Gradient STABilized) <ref> [40] </ref> | an improved variant of BiCG | and CGS (Conjugate Gradient Squared) [38], QMR and TFQMR show a markedly better and more regular convergence behavior [4] [17] [19].
Reference: [41] <author> H. Vereecken, G. Lindenmayr, A. Kuhr, D. H. Welte & A. Basermann, </author> <title> Numerical modeling of field scale transport in heterogeneous variably saturated porous media, KFA/ICG-4 Internal Report No. </title> <type> 500393, </type> <institution> Julich, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: An important subject of environmental science is the simulation of soil and air pollution. In a project at the Research Centre Julich GmbH, the behavior of pollutants in geological systems is examined by an FE model <ref> [41] </ref> [42] [43]. Pollutants, for example from fertilization, come to the soil surface and are swept into the underground by precipitation.
Reference: [42] <author> H. Vereecken, G. Lindenmayr, O. Neuendorf, U. Doring, & R. Seidemann, </author> <title> TRACE: a mathematical model for reactive transport in 3D variably saturated porous media, KFA/ICG-4 Internal Report No. </title> <type> 501494, </type> <institution> Julich, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: An important subject of environmental science is the simulation of soil and air pollution. In a project at the Research Centre Julich GmbH, the behavior of pollutants in geological systems is examined by an FE model [41] <ref> [42] </ref> [43]. Pollutants, for example from fertilization, come to the soil surface and are swept into the underground by precipitation.
Reference: [43] <author> G. T. Yeh, 3DFEMWATER: </author> <title> A three-dimensional finite element model of water flow through saturated-unsaturated media, </title> <institution> ORNL-6386, Oak Ridge National Laboratory, </institution> <year> 1987. </year>
Reference-contexts: An important subject of environmental science is the simulation of soil and air pollution. In a project at the Research Centre Julich GmbH, the behavior of pollutants in geological systems is examined by an FE model [41] [42] <ref> [43] </ref>. Pollutants, for example from fertilization, come to the soil surface and are swept into the underground by precipitation.
References-found: 43

