URL: http://charm.cs.uiuc.edu/manuals/d-memo-2.ps.gz
Refering-URL: http://charm.cs.uiuc.edu/manuals/
Root-URL: http://www.cs.uiuc.edu
Keyword: Index Terms Parallel and distributed processing. Languages, Heterogeneous computing. Directories of unordered queues. Dynamic Data Migration. Portability.  
Affiliation: Argonne National Laboratory High-Performance Computing Research Facility  
Note: *Research in coorperation with  
Abstract: Heterogeneously distributed and parallel computing environments are highly dependent on hardware, data migration, and protocols. The result is significant difficulty in software reuse, portability across platforms, and an increased overall development effort. The appearance of a shared directory of unordered queues can be provided by integrating heterogeneous computers transparently. This integration provides a conducive environment for parallel and distributed application development, by abstracting the issues of hardware and communication. Object oriented technology is exploited to provide this seamless environment. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Benguelin, et al., </author> <title> Solving Computational Grand Challenges Using a Network of Heterogeneous Supercomputers, </title> <booktitle> Proc. fifth SIAM conf. on parallel proc. for scientific computing, </booktitle> <address> Houston, TX, </address> <month> Mar., 91. </month>
Reference-contexts: 1 Introduction Heterogeneously distributed parallel computing allows applications to execute over an interconnected cluster instead of a single supercomputer or Massively Parallel Processing (MPP) machine. This provides the exibility of using under-utilized high-performance workstations, single MPP machines, multiprocessor machines, and/or vector supercomputers to accomplish a task <ref> [1] </ref>. The purpose is to exploit this under-utilization by combining the power of heterogeneous high-performance machines into a unified virtual parallel machine. The last several years have seen a widespread acceptance of this alternative approach to parallel processing. <p> The last several years have seen a widespread acceptance of this alternative approach to parallel processing. This does not imply that a cluster of workstations can replace a $30M supercomputer. However, a cluster of high-performance workstations using the proper granularity can be used with some impressive results <ref> [1] </ref>. The key to this ability is the scalability of several hundred million dollars worth of networked computers, described by Gordon Bell as the Ultracomputer: a Scalable computer, by creating a teraops worth of networked computers for an application [2]. <p> This is a typical example of a SPMD application, where the FOLDER_NAME key; SYMBOL a; ... a = Memo.Symbol (); key.S = a; key.X <ref> [1] </ref> = j; Message record_obj *ptr; ... ptr = (record_obj *) Memo.Get ( record ); ... /* Operate on record ptr*/ Memo.Put ( record, ptr ); FOLDER_NAME record; FOLDER_NAME lock; char8 *token; ... token = (char8 *) Memo.Get ( lock ); ... /* Perform critical section */ Memo.Put ( lock, token <p> For example, the producer can write memos into folders as: Message operation *op; ... FOLDER_NAME future, job_jar; Memo.PutDelayed ( future, job_jar, op ); FOLDER_NAME dest; #define QUEUE ... ... int outposition = 0 ; dest.S = QUEUE; dest.X <ref> [1] </ref> = dest.X [2] = 0; ... while ( 1 ) - ...
Reference: [2] <author> G. Bell, </author> <title> Ultracomputers: A Teraop Before its Time, </title> <journal> Comm. of the ACM, </journal> <volume> Vol. 35, No. 8, </volume> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: The key to this ability is the scalability of several hundred million dollars worth of networked computers, described by Gordon Bell as the Ultracomputer: a Scalable computer, by creating a teraops worth of networked computers for an application <ref> [2] </ref>. The irony to this enormous parallel computation through scalable clustering is that software technology has taken second seat to the hardware movement which has taken place over the last two decades [3]. <p> For example, the producer can write memos into folders as: Message operation *op; ... FOLDER_NAME future, job_jar; Memo.PutDelayed ( future, job_jar, op ); FOLDER_NAME dest; #define QUEUE ... ... int outposition = 0 ; dest.S = QUEUE; dest.X [1] = dest.X <ref> [2] </ref> = 0; ... while ( 1 ) - ... <p> Sample code using a barrier: ... while ( 1 ) - mine.X [0] = inposition++; ptr = (message *) Memo.Get ( mine ); - ... Consume item M ... FOLDER_NAME barrier <ref> [2] </ref> = int which = 0; int16 Tcount = NUM_TO_SYNCH; ... /* Initialize barrier */ Memo.Put (Barrier [0],&Tcount); ... --BARR0,0,0,0-, -BARR1,0,0,0--; while ( TRUE ) - int16 *count; ...
Reference: [3] <author> W. OConnell, </author> <title> A Generic Modelling Framework for Building High-Performance Environments, </title> <type> Ph.D. Thesis, </type> <institution> IIT-HPLS-94-4, Illinois Inst. of Tech., </institution> <year> 1994. </year>
Reference-contexts: The irony to this enormous parallel computation through scalable clustering is that software technology has taken second seat to the hardware movement which has taken place over the last two decades <ref> [3] </ref>. It has become extremely difficult to design, write and maintain parallel applications which are distributed over heterogeneous machines, especially for the general computer scientist. <p> The scientist must not only focus on the problem space to be solved but is burdened with differences in hardware and operating system interfaces over multiple machines <ref> [3] </ref>. To further separate the real problem space from the programmer, transport protocols must be differentiated. For example, a typical application may be spread over several machines using multiple transport protocols, such as an IBM SP-1 MPP and a dedicated alpha cluster interconnected with a HiPPI switch. <p> Christopher Illinois Institute of Technology 10 West Federal Street Chicago, IL 60616 tc@iitmax.acc.iit.edu Proceedings of the IASTED Intl Conference on Parallel and Distributed Computing and Systems, Wash. D.C., Oct., 1994 [6] to study the problem space of heterogeneously distributed parallel computing for the general programmer <ref> [3] </ref>. Our emphasis is on: The need for an easy to use application programming interface (API). The need to model the heterogeneous platforms and protocols, including dynamic data modeling. <p> This creates a sense of artificial complexity in programs. This notion of virtual shared memory (or a shared directory) is not new, for example distributed shared memory operating systems and file systems, such as the Andrew File System <ref> [3] </ref>, have been around for some time. A notable parallel processing package supporting virtual shared memory is Linda [6]. The Linda parallel programming system attempts to provide a more natural parallel programming environment through a shared associative memory, referred to as tuple space.
Reference: [4] <author> W. OConnell, G. Thiruvathukal, T. Christopher, </author> <title> A Generic Modelling Framework for Building Parallel and Distributed Programming Environments, </title> <booktitle> Proc. 10th Intl Conf. on Adv. </booktitle> <institution> Sci. and Technology, Naperville, IL, </institution> <month> Mar. 26, </month> <year> 1994, </year> <note> FTP access: glen ellyn.iit.edu;/pub/research/parallel/papers. </note>
Reference-contexts: The communication method is provided through a shared directory of unordered queues (or a shared table of bags). A secondary method is also available by accessing methods provided directly within the implementation of the Generic Modelling Framework <ref> [4] </ref>. This implementation serves as the kernel of the D-Memo system. Such communication methods offered by this model are direct process to process communication. Since we are primarily concerned with concurrent programming with the shared directory, our discussion will focus on it. <p> The programmer creates a message similarly to structures in C. A translator is used to convert each message reference into an object a . Each object then becomes what is known as a complex transferable which is constructed using other complex or scalar transferables <ref> [4] </ref>. In this case the int16 is a scalar transferable supplied by the system, which is a 16-bit integer. <p> By eliminating platform specific issues, the task of writing software in this complex environment is cleaner and allows the application to concentrate on the problem space. In addition, transparency offered by proper layering provides better code reusability and portability when moving applications to new architectures. As we described in <ref> [4] </ref>, the Generic Modelling Framework describes the foundation for building the core of any heterogeneous parallel application. We have built a set of generic software modelling techniques through the utilization of object frameworks, each being an object cluster providing a generic interface. <p> It also facilitates macro-dataow [13] and reactive object programming. If an application requires direct process to process communication, the system allows access to the capabilities of the Generic Modelling Framework <ref> [4] </ref>. This not only includes direct process communication, but generic shared memory and locking interfaces to the software. This provides a level of transparency to the application, so that it maintains a high level of portability, extensibility, and reusability over multiple architectures, operating systems, and protocols.
Reference: [5] <author> W. OConnell, G. Thiruvathukal, T. Christopher, Distributed-Memo: </author> <title> A Heterogeneously Distributed Parallel Software Development Environment, </title> <booktitle> Proc. 23rd Intl Conf. on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> Aug. </month> <year> 1994, </year> <note> FTP access: glen-ellyn.iit.edu;/pub/ research/parallel/papers. </note>
Reference-contexts: Out intent is to show that we have taken Linda, an elegant coordination language over heterogeneous machines, saved the good ideas, disposed of the rest, and added simple but powerful mechanisms for communication and synchronization. In <ref> [5] </ref>, we have illustrated mechanisms of Distributed-Memo which greatly enhance system efficiency along with how Application Description Files a can be used to vastly improve exibility. In this paper, we will emphasize the Distributed-Memo programming model by showing examples of common programming techniques and shared data structures. <p> In this paper, we will emphasize the Distributed-Memo programming model by showing examples of common programming techniques and shared data structures. We will also describe the systems integrated framework structure which also provides additional communication techniques to the application <ref> [5] </ref>. We will then contrast its primary interface with the similar Linda programming model illustrating areas of enhancements and deletions. Finally, we will present conclusions. 2 Problem Spaces to be Addressed We have taken a step back from the direct Linda approach a. <p> Heterogeneity issues are accomplished through the integration of the Generic Modelling Framework by incorporating its design into the kernel of the Distributed-Memo system <ref> [5] </ref>. 3 Distributed-Memo Programming Model The Distributed-Memo (D-Memo) system provides simple synchronization and communication mechanisms for parallel processes distributed over heterogeneous machines. The communication method is provided through a shared directory of unordered queues (or a shared table of bags). <p> Readers interested in access methods provided by the Generic Modelling Framework should see Integrated Framework for Heterogeneity on page 6 and [3][4]. In addition, readers interested in application start up methods along with application control through Application Description Files are referred to <ref> [5] </ref>. Many conventional distributed-memory programming models allocate one process to each node, achieving communication through message passing. The issue with such systems is that data structures are not global, but rather localized in each process. One of the most common programming techniques is to manipulate a data structure. <p> Each application has the ability to override any part of the configuration through the use of Application Description Files <ref> [5] </ref>. Typically, applications just override where to place processes during start-up and where the executables located. 3.2 Basic Distributed-Memo Facilities Before discussing the basic D-Memo facilities, folder names will be described. <p> The system easily supports any of the major parallel programming paradigms, such as SPMD a , MPMD b , host-node, and data parallel. The type of paradigm being used by an application is mainly attributed to its layout in the application description file <ref> [5] </ref>. The following list is some of the most useful programming techniques. 3.2.2.1 Named Objects A folder that holds at most one memo can represent a dynamically allocated object on a heap. Instead of pointers to the objects, we use folder names. b. <p> Each array element is itself stored in a separate folder. All array elements are not necessarily associated with the same folder server (one or more folder servers may be used by the run-time system, each managing a set of folders <ref> [5] </ref>). By spreading folders (that will be referenced within close locality of each other in the application code) throughout the distributed machines, the system does not congest one server by creating communication hot spots. <p> Two versions exist, C-Linda and F-Linda for C and fortran programming languages respectively. not only considered the shared directory of queues design, but an implementation on top of the Generic Modelling Framework. This allows the system to easily support high-performance heterogeneous systems by seamlessly building a virtual machine <ref> [5] </ref>. When reviewing the vast majority of published Linda algorithms, it is obvious that many of the code segments are directory-of-queue algorithms. In most Linda algorithms, the first several fields of a tuple, which we will call the key fields, area specified explicitly in both out and in operations. <p> The result is just as elegant as Linda, but much more efficient and vastly more exible <ref> [5] </ref>. 7 Acknowledgments The authors gratefully acknowledge use of the Argonne High-Performance Computing Research Facility. The HPCRF is funded principally by the U.S. Department of Energy Office of Scientific Computing.
Reference: [6] <author> D. Gelernter, </author> <title> Generative Communication in Linda, </title> <journal> ACM Transactions on Parallel Languages and Systems, </journal> <volume> Vol. 7, No 1, </volume> <month> Jan. </month> <year> 1985, </year> <pages> Pages 80-112. </pages>
Reference-contexts: The goal of supercomputings recent software movement in high-performance computing is to make parallel processing a more attractive option. This includes providing improved systems, libraries, tools, and languages which are easier to use. One notable system that attempts this is Linda, which provides virtual shared memory over multiple machines <ref> [6] </ref>. Out intent is to show that we have taken Linda, an elegant coordination language over heterogeneous machines, saved the good ideas, disposed of the rest, and added simple but powerful mechanisms for communication and synchronization. <p> OConnell AT&T Bell Laboratories 600 Mountain Ave. Murray Hill, N.J. 07974 wto@research.att.com Thomas W. Christopher Illinois Institute of Technology 10 West Federal Street Chicago, IL 60616 tc@iitmax.acc.iit.edu Proceedings of the IASTED Intl Conference on Parallel and Distributed Computing and Systems, Wash. D.C., Oct., 1994 <ref> [6] </ref> to study the problem space of heterogeneously distributed parallel computing for the general programmer [3]. Our emphasis is on: The need for an easy to use application programming interface (API). The need to model the heterogeneous platforms and protocols, including dynamic data modeling. <p> This notion of virtual shared memory (or a shared directory) is not new, for example distributed shared memory operating systems and file systems, such as the Andrew File System [3], have been around for some time. A notable parallel processing package supporting virtual shared memory is Linda <ref> [6] </ref>. The Linda parallel programming system attempts to provide a more natural parallel programming environment through a shared associative memory, referred to as tuple space. It is obvious that programming in a shared memory paradigm is more intuitive and easier than a distributed memory paradigm. <p> Some of these data structures and techniques are discussed in the seminal paper on generative communication in Linda <ref> [6] </ref>. D-Memo retains the simplicity and power of Linda in its ability to support the major parallel programming paradigms but is also able to support less popular paradigms, such as dataow. In this system, queues are referred to as folders and messages as memos. <p> note that while these frameworks are being used by D-Memo, they are also accessible to upper layering software or software at large. 5 Contrasting Linda Linda has been considered by many as a candidate for a more natural parallel programming system which provides a shared associative memory, the tuple space <ref> [6] </ref>. A tuple is a sequence of fields, each of which has a type and contains a value or a variable. Tuples are written into the tuple space with an out operation, are removed with an in, and are read without being removed with a rd. <p> Linda provides an alternative, asynchronous write: server processes wait for tuples specifying their individual names. A request for service by a particular server specifies the servers name. A request for service by any server specifies a variable in the name field, so that any server can pick it up <ref> [6] </ref>. Is cleaner and more efficient to have one queue for all servers in addition to an individual queue for each, and to have the servers wait for commands with the Alt operation. Linda lacks a read with alternatives, so this solution is unavailable to it.
Reference: [7] <author> T. Mattson, </author> <title> Programming Environments for Parallel Computing: A Comparison of CPS, Linda, P4, PVM, </title> <institution> PSYBL, and TCGMSG, Intel Corp Research Rep, </institution> <year> 1993. </year>
Reference: [8] <author> N. Carriero, D. Gelernter, </author> <title> Linda and Message Passing: What have we learned?, </title> <type> Tech. </type> <institution> Rept. YALEU/DCS/RR-984, </institution> <month> Aug., </month> <year> 1993. </year>
Reference: [9] <author> Arvind. I-structures: </author> <title> An Efficient Data Type for Functional Languages, TR LCS/TM-178, </title> <publisher> MIT, </publisher> <pages> 80. </pages>
Reference-contexts: Both the producer and consumer may run in parallel, with the consumer only being delayed if it attempts to fetch from a variable before it has been assigned. An I-Structure (an incremental structure) is a collection (e.g. an array) of futures. I-Structures were invented for dataow hardware <ref> [9] </ref>. In D-Memo, any folder that will have only one memo ever placed in it may correspond to a future. The consumer executing either a Get, GetCopy, or Alt fetching from that folder will be delayed until the value as been produced.
Reference: [10] <author> A.H. </author> <title> Veen Data Flow Architecture, </title> <journal> ACM Computing Surveys, </journal> <volume> 18, 4, </volume> <month> Dec. </month> <year> 1986. </year> <pages> pp. 365-396. </pages>
Reference: [11] <author> T.W. Christopher, </author> <title> Message Driven Computing and its Relationship to Actors, </title> <booktitle> Proc. ACM Sigplan Wkshop on Object-Based Conc. Prog., </booktitle> <address> San Diego, CA. </address> <year> 1988. </year>
Reference-contexts: Reactive objects are central to the Actors model of parallel computation <ref> [11] </ref>. A reactive object can be implemented with a job jar folder plus one input folder per object. A memo containing the object is delayed (using PutDelayed) on its input folder. When an input memo arrives, the object is placed into the job jar.
Reference: [12] <author> G. Thiruvathukal and T. Christopher, </author> <title> A Simulation of Demand Driven Dataow: Translation of Lucid into Message Driven Computing Language., </title> <booktitle> 5th Intl Symp. on Parallel Proc., </booktitle> <address> Anaheim, Ca. </address> <year> 1991. </year>
Reference: [13] <author> G. Thiruvathukal and T. Christopher, </author> <title> Macrodataow Implementation of Distributed Array Objects, </title> <type> Tech. </type> <note> Rpt. TR-HPLS-94-100. FTP access: glen ellyn.iit.edu;/pub/research/parallel/papers. </note>
Reference-contexts: It makes possible a variety of shared data structures, including named objects, arrays of objects, locks and semaphores, unordered and ordered queues, job jars, futures, I-structures, remote procedure calls, and barriers. It also facilitates macro-dataow <ref> [13] </ref> and reactive object programming. If an application requires direct process to process communication, the system allows access to the capabilities of the Generic Modelling Framework [4]. This not only includes direct process communication, but generic shared memory and locking interfaces to the software.
Reference: [14] <author> G. Thiruvathukal, W. OConnell, and T. Christopher, </author> <title> Towards Scalable Parallel Software: Interfacing to Non-von Neumann Programming Environments, </title> <booktitle> Proceedings of the SIAM95, </booktitle> <address> San Francisco, CA. </address> <month> Feb. </month> <year> 1995. </year>
References-found: 14

