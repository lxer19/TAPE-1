URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/soft-ext-isca94.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/soft-ext-isca94.html
Root-URL: 
Title: Software-Extended Coherent Shared Memory: Performance and Cost  
Author: David Chaiken and Anant Agarwal 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science, NE43-633 Massachusetts Institute of Technology  
Abstract: This paper evaluates the tradeoffs involved in the design of the software-extended memory system of Alewife, a multiprocessor architecture that implements coherent shared memory through a combination of hardware and software mechanisms. For each block of memory, Alewife implements between zero and five coherence directory pointers in hardware and allows software to handle requests when the pointers are exhausted. The software includes a flexible coherence interface that facilitates protocol software implementation. This interface is indispensable for conducting experiments and has proven important for implementing enhancements to the basic system. Simulations of a number of applications running on a complete system (with up to 256 processors) demonstrate that the hybrid architecture with five pointers achieves between 71% and 100% of full-map directory performance at a constant cost per processing element. Our experience in designing the software protocol interfaces and experiments with a variety of system configurations lead to a detailed understanding of the interaction of the hardware and software components of the system. The results show that a small amount of shared memory hardware provides adequate performance: One-pointer systems reach between 42% and 100% of full-map performance on our parallel benchmarks. A software-only directory architecture with no hardware pointers has lower performance but minimal cost. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, Godfrey D'Souza, and Mike Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Multiprocessors. </title> <journal> IEEE Micro, </journal> <volume> 13(3) </volume> <pages> 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Figure 1 shows an enlarged view of a node in the Alewife machine. Each node consists of a 33 MHz Sparcle processor <ref> [1] </ref>, 64K bytes of direct-mapped cache, 4 Mbytes of globally-shared main memory, and a floating-point coprocessor. The nodes communicate via messages through a network [29] with a mesh topology.
Reference: [2] <author> Anant Agarwal, Richard Simoni, John Hennessy, and Mark Horowitz. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <address> New York, </address> <month> June </month> <year> 1988. </year> <note> IEEE. </note>
Reference-contexts: A promising design strategy, central to the Alewife architecture, uses a combination of hardware and software to implement a cost-efficient directory [9]. Since most data blocks in a shared memory system are shared by a small number of processing nodes <ref> [2, 32, 8] </ref>, the hardware can implement a small set of pointers, and provide mechanisms to allow the system's software to extend the directory when the set of pointers is insufficient for enforcing coherence. This software-extension technique catalyzes the balance between a system's performance and cost. <p> Our notation is derived from a nomenclature for directory-based coherence protocols introduced in <ref> [2] </ref>. In the previous notation, a protocol was represented as Dir i X, where i represented the number of explicit copies tracked, and X was B or NB depending on whether or not the protocol issued broadcasts.
Reference: [3] <author> A. Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: Shared memory itself helps control the complexity of the application software written for a machine, but it requires an efficient design to achieve this goal. The Alewife architecture <ref> [3] </ref> uses a combination of hardware and software to provide shared memory at a constant cost per processing node, without sacrificing performance. Following the integrated systems approach, the architecture uses hardware to implement common memory accesses and uses software to extend the hardware by handling potentially complex scenarios.
Reference: [4] <author> Henri E. Bal and M. Frans Kaashoek. </author> <title> Object Distribution in Orca using Compile-Time and Run-Time Techniques. </title> <booktitle> In Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA'93)., </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: These schemes do not require the mechanisms listed above, but they lack the flexibility of protocol and application software design. A number of systems rely primarily on software to implement the mechanisms required to support shared memory <ref> [10, 23, 11, 6, 5, 4] </ref>. These systems implement coherent shared memory at low cost; however, providing location-independent addressing in software forces the granularity of data sharing to be much larger than in software-extended systems.
Reference: [5] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Saw-don. </author> <title> The Midway Distributed Shared Memory System. </title> <booktitle> In Proceedingsof the 38th IEEE Computer Society International Conference (COMPCON'93), </booktitle> <pages> pages 528-537. </pages> <publisher> IEEE, </publisher> <month> Febru-ary </month> <year> 1993. </year>
Reference-contexts: These schemes do not require the mechanisms listed above, but they lack the flexibility of protocol and application software design. A number of systems rely primarily on software to implement the mechanisms required to support shared memory <ref> [10, 23, 11, 6, 5, 4] </ref>. These systems implement coherent shared memory at low cost; however, providing location-independent addressing in software forces the granularity of data sharing to be much larger than in software-extended systems.
Reference: [6] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and Performance of MUNIN. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year> <month> 10 </month>
Reference-contexts: These schemes do not require the mechanisms listed above, but they lack the flexibility of protocol and application software design. A number of systems rely primarily on software to implement the mechanisms required to support shared memory <ref> [10, 23, 11, 6, 5, 4] </ref>. These systems implement coherent shared memory at low cost; however, providing location-independent addressing in software forces the granularity of data sharing to be much larger than in software-extended systems.
Reference: [7] <author> Lucien M. Censier and Paul Feautrier. </author> <title> A New Solution to Co--herence Problems in Multicache Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> December </month> <year> 1978. </year>
Reference-contexts: Having fewer hardware pointers means a lower implementation cost, at the expense of reduced performance. This tradeoff suggests a whole spectrum of protocols, ranging from 0 pointers to n pointers, where n is the number of nodes in the system. 2.1 The n pointer protocol The full-map protocol <ref> [7] </ref>, which is implemented in the DASH multiprocessor [21], uses n pointers for every block of memory in the system and requires no software extension.
Reference: [8] <author> David Chaiken, Craig Fields, Kiyoshi Kurihara, and Anant Agarwal. </author> <title> Directory-Based Cache-Coherence in Large-Scale Multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 41-58, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: A promising design strategy, central to the Alewife architecture, uses a combination of hardware and software to implement a cost-efficient directory [9]. Since most data blocks in a shared memory system are shared by a small number of processing nodes <ref> [2, 32, 8] </ref>, the hardware can implement a small set of pointers, and provide mechanisms to allow the system's software to extend the directory when the set of pointers is insufficient for enforcing coherence. This software-extension technique catalyzes the balance between a system's performance and cost.
Reference: [9] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASP-LOS IV), </booktitle> <pages> pages 224-234. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: Typically, directories are not monolithic structures but are distributed to the processing nodes along with a system's shared memory. A promising design strategy, central to the Alewife architecture, uses a combination of hardware and software to implement a cost-efficient directory <ref> [9] </ref>. <p> This software-extension technique catalyzes the balance between a system's performance and cost. LimitLESS directories, a scheme proposed in <ref> [9] </ref>, is a software-extended coherence protocol that permits a tradeoff between the cost and the performance of a shared memory system. <p> All software-extended memory systems require a battery of ar-chitectural mechanisms to permit a designer to make the cost versus performance tradeoff. First, the shared memory hardware must be able to invoke extension software on the processor, and the processor must have complete access to the memory and network hardware <ref> [9, 19, 34] </ref>. Second, the hardware must guarantee forward progress in the face of protocol thrashing scenarios and high-availability interrupts [20]. Each processing node must also provide support for location-independent addressing, which is a fundamental requirement of shared memory. <p> In addition, processor caches should include more associativity than a simple direct-mapped cache. Alewife uses a victim cache [16, 20] to provide the required extra associativity. The paper extends previous work in the performance analysis of software-extended coherence protocols <ref> [9, 34] </ref> to a much wider spectrum, ranging from zero hardware pointers (the software-only directory architecture) to a full-map protocol, through the use of controlled experiments using a synthetic benchmark and a set of application programs. <p> By studying the behavior of real software protocol handlers, this paper confirms results presented in <ref> [9] </ref> on the similarity in performance of LimitLESS 1 (one hardware pointer), LimitLESS 2 , LimitLESS 4 , and full-map protocols. We also con firm the findings in [34] that the performance of suitably tuned one-pointer protocols is competitive with that of multiple-pointer protocols. <p> Our notation attempts to capture the spectrum of features of software-extended protocols that have evolved over the past several years, and previously termed LimitLESS 1 , LimitLESS 4 , and others in <ref> [9] </ref>, and Dir 1 SW, Dir 1 SW+, and others in [14, 34]. For both hardware and software, our notation divides the mechanisms into two classes: those that dictate directory actions upon receipt of processor requests, and those that dictate directory actions for acknowledgments. <p> A missing A field implies that the hardware keeps an updated count of acknowledgments received. Finally, the A parameter is LACK if a software trap is invoked only on the last acknowledgment. According to this notation, the LimitLESS 1 protocol defined in <ref> [9] </ref> is termed Dir n H 1 S NB , denoting that it records n pointers, of which only one is in hardware. The hardware handles all acknowledgments and the software issues invalidations to shared copies when a write request occurs after an overflow. <p> The generality of the flexible coherence interface described in Section 4 provides a platform for experimenting with schemes that enhance the performance and the functionality of the base protocols. <ref> [9] </ref> suggests several extensions to the basic software such as a FIFO lock data type. To date, the protocol extension software has been used to implement a FIFO lock data type, stack overflow exceptions, and a fast barrier implementation. <p> To date, the protocol extension software has been used to implement a FIFO lock data type, stack overflow exceptions, and a fast barrier implementation. These enhancements are aimed at providing efficient functions that improve the programmability of the machine. <ref> [9] </ref> also indicates that LimitLESS software could be enhanced to improve the performance of normal shared memory variables, such as variables with large worker sets.
Reference: [10] <author> David R. Cheriton, Gert A. Slavenberg, and Patrick D. Boyle. </author> <title> Software-Controlled Caches in the VMP Multiprocessor. </title> <booktitle> In Proceedings of the 13th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 367-374, </pages> <address> New York, </address> <month> June </month> <year> 1986. </year> <note> IEEE. </note>
Reference-contexts: These schemes do not require the mechanisms listed above, but they lack the flexibility of protocol and application software design. A number of systems rely primarily on software to implement the mechanisms required to support shared memory <ref> [10, 23, 11, 6, 5, 4] </ref>. These systems implement coherent shared memory at low cost; however, providing location-independent addressing in software forces the granularity of data sharing to be much larger than in software-extended systems.
Reference: [11] <author> A. Cox and R. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <month> December </month> <year> 1989. </year> <note> Also as a Univ. </note> <institution> Rochester TR-263, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: These schemes do not require the mechanisms listed above, but they lack the flexibility of protocol and application software design. A number of systems rely primarily on software to implement the mechanisms required to support shared memory <ref> [10, 23, 11, 6, 5, 4] </ref>. These systems implement coherent shared memory at low cost; however, providing location-independent addressing in software forces the granularity of data sharing to be much larger than in software-extended systems.
Reference: [12] <author> Alan L. Cox and Robert J. Fowler. </author> <title> Adaptive Cache Coherence for Detecting Migratory Shared Data. </title> <booktitle> In Proceedings of the 20th Annual Symposium on Computer Architecture 1993, </booktitle> <address> New York, </address> <month> May </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: The studies show that given appropriate annotations, a large class of applications can perform well on Dir 1 H 1 S B,LACK . [24] demonstrates a compiler annotation scheme for optimizing the performance of protocols that dynamically allocate directory pointers. Dynamic detection <ref> [12] </ref> and [27] propose a hardware mechanism that dynamically adapts to migratory data. Protocol extension software could perform similar optimizations. In addition, there are some classes of data that create severe performance bottlenecks. These classes tend to be the result of a simplistic programming style or a performance bug.
Reference: [13] <author> W. Hackbusch, </author> <title> editor. Multigrid Methods and Applications. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1985. </year>
Reference-contexts: Again, Dir n H 0 S NB,ACK performs respectably due to the favorable memory access patterns 8 in the application. Static Multigrid SMGRID uses the multigrid method to solve elliptical partial differential equations <ref> [13] </ref>. The algorithm consists of performing a series of Jacobi-style iterations on multiple grids of varying granularities. The speedup over sequential is limited by the fact that only a subset of nodes work during the relaxation on the upper levels of the pyramid of grids.
Reference: [14] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 262-273, </pages> <address> Boston, </address> <month> October </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: When these pointers are exhausted, the memory system hardware interrupts a local processor, thereby requesting it to maintain correct shared memory behavior by extending the hardware directory with software. Another set of software-extended protocols (termed Dir 1 SW) were proposed in <ref> [14] </ref> and [34]. These protocols use only one hardware pointer, rely on software to broadcast invalidates, and use hardware to accumulate the acknowledgments. In addition, they allow the programmer or compiler to insert Check-In/CheckOut (CICO) directives into programs to minimize the number of software traps. <p> Our notation attempts to capture the spectrum of features of software-extended protocols that have evolved over the past several years, and previously termed LimitLESS 1 , LimitLESS 4 , and others in [9], and Dir 1 SW, Dir 1 SW+, and others in <ref> [14, 34] </ref>. For both hardware and software, our notation divides the mechanisms into two classes: those that dictate directory actions upon receipt of processor requests, and those that dictate directory actions for acknowledgments. <p> In our notation, the three one-pointer protocols are Dir n H 1 S NB,ACK , Dir n H 1 S NB,LACK , and Dir n H 1 S NB , respectively. The set of software-extended protocols introduced in <ref> [14] </ref> 3 and [34] can also be expressed in terms of our notation. The Dir 1 SW protocol maintains one pointer in hardware, resorts to software broadcasts when more than one copy exists, and counts acknowledgments in hardware. In addition, their protocol traps into software on the last acknowledgment [33]. <p> The following types of extensions give examples of current research: Program and compiler annotations Program annotations allow a programmer to give the system information about the way that an application interacts with shared memory. <ref> [14] </ref> and [34] propose and evaluate this method for improving the performance 9 of software-extended shared memory.
Reference: [15] <author> Kirk Johnson. </author> <title> Semi-C Reference Manual. ALEWIFE Memo No. </title> <type> 20, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: The names and characteristics of the applications we analyze are given by Table 3. They are written in C, Mul-T [18] (a parallel dialect of LISP), and Semi-C <ref> [15] </ref> (a language akin to C with support for fine-grain parallelism). Each application (except MP3D) is studied with a problem size that realizes more than 50% processor utilization on a simulated 64 node machine with a full-map directory.
Reference: [16] <author> N.P. Jouppi. </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proceedings, International Symposium on Computer Architecture '90, </booktitle> <pages> pages 364-373, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Beyond this level of hardware support, the cost and mapping of a system's DRAM become more important factors than performance. In addition, processor caches should include more associativity than a simple direct-mapped cache. Alewife uses a victim cache <ref> [16, 20] </ref> to provide the required extra associativity. <p> While perfect instruction fetching is not possible in real systems, there are various methods for relieving instruction/data thrashing by increasing the associativity of the cache system. Alewife's approach to the problem is to implement a version of victim caching <ref> [16] </ref>, which uses the transaction store [20] to provide a small number of buffers for storing blocks that are evicted from the cache. The black bars in Figures 4 (a) and 3 show the performance for TSP on a system with victim caching enabled.
Reference: [17] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatow-icz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory; Early Experience. </title> <booktitle> In Practice and Principles of Parallel Programming (PPoPP) 1993, </booktitle> <pages> pages 54-63, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year> <note> ACM. Also as MIT/LCS TM-478, </note> <month> January </month> <year> 1993. </year>
Reference-contexts: Since these mechanisms comprise the bulk of the complexity of a software-extended system, it is important to note that the benefits of these mechanisms extend far beyond the implementation of shared memory <ref> [17] </ref>. Alternative approaches to implementing shared memory proposed in [26, 30, 21] use hardware mechanisms that allocate directory pointers dynamically. These schemes do not require the mechanisms listed above, but they lack the flexibility of protocol and application software design.
Reference: [18] <author> David A. Kranz, R. Halstead, and E. Mohr. Mul-T: </author> <title> A High-Performance Parallel Lisp. </title> <booktitle> In Proceedings of SIGPLAN '89, Symposium on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: The names and characteristics of the applications we analyze are given by Table 3. They are written in C, Mul-T <ref> [18] </ref> (a parallel dialect of LISP), and Semi-C [15] (a language akin to C with support for fine-grain parallelism). Each application (except MP3D) is studied with a problem size that realizes more than 50% processor utilization on a simulated 64 node machine with a full-map directory.
Reference: [19] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In Proceedings of the International Supercomputing Conference(ISC) 1993, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year> <note> IEEE. Also as MIT/LCS TM, </note> <month> December </month> <year> 1992. </year>
Reference-contexts: All software-extended memory systems require a battery of ar-chitectural mechanisms to permit a designer to make the cost versus performance tradeoff. First, the shared memory hardware must be able to invoke extension software on the processor, and the processor must have complete access to the memory and network hardware <ref> [9, 19, 34] </ref>. Second, the hardware must guarantee forward progress in the face of protocol thrashing scenarios and high-availability interrupts [20]. Each processing node must also provide support for location-independent addressing, which is a fundamental requirement of shared memory.
Reference: [20] <author> John Kubiatowicz, David Chaiken, and Anant Agarwal. </author> <title> Closing the Window of Vulnerability in Multiphase Memory Transactions. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 274-284, </pages> <address> Boston, </address> <month> October </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: First, the shared memory hardware must be able to invoke extension software on the processor, and the processor must have complete access to the memory and network hardware [9, 19, 34]. Second, the hardware must guarantee forward progress in the face of protocol thrashing scenarios and high-availability interrupts <ref> [20] </ref>. Each processing node must also provide support for location-independent addressing, which is a fundamental requirement of shared memory. Hardware support for location-independent addressing permits the software to issue an address that refers to an object without knowledge of where it is resident. <p> Beyond this level of hardware support, the cost and mapping of a system's DRAM become more important factors than performance. In addition, processor caches should include more associativity than a simple direct-mapped cache. Alewife uses a victim cache <ref> [16, 20] </ref> to provide the required extra associativity. <p> While perfect instruction fetching is not possible in real systems, there are various methods for relieving instruction/data thrashing by increasing the associativity of the cache system. Alewife's approach to the problem is to implement a version of victim caching [16], which uses the transaction store <ref> [20] </ref> to provide a small number of buffers for storing blocks that are evicted from the cache. The black bars in Figures 4 (a) and 3 show the performance for TSP on a system with victim caching enabled.
Reference: [21] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Since these mechanisms comprise the bulk of the complexity of a software-extended system, it is important to note that the benefits of these mechanisms extend far beyond the implementation of shared memory [17]. Alternative approaches to implementing shared memory proposed in <ref> [26, 30, 21] </ref> use hardware mechanisms that allocate directory pointers dynamically. These schemes do not require the mechanisms listed above, but they lack the flexibility of protocol and application software design. <p> This tradeoff suggests a whole spectrum of protocols, ranging from 0 pointers to n pointers, where n is the number of nodes in the system. 2.1 The n pointer protocol The full-map protocol [7], which is implemented in the DASH multiprocessor <ref> [21] </ref>, uses n pointers for every block of memory in the system and requires no software extension. Although this protocol permits an efficient implementation that uses only one bit for each pointer, the sheer number of pointers makes it extremely expensive for systems with large numbers of nodes. <p> X is NB if the number of hardware pointers is i and no more than i shared copies are allowed, and is B if the number of hardware pointers is i and broadcasts are used when more than i shared copies exist. Thus the full-map protocol in DASH <ref> [21] </ref> is termed Dir n H NB S . The parameter Y is NB if the hardware-software combination records i explicit pointers and allows no more than i copies. Y is B if the software resorts to a broadcast when more than i copies exist.
Reference: [22] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH Prototype: Logic Overhead and Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 41-60, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: For our simulations, we use a problem size of 10,000 particles, turn the locking option off, and augment the standard p4 macros with Alewife's parallel C library [25]. Since this application is notorious for exhibiting low speedups <ref> [22] </ref>, the results in Figure 4 (e) are encouraging: Dir n H NB S achieves a speedup of 64 nodes. 24 and Dir n H 5 S NB realizes a speedup of 20.
Reference: [23] <author> Kai Li. IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing. </title> <booktitle> In International Conference on Parallel Computing, </booktitle> <pages> pages 94-101, </pages> <year> 1988. </year>
Reference-contexts: These schemes do not require the mechanisms listed above, but they lack the flexibility of protocol and application software design. A number of systems rely primarily on software to implement the mechanisms required to support shared memory <ref> [10, 23, 11, 6, 5, 4] </ref>. These systems implement coherent shared memory at low cost; however, providing location-independent addressing in software forces the granularity of data sharing to be much larger than in software-extended systems.
Reference: [24] <author> David J. Lilja and Pen-Chung Yew. </author> <title> Improving Memory Utilization in Cache Coherence Directories. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(10) </volume> <pages> 1130-1146, </pages> <month> Oc-tober </month> <year> 1993. </year>
Reference-contexts: The studies show that given appropriate annotations, a large class of applications can perform well on Dir 1 H 1 S B,LACK . <ref> [24] </ref> demonstrates a compiler annotation scheme for optimizing the performance of protocols that dynamically allocate directory pointers. Dynamic detection [12] and [27] propose a hardware mechanism that dynamically adapts to migratory data. Protocol extension software could perform similar optimizations.
Reference: [25] <author> Beng-Hong Lim. </author> <title> Functions for Parallel C on the Alewife System. ALEWIFE Memo No. </title> <type> 37, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: MP3D The MP3D application is part of the SPLASH parallel benchmark suite [31]. For our simulations, we use a problem size of 10,000 particles, turn the locking option off, and augment the standard p4 macros with Alewife's parallel C library <ref> [25] </ref>. Since this application is notorious for exhibiting low speedups [22], the results in Figure 4 (e) are encouraging: Dir n H NB S achieves a speedup of 64 nodes. 24 and Dir n H 5 S NB realizes a speedup of 20.
Reference: [26] <author> Brian W. O'Krafka and A. Richard Newton. </author> <title> An Empirical Evaluation of Two Memory-Efficient Directory Methods. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 138-147, </pages> <address> New York, </address> <month> June </month> <year> 1990. </year> <note> IEEE. </note>
Reference-contexts: Since these mechanisms comprise the bulk of the complexity of a software-extended system, it is important to note that the benefits of these mechanisms extend far beyond the implementation of shared memory [17]. Alternative approaches to implementing shared memory proposed in <ref> [26, 30, 21] </ref> use hardware mechanisms that allocate directory pointers dynamically. These schemes do not require the mechanisms listed above, but they lack the flexibility of protocol and application software design.
Reference: [27] <author> Per Stenstrom, Mats Brorsson, and Lars Sandberg. </author> <title> An Adaptive Cache Coherence Protocol Optimized for Migratory Sharing. </title> <booktitle> In Proceedings of the 20th Annual Symposium on Computer Architecture 1993, </booktitle> <address> New York, </address> <month> May </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: The studies show that given appropriate annotations, a large class of applications can perform well on Dir 1 H 1 S B,LACK . [24] demonstrates a compiler annotation scheme for optimizing the performance of protocols that dynamically allocate directory pointers. Dynamic detection [12] and <ref> [27] </ref> propose a hardware mechanism that dynamically adapts to migratory data. Protocol extension software could perform similar optimizations. In addition, there are some classes of data that create severe performance bottlenecks. These classes tend to be the result of a simplistic programming style or a performance bug.
Reference: [28] <author> John D. Piscitello. </author> <title> A Software Cache Coherence Protocol for Alewife. </title> <type> Master's thesis, </type> <institution> MIT, Department of Electrical Engineering and Computer Science, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The software then returns the hardware directory to a mode that collects one acknowledgment message for each transmitted invalidation. 2.3 Zero-pointer protocols Since the software-only directory <ref> [28] </ref> has no directory memory, it requires substantially different software than the 2 $ (n1) range of protocols. This software must implement all coherence protocol state transitions for inter-node accesses.
Reference: [29] <author> Charles L. Seitz. </author> <title> Concurrent VLSI Architectures. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-33(12):1247-1265, </volume> <month> December </month> <year> 1984. </year>
Reference-contexts: Figure 1 shows an enlarged view of a node in the Alewife machine. Each node consists of a 33 MHz Sparcle processor [1], 64K bytes of direct-mapped cache, 4 Mbytes of globally-shared main memory, and a floating-point coprocessor. The nodes communicate via messages through a network <ref> [29] </ref> with a mesh topology. A single-chip communications and memory management (CMMU) on each node holds the cache tags and implements the memory coherence protocol by synthesizing messages to other nodes. All of the node components, with the exception of the CMMU, have been fabricated and tested.
Reference: [30] <author> Richard Simoni and Mark Horowitz. </author> <title> Dynamic Pointer Allocation for Scalable Cache Coherence Directories. </title> <booktitle> In Proceedings International Symposium on Shared Memory Multiprocessing, </booktitle> <address> Japan, April 1991. </address> <publisher> IPS Press. </publisher>
Reference-contexts: Since these mechanisms comprise the bulk of the complexity of a software-extended system, it is important to note that the benefits of these mechanisms extend far beyond the implementation of shared memory [17]. Alternative approaches to implementing shared memory proposed in <ref> [26, 30, 21] </ref> use hardware mechanisms that allocate directory pointers dynamically. These schemes do not require the mechanisms listed above, but they lack the flexibility of protocol and application software design.
Reference: [31] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-92-526, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: The large worker sets sizes impact the 0 and 1 pointer protocols most severely. Thus, EVOLVE provides a good example of a program that can benefit from a system's hardware directory pointers. MP3D The MP3D application is part of the SPLASH parallel benchmark suite <ref> [31] </ref>. For our simulations, we use a problem size of 10,000 particles, turn the locking option off, and augment the standard p4 macros with Alewife's parallel C library [25].
Reference: [32] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Analysis of Cache Invalidation Patterns in Multiprocessors. </title> <booktitle> In Third International Conference on Architectural Support for Programming Languagesand Operating Systems (ASPLOS III), </booktitle> <month> April </month> <year> 1989. </year>
Reference-contexts: A promising design strategy, central to the Alewife architecture, uses a combination of hardware and software to implement a cost-efficient directory [9]. Since most data blocks in a shared memory system are shared by a small number of processing nodes <ref> [2, 32, 8] </ref>, the hardware can implement a small set of pointers, and provide mechanisms to allow the system's software to extend the directory when the set of pointers is insufficient for enforcing coherence. This software-extension technique catalyzes the balance between a system's performance and cost.
Reference: [33] <author> David A. Wood. </author> <title> Private Communication, </title> <month> October </month> <year> 1993. </year>
Reference-contexts: The Dir 1 SW protocol maintains one pointer in hardware, resorts to software broadcasts when more than one copy exists, and counts acknowledgments in hardware. In addition, their protocol traps into software on the last acknowledgment <ref> [33] </ref>. In our notation, this protocol is represented as Dir 1 H 1 S B,LACK .
Reference: [34] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In In Proceedings of the 20th Annual International Symposium on Computer Architecture 1993, </booktitle> <pages> pages 156-167, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year> <journal> ACM. </journal> <volume> 11 </volume>
Reference-contexts: When these pointers are exhausted, the memory system hardware interrupts a local processor, thereby requesting it to maintain correct shared memory behavior by extending the hardware directory with software. Another set of software-extended protocols (termed Dir 1 SW) were proposed in [14] and <ref> [34] </ref>. These protocols use only one hardware pointer, rely on software to broadcast invalidates, and use hardware to accumulate the acknowledgments. In addition, they allow the programmer or compiler to insert Check-In/CheckOut (CICO) directives into programs to minimize the number of software traps. <p> All software-extended memory systems require a battery of ar-chitectural mechanisms to permit a designer to make the cost versus performance tradeoff. First, the shared memory hardware must be able to invoke extension software on the processor, and the processor must have complete access to the memory and network hardware <ref> [9, 19, 34] </ref>. Second, the hardware must guarantee forward progress in the face of protocol thrashing scenarios and high-availability interrupts [20]. Each processing node must also provide support for location-independent addressing, which is a fundamental requirement of shared memory. <p> In addition, processor caches should include more associativity than a simple direct-mapped cache. Alewife uses a victim cache [16, 20] to provide the required extra associativity. The paper extends previous work in the performance analysis of software-extended coherence protocols <ref> [9, 34] </ref> to a much wider spectrum, ranging from zero hardware pointers (the software-only directory architecture) to a full-map protocol, through the use of controlled experiments using a synthetic benchmark and a set of application programs. <p> By studying the behavior of real software protocol handlers, this paper confirms results presented in [9] on the similarity in performance of LimitLESS 1 (one hardware pointer), LimitLESS 2 , LimitLESS 4 , and full-map protocols. We also con firm the findings in <ref> [34] </ref> that the performance of suitably tuned one-pointer protocols is competitive with that of multiple-pointer protocols. In order to study the software side of hybrid shared memory systems, this paper investigates two different software systems that use the same hardware to achieve different goals. <p> Our notation attempts to capture the spectrum of features of software-extended protocols that have evolved over the past several years, and previously termed LimitLESS 1 , LimitLESS 4 , and others in [9], and Dir 1 SW, Dir 1 SW+, and others in <ref> [14, 34] </ref>. For both hardware and software, our notation divides the mechanisms into two classes: those that dictate directory actions upon receipt of processor requests, and those that dictate directory actions for acknowledgments. <p> In our notation, the three one-pointer protocols are Dir n H 1 S NB,ACK , Dir n H 1 S NB,LACK , and Dir n H 1 S NB , respectively. The set of software-extended protocols introduced in [14] 3 and <ref> [34] </ref> can also be expressed in terms of our notation. The Dir 1 SW protocol maintains one pointer in hardware, resorts to software broadcasts when more than one copy exists, and counts acknowledgments in hardware. In addition, their protocol traps into software on the last acknowledgment [33]. <p> The following types of extensions give examples of current research: Program and compiler annotations Program annotations allow a programmer to give the system information about the way that an application interacts with shared memory. [14] and <ref> [34] </ref> propose and evaluate this method for improving the performance 9 of software-extended shared memory.
References-found: 34

