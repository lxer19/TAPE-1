URL: http://www.cs.utah.edu/~ald/las-vegas.ps
Refering-URL: http://www.cs.utah.edu/~ald/
Root-URL: 
Title: How Much Adaptivity is Required for Bursty Traffic?  
Author: Ludmila Cherkasova Al Davis Vadim Kotov Ian Robinson Tomas Rokicki 
Address: 1501 Page Mill Road, Palo Alto, CA 94303  Salt Lake City, UT 84112  
Affiliation: Hewlett-Packard Laboratories,  Department of Computer Science, University of Utah,  
Abstract: Deterministic routing strategies are cheap and fast to implement but suffer from increased message latency due to contention for resources in a packet switching fabric. Adaptive routing strategies are inherently more complex which may result in slower routing. Our goal is to investigate the trade-offs involved in using different routing strategies. This paper presents the results of a simulation study designed to answer this question for realistic bursty traffic workloads. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cherkasova, L. and Rokicki, T.: </author> <title> Alpha Message Scheduling for Packet-Switched Interconnects. </title> <note> To be published. </note>
Reference-contexts: This paper does not take into account the effects of intelligently scheduling the packets from various messages for injection into the interconnect. Early results <ref> [1] </ref> show that via the scheduling strategy the workload can be made to appear closer to a random traffic model. This has the effect-as shown earlier-of reducing the performance advantage of the Derouting strategy.
Reference: [2] <author> Chien, Andrew A.: </author> <title> A Cost and Speed Model for k-ary n-cube Wormwhole Routers. </title> <booktitle> In Proceedings of Hot Interconnects'93, A Symposium on High Performance Interconnects, </booktitle> <year> 1993. </year>
Reference-contexts: Since latency may be more difficult to hide in a more conventional PE design, low latency message traffic becomes the primary goal. Adaptivity is costly <ref> [2] </ref> both in terms of router complexity and in terms of latency when suboptimal paths are chosen.
Reference: [3] <author> Dally, W. J. et al.: </author> <title> The J-Machine: A Fine-Grain Concurrent Computer. </title> <booktitle> In Proceedings of the IFIP Conference, </booktitle> <publisher> North-Holland, </publisher> <pages> pp. 1147-1153, </pages> <year> 1989. </year>
Reference-contexts: Adaptivity is costly [2] both in terms of router complexity and in terms of latency when suboptimal paths are chosen. Several low latency deterministic routers have been developed <ref> [7, 3] </ref> but we are still interested in the potential use of limited adaptivity to bypass temporary congestion in the fabric rather than the added latency required to just wait for the resource.
Reference: [4] <author> Davis A., Mayfly: </author> <title> A General-Purpose, Scalable, Parallel Processing Architecture. </title> <journal> J. LISP and Symbolic Computation, vol.5, </journal> <volume> No.1/2, </volume> <pages> pp. 7-48, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction The work presented here is a natural extension of our previous work on a high perfomance router called the Post Office which was used to form the interconnect fabric for a scalable parallel multiprocessing system called Mayfly <ref> [4] </ref>. The Mayfly processing element (PE) architecture was designed to hide communication latency and hence the Post Office was designed primarily to provide a high capacity fabric. The Post Office was a fully adaptive virtual cut-through architecture [5].
Reference: [5] <author> Fujimoto R. M. </author> <title> VLSI Communication Components for Multicomputer Networks.Ph.D. </title> <institution> Thesis,University of California at Berkeley,1983. </institution>
Reference-contexts: The Mayfly processing element (PE) architecture was designed to hide communication latency and hence the Post Office was designed primarily to provide a high capacity fabric. The Post Office was a fully adaptive virtual cut-through architecture <ref> [5] </ref>. Congested Post Office packets would not adapt immediately but would wait for a certain stagnation delay before choosing an alternate path. <p> Each port is bidirectional, the link between them being half-duplex. Routing logic decides which port or ports an arriving packet should be forwarded to. If the port is available, the packet transmission starts, even if it has not all been received. This virtual cut-through technique <ref> [5] </ref> leads to lower per-hop latencies than the alternative of store-and-forward. If the desired port or ports are not available then the packet waits in the buffer. Ports service waiting packets in a first-come, first-served manner. A PO2 node can reject a packet if no buffers are available.
Reference: [6] <author> Jain, R.: </author> <title> Myths About Congestion Management in High-speed Networks. Internetworking: </title> <journal> Research and Experience, </journal> <volume> Vol.3, </volume> <pages> pp. 101-113, </pages> <year> 1992. </year>
Reference-contexts: Instead of occasional packets competing briefly for the same port, two bursts compete for some port during a longer period of time. If this contention is not controlled, packets can build up in the preceding nodes, leading to more contention and eventually complete interconnect saturation <ref> [6] </ref>. To prevent this situation from happening a flow control mechanism based on backpressure is used. However, the flow control provided by backpressure and the routing freedom provided by adaptivity are in conflict with each other.
Reference: [7] <author> Seitz, C.: </author> <title> The Cosmic Cube. </title> <journal> J.Communications of the ACM, Vol.28, </journal> <volume> No.1, </volume> <pages> pp. 22-33, </pages> <year> 1984. </year>
Reference-contexts: Adaptivity is costly [2] both in terms of router complexity and in terms of latency when suboptimal paths are chosen. Several low latency deterministic routers have been developed <ref> [7, 3] </ref> but we are still interested in the potential use of limited adaptivity to bypass temporary congestion in the fabric rather than the added latency required to just wait for the resource.
References-found: 7

