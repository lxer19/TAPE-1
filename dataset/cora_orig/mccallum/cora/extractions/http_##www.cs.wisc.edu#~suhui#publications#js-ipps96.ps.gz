URL: http://www.cs.wisc.edu/~suhui/publications/js-ipps96.ps.gz
Refering-URL: http://www.cs.wisc.edu/~suhui/suhui.html
Root-URL: 
Email: suhui@cs.wisc.edu vernon@cs.wisc.edu  
Title: Dynamic vs. Static Quantum-Based Parallel Processor Allocation  
Author: Su-Hui Chiang and Mary K. Vernon 
Address: Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin  
Abstract: This paper improves upon previous synthetic workload models and compares the performance of dynamic spatial equipartitioning (EQS) and the semi-static quantum-based FB-PWS processor allocation defined in [23], under synthetic workloads that have not previously been considered. These new workloads include realistic repartitioning overheads and job characteristics that are consistent with system measurement, anticipated trends, and experience. The overall conclusion from the results is that the EQS policy is generally superior to the FB-PWS policy even under realistic repartitioning overheads. We find cases where the EQS system saturates earlier than the FB-PWS system, and vice versa. This leads to the definition of a modified EQS policy, called EQS-PWS, which has performance equal to or better than EQS and FB-PWS for all workloads examined in this paper. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. H. Arpaci, A. C. Dusseau, A. M. Vahdat, L. T. Liu, T. E. Anderson, D. A. Patterson, </author> <title> The Interactions of Parallel and Sequential Workloads on a Network of Workstations. </title> <booktitle> Proc. 1995 ACM Sigmetrics Joint Int'l. Conf. on Measurement and Modeling of Computer Systems, Ottawa, </booktitle> <pages> pp. 267-278, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Such a system requires an effective policy for recruiting idle nodes as well as efficient mechanisms for migrating the processes of parallel jobs away from nodes that are preempted by a higher priority user <ref> [2, 30, 1] </ref>. Although we do not consider the impact of node interruptions nor particular policy customizations that might be needed, we consider synthetic workloads and repartitioning overheads that are relevant to such environments.
Reference: [2] <author> A. Bricker, M. Litzkow, M. Livny, </author> <title> Condor Technical Summary. </title> <type> Technical Report TR 1069, </type> <institution> Computer Sciences Dept., University of Wisconsin, Madi-son, WI, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Such a system requires an effective policy for recruiting idle nodes as well as efficient mechanisms for migrating the processes of parallel jobs away from nodes that are preempted by a higher priority user <ref> [2, 30, 1] </ref>. Although we do not consider the impact of node interruptions nor particular policy customizations that might be needed, we consider synthetic workloads and repartitioning overheads that are relevant to such environments.
Reference: [3] <author> R. Chandra, S. Devine, B. Verghese, A. Gupta, M. Rosenblum, </author> <title> Scheduling and Page Migration for Multiprocessor Compute Servers. </title> <booktitle> Proc. 6th Int'l. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <address> San Jose, CA, </address> <pages> pp. 12-24, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: This estimate was arrived at by computing the time to fetch 32 megabytes of data from a remote memory, either in a network of workstations that runs the GMS global memory management system [7] or in the KSR or DASH memory systems <ref> [20, 3] </ref>. In GMS, each remote fetch of an 8-kilobyte page requires 2 milliseconds. In KSR, it takes 30 milliseconds to fill a 256KB cache from remote memory [20]. In DASH, each remote fetch of a 16-byte cache block requires approximately 170 cycles on a 33 MHz processor.
Reference: [4] <author> S.-H. Chiang, R. K. Mansharamani, M. K. Vernon, </author> <title> Use of Application Characteristics and Limited Preemption for Run-to-Completion Parallel Processor Scheduling Policies. </title> <booktitle> Proc. 1994 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Nashville, TN, </address> <pages> pp. 33-44, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: This is an impressive result since FB-PWS commits to a processor allocation at job arrival time. Previous static allocation policies have generally not been competitive with dynamic policies such as EQS under zero repartitioning cost <ref> [13, 29, 19, 4] </ref>. They also show that FB-PWS and FB-ASP can substantially outperform EQS under an ad hoc model of repartitioning costs that is intended to illustrate the possible impact of repartitioning overheads on relative policy performance. <p> Finally, we consider an alternate speedup model [5, 16], that has been used widely in studies of scheduling policy performance <ref> [13, 17, 4, 20] </ref>: S j (n) = ffi + n We note that this function is a special case of equation (5) in which fi 0 = 0 and 0 = 1 + ffi Thus, the curves in Figure 2 (a) are also examples of the speedup function in equation <p> The EQS-PWS policy is identical to the EQS-AVG policy defined in <ref> [4] </ref>, except that the pws measure is used in place of average parallelism (avg). In [4], the EQS-AVG was found to have approximately the same performance as EQS, but new workload parameters are considered in this paper. <p> The EQS-PWS policy is identical to the EQS-AVG policy defined in <ref> [4] </ref>, except that the pws measure is used in place of average parallelism (avg). In [4], the EQS-AVG was found to have approximately the same performance as EQS, but new workload parameters are considered in this paper. Furthermore, figure 2 (b) shows that eliminating the allocations above pws processors at high load may be more favorable than eliminating allocations above avg processors.
Reference: [5] <author> L. W. Dowdy, </author> <title> On the Partitioning of Multiprocessor Systems. </title> <type> Technical Report, </type> <institution> Vanderbilt University, </institution> <month> July </month> <year> 1988. </year>
Reference-contexts: Finally, we consider an alternate speedup model <ref> [5, 16] </ref>, that has been used widely in studies of scheduling policy performance [13, 17, 4, 20]: S j (n) = ffi + n We note that this function is a special case of equation (5) in which fi 0 = 0 and 0 = 1 + ffi Thus, the curves
Reference: [6] <author> G. Edjlali, G. Agrawal, A. Sussman, J. Saltz, </author> <title> Data Parallel Programming in an Adaptive Environment. </title> <booktitle> Proc. 9th Int'l. Parallel Processing Symposium Santa Barbara, </booktitle> <address> CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: We assume that jobs are capable of adapting to changes in the number of processors that are allocated to them. Adaptive programming techniques and runtime support for program restructuring are active areas of research and appear to be feasible for both shared memory and message passing systems (e.g., <ref> [27, 20, 6] </ref>). Although job reconfiguration can involve substantial cost, particularly if massive data movement is required, the results in [6, 11] show that the benefit of better processor scheduling can outweigh the associated cost. This key issue is explored further for EQS and FB-PWS in section 4. <p> Adaptive programming techniques and runtime support for program restructuring are active areas of research and appear to be feasible for both shared memory and message passing systems (e.g., [27, 20, 6]). Although job reconfiguration can involve substantial cost, particularly if massive data movement is required, the results in <ref> [6, 11] </ref> show that the benefit of better processor scheduling can outweigh the associated cost. This key issue is explored further for EQS and FB-PWS in section 4.
Reference: [7] <author> M. J. Feeley, W. E. Morgan, F. H. Pighin, A. R. Karlin, H. M. Levy, C. A. Thekkath, </author> <title> Implementing Global Memory Management in a Workstation Cluster. </title> <booktitle> Proc. Symp. on Operating Systems Principles, </booktitle> <address> Copper Mountain, </address> <publisher> CO, </publisher> <pages> pp. 201-212, </pages> <month> December, </month> <year> 1995. </year>
Reference-contexts: This estimate was arrived at by computing the time to fetch 32 megabytes of data from a remote memory, either in a network of workstations that runs the GMS global memory management system <ref> [7] </ref> or in the KSR or DASH memory systems [20, 3]. In GMS, each remote fetch of an 8-kilobyte page requires 2 milliseconds. In KSR, it takes 30 milliseconds to fill a 256KB cache from remote memory [20].
Reference: [8] <author> D. G. Feitelson, B. Nitzberg, </author> <title> Job Characteristics of a Production Parallel Scientific Workload on the NASA Ames iPSC/860. </title> <booktitle> Proc. IPPS '95 Workshop on Job Scheduling Strategies for Parallel Systems, </booktitle> <address> Santa Barbara, CA, </address> <pages> pp. 337-360, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: First, typically only the jobs with small processing requirement have non-negligible fixed overhead, and these jobs tend to account for negligible amounts of total processor usage in parallel systems <ref> [8] </ref>. Second, the approximation will be imperceptible even for these jobs if the other parallelism overheads are non-negligible. Finally, due to the assumed linear increase in load imbalance with n in equation (4), ff has approximately the same impact on the shape of the speedup function as does 0 . <p> The approach of setting N j = M j also limits the types of correlations that can be specified among work, parallelism, and execution efficiency. For example, one cannot model both high correlation between work and job parallelism (as observed in <ref> [8] </ref>) but weak correlation between work and efficiency. This may be desirable because, for example, both small program development runs of highly efficient parallel codes, as well as large jobs with moderate communication overheads may be expected in a parallel system of interest. <p> The distributions of job parallelism that will be used to compare policy performance are illustrated in Figure 4. These distributions are motivated by the variation in job parallelism reported for daytime user jobs on the iPSC/860 machine at NASA Ames <ref> [8] </ref>, and also on the SP/2 machine at the Cornell Theory Center [10]. <p> In this class of jobs, one can perhaps expect the probability to decrease as the parallelism increases. We note that one discrepancy between this model and the data in <ref> [8] </ref> is that parallelism equal to two has lower probability in the measured system than suggested by the bounded geometric. This type of discrepancy shouldn't have great impact on the relative policy performance comparisons in this paper. <p> This type of discrepancy shouldn't have great impact on the relative policy performance comparisons in this paper. Finally, the parameter N fl is included to model the preferred parallelism equal to 32 in the measured iPSC/860 and SP/2 workloads <ref> [8, 10] </ref>. 3 Note that the restriction N j M j assumes that either users are sophisticated enough not to request more than M j processors (because the job execution time will be longer), or self-tuning [20] is used to achieve same result. <p> If mean service requirement is correlated with the square of job parallelism, then the multiplier for W is replaced by n 2 N 2 . The reported measures of W jjn vs. n for the iPSC/860 workloads at NASA Ames <ref> [8] </ref> lie between these two cases. 3.2.4 Correlation Between Workload Parameters In some experiments, we will assume that execution overheads are on average lower for jobs with larger total service requirement. In these cases, we will use the same notation as in equation (11). <p> However, larger W j does not necessarily imply larger pws or higher efficiency. Due to the correlation between mean service requirement and N j 2 , jobs with N j 32 account for 98% of the system resource usage by this workload, in agreement with system measurements in <ref> [8] </ref>. For the workload in Figure 6 (a) ( 0 = 0), FB-PWS is comparable to the EQS policy throughout the entire range of offered load. <p> In this case we use a new synthetic workload one that corresponds closely with the workload measurement data in <ref> [8] </ref>. Below we explain the workload, the repartitioning overheads, and the results. The key features of the workload are: * Distribution 4 of Figure 4 is used for job parallelism. This was derived from the measured distribution in [8], by shifting some of the probability mass for n = 32 to <p> synthetic workload one that corresponds closely with the workload measurement data in <ref> [8] </ref>. Below we explain the workload, the repartitioning overheads, and the results. The key features of the workload are: * Distribution 4 of Figure 4 is used for job parallelism. This was derived from the measured distribution in [8], by shifting some of the probability mass for n = 32 to higher values of parallelism. The shift corrects for the incentives in the measured system for parallelism equal to 32 during the daytime. <p> In a system with EQS or FB-PWS scheduling, such incentives would not be necessary. * The service requirement is determined by a set of values for W jjn and C W jn , n = 1; 2; 4; 8; :::; 128, that are computed from Table 2 in <ref> [8] </ref>. <p> The ratio for FB-PWS with zero repartitioning overhead is also given. Job arrival rate is varied up to 30 jobs/hour, which is higher than observed on the NASA Ames iPSC/860 (Figure 12 of <ref> [8] </ref>) or the Cornell Theory Center SP/2 [10]. The system utilization at arrival rate of 30 jobs/hour is 82%-85%. <p> Figure 10 (b) shows that this result holds even if the coefficient of variation of interarrival times is increased to three by using a two-stage hyper-exponential distribution of interarrival times, reflecting the measured coefficient of variation in <ref> [8] </ref>. To see what would happen if arrival rate is doubled to 60/hour, we halved each of the values of W jjn and re-ran the experiment. The results are shown in system performs full repartitioning at most once per every 500 second quantum. <p> when realistic data repartitioning overheads are considered, yet the FB-PWS policy is still a remarkably competitive alternative over a wide range of workloads. 5 Conclusions In this paper, we have compared the EQS and FB-PWS policies under synthetic workloads that have not previously been considered, yet have realistic job characteristics <ref> [8, 10] </ref> and repartitioning overheads. As part of this effort, we have improved the previous workload models in [13, 17, 23] and we have shown how the different speedup functions used in the previous models are related. <p> A key feature of our realistic workloads is that job service requirements are substantial enough to warrant execution on a parallel system, and thus job arrival rate is at most 3060 jobs/hour <ref> [8, 10] </ref>.
Reference: [9] <author> D. Ghosal, G. Serazzi, S. Tripathi, </author> <title> The Processor Working Set and Its Use in Scheduling Multiprocessor Systems. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> Vol. 17, No. 5, </volume> <pages> pp. 443-453, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: processor allocation policy, FB-PWS, that has the following characteristics: load-adaptability: the number of processors allocated to a newly arriving job decreases as the number of jobs in the system increases, processor working set (pws): as load increases, the allocation for a newly arriving job is proportional to it's pws measure <ref> [9] </ref>, where pws is the num fl This research was partially supported by the National Science Foundation under grants CCR-9024144, CDA-9024618, and GER-9550429. ber of processors that permit the job to run at approximately the knee of its execution-time vs efficiency profile [15] 1 , Multilevel-Feedback (FB): in each quantum, priority
Reference: [10] <author> S. Hotovy, </author> <title> Workload Evolution on the Cornell Theory Center IBM SP2. </title> <booktitle> Proc. IPPS '96 Workshop on Job Scheduling Strategies for Parallel Systems, </booktitle> <address> Honolulu, Hawaii, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: These distributions are motivated by the variation in job parallelism reported for daytime user jobs on the iPSC/860 machine at NASA Ames [8], and also on the SP/2 machine at the Cornell Theory Center <ref> [10] </ref>. <p> This type of discrepancy shouldn't have great impact on the relative policy performance comparisons in this paper. Finally, the parameter N fl is included to model the preferred parallelism equal to 32 in the measured iPSC/860 and SP/2 workloads <ref> [8, 10] </ref>. 3 Note that the restriction N j M j assumes that either users are sophisticated enough not to request more than M j processors (because the job execution time will be longer), or self-tuning [20] is used to achieve same result. <p> The ratio for FB-PWS with zero repartitioning overhead is also given. Job arrival rate is varied up to 30 jobs/hour, which is higher than observed on the NASA Ames iPSC/860 (Figure 12 of [8]) or the Cornell Theory Center SP/2 <ref> [10] </ref>. The system utilization at arrival rate of 30 jobs/hour is 82%-85%. <p> when realistic data repartitioning overheads are considered, yet the FB-PWS policy is still a remarkably competitive alternative over a wide range of workloads. 5 Conclusions In this paper, we have compared the EQS and FB-PWS policies under synthetic workloads that have not previously been considered, yet have realistic job characteristics <ref> [8, 10] </ref> and repartitioning overheads. As part of this effort, we have improved the previous workload models in [13, 17, 23] and we have shown how the different speedup functions used in the previous models are related. <p> A key feature of our realistic workloads is that job service requirements are substantial enough to warrant execution on a parallel system, and thus job arrival rate is at most 3060 jobs/hour <ref> [8, 10] </ref>.
Reference: [11] <author> N. Islam, A. Prodromidis, M. S. Squillante, </author> <title> Dynamic Partitioning in Different Distributed-Memory Environments. </title> <booktitle> Proc. IPPS '96 Workshop on Job Scheduling Strategies for Parallel Systems, </booktitle> <address> Honolulu, Hawaii, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Adaptive programming techniques and runtime support for program restructuring are active areas of research and appear to be feasible for both shared memory and message passing systems (e.g., [27, 20, 6]). Although job reconfiguration can involve substantial cost, particularly if massive data movement is required, the results in <ref> [6, 11] </ref> show that the benefit of better processor scheduling can outweigh the associated cost. This key issue is explored further for EQS and FB-PWS in section 4.
Reference: [12] <author> L. Kleinrock. </author> <title> Queueing Systems, Vol II: Applications. </title> <publisher> John Wiley & Sons, </publisher> <year> 1976. </year>
Reference-contexts: For example, the EQS policy like the processor sharing policy for uniprocessor systems, has the key property that expected response time is proportional to the job's service requirement. This property is called "fairness" in <ref> [12] </ref>. We also note that the FB-PWS policy has reduced potential for starvation of large jobs as compared with the FB policy for uniprocessor systems, due to it's load-adaptive space-sharing property.
Reference: [13] <author> S. T. Leutenegger, M. K. Vernon, </author> <title> The Performance of Multiprogrammed Multiprocessor Scheduling Policies. </title> <booktitle> Proceedings of the ACM SIGMETRICS Conference on Measurement & Modeling of Computer Systems, </booktitle> <address> Boulder, </address> <publisher> CO, </publisher> <pages> pp. 226-236, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This is an impressive result since FB-PWS commits to a processor allocation at job arrival time. Previous static allocation policies have generally not been competitive with dynamic policies such as EQS under zero repartitioning cost <ref> [13, 29, 19, 4] </ref>. They also show that FB-PWS and FB-ASP can substantially outperform EQS under an ad hoc model of repartitioning costs that is intended to illustrate the possible impact of repartitioning overheads on relative policy performance. <p> This allows efficient jobs with long service times to execute on a reasonable number of processors after shorter jobs have departed. 2.2 EQS The spatial equipartitioning policy, EQS, was initially proposed by Tucker and Gupta in [27] and has been evaluated and/or refined in many subsequent studies (e.g., <ref> [13, 18, 16] </ref>). When a job arrival or departure occurs, the processors are dynamically reallocated so that each job has an equal fraction of the processors unless a job has smaller maximum parallelism than the equipartition value. <p> Finally, we consider an alternate speedup model [5, 16], that has been used widely in studies of scheduling policy performance <ref> [13, 17, 4, 20] </ref>: S j (n) = ffi + n We note that this function is a special case of equation (5) in which fi 0 = 0 and 0 = 1 + ffi Thus, the curves in Figure 2 (a) are also examples of the speedup function in equation <p> The distributions in Figure 4 were generated from a parameterized bounded geometric distribution of job parallelism that is adapted from prior work <ref> [13, 17] </ref> and has the following four parameters: N max the maximum value for job parallelism, P N max the probability that an arriving job has parallelism equal to N max , p the parameter of the bounded geometric distribution of parallelism for all other jobs, and N fl N max <p> As part of this effort, we have improved the previous workload models in <ref> [13, 17, 23] </ref> and we have shown how the different speedup functions used in the previous models are related. Finally, we have defined a new policy, EQS-PWS, which has what appear to be the most promising characteristics of both EQS and FB-PWS.
Reference: [14] <author> S. Majumdar, D. L. Eager, R. B. Bunt, </author> <title> Scheduling in Multiprogrammed Parallel Systems. </title> <booktitle> Proc. 1988 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Santa Fe, NM, </address> <pages> pp. 104-113, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: specify mean service requirement proportional to job parallelism, W jjn has a two-stage hyperexponential distribution with mean n N W , and coefficient of variation called C W jn ; i.e., W jjn ~ Hyperexp 2 ( n W ; C W jn ): (11) This model was proposed in <ref> [14] </ref> and formalized in [16]. If mean service requirement is correlated with the square of job parallelism, then the multiplier for W is replaced by n 2 N 2 .
Reference: [15] <author> S. Majumdar, D. Eager, and R. Bunt. </author> <title> Characterisation of programs for scheduling in multiprogrammed parallel systems. </title> <journal> Performance Evaluation, </journal> <volume> Vol. 13, </volume> <pages> pp. 109-130, </pages> <year> 1991. </year>
Reference-contexts: arriving job is proportional to it's pws measure [9], where pws is the num fl This research was partially supported by the National Science Foundation under grants CCR-9024144, CDA-9024618, and GER-9550429. ber of processors that permit the job to run at approximately the knee of its execution-time vs efficiency profile <ref> [15] </ref> 1 , Multilevel-Feedback (FB): in each quantum, priority is given to the jobs that have so far received least service, infrequent repartitioning: In each quantum, at most one job runs on a smaller number processors than it's initial allocation (and those processors would otherwise be idle).
Reference: [16] <author> R. Mansharamani. </author> <title> Efficient Analysis of Parallel Processor Scheduling Policies. </title> <type> Ph.D. Thesis, </type> <institution> Computer Sciences Dept., University of Wisconsin, Madison, WI, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: This allows efficient jobs with long service times to execute on a reasonable number of processors after shorter jobs have departed. 2.2 EQS The spatial equipartitioning policy, EQS, was initially proposed by Tucker and Gupta in [27] and has been evaluated and/or refined in many subsequent studies (e.g., <ref> [13, 18, 16] </ref>). When a job arrival or departure occurs, the processors are dynamically reallocated so that each job has an equal fraction of the processors unless a job has smaller maximum parallelism than the equipartition value. <p> Finally, we consider an alternate speedup model <ref> [5, 16] </ref>, that has been used widely in studies of scheduling policy performance [13, 17, 4, 20]: S j (n) = ffi + n We note that this function is a special case of equation (5) in which fi 0 = 0 and 0 = 1 + ffi Thus, the curves <p> proportional to job parallelism, W jjn has a two-stage hyperexponential distribution with mean n N W , and coefficient of variation called C W jn ; i.e., W jjn ~ Hyperexp 2 ( n W ; C W jn ): (11) This model was proposed in [14] and formalized in <ref> [16] </ref>. If mean service requirement is correlated with the square of job parallelism, then the multiplier for W is replaced by n 2 N 2 .
Reference: [17] <author> R. K. Mansharamani, M. K. Vernon, </author> <title> Properties of the EQS Parallel Processor Allocation Policy. </title> <type> Technical Report #1192, </type> <institution> Univ. of Wisconsin - Madison Computer Sciences Dept., </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Finally, we consider an alternate speedup model [5, 16], that has been used widely in studies of scheduling policy performance <ref> [13, 17, 4, 20] </ref>: S j (n) = ffi + n We note that this function is a special case of equation (5) in which fi 0 = 0 and 0 = 1 + ffi Thus, the curves in Figure 2 (a) are also examples of the speedup function in equation <p> The distributions in Figure 4 were generated from a parameterized bounded geometric distribution of job parallelism that is adapted from prior work <ref> [13, 17] </ref> and has the following four parameters: N max the maximum value for job parallelism, P N max the probability that an arriving job has parallelism equal to N max , p the parameter of the bounded geometric distribution of parallelism for all other jobs, and N fl N max <p> The execution overhead parameters must also satisfy the constraint in equation (10), which guarantees that the job's speedup function is non-decreasing up to N j processors. This workload model is very similar to the previous workload model in <ref> [17] </ref>. <p> As part of this effort, we have improved the previous workload models in <ref> [13, 17, 23] </ref> and we have shown how the different speedup functions used in the previous models are related. Finally, we have defined a new policy, EQS-PWS, which has what appear to be the most promising characteristics of both EQS and FB-PWS.
Reference: [18] <author> C. McCann, R. Vaswani, J. Zahorjan, </author> <title> A Dynamic Processor Allocation Policy for Multiprogrammed, Shared Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 11, No. 2, </volume> <pages> pp. 146-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: This allows efficient jobs with long service times to execute on a reasonable number of processors after shorter jobs have departed. 2.2 EQS The spatial equipartitioning policy, EQS, was initially proposed by Tucker and Gupta in [27] and has been evaluated and/or refined in many subsequent studies (e.g., <ref> [13, 18, 16] </ref>). When a job arrival or departure occurs, the processors are dynamically reallocated so that each job has an equal fraction of the processors unless a job has smaller maximum parallelism than the equipartition value.
Reference: [19] <author> V.Naik, S. Setia, and M. Squillante. </author> <title> Performance Analysis of Job Scheduling Policies in Parallel Supercomputing Environments. </title> <booktitle> Proceedings of Supercomputing'93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: This is an impressive result since FB-PWS commits to a processor allocation at job arrival time. Previous static allocation policies have generally not been competitive with dynamic policies such as EQS under zero repartitioning cost <ref> [13, 29, 19, 4] </ref>. They also show that FB-PWS and FB-ASP can substantially outperform EQS under an ad hoc model of repartitioning costs that is intended to illustrate the possible impact of repartitioning overheads on relative policy performance.
Reference: [20] <author> T. D. Nguyen, R. Vaswani, J. Zahorjan, </author> <title> Using Runtime Measured Workload Characteristics in Parallel Processor Scheduling. </title> <booktitle> Proc. IPPS '96 Workshop on Job Scheduling Strategies for Parallel Systems, </booktitle> <address> Honolulu, Hawaii, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: We assume that jobs are capable of adapting to changes in the number of processors that are allocated to them. Adaptive programming techniques and runtime support for program restructuring are active areas of research and appear to be feasible for both shared memory and message passing systems (e.g., <ref> [27, 20, 6] </ref>). Although job reconfiguration can involve substantial cost, particularly if massive data movement is required, the results in [6, 11] show that the benefit of better processor scheduling can outweigh the associated cost. This key issue is explored further for EQS and FB-PWS in section 4. <p> the maximum number of processing nodes that each job can make productive use of, either because this information is specified when the job is submitted or because the system is capable of determining this information at runtime using methods such as the self-tuning approach recently proposed by Nguyen et. al. <ref> [20] </ref>. Similarly, for the FB-PWS policy (or the EQS-PWS policy yet to be defined), we assume that the system is capable of knowing the pws measure for each job, perhaps from runtime estimation techniques similar to those described in [20]. <p> such as the self-tuning approach recently proposed by Nguyen et. al. <ref> [20] </ref>. Similarly, for the FB-PWS policy (or the EQS-PWS policy yet to be defined), we assume that the system is capable of knowing the pws measure for each job, perhaps from runtime estimation techniques similar to those described in [20]. In the remainder of the paper we assume the set of processing nodes are dedicated to servicing a parallel workload. One might also imagine that the nodes are a set of currently idle nodes in a non-dedicated network of workstations (NOW) that is available for serving large (parallel) jobs. <p> Finally, we consider an alternate speedup model [5, 16], that has been used widely in studies of scheduling policy performance <ref> [13, 17, 4, 20] </ref>: S j (n) = ffi + n We note that this function is a special case of equation (5) in which fi 0 = 0 and 0 = 1 + ffi Thus, the curves in Figure 2 (a) are also examples of the speedup function in equation <p> model the preferred parallelism equal to 32 in the measured iPSC/860 and SP/2 workloads [8, 10]. 3 Note that the restriction N j M j assumes that either users are sophisticated enough not to request more than M j processors (because the job execution time will be longer), or self-tuning <ref> [20] </ref> is used to achieve same result. <p> This estimate was arrived at by computing the time to fetch 32 megabytes of data from a remote memory, either in a network of workstations that runs the GMS global memory management system [7] or in the KSR or DASH memory systems <ref> [20, 3] </ref>. In GMS, each remote fetch of an 8-kilobyte page requires 2 milliseconds. In KSR, it takes 30 milliseconds to fill a 256KB cache from remote memory [20]. In DASH, each remote fetch of a 16-byte cache block requires approximately 170 cycles on a 33 MHz processor. <p> In GMS, each remote fetch of an 8-kilobyte page requires 2 milliseconds. In KSR, it takes 30 milliseconds to fill a 256KB cache from remote memory <ref> [20] </ref>. In DASH, each remote fetch of a 16-byte cache block requires approximately 170 cycles on a 33 MHz processor. Thus, the transfer of 32 megabytes requires approximately 4-10 seconds in these systems. <p> (1) quantifying the workload characteristics that lead to differences in relative mean response times of the policies, (2) examination of more detailed measures such as expected response time conditioned on job service requirement, (3) how well the pws measure can be estimated at runtime using techniques similar to those in <ref> [20] </ref>, and (4) suitable modifications to the policies to support jobs with large memory requirements. Acknowledgements The authors gratefully acknowledge comments and suggestions by Thu Nguyen, John Zahorjan, other workshop participants, and the anonymous reviewers, which helped to improve this paper.
Reference: [21] <author> J. K. Ousterhout, </author> <title> Scheduling Techniques for Concurrent Systems, </title> <booktitle> Proc. 3rd Int'l. Conf. on Distributed Computing Systems. </booktitle> <pages> pp. 22-30, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: 1 Introduction In static quantum-based parallel processor allocation policies, each job is configured for a static number of processors and timesharing is used to share the processors among the jobs. Ousterhout's coscheduling policies <ref> [21] </ref> are examples of this class of policies. Semi-static quantum-based policies allow limited changes in processor allocations as system load changes. Such policies greatly reduce the frequency of job reconfiguration (which can involve significant data repartitioning overheads) as compared with dynamic policies such as the spatial equipartitioning (EQS) policy.
Reference: [22] <author> J. D. Padhye, L. W. Dowdy, </author> <title> Dynamic versus Adaptive Processor Allocation Policies for Message Passing Parallel Computers: An Empirical Comparison. </title> <booktitle> Proc. IPPS '96 Workshop on Job Scheduling Strategies for Parallel Systems, </booktitle> <address> Honolulu, Hawaii, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: The distributions for the execution overhead parameters will be explained as each experiment is introduced; these distributions are motivated by comparisons with earlier results [23] or by speedups that are encountered in practice (e.g., <ref> [24, 22] </ref>). Arrivals are assumed to be Poisson unless otherwise specified, and the system size (P ) is 128 processing nodes in all experiments.
Reference: [23] <author> E. W. Parsons, K. C. Sevcik, </author> <title> Multiprocessor Scheduling for High-Variability Service Time Distributions. </title> <booktitle> Proc. IPPS '95 Workshop on Job Scheduling Strategies for Parallel Systems Santa Barbara, </booktitle> <address> CA, </address> <pages> pp. 127-145, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Semi-static quantum-based policies allow limited changes in processor allocations as system load changes. Such policies greatly reduce the frequency of job reconfiguration (which can involve significant data repartitioning overheads) as compared with dynamic policies such as the spatial equipartitioning (EQS) policy. In a recent paper <ref> [23] </ref> Parsons and Sevcik have proposed a new semi-static quantum-based parallel processor allocation policy, FB-PWS, that has the following characteristics: load-adaptability: the number of processors allocated to a newly arriving job decreases as the number of jobs in the system increases, processor working set (pws): as load increases, the allocation for <p> In this paper, we further investigate the relative performance of EQS and FB-PWS. First, we examine how idealized EQS and FB-PWS compare for synthetic workloads that are not considered in <ref> [23] </ref>, but that are designed to represent parallel workloads encountered in practice. An improved approach to representing application speedup characteristics is developed as part of this effort (see section 3). Second, we assess whether specific features of the FB-PWS policy might be incorporated in the EQS policy to improve performance. <p> Section 5 contains the conclusions of this work. 2 Policy Definitions The FB-PWS, FB-ASP, and EQS policies considered initially in this paper are each defined below. 2.1 FB-PWS and FB-ASP The FB-PWS policy is proposed and clearly defined in <ref> [23] </ref>. The brief definition is repeated here for the sake of reader convenience. <p> illustrated in Figure 2 (a). (a) Speedup Function of Equation (2) W j ~ Hyperexp 2 (1000; C W ), =1.3, ff=25, fi=25 (b) Speedup Function of Equation (5) 0 ~ Uniform (0; 1%), fi 0 ~ Uniform (0; 0:1%) 3.2.2 Job Parallelism One approach to generating a synthetic workload <ref> [23] </ref> is to specify the distributions of the parameters that characterize the job speedups, and then to let job parallelism, N j , be equal to the point at which the speedup function is maximized, M j . <p> The discussion is focussed on comparisons between the FB-PWS, EQS, and EQS-PWS policies, although results for the FB-ASP policy <ref> [23] </ref> are also provided in the figures for the sake of completeness. The four parameters that characterize each job in the synthetic workloads are summarized in section 3.2.5. <p> The distribution of job parallelism, N j , will be one of the four distributions depicted and numbered in Figure 4, depending on the experiment. The distributions for the execution overhead parameters will be explained as each experiment is introduced; these distributions are motivated by comparisons with earlier results <ref> [23] </ref> or by speedups that are encountered in practice (e.g., [24, 22]). Arrivals are assumed to be Poisson unless otherwise specified, and the system size (P ) is 128 processing nodes in all experiments. <p> Finally, Section 4.3 compares the three policies under a case of realistic partitioning overheads. 4.1 Comparisons under Zero Repartitioning Cost We first compare the EQS and FB-PWS under a workload that is nearly identical to Workload 2 in <ref> [23] </ref>. That is, we use the speedup function in equation 5, set the overhead parameters and distribution of W j as given in Figure 5, and let N j = M j . <p> When = 0:9, system utilization for FB-PWS or EQS is in the range of 92% - 99%. The response time ratio for FB-ASP is also given for completeness, as noted above. Similar to the results reported in <ref> [23] </ref>, these results show that FB-PWS is competitive with EQS throughout the range of offered load, and also that the system with the EQS policy saturates at a slightly earlier point than the FB-PWS system. 4 The workload in Figure 5 has a distribution for N j that is very similar <p> Thus, nearly all of the jobs have parallelism &lt; 10, and a negligible fraction of jobs have parallelism greater than 50. Furthermore, in this workload, larger W j implies larger pws and larger pws implies higher effi 4 We also reproduced several of the graphs in <ref> [23] </ref>, not shown in this paper, to validate that we have correctly implemented the policy simulations. (a) C W = 5 (b) C W = 30 W j ~ Hyperexp 2 (1000; C W ); N j = M j ; 0 = 0:003; ff = 25; fi = 25 ciency <p> As part of this effort, we have improved the previous workload models in <ref> [13, 17, 23] </ref> and we have shown how the different speedup functions used in the previous models are related. Finally, we have defined a new policy, EQS-PWS, which has what appear to be the most promising characteristics of both EQS and FB-PWS.
Reference: [24] <author> V. G. J. Peris, M. S. Squillante, V. K. Naik, </author> <title> Analysis of the Impact of Memory in Distributed Parallel Processing Systems. </title> <booktitle> Proc. 1994 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Nashville, TN, </address> <pages> pp. 5-18, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The distributions for the execution overhead parameters will be explained as each experiment is introduced; these distributions are motivated by comparisons with earlier results [23] or by speedups that are encountered in practice (e.g., <ref> [24, 22] </ref>). Arrivals are assumed to be Poisson unless otherwise specified, and the system size (P ) is 128 processing nodes in all experiments.
Reference: [25] <author> K. C. Sevcik, </author> <title> Characterizations of Parallelism in Applications and Their Use in Scheduling. </title> <booktitle> Proc. 1989 ACM SIGMETRICS/Performance '89 Int'l. Conf. on Measurement and Modeling of Computer Systems, </booktitle> <address> Berkeley, CA, </address> <pages> pp. 171-180, </pages> <month> May </month> <year> 1989. </year>
Reference: [26] <author> K. C. Sevcik, </author> <title> Application Scheduling and Processor Allocation in Multi-programmed Parallel Processing Systems. </title> <journal> Performance Evaluation, </journal> <volume> Vol. 19, No. 2/3, </volume> <pages> pp. 107-140, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: We then describe the characterization of job parallelism (section 3.2.2), job service requirement (section 3.2.3), and correlation among the various model parameters (section 3.2.4). The workload model is summarized in section 3.2.5. 3.2.1 Job Execution Overheads One possible functional form for job execution time, proposed in <ref> [26] </ref>, is defined as follows: T j (n) = n where - n is the number of processors allocated to the job, is an inflation factor that models load imbalance in the computation, - ff represents fixed overheads such as per-processor initialization, and - fi represents communication overhead, which increases with
Reference: [27] <author> A. Tucker, A. Gupta, </author> <title> Process Control and Scheduling Issues for Multipro--grammed Shared-Memory Multiprocessors. </title> <booktitle> Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pp. 159-166, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: This allows efficient jobs with long service times to execute on a reasonable number of processors after shorter jobs have departed. 2.2 EQS The spatial equipartitioning policy, EQS, was initially proposed by Tucker and Gupta in <ref> [27] </ref> and has been evaluated and/or refined in many subsequent studies (e.g., [13, 18, 16]). When a job arrival or departure occurs, the processors are dynamically reallocated so that each job has an equal fraction of the processors unless a job has smaller maximum parallelism than the equipartition value. <p> We assume that jobs are capable of adapting to changes in the number of processors that are allocated to them. Adaptive programming techniques and runtime support for program restructuring are active areas of research and appear to be feasible for both shared memory and message passing systems (e.g., <ref> [27, 20, 6] </ref>). Although job reconfiguration can involve substantial cost, particularly if massive data movement is required, the results in [6, 11] show that the benefit of better processor scheduling can outweigh the associated cost. This key issue is explored further for EQS and FB-PWS in section 4.
Reference: [28] <author> C.-S. Wu, </author> <title> Processor Scheduling in Multiprogrammed Shared Memory NUMA Multiprocessors, </title> <type> Master's thesis, </type> <institution> University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: j , is: M j = W j 1 if fi = 0: The parameters of equation (1) correspond to overheads that are observed in practice, and the equation has been shown to match well with measured speedup functions if , ff, and fi are adjusted to yield best fit <ref> [28] </ref>. Below we propose a few modifications to the equation that improve its intuitive appeal and are needed for our study. We then point out some important characteristics of the revised speedup function.
Reference: [29] <author> J. Zahorjan, C. McCann, </author> <title> Processor Scheduling in Shared Memory Multiprocessors. </title> <booktitle> Proc. 1990 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Boulder, </address> <publisher> CO, </publisher> <pages> pp. 214-225, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This is an impressive result since FB-PWS commits to a processor allocation at job arrival time. Previous static allocation policies have generally not been competitive with dynamic policies such as EQS under zero repartitioning cost <ref> [13, 29, 19, 4] </ref>. They also show that FB-PWS and FB-ASP can substantially outperform EQS under an ad hoc model of repartitioning costs that is intended to illustrate the possible impact of repartitioning overheads on relative policy performance.
Reference: [30] <author> S. Zhou, J. Wang, X. Zheng, P. Delisle, </author> <title> Utopia: A Load Sharing Facility for Large Heterogeneous Distributed Computing Systems. </title> <type> Technical Report, </type> <institution> University of Toronto, </institution> <year> 1992. </year>
Reference-contexts: Such a system requires an effective policy for recruiting idle nodes as well as efficient mechanisms for migrating the processes of parallel jobs away from nodes that are preempted by a higher priority user <ref> [2, 30, 1] </ref>. Although we do not consider the impact of node interruptions nor particular policy customizations that might be needed, we consider synthetic workloads and repartitioning overheads that are relevant to such environments.
References-found: 30

