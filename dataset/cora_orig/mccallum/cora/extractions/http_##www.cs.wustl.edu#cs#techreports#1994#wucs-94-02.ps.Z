URL: http://www.cs.wustl.edu/cs/techreports/1994/wucs-94-02.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Email: mfrazier@uiuc.edu  sg@cs.wustl.edu  nmishra@uiuc.edu  pitt@cs.uiuc.edu  
Title: Learning From a Consistently Ignorant Teacher  
Author: Michael Frazier Sally Goldman Nina Mishra Leonard Pitt 
Note: "Consistency requires you to be as ignorant today as you were a year ago." Bernard Berenson (1865-1959) Supported in part by NSF Grant IRI-9014840, and by NASA grant NAG 1-613. Supported in part by NSF Grant CCR-9110108 and an NSF NYI Grant CCR-9357707. Supported in part by NSF Grant IRI-9014840.  
Date: May 12, 1994  
Address: Urbana, IL 61801  St. Louis, MO 63130  Urbana, IL 61801  Urbana, IL 61801  
Affiliation: Dept. of Computer Science University of Illinois  Dept. of Computer Science Washington University  Dept. of Computer Science University of Illinois  Dept. of Computer Science University of Illinois  
Pubnum: WUCS-94-02  
Abstract: One view of computational learning theory is that of a learner acquiring the knowledge of a teacher. We introduce a formal model of learning capturing the idea that teachers may have gaps in their knowledge. The goal of the learner is still to acquire the knowledge of the teacher, but now the learner must also identify the gaps. This is the notion of learning from a consistently ignorant teacher. We consider the impact of knowledge gaps on learning, for example, monotone DNF and d-dimensional boxes, and show that learning is still possible. Negatively, we show that knowledge gaps make learning conjunctions of Horn clauses as hard as learning DNF. We also present general results describing when known learning algorithms can be used to obtain learning algorithms using a consistently ignorant teacher. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Howard Aizenstein, Lisa Hellerstein, and Leonard Pitt. </author> <title> Read-thrice DNF is hard to learn with membership and equivalence queries. </title> <booktitle> In 33nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 523-532, </pages> <month> October </month> <year> 1992. </year>
Reference: [2] <author> Howard Aizenstein and Leonard Pitt. </author> <title> Exact learning of read-twice DNF formulas. </title> <booktitle> In 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 170-179, </pages> <month> October </month> <year> 1991. </year>
Reference: [3] <author> Dana Angluin. </author> <title> Learning k-term DNF formulas using queries and counterexamples. </title> <type> Technical Report YALEU/DCS/RR-559, </type> <institution> Yale University, </institution> <month> August </month> <year> 1987. </year>
Reference: [4] <author> Dana Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year> <month> 26 </month>
Reference-contexts: We now consider learning agreements of at most a constant number k of concepts from a class C. Let C ? (k) = fAgree F j F C; jF j kg. Applying the known learning results for monotone DNF [41] and DNF formulas with a constant number of terms <ref> [4, 14] </ref> and the known learning results for decision trees [16] and DFAs [4], we obtain the following corollary. (The corollary follows because the intersection and union of a constant number of concepts from each of the preceding classes can be represented by a single concept in the corresponding class that <p> Let C ? (k) = fAgree F j F C; jF j kg. Applying the known learning results for monotone DNF [41] and DNF formulas with a constant number of terms [4, 14] and the known learning results for decision trees [16] and DFAs <ref> [4] </ref>, we obtain the following corollary. (The corollary follows because the intersection and union of a constant number of concepts from each of the preceding classes can be represented by a single concept in the corresponding class that is at most polynomially larger.) Corollary 6 Let C be the class of
Reference: [5] <author> Dana Angluin. </author> <title> Negative results for equivalence queries. </title> <booktitle> Machine Learning, </booktitle> <address> 5:121--150, </address> <year> 1990. </year>
Reference: [6] <author> Dana Angluin. </author> <title> Exact learning of -DNF formulas with malicious membership queries. </title> <type> Technical Report YALEU/DCS/TR-1020, </type> <institution> Yale University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Indeed, they note that their algorithm for learning monotone DNF with an incomplete membership oracle can be used to learn monotone DNF with random 1 ! 0 one-sided errors. 7 Krikis [10], and Angluin <ref> [6] </ref> consider learning with a bounded number of such erroneous responses, and Frazier and Pitt [23] consider learning when such incorrect responses occur randomly with probability at most 1 2 . In other related work, Kearns and Schapire [32] generalized the PAC setting to non-binary values using Haussler's framework [28].
Reference: [7] <author> Dana Angluin, Michael Frazier, and Leonard Pitt. </author> <title> Learning conjunctions of Horn clauses. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 147-164, </pages> <year> 1992. </year>
Reference-contexts: Finally, it is easily shown that the time complexity is polynomial in the sample complexity. 4 A Negative Result The class of conjunctions of Horn clauses (Horn sentences) is known to be PAC-MEMB-learnable <ref> [7] </ref>. Furthermore, Frazier and Pitt [22] have shown that Horn sentences are efficiently learnable using membership and equivalence queries from a different model in which entailed examples are provided. We provide evidence that this result cannot be strengthened to allow learning blurry 22 LearnBoxesAgreement () 1.
Reference: [8] <author> Dana Angluin, Lisa Hellerstein, and Marek Karpinski. </author> <title> Learning read-once formulas with queries. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 40(1) </volume> <pages> 185-210, </pages> <year> 1993. </year>
Reference-contexts: are sufficient to construct a PAC-MEMB algorithm to learn the class of blurry monomials (for which P is nonempty): run a known algorithm for learning (nonblurry) monomials [41] to learn the set P of positive examples, and at the same time run a known learning algorithm for (nonblurry) unate DNF <ref> [8] </ref> to learn the set P [ Q of nonnegative examples. Then Q and N can be easily determined from knowledge of P and P [ Q. (See Corollary 5 for more details.) Is this an efficient learning algorithm? It depends on our choice of complexity parameters. <p> Thus Union F is a unate DNF formula, that is efficiently PAC-MEMB learnable <ref> [8] </ref>.
Reference: [9] <author> Dana Angluin and Michael Kharitonov. </author> <booktitle> When won't membership queries help? In Proceedings of the Twenty Third Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 444-454, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Theorem 11 PAC-MEMB learning the agreement of Horn sentences for which the intersection region is samplable is as hard as PAC-MEMB learning the class of DNF formulas. Proof: We prove this through a sequence of prediction preserving reductions <ref> [9, 37] </ref>. Let DHF-1pos be the class of DHF formulas with exactly one positive example p that satisfies every disjunct. Let agree-Horn-1pos be the agreement of Horn sentences that have exactly one example in their intersection. <p> Thus it follows from this sequence of reductions that PAC-MEMB learning the agreement of Horn sentences with a samplable positive region is as hard as PAC-MEMB learning the class of DNF formulas. Finally, we strengthen this result by using the hardness result of Angluin and Kharitonov <ref> [9] </ref> which shows, under the assumption that public key encryption is secure, membership queries do not help in learning DNF formulas (with an unbounded number of terms).
Reference: [10] <author> Dana Angluin and Martin Krikis. </author> <title> Learning with malicious membership queries and exceptions. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Computational Learning Theory, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Indeed, they note that their algorithm for learning monotone DNF with an incomplete membership oracle can be used to learn monotone DNF with random 1 ! 0 one-sided errors. 7 Krikis <ref> [10] </ref>, and Angluin [6] consider learning with a bounded number of such erroneous responses, and Frazier and Pitt [23] consider learning when such incorrect responses occur randomly with probability at most 1 2 .
Reference: [11] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples <ref> [11, 33, 40, 30] </ref> and some addressing the issue of noise in the attributes [39, 26, 34]. 6 In these situations, the border between the positive and negative examples may ap-pear blurry to the learner, but this is just the result of the noise process that has been applied to the
Reference: [12] <author> Dana Angluin and Donna K. </author> <title> Slonim. Randomly fallible teachers: Learning monotone DNF with an incomplete membership oracle Machine Learning, </title> <booktitle> 14 </booktitle> <pages> 7-26, </pages> <year> 1994. </year>
Reference-contexts: There has also been some work considering learning from noisy membership queries [25, 38]. Angluin and Slonim <ref> [12] </ref> introduced a model of incomplete membership queries in which each membership query is answered "don't know" with a given probability. Furthermore, this information is persistent|repeatedly making a query that was answered "don't know" always results in a "don't know" answer.
Reference: [13] <author> Dana Angluin and Leslie G. Valiant. </author> <title> Fast probabilistic algorithms for Hamiltonian circuits and matchings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18(2) </volume> <pages> 155-193, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: It follows directly from Blumer et al. [15] that our sample suffices to ensure that the hypothesis output by IBox (T ) has error at most *=3 d with probability at least 1 ffi 23 d . Applying Chernoff bounds <ref> [13] </ref> it can be shown that for each of the 3 d 1 remaining sub-regions, the sample is sufficiently large so that with probability at least 1 ffi 23 d there are enough points in any sub-region of weight at least *=3 d so that the hypothesis output by LearnBPQ for
Reference: [14] <author> Avrim Blum and Steven Rudich. </author> <title> Fast learning of k-term DNF formulas with queries. </title> <booktitle> In Proceedings of the Twenty Fourth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 382-389, </pages> <month> May </month> <year> 1992. </year> <month> 27 </month>
Reference-contexts: We now consider learning agreements of at most a constant number k of concepts from a class C. Let C ? (k) = fAgree F j F C; jF j kg. Applying the known learning results for monotone DNF [41] and DNF formulas with a constant number of terms <ref> [4, 14] </ref> and the known learning results for decision trees [16] and DFAs [4], we obtain the following corollary. (The corollary follows because the intersection and union of a constant number of concepts from each of the preceding classes can be represented by a single concept in the corresponding class that
Reference: [15] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Within the PAC model, Long and Warmuth [35] have given an algorithm to learn this class that runs in time polynomial in d for s constant, and Blumer et al. <ref> [15] </ref> have given an algorithm for this class that runs in time polynomial in s for d constant. <p> It is easy to show that this class is a generalization of unate DNF formulas, and a specialization of the class of unions of boxes in E d . Blumer et al. <ref> [15] </ref> present an algorithm to PAC-learn an s-fold union of boxes in E d by drawing a sufficiently large sample of size m = poly * ; log 1 , and then performing a greedy covering over the at most 2d rectangles defined by the sample. <p> Our algorithm to learn the union of s origin-incident boxes runs in time polynomial in both d and s. To aid in learning the agreement of boxes, we also use the known algorithm for computing the intersection of boxes <ref> [15] </ref>. Namely, we first learn an approximation for the intersection region by applying the standard algorithm with all "?" examples treated as negative. Since the boxes have a non-empty intersection, we can subdivide E d into at most 3 d sub-regions based on this common intersection. <p> The VC-dimension 5 of BPQ [ (s) grows polynomially with s and d (namely, it is at most 2ds log 3s). It then follows from Theorem 2.1 of Blumer et al. <ref> [15] </ref> that if LearnBPQ is given a sample of cardinality at least m = max n * log 2 16ds log (3s) * , then with probability at least 1 ffi, it will output a hypothesis h with error at most *. <p> To see that (2) is true, note that the VC-dimension of BPQ is at most d (this is easily shown), and by Lemma 3.2.3 of Blumer et al. <ref> [15] </ref>, the VC-dimension of BPQ [ (s) is at most 2ds log (3s). To complete the proof, it remains to be shown that (1) holds. We first show that LearnBPQ produces a hypothesis that is consistent with the sample S. <p> Thus p i must be contained within b i . Likewise, let p j be the point from P selected during 5 The VC-dimension is a combinatorial parameter of a concept class that directly relates to the number of examples necessary (and sufficient) for sufficient generalization <ref> [15] </ref>. 17 the iteration of the while loop in which b j was added to h. (So p j is in b j .) Since p j 2 P after b i was placed in h, a membership query must have been performed on maxCornerfp 0 i ; p j g; <p> Removing Intersection Box Estimation Error: There remains a subtle point that we must address. So far we have assumed that we know the intersection region exactly. However, in reality, we apply a known PAC-algorithm <ref> [15] </ref> to obtain a good approximation of the intersection region; the approximation box is contained in the intersection region. <p> Proof Sketch: Note that by drawing a sample of size 1=p + ln 2=ffi with probability at least 1 ffi=2 we will obtain a positive example. It follows directly from Blumer et al. <ref> [15] </ref> that our sample suffices to ensure that the hypothesis output by IBox (T ) has error at most *=3 d with probability at least 1 ffi 23 d .
Reference: [16] <author> Nader H. Bshouty. </author> <title> Exact Learning via the Monotone Theory. </title> <booktitle> In 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 302-311, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Let C ? (k) = fAgree F j F C; jF j kg. Applying the known learning results for monotone DNF [41] and DNF formulas with a constant number of terms [4, 14] and the known learning results for decision trees <ref> [16] </ref> and DFAs [4], we obtain the following corollary. (The corollary follows because the intersection and union of a constant number of concepts from each of the preceding classes can be represented by a single concept in the corresponding class that is at most polynomially larger.) Corollary 6 Let C be
Reference: [17] <author> Nader H. Bshouty, Thomas R. Hancock, and Lisa Hellerstein. </author> <title> Learning arithmetic read-once formulas. </title> <booktitle> In Proceedings of the Twenty Fourth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 370-381, </pages> <month> May </month> <year> 1992. </year>
Reference: [18] <author> Nader H. Bshouty, Thomas R. Hancock, and Lisa Hellerstein. </author> <title> Learning Boolean read-once formulas with arbitrary symmetric and constant fan-in gates. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 1-15, </pages> <month> August </month> <year> 1992. </year>
Reference: [19] <author> Zhixiang Chen. </author> <title> Learning unions of two rectangles in the plane with equivalence queries. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 243-252. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1993. </year>
Reference-contexts: In addition, there has been work on learning unions of boxes in the discretized plane (i.e. when d = 2) <ref> [21, 19, 20] </ref>. (See Section 3.3 for a brief discussion of such work.) Our algorithm to PAC-MEMB learn the agreement of s boxes in E d runs in time polynomial in 1=*, 1=ffi, s, and 9 d . <p> While learning the union of these two rectangles within these time bounds was difficult, learning the agreement of the rectangles is quite simple since the learner needs only learn the intersection of the two rectangles which is easily achieved. Chen <ref> [19] </ref> gave an algorithm that uses O (log 2 n) equivalence queries to learn the union of two rectangles in the discretized plane (i.e. f1; : : : ; ng 2 ).
Reference: [20] <author> Zhixiang Chen and Steven Homer. </author> <title> Fast learning unions of rectangles with queries. </title> <type> Unpublished manuscript, </type> <month> July </month> <year> 1993. </year>
Reference-contexts: In addition, there has been work on learning unions of boxes in the discretized plane (i.e. when d = 2) <ref> [21, 19, 20] </ref>. (See Section 3.3 for a brief discussion of such work.) Our algorithm to PAC-MEMB learn the agreement of s boxes in E d runs in time polynomial in 1=*, 1=ffi, s, and 9 d . <p> Chen [19] gave an algorithm that uses O (log 2 n) equivalence queries to learn the union of two rectangles in the discretized plane (i.e. f1; : : : ; ng 2 ). Also, Chen and Homer <ref> [20] </ref> gave an algorithm to learn the union of s rectangles in the discretized plane using O (s 3 log n) membership and equivalence queries and O (s 5 log n) time.
Reference: [21] <author> Zhixiang Chen and Wolfgang Maass. </author> <title> On-line learning of rectangles. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 16-27. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: In addition, there has been work on learning unions of boxes in the discretized plane (i.e. when d = 2) <ref> [21, 19, 20] </ref>. (See Section 3.3 for a brief discussion of such work.) Our algorithm to PAC-MEMB learn the agreement of s boxes in E d runs in time polynomial in 1=*, 1=ffi, s, and 9 d . <p> There has also been work on learning unions of s boxes in the discretized space f1; : : : ; ng d . Most of this work has focused on the special case in which d = 2. Chen and Maass <ref> [21] </ref> gave an algorithm to learn the union of two axis-parallel rectangles in the discretized space f1; : : : ; ng fi f1; : : : ; mg in time polynomial in log n and log m, where one rectangle has a corner in the top left corner and the
Reference: [22] <author> Michael Frazier and Leonard Pitt. </author> <title> Learning from entailments: an application to propositional Horn sentences. </title> <booktitle> In Machine Learning Proceedings of the Tenth International Conference, </booktitle> <pages> pages 120-127, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Finally, it is easily shown that the time complexity is polynomial in the sample complexity. 4 A Negative Result The class of conjunctions of Horn clauses (Horn sentences) is known to be PAC-MEMB-learnable [7]. Furthermore, Frazier and Pitt <ref> [22] </ref> have shown that Horn sentences are efficiently learnable using membership and equivalence queries from a different model in which entailed examples are provided. We provide evidence that this result cannot be strengthened to allow learning blurry 22 LearnBoxesAgreement () 1.
Reference: [23] <editor> Michael Frazier and Leonard Pitt. </editor> <booktitle> CLASSIC Learning. In Proceedings of the Seventh Annual Workshop on Computational Learning Theory, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Indeed, they note that their algorithm for learning monotone DNF with an incomplete membership oracle can be used to learn monotone DNF with random 1 ! 0 one-sided errors. 7 Krikis [10], and Angluin [6] consider learning with a bounded number of such erroneous responses, and Frazier and Pitt <ref> [23] </ref> consider learning when such incorrect responses occur randomly with probability at most 1 2 . In other related work, Kearns and Schapire [32] generalized the PAC setting to non-binary values using Haussler's framework [28].
Reference: [24] <author> Paul W. Goldberg, Sally A. Goldman, and H. David Mathias. </author> <title> Learning unions of rectangles with membership and equivalence queries. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Computational Learning Theory, </booktitle> <month> July </month> <year> 1994. </year> <month> 28 </month>
Reference-contexts: Recently, Goldberg, Goldman and Mathias <ref> [24] </ref> have given an efficient algorithm to exactly learn the union of discretized boxes over the domain f1; : : : ; ng d with membership and equivalence queries when either d or s are constant. <p> Also, Chen and Homer [20] gave an algorithm to learn the union of s rectangles in the discretized plane using O (s 3 log n) membership and equivalence queries and O (s 5 log n) time. More recently, Goldberg, Goldman and Mathias <ref> [24] </ref> have presented two algorithms to learn the union of s discretized boxes in f1; : : : ; ng d . The first makes at most sd + 1 equivalence queries and uses O ((4s) d + sd log n) time and membership queries.
Reference: [25] <author> Sally A. Goldman, Michael J. Kearns, and Robert E. Schapire. </author> <title> Exact identifi-cation of circuits using fixed points of amplification functions. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22(4) </volume> <pages> 705-726, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: There has also been some work considering learning from noisy membership queries <ref> [25, 38] </ref>. Angluin and Slonim [12] introduced a model of incomplete membership queries in which each membership query is answered "don't know" with a given probability. Furthermore, this information is persistent|repeatedly making a query that was answered "don't know" always results in a "don't know" answer.
Reference: [26] <author> Sally A. Goldman and Robert H. Sloan. </author> <title> Can PAC learning algorithms tolerate random attribute noise? Technical Report WUCS-92-25, </title> <institution> Washington University, Department of Computer Science, </institution> <month> July </month> <year> 1992. </year> <note> To appear in Algorthmica. </note>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples [11, 33, 40, 30] and some addressing the issue of noise in the attributes <ref> [39, 26, 34] </ref>. 6 In these situations, the border between the positive and negative examples may ap-pear blurry to the learner, but this is just the result of the noise process that has been applied to the properly labeled example.
Reference: [27] <author> Thomas R. Hancock. </author> <title> Learning 2 DNF formulas and k decision trees. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 199-209, </pages> <month> August </month> <year> 1991. </year>
Reference: [28] <author> David Haussler. </author> <title> Generalizing the PAC model: sample size bounds from metric dimension-based uniform convergence results. </title> <booktitle> In 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 40-45, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: In other related work, Kearns and Schapire [32] generalized the PAC setting to non-binary values using Haussler's framework <ref> [28] </ref>. They define a p-concept in which each instance x 2 X has some probability p (x) of being classified as positive. In their model, the goal of the learner is to make optimal predictions, or more commonly, to accurately predict p (x) for all x 2 X .
Reference: [29] <author> David Haussler, Michael Kearns, Nick Littlestone, and Manfred K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Information and Computation, </journal> <volume> 25(2) </volume> <pages> 129-161, </pages> <month> December </month> <year> 1991. </year>
Reference: [30] <author> Michael Kearns and Ming Li. </author> <title> Learning in the presence of malicious errors. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22(4) </volume> <pages> 807-837, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples <ref> [11, 33, 40, 30] </ref> and some addressing the issue of noise in the attributes [39, 26, 34]. 6 In these situations, the border between the positive and negative examples may ap-pear blurry to the learner, but this is just the result of the noise process that has been applied to the
Reference: [31] <author> Michael Kearns and Leslie Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 433-444, </pages> <month> May </month> <year> 1989. </year> <note> To appear in JACM. </note>
Reference: [32] <author> Michael J. Kearns and Robert E. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <booktitle> In 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 382-391, </pages> <year> 1990. </year>
Reference-contexts: In other related work, Kearns and Schapire <ref> [32] </ref> generalized the PAC setting to non-binary values using Haussler's framework [28]. They define a p-concept in which each instance x 2 X has some probability p (x) of being classified as positive.
Reference: [33] <author> Philip D. Laird. </author> <title> Learning from Good and Bad Data. </title> <booktitle> Kluwer international series in engineering and computer science. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year> <month> 29 </month>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples <ref> [11, 33, 40, 30] </ref> and some addressing the issue of noise in the attributes [39, 26, 34]. 6 In these situations, the border between the positive and negative examples may ap-pear blurry to the learner, but this is just the result of the noise process that has been applied to the
Reference: [34] <author> Nicholas Littlestone. </author> <title> Redundant noisy attributes, attribute errors, and lineary--threshold learning using winnow. </title> <booktitle> In Fourth Workshop on Computational Learning Theory, </booktitle> <pages> pages 147-156, </pages> <year> 1991. </year>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples [11, 33, 40, 30] and some addressing the issue of noise in the attributes <ref> [39, 26, 34] </ref>. 6 In these situations, the border between the positive and negative examples may ap-pear blurry to the learner, but this is just the result of the noise process that has been applied to the properly labeled example.
Reference: [35] <author> Philip M. Long and Manfred K. Warmuth. </author> <title> Composite geometric concepts and polynmomial predictability. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 273-287. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Thus, the problem of learning the agreement of blurry boxes is a specialization of the widely studied problem of learning the s-fold union of boxes in E d . Within the PAC model, Long and Warmuth <ref> [35] </ref> have given an algorithm to learn this class that runs in time polynomial in d for s constant, and Blumer et al. [15] have given an algorithm for this class that runs in time polynomial in s for d constant. <p> Thus for d constant this algorithm runs in polynomial time. Long and Warmuth <ref> [35] </ref> 13 present an algorithm to PAC-learn this same class by again drawing a sufficiently large sample and constructing a hypothesis consistent with the sample that consists of at most s (2d) s boxes.
Reference: [36] <author> Thomas Mitchell. </author> <title> Generalization as Search. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: of concepts from C, and the other a union of concepts from C. 4 Thus when unions and intersections of concepts from C are learnable, the blurry class C ? is learnable. 4 There is an interesting relationship between the definition of agreements and Mitchell's definition of a version space <ref> [36] </ref> that is discussed in a more complete version of this paper. 9 Learn-Agreement-Nested-Concepts (F; *; ffi) 1. Let F := ff s ; f g g such that f s f g . 2. Let A S be the PAC-MEMB learning algorithm for C S . 3.
Reference: [37] <author> Leonard Pitt and Manfred K. Warmuth. </author> <title> Prediction preserving reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 41(3) </volume> <pages> 430-467, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Theorem 11 PAC-MEMB learning the agreement of Horn sentences for which the intersection region is samplable is as hard as PAC-MEMB learning the class of DNF formulas. Proof: We prove this through a sequence of prediction preserving reductions <ref> [9, 37] </ref>. Let DHF-1pos be the class of DHF formulas with exactly one positive example p that satisfies every disjunct. Let agree-Horn-1pos be the agreement of Horn sentences that have exactly one example in their intersection.
Reference: [38] <author> Yasubumi Sakakibara. </author> <title> On learning from queries and counterexamples in the presence of noise. </title> <journal> Information Processing Letters. </journal> <note> To appear. </note>
Reference-contexts: There has also been some work considering learning from noisy membership queries <ref> [25, 38] </ref>. Angluin and Slonim [12] introduced a model of incomplete membership queries in which each membership query is answered "don't know" with a given probability. Furthermore, this information is persistent|repeatedly making a query that was answered "don't know" always results in a "don't know" answer.
Reference: [39] <author> George Shackelford and Dennis Volper. </author> <title> Learning k-DNF with noise in the attributes. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 97-103. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1988. </year>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples [11, 33, 40, 30] and some addressing the issue of noise in the attributes <ref> [39, 26, 34] </ref>. 6 In these situations, the border between the positive and negative examples may ap-pear blurry to the learner, but this is just the result of the noise process that has been applied to the properly labeled example.
Reference: [40] <author> Robert H. Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 91-96. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples <ref> [11, 33, 40, 30] </ref> and some addressing the issue of noise in the attributes [39, 26, 34]. 6 In these situations, the border between the positive and negative examples may ap-pear blurry to the learner, but this is just the result of the noise process that has been applied to the
Reference: [41] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year> <month> 30 </month>
Reference-contexts: These observations are sufficient to construct a PAC-MEMB algorithm to learn the class of blurry monomials (for which P is nonempty): run a known algorithm for learning (nonblurry) monomials <ref> [41] </ref> to learn the set P of positive examples, and at the same time run a known learning algorithm for (nonblurry) unate DNF [8] to learn the set P [ Q of nonnegative examples. <p> Then C + ? is polynomially PAC-MEMB learnable. Proof: The class C " is learnable since C is closed under intersection and known to be learnable <ref> [41] </ref>. If F C is a subset for which there is some example x such that Intersect F (x) = 1, then x satisfies every monomial in F and so it cannot be the case that some variable appears both negated and unnegated in F . <p> We now consider learning agreements of at most a constant number k of concepts from a class C. Let C ? (k) = fAgree F j F C; jF j kg. Applying the known learning results for monotone DNF <ref> [41] </ref> and DNF formulas with a constant number of terms [4, 14] and the known learning results for decision trees [16] and DFAs [4], we obtain the following corollary. (The corollary follows because the intersection and union of a constant number of concepts from each of the preceding classes can be
References-found: 41

