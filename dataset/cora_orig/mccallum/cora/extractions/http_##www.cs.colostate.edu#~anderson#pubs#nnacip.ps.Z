URL: http://www.cs.colostate.edu/~anderson/pubs/nnacip.ps.Z
Refering-URL: http://www.cs.colostate.edu/~anderson/pubs/pubs.html
Root-URL: 
Email: anderson@cs.colostate.edu  
Title: Reinforcement Learning with Modular Neural Networks for Control  
Author: Charles W. Anderson Zhaohui Hong 
Address: Fort Collins, CO 80523  
Affiliation: Department of Computer Science Colorado State University  
Abstract: Reinforcement learning methods can be applied to control problems with the objective of optimizing the value of a function over time. They have been used to train single neural networks that learn solutions to whole tasks. Jacobs and Jordan [5] have shown that a set of expert networks combined via a gating network can more quickly learn tasks that can be decomposed. Even the decomposition can be learned. Inspired by Boyan's work of modular neural networks for learning with temporal-difference methods [4], we modify the reinforcement learning algorithm called Q-Learning to train a modular neural network to solve a control problem. The resulting algorithm is demonstrated on the classical pole-balancing problem. The advantage of such a method is that it makes it possible to deal with complex dynamic control problem effectively by using task decomposition and competitive learning. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. W. Anderson. </author> <title> Strategy learning with multilayer connectionist representations. </title> <type> Technical Report TR87-509.3, </type> <institution> GTE Laboratories, </institution> <address> Waltham, MA, </address> <year> 1987. </year> <booktitle> Revision of article that was published in Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pp. 103-114, </pages> <month> June, </month> <year> 1987. </year>
Reference-contexts: It involves a pole hinged to the top of a wheeled cart which can move along a track of limited length. The system is mod-elled by two differential equations, taken from Ander-son <ref> [1] </ref>. The neural network receives a performance feedback, which indicates failure when the pole falls past 12 degrees from vertical and when the cart hits the bounds of the track, and a four-component vector as the current state including the velocities and positions from the pole-cart system.
Reference: [2] <author> Charles W. Anderson. </author> <title> Q-learning with hidden-unit restarting. </title> <editor> In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 81-88. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The Q-function is learned and the discounted cumulative reinforcement in the future is maximized. In our experiments, the expert networks used radial basis functions in the hidden units in order to compare directly to the results of Anderson <ref> [2] </ref>. 2 Experiments and Results The pole-balancing problem is a classic example of an inherently unstable system. It involves a pole hinged to the top of a wheeled cart which can move along a track of limited length. The system is mod-elled by two differential equations, taken from Ander-son [1].
Reference: [3] <author> A. G. Barto. </author> <title> Connectionist learning for control: An overview. </title> <editor> In W. T. Miller, R. S. Sutton, and P. J. Werbos, editors, </editor> <booktitle> Neural Networks for Control, chapter 1, </booktitle> <pages> pages 5-58. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction Neural networks have been applied in a number of ways to the problem of learning to control a system <ref> [3] </ref>. Usually there is a single network in a system. The performance of the system depends on many factors, such as the structure and the size of the problem to which it is applied, the amount of training data, the type of neurons in the network, and so on.
Reference: [4] <author> Justin A. Boyan. </author> <title> Modular neural networks for learning context-dependend game strategies. </title> <type> Master's thesis, </type> <institution> University of Cambridge, </institution> <year> 1992. </year>
Reference-contexts: They concluded that faster learning speed over the single network can be achieved by developing a piecewise control strategy for each sub-network. Boyan <ref> [4] </ref> developed a modular neural network for learning game strategies. He applied his "Designer Domain Decomposition" and "Meta-Pi" architectures to the Tic-Tac-Toe and Backgammon games.
Reference: [5] <author> R. A. Jacobs and M. I. Jordan. </author> <title> A modular connectionist architecture for learning piecewise control strategies. </title> <booktitle> In Proceedings of the 1991 American Control Conference, </booktitle> <year> 1991. </year>
Reference-contexts: The second type of network consists of several expert networks and a gating network that learns to control the final output of the whole network. This network starts training from scratch and doesn't need any a priori knowledge. Jacobs and Jordan <ref> [5] </ref> studied the modular network architecture in a "multiple payload robotics" task. They tested four architectures: a single network, a modular architecture, a modular architecture with a share network, and a constrained modular architecture with a share network, respectively.
Reference: [6] <author> C. Watkins. </author> <title> Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University Psychology Department, </institution> <year> 1989. </year>
Reference-contexts: The objective is to determine an optimal policy which can determine the actions that the agent is going to take given each state. Q-learning is a family of reinforcement learning algorithms initially developed by Watkins <ref> [6] </ref>. The prevalence of these algorithm is partially due to the existence of convergence proofs [7]. In Q-learning, the predicted long term cumulative reinforcement, called the Q-value, is a function of actions as well as input states. The Q-function acts as an evaluation function that predicts the discounted cumulative reinforcement.
Reference: [7] <author> Whitley, Dominic, Das, and Anderson. </author> <title> Genetic reinforcement learning for neurocontrol problems. </title> <type> Technical Report CS92-102, </type> <institution> CSU, </institution> <year> 1992. </year>
Reference-contexts: Q-learning is a family of reinforcement learning algorithms initially developed by Watkins [6]. The prevalence of these algorithm is partially due to the existence of convergence proofs <ref> [7] </ref>. In Q-learning, the predicted long term cumulative reinforcement, called the Q-value, is a function of actions as well as input states. The Q-function acts as an evaluation function that predicts the discounted cumulative reinforcement.
References-found: 7

