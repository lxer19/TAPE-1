URL: http://www.cogs.susx.ac.uk/users/christ/papers/cntnt.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Phone: Tel: (44)273 606755 3239  
Title: Links between Content and Information-Content  
Author: Chris Thornton 
Note: the cognitive abilities of the receiver.  
Date: October 5, 1994  
Address: Brighton BN1 9QN  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Baierlein, R. </author> <year> (1971). </year> <title> Atoms and Information Theory. </title> <address> San Francisco: </address> <publisher> W.H. Freeman. </publisher>
Reference: [2] <author> Gatlin, L. </author> <year> (1972). </year> <title> Information Theory and the Living System. </title> <publisher> London: Columbia University Press. </publisher> <pages> 9 </pages>
Reference: [3] <author> Kolmogorov, A. </author> <year> (1968). </year> <title> On the logical basis of information theory and probability theory. </title> <journal> IEEE Trans. Inform. Theory IT-14 (pp. </journal> <pages> 662-664). </pages>
Reference: [4] <author> Mackay, D. </author> <year> (1969). </year> <title> Information, Mechanism and Meaning. </title> <publisher> London: MIT Press. </publisher>
Reference-contexts: As Mackay notes, `Claude Shannon's measure of selective information-content was framed explicitly to require no reference to the meaning of the information' <ref> [4, p.19] </ref>. However, the implications of this are not completely clear. The fact that information 2 is to be clearly distinguished from content does not necessarily imply that the two properties are not linked together in some way. <p> The dual nature of the equation is well-known. For instance, Mackay notes that Shannon's [information] measure indicates the minimum equivalent number of binary steps by which the representation concerned may be selected from an ensemble of possible repre sentations, <ref> [4, p.80] </ref>. 5 We can say, then, that the definition of uncertainty (and therefore information) provided by information theory has an intuitive interpretation as follows. <p> Another important analysis, associated with Donald Mackay, bears a greater similarity to the present approach. It involves an argument seeking to demonstrate that `the theory of information has a natural, precise, and objectively definable place for the concept of meaning' <ref> [4, p.92] </ref>. Mackay's approach exploits the notion, used extensively above, that the meaning of a message can be construed in terms of a vector (i.e. point) in an n-dimensional vector space (which Mackay calls an `information-space'). <p> A particular message can now be pictured as selecting a particular region, which may be identified by the vector (or distribution of vectors) linking it to the origin. The meaning of the message is then represented by the orientation of this vector, relative to the vector basis <ref> [4, p.92] </ref>. 8 Concluding comments The paper has shown that information theory defines a receiver's level of uncertainty in any given case in terms of the amount of work it would take to dispel the uncertainty completely; and that it defines information-content in terms of uncertainty-reduction.
Reference: [5] <author> Shannon, C. and Weaver, W. </author> <year> (1949). </year> <title> The Mathematical Theory of Information. </title> <institution> Urbana: University of Illinois Press. </institution>
Reference-contexts: The level of information in any given case always depends on the a priori probability of the message as assumed by the receiver. Of course this caveat relating to the source of probability assumptions does not figure in the original presentation of communication theory <ref> [5] </ref>. But this is simply because, in that formulation, the focus was on electronic communications devices rather than on arbitrarily complex cognitive mechanisms. It obviously makes no sense to assume that a communications device is capable of varying its `assumptions' with respect to the a priori probabilities of messages.
Reference: [6] <author> Watanabe, S. </author> <year> (1969). </year> <title> Knowing and Guessing: A Quantitative Study of Inference and Information. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: with respect to an event E i which has an a priori probability of P i is simply log 2 P i so to calculate the overall uncertainty when there are range of possible events all having different a priori probabilities, we simply work out the average or `expected' uncertainty <ref> [6] </ref>. The probability of the uncertainty being exactly log 2 P i is simply P i. So to get the overall (expected) uncertainty we simply multiply all the possible uncertainty levels by their corresponding probabilities.
Reference: [7] <author> Brillouin, L. </author> <year> (1962). </year> <booktitle> Science and Information Theory. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: According to our definition, a set of 100 letters from a newspaper, a piece of Shakespeare or a theorem of Einstein are given exactly the same information value. In other words, we define `information' as distinct from `knowledge,' for which we have no numerical measure <ref> [7, p.9] </ref>. Bar-Hillel and Carnap make the point as follows. Theory of information, as practised nowadays, is not interested in the content of symbols whose information it measures. <p> Thirdly, there is the position which states that information quantifies content and therefore necessarily tells us something about its qualitative properties. It is not always possible to associate a given author with any one of these three positions. However, we might very tentatively associate Brillouin <ref> [7] </ref> with the first position, Bar-Hillel and Carnap [8] with the second position and Mackay and Campbell [4,9] with the third. It is notable that all three positions can be `justified' using examples. <p> We need to ask: what does this equation mean? And, can it be given an intuitive interpretation? Turning to texts on information theory may help us here rather less than we might hope. In general, the equation is explained through reference to the problems facing the communications engineer <ref> [7] </ref>, or through reference to probability theory [10]. Fortunately, the equation itself is of a sufficiently simple form that some progress can be made simply by considering the role played by its component terms. <p> Thus we take the sum of P i ( log 2 P i ) for all i. This expression makes up the right hand size of Shannon's generalised `entropy of information' equation, referred to as the negentropy equation by Brillouin <ref> [7] </ref>: I = P i log 2 P i 5 The sense in which information quantifies content An intuitive interpretation of basic information theory has now been laid out.
Reference: [8] <author> Bar-Hillel, Y. and Carnap, R. </author> <year> (1953). </year> <title> Semantic information. </title> <editor> In W. Jackson (Ed.), </editor> <title> Communication Theory. </title> <publisher> London: Butterworths. </publisher>
Reference-contexts: Theory of information, as practised nowadays, is not interested in the content of symbols whose information it measures. The measures, as defined, for instance, by Wiener and Shannon, have nothing to do with what these symbols symbolise, but only with the frequency of their occurrence. <ref> [8, p.503] </ref> The view that information-content is to be clearly distinguished from meaning seems to be fairly well supported in the original formulation of information theory. As Mackay notes, `Claude Shannon's measure of selective information-content was framed explicitly to require no reference to the meaning of the information' [4, p.19]. <p> It is not always possible to associate a given author with any one of these three positions. However, we might very tentatively associate Brillouin [7] with the first position, Bar-Hillel and Carnap <ref> [8] </ref> with the second position and Mackay and Campbell [4,9] with the third. It is notable that all three positions can be `justified' using examples. <p> In addition, Dretske draws a distinction between information and meaning which is not echoed in or compatible with the approach worked out above. In order to deal with meaning, Dretske extends the basic framework of information theory. This approach is also taken by Bar-Hillel and Carnap <ref> [8] </ref> who, according to Dretske, are perhaps `the best known (to philosophers) critics of the statistical theory of information as an adequate tool for semantic studies' [13, p.241]. <p> that any two interpretations A and B which are not preferred equally by the receiver (perhaps as a result of contextual effects) will always be in different classes. 8 to be measured in terms of the set of statements logically implied by the message within a previously specified formal system <ref> [8] </ref>. Another important analysis, associated with Donald Mackay, bears a greater similarity to the present approach. It involves an argument seeking to demonstrate that `the theory of information has a natural, precise, and objectively definable place for the concept of meaning' [4, p.92].
Reference: [9] <author> Campbell, J. </author> <year> (1984). </year> <title> Grammatical Man. Harmondsworth: </title> <publisher> Penguin. </publisher>
Reference: [10] <author> Jones, D. </author> <year> (1979). </year> <title> Elementary information theory. </title> <publisher> Oxford: Clarendon Press. </publisher>
Reference-contexts: In general, the equation is explained through reference to the problems facing the communications engineer [7], or through reference to probability theory <ref> [10] </ref>. Fortunately, the equation itself is of a sufficiently simple form that some progress can be made simply by considering the role played by its component terms.
Reference: [11] <author> Rucker, R. </author> <year> (1988). </year> <title> Mind Tools: </title> <booktitle> The Mathematics of Information. </booktitle> <address> London: </address> <publisher> Penguin. </publisher>
Reference-contexts: Thus the message about growth of wings reduced a large amount of uncertainty in X, but the message about drawing breath reduced only a very small amount. This way of interpreting the property measured by the information equation was originally advocated by Shannon himself <ref> [11] </ref>. It is reasonably satisfactory from an intuitive point of view. But it is important to note that it shifts the focus of attention from the message itself to the receiver of the message.
Reference: [12] <author> MacKay, D. </author> <year> (1953). </year> <title> Generators of information. </title> <editor> In W. Jackson (Ed.), </editor> <booktitle> Communication Theory (pp. </booktitle> <pages> 475-485). </pages> <address> London: </address> <publisher> Butterworths. </publisher>
Reference-contexts: But it is important to note that it shifts the focus of attention from the message itself to the receiver of the message. It lets us see that a single message might have differing degrees of information depending on who or what is receiving it <ref> [12] </ref>. The level of information in any given case always depends on the a priori probability of the message as assumed by the receiver. Of course this caveat relating to the source of probability assumptions does not figure in the original presentation of communication theory [5].
Reference: [13] <author> Dretske, F. </author> <year> (1981). </year> <title> Knowledge and the Flow of Information. </title> <publisher> Oxford: Basil Blackwood. </publisher>
Reference-contexts: The present analysis is just one of quite a large collection of perspectives on the basic problem. One of the best-known of these is attributable to Dretske, who has constructed an analysis of meaning, knowledge and belief within a broadly information-theoretic context <ref> [13] </ref>. His approach differs markedly from the current one in that it acknowledges (and tries to take account of) the information source as an absolute entity, independent of the receiver. <p> In order to deal with meaning, Dretske extends the basic framework of information theory. This approach is also taken by Bar-Hillel and Carnap [8] who, according to Dretske, are perhaps `the best known (to philosophers) critics of the statistical theory of information as an adequate tool for semantic studies' <ref> [13, p.241] </ref>.
Reference: [14] <author> Sayre, K. </author> <year> (1965). </year> <title> Recognition: </title> <booktitle> A Study in the Philosophy of Artificial Intelligence. </booktitle> <institution> University of Notre Dame Press. </institution>
Reference: [15] <author> Bar-Hillel, Y. </author> <year> (1965). </year> <title> Language and Information. </title> <address> Reading, Mass. </address> <month> 10 </month>
References-found: 15

