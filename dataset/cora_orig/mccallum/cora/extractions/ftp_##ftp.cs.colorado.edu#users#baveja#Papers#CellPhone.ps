URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/CellPhone.ps
Refering-URL: http://www.cs.orst.edu/~margindr/ML_RG/winter97-mlrg.html
Root-URL: 
Email: singh@psyche.mit.edu  bertsekas@lids.mit.edu  
Title: Submitted to NIPS96, Section: Applications. Preference: Oral presentation Reinforcement Learning for Dynamic Channel Allocation in
Author: Satinder Singh Dimitri Bertsekas 
Address: One Cambridge Center Cambridge, MA 02142  Cambridge, MA 02139  
Affiliation: Harlequin Inc.  Lab. for Information and Decision Sciences MIT  
Abstract: In cellular telephone systems, an important problem is to dynamically allocate the communication resource (channels) so as to maximize service in a stochastic caller environment. This problem is naturally formulated as a dynamic programming problem and we use a reinforcement learning (RL) method to find dynamic channel allocation policies that are better than previous heuristic solutions. The policies obtained perform well for a broad variety of call traffic patterns. We present results on a large cellular system In cellular communication systems, an important problem is to allocate the communication resource (bandwidth) so as to maximize the service provided to a set of mobile callers whose demand for service changes stochastically. A given geographical area is divided into mutually disjoint cells, and each cell serves the calls that are within its boundaries (see Figure 1a). The total system bandwidth is divided into channels, with each channel centered around a frequency. Each channel can be used simultaneously at different cells, provided these cells are sufficiently separated spatially, so that there is no interference between them. The minimum separation distance between simultaneous reuse of the same channel is called the channel reuse constraint . When a call requests service in a given cell either a free channel (one that does not violate the channel reuse constraint) may be assigned to the call, or else the call is blocked from the system; this will happen if no free channel can be found. Also, when a mobile caller crosses from one cell to another, the call is "handed off" to the cell of entry; that is, a new free channel is provided to the call at the new cell. If no such channel is available, the call must be dropped/disconnected from the system. One objective of a channel allocation policy is to allocate the available channels to calls so that the number of blocked calls is minimized. An additional objective is to minimize the number of calls that are dropped when they are handed off to a busy cell. These two objectives must be weighted appropriately to reflect their relative importance, since dropping existing calls is generally more undesirable than blocking new calls. with approximately 70 49 states.
Abstract-found: 1
Intro-found: 1
Reference: <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> (1995) Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72 </volume> <pages> 81-138. </pages>
Reference-contexts: We presented such a solution using Sutton's (1988) TD (0) with a feature-based linear network and demonstrated its superiority on a problem with approximately 70 49 states. Other recent examples of similar successes are the game of backgammon (Tesauro, 1992), elevator-scheduling <ref> (Crites & Barto, 1995) </ref>, and job-shop scheduling (Zhang & Dietterich, 1995). The neuro-dynamic programming textbook (Bertsekas and Tsitsiklis, 1996) presents a variety of related case studies.
Reference: <author> D. P. Bertsekas. </author> <title> (1995) Dynamic Programming and Optimal Control: Vols 1 and 2. </title> <address> Athena-Scientific, Belmont, MA. </address>
Reference-contexts: In this paper, we compare the performance of dynamic channel allocation policies learned by RL with both FA and BDCL. 1.1 Dynamic programming formulation We can formulate the dynamic channel allocation problem using dynamic programming <ref> (e.g., Bertsekas, 1995) </ref>.
Reference: <author> D. P. Bertsekas and J. </author> <title> Tsitsiklis. </title> <booktitle> (1996) Neuro-Dynamic Programming Athena-Scientific, </booktitle> <address> Belmont, MA. </address>
Reference-contexts: Other recent examples of similar successes are the game of backgammon (Tesauro, 1992), elevator-scheduling (Crites & Barto, 1995), and job-shop scheduling (Zhang & Dietterich, 1995). The neuro-dynamic programming textbook <ref> (Bertsekas and Tsitsiklis, 1996) </ref> presents a variety of related case studies.
Reference: <author> R. H. Crites and A. G. Barto. </author> <title> (1996) Improving elevator performance using reinforcement learning. </title> <booktitle> In Advances is Neural Information Processing Systems 8. </booktitle>
Reference: <author> W. Del Re, R. Fantacci, and L. </author> <title> Ronga (1996) A dynamic channel allocation technique based on Hopfield Neural Networks IEEE Transactions on Vehicular Technology, </title> <publisher> 45:1. </publisher>
Reference: <author> R.J. McEliece and K.N. </author> <month> Sivarajan </month> <year> (1994), </year> <title> Performance limits for channelized cellular telephone systems. </title> <journal> IEEE Trans. Inform. Theory, </journal> <pages> pp. 21-34, </pages> <month> Jan. </month>
Reference-contexts: The RL curves in quite rapid. As the mean call arrival rate is increased the relative difference between the 3 algorithms decreases. In fact, FA can be shown to be optimal in the limit of infinite call arrival rates <ref> (see McEliece and Sivarajan, 1994) </ref>. With so many customers in every cell there are no short-term fluctuations to exploit. However, as demonstrated in Figure 3, for practical traffic rates RL consistently gives a big win over FA and a smaller win over BDCL.
Reference: <author> R. S. Sutton. </author> <title> (1988) Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference: <author> G. J. Tesauro. </author> <title> (1992) Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> 8(3/4):257-277. 
Reference-contexts: We presented such a solution using Sutton's (1988) TD (0) with a feature-based linear network and demonstrated its superiority on a problem with approximately 70 49 states. Other recent examples of similar successes are the game of backgammon <ref> (Tesauro, 1992) </ref>, elevator-scheduling (Crites & Barto, 1995), and job-shop scheduling (Zhang & Dietterich, 1995). The neuro-dynamic programming textbook (Bertsekas and Tsitsiklis, 1996) presents a variety of related case studies.
Reference: <author> M. Zhang and T. P. Yum. </author> <title> (1989) Comparisons of Channel-Assignment Strategies in Cellular Mobile Telephone Systems. </title> <journal> In IEEE Transactions on Vehicular Technology Vol. </journal> <volume> 38, No. </volume> <pages> 4. </pages>
Reference: <author> W. Zhang and T. G. Dietterich. </author> <title> (1996) High-performance job-shop scheduling with a time-delay TD(lambda) network. </title> <booktitle> In Advances is Neural Information Processing Systems 8. </booktitle>
References-found: 10

