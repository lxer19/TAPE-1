URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94628.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Distributed Computational Electromagnetics Systems  
Author: Gang Cheng Kenneth A. Hawick Gerald Mortensen Geoffrey C. Fox 
Abstract: We describe our development of a "real world" electromagnetic application on distributed computing systems. A computational electromagnetics (CEM) simulation for radar cross-section(RCS) modeling of full scale airborne systems has been ported to three networked workstation cluster systems: an IBM RS/6000 cluster with Ethernet connection; a DEC Alpha farm connected by a FDDI-based Gigaswitch; and an ATM-connected SUN IPXs testbed. We used the ScaLAPACK LU solver from Oak Ridge National Laboratory/University of Tennessee in our parallel implementation for solving the dense matrix which forms the computationally intensive kernel of this application, and we have adopted BLACS as the message passing interface in all of our code development to achieve high portability across the three configurations. The performance data from this work is reported, together with timing data from other MPP systems on which we have implemented this application including an Intel iPSC/860 and a CM-5, and which we include for comparison. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, A. Benzoni, J. J. Dongarra, et al, </author> <title> Basic Linear Algebra Communication Subprograms, </title> <booktitle> Proc. of the 6th Distributed Memory Computing Conference, </booktitle> <year> 1991. </year>
Reference-contexts: Portability, as well as performance scalability, was among our top objectives [7]. Our use of the distributed linear algebra packages ScaLAPACK [3, 8] and BLACS <ref> [1] </ref> in our parallel implementation allowed us to achieve true code portability across: Intel machines (the Intel Delta, an Intel Paragon and a Intel iPSC/860); an IBM SP-1 with either Ethernet or switch connection, and the three distributed configurations described above.
Reference: [2] <author> G. Cheng, G.C. Fox, and K. A. Hawick, </author> <title> A Scaleable Paradigm for Effectively-Dense Matrix Formulated Applications, </title> <booktitle> Proc. of the European Conference on High-Performance Computing and Networking, </booktitle> <address> Munich, Germany, </address> <year> 1994. </year>
Reference-contexts: We were fortunate in gaining early access to the ScaLAPACK library and adopted an early release of the REAL variable arithmetic version in writing our COMPLEX routines. It is important to parallelize the full applications code so as to optimize and balance the overall performance of this application <ref> [2] </ref>. The BLACS interface also provided a suitable level of abstraction for us to implement the "Setup" and "Far-field" components. The full schematic of our parallelized ParaMoM code is shown in Figure 2.
Reference: [3] <author> J. Choi, J. J. Dongarra, R. Pozo, and D. W. Walker, </author> <title> Scalapack: A scalable linear algebra library for distributed memory concurrent computers, </title> <booktitle> Proc. of the 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pp. 120-127, </pages> <year> 1992. </year>
Reference-contexts: One of the major motivations for our work was to port the CEM application to multiple MPP and distributed computing systems and to evaluate and demonstrate their capabilities. Portability, as well as performance scalability, was among our top objectives [7]. Our use of the distributed linear algebra packages ScaLAPACK <ref> [3, 8] </ref> and BLACS [1] in our parallel implementation allowed us to achieve true code portability across: Intel machines (the Intel Delta, an Intel Paragon and a Intel iPSC/860); an IBM SP-1 with either Ethernet or switch connection, and the three distributed configurations described above. <p> The BLACS interface also provided a suitable level of abstraction for us to implement the "Setup" and "Far-field" components. The full schematic of our parallelized ParaMoM code is shown in Figure 2. The internal storage mechanisms of ScaLAPACK have been well described elsewhere <ref> [3] </ref>, however it is necessary to explain how the matrix and vectors involved are blocked and scattered so as minimize the inter-node communication requirements. There are three major user tunable parameters in BLACS/ScaLAPACK: P; Q and N b .
Reference: [4] <author> J. Choi, J. J. Dongarra, and D. W. Walker, PB-BLAS: </author> <title> A Set of Parallel Block Basic Linear Algebra Subprograms, </title> <booktitle> Proc. of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May, </month> <year> 1994. </year>
Reference-contexts: ScaLAPACK is a two Distributed Computational Electromagnetics Systems 3 dimensional distributed memory version of the LAPACK [10] package and relies on calls to the Basic Linear Algebra Subprograms (BLAS) for local computation, and calls to the PB-BLAS <ref> [4] </ref> for global computation. For portability reasons, communications takes place inside the PB-BLAS through calls to the BLACS. We were fortunate in gaining early access to the ScaLAPACK library and adopted an early release of the REAL variable arithmetic version in writing our COMPLEX routines.
Reference: [5] <author> T. Cwik, J. Patterson, D. Scott, </author> <title> "Electromagnetic Scattering Calculations on the Intel Touchstone Delta," </title> <booktitle> Proc. of Supercomputing `92, </booktitle> <publisher> IEEE Press, </publisher> <year> 1992, </year> <pages> pp. 538-542. </pages>
Reference-contexts: 1 Introduction Traditional electromagnetic engineering simulations are largely limited by memory requirements, as well as by sequential processing time. Most electromagnetic applications on high performance computing systems so far implemented have been on either massively parallel processors or traditional vector-based supercomputers <ref> [5, 6] </ref>. The advances in workstation cluster technology, represented by clusterable IBM RS/6000 systems and Digital's Alpha farm with both high performance node processors and large memory capacity, provides new opportunities for computational electromagnetic applications which are both CPU time and memory intensive.
Reference: [6] <editor> T. Cwik and J. Patterson (Eds.) </editor> <booktitle> Progress in Electromagnetic Research: Computational Electromag-netics and Supercomputer Architecture, </booktitle> <publisher> EMW Publishing, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Traditional electromagnetic engineering simulations are largely limited by memory requirements, as well as by sequential processing time. Most electromagnetic applications on high performance computing systems so far implemented have been on either massively parallel processors or traditional vector-based supercomputers <ref> [5, 6] </ref>. The advances in workstation cluster technology, represented by clusterable IBM RS/6000 systems and Digital's Alpha farm with both high performance node processors and large memory capacity, provides new opportunities for computational electromagnetic applications which are both CPU time and memory intensive.
Reference: [7] <institution> Development and Implementation of Computational Electromagnetics Techniques on Massively Parallel Architectures, </institution> <note> Vol. 1-5, final project reports, </note> <institution> SRC/NPAC, Syracuse, </institution> <address> NY, </address> <year> 1994. </year>
Reference-contexts: One of the major motivations for our work was to port the CEM application to multiple MPP and distributed computing systems and to evaluate and demonstrate their capabilities. Portability, as well as performance scalability, was among our top objectives <ref> [7] </ref>.
Reference: [8] <author> J. J. Dongarra, R. A. van de Geijn and D. W. Walker, </author> <title> A look at scalable dense linear algebra libraries, </title> <booktitle> Proc. of the Scalable High-Performance Computing Conference, </booktitle> <pages> pp. 372-379, </pages> <year> 1992. </year>
Reference-contexts: One of the major motivations for our work was to port the CEM application to multiple MPP and distributed computing systems and to evaluate and demonstrate their capabilities. Portability, as well as performance scalability, was among our top objectives [7]. Our use of the distributed linear algebra packages ScaLAPACK <ref> [3, 8] </ref> and BLACS [1] in our parallel implementation allowed us to achieve true code portability across: Intel machines (the Intel Delta, an Intel Paragon and a Intel iPSC/860); an IBM SP-1 with either Ethernet or switch connection, and the three distributed configurations described above.
Reference: [9] <author> J. J. Dongarra, R. A. van de Geijn and R. Clint Whaley, </author> <title> A Users Guide to the BLACS, </title> <type> Technical Report, </type> <institution> ORNL/UTK, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: The Basic Linear Algebra Communication Subprograms (BLACS) <ref> [9] </ref> is a library providing simple, portable message-passing for matrix-based operations in parallel linear algebra programs on distributed memory machines. It supports both point-to-point operations between processors and collective communications on a virtual two-dimensional processor grid.
Reference: [10] <editor> LAPACK Users' Guide, E.Anderson, Z.Bai, C.Bischof, J.Demmel, J.Dongarra, J. Du Croz, A.Greenbaum, S.Hammarling, A.McKenney, S.Ostrouchov and D.Sorensen, </editor> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: It supports both point-to-point operations between processors and collective communications on a virtual two-dimensional processor grid. ScaLAPACK is a two Distributed Computational Electromagnetics Systems 3 dimensional distributed memory version of the LAPACK <ref> [10] </ref> package and relies on calls to the Basic Linear Algebra Subprograms (BLAS) for local computation, and calls to the PB-BLAS [4] for global computation. For portability reasons, communications takes place inside the PB-BLAS through calls to the BLACS.
Reference: [11] <institution> Parametric Method of Moments (ParaMoM) RCS Prediction Packages, </institution> <note> Version 1.0, User's Manual, </note> <institution> Syracuse Research Corporation, TD92-1321, </institution> <month> October, </month> <year> 1992. </year>
Reference-contexts: The domains of these functions are curved three-sided surface patches which conform exactly to the underlying global surface representation. The parametric surface patch approach is a foundation to which a comprehensive set of capabilities have been developed and included in ParaMoM 1.0 <ref> [11] </ref>.

References-found: 11

