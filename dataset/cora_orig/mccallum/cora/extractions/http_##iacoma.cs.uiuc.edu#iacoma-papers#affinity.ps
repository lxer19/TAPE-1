URL: http://iacoma.cs.uiuc.edu/iacoma-papers/affinity.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Title: Evaluating the Performance of Cache-Affinity Scheduling in Shared-Memory Multiprocessors  
Author: Josep Torrellas Andrew Tucker, and Anoop Gupta 
Keyword: Shared-Memory Multiprocessors, Cache Memories, Cache Performance, Operat ing System Process Scheduler, Multiprogramming.  
Note: Currently  
Address: CA 94305  1308 West Main Street, Urbana, IL 61801.  
Affiliation: Computer Systems Laboratory Stanford University,  at: Center for Supercomputing Research and Development. University of Illinois at Urbana Champaign. 465 Computer and Systems Research Laboratory,  
Abstract: As a process executes on a CPU, it builds up state in that CPU's cache. In multipro-grammed workloads, the opportunity to reuse this state may be lost when a process gets rescheduled, either because intervening processes destroy its cache state or because the process may migrate to another processor. In this paper, we explore affinity scheduling, a technique that helps reduce cache misses by preferentially scheduling a process on a CPU where it has run recently. Our study focuses on a bus-based multiprocessor executing a variety of workloads, including mixes of scientific, software development, and database applications. In addition to quantifying the performance benefits of exploiting affinity, our study is distinctive in that it provides low-level data from a hardware performance monitor that details why the workloads perform as they do. Overall, for the workloads studied, we show that affinity scheduling reduces the number of cache misses by 7-36%, resulting in execution time improvements of up to 10%. Although the overall improvements are small, modifying the OS scheduler to exploit affinity appears worthwhile|affinity has no negative impact on the workloads and we show that it is extremely simple to add to existing schedulers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Baskett, T. Jermoluk, and D. Solomon. </author> <title> The 4D-MP Graphics Superworkstation: Computing + Graphics = 40 MIPS + 40 MFLOPS and 100,000 Lighted Polygons per Second. </title> <booktitle> In Proceedings of the 33rd IEEE Computer Society International Conference - COMPCON 88, </booktitle> <pages> pages 468-471, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: We conclude in Section 6. 2 Experimental Environment and Workloads The results presented in this paper are based on a Silicon Graphics POWER Station 4D/340 <ref> [1] </ref>, a 33 MHz bus-based multiprocessor with four CPUs. Each processor is a MIPS R3000 with a 64 Kbyte I-cache and a two-level D-cache: the first level is 64 Kbytes and the second level is 256 Kbytes.
Reference: [2] <author> M. Devarakonda and A. Mukherjee. </author> <title> Issues in Implementation of Cache-Affinity Scheduling. </title> <booktitle> In Proceedings Winter 1992 USENIX Conference, </booktitle> <pages> pages 345-357, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Simulations driven by application-only traces of real multiprocessor applications by Gupta et al. [4] with a short time quantum (10 ms) point to small benefits of affinity. However, I/O and other load variations were not considered. A study by Devarakonda and Mukherjee <ref> [2] </ref> evaluated the performance of a real implementation of cache affinity on a multiprocessor, but used synthetic workloads.
Reference: [3] <author> J. Gray. </author> <title> The Benchmark Handbook for Database and Transaction Processing Systems. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Mp3d Scientific Simulation of a rarefied hypersonic flow of 50,000 particles over 15 iterations [8]. Pmake Softw. Develop. 8-process parallel compilation of 24 C files, averaging 460 lines of code each. Oracle Commercial Oracle database [10] running a memory-resident 12-process TP1 benchmark <ref> [3] </ref>. Turning to the other two applications, we see that, while Pmake's processes synchronize infrequently, Oracle's need to synchronize regularly to keep the index structures consistent. Neither workload causes significant invalidation traffic. However, while different processors rarely share the same data in Pmake, processors may share read-mostly data in Oracle.
Reference: [4] <author> A. Gupta, A. Tucker, and S. Urushibara. </author> <title> The Impact of Operating System Scheduling Policies and Synchronization Methods on the Performance of Parallel Applications. </title> <booktitle> In ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 120-132, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Their conclusions were based on the use of a fairly short 16 ms time quantum. Simulations driven by application-only traces of real multiprocessor applications by Gupta et al. <ref> [4] </ref> with a short time quantum (10 ms) point to small benefits of affinity. However, I/O and other load variations were not considered. A study by Devarakonda and Mukherjee [2] evaluated the performance of a real implementation of cache affinity on a multiprocessor, but used synthetic workloads.
Reference: [5] <author> S. Le*er, M. McKusick, M. Karels, and J. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: When an interactive high-priority process wakes up in response to an interrupt, it will immediately preempt a lower-priority process running on the processor that received the interrupt. A similar approach is used in 4.3BSD UNIX <ref> [5] </ref>. In summary, longer time quanta are not as powerful as affinity scheduling. Both schemes are equivalent when processes rarely block. This explains why Vaswani and Zahorjan [15] found that affinity scheduling accomplishes little for scientific applications on a system with long time quanta.
Reference: [6] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The advantages of affinity scheduling would also be more pronounced in a machine with longer miss latencies. This is the case for the new generation of NUMA machines, where remote accesses are costly. For example, on the cluster-based DASH machine developed at Stanford <ref> [6] </ref>, a remote miss takes as long as 130 cycles to be serviced. As the latency increases, the time taken to fill the cache increases, and thus affinity becomes more important. On these machines, issues of geographical affinity for a processor (remote versus local cluster memories) also come into play.
Reference: [7] <editor> E. Lusk, R. Overbeek, et al. </editor> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart, and Winston, Inc., </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: For Oracle, we chose the number of processes for which the application is fastest. The scientific applications are written in C and use the synchronization and sharing primitives provided by the Argonne National Laboratory macro package <ref> [7] </ref>. These three applications have different synchronization and sharing characteristics. For example, while processes in Matrix rarely synchronize, processes in Mp3d synchronize frequently, and Cholesky's processes display an intermediate behavior. Mp3d and Matrix show a low invalidation traffic.
Reference: [8] <author> J. D. McDonald and D. Baganoff. </author> <title> Vectorization of a Particle Simulation Method for Hypersonic Rarified Flow. </title> <booktitle> In AIAA Thermodynamics, Plasmadynamics and Lasers Conference, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: Application Domain Description Matrix Scientific Blocked multiplication of two 512x512 matrices using 64x64 blocks. Cholesky Scientific Cholesky factorization of a sparse matrix with 10K rows and 200K non-zeroes in its lower triangle [11]. Mp3d Scientific Simulation of a rarefied hypersonic flow of 50,000 particles over 15 iterations <ref> [8] </ref>. Pmake Softw. Develop. 8-process parallel compilation of 24 C files, averaging 460 lines of code each. Oracle Commercial Oracle database [10] running a memory-resident 12-process TP1 benchmark [3].
Reference: [9] <author> J. Mogul and A. Borg. </author> <title> The Effect of Context Switches on Cache Performance. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 75-84, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: They concluded that unconditionally fixing processes onto processors, while allowing processes to reuse more cache state, causes too much load imbalance. This load imbalance then results in fairness problems and idle time. Mogul and Borg <ref> [9] </ref> used address traces of a variety of real workloads to study the potential for cache reuse. They found cache reload overheads of up to 8% of the execution time, depending on the workload.
Reference: [10] <author> Oracle Corporation. </author> <title> Oracle Database Administrator's Guide. Oracle Corporation, </title> <address> Belmont CA, </address> <year> 1989. </year>
Reference-contexts: Mp3d Scientific Simulation of a rarefied hypersonic flow of 50,000 particles over 15 iterations [8]. Pmake Softw. Develop. 8-process parallel compilation of 24 C files, averaging 460 lines of code each. Oracle Commercial Oracle database <ref> [10] </ref> running a memory-resident 12-process TP1 benchmark [3]. Turning to the other two applications, we see that, while Pmake's processes synchronize infrequently, Oracle's need to synchronize regularly to keep the index structures consistent. Neither workload causes significant invalidation traffic.
Reference: [11] <author> E. Rothberg and A. Gupta. </author> <title> Techniques for Improving the Performance of Sparse Matrix Factorization on Multiprocessor Workstations. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 232-243, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Table I: Parallel applications used in the workloads measured. Application Domain Description Matrix Scientific Blocked multiplication of two 512x512 matrices using 64x64 blocks. Cholesky Scientific Cholesky factorization of a sparse matrix with 10K rows and 200K non-zeroes in its lower triangle <ref> [11] </ref>. Mp3d Scientific Simulation of a rarefied hypersonic flow of 50,000 particles over 15 iterations [8]. Pmake Softw. Develop. 8-process parallel compilation of 24 C files, averaging 460 lines of code each. Oracle Commercial Oracle database [10] running a memory-resident 12-process TP1 benchmark [3].
Reference: [12] <author> M. Squillante and E. Lazowska. </author> <title> Using Processor-Cache Affinity in Shared-Memory Multiprocessor Scheduling. </title> <type> Technical Report 89-060-01, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: However, few of them have conducted a detailed analysis of the benefits of affinity scheduling on a real multiprocessor with real workloads. As a consequence, they do not consider many of the subtle issues and complex interactions of workload, architecture, and scheduler discussed here. Squillante and Lazowska <ref> [12] </ref> measured response time under different affinity based scheduling policies. Their results suggest that affinity scheduling provides substantial benefits. Their results, however, are based on analytical calculations with simple machine and application models, rather than a real implementation.
Reference: [13] <author> M. Squillante and R. Nelson. </author> <title> Analysis of Task Migration in Shared-Memory Multiprocessor Scheduling. </title> <booktitle> 19 In ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 143-155, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Their results suggest that affinity scheduling provides substantial benefits. Their results, however, are based on analytical calculations with simple machine and application models, rather than a real implementation. Another analytical study was performed by Squillante and Nelson <ref> [13] </ref> studying the effects of migrating processes among processors. They concluded that unconditionally fixing processes onto processors, while allowing processes to reuse more cache state, causes too much load imbalance. This load imbalance then results in fairness problems and idle time.
Reference: [14] <author> S. S. Thakkar and M. Sweiger. </author> <title> Performance of an OLTP Application on Symmetry Multiprocessor System. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 228-238, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: One study that did measure real applications on a real system, by Thakkar and Sweiger <ref> [14] </ref>, looked only at the performance of a database system under an extreme form of affinity. The authors studied a database application running in a 12 to 24 processor machine.
Reference: [15] <author> R. Vaswani and J. Zahorjan. </author> <title> The Implications of Cache Affinity on Processor Scheduling for Multiprogrammed, Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 26-40, </pages> <month> October </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: Bus contention increases miss latency, increasing the potential for gains from affinity. Finally, Vaswani and Zahorjan <ref> [15] </ref> also used real applications and a real implementation of cache affinity. They studied a system under space sharing, a sophisticated scheduling scheme that partitions processors among applications. They found that, due to the relative infrequency of process preemption, the benefits of affinity scheduling were minimal. <p> A similar approach is used in 4.3BSD UNIX [5]. In summary, longer time quanta are not as powerful as affinity scheduling. Both schemes are equivalent when processes rarely block. This explains why Vaswani and Zahorjan <ref> [15] </ref> found that affinity scheduling accomplishes little for scientific applications on a system with long time quanta. <p> Attaching processes to processors creates load imbalance and, in addition, does not decrease the rate of context switches. Extending the time quantum accomplishes most of the gains possible for processes that rarely block (making affinity scheduling fairly useless for such workloads as show by Vaswani and Za-horjan <ref> [15] </ref>), but, unlike affinity scheduling, can not benefit processes that run only for a short time before blocking. Some obvious hardware factors affect our results.
References-found: 15

