URL: http://www.cse.ogi.edu/~denni/Publications/somjets.ps.Z
Refering-URL: http://www.cse.ogi.edu/~denni/publications.html
Root-URL: http://www.cse.ogi.edu
Title: LU TP 91-4 Self-organizing Networks for Extracting Jet Features  
Author: Leif Lonnblad Carsten Peterson Hong Pi and Thorsteinn Rognvaldsson 
Note: Computer Physics Communications  1 thepll@seldc52 (bitnet) leif@thep.lu.se (internet) 2 thepcap@seldc52 (bitnet) carsten@thep.lu.se (internet) 3 thephp@seldc52 (bitnet) pihong@thep.lu.se (internet) 4 thepdr@seldc52 (bitnet) denni@thep.lu.se (internet)  
Address: Solvegatan 14A, S-22362 Lund, Sweden  
Affiliation: Department of Theoretical Physics, University of Lund  
Date: March 1991  67, 193 (1991)  
Abstract: Self-organizing neural networks are briefly reviewed and compared with supervised learning algorithms like back-propagation. The power of self-organization networks is in their capability of displaying typical features in a transparent manner. This is successfully demonstrated with two applications from hadronic jet physics; hadronization model discrimination and separation of b,c and light quarks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Lonnblad, C. Peterson and T. Rognvaldsson, </author> <title> "Finding Gluon Jets with a Neural Trigger", </title> <journal> Physical Review Letters 65, </journal> <month> 1321 </month> <year> (1990). </year>
Reference-contexts: It hence depends both on the shape of g (x) and the type of input. If g (x) is a tanh (x) type sigmoid function and the input is normalized, the argument x 2 <ref> [0; 1] </ref>, the temperature should be T&lt; 1=3. Alternatively, if g (x) is a Gaussian function (16) it is important not to get overflow in the exponent, in which case the network will not respond.
Reference: [2] <author> L. Lonnblad, C. Peterson and T. Rognvaldsson, </author> <title> "Using Neural Networks to Identify Jets", </title> <journal> Nuclear Physics B 349, </journal> <month> 675 </month> <year> (1991). </year>
Reference-contexts: These inhibitory connections constitute a "soft" way of implementing a winner-takes-all mechanism. A "hard" way of doing the same thing would be to replace the different N h sigmoidal nodes with a single Potts unit <ref> [2] </ref> with N h components, each updated according to h j = P with a j = k where T is a "temperature" setting the gain of the generalized sigmoid of eq. (13). 2.3 Topological ordering It would be desirable to have a topological order among the feature units, where neighbouring <p> In this section we demonstrate how self-organizing networks can be used to accomplish this for two problems: jet flavor separation and hadron model discrimination. Heavy quark jets were separated from those stemming from light quarks in ref. <ref> [2] </ref> using the supervised BP algorithm with very good results. Small but significant differences between data generated by the different MC programs were observed, especially in the case of bottom jets. <p> The output layer has 3 output units encoding the flavor (100, 010 and 001 for uds, c and b respectively). Winner-takes-all criterium <ref> [2] </ref> is used for establishing the output class. The resulting overall generalization performance is 51%. by using the winner-takes-all criterion. To visualize the output space of the BP network we exploit the fact that all possible output vectors fall inside the cube defined by the unit vectors (1,0,0,) (0,1,0), (0,0,1). <p> jets with large momentum leading particles have ~ 90% of the total jet energy gathered in the four leading particles, and the jets with an even distribution of the momentum only contain ~ 50% of the total jet energy in the four leading particles. 3.3 Hadronization model discrimination In ref. <ref> [2] </ref>, when using BP learning on a network to separate out b-jets from others, we noticed a small but significant difference between b-jets produced by ARIADNE and b-jets produced by HERWIG. <p> As we have seen in the previous subsection this leading particle effect is exactly what the network uses for distinguishing light quark jets from b-jets. Hence it is clear why this separation was more difficult in the ARIADNE case than in the HERWIG case in ref. <ref> [2] </ref>. This leading particle effect does not come primarily from the hadronization process but rather from the weak decay of the produced B-meson which typically gives rise to 5 10 hadrons. There are however differences in the B-decay in the two programs.
Reference: [3] <author> P. Bhat, L. Lonnblad, K. Meier and K. Sugano, </author> <title> "Using Neural Networks to Identify Jets in Hadron-Hadron Collisions", LU TP 90-13 (to appear in Proc. </title> <booktitle> of the 1990 DPF Summer Study on High Energy Physics Research Directions for the Decade, </booktitle> <address> Colorado, </address> <year> 1990). </year>
Reference: [4] <author> D. E. Rumelhart, G. E. Hinton and R. J. Williams, </author> <title> "Learning Internal Representations by Error Propagation", </title> <editor> in D. E. Rumelhart and J. L. McClelland (eds.) </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition (Vol. </booktitle> <volume> 1), </volume> <publisher> MIT Press (1986). </publisher>
Reference-contexts: This is exactly what artificial neural networks (ANN) aim at. In refs. [1][2][3] this approach was successfully applied to the problem of identifying the origin of jets in e + e annihilation and hadron-induced large p T reactions using feed-forward networks with the back-propagation learning algorithm <ref> [4] </ref>. This approach is an example of supervised learning where the features to be recognized are known beforehand. <p> The purpose of this work is to explore such self-organizing networks in the context of jet identification. The most commonly used supervised learning algorithm is back-propagation <ref> [4] </ref> in multi-layered feed-forward neural networks.
Reference: [5] <author> See e.g. R. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley & Sons (1973). </publisher>
Reference-contexts: In particular one has the following virtues: * Very good generalization performance. In some low-dimensional jet identification applications, where it has been computationally feasable to compute overlaps between distributions, one reaches values close to the theoretical Bayesian limit <ref> [5] </ref>, given by the minimum overlap of the multidimensional distributions. * The method is fairly insensitive to choice of parameters. * Limited experimental acceptance and noise problems are easily handled since the network is forced to deal with and to ignore these things respectively. These are algorithmic issues.
Reference: [6] <author> J. MacQueen. </author> <title> "Some Methods for Classification and Analysis of Multivariate Observations", </title> <editor> in L. M. LeCam and J. Neyman (eds.), </editor> <booktitle> Proc. 5th Berkeley Symposium on Math. </booktitle> <editor> Stat. and Prob., U. </editor> <publisher> California Press, </publisher> <address> Berkeley (1967). </address>
Reference-contexts: This procedure is identical to the "adaptive k-means" clustering algorithm <ref> [6] </ref>.
Reference: [7] <author> T. Kohonen, </author> <title> "Self Organized Formation of Topologically Correct Feature Maps", </title> <booktitle> Biolological Cybernetics 43, 59 (1982); T. Kohonen, Self-organization and Associative Memory, third edition, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg (1990). </address>
Reference-contexts: A commonly 5 used technique to achieve such a topological map is by updating units close to the winner node in the same way as the winner node <ref> [7] </ref>. In this way neighbouring units will end up with similar weight vectors. <p> Annealing the network using this potential will create a group of winners instead of one single winner. The "hat" width is decreased as learning proceeds. This dynamic method, and other similar versions [8], is very sensitive to the choice of parameters and there exists a robust short-cut <ref> [7] </ref> version of this procedure where the entire neighbourhood defined by is updated. This modified algorithm is used throughout this paper and is implemented in JETMAP 1.0 [9] (see Appendix B). <p> We have subsequently used it for all the applications studied in this paper. 2.5 Learning vector quantization Although the self-organizing map is an unsupervised learning algorithm, for classification purpose one can augment it with supervised learning for fine tuning the units specific to certain features <ref> [7] </ref>. This is called learning vector quantization (LVQ) and amounts to learning correct answers and unlearning incorrect answers, i.e., the learning rate parameter in Eq. (7) supplemented with a minus sign whenever an input data activates a unit that has been assigned to the other types of data. <p> This takes much longer time than finding the coarse mapping. We therefore let the learning rate decrease geometrically with the number of updatings, (t + 1) = k (t), where k 1. Typically 0:9 in the beginning and 0:01 towards the end of training. An alternative way <ref> [7] </ref> is to specify a start up value and a final value for and then let it decrease as a linear function of the updating steps. Temperature. The temperature T sets the slope of the response function g (x).
Reference: [8] <author> G. Tattersall, </author> <title> "Neural Map Applications", </title> <editor> in I. Aleksander (ed.) </editor> <booktitle> Neural Computing Architectures, </booktitle> <publisher> North Oxford Academic (1989). </publisher>
Reference-contexts: The "mexican hat" potential enforces signals to neighbouring units and inhibits signals to units some distance apart. Annealing the network using this potential will create a group of winners instead of one single winner. The "hat" width is decreased as learning proceeds. This dynamic method, and other similar versions <ref> [8] </ref>, is very sensitive to the choice of parameters and there exists a robust short-cut [7] version of this procedure where the entire neighbourhood defined by is updated. This modified algorithm is used throughout this paper and is implemented in JETMAP 1.0 [9] (see Appendix B). <p> If the dimensionality of the network equals the inherent dimensionality, then the network will be globally ordered. If the dimensionality of the network is lower than the inherent dimensionality of the data, the network will be locally ordered but not globally <ref> [8] </ref>. In our applications we have used a two-dimensional net, which has worked well. If one class should occur in several small "islands" in the map, it is usually a sign of a too small dimensionality of the net.
Reference: [9] <author> L. Lonnblad, C. Peterson, H. Pi and T. Rognvaldsson, </author> <note> JETMAP 1.0 program and manual [Available via BITNET request]. </note>
Reference-contexts: This dynamic method, and other similar versions [8], is very sensitive to the choice of parameters and there exists a robust short-cut [7] version of this procedure where the entire neighbourhood defined by is updated. This modified algorithm is used throughout this paper and is implemented in JETMAP 1.0 <ref> [9] </ref> (see Appendix B).
Reference: [10] <author> T. Kohonen, G. Barna and R. Chrisley, </author> <title> "Statistical Pattern Recognition: </title> <booktitle> Bench-marking Studies", Proceedings of the IEEE Second International Conference on Neural Networks, </booktitle> <address> San Diego, California (1988). </address>
Reference-contexts: Even with a number of feature nodes that increased with the dimensionality of the problem (q) the performance was not impressive 6 . 5 The conclusions with respect to performance of BP in <ref> [10] </ref> are in error since the architecture used for BP was inappropriate for the problem. We refer to ref. [13] for a proper discussion of this issue. 6 LVQ would presumably have reached the Bayesian limit if the authors of ref. [10] would have allowed for a number of feature nodes <p> The conclusions with respect to performance of BP in <ref> [10] </ref> are in error since the architecture used for BP was inappropriate for the problem. We refer to ref. [13] for a proper discussion of this issue. 6 LVQ would presumably have reached the Bayesian limit if the authors of ref. [10] would have allowed for a number of feature nodes increasing like e.g. 2 q . 7 3 Jet Feature Applications There exists today a number of QCD Monte Carlo (MC) programs which describe experimental data from e.g. LEP very well [16].
Reference: [11] <author> J. Moody and C. Darken, </author> <title> "Learning with Localized Receptive Fields", </title> <editor> in D. Touretzky, G. Hinton and T.S. Sejnowski (eds.), </editor> <booktitle> Proceedings of the 1988 Con-nectionists Models Summer School, </booktitle> <institution> Carnegie Mellon University, </institution> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann (1988). </publisher>
Reference-contexts: In many cases, like jet classification, the magnitude of the input vector is a significant feature and one would prefer the network to work with unnormalized input. One simple way to do this is by introducing a Gaussian response function for the feature 6 units <ref> [11] </ref> h j = e (~! j ~x) 2 =T (16) where the updating again is done according to eq. (8), with the difference that the weight vectors ~! are not normalized after each updating. The temperature T sets the width of the function (see appendix A).
Reference: [12] <author> I. Scabai, F. Czako and Z. Fodor, </author> <title> "Quark and Gluon Jet Separation using Neural Networks", </title> <note> ITP Budapest Report 477 (1990). 25 </note>
Reference-contexts: In this way it is sometimes possible to achieve as good results for pattern classification as with a feed-forward network like back-propagation (BP) and it has successfully been used for quark/gluon separation in ref. <ref> [12] </ref>. The relative performance of LVQ and BP strongly depends upon the dimensionality of the problem and the number of feature nodes. For most practical purposes one uses Gaussian units in feature maps (eq. (16)). <p> Learning vector quantization. Standard LVQ can be done in two different ways: One is to assign classes to the feature nodes right from the start <ref> [12] </ref>, another is to allow the network to do a map first and then assign classes to the nodes. We have used the latter method in this paper but both methods are possible in JETMAP 1.0.
Reference: [13] <author> C. Peterson and E. Hartman, </author> <title> "Explorations of the Mean Field Theory Learning Algorithm", </title> <booktitle> Neural Networks 2 475 (1989). </booktitle>
Reference-contexts: h j 's settle according to h j = g ( j 0 X ! jk x k ) (10) Eq. (10) corresponds to minimizing the energy E = 2 jj 0 X ! jk x k (11) averaging different clusters of input categories. using the mean field theory approximation <ref> [13] </ref> on binary decision elements H j 2 f0; 1g. A straight-forward choice of inhibitory T jj 0 is T jj 0 = ff (1 ffi jj 0 ) (12) where ff is a parameter governing the relative strength between the inhibition and the external input. <p> We refer to ref. <ref> [13] </ref> for a proper discussion of this issue. 6 LVQ would presumably have reached the Bayesian limit if the authors of ref. [10] would have allowed for a number of feature nodes increasing like e.g. 2 q . 7 3 Jet Feature Applications There exists today a number of QCD Monte <p> The solid curve is for the test set and the dashed one for the training set. The number accompanying the arrow is the neighbourhood size . The arrow marked by "LVQ" indicates the duration when Learning Vector Quantization is applied. Mean Field Theory learning <ref> [13] </ref> [15] would then be preferable over the feed-forward BP algorithm. 16 by ARIADNE and (b) jets generated by HERWIG. (c) The resulting map where the shading indicates the dominant model for that unit (cf. fig. 7).
Reference: [14] <author> D.H. Ackley, G.E. Hinton and T.J. Sejnowski, </author> <title> "A Learning Algorithm for Boltzmann Machines", </title> <booktitle> Cognitive Science 9, </booktitle> <month> 147 </month> <year> (1985). </year>
Reference-contexts: Indeed, from the "benchmark" studies in refs. [10][13] it is clear that LVQ had difficulties in reaching the Bayesian limit for synthetic problems consisting of overlapping high dimensional Gaussians in contrast to BP and other supervised learning algorithms based on sigmoidal units like the Boltzmann machine <ref> [14] </ref> and Mean Field Theory learning [13][15] 5 .
Reference: [15] <author> C. Peterson and J.R. Anderson, </author> <title> "A Mean Field Theory Learning Algorithm for Neural Networks", </title> <journal> Complex Systems 1, </journal> <month> 995 </month> <year> (1987). </year>
Reference-contexts: The solid curve is for the test set and the dashed one for the training set. The number accompanying the arrow is the neighbourhood size . The arrow marked by "LVQ" indicates the duration when Learning Vector Quantization is applied. Mean Field Theory learning [13] <ref> [15] </ref> would then be preferable over the feed-forward BP algorithm. 16 by ARIADNE and (b) jets generated by HERWIG. (c) The resulting map where the shading indicates the dominant model for that unit (cf. fig. 7).
Reference: [16] <author> M. Z. Akrawy et. al., </author> <title> "Event Shape Distributions in the Hadronic Decays of the Z 0 ", Z. </title> <journal> Phys C47, </journal> <month> 505 </month> <year> (1990). </year>
Reference-contexts: LEP very well <ref> [16] </ref>. Most of them are structured in the same way; one part dealing with the hard interaction (e.g. e + e ! Z 0 ! q q) and the perturbative evolution into multi-parton states. <p> Whereas the perturbative part is fairly well understood theoretically, the non-perturbative hadronization can only be described by phenomenological models. Comparisons between MC models and experimental data as in ref. <ref> [16] </ref> are all based on averaged features such as inclusive distributions, event shape parameters, multiplicities etc..
Reference: [17] <author> L. Lonnblad, "ARIADNE-3, </author> <title> A Monte Carlo for QCD Cascades in the Colour Dipole Formulation", </title> <type> Lund preprint LU TP 89-10. </type>
Reference-contexts: For both applications studied in this section we also compare the classification powers of self-organizing networks with those of supervised BP. 3.1 The Monte Carlo programs We use two different MC programs to generate jets; ARIADNE 3.1 <ref> [17] </ref> and HERWIG 3.4 [18]. ARIADNE is a program implementing the colour dipole approximation of perturbative QCD cascades [19] which uses the Lund string fragmentation [20] as implemented in JETSET 7.3 [21]. It also uses JETSET to perform particle decays. <p> This string is then fragmented into hadrons by forming q q pairs along the string. For a more detailed description of how these models are implemented in the programs we refer to refs. <ref> [17] </ref> [18] [21]. All jets used in our analysis are taken from two-jet events generated in e + e annihilation at the Z 0 peak.
Reference: [18] <author> G. Marchesini and B.R. Webber, </author> <title> "Monte Carlo Simulations of General Hard Processes with Coherent QCD Radiation", </title> <journal> Nuclear Physics B310, </journal> <note> 461 (1988); I. </note> <author> G. Knowles, </author> <title> "Spin Correlations in Parton-parton Scattering", </title> <journal> Nuclear Physics B310, </journal> <month> 571 </month> <year> (1988). </year>
Reference-contexts: For both applications studied in this section we also compare the classification powers of self-organizing networks with those of supervised BP. 3.1 The Monte Carlo programs We use two different MC programs to generate jets; ARIADNE 3.1 [17] and HERWIG 3.4 <ref> [18] </ref>. ARIADNE is a program implementing the colour dipole approximation of perturbative QCD cascades [19] which uses the Lund string fragmentation [20] as implemented in JETSET 7.3 [21]. It also uses JETSET to perform particle decays. <p> It also uses JETSET to perform particle decays. Throughout this section we only generate 2-jet events where the perturbative QCD shower is not very important, hence ARIADNE can be considered essentially equivalent to JETSET. HERWIG differs from ARIADNE/JETSET mainly in the hadronization part where it uses a cluster fragmentation <ref> [18] </ref> procedure. Here the gluons formed in the perturbative 8 QCD shower are forced to split into q q pairs, which are then combined to form colorless clusters. If the mass of such a cluster is above a certain limit it will decay anisotropically into two smaller clusters. <p> This string is then fragmented into hadrons by forming q q pairs along the string. For a more detailed description of how these models are implemented in the programs we refer to refs. [17] <ref> [18] </ref> [21]. All jets used in our analysis are taken from two-jet events generated in e + e annihilation at the Z 0 peak. The clustering into jets is performed using the LUCLUS algorithm found in the JETSET program, with the value of the "d-join" parameter set to 5 GeV.
Reference: [19] <author> G. Gustafson, </author> <title> "Dual Description of a Confined Gluon Field", </title> <journal> Physics Letters B175, </journal> <note> 453 (1986); G. </note> <author> Gustafson, U. Pettersson, </author> <title> "Dipole Formulation of QCD Cascades", </title> <journal> Nuclear Physics B306, </journal> <note> 746 (1988); B. </note> <author> Andersson, G. Gustafson, L. Lonnblad, </author> <title> "Gluon Splitting in the Colour Dipole Cascades", </title> <journal> Nuclear Physics B339, </journal> <month> 393 </month> <year> (1990). </year>
Reference-contexts: ARIADNE is a program implementing the colour dipole approximation of perturbative QCD cascades <ref> [19] </ref> which uses the Lund string fragmentation [20] as implemented in JETSET 7.3 [21]. It also uses JETSET to perform particle decays. Throughout this section we only generate 2-jet events where the perturbative QCD shower is not very important, hence ARIADNE can be considered essentially equivalent to JETSET.
Reference: [20] <author> B. Andersson and G. Gustafson, </author> <title> "Semiclassical Models for Gluon Jets and Lep-toproduction Based on the Massless Relativistic String", </title> <journal> Z. Phys. </journal> <note> C3 (1980) 223; B. </note> <author> Andersson, G. Gustafson, G. Ingelman, T. Sjostrand, </author> <title> "Parton Fragmentation and String Dynamics", </title> <journal> Phys. Rep. </journal> <volume> 97, </volume> <month> 31 </month> <year> (1983). </year>
Reference-contexts: ARIADNE is a program implementing the colour dipole approximation of perturbative QCD cascades [19] which uses the Lund string fragmentation <ref> [20] </ref> as implemented in JETSET 7.3 [21]. It also uses JETSET to perform particle decays. Throughout this section we only generate 2-jet events where the perturbative QCD shower is not very important, hence ARIADNE can be considered essentially equivalent to JETSET.
Reference: [21] <author> T. Sjostrand, </author> <title> JETSET 7.2 program and manual. </title> <editor> See B. Bambah et al., </editor> <title> QCD Generators for LEP, </title> <publisher> CERN-TH.5466/89. </publisher>
Reference-contexts: ARIADNE is a program implementing the colour dipole approximation of perturbative QCD cascades [19] which uses the Lund string fragmentation [20] as implemented in JETSET 7.3 <ref> [21] </ref>. It also uses JETSET to perform particle decays. Throughout this section we only generate 2-jet events where the perturbative QCD shower is not very important, hence ARIADNE can be considered essentially equivalent to JETSET. <p> This string is then fragmented into hadrons by forming q q pairs along the string. For a more detailed description of how these models are implemented in the programs we refer to refs. [17] [18] <ref> [21] </ref>. All jets used in our analysis are taken from two-jet events generated in e + e annihilation at the Z 0 peak. The clustering into jets is performed using the LUCLUS algorithm found in the JETSET program, with the value of the "d-join" parameter set to 5 GeV.
Reference: [22] <author> L. Lonnblad, C. Peterson and T. Rognvaldsson, </author> <note> JETNET 2.0 program and manual [to be released; an earlier version JETNET 1.1 is available via BITNET request]. </note>
Reference-contexts: The test set is used to measure the networks ability to generalize. In the unsupervised case, when no classification is done, we use all 9000 jets. With the supervised BP algorithm, as implemented in the subroutine package JETNET 2.0 <ref> [22] </ref>, we first make a "benchmark" study to establish the existence of a difference between jets of different flavors. <p> It implements the self-organizing ANN as it is described in section 2 and appendix A. The package and manual is available on request via BITNET. It is designed to be similar to the subroutine package JETNET 2.0 <ref> [22] </ref> that uses the BP algorithm. The user interface consists of one common block and eight subroutines. All information handling between the program and the user is executed via the common block /JMDAT1/.
Reference: [23] <author> J.W. Gary, </author> <title> Private communication. </title>
Reference-contexts: There are however differences in the B-decay in the two programs. For example; it has been observed that in HERWIG the multiplicity in B-decays is somewhat larger than in JETSET <ref> [23] </ref> (which is used by ARIADNE). Since we are here mainly interested in differences in the hadronization scheme we generated one set of b-jets where the hadronization was made by HERWIG but all hadronic decays were performed by JETSET.
Reference: [24] <author> B.A. Kniehl, J.H. Kuhn, and R.G. Stuart, </author> <note> "Hadronic and Photonic Corrections to A LR ", in Polarization at LEP (Vol. 1), CERN-88-06. </note>
Reference: [25] <author> G. Arnison et. al. </author> <title> (UA1 collaboration), "Analysis of the Fragmentation Properties of Quark and Gluon Jets at the CERN SPS p(p) Collider", </title> <journal> Nuclear Physics B276, </journal> <month> 253 </month> <year> (1986). </year>
Reference: [26] <author> B. Andersson, G. Gustafson, T. Sjostrand, </author> <title> "How to find the Gluon Jets in e + e Annihilation", </title> <journal> Physics Letters B94, </journal> <month> 211 </month> <year> (1980). </year>
Reference: [27] <author> L. Bellantoni, J. Conway, J. Jacobsen, Y.B. Pan and S.L. Wu, </author> <title> "Identifying b-jets with a Neural Network Method", </title> <note> to be published. 27 </note>
Reference-contexts: This analysis could fairly easily be compared with data from e.g. LEP by selecting two-jet events where one jet has been identified as a b-jet by e.g. lepton tagging. The remaining jets in these events would then make a pure sample of b-jets <ref> [27] </ref> which could be analyzed with this self-organized network, with the prospect of experimental discrimination of the two models. 14 described in the text. 4 Summary We have demonstrated how self-organizing networks can reach similar performance levels to those of supervised learning in jet classifying situations.
References-found: 27

