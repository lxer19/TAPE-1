URL: ftp://ftp.cs.brown.edu/pub/techreports/96/cs96-09.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-96-09.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> N. Abe and M. Warmuth. </author> <title> On the computational complexity of approximating distributions by probabilistic automata. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 205-260, </pages> <year> 1992. </year>
Reference: [2] <author> David H. Ackley and Michael L. Littman. </author> <title> Generalization and scaling in reinforcement learning. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 550-557, </pages> <address> San Mateo, California, 1990. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: [3] <author> David H. Ackley and Michael L. Littman. </author> <title> Interactions between learning and evolution. </title> <editor> In C. Langton, C. Taylor, J. D. Farmer, and S. Ramussen, editors, </editor> <booktitle> Artificial Life II: Santa Fe Institute Studies in the Sciences of Complexity, </booktitle> <volume> volume 10, </volume> <pages> pages 487-509. </pages> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference: [4] <author> Aristotle Arapostathis, Vivek S. Borkar, Emmanuel Fernandez-Gaucherand, Mri-nal K. Ghosh, and Steven I. Marcus. </author> <title> Discrete-time controlled Markov processes with average cost criterion: A survey. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 31(2) </volume> <pages> 282-344, </pages> <month> March </month> <year> 1993. </year>
Reference: [5] <author> K. J. Astrom. </author> <title> Optimal control of Markov decision processes with incomplete state estimation. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 10 </volume> <pages> 174-205, </pages> <year> 1965. </year>
Reference: [6] <author> Yuri Bahturin. </author> <title> Basic Structures of Modern Algebra. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> Massachusetts, </address> <year> 1993. </year>
Reference-contexts: A stronger result might be to show that solving the optimization problem is equivalent to finding exact solutions to arbitrary polynomial equations. Galois theory tells us that there is no finite-time algorithm (restricted to simple arithmetic operations and roots) for solving arbitrary polynomial equations <ref> [6] </ref>. Thus, such problems are, in a sense, uncomputable.
Reference: [7] <author> Leemon Baird. </author> <title> Residual algorithms: Reinforcement learning with function approximation. </title> <editor> In Armand Prieditis and Stuart Russell, editors, </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 30-37, </pages> <address> San Francisco, California, 1995. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 246 247 </pages>
Reference: [8] <author> Leemon C. Baird and A. H. Klopf. </author> <title> Reinforcement learning with high-dimensional, continuous actions. </title> <type> Technical Report WL-TR-93-1147, </type> <institution> Wright-Patterson Air Force Base Ohio: Wright Laboratory, </institution> <year> 1993. </year>
Reference: [9] <author> Andrew G. Barto, S. J. Bradtke, and Satinder P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72(1) </volume> <pages> 81-138, </pages> <year> 1995. </year>
Reference: [10] <author> Andrew G. Barto, Richard S. Sutton, and Christopher J. C. H. Watkins. </author> <title> Learning and sequential decision making. </title> <type> Technical Report 89-95, </type> <institution> Department of Computer and Information Science, University of Massachusetts, Amherst, Mas-sachusetts, </institution> <year> 1989. </year> <title> Also published in Learning and Computational Neuroscience: Foundations of Adaptive Networks, </title> <editor> Michael Gabriel and John Moore, editors. </editor> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference: [11] <author> Leonard E. Baum, Ted Petrie, George Soules, and Norman Weiss. </author> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> Annals of Mathmatical Statistics, </journal> <volume> 41(1) </volume> <pages> 164-171, </pages> <year> 1970. </year>
Reference: [12] <author> R. Beckers, O. E. Holland, and J. L. Deneubourg. </author> <title> From local actions to global tasks: Stigmergy and collective robotics. </title> <editor> In Rodney A. Brooks and Pattie Maes, editors, </editor> <booktitle> Artificial Life IV: Proceedings of the Fourth International Workshop on the Synthesis and Simulation of Living Systems, </booktitle> <pages> pages 181-189, </pages> <address> Cambridge, Mas-sachusetts, 1994. </address> <publisher> Bradford Books/MIT Press. </publisher>
Reference: [13] <author> Richard Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1957. </year>
Reference: [14] <author> Y. Bengio and P. Frasconi. </author> <title> An input/output HMM architecture. </title> <editor> In G. Tesauro, D. S. Touretzky, and T.K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 427-434, </pages> <address> Cambridge, Massachusetts, 1995. </address> <publisher> The MIT Press. </publisher>
Reference: [15] <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1987. </year>
Reference: [16] <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, Massachusetts, </address> <year> 1995. </year> <note> Volumes 1 and 2. 248 </note>
Reference: [17] <author> Dimitri P. Bertsekas and David A. Casta~non. </author> <title> Adaptive aggregation methods for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 34(6) </volume> <pages> 589-598, </pages> <month> June </month> <year> 1989. </year>
Reference: [18] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: Alternate proofs for the mdp case are given by Bertsekas and Tsitsiklis <ref> [18] </ref>, and Tseng [162]. Define w (x 0 ) = 0 and w (x) = max u P ; that is, w (x) is the maximum expected steps to absorption from state x over all policies.
Reference: [19] <author> David Blackwell. </author> <title> Discrete dynamic programming. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 33(2) </volume> <pages> 719-726, </pages> <month> June </month> <year> 1962. </year>
Reference: [20] <author> R. G. Bland. </author> <title> New finite pivoting rules for the simplex method. </title> <journal> Mathematics of Operations Research, </journal> <volume> 2(2) </volume> <pages> 103-107, </pages> <month> May </month> <year> 1977. </year>
Reference: [21] <author> Jim Blythe. </author> <title> Planning with external events. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <year> 1994. </year>
Reference: [22] <author> Craig Boutilier. </author> <title> Imposed and learned conventions in multiagent decision processes: Extended abstract. </title> <type> Unpublished manuscript, </type> <year> 1995. </year>
Reference: [23] <author> Craig Boutilier, Thomas Dean, and Steve Hanks. </author> <title> Planning under uncertainty: Structural assumptions and computational leverage. </title> <booktitle> In Proceedings of the Second European Workshop on Planning, </booktitle> <year> 1995. </year>
Reference: [24] <author> Craig Boutilier, Richard Dearden, and Moises Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference: [25] <author> Craig Boutilier and David Poole. </author> <title> Computing optimal policies for partially observable decision processes using compact representations. </title> <type> Unpublished manuscript, </type> <year> 1995. </year>
Reference: [26] <author> Justin A. Boyan. </author> <title> Modular neural networks for learning context-dependent game strategies. </title> <type> Master's thesis, </type> <institution> Department of Engineering and Computer Laboratory, University of Cambridge, </institution> <address> Cambridge, United Kingdom, </address> <month> August </month> <year> 1992. </year>
Reference: [27] <author> Justin A. Boyan and Michael L. Littman. </author> <title> Packet routing in dynamically changing networks: A reinforcement learning approach. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 671-678. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1994. </year> <month> 249 </month>
Reference: [28] <author> Justin A. Boyan and Andrew W. Moore. </author> <title> Algorithms for approximating optimal value functions in acyclic domains. </title> <type> Unpublished manuscript, </type> <year> 1995. </year>
Reference: [29] <author> Justin A. Boyan and Andrew W. Moore. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <address> Cambridge, Massachusetts, 1995. </address> <publisher> The MIT Press. </publisher>
Reference: [30] <author> Justin A. Boyan, Andrew W. Moore, and Richard S. Sutton. </author> <title> Proceedings of the workshop on value function approximation, Machine Learning Conference 1995. </title> <type> Technical Report CMU-CS-95-206, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1995. </year>
Reference: [31] <author> Dima Burago, Michel de Rougemont, and Anatol Slissenko. </author> <title> On the complexity of partially observed Markov decision processes. </title> <note> Theoretical Computer Science, to appear. </note>
Reference: [32] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, Washington, </address> <year> 1994. </year>
Reference: [33] <author> Hsien-Te Cheng. </author> <title> Algorithms for Partially Observable Markov Decision Processes. </title> <type> PhD thesis, </type> <institution> University of British Columbia, British Columbia, Canada, </institution> <year> 1988. </year>
Reference: [34] <author> Lonnie Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 183-188, </pages> <address> San Jose, California, 1992. </address> <publisher> AAAI Press. </publisher>
Reference: [35] <author> Dave Cliff and Susi Ross. </author> <title> Adding temporary memory to ZCS. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 3(2) </volume> <pages> 101-150, </pages> <year> 1994. </year>
Reference: [36] <author> Anne Condon. </author> <title> The complexity of stochastic games. </title> <journal> Information and Computation, </journal> <volume> 96(2) </volume> <pages> 203-224, </pages> <month> February </month> <year> 1992. </year>
Reference: [37] <author> Anne Condon. </author> <title> On algorithms for simple stochastic games. </title> <booktitle> DIMACS Series in Discrete Mathematics and Theoretical Computer Science, </booktitle> <volume> 13 </volume> <pages> 51-71, </pages> <year> 1993. </year>
Reference: [38] <author> Jonathan Connell and Sridhar Mahadevan. </author> <title> Rapid task learning for real robots. </title> <booktitle> In Robot Learning, </booktitle> <pages> pages 105-140. </pages> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Mas-sachusetts, </address> <year> 1993. </year> <month> 250 </month>
Reference: [39] <author> Don Coppersmith and Schmuel Winograd. </author> <title> Matrix multiplication via arithmetic progressions. </title> <booktitle> In Proceedings of 19th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 1-6, </pages> <year> 1987. </year>
Reference: [40] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference: [41] <author> George Dantzig. </author> <title> Linear Programming and Extensions. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1963. </year>
Reference-contexts: The first theoretically efficient algorithm for solving linear programs, the ellipsoid algorithm [79], does not appear to be of practical use; however, refinements of Karmarkar's [78] polynomial-time algorithm are competitive with the fastest practical algorithms. Another algorithm for solving linear programs, the simplex method <ref> [41] </ref>, is theoretically inefficient but runs extremely quickly in practice. An excellent book by Schrijver [140] describes the theory of linear programs and the algorithms used to solve them.
Reference: [42] <author> Peter Dayan and Terrence J. Sejnowski. </author> <title> Exploration bonuses and dual control. </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference: [43] <author> Thomas Dean, Leslie Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence, </journal> <volume> 76(1-2):35-74, </volume> <year> 1995. </year>
Reference: [44] <author> Eric V. Denardo. </author> <title> Dynamic Programming: Models and Applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1982. </year>
Reference: [45] <author> F. D'Epenoux. </author> <title> A probabilistic production and inventory problem. </title> <journal> Management Science, </journal> <volume> 10 </volume> <pages> 98-108, </pages> <year> 1963. </year>
Reference: [46] <author> Cyrus Derman. </author> <title> Finite State Markovian Decision Processes. </title> <publisher> Academic Press, </publisher> <address> New York, New York, </address> <year> 1970. </year>
Reference: [47] <author> David P. Dobkin and Steven P. Reiss. </author> <title> The complexity of linear programming. </title> <journal> Theoretical Computer Science, </journal> <volume> 11 </volume> <pages> 1-18, </pages> <year> 1980. </year>
Reference: [48] <author> A. W. Drake. </author> <title> Observation of a Markov Process Through a Noisy Channel. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts, </institution> <year> 1962. </year>
Reference: [49] <author> Denise Draper, Steve Hanks, and Dan Weld. </author> <title> Probabilistic planning with information gathering and contingent execution. </title> <type> Technical Report 93-12-04, </type> <institution> University of Washington, </institution> <address> Seattle, Washington, </address> <month> December </month> <year> 1993. </year>
Reference: [50] <author> Denise Draper, Steve Hanks, and Daniel Weld. </author> <title> Probabilistic planning with information gathering and contingent execution. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Decision Theoretic Planning, </booktitle> <pages> pages 76-82, </pages> <year> 1994. </year> <month> 251 </month>
Reference: [51] <author> James N. Eagle. </author> <title> The optimal search for a moving target when the search path is constrained. </title> <journal> Operations Research, </journal> <volume> 32(5) </volume> <pages> 1107-1115, </pages> <year> 1984. </year>
Reference: [52] <author> E. Fernandez-Gaucherand, M. K. Ghosh, and S. I. Marcus. </author> <title> Controlled Markov processes on the infinite planning horizon: Weighted and overtaking cost criteria. </title> <type> Technical Report TR 93-6, </type> <institution> The University of Maryland, </institution> <year> 1995. </year>
Reference: [53] <author> Richard E. Fikes and Nils J. Nilsson. </author> <title> STRIPS: A new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 189-208, </pages> <year> 1971. </year> <note> Reprinted in Readings in Planning, </note> <editor> J. Allen, J. Hendler, and A. Tate, eds., </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: [54] <author> J. A. Filar. </author> <title> Ordered field property for stochastic games when the player who controls transitions changes from state to state. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 34 </volume> <pages> 503-515, </pages> <year> 1981. </year>
Reference: [55] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-completeness. </title> <publisher> Freeman, </publisher> <address> San Francisco, California, </address> <year> 1979. </year>
Reference-contexts: Most co-NP problems have the property that a "no" answer can be supported by a short example. * NP"co-NP: This is the set of problems that are in both NP and co-NP. Membership in this class can be taken as evidence that a problem is in P <ref> [55] </ref>, although there are some significant examples in this class whose exact complexity remains unknown. * NC: The set of problems that can be decided on a parallel computer using a polynomial number of processors and polylogarithmic (O (log k n), for some k) time. <p> For more background information on decision problems and NP-completeness, see Garey and Johnson's book <ref> [55] </ref>. 188 A.1.2 Reductions A reduction is simply a mapping of an instance of one problem A to an instance of another B, so that the solution to B can be used to solve A. <p> Proof: This theorem follows easily from the reduction in Lemma F.3 and the PSPACE-completeness of the finite-state-automata-intersection problem <ref> [55] </ref>. fl F.2 Hardness of Stochastic pomdps In this section, I show that solving infinite-horizon boolean-reward pomdps with stochastic transitions and observations is EXPTIME-hard by showing that solving such pomdps yields a solution to a particular type of game on boolean formulas. 222 The game was devised by Stockmeyer and Chandra
Reference: [56] <author> J. C. Gittins. </author> <title> Multi-armed Bandit Allocation Indices. Wiley-Interscience series in systems and optimization. </title> <publisher> Wiley, </publisher> <address> Chichester, New York, </address> <year> 1989. </year>
Reference: [57] <author> Leslie M. Goldschlager. </author> <title> The monotone and planar circuit value problems are log space complete for P. </title> <journal> SIGACT News, </journal> <pages> pages 25-29, </pages> <year> 1977. </year>
Reference: [58] <author> Geoffrey J. Gordon. </author> <title> Stable function approximation in dynamic programming. </title> <editor> In Armand Prieditis and Stuart Russell, editors, </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 261-268, </pages> <address> San Francisco, Cali-fornia, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [59] <author> Vijaykumar Gullapalli and Andrew G. Barto. </author> <title> Convergence of indirect adaptive asynchronous value iteration algorithms. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Al-spector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 695-702, </pages> <address> San Mateo, California, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [60] <author> Eric A. Hansen. </author> <title> Completely observable Markov decision processes with observation costs. </title> <type> Unpublished manuscript, </type> <year> 1995. </year> <month> 252 </month>
Reference: [61] <author> Mance E. Harmon, Leemon C. Baird, III, and Harry Klopf. </author> <title> Reinforcement learning applied to a differential game. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 4(1) </volume> <pages> 3-28, </pages> <year> 1995. </year>
Reference: [62] <author> Matthias Heger. </author> <title> Consideration of risk in reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 105-111, </pages> <address> San Francisco, California, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [63] <author> Matthias Heger. </author> <title> The loss from imperfect value functions in expectation-based and minimax-based tasks. </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference: [64] <author> O. Hernandez-Lerma and S. I. Marcus. </author> <title> Adaptive control of discounted Markov decision chains. </title> <journal> Journal of optimization theory and applications, </journal> <volume> 46(2) </volume> <pages> 227-235, </pages> <month> June </month> <year> 1985. </year>
Reference: [65] <author> O. Hernandez-Lerma and S. I. Marcus. </author> <title> Adaptive control of Markov processes with incomplete state information and unknown parameters. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 52(2) </volume> <pages> 227-241, </pages> <month> February </month> <year> 1987. </year>
Reference: [66] <author> John Hertz, Anders Krogh, and Richard G. Palmer. </author> <title> Introduction to the Theory of Neural Compuation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood, California, </address> <year> 1991. </year>
Reference: [67] <author> A. J. Hoffman and R. M. Karp. </author> <title> On nonterminating stochastic games. </title> <journal> Management Science, </journal> <volume> 12 </volume> <pages> 359-370, </pages> <year> 1966. </year>
Reference: [68] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1960. </year>
Reference: [69] <author> Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6(6), </volume> <month> November </month> <year> 1994. </year>
Reference-contexts: Then x t (z) converges to 0 with probability 1. Proof: The proof is in a paper by Szepesvari and Littman [158]. A similar claim is proven by Jaakkola, Jordan and Singh <ref> [69] </ref>. fl Let H be a contraction mapping with respect to a weighted max norm with fixed point V fl , and let H t approximate H at V fl .
Reference: [70] <author> Tommi Jaakkola, Satinder Pal Singh, and Michael I. Jordan. </author> <title> Monte-carlo reinforcement learning in non-Markovian decision problems. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <address> Cambridge, Massachusetts, 1995. </address> <publisher> The MIT Press. </publisher>
Reference: [71] <author> George H. John. </author> <title> When the best move isn't optimal: Q-learning with exploration. </title> <type> Unpublished manuscript, </type> <year> 1995. </year> <month> 253 </month>
Reference: [72] <author> Leslie Pack Kaelbling. </author> <title> Learning in Embedded Systems. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1993. </year>
Reference: [73] <author> Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. </author> <title> Planning and acting in partially observable stochastic domains. </title> <type> Technical Report CS-96-08, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1995. </year>
Reference: [74] <author> Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <note> to appear. </note>
Reference: [75] <author> G. Kalai. </author> <title> A subexponential randomized simplex algorithm. </title> <booktitle> In Proceedings of 24th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 475-482, </pages> <year> 1992. </year>
Reference: [76] <author> L. C. M. Kallenberg. </author> <title> Linear Programming and Finite Markovian Control Problems. Number 148 in Mathematical Centre Tracts. </title> <publisher> Mathematisch Centrum, </publisher> <address> Am-sterdam, </address> <year> 1983. </year>
Reference: [77] <author> R. E. </author> <title> Kalman. A new approach to linear filtering and prediction problems. </title> <journal> Transactions of the American Society of Mechanical Engineers, Journal of Basic Engineering, </journal> <volume> 82 </volume> <pages> 35-45, </pages> <month> March </month> <year> 1960. </year>
Reference: [78] <author> N. Karmarkar. </author> <title> A new polynomial-time algorithm for linear programming. </title> <journal> Com-binatorica, </journal> <volume> 4(4) </volume> <pages> 373-395, </pages> <year> 1984. </year>
Reference-contexts: Linear programming is interesting because it is one of the most difficult and general problems that can be solved in polynomial time. The first theoretically efficient algorithm for solving linear programs, the ellipsoid algorithm [79], does not appear to be of practical use; however, refinements of Karmarkar's <ref> [78] </ref> polynomial-time algorithm are competitive with the fastest practical algorithms. Another algorithm for solving linear programs, the simplex method [41], is theoretically inefficient but runs extremely quickly in practice. An excellent book by Schrijver [140] describes the theory of linear programs and the algorithms used to solve them.
Reference: [79] <author> L. G. </author> <title> Khachian. A polynomial algorithm for linear programming. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 20(1) </volume> <pages> 191-194, </pages> <year> 1979. </year>
Reference-contexts: Linear programming is interesting because it is one of the most difficult and general problems that can be solved in polynomial time. The first theoretically efficient algorithm for solving linear programs, the ellipsoid algorithm <ref> [79] </ref>, does not appear to be of practical use; however, refinements of Karmarkar's [78] polynomial-time algorithm are competitive with the fastest practical algorithms. Another algorithm for solving linear programs, the simplex method [41], is theoretically inefficient but runs extremely quickly in practice.
Reference: [80] <author> Victor Klee. </author> <title> On the number of vertices of a convex polytope. </title> <journal> Canada Journal of Mathematics, </journal> <volume> XVI:701-720, </volume> <year> 1964. </year>
Reference: [81] <author> Victor Klee and G. J. Minty. </author> <title> How good is the simplex algorithm? In O. Shisha, editor, Inequalities, </title> <booktitle> III, </booktitle> <pages> pages 159-175. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference: [82] <author> Daphne Koller and Nimrod Megiddo. </author> <title> The complexity of two-person zero-sum games in extensive form. </title> <journal> Games and Economic Behavior, </journal> <volume> 4 </volume> <pages> 528-552, </pages> <year> 1992. </year>
Reference: [83] <author> Daphne Koller and Nimrod Megiddo. </author> <title> Finding mixed strategies with small supports in extensive form games. </title> <journal> International Journal of Game Theory, </journal> <note> to appear. 254 </note>
Reference: [84] <author> Daphne Koller, Nimrod Megiddo, and Bernhard von Stengel. </author> <title> Fast algorithms for finding randomized strategies in game trees. </title> <booktitle> In Proceedings of the 26th ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 750-759, </pages> <year> 1994. </year>
Reference: [85] <author> Daphne Koller, Nimrod Megiddo, and Bernhard von Stengel. </author> <title> Efficient computation of equilibria for extensive two-person games. Games and Economic Behavior, </title> <note> to appear. </note>
Reference: [86] <author> P. R. Kumar and P. P. Varaiya. </author> <title> Stochastic Systems: Estimation, Identification, and Adaptive Control. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1986. </year>
Reference: [87] <author> Nicholas Kushmerick, Steve Hanks, and Daniel S. Weld. </author> <title> An algorithm for probabilistic planning. </title> <journal> Artificial Intelligence, </journal> <volume> 76(1-2):239-286, </volume> <month> September </month> <year> 1995. </year>
Reference: [88] <author> Harold J. Kushner and A. J. Kleinman. </author> <title> Mathematical programming and the control of Markov chains. </title> <journal> International Journal of Control, </journal> <volume> 13(5) </volume> <pages> 801-820, </pages> <year> 1971. </year>
Reference: [89] <author> Long-Ji Lin and Tom M. Mitchell. </author> <title> Memory approaches to reinforcement learning in non-Markovian domains. </title> <type> Technical Report CMU-CS-92-138, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <month> May </month> <year> 1992. </year>
Reference: [90] <author> Michael L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 157-163, </pages> <address> San Francisco, California, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [91] <author> Michael L. Littman. </author> <title> Memoryless policies: Theoretical limitations and practical results. </title> <editor> In Dave Cliff, Philip Husbands, Jean-Arcady Meyer, and Stew-art W. Wilson, editors, </editor> <booktitle> From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, Mas-sachusetts, 1994. </address> <publisher> The MIT Press. </publisher>
Reference: [92] <author> Michael L. Littman. </author> <title> The witness algorithm: Solving partially observable Markov decision processes. </title> <type> Technical Report CS-94-40, </type> <institution> Brown University, Department of Computer Science, </institution> <address> Providence, Rhode Island, </address> <month> December </month> <year> 1994. </year>
Reference: [93] <author> Michael L. Littman and David H. Ackley. </author> <title> Adaptation in constant utility nonstationary environments. </title> <editor> In Rik K. Belew and Lashon Booker, editors, </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 136-142, </pages> <address> San Mateo, California, 1991. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 255 </pages>
Reference: [94] <author> Michael L. Littman, Anthony Cassandra, and Leslie Pack Kaelbling. </author> <title> Learning policies for partially observable environments: Scaling up. </title> <editor> In Armand Prieditis and Stuart Russell, editors, </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 362-370, </pages> <address> San Francisco, California, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [95] <author> Michael L. Littman, Anthony R. Cassandra, and Leslie Pack Kaelbling. </author> <title> Efficient dynamic-programming updates in partially observable Markov decision processes. </title> <type> Technical Report CS-95-19, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1996. </year>
Reference: [96] <author> Michael L. Littman, Thomas L. Dean, and Leslie Pack Kaelbling. </author> <title> On the complexity of solving Markov decision problems. </title> <booktitle> In Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), </booktitle> <address> Montreal, Quebec, Canada, </address> <year> 1995. </year>
Reference: [97] <author> Michael L. Littman and Csaba Szepesvari. </author> <title> A generalized reinforcement-learning model: Convergence and applications. </title> <type> Technical Report CS-96-10, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1996. </year>
Reference: [98] <author> Michael Lederman Littman. </author> <title> Algorithms for sequential decision making. </title> <type> Technical Report CS-96-09, </type> <institution> Brown University, </institution> <month> March </month> <year> 1996. </year>
Reference: [99] <author> William S. Lovejoy. </author> <title> Computationally feasible bounds for partially observed Markov decision processes. </title> <journal> Operations Research, </journal> <volume> 39(1) </volume> <pages> 162-175, </pages> <month> January-February </month> <year> 1991. </year>
Reference: [100] <author> William S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observable Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28 </volume> <pages> 47-66, </pages> <year> 1991. </year>
Reference: [101] <author> Walter Ludwig. </author> <title> A subexponential randomized algorithm for the simple stochastic game problem. </title> <journal> Information and Computation, </journal> <volume> 117 </volume> <pages> 151-155, </pages> <year> 1995. </year>
Reference: [102] <author> Sridhar Mahadevan. </author> <title> Average reward reinforcement learning: Foundations, algorithms, and empirical results. </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference: [103] <author> Andrew Kachites McCallum. </author> <title> Reinforcement Learning with Selective Perception and Hidden State. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Rochester, </institution> <month> December </month> <year> 1995. </year> <month> 256 </month>
Reference: [104] <author> R. Andrew McCallum. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 190-196, </pages> <address> Amherst, Massachusetts, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [105] <author> R. Andrew McCallum. </author> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 387-395, </pages> <address> San Francisco, California, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [106] <author> Lisa Meeden, G. McGraw, and D. Blank. </author> <title> Emergent control and planning in an autonomous vehicle. </title> <editor> In D.S. Touretsky, editor, </editor> <booktitle> Proceedings of the Fifteenth Annual Meeting of the Cognitive Science Society, </booktitle> <pages> pages 735-740. </pages> <publisher> Lawerence Erlbaum Associates, </publisher> <address> Hillsdale, New Jersey, </address> <year> 1993. </year>
Reference: [107] <author> Mary Melekopoglou and Anne Condon. </author> <title> On the complexity of the policy iteration algorithm for stochastic games. </title> <type> Technical Report CS-TR-90-941, </type> <institution> Computer Sciences Department, University of Wisconsin Madison, </institution> <year> 1990. </year> <note> To appear in the ORSA Journal on Computing. </note>
Reference: [108] <author> Nicolas Meuleau. </author> <title> Exploration or Exploitation? Real-time learning. </title> <type> PhD thesis, </type> <institution> Universite de Caen, forthcoming. </institution>
Reference: [109] <author> George E. Monahan. </author> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28 </volume> <pages> 1-16, </pages> <month> January </month> <year> 1982. </year>
Reference: [110] <author> P.R. Montague and T.J. Sejnowski. </author> <title> The predictive brain: Temporal coincidence and temporal order in synaptic learning mechanisms. </title> <booktitle> Learning and Memory, </booktitle> <volume> 1(1) </volume> <pages> 1-33, </pages> <year> 1994. </year>
Reference: [111] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <year> 1993. </year>
Reference: [112] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state spaces. </title> <journal> Machine Learning, </journal> <volume> 21, </volume> <year> 1995. </year>
Reference: [113] <author> Ann Nicholson and Leslie Pack Kaelbling. </author> <title> Toward approximate planning in very large stochastic domains. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Decision Theoretic Planning, </booktitle> <address> Stanford, California, </address> <year> 1994. </year> <month> 257 </month>
Reference: [114] <author> Guillermo Owen. </author> <title> Game Theory: Second edition. </title> <publisher> Academic Press, </publisher> <address> Orlando, Florida, </address> <year> 1982. </year>
Reference: [115] <author> Christos H. Papadimitriou. </author> <title> Computational Complexity. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1994. </year>
Reference: [116] <author> Christos H. Papadimitriou and John N. Tsitsiklis. </author> <title> The complexity of Markov decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3) </volume> <pages> 441-450, </pages> <month> August </month> <year> 1987. </year>
Reference: [117] <author> Ronald Parr and Stuart Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference: [118] <author> Azaria Paz. </author> <title> Introduction to Probabilistic Automata. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year>
Reference: [119] <author> Jing Peng and Ronald J. Williams. </author> <title> Efficient learning and planning within the Dyna framework. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 1(4) </volume> <pages> 437-454, </pages> <year> 1993. </year>
Reference: [120] <author> H. J. M. Peters and O. J. Vrieze, </author> <title> editors. Surveys in game theory and related topics. Number 39 in CWI Tract. </title> <publisher> Stichting Mathematisch Centrum, </publisher> <address> Amsterdam, </address> <year> 1987. </year>
Reference: [121] <author> Loren K. Platzman. </author> <title> Finite-memory Estimation and Control of Finite Probabilistic Systems. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1977. </year>
Reference: [122] <author> Loren K. Platzman. </author> <title> Optimal infinite-horizon undiscounted control of finite prob-abilistc systems. </title> <journal> SIAM Journal of Control and Optimization, </journal> <volume> 18 </volume> <pages> 362-380, </pages> <year> 1980. </year>
Reference: [123] <author> Loren K. Platzman. </author> <title> A feasible computational approach to infinite-horizon partially-observed Markov decision problems. </title> <type> Technical report, </type> <institution> Georgia Institute of Technology, Atlanta, Georgia, </institution> <month> January </month> <year> 1981. </year>
Reference: [124] <author> M. L. Puterman and S. L. Brumelle. </author> <title> The analytic theory of policy iteration. </title> <editor> In Martin L. Puterman, editor, </editor> <booktitle> Dynamic Programming and its applications, </booktitle> <pages> pages 91-114. </pages> <publisher> Academic Press, </publisher> <address> New York, New York, </address> <year> 1978. </year> <month> 258 </month>
Reference-contexts: Section 2.4.3 shows that policy iteration, when the discount factor is bounded away from one, runs in polynomial time, so it appears that Theorem B.2 contributes little to the analysis. Theorems related to Theorem B.2 are presented by Puterman and Brumelle <ref> [124, 125] </ref> in a more abstract setting that might make it possible to prove superlinear convergence of policy iteration, given some additional analysis. B.3 Deterministic mdps as Closed Semirings In this section, I show how to define deterministic mdps as closed semirings.
Reference: [125] <author> M. L. Puterman and S. L. Brumelle. </author> <title> On the convergence of policy iteration in stationary dynamic programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4 </volume> <pages> 60-69, </pages> <year> 1979. </year>
Reference-contexts: Section 2.4.3 shows that policy iteration, when the discount factor is bounded away from one, runs in polynomial time, so it appears that Theorem B.2 contributes little to the analysis. Theorems related to Theorem B.2 are presented by Puterman and Brumelle <ref> [124, 125] </ref> in a more abstract setting that might make it possible to prove superlinear convergence of policy iteration, given some additional analysis. B.3 Deterministic mdps as Closed Semirings In this section, I show how to define deterministic mdps as closed semirings.
Reference: [126] <author> Martin L. Puterman. </author> <title> Markov Decision Processes|Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, New York, </address> <year> 1994. </year>
Reference-contexts: An excellent book by Schrijver [140] describes the theory of linear programs and the algorithms used to solve them. Appendix B Supplementary Information on Markov Decision Processes In this appendix, I present an analysis of two results from Puterman's mdp textbook <ref> [126] </ref> and discuss their implications for the complexity of mdps. I also prove that deterministic mdps are closed semirings. B.1 Comparing Policy Iteration and Value Iteration This result is given much more precisely in Puterman's mdp book [126] as Theorem 6.4.6. <p> this appendix, I present an analysis of two results from Puterman's mdp textbook <ref> [126] </ref> and discuss their implications for the complexity of mdps. I also prove that deterministic mdps are closed semirings. B.1 Comparing Policy Iteration and Value Iteration This result is given much more precisely in Puterman's mdp book [126] as Theorem 6.4.6. The goal of this section is to provide intuitive verbal arguments so that the critical points can be examined more closely. We start with a host of definitions. Let 0 be an arbitrary policy. <p> As a result, only parallel improvement policy iteration is covered by this theorem. Variations of policy iteration can be discussed, but only as they relate to analogous variations of value iteration. B.2 On the Quadratic Convergence of Policy Iteration Puterman <ref> [126] </ref> proves a theorem concerning the rate of convergence of policy iteration. It shows that, under the appropriate conditions, policy iteration converges at a rate that is quadratic (i.e., the error is squared on each iteration). The linear convergence of policy iteration [126] can be used to show that policy iteration <p> On the Quadratic Convergence of Policy Iteration Puterman <ref> [126] </ref> proves a theorem concerning the rate of convergence of policy iteration. It shows that, under the appropriate conditions, policy iteration converges at a rate that is quadratic (i.e., the error is squared on each iteration). The linear convergence of policy iteration [126] can be used to show that policy iteration runs in pseudopolynomial time (see Section 2.4.3). A proof of quadratic convergence would have even more important implications to the complexity analysis of policy iteration, so it is worth understanding what the theorem implies in this case. <p> Then kV t+1 V fl k K 1 fi Proof: See Puterman <ref> [126] </ref>, Theorem 6.4.8. fl I begin by considering the conditions of the theorem, particularly Inequality B.1. First of all, as there may be more than one optimal policy, it is not necessarily the case that kP t P fl k goes to zero as n increases. <p> Define w (x 0 ) = 0 and w (x) = max u P ; that is, w (x) is the maximum expected steps to absorption from state x over all policies. This is the Bellman equation for a simple all-policies-proper mdp, and is well defined <ref> [126] </ref>. Because of the all-policies-proper condition and the definition of w, 0 w (x) &lt; 1 for all x 2 X . Define fi w = fi max x ((w (x) 1)=w (x)); it is strictly less than one because both w (x) and jX j are finite.
Reference: [127] <author> Martin L. Puterman and Moon Chirl Shin. </author> <title> Modified policy iteration algorithms for discounted Markov decision processes. </title> <journal> Management Science, </journal> <volume> 24 </volume> <pages> 1127-1137, </pages> <year> 1978. </year>
Reference: [128] <author> Eric Rasmusen. </author> <title> Games and Information: An Introduction to Game Theory. </title> <publisher> Oxford, </publisher> <address> New York, New York, </address> <year> 1989. </year>
Reference: [129] <author> Mark B. </author> <title> Ring. Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, Austin, Texas, </institution> <month> August </month> <year> 1994. </year>
Reference: [130] <author> Herbert Robbins and Sutton Monro. </author> <title> A stochastic approximation method. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22 </volume> <pages> 400-407, </pages> <year> 1951. </year>
Reference: [131] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error backpropagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructures of Cognition. Volume 1: Foundations, chapter 8. </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1986. </year>
Reference: [132] <author> Stuart J. Russell and Peter Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1994. </year>
Reference: [133] <author> John Rust. </author> <title> Numerical dynamic programming in economics. In Handbook of Computational Economics. </title> <publisher> Elsevier, North Holland, </publisher> <year> 1996. </year>
Reference: [134] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3 </volume> <pages> 211-229, </pages> <year> 1959. </year> <note> Reprinted in E. </note> <editor> A. Feigenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought, </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York 1963. </address>
Reference: [135] <author> Katsushige Sawaki and Akira Ichikawa. </author> <title> Optimal control for partially observable Markov decision processes over an infinite horizon. </title> <journal> Journal of the Operations Research Society of Japan, </journal> <volume> 21(1) </volume> <pages> 1-14, </pages> <month> March </month> <year> 1978. </year> <month> 259 </month>
Reference: [136] <author> Robert E. Schapire and Manfred K. Warmuth. </author> <title> On the worst-case analysis of temporal-difference learning algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 266-274, </pages> <address> San Francisco, Califor-nia, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [137] <author> Jurgen H. Schmidhuber. </author> <title> Reinforcement learning in Markovian and non-Markovian environments. </title> <editor> In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 500-506, </pages> <address> San Mateo, California, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [138] <author> Marcel J. Schoppers. </author> <title> Universal plans for reactive robots in unpredictable environments. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence 10, </booktitle> <pages> pages 1039-1046, </pages> <year> 1987. </year>
Reference: [139] <author> Nicol N. Schraudolph, Peter Dayan, and Terrence J. Sejnowski. </author> <title> Temporal difference learning of position evaluation in the game of Go. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 817-824, </pages> <address> San Mateo, California, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [140] <author> Alexander Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, New York, </address> <year> 1986. </year>
Reference-contexts: Another algorithm for solving linear programs, the simplex method [41], is theoretically inefficient but runs extremely quickly in practice. An excellent book by Schrijver <ref> [140] </ref> describes the theory of linear programs and the algorithms used to solve them. Appendix B Supplementary Information on Markov Decision Processes In this appendix, I present an analysis of two results from Puterman's mdp textbook [126] and discuss their implications for the complexity of mdps.
Reference: [141] <author> Anton Schwartz. </author> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 298-305, </pages> <address> Amherst, Massachusetts, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [142] <author> Tom Seaver. </author> <title> How I Would Pitch to Babe Ruth. </title> <publisher> Playboy Press, </publisher> <address> Chicago, Illinois, </address> <year> 1974. </year>
Reference: [143] <author> L.S. Shapley. </author> <title> Stochastic games. </title> <booktitle> Proceedings of the National Academy of Sciences of the United States of America, </booktitle> <volume> 39 </volume> <pages> 1095-1100, </pages> <year> 1953. </year>
Reference-contexts: This fact was mentioned in Shapley's original paper <ref> [143] </ref>, and a specific example appears in Vrieze's survey article [170].
Reference: [144] <author> Reid Simmons and Sven Koenig. </author> <title> Probabilistic robot navigation in partially observable environments. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1080-1087, </pages> <year> 1995. </year>
Reference: [145] <author> Satinder Pal Singh. </author> <title> Learning to Solve Markovian Decision Processes. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <year> 1993. </year> <note> Also, CMPSCI Technical Report 93-77. 260 </note>
Reference: [146] <author> Satinder Pal Singh, Tommi Jaakkola, and Michael I. Jordan. </author> <title> Model-free reinforcement learning for non-Markovian decision problems. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 284-292, </pages> <address> San Francisco, California, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [147] <author> Satinder Pal Singh and Richard C. Yee. </author> <title> An upper bound on the loss from approximate optimal-value functions. </title> <journal> Machine Learning, </journal> <volume> 16, </volume> <year> 1994. </year>
Reference: [148] <author> Richard D. Smallwood and Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 1071-1088, </pages> <year> 1973. </year>
Reference: [149] <author> Edward Sondik. </author> <title> The Optimal Control of Partially Observable Markov Processes. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1971. </year>
Reference: [150] <author> Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Research, </journal> <volume> 26(2), </volume> <year> 1978. </year>
Reference: [151] <author> Larry J. Stockmeyer and Ashok K. Chandra. </author> <title> Provably difficult combinatorial games. </title> <journal> SIAM Journal of Computing, </journal> <volume> 8(2) </volume> <pages> 151-174, </pages> <month> May </month> <year> 1979. </year>
Reference-contexts: fl F.2 Hardness of Stochastic pomdps In this section, I show that solving infinite-horizon boolean-reward pomdps with stochastic transitions and observations is EXPTIME-hard by showing that solving such pomdps yields a solution to a particular type of game on boolean formulas. 222 The game was devised by Stockmeyer and Chandra <ref> [151] </ref> in their paper linking combinatorial two-player games to the class EXPTIME. The specific EXPTIME-hard game I use in this section is referred to as G 4 , or "Peek," in their paper.
Reference: [152] <author> Gilbert Strang. </author> <title> Linear Algebra and its Applications: Second Edition. </title> <publisher> Academic Press, </publisher> <address> Orlando, Florida, </address> <year> 1980. </year>
Reference: [153] <author> Richard S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Amherst, Massachusetts, </institution> <year> 1984. </year>
Reference: [154] <author> Richard S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference: [155] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <address> Austin, Texas, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [156] <author> Richard S. Sutton. </author> <title> Planning by incremental dynamic programming. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 353-357. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year> <month> 261 </month>
Reference: [157] <author> Csaba Szepesvari. </author> <title> General framework for reinforcement learning. </title> <booktitle> In Proceedings of ICANN'95 Paris, </booktitle> <year> 1995. </year>
Reference: [158] <author> Csaba Szepesvari and Michael L. Littman. </author> <title> Generalized Markov decision processes: Dynamic-programming and reinforcement-learning algorithms. </title> <type> Technical Report CS-96-11, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1996. </year>
Reference-contexts: Assume that for all k, lim n uniformly in z with probability 1 and f t (z) fi (1g t (z)) with probability 1. Then x t (z) converges to 0 with probability 1. Proof: The proof is in a paper by Szepesvari and Littman <ref> [158] </ref>. A similar claim is proven by Jaakkola, Jordan and Singh [69]. fl Let H be a contraction mapping with respect to a weighted max norm with fixed point V fl , and let H t approximate H at V fl .
Reference: [159] <author> Gerald Tesauro. </author> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <pages> pages 58-67, </pages> <month> March </month> <year> 1995. </year>
Reference: [160] <author> Gerald J. Tesauro. </author> <title> Practical issues in temporal difference. </title> <editor> In J. E. Moody, S. J. Hanson, and D. S. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 259-266, </pages> <address> San Mateo, California, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [161] <author> Sebastian Thrun. </author> <title> Learning to play the game of chess. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <address> Cambridge, Massachusetts, 1995. </address> <publisher> The MIT Press. </publisher>
Reference: [162] <author> Paul Tseng. </author> <title> Solving H-horizon, stationary Markov decision problems in time proportional to log(H). </title> <journal> Operations Research Letters, </journal> <volume> 9(5) </volume> <pages> 287-297, </pages> <year> 1990. </year>
Reference-contexts: In contrast, we know that kV t V fl k goes to zero in a series of steps. Tseng <ref> [162] </ref> argues that when the rewards and transition probabilities of an mdp are all rational numbers, and V t is the value function for some policy, then there is a value * &gt; 0 such that kV t V fl k is either zero or greater than or equal to *. <p> Alternate proofs for the mdp case are given by Bertsekas and Tsitsiklis [18], and Tseng <ref> [162] </ref>. Define w (x 0 ) = 0 and w (x) = max u P ; that is, w (x) is the maximum expected steps to absorption from state x over all policies. This is the Bellman equation for a simple all-policies-proper mdp, and is well defined [126].
Reference: [163] <author> John N. Tsitsiklis. </author> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16(3), </volume> <month> September </month> <year> 1994. </year>
Reference: [164] <author> John N. Tsitsiklis and Benjamin Van Roy. </author> <title> Feature-based methods for large scale dynamic programming. </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference: [165] <author> L. G. Valiant and V. V. Vazirani. </author> <title> NP is as easy as detecting unique solutions. </title> <journal> Theoretical Computer Science, </journal> <volume> 47(1) </volume> <pages> 85-93, </pages> <year> 1986. </year>
Reference-contexts: A satisfying assignment maps each of the variables to either "true" or "false" so the entire formula evaluates to "true." There is a result, proved by Valiant and Vazirani <ref> [165] </ref>, that implies that there exists a polynomial-time algorithm for finding a satisfying assignment for a formula that is 241 guaranteed to have at most one satisfying assignment only if RP=NP 1 .
Reference: [166] <author> J. van der Wal. </author> <title> Stochastic Dynamic Programming. Number 139 in Mathematical Centre tracts. </title> <publisher> Mathematisch Centrum, </publisher> <address> Amsterdam, </address> <year> 1981. </year>
Reference: [167] <author> Sergio Vendu and H. Vincent Poor. </author> <title> Abstract dynamic programming models under commutativity conditions. </title> <journal> SIAM Journal of Control and Optimization, </journal> <volume> 25(4) </volume> <pages> 990-1006, </pages> <month> July </month> <year> 1987. </year>
Reference: [168] <author> J. von Neumann and O. Morgenstern. </author> <title> Theory of Games and Economic Behavior. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1947. </year> <month> 262 </month>
Reference: [169] <author> Koos Vrieze. </author> <title> Zero-sum stochastic games. </title> <editor> In H. J. M. Peters and O. J. Vrieze, editors, </editor> <booktitle> Surveys in game theory and related topics, </booktitle> <pages> pages 103-132. </pages> <publisher> Stichting Math-ematisch Centrum, </publisher> <address> Amsterdam, </address> <year> 1987. </year>
Reference: [170] <author> O. J. Vrieze. </author> <title> Stochastic games with finite state and action spaces. Number 33 in CWI Tract. </title> <publisher> Stichting Mathematisch Centrum, </publisher> <address> Amsterdam, </address> <year> 1987. </year>
Reference-contexts: This fact was mentioned in Shapley's original paper [143], and a specific example appears in Vrieze's survey article <ref> [170] </ref>.
Reference: [171] <author> O. J. Vrieze and S. H. Tijs. </author> <title> Fictitious play applied to sequences of games and discounted stochastic games. </title> <journal> International Journal of Game Theory, </journal> <volume> 11(2) </volume> <pages> 71-85, </pages> <year> 1982. </year>
Reference: [172] <author> K.-H. Waldmann. </author> <title> On bounds for dynamic programs. </title> <journal> Mathematics of Operations Research, </journal> <volume> 10(2) </volume> <pages> 220-232, </pages> <month> May </month> <year> 1985. </year>
Reference: [173] <author> Christopher J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, United Kingdom, </address> <year> 1989. </year>
Reference: [174] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference: [175] <author> C. C. White and D. Harrington. </author> <title> Application of Jensen's inequality for adaptive suboptimal design. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 32(1) </volume> <pages> 89-99, </pages> <year> 1980. </year>
Reference: [176] <author> Chelsea C. White, III. </author> <title> Partially observed Markov decision processes: A survey. </title> <journal> Annals of Operations Research, </journal> <volume> 32, </volume> <year> 1991. </year>
Reference: [177] <author> Chelsea C. White, III and William T. Scherer. </author> <title> Solution procedures for partially observed Markov decision processes. </title> <journal> Operations Research, </journal> <volume> 37(5) </volume> <pages> 791-797, </pages> <month> September-October </month> <year> 1989. </year>
Reference: [178] <author> Chelsea C. White, III and William T. Scherer. </author> <title> Finite-memory suboptimal design for partially observed Markov decision processes. </title> <journal> Operations Research, </journal> <volume> 42(3) </volume> <pages> 439-455, </pages> <month> May-June </month> <year> 1994. </year>
Reference: [179] <author> Steven D. Whitehead and Long-Ji Lin. </author> <title> Reinforcement learning of non-Markov decision processes. </title> <journal> Artificial Intelligence, </journal> <volume> 73(1-2):271-306, </volume> <month> February </month> <year> 1995. </year> <month> 263 </month>
Reference: [180] <author> Ronald J. Williams and Leemon C. Baird, III. </author> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-14, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, Mas-sachusetts, </address> <month> November </month> <year> 1993. </year>
Reference: [181] <author> Stewart Wilson. </author> <title> Classifier fitness based on accuracy. </title> <journal> Evolutionary Computation, </journal> <volume> 3(2) </volume> <pages> 147-173, </pages> <year> 1995. </year>
Reference: [182] <author> Stewart W. Wilson. </author> <title> Knowledge growth in an artificial animal. </title> <booktitle> In Proceedings of the First International Conference on Genetic Algorithms and Their Applications, </booktitle> <pages> pages 16-23, </pages> <address> Hillsdale, New Jersey, 1985. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: [183] <author> Uri Zwick and Mike S. Paterson. </author> <title> The complexity of mean payoff games. </title> <note> Theoretical Computer Science, to appear. </note>
References-found: 183

