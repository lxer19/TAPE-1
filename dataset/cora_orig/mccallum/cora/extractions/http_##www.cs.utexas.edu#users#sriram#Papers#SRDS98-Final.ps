URL: http://www.cs.utexas.edu/users/sriram/Papers/SRDS98-Final.ps
Refering-URL: http://www.cs.utexas.edu/users/sriram/mm/papers.html
Root-URL: http://www.cs.utexas.edu
Email: E-mail: fsriram,lorenzo,ving@cs.utexas.edu  
Title: The Cost of Recovery in Message Logging Protocols  
Author: Sriram Rao Lorenzo Alvisi Harrick M. Vin 
Web: URL: http://www.cs.utexas.edu/users/fsriram,lorenzo,ving  
Address: Austin, Texas 78712-1188, USA.  
Affiliation: Department of Computer Sciences The University of Texas at Austin  
Abstract: Past research in message logging has focused on studying the relative overhead imposed by pessimistic, optimistic, and causal protocols during failure-free executions. In this paper, we give the first experimental evaluation of the performance of these protocols during recovery. We discover that, if a single failure is to be tolerated, pessimistic and causal protocols perform best, because they avoid rollbacks of correct processes. For multiple failures, however, the dominant factor in determining performance becomes where the recovery information is logged (i.e. at the sender, at the receiver, or replicated at a subset of the processes in the system) rather than when this information is logged (i.e. if logging is synchronous or asynchronous). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Alvisi and K. Marzullo. </author> <title> Tradeoffs in Implementing Optimal Message Logging Protocols. </title> <booktitle> In Proceedings of the Fifteenth Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 5867. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1996. </year>
Reference-contexts: This optimization allows the roll-forward of recovering processes to proceed in parallel with the identification, roll-back and eventual roll-forward of orphan processes. This optimization dramatically improves the performance of the protocol during recovery (see Section 4). Causal logging: We have implemented the det family-based message-logging protocol <ref> [1] </ref>. This protocol is based on the following observation: in a system where processes fail independently and no more than f processes fail concurrently, one can ensure the availability of determinants during recovery by replicating them in the volatile memory of f + 1 processes.
Reference: [2] <author> L. Alvisi and K. Marzullo. </author> <title> Message Logging: Pessimistic, Optimistic, Causal, and Optimal. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 24(2):149 159, </volume> <month> February </month> <year> 1998. </year>
Reference-contexts: 1 Introduction Message-logging protocols (for example, <ref> [2, 3, 4, 6, 9, 10, 14, 15] </ref>) are popular techniques for building systems that can tolerate process crash failures. These protocols are built on the assumption that the state of a process is determined by its initial state and by the sequence of messages it delivers. <p> These protocols only require that determinants reach stable storage eventually. However, if any of the determinants are lost when a process crashes, then orphans may be created. To reach a consistent global state, these processes must be identified and rolled back. * Causal protocols <ref> [2, 6] </ref> combine some of the positive aspects of pessimistic and optimistic protocols: They never create orphans, yet they do not write determinants to stable storage synchronously. In causal protocols, determinants are logged in volatile memory. <p> In our implementation, this is accomplished by piggybacking determinants on existing application messages until they are logged by at least f + 1 processes <ref> [2, 6] </ref>. Recovery of a failed process proceeds in two phases. In the first phase, the process obtains from the logs of the remaining processes (1) its determinants and (2) content of messages it delivered before crashing. <p> Interestingly, this principle is in stark contrast with recent trends in the design of message logging protocols, which, in order to improve performance during failure free executions, have messages logged by the senders <ref> [2, 6, 9, 15] </ref>. Indeed, a conclusion of our experiments is that there exists a tradeoff between performance during failure-free executions and recovery. Second, it is a good idea to avoid rollbacks.
Reference: [3] <author> A. Borg, J. Baumbach, and S. Glazer. </author> <title> A message system supporting fault tolerance. </title> <booktitle> In Proceedings of the Symposium on Operating Systems Principles, pages 9099. ACM SIGOPS, </booktitle> <month> October </month> <year> 1983. </year>
Reference-contexts: 1 Introduction Message-logging protocols (for example, <ref> [2, 3, 4, 6, 9, 10, 14, 15] </ref>) are popular techniques for building systems that can tolerate process crash failures. These protocols are built on the assumption that the state of a process is determined by its initial state and by the sequence of messages it delivers. <p> An orphan process is an operational process whose state is inconsistent with the recovered state of a crashed process. All message-logging protocols guarantee that upon recovery no process is an orphan, but differ in the way they enforce this consistency condition: * Pessimistic protocols <ref> [3, 9] </ref> require that a process, before sending a message, synchronously log on stable storage the determinants and the content of all messages delivered so far.
Reference: [4] <author> O. P. Damani and V. K. Garg. </author> <title> How to Recover Efficiently and Asynchronously when Optimism Fails. </title> <booktitle> In Proceedings of the 16th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 108115, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Message-logging protocols (for example, <ref> [2, 3, 4, 6, 9, 10, 14, 15] </ref>) are popular techniques for building systems that can tolerate process crash failures. These protocols are built on the assumption that the state of a process is determined by its initial state and by the sequence of messages it delivers. <p> Thus, pessimistic protocols never create orphan processes. * Optimistic protocols <ref> [4, 10, 14] </ref> allow processes to communicate even if the determinants they depend upon are not yet logged on stable storage. These protocols only require that determinants reach stable storage eventually. However, if any of the determinants are lost when a process crashes, then orphans may be created. <p> Although the literature contains careful analyses of the cost of recovery for different optimistic protocols in terms of the number of messages and the rounds of communication needed to identify and roll back orphan processes (for example, <ref> [4, 8, 14] </ref>), in general no experimental evaluations of their performance during recovery are offered. The performance of causal protocols during recovery has also been debated. Proponents of these protocols have observed that causal protocols, like pessimistic protocols, never create orphans and therefore never roll back correct processes. <p> However, with causal protocols a process can start its recovery only after collecting the necessary determinants from the volatile logs of the operational processes. It has been qualitatively argued <ref> [4] </ref> that optimistic protocols that start recovery without waiting for data from other processes may have a shorter recovery time than causal protocols. <p> These messages are matched by p with the corresponding determinants logged on stable storage and then replayed in the appropriate order. Optimistic logging: Among the numerous optimistic protocols that have been proposed in the the literature, we have implemented the protocol described in <ref> [4] </ref>. This protocol, in addition to tolerating an arbitrary number of failures and preventing the uncontrolled cascading of rollbacks known as the domino effect [14], implements a singularly efficient method for detecting orphans processes. In this protocol, causal dependencies are tracked using vector clocks [12]. <p> We have not implemented these protocols because they can only tolerate at most two concurrent failures. In our implementation, we have modified the pseudo-code presented in <ref> [4] </ref> so that the recovering process sends the failure announcements before replaying any message from the log, rather than after all messages in the log have been replayed. This optimization allows the roll-forward of recovering processes to proceed in parallel with the identification, roll-back and eventual roll-forward of orphan processes.
Reference: [5] <author> E. N. Elnozahy. </author> <title> On the relevance of communication costs of rollback-recovery protocols. </title> <booktitle> In Proceedings of the Fourteenth Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 7479, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: In the second phase, the collected data is replayed, restoring the process to its pre-crash state. To handle multiple concurrent failures, we implemented a protocol that recovers crashed processes without blocking operational processes <ref> [5] </ref>. In this protocol, the recovering processes elect a leader, that collects determinants and messages on behalf of all recovering processes.
Reference: [6] <author> E. N. Elnozahy and W. Zwaenepoel. Manetho: </author> <title> Transparent rollback-recovery with low overhead, limited rollback and fast output commit. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(5):526531, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Message-logging protocols (for example, <ref> [2, 3, 4, 6, 9, 10, 14, 15] </ref>) are popular techniques for building systems that can tolerate process crash failures. These protocols are built on the assumption that the state of a process is determined by its initial state and by the sequence of messages it delivers. <p> These protocols only require that determinants reach stable storage eventually. However, if any of the determinants are lost when a process crashes, then orphans may be created. To reach a consistent global state, these processes must be identified and rolled back. * Causal protocols <ref> [2, 6] </ref> combine some of the positive aspects of pessimistic and optimistic protocols: They never create orphans, yet they do not write determinants to stable storage synchronously. In causal protocols, determinants are logged in volatile memory. <p> In our implementation, this is accomplished by piggybacking determinants on existing application messages until they are logged by at least f + 1 processes <ref> [2, 6] </ref>. Recovery of a failed process proceeds in two phases. In the first phase, the process obtains from the logs of the remaining processes (1) its determinants and (2) content of messages it delivered before crashing. <p> Interestingly, this principle is in stark contrast with recent trends in the design of message logging protocols, which, in order to improve performance during failure free executions, have messages logged by the senders <ref> [2, 6, 9, 15] </ref>. Indeed, a conclusion of our experiments is that there exists a tradeoff between performance during failure-free executions and recovery. Second, it is a good idea to avoid rollbacks.
Reference: [7] <author> E. N. Elnozahy and W. Zwaenepoel. </author> <title> On the use and implementation of message logging. </title> <booktitle> In Digest of Papers: 24 Annual International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 298307. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: This property is sufficient to restore a crashed process in a state consistent with the state of all opera tional processes. Although several studies have measured the overhead imposed by each of these approaches during failure-free executions <ref> [7, 8] </ref>, their merits during recovery have been so far argued mostly qualitatively. For instance, there is consensus that pessimistic protocols are well-suited for supporting fast recovery, since they guarantee that all determinants can be readily retrieved from stable storage. The opinions about optimistic protocols are less unanimous. <p> Such receiver-based logging protocols, however, are known to impose a high overhead on application performance during failure-free runs <ref> [7] </ref>. We are currently developing new protocols to overcome this tradeoff and simultaneously provide both low-overhead during failure-free executions and fast crash recovery.
Reference: [8] <author> D. B. Johnson. </author> <title> Distributed System Fault Tolerance Using Message Logging and Checkpointing. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> December </month> <year> 1989. </year> <note> Available as report COMP TR89-101. </note>
Reference-contexts: This property is sufficient to restore a crashed process in a state consistent with the state of all opera tional processes. Although several studies have measured the overhead imposed by each of these approaches during failure-free executions <ref> [7, 8] </ref>, their merits during recovery have been so far argued mostly qualitatively. For instance, there is consensus that pessimistic protocols are well-suited for supporting fast recovery, since they guarantee that all determinants can be readily retrieved from stable storage. The opinions about optimistic protocols are less unanimous. <p> Although the literature contains careful analyses of the cost of recovery for different optimistic protocols in terms of the number of messages and the rounds of communication needed to identify and roll back orphan processes (for example, <ref> [4, 8, 14] </ref>), in general no experimental evaluations of their performance during recovery are offered. The performance of causal protocols during recovery has also been debated. Proponents of these protocols have observed that causal protocols, like pessimistic protocols, never create orphans and therefore never roll back correct processes.
Reference: [9] <author> D. B. Johnson and W. Zwaenepoel. </author> <title> Sender-Based Message Logging. </title> <booktitle> In Digest of Papers: 17 Annual International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 1419. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Message-logging protocols (for example, <ref> [2, 3, 4, 6, 9, 10, 14, 15] </ref>) are popular techniques for building systems that can tolerate process crash failures. These protocols are built on the assumption that the state of a process is determined by its initial state and by the sequence of messages it delivers. <p> An orphan process is an operational process whose state is inconsistent with the recovered state of a crashed process. All message-logging protocols guarantee that upon recovery no process is an orphan, but differ in the way they enforce this consistency condition: * Pessimistic protocols <ref> [3, 9] </ref> require that a process, before sending a message, synchronously log on stable storage the determinants and the content of all messages delivered so far. <p> The first protocol is receiver-based: a process, before sending a message, logs to stable storage both the determinants and the contents of the messages delivered so far. The second protocol is instead sender-based <ref> [9] </ref>: the receiver logs synchronously to stable storage only the determinant of every message it delivers, while the contents of the message are stored in a volatile log kept by the message's sender 2 . This protocol is similar to the one described in [15]. <p> Then, it rolls back to a checkpoint consistent with the recovered state of the failed process and uses its logs to roll forward to the latest possible consistent state. 2 Some sender-based pessimistic protocols keep both determinants and message contents at the senders <ref> [9, 10] </ref>. We have not implemented these protocols because they can only tolerate at most two concurrent failures. <p> Interestingly, this principle is in stark contrast with recent trends in the design of message logging protocols, which, in order to improve performance during failure free executions, have messages logged by the senders <ref> [2, 6, 9, 15] </ref>. Indeed, a conclusion of our experiments is that there exists a tradeoff between performance during failure-free executions and recovery. Second, it is a good idea to avoid rollbacks.
Reference: [10] <author> T. Y. Juang and S. Venkatesan. </author> <title> Crash recovery with little overhead. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 454461. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Message-logging protocols (for example, <ref> [2, 3, 4, 6, 9, 10, 14, 15] </ref>) are popular techniques for building systems that can tolerate process crash failures. These protocols are built on the assumption that the state of a process is determined by its initial state and by the sequence of messages it delivers. <p> Thus, pessimistic protocols never create orphan processes. * Optimistic protocols <ref> [4, 10, 14] </ref> allow processes to communicate even if the determinants they depend upon are not yet logged on stable storage. These protocols only require that determinants reach stable storage eventually. However, if any of the determinants are lost when a process crashes, then orphans may be created. <p> Then, it rolls back to a checkpoint consistent with the recovered state of the failed process and uses its logs to roll forward to the latest possible consistent state. 2 Some sender-based pessimistic protocols keep both determinants and message contents at the senders <ref> [9, 10] </ref>. We have not implemented these protocols because they can only tolerate at most two concurrent failures.
Reference: [11] <author> L. Lamport. </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System. </title> <journal> Communications of the ACM, </journal> <volume> 21(7):558565, </volume> <month> July </month> <year> 1978. </year>
Reference-contexts: This 1 If there exists an upper bound f on the number of concurrent crashes and processes fail independently, then a determinant logged by f + 1 pro cesses does not need to be piggybacked further. guarantees that if the state of an operational process p causally depends <ref> [11] </ref> on the delivery of a message m, then p has a copy of m's determinant in its volatile memory. This property is sufficient to restore a crashed process in a state consistent with the state of all opera tional processes.
Reference: [12] <author> F. Mattern. </author> <title> Virtual Time and Global States of Distributed Systems. </title> <editor> In M. Cosnard et. al., editor, </editor> <booktitle> Parallel and Distributed Algorithms, </booktitle> <pages> pages 215226. </pages> <publisher> Else-vir Science Publishers B. V., </publisher> <year> 1989. </year>
Reference-contexts: This protocol, in addition to tolerating an arbitrary number of failures and preventing the uncontrolled cascading of rollbacks known as the domino effect [14], implements a singularly efficient method for detecting orphans processes. In this protocol, causal dependencies are tracked using vector clocks <ref> [12] </ref>. On a message send, the sender piggybacks its vector clock on the message; on a message deliver, the receiver updates its vector clock by computing a component-wise maximum with the piggybacked vector clock.
Reference: [13] <author> F. B. Schneider. </author> <title> Implementing faulttolerant services using the state machine approach: A Tutorial. </title> <journal> Computing Surveys, </journal> <volume> 22(3):299319, </volume> <month> September </month> <year> 1990. </year>
Reference-contexts: Distributed applications requiring both fault-tolerance and high availability were few and highly sophisticated, and its users could typically afford to invest the resources necessary to mask failures through explicit replication in space <ref> [13] </ref> instead of recovering from failures through replication in time. As distributed computing becomes commonplace and many more applications are faced with the current costs of high availability, there is a fresh need for recovery-based techniques that combine high performance during failure free executions with fast recovery.
Reference: [14] <author> R. B. Strom and S. Yemeni. </author> <title> Optimistic recovery in distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3):204226, </volume> <month> April </month> <year> 1985. </year>
Reference-contexts: 1 Introduction Message-logging protocols (for example, <ref> [2, 3, 4, 6, 9, 10, 14, 15] </ref>) are popular techniques for building systems that can tolerate process crash failures. These protocols are built on the assumption that the state of a process is determined by its initial state and by the sequence of messages it delivers. <p> Thus, pessimistic protocols never create orphan processes. * Optimistic protocols <ref> [4, 10, 14] </ref> allow processes to communicate even if the determinants they depend upon are not yet logged on stable storage. These protocols only require that determinants reach stable storage eventually. However, if any of the determinants are lost when a process crashes, then orphans may be created. <p> Although the literature contains careful analyses of the cost of recovery for different optimistic protocols in terms of the number of messages and the rounds of communication needed to identify and roll back orphan processes (for example, <ref> [4, 8, 14] </ref>), in general no experimental evaluations of their performance during recovery are offered. The performance of causal protocols during recovery has also been debated. Proponents of these protocols have observed that causal protocols, like pessimistic protocols, never create orphans and therefore never roll back correct processes. <p> Optimistic logging: Among the numerous optimistic protocols that have been proposed in the the literature, we have implemented the protocol described in [4]. This protocol, in addition to tolerating an arbitrary number of failures and preventing the uncontrolled cascading of rollbacks known as the domino effect <ref> [14] </ref>, implements a singularly efficient method for detecting orphans processes. In this protocol, causal dependencies are tracked using vector clocks [12].
Reference: [15] <author> R. E. Strom, D. F. Bacon, and S. A. Yemini. </author> <title> Volatile Logging in n-Fault-Tolerant Distributed Systems. </title> <booktitle> In Proceedings of the Eighteenth Annual International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 44 49, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Message-logging protocols (for example, <ref> [2, 3, 4, 6, 9, 10, 14, 15] </ref>) are popular techniques for building systems that can tolerate process crash failures. These protocols are built on the assumption that the state of a process is determined by its initial state and by the sequence of messages it delivers. <p> This protocol is similar to the one described in <ref> [15] </ref>. In both of these protocols, the first step of recovering a process p consists in restoring it to its latest checkpoint. Then, in the receiver-based protocol, the messages logged on stable storage are replayed to p in the appropriate order. <p> Interestingly, this principle is in stark contrast with recent trends in the design of message logging protocols, which, in order to improve performance during failure free executions, have messages logged by the senders <ref> [2, 6, 9, 15] </ref>. Indeed, a conclusion of our experiments is that there exists a tradeoff between performance during failure-free executions and recovery. Second, it is a good idea to avoid rollbacks.
References-found: 15

