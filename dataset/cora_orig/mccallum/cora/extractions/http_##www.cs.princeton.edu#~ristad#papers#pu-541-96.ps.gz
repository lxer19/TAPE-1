URL: http://www.cs.princeton.edu/~ristad/papers/pu-541-96.ps.gz
Refering-URL: http://www.cs.princeton.edu/~ristad/papers/pu-541-96.html
Root-URL: http://www.cs.princeton.edu
Title: On the Strange a Posteriori degeneracy of Normal Mixtures, and Related Reparameterization Theorems  
Author: Eric S. Ristad Peter N. Yianilos 
Keyword: Expectation Maximization (EM), Normal Mixtures, Gaussian Mixtures, Supervised Learning, Maximize Mutual Information (MMI).  
Date: December, 1996  
Pubnum: Research Report CS-TR-541-96  
Abstract: This short paper illuminates certain fundamental aspects of the nature of normal (Gaussian) mixtures. Thinking of each mixture component as a class, we focus on the corresponding a posteriori class probability functions. It is shown that the relationship between these functions and the mixture's parameters, is highly degenerate and that the precise nature of this degeneracy leads to somewhat unusual and counter-intuitive behavior. Even complete knowledge of a mixture's a posteriori class behavior, reveals essentially nothing of its absolute nature, i.e. mean locations and covariance norms. Consequently a mixture whose means are located in a small ball anywhere in space, can project arbitrary class structure everywhere in space. The well-known expectation maximization (EM) algorithm for Maximum Likelihood (ML) optimization may be thought of as a reparameterization of the problem in which the search takes place over the space of sample point weights. Motivated by EM we characterize the expressive power of similar reparameterizations, where the objective is instead to maximize the a posteriori likelihood of a labeled training set. This is relevant to, and a generalization of a common heuristic in machine learning in which one increases the weight of a mistake in order to improve classification accuracy. We prove that EM-style reparameterization is not capable of expressing arbitrary a posteriori behavior, and is therefore incapable of expressing some solutions. However a slightly different reparameterization is presented which is almost always fully expressive a fact proven by exploiting the degeneracy described above. fl Both authors are with the Department of Computer Science, Princeton University, 35 Olden Street, Princeton, NJ 08544. The second author is also with the NEC Research Institute, 4 Independence Way, Princeton, NJ 08540. The first author is partially supported by Young Investigator Award IRI-0258517 from the National Science Foundation. Email: fristad,pnyg@cs.princeton.edu. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. E. Baum and J. E. Eagon, </author> <title> An inequality with application to statistical estimation for probabalistic functions of a Markov process and to models for ecology, </title> <journal> Bull. AMS, </journal> <volume> 73 (1967), </volume> <pages> pp. 360-363. </pages>
Reference-contexts: Here the objective is to predict the correct labels, not model the observation vectors themselves. Section 3 shows that such search problems may almost always be carried out using reparameter-izations we refer to as emphasis methods that are motivated by the simple and intuitive expectation maximization (EM) algorithm <ref> [1, 3, 7] </ref> for unsupervised maximum likelihood (ML) parameter estimation. The reparameterized problem cannot express an arbitrary mixture but nevertheless, because of degeneracy, can express a mixture that induces optimal class functions with respect to the prediction of training labels.
Reference: [2] <author> P. F. Brown, </author> <title> Acoustic-phonetic modeling problem in automatic speech recognition, </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1987. </year>
Reference-contexts: Each constituent normal density is referred to as a component of the mixture, and m 1 ; : : : ; m k are the mixing coefficients. Normal mixtures have proven useful in several areas including pattern recognition [4] and speech recognition <ref> [6, 2, 5] </ref> along with vector quantization and many others. <p> That is, to find a model M which maximizes: n Y p (!(s i )js j ; M ) where !(s i ) is the class label associated with sample s j . This is sometimes called the maximum mutual information (MMI) criterion in the speech recognition literature <ref> [2] </ref>. We remark that more general forms of this problem may be stated in which the labels are themselves probabilities. Sections 3 and 4 consider the question of whether emphasis-like reparameterizations may be used to attack the more general form of this problem.
Reference: [3] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin, </author> <title> Maximum-likelihood from incomplete data via the EM algorithm, </title> <journal> J. Royal Statistical Society Ser. B (methodological), </journal> <volume> 39 (1977), </volume> <pages> pp. 1-38. </pages>
Reference-contexts: Here the objective is to predict the correct labels, not model the observation vectors themselves. Section 3 shows that such search problems may almost always be carried out using reparameter-izations we refer to as emphasis methods that are motivated by the simple and intuitive expectation maximization (EM) algorithm <ref> [1, 3, 7] </ref> for unsupervised maximum likelihood (ML) parameter estimation. The reparameterized problem cannot express an arbitrary mixture but nevertheless, because of degeneracy, can express a mixture that induces optimal class functions with respect to the prediction of training labels.
Reference: [4] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1973. </year>
Reference-contexts: Each constituent normal density is referred to as a component of the mixture, and m 1 ; : : : ; m k are the mixing coefficients. Normal mixtures have proven useful in several areas including pattern recognition <ref> [4] </ref> and speech recognition [6, 2, 5] along with vector quantization and many others. <p> A convenient visualization of these a posteriori class functions, focuses on decision boundaries, i.e. the surfaces along which classification is ambiguous. 2 Imagery like ours in figure 1, and pages 28-31 of <ref> [4] </ref>, suggest an intuitive relationship between mixture component locations, and the resulting a posteriori class structure and decision surfaces. One imagines each mean to be asserting ownership over some volume of space surrounding it.
Reference: [5] <author> X. D. Huang, Y. Ariki, and M. A. Jack, </author> <title> Hidden Markov Models for Speech Recognition, </title> <publisher> Edinburgh University Press, </publisher> <year> 1990. </year>
Reference-contexts: Each constituent normal density is referred to as a component of the mixture, and m 1 ; : : : ; m k are the mixing coefficients. Normal mixtures have proven useful in several areas including pattern recognition [4] and speech recognition <ref> [6, 2, 5] </ref> along with vector quantization and many others.
Reference: [6] <author> L. R. Rabiner, B. H. Juang, S. E. Levinson, and M. M. Sondhi, </author> <title> Recognition of isolated digits using hidden markov models with continuous mixture densities, </title> <journal> AT&T Technical Journal, </journal> <year> (1985). </year>
Reference-contexts: Each constituent normal density is referred to as a component of the mixture, and m 1 ; : : : ; m k are the mixing coefficients. Normal mixtures have proven useful in several areas including pattern recognition [4] and speech recognition <ref> [6, 2, 5] </ref> along with vector quantization and many others.
Reference: [7] <author> R. A. Redner and H. F. Walker, </author> <title> Mixture densities, maximum likelihood, and the EM algorithm, </title> <journal> SIAM Review, </journal> <volume> 26 (1984), </volume> <pages> pp. 195-239. 16 </pages>
Reference-contexts: Here the objective is to predict the correct labels, not model the observation vectors themselves. Section 3 shows that such search problems may almost always be carried out using reparameter-izations we refer to as emphasis methods that are motivated by the simple and intuitive expectation maximization (EM) algorithm <ref> [1, 3, 7] </ref> for unsupervised maximum likelihood (ML) parameter estimation. The reparameterized problem cannot express an arbitrary mixture but nevertheless, because of degeneracy, can express a mixture that induces optimal class functions with respect to the prediction of training labels.
References-found: 7

