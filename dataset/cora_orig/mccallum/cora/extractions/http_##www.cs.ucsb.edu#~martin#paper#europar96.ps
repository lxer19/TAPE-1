URL: http://www.cs.ucsb.edu/~martin/paper/europar96.ps
Refering-URL: http://www.cs.ucsb.edu/~martin/paper/index.html
Root-URL: http://www.cs.ucsb.edu
Email: fmartin,pedrog@cs.ucsb.edu  
Title: Semantic Foundations of Commutativity Analysis  
Author: Martin C. Rinard and Pedro C. Diniz 
Web: http://www.cs.ucsb.edu/~fmartin,pedrog  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: This paper presents the semantic foundations of commuta-tivity analysis, an analysis technique for automatically parallelizing programs written in a sequential, imperative programming language. Com-mutativity analysis views the computation as composed of operations on objects. It then analyzes the program at this granularity to discover when operations commute (i.e. generate the same result regardless of the order in which they execute). If all of the operations required to perform a given computation commute, the compiler can automatically generate parallel code. This paper shows that the basic analysis technique is sound. We have implemented a parallelizing compiler that uses commu-tativity analysis as its basic analysis technique; this paper also presents performance results from two automatically parallelized applications. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D. Padua. </author> <title> Automatic program paral-lelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 211-243, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Current parallelizing compilers preserve the semantics of the original serial program by preserving the data dependences <ref> [1] </ref>. They analyze the program to identify independent pieces of computation (two pieces of computation are independent if neither writes a piece of memory that the other accesses), then generate code that executes independent pieces concurrently. This paper presents the semantic foundations of a new analysis technique called commutativity analysis.
Reference: 2. <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(NlogN) force-calculation algorithm. </title> <booktitle> Nature, </booktitle> <pages> pages 446-449, </pages> <month> December </month> <year> 1976. </year>
Reference-contexts: The compiler also uses several other analysis techniques to extend the model of computation significantly beyond the basic model of computation presented in Section 3 [5]. We used the compiler to automatically parallelize two applications: the Barnes-Hut hierarchical N-body code <ref> [2] </ref> and Water, which evaluates forces and potentials in a system of water molecules in the liquid state.
Reference: 3. <author> G. Huet. </author> <title> Confluent reductions: Abstract properties and applications to term rewriting systems. </title> <journal> Journal of the ACM, </journal> <volume> 27(4) </volume> <pages> 797-821, </pages> <year> 1980. </year>
Reference-contexts: ;i implies not hm; fr-&gt;op (o)gi ) (i.e. there is no infinite parallel execution), and - hm; fr-&gt;op (o)gi ) ) hm 2 ; ;i implies m 1 = m 2 Proof Sketch: If all of the invoked operations commute then the transition system is confluent, which guarantees deterministic execution <ref> [3] </ref>. Lemma 9 characterizes the situation when the computation may not terminate. It says that if all of the operations invoked in the parallel executions commute, then it is possible to take any two partial parallel executions and extend them to identical states. Lemma 9. <p> M; p 2 mst (A) : hm 1 ; p 1 i ) ) hm 0 ; pi and hm 2 ; p 2 i ) ) hm 0 ; pi Proof Sketch: If all of the invoked operations commute then the transition system is confluent, which guarantees deterministic execution <ref> [3] </ref>. An immediate corollary of these two lemmas is that if the serial computation terminates, then all parallel computations terminate with identical memories.
Reference: 4. <author> R. Kemmerer and S. Eckmann. UNISEX: </author> <title> a UNIx-based Symbolic EXecutor for pascal. </title> <journal> Software|Practice and Experience, </journal> <volume> 15(5) </volume> <pages> 439-458, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: We have also developed a static analysis algorithm that analyzes pairs of methods to determine if they meet the commutativity testing conditions in Section 3. The foundation of this analysis algorithm is symbolic execution <ref> [4] </ref>. Symbolic execution simply executes the methods, computing with expressions instead of values. It maintains a set of bindings that map variables to the expressions that denote their values and updates the bindings as it executes the methods.
Reference: 5. <author> M. Rinard and P. Diniz. </author> <title> Commutativity analysis: A new analysis framework for parallelizing compilers. </title> <type> Technical Report TRCS96-08, </type> <institution> Dept. of Computer Science, University of California at Santa Barbara, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: The compiler also uses several other analysis techniques to extend the model of computation significantly beyond the basic model of computation presented in Section 3 <ref> [5] </ref>. We used the compiler to automatically parallelize two applications: the Barnes-Hut hierarchical N-body code [2] and Water, which evaluates forces and potentials in a system of water molecules in the liquid state. <p> We briefly present several performance results; we provide a more complete description of the applications and the experimental methodology elsewhere <ref> [5] </ref>. sets; this graph plots the running time of the sequential version running with no parallelization overhead divided by the running time of the automatically parallel version as a function of the number of processors executing the parallel computation. <p> The primary limiting factor on the speedup is the fact that the compiler does not parallelize one of the phases of the computation; as the number of processors grows that phase becomes the limiting factor on the performance <ref> [5] </ref>. The limiting factor on the speedup is contention for shared objects updated by multiple operations [5]. 7 Conclusion Existing parallelizing compilers all preserve the data dependences of the original serial program. <p> on the speedup is the fact that the compiler does not parallelize one of the phases of the computation; as the number of processors grows that phase becomes the limiting factor on the performance <ref> [5] </ref>. The limiting factor on the speedup is contention for shared objects updated by multiple operations [5]. 7 Conclusion Existing parallelizing compilers all preserve the data dependences of the original serial program. We believe that this strategy is too conservative: compilers must recognize and exploit commuting operations if they are to effectively par-allelize a range of applications.
Reference: 6. <author> M. Rinard and P. Diniz. </author> <title> Semantic foundations of commutativity analysis. </title> <type> Technical Report TRCS96-09, </type> <institution> Dept. of Computer Science, University of California at Santa Barbara, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: Conversely, if a parallel computation terminates, then the serial computation also terminates with an identical memory. 5 Analysis We have developed a formal semantics that, given a program, defines the receiver effect and invoked operation functions for that program <ref> [6] </ref>. We have also developed a static analysis algorithm that analyzes pairs of methods to determine if they meet the commutativity testing conditions in Section 3. The foundation of this analysis algorithm is symbolic execution [4]. Symbolic execution simply executes the methods, computing with expressions instead of values. <p> If the expressions denote the same value, the operations commute. We have proved a correspondence between the static analysis algorithm and the formal semantics, and used the correspondence to prove that the algorithms used in the compiler correctly identify parallelizable computations <ref> [6] </ref>. 6 Experimental Results We have implemented a prototype parallelizing compiler that uses commutativ-ity analysis as its basic analysis technique. The compiler also uses several other analysis techniques to extend the model of computation significantly beyond the basic model of computation presented in Section 3 [5].
References-found: 6

