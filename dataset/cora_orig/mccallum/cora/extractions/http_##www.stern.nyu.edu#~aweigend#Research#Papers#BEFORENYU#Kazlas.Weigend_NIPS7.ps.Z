URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/Kazlas.Weigend_NIPS7.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: pkazlas@colorado.edu  andreas@cs.colorado.edu  
Title: Direct Multi-Step Time Series Prediction Using TD()  
Author: Peter T. Kazlas Andreas S. Weigend 
Address: Boulder, CO 80309-0425  Boulder, CO 80309-0430  
Affiliation: Department of Electrical and Computer Engineering University of Colorado  Department of Computer Science and Institute of Cognitive Science University of Colorado  
Abstract: This paper explores the application of Temporal Difference (TD) learning (Sutton, 1988) to forecasting the behavior of dynamical systems with real-valued outputs (as opposed to game-like situations). The performance of TD learning in comparison to standard supervised learning depends on the amount of noise present in the data. In this paper, we use a deterministic chaotic time series from a low-noise laser. For the task of direct five-step ahead predictions, our experiments show that standard supervised learning is better than TD learning. The TD algorithm can be viewed as linking adjacent predictions. A similar effect can be obtained by sharing the internal representation in the network. We thus compare two architectures for both paradigms: the first architecture (separate hidden units) consists of individual networks for each of the five direct multi-step prediction tasks, the second (shared hidden units) has a single (larger) hidden layer that finds a representation from which all five predictions for the next five steps are generated. For this data set we do not find any significant difference between the two architectures. fl http://www.cs.colorado.edu/~andreas/Home.html. This paper is available as ftp://ftp.cs.colorado.edu/pub/Time-Series/MyPapers/kazlas.weigend nips7.ps.Z 
Abstract-found: 1
Intro-found: 1
Reference: <author> L. Breiman and J. H. </author> <title> Friedman (1994) A New Look at Multiple Outputs". Abstract, Neural Networks for Computing, </title> <address> Snowbird, UT, </address> <note> April 1994 R. </note> <editor> A. </editor> <booktitle> Caruana (1994) Multitask Connectionist Learning" In Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <editor> edited by M. C. Mozer, P. Smolensky, D. S. Touretzky, J. L. Elman, and A. S. Weigend, p. </editor> <address> 372-379. Hillsdale, NJ: </address> <publisher> Erlbaum Associates. </publisher>
Reference: <author> R.S. </author> <title> Sutton (1988) Learning to Predict by the Methods of Temporal Differences. </title> <booktitle> Machine Learning 3: </booktitle> <pages> 9-44. </pages>
Reference-contexts: Temporal difference (TD) learning, on the other hand, takes a different approach: it adjusts the parameters based on differences between successive predictions in time <ref> (Sutton, 1988) </ref>. TD learning has been shown to be very successful in the context of games such as backgammon (Tesauro, 1992). This paper investigates whether the TD paradigm can also be applied to the somewhat different task of time series prediction. <p> This is in contrast to the traditional SL approach where the errors are based on the difference between the prediction and the observed value. The general expression for the TD weight update rule (linear case), TD (), is given by <ref> (Sutton, 88) </ref> t X tk r w y k (1) where is the learning rate; y t+1 and y t are two adjacent predictions of the equivalent target; is the recency weight with 0 1; and r w y k is the gradient of the prediction at time k with respect <p> To cast the multi-step prediction problem into the TD framework, we first form an overlapping sequence of predictions as described by <ref> (Sutton, 88) </ref>: For an n-step ahead prediction problem, we form n successive predictions of the same target y t+n : y n t+1 , : : : y 1 t is the prediction at time t for the time series ffi steps ahead).
Reference: <author> G. </author> <title> Tesauro (1992) Practical Issues in Temporal Difference learning. </title> <note> Machine Learning 8 : 257-277. </note>
Reference-contexts: Temporal difference (TD) learning, on the other hand, takes a different approach: it adjusts the parameters based on differences between successive predictions in time (Sutton, 1988). TD learning has been shown to be very successful in the context of games such as backgammon <ref> (Tesauro, 1992) </ref>. This paper investigates whether the TD paradigm can also be applied to the somewhat different task of time series prediction.
Reference: <author> A.S. Weigend, B.A. Huberman, and D.E. </author> <title> Rumelhart (1992) Predicting Sunspots and Exchange Rates with Connectionist Networks. In Nonlinear Modeling and Forecasting, edited by M. </title> <editor> Casdagli, and S. Eubank, p. </editor> <address> 395-432. </address> <publisher> Addison-Wesley. units. </publisher>
Reference: <editor> A.S. Weigend & N.A. Gershenfeld (eds.). </editor> <title> (1994) Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: 1 Introduction The Santa Fe Time Series Prediction and Analysis Competition <ref> (Weigend & Gershenfeld, 1994) </ref> saw a relatively large number of different nonlinear techniques applied to the prediction of a few time series. One of the results was that some neural networks did very well (but incidentally some other neural networks also did very poorly).
References-found: 5

