URL: ftp://robotics.stanford.edu/pub/gjohn/papers/mlc++-tools.ps
Refering-URL: http://www.robotics.stanford.edu/~gjohn/pubs.html
Root-URL: http://www.robotics.stanford.edu
Email: mlc@CS.Stanford.EDU  
Title: MLC A Machine Learning Library in C  
Author: Ron Kohavi George John Richard Long David Manley Karl Pfleger 
Keyword: functionality.  
Address: Stanford, CA 94305  
Affiliation: Computer Science Department Stanford University  
Abstract: We present MLC ++ , a library of C ++ classes and tools for supervised Machine Learning. While MLC ++ provides general learning algorithms that can be used by end users, the main objective is to provide researchers and experts with a wide variety of tools that can accelerate algorithm development, increase software reliability, provide comparison tools, and display information visually. More than just a collection of existing algorithms, MLC ++ is an attempt to extract commonalities of algorithms and decompose them for a unified view that is simple, coherent, and extensible. In this paper we discuss the problems MLC ++ aims to solve, the design of MLC ++ , and the current 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. W. Aha, D. Kibler, and M. K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: To generate multivariate trees with perceptrons at nodes, the induction algorithm can put percep tron categorizers at the nodes. Induction algorithms Induction algorithms induce categorizers. MLC ++ currently provides a majority inducer, a nearest-neighbor inducer <ref> [1] </ref>, an ID3-like decision tree inducer [11], and HOODG for inducing oblivious read-once decision graphs [7]. Visualization tools From the outset, one of our top priorities was to provide visualization tools to the user. Graphical displays of datasets and induced concepts can provide key insights.
Reference: [2] <author> Dana Angluin. </author> <title> Computational learning theory: Survey and selected bibliography. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 351-369. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction Published in IEEE Conference on Tools with Artificial Intelligence, New Orleans, November 1994, pp. 740-743 Newton said he saw farther because he stood on the shoulders of giants. Computer programmers stand on each other's toes - James Coggins In supervised machine learning <ref> [13, 2] </ref>, one tries to find a rule (a categorizer) that can be used to accurately predict the class label of novel instances. During the last decade, the machine learning community has developed a plethora of algorithms for this task.
Reference: [3] <author> Leo Breiman, Jerome H. Friedman, Richard A. Ol-shen, and Charles J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: Display of information In order to understand the problem better, and perhaps bias a learning algorithm, it is helpful to view the resulting structures (e.g., decision tree), or the data. Even commercially available programs such as C4.5 [12] and CART <ref> [3] </ref> give only a rudimentary display of an induced tree. A library providing a common framework and basic tools for implementing learning algorithms would alleviate the pain involved in programming from scratch.
Reference: [4] <author> E. R. Gansner, E. Koutsofios, S. C. North, and K. P. Vo. </author> <title> A technique for drawing directed graphs. </title> <journal> In IEEE Transactions on Software Engineering, </journal> <pages> pages 214-230, </pages> <year> 1993. </year>
Reference-contexts: We attempt to use as much educational and public domain software as possible for this part of MLC ++ . For example, the graph manipulations are done using LEDA (Library of Efficient Data Structures) written by Stefan Naher [10], and dot from AT&T <ref> [4] </ref>. Core classes These are the basic tools that are shared by many algorithms in supervised machine learning. They further divide into three types of functionality: Input/Output Classes for reading and writing data files. <p> Decision trees and decision graphs are excellent examples of interpretable structures, and MLC ++ interfaces the excellent graph-drawing programs dot and dotty provided by AT&T <ref> [4] </ref>. Visualization of discrete data For viewing datasets and induced concepts, we have implemented General Logic Diagrams. After running an induction algorithm, users may gain insight about the induced concept by inspecting the GLD.
Reference: [5] <author> Robert C. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 63-90, </pages> <year> 1993. </year>
Reference-contexts: : the number of runs varies considerably, as does the ratio of the sizes of the training and test set. : : : it is virtually certain that some papers reporting results on a dataset have used slightly different versions of the dataset than others : : : Robert Holte <ref> [5] </ref> Although many experimental results have appeared in the literature, the field seems to be in a state of disarray. There are too many algorithms and variations of algorithms, each claiming to do better on a few datasets.
Reference: [6] <author> George John, Ron Kohavi, and Karl Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: We described MLC ++ , our attempt at building such a tool. Much work has already been done on MLC ++ , and it has already aided some of us in our research. <ref> [6, 7] </ref>. We trust that other researchers will also enjoy productivity gains when using MLC ++ , and will contribute to this effort. Acknowledgments The MLC ++ project is partly funded by ONR grant N00014-94-1-0448. George John is supported by an NSF Graduate Research Fellowship.
Reference: [7] <author> Ron Kohavi. </author> <title> Bottom-up induction of oblivious, read-once decision graphs : Strengths and limitations. </title> <booktitle> In Twelfth National Conference on Artificial Intelligence, </booktitle> <year> 1994. </year> <note> Paper available by anonymous ftp from Starry.Stanford.EDU:pub/ronnyk/aaai94.ps. </note>
Reference-contexts: Induction algorithms Induction algorithms induce categorizers. MLC ++ currently provides a majority inducer, a nearest-neighbor inducer [1], an ID3-like decision tree inducer [11], and HOODG for inducing oblivious read-once decision graphs <ref> [7] </ref>. Visualization tools From the outset, one of our top priorities was to provide visualization tools to the user. Graphical displays of datasets and induced concepts can provide key insights. <p> We described MLC ++ , our attempt at building such a tool. Much work has already been done on MLC ++ , and it has already aided some of us in our research. <ref> [6, 7] </ref>. We trust that other researchers will also enjoy productivity gains when using MLC ++ , and will contribute to this effort. Acknowledgments The MLC ++ project is partly funded by ONR grant N00014-94-1-0448. George John is supported by an NSF Graduate Research Fellowship.
Reference: [8] <author> Mike Meyer. Statlib. </author> <note> Available at lib.stat.cmu.edu. </note>
Reference-contexts: We do not intend to duplicate any of this effort; in fact, we use their data formats as much as possible. There is also a large repository of data used by statisticians in the StatLib archive, created and maintained by Meyer <ref> [8] </ref>. Mooney has a collection of a few machine learning algorithms implemented in Lisp at UCI [9], but they are not an integrated environment, and are not very efficient. StatLog [14] is an ESPRIT project studying the behavior of over twenty algorithms (mostly in the ML-Toolbox), on over twenty datasets.
Reference: [9] <author> Patrick M. Murphy and David W. Aha. </author> <title> UCI repository of machine learning databases. For information contact ml-repository@ics.uci.edu, </title> <year> 1994. </year>
Reference-contexts: Comparisons with other algorithms Results are usually given in isolation. When a learning algorithm is presented, comparison is usually limited to a trivial straw algorithm, as opposed to state-of-the-art algorithms. Comparisons on different datasets Results are given on different datasets, and cannot be easily compared. The Irvine repository <ref> [9] </ref> partially solves this problem, as it supplies a set of "standard" datasets. <p> Below, we mention previous projects addressing similar concerns. An extensive collection of over 100 datasets has been collected by Murphy and Aha at the University of California at Irvine <ref> [9] </ref>. We do not intend to duplicate any of this effort; in fact, we use their data formats as much as possible. There is also a large repository of data used by statisticians in the StatLib archive, created and maintained by Meyer [8]. <p> There is also a large repository of data used by statisticians in the StatLib archive, created and maintained by Meyer [8]. Mooney has a collection of a few machine learning algorithms implemented in Lisp at UCI <ref> [9] </ref>, but they are not an integrated environment, and are not very efficient. StatLog [14] is an ESPRIT project studying the behavior of over twenty algorithms (mostly in the ML-Toolbox), on over twenty datasets.
Reference: [10] <author> Stefan Naeher. LEDA: </author> <title> A Library of Efficient Data Types and Algorithms. </title> <institution> Max-Planck-Institut fuer Informatik, </institution> <address> IM Stadtwald, D-66123 Saarbruecken, FRG, 3.0 edition, </address> <year> 1992. </year> <note> Available by anonymous ftp in ftp.cs.uni-sb.de:LEDA. </note>
Reference-contexts: We attempt to use as much educational and public domain software as possible for this part of MLC ++ . For example, the graph manipulations are done using LEDA (Library of Efficient Data Structures) written by Stefan Naher <ref> [10] </ref>, and dot from AT&T [4]. Core classes These are the basic tools that are shared by many algorithms in supervised machine learning. They further divide into three types of functionality: Input/Output Classes for reading and writing data files.
Reference: [11] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year> <note> Reprinted in Shavlik and Dietterich (eds.) Readings in Machine Learning. </note>
Reference-contexts: Categorizers are built recursively; for example, in a decision tree categorizer, the branching nodes are categorizers themselves (mapping the set of instances into the set of children of that node), and the induction algorithm can use any categorizer, including the possibility of recursive decision trees. ID3 <ref> [11] </ref> always uses attribute categorizers for nominal attributes and threshold categorizers for real attributes. To generate multivariate trees with perceptrons at nodes, the induction algorithm can put percep tron categorizers at the nodes. Induction algorithms Induction algorithms induce categorizers. <p> To generate multivariate trees with perceptrons at nodes, the induction algorithm can put percep tron categorizers at the nodes. Induction algorithms Induction algorithms induce categorizers. MLC ++ currently provides a majority inducer, a nearest-neighbor inducer [1], an ID3-like decision tree inducer <ref> [11] </ref>, and HOODG for inducing oblivious read-once decision graphs [7]. Visualization tools From the outset, one of our top priorities was to provide visualization tools to the user. Graphical displays of datasets and induced concepts can provide key insights.
Reference: [12] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1992. </year>
Reference-contexts: Display of information In order to understand the problem better, and perhaps bias a learning algorithm, it is helpful to view the resulting structures (e.g., decision tree), or the data. Even commercially available programs such as C4.5 <ref> [12] </ref> and CART [3] give only a rudimentary display of an induced tree. A library providing a common framework and basic tools for implementing learning algorithms would alleviate the pain involved in programming from scratch.
Reference: [13] <author> Jeffrey C. Schlimmer and Pat Langley. </author> <title> Learning, Machine. </title> <editor> In Stuart Shapiro and David Eckroth, editors, </editor> <booktitle> The Encyclopedia of Artificial Intelligence, </booktitle> <pages> pages 785-805. </pages> <publisher> Wiley-Interscience, </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Published in IEEE Conference on Tools with Artificial Intelligence, New Orleans, November 1994, pp. 740-743 Newton said he saw farther because he stood on the shoulders of giants. Computer programmers stand on each other's toes - James Coggins In supervised machine learning <ref> [13, 2] </ref>, one tries to find a rule (a categorizer) that can be used to accurately predict the class label of novel instances. During the last decade, the machine learning community has developed a plethora of algorithms for this task.
Reference: [14] <editor> C.C. Taylor, D. Michie, and D.J. Spiegalhalter. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Paramount Publishing International, </publisher> <year> 1994. </year>
Reference-contexts: Mooney has a collection of a few machine learning algorithms implemented in Lisp at UCI [9], but they are not an integrated environment, and are not very efficient. StatLog <ref> [14] </ref> is an ESPRIT project studying the behavior of over twenty algorithms (mostly in the ML-Toolbox), on over twenty datasets. StatLog is an instance of a good experimental study, but does not provide the tools to aid researchers in performing similar studies.
Reference: [15] <author> Janusz Wnek and Ryszard S. Michalski. </author> <title> Hypothesis-driven constructive induction in AQ17-HCI : A method and experiments. </title> <journal> Machine Learning, </journal> <volume> 14(2) </volume> <pages> 139-168, </pages> <year> 1994. </year>
Reference-contexts: Graphically display the learned structure (e.g., display the decision tree). 6. Graphically display the learned concept and deviations from the target concept (when it is known). A useful tool for this purpose is Michalski's Gen- eral Logic Diagrams <ref> [15] </ref>. 1.1 Problems with Current Methodology The results included in this survey were produced under a very wide variety of experimental conditions, and therefore it is impossible to compare them in any detailed manner. : : : the number of runs varies considerably, as does the ratio of the sizes of <p> Visualization of discrete data For viewing datasets and induced concepts, we have implemented General Logic Diagrams. After running an induction algorithm, users may gain insight about the induced concept by inspecting the GLD. High dimensional concepts are hard to visualize; Michalski <ref> [15] </ref> introduced General Logic Diagrams as a visualization technique to help understand concepts. We provide General Logic Diagrams for discrete concepts and allow users to view the induced concept, and, whenever the target concept is known, the symmetric errors.
References-found: 15

