URL: http://polaris.cs.uiuc.edu/reports/1433.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Title: c  
Author: flCopyright by William Joseph Blume 
Date: 1995  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> H. Abelson, G. J. Sussman, and J. Sussman. </author> <title> Structure and Interpretation of Computer Programs. </title> <publisher> The MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: The formal definition of the intersection operator for ranges can be found in Table 4.4. The next two sections will describe how these control and data ranges are computed. 2 This storing and reusing values to avoid recomputation is called memoization <ref> [37, 1] </ref>. 106 5.4 Computing control ranges 5.4.1 Needed functionality Our demand-driven control range propagation algorithm assumes that the immediate dominating control-flow edge is known for each statement and that the program is in SSA form.
Reference: [2] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1986. </year>
Reference-contexts: For each of these phases, mappings of variables to their ranges, (i.e., range dictionaries), are associated with each statement and each control-flow edge in the program unit. For both phases, iterative data-flow analysis <ref> [2] </ref> is used to compute the final values of these range dictionaries. For the widening phase, the ranges for all variables for all program entry points is initially defined to be [1 : 1]. <p> They are more accurate in the computation and propagation of affine variable constraints. However, they cannot handle non-affine variable constraints, such as a &lt; b fl c. Another strength in our representation is that our analyses can use a sparse data-flow form, such as definition-use chains <ref> [2] </ref> or Static Single Assignment [18], while theirs cannot. Performing range propagation on such a sparse form can greatly increase its efficiency. <p> Vertex u is the immediate dominator of vertex v if and only if u strictly dominates v and there is no vertex w such that u strictly dominates w and w strictly dominates v. See Aho, Sethi, and Ullman <ref> [2] </ref> for more details on these definitions. Similar dominance relationships can be defined for the control-flow edges in the program. For example, a control-flow edge dominates a statement if all paths from start to that statement pass through that control-flow edge.
Reference: [3] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer. </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: example is to use constraint propagation to eliminate redundant conditional jumps, which should improve performance since conditional jumps may cause the processor's pipeline to stall. 38 Chapter 3 THE RANGE TEST 3.1 Open issues in data dependence testing There has been much research in the area of data dependence analysis <ref> [3, 25, 37, 42, 50] </ref>. Modern day data dependence tests have become very accurate and efficient. However, most of these tests require the loop bounds and array subscripts to be represented as a linear (affine) function of loop index variables. <p> For a more thorough description of data dependence and dependence analysis, see Banerjee et al <ref> [5, 3, 50] </ref>. To ease the presentation of the Range Test, we will assume that we have a perfectly nested FORTRAN-77 loop nest as shown in Figure 3.1. We will also assume that the tested array A has only one dimension. <p> The following discussion compares our test to one of the most effective state-of-the-art tests and points out related ideas of other projects. Mathematically, the Range Test can be thought of as an extension of a symbolic version of the Triangular Banerjee's Inequalities test with dependence direction vectors <ref> [3, 49] </ref>, although our implementation differs. The only drawback of our test, compared to the Triangular Baner-jee's test with directions, is that it cannot test arbitrary direction vectors, particularly those containing more than one `&lt;' or `&gt;' (e.g., (&lt;; &lt;)).
Reference: [4] <author> Utpal Banerjee. </author> <title> A Theory of Loop Permutations. </title> <booktitle> In 2nd Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 54-74. </pages> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: To prove that all loops not carrying dependences in the permuted loop nest generated by the heuristic above also do not carry dependences in the original loop nest, we will need the following lemma. For similar lemmas and theorems on loop permutations, see the paper by Banerjee <ref> [4] </ref>. Lemma 1 A loop that does not carry a dependence can be legally moved deeper into the loop nest and all loops that didn't carry a dependence beforehand would still not do so. Proof. Suppose that Lemma 1 is false.
Reference: [5] <author> Utpal Banerjee, Rudolf Eigenmann, Alexandru Nicolau, and David Padua. </author> <title> Automatic Program Parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2), </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: Introductions to and summaries of the research performed on parallelizing compilers can be found in Banerjee, Eigenmann, Nicolau, and Padua <ref> [5] </ref>, Padua and Wolfe [40], or the textbook by Zima and Chapman [51]. 2 1.2 Effectiveness of late '80s parallelizing compilers As described in the previous section, much research has been done in developing techniques to identify and exploit parallelism in programs. <p> For a more thorough description of data dependence and dependence analysis, see Banerjee et al <ref> [5, 3, 50] </ref>. To ease the presentation of the Range Test, we will assume that we have a perfectly nested FORTRAN-77 loop nest as shown in Figure 3.1. We will also assume that the tested array A has only one dimension.
Reference: [6] <author> M. Berry, D. Chen, P. Koss, D. Kuck, L. Pointer, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin. </author> <title> The Perfect Club Benchmarks: Effective Performance Evalution of Supercomputers. </title> <booktitle> Int'l. Journal of Supercomputer Applications, Fall 1989, </booktitle> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: By speedup, we mean the time taken by the original sequential code divided by the time taken by the parallelized version of the code. The Perfect Benchmarks is a suite of 13 Fortran 77 programs representing applications in a number of areas in engineering and scientific computing <ref> [6] </ref>. The measurements were made on an Alliant FX/80, which is an 8 vector-processor machine. Since the pipeline of each vector unit is four deep, the theoretical achievable speedup from both vectorization and parallelism is 32. The practical maximum speedup is more around 16 to 20. <p> We then examined these manually parallelized loop nests to determine what kinds of symbolic analysis are required to guarantee that the applied transformations are legal. We have chosen the Perfect Benchmarks <ref> [6] </ref> for our analyses. The Perfect Benchmarks is a suite of 13 Fortran 77 programs that total about 60,000 lines of source code. They represent applications in a number of areas of engineering and scientific computing. <p> We perform the same optimization to g min j and g max j to eliminate loops that enclose only access A (g (~- 0 )). 3.6 Examples In this section, we will provide examples of important loop nests, taken from the Perfect Bench marks <ref> [6] </ref>, that the Range Test can determine to be parallel but which conventional data dependence tests cannot. One example is a loop nest taken from subroutine FTRVMT from the code OCEAN. This loop nest accounts for 44% of the code's sequential execution time on an Alliant FX/80. <p> These times are displayed in Table 5.1. The code column displays the name of each Fortran code examined. These codes were taken from the Perfect Benchmarks, which is a suite of Fortran 77 programs representing applications in a number of areas in engineering and scientific computing <ref> [6] </ref>. The Number of lines column displays the number of lines in each code after being converted into SSA form. The Computing control ranges and Computing data ranges columns give the total times taken to compute every control and data range respectively in each of the codes.
Reference: [7] <author> Bill Blume, Rudolf Eigenmann, Keith Faigin, John Grout, Jay Hoeflinger, David Padua, Paul Petersen, Bill Pottenger, Lawrence Rauchwerger, Peng Tu, and Stephen Weatherford. </author> <title> Polaris: The Next Generation in Parallelizing Compilers. </title> <booktitle> Proceedings of the Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York, pages 10.1 - 10.18, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: A description of Polaris and algorithms for these techniques can be found in <ref> [24, 9, 7] </ref>. Many of these additional techniques must perform some sort of symbolic analysis to be effective. The goal of this dissertation is to identify and develop symbolic analysis techniques that will improve the effectiveness of these techniques, with the emphasis upon data dependence analysis. <p> Because of this, dependence arcs from reductions, induction variables, and private arrays and scalars have already been eliminated when the 63 Range and Omege Tests were executed. Details of these advanced techniques can be found in <ref> [9, 7] </ref>. From Table 3.2, we can see that there are cases where the Range Test does better, and cases where the Omega Test does better. This should not be surprising, because the Omega Test has difficulties with non-affine expressions while the Range Test was designed to handle such cases.
Reference: [8] <author> William Blume and Rudolf Eigenmann. </author> <title> Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks Programs. </title> <journal> IEEE Transactions of Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Additionally, almost all of these studies used only small, synthetic kernels. To determine the effectiveness of parallelizing compilers on real programs, we measured the speedups of the Perfect Benchmarks that were parallelized by the 1988 versions of the two commercial parallelizing compilers: Kap and Vast <ref> [20, 8, 10] </ref>. We also measured the effectiveness of the individual restructuring techniques used by these compilers. By speedup, we mean the time taken by the original sequential code divided by the time taken by the parallelized version of the code. <p> Unfortunately, an effectiveness study of par-allelizing compilers performed by our research group in 1989-1992 on the Perfect Benchmarks 13 found that parallelizing compilers are not very effective at parallelizing real programs, (i.e., the transformed programs do not get good speedups from the original sequential codes.) <ref> [20, 8, 10] </ref>. In response to this, our research group manually parallelized the Perfect Benchmarks, using only those transformations that are theoretically implementable in a compiler, to determine how effective a parallelizing compiler can be [23, 22, 21].
Reference: [9] <author> William Blume, Rudolf Eigenmann, Jay Hoeflinger, David Padua, Paul Petersen, Lawrence Rauchwerger, and Peng Tu. </author> <title> Automatic Detection of Parallelism: A Grand Challenge for High-Performance Computing. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 37-47, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: A description of Polaris and algorithms for these techniques can be found in <ref> [24, 9, 7] </ref>. Many of these additional techniques must perform some sort of symbolic analysis to be effective. The goal of this dissertation is to identify and develop symbolic analysis techniques that will improve the effectiveness of these techniques, with the emphasis upon data dependence analysis. <p> Because of this, dependence arcs from reductions, induction variables, and private arrays and scalars have already been eliminated when the 63 Range and Omege Tests were executed. Details of these advanced techniques can be found in <ref> [9, 7] </ref>. From Table 3.2, we can see that there are cases where the Range Test does better, and cases where the Omega Test does better. This should not be surprising, because the Omega Test has difficulties with non-affine expressions while the Range Test was designed to handle such cases.
Reference: [10] <author> William Joseph Blume. </author> <title> Success and Limitations in Automatic Parallelization of the Perfect Benchmarks Programs. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> July </month> <year> 1992. </year> <month> 139 </month>
Reference-contexts: Additionally, almost all of these studies used only small, synthetic kernels. To determine the effectiveness of parallelizing compilers on real programs, we measured the speedups of the Perfect Benchmarks that were parallelized by the 1988 versions of the two commercial parallelizing compilers: Kap and Vast <ref> [20, 8, 10] </ref>. We also measured the effectiveness of the individual restructuring techniques used by these compilers. By speedup, we mean the time taken by the original sequential code divided by the time taken by the parallelized version of the code. <p> Unfortunately, an effectiveness study of par-allelizing compilers performed by our research group in 1989-1992 on the Perfect Benchmarks 13 found that parallelizing compilers are not very effective at parallelizing real programs, (i.e., the transformed programs do not get good speedups from the original sequential codes.) <ref> [20, 8, 10] </ref>. In response to this, our research group manually parallelized the Perfect Benchmarks, using only those transformations that are theoretically implementable in a compiler, to determine how effective a parallelizing compiler can be [23, 22, 21].
Reference: [11] <author> Franc~ois Bourdoncle. </author> <title> Abstract Debugging of Higher-Order Imperative Languages. </title> <booktitle> Pro--ceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 46-55, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: There has been some work in the determination of variable constraints. Much work has been spent in determining the possible range, or interval, of values that variables can take, 27 for the purpose of array bounds checking or program verification <ref> [31, 11] </ref>. These algorithms, however, only propagate integer ranges. Cousot and Halbwachs [17] offer a powerful algorithm for determining symbolic linear constraints between variables. Their algorithm is based upon the calculation, intersection, and merging of convex polyhedrons in the n-space of variable values. <p> The narrowing operator is used to regain some of the information lost by the widening operator. Our definitions of the narrowing and widening operators were influenced by the operators given by Bourdoncle <ref> [11] </ref>. The range propagation algorithm uses a special range, denoted as &gt;, which represents an undefined value. <p> Although he does propose simple techniques to handle symbolic ranges, our symbolic analysis techniques are superior. (He restricts the bounds of his symbolic ranges to the form &lt; variable &gt; + &lt; constant &gt;.) 99 Bourdoncle <ref> [11] </ref> greatly improves the accuracy of the integer range propagation algorithm by Harrison through the use of abstract interpretation [16]. Our use of the narrowing operator was influenced by his algorithm. Bourdoncle's algorithm is unable to generate symbolic ranges. <p> To partially overcome this, compute data ranges phase is called twice by compute data ranges. The second invocation of compute data ranges phase applies a special operator called the narrowing operator <ref> [16, 11] </ref>, denoted as 4, at those nodes where the widening operator was applied in the previous invocation of compute data ranges phase. (The widening operator is not applied in this phase.) This narrowing operator allows the currently computed data range to replace any infinite bounds in the old data ranges <p> Although he does propose simple techniques to handle symbolic ranges, our symbolic analysis techniques are superior. (He restricts the bounds of his symbolic ranges to the form &lt; variable &gt; + &lt; constant &gt;.) Bourdoncle <ref> [11] </ref> greatly improves the accuracy of the integer range propagation algorithm by Harrison, through the use of abstract interpretation [16]. Our use of the narrowing operator was influenced by his algorithm. Bourdoncle's algorithm is unable to generate symbolic ranges.
Reference: [12] <author> Prestion Briggs, Keith D. Cooper, Mary W. Hall, and Linda Torczon. </author> <title> Goal-Directed Interprocedural Optimization. </title> <type> Technical report, </type> <institution> Rice University, </institution> <month> November </month> <year> 1990. </year> <month> TR90-147. </month>
Reference-contexts: We will assume that the compiler can privatize arrays [43], parallelize loops containing reduction statements, and perform interprocedural dependence analysis to parallelize loops with function calls <ref> [13, 12] </ref>. We 15 will also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others. This includes the constant propagation of symbolic expressions [46], the elimination of induction variables [28, 48], and symbolic simplification of expressions [14, 26]. <p> Many more constants can be found if the propagator is allowed to pass constants across procedure boundaries. To do this effectively, procedure cloning <ref> [12] </ref> is often required. One good example for interprocedural constant propagation is the code OCEAN, which requires extensive amount of interprocedural constant propagation, along with some procedure cloning, to parallelize seven of its most important loop nests.
Reference: [13] <author> David Callahan, Keith D. Cooper, Ken Kennedy, and Linda Torczon. </author> <title> Interprocedural Constant Propagation. </title> <booktitle> Proceedings of the SIGPLAN `86 Symposium on Compiler Construction, </booktitle> <pages> pages 152-161, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Finally, chapter 6 will summarize the results of this dissertation. 4 The third of the three most important symbolic analysis techniques is interprocedural constant propagation with procedure cloning. Although we have implemented this technique in Polaris, it is a faithful implementation of the algorithm described by Callahan et. al. <ref> [13] </ref>. <p> We will assume that the compiler can privatize arrays [43], parallelize loops containing reduction statements, and perform interprocedural dependence analysis to parallelize loops with function calls <ref> [13, 12] </ref>. We 15 will also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others. This includes the constant propagation of symbolic expressions [46], the elimination of induction variables [28, 48], and symbolic simplification of expressions [14, 26]. <p> The constant propagation of symbolic expressions is sometimes called forward substitution by the parallelizing compiler community. 2.4.1 Interprocedural constant propagation with procedure cloning From our experience with the Perfect Benchmarks, we have found that a constant propagation pass, whether it is for integers or for symbolic expressions, must work interprocedurally <ref> [13] </ref> to allow many of the codes to be parallelized. Many more constants can be found if the propagator is allowed to pass constants across procedure boundaries. To do this effectively, procedure cloning [12] is often required.
Reference: [14] <author> Thomas E. Cheatham Jr., Glenn H. Holloway, and Judy A. Townley. </author> <title> Symbolic Evaluation and the Analysis of Programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-5(4):402-417, </volume> <month> July </month> <year> 1979. </year>
Reference-contexts: Constant propagation [46] and induction variable substitution [28, 48] are the most common methods used to transform array subscripts into testable linear expressions. Symbolic simplification of expressions <ref> [14, 29, 32] </ref> is also important for canceling common terms and eliminating complex expressions. Although transforming a subscript expression or loop bound into a testable linear form is preferred, it is not always possible. <p> We 15 will also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others. This includes the constant propagation of symbolic expressions [46], the elimination of induction variables [28, 48], and symbolic simplification of expressions <ref> [14, 26] </ref>. However, we will mention cases where the transformations or analysis techniques above need minor modifications or more accurate information. <p> These rules are displayed in Table 4.2. In addition to the range, MIN, or MAX specific simplifications given earlier, we apply conventional symbolic simplification techniques to the expression. These techniques include constant folding, distribution of products-of-sums, and the combination and cancellation of common symbolic terms <ref> [14, 26, 28, 32] </ref>. We also use advanced techniques developed by Haghighat [28] to simplify expressions containing integer divisions. Where they fail, we multiply out the divisions in the difference range, being careful to include the truncation errors of the divisions.
Reference: [15] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Efficient algorithms for finding SCCs and back-edges, and performing topological sorts can be found in <ref> [15] </ref>. As an example, we will compute the replacement order for the RDG graph shown in Figure 4.4. The algorithm first computes the strongly connected components of the graph, then topologically sorts them into the order (SCC1, SCC2, SCC3, SCC4).
Reference: [16] <author> Partrick Cousot and Radhia Cousot. </author> <title> Abstract Interpretation: A unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints. </title> <booktitle> Proceedings of the 4th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 238-252, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: However, we do believe they are worth mentioning. 2.8.1 Compile time interpretation of programs One important, but potentially expensive technique is the compile time interpretation of programs. Essentially, the idea is to execute the program without input data. Or in another words, to perform abstract interpretation <ref> [16] </ref> where the abstractions in the algorithm are kept to a minimum. One example that needs such an analysis is the determination that an array is filled with the factors of some scalar, as described in the example for ADM in Section 2.7. <p> The expression comparison facility uses these variable constraints to determine arithmetic relationships between two symbolic expressions. The range propagation algorithm centers on the collection and propagation of symbolic lower and upper bounds on variables, called ranges, through a program unit. Abstract interpretation <ref> [16] </ref> is used to compute the ranges for variables at each point of a program unit. <p> Abstract interpretation <ref> [16] </ref> is used to compute the ranges for variables at each point of a program unit. <p> techniques to handle symbolic ranges, our symbolic analysis techniques are superior. (He restricts the bounds of his symbolic ranges to the form &lt; variable &gt; + &lt; constant &gt;.) 99 Bourdoncle [11] greatly improves the accuracy of the integer range propagation algorithm by Harrison through the use of abstract interpretation <ref> [16] </ref>. Our use of the narrowing operator was influenced by his algorithm. Bourdoncle's algorithm is unable to generate symbolic ranges. The accuracy of the ranges generated by his (and Harrison's) technique can be improved with our monotonicity replacement method in Section 4.2. <p> For example, the data range of an induction variable, (i = i + 1), inside a loop can take on the successive ranges [1 : 1], [1 : 2], [1 : 3], : : : . To guarantee that such ranges would reach a fixed point, a widening operator <ref> [16] </ref>, denoted as 5, is applied to selected ranges in the data-flow graph. <p> To partially overcome this, compute data ranges phase is called twice by compute data ranges. The second invocation of compute data ranges phase applies a special operator called the narrowing operator <ref> [16, 11] </ref>, denoted as 4, at those nodes where the widening operator was applied in the previous invocation of compute data ranges phase. (The widening operator is not applied in this phase.) This narrowing operator allows the currently computed data range to replace any infinite bounds in the old data ranges <p> simple techniques to handle symbolic ranges, our symbolic analysis techniques are superior. (He restricts the bounds of his symbolic ranges to the form &lt; variable &gt; + &lt; constant &gt;.) Bourdoncle [11] greatly improves the accuracy of the integer range propagation algorithm by Harrison, through the use of abstract interpretation <ref> [16] </ref>. Our use of the narrowing operator was influenced by his algorithm. Bourdoncle's algorithm is unable to generate symbolic ranges.
Reference: [17] <author> Patrick Cousot and Nicolas Halbwachs. </author> <title> Automatic Discovery of Linear Restraints Among Variables of a Program. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 84-97, </pages> <year> 1978. </year>
Reference-contexts: Much work has been spent in determining the possible range, or interval, of values that variables can take, 27 for the purpose of array bounds checking or program verification [31, 11]. These algorithms, however, only propagate integer ranges. Cousot and Halbwachs <ref> [17] </ref> offer a powerful algorithm for determining symbolic linear constraints between variables. Their algorithm is based upon the calculation, intersection, and merging of convex polyhedrons in the n-space of variable values. However, their algorithm cannot handle nonlinear expressions such as a &lt; b fl c. <p> The same authors presented ideas to calculate the set of variable constraints holding for each statement of the program unit, then to use these constraints to prove or disprove symbolic inequalities for dependence testing. More specifically, they use an algorithm by Cousot and Halbwachs <ref> [17] </ref> to compute the set of variable constraints. We also determine variable constraints and perform symbolic inequality tests, although we use different techniques, (i.e., Range Propagation). We will compare these two methods to compute variable constraints later in Chapter 4. <p> A simple way to perform this is to replace all occurrences of that variable with the variable's range, (i.e., call replace var). However, a more accurate result can be achieved for variable modifications caused by a special class of assignment statement called an invertible assignment <ref> [17] </ref>. <p> Our use of the narrowing operator was influenced by his algorithm. Bourdoncle's algorithm is unable to generate symbolic ranges. The accuracy of the ranges generated by his (and Harrison's) technique can be improved with our monotonicity replacement method in Section 4.2. Cousot and Halbwachs <ref> [17] </ref> presents a different method to compute and propagate constraints through a program. In their technique, sets of constraints between variables are represented as a convex polyhedron in the n-space of variable values. <p> Bourdoncle's algorithm is unable to generate symbolic ranges. Neither Harrison's nor Bourdoncle's algorithms are demand-driven nor do they use a sparse 132 data-flow representation of a program, such as SSA form or definition-use chains, to improve the efficiency of their algorithms. Cousot and Halbwachs <ref> [17] </ref> presents a different method to compute and propagate constraints through a program. In their technique, sets of constraints between variables are represented as a convex polyhedron in the n-space of variable values.
Reference: [18] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently Computing Static Single Assignment Form and the Control Dependence Graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: However, they cannot handle non-affine variable constraints, such as a &lt; b fl c. Another strength in our representation is that our analyses can use a sparse data-flow form, such as definition-use chains [2] or Static Single Assignment <ref> [18] </ref>, while theirs cannot. Performing range propagation on such a sparse form can greatly increase its efficiency. In a complementary paper, Tu and Padua [44] present a symbolic expression comparison and constraint propagation technique, based on an extension of Static Single Assignment (SSA) 100 form [18]. <p> [2] or Static Single Assignment <ref> [18] </ref>, while theirs cannot. Performing range propagation on such a sparse form can greatly increase its efficiency. In a complementary paper, Tu and Padua [44] present a symbolic expression comparison and constraint propagation technique, based on an extension of Static Single Assignment (SSA) 100 form [18]. Expressions are compared by repeatedly substituting variables with their constant symbolic values until the two expressions differ by only an integer constant. The values to substitute are determined by a demand-driven analysis of the program. <p> An efficient algorithm to translate programs into SSA form is described in <ref> [18] </ref>. In this chapter, we will assume that the function def (v) would return the single statement in the program that defines v. 5.3 Range propagation Briefly the range propagation algorithm computes the range of each variable at each point of the program. <p> STOP 100 CONTINUE 3 A statement s is control-dependent upon another statement t if the execution of statement t may determine whether statement s would be executed or not. A formal definition of control-dependences, as well as how to efficiently compute them, can be found in <ref> [18] </ref>. 109 the algorithm would not be able to prove that n &gt; 0 at the CONTINUE statement, since neither of the two exiting control-flow edges of the two IF statements dominate the CONTINUE statement. We have chosen to look at only dominating control-flow edges for simplicity and efficiency.
Reference: [19] <author> R. J. Duffin. </author> <title> On Fourier's Analysis of Linear Inequality Systems. </title> <booktitle> Mathematical Programming Study 1, </booktitle> <pages> pages 71-95, </pages> <year> 1974. </year>
Reference-contexts: Roughly, the Omega Test is a variant of integer Fourier-Motzkin analysis <ref> [19, 47] </ref> with optimizations to make the common cases fast. For affine array subscripts and loop bounds, the Omega Test is an exact data dependence test. The Omega Test handles non-affine expressions using uninterpreted function symbols. Our implementation of the Omega Test uses the Omega Library version 0.91 [35].
Reference: [20] <author> Rudolf Eigenmann and William Blume. </author> <title> An Effectiveness Study of Parallelizing Compiler Techniques. </title> <booktitle> Proceedings of ICPP'91, </booktitle> <address> St. Charles, IL, II:17-25, </address> <month> August 12-16, </month> <year> 1991. </year>
Reference-contexts: Additionally, almost all of these studies used only small, synthetic kernels. To determine the effectiveness of parallelizing compilers on real programs, we measured the speedups of the Perfect Benchmarks that were parallelized by the 1988 versions of the two commercial parallelizing compilers: Kap and Vast <ref> [20, 8, 10] </ref>. We also measured the effectiveness of the individual restructuring techniques used by these compilers. By speedup, we mean the time taken by the original sequential code divided by the time taken by the parallelized version of the code. <p> Unfortunately, an effectiveness study of par-allelizing compilers performed by our research group in 1989-1992 on the Perfect Benchmarks 13 found that parallelizing compilers are not very effective at parallelizing real programs, (i.e., the transformed programs do not get good speedups from the original sequential codes.) <ref> [20, 8, 10] </ref>. In response to this, our research group manually parallelized the Perfect Benchmarks, using only those transformations that are theoretically implementable in a compiler, to determine how effective a parallelizing compiler can be [23, 22, 21].
Reference: [21] <author> Rudolf Eigenmann, Jay Hoeflinger, G. Jaxon, and David Padua. </author> <title> The Cedar Fortran Project. </title> <type> Technical Report 1262, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res, & Dev., </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: are not sufficiently powerful enough to identify and exploit the parallelism in the programs? To answer this question, our research group at the Center of Supercomputing Research and Development decided to manually parallelize the Perfect Benchmarks, using only those techniques that are likely to be implementable in a parallelizing compiler <ref> [23, 22, 21] </ref>. 5 The results of this manual parallelization effort were very heartening. From the twelve codes in the Perfect Benchmarks that were hand-parallelized, we were able to achieve a mean speedup of ten to twenty on the Cedar multiprocessor, which consists of four clusters of Alliant FX/8's. <p> Hence, there exists significant room for improvement for parallelizing compilers. Although the speedups achieved by this manual parallelization effort were impressive, the main lesson that we learned from this effort was that only a few additional techniques were needed to achieve these speedups for most of the codes <ref> [23, 22, 21] </ref>. These techniques were: * Interprocedural analysis * Nonlinear dependence analysis * Array privatization * Generalized induction variable substitution * Reduction parallelization Interprocedural analysis is needed since the important 1 parallel loops in many programs contain procedure calls. <p> In response to this, our research group manually parallelized the Perfect Benchmarks, using only those transformations that are theoretically implementable in a compiler, to determine how effective a parallelizing compiler can be <ref> [23, 22, 21] </ref>. This manual parallelization effort shown that it was possible for parallelizing compilers to transform programs into a parallel form that gets good speedups from the original sequential code.
Reference: [22] <author> Rudolf Eigenmann, Jay Hoeflinger, Greg Jaxon, Zhiyuan Li, and David Padua. </author> <title> Restructuring Fortran Programs for Cedar. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(7) </volume> <pages> 553-573, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: are not sufficiently powerful enough to identify and exploit the parallelism in the programs? To answer this question, our research group at the Center of Supercomputing Research and Development decided to manually parallelize the Perfect Benchmarks, using only those techniques that are likely to be implementable in a parallelizing compiler <ref> [23, 22, 21] </ref>. 5 The results of this manual parallelization effort were very heartening. From the twelve codes in the Perfect Benchmarks that were hand-parallelized, we were able to achieve a mean speedup of ten to twenty on the Cedar multiprocessor, which consists of four clusters of Alliant FX/8's. <p> Hence, there exists significant room for improvement for parallelizing compilers. Although the speedups achieved by this manual parallelization effort were impressive, the main lesson that we learned from this effort was that only a few additional techniques were needed to achieve these speedups for most of the codes <ref> [23, 22, 21] </ref>. These techniques were: * Interprocedural analysis * Nonlinear dependence analysis * Array privatization * Generalized induction variable substitution * Reduction parallelization Interprocedural analysis is needed since the important 1 parallel loops in many programs contain procedure calls. <p> In response to this, our research group manually parallelized the Perfect Benchmarks, using only those transformations that are theoretically implementable in a compiler, to determine how effective a parallelizing compiler can be <ref> [23, 22, 21] </ref>. This manual parallelization effort shown that it was possible for parallelizing compilers to transform programs into a parallel form that gets good speedups from the original sequential code.
Reference: [23] <author> Rudolf Eigenmann, Jay Hoeflinger, Zhiyuan Li, and David Padua. </author> <title> Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Lecture Notes in Computer Science 589. Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 65-83, </pages> <month> August </month> <year> 1991. </year> <month> 140 </month>
Reference-contexts: are not sufficiently powerful enough to identify and exploit the parallelism in the programs? To answer this question, our research group at the Center of Supercomputing Research and Development decided to manually parallelize the Perfect Benchmarks, using only those techniques that are likely to be implementable in a parallelizing compiler <ref> [23, 22, 21] </ref>. 5 The results of this manual parallelization effort were very heartening. From the twelve codes in the Perfect Benchmarks that were hand-parallelized, we were able to achieve a mean speedup of ten to twenty on the Cedar multiprocessor, which consists of four clusters of Alliant FX/8's. <p> Hence, there exists significant room for improvement for parallelizing compilers. Although the speedups achieved by this manual parallelization effort were impressive, the main lesson that we learned from this effort was that only a few additional techniques were needed to achieve these speedups for most of the codes <ref> [23, 22, 21] </ref>. These techniques were: * Interprocedural analysis * Nonlinear dependence analysis * Array privatization * Generalized induction variable substitution * Reduction parallelization Interprocedural analysis is needed since the important 1 parallel loops in many programs contain procedure calls. <p> In response to this, our research group manually parallelized the Perfect Benchmarks, using only those transformations that are theoretically implementable in a compiler, to determine how effective a parallelizing compiler can be <ref> [23, 22, 21] </ref>. This manual parallelization effort shown that it was possible for parallelizing compilers to transform programs into a parallel form that gets good speedups from the original sequential code. <p> jl, 64, 2*i2k js = 129*jj + mm - 129 h = data (js) - data (js2) data (js) = data (js) + data (js2) data (js2) = h * exj end do end do The need for a such a dependence test was first discussed by Eigenmann et. al. <ref> [23] </ref>. Maslov [36] presented the delinearization algorithm, which can handle any subscript expression c 0 + j=1 c j i j with symbolic loop-invariant expressions for the c j 's. Essentially, the delineariza-tion algorithm partitions the array expression into several independent subexpressions, and tests these partitions separately for dependences. <p> Early ideas of such a test were described in <ref> [23, 34] </ref>. The most distinguished feature of the test may be the fact that it is now available in an actual compiler, which has proven to parallelize important programs to an unprecedented degree.
Reference: [24] <author> Keith A. Faigin, Jay P. Hoeflinger, David A. Padua, Paul M. Petersen, and Stephen A. Weatherford. </author> <title> The Polaris Internal Representation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(5) </volume> <pages> 553-286, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: A description of Polaris and algorithms for these techniques can be found in <ref> [24, 9, 7] </ref>. Many of these additional techniques must perform some sort of symbolic analysis to be effective. The goal of this dissertation is to identify and develop symbolic analysis techniques that will improve the effectiveness of these techniques, with the emphasis upon data dependence analysis.
Reference: [25] <author> Gina Goff, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Practical Dependence Testing. </title> <booktitle> In Proceedings of the ACM SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 15-29, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: example is to use constraint propagation to eliminate redundant conditional jumps, which should improve performance since conditional jumps may cause the processor's pipeline to stall. 38 Chapter 3 THE RANGE TEST 3.1 Open issues in data dependence testing There has been much research in the area of data dependence analysis <ref> [3, 25, 37, 42, 50] </ref>. Modern day data dependence tests have become very accurate and efficient. However, most of these tests require the loop bounds and array subscripts to be represented as a linear (affine) function of loop index variables. <p> For example, constant propagation and induction variable substitution are used to remove loop-variant variables. Other techniques have also been developed to handle additive, loop-invariant, symbolic terms or to eliminate unwanted operations such as divisions <ref> [25, 37, 42] </ref>. 39 L 1 : DO i 1 = P 1 ; Q 1 ; R 1 L n : DO i n = P n ; Q n ; R n S 2 : = A (g (i 1 ; : : : ; i n )) ENDDO
Reference: [26] <author> Mohammad Haghighat and Constantine Polychronopoulos. </author> <title> Symbolic Dependence Analysis for High-Performance Parallelizing Compilers. </title> <booktitle> Parallel and Distributed Computing: Advances in Languages and Compilers for Parallel Processing, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> pages 310-330, </pages> <year> 1991. </year>
Reference-contexts: We 15 will also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others. This includes the constant propagation of symbolic expressions [46], the elimination of induction variables [28, 48], and symbolic simplification of expressions <ref> [14, 26] </ref>. However, we will mention cases where the transformations or analysis techniques above need minor modifications or more accurate information. <p> With this information, a compiler can determine that there is no dependence between S1 and S2. Constraint propagation can also be part of the dependence test itself. For example, Banerjee's Inequalities Test can be extended to work with symbolic expressions with the additional information calculated by constraint propagation <ref> [26] </ref>. Constraint propagation can also be used for array privatization. An array privatizer often needs to determine whether a range of array elements that are defined (a (1:m)) covers another constraint of elements used (a (1:n)). This requires the comparison of the bounds of these ranges. (e.g. is m n?). <p> direction vectors tested is better than Banerjee's Inequalities with directions, since we test at most O (n 2 ) direction vectors while Banerjee's Inequalities with directions may test as many as O (3 n ) direction vectors. 69 Haghighat and Polychronopoulos presented a dependence test to handle nonlinear, symbolic expressions <ref> [26] </ref>. Their algorithm is essentially a symbolic version of Banerjee's Inequalities test. <p> We will then discuss how two major components of the algorithm are implemented. A cost analysis of this algorithm will then be performed. 4.2.1 Algorithm This algorithm assumes that the compiler can manipulate and simplify arbitrary symbolic expressions. Efficient simplification techniques for parallelizing compilers can be found in <ref> [26, 28, 32] </ref>. <p> These rules are displayed in Table 4.2. In addition to the range, MIN, or MAX specific simplifications given earlier, we apply conventional symbolic simplification techniques to the expression. These techniques include constant folding, distribution of products-of-sums, and the combination and cancellation of common symbolic terms <ref> [14, 26, 28, 32] </ref>. We also use advanced techniques developed by Haghighat [28] to simplify expressions containing integer divisions. Where they fail, we multiply out the divisions in the difference range, being careful to include the truncation errors of the divisions.
Reference: [27] <author> Mohammad Haghighat and Constantine Polychronopoulos. </author> <title> Symbolic Analysis: A Basis for Paralleliziation, Optimization, and Scheduling of Programs. </title> <booktitle> Proceedings of the Sixth Annual Languages and Compilers for Parallelism Workshop, </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Maslov [36] presented the delinearization algorithm, which can handle any subscript expression c 0 + j=1 c j i j with symbolic loop-invariant expressions for the c j 's. Essentially, the delineariza-tion algorithm partitions the array expression into several independent subexpressions, and tests these partitions separately for dependences. Haghighat <ref> [27] </ref> describes how to prove that a subscript expression is strictly increasing or decreasing. By proving that a subscript expression is strictly increasing or decreasing, one can eliminate all self-dependences on the array access with the subscript expression. It can handle a more general class of symbolic expressions than Maslov's. <p> We also determine variable constraints and perform symbolic inequality tests, although we use different techniques, (i.e., Range Propagation). We will compare these two methods to compute variable constraints later in Chapter 4. In a separate paper, Haghighat and Polychronopoulos <ref> [27] </ref> describe a technique to prove that a symbolic expression is strictly increasing or decreasing. By using this technique, self-dependences for an array reference can be eliminated. Their example can prove that all the loops in Figure 3.7, after induction variable substitution, are parallel.
Reference: [28] <author> Mohammad R. Haghighat and Constantine D. Polychronopoulos. </author> <title> Symbolic Program Analysis and Optimization for Parallelizing Compilers. </title> <booktitle> Presented at the 5th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August 3-5, </month> <year> 1992. </year>
Reference-contexts: Generalized induction variable substitution is simply induction variable substitution extended to handle cases such as triangular loop nests, multiply nested loop nests with symbolic bounds, or loop nests with coupled induction variables <ref> [28] </ref>. Finally we have seen that a large number of important loop nests have cross-iteration dependences from reduction statements. <p> Extend data dependence analysis to cope with such expressions. The most straightforward, and usually the easiest, way to handle nonlinear subscript expressions or expressions containing loop variant variables is to eliminate these nonlinearities or loop variants. Constant propagation [46] and induction variable substitution <ref> [28, 48] </ref> are the most common methods used to transform array subscripts into testable linear expressions. Symbolic simplification of expressions [14, 29, 32] is also important for canceling common terms and eliminating complex expressions. <p> We 15 will also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others. This includes the constant propagation of symbolic expressions [46], the elimination of induction variables <ref> [28, 48] </ref>, and symbolic simplification of expressions [14, 26]. However, we will mention cases where the transformations or analysis techniques above need minor modifications or more accurate information. <p> We will then discuss how two major components of the algorithm are implemented. A cost analysis of this algorithm will then be performed. 4.2.1 Algorithm This algorithm assumes that the compiler can manipulate and simplify arbitrary symbolic expressions. Efficient simplification techniques for parallelizing compilers can be found in <ref> [26, 28, 32] </ref>. <p> These rules are displayed in Table 4.2. In addition to the range, MIN, or MAX specific simplifications given earlier, we apply conventional symbolic simplification techniques to the expression. These techniques include constant folding, distribution of products-of-sums, and the combination and cancellation of common symbolic terms <ref> [14, 26, 28, 32] </ref>. We also use advanced techniques developed by Haghighat [28] to simplify expressions containing integer divisions. Where they fail, we multiply out the divisions in the difference range, being careful to include the truncation errors of the divisions. <p> These techniques include constant folding, distribution of products-of-sums, and the combination and cancellation of common symbolic terms [14, 26, 28, 32]. We also use advanced techniques developed by Haghighat <ref> [28] </ref> to simplify expressions containing integer divisions. Where they fail, we multiply out the divisions in the difference range, being careful to include the truncation errors of the divisions.
Reference: [29] <author> Mohammad Reza Haghighat. </author> <title> Symbolic Analysis for Parallelizing Compilers. </title> <type> Master's thesis, </type> <institution> Univ of Illinois at Urbana-Champaign, Cntr for Supercomputing Res & Dev, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Constant propagation [46] and induction variable substitution [28, 48] are the most common methods used to transform array subscripts into testable linear expressions. Symbolic simplification of expressions <ref> [14, 29, 32] </ref> is also important for canceling common terms and eliminating complex expressions. Although transforming a subscript expression or loop bound into a testable linear form is preferred, it is not always possible.
Reference: [30] <author> D. Harel. </author> <title> A linear time algorithm for finding dominators in a flow graph and related problems. </title> <booktitle> Proceedings of the 17th ACM Symposium of Theory of Computing, </booktitle> <pages> pages 185-194, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: The function icdom (s) will represent the immediate dominating control-flow edge of the statement s. An linear-time algorithm for computing dominators has been developed by Harel <ref> [30] </ref>. Alternatively, one can approximate the dominating control-flow edges from the statement dominators, which must be computed when translating into SSA form.
Reference: [31] <author> William H. Harrison. </author> <title> Compiler Analysis of the Value Ranges for Variables. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-3(3):243-250, </volume> <month> May </month> <year> 1977. </year>
Reference-contexts: There has been some work in the determination of variable constraints. Much work has been spent in determining the possible range, or interval, of values that variables can take, 27 for the purpose of array bounds checking or program verification <ref> [31, 11] </ref>. These algorithms, however, only propagate integer ranges. Cousot and Halbwachs [17] offer a powerful algorithm for determining symbolic linear constraints between variables. Their algorithm is based upon the calculation, intersection, and merging of convex polyhedrons in the n-space of variable values. <p> the algorithm is also sensitive to other factors, such as the number of integer variables, the complexity of the expressions that these variables are assigned to, and the complexity of the program's control flow. 4.4 Related work The idea for representing program constraints as ranges was first proposed by Harrison <ref> [31] </ref> for array bounds checking and program verification. In his paper, Harrison describes how one can compute the range of integer values that variables can take in a program unit, using data-flow analysis. <p> up of only symbolic constants and enclosing loop indices, and one only needs to know the constraints imposed upon these loop indices and symbolic constants to compare or compute the ranges of such expressions. 5.8 Related work The idea for representing program constraints as ranges was first proposed by Harrison <ref> [31] </ref> for array bounds checking and program verification. In his paper, Harrison describes how one can compute the range of integer values that variables can take in a program unit, using data-flow analysis.
Reference: [32] <author> Paul Havlak. </author> <title> Interprocedural Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Constant propagation [46] and induction variable substitution [28, 48] are the most common methods used to transform array subscripts into testable linear expressions. Symbolic simplification of expressions <ref> [14, 29, 32] </ref> is also important for canceling common terms and eliminating complex expressions. Although transforming a subscript expression or loop bound into a testable linear form is preferred, it is not always possible. <p> We will then discuss how two major components of the algorithm are implemented. A cost analysis of this algorithm will then be performed. 4.2.1 Algorithm This algorithm assumes that the compiler can manipulate and simplify arbitrary symbolic expressions. Efficient simplification techniques for parallelizing compilers can be found in <ref> [26, 28, 32] </ref>. <p> These rules are displayed in Table 4.2. In addition to the range, MIN, or MAX specific simplifications given earlier, we apply conventional symbolic simplification techniques to the expression. These techniques include constant folding, distribution of products-of-sums, and the combination and cancellation of common symbolic terms <ref> [14, 26, 28, 32] </ref>. We also use advanced techniques developed by Haghighat [28] to simplify expressions containing integer divisions. Where they fail, we multiply out the divisions in the difference range, being careful to include the truncation errors of the divisions. <p> Typically these functions are of the order O (n) or O (n log n), where n is the maximum of the sizes of the initial and final expressions. (See Havlak for a detailed analysis of the costs of simplifying symbolic expressions encountered by parallelizing compilers <ref> [32] </ref>.) Using these costs of substitutions and simplifications, one can compute the cost of the expression comparison algorithm. Unfortunately, each substitution or simplification can cause a multiplicative growth in the size of the final expression, causing subsequent substitutions and simplifications to run longer.
Reference: [33] <author> Matthew S. Hecht and Jeffrey D. Ullman. </author> <title> A Simple Algorithm for Global Data Flow Analysis Problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 4(4) </volume> <pages> 519-532, </pages> <month> December </month> <year> 1975. </year>
Reference-contexts: The algorithm quits only when the work list becomes empty. To minimize the number of updates performed upon the graph's nodes, the nodes in work list should be ordered by a topological order of the data-flow graph, ignoring any back-edges, (that is, in rPOSTORDER, as described in <ref> [33] </ref>). The data range (r) of a node x, whose variable's (v) definition contains a -function, is computed by unioning ([) the ranges of the arguments of its -function.
Reference: [34] <author> Jay Hoeflinger. </author> <title> Run-Time Dependence Testing by Integer Sequence Analysis. </title> <type> Technical Report 1194, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Early ideas of such a test were described in <ref> [23, 34] </ref>. The most distinguished feature of the test may be the fact that it is now available in an actual compiler, which has proven to parallelize important programs to an unprecedented degree.
Reference: [35] <author> Wayne Kelly, Vadim Maslov, William Pugh, Evan Rosser, Tatiana Shpeisman, and David Wonnacott. </author> <title> The Omega Library (version 0.91) Interface Guide. </title> <type> Technical report, </type> <institution> University of Maryland, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: For affine array subscripts and loop bounds, the Omega Test is an exact data dependence test. The Omega Test handles non-affine expressions using uninterpreted function symbols. Our implementation of the Omega Test uses the Omega Library version 0.91 <ref> [35] </ref>. Since uninterpreted function symbols are the Omega Test's solution to non-affine expressions, the functionality of uninterpreted function symbols needs some further explanation. An uninterpreted function symbol is simply a variable with one or more arguments, (e.g., f (i; j)), representing a side-effect-free function.
Reference: [36] <author> Vadim Maslov. Delinearization: </author> <title> An Efficient Way to Break Multiloop Dependence Equations. </title> <booktitle> Proceedings of the SIGPLAN `92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 152-161, </pages> <month> June </month> <year> 1992. </year> <month> 141 </month>
Reference-contexts: Maslov <ref> [36] </ref> presented the delinearization algorithm, which can handle any subscript expression c 0 + j=1 c j i j with symbolic loop-invariant expressions for the c j 's. Essentially, the delineariza-tion algorithm partitions the array expression into several independent subexpressions, and tests these partitions separately for dependences. <p> However, as described, the test only handles self-dependences. The subroutine OLDA in TRFD has other important loop nests that has multiple array accesses with nonlinear subscript expressions similar to the subscripts from Figure 3.7. Maslov <ref> [36] </ref> presents an alternate way to handle symbolic, non-linear expressions. Instead of testing these expressions directly, his algorithm partitions the expression into several independent subexpressions, then tests these partitions using conventional data dependence tests. Es 70 sentially, it delinearizes array references.
Reference: [37] <author> D. Maydan, J. Hennessy, and M. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In SIGPLAN NOTICES: Proceedings of the ACM SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Ontario, Canada, </address> <month> June 26-28, </month> <pages> pages 1-14. </pages> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: example is to use constraint propagation to eliminate redundant conditional jumps, which should improve performance since conditional jumps may cause the processor's pipeline to stall. 38 Chapter 3 THE RANGE TEST 3.1 Open issues in data dependence testing There has been much research in the area of data dependence analysis <ref> [3, 25, 37, 42, 50] </ref>. Modern day data dependence tests have become very accurate and efficient. However, most of these tests require the loop bounds and array subscripts to be represented as a linear (affine) function of loop index variables. <p> For example, constant propagation and induction variable substitution are used to remove loop-variant variables. Other techniques have also been developed to handle additive, loop-invariant, symbolic terms or to eliminate unwanted operations such as divisions <ref> [25, 37, 42] </ref>. 39 L 1 : DO i 1 = P 1 ; Q 1 ; R 1 L n : DO i n = P n ; Q n ; R n S 2 : = A (g (i 1 ; : : : ; i n )) ENDDO <p> Two conclusions can be derived from this result. First, there are a significant number of important parallel loops that contain nonlinear expressions in real programs. Second, the Range Test can determine that some, if not all, of these loops are parallel. With the aid of memoization <ref> [37] </ref>, or the caching of already tested array subscript pairs, we believe that the execution time of the Range Test is acceptable, even when applied as the only test. Our timings of the Range Test support this belief. <p> The formal definition of the intersection operator for ranges can be found in Table 4.4. The next two sections will describe how these control and data ranges are computed. 2 This storing and reusing values to avoid recomputation is called memoization <ref> [37, 1] </ref>. 106 5.4 Computing control ranges 5.4.1 Needed functionality Our demand-driven control range propagation algorithm assumes that the immediate dominating control-flow edge is known for each statement and that the program is in SSA form.
Reference: [38] <author> Kathryn S. McKinley. </author> <title> Dependence Analysis of Arrays Subscripted by Index Arrays. </title> <type> Technical report, </type> <institution> Rice University, </institution> <month> June </month> <year> 1991. </year> <month> TR91-162. </month>
Reference-contexts: Knowledge whether the array is singly valued or monotonically increasing or decreasing is very useful for eliminating false dependences <ref> [38] </ref>. Knowing the forward difference between elements also aids dependence analysis, which will be shown in the example below. The minimum or maximum value of the array is very useful for array privatization, as shown previously in the example for the usefulness of constraint propagation in BDNA.
Reference: [39] <author> D. Padua, R. Eigenmann, J. Hoeflinger, P. Petersen, P. Tu, S. Weatherford, and K. Faigin. </author> <title> Polaris: A New-Generation Parallelizing Compiler for MPP's. </title> <type> Technical Report 1306, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: The most interesting of these techniques were: symbolic, nonlinear expression data dependence tests, constraint propagation, guarded constant propagation, constant array propagation, subscript array analysis, and the generation of run time tests. We have implemented some of these techniques within the Polaris parallelizing compiler <ref> [39] </ref>, which is being developed at the University of Illinois.
Reference: [40] <author> D. Padua and M. Wolfe. </author> <title> Advanced Compiler Optimization for Supercomputers. </title> <journal> CACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December, </month> <year> 1986. </year>
Reference-contexts: Introductions to and summaries of the research performed on parallelizing compilers can be found in Banerjee, Eigenmann, Nicolau, and Padua [5], Padua and Wolfe <ref> [40] </ref>, or the textbook by Zima and Chapman [51]. 2 1.2 Effectiveness of late '80s parallelizing compilers As described in the previous section, much research has been done in developing techniques to identify and exploit parallelism in programs.
Reference: [41] <author> Bill Pottenger and Rudolf Eigenmann. </author> <title> Idiom Recognition in the Polaris Parallelizing Compiler. </title> <booktitle> Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <year> 1995, </year> <month> April </month> <year> 1995. </year>
Reference-contexts: For such cases, a compiler can extract such statements from a loop and replace them with a parallel version, similar to a parallel-prefix computation <ref> [41] </ref>. Because of the potentially great impact that these additional transformations would have on the effectiveness on parallelizing compilers, our research group has developed and implemented these transformations in Polaris, a state-of-the-art research parallelizing compiler being devel 7 oped at the University of Illinois.
Reference: [42] <author> William Pugh. </author> <title> A Practical Algorithm for Exact Array Dependence Analysis. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: That is, they are of the form c 0 + j=1 where c j are integer constants and i j are loop index variables. Some tests are more lenient, allowing some i j 's to also be loop invariant variables <ref> [42] </ref>. Unfortunately, array subscripts and loop bounds in real programs are not always in this format. Instead, the coefficients may not be integer constants, or the subscripts or array bounds may contain loop variant 8 variables or subscript array references. We will call such expressions nonlinear. <p> example is to use constraint propagation to eliminate redundant conditional jumps, which should improve performance since conditional jumps may cause the processor's pipeline to stall. 38 Chapter 3 THE RANGE TEST 3.1 Open issues in data dependence testing There has been much research in the area of data dependence analysis <ref> [3, 25, 37, 42, 50] </ref>. Modern day data dependence tests have become very accurate and efficient. However, most of these tests require the loop bounds and array subscripts to be represented as a linear (affine) function of loop index variables. <p> For example, constant propagation and induction variable substitution are used to remove loop-variant variables. Other techniques have also been developed to handle additive, loop-invariant, symbolic terms or to eliminate unwanted operations such as divisions <ref> [25, 37, 42] </ref>. 39 L 1 : DO i 1 = P 1 ; Q 1 ; R 1 L n : DO i n = P n ; Q n ; R n S 2 : = A (g (i 1 ; : : : ; i n )) ENDDO <p> The Range Test, on the other hand, would have no difficulties in proving that this array has no self-dependences. 3.7 Measurements To measure the effectiveness and speed of the Range Test, we compared its results with the Omega Test <ref> [42] </ref>. Roughly, the Omega Test is a variant of integer Fourier-Motzkin analysis [19, 47] with optimizations to make the common cases fast. For affine array subscripts and loop bounds, the Omega Test is an exact data dependence test. The Omega Test handles non-affine expressions using uninterpreted function symbols. <p> For example, it converts an array reference A (nfli+j), where 1 j n, into a two-dimensional array A (j; i). The greatest strength of this technique is that it can convert non-linear expressions into linear ones, allowing exact data tests like the Omega Test <ref> [42] </ref> to be applied. Because of this, there are situations where Maslov's algorithm proves independence whereas we cannot, such as the array references A (nfli+j) and A (i+nflj), where 1 i j n.
Reference: [43] <author> Peng Tu and David Padua. </author> <title> Automatic Array Privatization. </title> <editor> In Utpal BanerjeeDavid GelernterAlex NicolauDavid Padua, editor, </editor> <booktitle> Proc. Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR. </address> <booktitle> Lecture Notes in Computer Science., </booktitle> <volume> volume 768, </volume> <pages> pages 500-521, </pages> <month> August 12-14, </month> <year> 1993. </year>
Reference-contexts: That is, no data is passed between iterations for such arrays. Array privatization simply gives each processor a local copy of these arrays, thus breaking the dependences due to the sharing of memory locations by such arrays <ref> [43] </ref>. Generalized induction variable substitution is simply induction variable substitution extended to handle cases such as triangular loop nests, multiply nested loop nests with symbolic bounds, or loop nests with coupled induction variables [28]. <p> These assumptions are needed to prevent one from incorrectly concluding that all symbolic analysis techniques are ineffective for a code just because some other part of the parallelizing compiler is not sufficiently powerful enough to effectively parallelize it. We will assume that the compiler can privatize arrays <ref> [43] </ref>, parallelize loops containing reduction statements, and perform interprocedural dependence analysis to parallelize loops with function calls [13, 12]. We 15 will also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others. <p> Using this information, the compiler can determine that the range work (1:jmax) is defined at the start of L4. Thus, the definitions of work covers every use in the same iteration and array work can be privatized. Tu and Padua <ref> [43] </ref> offer an efficient algorithm to propagate guarded constants. We have also seen several examples in ARC2D, MDG, and QCD where control flow must be taken into account in array def/use analysis for array privatization if some important arrays is to be 23 parallelized. <p> The differences between our algorithm and theirs is mainly due to that the two applications were designed to handle two different problems. Their algorithm was designed to compare the bounds of array sections for array privatization <ref> [43] </ref>. Because conditional array definitions and uses occur in a significant fraction of important loop nests, flow-sensitive analysis is essential to successfully perform such comparisons.
Reference: [44] <author> Peng Tu and David Padua. </author> <title> Demand-Driven Symbolic Analysis. </title> <type> Technical Report 1336, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> Febraury </month> <year> 1994. </year>
Reference-contexts: Another strength in our representation is that our analyses can use a sparse data-flow form, such as definition-use chains [2] or Static Single Assignment [18], while theirs cannot. Performing range propagation on such a sparse form can greatly increase its efficiency. In a complementary paper, Tu and Padua <ref> [44] </ref> present a symbolic expression comparison and constraint propagation technique, based on an extension of Static Single Assignment (SSA) 100 form [18]. Expressions are compared by repeatedly substituting variables with their constant symbolic values until the two expressions differ by only an integer constant.
Reference: [45] <author> Peng Tu and David Padua. </author> <title> Gated SSA-Based Demand-Driven Symbolic Analysis for Parallelizing Compilers. </title> <type> Technical Report 1399, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Because of this, their algorithm can be much less efficient than ours. Also, their convex hull representation prevents one from creating a demand-driven version of their algorithm that is not overly complex. Tu and Padua <ref> [45] </ref> also present a demand-driven, symbolic expression comparison and constraint propagation technique, based on an extension of SSA called gated SSA form. Their technique compares expressions by repeatedly substituting variables with their constant symbolic values until the two expressions differ by only an integer constant. <p> Another improvement is to extend Range Propagation to determine and use ranges on array elements. This would allow the Range Test to handle subscripted subscripts. Finally, it would be desirable to merge our Range Propagation with Tu's and Padua's demand driven analysis <ref> [45] </ref>, combining the strengths of both techniques. 138
Reference: [46] <author> Mark N. Wegman and Kenneth Zadeck. </author> <title> Constant Propagation with Conditional Branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Eliminate the offending variable or subexpression from the subscript expression. 2. Extend data dependence analysis to cope with such expressions. The most straightforward, and usually the easiest, way to handle nonlinear subscript expressions or expressions containing loop variant variables is to eliminate these nonlinearities or loop variants. Constant propagation <ref> [46] </ref> and induction variable substitution [28, 48] are the most common methods used to transform array subscripts into testable linear expressions. Symbolic simplification of expressions [14, 29, 32] is also important for canceling common terms and eliminating complex expressions. <p> We 15 will also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others. This includes the constant propagation of symbolic expressions <ref> [46] </ref>, the elimination of induction variables [28, 48], and symbolic simplification of expressions [14, 26]. However, we will mention cases where the transformations or analysis techniques above need minor modifications or more accurate information.
Reference: [47] <author> H. P. Williams. </author> <title> Fourier's method of Linear Programming and its Dual. </title> <journal> The American Mathematical Monthly, </journal> <volume> 93(9) </volume> <pages> 681-695, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: Roughly, the Omega Test is a variant of integer Fourier-Motzkin analysis <ref> [19, 47] </ref> with optimizations to make the common cases fast. For affine array subscripts and loop bounds, the Omega Test is an exact data dependence test. The Omega Test handles non-affine expressions using uninterpreted function symbols. Our implementation of the Omega Test uses the Omega Library version 0.91 [35].
Reference: [48] <author> Michael Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> In Proc. ACM SIGPLAN'92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 162-174, </pages> <year> 1992. </year>
Reference-contexts: Extend data dependence analysis to cope with such expressions. The most straightforward, and usually the easiest, way to handle nonlinear subscript expressions or expressions containing loop variant variables is to eliminate these nonlinearities or loop variants. Constant propagation [46] and induction variable substitution <ref> [28, 48] </ref> are the most common methods used to transform array subscripts into testable linear expressions. Symbolic simplification of expressions [14, 29, 32] is also important for canceling common terms and eliminating complex expressions. <p> We 15 will also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others. This includes the constant propagation of symbolic expressions [46], the elimination of induction variables <ref> [28, 48] </ref>, and symbolic simplification of expressions [14, 26]. However, we will mention cases where the transformations or analysis techniques above need minor modifications or more accurate information.
Reference: [49] <author> Michael Wolfe. </author> <title> Triangular Banerjee's Inequalities with Directions. </title> <type> Technical report, </type> <institution> Ore-gon Graduate Institute of Science and Technology, </institution> <month> June </month> <year> 1992. </year> <pages> CS/E 92-013. </pages>
Reference-contexts: The following discussion compares our test to one of the most effective state-of-the-art tests and points out related ideas of other projects. Mathematically, the Range Test can be thought of as an extension of a symbolic version of the Triangular Banerjee's Inequalities test with dependence direction vectors <ref> [3, 49] </ref>, although our implementation differs. The only drawback of our test, compared to the Triangular Baner-jee's test with directions, is that it cannot test arbitrary direction vectors, particularly those containing more than one `&lt;' or `&gt;' (e.g., (&lt;; &lt;)).
Reference: [50] <author> Michael Wolfe and Utpal Banerjee. </author> <title> Data Dependence and its Application to Parallel Processing. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(2) </volume> <pages> 137-178, </pages> <year> 1987. </year> <month> 142 </month>
Reference-contexts: example is to use constraint propagation to eliminate redundant conditional jumps, which should improve performance since conditional jumps may cause the processor's pipeline to stall. 38 Chapter 3 THE RANGE TEST 3.1 Open issues in data dependence testing There has been much research in the area of data dependence analysis <ref> [3, 25, 37, 42, 50] </ref>. Modern day data dependence tests have become very accurate and efficient. However, most of these tests require the loop bounds and array subscripts to be represented as a linear (affine) function of loop index variables. <p> For a more thorough description of data dependence and dependence analysis, see Banerjee et al <ref> [5, 3, 50] </ref>. To ease the presentation of the Range Test, we will assume that we have a perfectly nested FORTRAN-77 loop nest as shown in Figure 3.1. We will also assume that the tested array A has only one dimension.

References-found: 50

