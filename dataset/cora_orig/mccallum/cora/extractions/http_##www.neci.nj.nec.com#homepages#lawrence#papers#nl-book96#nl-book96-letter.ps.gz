URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/nl-book96/nl-book96-letter.ps.gz
Refering-URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/nl-book96/nl-book96-11.html
Root-URL: 
Email: lawrence@research.nj.nec.com, fsandiway,gilesg@research.nj.nec.com  
Phone: 1  2  
Title: Appears in Symbolic, Connectionist, and Statistical Approaches to Learning for Natural Language Processing, Lecture Notes
Author: Stefan Wermter, Ellen Riloff and Gabriele Scheler, Steve Lawrence ;fl Sandiway Fong C. Lee Giles yz 
Address: 4 Independence Way, Princeton, NJ 08540  St. Lucia 4072, Australia  
Affiliation: NEC Research Institute,  Electrical and Computer Engineering, University of Queensland,  
Note: Springer Verlag, New York, pp. 3347, 1996.  These models exhibit the best performance.  
Abstract: We consider the task of training a neural network to classify natural language sentences as grammatical or un-grammatical, thereby exhibiting the same kind of discriminatory power provided by the Principles and Parameters linguistic framework, or Government and Binding theory. We investigate the following models: feed-forward neural networks, Frasconi-Gori-Soda and Back-Tsoi locally recurrent neural networks, Williams and Zipser and Elman recurrent neural networks, Euclidean and edit-distance nearest-neighbors, and decision trees. Non-neural network machine learning methods are included primarily for comparison. We find that the Elman and Williams & Zipser recurrent neural networks are able to find a representation for the grammar which we believe is more parsimonious. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, R. B. </author> <year> (1983), </year> <title> Sequential connectionist networks for answering simple questions about a microworld, </title> <booktitle> in `5th Annual Proceedings of the Cognitive Science Society', </booktitle> <pages> pp. 489495. </pages>
Reference: <author> Back, A. & Tsoi, A. </author> <year> (1991), </year> <title> `FIR and IIR synapses, a new neural network architecture for time series modeling', </title> <booktitle> Neural Computation 3(3), </booktitle> <pages> 375385. </pages>
Reference-contexts: Frasconi-Gori-Soda locally recurrent networks. A network with local feedback connection around the hidden nodes as described in (Frasconi, Gori & Soda 1992). 3. Back-Tsoi FIR. A multi-layer perceptron network with an FIR filter and a gain term included in every synapse as described in <ref> (Back & Tsoi 1991) </ref>. 4. Williams and Zipser. A fully connected recurrent network where all nodes are connected to all other nodes as described in (Williams & Zipser 1989). 5. Elman (Elman 1990, Elman 1991).
Reference: <author> Berg, G. </author> <year> (1992), </year> <title> A connectionist parser with recursive sentence structure and lexical disambiguation, </title> <booktitle> in `Proceedings AAAI', </booktitle> <pages> pp. 3237. </pages>
Reference: <author> Chomsky, N. </author> <year> (1981), </year> <title> Lectures on Government and Binding, </title> <publisher> Foris Publications. </publisher>
Reference-contexts: In the light of such examples and the fact that such contrasts crop up not just in English but in other languages (for example, the stubborn contrast also holds in Dutch), some linguists (chiefly Chomsky <ref> (Chomsky 1981) </ref>) have hypothesized that it is only reasonable that such knowledge is only partially acquired: the lack of variation found across speakers, and indeed, languages for certain classes of data suggests that there exists a fixed component of the 2 As is conventional, we use the asterisk to indicate ungrammaticality
Reference: <author> Chomsky, N. </author> <year> (1986), </year> <title> Knowledge of Language: Its Nature, Origin, and Use, </title> <type> Prager. </type>
Reference-contexts: and Its Acquisition Certainly one of the most important questions for the study of human language is: How do people unfailingly manage to acquire such a complex rule system? A system so complex that it has resisted the efforts of linguists to date to adequately describe in a formal system <ref> (Chomsky 1986) </ref>? Here, we will provide a couple of examples of the kind of knowledge native speakers often take for granted.
Reference: <author> Clouse, D., Giles, C. L., Horne, B. & Cottrell, G. </author> <year> (1994), </year> <title> Learning large DeBruijn automata with feed-forward neural networks, </title> <type> Technical Report CS94-398, </type> <institution> Computer Science and Engineering, University of California at San Diego, La Jolla, CA. </institution> <note> 9 Crutchfield, </note> <author> J. P. & Young, K. </author> <year> (1989), </year> <title> Computation at the onset of chaos, </title> <editor> in W. Zurek, ed., </editor> <title> `Complexity, Entropy and the Physics of Information', </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference: <author> Darken, C. & Moody, J. </author> <year> (1991), </year> <title> Note on learning rate schedules for stochastic optimization, </title> <editor> in R. Lippmann, J. Moody & D. S. Touretzky, eds, </editor> <booktitle> `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. 3, </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> pp. 832838. </pages>
Reference-contexts: Moody and Darkin have proposed search then converge learning rate schedules of the form <ref> (Darken & Moody 1991) </ref>: (t) = 0 = (1 + t=t ) where (t) is the learning rate at time t, 0 is the initial learning rate, and t is a constant.
Reference: <author> Das, S., Giles, C. L. & Sun, G. </author> <year> (1992), </year> <title> Learning context-free grammars: Limitations of a recurrent neural network with an external stack memory, </title> <booktitle> in `Proceedings of The Fourteenth Annual Conference of the Cognitive Science Society', </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <pages> pp. 791795. </pages>
Reference: <author> Elman, J. </author> <year> (1984), </year> <title> Structured representations and connectionist models, </title> <booktitle> in `6th Annual Proceedings of the Cognitive Science Society', </booktitle> <pages> pp. 1725. </pages>
Reference: <author> Elman, J. </author> <year> (1990), </year> <title> `Finding structure in time', </title> <booktitle> Cognitive Science 14, </booktitle> <pages> 179211. </pages>
Reference-contexts: Williams and Zipser. A fully connected recurrent network where all nodes are connected to all other nodes as described in (Williams & Zipser 1989). 5. Elman <ref> (Elman 1990, Elman 1991) </ref>. A recurrent network where each hidden layer node has a feedback connection to all other hidden layer nodes. The feedback connections are trainable and we use full backpropagation through time for training instead of the truncated version used by Elman.
Reference: <author> Elman, J. </author> <year> (1991), </year> <title> `Distributed representations, simple recurrent networks, and grammatical structure', Machine Learning 7(2/3), </title> <publisher> 195226. </publisher>
Reference-contexts: However, these grammars have unusual properties when implemented as sequential machines in the sense that they have little logic. It has been shown that RNNs have the representational power required for hierarchical solutions <ref> (Elman 1991) </ref>, and that they are Turing equivalent (Siegelmann & Sontag 1995). The RNNs investigated in this paper constitute complex, dynamical systems. Pollack (Pollack 1991) points out that Crutchfield and Young (Crutchfield & Young 1989) have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling. <p> This continued until the working set contained the entire training set. These trials were performed with the data ordered alphabetically, which enabled the networks to focus on the simpler data first. Elman suggests that the initial training constrains later training in a useful way <ref> (Elman 1991) </ref>. The use of sectioning has consistently decreased performance in our case. 3. Cost function. The relative entropy cost function has received particular attention (Haykin 1994) and has a natural interpretation in terms of learning probabilities (Kullback 1959).
Reference: <author> Frasconi, P., Gori, M. & Soda, G. </author> <year> (1992), </year> <title> `Local feedback multilayered networks', </title> <booktitle> Neural Computation 4(1), </booktitle> <pages> 120130. </pages>
Reference-contexts: Frasconi-Gori-Soda locally recurrent networks. A network with local feedback connection around the hidden nodes as described in <ref> (Frasconi, Gori & Soda 1992) </ref>. 3. Back-Tsoi FIR. A multi-layer perceptron network with an FIR filter and a gain term included in every synapse as described in (Back & Tsoi 1991). 4. Williams and Zipser.
Reference: <author> Gasser, M. & Lee, C. </author> <year> (1990), </year> <title> Networks that learn phonology, </title> <type> Technical report, </type> <institution> Computer Science Department, Indiana University. </institution>
Reference-contexts: John & McClelland 1990). Neural network models have been shown to be able to account for a variety of phenomena in phonology <ref> (Gasser & Lee 1990, Hare 1990, Touretzky 1989a, Touretzky 1989b) </ref>, morphology (Hare, Corina & Cottrell 1989, MacWhinney, Leinbach, Taraban & McDonald 1989) and role assignment (Miikkulainen & Dyer 1989, St. John & McClelland 1990).
Reference: <author> Giles, C. L., Horne, B. & Lin, T. </author> <year> (1995), </year> <title> `Learning a class of large finite state machines with a recurrent neural network', </title> <booktitle> Neural Networks 8(9), </booktitle> <pages> 13591365. </pages>
Reference-contexts: The algorithm is currently only practical for relatively small grammars (Pereira & Schabes 1992). stacks. The grammars learned were not large. Our task differs from these in that the grammar is considerably more complex. Recently large regular grammars have been learned by RNNs <ref> (Giles, Horne & Lin 1995, Clouse, Giles, Horne & Cottrell 1994) </ref>. However, these grammars have unusual properties when implemented as sequential machines in the sense that they have little logic. <p> Indeed, all models attain 50% or less correct classification on average for the Japanese data. More details can be found in Sect. 9 and <ref> (Lawrence, Giles & Fong 1995) </ref>. 3 Following classical GB theory, these classes are synthesized from the theta-grids of individual predicates via the Canonical Structural Realization (CSR) mechanism of Pesetsky (Pesetsky 1982). 4 For an output range of 0 to 1 and softmax outputs. 4 4 Nearest-Neighbors In the nearest-neighbors technique, the
Reference: <author> Giles, C. L., Miller, C., Chen, D., Chen, H., Sun, G. & Lee, Y. </author> <year> (1992), </year> <title> `Learning and extracting finite state automata with second-order recurrent neural networks', </title> <booktitle> Neural Computation 4(3), </booktitle> <pages> 393405. </pages>
Reference-contexts: We believe that the basic assumptions of smoothness in the required function approximation and the nature of parameter updating provide opportunities for improvement (as demonstrated by the techniques described within). The grammar learnt by the recurrent networks could be extracted and examined <ref> (Giles et al. 1992, Watrous & Kuhn 1992) </ref>. Further progress can be made by continuing to address the convergence of the backpropagation algorithm. However, we envisage a point after which advances will depend on considering the connectionist treatment of hierarchical representations.
Reference: <author> Hare, M. </author> <year> (1990), </year> <title> The role of similarity in Hungarian vowel harmony: A connectionist account, </title> <type> Technical Report CRL 9004, </type> <institution> Centre for Research in Language, University of California, </institution> <address> San Diego. </address>
Reference-contexts: John & McClelland 1990). Neural network models have been shown to be able to account for a variety of phenomena in phonology <ref> (Gasser & Lee 1990, Hare 1990, Touretzky 1989a, Touretzky 1989b) </ref>, morphology (Hare, Corina & Cottrell 1989, MacWhinney, Leinbach, Taraban & McDonald 1989) and role assignment (Miikkulainen & Dyer 1989, St. John & McClelland 1990).
Reference: <author> Hare, M., Corina, D. & Cottrell, G. </author> <year> (1989), </year> <title> Connectionist perspective on prosodic structure, </title> <type> Technical Report CRL Newsletter Volume 3 Number 2, </type> <institution> Centre for Research in Language, University of California, </institution> <address> San Diego. </address>
Reference-contexts: John & McClelland 1990). Neural network models have been shown to be able to account for a variety of phenomena in phonology (Gasser & Lee 1990, Hare 1990, Touretzky 1989a, Touretzky 1989b), morphology <ref> (Hare, Corina & Cottrell 1989, MacWhinney, Leinbach, Taraban & McDonald 1989) </ref> and role assignment (Miikkulainen & Dyer 1989, St. John & McClelland 1990). Induction of simpler grammars has been addressed often e.g. (Watrous & Kuhn 1992, Giles, Miller, Chen, Chen, Sun & Lee 1992) on learning Tomita languages.
Reference: <author> Harris, C. L. & Elman, J. </author> <year> (1984), </year> <title> Representing variable information with simple recurrent networks, </title> <booktitle> in `6th Annual Proceedings of the Cognitive Science Society', </booktitle> <pages> pp. 635642. </pages>
Reference: <author> Haykin, S. </author> <year> (1994), </year> <title> Neural Networks, A Comprehensive Foundation, </title> <publisher> Macmillan, </publisher> <address> New York, NY. </address>
Reference-contexts: Elman suggests that the initial training constrains later training in a useful way (Elman 1991). The use of sectioning has consistently decreased performance in our case. 3. Cost function. The relative entropy cost function has received particular attention <ref> (Haykin 1994) </ref> and has a natural interpretation in terms of learning probabilities (Kullback 1959). <p> The network was trained for a total of 1 million stochastic updates. All inputs and outputs were within the range zero to one. Bias inputs were used. The best of 50 random weight sets was chosen based on training set performance. Weights were initialized as shown in Haykin <ref> (Haykin 1994) </ref>. Targets outputs were 0.1 and 0.9 using the logistic output activation function. The quadratic cost function was used.
Reference: <author> Joshi, A. K. </author> <year> (1985), </year> <title> Tree adjoining grammars: how much context-sensitivity is required to provide reasonable structural descriptions?, </title> <editor> in L. K. D. R. Dowty & A. M. Zwicky, eds, </editor> <booktitle> `Natural Language Parsing', </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference-contexts: They have shown that these systems are not regular, but are finitely described by indexed context-free-grammars. Several modern computational linguistic grammatical theories fall in this class <ref> (Joshi 1985, Pollard 1984) </ref>. 1.2 Language and Its Acquisition Certainly one of the most important questions for the study of human language is: How do people unfailingly manage to acquire such a complex rule system? A system so complex that it has resisted the efforts of linguists to date to adequately
Reference: <author> Kruskal, J. B. </author> <year> (1983), </year> <title> An overview of sequence comparison, </title> <editor> in D. Sankoff & J. B. Kruskal, eds, </editor> <title> `Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison', </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts. </address>
Reference-contexts: The following equations are used iteratively to calculate the distances ending in the distance between the two complete sequences. i and j range from 0 to the length of the respective sequences and the superscripts denote sequences of the corresponding length. For more details see <ref> (Kruskal 1983) </ref>. d (a i ; b j ) = min &gt; &lt; d (a i1 ; b j + w (a i ; 0) deletion ofa i d (a i1 ; b j1 ) + w (a i ; b j ) b j replacesa i d (a i ;
Reference: <author> Kullback, S. </author> <year> (1959), </year> <title> Information Theory and Statistics, </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: The use of sectioning has consistently decreased performance in our case. 3. Cost function. The relative entropy cost function has received particular attention (Haykin 1994) and has a natural interpretation in terms of learning probabilities <ref> (Kullback 1959) </ref>.
Reference: <author> Lasnik, H. & Uriagereka, J. </author> <year> (1988), </year> <title> A Course in GB Syntax: Lectures on Binding and Empty Categories, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Full details on a successful simulation are given in Sect. 9 and we draw conclusions in Sect. 10. 2 Data Our primary data consists of 552 English positive and negative examples taken from an introductory GB-linguistics textbook by Lasnik and Uriagereka <ref> (Lasnik & Uriagereka 1988) </ref>. Most of these examples are organized into minimal pairs like the example I am eager for John to be here/*I am eager John to be here that we have seen earlier.
Reference: <author> Lawrence, S., Giles, C. L. & Fong, S. </author> <year> (1995), </year> <title> On the applicability of neural network and machine learning methodologies to natural language processing, </title> <institution> Technical Report UMIACS-TR-95-64 and CS-TR-3479, Institute for Advanced Computer Studies, University of Maryland, College Park MD 20742, </institution> <note> http://www.neci.nj.nec.com/homepages/lawrence/papers/nl-tr95/. </note>
Reference-contexts: Indeed, all models attain 50% or less correct classification on average for the Japanese data. More details can be found in Sect. 9 and <ref> (Lawrence, Giles & Fong 1995) </ref>. 3 Following classical GB theory, these classes are synthesized from the theta-grids of individual predicates via the Canonical Structural Realization (CSR) mechanism of Pesetsky (Pesetsky 1982). 4 For an output range of 0 to 1 and softmax outputs. 4 4 Nearest-Neighbors In the nearest-neighbors technique, the
Reference: <author> MacWhinney, B., Leinbach, J., Taraban, R. & McDonald, J. </author> <year> (1989), </year> <title> `Language learning: cues or rules?', Journal of Memory and Language 28, </title> <type> 255277. </type> <note> 10 Miikkulainen, </note> <author> R. & Dyer, M. </author> <year> (1989), </year> <title> Encoding input/output representations in connectionist cognitive systems, </title> <editor> in D. S. Touretzky, G. E. Hinton & T. J. Sejnowski, eds, </editor> <booktitle> `Proceedings of the 1988 Connectionist Models Summer School', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <pages> pp. 188195. </pages>
Reference-contexts: John & McClelland 1990). Neural network models have been shown to be able to account for a variety of phenomena in phonology (Gasser & Lee 1990, Hare 1990, Touretzky 1989a, Touretzky 1989b), morphology <ref> (Hare, Corina & Cottrell 1989, MacWhinney, Leinbach, Taraban & McDonald 1989) </ref> and role assignment (Miikkulainen & Dyer 1989, St. John & McClelland 1990). Induction of simpler grammars has been addressed often e.g. (Watrous & Kuhn 1992, Giles, Miller, Chen, Chen, Sun & Lee 1992) on learning Tomita languages.
Reference: <author> Pereira, F. & Schabes, Y. </author> <year> (1992), </year> <title> Inside-outside re-estimation from partially bracketed corpora, </title> <booktitle> in `Proceedings of the 30th annual meeting of the ACL', Newark, </booktitle> <pages> pp. 128135. </pages>
Reference-contexts: The most successful stochastic language models have been based on finite-state descriptions such as n-grams or hidden Markov models. However, finite-state models cannot represent hierarchical structures as found in natural language 1 <ref> (Pereira & Schabes 1992) </ref>. In the past few years several recurrent neural network (RNN) architectures have emerged which have been used for grammatical inference. <p> The algorithm is currently only practical for relatively small grammars <ref> (Pereira & Schabes 1992) </ref>. stacks. The grammars learned were not large. Our task differs from these in that the grammar is considerably more complex. Recently large regular grammars have been learned by RNNs (Giles, Horne & Lin 1995, Clouse, Giles, Horne & Cottrell 1994).
Reference: <author> Pesetsky, D. M. </author> <year> (1982), </year> <title> Paths and Categories, </title> <type> PhD thesis, </type> <institution> MIT. </institution>
Reference-contexts: More details can be found in Sect. 9 and (Lawrence, Giles & Fong 1995). 3 Following classical GB theory, these classes are synthesized from the theta-grids of individual predicates via the Canonical Structural Realization (CSR) mechanism of Pesetsky <ref> (Pesetsky 1982) </ref>. 4 For an output range of 0 to 1 and softmax outputs. 4 4 Nearest-Neighbors In the nearest-neighbors technique, the nearest-neighbors to a test sentence are found using a similarity measure. The class of the test sentence is inferred from the classes of the neighbors.
Reference: <author> Pollack, J. </author> <year> (1990), </year> <title> `Recursive distributed representations', </title> <journal> Artificial Intelligence 46, </journal> <volume> 77105. </volume>
Reference-contexts: John & McClelland 1990). Induction of simpler grammars has been addressed often e.g. (Watrous & Kuhn 1992, Giles, Miller, Chen, Chen, Sun & Lee 1992) on learning Tomita languages. There has been some interest in learning more than regular grammars with RNNs. Jordan and others <ref> (Pollack 1990, Berg 1992, Sperduti, Starita & Goller 1995) </ref> have used recursive auto-associative distributed memories (RAAMs) in RNNs.
Reference: <author> Pollack, J. </author> <year> (1991), </year> <title> `The induction of dynamical recognizers', </title> <booktitle> Machine Learning 7, </booktitle> <pages> 227252. </pages>
Reference-contexts: It has been shown that RNNs have the representational power required for hierarchical solutions (Elman 1991), and that they are Turing equivalent (Siegelmann & Sontag 1995). The RNNs investigated in this paper constitute complex, dynamical systems. Pollack <ref> (Pollack 1991) </ref> points out that Crutchfield and Young (Crutchfield & Young 1989) have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling. They have shown that these systems are not regular, but are finitely described by indexed context-free-grammars.
Reference: <author> Pollard, C. </author> <year> (1984), </year> <title> Generalised context-free grammars, head grammars and natural language, </title> <type> PhD thesis, </type> <institution> Department of Linguistics, Stanford University, </institution> <address> Palo Alto, CA. </address>
Reference: <author> Quinlan, R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: Although we expect careful selection of the cost table to improve performance, analysis of the operation of the method leads us to believe that it will never obtain very good performance 6 . 5 Decision Trees For comparison purposes, we tested the C4.5 decision tree induction algorithm by Ross Quinlan <ref> (Quinlan 1993) </ref>. Decision tree methods construct a tree which partitions the data at each level in the tree based on a particular feature of the data. C4.5 only deals with strings of constant length, hence we used an input window corresponding to the longest string.
Reference: <author> Siegelmann, H. & Sontag, E. </author> <year> (1995), </year> <title> `On the computational power of neural nets', </title> <journal> Journal of Computer and System Sciences 50(1), </journal> <volume> 132150. </volume>
Reference-contexts: However, these grammars have unusual properties when implemented as sequential machines in the sense that they have little logic. It has been shown that RNNs have the representational power required for hierarchical solutions (Elman 1991), and that they are Turing equivalent <ref> (Siegelmann & Sontag 1995) </ref>. The RNNs investigated in this paper constitute complex, dynamical systems. Pollack (Pollack 1991) points out that Crutchfield and Young (Crutchfield & Young 1989) have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling.
Reference: <author> Sperduti, A., Starita, A. & Goller, C. </author> <year> (1995), </year> <title> Learning distributed representations for the classification of terms, </title> <booktitle> in `Proceedings of the International Joint Conference on Artificial Intelligence', </booktitle> <pages> pp. 509515. </pages>
Reference: <author> St. John, M. F. & McClelland, J. </author> <year> (1990), </year> <title> `Learning and applying contextual constraints in sentence comprehension', </title> <journal> Artificial Intelligence 46, </journal> <volume> 546. </volume>
Reference-contexts: John & McClelland 1990). Induction of simpler grammars has been addressed often e.g. (Watrous & Kuhn 1992, Giles, Miller, Chen, Chen, Sun & Lee 1992) on learning Tomita languages. There has been some interest in learning more than regular grammars with RNNs. Jordan and others <ref> (Pollack 1990, Berg 1992, Sperduti, Starita & Goller 1995) </ref> have used recursive auto-associative distributed memories (RAAMs) in RNNs.
Reference: <author> Stolcke, A. </author> <year> (1990), </year> <title> Learning feature-based semantics with simple recurrent networks, </title> <type> Technical Report TR-90-015, </type> <institution> International Computer Science Institute, Berkeley, California. </institution>
Reference: <author> Sun, G., Giles, C. L., Chen, H. & Lee, Y. </author> <year> (1993), </year> <title> The neural network pushdown automaton: Model, stack and learning simulations, </title> <type> Technical Report UMIACS-TR-93-77, </type> <institution> Institute for Advanced Computer Studies, University of Maryland, College Park, MD. </institution>
Reference: <author> Touretzky, D. S. </author> <year> (1989a), </year> <title> Rules and maps in connectionist symbol processing, </title> <type> Technical Report CMU-CS-89-158, </type> <institution> Carnegie Mellon University: Department of Computer Science, </institution> <address> Pittsburgh, PA. </address>
Reference: <author> Touretzky, D. S. </author> <year> (1989b), </year> <title> Towards a connectionist phonology: The many maps approach to sequence manipulation, </title> <booktitle> in `Proceedings of the 11th Annual Conference of the Cognitive Science Society', </booktitle> <pages> pp. 188195. </pages>
Reference: <author> Watrous, R. & Kuhn, G. </author> <year> (1992), </year> <title> `Induction of finite-state languages using second-order recurrent networks', </title> <booktitle> Neural Computation 4(3), </booktitle> <pages> 406. </pages>
Reference-contexts: We believe that the basic assumptions of smoothness in the required function approximation and the nature of parameter updating provide opportunities for improvement (as demonstrated by the techniques described within). The grammar learnt by the recurrent networks could be extracted and examined <ref> (Giles et al. 1992, Watrous & Kuhn 1992) </ref>. Further progress can be made by continuing to address the convergence of the backpropagation algorithm. However, we envisage a point after which advances will depend on considering the connectionist treatment of hierarchical representations.
Reference: <author> Williams, R. & Zipser, D. </author> <year> (1989), </year> <title> `A learning algorithm for continually running fully recurrent neural networks', </title> <booktitle> Neural Computation 1(2), </booktitle> <pages> 270280. </pages>
Reference-contexts: Back-Tsoi FIR. A multi-layer perceptron network with an FIR filter and a gain term included in every synapse as described in (Back & Tsoi 1991). 4. Williams and Zipser. A fully connected recurrent network where all nodes are connected to all other nodes as described in <ref> (Williams & Zipser 1989) </ref>. 5. Elman (Elman 1990, Elman 1991). A recurrent network where each hidden layer node has a feedback connection to all other hidden layer nodes. The feedback connections are trainable and we use full backpropagation through time for training instead of the truncated version used by Elman. <p> Batch update attempts to follow the true gradient, whereas stochastic is similar to adding noise to the true gradient this 8 Backpropagation-through-time extends backpropagation to include temporal aspects and arbitrary connection topologies by considering an equivalent feedforward network created by unfolding the recurrent network in time. 9 Real-time recurrent learning <ref> (Williams & Zipser 1989) </ref> was also tested but did not show any significant convergence for our problem. 7 noise can help the algorithm avoid local minima. We found that stochastic update produced significantly better results. 4. Learning rate schedule.
Reference: <author> Williams, R. & Zipser, D. </author> <year> (1990), </year> <title> Gradient-based learning algorithms for recurrent connectionist networks, </title> <editor> in Y. Chau-vin & D. Rumelhart, eds, </editor> <title> `Backpropagation: Theory, Architectures, and Applications', </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: Percentage correct classification for the test data. TEST large small window window Edit-distance 55 N/A Euclidean 65 55 Decision trees 60 N/A MLP 63 54 BT-FIR 64 54 Elman 65 74 W&Z 59 71 8 Gradient Descent We have used backpropagation-through-time 8 <ref> (Williams & Zipser 1990) </ref> to train the globally-recurrent networks 9 and the gradient descent algorithm described by the authors for the FGS and Back-Tsoi FIR networks. The error surface of a multilayer network is generally non-convex, non-quadratic, and often has large dimensionality.
Reference: <author> Zeng, Z., Goodman, R. & Smyth, P. </author> <year> (1994), </year> <title> `Discrete recurrent neural networks for grammatical inference', </title> <journal> IEEE Transactions on Neural Networks 5(2), </journal> <volume> 320330. </volume> <pages> 11 </pages>
References-found: 42

