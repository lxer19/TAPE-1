URL: http://ini.cs.tu-berlin.de/~ao/pubs/cluster.ps.gz
Refering-URL: http://ini.cs.tu-berlin.de/~ao/pubs-e.html
Root-URL: http://ini.cs.tu-berlin.de/~ao/pubs-e.html
Title: Clustering in Weight Space of Feedforward Nets  
Author: Stefan M. Ruger and Arnfried Ossen 
Address: Sekr. FR 5-9  Franklinstr. 28/29, 10 587 Berlin, Germany  
Affiliation: Informatik,  Technische Universitat Berlin  
Abstract: We study symmetries of feedforward networks in terms of their corresponding groups and find that these groups naturally act on and partition weight space. We specify an algorithm to generate representative weight vectors in a specific fundamental domain. The analysis of the metric structure of the fundamental domain enables us to use the location information of weight vector estimates, e. g. for cluster analysis. This can be implemented efficiently even for large networks. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> An Mei Chen, Haw-minn Lu, and Robert Hecht-Nielsen. </author> <title> On the geometry of feed-forward neural network error surfaces. </title> <journal> Neural Computation, </journal> <volume> 5(6) </volume> <pages> 910-927, </pages> <year> 1993. </year>
Reference-contexts: There have been several attempts to analyze the symmetries of the network function <ref> [5, 1] </ref>. We summarize an independent approach [4], in which the arising symmetries are described and analyzed in terms of their groups. <p> So far the symmetry group S of the weight space has been identified as a certain subgroup of GL (E; R). As pointed out by <ref> [1] </ref> no analytic function other than an element of S can represent a symmetry in this context. However, there exist a lot of discontinuous functions that give rise to a symmetry: Fix two hidden nodes a; b from the same hidden layer.
Reference: 2. <author> Richard O. Duda and Peter E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: A data set with 32 points was generated by a true model (dotted line in Fig. 3). Several weight vectors were obtained by different runs of a standard algorithm to maximize the likelihood of the data. We calculated a respective canonical distance matrix for a hierarchical clustering algorithm <ref> [2] </ref>. In contrast to the raw projection of weight vectors (Fig. 1a), we obtained several clusters (Fig. 1b) indicating the respective local minima of the error surface. W14 W14 a) b) Fig. 1.
Reference: 3. <author> Bradley Efron and Robert J. Tibshirani. </author> <title> An Introduction to the Bootstrap. </title> <publisher> Chap-man & Hall, </publisher> <year> 1993. </year>
Reference-contexts: Under quite general assumptions, the weight vector ^w n that maximizes the likelihood is asymptotically (w. r. t. n) unbiased, consistent, asymptotically efficient and asymptotically normal-distributed [6]. The bootstrap method <ref> [3] </ref> is based on re-estimations of the parameter vector on B bootstrap samples of the training set. The bth bootstrap sample is a random multiset D flb = f (x flb 1 ; y flb n ; y flb n )g drawn from the training data with replacement.
Reference: 4. <author> Arnfried Ossen and Stefan M. Ruger. </author> <title> Weight space analysis and forecast uncertainty. </title> <note> Submitted to Journal of Forecasting, </note> <year> 1996. </year>
Reference-contexts: There have been several attempts to analyze the symmetries of the network function [5, 1]. We summarize an independent approach <ref> [4] </ref>, in which the arising symmetries are described and analyzed in terms of their groups. A permutation of the nodes in a hidden layer L i induces a certain linear operation i on the weight space R E , which leaves the network function invariant. <p> The approximate 1 2ff confidence interval of a predicted value out ^w (x) is [out ff ^w fl (x)]: 3.2 An Application For the sake of clarity, we demonstrate our techniques using a 1-3-1 network; a more advanced application may be found in <ref> [4] </ref>. A data set with 32 points was generated by a true model (dotted line in Fig. 3). Several weight vectors were obtained by different runs of a standard algorithm to maximize the likelihood of the data. We calculated a respective canonical distance matrix for a hierarchical clustering algorithm [2].
Reference: 5. <author> Hector J. Sussmann. </author> <title> Uniqueness of the weights for minimal feedforward nets with a given input-output map. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 589-593, </pages> <year> 1992. </year>
Reference-contexts: There have been several attempts to analyze the symmetries of the network function <ref> [5, 1] </ref>. We summarize an independent approach [4], in which the arising symmetries are described and analyzed in terms of their groups.
Reference: 6. <author> Halbert White. </author> <title> Artificial Neural Networks | Approximation & Learning Theory. </title> <publisher> Blackwell, Oxford, </publisher> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: This class of networks is quite universal: choosing the activation functions of the output layer as identity and using one hidden layer makes this type of network an universal approximator <ref> [6] </ref>. 2 Metric Structure of Weight Space Owing to Symmetries 2.1 Symmetries A symmetry of weight space R E is a transformation t: R E ! R E that leaves the network function invariant, i. e., out w = out t (w) . <p> The function w 7! L D (w), a. k. a likelihood, describes how likely the data have been generated by w. Under quite general assumptions, the weight vector ^w n that maximizes the likelihood is asymptotically (w. r. t. n) unbiased, consistent, asymptotically efficient and asymptotically normal-distributed <ref> [6] </ref>. The bootstrap method [3] is based on re-estimations of the parameter vector on B bootstrap samples of the training set.
References-found: 6

