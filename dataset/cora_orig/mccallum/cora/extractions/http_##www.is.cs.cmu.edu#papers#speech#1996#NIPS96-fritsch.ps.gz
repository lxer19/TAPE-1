URL: http://www.is.cs.cmu.edu/papers/speech/1996/NIPS96-fritsch.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Email: ffritsch+,finkem,waibelg@cs.cmu.edu  
Title: Adaptively Growing Hierarchical Mixtures of Experts  
Author: Jurgen Fritsch, Michael Finke, Alex Waibel 
Address: Pittsburgh, PA 15213  
Affiliation: Interactive Systems Laboratories Carnegie Mellon University  
Abstract: We propose a novel approach to automatically growing and pruning Hierarchical Mixtures of Experts. The constructive algorithm proposed here enables large hierarchies consisting of several hundred experts to be trained effectively. We show that HME's trained by our automatic growing procedure yield better generalization performance than traditional static and balanced hierarchies. Evaluation of the algorithm is performed (1) on vowel classification and (2) within a hybrid version of the JANUS [9] speech recognition system using a subset of the Switchboard large-vocabulary speaker-independent continuous speech recognition database.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dempster, A.P., Laird, N.M. & Rubin, </author> <title> D.B. (1977) Maximum likelihood from incomplete data via the EM algorithm. J.R. </title> <journal> Statist. Soc. </journal> <volume> B 39 , 1-38. </volume>
Reference-contexts: The overall output of the hierarchy is P (yjx; fi) = i=1 N X g jji (x; v ij )P (yjx; ij ) where the g i and g jji are the outputs of the gating networks. The HME is trained using the EM algorithm <ref> [1] </ref> (see [3] for the application of EM to the HME architecture).
Reference: [2] <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. </author> <title> (1991) Adaptive mixtures of local experts. </title> <booktitle> In Neural Computation 3, </booktitle> <pages> pp. 79-87, </pages> <publisher> MIT press. </publisher>
Reference: [3] <author> Jordan, M.I. & Jacobs R.A. </author> <title> (1994) Hierarchical Mixtures of Experts and the EM Algorithm. </title> <booktitle> In Neural Computation 6, </booktitle> <pages> pp. 181-214. </pages> <publisher> MIT press. </publisher>
Reference-contexts: Training of the hierarchy is based on a generative model using the Expectation Maximisation (EM) [1,3] algorithm as a powerful and efficient tool for estimating the network parameters. In <ref> [3] </ref>, the architecture of the HME is considered pre-determined and remains fixed during training. This requires choice of structural parameters such as tree depth and branching factor in advance. <p> The overall output of the hierarchy is P (yjx; fi) = i=1 N X g jji (x; v ij )P (yjx; ij ) where the g i and g jji are the outputs of the gating networks. The HME is trained using the EM algorithm [1] (see <ref> [3] </ref> for the application of EM to the HME architecture). <p> The above maximum likelihood equations might be solved by gradient ascent, weighted least squares or Newton methods. In our implementation, we use a variant of Jordan & Jacobs' <ref> [3] </ref> least squares approach. GROWING MIXTURES In order to grow an HME, we have to define an evaluation criterion to score the experts performance on the training data.
Reference: [4] <author> Jordan, M.I. & Jacobs, R.A. </author> <title> (1992) Hierarchies of adaptive experts. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <editor> J. Moody, S. Hanson, and R. Lippmann, </editor> <booktitle> eds., </booktitle> <pages> pp. 985-993. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: [5] <author> McCullagh, P. & Nelder, J.A. </author> <title> (1983) Generalized Linear Models. </title> <publisher> Chapman and Hall, London. </publisher>
Reference-contexts: All networks in the tree are linear, with a softmax non-linearity as their activation function. Such networks are known in statistics as multinomial logit models, a special case of Generalized Linear Models (GLIM) <ref> [5] </ref> in which the probabilistic component is the multinomial density. This allows for a probabilistic interpretation of the hierarchy in terms of a generative likelihood-based model.
Reference: [6] <author> Peterson, G. E. & Barney, H. L. </author> <title> (1952) Control measurements used in a study of the vowels. </title> <journal> Journal of the Acoustical Society of America 24, </journal> <pages> 175-184. </pages>
Reference-contexts: VOWEL CLASSIFICATION In initial experiments, we investigated the usefulness of the proposed tree growing algorithm on Peterson and Barney's <ref> [6] </ref> vowel classification data that uses formant frequencies as features. We chose this data set since it is small, non-artificial and low-dimensional, which allows for visualization and understanding of the way the growing HME tree performs classification tasks.
Reference: [7] <institution> Proceedings of LVCSR Hub 5 workshop, </institution> <month> Apr. </month> <note> 29 May 1 (1996) MITAGS, </note> <institution> Linthicum Heights, Maryland. </institution>
Reference-contexts: We evaluate the system on the Switchboard spontaneous telephone speech corpus. Our best current mixture of Gaussians based context-dependent HMM system achieves a word accuracy of 61.4% on this task, which is among the best current systems <ref> [7] </ref>. We started by using phonetic context-independent (CI) HME's for 3-state HMM's. We restricted the training set to all dialogues involving speakers from one dialect region (New York City), since the whole training set contains over 140 hours of speech.
Reference: [8] <author> Syrdal, A. K. & Gopal, H. S. </author> <title> (1986) A perceptual model of vowel recognition based on the auditory representation of American English vowels. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 79 </volume> (4):1086-1100. 
Reference: [9] <author> Zeppenfeld T., Finke M., Ries K., Westphal M. & Waibel A. </author> <title> (1997) Recognition of Conversational Telephone Speech using the Janus Speech Engine. </title> <booktitle> Proceedings of ICASSP 97, </booktitle> <address> Muenchen, Germany </address>
Reference-contexts: There are strong dependencies visible between the class boundaries and some of the experts activation regions. EXPERIMENTS ON SWITCHBOARD We recently started experiments using standard and growing HME's as estimators of posterior phone probabilities in a hybrid version of the JANUS <ref> [9] </ref> speech recognizer. Following the work in [12], we use different HME's for each state of a phonetic HMM. The posteriors for 52 phonemes computed by the HME's are converted into scaled likelihoods by dividing by prior probabilities to account for the likelihood based training and decoding of HMM's.
Reference: [10] <author> Waterhouse, S.R., Robinson, A.J. </author> <title> (1994) Classification using Hierarchical Mixtures of Experts. </title> <booktitle> In Proc. 1994 IEEE Workshop on Neural Networks for Signal Processing IV, </booktitle> <pages> pp. 177-186. </pages>
Reference: [11] <author> Waterhouse, S.R., Robinson, A.J. </author> <title> (1995) Constructive Algorithms for Hierarchical Mixtures of Experts. </title> <booktitle> In Advances in Neural Information Processing Systems 8. </booktitle>
Reference-contexts: As with other classification and regression techniques, it may be advantageous to have some sort of data-driven model-selection mechanism to (1) overcome false initialisations (2) speed-up training time and (3) adapt model size to task complexity for optimal generalization performance. In <ref> [11] </ref>, a constructive algorithm for the HME is presented and evaluated on two small classification tasks: the two spirals and the 8-bit parity problems. <p> Continue with step (2) until desired tree size is reached. The number of tree growing phases may either be pre-determined, or based on difference in the likelihoods before and after splitting a node. In contrast to the growing algorithm in <ref> [11] </ref>, our algorithm does not hypothesize all possible node splits, but determines the expansion node (s) directly, which is much faster, especially when dealing with large hierarchies. Furthermore, we implemented a path pruning technique similar to the one proposed in [11], which speeds up training and testing times significantly. <p> In contrast to the growing algorithm in <ref> [11] </ref>, our algorithm does not hypothesize all possible node splits, but determines the expansion node (s) directly, which is much faster, especially when dealing with large hierarchies. Furthermore, we implemented a path pruning technique similar to the one proposed in [11], which speeds up training and testing times significantly. During the recursive depth-first traversal of the tree (needed for forward evaluation, posterior probability computation and accumulation of node statistics) a path is pruned temporarily if the current node's probability of activation falls below a certain threshold.
Reference: [12] <author> Zhao, Y., Schwartz, R., Sroka, J. & Makhoul, J. </author> <title> (1995) Hierarchical Mixtures of Experts Methodology Applied to Continuous Speech Recognition. </title> <booktitle> In ICASSP 1995, </booktitle> <volume> volume 5, </volume> <pages> pp. 3443-6, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: There are strong dependencies visible between the class boundaries and some of the experts activation regions. EXPERIMENTS ON SWITCHBOARD We recently started experiments using standard and growing HME's as estimators of posterior phone probabilities in a hybrid version of the JANUS [9] speech recognizer. Following the work in <ref> [12] </ref>, we use different HME's for each state of a phonetic HMM. The posteriors for 52 phonemes computed by the HME's are converted into scaled likelihoods by dividing by prior probabilities to account for the likelihood based training and decoding of HMM's.
References-found: 12

