URL: http://www.bell-labs.com/user/seung/papers/minimax.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00428.html
Root-URL: 
Email: fseung|tjrg@bell-labs.com  jcl@research.att.com  jhopfield@watson.princeton.edu  
Title: Minimax and Hamiltonian Dynamics of Excitatory-Inhibitory Networks  
Author: H. S. Seung, T. J. Richardson J. C. Lagarias J. J. Hopfield 
Address: Murray Hill, NJ 07974  180 Park Ave. D-130 Florham Park, NJ 07932  Princeton, NJ 08544  
Affiliation: Bell Labs, Lucent Technologies  AT&T Labs-Research  Dept. of Molecular Biology Princeton University  
Abstract: A Lyapunov function for excitatory-inhibitory networks is constructed. The construction assumes symmetric interactions within excitatory and inhibitory populations of neurons, and antisymmetric interactions between populations. The Lyapunov function yields sufficient conditions for the global asymptotic stability of fixed points. If these conditions are violated, limit cycles may be stable. The relations of the Lyapunov function to optimization theory and classical mechanics are revealed by The dynamics of a neural network with symmetric interactions provably converges to fixed points under very general assumptions[1, 2]. This mathematical result helped to establish the paradigm of neural computation with fixed point attractors[3]. But in reality, interactions between neurons in the brain are asymmetric. Furthermore, the dynamical behaviors seen in the brain are not confined to fixed point attractors, but also include oscillations and complex nonperiodic behavior. These other types of dynamics can be realized by asymmetric networks, and may be useful for neural computation. For these reasons, it is important to understand the global behavior of asymmetric neural networks. The interaction between an excitatory neuron and an inhibitory neuron is clearly asymmetric. Here we consider a class of networks that incorporates this fundamental asymmetry of the brain's microcircuitry. Networks of this class have distinct populations of excitatory and inhibitory neurons, with antisymmetric interactions minimax and dissipative Hamiltonian forms of the network dynamics.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. A. Cohen and S. Grossberg. </author> <title> Absolute stability of global pattern formation and parallel memory storage by competitive neural networks. </title> <journal> IEEE, </journal> <volume> 13 </volume> <pages> 815-826, </pages> <year> 1983. </year>
Reference-contexts: The relations of the Lyapunov function to optimization theory and classical mechanics are revealed by minimax and dissipative Hamiltonian forms of the network dynamics. The dynamics of a neural network with symmetric interactions provably converges to fixed points under very general assumptions <ref> [1, 2] </ref>. This mathematical result helped to establish the paradigm of neural computation with fixed point attractors [3]. But in reality, interactions between neurons in the brain are asymmetric. <p> The connection between neural networks and optimization [3] was established by proofs that symmetric networks could find minima of objective functions <ref> [1, 2] </ref>. Later it was discovered that excitatory-inhibitory networks could perform the minimax computation of finding saddle points [9, 10, 11], though no general proof of this was given at the time. <p> The time constants t x and t y set the speed of excitatory and inhibitory synapses, respectively. In the limit of infinitely fast inhibition, t y = 0, the convergence theorems for symmetric networks are applicable <ref> [1, 2] </ref>, though some effort is required in applying them to the case C 6= 0. If the dynamics converges for t y = 0, then there exists some neighborhood of zero in which it still converges [7].
Reference: [2] <author> J. J. </author> <title> Hopfield. Neurons with graded response have collective computational properties like those of two-state neurons. </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. USA, </institution> <month> 81 </month> <pages> 3088-3092, </pages> <year> 1984. </year>
Reference-contexts: The relations of the Lyapunov function to optimization theory and classical mechanics are revealed by minimax and dissipative Hamiltonian forms of the network dynamics. The dynamics of a neural network with symmetric interactions provably converges to fixed points under very general assumptions <ref> [1, 2] </ref>. This mathematical result helped to establish the paradigm of neural computation with fixed point attractors [3]. But in reality, interactions between neurons in the brain are asymmetric. <p> The connection between neural networks and optimization [3] was established by proofs that symmetric networks could find minima of objective functions <ref> [1, 2] </ref>. Later it was discovered that excitatory-inhibitory networks could perform the minimax computation of finding saddle points [9, 10, 11], though no general proof of this was given at the time. <p> The time constants t x and t y set the speed of excitatory and inhibitory synapses, respectively. In the limit of infinitely fast inhibition, t y = 0, the convergence theorems for symmetric networks are applicable <ref> [1, 2] </ref>, though some effort is required in applying them to the case C 6= 0. If the dynamics converges for t y = 0, then there exists some neighborhood of zero in which it still converges [7].
Reference: [3] <author> J. J. Hopfield and D. W. Tank. </author> <title> Computing with neural circuits: a model. </title> <journal> Science, </journal> <volume> 233 </volume> <pages> 625-633, </pages> <year> 1986. </year>
Reference-contexts: The dynamics of a neural network with symmetric interactions provably converges to fixed points under very general assumptions [1, 2]. This mathematical result helped to establish the paradigm of neural computation with fixed point attractors <ref> [3] </ref>. But in reality, interactions between neurons in the brain are asymmetric. Furthermore, the dynamical behaviors seen in the brain are not confined to fixed point attractors, but also include oscillations and complex nonperiodic behavior. <p> But the rest of the paper explains what makes the Lyapunov function especially interesting, beyond the convergence results it yields: its role in a conceptual framework that relates excitatory-inhibitory networks to optimization theory and classical mechanics. The connection between neural networks and optimization <ref> [3] </ref> was established by proofs that symmetric networks could find minima of objective functions [1, 2]. Later it was discovered that excitatory-inhibitory networks could perform the minimax computation of finding saddle points [9, 10, 11], though no general proof of this was given at the time.
Reference: [4] <author> H. R. Wilson and J. D. Cowan. </author> <title> A mathematical theory of the functional dynamics of cortical and thalamic nervous tissue. </title> <journal> Kybernetik, </journal> <volume> 13 </volume> <pages> 55-80, </pages> <year> 1973. </year>
Reference-contexts: Networks of this class have distinct populations of excitatory and inhibitory neurons, with antisymmetric interactions between populations and symmetric interactions within each population. Such net-works display a rich repertoire of dynamical behaviors including fixed points, limit cycles <ref> [4, 5] </ref> and traveling waves [6]. After defining the class of excitatory-inhibitory networks, we introduce a Lyapunov function that establishes sufficient conditions for the global asymptotic stability of fixed points. <p> Our Lyapunov function goes further, as it is valid for more general t y . The potential for oscillatory behavior in excitatory-inhibitory networks like (1) has long been known <ref> [4, 7] </ref>. The origin of oscillations can be understood from a simple two neuron model. Suppose that neuron 1 excites neuron 2, and receives inhibition back from neuron 2.
Reference: [5] <author> Z. Li and J. J. </author> <title> Hopfield. Modeling the olfactory bulb and its neural oscillatory pro-cessings. </title> <journal> Biol. Cybern., </journal> <volume> 61 </volume> <pages> 379-392, </pages> <year> 1989. </year>
Reference-contexts: Networks of this class have distinct populations of excitatory and inhibitory neurons, with antisymmetric interactions between populations and symmetric interactions within each population. Such net-works display a rich repertoire of dynamical behaviors including fixed points, limit cycles <ref> [4, 5] </ref> and traveling waves [6]. After defining the class of excitatory-inhibitory networks, we introduce a Lyapunov function that establishes sufficient conditions for the global asymptotic stability of fixed points. <p> After defining the class of excitatory-inhibitory networks, we introduce a Lyapunov function that establishes sufficient conditions for the global asymptotic stability of fixed points. The generality of these conditions contrasts with the restricted nature of previous convergence results, which applied only to linear networks <ref> [5] </ref>, or to nonlinear networks with infinitely fast inhibition [7]. The use of the Lyapunov function is illustrated with a competitive or winner-take-all network, which consists of an excitatory population of neurons with recurrent inhibition from a single neuron [8]. <p> Depending on the choice of f and g, self-feedback ff, and time scales t x and t y , this network exhibits a variety of dynamical behaviors, including a single point attractor, multiple point attractors, and limit cycles <ref> [5, 7] </ref>. We will consider the specific case where f and g are the rectification nonlinearity [x] + = maxfx; 0g. The behavior of this network will be described in detail elsewhere; only a brief summary is given here.
Reference: [6] <author> S. Amari. </author> <title> Dynamics of pattern formation in lateral-inhibition type neural fields. </title> <journal> Biol. Cybern., </journal> <volume> 27 </volume> <pages> 77-87, </pages> <year> 1977. </year>
Reference-contexts: Networks of this class have distinct populations of excitatory and inhibitory neurons, with antisymmetric interactions between populations and symmetric interactions within each population. Such net-works display a rich repertoire of dynamical behaviors including fixed points, limit cycles [4, 5] and traveling waves <ref> [6] </ref>. After defining the class of excitatory-inhibitory networks, we introduce a Lyapunov function that establishes sufficient conditions for the global asymptotic stability of fixed points.
Reference: [7] <author> B. Ermentrout. </author> <title> Complex dynamics in winner-take-all neural nets with slow inhibition. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 415-431, </pages> <year> 1992. </year>
Reference-contexts: The generality of these conditions contrasts with the restricted nature of previous convergence results, which applied only to linear networks [5], or to nonlinear networks with infinitely fast inhibition <ref> [7] </ref>. The use of the Lyapunov function is illustrated with a competitive or winner-take-all network, which consists of an excitatory population of neurons with recurrent inhibition from a single neuron [8]. For this network, the sufficient conditions for global stability of fixed points also happen to be necessary conditions. <p> In other words, we have proved global stability over the largest possible parameter regime in which it holds, demonstrating the power of the Lyapunov function. There exists another parameter regime in which numerical simulations display limit cycle oscillations <ref> [7] </ref>. Similar convergence proofs for other excitatory-inhibitory networks may be obtained by tedious but straightforward calculations. All the necessary tools are given in the first half of the paper. <p> If the dynamics converges for t y = 0, then there exists some neighborhood of zero in which it still converges <ref> [7] </ref>. Our Lyapunov function goes further, as it is valid for more general t y . The potential for oscillatory behavior in excitatory-inhibitory networks like (1) has long been known [4, 7]. The origin of oscillations can be understood from a simple two neuron model. <p> Our Lyapunov function goes further, as it is valid for more general t y . The potential for oscillatory behavior in excitatory-inhibitory networks like (1) has long been known <ref> [4, 7] </ref>. The origin of oscillations can be understood from a simple two neuron model. Suppose that neuron 1 excites neuron 2, and receives inhibition back from neuron 2. <p> If f and g were linear, the left hand sides of these inequalities would be equal to the maximum eigenvalue of A and the minimum eigenvalue of C. 3 AN EXAMPLE: COMPETITIVE NETWORK The competitive or winner-take-all network is a classic example of an excitatory inhibitory network <ref> [8, 7] </ref>. <p> Depending on the choice of f and g, self-feedback ff, and time scales t x and t y , this network exhibits a variety of dynamical behaviors, including a single point attractor, multiple point attractors, and limit cycles <ref> [5, 7] </ref>. We will consider the specific case where f and g are the rectification nonlinearity [x] + = maxfx; 0g. The behavior of this network will be described in detail elsewhere; only a brief summary is given here.
Reference: [8] <author> S. Amari and M. A. Arbib. </author> <title> Competition and cooperation in neural nets. </title> <editor> In J. Metzler, editor, </editor> <booktitle> Systems Neuroscience, </booktitle> <pages> pages 119-165. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: The use of the Lyapunov function is illustrated with a competitive or winner-take-all network, which consists of an excitatory population of neurons with recurrent inhibition from a single neuron <ref> [8] </ref>. For this network, the sufficient conditions for global stability of fixed points also happen to be necessary conditions. In other words, we have proved global stability over the largest possible parameter regime in which it holds, demonstrating the power of the Lyapunov function. <p> If f and g were linear, the left hand sides of these inequalities would be equal to the maximum eigenvalue of A and the minimum eigenvalue of C. 3 AN EXAMPLE: COMPETITIVE NETWORK The competitive or winner-take-all network is a classic example of an excitatory inhibitory network <ref> [8, 7] </ref>.
Reference: [9] <author> E. Mjolsness and C. Garrett. </author> <title> Algebraic transformations of objective functions. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 651-669, </pages> <year> 1990. </year>
Reference-contexts: The connection between neural networks and optimization [3] was established by proofs that symmetric networks could find minima of objective functions [1, 2]. Later it was discovered that excitatory-inhibitory networks could perform the minimax computation of finding saddle points <ref> [9, 10, 11] </ref>, though no general proof of this was given at the time. Our Lyapunov function finally supplies such a proof, and one of its components is the objective function of the network's minimax computation. <p> This section of the paper explains the close relationship between gradient descent-ascent and excitatory-inhibitory networks <ref> [9, 10] </ref>. Furthermore, it reviews existing results on the convergence of gradient descent-ascent to saddle points [13, 10], which are the precedents of the convergence proofs of this paper.
Reference: [10] <author> J. C. Platt and A. H. Barr. </author> <title> Constrained differential optimization. </title> <editor> In D. Z. Anderson, editor, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <pages> page 55, </pages> <address> New York, </address> <year> 1987. </year> <journal> American Institute of Physics. </journal>
Reference-contexts: The connection between neural networks and optimization [3] was established by proofs that symmetric networks could find minima of objective functions [1, 2]. Later it was discovered that excitatory-inhibitory networks could perform the minimax computation of finding saddle points <ref> [9, 10, 11] </ref>, though no general proof of this was given at the time. Our Lyapunov function finally supplies such a proof, and one of its components is the objective function of the network's minimax computation. <p> This section of the paper explains the close relationship between gradient descent-ascent and excitatory-inhibitory networks <ref> [9, 10] </ref>. Furthermore, it reviews existing results on the convergence of gradient descent-ascent to saddle points [13, 10], which are the precedents of the convergence proofs of this paper. <p> This section of the paper explains the close relationship between gradient descent-ascent and excitatory-inhibitory networks [9, 10]. Furthermore, it reviews existing results on the convergence of gradient descent-ascent to saddle points <ref> [13, 10] </ref>, which are the precedents of the convergence proofs of this paper. <p> If the sign relation ~ is replaced by equality in (13), we obtain a true gradient descent-ascent dynamics, t x _x = @x @S : (15) Sufficient conditions for convergence of gradient descent-ascent to saddle points are known <ref> [13, 10] </ref>. <p> In this case, the existence of a unique saddle point is guaranteed, as S is convex in x for all y, and concave in y for all x <ref> [13, 10] </ref>. If there is more than one saddle point, the kinetic energy by itself is generally not a Lyapunov function.
Reference: [11] <author> I. M. Elfadel. </author> <title> Convex potentials and their conjugates in analog mean-field optimization. </title> <journal> Neural Computation, </journal> <volume> 7(5) </volume> <pages> 1079-1104, </pages> <year> 1995. </year>
Reference-contexts: The connection between neural networks and optimization [3] was established by proofs that symmetric networks could find minima of objective functions [1, 2]. Later it was discovered that excitatory-inhibitory networks could perform the minimax computation of finding saddle points <ref> [9, 10, 11] </ref>, though no general proof of this was given at the time. Our Lyapunov function finally supplies such a proof, and one of its components is the objective function of the network's minimax computation.
Reference: [12] <author> J. D. Cowan. </author> <title> A statistical mechanics of nervous activity. In Some mathematical questions in biology, volume III. </title> <publisher> AMS, </publisher> <year> 1972. </year>
Reference-contexts: If the extra terms are not purely dissipative, limit cycles are possible. Previous Hamiltonian formalisms for neural networks made the more restrictive assumption of purely antisymmetric interactions, and did not include the effect of dissipation <ref> [12] </ref>. This paper establishes sufficient conditions for global asymptotic stability of fixed points. The problem of finding sufficient conditions for oscillatory and chaotic behavior remains open.
Reference: [13] <author> K. J. Arrow, L. Hurwicz, and H. Uzawa. </author> <title> Studies in linear and non-linear programming. </title> <institution> Stanford University, Stanford, </institution> <year> 1958. </year>
Reference-contexts: This section of the paper explains the close relationship between gradient descent-ascent and excitatory-inhibitory networks [9, 10]. Furthermore, it reviews existing results on the convergence of gradient descent-ascent to saddle points <ref> [13, 10] </ref>, which are the precedents of the convergence proofs of this paper. <p> If the sign relation ~ is replaced by equality in (13), we obtain a true gradient descent-ascent dynamics, t x _x = @x @S : (15) Sufficient conditions for convergence of gradient descent-ascent to saddle points are known <ref> [13, 10] </ref>. <p> In this case, the existence of a unique saddle point is guaranteed, as S is convex in x for all y, and concave in y for all x <ref> [13, 10] </ref>. If there is more than one saddle point, the kinetic energy by itself is generally not a Lyapunov function.
References-found: 13

