URL: http://www.demo.cs.brandeis.edu/papers/raam.ps.gz
Refering-URL: http://www.demo.cs.brandeis.edu/papers/long.html
Root-URL: http://www.cs.brandeis.edu
Email: pollack@cis.ohio-state.edu  
Phone: (614) 292-4890  
Title: Recursive Distributed Representations  
Author: Jordan B. Pollack 
Address: 2036 Neil Avenue Columbus, OH 43210  
Affiliation: Laboratory for AI Research Computer Information Science Department The Ohio State University  
Abstract: A long-standing difficulty for connectionist modeling has been how to represent variable-sized recursive data structures, such as trees and lists, in fixed-width patterns. This paper presents a connectionist architecture which automatically develops compact distributed representations for such compositional structures, as well as efficient accessing mechanisms for them. Patterns which stand for the internal nodes of fixed-valence trees are devised through the recursive use of back-propagation on three-layer auto-associative encoder networks. The resulting representations are novel, in that they combine apparently immiscible aspects of features, pointers, and symbol structures. They form a bridge between the data structures necessary for high-level cognitive tasks and the associative, pattern recognition machinery provided by neural networks. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> G. W. Cottrell, </author> <title> Connectionist Parsing, </title> <booktitle> Proceedings of the Seventh Annual Conference of the Cognitive Science Society, </booktitle> <address> Irvine, CA, </address> <year> 1985. </year>
Reference: 2. <author> M. Fanty, </author> <title> Context-free parsing in Connectionist Networks, </title> <institution> TR174, University of Rochester, Computer Science Department, Rochester, </institution> <address> N.Y., </address> <year> 1985. </year>
Reference: 3. <author> B. Selman, </author> <title> Rule-Based Processing in a Connectionist System for Natural Language Understanding, </title> <institution> CSRI-168, University of Toronto, Computer Systems Research Institute, Toronto, Canada, </institution> <year> 1985. </year>
Reference: 4. <author> S. J. Hanson and J. Kegl, PARSNIP: </author> <title> A connectionist network that learns natural language grammar from exposure to natural language sentences, </title> <booktitle> Proceedings of the Ninth Conference of the Cognitive Science Society, </booktitle> <address> Seattle, </address> <year> 1987, </year> <pages> 106-119. </pages>
Reference-contexts: Furthermore, local representational schemes do not efficiently represent sequential information. The standard approach involves converting time into space by duplicating sub-networks into a fixed set of buffers for sequential input. Both early connectionist work, such as McClelland & Rumelhart's word recognition model [13], as well as more modern efforts <ref> [4, 14] </ref> use this approach, which is not able to represent or process sequences longer than a predetermined bound. One way to overcome this length limitation is by "sliding" the input across the buffer [15, 16]. <p> Along the way, representations will also be devised for all subtrees, in this case, (A B) and (C D). Note that, as will be hhhhhhhhhhhhhhh 5 Rumelhart et al. [12] demonstrated only a 8-3-8 network, but other successful uses include a 64-16-64 network [28] and a 270-45-270 network <ref> [4] </ref>. The three numbers correspond to the number of units in the input, hidden, and output layers of a network. Recursive Distributed Representations 9 demonstrated later, this strategy works on a collection of trees just as it does on a single tree.
Reference: 5. <author> J. McClelland and A. Kawamoto, </author> <title> Mechanisms of Sentence Processing: Assigning Roles to Constituents, </title> <booktitle> in Parallel Distributed Processing: Experiments in the Microstructure of Cognition, </booktitle> <volume> vol. 2, </volume> <editor> J. L. McClelland, D. E. Rumelhart and the PDP research Group (ed.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: Both local and distributed representations have, thus far, been unsuitable for capturing the dynamically-allocated variable-sized symbolic data-structures traditionally used in AI. The limitation shows in the fact that pure connectionism has generated somewhat unsatisfying systems in this domain; for example, parsers for fixed length sentences [1-4], without embedded structures <ref> [5] </ref>. 1 Indeed, some of the recent attacks on connectionism have been aimed precisely at the question of representational adequacy. <p> To solve the problem of feature superposition, one might use full-size constituent buffers, such as Agent, Action, and Object <ref> [5] </ref>. In each buffer would reside a feature pattern filling these roles such as NURSE, RIDING, and ELEPHANT.
Reference: 6. <author> J. B. Pollack and D. L. Waltz, </author> <title> Natural Language Processing Using Spreading Activation and Lateral Inhibition, </title> <booktitle> Proceedings of the Fourth Annual Cognitive Science Conference, </booktitle> <address> Ann Arbor, MI, </address> <year> 1982, </year> <pages> 50-53. </pages>
Reference: 7. <author> D. L. Waltz and J. B. Pollack, </author> <title> Massively Parallel Parsing: A strongly interactive model of Natural Language Interpretation, </title> <booktitle> Cognitive Science 9, 1 (1985), </booktitle> <pages> 51-74. </pages>
Reference: 8. <author> W. G. Lehnert, </author> <title> Case-based problem-solving with a large knowledge base of learned cases, </title> <booktitle> Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, </address> <year> 1987, </year> <pages> 301-306. </pages>
Reference: 9. <author> G. Berg, </author> <title> A Parallel Natural Language Processing Architecture with Distributed Control, </title> <booktitle> Proceedings of the Ninth Annual Conference of the Cognitive Science Society, </booktitle> <address> Seattle, </address> <year> 1987, </year> <pages> 487-495. </pages>
Reference: 10. <author> M. Minsky and S. Papert, </author> <title> Perceptrons, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year> <title> Recursive Distributed Representations 31 </title>
Reference-contexts: According to Minsky & Papert <ref> [10] </ref>, for example, work on neural network and other learning machines was stopped by the need for AI to focus on knowledge representation in the 1970's, because of the principle that "no machine can learn to recognize X unless it possesses, at least potentially, some scheme for representing X (p. xiii)."
Reference: 11. <author> J. Fodor and A. Pylyshyn, </author> <title> Connectionism and Cognitive Architecture: A Critical Analysis, </title> <journal> Cognition 28, </journal> <year> (1988), </year> <pages> 3-71. </pages>
Reference-contexts: on neural network and other learning machines was stopped by the need for AI to focus on knowledge representation in the 1970's, because of the principle that "no machine can learn to recognize X unless it possesses, at least potentially, some scheme for representing X (p. xiii)." Fodor and Pylyshyn's <ref> [11] </ref> arguments against connectionism are based on their belief that connectionist machines do not even have the potential for representing X, where X is combinatorial (syntactic) constituent structure, and hence cannot exhibit (semantic) "systematicity" of thought processes. <p> N))) ((D N) (V ((D N) (P (D (A N)))))) ((D N) (V ((D (A N)) (P (D (A N)))))) (((D N) (P (D N))) (V ((D N) (P (D (A N)))))) (((D N) (P (D N))) (V ((D (A N)) (P (D (A N)))))) according to Fodor & Pylyshyn's <ref> [11, p. 39] </ref> own definition: What does it mean to say that thought is systematic? Well, just as you don't find people who can understand the sentence `John loves the girl' but not the sentence `the girl loves John,' so too you don't find people who can think the thought that
Reference: 12. <author> D. E. Rumelhart, G. Hinton and R. Williams, </author> <title> Learning Internal Representations through Error Propagation, </title> <booktitle> in Parallel Distributed Processing: Experiments in the Microstructure of Cognition, </booktitle> <volume> vol. 1, </volume> <editor> D. E. Rumelhart, J. L. McClelland and the PDP research Group (ed.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986, </year> <pages> 25-40. </pages>
Reference-contexts: Agreeing thoroughly that compositional symbolic structures are important, in this paper I show a connectionist architecture which can discover compact distributed representations for them. Recursive Auto-Associative Memory (RAAM) uses backpropagation <ref> [12] </ref> on a non-stationary environment to devise patterns which stand for all of the internal nodes of fixed-valence trees. Further, the representations discovered are not merely connectionist implementations of classic concatenative data structures, but are in fact new, interesting, and potentially very useful. <p> Consider hhhhhhhhhhhhhhh 4 I also assume that the reader is, by now, familiar with this standard, as well as with the backpropagation technique for adjusting weights <ref> [12] </ref>, and will not attempt a re-presentation of the mathematics. The work herein does not crucially depend on the default assumptions of semi-linearity and full-connectedness. By relying on these standard defaults, however, I hope to keep the focus on issue of representation. 8 J. B. <p> Along the way, representations will also be devised for all subtrees, in this case, (A B) and (C D). Note that, as will be hhhhhhhhhhhhhhh 5 Rumelhart et al. <ref> [12] </ref> demonstrated only a 8-3-8 network, but other successful uses include a 64-16-64 network [28] and a 270-45-270 network [4]. The three numbers correspond to the number of units in the input, hidden, and output layers of a network. <p> This form of non-stationary, or ``Moving Target,'' learning has also been explored by others [29, 30]. The stability and convergence of the network are sensitive to the learning parameters. Following the explication of Rumelhart et al. <ref> [12, p. 330] </ref>, there are two such parameters: the learning rate h, which controls the the gradient descent step size, and the momentum a, which integrates the effects of previous steps.
Reference: 13. <author> J. L. McClelland and D. E. Rumelhart, </author> <title> An interactive activation model of the effect of context in perception: Part 1. An account of basic findings, Psychology Review 88, </title> <booktitle> (1981), </booktitle> <pages> 375-407. </pages>
Reference-contexts: Furthermore, local representational schemes do not efficiently represent sequential information. The standard approach involves converting time into space by duplicating sub-networks into a fixed set of buffers for sequential input. Both early connectionist work, such as McClelland & Rumelhart's word recognition model <ref> [13] </ref>, as well as more modern efforts [4, 14] use this approach, which is not able to represent or process sequences longer than a predetermined bound. One way to overcome this length limitation is by "sliding" the input across the buffer [15, 16].
Reference: 14. <author> R. Allen, </author> <title> Several Studies on Natural Language and Back Propagation, </title> <booktitle> Institute of Electrical and Electronics Engineers First International Conference on Neural Networks, </booktitle> <address> San Diego, </address> <year> 1987, </year> <month> II-335-342. </month>
Reference-contexts: Furthermore, local representational schemes do not efficiently represent sequential information. The standard approach involves converting time into space by duplicating sub-networks into a fixed set of buffers for sequential input. Both early connectionist work, such as McClelland & Rumelhart's word recognition model [13], as well as more modern efforts <ref> [4, 14] </ref> use this approach, which is not able to represent or process sequences longer than a predetermined bound. One way to overcome this length limitation is by "sliding" the input across the buffer [15, 16].
Reference: 15. <author> T. J. Sejnowski and C. R. Rosenberg, </author> <title> Parallel Networks that Learn to Pronounce English Text, </title> <journal> Complex Systems 1, </journal> <year> (1987), </year> <pages> 145-168. </pages>
Reference-contexts: One way to overcome this length limitation is by "sliding" the input across the buffer <ref> [15, 16] </ref>. While such systems are capable of processing sequences longer than the predetermined bound, they are not really representing them.
Reference: 16. <author> E. Charniak and E. Santos, </author> <title> A context-free connectionist parser which is not connectionist, but then it is not really context-free either., in Advances in Connectionist & Neural Computation Theory, </title> <editor> J. Barnden and J. Pollack (ed.), </editor> <publisher> Ablex, </publisher> <address> Norwood, NJ, </address> <year> 1989. </year>
Reference-contexts: One way to overcome this length limitation is by "sliding" the input across the buffer <ref> [15, 16] </ref>. While such systems are capable of processing sequences longer than the predetermined bound, they are not really representing them.
Reference: 17. <author> G. E. Hinton, </author> <title> Distributed Representations, </title> <type> CMU-CS-84-157, </type> <institution> Carnegie-Mellon University, Computer Science Department, </institution> <address> Pittsburgh, PA, </address> <year> 1984. </year>
Reference-contexts: While such systems are capable of processing sequences longer than the predetermined bound, they are not really representing them. Distributed Representations have been the focus of much research (including the work reported herein) since the circulation of Hinton's 1984 report <ref> [17] </ref> discussing the properties of representations in which "each entity is represented by a pattern of activity distributed over many computing elements, and each computed element is involved in representing many different entities." The most obvious and natural distributed representation is a feature (or micro-feature) system, traditionally used in linguistics. <p> However, it was not immediately obvious how to develop such reduced descriptions. Instead, avant-garde connectionist representations were based on coarse-coding <ref> [17] </ref>, which allows multiple semi-independent representational elements to be simultaneously present, by superposition, in a feature vector. Once multiple elements can be present, conventional groupings of the elements can be interpreted as larger structures.
Reference: 18. <author> A. H. Kawamoto, </author> <title> Dynamic Processes in the (Re)Solution of Lexical Ambiguity, </title> <type> Doctoral Dissertation, </type> <institution> Department of Psychology, Brown University, Providence, </institution> <year> 1985. </year>
Reference-contexts: A good example of a connectionist model using such a representation is Kawamoto's work on lexical access <ref> [18] </ref>. However, since the entire feature system is needed to represent a single concept, attempts at 4 J. B. Pollack representing structures involving those concepts cannot be managed in the same system.
Reference: 19. <author> G. Hinton, </author> <title> Representing Part-Whole hierarchies in connectionist networks, </title> <booktitle> Proceedings of the Tenth Annual Conference of the Cognitive Science Society, </booktitle> <address> Montreal, </address> <year> 1988, </year> <pages> 48-54. </pages>
Reference-contexts: between the representation of a structure (by concatenation) and the representation of an element of the structure (by features), this type of system cannot represent embedded structures such as "John saw the nurse riding an elephant." A solution to the feature-buffer dichotomy problem was anticipated and sketched out by Hinton <ref> [19] </ref>, and involved having a "reduced description" for NURSE RIDING ELEPHANT which would fit into the constituent buffers along with patterns for JOHN and SAW. However, it was not immediately obvious how to develop such reduced descriptions. <p> The systematic patterns developed by RAAM are a very new kind of representation, a recursive, distributed representation, which seems to instantiate Hinton's notion of the "reduced description" mentioned earlier <ref> [19] </ref>.
Reference: 20. <author> D. S. Touretzky and G. E. Hinton, </author> <title> Symbols among the neurons: details of a connectionist inference architecture, </title> <booktitle> Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <address> Los Angeles, CA, </address> <year> 1985. </year>
Reference-contexts: Once multiple elements can be present, conventional groupings of the elements can be interpreted as larger structures. For example, Touretzky has developed a coarse-coded memory system and used it in a production system <ref> [20] </ref>, a primitive lisp data-structuring system called BoltzCONS [21], and a combination of the two for simple tree manipulations [22]. In his representation, the 15,625 triples of 25 symbols (A-Y) are elements to be represented, and using patterns over 2000 bits, small sets of such triples could be reliably represented. <p> there remain some problems: (1) A large amount of human effort was involved in the design, compression and tuning of these representations, and it is often not clear how to translate that effort across domains. (2) Coarse-coding requires expensive and complex access mechanisms, such as pullout networks [25] or clause-spaces <ref> [20] </ref>. (3) Coarse-coded symbol memories can only simultaneously instantiate a small number of representational elements (like triples of 25 tokens) before spurious elements are introduced 3 .
Reference: 21. <author> D. S. Touretzky, BoltzCONS: </author> <title> Reconciling connectionism with the recursive nature of stacks and trees, </title> <booktitle> Proceedings of the 8th Annual Conference of the Cognitive Science Society, </booktitle> <address> Amherst, MA, </address> <year> 1986, </year> <pages> 522-530. </pages>
Reference-contexts: Once multiple elements can be present, conventional groupings of the elements can be interpreted as larger structures. For example, Touretzky has developed a coarse-coded memory system and used it in a production system [20], a primitive lisp data-structuring system called BoltzCONS <ref> [21] </ref>, and a combination of the two for simple tree manipulations [22]. In his representation, the 15,625 triples of 25 symbols (A-Y) are elements to be represented, and using patterns over 2000 bits, small sets of such triples could be reliably represented.
Reference: 22. <author> D. S. Touretzky, </author> <title> Representing and transforming recursive objects in a neural network, or ``trees do grow on Boltzmann machines'', </title> <booktitle> Proceedings of the 1986 Institute of Electrical and Electronics Engineers International Conference on Systems, Man, and Cybernetics, </booktitle> <address> Atlanta, GA, </address> <year> 1986. </year>
Reference-contexts: For example, Touretzky has developed a coarse-coded memory system and used it in a production system [20], a primitive lisp data-structuring system called BoltzCONS [21], and a combination of the two for simple tree manipulations <ref> [22] </ref>. In his representation, the 15,625 triples of 25 symbols (A-Y) are elements to be represented, and using patterns over 2000 bits, small sets of such triples could be reliably represented. Interpreting the set of triples as pseudo-CONS cells, a limited representation of sequences and trees could be achieved.
Reference: 23. <author> D. E. Rumelhart and J. L. McClelland, </author> <title> On Learning the Past Tenses of English Verbs, </title> <booktitle> in Parallel Distributed Processing: Experiments in the Microstructure of Cognition, </booktitle> <volume> vol. 2, </volume> <editor> J. L. McClelland, D. E. Rumelhart and the PDP research Group (ed.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986, </year> <pages> 216-271. </pages>
Reference-contexts: Interpreting the set of triples as pseudo-CONS cells, a limited representation of sequences and trees could be achieved. Similarly, in their past-tense model, Rumelhart and McClelland <ref> [23] </ref> developed an implicitly sequential representation, where a set of well-formed overlapping triples could be interpreted as a sequence.
Reference: 24. <author> S. Pinker and A. Prince, </author> <title> On Language and Connectionism: Analysis of a parallel distributed processing model of language inquisition., </title> <journal> Cognition 28, </journal> <year> (1988), </year> <pages> 73-193. </pages>
Reference-contexts: In contrast, the distributed representations devised by the RAAM architecture demonstrate better properties: (1) Encodings are developed mechanically by an adaptive network. hhhhhhhhhhhhhhh 2 To point out this "Banana Problem" with Rumelhart & McClelland's actual representation, which was phonological rather than orthographic, Pinker and Prince <ref> [24] </ref> discovered words with enough internal duplication in the Oykangand language. 3 Rosenfeld and Touretzky [26] provide a nice analysis of coarse-coded symbol memories. 6 J. B.
Reference: 25. <author> M. Mozer, </author> <title> Inductive information retrieval using parallel distributed computation, </title> <type> Technical Report, </type> <institution> Institute for Cognitive Science, UCSD, La Jolla, </institution> <year> 1984. </year>
Reference-contexts: their circumscribed tasks, there remain some problems: (1) A large amount of human effort was involved in the design, compression and tuning of these representations, and it is often not clear how to translate that effort across domains. (2) Coarse-coding requires expensive and complex access mechanisms, such as pullout networks <ref> [25] </ref> or clause-spaces [20]. (3) Coarse-coded symbol memories can only simultaneously instantiate a small number of representational elements (like triples of 25 tokens) before spurious elements are introduced 3 .
Reference: 26. <editor> R. Rosenfeld and D. Touretzky, </editor> <title> Four capacity models for coarse-coded symbol memories, </title> <journal> Complex Systems 2, </journal> <year> (1988), </year> <pages> 463-484. </pages>
Reference-contexts: demonstrate better properties: (1) Encodings are developed mechanically by an adaptive network. hhhhhhhhhhhhhhh 2 To point out this "Banana Problem" with Rumelhart & McClelland's actual representation, which was phonological rather than orthographic, Pinker and Prince [24] discovered words with enough internal duplication in the Oykangand language. 3 Rosenfeld and Touretzky <ref> [26] </ref> provide a nice analysis of coarse-coded symbol memories. 6 J. B. Pollack (2) The access mechanisms are simple and deterministic. (3) A potentially very large number of primitive elements can selectively combine into constituent structures.
Reference: 27. <author> D. H. Ackley, G. E. Hinton and T. J. Sejnowski, </author> <title> A learning algorithm for Boltzmann Machines, </title> <booktitle> Cognitive Science 9, </booktitle> <year> (1985), </year> <pages> 147-169. </pages>
Reference-contexts: By relying on these standard defaults, however, I hope to keep the focus on issue of representation. 8 J. B. Pollack simultaneously training these two mechanisms as a single 2k -k -2k network, as shown in This looks suspiciously like a network for the Encoder Problem <ref> [27] </ref>. Backpropagation has been quite successful at this problem, 5 when used in a self-supervised auto-associative mode on a three layer network. The network is trained to reproduce a set of input patterns; i.e., the input patterns are also used as desired (or target) patterns.
Reference: 28. <author> G. Cottrell, P. Munro and D. Zipser, </author> <title> Learning Internal Representations from Gray-Scales Images: An Example of Extensional Programming., </title> <booktitle> Proceedings of the Seventh Annual Conference of the Cognitive Science Society, </booktitle> <address> Seattle, </address> <year> 1987, </year> <pages> 461-473. </pages>
Reference-contexts: Along the way, representations will also be devised for all subtrees, in this case, (A B) and (C D). Note that, as will be hhhhhhhhhhhhhhh 5 Rumelhart et al. [12] demonstrated only a 8-3-8 network, but other successful uses include a 64-16-64 network <ref> [28] </ref> and a 270-45-270 network [4]. The three numbers correspond to the number of units in the input, hidden, and output layers of a network. Recursive Distributed Representations 9 demonstrated later, this strategy works on a collection of trees just as it does on a single tree.
Reference: 29. <author> J. L. Elman, </author> <title> Finding Structure in Time, </title> <type> Report 8801, </type> <institution> Center for Research in Language, </institution> <address> UCSD, San Diego, </address> <year> 1988. </year>
Reference-contexts: Therefore, as the weights in the network evolve, so do some of the patterns that comprise the training environment. This form of non-stationary, or ``Moving Target,'' learning has also been explored by others <ref> [29, 30] </ref>. The stability and convergence of the network are sensitive to the learning parameters.
Reference: 30. <author> R. Miikkulainen and D. G. Dyer, </author> <title> Forming Global Representations with Back Propagation, </title> <booktitle> Proceedings of the Institute of Electrical and Electronics Engineers Second Annual International 32 J. B. Pollack Conference on Neural Networks, </booktitle> <address> San Diego, </address> <year> 1988. </year>
Reference-contexts: Therefore, as the weights in the network evolve, so do some of the patterns that comprise the training environment. This form of non-stationary, or ``Moving Target,'' learning has also been explored by others <ref> [29, 30] </ref>. The stability and convergence of the network are sensitive to the learning parameters.
Reference: 31. <author> D. C. Plaut, S. Nowlan and G. E. Hinton, </author> <title> Experiments on learning by back-propagation, </title> <institution> CMU-CS-86-126, Computer Science Dept., Carnegie Mellon University, Pittsburgh, </institution> <year> 1986. </year>
Reference-contexts: In the experiments described later in this paper, h was usually set to 0.1 (less for the larger experiments), and a to 0.3. As the learning curve flattens out, a is slowly increased up to 0.9, following <ref> [31] </ref>. (2) The induction relied upon is outside the mechanical framework of learning. This induction, of global success arising from only local improvements, is similar to the Bucket Brigade principle used in classifier systems [32].
Reference: 32. <author> J. H. Holland, K. J. Holyoak, R. E. Nisbett and P. R. Thagard, </author> <title> Induction: Processes of Inference, Learning, and Discovery, </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: As the learning curve flattens out, a is slowly increased up to 0.9, following [31]. (2) The induction relied upon is outside the mechanical framework of learning. This induction, of global success arising from only local improvements, is similar to the Bucket Brigade principle used in classifier systems <ref> [32] </ref>. Since the training strategy never reconstructs the terminals from R 1 or R 2 , only the fact that they are equal, in the limit, to R 1 and R 2 allows this strategy to work.
Reference: 33. <author> M. I. Jordan, </author> <title> Serial Order: A Parallel Distributed Processing Approach, </title> <type> ICS report 8608, </type> <institution> Institute for Cognitive Science, UCSD, La Jolla, </institution> <year> 1986. </year>
Reference-contexts: Figure 6 shows the representations for these letter sequences, and the clus ter diagram in Figure 7 shows that, unlike a decaying sum representation in which infor mation about older elements gets lost <ref> [33] </ref>, this sequential representation is devoting the most resources to keeping older elements alive. And even though there are enough resources to build a 5-letter shift register, the network cannot take this easy solution path because of its need to represent the 6- and 9-letter words. 14 J. B.
Reference: 34. <author> E. Saund, </author> <title> Dimensionality Reduction and Constraint in Later Vision, </title> <booktitle> Proceedings of the Ninth Annual Conference of the Cognitive Science Society, </booktitle> <address> Seattle, </address> <year> 1987, </year> <pages> 908-915. </pages>
Reference-contexts: Recursive Distributed Representations 25 developed for these two trees. If one stares long enough, taking each pair of composable points in one's mental left and right hands, one can see triangles falling forward as they reduce in scale. Saund <ref> [34] </ref> has investigated (non-recursive) auto-association as a method of dimensionality reduction, and asserted that, in order to work, the map must be constrained to form a small dimensional parametric surface in the larger dimensional space. Consider just a 2-1-2 auto-associator.
Reference: 35. <author> B. Mandelbrot, </author> <title> The Fractal Geometry of Nature, </title> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1982. </year>
Reference-contexts: In the limit, especially if there are any dense "patches" of 2-space which need to be covered, it can no longer be a 1-dimensional curve, but must become a space-filling curve with a fractal dimension <ref> [35] </ref>. The notions of associative and reconstructive memories with fractal dimensions are further discussed elsewhere [36]. 4.3. Applications 4.3.1.
Reference: 36. <author> J. B. Pollack, </author> <title> Implications of Recursive Distributed Representations, </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <editor> D. Touretzky (ed.), </editor> <publisher> Morgan Kaufman, </publisher> <address> Los Gatos, CA, </address> <year> 1989. </year>
Reference-contexts: The notions of associative and reconstructive memories with fractal dimensions are further discussed elsewhere <ref> [36] </ref>. 4.3. Applications 4.3.1. Associative Inference Since RAAM can devise representations of trees as numeric vectors which then can be attacked with the fixed-width techniques of neural networks, this work might lead to very fast inference and structural transformation engines.
Reference: 37. <author> K. Hornik, M. Stinchcombe and H. White, </author> <title> Multi-layer Feedforward Networks are Universal Approximators, Neural Networks, </title> <note> To Appear. </note>
Reference: 38. <author> R. P. Lippman, </author> <title> An introduction to computing with neural networks, </title> <journal> Institute of Electrical and Electronics Engineers ASSP Magazine April, </journal> <year> (1987), </year> <pages> 4-22. </pages>
Reference: 39. <author> A. S. Lapedes and R. M. Farber, </author> <title> How Neural Nets Work, </title> <address> LAUR-88-418, Los Alamos, </address> <year> 1988. </year>
Reference: 40. <author> J. B. Pollack, </author> <title> Cascaded Back Propagation on Dynamic Connectionist Networks, </title> <booktitle> Proceedings of the Ninth Conference of the Cognitive Science Society, </booktitle> <address> Seattle, </address> <year> 1987, </year> <pages> 391-404. </pages>
Reference-contexts: RAAMs can devise these compositional representations, as shown by the experiment on semantic triples, which can then be used as the target patterns for recurrent networks which accept sequences of words as input. A feasibility study of this concept has been performed as well, using a sequential cascaded network <ref> [40] </ref>, a higher-order network with a more restricted topology than Sigma-Pi [41]. Basically, a cascaded network consists of two subnetworks: The function network is an ordinary feed-forward network, but its weights are dynamically computed by the purely linear context network, whose outputs determine each weight of the function net.
Reference: 41. <author> R. Williams, </author> <title> The Logic of Activation Functions, </title> <booktitle> in Parallel Distributed Processing: Experiments in the Microstructure of Cognition, </booktitle> <volume> vol. 1, </volume> <editor> D. E. Rumelhart, J. L. McClelland and the PDP research Group (ed.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986, </year> <pages> 423-443. </pages>
Reference-contexts: A feasibility study of this concept has been performed as well, using a sequential cascaded network [40], a higher-order network with a more restricted topology than Sigma-Pi <ref> [41] </ref>. Basically, a cascaded network consists of two subnetworks: The function network is an ordinary feed-forward network, but its weights are dynamically computed by the purely linear context network, whose outputs determine each weight of the function net.
Reference: 42. <author> G. E. Hinton, </author> <title> Learning Distributed Representations of Concepts, </title> <booktitle> Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <address> Amherst, MA, </address> <year> 1986, </year> <pages> 1-12. </pages>
Reference-contexts: The RAAM architecture has been inspired by two powerful ideas. The first is due to Hinton <ref> [42] </ref>, who showed that, when properly constrained, a connectionist network can develop semantically interpretable representations on its hidden units. The second is an old idea, that given a sufficiently powerful form of learning, a machine can learn to efficiently perform a task by example, rather than by design.
References-found: 42

