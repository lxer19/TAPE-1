URL: http://www.research.att.com/~mkearns/papers/topdown.ps.Z
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/avrim/www/ML98/presentations.html
Root-URL: 
Title: On the Boosting Ability of Top-Down Decision Tree Learning Algorithms provably optimal for decision tree
Author: Michael Kearns Yishay Mansour 
Note: bound of (1=*) O(1= 2 (which is polynomial in 1=*) is obtained, which is  The primary contribution of this work is in  
Date: May 1996  
Affiliation: AT&T Research  Tel-Aviv University  
Abstract: We analyze the performance of top-down algorithms for decision tree learning, such as those employed by the widely used C4.5 and CART software packages. Our main result is a proof that such algorithms are boosting algorithms. By this we mean that if the functions that label the internal nodes of the decision tree can weakly approximate the unknown target function, then the top-down algorithms we study will amplify this weak advantage to build a tree achieving any desired level of accuracy. The bounds we obtain for this amplification show an interesting dependence on the splitting criterion used by the top-down algorithm. More precisely, if the functions used to label the internal nodes have error 1=2 fl as approximations to the target function, then for the splitting criteria used by CART and C4.5, trees of size (1=*) O(1=fl 2 * 2 ) and (1=*) O(log(1=*)=fl 2 ) (respectively) suffice to drive the error below *. Thus (for example), a small constant advantage over random guessing is amplified to any larger constant advantage with trees of constant size. For a new splitting criterion suggested by our analysis, the much stronger fl A preliminary version of this paper appears in Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, pages 459-468, ACM Press, 1996. Authors' addresses: M. Kearns, AT&T Research, 600 Mountain Avenue, Room 2A-423, Murray Hill, New Jersey 07974; electronic mail mkearns@research.att.com. Y. Mansour, Department of Computer Science, Tel Aviv University, Tel Aviv, Israel; electronic mail mansour@math.tau.ac.il. Y. Mansour was supported in part by the Israel Science Foundation, administered by the Israel Academy of Science and Humanities, and by a grant of the Israeli Ministry of Science and Technology. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> In Machine Learning: </editor> <booktitle> Proceedings of the International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> 1982-1995. </pages>
Reference-contexts: Dozens of papers describing experiments and applications involving top-down decision tree learning algorithms appear in the machine learning literature each year <ref> [1] </ref>. There has been difficulty in finding natural theoretical models that provide a precise yet useful language in which to discuss the performance of top-down decision tree learning heuristics, to compare variants of these heuristics, and to compare them to learning algorithms that use entirely different representations and approaches. <p> Here F is a class of boolean functions with input domain X, and G : <ref> [0; 1] </ref> ! [0; 1] is a function having three properties: * G is symmetric about 1=2. Thus, G (x) = G (1 x) for any x 2 [0; 1]. * G (1=2) = 1 and G (0) = G (1) = 0. * G is concave. <p> Here F is a class of boolean functions with input domain X, and G : <ref> [0; 1] </ref> ! [0; 1] is a function having three properties: * G is symmetric about 1=2. Thus, G (x) = G (1 x) for any x 2 [0; 1]. * G (1=2) = 1 and G (0) = G (1) = 0. * G is concave. <p> Here F is a class of boolean functions with input domain X, and G : <ref> [0; 1] </ref> ! [0; 1] is a function having three properties: * G is symmetric about 1=2. Thus, G (x) = G (1 x) for any x 2 [0; 1]. * G (1=2) = 1 and G (0) = G (1) = 0. * G is concave. We will call such a function G a permissible splitting criterion. <p> In particular, if G is a permissible splitting criterion, then we have G (q) min (q; 1 q) for all q 2 <ref> [0; 1] </ref>, and thus G (T ) *(T ) for all T . <p> We think of G t as a "potential function" that (hopefully) decreases with each new split. Algorithms similar to TopDown F;G (t) are in widespread use in both applications and the experimental machine learning community <ref> [1, 16, 3] </ref>. There are of course many important issues of implementation that we have omitted that must be addressed in practice. <p> See Figure 4 for a diagram summarizing the split parameters. 6 We are now in position to make some simple but central insights regarding how the split at ` reduces the quantity G t . Note that since q = (1 t )p + t r and t 2 <ref> [0; 1] </ref>, one of p and r is less than or equal to q and the other is greater than or equal to q. Without loss of generality, let p q r. Now before the split, the contribution of the leaf ` to G t was w G (q). <p> Lemma 3 Let G (q) be one of 4q (1 q); H (q) and 2 p q (1 q). Then for any fixed values for t; ffi 2 <ref> [0; 1] </ref>, G (q; t; ffi) 2 t (1 t )(1 2t )ffi 3 G 000 (q): (12) Proof:With t and ffi fixed, we perform a Taylor expansion of G (q) at q, and replace the occurrences of G (q), G (q t ffi) and G (q + (1 t )ffi) <p> We begin by substituting for ffi under the given constraint t (1 t )ffi flq (1 q). It is clear from Equation 11 and Figure 5 that for any fixed values q; t 2 <ref> [0; 1] </ref>, G (q; t; ffi) is minimized by choosing ffi as small as possible. <p> This will allow us to replace occurrences of t with constant values. Lemma 4 Let G (q) be one of 4q (1 q); H (q) and 2 p q (1 q). Let fl 2 <ref> [0; 1] </ref> be any fixed value, and let G (q; t ) be as defined in Equation 15. Then for any fixed q 2 [0; 1], G (q; t ) is minimized by a value of t falling in the interval [0:4; 0:6]. <p> Lemma 4 Let G (q) be one of 4q (1 q); H (q) and 2 p q (1 q). Let fl 2 <ref> [0; 1] </ref> be any fixed value, and let G (q; t ) be as defined in Equation 15. Then for any fixed q 2 [0; 1], G (q; t ) is minimized by a value of t falling in the interval [0:4; 0:6]. <p> We would like to show that for some particular choices for G, any minimizing value of t is always (for all q; 2 <ref> [0; 1] </ref>, fl 2 (0; 1=2], and ffi = flq (1 q)=(t (1 t )) bounded away from 0 and 1. <p> Figures 8 through 13 (and formal proofs for all fl are given in Lemma 12, Lemma 13 and Lemma 11 in the Appendix) that if G (q) = 4q (1 q); G (q) = H (q) or G (q) = 2 p q (1 q), then for all q; 2 <ref> [0; 1] </ref>, if t 0:4 then G (q t ffi) + t ffi G 0 (q t ffi) G (q + (1 t )ffi) (1 t )ffi G 0 (q + (1 t )ffi) (and thus @ G (q; t )=@t 0), and if t 0:6 then G (q t ffi) <p> We now apply these general results to obtain lower bounds for specific choices of G (). Lemma 5 Under the constraint t (1 t )ffi flq (1 q) given by Lemma 2, if G (q) = 4q (1 q) then for any q 2 <ref> [0; 1] </ref> Proof:Here we have derivatives G 00 (q) = 8 and G 000 (q) = 0. <p> 16fl 2 (q (1 q)) 2 : (21) Here we have used the fact that t (1 t ) 1=4. 2 (Lemma 5) Lemma 6 Under the constraint t (1 t )ffi flq (1 q) given by Lemma 2, if G (q) = H (q) then for any q 2 <ref> [0; 1] </ref> Proof:Using the fact that for G (q) = H (q) we have derivatives G 00 (q) = (1=(1 q) + 1=q) and G 000 (q) = (1=(1 q) 2 1=q 2 ), application of Lemma 3 yields G (q; t; ffi) 2 1 + q + 6 1 1 <p> which states that the minimizing value of t lies in the interval [0:4; 0:6] for all q. 2 (Lemma 6) Lemma 7 Under the constraint t (1 t )ffi flq (1 q) given by Lemma 2, if G (q) = 2 p q (1 q) then for any q 2 <ref> [0; 1] </ref> fl 2 (1 2q) 2 (q (1 q)) 1=2 + 2fl 2 (q (1 q)) 3=2 : (28) Proof:Here we have derivatives G 00 (q) = 2 (q (1 q)) 3=2 (q (1 q)) 1=2 (29) G 000 (q) = 4 (q (1 q)) 5=2 + (q (1 q)) <p> We begin with the Gini criterion G (q) = 4q (1 q). Theorem 8 Let G (q) = 4q (1 q), and let * t and G t be as defined in Equations 2 and 3. Then under the Weak Hypothesis Assumption, for any * 2 <ref> [0; 1] </ref>, to obtain * t * it suffices to make t 2 c=fl 2 * 2 splits, for some constant c. <p> Then under the Weak Hypothesis Assumption, for any * 2 <ref> [0; 1] </ref>, to obtain * t * it suffices to make t 1 c log (1=*)=fl 2 splits, for some constant c. Proof:By Equation 2, after t splits there must be a leaf ` such that w (`) min (q (`); 1 q (`)) * t =t. <p> It can be shown that H 1 (y) y=(2 log (2=y)) for all y 2 <ref> [0; 1] </ref>. Thus we have G t+1 G t 2t G t 4t log (2=G t ) It can be verified that G t e fl p log (t)=c is a solution to this recurrence inequality, as desired. <p> Then under the Weak Hypothesis Assumption, for any * 2 <ref> [0; 1] </ref>, to obtain * t * it suffices to make t 1 32=fl 2 splits. Proof:By the definition of G t , there must exist a leaf ` such that 2w (`)(q (`)(1 q (`))) 1=2 G t =t.
Reference: [2] <author> A. Blum, M. Furst, J. Jackson, M. Kearns, Y. Mansour, and S. Rudich. </author> <title> Weakly learning DNF and characterizing statistical query learning using Fourier analysis. </title> <booktitle> In Proceedings of the 26th ACM Symposium on the Theory of Computing. </booktitle> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Furthermore, superpolynomial lower bounds for this same problem have been proven for a wide class of algorithms that includes the top-down decision tree approach (and also all variants of this approach that have been proposed to date) <ref> [2] </ref>. The positive results for efficient decision tree learning in computational learning theory all make extensive use of membership queries [14, 5, 4, 11], which provide the learning algorithm with black-box access to the target function (experimentation), rather than only an oracle for random examples.
Reference: [3] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: The tremendous popularity of such programs (which include the C4.5 and CART software packages <ref> [16, 3] </ref>) is due to their efficiency and simplicity, the advantages of using decision trees (such as potential interpretability to humans), and of course, to their success in generating trees with good generalization ability (that is, performance on new data). <p> We think of G t as a "potential function" that (hopefully) decreases with each new split. Algorithms similar to TopDown F;G (t) are in widespread use in both applications and the experimental machine learning community <ref> [1, 16, 3] </ref>. There are of course many important issues of implementation that we have omitted that must be addressed in practice. <p> This is done in order to avoid the phenomenon known as overfitting, in which the error on the sample and the error on the distribution diverge <ref> [16, 3] </ref>. Despite such issues, our idealization TopDown F;G (t) captures the basic algorithmic ideas behind many widely used decision tree algorithms. <p> Similarly, the CART program uses the splitting criterion G (q) = 4q (1 q), known as the Gini criterion <ref> [3] </ref>. We feel that the analysis given here provides some insight into why such top-down decision tree algorithms succeed, and what their limitations are. Let us now discuss the choice of the node function class F in more detail.
Reference: [4] <author> N. Bshouty and Y. Mansour. </author> <title> Simple learning algorithms for decision trees and multivariate polynomials. </title> <booktitle> In Proceedings of the 36th IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 304-311. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1995. </year>
Reference-contexts: The positive results for efficient decision tree learning in computational learning theory all make extensive use of membership queries <ref> [14, 5, 4, 11] </ref>, which provide the learning algorithm with black-box access to the target function (experimentation), rather than only an oracle for random examples.
Reference: [5] <author> N. H. Bshouty. </author> <title> Exact learning via the monotone theory. </title> <booktitle> In Proceedings of the 34th IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 302-311. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1993. </year>
Reference-contexts: The positive results for efficient decision tree learning in computational learning theory all make extensive use of membership queries <ref> [14, 5, 4, 11] </ref>, which provide the learning algorithm with black-box access to the target function (experimentation), rather than only an oracle for random examples.
Reference: [6] <author> W. Buntine and T. Niblett. </author> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 75-86, </pages> <year> 1992. </year>
Reference-contexts: The best choice of G in practice is far being from a settled issue, as evidenced by the fact that the two most popular decision tree learning packages (C4.5 and CART) use different choices for G. There have also been a number of experimental papers examining various choices <ref> [15, 6] </ref>.
Reference: [7] <author> T. Dietterich, M. Kearns, and Y. Mansour. </author> <title> Applying the weak learning framework to understand and improve C4.5. </title> <booktitle> In Machine Learning: Proceedings of the International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: Our analysis yields radically different bounds for the choices made for G by C4.5 and CART, and indicates that both may be inferior to a new choice for G suggested by our analysis. (Preliminary experiments supporting this view are reported in our recent follow-up paper <ref> [7] </ref>.) In addition to providing a nontrivial analysis of the performance of top-down 1 decision tree learning, the proof of our results gives a number of specific technical insights into the success and limitations of these heuristics. <p> The top-down algorithms we examine make local modifications to the current tree T in an effort to reduce G (T ), and therefore hopefully reduce *(T ) as 1 The recent paper of Dietterich et al. <ref> [7] </ref> gives detailed experimental comparisons of top-down decision tree algorithms and the best of the standard boosting methods; see also the recent paper of Freund and Schapire [8]. 2 TopDown F;G (t): 1 Initialize T to be the single-leaf tree, with binary label equal to the majority label of the sample. <p> There have also been a number of experimental papers examining various choices [15, 6]. Perhaps the insights in this paper most relevant to the practice of machine learning are those regarding the behavior of TopDown F;G as a function of G <ref> [7] </ref>. 3 The Weak Hypothesis Assumption We now quantify what we mean by a "favorable" choice of the node splitting class F . The definition we adopt is essentially the one used by a number of previous papers on the topic of weak learning [17, 9, 10]. <p> The proof of Theorem 1 will show that there are good technical reasons for believing that the claimed differences between the criteria are qualitatively real, and this seems to be borne out by preliminary experimental results <ref> [7] </ref>. The remainder of the paper is devoted to the proof of Theorem 1. <p> Thus, the resulting class of filtered distributions is in some sense simpler than those generated by other boosting algorithms [17, 9, 10]. See the recent paper of Dietterich et al. <ref> [7] </ref> for further discussion of this issue. Our goal now is to obtain for each G a lower bound on the local drop G (q) (1 t )G (p) t G (r) to G t under the condition t (1 t )ffi flq (1 q) given by Lemma 2.
Reference: [8] <author> Y. Freund and R. Schapire. </author> <title> Some experiments with a new boosting algorithm. </title> <booktitle> In Machine Learning: Proceedings of the International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: in an effort to reduce G (T ), and therefore hopefully reduce *(T ) as 1 The recent paper of Dietterich et al. [7] gives detailed experimental comparisons of top-down decision tree algorithms and the best of the standard boosting methods; see also the recent paper of Freund and Schapire <ref> [8] </ref>. 2 TopDown F;G (t): 1 Initialize T to be the single-leaf tree, with binary label equal to the majority label of the sample. 2 while T has fewer than t internal nodes: 3 best 0. 4 for each pair (`; h) 2 leaves (T ) fi F : 5 G
Reference: [9] <author> Yoav Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <journal> Information and Computation, </journal> <volume> 121(2) </volume> <pages> 256-285, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: In this paper, we attempt to remedy this state of affairs by analyzing top-down decision tree learning algorithms in the model of weak learning <ref> [17, 10, 9] </ref>. In the language of weak learning, we prove here that the standard top-down decision tree algorithms are in fact boosting algorithms. <p> The definition we adopt is essentially the one used by a number of previous papers on the topic of weak learning <ref> [17, 9, 10] </ref>. Definition 1 Let f be any boolean function over an input space X. Let F be any class of boolean functions over X. Let fl 2 (0; 1=2]. <p> If F does not contain f , then the Weak Hypothesis Assumption amounts to an assumption on f. More precisely, it is known that Weak Hypothesis Assumption is equivalent to the assumption that on any distribution, f can be approximated by thresholded linear combinations of functions in F <ref> [9] </ref>. <p> previous papers have proposed boosting algorithms that combine many different functions from F , each with a small predictive advantage over random guessing on a different filtered distribution, to obtain a single hybrid function whose generalization error on the target 4 distribution P is less than any desired value * <ref> [17, 9, 10] </ref>. The central question examined for such algorithms is: As a function of the advantage fl and the desired error *, how many functions must be combined, and how many random examples drawn? Several boosting algorithms enjoy very strong upper bounds on these quantities [17, 9, 10]: the number <p> any desired value * <ref> [17, 9, 10] </ref>. The central question examined for such algorithms is: As a function of the advantage fl and the desired error *, how many functions must be combined, and how many random examples drawn? Several boosting algorithms enjoy very strong upper bounds on these quantities [17, 9, 10]: the number of functions that must be combined is polynomial in 1=fl and log (1=*), and the number of examples required is polynomial in 1=fl and 1=*. <p> Note, however, that this limitation is entirely representational | it arises solely from the fact that we have chosen to learn using a decision tree over F , not from any properties of the algorithm we use to construct the tree. A more sophisticated construction (due to Freund <ref> [9] </ref> and discussed following Theorem 1) implies that this representational lower bound can be improved to ((1=*) c=fl 2 ) for some constant c &gt; 0. We will show that for appropriately chosen G, algorithm TopDown F;G (t) in fact achieves this optimal bound of O ((1=*) c=fl 2 ). <p> Only for the new criterion G (q) = 2 p q (1 q) do we obtain a bound polynomial in 1=*. The following argument, based on ideas of Freund <ref> [9] </ref>, demonstrates that this bound is in fact optimal for decision tree algorithms (top-down or otherwise). <p> Thus, the resulting class of filtered distributions is in some sense simpler than those generated by other boosting algorithms <ref> [17, 9, 10] </ref>. See the recent paper of Dietterich et al. [7] for further discussion of this issue.
Reference: [10] <author> Yoav Freund and Robert E. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In Second European Conference on Computational Learning Theory, </booktitle> <pages> pages 23-37. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <month> 13 </month>
Reference-contexts: In this paper, we attempt to remedy this state of affairs by analyzing top-down decision tree learning algorithms in the model of weak learning <ref> [17, 10, 9] </ref>. In the language of weak learning, we prove here that the standard top-down decision tree algorithms are in fact boosting algorithms. <p> In the next section, we adopt the Weak Hypothesis Assumption (motivated by and closely related to the model of Weak Learning <ref> [13, 17, 10] </ref>) to quantify this relationship. We defer detailed discussion of the choice of the permissible splitting criterion G, since one of the main results of our analysis is a rather precise reason why some choices may be vastly preferable to others. <p> The definition we adopt is essentially the one used by a number of previous papers on the topic of weak learning <ref> [17, 9, 10] </ref>. Definition 1 Let f be any boolean function over an input space X. Let F be any class of boolean functions over X. Let fl 2 (0; 1=2]. <p> previous papers have proposed boosting algorithms that combine many different functions from F , each with a small predictive advantage over random guessing on a different filtered distribution, to obtain a single hybrid function whose generalization error on the target 4 distribution P is less than any desired value * <ref> [17, 9, 10] </ref>. The central question examined for such algorithms is: As a function of the advantage fl and the desired error *, how many functions must be combined, and how many random examples drawn? Several boosting algorithms enjoy very strong upper bounds on these quantities [17, 9, 10]: the number <p> any desired value * <ref> [17, 9, 10] </ref>. The central question examined for such algorithms is: As a function of the advantage fl and the desired error *, how many functions must be combined, and how many random examples drawn? Several boosting algorithms enjoy very strong upper bounds on these quantities [17, 9, 10]: the number of functions that must be combined is polynomial in 1=fl and log (1=*), and the number of examples required is polynomial in 1=fl and 1=*. <p> Thus, the resulting class of filtered distributions is in some sense simpler than those generated by other boosting algorithms <ref> [17, 9, 10] </ref>. See the recent paper of Dietterich et al. [7] for further discussion of this issue.
Reference: [11] <author> J. Jackson. </author> <title> An efficient membership query algorithm for learning DNF with respect to the uniform distribution. </title> <booktitle> In Proceedings of the 35th IEEE Symposium on the Foundations of Computer Science. </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994. </year>
Reference-contexts: The positive results for efficient decision tree learning in computational learning theory all make extensive use of membership queries <ref> [14, 5, 4, 11] </ref>, which provide the learning algorithm with black-box access to the target function (experimentation), rather than only an oracle for random examples.
Reference: [12] <author> M. Kearns and R. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 48(3) </volume> <pages> 464-497, </pages> <year> 1994. </year>
Reference-contexts: We call the parameter fl the advantage. It is worth mentioning that this definition can be extended to the case where F is a class of probabilistic boolean functions <ref> [12] </ref>. All of our results hold for this more general setting. Note that if F actually contains the function f , then f trivially 1=2-satisfies the Weak Hypothesis Assumption with respect to F .
Reference: [13] <author> M. Kearns and L. G. Valiant. </author> <title> Cryptographic limitations on learning boolean formulae and finite automata. </title> <journal> Journal of the ACM, </journal> <volume> 41(1) </volume> <pages> 67-95, </pages> <year> 1994. </year>
Reference-contexts: In the next section, we adopt the Weak Hypothesis Assumption (motivated by and closely related to the model of Weak Learning <ref> [13, 17, 10] </ref>) to quantify this relationship. We defer detailed discussion of the choice of the permissible splitting criterion G, since one of the main results of our analysis is a rather precise reason why some choices may be vastly preferable to others.
Reference: [14] <author> E. Kushilevitz and Y. Mansour. </author> <title> Learning decision trees using the Fourier spectrum. </title> <booktitle> In Proc. of the 23rd Symposium on Theory of Computing, </booktitle> <pages> pages 455-464. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: The positive results for efficient decision tree learning in computational learning theory all make extensive use of membership queries <ref> [14, 5, 4, 11] </ref>, which provide the learning algorithm with black-box access to the target function (experimentation), rather than only an oracle for random examples.
Reference: [15] <author> J. Mingers. </author> <title> An empirical comparison of selection measures for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 319-342, </pages> <year> 1989. </year>
Reference-contexts: The best choice of G in practice is far being from a settled issue, as evidenced by the fact that the two most popular decision tree learning packages (C4.5 and CART) use different choices for G. There have also been a number of experimental papers examining various choices <ref> [15, 6] </ref>.
Reference: [16] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: The tremendous popularity of such programs (which include the C4.5 and CART software packages <ref> [16, 3] </ref>) is due to their efficiency and simplicity, the advantages of using decision trees (such as potential interpretability to humans), and of course, to their success in generating trees with good generalization ability (that is, performance on new data). <p> We think of G t as a "potential function" that (hopefully) decreases with each new split. Algorithms similar to TopDown F;G (t) are in widespread use in both applications and the experimental machine learning community <ref> [1, 16, 3] </ref>. There are of course many important issues of implementation that we have omitted that must be addressed in practice. <p> This is done in order to avoid the phenomenon known as overfitting, in which the error on the sample and the error on the distribution diverge <ref> [16, 3] </ref>. Despite such issues, our idealization TopDown F;G (t) captures the basic algorithmic ideas behind many widely used decision tree algorithms. <p> For example, it is fair to think of the popular C4.5 software package as a variant of TopDown F;G (t) in which G (q) is the binary entropy function, and F is the class of single variables (projection functions) <ref> [16] </ref>. Similarly, the CART program uses the splitting criterion G (q) = 4q (1 q), known as the Gini criterion [3]. We feel that the analysis given here provides some insight into why such top-down decision tree algorithms succeed, and what their limitations are.
Reference: [17] <author> R. E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: In this paper, we attempt to remedy this state of affairs by analyzing top-down decision tree learning algorithms in the model of weak learning <ref> [17, 10, 9] </ref>. In the language of weak learning, we prove here that the standard top-down decision tree algorithms are in fact boosting algorithms. <p> In the next section, we adopt the Weak Hypothesis Assumption (motivated by and closely related to the model of Weak Learning <ref> [13, 17, 10] </ref>) to quantify this relationship. We defer detailed discussion of the choice of the permissible splitting criterion G, since one of the main results of our analysis is a rather precise reason why some choices may be vastly preferable to others. <p> The definition we adopt is essentially the one used by a number of previous papers on the topic of weak learning <ref> [17, 9, 10] </ref>. Definition 1 Let f be any boolean function over an input space X. Let F be any class of boolean functions over X. Let fl 2 (0; 1=2]. <p> previous papers have proposed boosting algorithms that combine many different functions from F , each with a small predictive advantage over random guessing on a different filtered distribution, to obtain a single hybrid function whose generalization error on the target 4 distribution P is less than any desired value * <ref> [17, 9, 10] </ref>. The central question examined for such algorithms is: As a function of the advantage fl and the desired error *, how many functions must be combined, and how many random examples drawn? Several boosting algorithms enjoy very strong upper bounds on these quantities [17, 9, 10]: the number <p> any desired value * <ref> [17, 9, 10] </ref>. The central question examined for such algorithms is: As a function of the advantage fl and the desired error *, how many functions must be combined, and how many random examples drawn? Several boosting algorithms enjoy very strong upper bounds on these quantities [17, 9, 10]: the number of functions that must be combined is polynomial in 1=fl and log (1=*), and the number of examples required is polynomial in 1=fl and 1=*. <p> Thus, the resulting class of filtered distributions is in some sense simpler than those generated by other boosting algorithms <ref> [17, 9, 10] </ref>. See the recent paper of Dietterich et al. [7] for further discussion of this issue.
Reference: [18] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: For example, even if we make the rather favorable assumption that there is a small decision tree labeling the data, and that the inputs are distributed uniformly (thus, we are in the well-known Probably Approximately Correct or PAC model <ref> [18] </ref>, with the additional restriction to the uniform distribution), the problem of finding any efficient algorithm with provably nontrivial performance remains an elusive open problem in computational learning theory.
References-found: 18

