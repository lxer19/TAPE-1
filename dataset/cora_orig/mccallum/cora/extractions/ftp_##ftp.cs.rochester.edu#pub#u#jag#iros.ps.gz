URL: ftp://ftp.cs.rochester.edu/pub/u/jag/iros.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/jag/publications.html
Root-URL: 
Email: fjag,nelsong@cs.rochester.edu  
Title: Adaptive Differential Visual Feedback for Uncalibrated Hand-Eye Coordination and Motor Control  
Author: Martin Jagersand, Randal Nelson 
Note: Support for this work was provided by ONR grant N00014-93-I-0221, Sverige-Amerika Stiftelsen and the Fulbright Commission  
Date: December 14, 1994  
Web: http://www.cs.rochester.edu/u/fjag,nelsong/  
Address: Rochester, Rochester, NY 14627  
Affiliation: Department of Computer Science, University of  
Abstract: We propose and implement a novel method for visual space trajectory planning, and adaptive high Degree Of Freedom (DOF) visual feedback control. The method requires no prior information either about the kinematics of the manipulator, or the placement or calibration of the cameras, and imposes no limitations on the number of degrees of freedom controlled or the number of kind of visual features utilized. The approach provides not only a means of low-level servoing but a means to integrate it with higher level visual space trajectory and task planning. We are thus able to specify and perform complex tasks composed of several primitive behaviors, using both visual servoing and open loop control, where the number of sensed and controlled signals varies during the task. We report experimental results demonstrating a factor of 5 improvement in the repeatability of manipulations using a PUMA arm when comparing visual closed-loop to traditional joint level servoing. We also present experiment statistics showing the advantages of adaptive over non-adaptive control systems, and of using redundant visual information when performing manipulation tasks. Finally, we demonstrate usefulness of the approach by using it to specify and execute complex tasks involving real-world robot manipulation of rigid and non-rigid objects in up to 12 degrees of freedom. The manipulation is performed in the context of a semi-autonomous robot manipulation system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sanderson A. C. Weiss L. E. </author> <title> "Adaptive visual servo control of robots" In Robot Vision Ed: </title> <editor> Pugh A. </editor> <booktitle> IFS 1988 25 26 </booktitle>
Reference-contexts: We experimentally evaluate its performance, and then illustrate its usefulness in several complex real-world manipulation tasks. Differential visual feedback control in robotics was pioneered in the eighties, primarily by Lee Weiss and Arthur Sanderson <ref> [1, 2, 3] </ref>. They proposed a classification of different types of visual control, and performed experiments in simulation with differential visual feedback controllers. Since then, real world implementations have been made by a number of researchers, e.g., [9, 10, 4, 5, 6, 7, 13, 19, 15, 16, 20, 22]. <p> After making the movement x and observing the change y measured in 6 the feature values, we wish to modify the Jacobian to bring our model into agreement with the measurement, i.e. to make y measured = J k+1 x. In general (for size (J ) &gt; <ref> [1; 1] </ref>), this problem does not have a unique solution. One choice is to make the minimal change necessary. In this case ^ J k+1 = J k + (y measured J k x)x T x T x which can be easily seen to satisfy our model agreement condition. <p> Right: Varying the length of the (random) trajectory does not affect joint control repeatability thus disturbing the internal model in the controller 12 equation J dist = a fl ^ J + (1 a) fl J random where a 2 <ref> [0; 1] </ref> indicates the model accuracy and J random is a random m fi n matrix with elements drawn from a uniform distribution on the interval [j2 ^ J i;j j; j2 ^ J i;j j].
Reference: [2] <author> Weiss L. E. </author> <title> "Dynamic Visual Servo Control of Robots: </title> <booktitle> An Adaptive Image-Based Approach" PhD thesis, </booktitle> <address> CMU 1984 </address>
Reference-contexts: We experimentally evaluate its performance, and then illustrate its usefulness in several complex real-world manipulation tasks. Differential visual feedback control in robotics was pioneered in the eighties, primarily by Lee Weiss and Arthur Sanderson <ref> [1, 2, 3] </ref>. They proposed a classification of different types of visual control, and performed experiments in simulation with differential visual feedback controllers. Since then, real world implementations have been made by a number of researchers, e.g., [9, 10, 4, 5, 6, 7, 13, 19, 15, 16, 20, 22]. <p> It was realized early on that accuracy could be improved by using residual visual error, measured after a movement was executed, to perform a correction move. <ref> [2] </ref> classified such schemes as Position based (iterated) look-and-move approaches. The next step was to notice that if goal points were provided in image coordinates and realized through joint angles, then the intermediate world coordinate system could be completely eliminated. Weiss calls this Image based look-and-move.
Reference: [3] <author> Weiss L. E. Sanderson A. C. Neumann C. P. </author> <title> "Dynamic Sensor-Based Control of Robots with Visual Feedback" J. </title> <booktitle> of Robotics and Automation v. </booktitle> <month> RA-3 Oct </month> <year> 1987 </year>
Reference-contexts: We experimentally evaluate its performance, and then illustrate its usefulness in several complex real-world manipulation tasks. Differential visual feedback control in robotics was pioneered in the eighties, primarily by Lee Weiss and Arthur Sanderson <ref> [1, 2, 3] </ref>. They proposed a classification of different types of visual control, and performed experiments in simulation with differential visual feedback controllers. Since then, real world implementations have been made by a number of researchers, e.g., [9, 10, 4, 5, 6, 7, 13, 19, 15, 16, 20, 22]. <p> Previous approaches have either assumed that the system need only be calibrated once (e.g. [9]) using a set of specific "test movements", that it can be decoupled into a set of single variable adaptive controllers (e.g. <ref> [3] </ref>), or that the system can be modeled using an ARMAX model (e.g. [5]). These systems can only model a small class of visual-motor transfer functions. <p> A large number of different visual measures have been proposed for visual feedback, including line length, projected area, area or line length ratios <ref> [3] </ref>, and Fourier descriptors [4]. Mitch Harris in [14] defines a more general "visual similarity" or "closeness" function based on orientation, length, and distance between line segments. Our philosophy has been to use the simplest measures that suffice for the task at hand.
Reference: [4] <author> Feddema J. T. Mitchell O. R. </author> <title> "Vision Guided Servoing with Feature Based Trajectory Generation" IEEE Trans. </title> <booktitle> on Robotics and Automation v 5 nr 5 p 691-670 1989 </booktitle>
Reference-contexts: A large number of different visual measures have been proposed for visual feedback, including line length, projected area, area or line length ratios [3], and Fourier descriptors <ref> [4] </ref>. Mitch Harris in [14] defines a more general "visual similarity" or "closeness" function based on orientation, length, and distance between line segments. Our philosophy has been to use the simplest measures that suffice for the task at hand. We tried various measures for our perception vectors.
Reference: [5] <author> Feddema J. T. Lee G. C. S. </author> <title> "Adaptive Image Feature Prediction and Control for Visual Tracking with a Hand-Eye Coordinated Camera" IEEE Trans. </title> <journal> on Systems, Man and Cybernetics, </journal> <volume> vol 20, no 5 1990 </volume>
Reference-contexts: Previous approaches have either assumed that the system need only be calibrated once (e.g. [9]) using a set of specific "test movements", that it can be decoupled into a set of single variable adaptive controllers (e.g. [3]), or that the system can be modeled using an ARMAX model (e.g. <ref> [5] </ref>). These systems can only model a small class of visual-motor transfer functions. Limitations include requiring the transfer function to be near linear over the desired operating space, restricting the number and type of visual features that can be used simultaneously, and limiting the number of cameras and/or camera placement. <p> Other approaches can be described as partially adaptive, being applicable only to a restricted set of visual motor transfer functions, requiring partial (analytic) modeling of the system (e.g. restricted to systems suitable for ARMAX modeling <ref> [5, 8] </ref>, or using only image location visual measurements under a weak perspective assumption [16]). We want to solve manipulation problems that are unsuitable for analytic modeling at all, such as the manipulation of flexible foam in 12 DOF shown in section 5.3.
Reference: [6] <author> Feddema J. T. Lee G. C. S. Mitchell O. R. </author> <title> "Weighted selection of Image Features for Resolved Rate Visual Feedback Control" IEEE Trans. </title> <journal> on Robotics and Automation, </journal> <note> v 7 nr 1 p 31-47 1991 </note>
Reference: [7] <author> Feddema J. T. Lee G. C. S. Mitchell O. R. </author> <title> "Model-Based Visual Feedback Control for a Hand-Eye Coordinated Robotic System" Computer, </title> <month> Aug </month> <year> 1992 </year>
Reference: [8] <author> Papanikolopoulos N. P. Khosla P. K. </author> <title> "Adaptive Robotic Visual Tracking: </title> <journal> Theory and Experiments" IEEE Trans. </journal> <note> on Automatic Control Vol 38 no 3 March 1993 </note>
Reference-contexts: Other approaches can be described as partially adaptive, being applicable only to a restricted set of visual motor transfer functions, requiring partial (analytic) modeling of the system (e.g. restricted to systems suitable for ARMAX modeling <ref> [5, 8] </ref>, or using only image location visual measurements under a weak perspective assumption [16]). We want to solve manipulation problems that are unsuitable for analytic modeling at all, such as the manipulation of flexible foam in 12 DOF shown in section 5.3.
Reference: [9] <author> Chongstitvatana P. Conkie A. </author> <title> "Behaviour Based Assembly Experiments using Vision Sensing" DAI TR #466, </title> <institution> U of Edinburgh 1990 </institution>
Reference-contexts: For a review of this work we direct the reader to [17] or [18]. The adaptiveness of the controller is a key to the success of our approach. Previous approaches have either assumed that the system need only be calibrated once (e.g. <ref> [9] </ref>) using a set of specific "test movements", that it can be decoupled into a set of single variable adaptive controllers (e.g. [3]), or that the system can be modeled using an ARMAX model (e.g. [5]). These systems can only model a small class of visual-motor transfer functions. <p> These two control laws implement the image based look-and-move and the image based visual servoing. These or similar control laws have been used in previously published work e.g. <ref> [9, 15, 20] </ref>. However, there are two compelling reasons why the above design is not sufficient. First, it is well known that Newton methods are not globally convergent [34]. <p> We also want to place the fewest possible restrictions on the class of transfer functions we can model. This rules out systems depending on near linear transfer functions in the desired operating space (e.g., <ref> [9, 15] </ref>), and methods that would be inefficient for high DOF systems (e.g. [22] where calculating a Jacobian update requires nm measurements, with nm being the product of the number of sensory and controlled signals) We use a Jacobian model estimation and updating scheme based on the information obtained more or
Reference: [10] <author> Conkie A. Chongstitvatana P. </author> <title> "An Uncalibrated Stereo Visual Servo System" DAI TR #475, </title> <institution> U of Edinburgh 1990 </institution>
Reference: [11] <author> J. Y. Zheng Q. Chen S. </author> <title> Tsuji "Active camera guided manipulation" In Proc. </title> <booktitle> of 1991 IEEE Int Conference of Robotics and Automation </booktitle>
Reference: [12] <author> Bennet D. Geiger D. Hollerbach J. </author> <title> "Autonomous Robot Calibration for Hand-Eye Coordination" Int. </title> <editor> J. </editor> <booktitle> of Robotics Research v 10 nr 5 1991. </booktitle>
Reference: [13] <author> Wijesoma S. W. Wolfe D. F. H. Richards R. J. </author> <title> "Eye-to-Hand Coordination for vision guided Robot Control Applications" Int. </title> <journal> J. of Robotics Research, </journal> <volume> v 12 No 1 1993 </volume>
Reference-contexts: At the expense of generality in the visual processing, some researchers (e.g. <ref> [13] </ref>) have circumvented this problem by using special purpose high speed "vision" systems. Drawing on results in optimization we instead use a globally convergent step restricted method, adapting the step length to keep it within a range where our local model is sufficiently accurate. <p> The closest approach is <ref> [13] </ref>, who showed for a 2 DOF implementation, the advantage of visual feedback over open loop control when model errors are large.
Reference: [14] <author> Harris M. </author> <title> "Vision Guided Part Alignment with Degraded Data" DAI TR #615, </title> <institution> U of Edinburgh 1993 </institution>
Reference-contexts: A large number of different visual measures have been proposed for visual feedback, including line length, projected area, area or line length ratios [3], and Fourier descriptors [4]. Mitch Harris in <ref> [14] </ref> defines a more general "visual similarity" or "closeness" function based on orientation, length, and distance between line segments. Our philosophy has been to use the simplest measures that suffice for the task at hand. We tried various measures for our perception vectors.
Reference: [15] <author> Hager G. Chang W.C. Morse S. </author> <title> "Robot Feedback Control Based on Stereo Vision: </title> <note> Towards Calibration-Free Hand-Eye Coordination" TR # 992 Yale 1993 </note>
Reference-contexts: These two control laws implement the image based look-and-move and the image based visual servoing. These or similar control laws have been used in previously published work e.g. <ref> [9, 15, 20] </ref>. However, there are two compelling reasons why the above design is not sufficient. First, it is well known that Newton methods are not globally convergent [34]. <p> We also want to place the fewest possible restrictions on the class of transfer functions we can model. This rules out systems depending on near linear transfer functions in the desired operating space (e.g., <ref> [9, 15] </ref>), and methods that would be inefficient for high DOF systems (e.g. [22] where calculating a Jacobian update requires nm measurements, with nm being the product of the number of sensory and controlled signals) We use a Jacobian model estimation and updating scheme based on the information obtained more or
Reference: [16] <author> Hollinghurst N. Cipolla R. </author> <booktitle> "Uncalibrated Stereo Hand-Eye Coordination" British Machine Vision Conference 1993 </booktitle>
Reference-contexts: Other approaches can be described as partially adaptive, being applicable only to a restricted set of visual motor transfer functions, requiring partial (analytic) modeling of the system (e.g. restricted to systems suitable for ARMAX modeling [5, 8], or using only image location visual measurements under a weak perspective assumption <ref> [16] </ref>). We want to solve manipulation problems that are unsuitable for analytic modeling at all, such as the manipulation of flexible foam in 12 DOF shown in section 5.3. We also want to place the fewest possible restrictions on the class of transfer functions we can model.
Reference: [17] <author> Jagersand M. </author> <title> "Perception level control for uncalibrated hand-eye coordination and motor actions" Areapaper, </title> <note> University of Rochester May 1994. Forthcoming as TR. </note>
Reference-contexts: Since then, real world implementations have been made by a number of researchers, e.g., [9, 10, 4, 5, 6, 7, 13, 19, 15, 16, 20, 22]. For a review of this work we direct the reader to <ref> [17] </ref> or [18]. The adaptiveness of the controller is a key to the success of our approach.
Reference: [18] <author> Corke P. I. </author> <title> "Visual control of robot manipulators a review" In Visual Servoing, </title> <booktitle> World Scientific 1993. </booktitle>
Reference-contexts: Since then, real world implementations have been made by a number of researchers, e.g., [9, 10, 4, 5, 6, 7, 13, 19, 15, 16, 20, 22]. For a review of this work we direct the reader to [17] or <ref> [18] </ref>. The adaptiveness of the controller is a key to the success of our approach.
Reference: [19] <author> Corke P. I. </author> <title> "High-Performance Visual Closed-Loop Robot Control" PhD thesis U of Melbourne July 1994. </title>
Reference: [20] <author> Hosoda K. Asada M. </author> <title> "Versatile Visual Servoing without Knowledge of True Jacobian" IROS Aug 1994. </title>
Reference-contexts: These two control laws implement the image based look-and-move and the image based visual servoing. These or similar control laws have been used in previously published work e.g. <ref> [9, 15, 20] </ref>. However, there are two compelling reasons why the above design is not sufficient. First, it is well known that Newton methods are not globally convergent [34].
Reference: [21] <author> W. Z. Chen U. A. Korde S. B. </author> <title> Skaar "Position Control Experiments Using Vision" Int. </title> <journal> Journal of Robotics Research, </journal> <volume> v13 No 3 p199-208, </volume> <year> 1994 </year>
Reference-contexts: The closest approach is [13], who showed for a 2 DOF implementation, the advantage of visual feedback over open loop control when model errors are large. Chen et al in <ref> [21] </ref> show that two heavy industrial robot arms can perform a peg-in-hole parts mating task with a clearance of 0.7mm between the peg and the hole. 8 In this section we describe some of the characteristics of our adaptive control method that makes it particularly suited to the kind of visual
Reference: [22] <author> B. H. Yoshimi P. K. </author> <title> Allen "Active, </title> <booktitle> Uncalibrated Visual Servoing" Submitted 1995 Int. Conference on Robotics and Automation </booktitle>
Reference-contexts: We also want to place the fewest possible restrictions on the class of transfer functions we can model. This rules out systems depending on near linear transfer functions in the desired operating space (e.g., [9, 15]), and methods that would be inefficient for high DOF systems (e.g. <ref> [22] </ref> where calculating a Jacobian update requires nm measurements, with nm being the product of the number of sensory and controlled signals) We use a Jacobian model estimation and updating scheme based on the information obtained more or less for free while performing the task.
Reference: [23] <author> Terzopoulus D. Szeliski R. </author> <title> "Tracking with Kalman Snakes" In Active Vision, </title> <editor> Ed Blake, </editor> <publisher> Yuille MIT Press 1992. </publisher>
Reference-contexts: In the demonstration we start out with the foam just barely compressed between the two robots end effectors as seen in the left image in 17. We originally intended to track the foam shape using snakes 24 <ref> [23, 24] </ref>. This proved hard, so we reverted to our convolution trackers, tracking a sparse representation of the foam contour using the tracking targets shown attached to the foam in fig 17. From a transfer function point of view this is a hard problem.
Reference: [24] <author> Terzopoulus D. Szeliski R. </author> <title> "Dynamic Contours: Real time active splines" In Active Vision Ed Blake, </title> <publisher> Yuille MIT Press 1992. </publisher>
Reference-contexts: In the demonstration we start out with the foam just barely compressed between the two robots end effectors as seen in the left image in 17. We originally intended to track the foam shape using snakes 24 <ref> [23, 24] </ref>. This proved hard, so we reverted to our convolution trackers, tracking a sparse representation of the foam contour using the tracking targets shown attached to the foam in fig 17. From a transfer function point of view this is a hard problem.
Reference: [25] <author> Brooks R. </author> <title> "A Robust Layered Control System For A Mobile Robot" IEEE J. </title> <journal> Robotics and Automa tion, </journal> <note> v RA-2 nr 1 March 1986 </note>
Reference-contexts: This can be seen as an extension of the precondition perception ! action used in many subsumption architecture systems <ref> [27, 25, 26] </ref>, where a process is watching sensory inputs for a precondition perception, and when a matching perception occurs, an open loop, canned action is carried out.
Reference: [26] <author> Brooks R. </author> <title> "Intelligence Without Reason" AI memo nr 1293 MIT 1991 </title>
Reference-contexts: This can be seen as an extension of the precondition perception ! action used in many subsumption architecture systems <ref> [27, 25, 26] </ref>, where a process is watching sensory inputs for a precondition perception, and when a matching perception occurs, an open loop, canned action is carried out.
Reference: [27] <author> Connell J. </author> <title> "Controlling a mobile robot using partial representations" SPIE 91 Mobile Robots pp 34-45 1991 </title>
Reference-contexts: This can be seen as an extension of the precondition perception ! action used in many subsumption architecture systems <ref> [27, 25, 26] </ref>, where a process is watching sensory inputs for a precondition perception, and when a matching perception occurs, an open loop, canned action is carried out.
Reference: [28] <author> Ikeuchi K. Suehiro T. </author> <title> "Towards an Assembly Plan from Observation" Proc. </title> <booktitle> Robotics and Automation 1992 </booktitle>
Reference-contexts: At the higher level end of task specification and trajectory planning, we have been inspired by the user friendly man-machine interfaces found in <ref> [28, 29, 30] </ref>. Our ultimate aim is to provide natural, low bandwidth human interaction with our system, combining the advantages of autonomous operation and telemanipulation.
Reference: [29] <author> Kuniyoshi Y. Inaba M. Inoue H. </author> <title> "Seeing, </title> <booktitle> Understanding and Doing Human Task" Proc. Robotics and Automation 1992 </booktitle>
Reference-contexts: At the higher level end of task specification and trajectory planning, we have been inspired by the user friendly man-machine interfaces found in <ref> [28, 29, 30] </ref>. Our ultimate aim is to provide natural, low bandwidth human interaction with our system, combining the advantages of autonomous operation and telemanipulation.
Reference: [30] <author> P. Pook, D. H. Ballard "Teleassistance: </author> <booktitle> Contextual guidance for autonomous manipulation" Proc. of AAAI, </booktitle> <month> Aug </month> <year> 1994. </year>
Reference-contexts: At the higher level end of task specification and trajectory planning, we have been inspired by the user friendly man-machine interfaces found in <ref> [28, 29, 30] </ref>. Our ultimate aim is to provide natural, low bandwidth human interaction with our system, combining the advantages of autonomous operation and telemanipulation. <p> This kind of system requires the manipulator to be near anthropomorphic, or requires a significant learning effort on the part of the operator to figure out the manipulator kinematic and dynamics <ref> [30] </ref>. 4.2.2 Trajectory planning Visual space trajectory planning actually takes place on two levels.
Reference: [31] <author> Lloyd J. </author> <title> "The Multi-RCCL Users Guide" ICRA Workshop on visual servoing 1994. </title>
Reference-contexts: On the Utah/MIT hand, an appropriate set point controller comes as part of the hardware. On the Puma, the set 7 point controller was implemented through primitives provided by RCCL <ref> [31] </ref>. By running the visual control loop fast enough to keep the set point just ahead of the robot, we are able to achieve smooth trajectories by essentially "leading" the robot around.
Reference: [32] <author> G. A. Geist, V. S. </author> <title> Sunderam "Network Based Concurrent Computing on the PVM System" TR Oak Ridge National Laboratory, </title> <year> 1993 </year>
Reference-contexts: Arrows between modules indicate direction of information transfer. The structure of the control algorithm we have implemented is shown in Fig. 3. In addition to the modules shown, there are real-time visual feature trackers that provide information about the current visual state. The system is implemented as a pvm <ref> [32] </ref> distributed program running vision processing, and visual space control routines on a 8 processor SPARCserver 2000 multiprocessor, and robot control on a SPARCserver 330. 3 Experiments with the Adaptive Control Method Surprisingly, a thorough experimental evaluation of the accuracy related properties of differential visual feedback control has never, to our
Reference: [33] <author> Fletcher R. </author> <title> Practical Methods of Optimization Chichester second ed. </title> <year> 1987 </year>
Reference-contexts: In order to avoid some of the above problems, we found it helpful to look outside the mainstream of traditional control theory. In particular we drew inspiration from numerical analysis, specifically, the Broyden class of optimization methods for nonlinear problems (a survey can be found in <ref> [33] </ref>). <p> The estimation technique we have described falls into a class called Broyden methods <ref> [33] </ref> used in nonlinear optimization. 2.3 Implementation issues If J were a square, full rank matrix, a unique x could be found for each y by Gaussian elimination. However, J is generally neither square, nor full rank.
Reference: [34] <author> Gustafsson I. </author> <note> Tillampad Optimeringslara Kompendium, Institutionen for Informationsbehandling, Chalmers 1991 </note>
Reference-contexts: These or similar control laws have been used in previously published work e.g. [9, 15, 20]. However, there are two compelling reasons why the above design is not sufficient. First, it is well known that Newton methods are not globally convergent <ref> [34] </ref>. Even in cases where the continuous system should be stable, the low sampling frequency of a TV camera based vision sensory signals makes it difficult to obtain both stability and reasonable performance in the discrete time version ([7]). <p> The distance between the way points is adapted to fulfill a Marquart condition of model accuracy. The Marquart algorithm is simple <ref> [34] </ref>: If the model error at time k is d k = y measured k y predicted k &gt; d u adjust the step size downward, for instance halve it. If d k &lt; d l , then increase the step size.
Reference: [35] <author> Nelson R. </author> <title> "Vision a Intelligent Behavior-An introduction to Machine Vision at the University of Rochester" IJCV 7:1 p 5-9 Kluwer 1991 28 </title>
References-found: 35

