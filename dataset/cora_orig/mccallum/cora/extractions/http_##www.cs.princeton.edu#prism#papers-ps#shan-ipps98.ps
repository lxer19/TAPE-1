URL: http://www.cs.princeton.edu/prism/papers-ps/shan-ipps98.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Email: fshz, jpsg@cs.princeton.edu  
Title: Parallel Tree Building on a Range of Shared Address Space Multiprocessors: Algorithms and Application Performance  
Author: Hongzhang Shan and Jaswinder Pal Singh 
Date: January 21, 1998  
Address: Princeton University  
Affiliation: Department of Computer Science  
Abstract: Irregular, particle-based applications that use trees, for example hierarchical N-body applications, are important consumers of multiprocessor cycles, and are argued to benefit greatly in programming ease from a coherent shared address space programming model. As more and more supercomputing platforms that can support different programming models become available to users, from tightly-coupled hardware-coherent machines to clusters of workstations or SMPs, to truly deliver on its ease of programming advantages to application users it is important that the shared address space model not only perform and scale well in the tightly-coupled case but also port well in performance across the range of platforms (as the message passing model can). For tree-based N-body applications, this is currently not true: While the actual computation of interactions ports well, the parallel tree building phase can become a severe bottleneck on coherent shared address space platforms, in particular on platforms with less aggressive, commodity-oriented communication architectures (even though it takes less than 3 percent of the time in most sequential executions). We therefore investigate the performance of five parallel tree building methods in the context of a complete galaxy simulation on four very different platforms that support this programming model: an SGI Origin2000 (an aggressive hardware cache-coherent machine with physically distributed memory), an SGI Challenge bus-based shared memory multiprocessor, an Intel Paragon running a shared virtual memory protocol in software at page granularity, and a Wisconsin Typhoon-zero in which the granularity of coherence can be varied using hardware support but the protocol runs in software (in the last case using both a page-based and a fine-grained protocol). We find that the algorithms used successfully and widely distributed so far for the first two platforms cause overall application performance to be very poor on the latter two commodity-oriented platforms. An alternative algorithm briefly considered earlier for hardware coherent systems but then ignored in that context helps to some extent but not enough. Nor does an algorithm that incrementally updates the tree every time-step rather than rebuilding it. The best algorithm by far is a new one we propose that uses a separate spatial partitioning of the domain for the tree building phasewhich is different than the partitioning used in the major force calculation and other phasesand eliminates locking at a significant cost in locality and load balance. By changing the tree building algorithm, we achieve improvements in overall application performance of more than factors of 4-40 on commodity-based systems, even on only 16 processors. This allows commodity shared memory platforms to perform well for hierarchical N-body applications for the first time, and more importantly achieves performance portability since it also performs very well on hardware-coherent systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Barnes and P.Hut, </author> <title> A hierarchical O(NlogN) force calculation algorithm, </title> <journal> nature,vol. </journal> <volume> 324, p.446, </volume> <year> 1986. </year>
Reference-contexts: Having specified the initial positions and velocities of the n bodies, the classical N-body problem is to find their positions after a number of time steps. In the last decade, several O (NlogN) algorithms have been proposed. The Barnes-Hut method <ref> [1] </ref> is the one widely used on sequential and parallel machines today; while the tree building issues and algorithms we discuss apply to all the methods, we use a 3-dimensional Barnes-Hut galaxy simulation as an example application.
Reference: [2] <author> M. Warren, J. Salmon, </author> <title> A Parallel Hashed Oct-Tree N-Body Algorithm, </title> <booktitle> Proceedings of Supercomputing'93. </booktitle>
Reference-contexts: N-body problems are among the most important applications of tree-based simulation methods today, and we use them as the driving domain in this paper. The performance of N-body applications has been well-studied on two kinds of platforms: message passing machines <ref> [4, 2] </ref> and tightly coupled hardware cache-coherent multiprocessors [9]. <p> Finally, Warren and Salmon implemented an approach very similar to costzones using message passing and a hashing approach to maintain a global tree <ref> [2] </ref>. There has been other research in parallel N-body applications, but none focusing on tree building methods for shared address space systems.
Reference: [3] <author> JP. Singh, C.Holt, et al, </author> <title> Load Balancing and Data Locality in Hierarchical N-body Methods,Technique Report, </title> <institution> Stan-ford University, CSL-TR-92-505, </institution> <year> 1992. </year>
Reference-contexts: Singh et al studied this application on a prototype CC-NUMA machine, the Stanford DASH, for both Barnes-Hut and the Fast Multipole Method, and developed a new partitioning technique called costzones that achieved better performance and ease of programming <ref> [3] </ref>. That paper also introduced the PARTREE algorithm in an appendix, and showed it to improve performance when only a single particle was allowed per leaf cell in the Barnes-Hut tree.
Reference: [4] <author> John K. Salmon. </author> <title> Parallel Hierarchical N-body Methods, </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <month> Decenber </month> <year> 1990. </year>
Reference-contexts: N-body problems are among the most important applications of tree-based simulation methods today, and we use them as the driving domain in this paper. The performance of N-body applications has been well-studied on two kinds of platforms: message passing machines <ref> [4, 2] </ref> and tightly coupled hardware cache-coherent multiprocessors [9]. <p> From ORIG, ORIG-LOCAL, UPDATE, PARTREE to SPACE, in order to avoid the expensive synchronization on commodity hardware oriented machines, the number of lock operations becomes less and less. 5 Related Work Parallelizing hierarchical N-body applications for hardware-coherent (CC-NUMA) and message passing machines has been well studied earlier. Salmon <ref> [4] </ref> implemented a Barnes-Hut application on a message passing machine. He used a orthogonal recursive bisection partitioning technique to obtain good speedups on a 512 processor NCUBE.
Reference: [5] <author> K. Li, P.Hudak, </author> <title> Memory coherence in shared virtual memory systems, </title> <journal> ACM Transaction on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <year> 1989 </year>
Reference-contexts: The focus is on using more commodity-oriented communication architectures, either by relaxing the integration and specialization of the communication controller [11] or by leveraging the virtual memory mechanism to produce coherence at page granularity (shared virtual memory) <ref> [5] </ref>, or by providing access control in software [6, 7], in almost all cases running its protocol in software.
Reference: [6] <author> Ioannis Schoinas et al, </author> <title> Fine-grain Access Control for Distributed Shared Memory, </title> <booktitle> Sixth International Conference on Architectural Support for Programming Languages and Operating Systems October, </booktitle> <year> 1994. </year>
Reference-contexts: The focus is on using more commodity-oriented communication architectures, either by relaxing the integration and specialization of the communication controller [11] or by leveraging the virtual memory mechanism to produce coherence at page granularity (shared virtual memory) [5], or by providing access control in software <ref> [6, 7] </ref>, in almost all cases running its protocol in software.
Reference: [7] <author> D.J.Scales, K. Gharachorloo, And C.A.Thekkath. </author> <title> Shasta: A low Overhead, Software-only Approach for supporting Fine-Grained Shared Memory, </title> <booktitle> International Conference on Architectural Support for programming Languages and Operating systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: The focus is on using more commodity-oriented communication architectures, either by relaxing the integration and specialization of the communication controller [11] or by leveraging the virtual memory mechanism to produce coherence at page granularity (shared virtual memory) [5], or by providing access control in software <ref> [6, 7] </ref>, in almost all cases running its protocol in software.
Reference: [8] <author> D. Jiang, H. Shan, JP. Singh, </author> <title> Application Restructuring and Performance Portability on Shared Virtual Memory and Hardware-Coherent Multiprocessors, </title> <booktitle> in Proceedings of Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: The question of performance portability of a shared address space has begun to be studied <ref> [8] </ref>, but there is a lot more work to be done. <p> Previously proposed algorithms are tested, and new algorithms proposed that outperform them and are the best across the range of systems. Another recent paper made a start on studying performance portability for a shared address space <ref> [8] </ref>. It includes a hierarchical N-body application as one of its many applications examined, but it does not describe the tree-building algorithms or examine performance on real commodity-based systems (only through simulation, and only for page-based shared virtual memory). <p> The PARTREE algorithm comes next in overall performance portability, and also because it reduces communication at the cost of some load imbalance. We might draw some more general conclusions from this, which corroborate the conclusions in <ref> [8] </ref>. First, coarse grain applications that synchronize less frequently have much better performance portability across the range of systems than fine grain applications, even at the cost of some load balance and locality.
Reference: [9] <author> JP Singh, C.Holt, T. Totsuka, A.Gupta and J.L.Hennessy, </author> <title> Load Balancing and Data Locality in Adaptive Hierarchical N-body Methods: Barnes-Hut, Fast Multipole, </title> <journal> and Ra-diosity, Journal of Parallel and Distributed Computing, </journal> <month> June </month> <year> 1995 </year>
Reference-contexts: N-body problems are among the most important applications of tree-based simulation methods today, and we use them as the driving domain in this paper. The performance of N-body applications has been well-studied on two kinds of platforms: message passing machines [4, 2] and tightly coupled hardware cache-coherent multiprocessors <ref> [9] </ref>. Due to their irregular and dynamically changing nature, a coherent shared address space programming model has been argued to have substantial ease of programming advantages for them, and to also deliver very good performance when cache coherence is supported efficiently in hardware. <p> The SPACE algorithm is a new algorithm we developed motivated by an understanding of performance bottlenecks on the commodity platforms. Here we only discuss the tree building phase; the force calculation and update phases are the same in all cases and are discussed in <ref> [9] </ref>. 2.1 ORIG In this and the next three algorithms, in the tree building phase each processor is responsible for only those particles which were assigned to it for force calculation (and update) in its previous time step. (For the first time step, the particles are evenly assigned to processors).
Reference: [10] <author> Y. Zhou, L. Iftode, and K. Li, </author> <title> Performance Evaluation of Two Home-Based Lazy Release Consistency Protocols for Shared Virtual Memory Systems, </title> <booktitle> In proceedings of the Operating Systems Design and Implementation Symposium, </booktitle> <month> October, </month> <year> 1996. </year>
Reference-contexts: The shared address space and coherence are provided in software at page granularity through the virtual memory system. The protocol we used is the Home-based Lazy Release Consistency model (HLRC) <ref> [10] </ref>. The granularity of coherence is larger (which causes more false sharing of the data), as are the costs of communication and synchronization, but more data is transfered in each communication. So protocol actually is delayed till synchronization points, and the protocol is multiple-written.
Reference: [11] <author> Reinhardt S., Larus J., and Wood D., Tempest and Typhoon: </author> <title> User-Level Shared Memory, </title> <booktitle> In proceedings of the 21th International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year> <month> 10 </month>
Reference-contexts: The focus is on using more commodity-oriented communication architectures, either by relaxing the integration and specialization of the communication controller <ref> [11] </ref> or by leveraging the virtual memory mechanism to produce coherence at page granularity (shared virtual memory) [5], or by providing access control in software [6, 7], in almost all cases running its protocol in software. <p> One is the same HLRC SVM protocol as on the Paragon with 4k pages. This allows us to examine the same protocol on platforms with very different performance characteristics. The other is sequential consistency with 64 bytes coherence granularity, the protocol for which this machine was designed <ref> [11] </ref>. With sequential consistency, protocol activity occurs at memory operations and is not delayed till synchronization points.
Reference: [12] <institution> Performance Tuning for the Origin2000, </institution> <note> http://www.sgi.com </note>
Reference-contexts: The platforms we study range from fine grained hardware coherent machines to fine-grained software coherent machines to page-coherent SVM systems, and from centralized shared memory systems to distributed ones. They include the SGI Challenge (centralized shared memory, hardware cache-coherent) <ref> [12] </ref>, SGI Origin 2000 (distributed shared memory, hardware cache-coherent) [12], Intel Paragon (page grained SVM)[10] and Typhoon-zero (hardware support for variable coherence-granularity but protocols running in software on a coprocessor)[11] (in the last case, we use a page-based SVM protocol as well as a fine-grained sequentially consistent protocol). <p> The platforms we study range from fine grained hardware coherent machines to fine-grained software coherent machines to page-coherent SVM systems, and from centralized shared memory systems to distributed ones. They include the SGI Challenge (centralized shared memory, hardware cache-coherent) <ref> [12] </ref>, SGI Origin 2000 (distributed shared memory, hardware cache-coherent) [12], Intel Paragon (page grained SVM)[10] and Typhoon-zero (hardware support for variable coherence-granularity but protocols running in software on a coprocessor)[11] (in the last case, we use a page-based SVM protocol as well as a fine-grained sequentially consistent protocol).
Reference: [13] <author> J. Laudon, D. Lenoski, </author> <title> The SGI Origin2000 : A CC-NUMA Highly Scalable Server, </title> <booktitle> In proceedings of the 24th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Each node contains two 200Mhz MIPS R10000 processors connected by a system bus, a fraction of the main memory on the machine (1-4GB per node), a combined communication/coherence controller and network interface called the Hub, and an I/O interface called Xbow <ref> [13] </ref>. Our system has 30 processors with a 4MB secondary cache per processor. The local memory bandwidth is 512MB/s, shared by the local two processors and other devices on the bus. the maximum local memory access time is 313ns, and the remote maximum access time is 703ns.
Reference: [14] <author> L. Iftode, JP Singh, K. Li, </author> <title> Understanding Application Performance on Shared Virtual Memory Systems, </title> <booktitle> In proceedings of the 23th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Also there is tremendous serialization at locks because critical sections are greatly dilated by expensive page faults and protocol activity within them <ref> [14] </ref>. The ORIG, ORIG-LOCAL, and UPDATE algorithms use a lot of locking, so they perform very poorly (see the end of section 4 for measurements of dynamic lock counts). Since the PARTREE version is more coarse grained and needs less locking, it performs better.
Reference: [15] <author> ROSS Technology, Inc. </author> <title> SPARC RISC User's Guide: </title> <address> hyper-SPARC Edition, </address> <month> September </month> <year> 1993. </year> <month> 11 </month>
Reference-contexts: There are 4 virtual channels for each link and they are bi-directional. The distributed directory protocol is used for cache coherence. The page size is 16kb. 3.3 Paragon Unlike previous systems, the Paragon was designed without hardware support for a shared address space <ref> [15] </ref>. The shared address space and coherence are provided in software at page granularity through the virtual memory system. The protocol we used is the Home-based Lazy Release Consistency model (HLRC) [10].
References-found: 15

