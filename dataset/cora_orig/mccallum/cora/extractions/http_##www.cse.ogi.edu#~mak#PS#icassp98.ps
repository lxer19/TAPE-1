URL: http://www.cse.ogi.edu/~mak/PS/icassp98.ps
Refering-URL: http://www.cse.ogi.edu/CSLU/personnel/bios/mak.html
Root-URL: http://www.cse.ogi.edu
Email: fmak, enricog@research.att.com  
Title: TRAINING OF SUBSPACE DISTRIBUTION CLUSTERING HIDDEN MARKOV MODEL  
Author: Brian Mak and Enrico Bocchieri 
Address: 180 Park Ave, Florham Park, NJ 07932, USA  
Affiliation: AT&T Labs Research  
Abstract: In [2] and [7], we presented our novel subspace distribution clustering hidden Markov models (SDCHMMs) which can be converted from continuous density hidden Markov models (CDHMMs) by clustering subspace Gaussians in each stream over all models. Though such model conversion is simple and runs fast, it has two drawbacks: (1) it does not take advantage of the fewer model parameters in SDCHMMs theoretically SDCHMMs may be trained with smaller amount of data; and, (2) it involves two separate optimization steps (first training CDHMMs, then clustering subspace Gaussians) and the resulting SDCHMMs are not guaranteed to be optimal. In this paper, we show how SDCHMMs may be trained directly from less speech data if we have a priori knowledge of their architecture. On the ATIS task, a speaker-independent, context-independent(CI) 20-stream SDCHMM system trained using our novel SDCHMM reestimation algorithm with only 8 minutes of speech performs as well as a CDHMM system trained using conventional CDHMM reestimation algorithm with 105 minutes of speech. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.E. Levinson B.H. Juang and M.M. Sondhi. </author> <title> Maximum likelihood estimation for multivariate mixture observations of Markov chains. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-32(2):307309, </volume> <month> March </month> <year> 1986. </year>
Reference-contexts: Since an HMM state with an M -mixture density is equivalent to a multi-state with single-mixture densities <ref> [1] </ref>, for simplicity reason, let us consider without loss of generality only the case that there is 1 mixture in each state observation pdf. <p> Now suppose in each subspace k, subspace pdf's are clustered into L pdf codewords h kl () where 1 l L. That is, 8 0 fl; 9l 2 <ref> [1; L] </ref> such that b jk () = h kl ().
Reference: [2] <author> E. Bocchieri and B. Mak. </author> <title> Subspace Distribution Clustering for Continuous Observation Density Hidden Markov Models. </title> <booktitle> In Proceedings of Eurospeech, </booktitle> <volume> volume 1, </volume> <pages> pages 107 110, </pages> <year> 1997. </year>
Reference-contexts: Reestimation of b in SDCHMM The state observation pdf for each state, j, is assumed to be a mixture density with M components, b jm , and mixture weights, c m , 1 m M . Then by the definition of SDCHMM <ref> [2, 7] </ref> with K locally independent streams, we have b j (o t ) = m=1 K Y b jmk (o tk ) (6) where b jmk and o tk are the projections of b jm and o t onto the k-th feature subspace (or stream).
Reference: [3] <editor> D. Dahl et al. </editor> <title> Expanding the Scope of the ATIS Task: The ATIS-3 Corpus. </title> <booktitle> Proceedings of ARPA Human Language Technology Workshop, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: EVALUATION ON ATIS We test the hypothesis that SDCHMMs should require less data to train than CDHMMs with the ARPA-ATIS <ref> [3] </ref> recognition task. ATIS (Airline Travel Information Service) is a medium-vocabulary task containing spontaneous goal-directed speech for air travel information queries. 4.1.
Reference: [4] <author> X. Huang and M.A. Jack. </author> <title> Semi-continuous Hidden Markov Models for Speech Signals. </title> <journal> Journal of Computer Speech and Language, </journal> <volume> 3:239251, </volume> <year> 1989. </year>
Reference-contexts: In the past the technique of parameter tying has been applied successfully to obtain such a balance by reducing the number of parameters in acoustic models at various granularities: phone (generalized biphones/triphones [6], context-independent phones), state (tied-state HMM [10]), observation distribution (tied-mixture/semi-continuous HMM <ref> [4] </ref>) and distribution parameters [9] have all been tied. In the past, our subspace distribution clustering hidden Markov model (SDCHMM) was mainly presented as an approximation to the continuous density hidden Markov model (CDHMM).
Reference: [5] <author> B.H. Juang and L.R. Rabiner. </author> <title> A Segmental K-means Algorithm for Estimating Parameters of Hidden Markov Models. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 38(9):16391641, </volume> <month> September </month> <year> 1990. </year>
Reference-contexts: The bootstrapped CDHMMs or SDCHMMs are used to perform state-level segmentation for the set of training data under experiment. The segmented data are then used to reestimate HMM parameters in the segmental k-means training (SKM) procedure <ref> [5] </ref>. The SKM procedure is repeated at most twice to obtain the final CDHMMs or SDCHMMs. However, for SDCHMM training with subsets A, B, C and D, only SDCHMM bootstrapping is done to obtain the final SDCHMMs as more SKM iterations do not improve results.
Reference: [6] <author> K.F. Lee. </author> <title> Context-Dependent Phonetic Hidden Markov Models for Speaker-Independent Continuous Speech Recognition. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 38(4):599609, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: In the past the technique of parameter tying has been applied successfully to obtain such a balance by reducing the number of parameters in acoustic models at various granularities: phone (generalized biphones/triphones <ref> [6] </ref>, context-independent phones), state (tied-state HMM [10]), observation distribution (tied-mixture/semi-continuous HMM [4]) and distribution parameters [9] have all been tied. In the past, our subspace distribution clustering hidden Markov model (SDCHMM) was mainly presented as an approximation to the continuous density hidden Markov model (CDHMM).
Reference: [7] <author> B. Mak, E. Bocchieri, and E. Barnard. </author> <title> Stream Derivation and Clustering Schemes for Subspace Distribution Clustering HMM. </title> <booktitle> In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: Reestimation of b in SDCHMM The state observation pdf for each state, j, is assumed to be a mixture density with M components, b jm , and mixture weights, c m , 1 m M . Then by the definition of SDCHMM <ref> [2, 7] </ref> with K locally independent streams, we have b j (o t ) = m=1 K Y b jmk (o tk ) (6) where b jmk and o tk are the projections of b jm and o t onto the k-th feature subspace (or stream).
Reference: [8] <author> L. Rabiner and B.H. Juang. </author> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: In this paper, we present the reestimation formulas of SD-CHMM parameters and show how to train SDCHMMs from speech data without intermediate CDHMMs. 2. REESTIMATION FORMULAS OF SDCHMM SDCHMM parameters may be reestimated using the EM algorithm in much the same way as CDHMM parameters <ref> [8] </ref>. 2.1.
Reference: [9] <author> S. Takahashi and S. Sagayama. </author> <title> Four-Level Tied-Structure for Efficient Representation of Acoustic Modeling. </title> <booktitle> In Proceedings of ICASSP, </booktitle> <volume> volume I, </volume> <pages> pages 520523, </pages> <year> 1995. </year>
Reference-contexts: In the past the technique of parameter tying has been applied successfully to obtain such a balance by reducing the number of parameters in acoustic models at various granularities: phone (generalized biphones/triphones [6], context-independent phones), state (tied-state HMM [10]), observation distribution (tied-mixture/semi-continuous HMM [4]) and distribution parameters <ref> [9] </ref> have all been tied. In the past, our subspace distribution clustering hidden Markov model (SDCHMM) was mainly presented as an approximation to the continuous density hidden Markov model (CDHMM).
Reference: [10] <author> S.J. Young and P.C. Woodland. </author> <title> The Use of State Tying in Continuous Speech Recogniser. </title> <booktitle> In Proceedings of Eu-rospeech, </booktitle> <pages> pages 22032206, </pages> <year> 1993. </year>
Reference-contexts: In the past the technique of parameter tying has been applied successfully to obtain such a balance by reducing the number of parameters in acoustic models at various granularities: phone (generalized biphones/triphones [6], context-independent phones), state (tied-state HMM <ref> [10] </ref>), observation distribution (tied-mixture/semi-continuous HMM [4]) and distribution parameters [9] have all been tied. In the past, our subspace distribution clustering hidden Markov model (SDCHMM) was mainly presented as an approximation to the continuous density hidden Markov model (CDHMM).
References-found: 10

