URL: ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1520.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/cbcl/old-course9.520/jordan.html
Root-URL: 
Email: jordan@ai.mit.edu  
Title: On Convergence Properties of the EM Algorithm for Gaussian Mixtures  
Author: Lei Xu and Michael I. Jordan 
Note: This publication can be retrieved by anonymous ftp to publications.ai.mit.edu. Copyright c Massachusetts Institute of Technology, 1994  
Date: 1520 January 17, 1995  111  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES  
Pubnum: A.I. Memo No.  C.B.C.L. Paper No.  
Abstract: We build up the mathematical connection between the "Expectation-Maximization" (EM) algorithm and gradient-based approaches for maximum likelihood learning of finite Gaussian mixtures. We show that the EM step in parameter space is obtained from the gradient via a projection matrix P , and we provide an explicit expression for the matrix. We then analyze the convergence of EM in terms of special properties of P and provide new results analyzing the effect that P has on the likelihood surface. Based on these mathematical results, we present a comparative discussion of the advantages and disadvantages of EM and other algorithms for the learning of Gaussian mixture models. This report describes research done at the Center for Biological and Computational Learning and the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the Center is provided in part by a grant from the National Science Foundation under contract ASC-9217041. Support for the laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N00000-00-A-0000. The authors were also supported by the HK RGC Earmarked Grant CUHK250/94E, by a grant from the McDonnell-Pew Foundation, by a grant from ATR Human Information Processing Research Laboratories, by a grant from Siemens Corporation, and by grant N00014-90-1-0777 from the Office of Naval Research. Michael I. Jordan is an NSF Presidential Young Investigator. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S. </author> <title> (in press) Information geometry of the EM and em algorithms for neural networks, Neural Networks. </title>
Reference: <author> Baum, L.E., and Sell, G.R. </author> <year> (1968), </year> <title> Growth transforma-tion for functions on manifolds, Pac. </title> <journal> J. Math., </journal> <volume> 27, </volume> <pages> 211227. </pages>
Reference: <author> Bengio, Y., and Frasconi, P., </author> <year> (1995), </year> <title> An input-output HMM architecture. </title> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <editor> eds., Tesauro, G., Touretzky, D.S., and Alspector, J., </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Boyles, R.A. </author> <year> (1983), </year> <title> On the convergence of the EM algo-rithm, </title> <journal> J. of Royal Statistical Society, B45, </journal> <volume> No.1, </volume> <pages> 47-50. </pages>
Reference: <author> Dempster, A.P., Laird, N.M., and Rubin, D.B. </author> <year> (1977), </year> <title> Maximum likelihood from incomplete data via the EM algorithm, </title> <journal> J. of Royal Statistical Society, B39, </journal> <pages> 1-38. </pages>
Reference-contexts: Given K and given N independent, identically distributed samples fx (t) g N 1 , we obtain the following log 1 likelihood: 2 l (fi) = log t=1 N X log P (x (t) jfi); (2) which can be optimized via the following iterative algo <p>- rithm <ref> (see, e.g, Dempster, Laird & Rubin, 1977) </ref>: ff j = t=1 h j (t) (3) (k+1) P N (k) P N (k) j = t=1 h j (t)[x (t) m j ][x (t) m j ] T t=1 h j (t)x (t) where the posterior probabilities h (k) j are defined
Reference: <author> Ghahramani, Z, and Jordan, M.I. </author> <year> (1994), </year> <title> Function ap-proximation via density estimation using the EM ap-proach, </title> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <editor> eds., Cowan, J.D., Tesauro, G., and Alspector, J., </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 120-127. </pages>
Reference: <author> Jordan, M.I. and Jacobs, R.A. </author> <year> (1994), </year> <title> Hierarchical mix-tures of experts and the EM algorithm. </title> <booktitle> Neural Computation 6, </booktitle> <pages> 181-214. </pages>
Reference-contexts: We also provide new results suggesting that under appropriate conditions EM may in fact approximate a superlin- ear method; this would explain some of the promising empirical results that have been obtained <ref> (Jordan & Ja- cobs, 1994) </ref>, and would further temper the critique of EM offered by Redner and Walker. The analysis in the current paper focuses on unsupervised learning; for related results in the supervised learning domain see Jordan and Xu (in press).
Reference: <author> Jordan, M.I. and Xu, L. </author> <title> (in press), Convergence results for the EM approach to mixtures-of-experts architec-tures, Neural Networks. </title>
Reference: <author> Levinson, S.E., Rabiner, </author> <title> L.R., and Sondhi, M.M. (1983), An introduction to the application of the theory of 4 In most applications of HMM's, the "parameter estima-tion" process is employed solely to yield models with high likelihood; the parameters are not generally endowed with a particular meaning. 8 probabilistic functions of Markov process to automatic speech recognition, </title> <journal> The Bell System Technical Journal, </journal> <volume> 62, </volume> <pages> 1035-1072. </pages>
Reference-contexts: They showed that the search direction of this recursive formula, i.e., T (x (k) ) x (k) , has a positive projection on the gradient of of J with respect to the x (k) <ref> (see also Levinson, Ra- biner & Sondhi, 1983) </ref>. It can be shown that Baum and Sell's recursive formula implies the EM update formula for A in a Gaussian mixture. Thus, the first statement in Theorem 1 is a special case of Baum and Sell's earlier work.
Reference: <author> Neal, R. N. and Hinton, G. E. </author> <year> (1993), </year> <title> A new view of the EM algorithm that justifies incremental and other vari-ants, </title> <institution> University of Toronto, Department of Computer Science preprint. </institution>
Reference-contexts: The popularity of gradient descent algorithms for neural networks is in part to the ease of obtaining on-line variants of gradient descent. It is worth noting that on-line variants of the EM algorithm can be derived <ref> (Neal & Hinton, 1993, Titterington, 1984) </ref>, and this is a further factor that weighs in favor of EM as compared to conjugate gradient and Newton methods. 5 Convergence rate comparisons In this section, we provide a comparative theoretical discussion of the convergence rates of constrained gradient ascent and EM.
Reference: <author> Nowlan, S.J. </author> <year> (1991). </year> <title> Soft competitive adaptation: Neural network learning algorithms based on fitting statistical mixtures. </title> <type> Tech. Rep. </type> <address> CMU-CS-91-126, CMU, Pittsburgh, PA. </address>
Reference: <author> Redner, R.A., and Walker, H.F. </author> <year> (1984), </year> <title> Mixture densi-ties, maximum likelihood, and the EM algorithm, </title> <journal> SIAM Review 26, </journal> <pages> 195-239. </pages>
Reference: <author> Titterington, D.M. </author> <year> (1984), </year> <title> Recursive parameter estima-tion using incomplete data, </title> <journal> J. of Royal Statistical Society, </journal> <volume> B46, </volume> <pages> 257-267. </pages>
Reference: <author> Tresp, V, Ahmad, S. and Neuneier, R. </author> <year> (1994), </year> <title> Training neural networks with deficient data, </title> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <editor> eds., Cowan, J.D., Tesauro, G., and Alspector, J., </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 128-135. </pages>
Reference: <author> Waterhouse, S. R., and Robinson, A. J., </author> <year> (1994), </year> <title> Classification using hierarchical mixtures of experts, </title> <booktitle> in IEEE Workshop on Neural Networks for Signal Processing. </booktitle>
Reference: <author> Wu. C.F. J. </author> <year> (1983), </year> <title> On the convergence properties of the EM algorithm, </title> <journal> The Annals of Statistics, </journal> <volume> 11, </volume> <pages> 95-103. </pages>
Reference: <author> Xu, L., and Jordan, M.I. </author> <year> (1993a), </year> <title> Unsupervised learning by EM algorithm based on finite mixture of Gaussians, </title> <booktitle> Proc. of WCNN'93, Portland, OR, </booktitle> <volume> Vol. II, </volume> <pages> 431-434. </pages>
Reference: <author> Xu, L., and Jordan, M.I. </author> <year> (1993b), </year> <title> EM learning on a generalized finite mixture model for combining multiple classifiers, </title> <booktitle> Proc. of WCNN'93, </booktitle> <address> Portland, OR, </address> <booktitle> Vol. IV, </booktitle> <pages> 227-230. </pages>
Reference: <author> Xu, L., and Jordan, M.I. </author> <year> (1993c), </year> <title> Theoretical and ex-perimental studies of the EM algorithm for unsupervised learning based on finite Gaussian mixtures, MIT Computational Cognitive Science, </title> <type> Technical Report 9302, </type> <institution> Dept. of Brain and Cognitive Science, MIT, </institution> <address> Cambridge, MA. </address>
Reference: <author> Xu, L., Jordan, M.I. and Hinton, G.E. </author> <year> (1994), </year> <title> A Modified gating network for the mixtures of experts architec-ture, </title> <booktitle> Proc. of WCNN'94, San Diego, </booktitle> <volume> Vol. 2, </volume> <pages> 405-410. </pages>
Reference: <author> Yuille, A. L., Stolorz, P. & Utans, J. </author> <year> (1994), </year> <title> Statistical physics, mixtures of distributions and the EM algorithm, </title> <booktitle> Neural Computation 6, </booktitle> <pages> 334-340. 9 </pages>
References-found: 21

