URL: ftp://ftp.ics.uci.edu/pub/CSP-repository/papers/connect_energy_min.ps
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: pinkas@cs.wustl.edu  
Title: On Improving Connectionist Energy Minimization  
Author: Gadi Pinkas Rina Dechter 
Address: Box 1045, St Louis, MO 63130,  Irvine, CA 92717  
Affiliation: Washington University,  Department of Information and Computer Science, University of California,  
Note: Center for Optimization and Semantic Control,  
Date: November 28, 1994  
Abstract: Symmetric networks designed for energy minimization such as Boltz-man machines and Hopfield nets are used frequently for optimization, constraint satisfaction and approximation of NP-hard problems. Nevertheless, finding a global solution (i.e., a global minimum for the energy function) is not guaranteed and even a local solution may take an exponential number of steps. We propose an improvement to the standard local activation function used for such networks. The improved algorithm guarantees that a global minimum is found in linear time for tree-like subnetworks. The algorithm is uniform and does not assume that the network is tree-like. It can identify tree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid local minima along these trees. For acyclic networks, the algorithm is guaranteed to converge to a global minimum from any initial state of the system (self-stabilization) and remains correct under various types of schedulers. For general (cyclic) topologies, we show how our tree-like algorithm can be extended using the cycle-cutset idea. The general algorithm optimizes tree-like subnetworks and has some performance guarantees that are related to the size of the network's cycle-cutset. In any case, the algorithm performs no worse than the standard algorithms. On the negative side, we show that in the presence of cycles, no uniform algorithm 
Abstract-found: 1
Intro-found: 1
Reference: [Ballard et al. 86] <author> D. H. Ballard, P. C. Gardner, M. A. Srinivas, </author> <title> "Graph problems and connectionist architectures," </title> <institution> University of Rochester, </institution> <type> Technical Report 167, </type> <year> 1986. </year>
Reference: [Becker, Geiger, 1994] <author> A. Becker and D. Geiger, </author> <booktitle> "Approximation algorithms for loop cutset problems" Proceedings of the 10th conference on Uncertainty in Artificial Intelligence (UAI-94), </booktitle> <pages> pp. 60-68, </pages> <address> Seat-tle Washington, </address> <month> August, </month> <year> 1994. </year>
Reference-contexts: Clearly the problem is unsolvable for completely uniform networks. For an almost uniform network, when just one node is allowed to execute a different algorithm, one can envision a algorithm that may simulate one of the approximation algorithms for cycle-cutset <ref> [Becker, Geiger, 1994] </ref>. This is one of the open issues that we leave for future work. 5.3 A local search with dynamic cutset values Another approach is to use the cutset method to improve local search without trying to enumerate all possible cutset values.
Reference: [Bertele, Brioschi 72] <author> U. Bertele, F. Brioschi, </author> <title> "Nonserial dynamic programming," </title> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: We then extend it to general topologies by dividing the network into fictitious tree-like subnetworks. Partially, the algorithm is based on a dynamic programming algorithm that belongs to the family of nonserial dynamic programming methods <ref> [Bertele, Brioschi 72] </ref>. This dynamic programming method was adapted also in [Dechter et al 90] for constraint optimization. <p> The positive results suggests improvements both to connectionist activation functions and to local repair techniques. 8 Appendix Proof sketch: of theorem 4.3: The second and third phases of the algorithm are adaptations of an existing dynamic programming algorithm <ref> [Bertele, Brioschi 72] </ref>, and their correctness is therefore not proved here. The self-stabilization of these steps is obvious because no variables are initialized. The proof therefore concentrates on the tree directing phase.
Reference: [Brandt et al. 88] <author> R. D. Brandt, Y. Wang, A. J. Laub, S. K. Mitra, </author> <title> "Alternative networks for solving the traveling salesman problem and the list-matching problem," </title> <booktitle> in IEEE International Conference on Neural Networks, </booktitle> <volume> Vol 2,pp. </volume> <pages> 333-340, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Symmetric networks such as Hopfield networks, Boltzmann machines, mean-field and Harmony networks are frequently used for optimization, constraint satisfaction and approximation of NP-hard problems [Hopfield 82], [Hopfield 84], [Hinton, Sejnowski 86], [Peterson, Hartman 89], [Smolensky 86], <ref> [Brandt et al. 88] </ref>. These models are characterized by a symmetric matrix of weights and a quadratic energy function that should be minimized. Usually, each unit computes the gradient of the energy function and updates its own activation value so that the free energy decreases gradually.
Reference: [Collin et al. 91] <author> Z. Collin, R. Dechter, S. Katz, </author> <title> "On the feasibility of distributed constraint satisfaction," </title> <booktitle> Proceedings of IJCAI, </booktitle> <address> Sydney, </address> <year> 1991. </year>
Reference-contexts: Our problem differs (from general leader selection problems) in that the network is a tree. In addition we require our algorithms to be self-stabilized. A related self-stabilizing algorithm appears in <ref> [Collin et al. 91] </ref>. That algorithm is based on finding a center of the tree as the root node and therefore creates more balanced trees. <p> The advantage of the algorithm presented here is that it 5 is space efficient requiring only O (logd) space, when d is the maximum num-ber of neighbors each node has. In contrast, the algorithm in <ref> [Collin et al. 91] </ref> requires O (logn), n being the network size. In the algorithm we present, each unit uses one bit per neighbor to keep the pointing information: P j i = 1 indicates that node i sees its jth neighbor as its parent. <p> generated by a distributed sched-uler, what is impossible for the central scheduler is also impossible for the distributed scheduler. 2 The same results are obtained if the steps of the algorithm executes as one atomic operation or if neighbors are mutually excluded. 12 4.1 Negative results for uniform algorithms In <ref> [Collin et al. 91] </ref> following [Dijkstra 74] negative results are presented regarding the feasibility of distributed constraint satisfaction. Since constraint satisfaction problems can be formulated as energy minimization problems, these feasibility results apply also for computing the global minimum of energy functions. <p> executed without j infinitely often (fair exclusion); 4 3. if one node is unique and acts as a root, that is, does not execute step 2 (an almost uniform algorithm); 4. if the network is cyclic (one node will be acting as a root). 5 Another negative result similar to <ref> [Collin et al. 91] </ref> is given in the following theorem. Theorem 4.2 If the network is cyclic, no deterministic uniform algorithm exists that guarantees a global minimum, even under a central scheduler, assuming that the algorithm needs to be insensitive to initial conditions.
Reference: [Dechter 90] <author> Dechter, R., </author> <title> "Enhancement schemes for constraint processing: Backjumping, learning and cutset decomposition," </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 41(3), </volume> <pages> pp. 273-312, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: In the following paragraphs we will discuss the principles underlying this method. A full account of this extension is deferred for future work. 16 Two well-known complementary schemes for extending tree algorithms to non-tree networks, are tree clustering, and cycle-cutset decomposition <ref> [Dechter 90] </ref> used both in Bayes networks and constraint networks. In tree clustering the idea is to combine subsets of variables into higher level meta variables until the interaction between the meta level variables is tree-like. <p> The complexity of the cycle-cutset method can be bounded exponentially in the size of the cutset set in each nonseperable component of the graph <ref> [Dechter 90] </ref>. <p> The overall maxima is achieved by enumerating over all possible assignments to Y . As a matter of fact, enumeration can be restricted to each nonseparable component and to each of its own cutset since the hyper structure between components is tree-like <ref> [Dechter 90] </ref>, [Even, 79]. Obviously, this scheme is effective only when the cycle-cutset is small. In section 5.2 we discuss some steps toward implementing this idea in a distributed environment.
Reference: [Dechter 90] <author> Dechter, R., </author> <title> "Constraint networks." </title> <booktitle> In Encyclopedia of Artificial Intelligence, 2nd ed., </booktitle> <publisher> John Wiley & Sons, Inc., </publisher> <pages> pp 276-285, </pages> <year> 1992. </year>
Reference-contexts: In the following paragraphs we will discuss the principles underlying this method. A full account of this extension is deferred for future work. 16 Two well-known complementary schemes for extending tree algorithms to non-tree networks, are tree clustering, and cycle-cutset decomposition <ref> [Dechter 90] </ref> used both in Bayes networks and constraint networks. In tree clustering the idea is to combine subsets of variables into higher level meta variables until the interaction between the meta level variables is tree-like. <p> The complexity of the cycle-cutset method can be bounded exponentially in the size of the cutset set in each nonseperable component of the graph <ref> [Dechter 90] </ref>. <p> The overall maxima is achieved by enumerating over all possible assignments to Y . As a matter of fact, enumeration can be restricted to each nonseparable component and to each of its own cutset since the hyper structure between components is tree-like <ref> [Dechter 90] </ref>, [Even, 79]. Obviously, this scheme is effective only when the cycle-cutset is small. In section 5.2 we discuss some steps toward implementing this idea in a distributed environment.
Reference: [Dechter et al 90] <author> R. Dechter, A. Dechter, J. Pearl, </author> <title> "Optimization in constraint networks," in R.M. Oliver, J.Q. Smith, Influence diagrams, belief nets and decision analysis, </title> <publisher> John Wiley and Sons, </publisher> <year> 1990. </year>
Reference-contexts: We then extend it to general topologies by dividing the network into fictitious tree-like subnetworks. Partially, the algorithm is based on a dynamic programming algorithm that belongs to the family of nonserial dynamic programming methods [Bertele, Brioschi 72]. This dynamic programming method was adapted also in <ref> [Dechter et al 90] </ref> for constraint optimization. Our adaptation is connectionist in style; i.e., the algorithm can be stated as a simple, uniform activation function [Rumelhart et al. 86], [Feldman, Ballard 82] and it can be executed in parallel architectures using synchronous or asynchronous scheduling policies.
Reference: [Dijkstra 74] <author> E.W. Dijkstra, </author> <title> "Self-stabilizing systems in spite of distributed control," </title> <journal> Communications of the ACM 17, </journal> <volume> no. 11, </volume> <pages> pp. 643-644, </pages> <year> 1974. </year>
Reference-contexts: what is impossible for the central scheduler is also impossible for the distributed scheduler. 2 The same results are obtained if the steps of the algorithm executes as one atomic operation or if neighbors are mutually excluded. 12 4.1 Negative results for uniform algorithms In [Collin et al. 91] following <ref> [Dijkstra 74] </ref> negative results are presented regarding the feasibility of distributed constraint satisfaction. Since constraint satisfaction problems can be formulated as energy minimization problems, these feasibility results apply also for computing the global minimum of energy functions.
Reference: [Even, 79] <author> S. </author> <title> Even "Graph Algorithms" Computer Science Press, </title> <year> 1979. </year>
Reference-contexts: The overall maxima is achieved by enumerating over all possible assignments to Y . As a matter of fact, enumeration can be restricted to each nonseparable component and to each of its own cutset since the hyper structure between components is tree-like [Dechter 90], <ref> [Even, 79] </ref>. Obviously, this scheme is effective only when the cycle-cutset is small. In section 5.2 we discuss some steps toward implementing this idea in a distributed environment.
Reference: [Feldman, Ballard 82] <author> J.A Feldman, D.H. Ballard, </author> <title> "Connectionist models and their properties," </title> <booktitle> Cognitive Science 6, </booktitle> <year> 1982. </year>
Reference-contexts: This dynamic programming method was adapted also in [Dechter et al 90] for constraint optimization. Our adaptation is connectionist in style; i.e., the algorithm can be stated as a simple, uniform activation function [Rumelhart et al. 86], <ref> [Feldman, Ballard 82] </ref> and it can be executed in parallel architectures using synchronous or asynchronous scheduling policies. It does not assume the desired topology (acyclic) and performs no worse than the standard local algorithms for all topologies.
Reference: [Hinton, Sejnowski 86] <author> G.E Hinton and T.J. Sejnowski, </author> <title> "Learning and relearning in Boltzmann Machines," </title> <editor> in J. L. McClelland and D. E. Rumelhart, </editor> <booktitle> Parallel Distributed Processing: Explorations in The Microstructure of Cognition I, </booktitle> <pages> pp. 282-317, </pages> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: 1 Introduction Symmetric networks such as Hopfield networks, Boltzmann machines, mean-field and Harmony networks are frequently used for optimization, constraint satisfaction and approximation of NP-hard problems [Hopfield 82], [Hopfield 84], <ref> [Hinton, Sejnowski 86] </ref>, [Peterson, Hartman 89], [Smolensky 86], [Brandt et al. 88]. These models are characterized by a symmetric matrix of weights and a quadratic energy function that should be minimized. <p> An algorithm is uniform if it is executed by all the units. We give examples for two of the most popular activation functions for connectionist energy minimization: the discrete Hopfield network [Hopfield 82] and the Boltzmann machine <ref> [Hinton, Sejnowski 86] </ref>.
Reference: [Hopfield 82] <author> J. J. </author> <title> Hopfield "Neural networks and physical systems with emergent collective computational abilities," </title> <booktitle> Proceedings of the National Academy of Sciences 79, </booktitle> <pages> pp. 2554-2558, </pages> <year> 1982. </year>
Reference-contexts: 1 Introduction Symmetric networks such as Hopfield networks, Boltzmann machines, mean-field and Harmony networks are frequently used for optimization, constraint satisfaction and approximation of NP-hard problems <ref> [Hopfield 82] </ref>, [Hopfield 84], [Hinton, Sejnowski 86], [Peterson, Hartman 89], [Smolensky 86], [Brandt et al. 88]. These models are characterized by a symmetric matrix of weights and a quadratic energy function that should be minimized. <p> The algorithm that is repeatedly executed in each unit/node is called the activation function. An algorithm is uniform if it is executed by all the units. We give examples for two of the most popular activation functions for connectionist energy minimization: the discrete Hopfield network <ref> [Hopfield 82] </ref> and the Boltzmann machine [Hinton, Sejnowski 86]. <p> When the information reaches the root, it can assign a value (0/1) that maximizes the 1 Standard algorithms need to assume the same condition in order to guarantee convergence to a local minimum (see <ref> [Hopfield 82] </ref>). This condition can be relaxed by restricting that only adjacent nodes are not activated at the same time (mutual exclusion). 4 goodness of the whole network. The assignment information propagates now toward the leaves.
Reference: [Hopfield 84] <author> J. J. </author> <title> Hopfield "Neurons with graded response have collective computational properties like those of two-state neurons," </title> <booktitle> Proceedings of the National Academy of Sciences 81, </booktitle> <pages> pp. 3088-3092, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction Symmetric networks such as Hopfield networks, Boltzmann machines, mean-field and Harmony networks are frequently used for optimization, constraint satisfaction and approximation of NP-hard problems [Hopfield 82], <ref> [Hopfield 84] </ref>, [Hinton, Sejnowski 86], [Peterson, Hartman 89], [Smolensky 86], [Brandt et al. 88]. These models are characterized by a symmetric matrix of weights and a quadratic energy function that should be minimized.
Reference: [Hopfield, Tank 85] <author> J.J. Hopfield, D.W. </author> <title> Tank "Neural Computation of Decisions in Optimization Problems," </title> <booktitle> Biological Cybernetics 52, </booktitle> <pages> pp. 144-152. </pages>
Reference-contexts: In many cases the problem at hand is formulated as a minimization problem and the best solutions (sometimes the only solutions) are the global minima <ref> [Hopfield, Tank 85] </ref>,[Ballard et al. 86], [Pinkas 90b]. The desired algorithm is therefore one that manages to reduce the impact of shallow local minima and improve the chances of finding a global minimum. Some models such as Boltzmann machines and Harmony nets, use simulated annealing to escape from local minima.
Reference: [Kasif et al. 89] <author> S. Kasif, S. Banerjee, A. Delcher, G. </author> <title> Sullivan "Some results on the computational complexity of symmetric connectionist networks," </title> <institution> Department of Computer Science, The John Hopkins University, </institution> <type> Technical Report JHU/CS-89/10, </type> <year> 1989. </year>
Reference-contexts: Usually, each unit computes the gradient of the energy function and updates its own activation value so that the free energy decreases gradually. Convergence to a local minimum is guaranteed although in the worst case it is exponential in the number of units <ref> [Kasif et al. 89] </ref>, [Papadimitriou et al. 90]. In many cases the problem at hand is formulated as a minimization problem and the best solutions (sometimes the only solutions) are the global minima [Hopfield, Tank 85],[Ballard et al. 86], [Pinkas 90b].
Reference: [Korach et. al, 84] <author> K. Korach, D. Rotem, and N. </author> <title> Santoro "Distributed algorithms for finding centers and medians in networks" ACM transaction on Programming Languages and Systems 6(3) </title> <type> 380-401, </type> <month> July </month> <year> 1984. </year>
Reference-contexts: Finally, a node that has at least two neighbors not pointing toward it, identifies itself as being outside the tree. The problem of directing a tree is related to the problem of selecting a leader in a distributed network, and of selecting a center in a tree <ref> [Korach et. al, 84] </ref>. Our problem differs (from general leader selection problems) in that the network is a tree. In addition we require our algorithms to be self-stabilized. A related self-stabilizing algorithm appears in [Collin et al. 91].
Reference: [Lehmann, Magidor 88] <author> D. Lehmann, M. Magidor, </author> <title> "Rational logics and their models: A study in cumulative logic", </title> <type> Technical Report, </type> <institution> TR-86-16, Leibnitz Center for Computer Science, Hebrew University, Jerusalem, </institution> <year> 1988. </year>
Reference-contexts: This observation will be used later in our discussion of independence. 6.2.2 Semantics We can use a formal logic and semantics called penalty logic [Pinkas 94], [Pinkas 91] that is based on ranked models [Shoham 88], [Pearl 90], <ref> [Lehmann, Magidor 88] </ref>. In this semantics, models are ranked according to a score computed by summing the penalties associated with violating the rules. Propositions that are held in all the models of minimal penalty are concluded.
Reference: [McClelland et al. 86] <author> J. L. McClelland, D. E. Rumelhart, G.E Hinton, </author> <title> "The appeal of PDP," </title> <editor> in J. L. McClelland and D. E. Rumelhart, </editor> <title> Parallel Distributed Processing: Explorations in The Microstructure of Cognition I, </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Standard algorithms may enter such local minimum and stay in a stable state that is clearly wrong. The example is a variation on a Harmony network example [Smolensky 86] (page 259), and an example from <ref> [McClelland et al. 86] </ref> (page 22). The task of the network is to identify words from low-level line segments. Certain patterns of line segments excite units that represent characters, and certain patterns of characters excite units that represent words.
Reference: [Minton et al 90] <author> S. Minton, M. D. Johnson, A. B. Phillips, </author> <title> "Solving large scale constraint satisfaction and scheduling problems using a heuristic repair method,", </title> <booktitle> Proceedings of the Eighth Conference on Artificial Intelligence, </booktitle> <pages> pp. 17-24, </pages> <year> 1990. </year>
Reference-contexts: Recently, such local repair algorithms were successfully used on various large-scale hard problems such as 3-SAT, n-queen, scheduling and constraint satisfaction <ref> [Minton et al 90] </ref>, [Selman et al. 92]. Since local repair algorithms may be viewed as sequential variations on the energy paradigm, it is reasonable to assume that improvement to energy minimization will also be applicable to local-repair algorithms.
Reference: [Papadimitriou et al. 90] <author> C. Papadimitriou, A. Shaffer, M. Yannakakis, </author> <title> "On the complexity of local search," </title> <booktitle> ACM Symposium on the Theory of Computation, </booktitle> <pages> pp. 438-445, </pages> <year> 1990. </year>
Reference-contexts: Usually, each unit computes the gradient of the energy function and updates its own activation value so that the free energy decreases gradually. Convergence to a local minimum is guaranteed although in the worst case it is exponential in the number of units [Kasif et al. 89], <ref> [Papadimitriou et al. 90] </ref>. In many cases the problem at hand is formulated as a minimization problem and the best solutions (sometimes the only solutions) are the global minima [Hopfield, Tank 85],[Ballard et al. 86], [Pinkas 90b].
Reference: [Pearl 88] <author> J. </author> <title> Pearl "Probabilistic Reasoning in Intelligent Systems: </title> <publisher> Networks of Plausible Inference" Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference-contexts: Independency assumptions of this kind are quite common in actual implementations of Bayes networks, influence diagrams <ref> [Pearl 88] </ref>, and certainty propagation of rule-based expert systems [Shortliffe 76]. 7 Summary We have shown a uniform self-stabilizing connectionist activation function that is guaranteed to find a global minimum of acyclic symmetric networks in linear time under a realistically weakened distributed scheduler.
Reference: [Pearl 90] <author> J. Pearl, </author> <title> "System Z: A natural ordering of defaults with tractable applications to nonmonotonic reasoning," </title> <booktitle> in Proceedings of the Workshop on Theoretical aspects of Knowledge Representation (TARC), </booktitle> <editor> M. Vardi (ed.), </editor> <booktitle> pp. </booktitle> <pages> 121-135, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: This observation will be used later in our discussion of independence. 6.2.2 Semantics We can use a formal logic and semantics called penalty logic [Pinkas 94], [Pinkas 91] that is based on ranked models [Shoham 88], <ref> [Pearl 90] </ref>, [Lehmann, Magidor 88]. In this semantics, models are ranked according to a score computed by summing the penalties associated with violating the rules. Propositions that are held in all the models of minimal penalty are concluded.
Reference: [Peterson, Hartman 89] <author> C. Peterson, E. Hartman, </author> <title> "Explorations of mean field theory learning algorithm," </title> <booktitle> Neural Networks 2, </booktitle> <volume> no. 6, </volume> <year> 1989. </year> <month> 37 </month>
Reference-contexts: 1 Introduction Symmetric networks such as Hopfield networks, Boltzmann machines, mean-field and Harmony networks are frequently used for optimization, constraint satisfaction and approximation of NP-hard problems [Hopfield 82], [Hopfield 84], [Hinton, Sejnowski 86], <ref> [Peterson, Hartman 89] </ref>, [Smolensky 86], [Brandt et al. 88]. These models are characterized by a symmetric matrix of weights and a quadratic energy function that should be minimized.
Reference: [Pinkas 90b] <author> G. Pinkas, </author> <title> "Energy minimization and the satisfiability of propo-sitional calculus," </title> <booktitle> Neural Computation 3, </booktitle> <volume> no. 2, </volume> <year> 1991. </year>
Reference-contexts: In many cases the problem at hand is formulated as a minimization problem and the best solutions (sometimes the only solutions) are the global minima [Hopfield, Tank 85],[Ballard et al. 86], <ref> [Pinkas 90b] </ref>. The desired algorithm is therefore one that manages to reduce the impact of shallow local minima and improve the chances of finding a global minimum. Some models such as Boltzmann machines and Harmony nets, use simulated annealing to escape from local minima. <p> Arbitrary constraints on the nodes of the graph may be introduced in this model. Constraints may be represented as proposition logic formulas and then translated into energy terms using the method in <ref> [Pinkas 90b] </ref>. In a "pure" inheritance network that has no multiple inherited nodes and no nonmonotonic relationships, the network is cycle-free. If we allow multiple inheritance, nonmonotonicity, or arbitrary propositional constraints, we may introduce cycles into the network that are generated. <p> Any propositional logic formula is allowed and nonmonotonicity may be expressed using conflicting constraints (augmented with importance factors). Quadratic energy function may be generated from arbitrary propositional constraints by introducing hidden variables (see <ref> [Pinkas 90b] </ref> for details). Note that any variable X may be used as a symptom in some rules and as a hypothesis in other rules.
Reference: [Pinkas 91] <author> G. Pinkas, </author> <title> "Propositional non-monotonic reasoning and inconsistency in symmetric neural networks," </title> <booktitle> Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 525-530, </pages> <address> Sydney, </address> <year> 1991. </year>
Reference-contexts: Note that any variable X may be used as a symptom in some rules and as a hypothesis in other rules. This observation will be used later in our discussion of independence. 6.2.2 Semantics We can use a formal logic and semantics called penalty logic [Pinkas 94], <ref> [Pinkas 91] </ref> that is based on ranked models [Shoham 88], [Pearl 90], [Lehmann, Magidor 88]. In this semantics, models are ranked according to a score computed by summing the penalties associated with violating the rules. Propositions that are held in all the models of minimal penalty are concluded.
Reference: [Pinkas, Dechter 92] <author> G. Pinkas, R. Dechter, </author> <title> "A new improved activation function for energy minimization," </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI), </booktitle> <pages> pp. 434-439, </pages> <address> San Jose, </address> <year> 1992. </year>
Reference-contexts: Our negative results on energy minimization involve conditions on the parallel model of execution and are applicable only to the parallel versions of local repair (where each problem variable is allocated a processing unit). This paper is a revised version and an extension of <ref> [Pinkas, Dechter 92] </ref>. The paper is organized as follows: Section 2 discusses connectionist energy minimization. Section 3 presents the new algorithm and gives an example where it out-performs the standard local algorithms. Section 4 discusses negative results, convergence under various schedulers and self-stabilization.
Reference: [Pinkas 94] <author> G. Pinkas, </author> <title> "Reasoning, nonmonotonicity and learning in connectionist networks that capture propositional knowledge," </title> <note> to appear in Artificial Intelligence Journal. </note>
Reference-contexts: The connectionist network that represents the complete inheritance graph is obtained by summing the energy terms that correspond to all the ISA and HAS relationships in the graph. Nonmonotonicity can be expressed if we add penalties to arcs and use the semantics discussed in <ref> [Pinkas 94] </ref>. Nonmonotonic relationships may cause cycles in both the inheritance graph and the connectionist network (eg. Penguin ISA Bird; Bird ISA FlyingAnimal; Penguin ISA not (FlyingAnimal)). <p> Note that any variable X may be used as a symptom in some rules and as a hypothesis in other rules. This observation will be used later in our discussion of independence. 6.2.2 Semantics We can use a formal logic and semantics called penalty logic <ref> [Pinkas 94] </ref>, [Pinkas 91] that is based on ranked models [Shoham 88], [Pearl 90], [Lehmann, Magidor 88]. In this semantics, models are ranked according to a score computed by summing the penalties associated with violating the rules. Propositions that are held in all the models of minimal penalty are concluded. <p> A set of rules such as the above can be translated into an energy function and be implemented as a connectionist network. Penalty logic, its semantics, and the algorithms for translating formulas into energy functions are discussed in <ref> [Pinkas 94] </ref>. 6.2.3 Cycles, trees and independence assumption Assume for now that our knowledge base consists only of diagnosis rules (ff 1 X 1 ; :::; ff m X m ! fiX) and (optional) causal constraints of the type (X ! X i ).
Reference: [Rumelhart et al. 86] <author> D.E. Rumelhart, G.E Hinton, J.L. McClelland, </author> <title> "A general framework for parallel distributed processing," </title> <editor> in J. L. Mc-Clelland and D. E. Rumelhart, </editor> <title> Parallel Distributed Processing: Explorations in The Microstructure of Cognition I, </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This dynamic programming method was adapted also in [Dechter et al 90] for constraint optimization. Our adaptation is connectionist in style; i.e., the algorithm can be stated as a simple, uniform activation function <ref> [Rumelhart et al. 86] </ref>, [Feldman, Ballard 82] and it can be executed in parallel architectures using synchronous or asynchronous scheduling policies. It does not assume the desired topology (acyclic) and performs no worse than the standard local algorithms for all topologies.
Reference: [Selman et al. 92] <author> B. Selman, H. Levesque, D. Mitchell, </author> <title> "A new method for solving hard satisfiability problems," </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 440-446, </pages> <year> 1992. </year>
Reference-contexts: Recently, such local repair algorithms were successfully used on various large-scale hard problems such as 3-SAT, n-queen, scheduling and constraint satisfaction [Minton et al 90], <ref> [Selman et al. 92] </ref>. Since local repair algorithms may be viewed as sequential variations on the energy paradigm, it is reasonable to assume that improvement to energy minimization will also be applicable to local-repair algorithms.
Reference: [Shoham 88] <author> Y. Shoham, </author> <title> Reasoning about Change, </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: This observation will be used later in our discussion of independence. 6.2.2 Semantics We can use a formal logic and semantics called penalty logic [Pinkas 94], [Pinkas 91] that is based on ranked models <ref> [Shoham 88] </ref>, [Pearl 90], [Lehmann, Magidor 88]. In this semantics, models are ranked according to a score computed by summing the penalties associated with violating the rules. Propositions that are held in all the models of minimal penalty are concluded.
Reference: [Shortliffe 76] <author> E. H. Shortliffe, </author> <title> Computer-based medical consultation, Mycin, </title> <publisher> Elsevier, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: Independency assumptions of this kind are quite common in actual implementations of Bayes networks, influence diagrams [Pearl 88], and certainty propagation of rule-based expert systems <ref> [Shortliffe 76] </ref>. 7 Summary We have shown a uniform self-stabilizing connectionist activation function that is guaranteed to find a global minimum of acyclic symmetric networks in linear time under a realistically weakened distributed scheduler.
Reference: [Smolensky 86] <author> P. Smolensky, </author> <title> "Information processing in dynamical systems: Foundations of harmony theory," in J.L.McClelland and D.E.Rumelhart, Parallel Distributed Processing: Explorations in The Microstructure of Cognition I , MIT Press, </title> <booktitle> 1986. </booktitle> <pages> 38 </pages>
Reference-contexts: 1 Introduction Symmetric networks such as Hopfield networks, Boltzmann machines, mean-field and Harmony networks are frequently used for optimization, constraint satisfaction and approximation of NP-hard problems [Hopfield 82], [Hopfield 84], [Hinton, Sejnowski 86], [Peterson, Hartman 89], <ref> [Smolensky 86] </ref>, [Brandt et al. 88]. These models are characterized by a symmetric matrix of weights and a quadratic energy function that should be minimized. Usually, each unit computes the gradient of the energy function and updates its own activation value so that the free energy decreases gradually. <p> Standard algorithms may enter such local minimum and stay in a stable state that is clearly wrong. The example is a variation on a Harmony network example <ref> [Smolensky 86] </ref> (page 259), and an example from [McClelland et al. 86] (page 22). The task of the network is to identify words from low-level line segments. Certain patterns of line segments excite units that represent characters, and certain patterns of characters excite units that represent words.
References-found: 33

