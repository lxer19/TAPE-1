URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/ibc-fading-mem.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Title: Worst-Case Identification of Nonlinear Fading Memory Systems  
Author: Munther A. Dahleh David N.C. Tse Eduardo D. Sontag John N. Tsitsiklis 
Date: 3, March 1995.  
Note: Published in Automatica, 31, no.  Research Supported by AFOSR under grant AFOSR-91-0368 and by NSF under grant 9157306-ECS Research supported by an NSERC fellowship from the government of Canada, and by the NSF under Grant ECS-8552419 Research Supported by AFOSR under grant AFOSR-91-0346 Research supported by the NSF under Grant ECS-8552419 and by AFOSR under grant AFOSR-91-0368  
Address: Cambridge, MA 02139  Cambridge, MA 02139  New Brunswick, NJ 08903  Cambridge, MA 02139  
Affiliation: Lab. for Information Decision Systems M.I.T.  Lab. for Information Decision Systems M.I.T.  Dept. of Mathematics, SYCON Rutgers University  Lab. for Information Decision Systems M.I.T.  
Abstract: In this paper, the problem of asymptotic identification for fading memory systems in the presence of bounded noise is studied. For any experiment, the worst-case error is characterized in terms of the diameter of the worst-case uncertainty set. Optimal inputs that minimize the radius of uncertainty are studied and characterized. Finally, a convergent algorithm that does not require knowledge of the noise upper bound is furnished. The algorithm is based on interpolating data with spline functions, which are shown to be well suited for identification in the presence of bounded noise; more so than other basis functions such as polynomials. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Boyd and L.O. Chua, </author> <title> "Fading Memory and the problem of approximating nonlinear operators with Volterra series," </title> <journal> IEEE Transactions on Circuits and Systems, </journal> <volume> Vol. CAS-32, </volume> <year> 1985, </year> <pages> pp. 1150-1161. </pages>
Reference-contexts: u is any infinite sequence the first n elements of which are given by the finite sequence w.) 3. h has fading memory (FM): for each " &gt; 0 there is some T = T (") such that for every k, every t T and every finite sequences v 2 <ref> [1; 1] </ref> k , w 2 [1; 1] t , jh t+k (vw) h t (w)j &lt; " To measure the identification error, we shall consider the metric to be the one corresponding to the operator gain: khk X = sup kh (u)k 1 : It can be seen that systems <p> first n elements of which are given by the finite sequence w.) 3. h has fading memory (FM): for each " &gt; 0 there is some T = T (") such that for every k, every t T and every finite sequences v 2 <ref> [1; 1] </ref> k , w 2 [1; 1] t , jh t+k (vw) h t (w)j &lt; " To measure the identification error, we shall consider the metric to be the one corresponding to the operator gain: khk X = sup kh (u)k 1 : It can be seen that systems in X satisfying the above property <p> It is easily to verify that these systems satisfy the first two conditions above. If we assume further that g is uniformly continuous, then it can be seen that the system also has fading memory. For further details on fading memory operators, see <ref> [1, 15] </ref>. 3 Identification Setup The plant to be identified is known to be in a model set M X . An input u is selected from the set U . <p> Proof. Define the class of pth-order memory systems, M p , to be the set of all f such that for every k and for every t &gt; p and every finite sequences v 2 <ref> [1; 1] </ref> k ,w 2 [1; 1] t , f t+k (vw) = f t (w). It is clear that any fading memory system can be approximated (in the operator-induced norm) arbitrarily closely by a pth-order memory system for sufficiently large p. <p> Proof. Define the class of pth-order memory systems, M p , to be the set of all f such that for every k and for every t &gt; p and every finite sequences v 2 <ref> [1; 1] </ref> k ,w 2 [1; 1] t , f t+k (vw) = f t (w). It is clear that any fading memory system can be approximated (in the operator-induced norm) arbitrarily closely by a pth-order memory system for sufficiently large p. Hence it suffices to prove that M p is separable for all p. <p> Hence it suffices to prove that M p is separable for all p. Now given any f 2 M p we can find some continuous function g : <ref> [1; 1] </ref> p ! &lt; such that for all time n, and all input u, f n (u) = g (u np ; : : : ; u n1 ) We call g the memory function for f . <p> Hence we have that kf k = kgk 1 , where the infinity norm is taken over <ref> [1; 1] </ref> p . But the space of continuous functions with the uniform topology induced by the ` 1 -norm, denoted by C ([1; 1] p ), is separable, and hence so is M p . <p> The next theorem shows that they also exist when the model set consists of nonlinear fading memory systems. Theorem 5.2 Let the model set M be some subset of the set of fading memory systems. Let W be any countable dense subset of <ref> [1; 1] </ref> and consider any input ! 0 2 [1; 1] 1 which contains all possible finite sequences of elements of W . Then, ! 0 is persistently exciting. Proof. Assume that h 2 M; khk = K &lt; 1. Pick any " &gt; 0. <p> Theorem 5.2 Let the model set M be some subset of the set of fading memory systems. Let W be any countable dense subset of <ref> [1; 1] </ref> and consider any input ! 0 2 [1; 1] 1 which contains all possible finite sequences of elements of W . Then, ! 0 is persistently exciting. Proof. Assume that h 2 M; khk = K &lt; 1. Pick any " &gt; 0. Let T = T (") as in the definition of FM. <p> This is a generalization of the univariate linear spline, but because in higher dimension there is no natural ordering of the data points, the description of the interpolant is more complicated. Consider the cube I = <ref> [1; 1] </ref> p &lt; p . Let x 1 ; x 2 ; : : : ; x m ; m &gt; p be m points in the interior of the cube. <p> Given these simplices, we can now define our interpolating linear spline f as follows. First define f (x i ) = y i at the given data points. For other x 2 <ref> [1; 1] </ref> p , if x 2 S j for some j, let f (x) i ff i f (u i ), where u i 's are the vertices of S j and x = P i ff i u i . <p> when the algorithm is collecting data to compute an estimate in M p .) Consider all the blocks (u np+1 ; : : : ; u n1 ; u n ); 8n = n (p 1); : : :; n (p) in the input as data points in the cube <ref> [1; 1] </ref> p . We maintain a simplex structure in [1; 1] p with these data points as vertex set, and the structure is incrementally modified more or less according to the procedure discussed earlier, with a slight twist. <p> in M p .) Consider all the blocks (u np+1 ; : : : ; u n1 ; u n ); 8n = n (p 1); : : :; n (p) in the input as data points in the cube <ref> [1; 1] </ref> p . We maintain a simplex structure in [1; 1] p with these data points as vertex set, and the structure is incrementally modified more or less according to the procedure discussed earlier, with a slight twist. <p> Let C n = [ j S j be the union of the simplices at time n, and d n be the distance between C n and the corner of <ref> [1; 1] </ref> p farthest away from C n . At time n + 1, one more data point is obtained. If d n &lt; ffi p and the new data point lies outside C n , then discard the new point. Otherwise update the simplex structure as described earlier. <p> We now claim that n (p) &lt; 1 for every p. First we see that because the input is persistently exciting, the p-blocks in u are dense in <ref> [1; 1] </ref> p (Otherwise, there is a ball in [1; 1] p which does not contain any blocks in u, and we can construct a p-step finite memory system with a continuous memory function f : &lt; p ! &lt; to be positive at the centre of the ball and zero <p> We now claim that n (p) &lt; 1 for every p. First we see that because the input is persistently exciting, the p-blocks in u are dense in <ref> [1; 1] </ref> p (Otherwise, there is a ball in [1; 1] p which does not contain any blocks in u, and we can construct a p-step finite memory system with a continuous memory function f : &lt; p ! &lt; to be positive at the centre of the ball and zero outside. <p> For p q, n (p) (h (u)+d) is the spline interpolant that approximates the unknown memory function, and y = h (u) + d is the output. We can also extend g * to a function on <ref> [1; 1] </ref> p which depends only on the last q coordinates. <p> We now show that the first term can be made arbitrarily small for large p. Since g * is continuous, g * is a uniformly continuous function on <ref> [1; 1] </ref> q . Choose * 1 such that kx 1 x 2 k 2 * 1 ) kg * (x 1 ) g * (x 2 )k 2 * : (3) Now pick p sufficiently large such that ffi p &lt; * 1 and p &gt; q. <p> Lipschitz condition on the order p memory function g, such as jg (x) g (y)j &lt; M kx yk, then to identify the function up to accuracy * (in the k k 1 norm), the number of data points needed is at least the minimum number of *-balls to cover <ref> [1; 1] </ref> p . Since the volume of an *-ball is proportional to * p , it is clear that this minimum number is at least proportional to ( 1 * ) p , and hence so is the experiment length.
Reference: [2] <author> M.A. Dahleh and M.H. Khammash, </author> <title> "Controller Design in the Presence of Structured Uncertainty," </title> <note> to appear in Automatica special issue on robust control. 11 </note>
Reference-contexts: 1 Introduction Recently, there has been an increasing interest among the control community in the problem of identifying plants for control purposes. This generally means that the identified model should approximate the plant in the operator topology, since this allows the immediate use of robust control tools for designing controllers <ref> [2, 4] </ref>. This problem is of special importance when the data are corrupted with bounded noise. The case where the objective is to optimize prediction for a fixed input was analyzed by many researchers (see [9] and the references therein).
Reference: [3] <author> M.A.Dahleh, T.V.Theodosopoulos, J.N. </author> <title> Tsitsiklis " The Sample Complexity of Worst-Case Identification of FIR Linear Systems", </title> <journal> Systems and Control Letters, </journal> <month> March, </month> <year> 1993. </year>
Reference-contexts: Most work on input design has been done in stochastic settings (e.g. [10, 20] ), but recently there are also some results in worst-case settings ([11, 12] Related work on the worst-case identification problem was also reported in <ref> [13, 14, 3, 8] </ref>. In this paper, the work of Tse et al [17] is extended to analyse the worst-case asymptotic identification of nonlinear fading memory systems. As in [17], the study is done in two steps. <p> For nonlinear systems the time complexity is exponential of the order, whether or not there is noise. For the linear case, while it takes only linear time to identify a FIR system exactly when there is no noise, it has been shown <ref> [3, 14] </ref> that the time complexity immediately becomes exponential once we introduce any worst-case noise. Moreover, it has been demonstrated that if we are willing to put a probability distribution on the noise, polynomial time complexity can often be obtained [18].
Reference: [4] <author> J. C. Doyle, </author> <title> "Analysis of feedback systems with structured uncertainty," </title> <booktitle> IEEE Proceedings 129, </booktitle> <address> 242-250,1982. </address>
Reference-contexts: 1 Introduction Recently, there has been an increasing interest among the control community in the problem of identifying plants for control purposes. This generally means that the identified model should approximate the plant in the operator topology, since this allows the immediate use of robust control tools for designing controllers <ref> [2, 4] </ref>. This problem is of special importance when the data are corrupted with bounded noise. The case where the objective is to optimize prediction for a fixed input was analyzed by many researchers (see [9] and the references therein).
Reference: [5] <author> G. Gu, P.P. Khargonekar and Y. Li, </author> <title> "Robust convergence of two stage nonlinear algorithms for identification in H 1 , Systems and Control Letters, </title> <booktitle> Vol 18, </booktitle> <volume> No. 4, </volume> <month> April </month> <year> 1992. </year>
Reference: [6] <author> G. Gu and P.P. Khargonekar, </author> " <title> Linear and nonlinear algorithms for identification in H 1 with error bounds, </title> <journal> IEEE Trans. A-C, </journal> <volume> Vol 37, No. 7, </volume> <month> July </month> <year> 1992, </year> <pages> pp 953-963. </pages>
Reference: [7] <author> A.J. Helmicki, C.A. Jacobson and C.N. Nett, </author> <title> "Control-oriented System Identification: A Worst-case/deterministic Approach in H 1 '," IEEE Trans. </title> <journal> A-C, </journal> <volume> Vol 36, No. 10, </volume> <month> October </month> <year> 1991, </year> <pages> pp 1163-1176. </pages>
Reference-contexts: This property of linear splines, which is not shared by methods such as global polynomial interpolation, turns out to be the key to guarantee 8 the consistency of the estimates. A similar situation is encountered in linear system identification from frequency response data <ref> [7] </ref>, where 1 dimensional splines are used instead of polynomials to interpolate the noisy data to guarantee robustness of the identification procedure. With the above basic discussions on multivariate linear splines, we may now state the main result of this section.
Reference: [8] <author> C.A. Jacobson and C.N. Nett, </author> <title> "Worst-case system identification in ` 1 : Optimal algorithms and error bounds," </title> <booktitle> in Proc. of the 1991 American Control Conference, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Most work on input design has been done in stochastic settings (e.g. [10, 20] ), but recently there are also some results in worst-case settings ([11, 12] Related work on the worst-case identification problem was also reported in <ref> [13, 14, 3, 8] </ref>. In this paper, the work of Tse et al [17] is extended to analyse the worst-case asymptotic identification of nonlinear fading memory systems. As in [17], the study is done in two steps.
Reference: [9] <author> L. Ljung, </author> <title> System Identification Theory for the User, </title> <publisher> Prentice-Hall, </publisher> <year> 1987. </year>
Reference-contexts: This problem is of special importance when the data are corrupted with bounded noise. The case where the objective is to optimize prediction for a fixed input was analyzed by many researchers (see <ref> [9] </ref> and the references therein). The problem is more interesting when the objective is to approximate the original system as an operator, a problem extensively discussed in [19], especially when the plant's order is not known a priori.
Reference: [10] <author> R.K. Mehra, </author> <title> "Optimal input signals for parameter estimation in dynamic systems-A survey and new results", </title> <journal> em IEEE Trans. Automatic Control, </journal> <volume> vol AC-19, pp.753-768, </volume> <year> 1974. </year>
Reference-contexts: When the topology is induced by the ` 1 norm, a complete study of asymptotic identification was furnished in [17] for arbitrary inputs, and the question of optimal input design was addressed. Most work on input design has been done in stochastic settings (e.g. <ref> [10, 20] </ref> ), but recently there are also some results in worst-case settings ([11, 12] Related work on the worst-case identification problem was also reported in [13, 14, 3, 8].
Reference: [11] <author> B. Kacewicz and M. Milanese, </author> <title> "On the Optimal Experiment Design in the Worst-Case ` 1 System Identification, </title> <note> submitted to CDC. </note>
Reference: [12] <author> P.M. Makila, </author> <title> "Robust Identification and Galois Sequences", </title> <type> Technical Report 91-1, </type> <institution> Process Control Laboratory, Swedish University of Abo, </institution> <month> January, </month> <year> 1991. </year>
Reference-contexts: It will be shown that for fading memory systems, one can achieve asymptotically optimal identification without knowing ffi, provided that we use a persistently exciting input. This is in fact a generalization of a result by Makila <ref> [12] </ref>, which was proved in the context of stable LTI systems. We shall make use of multivariate piecewise linear spline functions to interpolate between the measured data to form an approximation to the unknown plant.
Reference: [13] <author> P.M. Makila and J.R. Partington, </author> <title> "Robust Approximation and Identification in H 1 ", Proc. </title> <booktitle> 1991 American Control Conference, </booktitle> <month> June, </month> <year> 1991. </year>
Reference-contexts: Most work on input design has been done in stochastic settings (e.g. [10, 20] ), but recently there are also some results in worst-case settings ([11, 12] Related work on the worst-case identification problem was also reported in <ref> [13, 14, 3, 8] </ref>. In this paper, the work of Tse et al [17] is extended to analyse the worst-case asymptotic identification of nonlinear fading memory systems. As in [17], the study is done in two steps.
Reference: [14] <author> K. Poolla and A. Tikku, </author> <title> "On the time complexity of worst-case system identification," </title> <type> preprint, </type> <year> 1992. </year>
Reference-contexts: Most work on input design has been done in stochastic settings (e.g. [10, 20] ), but recently there are also some results in worst-case settings ([11, 12] Related work on the worst-case identification problem was also reported in <ref> [13, 14, 3, 8] </ref>. In this paper, the work of Tse et al [17] is extended to analyse the worst-case asymptotic identification of nonlinear fading memory systems. As in [17], the study is done in two steps. <p> For nonlinear systems the time complexity is exponential of the order, whether or not there is noise. For the linear case, while it takes only linear time to identify a FIR system exactly when there is no noise, it has been shown <ref> [3, 14] </ref> that the time complexity immediately becomes exponential once we introduce any worst-case noise. Moreover, it has been demonstrated that if we are willing to put a probability distribution on the noise, polynomial time complexity can often be obtained [18].
Reference: [15] <author> J.S. Shamma and R. Zhao, </author> <title> "Fading memory feedback systems and robust stability," </title> <note> to appear in Automatica. </note>
Reference-contexts: It is easily to verify that these systems satisfy the first two conditions above. If we assume further that g is uniformly continuous, then it can be seen that the system also has fading memory. For further details on fading memory operators, see <ref> [1, 15] </ref>. 3 Identification Setup The plant to be identified is known to be in a model set M X . An input u is selected from the set U .
Reference: [16] <author> J.F. Traub and H. Wozniakowski, </author> <title> A General Theory of Optimal Algorithms, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: We now investigate how to choose an input which achieves this bound. Recall that M is balanced if h 2 M implies h 2 M. For balanced and convex model sets, it is well known from information-based complexity theory <ref> [16] </ref> that the worst case diameter is equal to the diameter of the uncertainty set when the output is identically equal to zero. The following lemma summarizes this. Lemma 5.2 Assume that M is balanced and convex.
Reference: [17] <author> D. Tse, M.A. Dahleh and J.N. Tsitsiklis. </author> <title> Optimal Asymptotic Identification under bounded disturbances. </title> <note> To appear in the IEEE Trans. Automat. Contr. </note>
Reference-contexts: When the topology is induced by the ` 1 norm, a complete study of asymptotic identification was furnished in <ref> [17] </ref> for arbitrary inputs, and the question of optimal input design was addressed. <p> In this paper, the work of Tse et al <ref> [17] </ref> is extended to analyse the worst-case asymptotic identification of nonlinear fading memory systems. As in [17], the study is done in two steps. The first step is concerned with obtaining tight upper and lower bounds on the optimal achievable error by any identification algorithm. <p> In this paper, the work of Tse et al <ref> [17] </ref> is extended to analyse the worst-case asymptotic identification of nonlinear fading memory systems. As in [17], the study is done in two steps. The first step is concerned with obtaining tight upper and lower bounds on the optimal achievable error by any identification algorithm. The bounds are functions of the input used for the experiments, and this can be arbitrary. <p> We say that the convergence is uniform if N (y; h; ") depends only on ". For more motivations and discussions on these definitions, see <ref> [17] </ref>. The optimal worst-case asymptotic error E 1 (u; ffi) is defined as the smallest error achievable by any algorithm: E 1 (u; ffi) inf e 1 (; u; ffi) Any algorithm for which the infimum is attained is said to be asymptotically optimal. <p> It is also untuned to the disturbance bound ffi. A slight extension of the above result yields essentially the same bounds for the case when M is separable. The proof is along the same lines as the proofs of Lemma 4.5 and Proposition 4.6 in <ref> [17] </ref>. The optimal algorithm has roughly the same structure as that for the -compact case. <p> Lemma 5.1 If 2ffi &lt; (M), then D (u; ffi) 2ffi for all u 2 U . Proof. See <ref> [17] </ref> Since (M) &gt; 2ffi for most of the reasonable model sets, the above result gives a general lower bound. We now investigate how to choose an input which achieves this bound. Recall that M is balanced if h 2 M implies h 2 M. <p> If u is persistently exciting, then there is an algorithm which can achieve an asymptotic error of less than 2ffi. A natural question which arises at this point is whether persistently exciting inputs exist. In the stable LTI case, this was shown to be the case <ref> [17] </ref>. The next theorem shows that they also exist when the model set consists of nonlinear fading memory systems. Theorem 5.2 Let the model set M be some subset of the set of fading memory systems.
Reference: [18] <author> D.N.C. Tse and J.N. Tsitsiklis, </author> " <title> Sample Complexity of Worst-Case System Identification in the Presence of Bounded Random Noise", </title> <note> in preparation. </note>
Reference-contexts: Moreover, it has been demonstrated that if we are willing to put a probability distribution on the noise, polynomial time complexity can often be obtained <ref> [18] </ref>.
Reference: [19] <author> G. Zames, </author> <title> "On the metric complexity of casual linear systems: *-entropy and *-dimension for continuous-time", </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> Vol. 24, </volume> <month> April </month> <year> 1979. </year>
Reference-contexts: The case where the objective is to optimize prediction for a fixed input was analyzed by many researchers (see [9] and the references therein). The problem is more interesting when the objective is to approximate the original system as an operator, a problem extensively discussed in <ref> [19] </ref>, especially when the plant's order is not known a priori. For linear time invariant plants, such approximation can be achieved by uniformly approximating the frequency response (H 1 -norm) or the impulse response (` 1 norm).
Reference: [20] <author> M. Zarrop, </author> <title> Optimal Experimental Design for Dynamic System Identification, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1979. </year> <month> 12 </month>
Reference-contexts: When the topology is induced by the ` 1 norm, a complete study of asymptotic identification was furnished in [17] for arbitrary inputs, and the question of optimal input design was addressed. Most work on input design has been done in stochastic settings (e.g. <ref> [10, 20] </ref> ), but recently there are also some results in worst-case settings ([11, 12] Related work on the worst-case identification problem was also reported in [13, 14, 3, 8].
References-found: 20

