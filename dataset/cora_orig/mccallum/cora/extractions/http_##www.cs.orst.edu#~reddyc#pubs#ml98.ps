URL: http://www.cs.orst.edu/~reddyc/pubs/ml98.ps
Refering-URL: http://www.cs.orst.edu/~reddyc/pubs.html
Root-URL: 
Email: freddyc,tadepallig@cs.orst.edu  
Title: Learning First-Order Acyclic Horn Programs from Entailment  
Author: Chandra Reddy Prasad Tadepalli 
Address: Corvallis, OR 97331-3202.  
Affiliation: Dearborn 303 Department of Computer Science Oregon State University,  
Abstract: In this paper, we consider learning first-order Horn programs from entailment. In particular, we show that any subclass of first-order acyclic Horn programs with constant arity is exactly learnable from equivalence and entailment membership queries provided it allows a polynomial-time subsumption procedure and satisfies some closure conditions. One consequence of this is that first-order acyclic determinate Horn programs with constant arity are exactly learnable from equiv alence and entailment membership queries.
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D. </author> <year> (1988). </year> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 319-342. </pages>
Reference: <author> Arimura, H. </author> <year> (1997). </year> <title> Learning acyclic first-order horn sentences from entailment. </title> <booktitle> In Proceedings of the Eigth International Workshop on Algorithmic Learning Theory. </booktitle> <address> Ohmsha/Springer-Verlag. </address>
Reference-contexts: In the following three definitions, we describe a class of Horn programs AH k for which minimal models are of polynomial size. Definition 6 <ref> (Arimura, 1997) </ref> Let 2 H.
Reference: <author> Cohen, W. </author> <year> (1995a). </year> <title> Pac-learning non-recursive prolog clauses. </title> <journal> Artificial Intelligence, </journal> <volume> 79 (1), </volume> <pages> 1-38. </pages>
Reference: <author> Cohen, W. </author> <year> (1995b). </year> <title> Pac-learning recursive logic programs: efficient algorithms. </title> <journal> Jl. of AI Research, </journal> <volume> 2, </volume> <pages> 500-539. </pages>
Reference: <author> De Raedt, L. </author> <year> (1997). </year> <title> Logical settings for concept learning. </title> <journal> Artificial Intelligence, </journal> <volume> 95 (1), </volume> <pages> 187-201. </pages>
Reference-contexts: It appears that simultaneously removing both the non-generative and simplicity restrictions could be difficult when functions are present, due to the unbounded nature of inference in that case. Learning from entailment and learning from interpretations are two of the standard settings for first-order learning <ref> (De Raedt, 1997) </ref>. In learning from interpretations, the learner is given a positive (or negative) interpretation for which the Horn sentence is true (or false). Interpretations can be partial in that the truth values of some ground atoms may be left unspecified.
Reference: <author> Dzeroski, S., Muggleton, S., & Russell, S. </author> <year> (1992). </year> <title> Pac-learnability of determinate logic programs. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pp. 128-135. </pages>
Reference: <author> Erol, K., Hendler, J., & Nau, D. </author> <year> (1994). </year> <title> HTN planning: complexity and expressivity. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94). </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: We believe that the algorithm discussed here and its extensions can be applied to learn d-rules, which is an important problem in speedup learning. d-rules are a special case of hierarchical task networks or HTNs <ref> (Erol, Hendler, & Nau, 1994) </ref>|in that HTNs allow partial ordering over subgoals and non-codesignation constraints over variables whereas d-rules do not. Nevertheless, it can be shown that HTNs can be expressed as Horn programs.
Reference: <author> Frazier, M., & Pitt, L. </author> <year> (1993). </year> <title> Learning from entailment: An application to propositional Horn sentences. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. 120-127. </pages>
Reference: <author> Khardon, R. </author> <year> (1996). </year> <title> Learning to take actions. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> pp. 787-792. </pages>
Reference-contexts: Khardon shows that "actions strategies" consisting of a variable number of constant-size first-order production rules can be learned from examples <ref> (Khardon, 1996) </ref>. However, Co-hen (1995a) proves that even predicting very restricted classes of Horn programs (viz. function-free 0-depth determinate constant arity) with variable number of clauses of variable size from examples alone is cryptographically hard. Frazier and Pitt (1993) first used the entailment setting for learning arbitrary propositional Horn programs. <p> In fact, Khardon shows that learning hierarchical strategies can be computationally hard when the structure of the hierarchy is not known <ref> (Khardon, 1996) </ref>. Our algorithm also assumes that the hierarchical order of the literals is known. The rest of the paper is organized as follows. Section 2 provides definitions for some of the terminology we use. Section 3 describes the learning model and the learning algorithm, and proves the learnability result.
Reference: <author> Khardon, R. </author> <year> (1998). </year> <title> Learning first order universal horn expressions. </title> <booktitle> In Proc. of the Eleventh Annual Conf. on Computational Learning Theory (COLT-98). </booktitle>
Reference: <author> Lloyd, J. </author> <year> (1987). </year> <booktitle> Foundations of Logic Programming (2nd ed.). </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: For brevity, we omit some of the standard terminology (as given in books such as <ref> (Lloyd, 1987) </ref>). In the following, we use p and its variants, and a and its variants each to stand for a conjunction of literals; and b; q; l and their variants each to stand for a single literal.
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1990). </year> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Learning Theory, </booktitle> <pages> pp. 368-381. </pages> <month> Ohmsha/Springer-Verlag. </month>
Reference-contexts: Note that there are at most O (n i ) such subsets. For each such subset, instan 1 This definition strictly generalizes the standard definition of determinacy <ref> (Muggleton & Feng, 1990) </ref>, in that a Horn clause (program) is determinate w.r.t. a set of literals p when it is 0-determinate w.r.t. p. i-determinacy should not be confused with ij-determinacy, or constant-depth fixed-arity determinacy, which is more restricted than determinacy. tiate all the ki variables in that subset in all
Reference: <author> Natarajan, B. </author> <year> (1989). </year> <title> On learning from exercises. </title> <booktitle> In Proceedings of the Second Workshop on Computational Learning Theory, </booktitle> <pages> pp. 72-87. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Page, C. D. </author> <year> (1993). </year> <title> Anti-Unification in Constraint Logics: Foundations and Applications to Learnability in First-Order Logic, to Speed-up Learning, and to Deduction. </title> <type> Ph.D. thesis, </type> <institution> Univ. of Illinois, Urbana, IL. </institution>
Reference: <author> Plotkin, G. </author> <year> (1970). </year> <title> A note on inductive generalization. </title> <editor> In Meltzer, B., & Michie, D. (Eds.), </editor> <booktitle> Machine Intelligence, </booktitle> <volume> Vol. 5, </volume> <pages> pp. 153-163. </pages> <publisher> Elsevier North-Holland, </publisher> <address> New York. </address>
Reference-contexts: We say that C 1 subsumes C 2 (denoted C 1 - C 2 ) iff there exists a substitution such that C 1 C 2 . We also say C 1 is a generalization of C 2 . Definition 3 <ref> (Plotkin, 1970) </ref> Let C, C 0 , C 1 and C 2 be sets of literals. <p> Definition 4 <ref> (Plotkin, 1970) </ref> A selection of clauses C 1 and C 2 is a pair of literals (l 1 ; l 2 ) such that l 1 2 C 1 and l 2 2 C 2 , and l 1 and l 2 have the same predicate symbol, arity, and sign.
Reference: <author> Reddy, C., & Tadepalli, P. </author> <year> (1997a). </year> <title> Learning goal-decomposition rules using exercises. </title> <booktitle> In Proceedings of the 14th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In many systems, learning hierarchically organized knowledge assumes that the structure of hierarchy or the order of the literals is known to the learner. Examples of such work include Mar-vin (Sammut & Banerji, 1986) and XLearn <ref> (Reddy & Tadepalli, 1997a) </ref>, on the experimental side; learning from exercises by Natarajan (1989) and learning acyclic Horn sentences by Arimura (1997), on the theoretical side. In fact, Khardon shows that learning hierarchical strategies can be computationally hard when the structure of the hierarchy is not known (Khardon, 1996).
Reference: <author> Reddy, C., & Tadepalli, P. </author> <year> (1997b). </year> <title> Learning Horn definitions using equivalence and membership queries. </title> <booktitle> In Proceedings of the 7th International Workshop on Inductive Logic Programming. </booktitle> <publisher> Springer Verlag. </publisher>
Reference-contexts: Learning all possible clauses while maintaining all consequents also does not seem to work, resulting in spurious matches between some of these redundant clauses and counterexamples in some cases. As shown in <ref> (Reddy & Tadepalli, 1997b) </ref>, Horn programs can be used to express goal-decomposition rules (d-rules) for planning using the situation-calculus formalism.
Reference: <author> Sammut, C. A., & Banerji, R. </author> <year> (1986). </year> <title> Learning concepts by asking questions. </title> <booktitle> In Machine learning: An artificial intelligence approach, </booktitle> <volume> Vol. 2. </volume> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: One aspect of learning in hierarchical domains is the hierarchical order of literals (goals or concepts). In many systems, learning hierarchically organized knowledge assumes that the structure of hierarchy or the order of the literals is known to the learner. Examples of such work include Mar-vin <ref> (Sammut & Banerji, 1986) </ref> and XLearn (Reddy & Tadepalli, 1997a), on the experimental side; learning from exercises by Natarajan (1989) and learning acyclic Horn sentences by Arimura (1997), on the theoretical side.
References-found: 18

