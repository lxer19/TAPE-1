URL: http://ai.iit.nrc.ca/DEIL/martin.ps.Z
Refering-URL: http://ai.iit.nrc.ca/DEIL/abstracts.html
Root-URL: 
Email: joel@ai.iit.nrc.ca  
Phone: 613-990-0113 (voice) 613-952-7151 (fax)  
Title: Clustering Full Text Documents IJCAI-95 Workshop on Data Engineering for Inductive Learning 1 Clustering Full
Author: Joel D. Martin 
Keyword: data engineering, information retrieval, clustering, unsupervised learning.  
Address: Ottawa, Ontario, Canada K1A 0R6  
Affiliation: Knowledge Systems Laboratory Institute for Information Technology National Research Council Canada  
Abstract: An index or topic hierarchy of full-text documents can organize a domain and speed information retrieval. Traditional indexes, like the Library of Congress system or Dewey Decimal system, are generated by hand, updated infrequently, and applied inconsistently. With machine learning, they can be generated automatically, updated as new documents arrive, and applied consistently. Despite the appeal of automatic indexing, organizing natural language documents is a difficult balance between what we want to do and what we can do. This paper describes an application of clustering to full-text databases, presents a new clustering method, and discusses the data engineering necessary to use clustering for this application. In particular, the paper deals with engineering the feature set to permit learning and otherwise engineering the data to match assumptions underlying the learning algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, J. R. & Matessa, M. </author> <year> (1992). </year> <title> Explorations of an incremental, Bayesian algorithm for categorization. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 275-308. </pages>
Reference: <author> Berners-Lee, T. </author> <title> The World Wide Web. </title> <journal> Communications of the ACM, </journal> <volume> 37. </volume>
Reference: <author> Can, F. & Ozkarahan, E. A. </author> <title> (1989) Dynamic Cluster Maintenance. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 25, </volume> <pages> 275-291. </pages>
Reference-contexts: FL is a significant improvement over those approaches because it permits cross classification using overlapping hierarchies. In addition, because FL is incremental, the structure of factors and clusters can be updated easily as new documents are added <ref> (cf. Can & Ozkarahan, 1989) </ref>. However, we are not here to sing the praises of this method. Rather we will highlight the ways in which FL is ill-suited to our task.
Reference: <author> Cooper, </author> <title> G.F. (1994). A Bayesian method for learning belief networks that contain hidden variables. </title> <journal> Journal of Intelligent Information Systems. </journal>
Reference: <author> Cooper, G.F. & Herskovits, E. </author> <year> (1992). </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 309-347. </pages>
Reference: <author> G. F. DeJong. </author> <year> (1979). </year> <title> Prediction and Substantiation: a New Approach to Natural Language Processing. </title> <journal> Cognitive Science, </journal> <volume> 3. </volume>
Reference: <author> Fisher, D. H. </author> <year> (1987). </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 139172. </pages>
Reference-contexts: Clearly, we cannot do this for the whole task of natural language understanding. However, we can address some of the mismatch by modifying our algorithm. The FL algorithm does not learn hierarchies. We can simply pull a page out of COBWEBs <ref> (Fisher, 1987) </ref> book and apply the clustering recursively. Once one level of factors has been formed, each cluster of documents can itself be further divided. This will clearly lead to a hierarchical structure. However, this is an ad hoc solution to achieve the task we want.
Reference: <author> Martin, J. D. and Billman, D. O. </author> <year> (1994). </year> <title> The acquisition and use of overlapping concepts, </title> <booktitle> Machine Learning. </booktitle>
Reference-contexts: Building a Hierarchical Index To build an index of documents, the learning or clustering method must group related documents together (Rasmussen, 1992). In addition, because different people will wish to use the resulting index in many different ways, the learner ideally should build several overlapping hierarchies of the documents <ref> (Martin & Billman, 1994) </ref>. We will use an algorithm designed to learn several different divisions of the documents. Given a set of n documents, it will learn one or more ways to divide the documents into related subsets.
Reference: <author> Mauldin, M. L. </author> <year> (1991). </year> <title> Retrieval Performance in FERRET: A Conceptual Information Retrieval System, </title> <booktitle> In the Proceedings of the 14th International Conference on Research and Development in Information Retrieval. </booktitle>
Reference: <author> Rasmussen, E. </author> <year> (1992). </year> <title> Clustering Algorithms. </title> <editor> In W. B. Frakes & R. Baeza-Yates (Eds), </editor> <booktitle> Information Retrieval: Data Structures and Algorithms. </booktitle> <address> New Jersey: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: Clustering Full Text Documents IJCAI-95 Workshop on Data Engineering for Inductive Learning 2 Generating a topic hierarchy is an ideal application for unsupervised learning, in which an integrated overview is created for a domain <ref> (cf. Rasmussen, 1992) </ref>. In traditional libraries, hierarchical indexes, like the Library of Congress system or Dewey Decimal system, are generated by hand, updated infrequently, and applied haphazardly by individual librarians. With machine learning, they can be generated automatically, updated as new documents arrive, and applied consistently. <p> Building a Hierarchical Index To build an index of documents, the learning or clustering method must group related documents together <ref> (Rasmussen, 1992) </ref>. In addition, because different people will wish to use the resulting index in many different ways, the learner ideally should build several overlapping hierarchies of the documents (Martin & Billman, 1994). We will use an algorithm designed to learn several different divisions of the documents. <p> Applying FL for Full-Text Clustering Although FL has many advantages over alternative document clustering approaches, it is still woefully inadequate for the task of automatic index generation. Several information retrieval researchers have applied statistical clustering methods to the organization of documents <ref> (Rasmussen, 1992) </ref>. FL is a significant improvement over those approaches because it permits cross classification using overlapping hierarchies. In addition, because FL is incremental, the structure of factors and clusters can be updated easily as new documents are added (cf. Can & Ozkarahan, 1989).
Reference: <author> Soderland, S. and Lehnert, W. </author> <year> (1994). </year> <title> Wrap-Up: a Trainable Discourse Module for Information Extraction, </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 131-158. </pages>
Reference-contexts: Even if we wanted to design a more appropriate learning algorithm that appeals to the rich structure of natural language, it is not clear where to start or even whether we could achieve better performance <ref> (cf. Soderland & Lehnert,1994) </ref>. In the rest of the paper, we present the learning task, our learning method, and the data engineering difficulties encountered. Section 2 discusses the task of learning a hierarchy of documents. Sec- tion 3 presents a method for learning overlapping clusters. <p> There are many ways to do this. The simplest of which is to identify keywords in the text and remove all sequential or proximity information. At the other extreme is attempting to parse the entire text. Such parsing is very slow and current methods paradoxically lead to diminished performance <ref> (e.g., Soderland & Lehnert, 1994) </ref>. Many information retrieval researchers have chosen some intermediate approach, including frequency counts , simple adjacency information and possibly rudimentary syntactic and semantic information (e.g., DeJong, 1979; Mauldin, 1991, Soderland & Lehnert, 1994).
References-found: 11

