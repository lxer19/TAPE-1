URL: http://www.iscs.nus.sg/~plong/papers/multinst.ps
Refering-URL: 
Root-URL: 
Title: PAC Learning Axis-aligned Rectangles with Respect to Product Distributions from Multiple-instance Examples  
Author: PHILIP M. LONG LEI TAN Editor: Dana Ron 
Keyword: time. Keywords: PAC learning, multiple-instance examples, axis-aligned hyperrectangles  
Address: Singapore, Singapore 119260, Republic of Singapore  One Microsoft Way, Redmond, WA 98052  
Affiliation: ISCS Department, National University of  
Note: 1-15 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  O d 5 n 12  
Email: plong@iscs.nus.sg  raymondt@microsoft.com  
Web: *ffi  
Abstract: We describe a polynomial-time algorithm for learning axis-aligned rectangles in Q d with respect to product distributions from multiple-instance examples in the PAC model. Here, each example consists of n elements of Q d together with a label indicating whether any of the n points is in the rectangle to be learned. We assume that there is an unknown product distribution D over Q d such that all instances are independently drawn according to D. The accuracy of a hypothesis is measured by the probability that it would incorrectly predict whether one of n more points drawn from D was in the rectangle to be learned. Our algorithm achieves accuracy * with probability 1 ffi in 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> P. Auer, P.M. Long, and A. Srinivasan. </author> <title> Approximating hyper-rectangles: learning and pseudo-random sets. </title> <booktitle> Proceedings of the 29th ACM Symposium on the Theory of Computing, </booktitle> <year> 1997. </year>
Reference-contexts: This increases the resources required by roughly a factor of n. Since they first appeared [8], the results described here have been improved upon. Auer, Long and Srinivasan <ref> [1] </ref> described an algorithm that does not require that the distribution generating the instances is a product distribution, and that runs in * 2 log * d time. <p> Blum and Kalai also established a stronger reduction (to learning with "one-sided" independent misclassification noise), allowing them to show that some classes which are not known to have efficient statistical query learning algorithms are nevertheless learnable from multiple-instance examples. The algorithm of this paper and that of <ref> [1] </ref> used only the first instance of each multiple-instance example for estimating probabilities; Blum and Kalai found an elegant way to use all n instances, resulting in an improvement in time and sample complexity over the results of [1] by roughly a factor of n. 4 Instead of measuring the accuracy <p> The algorithm of this paper and that of <ref> [1] </ref> used only the first instance of each multiple-instance example for estimating probabilities; Blum and Kalai found an elegant way to use all n instances, resulting in an improvement in time and sample complexity over the results of [1] by roughly a factor of n. 4 Instead of measuring the accuracy of the algorithm's hypothesis by the probability that it misclassifies another n instances as to whether any of them is in the target rectangle, one might instead want to measure the accuracy with the probability that a single <p> We will first need to establish some definitions. 3.1. Definitions This is the subset of Kearns and Schapire's [7] p-concept model that we need for this paper. (A similar model was independently introduced by Yamanishi [13].) For each distribution D over Q and each f : Q ! <ref> [0; 1] </ref>, define the probability distribution P D;f over Q fi f0; 1g to be the distribution obtained by first choosing the first component x 2 Q according to D and then choosing the second component so that the probability that it is 1 is f (x). <p> A p-concept learning algorithm A takes as input a finite sequence of elements of Q fi f0; 1g, and outputs a hypothesis h : Q ! <ref> [0; 1] </ref>. For *; fl; ffi &gt; 0, we say that A (*; fl; ffi)-learns a set F functions from Q to [0; 1] from m examples if and only if for all f 2 F , for all distributions D over Q, (P D;f ) m f : Dfx : <p> A p-concept learning algorithm A takes as input a finite sequence of elements of Q fi f0; 1g, and outputs a hypothesis h : Q ! <ref> [0; 1] </ref>. For *; fl; ffi &gt; 0, we say that A (*; fl; ffi)-learns a set F functions from Q to [0; 1] from m examples if and only if for all f 2 F , for all distributions D over Q, (P D;f ) m f : Dfx : j (A ())(x) f (x)j &gt; flg &gt; *g ffi: If the three parameters are small, this says that it is highly <p> If, in addition, A only outputs hypotheses in F , following Pitt and Valiant [10], we say that A properly (*; fl; ffi)- learns F . Finally, for each a; b 2 Q; ff; fi 2 <ref> [0; 1] </ref>, define a;b;ff;fi (x) = fi if x 2 [a; b] ff otherwise. 6 s s - 6 fi A graph of one such a;b;ff;fi is shown in Figure 1. For each 0; ; 2 [0; 1], let PINT ;; (PINT stands for "Probabilistic INTervals") consist of all those a;b;ff;fi <p> Finally, for each a; b 2 Q; ff; fi 2 <ref> [0; 1] </ref>, define a;b;ff;fi (x) = fi if x 2 [a; b] ff otherwise. 6 s s - 6 fi A graph of one such a;b;ff;fi is shown in Figure 1. For each 0; ; 2 [0; 1], let PINT ;; (PINT stands for "Probabilistic INTervals") consist of all those a;b;ff;fi for which * fi ff + (i.e., fi is significantly bigger than ff), and * ff 2 [ ; + ] (i.e., ff is close to ). 3.2. <p> Now we are ready for the lemma giving the reduction. Lemma 4 Assume that there are functions ' 1 ; ' 2 : R fi R fi R ! N such that for each ; ; 2 <ref> [0; 1] </ref>, there is an algorithm A such that for each *; fl; ffi &gt; 0, A properly (*; fl; ffi)-learns PINT ;; with respect to product distributions from ' 1 (*; fl; ffi) examples in ' 2 (*; fl; ffi) time. <p> Definitions The following definition is due to Pollard [11]. Choose a set F of functions from Q to <ref> [0; 1] </ref>. <p> Learning PINT ;; We begin by recording a lemma that follows directly from the work of Kearns and Schapire [7, Lemma 2.2 and Theorem 5.1]. Lemma 5 ([7]) Choose a set F of functions from Q to <ref> [0; 1] </ref>. If d is the pseudo-dimension of F , then any algorithm which minimizes quadratic loss with respect to F properly (*; fl; ffi)-learns F from O 1 1 + log ffi examples. Next, we bound the pseudo-dimension of PINT ;; . <p> Next, we bound the pseudo-dimension of PINT ;; . The proof uses ideas from the bound on the VC-dimension of BOXES 2 of Blumer, Ehrenfeucht, Haussler and Warmuth [3]. Lemma 6 For any 0; ; 2 <ref> [0; 1] </ref>, the pseudo-dimension of PINT ;; is at most 4. Proof: Choose 0; ; 2 [0; 1]. Assume for contradiction that (u 1 ; s 1 ); :::; (u 5 ; s 5 ) is shattered by PINT ;; . <p> The proof uses ideas from the bound on the VC-dimension of BOXES 2 of Blumer, Ehrenfeucht, Haussler and Warmuth [3]. Lemma 6 For any 0; ; 2 <ref> [0; 1] </ref>, the pseudo-dimension of PINT ;; is at most 4. Proof: Choose 0; ; 2 [0; 1]. Assume for contradiction that (u 1 ; s 1 ); :::; (u 5 ; s 5 ) is shattered by PINT ;; . <p> The definition of shattering implies that there exist a; b 2 Q; ff; fi 2 <ref> [0; 1] </ref>; fi ff such that * a;b;ff;fi (u 1 ) &lt; s 1 , * for each i 2 fleft; right; upg, a;b;ff;fi (u i ) s i , and * a;b;ff;fi (u middle ) &lt; s middle : Fix such a; b; ff; fi. <p> One can therefore solve the three such problems arising from making the three constraints tight, and output the minimal solution. Now we are ready to give the algorithm for learning PINT ;; . 14 Lemma 8 Choose ; ; 2 <ref> [0; 1] </ref>. There is an algorithm A such that, for any *; fl; ffi &gt; 0, A properly (*; fl; ffi)-learns PINT ;; from O 1 1 + log ffi examples in O 1 1 + log ffi ! time.
Reference: 2. <author> A. Blum and A. Kalai. </author> <title> A note on learning from multiple-instance examples. </title> <journal> Machine Learning, </journal> <note> this issue. </note>
Reference-contexts: Blum and Kalai <ref> [2] </ref> showed that, if the instances are assumed to be independent, learning from multiple-instance examples reduces to single-instance learning with independent misclassification noise, i.e. where the classification in each example is "flipped" with a certain probability.
Reference: 3. <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> JACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: The problem studied in this paper can be viewed as that of PAC learning a subclass of the set of unions of n axis-aligned rectangles with respect to product distributions on Q dn . Blumer, Ehrenfeucht, Haussler and Warmuth <ref> [3] </ref> showed how to learn unions of s rectangles in Q d in time polynomial in s for fixed d; Long and Warmuth [9] described an algorithm taking time polynomial in d for fixed s. <p> However, if the probability according to D of the region outside the rectangle to be learned is *, then the probability that a given multiple-instance example is labelled 0 is * n . Using known techniques <ref> [3, 5] </ref>, this implies that learning from polynomially many examples when the accuracy of the hypothesis is measured with respect to the distribution on individual instances is impossible. 1 Nevertheless, for the drug discovery application, the classification of a collection of n instances corresponds to the binding property of a given <p> Next, we bound the pseudo-dimension of PINT ;; . The proof uses ideas from the bound on the VC-dimension of BOXES 2 of Blumer, Ehrenfeucht, Haussler and Warmuth <ref> [3] </ref>. Lemma 6 For any 0; ; 2 [0; 1], the pseudo-dimension of PINT ;; is at most 4. Proof: Choose 0; ; 2 [0; 1]. Assume for contradiction that (u 1 ; s 1 ); :::; (u 5 ; s 5 ) is shattered by PINT ;; .
Reference: 4. <author> T.G. Dietterich, R.H. Lathrop, and T. Lozano-Perez. </author> <title> Solving the multiple-instance problem with axis-parallel rectangles. </title> <journal> Artificial Intelligence, </journal> <volume> 89(1-2):31-71, </volume> <year> 1997. </year>
Reference-contexts: 1. Introduction Dietterich, Lathrop and Lozano-Perez <ref> [4] </ref> recently introduced the notion of learning from multiple-instance examples, where, rather than learning a function f : X ! f0; 1g from examples (x; f (x)) of its behavior, the learning algorithm instead receives examples of the form ((x 1 ; :::; x n ); f (x 1 ) _
Reference: 5. <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L.G. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 247-251, </pages> <year> 1989. </year>
Reference-contexts: However, if the probability according to D of the region outside the rectangle to be learned is *, then the probability that a given multiple-instance example is labelled 0 is * n . Using known techniques <ref> [3, 5] </ref>, this implies that learning from polynomially many examples when the accuracy of the hypothesis is measured with respect to the distribution on individual instances is impossible. 1 Nevertheless, for the drug discovery application, the classification of a collection of n instances corresponds to the binding property of a given
Reference: 6. <author> M.J. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> Proceedings of the 25th ACM Symposium on the Theory of Computing, </booktitle> <year> 1993. </year>
Reference-contexts: Blum and Kalai [2] showed that, if the instances are assumed to be independent, learning from multiple-instance examples reduces to single-instance learning with independent misclassification noise, i.e. where the classification in each example is "flipped" with a certain probability. Kearns <ref> [6] </ref> had introduced the notion of a statistical query, and showed that any class that can be learned from statistical queries can be learned with independent misclassification noise. He then demonstrated that typical learning algorithms can easily be modified to use statistical queries.
Reference: 7. <author> M.J. Kearns and R.E. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 48(3) </volume> <pages> 464-497, </pages> <year> 1994. </year>
Reference-contexts: It works by running d copies of an algorithm A pcon for learning p-concepts <ref> [7, 13] </ref> to learn the conditional probability that any of the instances of a multiple-instance example will be in the rectangle to be learned, given each of the components of the first instance. <p> Generating p-concepts in this way is reminiscent of the "hidden variable" problems studied by Kearns and Schapire <ref> [7] </ref>. The p-concepts that arise are piecewise constant, with a constant conditional probability in an interval, and lower conditional probability outside this interval (see Figure 1 for a representative graph). Using the tools of Kearns and Schapire [7], it is fairly easy to see that these p-concepts are of a simple <p> way is reminiscent of the "hidden variable" problems studied by Kearns and Schapire <ref> [7] </ref>. The p-concepts that arise are piecewise constant, with a constant conditional probability in an interval, and lower conditional probability outside this interval (see Figure 1 for a representative graph). Using the tools of Kearns and Schapire [7], it is fairly easy to see that these p-concepts are of a simple enough form that they can be efficiently learned. The difficulty arises from enforcing the fact that one can reconstruct a good hypothesis for the hidden rectangle from the hypotheses returned by A pcon . <p> Reducing to a p-concept problem In this section, we show how the problem of learning rectangles with respect to product distributions from multiple-instance examples reduces to a p-concept learning problem. We will first need to establish some definitions. 3.1. Definitions This is the subset of Kearns and Schapire's <ref> [7] </ref> p-concept model that we need for this paper. (A similar model was independently introduced by Yamanishi [13].) For each distribution D over Q and each f : Q ! [0; 1], define the probability distribution P D;f over Q fi f0; 1g to be the distribution obtained by first choosing <p> Solving the p-concept problem The results of Kearns and Schapire <ref> [7] </ref> imply that each PINT ;; can be learned in polynomial time, but for our application we need a proper learning algorithm. 4.1. Definitions The following definition is due to Pollard [11]. Choose a set F of functions from Q to [0; 1]. <p> Learning PINT ;; We begin by recording a lemma that follows directly from the work of Kearns and Schapire <ref> [7, Lemma 2.2 and Theorem 5.1] </ref>. Lemma 5 ([7]) Choose a set F of functions from Q to [0; 1].
Reference: 8. <author> P.M. Long and L. Tan. </author> <title> PAC learning axis-aligned rectangles with respect to product distributions from multiple-instance examples. </title> <booktitle> The 1995 Conference on Computational Learning Theory, </booktitle> <pages> pages 228-234, </pages> <year> 1996. </year>
Reference-contexts: This increases the resources required by roughly a factor of n. Since they first appeared <ref> [8] </ref>, the results described here have been improved upon. Auer, Long and Srinivasan [1] described an algorithm that does not require that the distribution generating the instances is a product distribution, and that runs in * 2 log * d time.
Reference: 9. <author> P.M. Long and M. K. Warmuth. </author> <title> Composite geometric concepts and polynomial predictability. </title> <journal> Information and Computation, </journal> <volume> 113(2) </volume> <pages> 203-252, </pages> <year> 1994. </year>
Reference-contexts: Blumer, Ehrenfeucht, Haussler and Warmuth [3] showed how to learn unions of s rectangles in Q d in time polynomial in s for fixed d; Long and Warmuth <ref> [9] </ref> described an algorithm taking time polynomial in d for fixed s. One apparent limitation of our result is that the number n of instances is the same for all examples the learner sees, where, for example, in the drug design application, different molecules assume different numbers of shapes.
Reference: 10. <author> L. Pitt and L.G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35(4) </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference-contexts: If, in addition, A only outputs hypotheses in F , following Pitt and Valiant <ref> [10] </ref>, we say that A properly (*; fl; ffi)- learns F .
Reference: 11. <author> D. Pollard. </author> <title> Convergence of Stochastic Processes. </title> <publisher> Springer Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Choose d; n 2 N, a product distribution D over Q d and ~a; ~ b 2 Q d . By the standard Hoeffding bound (see <ref> [11, Appendix B] </ref>), the probability that j^p D n (OR r ~a; ~ b )j &gt; * 2:5 128n is at most ffi=2. (Here and elsewhere in this proof, we will refer to OR r ~a; ~ b and the set on which it evaluates to 1 interchangeably.) Therefore, it is <p> Solving the p-concept problem The results of Kearns and Schapire [7] imply that each PINT ;; can be learned in polynomial time, but for our application we need a proper learning algorithm. 4.1. Definitions The following definition is due to Pollard <ref> [11] </ref>. Choose a set F of functions from Q to [0; 1].
Reference: 12. <author> L.G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: They presented experimental studies of different learning algorithms using both natural and synthetic data. In this paper, we consider the problem of learning axis-aligned rectangles from multiple-instance examples in Valiant's PAC model <ref> [12] </ref>. We assume that each of 2 the instances is drawn independently at random according to a fixed, unknown, product distribution D on Q d . <p> Definitions and main result Denote the rationals by Q, the positive integers by N, and the real numbers by R. The following is based on Valiant's PAC model <ref> [12] </ref>. A multiple-instance learning algorithm takes as input *; ffi &gt; 0, and a finite sequence of elements of (Q d ) n fi f0; 1g (n-instance examples), where d; n 2 N, and outputs a hypothesis h : Q d ! f0; 1g.
Reference: 13. <author> Kenji Yamanishi. </author> <title> A learning criterion for stochastic rules. </title> <booktitle> Machine Learning, </booktitle> <year> 1992. </year> <note> Special Issue on the Proceedings of the 3nd Workshop on Computational Learning Theory, to appear. </note>
Reference-contexts: It works by running d copies of an algorithm A pcon for learning p-concepts <ref> [7, 13] </ref> to learn the conditional probability that any of the instances of a multiple-instance example will be in the rectangle to be learned, given each of the components of the first instance. <p> We will first need to establish some definitions. 3.1. Definitions This is the subset of Kearns and Schapire's [7] p-concept model that we need for this paper. (A similar model was independently introduced by Yamanishi <ref> [13] </ref>.) For each distribution D over Q and each f : Q ! [0; 1], define the probability distribution P D;f over Q fi f0; 1g to be the distribution obtained by first choosing the first component x 2 Q according to D and then choosing the second component so that
References-found: 13

