URL: http://ltpwww.gsfc.nasa.gov/Smid/proj/cmp-p.ps
Refering-URL: http://ltpwww.gsfc.nasa.gov/Smid/proj/projects.html
Root-URL: 
Email: E-mail smid@ltpsun.gsfc.nasa.gov  E-mail volf@utia.cas.cz  
Phone: 8, CZ 182 08,  Tel. 301-286-6654  
Title: Monte Carlo Approach to Bayesian Regression Modeling  
Author: J. Smid P. Volf G. Rao 
Keyword: Key words: Bayesian approach, Markov chain Monte Carlo, Regression analysis, Statistical estimation and modeling.  
Address: Baltimore, MD 21239,  Prague  Code 734 Greenbelt, MD 20770,  
Affiliation: Department of Mathematics, Morgan State University  UTIA, The Czech Academy of Sciences  NASA-GSFC,  
Abstract: In the framework of a functional response model (i.e. a regression model, or a feedforward neural network) an estimator of a nonlinear response function is constructed from a set of functional units. The parameters defining these functional units are estimated using the Bayesian approach. A sample representing the Bayesian posterior distribution is obtained by applying the Markov chain Monte Carlo procedure, namely the combination of Gibbs and Metropolis-Hastings algorithms. The method is described for histogram, B-spline and radial basis function estimators of a response function. In general, the proposed approach is suitable for finding Bayes-optimal values of parameters in a complicated parameter space. We illustrate the method on numerical examples. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Arjas and D. Gasbarra, </author> <title> "Nonparametric Bayesian inference from right censored survival data, using Gibbs sampler," </title> <type> Manuscript, </type> <year> 1993. </year>
Reference-contexts: We can consider M as an integer-valued random variable and make it the part of Bayesian scheme. However, the disadvantage of this approach is that the whole model has to be re-estimated for each chosen M . Arjas and Gasbarra <ref> [1] </ref> suggested an approach which, at each step, changed only a part of the model. We have experimented with a similar pro cedure.
Reference: [2] <author> J.M. Bernardo and A.F.M. Smith, </author> <title> Bayesian Theory. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The Bayesian method yields the posterior distribution, i.e. the probability of possible values of the parameters conditioned on the observed data. As a rule, the posterior distribution cannot be reasonably obtained by direct computation. Therefore, the method called the Markov chain Monte Carlo (MCMC, <ref> [2] </ref>) is used. This method generates a Markov chain (a random sequence) of suboptimal solutions. It has been proved that the distribution of a properly generated chain converges to the posterior distribution of the parameters. Its mean (median, modus) then serves as the point estimate of unknown parameters.
Reference: [3] <author> J. Besag, D. Green, D. Hig-don and K. Mengersen, </author> <title> "Bayesian computation and stochastic systems; with discussion," </title> <booktitle> Statist. Science 10, </booktitle> <pages> 3-66, </pages> <year> 1995. </year>
Reference-contexts: Conclusion Some variants of the MCMC methods have been actually used in statistical physics over 40 years, in spatial statistics for the past 20 years, and, in connection with Bayesian image analysis, at least over one decade, [6], <ref> [3] </ref>. However, the fast development of these techniques is a matter of no more than last current 3 years. It concerns many important areas of data analysis, this is also the case of nonparametric regression, and of related areas, e.g. modeling and classification via neural networks.
Reference: [4] <author> S. Chen, C.F.N. Cowan and P.M. Grant, </author> <title> "Orthogonal least squares learning for radial basis function networks," </title> <journal> IEEE Trans. Neural. </journal> <volume> Networks 2, </volume> <pages> 302-309, </pages> <year> 1991. </year>
Reference-contexts: The addition of new units can be complementary controlled by a rule guaranteeing a reasonable minimal distance between them (similarly as in Chen et al. <ref> [4] </ref>). 6. Multidimensional Case Assume that the input variable X = (X 1 ; :::; X p ) 0 is now a pvariate vector. Many authors, even in situation of normal regression model, consider the additive form of multivariate regression.
Reference: [5] <author> J.H. Friedman, </author> <title> "Multivariate adaptive regression splines," </title> <journal> Annals Statist. </journal> <volume> 19, </volume> <pages> 1-141, </pages> <year> 1991. </year>
Reference-contexts: We present a method which, in the framework of the Bayesian MCMC procedure, supports changes of the number of units. The randomized decision rule of the Metropolis-Hastings algorithm is based on the penalty criteria of Akaike's IC type (similar criteria are used in adaptive non-Bayesian procedures, see e.g. MARS <ref> [5] </ref>). Naturally, this procedure leads to deletion of irrelevant input variables. In such a way, dimensionality of the model is optimized. Further, the computation can be reduced considerably by a convenient choice of the functional basis. <p> We assume that each B j depends only on a few adjacent fi's. While the estimates of parameters ff can be obtained from a linear regression context, direct estimation of fi is practically intractable. Different adaptive approaches to this problem are suggested in several papers, see MARS, Friedman <ref> [5] </ref>, however, proposed solutions have not solved this problem sufficiently . As an alternative solution to the nonlinear problem for coefficients fi we propose to use the Bayesian methodology. In this framework, the parameter fi is considered to be a multi-dimensional random vector, with a prior distribution satisfying certain constraints. <p> Nevertheless, we examine whether the addition of one unit from corresponding functional basis improves the fit of the model 'sufficiently'. In a non-Bayesian setting, this is often measured by a penalty criterion. For example, it is recommended (among other criteria, e.g. AIC, GCV, see also Friedman <ref> [5] </ref>), to use the crite rion ^ 2 M exp ( M N fl ), where fl is a number from (0; 1), ^ 2 M is the estimate of residual variance. Quite similarly, we suggest to employ the penalty term in the ratio (4).
Reference: [6] <author> S. Geman and D. Geman, </author> <title> "Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images," </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell. </journal> <volume> 6, </volume> <pages> 724-741, </pages> <year> 1984. </year>
Reference-contexts: Conclusion Some variants of the MCMC methods have been actually used in statistical physics over 40 years, in spatial statistics for the past 20 years, and, in connection with Bayesian image analysis, at least over one decade, <ref> [6] </ref>, [3]. However, the fast development of these techniques is a matter of no more than last current 3 years. It concerns many important areas of data analysis, this is also the case of nonparametric regression, and of related areas, e.g. modeling and classification via neural networks.
Reference: [7] <author> W.K. Hastings, </author> <title> "Monte Carlo sampling methods using Markov chains and their applications," </title> <journal> Biometrika 57, </journal> <pages> 97-109, </pages> <year> 1970. </year>
Reference: [8] <author> G.O. Roberts and A.F.M. Smith, </author> <title> "Simple conditions for the convergence of the Gibbs sampler and Metropolis-Hastings algorithms," </title> <journal> Stoch. Processes and Applic. </journal> <volume> 49, </volume> <pages> 207-216, </pages> <year> 1994. </year>
Reference-contexts: It is easy to show (at least for Gibbs and Metropolis-Hastings algorithms) that the Bayes posterior distribution is the invariant distribution of resulting Markov chain. Moreover, it has been proved, under rather mild and natural conditions, that the distribution of such a chain converges to this posterior distribution <ref> [8] </ref>. The same proof can be extended for the algorithm we propose in the present paper. In accordance with the theory of Markov random sequences, it suffices to show that the chain is irreducible and aperiodic.
References-found: 8

