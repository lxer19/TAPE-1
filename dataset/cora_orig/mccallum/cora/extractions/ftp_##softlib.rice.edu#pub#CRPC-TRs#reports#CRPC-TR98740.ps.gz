URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR98740.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: A Ferris-Mangasarian Technique Applied to Linear Least Squares Problems  
Author: J. E. Dennis Trond Steihaug 
Note: Research supported by DOE FG03-93ER25178, CRPC CCR-9120008, AFOSR F49620-95-1-0210, The Boeing Company, and the REDI Foundation. Research supported by The Research Council of Norway and VISTA a research cooperation between the Norwegian Academy of Science and Den norske stats oljeselskap a.s (Statoil).  
Date: May 1, 1998  
Address: Houston TX 77005-1892  N-5020 Bergen Norway  
Affiliation: Computational and Applied Mathematics Rice University  Department of Informatics University of Bergen Htyteknologisenteret  
Abstract: This note specializes to linear least squares problems an approach suggested by Ferris and Mangasarian [4] for solving constrained optimization problems on parallel computers. It will be shown here that this specialization leads to an algorithm which is mathematically equivalent to an acceleration and convergence forcing modification of the block Jacobi iteration applied to the normal equations. The resulting algorithm is a promising way to speed up a parallel multisplitting algorithm of Renaut [9] for linear least squares. Renaut's algorithm is related to a specialization of part of the Ferris and Mangasarian approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bjorck, </author> <title> Numerical methods for least squares problems, </title> <publisher> SIAM, </publisher> <year> 1996. </year>
Reference-contexts: It is well known that the iteration defined by incrementing each variable by the amount indicated in this first stage and then updating the residual for the next iteration is the block Jacobi iteration applied to the normal equations for the original linear least squares problem. See Bjorck <ref> [1] </ref> and the references therein. The classical Jacobi iteration alone may not converge [5, 8]. The second stage of each iteration is to compute a new iterate by a synchronization step that involves solving a least squares problem in a smaller space of surrogate variables identified by the first stage. <p> Define the direction d k : d k = j=1 j : Then (5) can be written as x k+1 = x k + d k . This is <ref> [1] </ref> the classical block Jacobi method on the normal equations A T M Ax = A T M b: (6) Assume that A has full rank. Then the following result says that the block Jacobi method converges if A T M A is sufficiently `block diagonally dominant'. <p> The Guass-Seidel method is applied to the normal equations without forming the normal equations (see for example <ref> [1, 3] </ref>). For the Jacobi method we use the subspace corrected step (15) to guarantee convergence. The column marked "p=1" are the iterations using the supplementary variables defined by p k = (1; : : : ; 1) T 2 R n .
Reference: [2] <author> J. E. Dennis, Jr., Robert B. Schnabel, </author> <title> Numerical methods for unconstrained optimization and nonlinear equations, </title> <publisher> Prentice-Hall, </publisher> <year> 1983. </year>
Reference-contexts: Theorem 2 Let A have full rank. Given x k , choose x k+1 = x k + k d k ; where k is defined by (8). Then lim k!1 x k = x fl . Proof: Chapter 6 of <ref> [2] </ref>. Of course, this linesearch functions as a synchronization step, and so the attractive parallelism in the Jacobi iteration is compromised. Note that k is the easy solution of the 1-dimensional least squares problem to solve for k in k (Ad k ) k + r k k M .
Reference: [3] <author> J. E. Dennis, Jr., and T. Steihaug, </author> <title> On the successive projections approach to least squares problems, </title> <journal> SIAM J. Numer. Anal. </journal> <month> 23 </month> <year> (1986) </year> <month> 717-733 </month>
Reference-contexts: k into x k 2 ; : : : ; x k Parallelization: i = 1; 2; : : : ; g Solve for x k+1 i 2 R n i : min kA i x i (b 1=j6=i j )k M : (3) Following the notation and derivation in <ref> [3] </ref>, we introduce the direction d k i = i x k i , and note that successive residuals satisfy r k+1 = j=1 j b = r k + j=1 j : Then the ith least squares subproblem (3) is: Solve for d k i 2 R n i : <p> The Guass-Seidel method is applied to the normal equations without forming the normal equations (see for example <ref> [1, 3] </ref>). For the Jacobi method we use the subspace corrected step (15) to guarantee convergence. The column marked "p=1" are the iterations using the supplementary variables defined by p k = (1; : : : ; 1) T 2 R n .
Reference: [4] <author> M. C. Ferris and O. L. Mangasarian, </author> <title> Parallel variable distribution, </title> <note> SIAM J. Optimization 4(1994) 815-832. </note>
Reference-contexts: 1 Introduction This note specializes to linear least squares problems an approach suggested by Ferris and Mangasarian <ref> [4] </ref> for solving constrained optimization problems on parallel computers. It will be shown here that this specialization leads to an acceleration and convergence forcing mechanism for the block Jacobi iteration applied to the normal equations. <p> For simplicity, we will restrict ourselves here to the case that at each of the two stages we accurately solve the smaller minimization problems, but this 2 is neither necessary to the theory nor even possible in the general setting considered in <ref> [4] </ref>. Mangasarian [6] and Ferris and Mangasarian [4] introduced in stage one auxiliary variables, which they called "forget-me-not" variables. <p> For simplicity, we will restrict ourselves here to the case that at each of the two stages we accurately solve the smaller minimization problems, but this 2 is neither necessary to the theory nor even possible in the general setting considered in <ref> [4] </ref>. Mangasarian [6] and Ferris and Mangasarian [4] introduced in stage one auxiliary variables, which they called "forget-me-not" variables. <p> Our results hint that perhaps the choice we advocate for least squares may be useful back in the general minimization setting considered in <ref> [4] </ref>. <p> Proof: Since A is full rank, f is strongly convex and the result follows from Theorem 2.3 of <ref> [4] </ref> or Theorem 5 of the next section. 3 The Ferris-Mangasarian Correction Step In the last section, we saw that the Jacobi iteration converges when the block cross terms in the coefficient matrix A T M A are weak enough to be neglected. <p> This section will introduce simple techniques for incorporating the influence of these cross terms into the iteration. Unfortunately, these all will take the form of a synchronization step and so parallelism will be compromised. Mangasarian [6] and Ferris and Mangasarian <ref> [4] </ref> introduced a synchronization step in which the step x k = x k+1 x k is chosen by approximately minimizing f (x k + x) in the subspace spanned by d i i = 1; : : : ; g. In the following, we call this a subspace-correction step. <p> Solve for s j : min k b A j s j + v j k M . z j+1 = z j + D j s j : Let p k = z l The following result follows from Ferris and Mangasarian <ref> [4] </ref> Theorem 2.3 by noticing that if A has full rank then the function f in (7) is strongly convex. Theorem 5 Assume that fp k g is bounded independent of k. <p> For the Jacobi method we use the subspace corrected step (15) to guarantee convergence. The column marked "p=1" are the iterations using the supplementary variables defined by p k = (1; : : : ; 1) T 2 R n . Ferris and Mangasarian <ref> [4] </ref> suggest using the vector p k to be [p k 1 = [A T for j = 1; 2; : : : ; n i ; i = 1; 2; : : : ; g (16) where e i 2 R n i is a vector with all ones and
Reference: [5] <author> H. B. Keller, </author> <title> On the solution of singular and semidefinite linear systems by iteration, </title> <journal> SIAM J. Numer.Anal. </journal> <volume> 2(1965) 281-290. </volume> <pages> 13 </pages>
Reference-contexts: See Bjorck [1] and the references therein. The classical Jacobi iteration alone may not converge <ref> [5, 8] </ref>. The second stage of each iteration is to compute a new iterate by a synchronization step that involves solving a least squares problem in a smaller space of surrogate variables identified by the first stage. <p> The corresponding block Jacobi method will converge to x fl , a solution of (1) if 2 C A T M A is positive definite. Proof: Corollary 2.1 of <ref> [5] </ref>. Even when 2 C A T M A is not positive definite, we can force convergence by the following two small modifications of the block Jacobi method.
Reference: [6] <author> O. L. Mangasarian, </author> <title> Parallel gradient distribution in unconstrained opti-mization, </title> <note> SIAM J. Control and Optimization 33(1995)1916-1925. </note>
Reference-contexts: For simplicity, we will restrict ourselves here to the case that at each of the two stages we accurately solve the smaller minimization problems, but this 2 is neither necessary to the theory nor even possible in the general setting considered in [4]. Mangasarian <ref> [6] </ref> and Ferris and Mangasarian [4] introduced in stage one auxiliary variables, which they called "forget-me-not" variables. <p> This section will introduce simple techniques for incorporating the influence of these cross terms into the iteration. Unfortunately, these all will take the form of a synchronization step and so parallelism will be compromised. Mangasarian <ref> [6] </ref> and Ferris and Mangasarian [4] introduced a synchronization step in which the step x k = x k+1 x k is chosen by approximately minimizing f (x k + x) in the subspace spanned by d i i = 1; : : : ; g.
Reference: [7] <author> D. P. O'Leary and R. E. White, </author> <title> Multisplitting of matrices and parallel solutions of linear systems, </title> <journal> SIAM J. Algebraic Discrete Methods, </journal> <pages> 6(1985) 630-640. </pages>
Reference-contexts: It turns out that this method has already been considered by Renaut [9]. She calls it Optimal Recombination Least Squares Multisplitting (ORLSMS), and she gives further developments based on the multisplitting method of O'Leary and White <ref> [7] </ref>. For simplicity, we will restrict ourselves here to the case that at each of the two stages we accurately solve the smaller minimization problems, but this 2 is neither necessary to the theory nor even possible in the general setting considered in [4].
Reference: [8] <author> J. M. Ortega, </author> <title> Introduction to parallel and vector solution of linear systems, </title> <publisher> Plenum Press, </publisher> <year> 1988. </year>
Reference-contexts: See Bjorck [1] and the references therein. The classical Jacobi iteration alone may not converge <ref> [5, 8] </ref>. The second stage of each iteration is to compute a new iterate by a synchronization step that involves solving a least squares problem in a smaller space of surrogate variables identified by the first stage.
Reference: [9] <author> R. A. Renaut, </author> <title> A parallel multisplitting solution of the least squares problem, Numerical Linear Algebra with Applications, </title> <type> 4(1997) 1-21. 14 </type>
Reference-contexts: It turns out that this method has already been considered by Renaut <ref> [9] </ref>. She calls it Optimal Recombination Least Squares Multisplitting (ORLSMS), and she gives further developments based on the multisplitting method of O'Leary and White [7].
References-found: 9

