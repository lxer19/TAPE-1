URL: http://csg-www.lcs.mit.edu:8001/~xwshen/msthesis.ps
Refering-URL: http://csg-www.lcs.mit.edu:8001/Users/xwshen/
Root-URL: 
Title: Implementing Global Cache Coherence In flT -N G  
Author: by Xiaowei Shen 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in Partial Fulfillment of the Requirements for the Degree of Master of Science in Electrical Engineering and Computer Science at the  Signature of Author  Certified by Arvind Professor of Electrical Engineering and Computer Science Thesis Supervisor Accepted by Chairman, Frederic R. Morgenthaler Chairman, Departmental Committee on Graduate Students  
Note: c Massachusetts Institute of Technology  
Date: May, 1995  1995  May 26, 1995  
Affiliation: Massachusetts Institute of Technology  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> Anant Agarwal, Ricaardo Bianchini, David Chaiken, kirk Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Ken Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <note> In To appear in ISCA'95. </note>
Reference-contexts: On the other hand, shared memory machines need to incorporate message passing mechanisms to handle user-level messages, a feature usually associated with message passing systems. The MIT flT -N G project [5, 6, 13], the Stanford FLASH system [26, 19, 20] and the MIT Alewife machine <ref> [1, 2, 24] </ref> are examples of such architecture convergence. 13 1.1 The flT -N G Pro ject flT -N G is a PowerPC 620-based multiprocessor system supporting both user-level message passing and cache-coherent shared memory. <p> Thus it is possible that a request is kept retrying and cannot be serviced forever, although the probability this happens is very low. 1.4.2 The Alewife Machine The MIT Alewife machine <ref> [1, 12, 25] </ref> is a set of processing nodes connected in a mesh topology. Each node has a Sparcle processor running at 33 MHz, a floating-point coprocessor, 64K bytes of direct-mapped cache, 4M bytes of globally-shared memory, a cache controller and a network router.
Reference: [2] <author> Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, Godgrey D'Souza, and Mike Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Large-scale Multiprocessors. </title> <type> Technical report, </type> <institution> Laboratory For Computer Science, MIT, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: On the other hand, shared memory machines need to incorporate message passing mechanisms to handle user-level messages, a feature usually associated with message passing systems. The MIT flT -N G project [5, 6, 13], the Stanford FLASH system [26, 19, 20] and the MIT Alewife machine <ref> [1, 2, 24] </ref> are examples of such architecture convergence. 13 1.1 The flT -N G Pro ject flT -N G is a PowerPC 620-based multiprocessor system supporting both user-level message passing and cache-coherent shared memory.
Reference: [3] <author> Anant Agarwal, Richard Simon, John Hennessy, and Mark Horowitz. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th International Symposium On Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: The elimination of broadcast overcomes the major limitation on scaling cache coherent machines to multiprocessors with a large number of processors. The directory organization affects not only protocol complexity but also protocol performance. With appropriate hardware support, directories can be implemented using different structures <ref> [3, 10, 34, 38] </ref>. The full-map directory [29] and chained directory [37] are the most well-known organizations. Other approaches are also possible [11, 36], especially when the protocol is partially implemented in software.
Reference: [4] <author> Tilak Agerwala, Joanne Martin, Jamshed Mirza, David Sadler, Dan Dias, and Marc Snir. </author> <title> SP2 System Architecture. </title> <type> Technical report, </type> <institution> Power Parallel Division, IBM Corporation and T.J.Waston Research Center, IBM Corporation. </institution>
Reference-contexts: In message passing systems, processors access their own memories and communicate with other processors by explicitly sending and receiving messages through networks. Current message passing machines such as the SP2 system <ref> [4] </ref>, however, have high overhead for user-level message passing. In shared memory systems, a global address space is provided and processors exchange information and synchronize one another by accessing shared variables.
Reference: [5] <author> Boon Seong Ang, Arvind, and Derek Chiou. </author> <title> StarT the Next Generation: Integrating Global Caches And Dataflow Architecture. </title> <type> CSG Memo 354, </type> <institution> Laboratory For Computer Science, MIT, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: On the other hand, shared memory machines need to incorporate message passing mechanisms to handle user-level messages, a feature usually associated with message passing systems. The MIT flT -N G project <ref> [5, 6, 13] </ref>, the Stanford FLASH system [26, 19, 20] and the MIT Alewife machine [1, 2, 24] are examples of such architecture convergence. 13 1.1 The flT -N G Pro ject flT -N G is a PowerPC 620-based multiprocessor system supporting both user-level message passing and cache-coherent shared memory.
Reference: [6] <author> Boon Seong Ang, Derek Chiou, and Arvind. </author> <title> Issues in Building a Cache-coherent Distributed Shared Memory Machine Using Commercial SMPs. </title> <type> CSG Memo 365, </type> <institution> Laboratory For Computer Science, MIT, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: On the other hand, shared memory machines need to incorporate message passing mechanisms to handle user-level messages, a feature usually associated with message passing systems. The MIT flT -N G project <ref> [5, 6, 13] </ref>, the Stanford FLASH system [26, 19, 20] and the MIT Alewife machine [1, 2, 24] are examples of such architecture convergence. 13 1.1 The flT -N G Pro ject flT -N G is a PowerPC 620-based multiprocessor system supporting both user-level message passing and cache-coherent shared memory.
Reference: [7] <author> James K. Archibald. </author> <title> The Cache Coherence Problem in Shared-Memory Multiprocessors. </title> <type> Phd thesis, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> February </month> <year> 1987. </year>
Reference-contexts: Likewise, when a device broadcasts its intention to write a location which it does not own exclusively, other devices need to either invalidate or update their copies. With slightly different states and bus operations, many snoopy protocols have been proposed <ref> [18, 35, 22, 7] </ref>. As an example, Figure 1.4 presents the state transition diagram of the MESI snoopy protocol adopted in the PowerPC architecture [33].
Reference: [8] <author> Arvind, Rishiyur Nikhil, and Keshav Pingali. I-structures: </author> <title> Data Structures for Parallel Computing. </title> <type> CSG Memo 269, </type> <institution> Laboratory For Computer Science, MIT, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: For the performance reason, deferred lists are allowed to be maintained in a distributed fashion under restricted situations. 4.1 Introduction Of I-structures An I-structure <ref> [8] </ref> is a data structure in which each element maintains a presence bit that has two possible states: full or empty, indicating whether a valid data has been stored in the element. There are two basic I-structure operations: Iload and Istore.
Reference: [9] <author> G. A. Boughton. </author> <title> Arctic Routing Chip. </title> <booktitle> In Proceedings of Hot Interconnects II, </booktitle> <pages> pages 164-173, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: The NIU buffers can be memory-mapped to an application address space, allowing user programs to send and receive messages without kernel intervention. Through NIUs the 620s may communicate di 14 rectly with the Arctic routing chips at the bandwidth of up to 200M bytes/sec/link <ref> [9] </ref>. Cache-coherent shared memory is realized in software with very limited hardware assistance. The address capture device (ACD) allows a 620 to serve as a protocol processor to emulate a shared memory for user programs running on the other 620s within the site.
Reference: [10] <author> David Chaiken, Craig Fields, and Kiyoshi Kurihara. </author> <title> Directory-based Cache Coherence in Large-scale Multiprocessors. </title> <booktitle> Computer, </booktitle> <pages> pages 49-58, </pages> <month> June </month> <year> 1990. </year> <month> 93 </month>
Reference-contexts: The elimination of broadcast overcomes the major limitation on scaling cache coherent machines to multiprocessors with a large number of processors. The directory organization affects not only protocol complexity but also protocol performance. With appropriate hardware support, directories can be implemented using different structures <ref> [3, 10, 34, 38] </ref>. The full-map directory [29] and chained directory [37] are the most well-known organizations. Other approaches are also possible [11, 36], especially when the protocol is partially implemented in software.
Reference: [11] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the 4th International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The directory organization affects not only protocol complexity but also protocol performance. With appropriate hardware support, directories can be implemented using different structures [3, 10, 34, 38]. The full-map directory [29] and chained directory [37] are the most well-known organizations. Other approaches are also possible <ref> [11, 36] </ref>, especially when the protocol is partially implemented in software. Full-map directory In a full-map directory scheme, each directory entry keeps a complete record of which processors are sharing the block.
Reference: [12] <author> David L. Chaiken. </author> <title> Mechanisms and Interfaces for Software-Extended Coherent Shared Memory. </title> <type> Phd thesis, </type> <institution> Department of Electrical Engineering and Computer Science, MIT, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: Thus it is possible that a request is kept retrying and cannot be serviced forever, although the probability this happens is very low. 1.4.2 The Alewife Machine The MIT Alewife machine <ref> [1, 12, 25] </ref> is a set of processing nodes connected in a mesh topology. Each node has a Sparcle processor running at 33 MHz, a floating-point coprocessor, 64K bytes of direct-mapped cache, 4M bytes of globally-shared memory, a cache controller and a network router.
Reference: [13] <author> Derek Chiou, Boon S. Ang, Arvind, Michael J. Beckerle, Andy Boughton, Robert Greiner, James E. Hicks, and James C. Hoe. StarT-NG: </author> <title> Delivering Seamless Parallel Computing. </title> <type> CSG Memo 371, </type> <institution> Laboratory For Computer Science, MIT, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: On the other hand, shared memory machines need to incorporate message passing mechanisms to handle user-level messages, a feature usually associated with message passing systems. The MIT flT -N G project <ref> [5, 6, 13] </ref>, the Stanford FLASH system [26, 19, 20] and the MIT Alewife machine [1, 2, 24] are examples of such architecture convergence. 13 1.1 The flT -N G Pro ject flT -N G is a PowerPC 620-based multiprocessor system supporting both user-level message passing and cache-coherent shared memory.
Reference: [14] <author> William Dally. </author> <title> Virtual-Channel Flow Control. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 194-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: At each routing step, an appropriate channel can be chosen among the eligible outgoing channels, taking into account the network traffic workload and the diagnostic knowledge. The benefit becomes more attractive when cut 38 through [23] and virtual channel <ref> [14] </ref> techniques are used, which allow message flits to be contiguously spread out along many channels when the head flit is blocked.
Reference: [15] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Memory Access Buffering in Multiprocessors. </title> <booktitle> In Proceedings of the 13rd International Symposium On Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: In modern microprocessors, multiple reads and writes are allowed to be issued in parallel and completed out of order. To avoid unnecessary penalties imposed by sequential consistency, more relaxed memory models such as location consistency [16], weak consistency <ref> [15] </ref> and release consistency [17] can be implemented.
Reference: [16] <author> Guang-Rong Gao and Vivek Sarkar. </author> <title> Location Consistency Stepping Beyond the Barriers of Memory Coherence and Serializability. </title> <type> Technical Memo 78, </type> <institution> ACAPS Laboratory, School of Computer Science, McGill University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: In modern microprocessors, multiple reads and writes are allowed to be issued in parallel and completed out of order. To avoid unnecessary penalties imposed by sequential consistency, more relaxed memory models such as location consistency <ref> [16] </ref>, weak consistency [15] and release consistency [17] can be implemented.
Reference: [17] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In modern microprocessors, multiple reads and writes are allowed to be issued in parallel and completed out of order. To avoid unnecessary penalties imposed by sequential consistency, more relaxed memory models such as location consistency [16], weak consistency [15] and release consistency <ref> [17] </ref> can be implemented.
Reference: [18] <author> James R. Goodman. </author> <title> Using Cache Memory To Reduce Processor-Memory Traffic. </title> <booktitle> In Proceedings of the 10th International Symposium On Computer Architecture, </booktitle> <pages> pages 124-131, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: Likewise, when a device broadcasts its intention to write a location which it does not own exclusively, other devices need to either invalidate or update their copies. With slightly different states and bus operations, many snoopy protocols have been proposed <ref> [18, 35, 22, 7] </ref>. As an example, Figure 1.4 presents the state transition diagram of the MESI snoopy protocol adopted in the PowerPC architecture [33].
Reference: [19] <author> John Heinlein, Kourosh Charachorloo, Scott Dresser, and Anoop Gupta. </author> <title> Integration of Message Passing and Shared Memory in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 6th International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 38-50, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: On the other hand, shared memory machines need to incorporate message passing mechanisms to handle user-level messages, a feature usually associated with message passing systems. The MIT flT -N G project [5, 6, 13], the Stanford FLASH system <ref> [26, 19, 20] </ref> and the MIT Alewife machine [1, 2, 24] are examples of such architecture convergence. 13 1.1 The flT -N G Pro ject flT -N G is a PowerPC 620-based multiprocessor system supporting both user-level message passing and cache-coherent shared memory.
Reference: [20] <author> Mark Heinrich, Jeffrey Kuskin, David Ofelt, John Heinlein, Joel Baxter, Jaswinder Pal Singh, Richard Simoni, Kourosh Gharachorloo, David Nakahira, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Performance Impact of Flexibility in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 274-285, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: On the other hand, shared memory machines need to incorporate message passing mechanisms to handle user-level messages, a feature usually associated with message passing systems. The MIT flT -N G project [5, 6, 13], the Stanford FLASH system <ref> [26, 19, 20] </ref> and the MIT Alewife machine [1, 2, 24] are examples of such architecture convergence. 13 1.1 The flT -N G Pro ject flT -N G is a PowerPC 620-based multiprocessor system supporting both user-level message passing and cache-coherent shared memory.
Reference: [21] <author> IBM Corporation/Motorola Incorporated. </author> <title> PowerPC 620 Microprocessor Implementation Definition (Version 5.0). </title> <booktitle> 1994. </booktitle> <pages> 94 </pages>
Reference-contexts: Typically a site contains four network-endpoint-subsystems, each including a PowerPC 620, 4M bytes L2 cache and a network interface unit. The 620 is an aggressive 64-bit 4-way superscalar microprocessor with separate on-chip instruction and data caches, and a dedicated 128-bit L2 path to an external L2 cache <ref> [21] </ref>. A four-state MESI snoopy protocol is incorporated to maintain the cache coherence of multiple processors sitting on the same bus. The MIT prototype system will have eight sites connected by a Fat-Tree network [28].
Reference: [22] <author> R. H. Katz, S. J. Eggers, D. A. Wood, C. L. Perkins, and R. G. Sheldon. </author> <title> Imple--menting A Cache Consistency Protocol. </title> <booktitle> In Proceedings of the 12nd International Symposium On Computer Architecture, </booktitle> <pages> pages 276-283, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Likewise, when a device broadcasts its intention to write a location which it does not own exclusively, other devices need to either invalidate or update their copies. With slightly different states and bus operations, many snoopy protocols have been proposed <ref> [18, 35, 22, 7] </ref>. As an example, Figure 1.4 presents the state transition diagram of the MESI snoopy protocol adopted in the PowerPC architecture [33].
Reference: [23] <author> Parviz Kermani and Leonard Kleinrock. </author> <title> Virtual Cut-Through: A New Computer Communication Switching Technique. </title> <journal> Computer Networks, </journal> <volume> 3 </volume> <pages> 267-286, </pages> <year> 1979. </year>
Reference-contexts: At each routing step, an appropriate channel can be chosen among the eligible outgoing channels, taking into account the network traffic workload and the diagnostic knowledge. The benefit becomes more attractive when cut 38 through <ref> [23] </ref> and virtual channel [14] techniques are used, which allow message flits to be contiguously spread out along many channels when the head flit is blocked.
Reference: [24] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Proceedings of the 4th Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: On the other hand, shared memory machines need to incorporate message passing mechanisms to handle user-level messages, a feature usually associated with message passing systems. The MIT flT -N G project [5, 6, 13], the Stanford FLASH system [26, 19, 20] and the MIT Alewife machine <ref> [1, 2, 24] </ref> are examples of such architecture convergence. 13 1.1 The flT -N G Pro ject flT -N G is a PowerPC 620-based multiprocessor system supporting both user-level message passing and cache-coherent shared memory.
Reference: [25] <author> David Kranz, Beng-Hong Lim, Anant Agarwal, and Donald Yeung. </author> <title> Low-cost Support for Fine-grain Synchronization in Multiprocessors. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference-contexts: Thus it is possible that a request is kept retrying and cannot be serviced forever, although the probability this happens is very low. 1.4.2 The Alewife Machine The MIT Alewife machine <ref> [1, 12, 25] </ref> is a set of processing nodes connected in a mesh topology. Each node has a Sparcle processor running at 33 MHz, a floating-point coprocessor, 64K bytes of direct-mapped cache, 4M bytes of globally-shared memory, a cache controller and a network router.
Reference: [26] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, Johe Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: On the other hand, shared memory machines need to incorporate message passing mechanisms to handle user-level messages, a feature usually associated with message passing systems. The MIT flT -N G project [5, 6, 13], the Stanford FLASH system <ref> [26, 19, 20] </ref> and the MIT Alewife machine [1, 2, 24] are examples of such architecture convergence. 13 1.1 The flT -N G Pro ject flT -N G is a PowerPC 620-based multiprocessor system supporting both user-level message passing and cache-coherent shared memory.
Reference: [27] <author> Leslie Lamport. </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: And so is the possible output of processor D. An interesting question is if it is correct that processor C outputs (1; 2) while processor D outputs (2; 1)? The simplest memory consistency model is sequential consistency <ref> [27] </ref>, in which all writes to a memory location are serialized in some order and are performed in that order with respect to any processor. In modern microprocessors, multiple reads and writes are allowed to be issued in parallel and completed out of order.
Reference: [28] <author> Charles E. Leiserson. Fat-Trees: </author> <title> Universal Networks for Hardware-Efficient Supercomputing. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):892-901, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: A four-state MESI snoopy protocol is incorporated to maintain the cache coherence of multiple processors sitting on the same bus. The MIT prototype system will have eight sites connected by a Fat-Tree network <ref> [28] </ref>. Fine-grain user-level message passing is supported with network interface units (NIUs), which are tightly coupled to the L2 cache interfaces. The NIU buffers can be memory-mapped to an application address space, allowing user programs to send and receive messages without kernel intervention.
Reference: [29] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The Directory-based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The directory organization affects not only protocol complexity but also protocol performance. With appropriate hardware support, directories can be implemented using different structures [3, 10, 34, 38]. The full-map directory <ref> [29] </ref> and chained directory [37] are the most well-known organizations. Other approaches are also possible [11, 36], especially when the protocol is partially implemented in software. Full-map directory In a full-map directory scheme, each directory entry keeps a complete record of which processors are sharing the block.
Reference: [30] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessy. </author> <title> The DASH Prototype: Implementation and Performance. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Special accesses conform to either sequential consistency or location consistency with respect to one another. 1.4 Related Work 1.4.1 The DASH System The Stanford DASH system <ref> [32, 30, 31] </ref> consists of a set of processing clusters connected by a mesh interconnection network. In the prototype machine, each cluster is a Silicon Graphics Power Station 4D/340, which contains 4 MIPS R3000 processors and R3010 24 floating-point coprocessors running at 33 MHz.
Reference: [31] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessy. </author> <title> The DASH Prototype: Logic Overhead and Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 41-61, </pages> <month> Jan-uary </month> <year> 1993. </year>
Reference-contexts: Special accesses conform to either sequential consistency or location consistency with respect to one another. 1.4 Related Work 1.4.1 The DASH System The Stanford DASH system <ref> [32, 30, 31] </ref> consists of a set of processing clusters connected by a mesh interconnection network. In the prototype machine, each cluster is a Silicon Graphics Power Station 4D/340, which contains 4 MIPS R3000 processors and R3010 24 floating-point coprocessors running at 33 MHz.
Reference: [32] <author> Daniel E. Lenoski. </author> <title> The Design And Analysis of DASH: A Scalable Directory-Based Multiprocessor. </title> <type> Phd thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Stanford University, </institution> <month> February </month> <year> 1992. </year> <month> 95 </month>
Reference-contexts: In shared memory systems, a global address space is provided and processors exchange information and synchronize one another by accessing shared variables. Although programming becomes much easier with a uniform address space, current shared memory machines like the DASH prototype <ref> [32] </ref> have been criticized for their hardware complexity to maintain cache coherence and lack of flexibility for various shared patterns of different applications. Recent research experience suggests that multiprocessor systems should incorporate both fast user-level message passing and efficient shared memory assistance, without sacrificing favorable operating system functionalities. <p> Special accesses conform to either sequential consistency or location consistency with respect to one another. 1.4 Related Work 1.4.1 The DASH System The Stanford DASH system <ref> [32, 30, 31] </ref> consists of a set of processing clusters connected by a mesh interconnection network. In the prototype machine, each cluster is a Silicon Graphics Power Station 4D/340, which contains 4 MIPS R3000 processors and R3010 24 floating-point coprocessors running at 33 MHz.
Reference: [33] <institution> IBM Microelectronics and Motorola. </institution> <note> PowerPC 601 RISC Microprocessor User's Manual. </note> <year> 1993. </year>
Reference-contexts: With slightly different states and bus operations, many snoopy protocols have been proposed [18, 35, 22, 7]. As an example, Figure 1.4 presents the state transition diagram of the MESI snoopy protocol adopted in the PowerPC architecture <ref> [33] </ref>. MESI has four possible states for 17 each cache line: Invalid , Shared , Exclusive and Modified . * Invalid: The addressed location is not resident in the cache. * Shared: The addressed location is resident in the cache and coherent with memory.
Reference: [34] <author> Brain W. O'Krafka and A. Richard. </author> <title> An Empirical Evaluation of Two Memory-Efficient Directory Methods. </title> <booktitle> In Proceedings of the 17th International Symposium On Computer Architecture, </booktitle> <pages> pages 138-147, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The elimination of broadcast overcomes the major limitation on scaling cache coherent machines to multiprocessors with a large number of processors. The directory organization affects not only protocol complexity but also protocol performance. With appropriate hardware support, directories can be implemented using different structures <ref> [3, 10, 34, 38] </ref>. The full-map directory [29] and chained directory [37] are the most well-known organizations. Other approaches are also possible [11, 36], especially when the protocol is partially implemented in software.
Reference: [35] <author> Mark S. Papamarcos and Janak H. Patel. </author> <title> A Low-Overhead Coherence Solution For Multiprocessors With Private Cache Memories. </title> <booktitle> In Proceedings of the 11th International Symposium On Computer Architecture, </booktitle> <pages> pages 348-354, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Likewise, when a device broadcasts its intention to write a location which it does not own exclusively, other devices need to either invalidate or update their copies. With slightly different states and bus operations, many snoopy protocols have been proposed <ref> [18, 35, 22, 7] </ref>. As an example, Figure 1.4 presents the state transition diagram of the MESI snoopy protocol adopted in the PowerPC architecture [33].
Reference: [36] <author> Richard Simoni and Mark Horowitz. </author> <title> Modeling the Performance of Limited Pointers Directories for Cache Coherence. </title> <booktitle> In Proceedings of the 18th International Symposium On Computer Architecture, </booktitle> <pages> pages 309-318, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The directory organization affects not only protocol complexity but also protocol performance. With appropriate hardware support, directories can be implemented using different structures [3, 10, 34, 38]. The full-map directory [29] and chained directory [37] are the most well-known organizations. Other approaches are also possible <ref> [11, 36] </ref>, especially when the protocol is partially implemented in software. Full-map directory In a full-map directory scheme, each directory entry keeps a complete record of which processors are sharing the block.
Reference: [37] <institution> IEEE Computer Society. IEEE Standard for Scalable Coherent Interface. </institution> <year> 1993. </year>
Reference-contexts: The directory organization affects not only protocol complexity but also protocol performance. With appropriate hardware support, directories can be implemented using different structures [3, 10, 34, 38]. The full-map directory [29] and chained directory <ref> [37] </ref> are the most well-known organizations. Other approaches are also possible [11, 36], especially when the protocol is partially implemented in software. Full-map directory In a full-map directory scheme, each directory entry keeps a complete record of which processors are sharing the block.
Reference: [38] <author> Per Stenstrom. </author> <title> A Survey of Cache Coherence Schemes for Multiprocessors. </title> <booktitle> Computer, </booktitle> <pages> pages 12-24, </pages> <month> June </month> <year> 1990. </year> <month> 96 </month>
Reference-contexts: The elimination of broadcast overcomes the major limitation on scaling cache coherent machines to multiprocessors with a large number of processors. The directory organization affects not only protocol complexity but also protocol performance. With appropriate hardware support, directories can be implemented using different structures <ref> [3, 10, 34, 38] </ref>. The full-map directory [29] and chained directory [37] are the most well-known organizations. Other approaches are also possible [11, 36], especially when the protocol is partially implemented in software.
References-found: 38

