URL: http://www.isi.edu/teamcore/tambe/papers/99/robocup-agents99.ps
Refering-URL: http://www.isi.edu/teamcore/tambe/agent.html
Root-URL: http://www.isi.edu
Email: robocup-sim@isi.edu  
Title: On being a teammate: Experiences acquired in the design of RoboCup teams  
Author: Stacy Marsella, Jafar Adibi, Yaser Al-Onaizan, Gal A. Kaminka, Ion Muslea, Milind Tambe 
Address: 4676 Admiralty Way Marina del Rey,CA 90292  
Affiliation: Information Sciences Institute and Computer Science Department University of Southern California  
Abstract: Increasingly, multi-agent systems are being designed for a variety of complex, dynamic domains. Effective agent interactions in such domains raise some of the most fundamental research challenges for agent-based systems, in teamwork, multi-agent learning and agent modelling. The RoboCup research initiative, particularly the simulation league, has been proposed to pursue such multi-agent research challenges, using the common testbed of simulation soccer. Despite the significant popularity of RoboCup within the research community, general lessons have not often been extracted from participation in RoboCup. This is what we attempt to do here. We have fielded two teams, ISIS97 and ISIS98, in RoboCup competitions. These teams have been in the top four teams in these competitions. We compare the teams, and attempt to analyze and generalize the lessons learned. This analysis reveals several surprises, pointing out lessons for teamwork and for multi-agent learning. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. F. Bersano-Begey, P. G. Kenny, and E. H. Durfee. </author> <title> Agent teamwork, adaptive learning, and adversarial planning in robocup using a prs architecture. In RoboCup-97: The first robot world cup soccer games and conferences. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, Germany, </address> <year> 1998. </year>
Reference-contexts: Their agents synchronize their individual beliefs periodically in a fixed manner, in contrast with ISIS's STEAM in which communications are issued dynamically and can be parameterized based on the domain of deployment. Another RoboCup effort focusing on explicit team plans and roles is <ref> [1] </ref>. They define levels of teamwork, starting at basic roles (which are static throughout the game), and building on top of those with formations and team plans for carrying out more complex tactics. These teamwork levels determine the agents coordination responsibilities and prioritize its actions.
Reference: [2] <author> S. Ch'ng and L. Padgham. </author> <title> Team description: Royal merl-bourne knights. In RoboCup-97: The first robot world cup soccer games and conferences. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, Germany, </address> <year> 1998. </year>
Reference-contexts: Some researchers investigating teamwork in RoboCup have used explicit team plans and roles, but they have relied on domain-dependent communication and coordination. A typical example includes work by Chng and Padgham <ref> [2] </ref>. They present an elaborate analysis of roles in motivating teamwork and team plans. In this scheme, agents dynamically adopt and abandon roles in pre-defined tactics. The responsibilities and actions of each agent are determined by its current role in the current plan.
Reference: [3] <author> P. R. Cohen and H. J. Levesque. </author> <title> Teamwork. </title> <journal> Nous, </journal> <volume> 35, </volume> <year> 1991. </year>
Reference-contexts: Team operators constitute activities that the agent takes on jointly as part of a team or subteam and are shown in []. ISIS97 and ISIS98 share the same general-purpose framework for teamwork modelling, STEAM [13]. STEAM models team members' responsibilities and joint commitments <ref> [3] </ref> in a domain-independent fashion. As a result, it enables team members to autonomously reason about coordination and communication, improving teamwork flexibility. The [Defend-Goal] team operator demonstrates part of STEAM. 1 1 Another part of STEAM deals with team reorganization, which are team operators, others are individual operators.
Reference: [4] <author> T. Dean, K. Basye, and J. Skewchuk. </author> <title> Reinforcement learning for planning and control. </title> <booktitle> In Machine Learning Methods for Planning, </booktitle> <pages> pages 67-92. </pages> <publisher> Morgan Kaufman, </publisher> <address> San Francisco, </address> <year> 1993. </year>
Reference-contexts: by the question: what would happen if players in ISIS98 are allowed to learn plans themselves, and what would that learning tell us? In particular, would there be differences in what is learned across different players? Would there be differences across different opponents? We therefore pursued a reinforcement learning approach <ref> [12, 4] </ref> to enable players to adapt their intercept online, under actual playing conditions using just the perceptual information provided by the server to the player: the ball's current direction, change in direction, distance.
Reference: [5] <author> H. Kitano, M. Asada, Y. Kuniyoshi, I. Noda, and E. Osawa. </author> <title> Robocup: The robot world cup initiative. </title> <booktitle> In Proceedings of the first international conference on autonomous agents, </booktitle> <year> 1997. </year>
Reference-contexts: To pursue research challenges such as these and stimulate research in multi-agents in general, the RoboCup research initiative has proposed simulation and robotic soccer as a common, unified testbed for multi-agent research <ref> [5] </ref> (www.robocup.org). The RoboCup initiative has proved extremely popular with researchers, with annual competitions in several different leagues. Of particular interest in this paper is the simulation league, which has attracted the largest number of participants.
Reference: [6] <author> H. Kitano, M. Tambe, P. Stone, S. Coradesci, H. Matsubara, M. Veloso, I. Noda, E. Osawa, and M. Asada. </author> <title> The robocup synthetic agents' challenge. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: Of particular interest in this paper is the simulation league, which has attracted the largest number of participants. The stated research goals of the simulation league are to investigate the areas of multi-agent teamwork, agent modelling, and multi-agent learning <ref> [6] </ref>. Yet, the lessons learned by researchers participating in RoboCup, particularly the simulation league, have largely not been reported in a form that would be accessible to the research community at large. There are just a few notable exceptions [11].
Reference: [7] <author> S. Luke, Hohn C., J. Farris, G. Jackson, and J. Hendler. </author> <title> Co-evolving soccer softbot team coordination with genetic programming. In RoboCup-97: The first robot world cup soccer games and conferences. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, Germany, </address> <year> 1998. </year>
Reference: [8] <author> J. R. Quinlan. C4.5: </author> <title> Programs for machine learning. </title> <publisher> Mor-gan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: With this approach, different modules (skills) within individual agents were learned separately, using different learning techniques. In particular, one of the skills, to pick a direction to shoot into the opponents' goal while avoiding opponents, was learned off-line using C4.5 <ref> [8] </ref>. Another skill, to intercept the ball, used a mix of off-line and on-line learning. One of the key surprises here was the degree to which individual agents specialized in their individual roles. <p> To address these problems, we decided to rely on automated, o*ine learning of the shooting rules. A human expert created a set of shooting situations, and selected the optimal shooting direction for each such situation. The learning system trained on these shooting scenarios. C4.5 <ref> [8] </ref> was used as the learning system, in part because it has the appropriate expressive power to express game situations and can handle both missing attributes and a large number of training cases.
Reference: [9] <author> A. S. Rao, A. Lucas, D. Morley, M. Selvestrel, and G. Mur-ray. </author> <title> Agent-oriented architecture for air-combat simulation. </title> <type> Technical Report Technical Note 42, </type> <institution> The Australian Artificial Intelligence Institute, </institution> <year> 1993. </year>
Reference-contexts: For each of these research problems, the uncertainty and the presence of multiple cooperative and non-cooperative agents, only conspires to exacerbate the difficulty. Consider for instance the challenge of multi-agent teamwork, which has become a critical requirement across a wide range of multi-agent domains <ref> [14, 9, 15] </ref>. Here, an agent team must address the challenge of designing roles for individuals (i.e., dividing up team responsibilities based on individuals' capabilities), doing so with fairness, and reorganizing roles based on new information.
Reference: [10] <author> P. Stone and M. Veloso. </author> <title> Task decomposition and dynamic role assignment for real-time strategic teamwork. </title> <booktitle> In Proceedings of the international workshop on Agent theories, Architectures and Languages, </booktitle> <year> 1998. </year>
Reference-contexts: A similar scheme is used by Stone and Veloso <ref> [10] </ref>. They offer an approach to managing flexible formations and roles within those formations, allowing agents to switch roles and formations dynamically in a domain-dependent manner.
Reference: [11] <author> P. Stone and M. Veloso. </author> <title> Using decision tree confidence factors for multiagent control. In RoboCup-97: The first robot world cup soccer games and conferences. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, Germany, </address> <year> 1998. </year>
Reference-contexts: Yet, the lessons learned by researchers participating in RoboCup, particularly the simulation league, have largely not been reported in a form that would be accessible to the research community at large. There are just a few notable exceptions <ref> [11] </ref>. <p> All communication between players are done via the server, and are subject to limitations such as bandwidth, range and latencies. Figure 1 shows a snapshot of the soccer server with two competing teams: CMUnited97 <ref> [11] </ref> versus our ISIS team. In RoboCup97, ISIS97 won the third place prize (out of 32 teams). It won five soccer games in the process, and lost one. In RoboCup98, ISIS98 came in fourth (out of 37 teams). <p> Our application learning in ISIS agents is similar to some of the other investigations of learning in RoboCup agents. For instance, Luke et al.[7] use genetic programming to build agents that learn to use their basic individual skills in coordination. Stone and Veloso <ref> [11] </ref> present a related approach, in which the agents learn a decision tree which enables them to select a recipient for a pass. 7 Lessons Learned from RoboCup Challenges of teamwork and multi-agent learning are critical in the design of multi-agent systems, and these are two of the critical research challenges
Reference: [12] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: by the question: what would happen if players in ISIS98 are allowed to learn plans themselves, and what would that learning tell us? In particular, would there be differences in what is learned across different players? Would there be differences across different opponents? We therefore pursued a reinforcement learning approach <ref> [12, 4] </ref> to enable players to adapt their intercept online, under actual playing conditions using just the perceptual information provided by the server to the player: the ball's current direction, change in direction, distance.
Reference: [13] <author> M. Tambe. </author> <title> Towards flexible teamwork. </title> <journal> Journal of Artificial Intelligence Research (JAIR), </journal> <volume> 7 </volume> <pages> 83-124, </pages> <year> 1997. </year>
Reference-contexts: The analysis does reveal several general lessons in the areas of teamwork and multi-agent learning. With respect to teamwork, in the past, we have reported on our ability to reuse STEAM, a general model of teamwork, in RoboCup <ref> [13] </ref>. This paper takes a step further, evaluating the effectiveness of STEAM in RoboCup, to improve our understanding of the utility of general teamwork models. It also provides an analysis of techniques for the division of team responsibilities among individuals. <p> Team operators constitute activities that the agent takes on jointly as part of a team or subteam and are shown in []. ISIS97 and ISIS98 share the same general-purpose framework for teamwork modelling, STEAM <ref> [13] </ref>. STEAM models team members' responsibilities and joint commitments [3] in a domain-independent fashion. As a result, it enables team members to autonomously reason about coordination and communication, improving teamwork flexibility. <p> In this way, agents coordinate their defense of the goal. All the communication decisions are handled automatically by STEAM. 4 Analysis of Teamwork 4.1 Lessons in (Re)using a Teamwork Model In past work, we have focused on STEAM's reuse in our ISIS teams <ref> [13] </ref>, illustrating that a significant portion (35-45% when measured in terms of the rules) was reused, and that it enabled reduced development time. The use of the teamwork model is a shared similarity between ISIS97 and ISIS98.
Reference: [14] <author> M. Tambe, W. L. Johnson, R. Jones, F. Koss, J. E. Laird, P. S. Rosenbloom, and K. Schwamb. </author> <title> Intelligent agents for interactive simulation environments. </title> <journal> AI Magazine, </journal> <volume> 16(1), </volume> <month> Spring </month> <year> 1995. </year>
Reference-contexts: For each of these research problems, the uncertainty and the presence of multiple cooperative and non-cooperative agents, only conspires to exacerbate the difficulty. Consider for instance the challenge of multi-agent teamwork, which has become a critical requirement across a wide range of multi-agent domains <ref> [14, 9, 15] </ref>. Here, an agent team must address the challenge of designing roles for individuals (i.e., dividing up team responsibilities based on individuals' capabilities), doing so with fairness, and reorganizing roles based on new information. <p> The lower-level does not make any decisions. Instead, all decision-making rests with the higher level, implemented in the Soar integrated AI architecture <ref> [14] </ref>. Once the Soar-based higher-level reaches a decision, it communicates with the lower-level, which then sends the relevant action information to the simulator. Soar's operation involves dynamically executing an operator (reactive plan) hierarchy. The operator hierarchy shown in Figure 2 illustrates a portion of the operator hierarchy for ISIS player-agents.
Reference: [15] <author> M. Williamson, K. Sycara, and K. Decker. </author> <title> Executing decision-theoretic plans in multi-agent environments. </title> <booktitle> In Proceedings of the AAAI Fall Symposium on Plan Execution: Problems and Issues, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: For each of these research problems, the uncertainty and the presence of multiple cooperative and non-cooperative agents, only conspires to exacerbate the difficulty. Consider for instance the challenge of multi-agent teamwork, which has become a critical requirement across a wide range of multi-agent domains <ref> [14, 9, 15] </ref>. Here, an agent team must address the challenge of designing roles for individuals (i.e., dividing up team responsibilities based on individuals' capabilities), doing so with fairness, and reorganizing roles based on new information.
Reference: [16] <author> K. Yokota, K. Ozako, Matsumoto A., T. Fujii, Asama H., and I. Endo. </author> <title> Cooperation towards team play. In RoboCup-97: The first robot world cup soccer games and conferences. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, Germany, </address> <year> 1998. </year>
References-found: 16

