URL: http://www.ics.uci.edu/~rickl/icml-hl-camera-ready.ps
Refering-URL: http://www.ics.uci.edu/~rickl/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: rickl@ics.uci.edu  
Title: On the Learnability of the Uncomputable  
Author: Richard H. Lathrop 
Note: In Proceedings of the 13th International Conference on Machine Learning, pp. 302-309, Bari, Italy, 3-6 July 1996, ed. Saitta, L., Morgan Kaufman Publishers,  
Address: Irvine, CA 92717-3425  San Francisco.  
Affiliation: Information and Computer Science Dept. University of California  
Abstract: Within Valiant's model of learning as formalized by Kearns, we show that computable total predicates for two formally uncomputable problems (the classical Halting Problem, and the Halting Problem relative to a specified oracle) are formally learnable from examples, to arbitrarily high accuracy with arbitrarily high confidence, under any probability distribution. The Halting Problem relative to the oracle is learnable in time polynomial in the measures of accuracy, confidence, and the length of the learned predicate. The classical Halting Problem is learnable in expected time polynomial in the measures of accuracy, confidence, and the (1 *=16) th percentile length and run-time of programs which do halt on their inputs (these quantities are always finite). Equivalently, the mean length and run-time may be substituted for the percentile values in the time complexity statement. The proofs are constructive. While the problems are learnable, they are not poly-nomially learnable, even though we do derive polynomial time bounds on the learning algo rithm.
Abstract-found: 1
Intro-found: 1
Reference: <author> Amsterdam, J. </author> <year> (1988). </year> <title> Some philosophical problems with formal learning theory. </title> <booktitle> Proc. of the Seventh Natl. Conf. on Artificial Intelligence (AAAI'88), </booktitle> <volume> vol. (2), </volume> <pages> pp. 580-584. </pages>
Reference-contexts: Amsterdam <ref> (Amsterdam 1988) </ref> discusses limitations of the framework. The problem of language identification in the limit differs from this work in finding an exact rather than probabilistic predicate, and in lacking polynomial time complexity bounds.
Reference: <author> Angluin, D. </author> <year> (1987). </year> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Control 75, </journal> (2):87-106. 
Reference: <author> Angluin, D., and Smith, C.H. </author> <year> (1983). </year> <title> Inductive infer-ence: theory and methods. </title> <journal> ACM Computing Surveys 15, </journal> (3):237-269. 
Reference: <author> Benedek, G.M., and Itai, A. </author> <year> (1991). </year> <title> Learnability with respect to fixed distributions. </title> <journal> Theoretical Computer Sci. </journal> <volume> 86, </volume> (2):377-389. 
Reference-contexts: Fulk and Jain (Fulk and Jain 1994) consider approximate inference under which the class of recursive functions is identifiable in the limit, but do not establish polynomial time bounds. Benedek and Itai <ref> (Benedek and Itai 1991) </ref> consider learnability when the distribution is fixed and known. They mention the possibility that the target class may be uncomputable but do not explicitly consider the case.
Reference: <author> Blum, L., and Blum, M. </author> <year> (1975). </year> <title> Toward a mathematical theory of inductive inference. </title> <journal> Information and Control 28, </journal> (2):125-155. 
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and War-muth, M.K. </author> <year> (1989). </year> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> J. of the Assoc. Computing Mach. </journal> <volume> 36, </volume> (4):929-965. 
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and War-muth, M.K. </author> <year> (1987). </year> <note> Occam's razor. Information Processing Letters 24 </note> , (6):377-380. 
Reference: <author> Cherniavsky, J.C., and Smith, C.H. </author> <year> (1987). </year> <title> A recursion theoretic approach to program testing. </title> <journal> IEEE Transactions on Software Engineering SE-13, </journal> (7):777-784. 
Reference-contexts: Gold (Gold 1967) showed that even the class of all regular languages is not inferable from positive data. Gasarch and Smith (Garasch and Smith 1992) consider identification in the limit under a model in which the learner is permitted to ask undecidable questions. Cherniavsky and Smith <ref> (Cherniavsky and Smith 1987) </ref> investigate the relationship between recursive enu-merability, inductive inference and program testing (which includes infinite loops), but do not extend it to the probabilistic learning considered here.
Reference: <author> Chernoff, H. </author> <year> (1952). </year> <title> A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. </title> <journal> Ann. Math. Stat. </journal> <note> 23, 493-509; see P. </note> <author> Erdos and J. Spencer, </author> <year> (1974), </year> <title> Probabilistic Methods in Combinatorics, </title> <publisher> Academic Press, </publisher> <address> New York, p. </address> <month> 18. </month>
Reference-contexts: Fact 6 P n P m+1 where m and n are non-negative integers and the co efficients c j are given by c 0 = 0 2 j 1 k=j+1 j 1 3 Fact 7 (Chernoff Bound <ref> (Chernoff 1952) </ref>, (Valiant 1984)) In n independent trials, each with probability of success at least p, the probability that there are at most k successes, where k &lt; np, is at most nk k 2.2 LEMMAS Lemma 1 In Fact 7, if p = (1 b*) and k = n (1
Reference: <author> Cutland, N. </author> <year> (1980). </year> <title> Computability: An Introduction to Recursive Function Theory, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK. </address>
Reference: <author> Ehrenfeucht, A., Haussler, D., Kearns, M., and Valiant, L. </author> <year> (1989). </year> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Control 82, </journal> (3):247-261. 
Reference: <author> Ehrenfeucht, A.,and Haussler, D. </author> <year> (1988). </year> <title> Learning decision trees from random examples. </title> <booktitle> Proc. 1988 Workshop on Computational Machine Learning, </booktitle> <pages> 182-194. </pages>
Reference-contexts: By separating the definition of learnabil-ity from statements about time complexity, Kearns is able to formally discuss and compare algorithms across a spectrum of complexities (for example, his analysis encompasses the super-polynomial but sub-exponential learning algorithm by Ehrenfeucht and Haussler <ref> (Ehrenfeucht and Haussler 1988) </ref> for decision trees with at most a fixed polynomial number of nodes).
Reference: <author> Garasch, W.I., and Smith, C.H. </author> <year> (1992). </year> <title> Learning via queries. </title> <journal> J. of the Assoc. Computing Mach. </journal> <volume> 39, </volume> (3):649-674. 
Reference-contexts: The problem of language identification in the limit differs from this work in finding an exact rather than probabilistic predicate, and in lacking polynomial time complexity bounds. Gold (Gold 1967) showed that even the class of all regular languages is not inferable from positive data. Gasarch and Smith <ref> (Garasch and Smith 1992) </ref> consider identification in the limit under a model in which the learner is permitted to ask undecidable questions.
Reference: <author> Gold, E.M. </author> <year> (1967). </year> <title> Language identification in the limit. </title> <journal> Information and Control 10, </journal> (5):447-474. 
Reference-contexts: Amsterdam (Amsterdam 1988) discusses limitations of the framework. The problem of language identification in the limit differs from this work in finding an exact rather than probabilistic predicate, and in lacking polynomial time complexity bounds. Gold <ref> (Gold 1967) </ref> showed that even the class of all regular languages is not inferable from positive data. Gasarch and Smith (Garasch and Smith 1992) consider identification in the limit under a model in which the learner is permitted to ask undecidable questions.
Reference: <editor> Herken, R., Ed. </editor> <year> (1988). </year> <title> The Universal Turing Machine: a Half-Century Survey, </title> <publisher> Clarendon Press, Oxford University Press, </publisher> <address> New York. </address>
Reference: <author> Hopcroft, J. and Ullman, J. </author> <year> (1979). </year> <title> Introduction to Automata Theory, Languages, and Computation, </title> <publisher> Addison-Wesley. </publisher>
Reference: <author> Kearns, M. J. </author> <year> (1990). </year> <title> The Computational Complexity of Machine Learning, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, USA. </address>
Reference-contexts: Valiant (Valiant 1984) proposed a formal analysis of learning as the phenomenon of knowledge acquisition in the absence of explicit programming. The methodology was systematically organized into a consistent and rigorous framework by Kearns <ref> (Kearns 1990) </ref>, which we follow. We consider two uncomputable problems and show that they are learnable within this formal model of learnability. <p> Finally, section 5 states some important limitations on our results. Major limitations are an impoverished knowledge representation capability, and the fact that positive instances are a priori known to be classified relative to the Halting Problem (these correspond to severe restrictions on what Kearns <ref> (Kearns 1990) </ref> terms the hypothesis class and the target class, respectively). 1.2 BACKGROUND Let C (the target class) and H (the hypothesis class) be representation classes over X (the domain, or instance space). <p> Then C is learnable from examples by H <ref> (Kearns 1990, p. 11) </ref> if there is an algorithm A with access to POS and NEG, taking inputs * (the accu racy parameter, 0 &lt; * &lt; 1) and ffi (the confidence parameter, 0 &lt; ffi &lt; 1), such that for any c 2 C, any probability distributions D + and
Reference: <author> Kearns, M. J., and Valiant, L. </author> <year> (1994). </year> <title> Cryptographic limitations on learning boolean formulae and finite automata. </title> <journal> J. of the Assoc. Computing Mach. </journal> <volume> 41, </volume> (1):67-95. 
Reference: <author> Lewis, H.R., and Papadimitriou, C.H. </author> <year> (1981). </year> <title> Elements of the Theory of Computation. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, USA, </address> <year> (1981). </year>
Reference: <author> Linial, N., Mansour, Y., and Rivest, R. </author> <year> (1991). </year> <title> Results on learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Information and Computation 90, </journal> (1):33-49. 
Reference: <author> Machlin, R., and Stout, Q.F. </author> <year> (1990). </year> <title> The complex behavior of complex machines. </title> <journal> Physica D 42, </journal> <pages> 85-98. </pages>
Reference-contexts: Benedek and Itai (Benedek and Itai 1991) consider learnability when the distribution is fixed and known. They mention the possibility that the target class may be uncomputable but do not explicitly consider the case. The Halting Probability Problem <ref> (Machlin and Stout 1990) </ref>, which uses the probability that a randomly drawn TM will halt to investigate randomness in algorithmic information theory, is only distantly related to the present work. 2 MATHEMATICAL PRELIMINARIES To be of interest, a learning system must perform better than a fair coin-flip.
Reference: <author> Minsky, M. </author> <year> (1967). </year> <title> Computation: Finite and Infinite Machines, </title> <publisher> (1967),. </publisher>
Reference: <author> Pitt, L. </author> <year> (1990). </year> <title> Special issue on computational learning theory | introduction. </title> <journal> Machine Learning 5, </journal> (2):117-120. 
Reference-contexts: The formal approach has been widely explored by the machine learning community <ref> (Pitt 1990) </ref>, and a great deal is known about necessary conditions, limitations, and bounds (Blumer et al. 1989; Blumer et al. 1987; Ehrenfeucht et al. 1989; Kearns and Valiant 1994; Linial et al. 1991; Pitt and Valiant 1988; Shvaytser 1990). Amsterdam (Amsterdam 1988) discusses limitations of the framework.
Reference: <author> Pitt, L., and Valiant, L.G. </author> <year> (1988). </year> <title> Computational limitations on learning from examples. </title> <journal> J. of the Assoc. Computing Mach. </journal> <volume> 35, </volume> (4):965-984. 
Reference: <author> Shvaytser, H. </author> <year> (1990). </year> <title> A necessary condition for learning from positive examples. </title> <journal> Machine Learning 5, </journal> (1):101-113. 
Reference: <author> Turing, A. M. </author> <year> (1936). </year> <title> On computable numbers, with an application to the Entscheidungsproblem. </title> <journal> Proc. of the London Math. Soc., ser. </journal> <volume> 2, vol. </volume> <pages> 42 </pages> <note> 230-265; reprinted in M. </note> <editor> Davis, ed., </editor> <booktitle> (1965), The Uncomputable pp. </booktitle> <pages> 115-154, </pages> <publisher> Raven Press, Hewlett, </publisher> <address> New York. </address>
Reference-contexts: 1 INTRODUCTION Can we learn what we cannot compute? We will show that in at least some cases the answer is "yes." The widely accepted formal model for computation is the Turing machine (TM), and the Halting Problem is the canonical uncomputable task <ref> (Turing 1936) </ref>. Valiant (Valiant 1984) proposed a formal analysis of learning as the phenomenon of knowledge acquisition in the absence of explicit programming. The methodology was systematically organized into a consistent and rigorous framework by Kearns (Kearns 1990), which we follow.
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Comm. of the Assoc. Computing Mach. </journal> <volume> 27, </volume> (11):1134-1142. 
Reference-contexts: 1 INTRODUCTION Can we learn what we cannot compute? We will show that in at least some cases the answer is "yes." The widely accepted formal model for computation is the Turing machine (TM), and the Halting Problem is the canonical uncomputable task (Turing 1936). Valiant <ref> (Valiant 1984) </ref> proposed a formal analysis of learning as the phenomenon of knowledge acquisition in the absence of explicit programming. The methodology was systematically organized into a consistent and rigorous framework by Kearns (Kearns 1990), which we follow. <p> Fact 6 P n P m+1 where m and n are non-negative integers and the co efficients c j are given by c 0 = 0 2 j 1 k=j+1 j 1 3 Fact 7 (Chernoff Bound (Chernoff 1952), <ref> (Valiant 1984) </ref>) In n independent trials, each with probability of success at least p, the probability that there are at most k successes, where k &lt; np, is at most nk k 2.2 LEMMAS Lemma 1 In Fact 7, if p = (1 b*) and k = n (1 c*) where
References-found: 27

