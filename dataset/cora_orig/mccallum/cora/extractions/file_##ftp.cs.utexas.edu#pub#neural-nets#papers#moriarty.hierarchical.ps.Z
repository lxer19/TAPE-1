URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/moriarty.hierarchical.ps.Z
Refering-URL: 
Root-URL: 
Email: moriarty,risto@cs.utexas.edu  
Title: Hierarchical Evolution of Neural Networks  
Author: David E. Moriarty and Risto Miikkulainen 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences The University of Texas at Austin  
Abstract: Technical Report AI96-242 January 1996 Abstract In most applications of neuro-evolution, each individual in the population represents a complete neural network. Recent work on the SANE system, however, has demonstrated that evolving individual neurons often produces a more efficient genetic search. This paper explores the merits of neuro-evolution both at the neuron level and at the network level. While SANE can solve easy tasks in just a few generations, in tasks that require high precision, its progress often stalls and is exceeded by a standard, network-level evolution. In this paper, a new approach called Hierarchical SANE is presented that combines the advantages of both approaches by integrating two levels of evolution in a single framework. Hierarchical SANE couples the early explorative quality of SANE's neuron-level search with the late exploitative quality of a more standard network-level evolution. In a sophisticated robot arm manipulation task, Hierarchical SANE significantly outperformed both SANE and a standard, network-level neuro-evolution approach, suggesting that it can more efficiently solve a broad range of tasks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Sutton, R. S., and Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, SMC-13:834-846. </journal>
Reference-contexts: Evolution at the neuron level promotes population diversity and allows SANE to better evaluate subcomponents of the final solution. In the pole balancing benchmark, SANE compared favorably to temporal difference methods such as the Adaptive Heuristic Critic <ref> (Barto et al. 1983) </ref> and Q-learning (Watkins 1989), and network-level neuro-evolution systems such as GENITOR (Whitley et al. 1993). The purpose of this paper is to compare and contrast the merits of evolution at the neuron level and evolution at the network level.
Reference: <author> Belew, R. K., McInerney, J., and Schraudolph, N. N. </author> <year> (1991). </year> <title> Evolving networks: Using genetic algorithm with connectionist learning. </title> <editor> In Farmer, J. D., Langton, C., Rasmussen, S., and Taylor, C., editors, </editor> <booktitle> Artificial Life II. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Cliff, D., Harvey, I., and Husbands, P. </author> <year> (1993). </year> <title> Explorations in evolutionary robotics. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 2 </volume> <pages> 73-110. </pages>
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: By treating each member as a separate full solution, the genetic algorithm focuses the search towards a single dominant individual. Such concentration leads the population to converge on a single solution <ref> (Goldberg 1989) </ref>. Once the population has converged, further movement is generally only possible through random mutation. Crossover operations become ineffective since the same genetic building blocks are joined in the new offspring. <p> A neuron-level evolution explicitly promotes genetic building blocks in the population that may be useful in building other networks. A network-level evolution does so only implicitly, along with various other sub- and superstructures <ref> (Goldberg 1989) </ref>.
Reference: <author> Kawato, M. </author> <year> (1990). </year> <title> Computational schemes and neural network models for formation and control of multi-joint arm trajectory. In Neural Networks for Control. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Koza, J. R., and Rice, J. P. </author> <year> (1991). </year> <title> Genetic generalization of both the weights and architecture for a neural network. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> 397-404. </pages> <address> New York, NY: </address> <publisher> IEEE. </publisher>
Reference: <author> Miller, W. T. </author> <year> (1989). </year> <title> Real-time application of neural networks for sensor-based control of robots with vision. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 19(4) </volume> <pages> 825-831. </pages>
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1996). </year> <title> Efficient reinforcement learning through symbiotic evolution. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 11-32. </pages>
Reference-contexts: level the genetic algorithm is no longer relied upon to identify neurons as important building blocks, since neurons are the object of evolution. 1 We chose the term specialization rather than species since each neuron does not represent a full solution to the problem. 3 The neuron evolution in SANE1 <ref> (Moriarty and Miikkulainen 1996) </ref> achieves each of these advan-tages, but also contains an Achilles' heel. In SANE1, networks are formed from randomly-selected subpopulations of neurons, which is undesirable for two reasons. First, the top neurons may not be combined with neurons that work well together. <p> In easy tasks such as pole-balancing, SANE1 performs quite well since solutions are plentiful and generally found in the first 10 generations <ref> (Moriarty and Miikkulainen 1996) </ref>. Experiments presented in section 4, however, show that in significantly difficult tasks that require high precision within the solution space, SANE1 is unable to build upon the best networks to improve the population in later generations.
Reference: <author> Nolfi, S., Floreano, D., Miglino, O., and Mondada, F. </author> <year> (1994). </year> <title> How to evolve autonomous robots: </title> <booktitle> Different approaches in evolutionary robotics. In Artificial Life IV. </booktitle> <address> Cambridge, MA. </address>
Reference: <author> Nolfi, S., and Parisi, D. </author> <year> (1992). </year> <title> Growing neural networks. </title> <booktitle> In Artificial Life III. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher> <editor> van der Smagt, P. </editor> <year> (1994). </year> <title> Simderella: A robot simulator for neuro-controller design. </title> <journal> Neurocomputing, </journal> <volume> 6(2). </volume> <editor> van der Smagt, P. </editor> <year> (1995). </year> <title> Visual Robot Arm Guidance using Neural Networks. </title> <type> PhD thesis, </type> <institution> The University of Amsterdam, </institution> <address> Amsterdam, The Netherlands. </address>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <address> England. </address>
Reference-contexts: Evolution at the neuron level promotes population diversity and allows SANE to better evaluate subcomponents of the final solution. In the pole balancing benchmark, SANE compared favorably to temporal difference methods such as the Adaptive Heuristic Critic (Barto et al. 1983) and Q-learning <ref> (Watkins 1989) </ref>, and network-level neuro-evolution systems such as GENITOR (Whitley et al. 1993). The purpose of this paper is to compare and contrast the merits of evolution at the neuron level and evolution at the network level.
Reference: <author> Werbos, P. J. </author> <year> (1992). </year> <title> Neurocontrol and supervised learning: An overview and evaluation. </title> <booktitle> In Handbook of Intelligent Control, </booktitle> <pages> 65-89. </pages> <address> New York: </address> <publisher> Van Nostrand Reinhold. </publisher>
Reference: <author> Whitley, D., Dominic, S., Das, R., and Anderson, C. W. </author> <year> (1993). </year> <title> Genetic reinforcement learning for neuro-control problems. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 259-284. </pages>
Reference-contexts: In the pole balancing benchmark, SANE compared favorably to temporal difference methods such as the Adaptive Heuristic Critic (Barto et al. 1983) and Q-learning (Watkins 1989), and network-level neuro-evolution systems such as GENITOR <ref> (Whitley et al. 1993) </ref>. The purpose of this paper is to compare and contrast the merits of evolution at the neuron level and evolution at the network level. SANE's neuron-level approach is very adept at finding quick, effective solutions, but often has difficulty pinpointing the best solutions.
Reference: <author> Yamauchi, B. M., and Beer, R. D. </author> <year> (1993). </year> <title> Sequential behavior and learning in evolved dynamical neural networks. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 2 </volume> <pages> 219-246. 16 </pages>
References-found: 14

