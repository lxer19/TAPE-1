URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-351.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: popat@media.mit.edu, picard@media.mit.edu  
Phone: Tel: (617) 253-6271 Fax: (617) 253-8874  
Title: probability model and its application to image and texture processing  
Author: Kris Popat and Rosalind W. Picard 
Date: 1.5)  
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: The MIT Media Laboratory,  
Note: Cluster-based  EDICS Number: IP 1.9 (with applications to 1.1, 1.4, and  
Pubnum: Rm E15-383,  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 351 Also appears in the IEEE Trans. on Image Processing, Vol. 6, No. 2, February 1997, pp. 268-284 Abstract We develop, analyze, and apply a specific form of mixture modeling for density estimation, within the context of image and texture processing. The technique captures much of the higher-order, nonlinear statistical relationships present among vector elements by combining aspects of kernel estimation and cluster analysis. Experimental results are presented in the following applications: image restoration, image and texture compression, and texture classifica tion.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Besag, </author> <title> "Spatial interaction and the statistical analysis of lattice systems (with discussion)," </title> <journal> J. Roy. Stat. Soc., Ser. B, </journal> <volume> vol. 36, </volume> <pages> pp. 192-236, </pages> <year> 1974. </year>
Reference-contexts: In particular, the focus is on obtaining accurate estimates of conditional distributions, where the number of conditioning variables is relatively large (on the order of ten). If conditional distributions are estimated directly, then care must be taken to ensure consistency <ref> [1] </ref>. In this work, we begin by estimating the joint distribution | in this way, we avoid consistency problems. Once the joint distribution has been estimated, the conditional can be computed by a simple normalization.
Reference: [2] <author> B. W. Silverman, </author> <title> Density Estimation for Statistics and Data Analysis. </title> <publisher> London: Chapman and Hall, </publisher> <year> 1986. </year>
Reference-contexts: As mentioned, the goal here is obtaining a high-dimensional joint probability distribution, i.e., on the order of d = 10 joint variables. Traditional attempts usually stop at d = 3 variables or less. Major obstacles exist when estimating high-d distributions <ref> [2, 3] </ref>. Foremost is the exponential growth of the amount of data required to obtain an estimate of prescribed quality as d is increased. Large regions in the d-dimensional space are likely to be devoid of observations. <p> The quality of an estimate can be measured in a variety of ways. The most commonly used criteria are the L 1 and L 2 norms <ref> [2, 10] </ref>. A criterion which is relevant in compression and classification applications is the relative entropy, defined as Z f (X) ln ~ f (X) and (1:4) X p (X) log 2 ~p (X) in the continuous and discrete case, respectively. <p> In higher dimensions, this type of economized kernel estimation becomes problematic, as the tails usually contain most of the total probability (for a nice illustration of this point, see <ref> [2] </ref>, pp. 91-93). Adaptive kernel estimates have been proposed to mitigate this problem [2, pp. 100-110], but they too rely on distribution sampling for the kernel locations, and therefore are prone to poor performance in tail regions. <p> In higher dimensions, this type of economized kernel estimation becomes problematic, as the tails usually contain most of the total probability (for a nice illustration of this point, see [2], pp. 91-93). Adaptive kernel estimates have been proposed to mitigate this problem <ref> [2, pp. 100-110] </ref>, but they too rely on distribution sampling for the kernel locations, and therefore are prone to poor performance in tail regions.
Reference: [3] <author> D. Scott and J. Thompson, </author> <title> "Probability density estimation in higher dimensions," </title> <booktitle> in Computer Science and Statistics: Proceedings of the Fifteenth Symposium on the Interface (J. </booktitle> <editor> Gentle, ed.), </editor> <publisher> (Amsterdam), </publisher> <pages> pp. 173-179, </pages> <publisher> North-Holland, </publisher> <year> 1983. </year>
Reference-contexts: As mentioned, the goal here is obtaining a high-dimensional joint probability distribution, i.e., on the order of d = 10 joint variables. Traditional attempts usually stop at d = 3 variables or less. Major obstacles exist when estimating high-d distributions <ref> [2, 3] </ref>. Foremost is the exponential growth of the amount of data required to obtain an estimate of prescribed quality as d is increased. Large regions in the d-dimensional space are likely to be devoid of observations. <p> These obstacles are consequences of what is commonly referred to fl This work was supported in part by HP Labs and NEC Corp. as "the curse of dimensionality." 1 A term that is more specific to the density estimation problem, coined by Scott and Thompson <ref> [3] </ref>, is the empty space phenomenon. The estimation technique described in this paper combines two weapons in combating the empty space phenomenon: kernel estimation and cluster analysis. Kernel estimation (reviewed in Section 1.2) provides a means of interpolating probability to fill in empty regions. <p> The result is a model that represents both mode and tail regions well, while combining the summarizing strength of histograms with the generalizing strength of kernel estimates. 2 Cluster-based probability estimation Scott and Thompson <ref> [3] </ref> have observed, ": : : the problem of density estimation in higher dimensions involves first of all finding where the action is." Cluster-based kernel probability estimation begins by identifying the locations of important regions of X d , by means of cluster analysis.
Reference: [4] <author> D. W. Scott, </author> <title> Multivariate density estimation. </title> <address> New York: </address> <publisher> John Wiley & Sons, </publisher> <year> 1992. </year>
Reference-contexts: In the discrete case, X is assumed to be a set of K real numbers on a bounded interval. The set of possible values of x is denoted X d R d ; it is the d-fold cartesian 1 D.W. Scott <ref> [4] </ref> has attributed the first use of this term to R.E. Bellman in describing the exponential growth with dimension of the complexity of combinatorial optimization. 1 product of X with itself. Note that, in the discrete case, the number of possible values for X d is K d . <p> A kernel estimate ~ f K is typically of the form ~ f K (X) = N n=1 where the kernel function k (X) is itself a PDF that is usually chosen to be spherically symmetric and local to the origin <ref> [4] </ref>. A Gaussian kernel is often used. The effect of kernel estimation is to "radiate" probability from each vector in the learning sample to the space immediately around it, which is justified by the smoothness assumption. In this way, the learning sample is generalized.
Reference: [5] <author> B. Everitt and D. </author> <title> Hand, Finite mixture distributions. </title> <publisher> London: Chapman and Hall, </publisher> <year> 1981. </year>
Reference-contexts: The cluster-based probability model is a type of mixture model, and mixture models are not new. Their estimation and use dates back at least to the 1894 work of Karl Pearson; see <ref> [5] </ref> or [6] for a survey. Mixture models have customarily been used in situations calling for unsupervised learning. <p> Alternatively, a mixture model may be viewed as a means of estimating an arbitrary probability law, even in situations where there is no reason to believe that the true probability law is a mixture <ref> [5, p. 118ff] </ref>. The cluster-based probability model is viewed in this way. Mixture models have received considerable attention from the speech processing community over the past two decades [8].
Reference: [6] <author> R. A. Redner and H. F. Walker, </author> <title> "Mixture densities, maximum likelihood, and the EM algorithm," </title> <journal> SIAM Review, </journal> <volume> vol. 26, </volume> <pages> pp. 195-239, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: The cluster-based probability model is a type of mixture model, and mixture models are not new. Their estimation and use dates back at least to the 1894 work of Karl Pearson; see [5] or <ref> [6] </ref> for a survey. Mixture models have customarily been used in situations calling for unsupervised learning. <p> of the learning sample | eventually, the LBG algorithm will stop creating new clusters on account of low cell populations. 2.2 Optimizing the model parameters via the EM algorithm An alternative to the method of estimating the weights and kernel parameters described above is to use the expectation-maximization (EM) algorithm <ref> [18, 6] </ref>. This algorithm results in a local maximum of the model likelihood, which for large training samples approximates a local minimum of the relative entropy D (pjj~p).
Reference: [7] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern classification and scene analysis. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: Specifically, a mixture model naturally arises when an observation x is believed to obey probability law p c (x j ! c ) with probability P (! c ), where ! c is one of several "states of nature," or classes <ref> [7] </ref>. Alternatively, a mixture model may be viewed as a means of estimating an arbitrary probability law, even in situations where there is no reason to believe that the true probability law is a mixture [5, p. 118ff]. The cluster-based probability model is viewed in this way. <p> Essentially, what we are after from cluster analysis is much the same as what vector quantization is after: representational efficiency, as opposed to efficiency of discrimination. A clustering technique that is widely used in vector quantization is the k-means procedure <ref> [7] </ref>, whose origin is often attributed to Forgy [14, 15]. Stagewise application of the k-means procedure, in which the initial guesses for the cluster centroids at each stage are obtained by splitting the centroids resulting from a previous stage, is known as the LBG algorithm [14, 16]. <p> In this case, the Bayes decision rule reduces to the familiar ML rule, which is to choose the class which makes the observed data most likely <ref> [7] </ref>. ML classification can be applied in many different ways. For instance, if it were known beforehand that a particular patch in an image consists of a single texture class, then the model with the greatest likelihood could be chosen.
Reference: [8] <author> L. R. and B.-H. Juang, </author> <title> Fundamentals of speech recognition. </title> <publisher> PTR Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: The cluster-based probability model is viewed in this way. Mixture models have received considerable attention from the speech processing community over the past two decades <ref> [8] </ref>. They are also a topic of current interest among researchers in the field of artificial neural networks (ANN's) [9], where the emphasis has been on estimating the system output values themselves, rather than on estimating predictive probability distributions for those values (see Section 6.2).
Reference: [9] <author> S. Haykin, </author> <title> Neural networks: a comprehensive foundation. </title> <publisher> Macmillan, </publisher> <year> 1994. </year>
Reference-contexts: The cluster-based probability model is viewed in this way. Mixture models have received considerable attention from the speech processing community over the past two decades [8]. They are also a topic of current interest among researchers in the field of artificial neural networks (ANN's) <ref> [9] </ref>, where the emphasis has been on estimating the system output values themselves, rather than on estimating predictive probability distributions for those values (see Section 6.2).
Reference: [10] <author> L. Devroye, </author> <title> A course in density estimation. </title> <address> Boston: </address> <publisher> Birkhauser, </publisher> <year> 1987. </year>
Reference-contexts: The quality of an estimate can be measured in a variety of ways. The most commonly used criteria are the L 1 and L 2 norms <ref> [2, 10] </ref>. A criterion which is relevant in compression and classification applications is the relative entropy, defined as Z f (X) ln ~ f (X) and (1:4) X p (X) log 2 ~p (X) in the continuous and discrete case, respectively.
Reference: [11] <author> T. M. Cover and J. A. Thomas, </author> <title> Elements of Information Theory. </title> <publisher> John Wiley and Sons, </publisher> <year> 1991. </year>
Reference-contexts: The relative entropy is directly related to efficiency in compression and to error rate in classification <ref> [11] </ref>.
Reference: [12] <author> J. A. Rice, </author> <title> Mathematical Statistics and Data Analysis. </title> <publisher> Wadsworth and Brooks/Cole, </publisher> <year> 1988. </year>
Reference-contexts: The histogram is the maximum-likelihood estimator of p, which implies that it is asymptotically unbiased (as N ! 1) and consistent <ref> [12] </ref>. However, in practice usually N t K d , so that asymptotic behavior is not reached. In fact, typically ~p H (X) = 0 for all but a small fraction of X d , even in regions of relatively high probability.
Reference: [13] <author> D. </author> <title> Hand, Kernel discriminant analysis. </title> <publisher> Research Studies Press, </publisher> <year> 1982. </year>
Reference-contexts: In this way, the learning sample is generalized. Kernel estimation is a powerful technique in nonparametric statistics with many practical successes reported and a rich supporting theory <ref> [13] </ref>. However, it is not without its shortcomings. Foremost is its inability to summarize the learning sample. In kernel estimation, a kernel is placed at each sample, requiring each training vector to be retained and used whenever the estimate is evaluated.
Reference: [14] <author> A. Gersho and R. M. Gray, </author> <title> Vector quantization and signal compression. </title> <publisher> Kluwer academic publishers, </publisher> <year> 1991. </year>
Reference-contexts: Essentially, what we are after from cluster analysis is much the same as what vector quantization is after: representational efficiency, as opposed to efficiency of discrimination. A clustering technique that is widely used in vector quantization is the k-means procedure [7], whose origin is often attributed to Forgy <ref> [14, 15] </ref>. Stagewise application of the k-means procedure, in which the initial guesses for the cluster centroids at each stage are obtained by splitting the centroids resulting from a previous stage, is known as the LBG algorithm [14, 16]. <p> Stagewise application of the k-means procedure, in which the initial guesses for the cluster centroids at each stage are obtained by splitting the centroids resulting from a previous stage, is known as the LBG algorithm <ref> [14, 16] </ref>. We have adopted the LBG algorithm for all of the experiments here. The clustering algorithm is now described. Define X m = [X m;1 ; : : : ; X m;d ]; for m = 1; : : : ; M . <p> Of interest here is the complementary research direction: toward more sophisticated models, enabling less sophisticated preprocessing to be used. Examples of techniques in this direction are vector quantization (VQ) in the areas of compression and interpolation <ref> [14] </ref>, and artificial neural networks in the areas of regression and classification [31]. These techniques reduce the effect of preprocessing on system performance, by exploiting nonlinear, higher order statistical relationships that exist among the signal elements.
Reference: [15] <author> A. K. Jain and R. C. Dubes, </author> <title> Algorithms for clustering data. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Essentially, what we are after from cluster analysis is much the same as what vector quantization is after: representational efficiency, as opposed to efficiency of discrimination. A clustering technique that is widely used in vector quantization is the k-means procedure [7], whose origin is often attributed to Forgy <ref> [14, 15] </ref>. Stagewise application of the k-means procedure, in which the initial guesses for the cluster centroids at each stage are obtained by splitting the centroids resulting from a previous stage, is known as the LBG algorithm [14, 16].
Reference: [16] <author> Y. Linde, A. Buzo, and R. M. Gray, </author> <title> "An algorithm for vector quantizer design," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. COM-28, </volume> <pages> pp. 84-95, </pages> <month> Jan. </month> <year> 1980. </year>
Reference-contexts: Stagewise application of the k-means procedure, in which the initial guesses for the cluster centroids at each stage are obtained by splitting the centroids resulting from a previous stage, is known as the LBG algorithm <ref> [14, 16] </ref>. We have adopted the LBG algorithm for all of the experiments here. The clustering algorithm is now described. Define X m = [X m;1 ; : : : ; X m;d ]; for m = 1; : : : ; M .
Reference: [17] <author> J. Rissanen, </author> <title> "Modeling by shortest desciption length," </title> <journal> Automatica, </journal> <volume> vol. 14, </volume> <pages> pp. 465-471, </pages> <year> 1978. </year>
Reference-contexts: If no such limiting value is found, then M can be chosen on the basis of the available computation resources, or alternatively, it can be chosen to minimize the overall description length of the model and data, i.e., application of the minimum-description length principle <ref> [17] </ref>.
Reference: [18] <author> A. P. Dempster, N. Laird, and D. Rubin, </author> <title> "Maximum likelihood from incomplete data via the EM algorithm," </title> <journal> J. Royal Stat. Soc., </journal> <volume> vol. 39, </volume> <pages> pp. 1-38, </pages> <year> 1977. </year>
Reference-contexts: of the learning sample | eventually, the LBG algorithm will stop creating new clusters on account of low cell populations. 2.2 Optimizing the model parameters via the EM algorithm An alternative to the method of estimating the weights and kernel parameters described above is to use the expectation-maximization (EM) algorithm <ref> [18, 6] </ref>. This algorithm results in a local maximum of the model likelihood, which for large training samples approximates a local minimum of the relative entropy D (pjj~p).
Reference: [19] <author> K. Popat and R. W. </author> <title> Picard, "A novel cluster-based probability model for texture synthesis, classification, and compression," </title> <booktitle> in Proc. SPIE Visual Communications '93, </booktitle> <address> (Cambridge, Mass.), </address> <year> 1993. </year>
Reference-contexts: One method is to weed out insignificant kernels as the product (2.5) is grown, by deleting those r m for which k m;i (X i ) is smaller than some suitable threshold (determined empirically). In a previous paper <ref> [19] </ref> it was suggested that the discretized kernels k m;i be precomputed to further speed execution time. This is appropriate when the need to save execution time far outweighs the need to save memory.
Reference: [20] <author> A. Gersho, </author> <title> "Optimal nonlinear interpolative vector quantization," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 38, </volume> <pages> pp. 1285-1287, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: Examples of restoration problems that fit this description are de-halftoning, film grain reduction, and compensating the effects of quantization in lossy compression schemes. 3.2 Relationship to nonlinear interpolation Cluster-based restoration is similar in spirit to a VQ-based nonlinear interpolation technique proposed by Gersho <ref> [20] </ref>. Both approaches have the potential to learn nonlinear statisti cal relationships from training data and to use those relationships to fill in missing values. However, the techniques differ in one important respect.
Reference: [21] <author> G. Langdon, A. Gulati, and E. Seiler, </author> <title> "On the JPEG model for lossless image compression," </title> <booktitle> in Proc. IEEE Data Comp. Conf., </booktitle> <address> (Utah), </address> <year> 1992. </year>
Reference-contexts: Such techniques routinely achieve compression ratios of 10:1 or more, with little or no noticeable distortion. On the other hand, lossless (strictly reversible) compression techniques typically yield compression ratios no better than 2:1 on natural images <ref> [21, 22] </ref>. This comparatively poor performance is a consequence of the requirement that the reconstructed image be bit-for-bit identical to the original. An upper bound on the compression ratio that can be achieved comes from noise that is inevitably introduced in the acquisition process. <p> The textures are shown in The performance listed in the first two rows of the table compares favorably with that reported in the literature for natural scenes <ref> [21, 22] </ref>. The compression performance for the two textures is more difficult to interpret, since no previous results seem to have been published. One could argue that the textures, having fewer blank regions, are more difficult to compress than natural scenes.
Reference: [22] <author> M. Rabbani and P. W. Jones, </author> <title> Digital image compression techniques. </title> <type> Bellingham, </type> <institution> Washington: SPIE Optical Engineering Press, </institution> <year> 1991. </year>
Reference-contexts: Such techniques routinely achieve compression ratios of 10:1 or more, with little or no noticeable distortion. On the other hand, lossless (strictly reversible) compression techniques typically yield compression ratios no better than 2:1 on natural images <ref> [21, 22] </ref>. This comparatively poor performance is a consequence of the requirement that the reconstructed image be bit-for-bit identical to the original. An upper bound on the compression ratio that can be achieved comes from noise that is inevitably introduced in the acquisition process. <p> The textures are shown in The performance listed in the first two rows of the table compares favorably with that reported in the literature for natural scenes <ref> [21, 22] </ref>. The compression performance for the two textures is more difficult to interpret, since no previous results seem to have been published. One could argue that the textures, having fewer blank regions, are more difficult to compress than natural scenes.
Reference: [23] <author> J. J. Rissanen and G. G. Langdon, </author> <title> "Arithmetic coding," </title> <journal> IBM J. Res. Develop., </journal> <volume> vol. 23, </volume> <pages> pp. 149-162, </pages> <month> March </month> <year> 1979. </year>
Reference-contexts: It is natural to question whether codes that are achievable in practice also perform best when ~p = p. As will be discussed in the following section, arithmetic coding is a practical technique for lossless compression that very nearly achieves Shannon's optimal code length assignment <ref> [23] </ref>, so that the above analysis pertains to practice as well as theory. 4.1 Lossless compression with arithmetic coding Arithmetic coding is a form of entropy coding that offers significant advantages over other methods in many applications TABLE I: Estimated Bits per Pixel (M = 2048) Image N 2 N 3 <p> coding is a form of entropy coding that offers significant advantages over other methods in many applications TABLE I: Estimated Bits per Pixel (M = 2048) Image N 2 N 3 N 4 N 5 cman 5.50 4.81 4.77 4.79 lenna 5.11 4.53 4.38 4.41 D1 4.83 4.40 4.24 4.14 <ref> [23, 24, 25, 26] </ref>. It has near-optimal efficiency (relative to the assumed probability law) for a broad class of sources and over a wide range of coding rates. It is also inherently adaptive, and simplifies the encoding of large-alphabet low-entropy sources.
Reference: [24] <author> J. Rissanen and G. G. Langdon, </author> <title> "Universal modeling and coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-27, </volume> <pages> pp. 12-23, </pages> <year> 1981. </year>
Reference-contexts: coding is a form of entropy coding that offers significant advantages over other methods in many applications TABLE I: Estimated Bits per Pixel (M = 2048) Image N 2 N 3 N 4 N 5 cman 5.50 4.81 4.77 4.79 lenna 5.11 4.53 4.38 4.41 D1 4.83 4.40 4.24 4.14 <ref> [23, 24, 25, 26] </ref>. It has near-optimal efficiency (relative to the assumed probability law) for a broad class of sources and over a wide range of coding rates. It is also inherently adaptive, and simplifies the encoding of large-alphabet low-entropy sources.
Reference: [25] <author> G. G. Langdon, </author> <title> "An introduction to arithmetic coding," </title> <institution> IBM J. Res. Develop., </institution> <month> Mar. </month> <year> 1984. </year> <title> 13 natural scenes. All are 8-bit monochrome, </title> <note> 512 fi 512, except for cman, which is 256 fi 256. (French canvas), </note> <editor> D22 (reptile skin), D55 (straw matting), D68 (wood grain), D77 (cotton canvas), </editor> <title> D84 (raffia looped to a high pile), D103 (loose burlap). These textures are 256 fi 256, cut out of a 512 fi 512 original. All images are 8-bit monochrome. </title>
Reference-contexts: coding is a form of entropy coding that offers significant advantages over other methods in many applications TABLE I: Estimated Bits per Pixel (M = 2048) Image N 2 N 3 N 4 N 5 cman 5.50 4.81 4.77 4.79 lenna 5.11 4.53 4.38 4.41 D1 4.83 4.40 4.24 4.14 <ref> [23, 24, 25, 26] </ref>. It has near-optimal efficiency (relative to the assumed probability law) for a broad class of sources and over a wide range of coding rates. It is also inherently adaptive, and simplifies the encoding of large-alphabet low-entropy sources.
Reference: [26] <author> K. Popat, </author> <title> "Scalar quantization with arithmetic coding," </title> <type> Master's thesis, </type> <institution> Dept. of Elec. Eng. and Comp. Science, M.I.T., </institution> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: coding is a form of entropy coding that offers significant advantages over other methods in many applications TABLE I: Estimated Bits per Pixel (M = 2048) Image N 2 N 3 N 4 N 5 cman 5.50 4.81 4.77 4.79 lenna 5.11 4.53 4.38 4.41 D1 4.83 4.40 4.24 4.14 <ref> [23, 24, 25, 26] </ref>. It has near-optimal efficiency (relative to the assumed probability law) for a broad class of sources and over a wide range of coding rates. It is also inherently adaptive, and simplifies the encoding of large-alphabet low-entropy sources.
Reference: [27] <author> G. Langdon and J. Rissanen, </author> <title> "Compression of black-white images with arithmetic coding," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. </volume> <pages> COM-29, pp. 858-867, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: It is also inherently adaptive, and simplifies the encoding of large-alphabet low-entropy sources. Most importantly, it allows the probabilistic model to be specified explicitly and separately from the actual encoder. The first use of arithmetic coding as an image compression technique was by Langdon and Rissanen <ref> [27] </ref>. In their system, which was for binary images, each pixel was encoded using a PMF conditioned on a nearby set of previously encoded pixels, i.e., on a causal neighborhood.
Reference: [28] <author> K. Popat and R. W. </author> <title> Picard, "Exaggerated consensus in lossless image compression," </title> <booktitle> in ICIP-94: 1994 International Conference on Image Processing, </booktitle> <address> (Austin, TX), </address> <publisher> IEEE, </publisher> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: The following heuristic can then be invoked: To be assigned to a certain class, a pixel must have high conditional likelihood simultaneously with respect to several different neighborhoods. This idea has recently been generalized as an independent method of attack on the curse of dimensionality in density estimation <ref> [28] </ref>. Consider now the problem of classifying regions in heterogeneous images. Suppose that there are J classes, and that vectors drawn from class j follow PMF p j (x). Associated with each class is a cluster-based probability model ~p j (x), trained previously.
Reference: [29] <author> R. W. </author> <title> Picard, "Content access for image/video coding: `The Fourth Criterion'," </title> <type> Tech. Rep. 295, </type> <institution> MIT Media Lab, Perceptual Computing, </institution> <address> Cambridge, MA, </address> <year> 1994. </year> <title> MPEG Doc. </title> <type> 127, </type> <institution> Lausanne, </institution> <year> 1995. </year>
Reference-contexts: The possibility of realizing the decision rule in this way illustrates a connection between data compression and classification. This connection seems especially important with the growth of large libraries of image data, where one will want to search and make decisions on compressed data <ref> [29] </ref>. Recent 9 original composite test image, classification using resolution averaging only, and classification using both resolution and spatial averaging.
Reference: [30] <author> K. Perlmutter, S. Perlmutter, R. Gray, R. Olshen, and K. Oehler, </author> <title> "Bayes risk weighted vector quantization with posterior estimation for image compression and classification," </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> vol. 5, </volume> <pages> pp. 347-360, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: SAMPLE VECTORS FROM PROBABILISITIC MODEL FOR CLASS J IDEAL ENTROPY CODER CLASSIFICATION DECISION MINIMUM AVERAGE BIT RATE CHOOSE CLASS WITH a bank of ideal entropy coders, each tuned to a different source. work by Perlmutter et al. indicates that combining these two tasks can result in improved performance for both <ref> [30] </ref>. The above assumes that nearby pixels come from the same class. In practice, when working with heterogeneous images, it is desirable to impose this assumption in a soft manner.
Reference: [31] <author> J. Hertz, A. Krogh, and R. G. Palmer, </author> <title> Introduction to the theory of neural computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Of interest here is the complementary research direction: toward more sophisticated models, enabling less sophisticated preprocessing to be used. Examples of techniques in this direction are vector quantization (VQ) in the areas of compression and interpolation [14], and artificial neural networks in the areas of regression and classification <ref> [31] </ref>. These techniques reduce the effect of preprocessing on system performance, by exploiting nonlinear, higher order statistical relationships that exist among the signal elements.
Reference: [32] <author> T. Poggio and F. Girosi, </author> <title> "Networks for approximation and learning," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 78, </volume> <pages> pp. 1481-1497, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: In particular, if the k m;i 's are chosen appropriately, then (2.3) amounts to a radial basis function (RBF) approximation to p (x). One might hope, therefore, the RBF literature would provide insight into such issues as training, means of implementation, and bounds on approximation accuracy <ref> [32, 33, 34] </ref>. This is true to some extent, but two differences are apparent. The first is that values of the function being approximated (a probability law) are never actually observed; instead our observations consist only of samples that we believe to be governed by the function.
Reference: [33] <author> T. Poggio and F. Girosi, </author> <title> "Extension of a theory of networks for approximation and learning: Dimensionality reduction and clustering," A.I. </title> <type> Memo #1167, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1990. </year>
Reference-contexts: In particular, if the k m;i 's are chosen appropriately, then (2.3) amounts to a radial basis function (RBF) approximation to p (x). One might hope, therefore, the RBF literature would provide insight into such issues as training, means of implementation, and bounds on approximation accuracy <ref> [32, 33, 34] </ref>. This is true to some extent, but two differences are apparent. The first is that values of the function being approximated (a probability law) are never actually observed; instead our observations consist only of samples that we believe to be governed by the function.
Reference: [34] <author> J. Moody and C. J. Darken, </author> <title> "Fast learning in networks of locally-tuned processing units," </title> <journal> Neural Computation, </journal> <volume> vol. 1, </volume> <pages> pp. 281-294, </pages> <year> 1989. </year>
Reference-contexts: In particular, if the k m;i 's are chosen appropriately, then (2.3) amounts to a radial basis function (RBF) approximation to p (x). One might hope, therefore, the RBF literature would provide insight into such issues as training, means of implementation, and bounds on approximation accuracy <ref> [32, 33, 34] </ref>. This is true to some extent, but two differences are apparent. The first is that values of the function being approximated (a probability law) are never actually observed; instead our observations consist only of samples that we believe to be governed by the function.
Reference: [35] <author> M. D. Richard and R. P. Lippmann, </author> <title> "Neural network classifiers estimate Bayesian a posteriori probabilities," </title> <journal> Neural Computation, </journal> <volume> vol. 3, </volume> <pages> pp. 461-483, </pages> <year> 1991. </year>
Reference-contexts: In contrast, the method of this paper provides an explicit probabilistic model for the source, which can be used equally well in a variety of applications like compression, restoration, and classification. In principle, this distinction vanishes when the goal of the ANN is specifically to estimate the PMF <ref> [35] </ref>, rather than to carry out the ultimate information processing task. In this case, the approaches may be accomplishing the same thing in different ways.
Reference: [36] <author> P. Pudil, J. Novovicova, and J. Kittler, </author> <title> "Simultaneous learning of decision rules and important attributes for classification problems in image analysis," </title> <journal> Image and vision computing, </journal> <volume> vol. 12, </volume> <pages> pp. 193-198, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Such hierarchies of models can be expected to play a substantial role when the cluster-based technique is applied to large, real-world classification problems, as occur in digital libraries. An interesting alternative means of sharing common attributes among several models has been suggested recently by Pudil et al. <ref> [36] </ref>, in a classification setting. The approach is to posit a common "background" density for all of the classes, and to express each class-conditional density as a mixture of products of this background density with a class-specific modulating function. <p> Expressing the density in this way simplifies the sharing of common attributes, and provides a basis for feature selection: choose the features that provide maximum deviation from the background. The classification results reported in <ref> [36] </ref> are for small mixtures (2, 3, and 4 components).
Reference: [37] <author> P. Brodatz, </author> <title> Textures: A Photographic Album for Artists and Designers. </title> <address> New York: </address> <publisher> Dover, </publisher> <year> 1966. </year> <month> 14 </month>
References-found: 37

