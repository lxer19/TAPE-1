URL: http://www.tns.lcs.mit.edu/~djw/library/osdi96/zhou.ps.gz
Refering-URL: http://www.tns.lcs.mit.edu/~djw/library/osdi96/index.html
Root-URL: 
Email: Email: office@usenix.org  
Title: Performance Evaluation of Two Home-Based Lazy Release Consistency Protocols for Shared Virtual Memory Systems  
Phone: 1. Phone: 510 528-8649 2. FAX: 510 548-5738 3.  4.  
Author: Yuanyuan Zhou, Liviu Iftode and Kai Li 
Affiliation: Princeton University  
Web: WWW URL: http://www.usenix.org  
Date: October 1996  
Note: The following paper was originally published in the Proceedings of the USENIX 2nd Symposium on Operating Systems Design and Implementation Seattle, Washington,  For more information about USENIX Association contact:  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. V. Adve, A. L. Cox, S. Dwarkadas, R. Rajamony, and W. Zwaenepoel. </author> <title> A Comparison of Entry Consistency and Lazy Release Consistency Implementation. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: The Release Consistency (RC) model was proposed in order to improve hardware cache coherence [12]. The model was used to implement shared virtual memory and reduce false sharing by allowing multiple writers [8]. Lazy Release Consistency (LRC) <ref> [22, 9, 1] </ref> further relaxed the RC protocol to reduce protocol overhead. Tread-Marks [21] was the first SVM implementation using the LRC protocol on a network of stock computers. That implementation has achieved respectable performance on small-scale machines.
Reference: [2] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. A Distributed Implementation of the Shared Data-Object Model. </title> <booktitle> In USENIX Workshop on Experiences with Building Distributed and Multiprocessor Systems, </booktitle> <pages> pages 1-19, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Other relaxed consistency models include Entry Consistency [3] and Scope Consistency [17]. Both models take advantage of the association of data with locks, either explicitly (Entry Consistency) or implicitly (Scope Consistency), to reduce the protocol overhead. Both Orca <ref> [2] </ref> and CRL [19] are designed to implement distributed shared memory by maintaining coherence at object level instead of page level. These methods require specialized APIs, unlike the prototype systems presented in this paper. Our systems allow programs written for a release-consistent, shared-memory multiprocessor to run without modification.
Reference: [3] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway Distributed Shared Memory System. In Proceedings of the IEEE COMPCON '93 Conference, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: Our home-based protocols support multiple writers using diffs but replace most of the diff traffic with full page traffic. The home-based protocols reduce to a single-writer protocol for applications that exhibit one-writer multiple-readers sharing patterns, like SOR or LU. Other relaxed consistency models include Entry Consistency <ref> [3] </ref> and Scope Consistency [17]. Both models take advantage of the association of data with locks, either explicitly (Entry Consistency) or implicitly (Scope Consistency), to reduce the protocol overhead.
Reference: [4] <author> R. Bianchini, L.I Kontothanassis, R. Pinto, M. De Maria, M. Abud, and C.L. Amorim. </author> <title> Hiding Communication Latency and Coherence Overhead in Software DSMs. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: In the Flash multiprocessor [24], each node contains a programmable processor called MAGIC that performs protocol operations and handles all communications within the node and among all nodes. Neither system uses LRC-based relaxed consistency models. Bianchini et al. <ref> [4] </ref> proposed a dedicated protocol controller to o*oad some of the communication and coherence overheads from the computation processor. Using simulations they show that such a protocol processor can double the performance of TreadMarks on a 16-node configuration and that diff prefetching is not always beneficial.
Reference: [5] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: To reduce memory consumption the shared virtual memory system must perform garbage collection frequently [21]. 2.2 Automatic Update Release Consistency The AURC protocol [16] implements Lazy Release Consistency without using any diff operations by taking advantage of the SHRIMP multicomputer's automatic update hardware mechanism <ref> [5, 6] </ref>.
Reference: [6] <author> Matthias Blumrich, Cezary Dubnick, Edward Felten, and Kai Li. </author> <title> Protected, User-Level DMA for the SHRIMP Network Interface. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: To reduce memory consumption the shared virtual memory system must perform garbage collection frequently [21]. 2.2 Automatic Update Release Consistency The AURC protocol [16] implements Lazy Release Consistency without using any diff operations by taking advantage of the SHRIMP multicomputer's automatic update hardware mechanism <ref> [5, 6] </ref>. <p> As a consequence, a roundtrip communication for either a page or lock transfer is at best on the order of a millisecond. Current network technologies <ref> [6, 13, 7] </ref>, as well 0 2 4 6 8 10 30 50 Execution Time (seconds) LRC, 8 Processors 0 10 20 30 5 15 Execution Time (seconds) LRC, 32 Processors Protocol Overhead Lock Data Transfer Computation 0 2 4 6 8 10 30 50 Execution Time (seconds) HLRC, 8 Processors
Reference: [7] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: As a consequence, a roundtrip communication for either a page or lock transfer is at best on the order of a millisecond. Current network technologies <ref> [6, 13, 7] </ref>, as well 0 2 4 6 8 10 30 50 Execution Time (seconds) LRC, 8 Processors 0 10 20 30 5 15 Execution Time (seconds) LRC, 32 Processors Protocol Overhead Lock Data Transfer Computation 0 2 4 6 8 10 30 50 Execution Time (seconds) HLRC, 8 Processors
Reference: [8] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Imple--mentation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The known software approach to reducing the overhead of shared virtual memory is to employ relaxed memory consistency models such as Release Consistency (RC) and Lazy Release Consistency (LRC) <ref> [8, 22] </ref>. Both protocols allow multiple writers while avoiding false sharing, and both reduce overheads by maintaining coherence only at acquire and release synchronization points. <p> The Release Consistency (RC) model was proposed in order to improve hardware cache coherence [12]. The model was used to implement shared virtual memory and reduce false sharing by allowing multiple writers <ref> [8] </ref>. Lazy Release Consistency (LRC) [22, 9, 1] further relaxed the RC protocol to reduce protocol overhead. Tread-Marks [21] was the first SVM implementation using the LRC protocol on a network of stock computers. That implementation has achieved respectable performance on small-scale machines.
Reference: [9] <author> A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The Release Consistency (RC) model was proposed in order to improve hardware cache coherence [12]. The model was used to implement shared virtual memory and reduce false sharing by allowing multiple writers [8]. Lazy Release Consistency (LRC) <ref> [22, 9, 1] </ref> further relaxed the RC protocol to reduce protocol overhead. Tread-Marks [21] was the first SVM implementation using the LRC protocol on a network of stock computers. That implementation has achieved respectable performance on small-scale machines.
Reference: [10] <author> C. Dubnicki, L. Iftode, E.W. Felten, and K. Li. </author> <title> Software Support for Virtual Memory-Mapped Communication. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: Overhead Lock Data Transfer Computation 0 2 4 6 8 10 30 50 Execution Time (seconds) HLRC, 8 Processors 0 10 20 30 5 15 Execution Time (seconds) HLRC, 32 Processors Protocol Overhead Lock Data Transfer Computation as aggressive software for fast interrupts, exceptions [30] and virtual memory mapped communication <ref> [10, 11] </ref> have brought such latencies down significantly to the neighborhood of a couple of microseconds. An interesting question is to what extent our results are specific to the Paragon architecture and how they would be affected by different architectural parameters.
Reference: [11] <author> E.W. Felten, R.D. Alpert, A. Bilas, M.A. Blumrich, D.W. Clark, S. Damianakis, C. Dubnicki, L. Iftode, and K. Li. </author> <title> Early Experience with Message-Passing on the SHRIMP Mul-ticomputer. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Overhead Lock Data Transfer Computation 0 2 4 6 8 10 30 50 Execution Time (seconds) HLRC, 8 Processors 0 10 20 30 5 15 Execution Time (seconds) HLRC, 32 Processors Protocol Overhead Lock Data Transfer Computation as aggressive software for fast interrupts, exceptions [30] and virtual memory mapped communication <ref> [10, 11] </ref> have brought such latencies down significantly to the neighborhood of a couple of microseconds. An interesting question is to what extent our results are specific to the Paragon architecture and how they would be affected by different architectural parameters.
Reference: [12] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The Release Consistency (RC) model was proposed in order to improve hardware cache coherence <ref> [12] </ref>. The model was used to implement shared virtual memory and reduce false sharing by allowing multiple writers [8]. Lazy Release Consistency (LRC) [22, 9, 1] further relaxed the RC protocol to reduce protocol overhead.
Reference: [13] <author> R. Gillett, M. Collins, and D. Pimm. </author> <title> Overview of Network Memory Channel for PCI. </title> <booktitle> In Proceedings of the IEEE Spring COMPCON '96, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: As a consequence, a roundtrip communication for either a page or lock transfer is at best on the order of a millisecond. Current network technologies <ref> [6, 13, 7] </ref>, as well 0 2 4 6 8 10 30 50 Execution Time (seconds) LRC, 8 Processors 0 10 20 30 5 15 Execution Time (seconds) LRC, 32 Processors Protocol Overhead Lock Data Transfer Computation 0 2 4 6 8 10 30 50 Execution Time (seconds) HLRC, 8 Processors
Reference: [14] <author> M. Homewood and M. McLaren. </author> <title> Meiko CS-2 Interconnect Elan-Elite Design. </title> <booktitle> In Proceedings of Hot Interconnects, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Thus, the HLRC method pays diffing overhead to detect updates and a regular message for each page, whereas the AURC method pays zero software overhead on update detection or message initiation. 2.4 Overlapped Protocols Many parallel architectures <ref> [27, 14, 24, 29] </ref> contain dedicated communication and/or protocol processors that take over most of the overhead of performing these operations from the compute processor (s). <p> These methods require specialized APIs, unlike the prototype systems presented in this paper. Our systems allow programs written for a release-consistent, shared-memory multiprocessor to run without modification. Several multicomputers use a dedicated co-processor for communication on each node. Examples include the Intel Paragon [27] and the Meiko CS-2 <ref> [14] </ref>. The Typhoon [29] system uses a special hardware board to detect access faults at fine granularity and implements distributed shared memory on a network of HyperSparc workstations. It uses one of the two CPUs in the dual-processor workstation as a protocol processor.
Reference: [15] <author> L. Iftode, C. Dubnicki, E. W. Felten, and Kai Li. </author> <title> Improving Release-Consistent Shared Virtual Memory using Automatic Update. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Our own experience shows that many applications do not speedup well using standard LRC-based shared virtual memory on a 32-node machine, and that the speedup curves go down when increasing the number of nodes to 64. Recently, Automatic Update Release Consistency (AURC) <ref> [15] </ref> has been proposed as an alternative LRC-based protocol. The two protocols, AURC and LRC, have different update detection schemes. AURC uses a hardware mechanism called automatic update for update detection while LRC detects the updates in software using diffs. <p> By studying detailed time breakdowns, communication traffic, and memory requirements, we show also that the home-based protocols scale better than the homeless ones. 2 LRC, AURC, and Home-based LRC In this section we briefly review the standard Lazy Release Consistency [22] and Automatic Update Release Consistency <ref> [15] </ref> protocols. We then describe our home-based and overlapped protocol variations. 2.1 Lazy Release Consistency The standard LRC [22] is an all-software, page-based, multiple-writer protocol. It has been implemented and evaluated in the TreadMarks system [21]. <p> Vector timestamps are used to ensure memory coherence before pages are either accessed or fetched. Figure 1 (b) shows how AURC works with a simple example. Further details can be found in <ref> [15, 16] </ref>. The AURC protocol has several advantages over the standard LRC protocol. First, it uses no diff operations. Second, there can be no page faults when accessing home node pages, so it can reduce the number of page faults if homes are chosen intelligently. <p> Tread-Marks [21] was the first SVM implementation using the LRC protocol on a network of stock computers. That implementation has achieved respectable performance on small-scale machines. The recently proposed Automatic Update Release Consistency protocol (AURC) <ref> [15] </ref> is an LRC protocol that takes advantage of the automatic update mechanism in virtual memory-mapped communication. The idea of using a home-based approach to build an all-software protocol similar to AURC was proposed in [17].
Reference: [16] <author> L. Iftode, J. P. Singh, and Kai Li. </author> <title> Understanding Application Performance on Shared Virtual Memory. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: This makes update resolution in AURC extremely simple: a single full-page fetch from the home. On the other hand, in standard LRC collections of updates (diffs) are distributed and homeless, making update resolution more difficult to perform. Extensive performance evaluation <ref> [16] </ref> has shown that AURC outperforms standard LRC in most cases. In this paper we propose two new home-based LRC protocols. The first, Home-based LRC (HLRC), is similar to the Automatic Update Release Consistency (AURC) protocol. <p> When implementing the protocol on a large-scale machine, memory consumption can become a severe problem. To reduce memory consumption the shared virtual memory system must perform garbage collection frequently [21]. 2.2 Automatic Update Release Consistency The AURC protocol <ref> [16] </ref> implements Lazy Release Consistency without using any diff operations by taking advantage of the SHRIMP multicomputer's automatic update hardware mechanism [5, 6]. <p> Vector timestamps are used to ensure memory coherence before pages are either accessed or fetched. Figure 1 (b) shows how AURC works with a simple example. Further details can be found in <ref> [15, 16] </ref>. The AURC protocol has several advantages over the standard LRC protocol. First, it uses no diff operations. Second, there can be no page faults when accessing home node pages, so it can reduce the number of page faults if homes are chosen intelligently. <p> Both types of access patterns are fine-grained and cause considerable false sharing and fragmentation at the page level. The original Splash-2 application was modified to reorganize the task queues and remove unnecessary synchronization to alleviate the problems observed in <ref> [16] </ref>. Table 1 shows the problem sizes and their sequential execution times. For all applications we chose relatively large problem sizes, each requiring approximately 20 minutes of sequential execution. <p> First, the home-based LRC protocols (HLRC and OHLRC) clearly outperform their "homeless" counterparts (LRC and OLRC) with one exception (Ray-trace on 8 node, non-overlapped protocols), in which case the speedups are comparable. These results are consistent with those obtained through simulation in the comparison between LRC and AURC <ref> [16] </ref>. Second, the performance gap between home and homeless protocols increases dramatically for 32 and 64 processors configurations. This result, which is consistent across all applications, reveals a significant difference in scalability between the two classes of protocols. <p> This situation occurs for instance in Water Nsquared, which exhibits a multiple-writer multiple-reader sharing pattern with coarse-grain read and writes <ref> [16] </ref>. The second dominating overhead is data transfer time. In all cases, the data transfer times of HLRC are smaller than those of LRC for three reasons. First, the data traffic to service the page misses is higher for the homeless protocol than for the home-based protocol.
Reference: [17] <author> L. Iftode, J.P. Singh, and K. Li. </author> <title> Scope Consistency: a Bridge Between Release Consistency and Entry Consistency. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1996. </year> <title> [18] i860 TM XP Microprocessor. Programmer's Reference Manual, </title> <year> 1991. </year>
Reference-contexts: Unlike AURC, HLRC requires no specialized hardware support, using diffs as in LRC for update detection and propagation. This idea of using a home-based approach to build an all-software protocol similar to AURC was introduced in <ref> [17] </ref>. Our second protocol, called Overlapped Home-based LRC (OHLRC), takes advantage of the communication processor found on each node of the Paragon to o*oad some of the protocol overhead of HLRC from the critical path followed by the compute processor. <p> The recently proposed Automatic Update Release Consistency protocol (AURC) [15] is an LRC protocol that takes advantage of the automatic update mechanism in virtual memory-mapped communication. The idea of using a home-based approach to build an all-software protocol similar to AURC was proposed in <ref> [17] </ref>. Our home-based LRC protocols are based on the AURC protocol, but the updates are detected in software using diffs, as in the standard LRC. <p> The home-based protocols reduce to a single-writer protocol for applications that exhibit one-writer multiple-readers sharing patterns, like SOR or LU. Other relaxed consistency models include Entry Consistency [3] and Scope Consistency <ref> [17] </ref>. Both models take advantage of the association of data with locks, either explicitly (Entry Consistency) or implicitly (Scope Consistency), to reduce the protocol overhead. Both Orca [2] and CRL [19] are designed to implement distributed shared memory by maintaining coherence at object level instead of page level.
Reference: [19] <author> K.L. Johnson, M.F. Kaashoek, and D.A. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the Fifteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 213-228, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Other relaxed consistency models include Entry Consistency [3] and Scope Consistency [17]. Both models take advantage of the association of data with locks, either explicitly (Entry Consistency) or implicitly (Scope Consistency), to reduce the protocol overhead. Both Orca [2] and CRL <ref> [19] </ref> are designed to implement distributed shared memory by maintaining coherence at object level instead of page level. These methods require specialized APIs, unlike the prototype systems presented in this paper. Our systems allow programs written for a release-consistent, shared-memory multiprocessor to run without modification.
Reference: [20] <author> M. Karlsson and P. Stenstrom. </author> <title> Performance Evaluation of Cluster-Based Multiprocessor Built from ATM Switches and Bus-Based Multiprocessor Servers. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Using simulations they show that such a protocol processor can double the performance of TreadMarks on a 16-node configuration and that diff prefetching is not always beneficial. The protocol they evaluate is similar to our overlapped homeless LRC protocol (OLRC). A recent study <ref> [20] </ref> investigated how to build an SVM system on a network of SMPs. They studied the tradeoffs of using a dedicated processor or the spare cycles of a compute processor to execute coherence protocol.
Reference: [21] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: The two differ on how eagerly they reconcile updates: the RC protocol propagates updates on release, whereas the LRC protocol postpones update propagations until the next acquire. Although previous prototypes have shown reasonably good performance for some applications on small systems <ref> [21] </ref>, protocol overhead becomes substantial on large-scale systems. Our own experience shows that many applications do not speedup well using standard LRC-based shared virtual memory on a 32-node machine, and that the speedup curves go down when increasing the number of nodes to 64. <p> We then describe our home-based and overlapped protocol variations. 2.1 Lazy Release Consistency The standard LRC [22] is an all-software, page-based, multiple-writer protocol. It has been implemented and evaluated in the TreadMarks system <ref> [21] </ref>. The LRC protocol postpones updates to shared pages and uses the causal orders to obtain up-to-date versions. The main idea of the protocol is to use timestamps, or intervals, to establish the happen-before ordering between causal-related events. Local intervals are delimited by synchronization events. <p> When implementing the protocol on a large-scale machine, memory consumption can become a severe problem. To reduce memory consumption the shared virtual memory system must perform garbage collection frequently <ref> [21] </ref>. 2.2 Automatic Update Release Consistency The AURC protocol [16] implements Lazy Release Consistency without using any diff operations by taking advantage of the SHRIMP multicomputer's automatic update hardware mechanism [5, 6]. <p> For LRC and OLRC, barrier synchronizations trigger garbage collection of protocol data structures when the free memory is below a certain threshold, similar to the approach used in TreadMarks <ref> [21] </ref>. Garbage collection is quite complex because it needs to collect all "live" diffs, which are distributed on various nodes. All last writers for each individual shared page need to validate the page by requesting all the missing diffs from other nodes. <p> The model was used to implement shared virtual memory and reduce false sharing by allowing multiple writers [8]. Lazy Release Consistency (LRC) [22, 9, 1] further relaxed the RC protocol to reduce protocol overhead. Tread-Marks <ref> [21] </ref> was the first SVM implementation using the LRC protocol on a network of stock computers. That implementation has achieved respectable performance on small-scale machines.
Reference: [22] <author> P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Lazy Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The known software approach to reducing the overhead of shared virtual memory is to employ relaxed memory consistency models such as Release Consistency (RC) and Lazy Release Consistency (LRC) <ref> [8, 22] </ref>. Both protocols allow multiple writers while avoiding false sharing, and both reduce overheads by maintaining coherence only at acquire and release synchronization points. <p> By studying detailed time breakdowns, communication traffic, and memory requirements, we show also that the home-based protocols scale better than the homeless ones. 2 LRC, AURC, and Home-based LRC In this section we briefly review the standard Lazy Release Consistency <ref> [22] </ref> and Automatic Update Release Consistency [15] protocols. We then describe our home-based and overlapped protocol variations. 2.1 Lazy Release Consistency The standard LRC [22] is an all-software, page-based, multiple-writer protocol. It has been implemented and evaluated in the TreadMarks system [21]. <p> the home-based protocols scale better than the homeless ones. 2 LRC, AURC, and Home-based LRC In this section we briefly review the standard Lazy Release Consistency <ref> [22] </ref> and Automatic Update Release Consistency [15] protocols. We then describe our home-based and overlapped protocol variations. 2.1 Lazy Release Consistency The standard LRC [22] is an all-software, page-based, multiple-writer protocol. It has been implemented and evaluated in the TreadMarks system [21]. The LRC protocol postpones updates to shared pages and uses the causal orders to obtain up-to-date versions. <p> The Release Consistency (RC) model was proposed in order to improve hardware cache coherence [12]. The model was used to implement shared virtual memory and reduce false sharing by allowing multiple writers [8]. Lazy Release Consistency (LRC) <ref> [22, 9, 1] </ref> further relaxed the RC protocol to reduce protocol overhead. Tread-Marks [21] was the first SVM implementation using the LRC protocol on a network of stock computers. That implementation has achieved respectable performance on small-scale machines.
Reference: [23] <author> P.J. Keleher. </author> <title> The Relative Importance of Concurrent Writers and Weak Consistency Models. </title> <booktitle> In Proceedings of the IEEE COMPCON '96 Conference, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: In a recent work <ref> [23] </ref>, Keleher has shown that a simple single-writer LRC protocol perform almost as well as a more complicated multiple-writer LRC. His protocol totally eliminates diff-ing at the expense of a higher bandwidth requirement for full page transfers.
Reference: [24] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Si-moni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford Flash Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Thus, the HLRC method pays diffing overhead to detect updates and a regular message for each page, whereas the AURC method pays zero software overhead on update detection or message initiation. 2.4 Overlapped Protocols Many parallel architectures <ref> [27, 14, 24, 29] </ref> contain dedicated communication and/or protocol processors that take over most of the overhead of performing these operations from the compute processor (s). <p> The Typhoon [29] system uses a special hardware board to detect access faults at fine granularity and implements distributed shared memory on a network of HyperSparc workstations. It uses one of the two CPUs in the dual-processor workstation as a protocol processor. In the Flash multiprocessor <ref> [24] </ref>, each node contains a programmable processor called MAGIC that performs protocol operations and handles all communications within the node and among all nodes. Neither system uses LRC-based relaxed consistency models.
Reference: [25] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The Stanford DASH Prototype: Logic Overhead and Performance. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Research in the last decade shows that it is difficult to build or provide shared memory on a large-scale system. Although the hardware approach to implementing cache coherence has been shown to perform quite well, it requires a high engineering cost <ref> [25] </ref>. Shared virtual memory (SVM) [26], on the other hand, is a cost-effective method to provide the shared memory abstraction on a network of computers since it requires no special hardware support.
Reference: [26] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 229-239, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Research in the last decade shows that it is difficult to build or provide shared memory on a large-scale system. Although the hardware approach to implementing cache coherence has been shown to perform quite well, it requires a high engineering cost [25]. Shared virtual memory (SVM) <ref> [26] </ref>, on the other hand, is a cost-effective method to provide the shared memory abstraction on a network of computers since it requires no special hardware support. The main problem with this approach has been its lack of scalable performance when compared with hardware cache coherence. <p> Despite these factors, the experimental results show that HLRC is still 10% better than LRC. This experiment suggests that HLRC is likely to have robust performance behavior for a large number of applications. 5 Related Work Since shared virtual memory was first proposed ten years ago <ref> [26] </ref>, a lot of work has been done on it. The Release Consistency (RC) model was proposed in order to improve hardware cache coherence [12]. The model was used to implement shared virtual memory and reduce false sharing by allowing multiple writers [8].
Reference: [27] <author> R. Pierce and G. Regnier. </author> <title> The Paragon Implementationof the NX Message Passing Interface. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Thus, the HLRC method pays diffing overhead to detect updates and a regular message for each page, whereas the AURC method pays zero software overhead on update detection or message initiation. 2.4 Overlapped Protocols Many parallel architectures <ref> [27, 14, 24, 29] </ref> contain dedicated communication and/or protocol processors that take over most of the overhead of performing these operations from the compute processor (s). <p> The co-processor runs exclusively in kernel mode, and it is dedicated to communication. The one-way message-passing latency of a 4-byte NX/2 message on the Paragon is about 50 sec <ref> [27] </ref>. The transfer bandwidth for large messages depends on data alignment. When data are aligned properly, the peak achievable bandwidth at the user level is 175 Mbytes/sec. Without proper alignment, the peak bandwidth is about 45 Mbytes/sec. <p> These methods require specialized APIs, unlike the prototype systems presented in this paper. Our systems allow programs written for a release-consistent, shared-memory multiprocessor to run without modification. Several multicomputers use a dedicated co-processor for communication on each node. Examples include the Intel Paragon <ref> [27] </ref> and the Meiko CS-2 [14]. The Typhoon [29] system uses a special hardware board to detect access faults at fine granularity and implements distributed shared memory on a network of HyperSparc workstations. It uses one of the two CPUs in the dual-processor workstation as a protocol processor.
Reference: [28] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <year> 1992. </year> <note> Also Stanford University Technical Report No. CSL-TR-92-526, </note> <month> June </month> <year> 1992. </year>
Reference-contexts: The standard LRC protocol is implemented exclusively at user-level using the NX/2 message library. For the overlapped protocols we modified the co-processor kernel to perform diff-related operations. 3.2 Shared Memory API All four prototypes support the programming interface used with the Splash-2 <ref> [28] </ref> benchmark suite. This is different from the APIs supported by other software shared virtual memory systems, such as TreadMarks. The main rationale for our decision to implement the Splash-2 API is to allow programs written for a release-consistent, shared-memory multiprocessor to run on our systems without any modification.
Reference: [29] <author> James R. Larus Steven K. Reinhardt and David A. Wood. Tempest and Typhoon: </author> <title> User-level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-337, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Thus, the HLRC method pays diffing overhead to detect updates and a regular message for each page, whereas the AURC method pays zero software overhead on update detection or message initiation. 2.4 Overlapped Protocols Many parallel architectures <ref> [27, 14, 24, 29] </ref> contain dedicated communication and/or protocol processors that take over most of the overhead of performing these operations from the compute processor (s). <p> Our systems allow programs written for a release-consistent, shared-memory multiprocessor to run without modification. Several multicomputers use a dedicated co-processor for communication on each node. Examples include the Intel Paragon [27] and the Meiko CS-2 [14]. The Typhoon <ref> [29] </ref> system uses a special hardware board to detect access faults at fine granularity and implements distributed shared memory on a network of HyperSparc workstations. It uses one of the two CPUs in the dual-processor workstation as a protocol processor.
Reference: [30] <author> Ch. A. Thekkath and H.M. Levy. </author> <title> Hardware and Software Support for Efficient Exception Handling. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 110-121, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Time (seconds) LRC, 32 Processors Protocol Overhead Lock Data Transfer Computation 0 2 4 6 8 10 30 50 Execution Time (seconds) HLRC, 8 Processors 0 10 20 30 5 15 Execution Time (seconds) HLRC, 32 Processors Protocol Overhead Lock Data Transfer Computation as aggressive software for fast interrupts, exceptions <ref> [30] </ref> and virtual memory mapped communication [10, 11] have brought such latencies down significantly to the neighborhood of a couple of microseconds. An interesting question is to what extent our results are specific to the Paragon architecture and how they would be affected by different architectural parameters.
Reference: [31] <author> Roger Traylor and Dave Dunning. </author> <title> Routing Chip Set for Intel Paragon Parallel Supercomputer. </title> <booktitle> In Proceedings of Hot Chips '92 Symposium, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: The data caches are coherent between the two processors. The memory bus provides a peak bandwidth of 400 MBytes/sec. The nodes are interconnected with a wormhole routed 2-D mesh network whose peak bandwidth is 200 Mbytes/sec per link <ref> [31] </ref>. The operating system is a micro-kernel based version of OSF/1 with multicomputer extensions for a parallel programming model and the NX/2 message passing primitives. The co-processor runs exclusively in kernel mode, and it is dedicated to communication.
References-found: 30

