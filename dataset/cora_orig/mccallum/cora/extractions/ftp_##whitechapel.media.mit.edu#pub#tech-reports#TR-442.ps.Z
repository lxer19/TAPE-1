URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-442.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: fsbasu,nuria,sandyg@media.mit.edu  
Title: Modeling and Tracking of Human Lip Motions  
Author: Sumit Basu, Nuria Oliver, and Alex Pentland 
Address: 20 Ames St., Cambridge, MA 02139 USA  
Affiliation: MIT Media Laboratory,  
Note: 3D  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 442 Appears: Proceedings of ICCV'98, Bombay, India, January 4-7, 1998 Abstract We address the problem of tracking and reconstructing 3D human lip motions from a 2D view. This problem is challenging due both to the complex nature of lip motions and the minimal data available from a raw video stream of the face. We counter both of these difficulties with statistical approaches. We first build a physically-based 3D model of lips and train it to cover only the subspace of lip motions. We then track this model in video by finding the shape within the subspace that maximizes the posterior probability of the model given the observed features. In this study, the features are the likelihoods of the lip and non-lip color classes: we iteratively derive forces from these values to apply to the physical model and converge to the final solution. Because of the full 3D nature of the model, this framework allows us to track the lips from any head pose. In addition, because of the constraints imposed by the learned subspace of the model, we are able to accurately estimate the full 3D lip shape from the 2D view.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Adjoudani and C. Benoit. </author> <title> "On the Integration of Auditory and Visual Parameters in an HMM-based ASR". </title> <booktitle> In NATO Advanced Study Institute: Speechreading by Man and Machine, </booktitle> <year> 1995. </year>
Reference-contexts: As a result, these models are only two-dimensional. Many are based directly on image data [5],[7]; others use such low level features to form a parametrized description of the lip shape <ref> [1] </ref>. Some of the most interesting work done in this area has been in using a statistically trained model of lip variations. Bregler and Omohundro's work and Luettin's work, for example ([4] and [9]), model the subspace of lip bitmaps and contours respectively.
Reference: [2] <author> Sumit Basu and Alex Pentland. </author> <title> "A Three-Dimensional Model of Human Lip Motions Trained from Video". </title> <booktitle> In Proceedings of the IEEE Non-Rigid and Articulated Motion Workshop, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: A much more detailed account is given in <ref> [2] </ref>. The underlying representation of our initial model is a mesh in the shape of the lips. The mesh is constructed from a linear elastic material modeled by the Finite Element Method (FEM). <p> The details of this method are described in many references (e.g., [3]). For this application, a thin-shell model was chosen. We constructed the model by beginning with a 2D plane-stress isotropic material formulation and adding a strain relationship for the out-of-plane components (see <ref> [2] </ref> for further details). It is important here to understand the difference between a physically-based and a physiological model. <p> We denote the the K 1 matrix with only the rows pertaining to the constrained degrees of freedom as P. The minimum strain solution can then be expressed as: ^ F = P (PP ) U g (2) Details of the derivation can be found in <ref> [2] </ref>. 2 modes 4.2 Modeling the Observations Once we have all the displacements for all of the frames, we can relate the observed deformations to a subset of the "correct" physics of the model.
Reference: [3] <author> Klaus-Jurgen Bathe. </author> <title> Finite Element Procedures in Engineering Analysis. </title> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference-contexts: The individual stress-strain matrices of the elements can be assembled into a single, overall matrix expressing the static equilibrium equation KU = F (1) where the displacements U and forces F are in a global coordinate system. The details of this method are described in many references (e.g., <ref> [3] </ref>). For this application, a thin-shell model was chosen. We constructed the model by beginning with a 2D plane-stress isotropic material formulation and adding a strain relationship for the out-of-plane components (see [2] for further details). <p> can break this expression up into a sum of integrals over the faces (the triangular facets) of the model: f (pjO) = f (p) i face i f (O (x; y)jp) (7) Furthermore, we can evaluate the integral over each face using Gaussian numerical integration with a single sample point <ref> [3] </ref>. In the one point case, the approximation to the integral of a function over a triangular patch is the area of the patch A i multiplied by the value at the center of the patch f (O (x i ; y i )jp).
Reference: [4] <author> Christoph Bregler and Stephen M. Omohundro. </author> <title> "Nonlinear Image Interpolation using Manifold Learning". </title> <booktitle> In NIPS 7, </booktitle> <year> 1995. </year>
Reference: [5] <author> Tarcisio Coianiz, Lorenzo Torresani, and Bruno Caprile. </author> <title> "2D Deformable Models for Visual Speech Analysis". </title> <booktitle> In NATO Advanced Study Institute: Speechreading by Man and Machine, </booktitle> <year> 1995. </year>
Reference: [6] <author> Irfan A. Essa. </author> <title> "Analysis, Interpretation, and Synthesis of Facial Expressions". </title> <type> PhD thesis, </type> <institution> MIT Department of Media Arts and Sciences, </institution> <year> 1995. </year>
Reference: [7] <author> Michael Kass, Andrew Witkin, and Demetri Terzopoulous. "Snakes: </author> <title> Active Contour Models". </title> <journal> International Journal of Computer Vision, </journal> <pages> pages 321-331, </pages> <year> 1988. </year>
Reference: [8] <author> Y. Lee, D. Terzopoulos, and K. Waters. </author> <title> "Realistic Modeling for Facial Animation". </title> <booktitle> In Proceedings of SIGGRAPH, </booktitle> <pages> pages 55-62, </pages> <year> 1995. </year> <month> 6 </month>
Reference-contexts: The other category of lip models includes those designed for synthesis and facial animation. These lip models are usually part of a larger facial animation system, and the lips themselves often have a limited repertoire of motions <ref> [8] </ref>. To their credit, these models are mostly in 3D. For many of the models, though, the control parameters are defined by hand. A few are based on the actual physics of the lips: they attempt to model the physical material and musculature in the mouth region [6],[12].
Reference: [9] <author> J. Luettin, N. Thacker, and S. Beet. </author> <title> "Visual Speech Recog--nition Using Active Shape Models And Hidden Markov Models". </title> <booktitle> In ICASSP96, </booktitle> <pages> pages 817-820. </pages> <booktitle> IEEE Signal Processing Society, </booktitle> <year> 1996. </year>
Reference-contexts: Some of the most interesting work done in this area has been in using a statistically trained model of lip variations. Bregler and Omohundro's work and Luettin's work, for example ([4] and <ref> [9] </ref>), model the subspace of lip bitmaps and contours respectively. However, since these are 2D models, the changes in the apparent lip shape due to rigid rotations have to be modeled as complex changes in the lip pose. One goal of this paper is to extend these ideas to 3D.
Reference: [10] <author> John Martin, Alex Pentland, and Ron Kikinis. </author> <title> Shape analysis of brain structures using physical and experimental modes. In CVPR94. </title> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year>
Reference-contexts: We began with the default physics (i.e., fairly uniform stiffness, only adjacent nodes connected) and have now observed how the model actually deforms. This new information can be used to form a new, "learned" K matrix. Martin et al. <ref> [10] </ref> described the connection between the strain matrix and the covariance of the displacements R u : if we consider the components of the force to be IID with unit variance, we have R u = K s (3) We can now take this mapping in the opposite direction.
Reference: [11] <author> Nuria Oliver and Alex Pentland. Lafter: </author> <title> Lips and face tracking. In CVPR97. </title> <publisher> IEEE Computer Society, </publisher> <year> 1997. </year>
Reference-contexts: We will describe each of these steps in detail in the following sections. 5.1 Training the Color Classes We derive the statistical models of the lip and skin (face) classes using the LAFTER system <ref> [11] </ref>, a real-time active-camera face tracking system. This system uses examples of lip and skin pixels to build models of the probability distributions of each class in color space. The distributions are modeled as mixtures of Gaussians and are estimated using the EM algorithm. <p> See <ref> [11] </ref> for the motivation and history of blob features. In our implementation, feature vectors are computed at each pixel by concatenating the (x; y) spatial coordinates and the color components at that point.
Reference: [12] <author> K. Waters and J. Frisbie. </author> <title> "A Coordinated Muscle Model for Speech Animation". </title> <booktitle> In Graphics Interface, </booktitle> <pages> pages 163-170, </pages> <year> 1995. </year>
Reference-contexts: Humans do not have inde-pendent control of all of these facial muscles: normal motions are a slim subspace of the possible muscle states. Some models have tried to approximate this subspace by modeling key lip positions (visemes) and then interpolating between them (for example <ref> [12] </ref>). However, this limits the accuracy of the resulting lip shapes, since only the key positions are learned from data. We hope to fill the gap in these approaches with a 3D model that can be used for both analysis and synthesis.
Reference: [13] <author> Christopher Wren, Ali Azarbayejani, Trevor Darrell, and Alex Pentland. Pfinder: </author> <title> Real-time tracking of the human body. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 19(7) </volume> <pages> 780-785, </pages> <month> July </month> <year> 1997. </year> <month> 7 </month>
References-found: 13

