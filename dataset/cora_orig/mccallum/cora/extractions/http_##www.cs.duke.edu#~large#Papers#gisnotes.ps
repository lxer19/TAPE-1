URL: http://www.cs.duke.edu/~large/Papers/gisnotes.ps
Refering-URL: http://www.cs.duke.edu/~large/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: External-Memory Algorithms with Applications in Geographic Information Systems  
Author: Lars Arge 
Date: December, 1996 (updated September, 1997)  
Address: Durham, NC 27708-0129 USA  
Affiliation: Department of Computer Science Duke University  
Abstract: In the design of algorithms for large-scale applications it is essential to consider the problem of minimizing Input/Output (I/O) communication. Geographical information systems (GIS) are good examples of such large-scale applications as they frequently handle huge amounts of spatial data. In this note we survey the recent developments in external-memory algorithms with applications in GIS. First we discuss the Aggarwal-Vitter I/O-model and illustrate why normal internal-memory algorithms for even very simple problems can perform terribly in an I/O-environment. Then we describe the fundamental paradigms for designing I/O-efficient algorithms by using them to design efficient sorting algorithms. We then go on and survey external-memory algorithms for computational geometry problems|with special emphasis on problems with applications in GIS|and techniques for designing such algorithms: Using the orthogonal line segment intersection problem we illustrate the distribution-sweeping and the buffer tree techniques which can be used to solve a large number of important problems. Using the batched range searching problem we introduce the external segment tree. We also discuss an algorithm for the reb/blue line segment intersection problem|an important subproblem in map overlaying. In doing so we introduce the batched filtering and the external fractional cascading techniques. Finally, we shortly describe TPIE|a Transparent Parallel I/O Environment designed to allow programmers to write I/O-efficient programs.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. K. Agarwal, L. Arge, T. M. Murali, K. Varadarajan, and J. S. Vitter. </author> <title> I/O-efficient algorithms for contour line extraction and planar graph blocking. </title> <note> In Proc. ACM-SIAM Symp. on Discrete Algorithms (to appear), </note> <year> 1998. </year>
Reference-contexts: In [27] an external on-line version of the topology tree is developed and this structure is used to obtain structures for a number of dynamic problems, including approximate nearest neighbor searching and closest pair maintenance. Very recently, an algorithm has been given <ref> [1] </ref> for preprocessing a TIN into an external data structure such that the contour lines of a query elevation can be computed I/O optimally. 4.1 The Orthogonal Line Segment Intersection Problem The orthogonal line segment intersection problem is that of reporting all intersecting orthogonal pairs in a set of N line
Reference: [2] <author> A. Aggarwal, B. Alpern, A. K. Chandra, and M. Snir. </author> <title> A model for hierarchical memory. </title> <booktitle> In Proc. ACM Symp. on Theory of Computation, </booktitle> <pages> pages 305-314, </pages> <year> 1987. </year>
Reference-contexts: We will discuss some of these experiments in later sections. Finally, it should be mentioned that several authors have considered extended theoretical models that try to model the hierarchical nature of the memory of real machines <ref> [2, 3, 4, 5, 8, 60, 81, 95, 98, 96] </ref>, but such models quickly become theoretically very complicated due to the large number of parameters.
Reference: [3] <author> A. Aggarwal and A. K. Chandra. </author> <title> Virtual memory algorithms. </title> <booktitle> In Proc. ACM Symp. on Theory of Computation, </booktitle> <pages> pages 173-185, </pages> <year> 1988. </year>
Reference-contexts: We will discuss some of these experiments in later sections. Finally, it should be mentioned that several authors have considered extended theoretical models that try to model the hierarchical nature of the memory of real machines <ref> [2, 3, 4, 5, 8, 60, 81, 95, 98, 96] </ref>, but such models quickly become theoretically very complicated due to the large number of parameters. <p> Segments such as CD in Figure 16 that spans at least one slab completely are called long segments. A copy of each long segment is stored in the list of the largest multislab it spans. Thus, CD is stored in the list associated with the multislab <ref> [ 1 ; 3 ] </ref>. All segments that are not long are called short segments. They are not stored in any multislab, but are passed down to lower levels of the tree where they may span recursively defined slabs and be stored. AB and EF are examples of short segments.
Reference: [4] <author> A. Aggarwal, A. K. Chandra, and M. Snir. </author> <title> Hierarchical memory with block transfer. </title> <booktitle> In Proc. IEEE Symp. on Foundations of Comp. Sci., </booktitle> <pages> pages 204-216, </pages> <year> 1987. </year>
Reference-contexts: We will discuss some of these experiments in later sections. Finally, it should be mentioned that several authors have considered extended theoretical models that try to model the hierarchical nature of the memory of real machines <ref> [2, 3, 4, 5, 8, 60, 81, 95, 98, 96] </ref>, but such models quickly become theoretically very complicated due to the large number of parameters. <p> Multislabs are then defined as contiguous ranges of slabs, such as for example <ref> [ 1 ; 4 ] </ref>. There are m=2 p m=2 multislabs and the lists associated with a node are precisely a list for each multislab. The key point is that the number of multislabs is a quadratic function of the branching factor.
Reference: [5] <author> A. Aggarwal and G. Plaxton. </author> <title> Optimal parallel sorting in multi-level storage. </title> <booktitle> Proc. ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 659-668, </pages> <year> 1994. </year>
Reference-contexts: We will discuss some of these experiments in later sections. Finally, it should be mentioned that several authors have considered extended theoretical models that try to model the hierarchical nature of the memory of real machines <ref> [2, 3, 4, 5, 8, 60, 81, 95, 98, 96] </ref>, but such models quickly become theoretically very complicated due to the large number of parameters. <p> The latter papers also deal with fundamental problems such as permutation, sorting and matrix transposition, and a number of authors have considered the difficult problem of sorting optimally on parallel disks <ref> [5, 21, 73, 71] </ref>. The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. <p> Vitter and Shriver [97] used randomization to ensure this and developed an algorithm which performs optimally with high probability. Later Nodine and Vitter [71] managed to develop a deterministic version of D disk distribution sort. An alternative distribution-like algorithm is develop by Aggarwal and Plaxton <ref> [5] </ref>. The buffer tree sorting algorithm can also be modified to work on D disks. Recall that the buffer-emptying process basically was performed like a distribution step, where a memory load of elements were distribute to m=2 buffers one level down the tree.
Reference: [6] <author> A. Aggarwal and J. S. Vitter. </author> <title> The Input/Output complexity of sorting and related problems. </title> <journal> Communications of the ACM, </journal> <volume> 31(9) </volume> <pages> 1116-1127, </pages> <year> 1988. </year>
Reference-contexts: In order to amortize the access time over a large amount of data, typical disks read or write large blocks of contiguous data at once. Therefore we use a theoretical model with the following parameters <ref> [6] </ref>: N = number of elements in the problem instance; M = number of elements that can fit into internal memory; B = number of elements per disk block; where M &lt; N and 1 B M=2. <p> In order to study the performance of external-memory algorithms, we use the standard notion of I/O complexity <ref> [6] </ref>. We define an I/O operation to be the process of simultaneously reading or writing a block of B contiguous data elements to or from the disk. <p> Initial theoretical work on I/O-complexity was done by Floyd [47] and by Hong and Kung [57] who studied matrix transposition and fast Fourier transformation in restricted I/O models. The general I/O model was introduced by Aggarwal and Vitter <ref> [6] </ref> and the notion of parallel disks was introduced by Vitter and Shriver [97]. The latter papers also deal with fundamental problems such as permutation, sorting and matrix transposition, and a number of authors have considered the difficult problem of sorting optimally on parallel disks [5, 21, 73, 71]. <p> Furthermore, one says that an algorithm uses a linear number of I/O operations if it uses O (n) I/Os. Similarly, we introduce m = M=B which is the number of blocks that fit in internal memory. Aggarwal and Vitter <ref> [6] </ref> showed that the number of I/O operations needed to sort N elements is (n log m n), which is then the external-memory equivalent of the well-known (N log 2 N ) internal-memory bound. 1 Furthermore, they showed that the number of I/Os needed to rearrange N elements according to a <p> Going from (N ) to O (n log m n) algorithm extremely important. Permutation bound equal to sorting bound. 3 Paradigms for Designing I/O-efficient Algorithms Originally Aggarwal and Vitter <ref> [6] </ref> presented two basic paradigms for designing I/O-efficient algorithms; the merging and the distribution paradigms. In Section 3.1 and 3.2 we demonstrate the main ideas in these paradigms by showing how to use them to sort N elements in the optimal number of I/Os. <p> However, now we cannot be sure that the p m buckets have equal size N= p m, but fortunately one can prove that they are of approximately equal size, namely that no bucket contains more than 5N=4 p m elements <ref> [6, 63] </ref>. Thus the number of levels in the distribution is less than log 4=5 p overall complexity remains the optimal O (n log m n) I/Os. <p> The non-optimality of disk striping can be demonstrated via the sorting bound. While sorting N elements using disk striping and one of the previously described one-disk sorting algorithms requires O ( n D log m=D n) I/Os, the optimal bound is O ( n D log m n) I/Os <ref> [6] </ref>. Note that the optimal bound gives a linear speedup in the number of disk.
Reference: [7] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: 1 Introduction Traditionally when designing computer programs people have focused on the minimization of the internal computation time and ignored the time spent on Input/Output (I/O). Theoretically one of the most commonly used machine models when designing algorithms is the Random Access Machine (RAM) (see e.g. <ref> [7, 88] </ref>).
Reference: [8] <author> B. Alpern, L. Carter, E. Feig, and T. Selker. </author> <title> The uniform memory hierarchy model of computation. </title> <journal> Algorithmica, </journal> <pages> 12(2-3), </pages> <year> 1994. </year>
Reference-contexts: We will discuss some of these experiments in later sections. Finally, it should be mentioned that several authors have considered extended theoretical models that try to model the hierarchical nature of the memory of real machines <ref> [2, 3, 4, 5, 8, 60, 81, 95, 98, 96] </ref>, but such models quickly become theoretically very complicated due to the large number of parameters.
Reference: [9] <author> R. J. Anderson and G. L. Miller. </author> <title> A simple randomized parallel algorithm for list-ranking. </title> <journal> Information Processing Letters, </journal> <volume> 33 </volume> <pages> 269-273, </pages> <year> 1990. </year>
Reference-contexts: In the parallel algorithm (PRAM) world list ranking is a very fundamental graph problem which extracts the essence in many other problems, and it is used as an important subroutine in many parallel algorithms <ref> [9] </ref>.
Reference: [10] <author> D. S. Andrews, J. Snoeyink, J. Boritz, T. Chan, G. Denham, J. Harrison, and C. Zhu. </author> <title> Further comparisons of algorithms for geometric intersection problems. </title> <booktitle> In Proc. 6th Int'l. Symp. on Spatial Data Handling, </booktitle> <year> 1994. </year>
Reference-contexts: One of most fundamental operations in many GIS systems is map overlaying|the computation of new scenes or maps from a number of existing maps. Some existing software packages are completely based on this operation <ref> [10, 11, 75, 87] </ref>. Given two thematic maps the problem is to compute a new map in which the thematic attributes of each location is a function of the thematic attributes of the corresponding locations in the two input maps. <p> It would be interesting to experimentally compare the algorithms performance to that of other (internal-memory) algorithms for the problem [28, 29, 30, 68, 75]. An experimental comparison of internal-memory algorithms for the problem is already reported in <ref> [10] </ref>. 4.4 Other External-Memory Computational Geometry Algorithms In the previous sections we have discussed the basic techniques for designing efficient external-memory computational geometry algorithms. We have illustrated the powerful distribution sweeping technique using the orthogonal line segment intersection problem.
Reference: [11] <author> ARC/INFO. </author> <title> Understanding GIS|the ARC/INFO method. </title> <address> ARC/INFO, </address> <year> 1993. </year> <note> Rev. 6 for workstations. </note>
Reference-contexts: One of most fundamental operations in many GIS systems is map overlaying|the computation of new scenes or maps from a number of existing maps. Some existing software packages are completely based on this operation <ref> [10, 11, 75, 87] </ref>. Given two thematic maps the problem is to compute a new map in which the thematic attributes of each location is a function of the thematic attributes of the corresponding locations in the two input maps.
Reference: [12] <author> L. Arge. </author> <title> The buffer tree: A new technique for optimal I/O-algorithms. </title> <booktitle> In Proc. Workshop on Algorithms and Data Structures, </booktitle> <volume> LNCS 955, </volume> <pages> pages 334-345, </pages> <year> 1995. </year> <note> A complete version appears as BRICS technical report RS-96-28, </note> <institution> University of Aarhus. </institution>
Reference-contexts: The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry <ref> [12, 14, 19, 32, 53, 99] </ref>, graph theoretical [13, 14, 32, 34, 52, 66] and string processing areas [15, 35, 45, 46]. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n. <p> Actually, it turns out that the permutation bound is a lower bound on the list ranking problem discussed above [34], and as an O (n log m n) I/O algorithm is known for the problem <ref> [12, 32, 34] </ref> we have an asymptotically optimal algorithm for all realistic systems. Even though the algorithm is more complicated than the simple RAM algorithm, Vengroff [90] has performed simulations showing that on large problem instances it has a much better performance than the simple internal-memory algorithm. <p> This approach has the extra benefit of isolating the I/O-specific parts of an algorithm in the data structures. We call the paradigm the data structuring paradigm, and in Section 3.3 we illustrate it by way of the so called buffer tree designed in <ref> [12] </ref>. As we shall see later I/O-efficient data structures turn out to be a very powerful tool in the development of efficient 6 I/O algorithms. For simplicity we only discuss the paradigms in the one disk (D = 1) model. <p> Below we sketch such a basic tree structure developed using what is called the buffer tree technique <ref> [12] </ref>. The structure can be used in the normal tree sort algorithm. In Section 3.3.1 we also sketch how the structure can be used to develop an I/O-efficient external priority queue. <p> Of course one also has to consider how to empty a buffer of a node on the last level in the tree, that is, how to insert elements among the leaves of the O (log m n) m=2 blocks m=2 9 tree and perform rebalancing. In <ref> [12] </ref> it is shown that by using an (a; b)-tree [58] as the basic tree structure this can also be handled in the mentioned I/O bound. An (a; b)-tree is a generalization of the B-tree. Deletions (and queries) can be handled using similar ideas [12]. <p> In <ref> [12] </ref> it is shown that by using an (a; b)-tree [58] as the basic tree structure this can also be handled in the mentioned I/O bound. An (a; b)-tree is a generalization of the B-tree. Deletions (and queries) can be handled using similar ideas [12]. Note that while N insertions (and deletions) in total take O (n log m n) I/Os, a single insertion (or deletion) can take a lot more than O ( log m n B ) I/Os, as a single operation can result in a lot of buffer-emptying processes. <p> In internal memory the plane-sweep paradigm [77] is a very powerful technique for designing computational geometry algorithms, and in [53] an external-memory version of this technique called distribution sweeping is developed. As the name suggests the technique relies on the distribution paradigm. In <ref> [12] </ref> it is shown how the data structuring paradigm can also be use to solve computational geometry problems. It is shown how data structures based on the buffer tree can be used in the standard internal-memory plane-sweep algorithm for a number of problems. <p> It is shown how data structures based on the buffer tree can be used in the standard internal-memory plane-sweep algorithm for a number of problems. In [53] two techniques called batched construction of persistent B-trees and batched filtering are also discussed. In [18] some results from <ref> [53, 12] </ref> are extended and generalized, and some external-memory computational geometry results are also reported in [49, 99]. <p> When we want to perform a rangesearch we create a special element which is pushed down the tree in a lazy way during buffer-emptying processes, just as all other elements. However, we now have to modify the buffer-emptying process. The basic idea in the modification is the following (see <ref> [12, 14] </ref> for details). <p> In <ref> [12, 14] </ref> it is show how we can report all elements in a subtree (now containing other rangesearch elements) in a linear number of I/Os. Using the normal argument it then follows that a rangesearch operation requires O ( log m n B + t 0 ) I/Os amortized. <p> In external memory the batched range searching problem can be solved optimally using distribution sweeping [53] or an external buffered version of the segment tree <ref> [12] </ref>. <p> N=B = n leaves m nodes and EF , are shown. The external segment tree <ref> [12] </ref> is sketched in Figure 16. The base structure is a perfectly balanced tree with branching factor p m over the endpoints. A buffer of size m=2 blocks and m=2 m=2 lists of segments are associated with each node in the tree. <p> Finally, using a complicated integration of all the ideas in the red/blue line segment intersection algorithm with the external priority queue discussed in Section 3.3 <ref> [12] </ref>, one can obtain an O ((n + t) log m n) I/O algorithm for the general line segment intersection problem, where one is just given N segments in the plane and should report their pairwise intersections. 27 4.5 Summary * Main paradigms for developing external computational geometry algorithms: Distribution sweeping.
Reference: [13] <author> L. Arge. </author> <title> The I/O-complexity of ordered binary-decision diagram manipulation. </title> <booktitle> In Proc. Int. Symp. on Algorithms and Computation, </booktitle> <volume> LNCS 1004, </volume> <pages> pages 82-91, </pages> <year> 1995. </year> <note> A complete version appears as BRICS technical report RS-96-29, </note> <institution> University of Aarhus. </institution>
Reference-contexts: The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry [12, 14, 19, 32, 53, 99], graph theoretical <ref> [13, 14, 32, 34, 52, 66] </ref> and string processing areas [15, 35, 45, 46]. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n.
Reference: [14] <author> L. Arge. </author> <title> Efficient External-Memory Data Structures and Applications. </title> <type> PhD thesis, </type> <institution> University of Aarhus, </institution> <month> February/August </month> <year> 1996. </year>
Reference-contexts: The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry <ref> [12, 14, 19, 32, 53, 99] </ref>, graph theoretical [13, 14, 32, 34, 52, 66] and string processing areas [15, 35, 45, 46]. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n. <p> The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry [12, 14, 19, 32, 53, 99], graph theoretical <ref> [13, 14, 32, 34, 52, 66] </ref> and string processing areas [15, 35, 45, 46]. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n. <p> When we want to perform a rangesearch we create a special element which is pushed down the tree in a lazy way during buffer-emptying processes, just as all other elements. However, we now have to modify the buffer-emptying process. The basic idea in the modification is the following (see <ref> [12, 14] </ref> for details). <p> In <ref> [12, 14] </ref> it is show how we can report all elements in a subtree (now containing other rangesearch elements) in a linear number of I/Os. Using the normal argument it then follows that a rangesearch operation requires O ( log m n B + t 0 ) I/Os amortized.
Reference: [15] <author> L. Arge, P. Ferragina, R. Grossi, and J. Vitter. </author> <title> On sorting strings in external memory. </title> <booktitle> In Proc. ACM Symp. on Theory of Computation, </booktitle> <pages> pages 540-548, </pages> <year> 1997. </year> <month> 30 </month>
Reference-contexts: The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry [12, 14, 19, 32, 53, 99], graph theoretical [13, 14, 32, 34, 52, 66] and string processing areas <ref> [15, 35, 45, 46] </ref>. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n.
Reference: [16] <author> L. Arge, M. Knudsen, and K. Larsen. </author> <title> A general lower bound on the I/O-complexity of comparison-based algorithms. </title> <booktitle> In Proc. Workshop on Algorithms and Data Structures, </booktitle> <volume> LNCS 709, </volume> <pages> pages 83-94, </pages> <year> 1993. </year>
Reference-contexts: For extremely small values of M and B the comparison model is assumed in the sorting lower bound|see also <ref> [16, 17] </ref> 5 Taking a closer look at the above bounds for typical values of B and M reveals that because of the large base of the logarithm, log m n is less than 3 or 4 for all realistic values of N and m. <p> The corresponding bound O (n log m n + t) can be obtained for the external versions of the problems either by redoing standard proofs [17, 53], or by using a conversion result from <ref> [16] </ref>. Computational geometry problems in external memory were first considered by Goodrich et al. [53], who developed a number of techniques for designing I/O-efficient algorithms for such problems. They used their techniques to develop I/O algorithms for a large number of important problems.
Reference: [17] <author> L. Arge and P. B. Miltersen. </author> <title> On showing lower bounds for external-memory computational geometry. </title> <note> In preparation. </note>
Reference-contexts: For extremely small values of M and B the comparison model is assumed in the sorting lower bound|see also <ref> [16, 17] </ref> 5 Taking a closer look at the above bounds for typical values of B and M reveals that because of the large base of the logarithm, log m n is less than 3 or 4 for all realistic values of N and m. <p> The corresponding bound O (n log m n + t) can be obtained for the external versions of the problems either by redoing standard proofs <ref> [17, 53] </ref>, or by using a conversion result from [16]. Computational geometry problems in external memory were first considered by Goodrich et al. [53], who developed a number of techniques for designing I/O-efficient algorithms for such problems.
Reference: [18] <author> L. Arge, O. Procopiuc, S. Ramaswamy, T. Suel, and J. S. Vitter. </author> <title> Theory and practice of I/O-efficient algorithms for multidimensional batched searching problems. </title> <note> In Proc. ACM-SIAM Symp. on Discrete Algorithms (to appear), </note> <year> 1998. </year>
Reference-contexts: It is shown how data structures based on the buffer tree can be used in the standard internal-memory plane-sweep algorithm for a number of problems. In [53] two techniques called batched construction of persistent B-trees and batched filtering are also discussed. In <ref> [18] </ref> some results from [53, 12] are extended and generalized, and some external-memory computational geometry results are also reported in [49, 99]. <p> Most of these problems have important applications in GIS systems. In <ref> [32, 33, 18] </ref> some experimental results on the practical performance of external-memory algorithms for computational geometry problems are reported. We divide our survey of external-memory computational geometry into four main parts. <p> On the other hand the buffer tree algorithm does not need to sort the input twice as the distribution sweeping algorithm does. We plan to perform experiments with the buffer tree solution in the future. Finally, it should be mentioned that Vengroff and Vitter [92] and Arge et al. <ref> [18] </ref> have also performed some experiments with I/O algorithms. <p> Also the external segment tree approach for solving the batched range searching problem can be extended from 2 to d dimensions. A couple of new ideas is needed and the resulting algorithm uses O (n log d1 n + t) I/Os <ref> [18] </ref>. <p> Several of these problems have applications in GIS systems. In <ref> [18] </ref> some of the algorithms are extended to work in d dimensions. Goodrich et al. [53] also discussed external-memory algorithms for the convex hull problem, that is, the problem of computing the smallest convex polytope completely enclosing a set of N points in d-dimensional space. <p> Also, for the implemented benchmarks, the time spent on I/O range from being negligible to being of the same order of magnitude as internal computation time, showing that using TPIE a large degree of overlap between computation and I/O can be accomplished. Very recently, Arge et al. <ref> [18] </ref> reported similar encouraging experiences with a TPIE implementation of an algorithms for the pairwise rectangle intersection problem. 6 Conclusions As GIS systems frequently handle huge amounts of data it is getting increasingly important to design algorithms with good I/O performance for problems arising in such systems.
Reference: [19] <author> L. Arge, D. E. Vengroff, and J. S. Vitter. </author> <title> External-memory algorithms for processing line segments in geographic information systems. </title> <journal> Algorithmica, </journal> <note> to appear. special issues on Geographical Information Systems. </note>
Reference-contexts: The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry <ref> [12, 14, 19, 32, 53, 99] </ref>, graph theoretical [13, 14, 32, 34, 52, 66] and string processing areas [15, 35, 45, 46]. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n. <p> In [53] two techniques called batched construction of persistent B-trees and batched filtering are also discussed. In [18] some results from [53, 12] are extended and generalized, and some external-memory computational geometry results are also reported in [49, 99]. In <ref> [19] </ref> efficient I/O algorithms for a large number of problems involving line segments in the plane are designed by combining the ideas of distribution sweeping, batched filtering, buffer trees and a new technique, which can be regarded as an external-memory version of fractional cascading [31]. <p> In the next subsection we try to illustrate this before we in subsections 4.3.2 and 4.3.3 sketch how to actually solve the problem in the optimal number of I/Os. 4.3.1 The Endpoint Dominance Problem Let us consider the endpoint dominance (EPD) problem defined as follows <ref> [19] </ref>: Given N nonintersecting line segments in the plane, find the segment directly above each endpoint of each segment|refer to Figure 17. 22 Even though EPD seems to be a rather simple problem, it is a powerful tool for solving other important problems. <p> The problem of sorting N non-intersecting segments is to extend the partial order defined in this way to a total order. vertical lines through the four endpoints to obtain their relation. Thus one way to sort N segments <ref> [19] </ref> is to add two "extreme" segments as indicated in Figure 19, and use EPD twice to find for each endpoint the segments immediately above and below it. <p> This gives us less than one I/O per query point per node! Fortunately, it is possible to modify the external segment tree and the query algorithm to overcome these difficulties <ref> [19] </ref>. To do so we first strengthen the definition of the external segment tree and require that the segments in the multislab lists are sorted. <p> Note that all pairs of segments in the same multislab list can be compared just by comparing the order of their endpoints on one of the boundaries of the multislab, and that a multislab list thus can be sorted using a standard sorting algorithm. In <ref> [19] </ref> it is shown how to build an external segment tree with sorted multislab lists on N non-intersecting segments in O (n log m n) I/Os. The construction is basically done using distribution sweeping. <p> The construction is rather technical, but the general idea is the following (the interested reader is referred to <ref> [19] </ref> for details): First a preprocessing step is used (like in fractional cascading) to sample a set 24 of segments from each slab in each node and the multislab lists of the corresponding child are augmented with these segments. <p> Next the actual filtering up the O (log m n) levels is performed, and on each level the dominating segment is found for all the query points. This is done I/O efficiently using the merging paradigm and the sampled segments from the preprocessing phase. In <ref> [19] </ref> it is shown that one filtering step can be performed in O (n) I/Os, and thus EPD can be solved in O (n log m n) I/O operations. 4.3.3 External Red/Blue Line Segment Intersection Algorithm Using the solution to the EPD problem, or rather the ability to sort non-intersecting segments, <p> Details in the algorithm appear in <ref> [19] </ref>, where it is also proved that the sweep can be performed in O (n + t 0 ) I/Os as required. To summarize, the red/blue line segment intersection problem can be solved in the optimal O (n log m n + t) I/Os. <p> The O (n log m n) solution to the EPD problem discussed in the last section, which lead to the segment sorting and the red/blue line segment intersection algorithms, has several other almost immediate consequences <ref> [19] </ref>. If one takes a closer look at the algorithm for EPD one realizes that it works in general with K query points, which are not necessarily endpoints of the segments.
Reference: [20] <author> L. Arge and J. S. Vitter. </author> <title> Optimal dynamic interval management in external memory. </title> <booktitle> In Proc. IEEE Symp. on Foundations of Comp. Sci., </booktitle> <pages> pages 560-569, </pages> <year> 1996. </year>
Reference-contexts: For completeness it should be mentioned that recently a number of researchers have considered the design of worst-case efficient external-memory "on-line" data structures, mainly for (special cases of) two and three dimensional range searching <ref> [20, 25, 59, 61, 78, 84, 93] </ref>. While B-trees [22, 37, 65] efficiently support range searching in one dimension they are inefficient in higher dimensions. Similarly the many sophisticated internal-memory data structures for range searching are not efficient when mapped to external memory.
Reference: [21] <author> R. D. Barve, E. F. Grove, and J. S. Vitter. </author> <title> Simple randomized mergesort on parallel disks. </title> <booktitle> In Proc. ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1996. </year>
Reference-contexts: An increasingly popular approach to further increase the throughput of the I/O system is to use a number of disks in parallel [50, 51, 97]. Several authors have considered an extension of the above model with a parameter D denoting the number of disks in the system <ref> [21, 73, 71, 72, 97] </ref>. In the parallel disk model [97] one can read or write one block from each of the D disks simultaneously in one I/O. The number of disks D range up to 10 2 in current disk arrays. <p> The latter papers also deal with fundamental problems such as permutation, sorting and matrix transposition, and a number of authors have considered the difficult problem of sorting optimally on parallel disks <ref> [5, 21, 73, 71] </ref>. The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. <p> Nevertheless, Nodine and Vitter [73] managed to developed a (rather complicated) D-disk sorting algorithm based on merge sort. Very recently Barve et al. <ref> [21] </ref> develop a very simple and practical randomize D-disk merge sort algorithm. Intuitively, it seems easier to make distribution sort work optimally on parallel disks.
Reference: [22] <author> R. Bayer and E. McCreight. </author> <title> Organization and maintenance of large ordered indexes. </title> <journal> Acta Informatica, </journal> <volume> 1 </volume> <pages> 173-189, </pages> <year> 1972. </year>
Reference-contexts: Why not use the same algorithms in external memory, exchanging the data structures with I/O-efficient versions of the structures? The standard well-known search tree structure for external memory is the B-tree <ref> [22, 37, 65] </ref>. <p> For completeness it should be mentioned that recently a number of researchers have considered the design of worst-case efficient external-memory "on-line" data structures, mainly for (special cases of) two and three dimensional range searching [20, 25, 59, 61, 78, 84, 93]. While B-trees <ref> [22, 37, 65] </ref> efficiently support range searching in one dimension they are inefficient in higher dimensions. Similarly the many sophisticated internal-memory data structures for range searching are not efficient when mapped to external memory.
Reference: [23] <author> J. L. Bentley. </author> <title> Algorithms for klee's rectangle problems. </title> <institution> Dept. of Computer Science, Carnegie Mellon Univ., </institution> <note> unpublished notes, </note> <year> 1977. </year>
Reference-contexts: Given N points and N (axis-parallel) rectangles in the plane (Figure 14) the problem consists of reporting for each rectangle all points that lie inside it. The optimal internal-memory plane-sweep algorithm for the problem uses a data structure called a segment tree <ref> [23, 77] </ref>. The segment tree is a well-known dynamic data structure used to store a set of N segments in one dimension, such that given a query point all segments containing the point can be found in O (log 2 N + T ) time.
Reference: [24] <author> J. L. Bentley and D. Wood. </author> <title> An optimal worst case algorithm for reporting intersections of rectangles. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 29 </volume> <pages> 571-577, </pages> <year> 1980. </year>
Reference-contexts: an algorithm for this problem, as well as for the orthogonal line segment intersection problem, one can also obtain an optimal external algorithm for the pairwise rectangle intersection problem|the problem of given N rectangles in the plane (with sides parallel to the axes) to report all intersecting pairs of rectangles <ref> [24] </ref>. Also the external segment tree approach for solving the batched range searching problem can be extended from 2 to d dimensions. A couple of new ideas is needed and the resulting algorithm uses O (n log d1 n + t) I/Os [18].
Reference: [25] <author> G. Blankenagel and R. H. Guting. </author> <title> XP-trees|External priority search trees. </title> <type> Technical report, </type> <institution> FernUniversitat Hagen, </institution> <note> Informatik-Bericht Nr. 92, </note> <year> 1990. </year>
Reference-contexts: For completeness it should be mentioned that recently a number of researchers have considered the design of worst-case efficient external-memory "on-line" data structures, mainly for (special cases of) two and three dimensional range searching <ref> [20, 25, 59, 61, 78, 84, 93] </ref>. While B-trees [22, 37, 65] efficiently support range searching in one dimension they are inefficient in higher dimensions. Similarly the many sophisticated internal-memory data structures for range searching are not efficient when mapped to external memory.
Reference: [26] <author> M. Blum, R. W. Floyd, V. Pratt, R. L. Rievest, and R. E. Tarjan. </author> <title> Time bounds for selection. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 7 </volume> <pages> 448-461, </pages> <year> 1973. </year>
Reference-contexts: The obvious way to find the p m pivot elements would be to find every N= p m'th element using the k-selection algorithm p m times. The k-selection algorithm <ref> [26] </ref> finds the k'th smallest 7 N items m buckets Size of bucket is M m 2 buckets element in O (N ) time in internal memory and it can easily be modified to work in O (n) I/Os in external memory.
Reference: [27] <author> P. Callahan, M. T. Goodrich, and K. Ramaiyer. </author> <title> Topology B-trees and their applications. </title> <booktitle> In Proc. Workshop on Algorithms and Data Structures, </booktitle> <volume> LNCS 955, </volume> <pages> pages 381-392, </pages> <year> 1995. </year>
Reference-contexts: Range searching is also considered in [74, 82, 83] where the problem of maintaining range trees in external memory is considered. However, the model used in this work is different from 13 the one considered here. In <ref> [27] </ref> an external on-line version of the topology tree is developed and this structure is used to obtain structures for a number of dynamic problems, including approximate nearest neighbor searching and closest pair maintenance.
Reference: [28] <author> T. M. Chan. </author> <title> A simple trapezoid sweep algorithm for reporting red/blue segment intersections. </title> <booktitle> In Proc. of 6th Canadian Conference on Computational Geometry, </booktitle> <year> 1994. </year>
Reference-contexts: However, as the segment sorting algorithm used in the solution is relatively complicated, its practical importance may be limited. It would be interesting to experimentally compare the algorithms performance to that of other (internal-memory) algorithms for the problem <ref> [28, 29, 30, 68, 75] </ref>. An experimental comparison of internal-memory algorithms for the problem is already reported in [10]. 4.4 Other External-Memory Computational Geometry Algorithms In the previous sections we have discussed the basic techniques for designing efficient external-memory computational geometry algorithms.
Reference: [29] <author> B. Chazelle and H. Edelsbrunner. </author> <title> An optimal algorithm for intersecting line segments in the plane. </title> <journal> Journal of the ACM, </journal> <volume> 39 </volume> <pages> 1-54, </pages> <year> 1992. </year>
Reference-contexts: However, as the segment sorting algorithm used in the solution is relatively complicated, its practical importance may be limited. It would be interesting to experimentally compare the algorithms performance to that of other (internal-memory) algorithms for the problem <ref> [28, 29, 30, 68, 75] </ref>. An experimental comparison of internal-memory algorithms for the problem is already reported in [10]. 4.4 Other External-Memory Computational Geometry Algorithms In the previous sections we have discussed the basic techniques for designing efficient external-memory computational geometry algorithms.
Reference: [30] <author> B. Chazelle, H. Edelsbrunner, L. J. Guibas, and M. Sharir. </author> <title> Algorithms for bichromatic line-segment problems and polyhedral terrains. </title> <journal> Algorithmica, </journal> <volume> 11 </volume> <pages> 116-132, </pages> <year> 1994. </year>
Reference-contexts: We also needs to be able to look for the dominating segment in many of the multislabs lists. However, one can overcome these problems using batched filtering [53] and a technique similar to what in internal memory is called fractional cascading <ref> [30, 31, 86] </ref>. The idea in batched filtering is to process all the queries at the same time and level by level, such that the dominating segments in nodes on one level of the structure are found for all the queries, before continuing to consider nodes on the next level. <p> However, as the segment sorting algorithm used in the solution is relatively complicated, its practical importance may be limited. It would be interesting to experimentally compare the algorithms performance to that of other (internal-memory) algorithms for the problem <ref> [28, 29, 30, 68, 75] </ref>. An experimental comparison of internal-memory algorithms for the problem is already reported in [10]. 4.4 Other External-Memory Computational Geometry Algorithms In the previous sections we have discussed the basic techniques for designing efficient external-memory computational geometry algorithms.
Reference: [31] <author> B. Chazelle and L. J. Guibas. Fractional cascading: I. </author> <title> A data structuring technique. </title> <journal> Algorith-mica, </journal> <volume> 1 </volume> <pages> 133-162, </pages> <year> 1986. </year>
Reference-contexts: In [19] efficient I/O algorithms for a large number of problems involving line segments in the plane are designed by combining the ideas of distribution sweeping, batched filtering, buffer trees and a new technique, which can be regarded as an external-memory version of fractional cascading <ref> [31] </ref>. Most of these problems have important applications in GIS systems. In [32, 33, 18] some experimental results on the practical performance of external-memory algorithms for computational geometry problems are reported. We divide our survey of external-memory computational geometry into four main parts. <p> We also needs to be able to look for the dominating segment in many of the multislabs lists. However, one can overcome these problems using batched filtering [53] and a technique similar to what in internal memory is called fractional cascading <ref> [30, 31, 86] </ref>. The idea in batched filtering is to process all the queries at the same time and level by level, such that the dominating segments in nodes on one level of the structure are found for all the queries, before continuing to consider nodes on the next level.
Reference: [32] <author> Y.-J. Chiang. </author> <title> Dynamic and I/O-Efficient Algorithms for Computational Geometry and Graph Problems: Theoretical and Experimental Results. </title> <type> PhD thesis, </type> <institution> Brown University, </institution> <month> August </month> <year> 1995. </year> <month> 31 </month>
Reference-contexts: This is supported by experimental results which show that implementing algorithms designed for the model can lead to significant runtime improvements in practice <ref> [32, 33, 89, 92] </ref>. We will discuss some of these experiments in later sections. <p> The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry <ref> [12, 14, 19, 32, 53, 99] </ref>, graph theoretical [13, 14, 32, 34, 52, 66] and string processing areas [15, 35, 45, 46]. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n. <p> The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry [12, 14, 19, 32, 53, 99], graph theoretical <ref> [13, 14, 32, 34, 52, 66] </ref> and string processing areas [15, 35, 45, 46]. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n. <p> Actually, it turns out that the permutation bound is a lower bound on the list ranking problem discussed above [34], and as an O (n log m n) I/O algorithm is known for the problem <ref> [12, 32, 34] </ref> we have an asymptotically optimal algorithm for all realistic systems. Even though the algorithm is more complicated than the simple RAM algorithm, Vengroff [90] has performed simulations showing that on large problem instances it has a much better performance than the simple internal-memory algorithm. <p> In the parallel algorithm (PRAM) world list ranking is a very fundamental graph problem which extracts the essence in many other problems, and it is used as an important subroutine in many parallel algorithms [9]. This turns out also to be the case in external memory <ref> [32, 34] </ref>. 2.2 Summary * RAM algorithms typically use (N ) I/Os when analyzed in parallel disk model. * Typical bounds in one-disk model (divide by D in D-disk model): Scanning bound: fi ( N B ) = fi (n). <p> Most of these problems have important applications in GIS systems. In <ref> [32, 33, 18] </ref> some experimental results on the practical performance of external-memory algorithms for computational geometry problems are reported. We divide our survey of external-memory computational geometry into four main parts. <p> algorithm for the orthogonal segment intersection problem, and doing so we obtain an optimal O (n log m n + t) I/O solution to the problem. 4.1.3 Experimental Results One main reason why we choose the orthogonal line segment intersection problem as our initial computational geometry problem is that Chiang <ref> [32, 33] </ref> has performed experiments on the practical performance of several of the described algorithms for the problem. Chiang considered four algorithms, namely the distribution sweeping algorithm, denoted Distribution, and three variants of the plane-sweep algorithm, denoted B-tree, 234-Tree, and 234-Tree-Core. <p> Much larger problems instances seem to be solvable with I/O algorithms than with main memory algorithms. 5 TPIE | A Transparent Parallel I/O Environment In Section 4.1 we discussed the experiments with orthogonal line segment intersection algorithms carried out by Chiang <ref> [32, 33] </ref>. As discussed these experiments suggest that algorithms developed for the parallel disk model perform well in practice, and that they can very well lead to the solution of problem instances one would not be able to solve in practice with algorithms developed for main memory. <p> The Transparent Parallel I/O Environment (TPIE) proposed by Vengroff [89, 91, 94] tries to take advantage of this. While Chiang <ref> [32, 33] </ref> performed experiments in order to compare the efficiency of algorithms designed for internal and external memory and to validate the I/O-model, TPIE is designed to assist programmers in the development of I/O-efficient (and easily portable) programs.
Reference: [33] <author> Y.-J. Chiang. </author> <title> Experiments on the practical I/O efficiency of geometric algorithms: Distribution sweep vs. plane sweep. </title> <booktitle> In Proc. Workshop on Algorithms and Data Structures, </booktitle> <volume> LNCS 955, </volume> <pages> pages 346-357, </pages> <year> 1995. </year>
Reference-contexts: This is supported by experimental results which show that implementing algorithms designed for the model can lead to significant runtime improvements in practice <ref> [32, 33, 89, 92] </ref>. We will discuss some of these experiments in later sections. <p> Most of these problems have important applications in GIS systems. In <ref> [32, 33, 18] </ref> some experimental results on the practical performance of external-memory algorithms for computational geometry problems are reported. We divide our survey of external-memory computational geometry into four main parts. <p> algorithm for the orthogonal segment intersection problem, and doing so we obtain an optimal O (n log m n + t) I/O solution to the problem. 4.1.3 Experimental Results One main reason why we choose the orthogonal line segment intersection problem as our initial computational geometry problem is that Chiang <ref> [32, 33] </ref> has performed experiments on the practical performance of several of the described algorithms for the problem. Chiang considered four algorithms, namely the distribution sweeping algorithm, denoted Distribution, and three variants of the plane-sweep algorithm, denoted B-tree, 234-Tree, and 234-Tree-Core. <p> The performance measures used was total running time (wall not cpu), number of I/O operations performed (i.e. number of blocks read and written by the program), and the number of page faults occurred (I/Os controlled by the operating system)|see <ref> [33] </ref> for a precise description of the experimental setting. The first surprising result of the experiments was that the main memory available for use is typically much smaller than what would be expected. <p> Much larger problems instances seem to be solvable with I/O algorithms than with main memory algorithms. 5 TPIE | A Transparent Parallel I/O Environment In Section 4.1 we discussed the experiments with orthogonal line segment intersection algorithms carried out by Chiang <ref> [32, 33] </ref>. As discussed these experiments suggest that algorithms developed for the parallel disk model perform well in practice, and that they can very well lead to the solution of problem instances one would not be able to solve in practice with algorithms developed for main memory. <p> The Transparent Parallel I/O Environment (TPIE) proposed by Vengroff [89, 91, 94] tries to take advantage of this. While Chiang <ref> [32, 33] </ref> performed experiments in order to compare the efficiency of algorithms designed for internal and external memory and to validate the I/O-model, TPIE is designed to assist programmers in the development of I/O-efficient (and easily portable) programs.
Reference: [34] <author> Y.-J. Chiang, M. T. Goodrich, E. F. Grove, R. Tamassia, D. E. Vengroff, and J. S. Vitter. </author> <title> External-memory graph algorithms. </title> <booktitle> In Proc. ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 139-149, </pages> <year> 1995. </year>
Reference-contexts: The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry [12, 14, 19, 32, 53, 99], graph theoretical <ref> [13, 14, 32, 34, 52, 66] </ref> and string processing areas [15, 35, 45, 46]. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n. <p> This fact is one of the important facts distinguishing the parallel disk model from the RAM-model, as any permutation can be performed in O (N ) time in the latter. Actually, it turns out that the permutation bound is a lower bound on the list ranking problem discussed above <ref> [34] </ref>, and as an O (n log m n) I/O algorithm is known for the problem [12, 32, 34] we have an asymptotically optimal algorithm for all realistic systems. <p> Actually, it turns out that the permutation bound is a lower bound on the list ranking problem discussed above [34], and as an O (n log m n) I/O algorithm is known for the problem <ref> [12, 32, 34] </ref> we have an asymptotically optimal algorithm for all realistic systems. Even though the algorithm is more complicated than the simple RAM algorithm, Vengroff [90] has performed simulations showing that on large problem instances it has a much better performance than the simple internal-memory algorithm. <p> In the parallel algorithm (PRAM) world list ranking is a very fundamental graph problem which extracts the essence in many other problems, and it is used as an important subroutine in many parallel algorithms [9]. This turns out also to be the case in external memory <ref> [32, 34] </ref>. 2.2 Summary * RAM algorithms typically use (N ) I/Os when analyzed in parallel disk model. * Typical bounds in one-disk model (divide by D in D-disk model): Scanning bound: fi ( N B ) = fi (n). <p> Using this information we create a (planar s; t-) graph where nodes correspond to segments and where the relations between the segments define the edges. Then the sorted order can be obtained by topologically sorting this graph in O (n log m n) I/Os using an algorithm developed in <ref> [34] </ref>. This means that if EPD can be solved in O (n log m n) I/Os then N segments can be sorted in the same number of I/Os. In internal memory EPD can be solved optimally with a simple plane-sweep algorithm.
Reference: [35] <author> D. R. Clark and J. I. Munro. </author> <title> Efficient suffix trees on secondary storage. </title> <booktitle> In Proc. ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 383-391, </pages> <year> 1996. </year>
Reference-contexts: The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry [12, 14, 19, 32, 53, 99], graph theoretical [13, 14, 32, 34, 52, 66] and string processing areas <ref> [15, 35, 45, 46] </ref>. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n.
Reference: [36] <author> A. Cockcroft. </author> <title> Sun Performance and Tuning. SPARC & Solaris. </title> <publisher> Sun Microsystems Inc., </publisher> <year> 1995. </year>
Reference-contexts: However, in practice there is a huge difference in access time of fast internal memory and slower external memory such as disks. While typical access time of main memory is measured in nanoseconds, a typical access time of a disk is on the order of milliseconds <ref> [36] </ref>.
Reference: [37] <author> D. Comer. </author> <title> The ubiquitous B-tree. </title> <journal> ACM Computing Surveys, </journal> <volume> 11(2) </volume> <pages> 121-137, </pages> <year> 1979. </year>
Reference-contexts: Why not use the same algorithms in external memory, exchanging the data structures with I/O-efficient versions of the structures? The standard well-known search tree structure for external memory is the B-tree <ref> [22, 37, 65] </ref>. <p> For completeness it should be mentioned that recently a number of researchers have considered the design of worst-case efficient external-memory "on-line" data structures, mainly for (special cases of) two and three dimensional range searching [20, 25, 59, 61, 78, 84, 93]. While B-trees <ref> [22, 37, 65] </ref> efficiently support range searching in one dimension they are inefficient in higher dimensions. Similarly the many sophisticated internal-memory data structures for range searching are not efficient when mapped to external memory.
Reference: [38] <author> T. H. Cormen. </author> <title> Virtual Memory for Data Parallel Computing. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: The latter papers also deal with fundamental problems such as permutation, sorting and matrix transposition, and a number of authors have considered the difficult problem of sorting optimally on parallel disks [5, 21, 73, 71]. The problem of implementing various classes of permutations has been addressed in <ref> [38, 39, 41] </ref>. More recently researchers have moved on to more specialized problems in the computational geometry [12, 14, 19, 32, 53, 99], graph theoretical [13, 14, 32, 34, 52, 66] and string processing areas [15, 35, 45, 46].
Reference: [39] <author> T. H. Cormen. </author> <title> Fast permuting in disk arrays. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 17(1-2):41-57, </volume> <year> 1993. </year>
Reference-contexts: The latter papers also deal with fundamental problems such as permutation, sorting and matrix transposition, and a number of authors have considered the difficult problem of sorting optimally on parallel disks [5, 21, 73, 71]. The problem of implementing various classes of permutations has been addressed in <ref> [38, 39, 41] </ref>. More recently researchers have moved on to more specialized problems in the computational geometry [12, 14, 19, 32, 53, 99], graph theoretical [13, 14, 32, 34, 52, 66] and string processing areas [15, 35, 45, 46].
Reference: [40] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: We assume that the reader has some basic knowledge about e.g. fundamental sorting algorithms and data structures like balanced search trees (especially B-trees) and priority queues. We also assume that the reader is familiar with asymptotic notation (O (), (),fi ()). One excellent textbook covering these subjects is <ref> [40] </ref>. 2 RAM-Complexity and I/O-Complexity In order to illustrate the difference in complexity of a problem in the RAM model and the parallel disk model, consider the following simple problem: We are given an N -vertex linked list stored as an (unsorted) sequence of vertices, each with a pointer to the <p> It uses O (n log m n) I/Os in the preprocessing phase and O (N log B n + t) I/Os to do the sweep. The second variation, 234-Tree, also uses external merge sort but uses a 2-3-4 Tree <ref> [40] </ref> (a generic search tree structure equivalent to a red-black tree) as sweep structure, viewing the internal memory as having an infinite size and letting the virtual memory feature of the operating systems handle the page faults during the sweep.
Reference: [41] <author> T. H. Cormen and L. F. Wisniewski. </author> <title> Asymptotically tight bounds for performing BMMC permutations on parallel disk systems. </title> <booktitle> In Proc. ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 130-139, </pages> <year> 1993. </year>
Reference-contexts: The latter papers also deal with fundamental problems such as permutation, sorting and matrix transposition, and a number of authors have considered the difficult problem of sorting optimally on parallel disks [5, 21, 73, 71]. The problem of implementing various classes of permutations has been addressed in <ref> [38, 39, 41] </ref>. More recently researchers have moved on to more specialized problems in the computational geometry [12, 14, 19, 32, 53, 99], graph theoretical [13, 14, 32, 34, 52, 66] and string processing areas [15, 35, 45, 46].
Reference: [42] <author> R. F. Cromp. </author> <title> An intellegent information fusion system for handling the archiving and querying of terabyte-sized spatial databases. </title> <editor> In S. R. Tate ed., </editor> <title> Report on the Workshop on Data and Image Compression Needs and Uses in the Scientific Community, </title> <type> CESDIS Technical Report Series, </type> <month> TR-93-99, </month> <pages> pages 75-84, </pages> <year> 1993. </year>
Reference-contexts: As geographic information systems (GIS) frequently store, manipulate, and search through enormous amounts of spatial data <ref> [42, 56, 67, 80, 87] </ref> they are good examples of such large-scale applications. The amount of data manipulated in such systems is often too large to fit in main memory and must reside on disk, hence the I/O communication can become a very severe bottleneck. <p> The amount of data manipulated in such systems is often too large to fit in main memory and must reside on disk, hence the I/O communication can become a very severe bottleneck. An especially good example is NASA's EOS project GIS system <ref> [42] </ref>, which is expected to manipulate petabytes (thousands of terabytes, or millions of gigabytes) of data! The effect of the I/O bottleneck is getting more pronounced as internal computation gets faster, and especially as parallel computing gains popularity [76].
Reference: [43] <author> M. de Berg, M. van Kreveld, M. Overmars, and O. Schwarzkopf. </author> <title> Computational Geometry - Algorithms and Applications. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1997. </year>
Reference-contexts: In this problem one is given a set of non-intersecting red segments and a set of non-intersecting blue segments and should compute all intersection red-blue segment pairs. In general many important problems from computational geometry are abstractions of important GIS operations <ref> [43, 87] </ref>.
Reference: [44] <author> H. Edelsbrunner and M. Overmars. </author> <title> Batched dynamic solutions to decomposable searching problems. </title> <journal> Journal of Algorithms, </journal> <volume> 6 </volume> <pages> 515-542, </pages> <year> 1985. </year>
Reference-contexts: Actually, in the plane-sweep algorithm the entire sequence of updates and queries on the data structure is known in advance, and the only requirement on the queries is that they must all eventually be answered. In general such problems are known as batched dynamic problems <ref> [44] </ref>.
Reference: [45] <author> P. Ferragina and R. Grossi. </author> <title> A fully-dynamic data structure for external substring search. </title> <booktitle> In Proc. ACM Symp. on Theory of Computation, </booktitle> <pages> pages 693-702, </pages> <year> 1995. </year>
Reference-contexts: The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry [12, 14, 19, 32, 53, 99], graph theoretical [13, 14, 32, 34, 52, 66] and string processing areas <ref> [15, 35, 45, 46] </ref>. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n.
Reference: [46] <author> P. Ferragina and R. Grossi. </author> <title> Fast string searching in secondary storage: Theoretical developments and experimental results. </title> <booktitle> In Proc. ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 373-382, </pages> <year> 1996. </year>
Reference-contexts: The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry [12, 14, 19, 32, 53, 99], graph theoretical [13, 14, 32, 34, 52, 66] and string processing areas <ref> [15, 35, 45, 46] </ref>. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n.
Reference: [47] <author> R. W. Floyd. </author> <title> Permuting information in idealized two-level storage. </title> <booktitle> In Complexity of Computer Calculations, </booktitle> <pages> pages 105-109, </pages> <year> 1972. </year> <editor> R. Miller and J. Thatcher, Eds. </editor> <publisher> Plenum, </publisher> <address> New York. </address>
Reference-contexts: For simplicity we give all the bounds in the one-disk model. In the general D-disk model they should all be divided by D. Initial theoretical work on I/O-complexity was done by Floyd <ref> [47] </ref> and by Hong and Kung [57] who studied matrix transposition and fast Fourier transformation in restricted I/O models. The general I/O model was introduced by Aggarwal and Vitter [6] and the notion of parallel disks was introduced by Vitter and Shriver [97].
Reference: [48] <author> A. Fournier and D. Y. Montuno. </author> <title> Triangulating simple polygons and equivalent problems. </title> <journal> ACM Trans. on Graphics, </journal> <volume> 3(2) </volume> <pages> 153-174, </pages> <year> 1984. </year>
Reference-contexts: First it leads to an algorithm for trapezoid decomposition of a set of N segments [69, 77], as the core of this problem precisely is to find for each segment endpoint the segment immediately above it. Using a slightly modified version of an internal-memory algorithm <ref> [48] </ref>, the ability to compute a trapezoid decomposition of a simple polygon then leads to an O (n log m n) polygon triangulation algorithm.
Reference: [49] <author> P. G. Franciosa and M. Talamo. </author> <title> Orders, implicit k-sets representation and fast halfplane searching. </title> <booktitle> In Proc. Workshop on Orders, Algorithms and Applications (ORDAL'94), </booktitle> <pages> pages 117-127, </pages> <year> 1994. </year>
Reference-contexts: In [53] two techniques called batched construction of persistent B-trees and batched filtering are also discussed. In [18] some results from [53, 12] are extended and generalized, and some external-memory computational geometry results are also reported in <ref> [49, 99] </ref>. In [19] efficient I/O algorithms for a large number of problems involving line segments in the plane are designed by combining the ideas of distribution sweeping, batched filtering, buffer trees and a new technique, which can be regarded as an external-memory version of fractional cascading [31].
Reference: [50] <author> G. R. Ganger, B. L. Worthington, R. Y. Hou, and Y. N. Patt. </author> <title> Disk arrays. High-performance, high-reliability storage subsystems. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 30-46, </pages> <year> 1994. </year>
Reference-contexts: Large-scale problem instances can be in the range N = 10 10 to N = 10 12 . An increasingly popular approach to further increase the throughput of the I/O system is to use a number of disks in parallel <ref> [50, 51, 97] </ref>. Several authors have considered an extension of the above model with a parameter D denoting the number of disks in the system [21, 73, 71, 72, 97].
Reference: [51] <author> D. Gifford and A. Spector. </author> <title> The TWA reservation system. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 650-665, </pages> <year> 1984. </year>
Reference-contexts: Large-scale problem instances can be in the range N = 10 10 to N = 10 12 . An increasingly popular approach to further increase the throughput of the I/O system is to use a number of disks in parallel <ref> [50, 51, 97] </ref>. Several authors have considered an extension of the above model with a parameter D denoting the number of disks in the system [21, 73, 71, 72, 97].
Reference: [52] <author> M. T. Goodrich, M. H. Nodine, and J. S. Vitter. </author> <title> Blocking for external graph searching. </title> <journal> Algorithmica, </journal> <volume> 16(2) </volume> <pages> 181-214, </pages> <year> 1996. </year>
Reference-contexts: The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry [12, 14, 19, 32, 53, 99], graph theoretical <ref> [13, 14, 32, 34, 52, 66] </ref> and string processing areas [15, 35, 45, 46]. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n.
Reference: [53] <author> M. T. Goodrich, J.-J. Tsay, D. E. Vengroff, and J. S. Vitter. </author> <title> External-memory computational geometry. </title> <booktitle> In Proc. IEEE Symp. on Foundations of Comp. Sci., </booktitle> <pages> pages 714-723, </pages> <year> 1993. </year>
Reference-contexts: The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry <ref> [12, 14, 19, 32, 53, 99] </ref>, graph theoretical [13, 14, 32, 34, 52, 66] and string processing areas [15, 35, 45, 46]. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n. <p> The corresponding bound O (n log m n + t) can be obtained for the external versions of the problems either by redoing standard proofs <ref> [17, 53] </ref>, or by using a conversion result from [16]. Computational geometry problems in external memory were first considered by Goodrich et al. [53], who developed a number of techniques for designing I/O-efficient algorithms for such problems. <p> The corresponding bound O (n log m n + t) can be obtained for the external versions of the problems either by redoing standard proofs [17, 53], or by using a conversion result from [16]. Computational geometry problems in external memory were first considered by Goodrich et al. <ref> [53] </ref>, who developed a number of techniques for designing I/O-efficient algorithms for such problems. They used their techniques to develop I/O algorithms for a large number of important problems. In internal memory the plane-sweep paradigm [77] is a very powerful technique for designing computational geometry algorithms, and in [53] an external-memory <p> et al. <ref> [53] </ref>, who developed a number of techniques for designing I/O-efficient algorithms for such problems. They used their techniques to develop I/O algorithms for a large number of important problems. In internal memory the plane-sweep paradigm [77] is a very powerful technique for designing computational geometry algorithms, and in [53] an external-memory version of this technique called distribution sweeping is developed. As the name suggests the technique relies on the distribution paradigm. In [12] it is shown how the data structuring paradigm can also be use to solve computational geometry problems. <p> In [12] it is shown how the data structuring paradigm can also be use to solve computational geometry problems. It is shown how data structures based on the buffer tree can be used in the standard internal-memory plane-sweep algorithm for a number of problems. In <ref> [53] </ref> two techniques called batched construction of persistent B-trees and batched filtering are also discussed. In [18] some results from [53, 12] are extended and generalized, and some external-memory computational geometry results are also reported in [49, 99]. <p> It is shown how data structures based on the buffer tree can be used in the standard internal-memory plane-sweep algorithm for a number of problems. In [53] two techniques called batched construction of persistent B-trees and batched filtering are also discussed. In [18] some results from <ref> [53, 12] </ref> are extended and generalized, and some external-memory computational geometry results are also reported in [49, 99]. <p> In the next two subsections we discuss I/O-optimal solutions to the problem using the distribution sweeping and buffer tree techniques. 4.1.1 Distribution Sweeping Distribution sweeping <ref> [53] </ref> is a powerful technique obtained by combining the distribution and the plane-sweep paradigms. Let us briefly sketch how it works in general. <p> As insertions and deletions can be performed in O (log 2 N ) time on a segment tree the algorithm runs in the optimal O (N log 2 N + T ) time. In external memory the batched range searching problem can be solved optimally using distribution sweeping <ref> [53] </ref> or an external buffered version of the segment tree [12]. <p> We also needs to be able to look for the dominating segment in many of the multislabs lists. However, one can overcome these problems using batched filtering <ref> [53] </ref> and a technique similar to what in internal memory is called fractional cascading [30, 31, 86]. <p> We have illustrated the powerful distribution sweeping technique using the orthogonal line segment intersection problem. We have also already men 26 tioned that the technique can be used to solve the batched range searching problem. In <ref> [53] </ref> it is discussed how it can be used to develop optimal algorithms for a number of other important problems, including for the problems of finding the pairwise intersection of N rectangles, finding all nearest neighbors for a set of N points in the plane, computing the measure (area) of the <p> Several of these problems have applications in GIS systems. In [18] some of the algorithms are extended to work in d dimensions. Goodrich et al. <ref> [53] </ref> also discussed external-memory algorithms for the convex hull problem, that is, the problem of computing the smallest convex polytope completely enclosing a set of N points in d-dimensional space.
Reference: [54] <author> R. L. Graham. </author> <title> An efficient algorithm for determining the convex hull of a finite planar set. </title> <journal> Information Processing Letters, </journal> <volume> 1 </volume> <pages> 132-133, </pages> <year> 1972. </year>
Reference-contexts: Goodrich et al. [53] also discussed external-memory algorithms for the convex hull problem, that is, the problem of computing the smallest convex polytope completely enclosing a set of N points in d-dimensional space. In the two-dimensional case the internal-memory algorithm known as Graham's scan <ref> [54, 77] </ref> can be modified in a simple way to obtain an O (n log m n) I/O external algorithm. They also discussed how to obtain an output-sensitive algorithm based upon an external version of the marriage-before-conquest technique [62]. The algorithm uses O (n log m t) I/Os.
Reference: [55] <author> L. J. Guibas and J. Stolfi. </author> <title> Primitives for the manipulation of general subdivisions and the computation of voronoi diagrams. </title> <journal> ACM Trans. on Graphics, </journal> <volume> 4 </volume> <pages> 74-123, </pages> <year> 1985. </year>
Reference-contexts: Finally, they developed O (n log m n) algorithms for the three-dimensional case which is particularly interesting because of the close relation to the two-dimensional versions of the problems of computing the Voronoi diagram and the Delaunay triangulation of a set of N points. Using the reduction described in <ref> [55] </ref> the 3-d convex hull algorithm immediately gives algorithms for the two latter problems with the same I/O performance.
Reference: [56] <author> L. M. Haas and W. F. Cody. </author> <title> Exploiting extensible dbms in integrated geographic information systems. </title> <booktitle> In Proc. of Advances in Spatial Databases, LNCS 525, </booktitle> <year> 1991. </year>
Reference-contexts: As geographic information systems (GIS) frequently store, manipulate, and search through enormous amounts of spatial data <ref> [42, 56, 67, 80, 87] </ref> they are good examples of such large-scale applications. The amount of data manipulated in such systems is often too large to fit in main memory and must reside on disk, hence the I/O communication can become a very severe bottleneck.
Reference: [57] <author> J. W. Hong and H. T. Kung. </author> <title> I/O complexity: The red-blue pebble game. </title> <booktitle> In Proc. ACM Symp. on Theory of Computation, </booktitle> <pages> pages 326-333, </pages> <year> 1981. </year>
Reference-contexts: For simplicity we give all the bounds in the one-disk model. In the general D-disk model they should all be divided by D. Initial theoretical work on I/O-complexity was done by Floyd [47] and by Hong and Kung <ref> [57] </ref> who studied matrix transposition and fast Fourier transformation in restricted I/O models. The general I/O model was introduced by Aggarwal and Vitter [6] and the notion of parallel disks was introduced by Vitter and Shriver [97].
Reference: [58] <author> S. Huddleston and K. Mehlhorn. </author> <title> A new data structure for representing sorted lists. </title> <journal> Acta Informatica, </journal> <volume> 17 </volume> <pages> 157-184, </pages> <year> 1982. </year>
Reference-contexts: In [12] it is shown that by using an (a; b)-tree <ref> [58] </ref> as the basic tree structure this can also be handled in the mentioned I/O bound. An (a; b)-tree is a generalization of the B-tree. Deletions (and queries) can be handled using similar ideas [12].
Reference: [59] <author> C. Icking, R. Klein, and T. Ottmann. </author> <title> Priority search trees in secondary memory. </title> <booktitle> In Proc. Graph-Theoretic Concepts in Computer Science, </booktitle> <volume> LNCS 314, </volume> <pages> pages 84-93, </pages> <year> 1987. </year>
Reference-contexts: For completeness it should be mentioned that recently a number of researchers have considered the design of worst-case efficient external-memory "on-line" data structures, mainly for (special cases of) two and three dimensional range searching <ref> [20, 25, 59, 61, 78, 84, 93] </ref>. While B-trees [22, 37, 65] efficiently support range searching in one dimension they are inefficient in higher dimensions. Similarly the many sophisticated internal-memory data structures for range searching are not efficient when mapped to external memory.
Reference: [60] <author> B. H. H. Juurlink and H. A. G. Wijshoff. </author> <title> The parallel hierarchical memory model. </title> <booktitle> In Proc. Scandinavian Workshop on Algorithms Theory, </booktitle> <volume> LNCS 824, </volume> <pages> pages 240-251, </pages> <year> 1993. </year>
Reference-contexts: We will discuss some of these experiments in later sections. Finally, it should be mentioned that several authors have considered extended theoretical models that try to model the hierarchical nature of the memory of real machines <ref> [2, 3, 4, 5, 8, 60, 81, 95, 98, 96] </ref>, but such models quickly become theoretically very complicated due to the large number of parameters.
Reference: [61] <author> P. C. Kanellakis, S. Ramaswamy, D. E. Vengroff, and J. S. Vitter. </author> <title> Indexing for data models with constraints and classes. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 52(3), </volume> <year> 1996. </year>
Reference-contexts: This way one is not able to take full advantage of the large internal memory. Actually, using a decision tree like argument as in <ref> [61] </ref>, one can show that the search bound is indeed optimal in such an "on-line" setting (assuming the comparison model). <p> For completeness it should be mentioned that recently a number of researchers have considered the design of worst-case efficient external-memory "on-line" data structures, mainly for (special cases of) two and three dimensional range searching <ref> [20, 25, 59, 61, 78, 84, 93] </ref>. While B-trees [22, 37, 65] efficiently support range searching in one dimension they are inefficient in higher dimensions. Similarly the many sophisticated internal-memory data structures for range searching are not efficient when mapped to external memory. <p> This has lead to the development of a large number of structures that do not have good theoretical worst-case update and query I/O bounds, but do have good average-case behavior for common problems| see <ref> [70, 61] </ref>. Range searching is also considered in [74, 82, 83] where the problem of maintaining range trees in external memory is considered. However, the model used in this work is different from 13 the one considered here.
Reference: [62] <author> D. G. Kirkpatrick and R. Seidel. </author> <title> The ultimate planar convex hull algorithm? SIAM Journal of Computing, </title> <booktitle> 15 </booktitle> <pages> 287-299, </pages> <year> 1986. </year>
Reference-contexts: In the two-dimensional case the internal-memory algorithm known as Graham's scan [54, 77] can be modified in a simple way to obtain an O (n log m n) I/O external algorithm. They also discussed how to obtain an output-sensitive algorithm based upon an external version of the marriage-before-conquest technique <ref> [62] </ref>. The algorithm uses O (n log m t) I/Os.
Reference: [63] <author> M. Knudsen and K. Larsen. </author> <title> I/O-complexity of comparison and permutation problems. </title> <type> Master's thesis, </type> <institution> University of Aarhus, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: However, now we cannot be sure that the p m buckets have equal size N= p m, but fortunately one can prove that they are of approximately equal size, namely that no bucket contains more than 5N=4 p m elements <ref> [6, 63] </ref>. Thus the number of levels in the distribution is less than log 4=5 p overall complexity remains the optimal O (n log m n) I/Os.
Reference: [64] <author> M. Knudsen and K. Larsen. </author> <title> Simulating I/O-algorithms. Master student project, </title> <institution> University of Aarhus, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: Thus the number of levels in the distribution is less than log 4=5 p overall complexity remains the optimal O (n log m n) I/Os. Even though merge sort is a lot simpler and efficient in practice than distribution sort <ref> [64] </ref>, the distribution paradigm is the most frequently used of the two paradigms. Mainly two factors make distribution sort less efficient in practice than merge sort, namely the larger number of levels in the recursion and the computation of the pivot elements.
Reference: [65] <author> D. Knuth. </author> <title> The Art of Computer Programming, Vol. 3 Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Why not use the same algorithms in external memory, exchanging the data structures with I/O-efficient versions of the structures? The standard well-known search tree structure for external memory is the B-tree <ref> [22, 37, 65] </ref>. <p> Instead, every time we want to read the next block of a run, we have to predict which fi (D) block we will need to load next and "prefetch" them together with the desired block. The prediction can be done with a technique due to Knuth called forecasting <ref> [65] </ref>. <p> For completeness it should be mentioned that recently a number of researchers have considered the design of worst-case efficient external-memory "on-line" data structures, mainly for (special cases of) two and three dimensional range searching [20, 25, 59, 61, 78, 84, 93]. While B-trees <ref> [22, 37, 65] </ref> efficiently support range searching in one dimension they are inefficient in higher dimensions. Similarly the many sophisticated internal-memory data structures for range searching are not efficient when mapped to external memory.
Reference: [66] <author> V. Kumar and E. Schwabe. </author> <title> Improved algorithms and data structures for solving graph problems in external memory. </title> <booktitle> In Proc. IEEE Symp. on Parallel and Distributed Processing, </booktitle> <year> 1996. </year>
Reference-contexts: The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry [12, 14, 19, 32, 53, 99], graph theoretical <ref> [13, 14, 32, 34, 52, 66] </ref> and string processing areas [15, 35, 45, 46]. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n. <p> It should be noted that recently an alternative priority queue was developed in <ref> [66] </ref>. 3.4 Sorting on Parallel Disks In the previous sections we have discussed a number of sorting algorithms working in the one-disk model. As mentioned in the introduction, one approach to increase the throughput of I/O systems 10 is to use a number of disks in parallel.
Reference: [67] <author> R. Laurini and A. D. Thompson. </author> <title> Fundamentals of Spatial Information Systems. A.P.I.C. Series, </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1992. </year> <month> 33 </month>
Reference-contexts: As geographic information systems (GIS) frequently store, manipulate, and search through enormous amounts of spatial data <ref> [42, 56, 67, 80, 87] </ref> they are good examples of such large-scale applications. The amount of data manipulated in such systems is often too large to fit in main memory and must reside on disk, hence the I/O communication can become a very severe bottleneck.
Reference: [68] <author> H. G. Mairson and J. Stolfi. </author> <title> Reporting and counting intersections between two sets of line segments. </title> <editor> In R. Earnshaw (ed.), </editor> <booktitle> Theoretical Foundation of Computer Graphics and CAD, NATO ASI Series, </booktitle> <volume> Vol. F40, </volume> <pages> pages 307-326, </pages> <year> 1988. </year>
Reference-contexts: However, as the segment sorting algorithm used in the solution is relatively complicated, its practical importance may be limited. It would be interesting to experimentally compare the algorithms performance to that of other (internal-memory) algorithms for the problem <ref> [28, 29, 30, 68, 75] </ref>. An experimental comparison of internal-memory algorithms for the problem is already reported in [10]. 4.4 Other External-Memory Computational Geometry Algorithms In the previous sections we have discussed the basic techniques for designing efficient external-memory computational geometry algorithms.
Reference: [69] <author> K. Mulmuley. </author> <title> Computational Geometry. An introduction through randomized algorithms. </title> <publisher> Prentice-Hall, </publisher> <year> 1994. </year>
Reference-contexts: Similarly the EPD algorithm leads to algorithms for a couple of region decomposition problems. First it leads to an algorithm for trapezoid decomposition of a set of N segments <ref> [69, 77] </ref>, as the core of this problem precisely is to find for each segment endpoint the segment immediately above it.
Reference: [70] <author> J. Nievergelt and P. Widmayer. </author> <title> Spatial data structures: Concepts and design choices. </title> <editor> In M. van Kreveld, J. Nievergelt, T. Roos, and P. Widmayer, editors, </editor> <title> Algorithmic Foundations of GIS. </title> <publisher> Springer-Verlag, </publisher> <year> 1997. </year> <booktitle> Lecture Notes in Computer Science (subseries: tutorials). </booktitle>
Reference-contexts: This has lead to the development of a large number of structures that do not have good theoretical worst-case update and query I/O bounds, but do have good average-case behavior for common problems| see <ref> [70, 61] </ref>. Range searching is also considered in [74, 82, 83] where the problem of maintaining range trees in external memory is considered. However, the model used in this work is different from 13 the one considered here.
Reference: [71] <author> M. H. Nodine and J. S. Vitter. </author> <title> Deterministic distribution sort in shared and distributed memory multiprocessors. </title> <booktitle> In Proc. ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 120-129, </pages> <year> 1993. </year>
Reference-contexts: An increasingly popular approach to further increase the throughput of the I/O system is to use a number of disks in parallel [50, 51, 97]. Several authors have considered an extension of the above model with a parameter D denoting the number of disks in the system <ref> [21, 73, 71, 72, 97] </ref>. In the parallel disk model [97] one can read or write one block from each of the D disks simultaneously in one I/O. The number of disks D range up to 10 2 in current disk arrays. <p> The latter papers also deal with fundamental problems such as permutation, sorting and matrix transposition, and a number of authors have considered the difficult problem of sorting optimally on parallel disks <ref> [5, 21, 73, 71] </ref>. The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. <p> Vitter and Shriver [97] used randomization to ensure this and developed an algorithm which performs optimally with high probability. Later Nodine and Vitter <ref> [71] </ref> managed to develop a deterministic version of D disk distribution sort. An alternative distribution-like algorithm is develop by Aggarwal and Plaxton [5]. The buffer tree sorting algorithm can also be modified to work on D disks. <p> The buffer tree sorting algorithm can also be modified to work on D disks. Recall that the buffer-emptying process basically was performed like a distribution step, where a memory load of elements were distribute to m=2 buffers one level down the tree. Thus using the techniques developed in <ref> [71] </ref> the buffer-emptying algorithm can be modified to work on D disks, and we obtain an optimal D-disk sorting algorithm. As already mentioned, distribution sort is rather inefficient in practice, mainly because of the overhead used to compute the pivot elements.
Reference: [72] <author> M. H. Nodine and J. S. Vitter. </author> <title> Paradigms for optimal sorting with multiple disks. </title> <booktitle> In Proc. of the 26th Hawaii Int. Conf. on Systems Sciences, </booktitle> <year> 1993. </year>
Reference-contexts: An increasingly popular approach to further increase the throughput of the I/O system is to use a number of disks in parallel [50, 51, 97]. Several authors have considered an extension of the above model with a parameter D denoting the number of disks in the system <ref> [21, 73, 71, 72, 97] </ref>. In the parallel disk model [97] one can read or write one block from each of the D disks simultaneously in one I/O. The number of disks D range up to 10 2 in current disk arrays.
Reference: [73] <author> M. H. Nodine and J. S. Vitter. </author> <title> Greed sort: An optimal sorting algorithm for multiple disks. </title> <journal> Journal of the ACM, </journal> <pages> pages 919-933, </pages> <year> 1995. </year>
Reference-contexts: An increasingly popular approach to further increase the throughput of the I/O system is to use a number of disks in parallel [50, 51, 97]. Several authors have considered an extension of the above model with a parameter D denoting the number of disks in the system <ref> [21, 73, 71, 72, 97] </ref>. In the parallel disk model [97] one can read or write one block from each of the D disks simultaneously in one I/O. The number of disks D range up to 10 2 in current disk arrays. <p> The latter papers also deal with fundamental problems such as permutation, sorting and matrix transposition, and a number of authors have considered the difficult problem of sorting optimally on parallel disks <ref> [5, 21, 73, 71] </ref>. The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. <p> But the way the blocks should be assigned to disks depends on the merge steps forming the other m 1 runs which will participate in the next merging pass, and therefore it seems hard to figure out how to assign the blocks to disks. Nevertheless, Nodine and Vitter <ref> [73] </ref> managed to developed a (rather complicated) D-disk sorting algorithm based on merge sort. Very recently Barve et al. [21] develop a very simple and practical randomize D-disk merge sort algorithm. Intuitively, it seems easier to make distribution sort work optimally on parallel disks.
Reference: [74] <author> M. Overmars, M. Smid, M. de Berg, and M. van Kreveld. </author> <title> Maintaining range trees in secundary memory. Part I: Partitions. </title> <journal> Acta Informatica, </journal> <volume> 27 </volume> <pages> 423-452, </pages> <year> 1990. </year>
Reference-contexts: This has lead to the development of a large number of structures that do not have good theoretical worst-case update and query I/O bounds, but do have good average-case behavior for common problems| see [70, 61]. Range searching is also considered in <ref> [74, 82, 83] </ref> where the problem of maintaining range trees in external memory is considered. However, the model used in this work is different from 13 the one considered here.
Reference: [75] <author> L. Palazzi and J. Snoeyink. </author> <title> Counting and reporting red/blue segment intersections. </title> <booktitle> In Proc. Workshop on Algorithms and Data Structures, </booktitle> <volume> LNCS 709, </volume> <pages> pages 530-540, </pages> <year> 1993. </year>
Reference-contexts: One of most fundamental operations in many GIS systems is map overlaying|the computation of new scenes or maps from a number of existing maps. Some existing software packages are completely based on this operation <ref> [10, 11, 75, 87] </ref>. Given two thematic maps the problem is to compute a new map in which the thematic attributes of each location is a function of the thematic attributes of the corresponding locations in the two input maps. <p> However, as the segment sorting algorithm used in the solution is relatively complicated, its practical importance may be limited. It would be interesting to experimentally compare the algorithms performance to that of other (internal-memory) algorithms for the problem <ref> [28, 29, 30, 68, 75] </ref>. An experimental comparison of internal-memory algorithms for the problem is already reported in [10]. 4.4 Other External-Memory Computational Geometry Algorithms In the previous sections we have discussed the basic techniques for designing efficient external-memory computational geometry algorithms.
Reference: [76] <author> Y. N. Patt. </author> <title> The I/O subsystem|a candidate for improvement. Guest Editor's Introduction in IEEE Computer, </title> <booktitle> 27(3) </booktitle> <pages> 15-16, </pages> <year> 1994. </year>
Reference-contexts: An especially good example is NASA's EOS project GIS system [42], which is expected to manipulate petabytes (thousands of terabytes, or millions of gigabytes) of data! The effect of the I/O bottleneck is getting more pronounced as internal computation gets faster, and especially as parallel computing gains popularity <ref> [76] </ref>. Currently, technological advances are increasing CPU speeds at an annual rate of 40-60% while disk transfer rates are only increasing by 7-10% annually [79]. Internal memory sizes are also increasing, but not nearly fast enough to meet the needs of important large-scale applications.
Reference: [77] <author> F. P. Preparata and M. I. Shamos. </author> <title> Computational Geometry: An Introduction. </title> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: Computational geometry problems in external memory were first considered by Goodrich et al. [53], who developed a number of techniques for designing I/O-efficient algorithms for such problems. They used their techniques to develop I/O algorithms for a large number of important problems. In internal memory the plane-sweep paradigm <ref> [77] </ref> is a very powerful technique for designing computational geometry algorithms, and in [53] an external-memory version of this technique called distribution sweeping is developed. As the name suggests the technique relies on the distribution paradigm. <p> In internal memory a simple optimal solution to the problem based on the plane-sweep paradigm <ref> [77] </ref> works as follows (refer to plane and every time we meet a horizontal segment we report all vertical segments intersecting the segment. To do so we maintain a balanced search tree containing the vertical segments currently crossing the sweep line, ordered according to x-coordinate. <p> Given N points and N (axis-parallel) rectangles in the plane (Figure 14) the problem consists of reporting for each rectangle all points that lie inside it. The optimal internal-memory plane-sweep algorithm for the problem uses a data structure called a segment tree <ref> [23, 77] </ref>. The segment tree is a well-known dynamic data structure used to store a set of N segments in one dimension, such that given a query point all segments containing the point can be found in O (log 2 N + T ) time. <p> Goodrich et al. [53] also discussed external-memory algorithms for the convex hull problem, that is, the problem of computing the smallest convex polytope completely enclosing a set of N points in d-dimensional space. In the two-dimensional case the internal-memory algorithm known as Graham's scan <ref> [54, 77] </ref> can be modified in a simple way to obtain an O (n log m n) I/O external algorithm. They also discussed how to obtain an output-sensitive algorithm based upon an external version of the marriage-before-conquest technique [62]. The algorithm uses O (n log m t) I/Os. <p> Similarly the EPD algorithm leads to algorithms for a couple of region decomposition problems. First it leads to an algorithm for trapezoid decomposition of a set of N segments <ref> [69, 77] </ref>, as the core of this problem precisely is to find for each segment endpoint the segment immediately above it.
Reference: [78] <author> S. Ramaswamy and S. Subramanian. </author> <title> Path caching: A technique for optimal external searching. </title> <booktitle> In Proc. ACM Symp. Principles of Database Systems, </booktitle> <year> 1994. </year>
Reference-contexts: For completeness it should be mentioned that recently a number of researchers have considered the design of worst-case efficient external-memory "on-line" data structures, mainly for (special cases of) two and three dimensional range searching <ref> [20, 25, 59, 61, 78, 84, 93] </ref>. While B-trees [22, 37, 65] efficiently support range searching in one dimension they are inefficient in higher dimensions. Similarly the many sophisticated internal-memory data structures for range searching are not efficient when mapped to external memory.
Reference: [79] <author> C. Ruemmler and J. Wilkes. </author> <title> An introduction to disk drive modeling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <year> 1994. </year>
Reference-contexts: Currently, technological advances are increasing CPU speeds at an annual rate of 40-60% while disk transfer rates are only increasing by 7-10% annually <ref> [79] </ref>. Internal memory sizes are also increasing, but not nearly fast enough to meet the needs of important large-scale applications. <p> In this note we consider basic paradigms for designing efficient external-memory algorithms and external-memory computational geometry algorithms with applications in GIS. P M D M P 2 1.1 The Parallel Disk Model Accurately modeling memory and disk systems is a complex task <ref> [79] </ref>. The primary feature of disks that we want to model is their extremely long access time relative to that of internal memory. In order to amortize the access time over a large amount of data, typical disks read or write large blocks of contiguous data at once.
Reference: [80] <author> H. Samet. </author> <title> Applications of Spatial Data Structures: Computer Graphics, Image Processing, and GIS. </title> <publisher> Addison Wesley, </publisher> <address> MA, </address> <year> 1989. </year>
Reference-contexts: As geographic information systems (GIS) frequently store, manipulate, and search through enormous amounts of spatial data <ref> [42, 56, 67, 80, 87] </ref> they are good examples of such large-scale applications. The amount of data manipulated in such systems is often too large to fit in main memory and must reside on disk, hence the I/O communication can become a very severe bottleneck.
Reference: [81] <author> J. E. Savage. </author> <title> Space-time tradeoffs in memory hierarchies. </title> <type> Technical Report CS-93-08, </type> <institution> Brown University, </institution> <year> 1993. </year>
Reference-contexts: We will discuss some of these experiments in later sections. Finally, it should be mentioned that several authors have considered extended theoretical models that try to model the hierarchical nature of the memory of real machines <ref> [2, 3, 4, 5, 8, 60, 81, 95, 98, 96] </ref>, but such models quickly become theoretically very complicated due to the large number of parameters.
Reference: [82] <author> M. Smid. </author> <title> Dynamic Data Structures on Multiple Storage Media. </title> <type> PhD thesis, </type> <institution> University of Amsterdam, </institution> <year> 1989. </year>
Reference-contexts: This has lead to the development of a large number of structures that do not have good theoretical worst-case update and query I/O bounds, but do have good average-case behavior for common problems| see [70, 61]. Range searching is also considered in <ref> [74, 82, 83] </ref> where the problem of maintaining range trees in external memory is considered. However, the model used in this work is different from 13 the one considered here.
Reference: [83] <author> M. Smid and M. Overmars. </author> <title> Maintaining range trees in secundary memory. Part II: Lower bounds. </title> <journal> Acta Informatica, </journal> <volume> 27 </volume> <pages> 453-480, </pages> <year> 1990. </year>
Reference-contexts: This has lead to the development of a large number of structures that do not have good theoretical worst-case update and query I/O bounds, but do have good average-case behavior for common problems| see [70, 61]. Range searching is also considered in <ref> [74, 82, 83] </ref> where the problem of maintaining range trees in external memory is considered. However, the model used in this work is different from 13 the one considered here.
Reference: [84] <author> S. Subramanian and S. Ramaswamy. </author> <title> The p-range tree: A new data structure for range searching in secondary memory. </title> <booktitle> In Proc. ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 378-387, </pages> <year> 1995. </year>
Reference-contexts: For completeness it should be mentioned that recently a number of researchers have considered the design of worst-case efficient external-memory "on-line" data structures, mainly for (special cases of) two and three dimensional range searching <ref> [20, 25, 59, 61, 78, 84, 93] </ref>. While B-trees [22, 37, 65] efficiently support range searching in one dimension they are inefficient in higher dimensions. Similarly the many sophisticated internal-memory data structures for range searching are not efficient when mapped to external memory.
Reference: [85] <author> R. E. Tarjan. </author> <title> Amortized computational complexity. </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 6(2) </volume> <pages> 306-318, </pages> <year> 1985. </year>
Reference-contexts: Thus we do not as in the B-tree case have a worst-case I/O bound on performing an update. Instead we say that each operation can be performed in O ( log m n amortized <ref> [85] </ref>. In order to use the structure in a sorting algorithm we also need an operation that reports all the elements in the structure in sorted order. To do so we first empty all buffers in the structure using the buffer-emptying process from the root of the tree and down.
Reference: [86] <author> V. K. Vaishnavi and D. Wood. </author> <title> Rectilinear line segment intersection, layered segment trees, </title> <journal> and dynamization. Journal of Algorithms, </journal> <volume> 3 </volume> <pages> 160-176, </pages> <year> 1982. </year>
Reference-contexts: We also needs to be able to look for the dominating segment in many of the multislabs lists. However, one can overcome these problems using batched filtering [53] and a technique similar to what in internal memory is called fractional cascading <ref> [30, 31, 86] </ref>. The idea in batched filtering is to process all the queries at the same time and level by level, such that the dominating segments in nodes on one level of the structure are found for all the queries, before continuing to consider nodes on the next level.
Reference: [87] <author> M. van Kreveld. </author> <title> Geographic information systems. </title> <institution> Utrecht University, INF/DOC-95-01, </institution> <year> 1995. </year>
Reference-contexts: As geographic information systems (GIS) frequently store, manipulate, and search through enormous amounts of spatial data <ref> [42, 56, 67, 80, 87] </ref> they are good examples of such large-scale applications. The amount of data manipulated in such systems is often too large to fit in main memory and must reside on disk, hence the I/O communication can become a very severe bottleneck. <p> One of most fundamental operations in many GIS systems is map overlaying|the computation of new scenes or maps from a number of existing maps. Some existing software packages are completely based on this operation <ref> [10, 11, 75, 87] </ref>. Given two thematic maps the problem is to compute a new map in which the thematic attributes of each location is a function of the thematic attributes of the corresponding locations in the two input maps. <p> In this problem one is given a set of non-intersecting red segments and a set of non-intersecting blue segments and should compute all intersection red-blue segment pairs. In general many important problems from computational geometry are abstractions of important GIS operations <ref> [43, 87] </ref>.
Reference: [88] <editor> J. van Leeuwen. </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity. </booktitle> <publisher> Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Traditionally when designing computer programs people have focused on the minimization of the internal computation time and ignored the time spent on Input/Output (I/O). Theoretically one of the most commonly used machine models when designing algorithms is the Random Access Machine (RAM) (see e.g. <ref> [7, 88] </ref>).
Reference: [89] <author> D. E. Vengroff. </author> <title> A transparent parallel I/O environment. </title> <booktitle> In Proc. 1994 DAGS Symposium on Parallel Computation, </booktitle> <year> 1994. </year>
Reference-contexts: This is supported by experimental results which show that implementing algorithms designed for the model can lead to significant runtime improvements in practice <ref> [32, 33, 89, 92] </ref>. We will discuss some of these experiments in later sections. <p> Many problems in computational geometry are abstractions of important GIS operations, and in Section 4 we survey techniques and algorithms in external-memory computational geometry. We also discuss some experimental results. Finally, we in Section 5 shortly describe a Transparent Parallel I/O Environment (TPIE) designed by Vengroff <ref> [89] </ref> to allow programmers to write I/O-efficient programs. We assume that the reader has some basic knowledge about e.g. fundamental sorting algorithms and data structures like balanced search trees (especially B-trees) and priority queues. We also assume that the reader is familiar with asymptotic notation (O (), (),fi ()). <p> Finally, it should be mentioned that Vengroff and Vitter [92] and Arge et al. [18] have also performed some experiments with I/O algorithms. We will return to these experiments in Section 5 when we discuss the TPIE environment developed by Vengroff <ref> [89] </ref>. 19 4.2 The Batched Range Searching Problem In this section we consider another computational geometry problem with applications in GIS which is normally solved using plane-sweep; the batched range searching problem. <p> On the other hand we have seen how a large number of problems can be solved using a relatively small number of paradigms, such as merging, distribution (and distribution sweeping), and buffered external data structures. The Transparent Parallel I/O Environment (TPIE) proposed by Vengroff <ref> [89, 91, 94] </ref> tries to take advantage of this. While Chiang [32, 33] performed experiments in order to compare the efficiency of algorithms designed for internal and external memory and to validate the I/O-model, TPIE is designed to assist programmers in the development of I/O-efficient (and easily portable) programs. <p> Finally, the AMI provides the high-level interface to the programmer and is the only component with which most programmers will need to interact directly. As mentioned the access methods supported by the AMI currently include scanning, distribution, merging, sorting, permuting, and matrix arithmetic. The interested reader is refereed to <ref> [89, 91, 94] </ref> for details. In [91] implementations of algorithms such as convex hull and list ranking are also discussed. Finally, it is discussed how to obtain the prototype version of TPIE.
Reference: [90] <author> D. E. Vengroff. </author> <title> Private communication, </title> <year> 1995. </year>
Reference-contexts: Even though the algorithm is more complicated than the simple RAM algorithm, Vengroff <ref> [90] </ref> has performed simulations showing that on large problem instances it has a much better performance than the simple internal-memory algorithm.
Reference: [91] <author> D. E. Vengroff. </author> <title> TPIE User Manual and Reference. </title> <institution> Duke University, </institution> <year> 1995. </year> <note> Available via WWW at http://www.cs.duke.edu/TPIE. </note>
Reference-contexts: On the other hand we have seen how a large number of problems can be solved using a relatively small number of paradigms, such as merging, distribution (and distribution sweeping), and buffered external data structures. The Transparent Parallel I/O Environment (TPIE) proposed by Vengroff <ref> [89, 91, 94] </ref> tries to take advantage of this. While Chiang [32, 33] performed experiments in order to compare the efficiency of algorithms designed for internal and external memory and to validate the I/O-model, TPIE is designed to assist programmers in the development of I/O-efficient (and easily portable) programs. <p> The paradigms supported by the current prototype of TPIE includes scanning, distribution, merging, sorting, permuting, and matrix arithmetic <ref> [91, 94] </ref>. In order to allow programmers to abstract away I/O, TPIE uses a stream approach. A computation is viewed as a continuous process in which a program is fed streams of data from an outside source and leave trails (in form of other streams of data) behind it. <p> Finally, the AMI provides the high-level interface to the programmer and is the only component with which most programmers will need to interact directly. As mentioned the access methods supported by the AMI currently include scanning, distribution, merging, sorting, permuting, and matrix arithmetic. The interested reader is refereed to <ref> [89, 91, 94] </ref> for details. In [91] implementations of algorithms such as convex hull and list ranking are also discussed. Finally, it is discussed how to obtain the prototype version of TPIE. <p> As mentioned the access methods supported by the AMI currently include scanning, distribution, merging, sorting, permuting, and matrix arithmetic. The interested reader is refereed to [89, 91, 94] for details. In <ref> [91] </ref> implementations of algorithms such as convex hull and list ranking are also discussed. Finally, it is discussed how to obtain the prototype version of TPIE. Currently, TPIE does not support external buffered data structures but we hope in the future to include such structures in the environment.
Reference: [92] <author> D. E. Vengroff and J. S. Vitter. </author> <title> Supporting I/O-efficient scientific computation in TPIE. </title> <booktitle> In Proc. IEEE Symp. on Parallel and Distributed Computing, </booktitle> <year> 1995. </year> <note> Appears also as Duke University Dept. of Computer Science technical report CS-1995-18. </note>
Reference-contexts: This is supported by experimental results which show that implementing algorithms designed for the model can lead to significant runtime improvements in practice <ref> [32, 33, 89, 92] </ref>. We will discuss some of these experiments in later sections. <p> Even though disk striping does not in theory achieve asymptotic optimality when D is very large, it is often the method of choice in practice for using parallel disks <ref> [92] </ref>. The non-optimality of disk striping can be demonstrated via the sorting bound. <p> On the other hand the buffer tree algorithm does not need to sort the input twice as the distribution sweeping algorithm does. We plan to perform experiments with the buffer tree solution in the future. Finally, it should be mentioned that Vengroff and Vitter <ref> [92] </ref> and Arge et al. [18] have also performed some experiments with I/O algorithms. <p> Similarly, it is the plan to extend TPIE with support for more application controlled I/O in order to allow implementation of the in GIS commonly used indexing structures. In <ref> [92] </ref> Vengroff and Vitter discuss applications of TPIE to problems in scientific computing, and report some performance results of programs written to solve certain benchmark problems. The TPIE paradigms used in these experiments are scanning, sorting, and matrix arithmetic.
Reference: [93] <author> D. E. Vengroff and J. S. Vitter. </author> <title> Efficient 3-d range searching in external memory. </title> <booktitle> In Proc. of the 28th Annual ACM Symposium on Theory of Computing (STOC '96), </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: For completeness it should be mentioned that recently a number of researchers have considered the design of worst-case efficient external-memory "on-line" data structures, mainly for (special cases of) two and three dimensional range searching <ref> [20, 25, 59, 61, 78, 84, 93] </ref>. While B-trees [22, 37, 65] efficiently support range searching in one dimension they are inefficient in higher dimensions. Similarly the many sophisticated internal-memory data structures for range searching are not efficient when mapped to external memory.
Reference: [94] <author> D. E. Vengroff and J. S. Vitter. </author> <title> I/O-efficient scientific computation using TPIE. </title> <booktitle> In Proceedings of the Goddard Conference on Mass Storage Systems and Technologies, NASA Conference Publication 3340, </booktitle> <volume> Volume II, </volume> <pages> pages 553-570, </pages> <address> College Park, MD, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: On the other hand we have seen how a large number of problems can be solved using a relatively small number of paradigms, such as merging, distribution (and distribution sweeping), and buffered external data structures. The Transparent Parallel I/O Environment (TPIE) proposed by Vengroff <ref> [89, 91, 94] </ref> tries to take advantage of this. While Chiang [32, 33] performed experiments in order to compare the efficiency of algorithms designed for internal and external memory and to validate the I/O-model, TPIE is designed to assist programmers in the development of I/O-efficient (and easily portable) programs. <p> The paradigms supported by the current prototype of TPIE includes scanning, distribution, merging, sorting, permuting, and matrix arithmetic <ref> [91, 94] </ref>. In order to allow programmers to abstract away I/O, TPIE uses a stream approach. A computation is viewed as a continuous process in which a program is fed streams of data from an outside source and leave trails (in form of other streams of data) behind it. <p> Finally, the AMI provides the high-level interface to the programmer and is the only component with which most programmers will need to interact directly. As mentioned the access methods supported by the AMI currently include scanning, distribution, merging, sorting, permuting, and matrix arithmetic. The interested reader is refereed to <ref> [89, 91, 94] </ref> for details. In [91] implementations of algorithms such as convex hull and list ranking are also discussed. Finally, it is discussed how to obtain the prototype version of TPIE.
Reference: [95] <author> J. S. Vitter. </author> <title> Efficient memory access in large-scale computation (invited paper). </title> <booktitle> In Symposium on Theoretical Aspects of Computer Science, </booktitle> <volume> LNCS 480, </volume> <pages> pages 26-41, </pages> <year> 1991. </year>
Reference-contexts: We will discuss some of these experiments in later sections. Finally, it should be mentioned that several authors have considered extended theoretical models that try to model the hierarchical nature of the memory of real machines <ref> [2, 3, 4, 5, 8, 60, 81, 95, 98, 96] </ref>, but such models quickly become theoretically very complicated due to the large number of parameters.
Reference: [96] <author> J. S. Vitter and M. H. Nodine. </author> <title> Large-scale sorting in uniform memory hierarchies. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 17 </volume> <pages> 107-114, </pages> <year> 1993. </year>
Reference-contexts: We will discuss some of these experiments in later sections. Finally, it should be mentioned that several authors have considered extended theoretical models that try to model the hierarchical nature of the memory of real machines <ref> [2, 3, 4, 5, 8, 60, 81, 95, 98, 96] </ref>, but such models quickly become theoretically very complicated due to the large number of parameters.
Reference: [97] <author> J. S. Vitter and E. A. M. Shriver. </author> <title> Algorithms for parallel memory, I: Two-level memories. </title> <journal> Algorithmica, </journal> <volume> 12(2-3):110-147, </volume> <year> 1994. </year>
Reference-contexts: Large-scale problem instances can be in the range N = 10 10 to N = 10 12 . An increasingly popular approach to further increase the throughput of the I/O system is to use a number of disks in parallel <ref> [50, 51, 97] </ref>. Several authors have considered an extension of the above model with a parameter D denoting the number of disks in the system [21, 73, 71, 72, 97]. <p> An increasingly popular approach to further increase the throughput of the I/O system is to use a number of disks in parallel [50, 51, 97]. Several authors have considered an extension of the above model with a parameter D denoting the number of disks in the system <ref> [21, 73, 71, 72, 97] </ref>. In the parallel disk model [97] one can read or write one block from each of the D disks simultaneously in one I/O. The number of disks D range up to 10 2 in current disk arrays. <p> Several authors have considered an extension of the above model with a parameter D denoting the number of disks in the system [21, 73, 71, 72, 97]. In the parallel disk model <ref> [97] </ref> one can read or write one block from each of the D disks simultaneously in one I/O. The number of disks D range up to 10 2 in current disk arrays. <p> The general I/O model was introduced by Aggarwal and Vitter [6] and the notion of parallel disks was introduced by Vitter and Shriver <ref> [97] </ref>. The latter papers also deal with fundamental problems such as permutation, sorting and matrix transposition, and a number of authors have considered the difficult problem of sorting optimally on parallel disks [5, 21, 73, 71]. <p> During one distribution pass we should "just" make sure to distribute the blocks belonging to the same bucket evenly among the D disks, such that we can read them efficiently in the next pass. Vitter and Shriver <ref> [97] </ref> used randomization to ensure this and developed an algorithm which performs optimally with high probability. Later Nodine and Vitter [71] managed to develop a deterministic version of D disk distribution sort. An alternative distribution-like algorithm is develop by Aggarwal and Plaxton [5].
Reference: [98] <author> J. S. Vitter and E. A. M. Shriver. </author> <title> Algorithms for parallel memory, II: Hierarchical multilevel memories. </title> <journal> Algorithmica, </journal> <volume> 12(2-3):148-169, </volume> <year> 1994. </year>
Reference-contexts: We will discuss some of these experiments in later sections. Finally, it should be mentioned that several authors have considered extended theoretical models that try to model the hierarchical nature of the memory of real machines <ref> [2, 3, 4, 5, 8, 60, 81, 95, 98, 96] </ref>, but such models quickly become theoretically very complicated due to the large number of parameters.
Reference: [99] <author> B. Zhu. </author> <title> Further computational geometry in secondary memory. </title> <booktitle> In Proc. Int. Symp. on Algorithms and Computation, </booktitle> <pages> pages 514-522, </pages> <year> 1994. </year> <month> 35 </month>
Reference-contexts: The problem of implementing various classes of permutations has been addressed in [38, 39, 41]. More recently researchers have moved on to more specialized problems in the computational geometry <ref> [12, 14, 19, 32, 53, 99] </ref>, graph theoretical [13, 14, 32, 34, 52, 66] and string processing areas [15, 35, 45, 46]. As already mentioned the number of I/O operations needed to read the entire input is N=B and for convenience we call this quotient n. <p> In [53] two techniques called batched construction of persistent B-trees and batched filtering are also discussed. In [18] some results from [53, 12] are extended and generalized, and some external-memory computational geometry results are also reported in <ref> [49, 99] </ref>. In [19] efficient I/O algorithms for a large number of problems involving line segments in the plane are designed by combining the ideas of distribution sweeping, batched filtering, buffer trees and a new technique, which can be regarded as an external-memory version of fractional cascading [31].
References-found: 99

