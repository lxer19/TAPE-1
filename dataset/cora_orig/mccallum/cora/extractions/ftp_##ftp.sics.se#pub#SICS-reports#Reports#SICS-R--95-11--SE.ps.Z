URL: ftp://ftp.sics.se/pub/SICS-reports/Reports/SICS-R--95-11--SE.ps.Z
Refering-URL: http://www.sics.se/libindex.html
Root-URL: 
Email: Email: janke@sics.se  
Title: Some Comments on the Information Stored in Sparse Distributed Memory  
Author: Jan Kristoferson 
Keyword: Sparse distributed memory, SDM.  
Note: Contents  
Date: December 1995  
Address: Box 1263, S-164 28 Kista, Sweden  
Affiliation: RWCP 1 Neuro SICS 2 Laboratory  
Abstract: Report R95:11 ISRN : SICS-R--95/11-SE ISSN : 0283-3638 Abstract We consider a sparse distributed memory with randomly chosen hard locations, in which an unknown number T of random data vectors have been stored. A method is given to estimate T from the content of the memory with high accuracy. In fact, our estimate is unbiased, the coefficient of variation being roughly inversely proportional to p MU , where M is the number of hard locations in the memory and U the length of data, so the accuracy can be made arbitrarily high by making the memory big enough. A consequence of this is that the good reading methods in [5] and [6] can be used without any need for the special extra location introduced there. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. A. Jaeckel. </author> <title> An Alternative Design for a Sparse Distributed Memory. </title> <type> RI-ACS Technical Report 89.28, </type> <institution> Research Institute for Advanced Computer Science, NASA Ames Research Center, </institution> <year> 1989. </year>
Reference-contexts: 1 Introduction We consider a Sparse Distributed Memory, either of Kanerva's original design, see Kanerva [2], or of Jaeckel's selected-coordinates design, see Jaeckel <ref> [1] </ref>. Let us give a short explanation of the concept: An address is a binary string (vector) of 1s and 0s. A datum is a binary string (vector) of 1s and 1s. Let N be the length (dimension) of the addresses, U the length (dimension) of the data.
Reference: [2] <author> P. Kanerva. </author> <title> Sparse distributed memory and related models. </title> <editor> In Mohamad H. Hassoun, editor, </editor> <title> Associative Neural Memories. </title> <publisher> Oxford University Press, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction We consider a Sparse Distributed Memory, either of Kanerva's original design, see Kanerva <ref> [2] </ref>, or of Jaeckel's selected-coordinates design, see Jaeckel [1]. Let us give a short explanation of the concept: An address is a binary string (vector) of 1s and 0s. A datum is a binary string (vector) of 1s and 1s.
Reference: [3] <author> R. Karlsson. </author> <title> Evaluation of the Fast Activation Mechanism for the Kanerva SDM Memory. </title> <institution> SICS Research Report R95:10, Swedish Institute of Computer Science, </institution> <year> 1995. </year>
Reference-contexts: s we get the following final results (approximating M M1 by 1) (S) = T p v u t 1 T We remark that the last square root in equation (10) is near p 2 for "normal" values of the parameters (cf. e.g., Kristoferson [4]). 4 Karlsson's design In Karlsson <ref> [3] </ref> a modification of Jaeckel's design is introduced, having great implementation advantages for simulations on a sequential computer. Karlsson's design differs from Jaeckel's thus: for each used mask all the 2 K possible bit combinations are used for hard locations.
Reference: [4] <author> J. Kristoferson. </author> <title> Best Probability of Activation and Performance Comparisons for Several Designs of Sparse Distributed Memory. </title> <institution> SICS Research Report R95:09, Swedish Institute of Computer Science, </institution> <year> 1995. </year>
Reference-contexts: the independence of the R u s we get the following final results (approximating M M1 by 1) (S) = T p v u t 1 T We remark that the last square root in equation (10) is near p 2 for "normal" values of the parameters (cf. e.g., Kristoferson <ref> [4] </ref>). 4 Karlsson's design In Karlsson [3] a modification of Jaeckel's design is introduced, having great implementation advantages for simulations on a sequential computer. Karlsson's design differs from Jaeckel's thus: for each used mask all the 2 K possible bit combinations are used for hard locations.
Reference: [5] <author> G. Sjodin. </author> <title> Convergence and New Operations in SDM. </title> <note> SICS Research Report to appear, </note> <institution> Swedish Institute of Computer Science, </institution> <year> 1995. </year>
Reference-contexts: The data are also assumed to be randomly chosen, but the databits don't need to be uniformly distributed over f1; 1g; instead we have for each position a separate (unknown) probability for the databits in that position being 1. In Sjodin <ref> [5] </ref> and [6] good reading methods are given. There an extra hard location is used to save the number T of stored data. Information on the value of T exists 3 The probability that a randomly chosen hard location is activated by a randomly chosen (write or read) address.
Reference: [6] <author> G. Sjodin. </author> <title> Improving the Capacity of SDM. </title> <note> SICS Research Report to appear, </note> <institution> Swedish Institute of Computer Science, </institution> <year> 1995. </year> <month> 5 </month>
Reference-contexts: The data are also assumed to be randomly chosen, but the databits don't need to be uniformly distributed over f1; 1g; instead we have for each position a separate (unknown) probability for the databits in that position being 1. In Sjodin [5] and <ref> [6] </ref> good reading methods are given. There an extra hard location is used to save the number T of stored data. Information on the value of T exists 3 The probability that a randomly chosen hard location is activated by a randomly chosen (write or read) address.
References-found: 6

