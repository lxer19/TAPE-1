URL: http://www.cs.columbia.edu/~sal/hpapers/kdd97-scaleup.ps
Refering-URL: http://www.cs.columbia.edu:80/~sal/recent-papers.html
Root-URL: 
Email: pkc@cs.fit.edu  sal@cs.columbia.edu  
Title: Scalability of Learning Arbiter and Combiner Trees from Partitioned Data  
Author: Philip K. Chan Salvatore J. Stolfo 
Keyword: scalability, arbiter and combiner trees, meta-learning, parallel/distributed processing, inductive learning  
Note: This work was partially funded by grants from NSF (IRI-96-32225 CDA-96-25374), ARPA (F30602 96-1-0311), NYSSTF (423115-445), and Citicorp.  
Date: March 7, 1997  
Address: Melbourne, FL 32901  New York, NY 10027  
Affiliation: Computer Science Florida Institute of Technology  Department of Computer Science Columbia University  
Abstract: Much of the research in inductive learning concentrates on problems with relatively small amounts of data residing at one location. In this paper we explore the scalability of learning arbiter and combiner trees from partitioned data. Arbiter and combiner trees integrate classifiers trained in parallel from small disjoint subsets. Previous work demonstrated their efficacy in terms of accuracy, this paper discusses their performance in terms of speedup and scalability. The performance of serial learning algorithms is evaluated. The performance of the algorithms used to construct combiner and arbiter trees in parallel is then analyzed. Our empirical results indicate that the techniques can effectively scale up to large datasets with millions of records. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: We then analyze in Section 3 the speedup and scalability that can be achieved by utilizing hierarchical meta-learning in a parallel and distributed environment. Section 4 summarizes our findings and describes future directions. 2 Serial Evaluation of Learning Algorithms To evaluate the ID3 [12], CART <ref> [1] </ref>, BAYES [6], and CN2 [5] learning algorithms in a serial environment, we empirically investigate their speed with varying amounts of training data. Their theoretical time complexity is formulated in [3].
Reference: [2] <author> J. Catlett. </author> <title> Megainduction: A test flight. </title> <booktitle> In Proc. Eighth Intl. Work. Machine Learning, </booktitle> <pages> pages 596-599, </pages> <year> 1991. </year>
Reference-contexts: Crossovers among ID3, CART, and BAYES occur between 100,000 and 1 million examples. With 5 million examples, CART was faster than ID3 and BAYES was the slowest. ID3 completed processing 5 million records in about 2,800 seconds (47 minutes), which is much less than Catlett's <ref> [2] </ref> projection of several months for ID3 to process 1 million records. The huge gap merits some explanation. First, the projection was made five years ago, state-of-the-art processor speed has much improved since then.
Reference: [3] <author> P. Chan. </author> <title> An Extensible Meta-Learning Approach for Scalable and Accurate Inductive Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, NY, </address> <year> 1996. </year>
Reference-contexts: Their theoretical time complexity is formulated in <ref> [3] </ref>. We performed a set of experiments using an artificial data set generated by a Boolean expression [3] with the training set size up to 10 million when all algorithms exceeded the main memory. <p> Their theoretical time complexity is formulated in <ref> [3] </ref>. We performed a set of experiments using an artificial data set generated by a Boolean expression [3] with the training set size up to 10 million when all algorithms exceeded the main memory. We measure the elapsed training time of ID3, CART, BAYES, and CN2 with different numbers of examples in the artificial domain. The experiments were performed on HP 9000/735 workstations. <p> To simplify the previous discussion, we did not take into consideration the classification time to generate the predictions, communication time to send the predictions to one site, and construction time to generate the meta-level training sets. A more detailed analysis, which includes the additional time consumption, is in <ref> [3] </ref>. 3.3 Scalability analysis Now we measure the scalability of our approach using scaled speedup (Equation 1). From Equation 3, the problem size W is n 2 =p.
Reference: [4] <author> P. Chan and S. Stolfo. </author> <title> Learning arbiter and combiner trees from partitioned data for scaling machine learning. </title> <booktitle> In Proc. Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 39-44, </pages> <year> 1995. </year>
Reference-contexts: With the advance of computer and networking technology, large amounts of data are generated each day. Hierarchical meta-learning techniques were developed to integrate classifiers learned from partitioned data. Previous work <ref> [4] </ref> concentrates on the accuracy performance of hierarchical meta-learning. In this paper we focus on the training time performance of learning arbiter and combiner trees from partitioned data. We first evaluate the training time performance of the individual learning algorithms we use in a serial environment in Section 2. <p> Third, they exhibit superlinear behavior with large amounts of data, which is particularly undesirable in applications like data mining. As we proposed in <ref> [4] </ref>, data reduction and meta-learning techniques are used to alleviate the problem of limited memory resource and prolonged execution when large amounts of data are present. <p> We next evaluate the speed of our proposed techniques in a parallel and distributed processing environment. 3 Parallel Evaluation of Hierarchical Meta-learning The hierarchical meta-learning strategy described in <ref> [4] </ref> is designed to be utilized in a parallel and distributed processing environment, such as JAM [14]. <p> Recall that the training set size for an arbiter is restricted to be no larger than the training set size for a leaf classifier <ref> [4] </ref>. Hence, in a parallel environment, the amount of computation at each level is approximately the same. <p> Each plotted point is the average of five runs on different data sets produced by our artificial data generator using different random seeds. We ran experiments using different combinations of learning algorithms (ID3, CART, BAYES, and CN2) and hierarchical meta-learning schemes (arbiter tree, class-combiner tree, and class-attribute-combiner tree) <ref> [4] </ref>. axes are in log scale. Each plot in the figure shows the results from a learning algorithm used in the three different hierarchical meta-learning schemes.
Reference: [5] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-285, </pages> <year> 1989. </year>
Reference-contexts: Section 4 summarizes our findings and describes future directions. 2 Serial Evaluation of Learning Algorithms To evaluate the ID3 [12], CART [1], BAYES [6], and CN2 <ref> [5] </ref> learning algorithms in a serial environment, we empirically investigate their speed with varying amounts of training data. Their theoretical time complexity is formulated in [3].
Reference: [6] <author> R. Duda and P. Hart. </author> <title> Pattern classification and scene analysis. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1973. </year>
Reference-contexts: We then analyze in Section 3 the speedup and scalability that can be achieved by utilizing hierarchical meta-learning in a parallel and distributed environment. Section 4 summarizes our findings and describes future directions. 2 Serial Evaluation of Learning Algorithms To evaluate the ID3 [12], CART [1], BAYES <ref> [6] </ref>, and CN2 [5] learning algorithms in a serial environment, we empirically investigate their speed with varying amounts of training data. Their theoretical time complexity is formulated in [3].
Reference: [7] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM 3 user's guide and reference manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <year> 1993. </year>
Reference-contexts: The next section describes our experiments and results on the meta-learning strategies. 5 3.4 Parallel implementation The hierarchical meta-learning strategies were implemented on a parallel and distributed platform based on the message-passing model. To satisfy our goal of portability, we chose PVM (Parallel Virtual Machine) <ref> [7] </ref> to provide message passing support. The computing platform we used consists of eight HP 9000/735 workstations on a dedicated FDDI (Fiber Distribution Data Interface) network. At the leaf level, the 8 processors generate 8 base classifiers, which are then used to produce predictions on the validation set.
Reference: [8] <author> C. Grammes. GNUFIT v1.2, </author> <year> 1993. </year> <note> (Available at ftp://ftp.dartmouth.edu/pub/gnuplot/gnufit12.tar.gz). </note>
Reference-contexts: We fitted linear, quadratic, and cubic equations to the training time of each algorithm and the plots are displayed in Figure 2. The curve approximations were computed using GNUFIT <ref> [8] </ref> with the Marquardt-Levenberg algorithm [13], a nonlinear least squares fit 2 mechanism. To approximate the training speed with polynomial equations, we inspect how closely the three polynomials fit the data points.
Reference: [9] <author> J. Gustafson. </author> <title> Reevaluating Amdahl's law. </title> <journal> Comm. ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <year> 1988. </year>
Reference-contexts: That is, Speedup = T S T P : For this metric, T S is usually the time consumption for the fastest serial algorithm, which could be the parallel algorithm running serially. * Scaled speedup <ref> [9, 11] </ref> provides a metric for scalability. It measures the speedup of a parallel system when the problem size increases linearly with the number of processors. Scaled speedup = T S (W fi p) ; (1) where T S and T P are expressed as functions of problem size.
Reference: [10] <author> V. Kumar, A. Grama, A. Gupta, and G. Karypis. </author> <title> Introduction to parallel computing: Design and analysis of algorithms. </title> <address> Benjamin-Cummings, Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: a parallel and distributed environment, the notation and definitions we use are as follows: * T S = serial execution time. 3 * T P = parallel execution time. * p = number of processors. * n = input size (number of training examples). * W = problem size (work) <ref> [10] </ref>, which measures the total number of computational units needed for serial execution. That is, T S = W fi t u , where t u is the time spent for a unit of computation. Hence, T S / W .
Reference: [11] <author> V. Kumar and A. Gupta. </author> <title> Analyzing scalability of parallel algorithms and architectures. </title> <journal> J. Parallel & Distributed Computing, </journal> <volume> 22 </volume> <pages> 379-391, </pages> <year> 1994. </year>
Reference-contexts: That is, Speedup = T S T P : For this metric, T S is usually the time consumption for the fastest serial algorithm, which could be the parallel algorithm running serially. * Scaled speedup <ref> [9, 11] </ref> provides a metric for scalability. It measures the speedup of a parallel system when the problem size increases linearly with the number of processors. Scaled speedup = T S (W fi p) ; (1) where T S and T P are expressed as functions of problem size. <p> Scaled speedup = T S (W fi p) ; (1) where T S and T P are expressed as functions of problem size. Parallel systems with linear or near-linear scaled speedup (with respect to p, the number of processors) are considered scalable. Other scalability metrics can be found in <ref> [11] </ref>. * Speed is characterized by the algorithm's time complexity. 3.2 Speedup analysis For pedagogical reasons, we are going to focus our analysis on arbiter trees; similar results can be obtained for combiner trees.
Reference: [12] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: We then analyze in Section 3 the speedup and scalability that can be achieved by utilizing hierarchical meta-learning in a parallel and distributed environment. Section 4 summarizes our findings and describes future directions. 2 Serial Evaluation of Learning Algorithms To evaluate the ID3 <ref> [12] </ref>, CART [1], BAYES [6], and CN2 [5] learning algorithms in a serial environment, we empirically investigate their speed with varying amounts of training data. Their theoretical time complexity is formulated in [3].
Reference: [13] <author> A. Ralston and P. Rabinowitz. </author> <title> A first course in numerical analysis. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1978. </year>
Reference-contexts: We fitted linear, quadratic, and cubic equations to the training time of each algorithm and the plots are displayed in Figure 2. The curve approximations were computed using GNUFIT [8] with the Marquardt-Levenberg algorithm <ref> [13] </ref>, a nonlinear least squares fit 2 mechanism. To approximate the training speed with polynomial equations, we inspect how closely the three polynomials fit the data points.
Reference: [14] <author> S. Stolfo, A. Prodormidis, S. Tselepis, W. Lee, W. Fan, and P. Chan. </author> <title> JAM: Java agents for meta-learning over distributed databases. </title> <note> Submitted to 3rd Intl. Conf. Know. Disc. Data Mining, </note> <year> 1997. </year>
Reference-contexts: We next evaluate the speed of our proposed techniques in a parallel and distributed processing environment. 3 Parallel Evaluation of Hierarchical Meta-learning The hierarchical meta-learning strategy described in [4] is designed to be utilized in a parallel and distributed processing environment, such as JAM <ref> [14] </ref>.
Reference: [15] <author> X. Sun and L. Ni. </author> <title> Scalable problems and memory-bounded speedup. </title> <journal> J. Parallel & Distributed Comp., </journal> <volume> 19 </volume> <pages> 27-37, </pages> <year> 1993. </year> <month> 10 </month>
Reference-contexts: Again, we note that the fastest serial case should be used for speedup analysis; the second analysis is presented for completeness. An alternate scalability metric is memory-bound scaled speedup <ref> [15] </ref>, which measures the increase in possible problem size with increasing number of processors, each with limited available memory.
References-found: 15

