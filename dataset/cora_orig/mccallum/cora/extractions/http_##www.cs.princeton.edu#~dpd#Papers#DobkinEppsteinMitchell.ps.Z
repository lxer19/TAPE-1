URL: http://www.cs.princeton.edu/~dpd/Papers/DobkinEppsteinMitchell.ps.Z
Refering-URL: http://www.cs.princeton.edu/~dpd/Research.html
Root-URL: http://www.cs.princeton.edu
Title: Computing the Discrepancy with Applications to Supersampling Patterns  
Author: David P. Dobkin David Eppstein Don P. Mitchell 
Abstract: Patterns used for supersampling in graphics have been analyzed from statistical and signal-processing viewpoints. We present an analysis based on a type of isotropic discrepancy|how good patterns are at estimating the area in a region of defined type. We present algorithms for computing discrepancy relative to regions that are defined by rectangles, halfplanes, and higher-dimensional figures. Experimental evidence shows that popular supersampling patterns have discrepancies with better asymptotic behavior than random sampling, which is not inconsistent with theoretical bounds on discrepancy.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alexander, R. </author> <title> Geometric methods in the study of irregularities of distribution, </title> <journal> Com-binatorica, </journal> <volume> 10 (1990), </volume> <pages> 115-136. </pages>
Reference-contexts: Crucial to this process are efficient algorithms for actually computing the discrepancy in various models. This topic is the focus of our work here. Our domain in all that follows will be the unit cube <ref> [0; 1] </ref> k . For many graphics applications, k = 2; the unit square then corresponds to a single pixel of image space. However, this need not be the case. <p> However, this need not be the case. Distributed ray tracing as introduced by Cook [6] involves sampling in a space including the pixel as well as time, lens area, light areas, and other affects. The goal will be to measure the quality of a point set X <ref> [0; 1] </ref> k . Quality will be measured as the maximum discrepancy of the set with respect to families of regions S R k of a particular type. We let (S) the Euclidean measure of S " [0; 1] k , and X (S) the discrete measure jS " X j=jX <p> The goal will be to measure the quality of a point set X <ref> [0; 1] </ref> k . Quality will be measured as the maximum discrepancy of the set with respect to families of regions S R k of a particular type. We let (S) the Euclidean measure of S " [0; 1] k , and X (S) the discrete measure jS " X j=jX j. <p> We note that this measure of discrepancy (in absolute value) will be realized by an interval (half-open or closed) having a point of X as its boundary: any other segment can be extended or shrunk, increasing or decreasing the length of its intersection with <ref> [0; 1] </ref> but not changing its intersection with X . Thus the problem can be reformulated as follows. <p> Thus the total time per update becomes O (log 2 n). 2 We have proved the following result. Theorem 1. We can insert or delete points from a set X <ref> [0; 1] </ref>, and recompute the discrepancy D (X) after each update, in time O (log 2 n) per update, and space O (n). Proof: We maintain the data structure described above, in time O (log 2 n) per update. <p> For example, in one dimension, there is the significant result: Theorem 2 (Koksma 1942). If f is a function of bounded variation V (f ) on the unit interval <ref> [0; 1] </ref> and X = fx 1 ; : : : x n g are points in [0; 1] with L 1 -discrepancy D (X), then fi fi fi n i=1 Z 1 f (t)dt fi fi V (f )D (X) Koksma's theorem was extended to higher dimensions [20], but the <p> For example, in one dimension, there is the significant result: Theorem 2 (Koksma 1942). If f is a function of bounded variation V (f ) on the unit interval <ref> [0; 1] </ref> and X = fx 1 ; : : : x n g are points in [0; 1] with L 1 -discrepancy D (X), then fi fi fi n i=1 Z 1 f (t)dt fi fi V (f )D (X) Koksma's theorem was extended to higher dimensions [20], but the definition of bounded variation is problematic. <p> Bounds on the worst-case (the best pattern you can get, given the worst-case line) are (n 3=4 ) and O (n 3=4 (log n) 1=2 ). These bounds also apply to our case of points in a unit square <ref> [3, 1, 5] </ref>. We now discuss algorithms for computing the arbitrary-edge discrepancy for point sets in [0; 1] 2 . <p> These bounds also apply to our case of points in a unit square [3, 1, 5]. We now discuss algorithms for computing the arbitrary-edge discrepancy for point sets in <ref> [0; 1] </ref> 2 . More generally we consider the halfspace discrepancy for point sets in [0; 1] k defined to be the maximum absolute value difference between the relative number of points intersected by a halfspace and the fraction of the hypercube [0; 1] k covered by the halfspace. <p> These bounds also apply to our case of points in a unit square [3, 1, 5]. We now discuss algorithms for computing the arbitrary-edge discrepancy for point sets in <ref> [0; 1] </ref> 2 . More generally we consider the halfspace discrepancy for point sets in [0; 1] k defined to be the maximum absolute value difference between the relative number of points intersected by a halfspace and the fraction of the hypercube [0; 1] k covered by the halfspace. <p> computing the arbitrary-edge discrepancy for point sets in <ref> [0; 1] </ref> 2 . More generally we consider the halfspace discrepancy for point sets in [0; 1] k defined to be the maximum absolute value difference between the relative number of points intersected by a halfspace and the fraction of the hypercube [0; 1] k covered by the halfspace. As we note below, this higher dimensional problem has applications even in the plane, to problems of computing discrepancy with respect to circles and ellipses. <p> Nevertheless, we can prove the following lemmas. Lemma 2. Let X 0 X <ref> [0; 1] </ref> k be the points on the boundary of the halfspace h with maximum discrepancy. Then (h) will be a local maximum or minimum in the space of all halfspaces having X 0 on their boundaries. <p> Then (h) will be a local maximum or minimum in the space of all halfspaces having X 0 on their boundaries. Proof: If not, we could find a different halfspace covering the same set of points but giving larger or smaller volume in <ref> [0; 1] </ref> k , and therefore having larger discrepancy. 2 Since the measure (h) can be expressed as an algebraic formula in h, it has a constant number of local minima. Thus we can compute the halfspace discrepancy D H (X) as follows. <p> But this also gives us a data structure for the dynamic global halfspace discrepancy problem: Theorem 6. For any dimension d, we can insert or delete points from a set X <ref> [0; 1] </ref> d , and recompute the discrepancy D H (X) after each update, in time O (n d1 log 2 n) per update, and space O (n d log n). Proof: We simply keep a separate data structure for each possible subset Y .
Reference: [2] <author> J. Beck and W.W.L. Chen. </author> <title> Irregularities of Distribution, </title> <publisher> Cambridge University Press, </publisher> <year> 1987. </year>
Reference-contexts: This is achieved with samples having a high-frequency spectrum ("blue-noise"). A third viewpoint which can be applied to the problem of sample-pattern analysis is the theory of discrepancy or irregularities of distribution <ref> [2] </ref>. This viewpoint was introduced to computer graphics by Shirley [26]. Niederreiter has also pointed out the possible importance of discrepancy in computer graphics [21]. This subject grew out of the study of certain low-discrepancy sampling patterns which have been used in quasi-Monte Carlo integration [14, 20]. <p> Nevertheless, for a given function obeying the bounded-variation conditions, the error of numerical integration is O (D N ). Roth has proven that, in k dimensions, the best sampling patterns have discrepancy tightly bounded by T (X) = fi (n 1 (log n) (k1)=2 ) <ref> [2] </ref>. Sampling patterns such as the Hammersley points have been constructed with discrepancies of D (X) = O (n 1 (log n) k1 ). Let r (i) be the radical-inverse function of i base r. <p> Consider the problem of estimating the area of disks or of boxes at arbitrary orientation. Surprisingly, the discrepancy is much larger. Work by Schmidt and others have shown that one can do no better than the lower bound of (n 1=(2k)1=2 ) in k dimensions <ref> [2] </ref>. Upper bounds on the best discrepancy are known, and are typically larger than the lower bounds by a polylogarithmic factor (for various generalized discrepancy problems). A common problem in computer graphics is a pixel in the neighborhood of an edge. <p> Theoretical analysis has been done on the similar problem of arbitrary edges through samples within a unit-area disk <ref> [2] </ref>. Bounds on the worst-case (the best pattern you can get, given the worst-case line) are (n 3=4 ) and O (n 3=4 (log n) 1=2 ). These bounds also apply to our case of points in a unit square [3, 1, 5].
Reference: [3] <author> J. Beck. </author> <type> Personal communication. </type>
Reference-contexts: Bounds on the worst-case (the best pattern you can get, given the worst-case line) are (n 3=4 ) and O (n 3=4 (log n) 1=2 ). These bounds also apply to our case of points in a unit square <ref> [3, 1, 5] </ref>. We now discuss algorithms for computing the arbitrary-edge discrepancy for point sets in [0; 1] 2 .
Reference: [4] <author> B. Chazelle. </author> <title> Geometric discrepancy revisited. </title> <booktitle> Proc. 34th IEEE Symp. </booktitle> <institution> Found. Comp. Sci. </institution> <year> (1993) </year> <month> 392-399. </month>
Reference-contexts: In particular Chazelle <ref> [4] </ref> has described fast approximation algorithms for arbitrary-edge discrepancy, Dobkin et al. [11] have described algorithms for computing discrepancy with respect to arbitrary rectangles (not having the origin as a corner), and de Berg has described algorithms for strip discrepancy [8].
Reference: [5] <author> B. Chazelle, J. Matousek and M. Sharir. </author> <title> An elementary approach to lower bounds in geometric discrepancy. </title> <note> Discrete and Computational Geometry, to appear. </note>
Reference-contexts: Bounds on the worst-case (the best pattern you can get, given the worst-case line) are (n 3=4 ) and O (n 3=4 (log n) 1=2 ). These bounds also apply to our case of points in a unit square <ref> [3, 1, 5] </ref>. We now discuss algorithms for computing the arbitrary-edge discrepancy for point sets in [0; 1] 2 .
Reference: [6] <author> R. L. Cook, Thomas Porter and Loren Carpenter, </author> <title> Distributed ray tracing. </title> <booktitle> Computer Graphics 18:3 (1984) 137-145. </booktitle>
Reference-contexts: Our domain in all that follows will be the unit cube [0; 1] k . For many graphics applications, k = 2; the unit square then corresponds to a single pixel of image space. However, this need not be the case. Distributed ray tracing as introduced by Cook <ref> [6] </ref> involves sampling in a space including the pixel as well as time, lens area, light areas, and other affects. The goal will be to measure the quality of a point set X [0; 1] k .
Reference: [7] <author> R. L. Cook. </author> <title> Stochastic sampling in computer graphics. </title> <journal> ACM Trans. </journal> <note> Graphics 5:1 (1986) 51-72. </note>
Reference-contexts: The Central Limit Theorem implies that pixel error will decrease with the number of samples as O (n 1=2 ), but this viewpoint does not describe overall image-noise characteristics. Some researchers have taken a signal-processing viewpoint of the image-sampling problem <ref> [7, 9, 18] </ref> and of the distribution ray-tracing problem [19].
Reference: [8] <author> M. de Berg. </author> <title> Computing half-plane and strip discrepancy of planar point sets. </title> <note> To appear. </note>
Reference-contexts: In particular Chazelle [4] has described fast approximation algorithms for arbitrary-edge discrepancy, Dobkin et al. [11] have described algorithms for computing discrepancy with respect to arbitrary rectangles (not having the origin as a corner), and de Berg has described algorithms for strip discrepancy <ref> [8] </ref>. Dobkin et al. also note applications of discrepancy computation to other areas of computer science including computational learning theory. Acknowledgements Work of D. P.
Reference: [9] <author> M. A. Z. Dippe and E. H. </author> <title> Wold. Antialiasing through stochastic sampling. </title> <booktitle> Computer Graphics 19:3 (1985) 69-78. </booktitle>
Reference-contexts: The Central Limit Theorem implies that pixel error will decrease with the number of samples as O (n 1=2 ), but this viewpoint does not describe overall image-noise characteristics. Some researchers have taken a signal-processing viewpoint of the image-sampling problem <ref> [7, 9, 18] </ref> and of the distribution ray-tracing problem [19].
Reference: [10] <author> D. P. Dobkin and D. Eppstein. </author> <title> Computing the discrepancy. </title> <booktitle> Proc. 9th ACM Symp. Computational Geometry (1993) 47-52. </booktitle>
Reference-contexts: Is there a family of sets which is interesting in practice and for which discrepancy can be computed in time which does not grow exponentially with the dimension of the problem? Since we first reported on our discrepancy algorithms <ref> [10] </ref>, some further research has extended our results and partially answered some of these questions. <p> Work of David Eppstein was supported in part by NSF grant CCR-9258355. Portions of this paper have previously appeared at the 9th ACM Symp. Computational Geometry <ref> [10] </ref> and at Graphics Interface '93 [12].
Reference: [11] <author> D. P. Dobkin, D. Gunopulos, and W. </author> <title> Mass. Computing the rectangle discrepancy. </title> <type> Manuscript, </type> <year> 1994. </year>
Reference-contexts: In particular Chazelle [4] has described fast approximation algorithms for arbitrary-edge discrepancy, Dobkin et al. <ref> [11] </ref> have described algorithms for computing discrepancy with respect to arbitrary rectangles (not having the origin as a corner), and de Berg has described algorithms for strip discrepancy [8]. Dobkin et al. also note applications of discrepancy computation to other areas of computer science including computational learning theory.
Reference: [12] <author> D. P. Dobkin and D. P. Mitchell. </author> <title> Random-edge discrepancy of supersampling patterns. Graphics Interface, </title> <address> York, Ontario (1993). </address>
Reference-contexts: Work of David Eppstein was supported in part by NSF grant CCR-9258355. Portions of this paper have previously appeared at the 9th ACM Symp. Computational Geometry [10] and at Graphics Interface '93 <ref> [12] </ref>.
Reference: [13] <author> H. Edelsbrunner and L. Guibas. </author> <title> Topologically sweeping an arrangement. </title> <institution> J. Comput. Sys. Sci. </institution> <month> 38 </month> <year> (1989) </year> <month> 165-194. </month>
Reference-contexts: We wish to know, for each pair of points (a; b) in this projected plane, the cardinality of the projection of X intersected with the halfplane below line ab. This can be solved in constant amortized time per pair using the topological sweeping algorithm of Edelsbrunner and Guibas <ref> [13] </ref>. We have proved the following: Theorem 5. For any dimension k &gt; 1, we can compute the halfspace discrepancy D H (X) in time O (n k ) and space O (n). This algorithm is practical. <p> We would then determine the O (n 2 ) vertices of the arrangement, compute the discrepancy at each and determine the maximum discrepancy. We can examine all vertices of the arrangement in O (n 2 ) time and O (n) space by the algorithm of <ref> [13] </ref>. In practice, this approach is prone to difficulties if data are not in general position. An alternative is to consider how vertices in the arrangement (in dual space) arise.
Reference: [14] <author> J. H. Halton. </author> <title> A retrospective and prospective survey of the Monte Carlo method. </title> <note> SIAM Review 12 (1970) 1-63. </note>
Reference-contexts: This viewpoint was introduced to computer graphics by Shirley [26]. Niederreiter has also pointed out the possible importance of discrepancy in computer graphics [21]. This subject grew out of the study of certain low-discrepancy sampling patterns which have been used in quasi-Monte Carlo integration <ref> [14, 20] </ref>. What is interesting about discrepancy is that it provides a fairly direct measurement of how good a sampling pattern is at estimating certain simple integrals.
Reference: [15] <author> J. Hershberger and S. Suri. </author> <title> O*ine maintenance of planar configurations. </title> <booktitle> Proc. 2nd ACM/SIAM Symp. Discrete Algorithms (1991) 32-41. </booktitle>
Reference-contexts: Thus we can compute the two-dimensional discrepancy in O (n) data structure operations, for a total time of O (n log 2 n). It may be possible that a more specialized convex hull data structure such as that of Hershberger and Suri <ref> [15] </ref> can reduce this to O (n log n). 5 For orthants in any higher dimension, we perform a hyperplane sweep, and each time we cross a point we compute the optimal orthant having the sweep plane as part of its boundary, using the discrepancy algorithm for the next lower dimension.
Reference: [16] <author> J. T. Kajiya. </author> <title> The Rendering Equation. </title> <booktitle> Computer Graphics 20 (1986) 143-150. </booktitle>
Reference-contexts: The quality of sampling patterns has been analyzed from several viewpoints. Estimating the integral of a pixel area by averaging samples can also be viewed as a statistical sampling problem, and variance-reducing techniques of experimental design (e.g., stratification) can be applied <ref> [16, 17, 24, 25] </ref>. The Central Limit Theorem implies that pixel error will decrease with the number of samples as O (n 1=2 ), but this viewpoint does not describe overall image-noise characteristics.
Reference: [17] <author> M. Lee, R. A. Redner, and S. P. Uselton. </author> <title> Statistically optimized sampling for distributed ray tracing. </title> <booktitle> Computer Graphics 19:3 (1985) 61-67. </booktitle>
Reference-contexts: The quality of sampling patterns has been analyzed from several viewpoints. Estimating the integral of a pixel area by averaging samples can also be viewed as a statistical sampling problem, and variance-reducing techniques of experimental design (e.g., stratification) can be applied <ref> [16, 17, 24, 25] </ref>. The Central Limit Theorem implies that pixel error will decrease with the number of samples as O (n 1=2 ), but this viewpoint does not describe overall image-noise characteristics.
Reference: [18] <author> D. P. Mitchell. </author> <title> Generating antialiased images at low sampling densities. </title> <booktitle> Computer Graphics 21:4 (1987) 65-72. </booktitle>
Reference-contexts: The Central Limit Theorem implies that pixel error will decrease with the number of samples as O (n 1=2 ), but this viewpoint does not describe overall image-noise characteristics. Some researchers have taken a signal-processing viewpoint of the image-sampling problem <ref> [7, 9, 18] </ref> and of the distribution ray-tracing problem [19].
Reference: [19] <author> D. P. Mitchell. </author> <title> Spectrally optimal sampling for distribution ray tracing. </title> <booktitle> Computer Graphics 25:4 (1991) 157-164. </booktitle>
Reference-contexts: The Central Limit Theorem implies that pixel error will decrease with the number of samples as O (n 1=2 ), but this viewpoint does not describe overall image-noise characteristics. Some researchers have taken a signal-processing viewpoint of the image-sampling problem [7, 9, 18] and of the distribution ray-tracing problem <ref> [19] </ref>. <p> These are very preliminary experiments. An on-line algorithm builds up patterns one point at a time by trying to find the best next point. Given a pattern of n points, this method generates mn random points and then selects the one which causes the smallest increase in discrepancy (see <ref> [19] </ref> for discussion of a similar on-line blue-noise algorithm). An off-line algorithm starts with n random points and attempts to replace points at random if the replacement will reduce the discrepancy. The fast discrepancy computation algorithms discussed earlier are critical for the efficient performance of such methods.
Reference: [20] <author> H. Neiderreiter. </author> <title> Quasi-Monte Carlo methods and pseudo-random numbers. </title> <journal> Bull. Amer. Math. Soc. </journal> <volume> 84 (1978) 957-1041. </volume> <pages> 19 </pages>
Reference-contexts: This viewpoint was introduced to computer graphics by Shirley [26]. Niederreiter has also pointed out the possible importance of discrepancy in computer graphics [21]. This subject grew out of the study of certain low-discrepancy sampling patterns which have been used in quasi-Monte Carlo integration <ref> [14, 20] </ref>. What is interesting about discrepancy is that it provides a fairly direct measurement of how good a sampling pattern is at estimating certain simple integrals. <p> unit interval [0; 1] and X = fx 1 ; : : : x n g are points in [0; 1] with L 1 -discrepancy D (X), then fi fi fi n i=1 Z 1 f (t)dt fi fi V (f )D (X) Koksma's theorem was extended to higher dimensions <ref> [20] </ref>, but the definition of bounded variation is problematic. Nevertheless, for a given function obeying the bounded-variation conditions, the error of numerical integration is O (D N ).
Reference: [21] <author> H. Neiderreiter. </author> <title> Quasirandom sampling computer graphics. </title> <booktitle> Proc. 3rd Internat. Sem. Digital Image Processing in Medicine, </booktitle> <address> Riga, </address> <month> Latvia </month> <year> (1992) </year> <month> 29-33. </month>
Reference-contexts: A third viewpoint which can be applied to the problem of sample-pattern analysis is the theory of discrepancy or irregularities of distribution [2]. This viewpoint was introduced to computer graphics by Shirley [26]. Niederreiter has also pointed out the possible importance of discrepancy in computer graphics <ref> [21] </ref>. This subject grew out of the study of certain low-discrepancy sampling patterns which have been used in quasi-Monte Carlo integration [14, 20]. What is interesting about discrepancy is that it provides a fairly direct measurement of how good a sampling pattern is at estimating certain simple integrals.
Reference: [22] <author> M. Overmars and J. van Leeuwen. </author> <title> Maintenance of configurations in the plane. </title> <journal> J. Com-put. Sys. Sci. </journal> <month> 23 </month> <year> (1981) </year> <month> 166-204. </month>
Reference-contexts: We use a modification of the convex hull algorithm of Overmars and van Leeuwen <ref> [22] </ref>. We maintain a balanced binary tree representing the sorted order of points. The points themselves are stored at the leaves of the tree; the internal nodes represent subsequences of several points. <p> For each vertex, the counters can be updated in constant time by adding the counters for its two children. The bridges (and positions of the bridge endpoints) can be found using binary searches, in O (log n) time each, as in <ref> [22] </ref>. Thus the total time per update becomes O (log 2 n). 2 We have proved the following result. Theorem 1.
Reference: [23] <author> M. Overmars and C.K. Yap. </author> <title> New upper bounds in Klee's measure problem. </title> <journal> SIAM J. Comput. </journal> <month> 20 </month> <year> (1991) </year> <month> 1034-1045. </month>
Reference-contexts: In higher dimensions, we can further improve this bound using an alternative approach which we now outline. Our algorithm for this case is based on an technique of Overmars and Yap for Klee's measure problem <ref> [23] </ref>, Instead of directly searching for the maximum discrepancy orthant, we find for each value of i from 1 to n the orthants with minimum and maximum area that contain exactly i of the n points. Once these are found, the discrepancy can then be computed in linear time. <p> The problem thus becomes one of finding, in this dual arrangement of orthants, the point contained in exactly i orthants and minimizing or maximizing the product of its coordinates. We now apply a technique of Overmars and Yap <ref> [23] </ref>, to subdivide space into O (n k=2 ) boxes, the positions of which depend on the input orthant arrangement. We say that an orthant of our dual problem crosses a box of the partition if some portion of the orthant boundary is interior to the box. <p> Theorem 4. For any dimension k, we can compute D (X) in time O (n k=2+1 ) and space O (n). Proof: Overmars and Yap <ref> [23] </ref> show how to enumerate the boxes of the partition in the given time and space bounds, without having to keep the entire partition in memory at once.
Reference: [24] <author> J. Painter and K. Sloan. </author> <title> Antialiased ray tracing by adaptive progressive refinement. </title> <booktitle> Computer Graphics 23:3 (1989) 281-288. </booktitle>
Reference-contexts: The quality of sampling patterns has been analyzed from several viewpoints. Estimating the integral of a pixel area by averaging samples can also be viewed as a statistical sampling problem, and variance-reducing techniques of experimental design (e.g., stratification) can be applied <ref> [16, 17, 24, 25] </ref>. The Central Limit Theorem implies that pixel error will decrease with the number of samples as O (n 1=2 ), but this viewpoint does not describe overall image-noise characteristics.
Reference: [25] <author> W. Purgathofer. </author> <title> A statistical model for adaptive stochastic sampling. </title> <booktitle> Proc. </booktitle> <month> Eurograph-ics </month> <year> (1986) </year> <month> 145-152. </month>
Reference-contexts: The quality of sampling patterns has been analyzed from several viewpoints. Estimating the integral of a pixel area by averaging samples can also be viewed as a statistical sampling problem, and variance-reducing techniques of experimental design (e.g., stratification) can be applied <ref> [16, 17, 24, 25] </ref>. The Central Limit Theorem implies that pixel error will decrease with the number of samples as O (n 1=2 ), but this viewpoint does not describe overall image-noise characteristics.
Reference: [26] <author> P. Shirley. </author> <title> Discrepancy as a quality measure for sample distributions. </title> <booktitle> Proc. </booktitle> <month> Eurograph-ics </month> <year> (1991) </year> <month> 183-193. </month>
Reference-contexts: This is achieved with samples having a high-frequency spectrum ("blue-noise"). A third viewpoint which can be applied to the problem of sample-pattern analysis is the theory of discrepancy or irregularities of distribution [2]. This viewpoint was introduced to computer graphics by Shirley <ref> [26] </ref>. Niederreiter has also pointed out the possible importance of discrepancy in computer graphics [21]. This subject grew out of the study of certain low-discrepancy sampling patterns which have been used in quasi-Monte Carlo integration [14, 20].
Reference: [27] <author> Tony T. Warnock. </author> <title> Computational investigations of low-discrepancy point sets. In Applications of Number Theory to Numerical Analysis, S.K. Zaremba, </title> <editor> ed., </editor> <publisher> Academic Press (1971) 319-344. </publisher>
Reference-contexts: prime numbers 2; 3; 5; : : :, one of n Hammersley points is given by: x i = (i=n; 2 (i); 3 (i); 5 (i); : : :): An improvement suggested by Zaremba and generalized to higher dimensions by Warnock is based on the folded radical inverse r (i) <ref> [27] </ref>. Here, the jth most significant digit a j is replaced by (a j + j) mod r before the reflection about the decimal point. In the same paper, Warnock presents an O (n 2 ) algorithm for computing T (X) and experimental results for several proposed low-discrepancy patterns.
Reference: [28] <author> T. Whitted. </author> <title> An improved illumination model for shaded display. </title> <journal> Commun. Assoc. Comput. Mach. </journal> <volume> 23 (1980) 343-349. </volume> <pages> 20 </pages>
Reference-contexts: Uniform sampling can lead to visually conspicuous aliasing artifacts like Moire patterns. This is worst-case behavior in the context of adaptive sampling, where a pure high-frequency signal aliased to a pure low-frequency pattern will fool schemes for deciding where to apply extra samples <ref> [28] </ref>. Randomizing the sampling pattern leads to random-noise aliasing and is more likely to avoid the worst-case scenario for adaptive sampling. The quality of sampling patterns has been analyzed from several viewpoints.
References-found: 28

