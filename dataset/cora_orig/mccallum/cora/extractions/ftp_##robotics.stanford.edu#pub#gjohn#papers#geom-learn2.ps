URL: ftp://robotics.stanford.edu/pub/gjohn/papers/geom-learn2.ps
Refering-URL: http://www.robotics.stanford.edu/~gjohn/pubs.html
Root-URL: http://www.robotics.stanford.edu
Email: gjohn@cs.Stanford.EDU  
Title: Geometry-Based Learning Algorithms  
Author: George H. John 
Date: March 1993  
Note: UNPUBLISHED DRAFT NOT FOR CITATION COMMENTS WELCOME  
Address: Stanford, CA 94305  
Affiliation: Computer Science Department Stanford University  
Abstract: We present CHILS , the Convex Hull Inductive Learning System, a novel supervised learning algorithm based on approximating concepts with sets of convex hulls. We introduce a theoretical methodology for describing the power of a concept representation language and use it to compare convex hulls with other geometrical concept representations. The Domain Transform framework (DT) provides a clear way to compare the power of supervised learning systems, allowing us to characterize a class of domains which is learnable by some systems but cannot be learned by other systems. DT can be used similarly to compare the expected generalization performance of different domains.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: When joining points together to form a sphere, pick the smallest enclosing sphere. To break a sphere, pick the partitioning of points that results in the smallest total volume of the 2 new spheres. 2.4 Nearest-Neighbor Learning: Learning Voronoi Partitions The VORONOI learning system is simply a nearest-neighbor classifier <ref> [1] </ref>. The decision surface is defined by the Voronoi diagram of the points. Figure 4 shows an example of a set of points and the corresponding Voronoi decision surface. 2.5 Learning with Hyper-Planes The PLANE learning system comes from Nilsson [6, Sec. 4.3]. <p> Domain transformation is a different idea entirely, and can be thought of as moving all instances in instance space without changing their class label. The notion of domain transformation independence under linear transformations has been thoroughly studied by Duda and Hart <ref> [1] </ref> and Fukunaga [2]. Anyone who deals with real data should be concerned about invariance to domain transformations. For example, measuring the length of an object in inches instead of centimeters changes the scale of the domain along one axis.
Reference: [2] <author> K. Fukunaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Electrical Science. Associated Press, </publisher> <address> London, </address> <year> 1972. </year>
Reference-contexts: Domain transformation is a different idea entirely, and can be thought of as moving all instances in instance space without changing their class label. The notion of domain transformation independence under linear transformations has been thoroughly studied by Duda and Hart [1] and Fukunaga <ref> [2] </ref>. Anyone who deals with real data should be concerned about invariance to domain transformations. For example, measuring the length of an object in inches instead of centimeters changes the scale of the domain along one axis. This effects an unscaled nearest-neighbor algorithm as well as the naive SPHERE algorithm.
Reference: [3] <author> K. Gross. </author> <title> Concept Acquisition Through Attribute Evolution and Experiment Selection. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1990. </year>
Reference-contexts: Our approach is similar to the CAT-AE algorithm developed by Gross <ref> [3] </ref>, except that our algorithm directly learns hyperplanes not parallel to any axis. <p> The right half shows that convex hulls can also have a kind of "nearest-neighbor" bias when the data warrants - in this case all points categorized as black have a black point as their nearest neighbor. 2.2 Learning with Hyper-Rectangles The RECT learning system is a simplification of CAT <ref> [3] </ref>.
Reference: [4] <author> M. J. Hudak. </author> <title> Rce networks: An experimental investigation. </title> <booktitle> In IJCNN-91, </booktitle> <year> 1991. </year>
Reference-contexts: See <ref> [4] </ref> for a brief overview and bibliography. Our SPHERE learning algorithm is again much like CHILS , except that concepts are represented as spheres. When joining points together to form a sphere, pick the smallest enclosing sphere.
Reference: [5] <author> J. McClelland and D. Rumelhart. </author> <title> Parallel Distributed Processing. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Domains that can be separated by hyperplanes are called linearly separable. Many different training algorithms have been proposed, see for example Widrow's review of the ADALINE [12]. 4 2.6 Learning with Neural Nets and Backpropagation Rummelhart and McClelland <ref> [5] </ref> introduce an algorithm for training a multi-layer neural network.
Reference: [6] <author> N. J. Nilsson. </author> <title> Learning Machines. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The decision surface is defined by the Voronoi diagram of the points. Figure 4 shows an example of a set of points and the corresponding Voronoi decision surface. 2.5 Learning with Hyper-Planes The PLANE learning system comes from Nilsson <ref> [6, Sec. 4.3] </ref>. The system attempts to find a hyperplane separating the instances in one class from instances in the other. It is a parametric learning method: the concept description is a half-space, and the decision surface is characterized by one hyperplane.
Reference: [7] <author> F. Preparata and M. Shamos. </author> <title> Computational Geometry: an Introduction. </title> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: Table 1: Training and classification algorithms for CHILS . Table 1 shows the training and classification algorithms for CHILS , which are quite simple. Much work has been done in the field of computational geometry to find efficient algorithms for finding convex hulls in high-dimensional spaces. Preparata in <ref> [7] </ref> discusses efficient ways to dynamically repair 2D hulls. Our incremental algorithm for learning hulls is shown in Table 1. The algorithms for extending and breaking hulls are described below.
Reference: [8] <author> R. L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2:229 - 246, </volume> <year> 1987. </year>
Reference-contexts: A variation of CHILS , Counterfactual-CHILS (CCHILS ) allows hulls from different classes to overlap, provided that some ordering exists on the hulls in the overlapping region. (This can be thought of as an extension of Rivest's Decision Lists <ref> [8] </ref> to continuous domains.) 1 Various approximations to this are quite fast. For example, one can simply sample many hyperplanes randomly and pick the one with minimal surface area. 3 as white by the hypersphere or nearest-neighbor algorithms discussed later, but is classified as black by the convex hull.
Reference: [9] <author> S. Salzberg. </author> <title> Nested hyper-rectangles for exemplar-based learning. </title> <editor> In K. P. Jantke, editor, </editor> <booktitle> Analogical and Inductive Inference, </booktitle> <pages> pages 184-201, </pages> <address> Berlin, Germany, </address> <month> October </month> <year> 1989. </year> <note> Springer-Verlag. </note>
Reference-contexts: We choose the split that maximizes the lost volume. (This should make intuitive sense if there were really two clusters of points at opposite ends of the rectangle, this procedure will break them into two separate rectangles.) Salzberg <ref> [10, 9] </ref> has also investigated hyper rectangle-based classifiers and found them to perform as well or better than other learning algorithms on three datasets. 2.3 Learning with Hyper-Spheres The SPHERE learning system borrows ideas from what are today called Restricted Coulomb Energy network classifiers, but were introduced as Hyperspherical Classifiers in
Reference: [10] <author> S. Salzberg. </author> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6(3) </volume> <pages> 251-276, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: We choose the split that maximizes the lost volume. (This should make intuitive sense if there were really two clusters of points at opposite ends of the rectangle, this procedure will break them into two separate rectangles.) Salzberg <ref> [10, 9] </ref> has also investigated hyper rectangle-based classifiers and found them to perform as well or better than other learning algorithms on three datasets. 2.3 Learning with Hyper-Spheres The SPHERE learning system borrows ideas from what are today called Restricted Coulomb Energy network classifiers, but were introduced as Hyperspherical Classifiers in
Reference: [11] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: PAC theory <ref> [11] </ref> allows us to state bounds on the worst-case number of examples needed to identify a concept given a particular representation language, but it gives us no other way to characterize a representation.
Reference: [12] <author> B. Widrow and M. A. Lehr. </author> <title> 30 years of adaptive neural networks: Perceptron, madaline, and backpropagation. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 79(8) </volume> <pages> 1415-1442, </pages> <month> September </month> <year> 1990. </year> <month> 8 </month>
Reference-contexts: The position and rotation of the plane are parameters that the learning mechanism adjusts in an attempt to separate the domain. Domains that can be separated by hyperplanes are called linearly separable. Many different training algorithms have been proposed, see for example Widrow's review of the ADALINE <ref> [12] </ref>. 4 2.6 Learning with Neural Nets and Backpropagation Rummelhart and McClelland [5] introduce an algorithm for training a multi-layer neural network.
References-found: 12

