URL: http://polaris.cs.uiuc.edu/reports/1389.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Email: tu,padua@csrd.uiuc.edu  
Title: Efficient Building and Placing of Gating Functions  
Author: Peng Tu and David Padua 
Note: (217) 333-6884  
Address: 1308 W. Main Street, Urbana, Illinois 61801-2307  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract-found: 0
Intro-found: 1
Reference: [ASU86] <author> A. V. Aho, R. Sethi, and J.D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: The dominator tree can be computed in O (Eff (E; N )) time using the dominator algorithm of Lengauer and Tarjan [LT79], or in O (E) time using a more complicated algorithm of Harel [Har85]. The dfn can be computed in linear time <ref> [ASU86] </ref>. The dfn number has the property of dfn (idom (v)) &lt; df n (v) for each node v 6= Entry. Each loop step of the algorithm processes a set of sibling nodes with a common parent u. The outer loop processes nodes in reverse dfn sequence.
Reference: [AWZ88] <author> B. Alpern, M. N. Wegman, and F. K. Zadeck. </author> <title> Detecting Equality of Variables in Programs. </title> <booktitle> In Proc. of the 15th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1-11, </pages> <year> 1988. </year>
Reference-contexts: GSA was introduced by Ballance, Maccabe and Ottenstein as a part of Program Dependence Web (PDW)[BMO90]. It is a convenient representation for several program analysis and optimization techniques, including constant propagation with conditional branches [WZ91]; equality of symbolic expressions <ref> [AWZ88, Hav93] </ref>; induction variable substitution [Wol92]; symbolic dependence analysis [BE94] and demand-driven symbolic analysis for array privatization [TP94, TP93]. <p> SSA form has been shown to be useful in capturing the data flow information required by some important program optimizations <ref> [AWZ88, RWZ88] </ref>. In the SSA form, each definition of a variable is given a unique new name, and each use of a variable is renamed to refer to a single reaching definition.
Reference: [BE94] <author> William Blume and Rudolf Eigenmann. </author> <title> The Range Test: A Dependence Test for Symbolic, Non-linear Expressions. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: GSA was introduced by Ballance, Maccabe and Ottenstein as a part of Program Dependence Web (PDW)[BMO90]. It is a convenient representation for several program analysis and optimization techniques, including constant propagation with conditional branches [WZ91]; equality of symbolic expressions [AWZ88, Hav93]; induction variable substitution [Wol92]; symbolic dependence analysis <ref> [BE94] </ref> and demand-driven symbolic analysis for array privatization [TP94, TP93]. In the SSA representation, functions of a single type are placed at the confluence nodes of a program flow graph to represent different definitions of a variable reaching from different incoming edges.
Reference: [BEF + 94] <author> Bill Blume, Rudolf Eigenmann, Keith Faigin, John Grout, Jay Hoeflinger, David Padua, Paul Petersen, Bill Pottenger, Lawrence Rauchwerger, Peng Tu, and Stephen Weather-ford. </author> <title> Polaris: The Next Generation in Parallelizing Compilers. </title> <booktitle> In Proc. 7th Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year> <month> 12 </month>
Reference-contexts: (P; A 0 ; fl (Q; A 9 ; )); fl (R; A 4 ; )); and for node 6, fl (P; fl (R; ; A 3 ); fl (Q; ; A 14 )). 6 Implementation and Measurement We implemented the algorithm using path compression in the POLARIS restructuring compiler <ref> [BEF + 94] </ref>. The simple algorithm uses only path compression and has a complexity of O (E log N ). The following is the timing result for all the programs in the Perfect Benchmark [CKPK90]. The time given is the execution time on a SUN-10 workstation.
Reference: [BMO90] <author> R. Ballance, A. Maccabe, and K. Ottenstein. </author> <title> The Program Dependence Web: a Repre--sentation Supporting Control Data- and Demand-Driven Interpretation of Imperative Languages. </title> <booktitle> In Proceedings of the SIGPLAN'90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Some extra parameters are introduced in the gating functions to represent the conditions. In this paper, we present an almost linear time algorithm to construct the GSA. The new algorithm is more efficient and simpler than the existing algorithms for GSA construction <ref> [BMO90, Hav93] </ref>. Since SSA is a special case of GSA, it can also be used as an efficient alternative algorithm for SSA construction. The existing algorithms for building the GSA follow two steps. The first step is the same function placement procedure as in the SSA construction [CFR + 91]. <p> The first step is the same function placement procedure as in the SSA construction [CFR + 91]. In the second step, the GSA conversion algorithms collect the control dependences of the definitions reaching a function and transforms the function into a gating function. The original GSA conversion algorithm <ref> [BMO90] </ref> assumed a Program Dependence Graph (PDG)[FOW87] as its initial representation. Havlak developed another algorithm [Hav93] to construct a variant of the GSA, known as Thinned GSA. Because it starts with the program flow graph, it is, therefore, somewhat simpler. <p> Whereas a function represents the merge of multiple reaching definitions, it does not contain the condition that specifies which reaching definition will be the value of the function. Gating functions were introduced by Ballance, Maccabe and Ottenstein <ref> [BMO90] </ref> to capture the control conditions that guard the paths to a function. There are three types of gating function: * The fl function, which is an if thenelse construct, captures the condition for each definition to reach a confluence node. <p> For instance, X 2 = (X 0 ; X 3 ) represents that X 2 's initial value is X 0 and its subsequent value is X 3 . * The function determines the value of a variable at the exit of the end of the loop. In <ref> [BMO90] </ref>, the conversion to GSA is done after placement. The algorithm works by expanding each node into a GSA gating tree that contains the control information for the different reaching definitions. The translation for each node may potentially scan all the edges in the flow graph.
Reference: [CFR + 91] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> Oc-tober </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The Gated Single-Assignment (GSA) program representation is an extension of the Static Single Assignment (SSA) representation <ref> [CFR + 91] </ref>. GSA was introduced by Ballance, Maccabe and Ottenstein as a part of Program Dependence Web (PDW)[BMO90]. <p> Since SSA is a special case of GSA, it can also be used as an efficient alternative algorithm for SSA construction. The existing algorithms for building the GSA follow two steps. The first step is the same function placement procedure as in the SSA construction <ref> [CFR + 91] </ref>. In the second step, the GSA conversion algorithms collect the control dependences of the definitions reaching a function and transforms the function into a gating function. The original GSA conversion algorithm [BMO90] assumed a Program Dependence Graph (PDG)[FOW87] as its initial representation. <p> SSA captures the data flow information (def-use chains) of a program in a compact form. An efficient algorithm for constructing SSA with a minimal number of functions was originally designed by Cytron, Ferrante, Rosen, Wegman and Zadeck <ref> [CFR + 91] </ref>. The algorithm for placing the functions is O (N 2 ) in the worst case, but often appears to be linear when applied to real programs. Johnson and Pingali [JP93, JPP94] proposed another algorithm to place functions in O (E) time. <p> The postdominator tree is defined similarly using the post-dominating relation. In the rest of the paper, the words predecessor, successor, and path refer to the flow graph, and the words parent, child, ancestor, and descendant refer to the dominator tree. The dominance frontier <ref> [CFR + 91] </ref> DF (X) of a CF G node X is the set of nodes Y 2 CF G such that X dominates a predecessor P of Y but does not strictly dominate Y : DF (X) = fY j (9P ! Y )(XP and X 6 Y )g: Given <p> A fundamental result proven in <ref> [CFR + 91] </ref> states that if ' is the set of assignment nodes for a variable V , then DF + (') is the minimum set of nodes that need function assignment nodes for V . 3 Gating Paths and -Function Nodes In this section, we present another way to determine <p> We prove that the phinodes computed are the same as those using the iterated dominance frontier algorithm in <ref> [CFR + 91] </ref>. This provides a way to look at the problem from a different perspective. Definition 1. <p> (if; endif ) = fl (B; ; fl) fl = fl (B; ; fl); P (if; endif ) = p t (if; endif ) [ p f (if; endif) = fl (B; fl; ) [ fl (B; ; fl) = fl (B; fl; fl): Applying Cytron et al.'s renaming procedure <ref> [CFR + 91] </ref> to insert variable names into a gating function R (u; v) = fl (B; R t ; R f ), we need to know from which predecessors of v that each of R t and R f reach v. <p> For further details of this procedure, interested readers should refer to <ref> [CFR + 91] </ref>. Consider, for example, that Block1 contains an assignment to a variable A, which after renaming becomes A B1 and that Block2 has no assignment to A. Let the definition of A reaching the if statement be A Orig .
Reference: [CKPK90] <author> George Cybenko, Lyle Kipp, Lynn Pointer, and David Kuck. </author> <title> Supercomputer Performance Evaluation and the Perfect Benchmarks. </title> <booktitle> In Proceedings of ICS, </booktitle> <address> Amsterdam, Netherlands, </address> <pages> pages 162-174, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: The simple algorithm uses only path compression and has a complexity of O (E log N ). The following is the timing result for all the programs in the Perfect Benchmark <ref> [CKPK90] </ref>. The time given is the execution time on a SUN-10 workstation.
Reference: [Far77] <author> R. Farrow. </author> <title> Efficient on-line evaluation of functions defined on paths in trees. </title> <type> Technical report, </type> <institution> Rice University, Dept. Math. Sci., </institution> <year> 1977. </year>
Reference-contexts: This work is not necessarily representative of the positions or policies of the U. S. Army or the government. 1 method, called stratified path compression by Farrow <ref> [Far77, Tar79] </ref>, can also be used. This GSA algorithm is also almost as efficient as the best known algorithms for function placement in SSA conversion. The rest of the paper is divided into the following sections. In Section 2, we introduce some background and notations.
Reference: [FOW87] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The Program Dependency Graph and its Uses in Optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> June </month> <year> 1987. </year>
Reference: [Har85] <author> P. Harel. </author> <title> A linear time algorithm for finding dominators in flowgraphs and related problems. </title> <booktitle> In Proc. of the 17th ACM Symposium on Theory of Computing, </booktitle> <month> May </month> <year> 1985. </year>
Reference-contexts: The dominator tree can be computed in O (Eff (E; N )) time using the dominator algorithm of Lengauer and Tarjan [LT79], or in O (E) time using a more complicated algorithm of Harel <ref> [Har85] </ref>. The dfn can be computed in linear time [ASU86]. The dfn number has the property of dfn (idom (v)) &lt; df n (v) for each node v 6= Entry. Each loop step of the algorithm processes a set of sibling nodes with a common parent u.
Reference: [Hav93] <author> Paul Havlak. </author> <title> Construction of thinned gated single-assignment form. </title> <booktitle> In Proc. 6rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: GSA was introduced by Ballance, Maccabe and Ottenstein as a part of Program Dependence Web (PDW)[BMO90]. It is a convenient representation for several program analysis and optimization techniques, including constant propagation with conditional branches [WZ91]; equality of symbolic expressions <ref> [AWZ88, Hav93] </ref>; induction variable substitution [Wol92]; symbolic dependence analysis [BE94] and demand-driven symbolic analysis for array privatization [TP94, TP93]. <p> Some extra parameters are introduced in the gating functions to represent the conditions. In this paper, we present an almost linear time algorithm to construct the GSA. The new algorithm is more efficient and simpler than the existing algorithms for GSA construction <ref> [BMO90, Hav93] </ref>. Since SSA is a special case of GSA, it can also be used as an efficient alternative algorithm for SSA construction. The existing algorithms for building the GSA follow two steps. The first step is the same function placement procedure as in the SSA construction [CFR + 91]. <p> In the second step, the GSA conversion algorithms collect the control dependences of the definitions reaching a function and transforms the function into a gating function. The original GSA conversion algorithm [BMO90] assumed a Program Dependence Graph (PDG)[FOW87] as its initial representation. Havlak developed another algorithm <ref> [Hav93] </ref> to construct a variant of the GSA, known as Thinned GSA. Because it starts with the program flow graph, it is, therefore, somewhat simpler. For each function, both algorithms traverse the control flow graph to find the gating conditions for each reaching definition.
Reference: [JP93] <author> R. Johnson and K. Pingali. </author> <title> Dependence-based program analysis. </title> <booktitle> In Proc. the SIGPLAN '93 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: The algorithm for placing the functions is O (N 2 ) in the worst case, but often appears to be linear when applied to real programs. Johnson and Pingali <ref> [JP93, JPP94] </ref> proposed another algorithm to place functions in O (E) time. Later, Cytron and Ferrante proposed an almost linear time O (Eff (E)) using path compression. Recently, Sreedhar and Gao [SG94] have developed another O (E) time algorithm.
Reference: [JPP94] <author> R. Johnson, D. Pearson, and K. Pingali. </author> <title> The program structure tree: Computing control regions in linear time. </title> <booktitle> In Proc. the SIGPLAN '94 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: The algorithm for placing the functions is O (N 2 ) in the worst case, but often appears to be linear when applied to real programs. Johnson and Pingali <ref> [JP93, JPP94] </ref> proposed another algorithm to place functions in O (E) time. Later, Cytron and Ferrante proposed an almost linear time O (Eff (E)) using path compression. Recently, Sreedhar and Gao [SG94] have developed another O (E) time algorithm.
Reference: [LT79] <author> Thomas Lengauer and Robert Endre Tarjan. </author> <title> A fast algorithm for finding dominators in a flowgraph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 121-141, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: We also assume that each node in the CF G is assigned a depth-first number dfn. The dominator tree can be computed in O (Eff (E; N )) time using the dominator algorithm of Lengauer and Tarjan <ref> [LT79] </ref>, or in O (E) time using a more complicated algorithm of Harel [Har85]. The dfn can be computed in linear time [ASU86]. The dfn number has the property of dfn (idom (v)) &lt; df n (v) for each node v 6= Entry.
Reference: [RWZ88] <author> B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> Global Value Numbers and Redundant Computation. </title> <booktitle> In Proc. of the 15th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 12-27, </pages> <year> 1988. </year>
Reference-contexts: SSA form has been shown to be useful in capturing the data flow information required by some important program optimizations <ref> [AWZ88, RWZ88] </ref>. In the SSA form, each definition of a variable is given a unique new name, and each use of a variable is renamed to refer to a single reaching definition.
Reference: [SG94] <author> V.C. Sreedhar and G.R. Gao. </author> <title> Computing -nodes in linear time using dj-graph. </title> <type> Technical Report Technical Report,ACAPS Technical Memo 75, </type> <institution> McGill University, School of Computer Science, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Johnson and Pingali [JP93, JPP94] proposed another algorithm to place functions in O (E) time. Later, Cytron and Ferrante proposed an almost linear time O (Eff (E)) using path compression. Recently, Sreedhar and Gao <ref> [SG94] </ref> have developed another O (E) time algorithm. We should point out that although our algorithm also uses the path compression technique, our problem is more complicated and our approach is completely different from Cytron and Ferrante's.
Reference: [Tar79] <author> Robert Endre Tarjan. </author> <title> Applications of path compression on balanced trees. </title> <journal> Journal of ACM, </journal> <volume> 26(4) </volume> <pages> 690-715, </pages> <month> October </month> <year> 1979. </year>
Reference-contexts: The algorithm in this paper constructs and places the gating functions from a program control flow graph in a single step. In our algorithm, SSA and GSA constructions are unified under a single process of gating path construction. It uses the path compression technique <ref> [Tar79] </ref> to reduce the total number of visits to the edges in the flow graph. Tarjan describes two ways to implement the path compression. <p> This work is not necessarily representative of the positions or policies of the U. S. Army or the government. 1 method, called stratified path compression by Farrow <ref> [Far77, Tar79] </ref>, can also be used. This GSA algorithm is also almost as efficient as the best known algorithms for function placement in SSA conversion. The rest of the paper is divided into the following sections. In Section 2, we introduce some background and notations. <p> The the path expression computed by EV AL (e) is later used to update G fl (v) to represent the path from the loop back edge e. G fl (v) is used to build a function for the loop. Using Tarjan's technique for operations on a forest <ref> [Tar79] </ref>, we define the following operations on the forest of subtrees in a dominator tree: * EV AL (e): Let e = (w; v). <p> Because the sequence of EV AL and LIN K can be easily determined beforehand, the off-line algorithm in <ref> [Tar79] </ref> can also be used to achieve O (Eff (E; N )) time complexity. Theorem 3. The time complexity of the algorithm is O (Eff (E; N )). For each node v, the algorithm computes the (v), GP (v) and G fl (v). <p> The algorithm is based on the well-known path compression technique <ref> [Tar79] </ref>. It is easy to implement and efficient for the programs in the Perfect Benchmark.
Reference: [Tar81a] <author> Robert Endre Tarjan. </author> <title> Fast algorithm for solving path problems. </title> <journal> Journal of the ACM, </journal> <volume> 28(3) </volume> <pages> 594-614, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: We can then obtain a topsort order among the siblings in children (u). Irreducible graphs can also be handled by computing a path sequence for each dominator strong components. Due to limited space, we cannot detail how to compute the path sequence. Interested readers should refer to <ref> [Tar81a] </ref>. The two existing GSA algorithms only handle reducible graph, but the algorithm here can be extended to handle irreducible graph. In the merge phase, the algorithm follows the topsort order and computes for each child of u a path expression GP (v) representing all the gating paths of v. <p> This algorithm is a variant of Tarjan's fast algorithm for solving path problems using dominator strong components decomposition <ref> [Tar81a] </ref>. Its correctness can be derived from the following Lemma, which we quote without proof here. We will work through an example to illustrate the algorithm. Lemma 7.[Tar81a] * For edges e = (w; v) in CF G such that w 6= u, the corresponding path expression in the ListP (v)
Reference: [Tar81b] <author> Robert Endre Tarjan. </author> <title> A unified approach to path problems. </title> <journal> Journal of the ACM, </journal> <volume> 28(3) </volume> <pages> 577-593, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: A path expression <ref> [Tar81b] </ref> P of type (u; v) is a simple regular expression over E such that every string in (P ) is a path from node u to node v (where (P ) represents the string generated by the regular expression P ).
Reference: [TP93] <author> Peng Tu and David Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proc. 6rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year> <month> 13 </month>
Reference-contexts: It is a convenient representation for several program analysis and optimization techniques, including constant propagation with conditional branches [WZ91]; equality of symbolic expressions [AWZ88, Hav93]; induction variable substitution [Wol92]; symbolic dependence analysis [BE94] and demand-driven symbolic analysis for array privatization <ref> [TP94, TP93] </ref>. In the SSA representation, functions of a single type are placed at the confluence nodes of a program flow graph to represent different definitions of a variable reaching from different incoming edges. The condition under which a definition reachs a confluence node is not represented in the function.
Reference: [TP94] <author> Peng Tu and David Padua. </author> <title> GSA based demand-driven symbolic analysis. </title> <type> Technical Report 1339, </type> <institution> University of Illinois at Urbana-Champaign, Cntr for Supercomputing Res & Dev, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: It is a convenient representation for several program analysis and optimization techniques, including constant propagation with conditional branches [WZ91]; equality of symbolic expressions [AWZ88, Hav93]; induction variable substitution [Wol92]; symbolic dependence analysis [BE94] and demand-driven symbolic analysis for array privatization <ref> [TP94, TP93] </ref>. In the SSA representation, functions of a single type are placed at the confluence nodes of a program flow graph to represent different definitions of a variable reaching from different incoming edges. The condition under which a definition reachs a confluence node is not represented in the function.
Reference: [Wol92] <author> Michael Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> ACM PLDI'92, </booktitle> <year> 1992. </year>
Reference-contexts: GSA was introduced by Ballance, Maccabe and Ottenstein as a part of Program Dependence Web (PDW)[BMO90]. It is a convenient representation for several program analysis and optimization techniques, including constant propagation with conditional branches [WZ91]; equality of symbolic expressions [AWZ88, Hav93]; induction variable substitution <ref> [Wol92] </ref>; symbolic dependence analysis [BE94] and demand-driven symbolic analysis for array privatization [TP94, TP93]. In the SSA representation, functions of a single type are placed at the confluence nodes of a program flow graph to represent different definitions of a variable reaching from different incoming edges.
Reference: [WZ91] <author> M. N. Wegman and F. K. Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <month> April </month> <year> 1991. </year> <month> 14 </month>
Reference-contexts: GSA was introduced by Ballance, Maccabe and Ottenstein as a part of Program Dependence Web (PDW)[BMO90]. It is a convenient representation for several program analysis and optimization techniques, including constant propagation with conditional branches <ref> [WZ91] </ref>; equality of symbolic expressions [AWZ88, Hav93]; induction variable substitution [Wol92]; symbolic dependence analysis [BE94] and demand-driven symbolic analysis for array privatization [TP94, TP93].
References-found: 23

