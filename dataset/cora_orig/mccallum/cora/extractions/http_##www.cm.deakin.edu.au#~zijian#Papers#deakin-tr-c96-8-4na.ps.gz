URL: http://www.cm.deakin.edu.au/~zijian/Papers/deakin-tr-c96-8-4na.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Email: (Email: zijian@deakin.edu.au)  
Title: A Comparison of Constructive Induction with Different Types of New Attribute  
Author: Zijian Zheng 
Address: Geelong Victoria 3217, Australia  
Affiliation: School of Computing and Mathematics Deakin University,  
Pubnum: Technical Report (TR C96/8)  
Abstract: This paper studies the effects on decision tree learning of constructing four types of attribute (conjunctive, disjunctive, Mof-N, and Xof-N representations). To reduce effects of other factors such as tree learning methods, new attribute search strategies, search starting points, evaluation functions, and stopping criteria, a single tree learning algorithm is developed. With different option settings, it can construct four different types of new attribute, but all other factors are fixed. The study reveals that conjunctive and disjunctive representations have very similar performance in terms of prediction accuracy and theory complexity on a variety of concepts, even on DNF and CNF concepts that are usually thought to be suited only to one of the two kinds of representation. In addition, the study demonstrates that the stronger representation power of Mof-N than conjunction and disjunction and the stronger representation power of Xof-N than these three types of new attribute can be reflected in the performance of decision tree learning in terms of higher prediction accuracy and lower theory complexity. 
Abstract-found: 1
Intro-found: 1
Reference: [Bloedorn et al., 1993] <author> E. Bloedorn, R.S. Michalski, and J. Wnek, </author> <title> Multistrategy constructive induction: </title> <booktitle> AQ17-mci. Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <pages> 188-203. </pages>
Reference: [Breiman et al., 1984] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone, </author> <title> Classification And Regression Trees, </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference: [Catlett, 1991a] <author> J. Catlett, </author> <title> On changing continuous attributes into ordered discrete attributes. </title> <booktitle> Proceedings of the Fifth European Working Session on Learning, </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag, </publisher> <pages> 164-178. </pages>
Reference: [Dietterich et al., 1990] <author> T.G. Dietterich, H. Hild, and G. Bakiri, </author> <title> A comparative study of ID3 and backpropagation for English text-to-speech mapping. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 24-31. </pages>
Reference: [Fayyad and Irani, 1993] <author> U.M. Fayyad and K.B. Irani, </author> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 1022-1027. </pages>
Reference: [Langley et al., 1987] <author> P. Langley, H.A. Simon, G.L. Bradshaw, and J.M. Zytkow, </author> <title> Scientific Discovery: Computational Explorations of the Creative Processes, </title> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: [Matheus, 1989] <author> C.J. Matheus, </author> <title> Feature Construction: An Analytic Framework and an Application to Decision Trees, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL. </institution>
Reference: [Michalski, 1978] <author> R.S. Michalski, </author> <title> Pattern recognition as knowledge-guided computer induction. </title> <type> Technical Reports 927, </type> <institution> Department of Computer Science, The University of Illinois at Urbana-Champaign, Urbana, IL. </institution> <month> 21 </month>
Reference-contexts: 1 Introduction Conventional inductive learning systems, called selective induction, learn theories by selecting task-supplied attributes. When these attributes are not adequate for describing theories, the performance of selective induction systems in terms of prediction accuracy and theory complexity is poor. One possible solution to this problem is constructive induction <ref> [Michalski, 1978] </ref>. This type of inductive system transforms the original instance space into a more adequate space by generating new attributes. Task-supplied attributes are called primitive attributes in contrast to new attributes. Many constructive induction systems have been developed. They perform differently in different domains. <p> That is, the constructed attributes are conjunctions or disjunctions of other attributes. A few systems use other constructive operators, for example, Mof-N [Murphy and Pazzani, 1991; Ting, 1994], mathematical operators such as multiplication and division <ref> [Michalski, 1978; Lang-ley, Simon, Bradshaw, and Zytkow, 1987] </ref>, "attribute counting operators" [Michal-ski, 1978; Bloedorn, Michalski, and Wnek , 1993], and Xof-N [Zheng, 1995a].
Reference: [Murphy and Aha, 1996] <author> P.M. Murphy and D.W. Aha, </author> <title> UCI Repository of machine learning databases [Machine-readable data repository]. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA, </address> <note> Available by anonymous ftp at ics.uci.edu in the pub/machine-learning-databases directory. </note>
Reference-contexts: For each concept, experiments are repeated ten times using different training and test sets. In addition to the fourteen artificial domains, ten real-world domains from the UCI repository of machine learning databases <ref> [Murphy and Aha, 1996] </ref> are used, in which Mof-N-like concepts are expected to be found [Spackman, 1988].
Reference: [Murphy and Pazzani, 1991] <author> P.M. Murphy and M.J. Pazzani, ID2-of-3: </author> <title> Constructive induction of M-of-N concepts for discriminators in decision trees. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 183-187. </pages>
Reference-contexts: Most constructive induction systems such as Fringe [Pagallo, 1990], LFC [Raga-van and Rendell, 1993], and AQ17-hci [Wnek and Michalski, 1994] use conjunction and/or disjunction as constructive operators. That is, the constructed attributes are conjunctions or disjunctions of other attributes. A few systems use other constructive operators, for example, Mof-N <ref> [Murphy and Pazzani, 1991; Ting, 1994] </ref>, mathematical operators such as multiplication and division [Michalski, 1978; Lang-ley, Simon, Bradshaw, and Zytkow, 1987], "attribute counting operators" [Michal-ski, 1978; Bloedorn, Michalski, and Wnek , 1993], and Xof-N [Zheng, 1995a]. <p> A comparison of two decision tree learning systems showed that the one constructing conjunctions as new attributes outperformed that constructing disjunctions for DNF concepts, and vice versa for CNF concepts [Pagallo, 1990]. <ref> [Murphy and Pazzani, 1991] </ref> demonstrated that constructing Mof-N representations can improve the performance of decision tree learning. Zheng [1995] shows that Xof-N representations are more representationally powerful than conjunctive, disjunctive, and Mof-N representations.
Reference: [Pagallo and Haussler, 1989] <author> G. Pagallo and D. Haussler, </author> <title> Two algorithms that learn DNF by discovering relevant features. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 119-123. </pages>
Reference: [Pagallo, 1990] <author> G. Pagallo, </author> <title> Adaptive Decision Tree Algorithms for Learning from Examples, </title> <type> Ph.D. Thesis, </type> <institution> University of California at Santa Cruz, </institution> <address> Santa Cruz, CA. </address>
Reference-contexts: Constructive operators play an important role in constructive induction. However, all other factors may also affect the performance of a learning system. Each of them might, either positively or negatively, contribute to performance differences between different algorithms. Most constructive induction systems such as Fringe <ref> [Pagallo, 1990] </ref>, LFC [Raga-van and Rendell, 1993], and AQ17-hci [Wnek and Michalski, 1994] use conjunction and/or disjunction as constructive operators. That is, the constructed attributes are conjunctions or disjunctions of other attributes. <p> A comparison of two decision tree learning systems showed that the one constructing conjunctions as new attributes outperformed that constructing disjunctions for DNF concepts, and vice versa for CNF concepts <ref> [Pagallo, 1990] </ref>. [Murphy and Pazzani, 1991] demonstrated that constructing Mof-N representations can improve the performance of decision tree learning. Zheng [1995] shows that Xof-N representations are more representationally powerful than conjunctive, disjunctive, and Mof-N representations. <p> When conducting comparisons, C4.5 is used as a reference. 5 For CNF concepts, the columns give the number of disjunctions and disjunction length. 8 4.1 Experimental domains and methods Table 1 summarizes the characteristics of a set of artificial domains from <ref> [Pagallo, 1990] </ref>. Each CNF is the dual concept of the corresponding DNF. 6 There are two reasons for choosing these domains. <p> Note that the majority concepts are a special kind of "at least Mof-N" concept. Second, all of them contain a large number of irrelevant attributes. By using them, we can see the ability of an algorithm to tolerate irrelevant attributes. We use the same experimental method given in <ref> [Pagallo, 1990] </ref>. For each experiment, a training set and a test set are independently drawn from the uniform distribution. The size of test sets is 2000. <p> The sizes of training sets, given in Table 1, are the results of a VC dimension analysis for finding the number of examples which would suffice for an ideal learning algorithm to create a consistent hypothesis with an error rate less than 10% on any test set (see <ref> [Pagallo, 1990] </ref> for details). For each concept, experiments are repeated ten times using different training and test sets.
Reference: [Pagallo and Haussler, 1990] <author> G. Pagallo and D. Haussler, </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 71-100. </pages>
Reference: [Quinlan and Rivest, 1989] <author> J. Ross. Quinlan and R.L. Rivest, </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80, </volume> <pages> 227-248. </pages>
Reference-contexts: To find an Mof-N representation, for each set of attribute-value pairs formed during the search, MofN uses the number that gives an Mof-N with the highest value of the evaluation function as M . For comparing and selecting new attributes, a combination of information gain ratio and coding cost <ref> [Quinlan and Rivest, 1989] </ref> is used as the evaluation function [Zheng, 1995a].
Reference: [Quinlan, 1993a] <author> J.R. Quinlan, C4.5: </author> <title> Programs for Machine Learning, </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: If target concepts are not complex or huge data sets are available, it is not a problem. 3 This means that Xof-N equals 1, or 2, ..., or N . 5 Two possible approaches to alleviating the fragmentation problem are subsetting <ref> [Quinlan, 1993a; Zheng, 1995a] </ref> and subranging. For an Xof-N representation, sub-setting merges its values into several subsets by carrying out search. When building a decision tree, one branch is created for each subset, rather than each value of the Xof-N. <p> Here, we only describe the main idea and a few specific points about the learning algorithm used in this paper. This algorithm is referred to as Conj, Disj, MofN, and XofN when constructing conjunctive, disjunctive, Mof-N, and Xof-N representations respectively. This learning algorithm uses C4.5 <ref> [Quinlan, 1993a] </ref> as its selective induction component. The test at each decision node of a tree is derived from either a primitive attribute or a new attribute (one of the four types: conjunctive, disjunctive, Mof-N, and Xof-N representations). The algorithm adopts the data-driven strategy for con 6 structing new attributes.
Reference: [Ragavan and Rendell, 1993] <author> H. Ragavan and L. Rendell, </author> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 252-259. </pages>
Reference: [Rao et al., 1995] <author> R.B. Rao, D. Gordon, and W. Spears, </author> <title> For every generalization action, is there really an equal and opposite reaction? analysis of the conservation law for generalization performance. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 471-479. </pages>
Reference: [Schaffer, 1994] <author> C. Schaffer, </author> <title> A conservation law for generalization performance. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> San Ma-teo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 259-265. 22 </pages>
Reference-contexts: The objective of doing so is to diminish effects that are created by differences of other factors which might affect the performance of constructive induction. Since generalization performance is concerned when comparing learning algorithms, the conservation law <ref> [Schaffer, 1994] </ref> is worth mentioning here. It implies that no algorithm can be superior to another in terms of generalization performance across all learning situations the whole universe.
Reference: [Spackman, 1988] <author> K.A. Spackman, </author> <title> Learning categorical decision criteria in biomedi-cal domains. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 36-46. </pages>
Reference-contexts: For each concept, experiments are repeated ten times using different training and test sets. In addition to the fourteen artificial domains, ten real-world domains from the UCI repository of machine learning databases [Murphy and Aha, 1996] are used, in which Mof-N-like concepts are expected to be found <ref> [Spackman, 1988] </ref>. They consist of five medical domains (Cleveland Heart Disease, Hepatitis, Liver Disorders, Pima Indians Diabetes, Wisconsin Breast Cancer), one molecular biology domain (Promoters), three linguistics domains (Nettalk (Phoneme), Nettalk (Stress), Nettalk (Letter)), and one game domain (Tic-Tac-Toe).
Reference: [Ting, 1994] <author> K.M. Ting, </author> <title> An M-of-N rule induction algorithm and its application to DNA domain. </title> <booktitle> Proceedings of the Twenty-seventh Annual Hawaii International Conference on System Sciences, Volume V: Biotechnology Computing, </booktitle> <address> Los Alami-tos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <pages> 133-140. </pages>
Reference-contexts: Most constructive induction systems such as Fringe [Pagallo, 1990], LFC [Raga-van and Rendell, 1993], and AQ17-hci [Wnek and Michalski, 1994] use conjunction and/or disjunction as constructive operators. That is, the constructed attributes are conjunctions or disjunctions of other attributes. A few systems use other constructive operators, for example, Mof-N <ref> [Murphy and Pazzani, 1991; Ting, 1994] </ref>, mathematical operators such as multiplication and division [Michalski, 1978; Lang-ley, Simon, Bradshaw, and Zytkow, 1987], "attribute counting operators" [Michal-ski, 1978; Bloedorn, Michalski, and Wnek , 1993], and Xof-N [Zheng, 1995a].
Reference: [Van de Merckt, 1993] <author> T. Van de Merckt, </author> <title> Decision trees in numerical attribute spaces. </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 1016-1021. </pages>
Reference: [Wnek and Michalski, 1994] <author> J. Wnek and R.S. Michalski, </author> <title> Hypothesis-driven constructive induction in AQ17-hci: a method and experiments. </title> <journal> Machine Learning, </journal> <volume> 14, </volume> <pages> 139-168. </pages>
Reference-contexts: Many constructive induction systems have been developed. They perform differently in different domains. Different constructive induction systems use different constructive operators, different theory description languages, different strategies such as a data-driven strategy, a hypothesis-driven strategy, and a knowledge-driven strategy <ref> [Wnek and Michalski, 1994] </ref>, different new attribute evaluation functions, different new attribute search methods, and so on. Constructive operators play an important role in constructive induction. However, all other factors may also affect the performance of a learning system. <p> However, all other factors may also affect the performance of a learning system. Each of them might, either positively or negatively, contribute to performance differences between different algorithms. Most constructive induction systems such as Fringe [Pagallo, 1990], LFC [Raga-van and Rendell, 1993], and AQ17-hci <ref> [Wnek and Michalski, 1994] </ref> use conjunction and/or disjunction as constructive operators. That is, the constructed attributes are conjunctions or disjunctions of other attributes.
Reference: [Yang et al., 1991] <author> D. Yang, L. Rendell, and G. Blix, </author> <title> A scheme for feature construction and a comparison of empirical methods. </title> <booktitle> Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 699-704. </pages>
Reference: [Zheng, 1995a] <author> Z. Zheng, </author> <title> Constructing nominal Xof-N attributes. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 1064-1070. </pages>
Reference-contexts: A few systems use other constructive operators, for example, Mof-N [Murphy and Pazzani, 1991; Ting, 1994], mathematical operators such as multiplication and division [Michalski, 1978; Lang-ley, Simon, Bradshaw, and Zytkow, 1987], "attribute counting operators" [Michal-ski, 1978; Bloedorn, Michalski, and Wnek , 1993], and Xof-N <ref> [Zheng, 1995a] </ref>. <p> These domains involve nominal attributes and/or multiple classes. In this paper, we consider Xof-N representations as nominal attributes <ref> [Zheng, 1995a] </ref>, although they can also be treated as continuous-valued attributes [Zheng, 1995b]. Among the four types of representations, Xof-N is the most representation-ally powerful. <p> If target concepts are not complex or huge data sets are available, it is not a problem. 3 This means that Xof-N equals 1, or 2, ..., or N . 5 Two possible approaches to alleviating the fragmentation problem are subsetting <ref> [Quinlan, 1993a; Zheng, 1995a] </ref> and subranging. For an Xof-N representation, sub-setting merges its values into several subsets by carrying out search. When building a decision tree, one branch is created for each subset, rather than each value of the Xof-N. <p> With different option settings, it can construct conjunctive, disjunctive, Mof-N, or Xof-N representations as new attributes. The algorithm borrows the control structure, tree learning method, new attribute construction strategy, new attribute search method, and evaluation function from the XofN algorithm <ref> [Zheng, 1995a] </ref>. If the option is set to construct Xof-N representations, this algorithm behaves exactly the same as the XofN algorithm. <p> For comparing and selecting new attributes, a combination of information gain ratio and coding cost [Quinlan and Rivest, 1989] is used as the evaluation function <ref> [Zheng, 1995a] </ref>. <p> The objective is to investigate the following three comparisons for constructive decision tree learning: * conjunction with disjunction, * Mof-N with conjunction and disjunction, and * Xof-N with conjunction, disjunction, and Mof-N. Here, we focus on prediction accuracy and theory complexity. The theory complex ity <ref> [Zheng, 1995a] </ref> is the modified tree size that includes both decision nodes and leaves, and takes into account the sizes of new attributes at decision nodes. <p> How noise and missing values affect XofN and the others is an issue for future investigation. 4.3 Computational requirements The search space for creating an Xof-N representation is the same as that for generating a conjunction or a disjunction. The search space for constructing an Mof-N is larger <ref> [Zheng, 1995a] </ref>. However, this does not mean that for a given learning task, Conj, Disj, and XofN need the same execution time, while MofN needs more time. The reason is that the sizes of learned trees can be very different.
Reference: [Zheng, 1995b] <author> Z. Zheng, </author> <title> Continuous-valued Xof-N attributes versus nominal Xof-N attributes for constructive induction: a case study. </title> <booktitle> Proceedings of the Fourth International Conference for Young Computer Scientists, </booktitle> <address> Beijing: </address> <publisher> Peking University Press, </publisher> <pages> 566-573. 23 </pages>
Reference-contexts: These domains involve nominal attributes and/or multiple classes. In this paper, we consider Xof-N representations as nominal attributes [Zheng, 1995a], although they can also be treated as continuous-valued attributes <ref> [Zheng, 1995b] </ref>. Among the four types of representations, Xof-N is the most representation-ally powerful.
References-found: 25

