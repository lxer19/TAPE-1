URL: http://www.caip.rutgers.edu/~yuk/papers/icassp96.qlin.ps
Refering-URL: 
Root-URL: 
Phone: 2  
Title: ROBUST DISTANT-TALKING SPEECH RECOGNITION  
Author: Q. Lin C. Che D.-S. Yuk L. Jin B. de Vries J. Pearson and J. Flanagan 
Address: Piscataway, NJ 08855  Princeton, NJ 08543  
Affiliation: 1 CAIP Center, Rutgers University,  David Sarnoff Research Center,  
Abstract: Most contemporary speech recognizers are designed to operate with close-talking speech and they work best in a quiet laboratory condition. There is an apparent need to render environment robustness to these systems. The objective of the paper is to explore utility of existing speech recognition technology in adverse "real-world" environments for distant-talking applications. A synergistic system consisting of Microphone Array and Neural Network (MANN) is utilized to mitigate environmental interference introduced by reverberation, ambient noise, and channel mismatch between training and testing conditions. The MANN system is evaluated with experiments on continuous distant-talking speech recognition. The results show that the MANN system elevates the word recognition accuracy to a level which is competitive with a retrained speech recognizer and that the neural network compensation performs better than some previously researched techniques. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Acero, A., and Stern, R., </author> <title> "Environmental robustness in automatic speech recognition," </title> <booktitle> ICASSP 90, </booktitle> <pages> pp. 849-852. </pages>
Reference-contexts: The speech recognizer uses gender independent, 3-state continuous density triphone HMMs, with 7 Gaussian mixtures per state. The HMMs are trained with close-talking speech unless explicitly specified. From distant-talking array speech produces an error rate of 61.6% with a recognizer trained on close-talking speech. The SDCN method <ref> [1] </ref> reduces the error rate to 35%. Greater 0 20 40 Array only Array+wrong LD neural nets Array+right LD neural nets Array+location independent net 47.6 14.1 Word error rate in % For two source locations in a lab room. LD refers to location depedent.
Reference: [2] <author> Che, C., Lin, Q., Pearson, J., de Vries, B., and Flana-gan, J., </author> <title> "Microphone Arrays and Neural Networks for Robust Speech Recognition," </title> <booktitle> Proc. 1994 ARPA HLT Workshop, </booktitle> <address> New Jersey, </address> <pages> pp. 342-347. </pages>
Reference-contexts: These sentences are used in the present study to train NNs. The trained NNs are then used for feature adaptation to approximate a matched training and testing condition of the speech recognizer. In our previous studies <ref> [2, 5] </ref>, speaker-dependent NNs Speaker-dependent Gender-dependent (SD) (GD) 7 Males 16.9% 13.7% 5 Females 19.1% 16.4% Table 1. Distant-talking (array at 12 ft) speech recognition word error rates, using either speaker-dependent NNs or gender-dependent NNs to compensate for environmental mismatches. <p> From Figure 6, it is seen that the location-independent NN is competitive to location-independent NNs. 5. CONCLUSIONS Increased attention has been devoted to robust distant talking speech recognition because of its advantageous feature of hands-free operation <ref> [2, 5, 4, 8] </ref>. Of particular interest is the ability to directly deploy existing speech recogniz-ers trained on close-talking so that expensive and tedious retraining of the recognizers can be avoided. Two issues then arise. <p> To compensate for mismatches between close-talking and distant-talking, a neural network is adapted using simultaneously recorded speech signals. Our experiments show that adapting a neural network requires much less speech data than (re-)training a large vocabulary speech recognizer <ref> [2, 5, 9] </ref>. Our experiments also show that the MANN system elevates recognition performance of distant-talking in a noisy and reverberant environment to a level which is competitive to a retrained recognizer. Both speaker-dependent and speaker-independent (but gender-dependent) NNs have been explored.
Reference: [3] <author> Flanagan, J., Berkley, D., Elko, G., West, J., and Sondhi, M., </author> <title> "Autodirective microphone systems," </title> <address> Acus-tica 73, </address> <year> 1991, </year> <pages> pp. 58-71. </pages>
Reference-contexts: Beamforming microphone arrays The microphone array we use is a one-dimensional beam forming line array composed of first-order gradient microphones. It uses direct-path arrivals to produce a single-beam delay-and-sum beamformer <ref> [3, 7] </ref>. Beamforming arrays effectively combat reverberation and multi-path distortion because the confined beam "sees" fewer sound images in reflecting surfaces. The array consists of 29 gradient sensors, which are non-uniformly positioned in a line (harmonically nested over four octaves).
Reference: [4] <author> Giuliani, D., Matassoni, M., Omologo, M., Svaizer, P., </author> <title> "Robust continuous speech recognition using a microphone array," </title> <booktitle> Eurospeech 95, </booktitle> <pages> pp. 2021-2024, </pages> <address> Madrid, Spain, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: From Figure 6, it is seen that the location-independent NN is competitive to location-independent NNs. 5. CONCLUSIONS Increased attention has been devoted to robust distant talking speech recognition because of its advantageous feature of hands-free operation <ref> [2, 5, 4, 8] </ref>. Of particular interest is the ability to directly deploy existing speech recogniz-ers trained on close-talking so that expensive and tedious retraining of the recognizers can be avoided. Two issues then arise.
Reference: [5] <author> Lin, Q., Che, C., Pearson, J., de Vries, B., and Flana-gan, J., </author> <title> "Experiments of distant-talking speech recognition," </title> <booktitle> Proc. 1995 ARPA SLT Workshop, </booktitle> <pages> pp. 187-192. </pages>
Reference-contexts: As will be shown later, adapting a neural network requires much less speech data than (re-)training a large vocabulary, speaker-independent speech recognizer. The MANN system has been evaluated on a distant-talking version of the ARPA Resource Management database (RMdt) <ref> [5] </ref>. In this paper, experimental results are presented and analyzed. The neural network compensation technique is also compared with other compensation techniques. <p> These sentences are used in the present study to train NNs. The trained NNs are then used for feature adaptation to approximate a matched training and testing condition of the speech recognizer. In our previous studies <ref> [2, 5] </ref>, speaker-dependent NNs Speaker-dependent Gender-dependent (SD) (GD) 7 Males 16.9% 13.7% 5 Females 19.1% 16.4% Table 1. Distant-talking (array at 12 ft) speech recognition word error rates, using either speaker-dependent NNs or gender-dependent NNs to compensate for environmental mismatches. <p> From Figure 6, it is seen that the location-independent NN is competitive to location-independent NNs. 5. CONCLUSIONS Increased attention has been devoted to robust distant talking speech recognition because of its advantageous feature of hands-free operation <ref> [2, 5, 4, 8] </ref>. Of particular interest is the ability to directly deploy existing speech recogniz-ers trained on close-talking so that expensive and tedious retraining of the recognizers can be avoided. Two issues then arise. <p> To compensate for mismatches between close-talking and distant-talking, a neural network is adapted using simultaneously recorded speech signals. Our experiments show that adapting a neural network requires much less speech data than (re-)training a large vocabulary speech recognizer <ref> [2, 5, 9] </ref>. Our experiments also show that the MANN system elevates recognition performance of distant-talking in a noisy and reverberant environment to a level which is competitive to a retrained recognizer. Both speaker-dependent and speaker-independent (but gender-dependent) NNs have been explored.
Reference: [6] <author> Lin, Q., Che, C., and French, J. </author> <title> "Description of the CAIP speech corpus," </title> <booktitle> Proc. ICSLP 94, </booktitle> <pages> pp. 1823-1826. </pages>
Reference-contexts: It has a reverberation time of approximately 0.5 second and the sound pressure level of ambient noise is 50 dBA or 71 dBC, indicating an interfering noise spectrum more tense in lower audio frequencies. A separate corpus with 80 live speakers has also been acquired in the same enclosure <ref> [6] </ref>. Both corpora are available to speech researchers upon request. 4. EXPERIMENTAL RESULTS In the following recognition experiments, the Entropic HTK V1.5 speech recognizer is employed. The recog-nizer uses gender-independent, continuous-density triphone HMM models.
Reference: [7] <author> Lin, Q., Jan, E., and Flanagan, J. </author> <title> "Microphone arrays and speaker identification," </title> <journal> IEEE-Trans. Speech & Audio Proc., </journal> <volume> Vol. 2, No. 4, </volume> <year> 1994, </year> <pages> pp. 622-629. </pages>
Reference-contexts: Beamforming microphone arrays The microphone array we use is a one-dimensional beam forming line array composed of first-order gradient microphones. It uses direct-path arrivals to produce a single-beam delay-and-sum beamformer <ref> [3, 7] </ref>. Beamforming arrays effectively combat reverberation and multi-path distortion because the confined beam "sees" fewer sound images in reflecting surfaces. The array consists of 29 gradient sensors, which are non-uniformly positioned in a line (harmonically nested over four octaves).
Reference: [8] <author> Nakamura, S., Takiguchi, T., Shikano, K. </author> <title> "Noise and room acoustics distorted speech recognition by HMM composition," </title> <booktitle> elsewhere in ICASSP 96. </booktitle>
Reference-contexts: From Figure 6, it is seen that the location-independent NN is competitive to location-independent NNs. 5. CONCLUSIONS Increased attention has been devoted to robust distant talking speech recognition because of its advantageous feature of hands-free operation <ref> [2, 5, 4, 8] </ref>. Of particular interest is the ability to directly deploy existing speech recogniz-ers trained on close-talking so that expensive and tedious retraining of the recognizers can be avoided. Two issues then arise. <p> Future work will be extended to quantifying effects of automatic source location on the MANN system, and to development of more general region-dependent, location independent NNs. The MANN system will also be compared with other compensation techniques, such as the PMC method of <ref> [8] </ref>.
Reference: [9] <author> Yuk, D., Che, C., Jin, L., and Lin, Q. </author> <title> "Toward environment-independent continuous speech recognition," </title> <booktitle> elsewhere in ICASSP 96. </booktitle>
Reference-contexts: The input layer incorporates a tapped delay line which allows simultaneous use of previous and preceding frames. Sepa rate measurements show that a input window size of 5 to 7 frames leads to better distant-talking recognition performance. The reader is referred to Yuk et al <ref> [9] </ref> for more detailed descriptions of different neural network architectures. Both speaker dependent and speaker independent MLPs have been explored. The NN is trained using a backpropagation method to establish a mapping function of the cep-strum coefficients between distant-talking and close-talking. <p> To compensate for mismatches between close-talking and distant-talking, a neural network is adapted using simultaneously recorded speech signals. Our experiments show that adapting a neural network requires much less speech data than (re-)training a large vocabulary speech recognizer <ref> [2, 5, 9] </ref>. Our experiments also show that the MANN system elevates recognition performance of distant-talking in a noisy and reverberant environment to a level which is competitive to a retrained recognizer. Both speaker-dependent and speaker-independent (but gender-dependent) NNs have been explored.
References-found: 9

