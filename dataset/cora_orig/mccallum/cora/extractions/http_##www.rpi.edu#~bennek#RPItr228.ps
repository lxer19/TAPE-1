URL: http://www.rpi.edu/~bennek/RPItr228.ps
Refering-URL: http://www.rpi.edu/~bennek/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: An Extreme Point Tabu Search Method for Data Mining  
Author: Kristin P. Bennett Jennifer A. Blue 
Keyword: Key Words: decision trees, tabu search, classification, machine learning, global optimization.  
Note: Knowledge Discovery and Data  This material is based on research supported by National Science Foundation Grant 949427.  
Address: Troy, NY 12180  Troy, NY 12180.  
Affiliation: Department of Mathematical Sciences Rensselaer Polytechnic Institute  R.P.I. Math  Mining Group, Department of Mathematical Sciences, Rensselaer Polytechnic Institute,  
Pubnum: Report No.  
Email: Email bennek@rpi.edu, bluej@rpi.edu.  
Date: 228  
Abstract: We propose an Extreme Point Tabu Search (EPTS) algorithm that constructs globally optimal decision trees for classification problems. Typically, decision tree algorithms are greedy. They optimize the misclassification error of each decision sequentially. Our non-greedy approach minimizes the misclassification error of all the decisions in the tree concurrently. Decision trees are ideal for data-mining because their logical structure makes them easily understandable. Using Global Tree Optimization (GTO), we can optimize existing decision trees. This capability can be used in data mining for avoiding overfitting, transferring knowledge, incorporating domain knowledge, and maintaining existing decision trees. Our method works by fixing the structure of the decision tree and then representing it as a set of disjunctive linear inequalities. An optimization problem is constructed that minimizes the errors within the disjunctive linear inequalities. To reduce the misclassification error, a nonlinear error function is minimized over a polyhedral region. A new EPTS algorithm is used to search the extreme points of the polyhedral region for an optimal solution. Promising computational results are given for both randomly generated and real-world problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <editor> In M. Evans, editor, </editor> <booktitle> Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <address> Utica, Illinois, </address> <year> 1992. </year>
Reference-contexts: Widely used decision tree algorithms such as CART [6] and C4.5 [16] are greedy. They recursively construct a tree by creating univariate decisions one node at a time. Greedy multivariate decisions have also proven very effective for classification problems <ref> [1, 7, 14] </ref>. In this paper, we investigate a nonparametric nongreedy algorithm to construct multivariate decision trees. The proposed approach allows us to optimize all of the decisions in the tree simultaneously. <p> The goal of the first experiment was to evaluate the effectiveness of EPTS on GTO using the three different objective functions and generated data. To test how well EPTS could improve an existing tree, an initial tree was generated using the greedy MSMT decision tree algorithm <ref> [1] </ref>. MSMT used a linear program to recursively construct the decisions. The tree was then pruned to three decisions. This tree was used to start EPTS.
Reference: [2] <author> K. P. Bennett. </author> <title> Global tree optimization: A non-greedy decision tree algorithm. </title> <journal> Computing Science and Statistics, </journal> <volume> 26 </volume> <pages> 156-160, </pages> <year> 1994. </year>
Reference-contexts: Unlike the Bayesian or parametric approaches [17, 8, 12], the strict logical rules of the tree are maintained. We consider the decision tree as a set of disjunctive linear inequalities. A Frank-Wolfe algorithm has been used to minimize the error in the disjunctive inequalities <ref> [5, 2] </ref>. The Frank-Wolfe algorithm is a descent technique that stops at the first local minimum it encounters. EPTS is a global optimization technique. While it is not guaranteed to find the global minimum, it is much more robust than the Frank-Wolfe algorithm. <p> However, we consider this situation undesirable and count it as an error when training. 2 Decision Trees as Disjunctive Inequalities The key idea to this approach is that a given multivariate decision tree can be represented as a set of disjunctive linear inequalities <ref> [5, 2, 3] </ref>. Each decision in the tree corresponds to a linear inequality. As a point traverses a path from the root of the tree to a classification leaf, each decision creates an inequality that must be satisfied. Several leaves may belong to a given class. <p> This same approach can be used for any binary multivariate decision tree with fixed structure <ref> [2, 3] </ref>. The trees may be of arbitrary depth and need not be symmetric. 3 Global Tree Error Functions The problem of determining whether there is a feasible solution to the disjunctive inequalities can be converted to the GTO problem. <p> In the Frank-Wolfe algorithm, nonlinear problems are solved using a series of linear programs created by linearizing the objective function. A Frank-Wolfe method was successfully applied to variants of problem (7) <ref> [5, 2] </ref>. However, the Frank-Wolfe algorithm is a descent method that stops at the first local minimum encountered and it is limited to differentiable functions. We would like a more robust approach that can be used for nondifferentiable objectives.
Reference: [3] <author> K. P. Bennett. </author> <title> Optimal decision trees through multilinear programming. </title> <type> Math Report 214, </type> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, New York, </address> <year> 1996. </year> <note> Revised. 9 </note>
Reference-contexts: However, we consider this situation undesirable and count it as an error when training. 2 Decision Trees as Disjunctive Inequalities The key idea to this approach is that a given multivariate decision tree can be represented as a set of disjunctive linear inequalities <ref> [5, 2, 3] </ref>. Each decision in the tree corresponds to a linear inequality. As a point traverses a path from the root of the tree to a classification leaf, each decision creates an inequality that must be satisfied. Several leaves may belong to a given class. <p> This same approach can be used for any binary multivariate decision tree with fixed structure <ref> [2, 3] </ref>. The trees may be of arbitrary depth and need not be symmetric. 3 Global Tree Error Functions The problem of determining whether there is a feasible solution to the disjunctive inequalities can be converted to the GTO problem. <p> 1 (y 1 (y 2 l ) i A i w 3 fl 3 + 1 l ) j B j w 1 fl 1 + 1 (z 1 (z 2 g ) j B j w 2 + fl 3 + 1 (7) Problem (7) has some interesting properties <ref> [3] </ref>. The objective with p = 1 is quasiconcave and differentiable. At optimality y and z are exactly equal to the point error. In addition there exists an optimal solution which is an extreme point of the polyhedral region. <p> It consistently outperformed FW and the other objectives on our experiments. Note however that no method consistently achieved 100% accuracy on the training set even though such a solution existed. This is not surprising since the underlying problem is NP-complete <ref> [3] </ref>. The current implementation of EPTS is surprisingly effective considering the simplicity of the algorithm. We are currently incorporating additional ideas from TS such as long term memory using frequency lists in order to make EPTS even more robust. EPTS was also evaluated on larger random problems and real-world datasets.
Reference: [4] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: inequalities become A i w fl 1 i = 1; : : : ; m A and B j w fl 1 j = 1; : : : ; m B (1) We can easily determine in polynomial time if such a w and fl exist using a linear program <ref> [4] </ref>.
Reference: [5] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Bilinear separation of two sets in n-space. </title> <journal> Computational Optimization and Applications, </journal> <volume> 2 </volume> <pages> 207-227, </pages> <year> 1993. </year>
Reference-contexts: Unlike the Bayesian or parametric approaches [17, 8, 12], the strict logical rules of the tree are maintained. We consider the decision tree as a set of disjunctive linear inequalities. A Frank-Wolfe algorithm has been used to minimize the error in the disjunctive inequalities <ref> [5, 2] </ref>. The Frank-Wolfe algorithm is a descent technique that stops at the first local minimum it encounters. EPTS is a global optimization technique. While it is not guaranteed to find the global minimum, it is much more robust than the Frank-Wolfe algorithm. <p> However, we consider this situation undesirable and count it as an error when training. 2 Decision Trees as Disjunctive Inequalities The key idea to this approach is that a given multivariate decision tree can be represented as a set of disjunctive linear inequalities <ref> [5, 2, 3] </ref>. Each decision in the tree corresponds to a linear inequality. As a point traverses a path from the root of the tree to a classification leaf, each decision creates an inequality that must be satisfied. Several leaves may belong to a given class. <p> In the Frank-Wolfe algorithm, nonlinear problems are solved using a series of linear programs created by linearizing the objective function. A Frank-Wolfe method was successfully applied to variants of problem (7) <ref> [5, 2] </ref>. However, the Frank-Wolfe algorithm is a descent method that stops at the first local minimum encountered and it is limited to differentiable functions. We would like a more robust approach that can be used for nondifferentiable objectives.
Reference: [6] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International, </booktitle> <address> California, </address> <year> 1984. </year>
Reference-contexts: 1 Introduction Decision trees have proven to be a very effective technique for data mining and classification problems. Widely used decision tree algorithms such as CART <ref> [6] </ref> and C4.5 [16] are greedy. They recursively construct a tree by creating univariate decisions one node at a time. Greedy multivariate decisions have also proven very effective for classification problems [1, 7, 14]. In this paper, we investigate a nonparametric nongreedy algorithm to construct multivariate decision trees.
Reference: [7] <author> C. E. Brodley and P. E. Utgoff. </author> <title> Multivariate decision trees. </title> <journal> Machine Learning, </journal> <volume> 19(1) </volume> <pages> 45-77, </pages> <year> 1995. </year>
Reference-contexts: Widely used decision tree algorithms such as CART [6] and C4.5 [16] are greedy. They recursively construct a tree by creating univariate decisions one node at a time. Greedy multivariate decisions have also proven very effective for classification problems <ref> [1, 7, 14] </ref>. In this paper, we investigate a nonparametric nongreedy algorithm to construct multivariate decision trees. The proposed approach allows us to optimize all of the decisions in the tree simultaneously.
Reference: [8] <author> W. Buntine. </author> <title> Learning classification trees. </title> <booktitle> In In Artificial Intelligence Frontiers in Statistics: AI and Statistics III, </booktitle> <pages> pages 183-201, </pages> <address> London, 1993. </address> <publisher> Chapman and Hall. </publisher>
Reference-contexts: The flexibility of the method is very promising. We are just beginning to explore these possibilities. Our nonparametric EPTS method for decision tree construction is fundamentally different from the few prior nongreedy algorithms. Unlike the Bayesian or parametric approaches <ref> [17, 8, 12] </ref>, the strict logical rules of the tree are maintained. We consider the decision tree as a set of disjunctive linear inequalities. A Frank-Wolfe algorithm has been used to minimize the error in the disjunctive inequalities [5, 2].
Reference: [9] <author> R. Detrano, A. Janosi, W. Steinbrunn, M. Pfisterer, J. Schmid, S. Sandhu, K. Guppy, S. Lee, and V. Froelicher. </author> <title> International application of a new probability algorithm for the diagnosis of coronary artery disease. </title> <journal> American Journal of Cardiology, </journal> <volume> 64 </volume> <pages> 304-310, </pages> <year> 1989. </year>
Reference-contexts: Then points were randomly generated in the unit cube and classified using the generated tree. The real-life data 2 consisted of: the BUPA Liver Disease dataset (Liver); the PIMA Indians Diabetes dataset (Diabetes), the Wisconsin Breast Cancer Database (Cancer) [18], and the Cleveland Heart Disease Database (Heart) <ref> [9] </ref>. The goal of the first experiment was to evaluate the effectiveness of EPTS on GTO using the three different objective functions and generated data. To test how well EPTS could improve an existing tree, an initial tree was generated using the greedy MSMT decision tree algorithm [1].
Reference: [10] <author> F. Glover. </author> <title> Tabu search part I. </title> <journal> ORSA Journal of Computing, </journal> <volume> 1(3) </volume> <pages> 190-206, </pages> <year> 1989. </year>
Reference-contexts: we have developed the EPTS algorithm discussed in the next section. 4 Extreme Point Tabu Search We have developed an Extreme Point Tabu Search (EPTS) to solve the GTO problem. 1 Tabu Search (TS) is a heuristic that has achieved great success on a wide variety of practical optimization problems <ref> [10] </ref>. A very basic TS search algorithm is described below. For each point in the search space a "neighborhood" is defined. The algorithm moves to the best neighbor of the current solution that is not "tabu".
Reference: [11] <author> F. Glover and A. Lokketangen. </author> <title> Probabilistic tabu search for zero-one mixed integer programming problems. </title> <type> Manuscript, </type> <institution> School of Business, University of Colorado, </institution> <year> 1994. </year>
Reference-contexts: However, our EPTS technique could be adapted to parametric formulations of the problem as well. This work is the first application of EPTS to classification problems. Glover and Lokketangen have applied Tabu Search (TS) using extreme points to integer programming <ref> [11] </ref>. The current implementation utilizes only the most basic ideas of TS. We expect further improvements in the algorithm as more advanced TS heuristics are incorporated. 2 This paper is organized as follows.
Reference: [12] <author> M. Jordan and R. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6(3) </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: The flexibility of the method is very promising. We are just beginning to explore these possibilities. Our nonparametric EPTS method for decision tree construction is fundamentally different from the few prior nongreedy algorithms. Unlike the Bayesian or parametric approaches <ref> [17, 8, 12] </ref>, the strict logical rules of the tree are maintained. We consider the decision tree as a set of disjunctive linear inequalities. A Frank-Wolfe algorithm has been used to minimize the error in the disjunctive inequalities [5, 2].
Reference: [13] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.4 user's guide. </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <year> 1993. </year>
Reference-contexts: The number of iterations that the variable remains tabu is called the tabu tenure. For the results presented here, we set the tabu tenure to q m=8. The tabu tenure is problem dependent. The linear programming package MINOS 5.4 <ref> [13] </ref> was customized to implement EPTS. 5 Computational Results Several series of experiments were performed applying EPTS to the seven node decision tree of Figure 3. EPTS was tested on two types of problems: randomly generated data with known solutions and real-life problems.
Reference: [14] <author> S. Murthy, S. Kasif, S. Salzberg, and R. Beigel. </author> <title> OC1: Randomized induction of oblique decision trees. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 322-327, </pages> <address> Boston, MA, 1993. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Widely used decision tree algorithms such as CART [6] and C4.5 [16] are greedy. They recursively construct a tree by creating univariate decisions one node at a time. Greedy multivariate decisions have also proven very effective for classification problems <ref> [1, 7, 14] </ref>. In this paper, we investigate a nonparametric nongreedy algorithm to construct multivariate decision trees. The proposed approach allows us to optimize all of the decisions in the tree simultaneously.
Reference: [15] <author> K. Murty. </author> <title> Linear Programming. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Note these properties hold for trees with any number of decisions. Specialized algorithms have been developed for searching the extreme points of polyhedral regions. We can take advantage of the numerically efficient methods for representing extreme points developed for the Simplex Method of linear programming <ref> [15] </ref>. In the Frank-Wolfe algorithm, nonlinear problems are solved using a series of linear programs created by linearizing the objective function. A Frank-Wolfe method was successfully applied to variants of problem (7) [5, 2].
Reference: [16] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction Decision trees have proven to be a very effective technique for data mining and classification problems. Widely used decision tree algorithms such as CART [6] and C4.5 <ref> [16] </ref> are greedy. They recursively construct a tree by creating univariate decisions one node at a time. Greedy multivariate decisions have also proven very effective for classification problems [1, 7, 14]. In this paper, we investigate a nonparametric nongreedy algorithm to construct multivariate decision trees.
Reference: [17] <author> P. Smyth, A. Gray, and U. Fayyad. </author> <title> Retrofitting decision tree classifiers using kernel density estimation. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 506-514, </pages> <address> Amherst, MA, </address> <year> 1995. </year>
Reference-contexts: The flexibility of the method is very promising. We are just beginning to explore these possibilities. Our nonparametric EPTS method for decision tree construction is fundamentally different from the few prior nongreedy algorithms. Unlike the Bayesian or parametric approaches <ref> [17, 8, 12] </ref>, the strict logical rules of the tree are maintained. We consider the decision tree as a set of disjunctive linear inequalities. A Frank-Wolfe algorithm has been used to minimize the error in the disjunctive inequalities [5, 2].
Reference: [18] <author> W. H. Wolberg and O.L. Mangasarian. </author> <title> Multisurface method of pattern separation for medical diagnosis applied to breast cytology. </title> <booktitle> Proceedings of the National Academy of Sciences,U.S.A., </booktitle> <volume> 87 </volume> <pages> 9193-9196, </pages> <year> 1990. </year> <month> 10 </month>
Reference-contexts: Then points were randomly generated in the unit cube and classified using the generated tree. The real-life data 2 consisted of: the BUPA Liver Disease dataset (Liver); the PIMA Indians Diabetes dataset (Diabetes), the Wisconsin Breast Cancer Database (Cancer) <ref> [18] </ref>, and the Cleveland Heart Disease Database (Heart) [9]. The goal of the first experiment was to evaluate the effectiveness of EPTS on GTO using the three different objective functions and generated data.
References-found: 18

