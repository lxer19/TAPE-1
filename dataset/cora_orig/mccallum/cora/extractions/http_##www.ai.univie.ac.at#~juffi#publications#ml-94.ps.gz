URL: http://www.ai.univie.ac.at/~juffi/publications/ml-94.ps.gz
Refering-URL: http://www.ai.univie.ac.at/~juffi/publications/publications.html
Root-URL: 
Email: juffi@ai.univie.ac.at  gerhard@ai.univie.ac.at  
Title: Incremental Reduced Error Pruning  
Author: Johannes F urnkranz Gerhard Widmer 
Address: Schottengasse 3 A-1010 Vienna, Austria  Schottengasse 3 A-1010 Vienna, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  Austrian Research Institute for Artificial Intelligence  
Abstract: This paper outlines some problems that may occur with Reduced Error Pruning in relational learning algorithms, most notably efficiency. Thereafter a new method, Incremental Reduced Error Pruning, is proposed that attempts to address all of these problems. Experiments show that in many noisy domains this method is much more efficient than alternative algorithms, along with a slight gain in accuracy. However, the experiments show as well that the use of the algorithm cannot be recommended for domains which require a very specific concept description.
Abstract-found: 1
Intro-found: 1
Reference: [Ali and Pazzani, 1993] <author> Kamal M. Ali and Michael J. Paz-zani. HYDRA: </author> <title> A noise-tolerant relational concept learning algorithm. </title> <booktitle> In Proceedings of the Thirteenth Joint International Conference on Artificial Intelligence, </booktitle> <pages> pages 1064-1071, </pages> <address> Chamb ery, France, </address> <year> 1993. </year>
Reference-contexts: The Votes (VI) set is the Votes data set with the most significant attribute removed. In all of the propositional domains the equality relation was added as background knowledge. The Promoters data also included two background relations specifying that the 4 DNA bases can be split into 2 groups <ref> [Ali and Pazzani, 1993] </ref>, and in the KRKN data the &lt; relation was added for the 6 integer valued attributes.
Reference: [Bratko and Kononenko, 1986] <author> Ivan Bratko and Igor Kononenko. </author> <title> Learning diagnostic rules from incomplete and noisy data. </title> <editor> In B. Phelps, editor, </editor> <booktitle> Interactions in AI and Statistical Methods, </booktitle> <pages> pages 142-153, </pages> <address> London, </address> <year> 1986. </year> <note> delete-any-literal needs to be performed. </note>
Reference-contexts: Not surprisingly, noise handling methods have also entered the emerging field of Inductive Logic Programming (ILP) [Muggleton, 1992]. LINUS [Lavrac and D zeroski, 1992] relies directly on the noise handling abilities of decision tree learning algorithms like CN2 [Clark and Niblett, 1989, Clark and Boswell, 1991] or ASSISTANT <ref> [Bratko and Kononenko, 1986] </ref>. Others, like mFOIL [D zeroski and Bratko, 1992], have adapted some of these well-known methods from attribute-value learning for the ILP framework. Pruning is a standard way of dealing with noise in decision tree learning (for an overview see [Mingers, 1989] or [Esposito et al., 1993]).
Reference: [Breiman et al., 1984] <author> L. Breiman, J. Friedman, R. Ol--shen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks, </publisher> <address> Pacific Grove, CA, </address> <year> 1984. </year>
Reference-contexts: 1 INTRODUCTION Being able to deal with noisy data is a must for algorithms that are meant to learn concepts in real-world domains. Significant effort has gone into investigating the effect of noisy data on decision tree learning algorithms (see e.g. <ref> [Breiman et al., 1984, Quinlan, 1993] </ref>). Not surprisingly, noise handling methods have also entered the emerging field of Inductive Logic Programming (ILP) [Muggleton, 1992]. <p> Post-Pruning means that first a concept description is generated that perfectly explains all training instances. This theory will subsequently be generalized by cutting off branches of the decision tree (as in [Quinlan, 1987] or <ref> [Breiman et al., 1984] </ref>). In Inductive Logic Programming, pre-pruning has been common in the form of stopping criteria as used in FOIL [Quinlan, 1990], mFOIL [D zeroski and Bratko, 1992], or FOSSIL [F urnkranz, 1994a].
Reference: [Brunk and Pazzani, 1991] <author> Clifford A. Brunk and Michael J. Pazzani. </author> <title> An investigation of noise-tolerant relational concept learning algorithms. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 389-393, </pages> <address> Evanston, Illinois, </address> <year> 1991. </year>
Reference-contexts: In Inductive Logic Programming, pre-pruning has been common in the form of stopping criteria as used in FOIL [Quinlan, 1990], mFOIL [D zeroski and Bratko, 1992], or FOSSIL [F urnkranz, 1994a]. Post-pruning was introduced to ILP with Reduced Error Pruning (REP) <ref> [Brunk and Pazzani, 1991] </ref> based on ideas by [Quinlan, 1987] and [Pagallo and Haussler, 1990]. First the training set is split into two subsets: a growing set and a pruning set. A concept description explaining all of the examples in the growing set is generated with a relational learning algorithm. <p> In section 4 we propose Incremental Reduced Error Pruning a method that integrates pre- and post-pruning as an alternative solution. Section 5 then reports some experiments with two versions of this algorithm. 2 SOME PROBLEMS WITH REDUCED ERROR PRUNING Reduced Error Pruning (REP) <ref> [Brunk and Pazzani, 1991] </ref> has proven to be quite effective in raising predictive accuracy in noisy domains. <p> For comparison, REP and GROW were implemented as described in [Cohen, 1993] with the exception that delete-last-literal was used as a clause pruning operator (as in <ref> [Brunk and Pazzani, 1991] </ref>) instead of Co hen's operator that deletes a final sequence of literals from a clause. 2 All algorithms were implemented in Sicstus PROLOG and had major parts of their implementations in common. <p> The approach builds upon ideas introduced by <ref> [Brunk and Pazzani, 1991] </ref> and [Cohen, 1993], but improves upon them in the following ways: Efficiency: [Cohen, 1993] has shown that the complexity of REP is W (n 4 ) on random data and has proposed an alternative algorithm GROW with time complexity W (n 2 log n). <p> run-time improvement over REP and GROW, although I-REP uses the more expensive, but presumably more powerful pruning operator delete-any-literal. 4 4 Preliminary experiments using delete-any-literal in the GROW algorithm indicate that its usage may not only result in an increase in run-time, but surprisingly also in a decrease in accuracy. <ref> [Brunk and Pazzani, 1991] </ref> also claim that in FOIL and similar systems the more expensive delete-any-literal operator is not needed, because of the order in which the literals are added to the body of a clause.
Reference: [Cestnik et al., 1987] <author> Bojan Cestnik, Igor Kononenko, and Ivan Bratko. </author> <title> ASSISTANT 86: A knowledge-elicitation tool for sophisticated users. </title> <editor> In Ivan Bratko and Nada Lavrac, editors, </editor> <booktitle> Progress in Machine Learning, </booktitle> <pages> pages 31-45, </pages> <address> Wilmslow, England, 1987. </address> <publisher> Sigma Press. </publisher>
Reference-contexts: of overfitting by learning an overly general concept from the training set in order to improve the prediction on unseen instances. fl This author is also affiliated with the Department of Medical Cybernetics and Artificial Intelligence at the University of Vienna There are two fundamentally different approaches to prun ing <ref> [Cestnik et al., 1987] </ref>: Pre-Pruning means that during concept generation some training examples are deliberately ignored, so that the final concept description does not classify all training instances correctly. Post-Pruning means that first a concept description is generated that perfectly explains all training instances.
Reference: [Clark and Boswell, 1991] <author> Peter Clark and Robin Boswell. </author> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proceedings of the 5th European Working Session of Learning, </booktitle> <pages> pages 151-163, </pages> <address> Porto, Portugal, </address> <year> 1991. </year>
Reference-contexts: Not surprisingly, noise handling methods have also entered the emerging field of Inductive Logic Programming (ILP) [Muggleton, 1992]. LINUS [Lavrac and D zeroski, 1992] relies directly on the noise handling abilities of decision tree learning algorithms like CN2 <ref> [Clark and Niblett, 1989, Clark and Boswell, 1991] </ref> or ASSISTANT [Bratko and Kononenko, 1986]. Others, like mFOIL [D zeroski and Bratko, 1992], have adapted some of these well-known methods from attribute-value learning for the ILP framework.
Reference: [Clark and Niblett, 1989] <author> Peter Clark and Tim Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 261-283, </pages> <year> 1989. </year>
Reference-contexts: Not surprisingly, noise handling methods have also entered the emerging field of Inductive Logic Programming (ILP) [Muggleton, 1992]. LINUS [Lavrac and D zeroski, 1992] relies directly on the noise handling abilities of decision tree learning algorithms like CN2 <ref> [Clark and Niblett, 1989, Clark and Boswell, 1991] </ref> or ASSISTANT [Bratko and Kononenko, 1986]. Others, like mFOIL [D zeroski and Bratko, 1992], have adapted some of these well-known methods from attribute-value learning for the ILP framework.
Reference: [Cohen, 1993] <author> William W. Cohen. </author> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 988-994, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: However, this approach has several disadvantages, which we will highlight in section 2. Section 3 briefly presents the approach of <ref> [Cohen, 1993] </ref> designed to solve some of these problems. In section 4 we propose Incremental Reduced Error Pruning a method that integrates pre- and post-pruning as an alternative solution. <p> However, this method has several shortcomings, which we will discuss in this section. 2.1 EFFICIENCY In <ref> [Cohen, 1993] </ref> it was shown that the worst-case time complexity of REP is as bad as W (n 4 ) on random data (n is the number of examples). The growing of the initial concept, on the other hand, is only W (n 2 log n). <p> The growing of the initial concept, on the other hand, is only W (n 2 log n). The derivation of these numbers as given in <ref> [Cohen, 1993] </ref> rests on the assumption that for random and thus incompressible data the concept description after the growing phase will contain about 1 rule for each example (n rules altogether), each of them having about log n conditions, because each literal will cover about half of the random instances. [Cohen, <p> <ref> [Cohen, 1993] </ref> rests on the assumption that for random and thus incompressible data the concept description after the growing phase will contain about 1 rule for each example (n rules altogether), each of them having about log n conditions, because each literal will cover about half of the random instances. [Cohen, 1993] further assumes that the size of the final (pruned) theory will not depend on the number of training examples, i.e. will be constant. <p> Therefore we get a total cost of W (n 4 ). A detailed proof can be found in <ref> [Cohen, 1993] </ref>. It has also been pointed out there that this result for random data generalizes to data containing noise, i.e. a constant fraction of random and incompressible data. <p> This method succeeded in improving both run-time and accuracy of REP. 3 COHEN'S GROW ALGORITHM In <ref> [Cohen, 1993] </ref> several of the problems of section 2 in particular efficiency have been recognized. Cohen has then proposed a pruning algorithm based on the technique used in the GROVE learning system [Pagallo and Haussler, 1990]. Like REP, GROW first finds a theory that overfits the data. <p> The generalizations of the clauses of the intermediate theory are formed by repeatedly deleting a final sequence of conditions from the clause so that the error on the growing set goes up the least. For more details see <ref> [Cohen, 1993] </ref>. <p> This is repeated until all clauses in the final concept description (which is assumed to be of constant size) have been found. Therefore the costs of this algorithm are O (n 2 log n). Again, consult <ref> [Cohen, 1993] </ref> for a detailed proof. * GROW replaces the bottom-up hill-climbing search of REP (see section 2.4) by a top-down approach. <p> For comparison, REP and GROW were implemented as described in <ref> [Cohen, 1993] </ref> with the exception that delete-last-literal was used as a clause pruning operator (as in [Brunk and Pazzani, 1991]) instead of Co hen's operator that deletes a final sequence of literals from a clause. 2 All algorithms were implemented in Sicstus PROLOG and had major parts of their implementations in <p> The approach builds upon ideas introduced by [Brunk and Pazzani, 1991] and <ref> [Cohen, 1993] </ref>, but improves upon them in the following ways: Efficiency: [Cohen, 1993] has shown that the complexity of REP is W (n 4 ) on random data and has proposed an alternative algorithm GROW with time complexity W (n 2 log n). <p> The approach builds upon ideas introduced by [Brunk and Pazzani, 1991] and <ref> [Cohen, 1993] </ref>, but improves upon them in the following ways: Efficiency: [Cohen, 1993] has shown that the complexity of REP is W (n 4 ) on random data and has proposed an alternative algorithm GROW with time complexity W (n 2 log n). <p> Similar problems may occur with small training sets. In the near future I-REP should be adapted to be capable of dealing with numeric data and multi-valued classes to allow a test in a broader variety of real-world domains as in <ref> [Cohen, 1993] </ref>. A direct comparison of our results and the work reported there is not possible, because Cohen's experiments were performed with a propositional learning algorithm, while in our experiments relations like equality were made available as background knowledge to all learners.
Reference: [Dol sak and Muggleton, 1992] <author> Bojan Dol sak and Stephen Muggleton. </author> <title> The application of Inductive Logic Programming to finite-element mesh design. </title> <editor> In Stephen Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 453-472. </pages> <publisher> Academic Press Ltd., </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: Tests were performed for most of the datasets in the UCI Machine Learning repository that had only two classes and only symbolic attributes, and also for the relational KRK [Muggleton et al., 1989] and Mesh <ref> [Dol sak and Muggleton, 1992] </ref> domains. The training and test data were the same for all algorithms. Table 1 gives an overview of the databases used along with a comparison of the run-times of the different algorithms.
Reference: [D zeroski and Bratko, 1992] <author> Sa so D zeroski and Ivan Bratko. </author> <title> Handling noise in Inductive Logic Programming. </title> <booktitle> In Proceedings of the International Workshop on Inductive Logic Programming, </booktitle> <address> Tokyo, Japan, </address> <year> 1992. </year>
Reference-contexts: LINUS [Lavrac and D zeroski, 1992] relies directly on the noise handling abilities of decision tree learning algorithms like CN2 [Clark and Niblett, 1989, Clark and Boswell, 1991] or ASSISTANT [Bratko and Kononenko, 1986]. Others, like mFOIL <ref> [D zeroski and Bratko, 1992] </ref>, have adapted some of these well-known methods from attribute-value learning for the ILP framework. Pruning is a standard way of dealing with noise in decision tree learning (for an overview see [Mingers, 1989] or [Esposito et al., 1993]). <p> This theory will subsequently be generalized by cutting off branches of the decision tree (as in [Quinlan, 1987] or [Breiman et al., 1984]). In Inductive Logic Programming, pre-pruning has been common in the form of stopping criteria as used in FOIL [Quinlan, 1990], mFOIL <ref> [D zeroski and Bratko, 1992] </ref>, or FOSSIL [F urnkranz, 1994a]. Post-pruning was introduced to ILP with Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] based on ideas by [Quinlan, 1987] and [Pagallo and Haussler, 1990]. <p> The best result (s) in each line are emphasized. On the Votes and Promoters data a 10-fold cross-validation was performed. The Mesh data were tested in 5 runs as described in <ref> [D zeroski and Bratko, 1992] </ref>, but classification accuracy on negative examples was measured as well.
Reference: [Esposito et al., 1993] <author> Floriana Esposito, Donato Malerba, and Giovanni Semeraro. </author> <title> Decision tree pruning as a search in the state space. </title> <booktitle> In Proceedings of the Euro-pean Conference on Machine Learning, </booktitle> <pages> pages 165-184, </pages> <address> Vienna, Austria, 1993. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Others, like mFOIL [D zeroski and Bratko, 1992], have adapted some of these well-known methods from attribute-value learning for the ILP framework. Pruning is a standard way of dealing with noise in decision tree learning (for an overview see [Mingers, 1989] or <ref> [Esposito et al., 1993] </ref>).
Reference: [F urnkranz, 1994a] <author> Johannes F urnkranz. FOSSIL: </author> <title> A robust relational learner. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 122-137, </pages> <address> Catania, Italy, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In Inductive Logic Programming, pre-pruning has been common in the form of stopping criteria as used in FOIL [Quinlan, 1990], mFOIL [D zeroski and Bratko, 1992], or FOSSIL <ref> [F urnkranz, 1994a] </ref>. Post-pruning was introduced to ILP with Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] based on ideas by [Quinlan, 1987] and [Pagallo and Haussler, 1990]. First the training set is split into two subsets: a growing set and a pruning set.
Reference: [F urnkranz, 1994b] <author> Johannes F urnkranz. </author> <title> Top-down pruning in relational learning. </title> <booktitle> In Proceedings of the 11th European Conference on Artificial Intelligence, </booktitle> <pages> pages 453-457, </pages> <address> Amsterdam, The Netherlands, </address> <year> 1994. </year>
Reference-contexts: Therefore REP's specific-to-general search can be expected to be slow and imprecise for noisy data, because it has to prune a significant portion of the theory previously generated in the growing phase and is likely to stop at a local maximum during this process. <ref> [F urnkranz, 1994b] </ref> reports experiments with an algorithm that is able to find a starting theory much closer to the final theory than the most specific theory. <p> Consequently Cohen tried to improve GROW by adding two stopping heuristics to the initial stage of overfitting, and thus achieved a further speed-up of the algorithm. Another way of combining pre-pruning and post-pruning methods to get better results can be found in <ref> [F urnkranz, 1994b] </ref>.
Reference: [Lavrac and D zeroski, 1992] <author> Nada Lavrac and Sa so D zeroski. </author> <title> Inductive learning of relations from noisy examples. </title> <editor> In Stephen Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 495-516. </pages> <publisher> Academic Press Ltd., </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: Significant effort has gone into investigating the effect of noisy data on decision tree learning algorithms (see e.g. [Breiman et al., 1984, Quinlan, 1993]). Not surprisingly, noise handling methods have also entered the emerging field of Inductive Logic Programming (ILP) [Muggleton, 1992]. LINUS <ref> [Lavrac and D zeroski, 1992] </ref> relies directly on the noise handling abilities of decision tree learning algorithms like CN2 [Clark and Niblett, 1989, Clark and Boswell, 1991] or ASSISTANT [Bratko and Kononenko, 1986].
Reference: [Mingers, 1989] <author> John Mingers. </author> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 227-243, </pages> <year> 1989. </year>
Reference-contexts: Others, like mFOIL [D zeroski and Bratko, 1992], have adapted some of these well-known methods from attribute-value learning for the ILP framework. Pruning is a standard way of dealing with noise in decision tree learning (for an overview see <ref> [Mingers, 1989] </ref> or [Esposito et al., 1993]).
Reference: [Muggleton et al., 1989] <author> Stephen Muggleton, Michael Bain, Jean Hayes-Michie, and Donald Michie. </author> <title> An experimental comparison of human and machine learning formalisms. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pages 113-118, </pages> <year> 1989. </year>
Reference-contexts: Tests were performed for most of the datasets in the UCI Machine Learning repository that had only two classes and only symbolic attributes, and also for the relational KRK <ref> [Muggleton et al., 1989] </ref> and Mesh [Dol sak and Muggleton, 1992] domains. The training and test data were the same for all algorithms. Table 1 gives an overview of the databases used along with a comparison of the run-times of the different algorithms.
Reference: [Muggleton, 1992] <author> Stephen Muggleton, </author> <title> editor. Inductive Logic Programming. </title> <publisher> Academic Press Ltd., </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: Significant effort has gone into investigating the effect of noisy data on decision tree learning algorithms (see e.g. [Breiman et al., 1984, Quinlan, 1993]). Not surprisingly, noise handling methods have also entered the emerging field of Inductive Logic Programming (ILP) <ref> [Muggleton, 1992] </ref>. LINUS [Lavrac and D zeroski, 1992] relies directly on the noise handling abilities of decision tree learning algorithms like CN2 [Clark and Niblett, 1989, Clark and Boswell, 1991] or ASSISTANT [Bratko and Kononenko, 1986].
Reference: [Pagallo and Haussler, 1990] <author> Giulia Pagallo and David Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 71-99, </pages> <year> 1990. </year>
Reference-contexts: Post-pruning was introduced to ILP with Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] based on ideas by [Quinlan, 1987] and <ref> [Pagallo and Haussler, 1990] </ref>. First the training set is split into two subsets: a growing set and a pruning set. A concept description explaining all of the examples in the growing set is generated with a relational learning algorithm. <p> This means that the training set is split into disjoint sets according to the outcome of the test chosen for the top level decision. After this, the algorithm is recursively applied to each of these sets independently. Greedy covering algorithms like FOIL follow a separate-and-conquer strategy <ref> [Pagallo and Haussler, 1990] </ref>. This method first learns a rule from the whole training set and subsequently removes all examples that are covered by this rule. Then the algorithm recursively tries to find rules that explain the remaining examples. <p> This method succeeded in improving both run-time and accuracy of REP. 3 COHEN'S GROW ALGORITHM In [Cohen, 1993] several of the problems of section 2 in particular efficiency have been recognized. Cohen has then proposed a pruning algorithm based on the technique used in the GROVE learning system <ref> [Pagallo and Haussler, 1990] </ref>. Like REP, GROW first finds a theory that overfits the data.
Reference: [Quinlan, 1987] <author> John Ross Quinlan. </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27 </volume> <pages> 221-234, </pages> <year> 1987. </year>
Reference-contexts: Post-Pruning means that first a concept description is generated that perfectly explains all training instances. This theory will subsequently be generalized by cutting off branches of the decision tree (as in <ref> [Quinlan, 1987] </ref> or [Breiman et al., 1984]). In Inductive Logic Programming, pre-pruning has been common in the form of stopping criteria as used in FOIL [Quinlan, 1990], mFOIL [D zeroski and Bratko, 1992], or FOSSIL [F urnkranz, 1994a]. <p> In Inductive Logic Programming, pre-pruning has been common in the form of stopping criteria as used in FOIL [Quinlan, 1990], mFOIL [D zeroski and Bratko, 1992], or FOSSIL [F urnkranz, 1994a]. Post-pruning was introduced to ILP with Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] based on ideas by <ref> [Quinlan, 1987] </ref> and [Pagallo and Haussler, 1990]. First the training set is split into two subsets: a growing set and a pruning set. A concept description explaining all of the examples in the growing set is generated with a relational learning algorithm.
Reference: [Quinlan, 1990] <author> John Ross Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: This theory will subsequently be generalized by cutting off branches of the decision tree (as in [Quinlan, 1987] or [Breiman et al., 1984]). In Inductive Logic Programming, pre-pruning has been common in the form of stopping criteria as used in FOIL <ref> [Quinlan, 1990] </ref>, mFOIL [D zeroski and Bratko, 1992], or FOSSIL [F urnkranz, 1994a]. Post-pruning was introduced to ILP with Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] based on ideas by [Quinlan, 1987] and [Pagallo and Haussler, 1990].
Reference: [Quinlan, 1993] <author> John Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: 1 INTRODUCTION Being able to deal with noisy data is a must for algorithms that are meant to learn concepts in real-world domains. Significant effort has gone into investigating the effect of noisy data on decision tree learning algorithms (see e.g. <ref> [Breiman et al., 1984, Quinlan, 1993] </ref>). Not surprisingly, noise handling methods have also entered the emerging field of Inductive Logic Programming (ILP) [Muggleton, 1992].
Reference: [Schaffer, 1993] <author> Cullen Schaffer. </author> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178, </pages> <year> 1993. </year>
Reference-contexts: If this accuracy is not estimated accurately, either because there are not enough remaining examples or because of a bad split, I-REP will be prone to over-generalization. Using the terminology of <ref> [Schaffer, 1993] </ref>, I-REP has a strong Overfitting Avoidance Bias, which can be detrimental in some domains. 5 EXPERIMENTS 5.1 IMPLEMENTATIONS OF THE ALGORITHMS We have tested two different implementations of I-REP, which differ in the way they prune the clauses (let p (n) be the number of positive (negative) examples covered <p> I-REP had the worst classification accuracy on these domains. The reason for this is that I-REP's top-down approach to pruning has an even stronger Overfitting Avoidance Bias than GROW, which can be inappropriate in some domains <ref> [Schaffer, 1993] </ref>. I-REP-2 in general seems to be worse than I-REP. Its purity criterion for evaluating a clause seems to have a preference for more specific clauses than I-REP (which can also be seen from the higher run-times).
References-found: 22

