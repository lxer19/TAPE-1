URL: http://www.cs.umn.edu/Users/dept/users/kumar/performance.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: Email: agupta@cs.umn.edu and kumar@cs.umn.edu  
Title: Performance Properties of Large Scale Parallel Systems  
Author: Anshul Gupta and Vipin Kumar 
Note: This work was supported by IST/SDIO through the Army Research Office grant 28408-MA-SDI to the University of Minnesota and by the University of Minnesota Army High Performance Computing Research Center under contract DAAL03-89-C-0038.  
Date: TR 92-32, September 1992 (Revised June 1993)  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science, University of Minnesota  
Abstract: To appear in Journal of Parallel and Distributed Computing, October 1993. A preliminary version of this paper appears in the Proceedings of the 26th Hawaii International Conference on System Sciences, 1993. Abstract There are several metrics that characterize the performance of a parallel system, such as, parallel execution time, speedup and efficiency. A number of properties of these metrics have been studied. For example, it is a well known fact that given a parallel architecture and a problem of a fixed size, the speedup of a parallel algorithm does not continue to increase with increasing number of processors. It usually tends to saturate or peak at a certain limit. Thus it may not be useful to employ more than an optimal number of processors for solving a problem on a parallel computer. This optimal number of processors depends on the problem size, the parallel algorithm and the parallel architecture. In this paper we study the impact of parallel processing overheads and the degree of concurrency of a parallel algorithm on the optimal number of processors to be used when the criterion for optimality is minimizing the parallel execution time. We then study a more general criterion of optimality and show how operating at the optimal point is equivalent to operating at a unique value of efficiency which is characteristic of the criterion of optimality and the properties of the parallel system under study. We put the technical results derived in this paper in perspective with similar results that have appeared in the literature before and show how this paper generalizes and/or extends these earlier results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: For example, for multiplying two N fi N matrices using Fox's parallel matrix multiplication algorithm [9], W = N 3 and C (W ) = N 2 = W 2=3 . It is easily seen that if the processor-time product <ref> [1] </ref> is fi (W ) (i.e., the algorithm is cost-optimal), then C (W ) fi (W ). 4 Maximum Number of Processors Usable, p max : The number of processors that yield max- imum speedup S max for a given W .
Reference: [2] <author> G. M. </author> <title> Amdahl. Validity of the single processor approach to achieving large scale computing capabilities. </title> <booktitle> In AFIPS Conference Proceedings, </booktitle> <pages> pages 483-485, </pages> <year> 1967. </year> <month> 17 </month>
Reference-contexts: As early as in 1967, Amdahl <ref> [2] </ref> made the observation that if s is the serial fraction in an algorithm, then its speedup for a fixed size problem is bounded by 1 s , no matter how many processors are used.
Reference: [3] <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice--Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: This is true only under the assumption that T o &lt; fi (p). For this assumption to be true, the parallel system has to be devoid of any global operation (such as broadcast, and one-to-all and all-to-all personalized communication <ref> [3, 19] </ref>) with a message passing latency or message startup time. The reason is that such operations always lead to T o fi (p).
Reference: [4] <author> E. A. Carmona and M. D. Rice. </author> <title> A model of parallel performance. </title> <type> Technical Report AFWL-TR-89-01, </type> <institution> Air Force Weapons Laboratory, </institution> <year> 1989. </year>
Reference: [5] <author> E. A. Carmona and M. D. Rice. </author> <title> Modeling the serial and parallel fractions of a parallel algorithm. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <year> 1991. </year>
Reference: [6] <author> D. L. Eager, J. Zahorjan, and E. D. Lazowska. </author> <title> Speedup versus efficiency in parallel systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(3) </volume> <pages> 408-423, </pages> <year> 1989. </year>
Reference-contexts: Hence they suggest that the number of processors should be chosen to maximize the product of efficiency and speedup. Flatt and Kennedy, and Tang and Li [33] also suggest maximizing a weighted geometric mean of efficiency and speedup. Eager et. al. <ref> [6] </ref> proposed that an optimal operating point should be chosen such the efficiency of execution is roughly 0.5. Many of the results presented in this paper are extensions, and in some cases, generalizations of the results of the above mentioned authors. <p> Therefore, in order to achieve a balance between speedup and efficiency, several researchers have proposed to operate at a point where the value of p (T P ) r is minimized for some constant r (r 1) and for a given problem size W <ref> [8, 6, 33] </ref>. <p> Eager et. al. <ref> [6] </ref> and Tang and Li [33] have proposed a criterion of optimality different from optimal speedup. They argue that the optimal operating point should be chosen so that a balance is struck between efficiency and speedup. It is proposed in [6] that the "knee" of the execution time verses efficiency curve <p> Eager et. al. <ref> [6] </ref> and Tang and Li [33] have proposed a criterion of optimality different from optimal speedup. They argue that the optimal operating point should be chosen so that a balance is struck between efficiency and speedup. It is proposed in [6] that the "knee" of the execution time verses efficiency curve is a good choice of the operating point because at this point the incremental benefit of adding processors is roughly 1 2 per processor, or, in other words, efficiency is 0.5. <p> Eager et. al. and Tang and Li also conclude that for T o = fi (p), this is also equivalent to operating at a point where the ES product is maximum or p (T P ) 2 is minimum. This conclusion in <ref> [6, 33] </ref> is a special case of the more general case that is captured in Equation (9). <p> If we substitute x j = 1 in Equation (9) (which is the case if T o = fi (p)), it can seen that we indeed get an efficiency of 0.5 for r = 2. In general, operating at the optimal point or the "knee" referred to in <ref> [6] </ref> and [33] for a parallel system with T o = fi (p x j ) will be identical to operating at a point where p (T P ) r is minimum, where r = 2 2x j . This is obtained from Equation (9) for E = 0:5.
Reference: [7] <author> Horace P. Flatt. </author> <title> Further applications of the overhead model for parallel systems. </title> <type> Technical Report G320-3540, </type> <institution> IBM Corporation, Palo Alto Scientific Center, </institution> <address> Palo Alto, CA, </address> <year> 1990. </year>
Reference-contexts: Worley [37] showed that for a class of parallel algorithms, if the parallel execution time is fixed, then there exists a problem size which cannot be solved in that fixed time no matter how many processors are used. Flatt and Kennedy <ref> [8, 7] </ref> derived some important upper bounds related to the performance of parallel computers in the presence of synchronization and communication overheads. <p> Flatt and Kennedy <ref> [8, 7] </ref> show that if the overhead function satisfies certain mathematical properties, then there exists a unique value p 0 of the number of processors for which T P is minimum for a given W .
Reference: [8] <author> Horace P. Flatt and Ken Kennedy. </author> <title> Performance of parallel processors. </title> <journal> Parallel Computing, </journal> <volume> 12 </volume> <pages> 1-20, </pages> <year> 1989. </year>
Reference-contexts: Worley [37] showed that for a class of parallel algorithms, if the parallel execution time is fixed, then there exists a problem size which cannot be solved in that fixed time no matter how many processors are used. Flatt and Kennedy <ref> [8, 7] </ref> derived some important upper bounds related to the performance of parallel computers in the presence of synchronization and communication overheads. <p> Therefore, in order to achieve a balance between speedup and efficiency, several researchers have proposed to operate at a point where the value of p (T P ) r is minimized for some constant r (r 1) and for a given problem size W <ref> [8, 6, 33] </ref>. <p> Flatt and Kennedy <ref> [8, 7] </ref> show that if the overhead function satisfies certain mathematical properties, then there exists a unique value p 0 of the number of processors for which T P is minimum for a given W . <p> As seen in Section 3.1 and Example 1, there exist parallel systems for which do 1 T o , as defined in <ref> [8] </ref>, is the overhead incurred per processor when all costs are normalized with respect to W = 1. So in the light of the definition of T o in this paper, the actual mathematical condition of [8], that T o is an increasing nonnegative function of p, has been translated to <p> there exist parallel systems for which do 1 T o , as defined in <ref> [8] </ref>, is the overhead incurred per processor when all costs are normalized with respect to W = 1. So in the light of the definition of T o in this paper, the actual mathematical condition of [8], that T o is an increasing nonnegative function of p, has been translated to the condition that T o grows faster than fi (p). 15 not obey this condition, and in such cases the point of peak performance is determined by the degree of concurrency of the algorithm being used. <p> Equations (4) and (5) in this paper provide results similar to Flatt and Kennedy's. But the analysis in <ref> [8] </ref> tends to conclude the following - (i) if the overhead function grows very fast with respect to p, then p 0 is small, and hence parallel processing cannot provide substantial speedups; (ii) if the overhead function grows slowly (i.e., closer to fi (p)), then the overall efficiency is very poor
Reference: [9] <author> G. C. Fox, M. Johnson, G. Lyzenga, S. W. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors: Volume 1. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: Clearly, for a given W , the parallel algorithm can not use more than C (W ) processors. C (W ) depends only on the parallel algorithm, and is independent of the architecture. For example, for multiplying two N fi N matrices using Fox's parallel matrix multiplication algorithm <ref> [9] </ref>, W = N 3 and C (W ) = N 2 = W 2=3 .
Reference: [10] <author> Ananth Grama, Anshul Gupta, and Vipin Kumar. Isoefficiency: </author> <title> Measuring the scalability of parallel algorithms and architectures. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 1(3) </volume> <pages> 12-21, </pages> <month> August, </month> <year> 1993. </year> <note> Also available as Technical Report TR 93-24, </note> <institution> Department of Computer Science, University of Minnesota, Min-neapolis, MN. </institution>
Reference-contexts: that for a class of overhead functions, given any problem size, operating at the point of maximum speedup is equivalent to operating at a fixed efficiency and the relation between the problem size and the number of processors that yields maximum speedup is given by the isoefficiency metric of scalability <ref> [22, 10, 23] </ref>. Next, a criterion of optimality is described that is more general than just maximizing the speedup and similar results are derived under this new condition for choosing the optimal operating point. The organization of the paper is as follows. <p> The isoefficiency metric <ref> [22, 10, 23] </ref> comes in as a handy tool to study the fixed efficiency characteristics of a parallel system. The isoefficiency function relates the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors used. <p> As discussed in <ref> [22, 10] </ref>, the relation between the problem size and the maximum number of processors that can be used in a cost-optimal fashion for solving the problem is given by the isoefficiency function. Often, using as many processors as possible results in a non-cost-optimal system.
Reference: [11] <author> Ananth Grama, Vipin Kumar, and V. Nageshwara Rao. </author> <title> Experimental evaluation of load balancing techniques for the hypercube. </title> <booktitle> In Proceedings of the Parallel Computing '91 Conference, </booktitle> <pages> pages 497-514, </pages> <year> 1991. </year>
Reference-contexts: The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [24, 25, 31, 14, 12, 15, 36, 35, 11] </ref>. As illustrated by a variety of examples in this paper, these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc., on almost all parallel architectures of interest.
Reference: [12] <author> Anshul Gupta and Vipin Kumar. </author> <title> The scalability of matrix multiplication algorithms on parallel computers. </title> <type> Technical Report TR 91-54, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1991. </year> <note> A short version appears in Proceedings of 1993 International Conference on Parallel Processing, pages III-115-III-119, </note> <year> 1993. </year>
Reference-contexts: The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [24, 25, 31, 14, 12, 15, 36, 35, 11] </ref>. As illustrated by a variety of examples in this paper, these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc., on almost all parallel architectures of interest. <p> In this case the minimum possible value for p (T P ) r will be obtained when C (W ) processors are used. The following example illustrates this case. Example 4: Matrix Multiplication on Mesh Consider a simple algorithm described in <ref> [12] </ref> for multiplying two N fi N matrices on a p p wrap-around mesh. <p> The reason is that such operations always lead to T o fi (p). This class of algorithms includes some fairly important algorithms such as matrix multiplication (all-to-all/one-to-all broadcast) <ref> [12] </ref>, vector dot products (single node accumulation) [15], shortest paths (one-to-all broadcast) [25], and FFT (all-to-all personalized communication) [14], etc. The readers should note that the presence of a global communication operation in an algorithm is a sufficient but not a necessary condition for T o fi (p).
Reference: [13] <author> Anshul Gupta and Vipin Kumar. </author> <title> Analyzing performance of large scale parallel systems. </title> <booktitle> In Proceedings of the 26th Hawaii International Conference on System Sciences, </booktitle> <year> 1993. </year> <note> To appear in Journal of Parallel and Distributed Computing, </note> <year> 1993. </year>
Reference-contexts: Although, for the sake of ease of presentation, the examples are restricted to simple and regular problems, the properties of parallel systems studied here apply to general parallel systems as well. A preliminary version of this paper appears in <ref> [13] </ref>. 2 Definitions and Assumptions In this section, we formally describe the terminology used in the rest of the paper. Parallel System : The combination of a parallel architecture and a parallel algorithm implemented on it.
Reference: [14] <author> Anshul Gupta and Vipin Kumar. </author> <title> The scalability of FFT on parallel computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(8) </volume> <pages> 922-932, </pages> <month> August </month> <year> 1993. </year> <note> A detailed version available as Technical Report TR 90-53, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: typically arise while using shared memory or SIMD machines which do not have a message startup time for data communication. 5 0 p 400 800 0 200 400 600 800 1000 1200 T p Example 1: Parallel FFT on a SIMD Hypercube Consider a parallel implementation of the FFT algorithm <ref> [14] </ref> on a SIMD hypercube connected machine (e.g., the CM-2 [20]). If an N point FFT is being attempted on such a machine with p processors, N p units of data will be communicated among directly connected processors in log p of the log N iterations of the algorithm. <p> For this parallel system W = N log N . As shown in <ref> [14] </ref>, T o = t w fi N p log p fi p = t w N log p, where t w is the message communication time per word. Clearly, for a given W , T o &lt; fi (p). <p> The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [24, 25, 31, 14, 12, 15, 36, 35, 11] </ref>. As illustrated by a variety of examples in this paper, these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc., on almost all parallel architectures of interest. <p> It also confirms the validity of Equation (9). Example 5: FFT on a Hypercube Consider the implementation of the FFT algorithm on an MIMD hypercube using the binary-exchange algorithm. As shown in <ref> [14] </ref>, for an N point FFT on p processors, W = N log N and T o = t s p log p + t w N log p for this algorithm. <p> The reason is that such operations always lead to T o fi (p). This class of algorithms includes some fairly important algorithms such as matrix multiplication (all-to-all/one-to-all broadcast) [12], vector dot products (single node accumulation) [15], shortest paths (one-to-all broadcast) [25], and FFT (all-to-all personalized communication) <ref> [14] </ref>, etc. The readers should note that the presence of a global communication operation in an algorithm is a sufficient but not a necessary condition for T o fi (p).
Reference: [15] <author> Anshul Gupta, Vipin Kumar, and A. H. Sameh. </author> <title> Performance and scalability of preconditioned conjugate gradient methods on parallel computers. </title> <type> Technical Report TR 92-64, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1992. </year> <booktitle> A short version appears in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 664-674, </pages> <year> 1993. </year>
Reference-contexts: The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [24, 25, 31, 14, 12, 15, 36, 35, 11] </ref>. As illustrated by a variety of examples in this paper, these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc., on almost all parallel architectures of interest. <p> The reason is that such operations always lead to T o fi (p). This class of algorithms includes some fairly important algorithms such as matrix multiplication (all-to-all/one-to-all broadcast) [12], vector dot products (single node accumulation) <ref> [15] </ref>, shortest paths (one-to-all broadcast) [25], and FFT (all-to-all personalized communication) [14], etc. The readers should note that the presence of a global communication operation in an algorithm is a sufficient but not a necessary condition for T o fi (p).
Reference: [16] <author> John L. Gustafson. </author> <title> Reevaluating Amdahl's law. </title> <journal> Communications of the ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <year> 1988. </year>
Reference-contexts: As early as in 1967, Amdahl [2] made the observation that if s is the serial fraction in an algorithm, then its speedup for a fixed size problem is bounded by 1 s , no matter how many processors are used. Gustafson, Montry and Benner <ref> [18, 16] </ref> experimentally demonstrated that the upper bound on speedup can be overcome by increasing the problem size as the number of processors is increased.
Reference: [17] <author> John L. Gustafson. </author> <title> The consequences of fixed time performance measurement. </title> <booktitle> In Proceedings of the 25th Hawaii International Conference on System Sciences: Volume III, </booktitle> <pages> pages 113-124, </pages> <year> 1992. </year>
Reference: [18] <author> John L. Gustafson, Gary R. Montry, and Robert E. Benner. </author> <title> Development of parallel methods for a 1024-processor hypercube. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 609-638, </pages> <year> 1988. </year>
Reference-contexts: As early as in 1967, Amdahl [2] made the observation that if s is the serial fraction in an algorithm, then its speedup for a fixed size problem is bounded by 1 s , no matter how many processors are used. Gustafson, Montry and Benner <ref> [18, 16] </ref> experimentally demonstrated that the upper bound on speedup can be overcome by increasing the problem size as the number of processors is increased.
Reference: [19] <author> S. L. Johnsson and C.-T. Ho. </author> <title> Optimum broadcasting and personalized communication in hypercubes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(9) </volume> <pages> 1249-1268, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: This is true only under the assumption that T o &lt; fi (p). For this assumption to be true, the parallel system has to be devoid of any global operation (such as broadcast, and one-to-all and all-to-all personalized communication <ref> [3, 19] </ref>) with a message passing latency or message startup time. The reason is that such operations always lead to T o fi (p).
Reference: [20] <author> S. L. Johnsson, R. Krawitz, R. Frye, and D. McDonald. </author> <title> A radix-2 FFT on the connection machine. </title> <type> Technical report, </type> <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA, </address> <year> 1989. </year> <month> 18 </month>
Reference-contexts: do not have a message startup time for data communication. 5 0 p 400 800 0 200 400 600 800 1000 1200 T p Example 1: Parallel FFT on a SIMD Hypercube Consider a parallel implementation of the FFT algorithm [14] on a SIMD hypercube connected machine (e.g., the CM-2 <ref> [20] </ref>). If an N point FFT is being attempted on such a machine with p processors, N p units of data will be communicated among directly connected processors in log p of the log N iterations of the algorithm. For this parallel system W = N log N .
Reference: [21] <author> L. Kleinrock and J.-H. Huang. </author> <title> On parallel processing systems: Amdahl's law generalized and some results on optimal design. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(5) </volume> <pages> 434-447, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: On the other hand, minimizing p (T P ) r for r &lt; 2 2x j will result in an operating point with efficiency higher than 0.5 and a lower speedup. In <ref> [21] </ref>, Kleinrock and Huang state that the mean service time for a job is minimum for p = 1, or for as many processors as possible. This is true only under the assumption that T o &lt; fi (p).
Reference: [22] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: that for a class of overhead functions, given any problem size, operating at the point of maximum speedup is equivalent to operating at a fixed efficiency and the relation between the problem size and the number of processors that yields maximum speedup is given by the isoefficiency metric of scalability <ref> [22, 10, 23] </ref>. Next, a criterion of optimality is described that is more general than just maximizing the speedup and similar results are derived under this new condition for choosing the optimal operating point. The organization of the paper is as follows. <p> The isoefficiency metric <ref> [22, 10, 23] </ref> comes in as a handy tool to study the fixed efficiency characteristics of a parallel system. The isoefficiency function relates the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors used. <p> As discussed in <ref> [22, 10] </ref>, the relation between the problem size and the maximum number of processors that can be used in a cost-optimal fashion for solving the problem is given by the isoefficiency function. Often, using as many processors as possible results in a non-cost-optimal system.
Reference: [23] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing scalability of parallel algorithms and architectures. </title> <type> Technical Report TR 91-18, </type> <institution> Department of Computer Science Department, University of Minnesota, Minneapolis, MN, </institution> <year> 1991. </year> <note> To appear in Journal of Parallel and Distributed Computing, </note> <year> 1994. </year> <booktitle> A shorter version appears in Proceedings of the 1991 International Conference on Supercomputing, </booktitle> <pages> pages 396-405, </pages> <year> 1991. </year>
Reference-contexts: that for a class of overhead functions, given any problem size, operating at the point of maximum speedup is equivalent to operating at a fixed efficiency and the relation between the problem size and the number of processors that yields maximum speedup is given by the isoefficiency metric of scalability <ref> [22, 10, 23] </ref>. Next, a criterion of optimality is described that is more general than just maximizing the speedup and similar results are derived under this new condition for choosing the optimal operating point. The organization of the paper is as follows. <p> The isoefficiency metric <ref> [22, 10, 23] </ref> comes in as a handy tool to study the fixed efficiency characteristics of a parallel system. The isoefficiency function relates the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors used.
Reference: [24] <author> Vipin Kumar and V. N. Rao. </author> <title> Parallel depth-first search, part II: Analysis. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(6) </volume> <pages> 501-519, </pages> <year> 1987. </year>
Reference-contexts: The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [24, 25, 31, 14, 12, 15, 36, 35, 11] </ref>. As illustrated by a variety of examples in this paper, these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc., on almost all parallel architectures of interest.
Reference: [25] <author> Vipin Kumar and Vineet Singh. </author> <title> Scalability of parallel algorithms for the all-pairs shortest path problem. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(2) </volume> <pages> 124-138, </pages> <month> October </month> <year> 1991. </year> <note> A short version appears in the Proceedings of the International Conference on Parallel Processing, </note> <year> 1990. </year>
Reference-contexts: The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [24, 25, 31, 14, 12, 15, 36, 35, 11] </ref>. As illustrated by a variety of examples in this paper, these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc., on almost all parallel architectures of interest. <p> Thus it is possible that C (W ) of a parallel algorithm may determine the minimum execution time rather than the mathematically derived conditions. The following example illustrates this case: Example 2: Floyd's Striped Shortest Path Algorithm on Mesh A number of parallel Shortest Path algorithms are discussed in <ref> [25] </ref>. Consider the implementation of Floyd's algorithm in which the N fi N adjacency matrix of the graph is striped among p processors such that each processor stores N p full rows of the matrix. <p> In each of the N iterations of this algorithm, a processor broadcasts a row of length N of the adjacency matrix of the graph to every other processor. As shown in <ref> [25] </ref>, if the p processor are connected in a mesh configuration with cut-through routing, the total overhead due to this communication is given by T o = t s N p 1:5 + t w (N + p p)N p. <p> Example 3: Floyd's Checkerboard Shortest Path Algorithm on Mesh In this example, we consider a different parallel system consisting of another variation of Floyd's algorithm discussed in <ref> [25] </ref> and a wrap-around mesh with store-and-forward routing. In this algorithm, the N fi N adjacency matrix is partitioned into p sub-blocks of size N p p fi N p p each, and these sub-blocks are mapped on a p processor mesh. <p> In this version of Floyd's algorithm, a processor broadcasts N p p elements among p p processors in each of the N iterations. As shown 9 0 p 10000 20000 30000 10000 10200 " p ! P when C (W ) &gt; p 0 . in <ref> [25] </ref>, this results in a total overhead of T o = t s N p 1:5 + t w N 2 p. Since the expression for T o is same as that in Example 2, p 0 = 1:59N 4=3 t 2=3 again. <p> The reason is that such operations always lead to T o fi (p). This class of algorithms includes some fairly important algorithms such as matrix multiplication (all-to-all/one-to-all broadcast) [12], vector dot products (single node accumulation) [15], shortest paths (one-to-all broadcast) <ref> [25] </ref>, and FFT (all-to-all personalized communication) [14], etc. The readers should note that the presence of a global communication operation in an algorithm is a sufficient but not a necessary condition for T o fi (p).
Reference: [26] <author> Y. W. E. Ma and Denis G. Shea. </author> <title> Downward scalability of parallel architectures. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 109-120, </pages> <year> 1988. </year>
Reference-contexts: the number of processors p at which the parallel execution time T P is minimized, is given by the isoefficiency function for a particular efficiency. 16 Several other researchers have used the minimum parallel execution time of a problem of a given size for analyzing the performance of parallel systems <ref> [28, 26, 30] </ref>. Nussbaum and Agarwal [30] define scalability of an architecture for a given algorithm as the ratio of the algorithm's asymptotic speedup when run on the architecture in question to its corresponding asymptotic speedup when run on an EREW PRAM.
Reference: [27] <author> Dan C. Marinescu and John R. Rice. </author> <title> On high level characterization of parallelism. </title> <type> Technical Report CSD-TR-1011, </type> <institution> CAPO Report CER-90-32, Computer Science Department, Purdue University, West Lafayette, </institution> <note> IN, Revised June 1991. To appear in Journal of Parallel and Distributed Computing, </note> <year> 1993. </year>
Reference-contexts: But as we saw in these examples, this is not the case because the the value of C (W ) in the two cases is different. In <ref> [27] </ref>, Marinescu and Rice develop a model to describe and analyze a parallel computation on a MIMD machine in terms of the number of threads of control p into which the computation is divided and the number events g (p) as a function of p. <p> Usually, the duration of an event or a communication step is not a constant as assumed in <ref> [27] </ref>. In general, both and T o are functions of W and p. If T o is of the form g (p), Marinescu and Rice [27] derive that the number of processors that will yield maximum speedup will be given by p = ( W + g (p)) 1 g 0 <p> Usually, the duration of an event or a communication step is not a constant as assumed in <ref> [27] </ref>. In general, both and T o are functions of W and p. If T o is of the form g (p), Marinescu and Rice [27] derive that the number of processors that will yield maximum speedup will be given by p = ( W + g (p)) 1 g 0 (p) , which can be rewritten as g 0 (p) = W +g (p) p .
Reference: [28] <author> David M. Nicol and Frank H. Willard. </author> <title> Problem size, parallel architecture, and optimal speedup. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 404-420, </pages> <year> 1988. </year>
Reference-contexts: the number of processors p at which the parallel execution time T P is minimized, is given by the isoefficiency function for a particular efficiency. 16 Several other researchers have used the minimum parallel execution time of a problem of a given size for analyzing the performance of parallel systems <ref> [28, 26, 30] </ref>. Nussbaum and Agarwal [30] define scalability of an architecture for a given algorithm as the ratio of the algorithm's asymptotic speedup when run on the architecture in question to its corresponding asymptotic speedup when run on an EREW PRAM.
Reference: [29] <author> Sam H. Noh, Dipak Ghosal, and Ashok K. Agrawala. </author> <title> An empirical study of the effect of granularity on parallel algorithms on the connection machine. </title> <type> Technical Report UMIACS-TR-89-124.1, </type> <institution> University of Maryland, College Park, MD, </institution> <year> 1989. </year>
Reference: [30] <author> Daniel Nussbaum and Anant Agarwal. </author> <title> Scalability of parallel machines. </title> <journal> Communications of the ACM, </journal> <volume> 34(3) </volume> <pages> 57-61, </pages> <year> 1991. </year>
Reference-contexts: the number of processors p at which the parallel execution time T P is minimized, is given by the isoefficiency function for a particular efficiency. 16 Several other researchers have used the minimum parallel execution time of a problem of a given size for analyzing the performance of parallel systems <ref> [28, 26, 30] </ref>. Nussbaum and Agarwal [30] define scalability of an architecture for a given algorithm as the ratio of the algorithm's asymptotic speedup when run on the architecture in question to its corresponding asymptotic speedup when run on an EREW PRAM. <p> Nussbaum and Agarwal <ref> [30] </ref> define scalability of an architecture for a given algorithm as the ratio of the algorithm's asymptotic speedup when run on the architecture in question to its corresponding asymptotic speedup when run on an EREW PRAM.
Reference: [31] <author> Vineet Singh, Vipin Kumar, Gul Agha, and Chris Tomlinson. </author> <title> Scalability of parallel sorting on mesh multicomputers. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(2), </volume> <year> 1991. </year>
Reference-contexts: The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [24, 25, 31, 14, 12, 15, 36, 35, 11] </ref>. As illustrated by a variety of examples in this paper, these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc., on almost all parallel architectures of interest.
Reference: [32] <author> Xian-He Sun and John L. Gustafson. </author> <title> Toward a better parallel performance metric. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 1093-1109, </pages> <month> December </month> <year> 1991. </year> <note> Also available as Technical Report IS-5053, </note> <institution> UC-32, Ames Laboratory, Iowa State University, Ames, IA. </institution>
Reference: [33] <author> Zhimin Tang and Guo-Jie Li. </author> <title> Optimal granularity of grid iteration problems. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages I111-I118, </pages> <year> 1990. </year>
Reference-contexts: However, at this point, the efficiency of the parallel execution is rather poor. Hence they suggest that the number of processors should be chosen to maximize the product of efficiency and speedup. Flatt and Kennedy, and Tang and Li <ref> [33] </ref> also suggest maximizing a weighted geometric mean of efficiency and speedup. Eager et. al. [6] proposed that an optimal operating point should be chosen such the efficiency of execution is roughly 0.5. <p> Therefore, in order to achieve a balance between speedup and efficiency, several researchers have proposed to operate at a point where the value of p (T P ) r is minimized for some constant r (r 1) and for a given problem size W <ref> [8, 6, 33] </ref>. <p> It can be shown <ref> [33] </ref> that this corresponds to the point where ES r1 is maximized for a given problem size. p (T P ) r = pT P ( S W r Thus p (T P ) r will be minimum when ES r1 is maximum for a given W and by minimizing p <p> Eager et. al. [6] and Tang and Li <ref> [33] </ref> have proposed a criterion of optimality different from optimal speedup. They argue that the optimal operating point should be chosen so that a balance is struck between efficiency and speedup. <p> Eager et. al. and Tang and Li also conclude that for T o = fi (p), this is also equivalent to operating at a point where the ES product is maximum or p (T P ) 2 is minimum. This conclusion in <ref> [6, 33] </ref> is a special case of the more general case that is captured in Equation (9). <p> In general, operating at the optimal point or the "knee" referred to in [6] and <ref> [33] </ref> for a parallel system with T o = fi (p x j ) will be identical to operating at a point where p (T P ) r is minimum, where r = 2 2x j . This is obtained from Equation (9) for E = 0:5.
Reference: [34] <author> Fredric A. Van-Catledge. </author> <title> Towards a general model for evaluating the relative performance of computer systems. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(2) </volume> <pages> 100-108, </pages> <year> 1989. </year>
Reference: [35] <author> Jinwoon Woo and Sartaj Sahni. </author> <title> Hypercube computing: Connected components. </title> <journal> Journal of Supercomputing, </journal> <note> 1991. Also available as TR 88-50 from the Department of Computer Science, </note> <institution> University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [24, 25, 31, 14, 12, 15, 36, 35, 11] </ref>. As illustrated by a variety of examples in this paper, these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc., on almost all parallel architectures of interest.
Reference: [36] <author> Jinwoon Woo and Sartaj Sahni. </author> <title> Computing biconnected components on a hypercube. </title> <journal> Journal of Supercomputing, </journal> <month> June </month> <year> 1991. </year> <note> Also available as Technical Report TR 89-7 from the Department of Computer Science, </note> <institution> University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [24, 25, 31, 14, 12, 15, 36, 35, 11] </ref>. As illustrated by a variety of examples in this paper, these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc., on almost all parallel architectures of interest.
Reference: [37] <author> Patrick H. Worley. </author> <title> The effect of time constraints on scaled speedup. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 11(5) </volume> <pages> 838-858, </pages> <year> 1990. </year>
Reference-contexts: Gustafson, Montry and Benner [18, 16] experimentally demonstrated that the upper bound on speedup can be overcome by increasing the problem size as the number of processors is increased. Worley <ref> [37] </ref> showed that for a class of parallel algorithms, if the parallel execution time is fixed, then there exists a problem size which cannot be solved in that fixed time no matter how many processors are used. <p> It is easily verified that this is a special case of Equation (2) for T o = g (p). Worley <ref> [37] </ref> showed that for certain algorithms, given a certain amount of time T P , there will exist a problem size large enough so that it cannot be solved in time T P , no matter how many processors are used.
Reference: [38] <author> Xiaofeng Zhou. </author> <title> Bridging the gap between Amdahl's law and Sandia laboratory's result. </title> <journal> Communications of the ACM, </journal> <volume> 32(8) </volume> <pages> 1014-5, </pages> <year> 1989. </year> <month> 19 </month>
References-found: 38

