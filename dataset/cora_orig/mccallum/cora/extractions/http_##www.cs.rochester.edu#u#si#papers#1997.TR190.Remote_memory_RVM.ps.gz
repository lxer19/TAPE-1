URL: http://www.cs.rochester.edu/u/si/papers/1997.TR190.Remote_memory_RVM.ps.gz
Refering-URL: http://www.cs.rochester.edu/stats/oldmonths/1998.05/docs-name.html
Root-URL: 
Email: markatos@ics.forth.gr  
Phone: tel: +(30) 81 391655 fax: +(30) 81 391601  
Title: On using Network Memory to Improve the Performance of Transaction-Based Systems  
Author: Sotiris Ioannidis Evangelos P. Markatos Julia Sevaslidou 
Date: htpp://www.ics.forth.gr/proj/avg/paging.html  
Address: P.O.Box 1385 Heraklio, Crete, GR-711-10 GREECE  
Affiliation: Computer Architecture and VLSI Systems Group Institute of Computer Science (ICS) Foundation for Research Technology Hellas (FORTH), Crete  
Pubnum: Technical Report 190, ICS-FORTH  
Abstract: Transactions have been valued for their atomicity and recoverability properties that are useful to several systems, ranging from CAD environment to large-scale databases. Unfortunately, adding transaction support to an existing data repository was traditionally thought to be expensive, mostly due to the fact that the performance of transaction-based systems is usually limited by the performance of the magnetic disks that are used to hold the data repository. In this paper we describe how to use the collective main memory in a Network of Workstations (NOW) to improve the performance of transaction-based systems. We describe the design of our system and its implementation in two independent transaction-based systems, namely EXODUS, and RVM. We evaluate the performance of our prototype using several database benchmarks (like OO7 and TPC-A). Our experimental results indicate that our system delivers up to two orders of magnitude performance improvement compared to its predecessors.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson, M. D. Dahlin, J. M. Neefe, D. A. Patterson, D. S. Roselli, and R. Y. Wang. </author> <title> Serverless Network File Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(1) </volume> <pages> 41-79, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: This performance disparity will continue to increase, according to current architecture trends. The first of the above issues (reading from remote memory) has been somewhat explored in the areas of file systems <ref> [1, 18, 21] </ref>, paging [22] and global memory databases for workstation clusters [15, 17]. All previous work suggests that the use of remote main memory as a large file (database) cache results in significant performance improvements. <p> Although disk read operations may be reduced with the help of large main memory caches (or even network main memory caches <ref> [1, 15] </ref>), disk write operations at transaction commit time are difficult to avoid, since the transaction's modified data and meta-data have to reach stable storage before the transaction is able to commit, otherwise a system crash would leave the data repository in a non-consistent state. <p> For example, several file systems <ref> [1, 8, 18, 21] </ref> use the collective main memory of several clients and servers as a large file system cache. Paging systems may also use remote main memory in a workstation cluster to improve application performance [14, 19, 20, 22]. <p> However, even Rio may lead to data loss in case of UPS malfunction. In these cases, our approach that keeps two copies of sensitive data in two workstations connected to two different power supplies, will be able to avoid data loss. Network file systems like Sprite [23] and xfs <ref> [1, 11] </ref>, can also be used to store replicated data and build a reliable network main memory. However, our approach, would still result in better performance due to the minimum (block) size transfers that all file systems are forced to have.
Reference: [2] <author> T.E. Anderson, D.E. Culler, and D.A. Patterson. </author> <title> A Case for NOW (Networks of Workstations). </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 54-64, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: In this paper we describe a novel way to improve the performance of transaction management by using the collective main memory (hereafter called remote memory) in a Network of Workstations (NOW) <ref> [2, 4, 16] </ref>. The main idea behind our approach is to reduce the number of disk accesses by substituting them with (remote) main memory accesses.
Reference: [3] <author> M. Baker, S. Asami, E. Deprit, J. Ousterhout, and M. Seltzer. </author> <title> Non-volatile Memory for Fast, Reliable File Systems. </title> <booktitle> In Proc. of the 5-th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 10-22, </pages> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Thus, eNVy would be used only for expensive and high-performance database servers, and not for ordinary workstations. As another example, Baker et al. have proposed the use of battery-backed SRAM to improve file system performance <ref> [3] </ref>.
Reference: [4] <author> G. Buzzard, D. Jacobson, M. Mackey, S. Marovich, and J. Wilkes. </author> <title> An Implementation of the Hamlyn Sender-Managed Interface Architecture. </title> <booktitle> In Second USENIX Symposium on Operating System Design and Implementation, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: In this paper we describe a novel way to improve the performance of transaction management by using the collective main memory (hereafter called remote memory) in a Network of Workstations (NOW) <ref> [2, 4, 16] </ref>. The main idea behind our approach is to reduce the number of disk accesses by substituting them with (remote) main memory accesses.
Reference: [5] <author> M. Carey, D. DeWitt, and J. Naughton. Ther OO7 Bechmark. </author> <booktitle> In Proceedings of the 1993 ACM SIGMOD Conference, </booktitle> <pages> pages 12-21, </pages> <year> 1993. </year>
Reference-contexts: The workstations are connected through a 100 Mbps FDDI, and a 10Mbps Ethernet interconnection network. 3.8.2 OO7 On top of EXODUS we run a common database benchmark called OO7 <ref> [5] </ref>.
Reference: [6] <author> M. Carey and D. DeWitt et. al. </author> <title> The EXODUS Extensible DBMS Project: An Overview. </title> <editor> In S.Zdonik and D.Maie, editors, </editor> <booktitle> Readings in Object-Oriented Database Systems. </booktitle> <publisher> Morgan Kaufman, </publisher> <year> 1990. </year>
Reference-contexts: To demonstrate our approach, we implemented our approach within two existing transaction-based systems: The EXODUS storage manager <ref> [6] </ref>, and the RVM (Recoverable Virtual Memory) System [25]. Section 2 describes the design and the implementation of our systems. We have run several benchmarks on top of the modified transaction systems and have observed performance improvements up to two orders of magnitude. <p> Several current transaction based systems use a magnetic disk as the stable storage, and force all dirty data to it using the fsync (2) system call <ref> [6, 25] </ref>. 2 Magnetic disks can usually survive power and software failures, thereby providing a stable medium to store data that must survive crashes. <p> Our described main memory system suffers from data loss once every several years, which is the same level of reliability current magnetic disks provide. 2.2 EXODUS and RVM To illustrate our approach we have modified a lightweight transaction-based system called RVM [25] and the EXODUS storage manager <ref> [6] </ref> to use remote memory (instead of disks) for synchronous write operations. After studying the performance of the systems, we concluded that they spend a significant amount of their time, synchronously writing transaction data to their log file, which is used to implement a two-phase commit protocol. <p> We used three versions of EXODUS: Accounts (fi1024) Unmodified RVM RRVM-ETHER RRVM-FDDI 32 43.4 186 239 512 41.9 127 154 11 * EXODUS: This is the unmodified EXODUS system <ref> [6] </ref>. * REX-FDDI: This is our modified EXODUS (REX) system running on top of an FDDI interconnec tion network. * REX-ETHERNET: This is REX running on top of an Ethernet interconnection network. We see that in all cases REX has superior performance compared to the unmodified EXODUS systems.
Reference: [7] <author> Peter M. Chen, Wee Teck Ng, Subhachandra Chandra, Christopher Aycock, Gurushankar Rajamani, and David Lowell. </author> <title> The Rio File Cache: Surviving Operating System Crashes. </title> <booktitle> In Proc. of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 74-83, </pages> <year> 1996. </year>
Reference-contexts: Summarizing, Harp is a kernel-level file system that sustains hardware and software failures, while our approach leads to open, portable, flexible, and lightweight user-level transaction-based systems. The Rio file system changes the operating system to avoid destroying its main memory contents in case of a crash <ref> [7] </ref>. Thus, if a workstation is equipped with a UPS and the Rio file system, it can survive all failures: power failures do not happen (due to the UPS), and software failures do not destroy the contents of the main memory.
Reference: [8] <author> T. Cortes, S. Girona, and J. Labarta. </author> <title> PACA: A Distributed File System Cache for Parallel Machines. Performance under Unix-like workload. </title> <type> Technical Report UPC-DAC-1995-20, </type> <institution> Departament d'Arquitectura de computadors, Universitat Politecnica de Catalunya (UPC), </institution> <month> June 15 </month> <year> 1995. </year>
Reference-contexts: For example, several file systems <ref> [1, 8, 18, 21] </ref> use the collective main memory of several clients and servers as a large file system cache. Paging systems may also use remote main memory in a workstation cluster to improve application performance [14, 19, 20, 22].
Reference: [9] <author> M. Costa, P. Guedes, M. Sequeira, N. Neves, and M. Castro. </author> <title> Lightweight Logging for Lazy Release Consistent Distributed Shared Memory. </title> <booktitle> In Second USENIX Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 59-74, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Paging systems may also use remote main memory in a workstation cluster to improve application performance [14, 19, 20, 22]. Even Distributed Shared Memory systems, can exploit the remote main memory in a NOW <ref> [13, 9] </ref> for increased performance and reliability. The closest of these systems to our research is the Harp file system [21]. Harp uses replicated file servers to tolerate single server failure. Each file server is equipped with a UPS to tolerate power failures, and speedup synchronous write operations.
Reference: [10] <author> M. Dahlin. </author> <title> Serverless Network File Systems. </title> <type> PhD thesis, </type> <institution> UC Berkeley, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: memory (over a high speed interconnection network), was shown to be significantly faster than reading data fl Current affiliation: Computer Science Department, University of Rochester, Rochester, NY14627 y Evangelos Markatos and Julia Sevaslidou are also with the University of Crete, Department of Computer Science 1 from a (local) magnetic disk <ref> [10] </ref>. This is due to the fact that current high-speed networks provide higher throughput and lower latency than current high-speed disks. Architecture trends suggest that this disparity between magnetic disks and interconnection networks will continue to increase with time [10]. <p> Crete, Department of Computer Science 1 from a (local) magnetic disk <ref> [10] </ref>. This is due to the fact that current high-speed networks provide higher throughput and lower latency than current high-speed disks. Architecture trends suggest that this disparity between magnetic disks and interconnection networks will continue to increase with time [10]. Thereby, the use of remote memory instead of magnetic disks (whenever possible) will continue to result in everincreasing performance improvements. * Speeding up Synchronous Write Operations to Reliable Storage: Transaction-based systems frequently use synchronous write operations to force all modified data to disk at transaction commit time.
Reference: [11] <author> M.D. Dahlin, R.Y. Wang, T.E. Anderson, and D.A. Patterson. </author> <title> Cooperative Cahing: Using Remote Client Memory to Improve File System Performance. </title> <booktitle> In First USENIX Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 267-280, </pages> <year> 1994. </year>
Reference-contexts: However, even Rio may lead to data loss in case of UPS malfunction. In these cases, our approach that keeps two copies of sensitive data in two workstations connected to two different power supplies, will be able to avoid data loss. Network file systems like Sprite [23] and xfs <ref> [1, 11] </ref>, can also be used to store replicated data and build a reliable network main memory. However, our approach, would still result in better performance due to the minimum (block) size transfers that all file systems are forced to have.
Reference: [12] <author> M. J. Feeley, W. E. Morgan, F. H. Pighin, A. R. Karlin, H. M. Levy, and C. A. Thekkath. </author> <title> Implementing Global Memory Management in a Workstation Cluster. </title> <booktitle> In Proc. 15-th Symposium on Operating Systems Principles, </booktitle> <pages> pages 201-212, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Feeley et. al. proposed a generalized memory management system, where the collective main memory of all workstations in a cluster is handled by the operating system <ref> [12] </ref>. Their experiments suggest that generalized memory management results in performance improvements. For example, OO7 on top of their system runs up to 2.5 times faster, than it used to run on top of a standard UNIX system. <p> For example, OO7 on top of their system runs up to 2.5 times faster, than it used to run on top of a standard UNIX system. We believe that our approach complements this work in the sense that both [15] and <ref> [12] </ref> improve the performance of read accesses (by providing large caches), while our approach improves the performance of synchronous write accesses.
Reference: [13] <author> Michael J. Feeley, Jeffrey S. Chase, Vivek R. Narasayya, and Henry M. Levy. </author> <title> Integrating Coherency and Recovery in Distributed Systems. </title> <booktitle> First USENIX Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 215-227, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Paging systems may also use remote main memory in a workstation cluster to improve application performance [14, 19, 20, 22]. Even Distributed Shared Memory systems, can exploit the remote main memory in a NOW <ref> [13, 9] </ref> for increased performance and reliability. The closest of these systems to our research is the Harp file system [21]. Harp uses replicated file servers to tolerate single server failure. Each file server is equipped with a UPS to tolerate power failures, and speedup synchronous write operations.
Reference: [14] <author> E. W. Felten and J. Zahorjan. </author> <title> Issues in the Implementation of a Remote Memory Paging System. </title> <type> Technical Report 91-03-09, </type> <institution> Computer Science Department, University of Washington, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: For example, several file systems [1, 8, 18, 21] use the collective main memory of several clients and servers as a large file system cache. Paging systems may also use remote main memory in a workstation cluster to improve application performance <ref> [14, 19, 20, 22] </ref>. Even Distributed Shared Memory systems, can exploit the remote main memory in a NOW [13, 9] for increased performance and reliability. The closest of these systems to our research is the Harp file system [21]. Harp uses replicated file servers to tolerate single server failure.
Reference: [15] <author> M. Franklin, M. Carey, and M. Livny. </author> <title> Global Memory Management in Client-Server DBMS Archi--tectures. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <pages> pages 596-609, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: This performance disparity will continue to increase, according to current architecture trends. The first of the above issues (reading from remote memory) has been somewhat explored in the areas of file systems [1, 18, 21], paging [22] and global memory databases for workstation clusters <ref> [15, 17] </ref>. All previous work suggests that the use of remote main memory as a large file (database) cache results in significant performance improvements. The thrust of this paper is on exploring the second issue: using remote memory to speed up synchronous disk write operations. <p> Although disk read operations may be reduced with the help of large main memory caches (or even network main memory caches <ref> [1, 15] </ref>), disk write operations at transaction commit time are difficult to avoid, since the transaction's modified data and meta-data have to reach stable storage before the transaction is able to commit, otherwise a system crash would leave the data repository in a non-consistent state. <p> Franklin, Carey and Livny have proposed the use of remote main memory in a NOW as a large database cache <ref> [15] </ref>. They validate their approach using simulation, and report very encouraging results. Griffioen et. al proposed that DERBY storage manager, that exploits remote memory and UPSs to reliably store a transaction's data [17]. They simulate the performance of their system and provide encouraging results. <p> For example, OO7 on top of their system runs up to 2.5 times faster, than it used to run on top of a standard UNIX system. We believe that our approach complements this work in the sense that both <ref> [15] </ref> and [12] improve the performance of read accesses (by providing large caches), while our approach improves the performance of synchronous write accesses.
Reference: [16] <author> R. Gillett. </author> <title> Memory Channel Network for PCI. </title> <journal> IEEE Micro, </journal> <volume> 16(1) </volume> <pages> 12-18, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: In this paper we describe a novel way to improve the performance of transaction management by using the collective main memory (hereafter called remote memory) in a Network of Workstations (NOW) <ref> [2, 4, 16] </ref>. The main idea behind our approach is to reduce the number of disk accesses by substituting them with (remote) main memory accesses. <p> Traditional interconnection networks (like Ethernet and FDDI) have latency in the range of several hundred microseconds. ATM networks have latency in the range of a few hundred microseconds [26], while more recent networks (like SCI [24] and Memory Channel <ref> [16] </ref>) have latency in the range of few microseconds. At the same time, disk latency has remained in the order of a few milliseconds for several years now, and is not expected to improve at a significant rate. <p> The workstations are connected through an Ethernet interconnection network, a 100 Mbps FDDI, and a Memory Channel Interconnection Network <ref> [16] </ref>. These three interconnection networks represent three different generations of LANs: Ethernet is a traditional low-bandwidth network that provides low-cost connectivity without ambitious performance goals. FDDI is a high-bandwidth network that provides higher speed at a slightly increased cost.
Reference: [17] <author> J. Griffioen, R. Vingralek, T. Anderson, and Y. Breitbart. Derby: </author> <title> A Memory Management System for Distributed Main Memory Databases. </title> <booktitle> In Proceedings of the 6th Internations Workshop on Research Issues in Data Engineering (RIDE '96), </booktitle> <pages> pages 150-159, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: This performance disparity will continue to increase, according to current architecture trends. The first of the above issues (reading from remote memory) has been somewhat explored in the areas of file systems [1, 18, 21], paging [22] and global memory databases for workstation clusters <ref> [15, 17] </ref>. All previous work suggests that the use of remote main memory as a large file (database) cache results in significant performance improvements. The thrust of this paper is on exploring the second issue: using remote memory to speed up synchronous disk write operations. <p> They validate their approach using simulation, and report very encouraging results. Griffioen et. al proposed that DERBY storage manager, that exploits remote memory and UPSs to reliably store a transaction's data <ref> [17] </ref>. They simulate the performance of their system and provide encouraging results.
Reference: [18] <author> J. Hartman and J. Ousterhout. </author> <title> The Zebra Striped Network File System. </title> <booktitle> Proc. 14-th Symposium on Operating Systems Principles, </booktitle> <pages> pages 29-43, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: This performance disparity will continue to increase, according to current architecture trends. The first of the above issues (reading from remote memory) has been somewhat explored in the areas of file systems <ref> [1, 18, 21] </ref>, paging [22] and global memory databases for workstation clusters [15, 17]. All previous work suggests that the use of remote main memory as a large file (database) cache results in significant performance improvements. <p> For example, several file systems <ref> [1, 8, 18, 21] </ref> use the collective main memory of several clients and servers as a large file system cache. Paging systems may also use remote main memory in a workstation cluster to improve application performance [14, 19, 20, 22].
Reference: [19] <author> L. Iftode, K. Li, and K. Petersen. </author> <title> Memory Servers for Multicomputers. </title> <booktitle> In Proceedings of COMPCON 93, </booktitle> <pages> pages 538-547, </pages> <year> 1993. </year>
Reference-contexts: For example, several file systems [1, 8, 18, 21] use the collective main memory of several clients and servers as a large file system cache. Paging systems may also use remote main memory in a workstation cluster to improve application performance <ref> [14, 19, 20, 22] </ref>. Even Distributed Shared Memory systems, can exploit the remote main memory in a NOW [13, 9] for increased performance and reliability. The closest of these systems to our research is the Harp file system [21]. Harp uses replicated file servers to tolerate single server failure.
Reference: [20] <author> K. Li and K. Petersen. </author> <title> Evaluation of Memory System Extensions. </title> <booktitle> In Proc. 18-th International Symposium on Comp. Arch., </booktitle> <pages> pages 84-93, </pages> <year> 1991. </year>
Reference-contexts: For example, several file systems [1, 8, 18, 21] use the collective main memory of several clients and servers as a large file system cache. Paging systems may also use remote main memory in a workstation cluster to improve application performance <ref> [14, 19, 20, 22] </ref>. Even Distributed Shared Memory systems, can exploit the remote main memory in a NOW [13, 9] for increased performance and reliability. The closest of these systems to our research is the Harp file system [21]. Harp uses replicated file servers to tolerate single server failure.
Reference: [21] <author> B. Liskov, S. Ghemawat, R. Gruber, P. Johnson, L. Shrira, and M. Williams. </author> <title> Replication in the Harp File System. </title> <booktitle> Proc. 13-th Symposium on Operating Systems Principles, </booktitle> <pages> pages 226-238, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: This performance disparity will continue to increase, according to current architecture trends. The first of the above issues (reading from remote memory) has been somewhat explored in the areas of file systems <ref> [1, 18, 21] </ref>, paging [22] and global memory databases for workstation clusters [15, 17]. All previous work suggests that the use of remote main memory as a large file (database) cache results in significant performance improvements. <p> For example, several file systems <ref> [1, 8, 18, 21] </ref> use the collective main memory of several clients and servers as a large file system cache. Paging systems may also use remote main memory in a workstation cluster to improve application performance [14, 19, 20, 22]. <p> Even Distributed Shared Memory systems, can exploit the remote main memory in a NOW [13, 9] for increased performance and reliability. The closest of these systems to our research is the Harp file system <ref> [21] </ref>. Harp uses replicated file servers to tolerate single server failure. Each file server is equipped with a UPS to tolerate power failures, and speedup synchronous write operations. <p> Our performance results suggest that on top of a workstation cluster connected via Ethernet, RRVM is able to sustain several hundred (short) transaction operations per second (see figure 1). Published results for Harp <ref> [21] </ref>, suggest that it is able to sustain several tens of NFS operations per second.
Reference: [22] <author> E.P. Markatos and G. Dramitinos. </author> <title> Implementation of a Reliable Remote Memory Pager. </title> <booktitle> In Proceedings of the 1996 Usenix Technical Conference, </booktitle> <pages> pages 177-190, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: This performance disparity will continue to increase, according to current architecture trends. The first of the above issues (reading from remote memory) has been somewhat explored in the areas of file systems [1, 18, 21], paging <ref> [22] </ref> and global memory databases for workstation clusters [15, 17]. All previous work suggests that the use of remote main memory as a large file (database) cache results in significant performance improvements. <p> For example, several file systems [1, 8, 18, 21] use the collective main memory of several clients and servers as a large file system cache. Paging systems may also use remote main memory in a workstation cluster to improve application performance <ref> [14, 19, 20, 22] </ref>. Even Distributed Shared Memory systems, can exploit the remote main memory in a NOW [13, 9] for increased performance and reliability. The closest of these systems to our research is the Harp file system [21]. Harp uses replicated file servers to tolerate single server failure.
Reference: [23] <author> M. Nelson, B. Welch, and J. Ousterhout. </author> <title> Caching in the Sprite Network File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: However, even Rio may lead to data loss in case of UPS malfunction. In these cases, our approach that keeps two copies of sensitive data in two workstations connected to two different power supplies, will be able to avoid data loss. Network file systems like Sprite <ref> [23] </ref> and xfs [1, 11], can also be used to store replicated data and build a reliable network main memory. However, our approach, would still result in better performance due to the minimum (block) size transfers that all file systems are forced to have.
Reference: [24] <institution> Dolphin Interconnect Solutions. </institution> <note> DIS301 SBus-to-SCI Adapter User's Guide. </note>
Reference-contexts: Traditional interconnection networks (like Ethernet and FDDI) have latency in the range of several hundred microseconds. ATM networks have latency in the range of a few hundred microseconds [26], while more recent networks (like SCI <ref> [24] </ref> and Memory Channel [16]) have latency in the range of few microseconds. At the same time, disk latency has remained in the order of a few milliseconds for several years now, and is not expected to improve at a significant rate.
Reference: [25] <author> M. Stayanarayanan, Henry H Mashburn, Puneet Kumar, David C. Steere, and James J. Kistler. </author> <title> Lightweight Recoverable Virtual Memory. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12(1) </volume> <pages> 33-57, </pages> <year> 1994. </year>
Reference-contexts: To demonstrate our approach, we implemented our approach within two existing transaction-based systems: The EXODUS storage manager [6], and the RVM (Recoverable Virtual Memory) System <ref> [25] </ref>. Section 2 describes the design and the implementation of our systems. We have run several benchmarks on top of the modified transaction systems and have observed performance improvements up to two orders of magnitude. We report our performance results in section 3. <p> Several current transaction based systems use a magnetic disk as the stable storage, and force all dirty data to it using the fsync (2) system call <ref> [6, 25] </ref>. 2 Magnetic disks can usually survive power and software failures, thereby providing a stable medium to store data that must survive crashes. <p> Our described main memory system suffers from data loss once every several years, which is the same level of reliability current magnetic disks provide. 2.2 EXODUS and RVM To illustrate our approach we have modified a lightweight transaction-based system called RVM <ref> [25] </ref> and the EXODUS storage manager [6] to use remote memory (instead of disks) for synchronous write operations. <p> Memory Channel is a very high-bandwidth network that was designed to make possible a wide range of high performance applications in Networks of Workstations. Each workstation is equipped with a 6GB local disk. In our experiments we demonstrated the performance of RRVM and compared it with its predecessor RVM <ref> [25] </ref>. We have experimented with four system configurations: * RVM: This is the unmodified RVM system [25]. <p> Each workstation is equipped with a 6GB local disk. In our experiments we demonstrated the performance of RRVM and compared it with its predecessor RVM <ref> [25] </ref>. We have experimented with four system configurations: * RVM: This is the unmodified RVM system [25]. <p> The same benchmark was run on the original RVM system and its performance was reported in <ref> [25] </ref>. In the original RVM system, the log file and the data file were stored on different local disks, so as to eliminate any interference between the accesses to the different files. as a function of the number of accounts. In this experiment accesses to the database are sequential.
Reference: [26] <author> Alec Wolman, Geoff Voelker, and Chandramohan A. Thekkath. </author> <title> Latency Analysis of TCP on an ATM Network. </title> <booktitle> In Proceedings of the USENIX Winter '94 Technical Conference, </booktitle> <pages> pages 167-179, </pages> <address> San Francisco, CA, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Traditional interconnection networks (like Ethernet and FDDI) have latency in the range of several hundred microseconds. ATM networks have latency in the range of a few hundred microseconds <ref> [26] </ref>, while more recent networks (like SCI [24] and Memory Channel [16]) have latency in the range of few microseconds. At the same time, disk latency has remained in the order of a few milliseconds for several years now, and is not expected to improve at a significant rate.
Reference: [27] <author> Michael Wu and Willy Zwaenepoel. eNVy: </author> <title> a Non-Volatile Main Memory Storage System. </title> <booktitle> In Proc. of the 6-th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 86-97, </pages> <year> 1994. </year> <month> 16 </month>
Reference-contexts: Thus, if used both, they improve the performance of database applications even further. 13 To speed up database and file system write performance, several researchers have proposed to use special hardware. For example, Wu and Zwaenepoel have designed and simulated eNVy <ref> [27] </ref>, a large non-volatile main memory storage system built primarily with FLASH memory. Their simulation results suggest that a 2 Gbyte eNVy system can support I/O rates corresponding to 30,000 transactions per second.
References-found: 27

