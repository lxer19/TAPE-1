URL: http://www.cs.rice.edu/~devika/papers/ants.ps
Refering-URL: http://www.cs.rice.edu/~devika/
Root-URL: 
Title: Ants and Reinforcement Learning: A Case Study in Routing in Dynamic Networks  
Author: Devika Subramanian Peter Druschel Johnny Chen 
Address: Houston, Texas 77005  
Affiliation: Department of Computer Science Rice University  
Abstract: We investigate two new distributed routing algorithms for data networks based on simple biological "ants" that explore the network and rapidly learn good routes, using a novel variation of reinforcement learning. These two algorithms are fully adaptive to topology changes and changes in link costs in the network, and have space and computational overheads that are competitive with traditional packet routing algorithms: although they can generate more routing traffic when the rate of failures in a network is low, they perform much better under higher failure rates. Both algorithms are more resilient than traditional algorithms, in the sense that random corruption of routing state has limited impact on the computation of paths. We present convergence theorems for both of our algorithms drawing on the theory of non-stationary and stationary discrete-time Markov chains over the reals. We present an extensive empirical evaluation of our algorithms on a simulator that is widely used in the computer networks community for validating and testing protocols. We present comparative results on data delivery performance, aggregate routing traffic (algorithm overhead), as well as the degree of resilience for our new algorithms and two traditional routing algorithms in current use. We also show that the performance of our algorithms scale well with increase in net work size using a realistic topology.
Abstract-found: 1
Intro-found: 1
Reference: [ Beckers et al., 1992 ] <author> R. Beckers, J. L. Deneuborg, and S. Goss. </author> <title> Trails and U turns in the selection of a path by the Ant lasius niger. </title> <journal> Journal of Theoretical Biology, </journal> <volume> 159 </volume> <pages> 397-415, </pages> <year> 1992. </year>
Reference-contexts: In this paper, we investigate two algorithms for this problem inspired by the dynamics of how ant colonies learn the shortest routes to food sources, using very little state and computation <ref> [ Beckers et al., 1992 ] </ref> . The first algorithm, which we call the regular ant algorithm, is based on earlier work by Holland et. al. [ Schooenderwoerd et al., 1996 ] for call routing in telephone networks.
Reference: [ Coltun, 1989 ] <author> R. Coltun. OSPF: </author> <title> an Internet routing protocol. </title> <journal> ConneXions, </journal> <volume> 3(8) </volume> <pages> 19-25, </pages> <year> 1989. </year>
Reference-contexts: By dynamic we mean a network subject to frequent and unpredictable changes in topology and link costs (e.g., due to congestion). One of the most widely used solutions for routing on such networks <ref> [ Coltun, 1989 ] </ref> places substantial space and time requirements on all the routers to guarantee effective performance, thus limiting its scalability. <p> Link state algorithms require all nodes to know the topology of the network before computing shortest paths. Link state methods have each node periodically broadcast its local topology and costs to the entire network. The Internet Open Shortest First (OSPF) Protocol <ref> [ Coltun, 1989 ] </ref> uses a link state algorithm. 3 Ant Routing Algorithms To facilitate the description of our algorithms we distinguish between two types of nodes on the network: hosts and routers. Hosts are communication end-points, that is, they can initiate and terminate data messages 1 .
Reference: [ Kaelbling et al., 1996 ] <author> L. Kaelbling, M. Littman, and A. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of AI Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: The update rules are similar in spirit to those in traditional reinforcement learning algorithms <ref> [ Kaelbling et al., 1996 ] </ref> ; the key technical difference is that the update rules are non-linear. <p> Note that the router's probabilities to the host which generated the ant are updated. Ants perform a form of backward learning. Unlike traditional adaptive algorithms in this context, including <ref> [ Littman and Boyan, 1994; Kaelbling et al., 1996 ] </ref> the extent of the reinforcement is not the direct cost c of the ant, rather it is a decreasing non-linear function 4 of c.
Reference: [ Littman and Boyan, 1994 ] <author> M. Littman and J. Boyan. </author> <title> Packet routing in dynamically changing networks: A reinforcement learning approach. </title> <booktitle> In Proceedings of NIPS-94, </booktitle> <year> 1994. </year>
Reference-contexts: Note that the router's probabilities to the host which generated the ant are updated. Ants perform a form of backward learning. Unlike traditional adaptive algorithms in this context, including <ref> [ Littman and Boyan, 1994; Kaelbling et al., 1996 ] </ref> the extent of the reinforcement is not the direct cost c of the ant, rather it is a decreasing non-linear function 4 of c. <p> With link-state and DV, the routing traffic increases more than linearly with the amount of change. This demonstrates one of the key advantages of ant algorithms over LS and DV. 5 Related Work 5.1 Q-Routing and Ants In Q-Routing <ref> [ Littman and Boyan, 1994 ] </ref> , each node keeps Q-values of the form Q x (d; y), representing node x's cost estimate to d via neighbor y. <p> The regular ant algorithm can be shown to always converge to the single shortest path. To illustrate this point, consider Figure 5, which shows the packet delivery delay for Q-Routing and regular ants in the 6x6 irregular network used in <ref> [ Littman and Boyan, 1994 ] </ref> .
Reference: [ Schooenderwoerd et al., 1996 ] <author> R. Schooenderwoerd, O. Holland, J. Bruten, and L. Rosenkrantz. </author> <title> Ants for load balancing in telecommunication networks. </title> <type> Technical Report HPL-96-35, </type> <institution> HP Labs, Bristol, </institution> <year> 1996. </year>
Reference-contexts: The first algorithm, which we call the regular ant algorithm, is based on earlier work by Holland et. al. <ref> [ Schooenderwoerd et al., 1996 ] </ref> for call routing in telephone networks. The regular ant algorithm is a single shortest path algorithm and is only applicable to networks with symmetric path costs. <p> These update rules are drawn from <ref> [ Schooenderwoerd et al., 1996 ] </ref> . The constant k is called the learning rate of the algorithm. It is generally less than 0.1. <p> We use the solution in <ref> [ Schooenderwoerd et al., 1996 ] </ref> to allow regular ants to perform a certain degree of random exploration.
Reference: [ Subramanian et al., 1997 ] <author> D. Subramanian, P. Dr-uschel, and J. Chen. </author> <title> Ants and reinforcement learning: A case study in routing in dynamic networks. </title> <type> Technical Report TR-97-10, </type> <institution> Rice University, </institution> <year> 1997. </year>
Reference-contexts: The system of equations above form a non-stationary discrete-time Markov chain on the reals. Space considerations prevent us from including the proof which can be found in our technical report <ref> [ Subramanian et al., 1997 ] </ref> . This proposition can be generalized to any symmetric network to show that regular ants will converge to the shortest path 5 . 5 the shortest path is with respect to any specified link cost metric.
References-found: 6

