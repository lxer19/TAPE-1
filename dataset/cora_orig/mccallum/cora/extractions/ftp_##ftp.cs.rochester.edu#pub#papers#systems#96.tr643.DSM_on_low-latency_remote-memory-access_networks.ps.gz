URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/96.tr643.DSM_on_low-latency_remote-memory-access_networks.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/gchunt/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: cashmere@cs.rochester.edu  
Title: VM-Based Shared Memory on Low-Latency, Remote-Memory-Access Networks 1  
Author: Leonidas Kontothanassis Galen Hunt, Robert Stets, Nikolaos Hardavellas, Micha Cierniak, Srinivasan Parthasarathy, Wagner Meira, Jr., Sandhya Dwarkadas, and Michael Scott 
Note: 1 This work was supported in part by NSF grants CDA-9401142, CCR-9319445, CCR-9409120, and CCR-9510173; ARPA contract F19628-94-C-0057; an external research grant from Digital Equipment Corporation; and graduate fellowships from Microsoft Research (Galen Hunt) and CNPq-Brazil (Wagner Meira, Jr., Grant 200.862/93-6).  
Date: November 1996  
Address: Rochester One Kendall Sq., Bldg. 700 Rochester, NY 14627-0226 Cambridge, MA 02139  
Affiliation: Department of Computer Science 2 DEC Cambridge Research Lab University of  
Pubnum: Technical Report 643  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> C. Amza, A. L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, W. Yu, and W. Zwaenepoel. TreadMarks: </author> <title> Shared Memory Computing on Networks of Workstations. </title> <note> In Computer, to appear. </note>
Reference-contexts: 1 Introduction Distributed shared memory (DSM) is an attractive design alternative for large-scale shared memory multiprocessing. Traditional DSM systems rely on virtual memory hardware and simple message passing to implement shared memory. State-of-the-art DSM systems (e.g. TreadMarks <ref> [1, 19] </ref>) employ sophisticated protocol optimizations, such as relaxed consistency models, multiple writable copies of a page, and lazy processing of all coherence-related events. <p> Further information on Tread-Marks can be found in other papers <ref> [1] </ref>. 3 Implementation Issues 3.1 Memory Channel mapped for both transmit and receive on node 1 and for receive on node 2. The gray region is mapped for receive on node 1 and for transmit on node 2.
Reference: [2] <author> A. W. Appel and K. Li. </author> <title> Virtual Memory Primitives for User Programs. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 96-107, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: This capability should be useful not only for DSM, but for any application that uses virtual memory protection to catch accesses to particular pages <ref> [2] </ref>. At a more aggressive level, we plan to evaluate the performance impact of embedding the page-fault handling software directly in the kernel. We are also continuing our evaluation of protocol alternatives.
Reference: [3] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings 13 of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Rather than rely on VM, however, it inserts consistency checks in-line when accessing shared memory. Aggressive compiler optimizations attempt to keep the cost of checks as low as possible. AURC [17] is a multi-writer protocol designed for the Shrimp network interface <ref> [3] </ref>. Like TreadMarks, AURC uses distributed information in the form of timestamps and write notices to maintain sharing information. Like Cashmere, it relies on remote memory access to write shared data updates to home nodes. <p> In some sys tems, such as Split-C [11] and Shrimp's Deliberate Update <ref> [3] </ref>, the programmer must use special primitives to read and write remote data. In others, including Shared Regions [31], Cid [28], and CRL [18], remote data is accessed with the same notation used for local data, but only in regions of code that have been bracketed by special operations. <p> Fast user-level messages were supported without shared memory on the CM-5, though the protection mechanism was relatively static. Among workstation networks, user-level IPC can also be found in the Princeton Shrimp <ref> [3] </ref>, the HP Hamlyn interface [6] to Myrinet [4], and Dolphin's snooping interface [26] for the SCI cache coherence protocol [16]. 6 Conclusion and Future Work We have presented results for two different DSM protocolsCashmere and TreadMarkson a remote-memory-access network, namely DEC's Memory Channel.
Reference: [4] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. E. Seitz, J. N. Seizovic, and W.-K. Su. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <booktitle> In IEEE Micro, </booktitle> <pages> pages 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Fast user-level messages were supported without shared memory on the CM-5, though the protection mechanism was relatively static. Among workstation networks, user-level IPC can also be found in the Princeton Shrimp [3], the HP Hamlyn interface [6] to Myrinet <ref> [4] </ref>, and Dolphin's snooping interface [26] for the SCI cache coherence protocol [16]. 6 Conclusion and Future Work We have presented results for two different DSM protocolsCashmere and TreadMarkson a remote-memory-access network, namely DEC's Memory Channel.
Reference: [5] <author> W. J. Bolosky, R. P. Fitzgerald, and M. L. Scott. </author> <title> Simple But Effective Techniques for NUMA Memory Management. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 19-31, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: Nitzberg and Lo [29] provide a survey of early VM-based systems. Several groups employed similar techniques to migrate and replicate pages in early, cache-less shared-memory multiprocessors <ref> [5, 10, 22] </ref>. Lazy, multi-writer protocols were pioneered by Keleher et al. [19], and later adopted by several other groups. Several of the ideas in Cashmere were based on Petersen's coherence algorithms for small-scale, non-hardware-coherent multiprocessors [30].
Reference: [6] <author> G. Buzzard, D. Jacobson, M. Mackey, S. Marovich, and J. Wilkes. </author> <title> An Implementation of the Hamlyn Sender-Managed Interface Architecture. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <address> Seattle, WA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Fast user-level messages were supported without shared memory on the CM-5, though the protection mechanism was relatively static. Among workstation networks, user-level IPC can also be found in the Princeton Shrimp [3], the HP Hamlyn interface <ref> [6] </ref> to Myrinet [4], and Dolphin's snooping interface [26] for the SCI cache coherence protocol [16]. 6 Conclusion and Future Work We have presented results for two different DSM protocolsCashmere and TreadMarkson a remote-memory-access network, namely DEC's Memory Channel.
Reference: [7] <author> N. Carriero and D. Gelernter. </author> <title> Linda in Context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year> <title> Relevant correspondence appears in Volume 32, Number 10. </title>
Reference-contexts: Several other systems use the member functions of an object-oriented programming model to trigger coherence operations [8, 14, 36], or support inter-process communication via sharing of special concurrent objects <ref> [7, 32] </ref>. Because they provide the coherence system with information not available in more general-purpose systems, special programming models have the potential to provide superior performance. It is not yet clear to what extent the extra effort required of programmers will be considered an acceptable burden.
Reference: [8] <author> J. S. Chase, F. G. Amador, E. D. Lazowska, H. M. Levy, and R. J. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: The Midway system [40] requires the programmer to associate shared data with synchronization objects, allowing ordinary synchronization acquires and releases to play the role of the bracketing operations. Several other systems use the member functions of an object-oriented programming model to trigger coherence operations <ref> [8, 14, 36] </ref>, or support inter-process communication via sharing of special concurrent objects [7, 32]. Because they provide the coherence system with information not available in more general-purpose systems, special programming models have the potential to provide superior performance.
Reference: [9] <author> R. W. Cottingham Jr., R. M. Idury, and A. A. Schaffer. </author> <title> Faster Sequential Genetic Linkage Computations. </title> <journal> American Journal of Human Genetics, </journal> <volume> 53 </volume> <pages> 252-263, </pages> <year> 1993. </year>
Reference-contexts: If there were any column operations performed in the first step these operations (which are stored in the vector SWAP) have to be carried out in reverse order on the vector X to calculate the final result. ILINK: ILINK <ref> [9, 23] </ref> is a widely used genetic linkage analysis program that locates disease genes on chromosomes. The input consists of several family trees. The main data structure is a pool of sparse genarrays. A genarray contains an entry for the probability of each genotype for an individual.
Reference: [10] <author> A. L. Cox and R. J. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: Nitzberg and Lo [29] provide a survey of early VM-based systems. Several groups employed similar techniques to migrate and replicate pages in early, cache-less shared-memory multiprocessors <ref> [5, 10, 22] </ref>. Lazy, multi-writer protocols were pioneered by Keleher et al. [19], and later adopted by several other groups. Several of the ideas in Cashmere were based on Petersen's coherence algorithms for small-scale, non-hardware-coherent multiprocessors [30].
Reference: [11] <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Phase (1) executes on only the master process. Parallel computation in the other phases is dynamically balanced using the cost-zone method, with most of the computation time spent in phase (3). Synchronization consists of barriers between phases. Em3d: a program to simulate electromagnetic wave propagation through 3D objects <ref> [11] </ref>. The major data structure is an array that contains the set of magnetic and electric nodes. These are equally distributed among the processors in the system. Dependencies between nodes are static; most of them are among nodes that belong to the same processor. <p> In some sys tems, such as Split-C <ref> [11] </ref> and Shrimp's Deliberate Update [3], the programmer must use special primitives to read and write remote data.
Reference: [12] <author> S. Dwarkadas, A. A. Schaffer, R. W. Cottingham Jr., A. L. Cox, P. Keleher, and W. Zwaenepoel. </author> <title> Parallelization of General Linkage Analysis Problems. </title> <booktitle> Human Heredity, </booktitle> <volume> 44 </volume> <pages> 127-141, </pages> <year> 1994. </year>
Reference-contexts: The computation either updates a parent's genarray conditioned on the spouse and all children, or updates one child conditioned on both parents and all the other siblings. We use the parallel algorithm described by Dwarkadas et al. <ref> [12] </ref>. Updates to each individual's genarray are parallelized. A master processor assigns individual genarray elements to processors in a round robin fashion in order to improve load balance. After each processor has updated its elements, the master processor sums the contributions.
Reference: [13] <author> S. Dwarkadas, A. L. Cox, and W. Zwaenepoel. </author> <title> An Integrated Compile-Time/Run-Time Software Distributed Shared Memory System. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: It is not yet clear to what extent the extra effort required of programmers will be considered an acceptable burden. In some cases, it may be possible for an optimizing compiler to obtain the performance of the special programming model without the special syntax <ref> [13] </ref>. 5.3 Fast User-Level Messages The Memory Channel is not unique in its support for user-level messages, though it is the first commercially-available workstation network with such an interface. <p> Finally, we are continuing our research into the relationship between run-time coherence management and static compiler analysis <ref> [13] </ref>.
Reference: [14] <author> M. J. Feeley, J. S. Chase, V. R. Narasayya, and H. M. Levy. </author> <title> Integrating Coherency and Recovery in Distributed Systems. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: The Midway system [40] requires the programmer to associate shared data with synchronization objects, allowing ordinary synchronization acquires and releases to play the role of the bracketing operations. Several other systems use the member functions of an object-oriented programming model to trigger coherence operations <ref> [8, 14, 36] </ref>, or support inter-process communication via sharing of special concurrent objects [7, 32]. Because they provide the coherence system with information not available in more general-purpose systems, special programming models have the potential to provide superior performance.
Reference: [15] <author> R. Gillett. </author> <title> Memory Channel: An Optimized Cluster Interconnect. </title> <journal> IEEE Micro, </journal> <volume> 16(2), </volume> <month> February </month> <year> 1996. </year>
Reference-contexts: In this paper we compare implementations of Cashmere and TreadMarks on a 32-processor cluster (8 nodes, 4 processors each) of DEC AlphaServers, connected by DEC's Memory Channel <ref> [15] </ref> network. Memory Channel allows a user-level application to write to the memory of remote nodes. The remote-write capability can be used for (non-coherent) shared memory, for broadcast/multicast, and for very fast user-level messages. Remote reads are not directly supported.
Reference: [16] <author> D. B. Gustavson. </author> <title> The Scalable Coherent Interface and Related Standards Projects. </title> <journal> IEEE Micro, </journal> <volume> 12(2) </volume> <pages> 10-22, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Among workstation networks, user-level IPC can also be found in the Princeton Shrimp [3], the HP Hamlyn interface [6] to Myrinet [4], and Dolphin's snooping interface [26] for the SCI cache coherence protocol <ref> [16] </ref>. 6 Conclusion and Future Work We have presented results for two different DSM protocolsCashmere and TreadMarkson a remote-memory-access network, namely DEC's Memory Channel. TreadMarks uses the Memory Channel only for fast messaging, while Cashmere uses it for directory maintenance and for fine-grained updates to shared data.
Reference: [17] <author> L. Iftode, C. Dubnicki, E. W. Felten, and K. Li. </author> <title> Improving Release-Consistent Shared Virtual Memory Using Automatic Update. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Computer Architecture, </booktitle> <address> San Jose, CA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: Like Cashmere, Shasta runs on the Memory Channel, with polling for remote requests. Rather than rely on VM, however, it inserts consistency checks in-line when accessing shared memory. Aggressive compiler optimizations attempt to keep the cost of checks as low as possible. AURC <ref> [17] </ref> is a multi-writer protocol designed for the Shrimp network interface [3]. Like TreadMarks, AURC uses distributed information in the form of timestamps and write notices to maintain sharing information. Like Cashmere, it relies on remote memory access to write shared data updates to home nodes. <p> We are also continuing our evaluation of protocol alternatives. Cashmere and TreadMarks differ in two fundamental dimensions: the mechanism used to manage coherence information (directories v. distributed intervals and timestamps) and the mechanism used to collect data updates (write-through v. twins and diffs). The Princeton AURC protocol <ref> [17] </ref> combines the TreadMarks approach to coherence management with the Cashmere approach to write collection. We plan to implement AURC on the Memory Channel, and to compare it to Cashmere, to TreadMarks, and to the fourth alternative, which would combine directories with twins and diffs (or with software dirty bits).
Reference: [18] <author> K. L. Johnson, M. F. Kaashoek, and D. A. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <address> Copper Mountain, CO, </address> <month> De-cember </month> <year> 1995. </year>
Reference-contexts: In some sys tems, such as Split-C [11] and Shrimp's Deliberate Update [3], the programmer must use special primitives to read and write remote data. In others, including Shared Regions [31], Cid [28], and CRL <ref> [18] </ref>, remote data is accessed with the same notation used for local data, but only in regions of code that have been bracketed by special operations.
Reference: [19] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the Nineteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <address> Gold Coast, Aus-tralia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Distributed shared memory (DSM) is an attractive design alternative for large-scale shared memory multiprocessing. Traditional DSM systems rely on virtual memory hardware and simple message passing to implement shared memory. State-of-the-art DSM systems (e.g. TreadMarks <ref> [1, 19] </ref>) employ sophisticated protocol optimizations, such as relaxed consistency models, multiple writable copies of a page, and lazy processing of all coherence-related events. <p> More detailed information on the Cashmere protocol (and its network-interface-specific variants) can be found in other papers [21, 20]. 2.2 TreadMarks TreadMarks is a distributed shared memory system based on lazy release consistency (LRC) <ref> [19] </ref>. Lazy release consistency is a variant of release consistency [24]. It guarantees memory consistency only at synchronization points and permits multiple writers per coherence block. Lazy release consistency divides time on each node into intervals delineated by remote synchronization operations. <p> Nitzberg and Lo [29] provide a survey of early VM-based systems. Several groups employed similar techniques to migrate and replicate pages in early, cache-less shared-memory multiprocessors [5, 10, 22]. Lazy, multi-writer protocols were pioneered by Keleher et al. <ref> [19] </ref>, and later adopted by several other groups. Several of the ideas in Cashmere were based on Petersen's coherence algorithms for small-scale, non-hardware-coherent multiprocessors [30]. Recent work by the Alewife group at MIT has addressed the implementation of software coherence on a collection of hardware-coherent nodes [39].
Reference: [20] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> High Performance Software Coherence for Current and Future Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 29(2) </volume> <pages> 179-195, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Simulation studies indicate that on an ideal remote-write network Cashmere will significantly outperform other DSM approaches, and will in fact approach the performance of full hardware cache coherence <ref> [20, 21] </ref>. In this paper we compare implementations of Cashmere and TreadMarks on a 32-processor cluster (8 nodes, 4 processors each) of DEC AlphaServers, connected by DEC's Memory Channel [15] network. Memory Channel allows a user-level application to write to the memory of remote nodes. <p> We assign home nodes at run-time, based on which processor first touches a page after the program has completed any initialization phase [27]. More detailed information on the Cashmere protocol (and its network-interface-specific variants) can be found in other papers <ref> [21, 20] </ref>. 2.2 TreadMarks TreadMarks is a distributed shared memory system based on lazy release consistency (LRC) [19]. Lazy release consistency is a variant of release consistency [24]. It guarantees memory consistency only at synchronization points and permits multiple writers per coherence block.
Reference: [21] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> Using Memory-Mapped Network Interfaces to Improve the Performance of Distributed Shared Memory. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Computer Architecture, </booktitle> <address> San Jose, CA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: Simulation studies indicate that on an ideal remote-write network Cashmere will significantly outperform other DSM approaches, and will in fact approach the performance of full hardware cache coherence <ref> [20, 21] </ref>. In this paper we compare implementations of Cashmere and TreadMarks on a 32-processor cluster (8 nodes, 4 processors each) of DEC AlphaServers, connected by DEC's Memory Channel [15] network. Memory Channel allows a user-level application to write to the memory of remote nodes. <p> Our write-doubling mechanism increases the first-level working set for certain applications beyond the 16K available, dramatically reducing performance. The larger caches of the 21264 should largely eliminate this problem. We are optimistic about the future of Cashmere-like systems as network interfaces continue to evolve. Based on previous simulations <ref> [21] </ref>, it is in fact somewhat surprising that Cashmere performs as well as it does on the current generation of hardware. The second-generation Memory Channel, due on the market very soon, will have something like half the latency, and an order of magnitude more bandwidth. <p> We assign home nodes at run-time, based on which processor first touches a page after the program has completed any initialization phase [27]. More detailed information on the Cashmere protocol (and its network-interface-specific variants) can be found in other papers <ref> [21, 20] </ref>. 2.2 TreadMarks TreadMarks is a distributed shared memory system based on lazy release consistency (LRC) [19]. Lazy release consistency is a variant of release consistency [24]. It guarantees memory consistency only at synchronization points and permits multiple writers per coherence block.
Reference: [22] <author> R. P. LaRowe Jr. and C. S. Ellis. </author> <title> Experimental Comparison of Memory Management Policies for NUMA Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(4) </volume> <pages> 319-363, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Nitzberg and Lo [29] provide a survey of early VM-based systems. Several groups employed similar techniques to migrate and replicate pages in early, cache-less shared-memory multiprocessors <ref> [5, 10, 22] </ref>. Lazy, multi-writer protocols were pioneered by Keleher et al. [19], and later adopted by several other groups. Several of the ideas in Cashmere were based on Petersen's coherence algorithms for small-scale, non-hardware-coherent multiprocessors [30].
Reference: [23] <author> G. M. Lathrop, J. M. Lalouel, C. Julier, and J. Ott. </author> <title> Strategies for Multilocus Linkage Analysis in Humans. </title> <booktitle> Proceedings of the National Academy of Science, USA, </booktitle> <volume> 81 </volume> <pages> 3443-3446, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: If there were any column operations performed in the first step these operations (which are stored in the vector SWAP) have to be carried out in reverse order on the vector X to calculate the final result. ILINK: ILINK <ref> [9, 23] </ref> is a widely used genetic linkage analysis program that locates disease genes on chromosomes. The input consists of several family trees. The main data structure is a pool of sparse genarrays. A genarray contains an entry for the probability of each genotype for an individual.
Reference: [24] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the Seventeenth International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: More detailed information on the Cashmere protocol (and its network-interface-specific variants) can be found in other papers [21, 20]. 2.2 TreadMarks TreadMarks is a distributed shared memory system based on lazy release consistency (LRC) [19]. Lazy release consistency is a variant of release consistency <ref> [24] </ref>. It guarantees memory consistency only at synchronization points and permits multiple writers per coherence block. Lazy release consistency divides time on each node into intervals delineated by remote synchronization operations.
Reference: [25] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: them into systems that support more-or-less generic shared-memory programs, such as might run on a machine with hardware coherence, and those that require a special programming notation or style. 5.1 Generic DSM The original idea of using virtual memory to implement coherence on networks dates from Kai Li's thesis work <ref> [25] </ref>. Nitzberg and Lo [29] provide a survey of early VM-based systems. Several groups employed similar techniques to migrate and replicate pages in early, cache-less shared-memory multiprocessors [5, 10, 22]. Lazy, multi-writer protocols were pioneered by Keleher et al. [19], and later adopted by several other groups.
Reference: [26] <author> O. Lysne, S. Gjessing, and K. Lochsen. </author> <title> Running the SCI Protocol over HIC Networks. </title> <booktitle> In Second International Workshop on SCI-based Low-cost/High-performance Computing (SCIzzL-2), </booktitle> <address> Santa Clara, CA, </address> <month> March </month> <year> 1995. </year> <month> 14 </month>
Reference-contexts: Fast user-level messages were supported without shared memory on the CM-5, though the protection mechanism was relatively static. Among workstation networks, user-level IPC can also be found in the Princeton Shrimp [3], the HP Hamlyn interface [6] to Myrinet [4], and Dolphin's snooping interface <ref> [26] </ref> for the SCI cache coherence protocol [16]. 6 Conclusion and Future Work We have presented results for two different DSM protocolsCashmere and TreadMarkson a remote-memory-access network, namely DEC's Memory Channel.
Reference: [27] <author> M. Marchetti, L. Kontothanassis, R. Bianchini, and M. L. Scott. </author> <title> Using Simple Page Placement Policies to Reduce the Cost of Cache Fills in Coherent Shared-Memory Systems. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: The home node itself can access the page directly, while the remaining processors have to use the slower Memory Channel interface. We assign home nodes at run-time, based on which processor first touches a page after the program has completed any initialization phase <ref> [27] </ref>. More detailed information on the Cashmere protocol (and its network-interface-specific variants) can be found in other papers [21, 20]. 2.2 TreadMarks TreadMarks is a distributed shared memory system based on lazy release consistency (LRC) [19]. Lazy release consistency is a variant of release consistency [24].
Reference: [28] <author> R. S. Nikhil. Cid: </author> <title> A Parallel, Shared-memory C for Distributed-Memory Machines. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: In some sys tems, such as Split-C [11] and Shrimp's Deliberate Update [3], the programmer must use special primitives to read and write remote data. In others, including Shared Regions [31], Cid <ref> [28] </ref>, and CRL [18], remote data is accessed with the same notation used for local data, but only in regions of code that have been bracketed by special operations.
Reference: [29] <author> B. Nitzberg and V. Lo. </author> <title> Distributed Shared Memory: A Survey of Issues and Algorithms. </title> <journal> Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Nitzberg and Lo <ref> [29] </ref> provide a survey of early VM-based systems. Several groups employed similar techniques to migrate and replicate pages in early, cache-less shared-memory multiprocessors [5, 10, 22]. Lazy, multi-writer protocols were pioneered by Keleher et al. [19], and later adopted by several other groups.
Reference: [30] <author> K. Petersen and K. Li. </author> <title> Cache Coherence for Shared Memory Multiprocessors Based on Virtual Memory Support. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <address> Newport Beach, CA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Lazy, multi-writer protocols were pioneered by Keleher et al. [19], and later adopted by several other groups. Several of the ideas in Cashmere were based on Petersen's coherence algorithms for small-scale, non-hardware-coherent multiprocessors <ref> [30] </ref>. Recent work by the Alewife group at MIT has addressed the implementation of software coherence on a collection of hardware-coherent nodes [39]. Wisconsin's Blizzard system [34] maintains coherence for cache-line-size blocks, either in software or by using ECC.
Reference: [31] <author> H. S. Sandhu, B. Gamsa, and S. Zhou. </author> <title> The Shared Regions Approach to Software Cache Coherence on Multiprocessors. </title> <booktitle> In Proceedings of the Fourth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In some sys tems, such as Split-C [11] and Shrimp's Deliberate Update [3], the programmer must use special primitives to read and write remote data. In others, including Shared Regions <ref> [31] </ref>, Cid [28], and CRL [18], remote data is accessed with the same notation used for local data, but only in regions of code that have been bracketed by special operations.
Reference: [32] <author> D. J. Scales and M. S. Lam. </author> <title> The Design and Evaluation of a Shared Object System for Distributed Memory Machines. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Several other systems use the member functions of an object-oriented programming model to trigger coherence operations [8, 14, 36], or support inter-process communication via sharing of special concurrent objects <ref> [7, 32] </ref>. Because they provide the coherence system with information not available in more general-purpose systems, special programming models have the potential to provide superior performance. It is not yet clear to what extent the extra effort required of programmers will be considered an acceptable burden.
Reference: [33] <author> D. J. Scales, K. Gharachorloo, and C. A. Thekkath. </author> <note> Shasta: </note>
Reference-contexts: Wisconsin's Blizzard system [34] maintains coherence for cache-line-size blocks, either in software or by using ECC. It runs on the Thinking Machines CM-5 and provides a sequentially-consistent programming model. The more recent Shasta system <ref> [33] </ref>, developed at DEC WRL, extends the software-based Blizzard approach with a relaxed consistency model and variable-size coherence blocks. Like Cashmere, Shasta runs on the Memory Channel, with polling for remote requests. Rather than rely on VM, however, it inserts consistency checks in-line when accessing shared memory. <p> We are currently pursuing several improvements to the Cashmere implementation. In addition to using dirty bits, we plan to experiment with hierarchical locking for directories [37] and with alternatives to write-doubling based on twins and diffs or on software dirty bits <ref> [33] </ref>. To reduce page fault handling overhead (both for TreadMarks and for Cashmere), we plan to modify the kernel to eliminate the need for mprotect system calls.
References-found: 33

